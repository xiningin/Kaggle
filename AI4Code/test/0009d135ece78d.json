{
	"cell_type": {
		"ddfd239c": "code",
		"c6cd22db": "code",
		"1372ae9b": "code",
		"90ed07ab": "code",
		"7f388a41": "code",
		"2843a25a": "code",
		"06dbf8cf": "code",
		"f9893819": "markdown",
		"ba55e576": "markdown",
		"39e937ec": "markdown",
		"e25aa9bd": "markdown",
		"0a226b6a": "markdown",
		"8cb8d28a": "markdown"
	},
	"source": {
		"ddfd239c": "import numpy as np # linear algebra\nimport pandas as pd # data processing,\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\nfrom sklearn.impute import SimpleImputer\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))",
		"c6cd22db": "df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf",
		"1372ae9b": "numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]\n\nlabels = df[\"diagnosis\"].factorize(['B','M'])[0]\n\nheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])",
		"90ed07ab": "def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):\n    # Scaling Data for testing\n    # data_1 = scale(data_1)\n    # data_2 = scale(data_2)\n\n    range =  np.random.randn(len(data_1))\n    plt.scatter(range, data_1, label=column_name_1, color='orange')\n    plt.scatter(range, data_2, label=column_name_2, color='green')\n    plt.title(name)\n    plt.xlabel('X-Axis')\n    plt.ylabel('Y-Axis')\n    plt.legend()\n    plt.show()\n     ",
		"7f388a41": "# Ploting data with different columns\n#####################################\ncomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Radius\", \"Worst Radius\")\ncomparison_plot_maker(numerical_data[\"perimeter_se\"], numerical_data[\"perimeter_worst\"], \"S.D Perimeter vs Worst Perimeter\", \"S.D Perimeter\", \"Worst Perimeter\")\ncomparison_plot_maker(numerical_data[\"compactness_mean\"], numerical_data[\"compactness_se\"], \"Mean Compactness vs S.D Compactness\", \"Mean Compactness\", \"S.D Compactness\")\ncomparison_plot_maker(numerical_data[\"smoothness_mean\"], numerical_data[\"smoothness_worst\"], \"Mean Smoothness vs Worst Smoothness\",\"Mean Smoothness\", \"Worst Smoothness\")\ncomparison_plot_maker(numerical_data[\"texture_se\"], numerical_data[\"texture_mean\"], \"S.D texture vs Mean Texture\", \"S.D texture\", \"Mean Texture\")\n\nprint('')\nprint('oh, it really works :O')",
		"2843a25a": "# Scaling Data\nscaler = StandardScaler()\nscaler.fit(numerical_data)\n# print(scaled_data)\n\n# Assigning Variables\nX = scaler.transform(numerical_data)\ny = labels\n\nmy_imputer = SimpleImputer()\npd.DataFrame(X).fillna(0)\nX = my_imputer.fit_transform(X)\n\nprint(\"Ignore the errors, they occurred because of NaN values\")\nprint()\nprint(\"But worry not human! The errors are fixed with Imputer >o>\")\nprint()",
		"06dbf8cf": "# 3. Implementing PCA on X (green for benign; red for malignant)\n################################################################\n\n# PCA\nPCA3=PCA(n_components=2)\n# print(X.shape)\nPCA3.fit(X)\nXPCA = PCA3.transform(X)\n# print(XPCA.shape)\n\n# Plotting\nplt.figure()\nplt.title(\"PCA\")\nplt.xlabel('X-Axis')\nplt.ylabel('Y-Axis')\n\nplt.plot(XPCA[y==0,0],XPCA[y==0,1],'g.')\nplt.plot(XPCA[y==1,0],XPCA[y==1,1],'r.')\n\nplt.show()",
		"f9893819": "# Scaling Data \u2696\nLet's scale the data so PCA can be applied",
		"ba55e576": "## Testing Plots >w>\nLet's these mystery soliving plots! :O",
		"39e937ec": "## Plotting PCA \ud83d\udcca\nThus, the sun boils down to this, the PCA is hence plotted \ud83d\ude2e",
		"e25aa9bd": "# Functions \ud83c\udf89\nNot in real life functions, but these functions hold the key to unravel the mystery of making plots :O",
		"0a226b6a": "# Importing Liberaries \ud83d\udcda\nLet's first import some cool liberaries to work with :D",
		"8cb8d28a": "# Reading Data \ud83d\udc53\nHere is everyone, reading and observing the data carefully >o>"
	}
}
