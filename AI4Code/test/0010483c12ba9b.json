{"cell_type":{"54c7cab3":"code","fe66203e":"code","7844d5f8":"code","5ce8863c":"code","4a0777c4":"code","4703bb6d":"code","4a32c095":"code","865ad516":"code","02a0be6d":"code","7f270e34":"markdown"},"source":{"54c7cab3":"%reset -f \n\nif 1:\n    # https:\/\/www.kaggle.com\/nbroad\/deberta-v2-3-fast-tokenizer\n    import shutil\n    from pathlib import Path\n\n    transformers_path = Path('\/opt\/conda\/lib\/python3.7\/site-packages\/transformers') \n    input_dir = Path('..\/input\/feedback-prize-submit-02\/deberta_v2_convert_tokenizer')\n\n    convert_file = input_dir \/ 'convert_slow_tokenizer.py'\n    conversion_path = transformers_path\/convert_file.name \n    if conversion_path.exists():\n        conversion_path.unlink() \n    shutil.copy(convert_file, transformers_path)\n    \n    deberta_v2_path = transformers_path \/ 'models' \/ 'deberta_v2' \n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n        filepath = deberta_v2_path\/filename\n        if filepath.exists():\n            filepath.unlink() \n        shutil.copy(input_dir\/filename, filepath)\n\n#----------------------------------------------------------------------------------------------------------\nimport sys\nsys.path.append('..\/input\/feedback-prize-submit-01')\nsys.path.append('..\/input\/feedback-prize-submit-02')\n\nimport os\nimport numpy as np\nimport glob\nimport pandas as pd\nfrom timeit import default_timer as timer\n\n  \nimport torch\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import *\n\nimport gc\nimport psutil \n\nimport torch.cuda.amp as amp\nis_amp   = True  #True #False\nis_cuda  = True\nis_debug = True\n\n\n#helper\ndef time_to_str(t, mode='min'): \n    if mode=='min':\n        t  = int(t)\/60\n        hr = t\/\/60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t\/\/60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError\n        \n\n#https:\/\/stackoverflow.com\/questions\/61366458\/how-to-find-memory-usage-of-kaggle-notebook\ndef memory_used_to_str():\n    pid = os.getpid()\n    processs = psutil.Process(pid)\n    memory_use = processs.memory_info()[0] \/ 2. ** 30\n    return 'use ram memory gb ' + str(np.round(memory_use, 2))\n\n#kaggle limit is 13 gb","fe66203e":"#config \n\ndiscourse_marker_to_label = {\n    'O': 0,\n    'B-Lead': 1,\n    'I-Lead': 2,\n    'B-Position': 3,\n    'I-Position': 4,\n    'B-Claim': 5,\n    'I-Claim': 6,\n    'B-Counterclaim': 7,\n    'I-Counterclaim': 8,\n    'B-Rebuttal': 9,\n    'I-Rebuttal': 10,\n    'B-Evidence': 11,\n    'I-Evidence': 12,\n    'B-Concluding Statement': 13,\n    'I-Concluding Statement': 14,\n    'IGNORE': -100,\n}\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\nnum_discourse_marker = 15 #len(label_to_discourse_marker)-1 #15\n\nlength_threshold = {\n    'Lead'                : 9,\n    'Position'            : 5,\n    'Claim'               : 3,\n    'Counterclaim'        : 6,\n    'Rebuttal'            : 4,\n    'Evidence'            : 14,\n    'Concluding Statement': 11,\n}\nprobability_threshold = {\n    'Lead'                : 0.70,\n    'Position'            : 0.55,\n    'Claim'               : 0.55,\n    'Counterclaim'        : 0.50,\n    'Rebuttal'            : 0.55,\n    'Evidence'            : 0.65,\n    'Concluding Statement': 0.70,\n}\nmax_length = 1600\n\n#-------------------------------\nsubmit_dir = ''\n\nif is_debug:\n    text_dir = '..\/input\/feedback-prize-2021\/train'\n    df = pd.read_csv('..\/input\/feedback-prize-submit-01\/train.fold10.csv')\n    valid_df = df[df['fold'] == 0].reset_index(drop=True)\n    valid_df = valid_df[:2000]\n    valid_id = valid_df['id'].unique()\n\nelse:\n    text_dir = '..\/input\/feedback-prize-2021\/test'\n    valid_id = [ f.split('\/')[-1][:-4] for f in glob.glob(text_dir+'\/*.txt') ] \n    \nsize = [os.path.getsize(text_dir+'\/%s.txt'%id) for id in valid_id] \nvalid_id = [id for id, s in sorted(zip(valid_id, size), key=lambda pair: -pair[1])]\nnum_valid = len(valid_id)\nprint('len(valid_id)',len(valid_id))\n\n#print(valid_id)\n#print([os.path.getsize(text_dir+'\/%s.txt'%id) for id in valid_id] )\n\n","7844d5f8":"#data\n\ndf_text=[]\nfor id in valid_id:\n    text_file = text_dir +'\/%s.txt'%id\n    with open(text_file, 'r') as f:\n        text = f.read()\n\n    text = text.replace(u'\\xa0', u' ')\n    text = text.rstrip()\n    text = text.lstrip()\n    df_text.append((id,text))\ndf_text = pd.DataFrame(df_text, columns=['id','text'])\nprint('df_text.shape',df_text.shape)\nprint(df_text)\n\nclass FeedbackDataset(Dataset):\n    def __init__(self, df_text, tokenizer, max_length = 1600):\n\n        self.df_text  = df_text\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.length = len(self.df_text)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d    = self.df_text.iloc[index]\n        id   = d['id']\n        text = d.text\n\n        #text to token\n        encoded = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=1600, #<todo>\n            truncation=True,\n        )\n        token_id     =  encoded['input_ids']\n        token_offset =  encoded['offset_mapping']\n\n        # add end, start token id\n        token_id = [self.tokenizer.cls_token_id] + token_id\n\n        token_id = token_id[: self.max_length - 1]\n        token_id = token_id + [self.tokenizer.sep_token_id]\n        \n        token_length = len(token_id)\n\n        # padding\n        token_mask = [1] * len(token_id)\n\n        padding_length = max_length - len(token_id)\n        if padding_length > 0:\n            if self.tokenizer.padding_side == 'right':\n                token_id    = token_id    + [self.tokenizer.pad_token_id] * padding_length\n                token_mask  = token_mask  + [0] * padding_length\n            else:\n                raise NotImplementedError\n\n        #-------------------------------------\n        r = {}\n        r['index'] = index\n        r['id'   ] = id\n        r['text' ] = text\n        r['token_offset'] = str(token_offset) #force batch loader store as list\n        r['token_id'    ] = torch.tensor(token_id,    dtype=torch.long)\n        r['token_mask'  ] = torch.tensor(token_mask,  dtype=torch.long)\n        r['token_length'] = token_length\n        return r\n\n","5ce8863c":"#net\n\nfrom bigbird_base_model import Net as BidBirdBaseNet\nfrom longformer_base_model import Net as LongformerBaseNet\nfrom bigbird_large_model import Net as BidBirdLargeNet\nfrom longformer_large_model import Net as LongformerLargeNet\nfrom funnel_medium_model import Net as FunnelMediumNet\nfrom funnel_large_model import Net as FunnelLargeNet\nfrom deberta_base_model import Net as DebertaBaseNet\nfrom deberta_large_model import Net as DebertaLargeNet\nfrom deberta_xlarge_model import Net as DebertaXLargeNet\nfrom deberta_v3_large_model import Net as DebertaV3LargeNet\n\n\n \n \n\nensemble =(\n\n   {\n        'net'  : DebertaXLargeNet,\n        'arch' : '..\/input\/feedback-prize-submit-01\/microsoft-deberta-xlarge',\n        'checkpoint' : \t[  \n                '..\/input\/feedback-prize-submit-01\/debert-xlarge-10kf-1600-fine-03-fold-1-00015000.model.pth',\n                #'..\/input\/feedback-prize-submit-01\/debert-xlarge-10kf-1600-03-fold-0-00014000.model.pth',  \n           ],\n        'batch_size' : 6,\n   },\n\n#    {\n#         'net'  : DebertaLargeNet,\n#         'arch' : '..\/input\/feedback-prize-submit-01\/microsoft-deberta-large',\n#         'checkpoint' : \t[\n#                 '..\/input\/feedback-prize-submit-01\/debert-large-10kf-1600-03-fold-0-00013000.model.pth',  \n#                 '..\/input\/feedback-prize-submit-01\/debert-large-10kf-1600-03-fold-1-00013000.model.pth',\n#                 #'..\/input\/feedback-prize-submit-02\/deberta-large-10kf-1600-ds-fold-0-00012000.model.pth',\n#             ],\n#         'batch_size' : 8,\n#    },\n \n)\nnum_model = len(ensemble)","4a0777c4":"#processing\n\ndef text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset\n\ndef word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['IGNORE'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df\n\n#evaluation for debug ----\n# https:\/\/www.kaggle.com\/cpmpml\/faster-metric-computation\n\ndef compute_overlap(predict, truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_truth   = len(truth)\n        len_predict = len(predict)\n        intersect = len(truth & predict)\n        overlap1 = intersect\/ len_truth\n        overlap2 = intersect\/ len_predict\n        return (overlap1, overlap2)\n    except:  # at least one of the input is NaN\n        return (0, 0)\n\ndef compute_f1_score_one(predict_df, truth_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    t_df = truth_df.loc[truth_df['discourse_type'] == discourse_type,   ['id', 'predictionstring']].reset_index(drop=True)\n    p_df = predict_df.loc[predict_df['class'] == discourse_type,  ['id', 'predictionstring']].reset_index(drop=True)\n\n    p_df.loc[:,'predict_id'] = p_df.index\n    t_df.loc[:,'truth_id'] = t_df.index\n    p_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in p_df['predictionstring']]\n    t_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in t_df['predictionstring']]\n\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = p_df.merge(t_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_p','_t')\n                          )\n    overlap = [compute_overlap(*predictionstring) for predictionstring in zip(joined.predictionstring_p, joined.predictionstring_t)]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['potential_TP'] = [(o[0] >= 0.5 and o[1] >= 0.5) for o in overlap]\n    joined['max_overlap' ] = [max(*o) for o in overlap]\n    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n    tp_pred_ids = joined_tp\\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','truth_id'])['predict_id'].first()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = set(joined['predict_id'].unique()) - set(tp_pred_ids)\n\n    matched_gt_ids   = joined_tp['truth_id'].unique()\n    unmatched_gt_ids = set(joined['truth_id'].unique()) -  set(matched_gt_ids)\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    f1 = TP \/ (TP + 0.5*(FP+FN))\n    return f1\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    f1_score = {}\n    for discourse_type in truth_df.discourse_type.unique():\n        f1_score[discourse_type] = compute_f1_score_one(predict_df, truth_df, discourse_type)\n    #f1 = np.mean([v for v in class_scores.values()])\n    return f1_score\n\n","4703bb6d":"## main submission function !!!!\n\n\ndef run_submit():\n    if is_debug: print(\"THIS IS DEBUG ####################################\")\n    all_time = 0\n    print('start', memory_used_to_str())\n\n    ensemble_result = []\n    for m in range(num_model):\n        model = ensemble[m]\n        num_net = len(model['checkpoint'])\n\n        net = model['net'](model['arch'])\n        tokenizer = net.get_tokenizer()\n\n        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)\n        valid_loader  = DataLoader(\n            valid_dataset,\n            sampler = SequentialSampler(valid_dataset),\n            batch_size  = model['batch_size'], #4, #\n            drop_last   = False,\n            num_workers = 2, #0, #\n            pin_memory  = False,\n            #collate_fn = null_collate_fn,\n        )\n\n\n        model_result = []\n        for n in range(num_net):\n            net.load_state_dict(torch.load(model['checkpoint'][n], map_location=lambda storage, loc: storage)['state_dict'],strict=False)\n            #net.half()\n            net.eval()\n            net.cuda()\n            print('load ok : [model=%d, net=%d] %s : %s'%(m, n, net.arch, model['checkpoint'][n].split('\/')[-1]))\n            print('         ', memory_used_to_str())\n            #---\n\n            net_result  = {\n                'token_offset':[],\n                'probability' :[],\n            }\n\n            T=0\n            start_timer = timer()\n            for t, batch in enumerate(valid_loader):\n                batch_size = len(batch['index'])\n                length = batch['token_length'].max().item()\n                token_mask_short = batch['token_mask'][:,:length]\n                token_id_short   = batch['token_id'][:,:length]\n                if is_cuda:\n                    token_mask_short = token_mask_short.cuda()\n                    token_id_short = token_id_short.cuda()\n\n                with torch.no_grad():\n                    with amp.autocast(enabled=is_amp):\n                        probability = data_parallel(net,(token_id_short, token_mask_short))\n\n                        probability = (probability*255).byte().data.cpu().numpy()\n                        probability = np.pad(probability, ((0, 0),(0,max_length-length),(0,0)), 'constant', constant_values=0)\n                        net_result['probability'].append( probability )\n                        net_result['token_offset' ] += [eval(x) for x in batch['token_offset']]\n\n                T += batch_size\n                print('\\r\\t%d\/%d  %s'%(T, len(valid_dataset), time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n            #------------\n            all_time += timer() - start_timer\n            print('')\n\n            model_result.append({\n                'probability' : np.concatenate(net_result['probability']),\n                'token_offset': np.array(net_result['token_offset'], object)\n            })\n\n            # sys.getsizeof(model_result[0]['probability'])\/ 2. ** 30\n            # model_result[0]['probability'].shape (202, 1600, 15)\n\n            #del net_result['probability'], net_result['token_offset']\n            gc.collect()\n            if is_cuda: torch.cuda.empty_cache()\n            #print('         ', memory_used_to_str())\n            print('gc.collect()', memory_used_to_str())\n            print('')\n\n        #--------------------------------------------\n        #average\n        probability = 0\n        for n in range(num_net):\n            probability += model_result[n]['probability']\/255.0\n        probability = probability\/num_net\n        probability = (probability*255).astype(np.uint8)\n        ensemble_result.append({\n            'probability' : probability,\n            'token_offset': model_result[0]['token_offset']\n        })\n\n\n    #------------------------------------------------------------------------\n\n    submit_df = []\n    for i in range(num_valid):\n        d  = df_text.iloc[i]\n        id = d.id\n        text = d.text\n        word, word_offset = text_to_word(text)\n        #print(i,id[i], len(text), len(word))\n\n        #ensemble -----\n        token_to_text_probability = np.full((len(text),num_discourse_marker),0, np.float32)\n        for m in range(num_model):\n            p = ensemble_result[m]['probability'][i][1:]\/255\n            for t,(start,end) in enumerate(ensemble_result[m]['token_offset'][i]):\n                if t==max_length-1: break #assume max_length, else use token_mask to get length\n                token_to_text_probability[start:end]+=p[t] #**0.5\n        token_to_text_probability = token_to_text_probability\/num_model\n        #ensemble -----\n\n\n        text_to_word_probability = np.full((len(word),num_discourse_marker),0, np.float32)\n        for t,(start,end) in enumerate(word_offset):\n            text_to_word_probability[t]=token_to_text_probability[start:end].mean(0)\n\n        predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n        submit_df.append(predict_df)\n        #print('\\r preparing submit_df :', i, id, len(text), len(word), end ='', flush=True)\n    print('')\n\n    #----------------------------------------\n    submit_df = pd.concat(submit_df).reset_index(drop=True)\n    submit_df.to_csv(submit_dir + '\/submission.csv', index=False)\n    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n    submit_df.to_csv('submission.csv', index=False)\n\n    print('------------------')\n    for t in range(3): print(submit_df.iloc[t],'\\n')\n    print('submission ok!----')\n\n    if is_debug:\n        print(\"THIS IS DEBUG ####################################\")\n        f1_score = compute_lb_f1_score(submit_df, valid_df)\n        print('f1 macro : %f\\n' % np.mean([v for v in f1_score.values()]))\n        for k,v in f1_score.items():\n            print('%20s : %05f'%(k,v))\n        print('')\n\n        #all_time = timer() - start_timer0\n        print('all_time : %s'%(time_to_str(all_time,'sec')))\n        print('estimated for 10k text files : %s'%(time_to_str(all_time\/len(valid_id)*10_000,'min')))\n\n","4a32c095":"#check function\ndef run_check_dataset():\n\n    tokenizer = net[0].get_tokenizer()\n    dataset = FeedbackDataset(df_text, tokenizer, max_length)\n\n    for i in range(5):\n        r = dataset[i]\n        print(r['index'],'-----------')\n        for k in ['token_id', 'token_mask']:\n            v = r[k]\n            print(k)\n            print('\\t',v.shape, v.is_contiguous())\n            print('\\t',v)\n        print('') ","865ad516":"# '''\n# cross validation results \n# WITHOUT SORTED TEXT INPUT #############################################\n# ..\/input\/feedback-prize-submit-01\/microsoft-deberta-large ( one model )\n# 202\/202   1 min 36 sec\n\n# f1 macro : 0.680797\n# estimated for 10k text files :  1 hr 19 min\n\n# ----\n# ..\/input\/feedback-prize-submit-01\/microsoft-deberta-xlarge ( one model )\n# 202\/202   3 min 10 sec\n\n# f1 macro : 0.687624\n# estimated for 10k text files :  2 hr 36 min\n\n\n# WITH SORTED TEXT INPUT #############################################\n\n# ..\/input\/feedback-prize-submit-01\/microsoft-deberta-xlarge ( one model )\n# 202\/202   0 min 59 sec\n    \n    \n# f1 macro : 0.687624\n# estimated for 10k text files :  0 hr 49 min","02a0be6d":"#run_check_dataset()\nrun_submit()","7f270e34":"This notebook illustrate how to speedup inference by :\n\n    - sort input text from decreasing length\n    \n    - each batch has samples of similar token lengths. we pad each sample to the longest length in the batch.\n    \nsince most of the input text are short, this method significantly speedup inference when compared to methdos that uses long fxied length padding.\n\nmake sure you will have to train your model to be robust against different length input. (which shouldn't be an issue since transformer has input attention mask)\n\nwith this code, i can run3x deberta-xlarge and 2x deberta-large model  (total 5 models) in under 3 hrs on the kaggle test set.\n    "}}