{"cell_type":{"aafc3d23":"code","80e077ec":"code","b190ebb4":"code","ed415c3c":"code","322850af":"code","c069ed33":"code","868c4eae":"code","80433cf3":"code","bd8fbd76":"code","0e2529e8":"code","1345b8b2":"code","cdae286f":"code","4907b9ef":"code","d65238ba":"code","641e45c1":"code","7f6a2fa8":"code","982d964e":"code","9f5d983e":"code","22776759":"code","ef01da10":"code","e0bf4b8b":"code","5793f12e":"code","3741e756":"code","bc8eaa53":"code","0115f7f5":"code","177f908c":"code","4356ab34":"code","8679f842":"markdown","4ae17669":"markdown","8ce62db4":"markdown","bac960d3":"markdown","f9e38e5a":"markdown","ea06b4d0":"markdown","50bc28b3":"markdown","a4875f3f":"markdown","3f4a105f":"markdown","584f6568":"markdown","3bff2378":"markdown","21b6fb8f":"markdown","7317e652":"markdown","e52e4a9e":"markdown","bbff12d4":"markdown","89b1fdd2":"markdown","f7f2ce31":"markdown","724d27d3":"markdown","5e8c5e7e":"markdown","7d157458":"markdown","35cd0771":"markdown","52fe98c4":"markdown","23607d04":"markdown","b78215d1":"markdown","5115ebe5":"markdown","1d4dbeae":"markdown","b7578789":"markdown","18ce8cc0":"markdown","7f53de45":"markdown","44eb815a":"markdown","d2f722a5":"markdown","8a0842b8":"markdown","03cb1feb":"markdown","83514fa3":"markdown","d3f5c397":"markdown"},"source":{"aafc3d23":"\n# Essential\nimport numpy as np\nimport pandas as pd\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\n\n# Models\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Model evaluation and tuning\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nimport shap\n\n\n# Imputing and scaling \nfrom sklearn.impute._knn import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\n","80e077ec":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\n# train_data['Survived'] = train_data['Survived'].astype(int)\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\nfull_data =  train_data.append(test_data)\n\ntrain_data.head()","b190ebb4":"train_data.describe()","ed415c3c":"print('Number of rows ',len(train_data))\nprint(train_data.isnull().sum())","322850af":"full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']\ntrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']","c069ed33":"\nfig,ax = plt.subplots(1,2,figsize=(10,6))\nsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')\nsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))\n","868c4eae":"fig,ax = plt.subplots(1,2,figsize=(10,6))\n\n\nsns.histplot(x='Age',hue='Survived',data=train_data,ax=ax[0],kde=True).set(title='How many passengers survived? (Age,Pclass) ')\nsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))","80433cf3":"#Passenger considered solo if he has no family members on board\nfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)\n\n# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']\n\n\n# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') \nfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)\n\n# Replace string value of sex to numbers 1 - female, 0 - male\nfull_data['Sex'] = (full_data['Sex'] == 'female').astype(int)\n\n\n\n#There's 1 missing value for Fare in test dataset, let's fill it with mean value\nfull_data['Fare'].fillna(full_data['Fare'].mean(), inplace=True)\n","bd8fbd76":"# Extracting last name from Name feature\nfull_data['Last_Name'] = full_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Filling default value of family\/group survival as mean of individual survival \nfull_data['Family_Survival'] = train_data['Survived'].mean()\n\n\n# for loop to find family members (family with same surname)\nfor grp, grp_df in full_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            #check if whole family doesn't have 'Survived' value  \n            if (np.isnan(grp_df['Survived']).all()):\n                continue\n            average_family_score = (grp_df.drop(ind)['Survived'].mean())\n            #check if average_family_score is nan, it happens when only current passenger has 'Survived' value\n            if np.isnan(average_family_score):\n                average_family_score = row['Survived']\n            average_family_score = round(average_family_score)   \n            passID = row['PassengerId']\n            full_data.loc[full_data['PassengerId'] == passID, 'Family_Survival'] = average_family_score\n\n# for loop to find group of passengers who bought ticket together (friends,relatives)\nfor _, grp_df in full_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival'] == 0.5):\n                if (np.isnan(grp_df['Survived']).all()):\n                    continue\n                average_family_score = (grp_df.drop(ind)['Survived'].mean())\n                if np.isnan(average_family_score):\n                    average_family_score = row['Survived']\n                average_family_score = round(average_family_score)   \n                passID = row['PassengerId']\n                full_data.loc[full_data['PassengerId'] == passID, 'Family_Survival'] = average_family_score","0e2529e8":"full_data.head()","1345b8b2":"train_data = full_data[:len(train_data)]\ntrain_data.head()","cdae286f":"features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\ny = train_data['Survived'].ravel()\nX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)","4907b9ef":"def test_models(model,X,y_train):\n    key = type(model).__name__\n    model.fit(X,y_train)\n    model_score =model.score(X,y_train)\n    model_score=cross_val_score(model,X,y_train,cv=5).mean()\n    if key not in summary:\n        summary[key] = []\n    summary[key].append(model_score)\n    return summary","d65238ba":"scaler = StandardScaler()\n\nfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\nsummary={}\nmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]\n\nfor item in models_to_check:\n    summary = test_models(item,X_train[features],y_train)\n\nprint(X_train[features].columns)\nX = scaler.fit_transform(X_train[features])\n\n\nfor item in models_to_check:\n    summary = test_models(item,X,y_train)\n\nsummary = pd.DataFrame.from_dict(summary,orient='index',columns=['Without scaler','With scaler'])\nprint(summary)\n;\n","641e45c1":"model = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')\nmodel.fit(X_train,y_train)\nfeature_importances = model.feature_importances_\n\n\nplt.yticks(range(len(feature_importances)), features[:len(feature_importances)])\nplt.xlabel('Relative Importance')\nplt.barh(range(len(feature_importances)), feature_importances[:len(feature_importances)], color='b', align='center')\nplt.title('Feature Importances')","7f6a2fa8":"model = model.fit(X_val,y_val)\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_val)\n\nshap.summary_plot(shap_values)","982d964e":"def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):\n\n    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,\n                    cv=StratifiedKFold(n_splits=5), \n                    scoring=['accuracy','recall','f1','roc_auc'],\n                    verbose=1,refit='roc_auc')\n    clf.fit(X_train,y_train)          \n    preds = clf.best_estimator_.predict(X_val)\n    print(classification_report(preds,y_val))\n    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring = \"roc_auc\")\n    print(\"Scores:\", scores)\n    print(f\"Mean:{scores.mean()} \u00b1 {scores.std()}\")\n\n    return clf ","9f5d983e":"features = ['Pclass','Sex','FamilyMembers','Family_Survival']\n\ntest_data = full_data[len(train_data):]\ntest_data_x = test_data[features].copy(deep=True)\ntrain_data = full_data[:len(train_data)]\n\nscaler = StandardScaler()\nX = train_data[features].copy(deep=True)\nX= scaler.fit_transform(X)\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.20,random_state=111)","22776759":"Logistic_model_params= {'penalty' : ['l1', 'l2'],\n                        'C' : np.logspace(-4, 4, 20),\n                        'solver' : ['liblinear']}\n\nLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for Logistic Regression are:\")\nprint(Logistic_model.best_estimator_)","ef01da10":"SVM_model_params = {'C':np.logspace(-2,1,4),\n                    'gamma':np.logspace(-2,1,4),}\n                    \nSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for SVM are:\")\nprint(SVM_model.best_estimator_)","e0bf4b8b":"\nRF_model_params = { 'n_estimators': [200,350,500],\n               'max_features': ['auto'],\n               'max_depth': [2,5,None],\n               'min_samples_split': [5, 10],\n               'min_samples_leaf': [2, 4],\n               'bootstrap': [True],\n               'random_state':[1]}\nRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for Random Forest are:\")\nprint(RF_model.best_estimator_)","5793f12e":"Gaussian_model_params = {'var_smoothing':np.logspace(0,-9,100)}\nGaussian_model = GridSearchCVWrapper(GaussianNB(),Gaussian_model_params, X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for Naive Bayes are:\")\nprint(Gaussian_model.best_estimator_)","3741e756":"#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.\nXgb_model_parameters = {\n            'n_estimators': [200],\n            'colsample_bytree': [0.7],\n            'max_depth': [15],\n            'reg_alpha': [1.1],\n            'reg_lambda': [1.2],\n            'n_jobs':[-1]}\n\nXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),Xgb_model_parameters,X_train,X_val,y_train,y_val)\nprint(f\"\\nBest params for XGBoost are:\")\nprint(Xgb_model.best_estimator_)\n","bc8eaa53":"KNN_model_params= {'n_neighbors':np.arange(1,30,2),\n                    'leaf_size':np.arange(1,15,2),\n                    'p':[1,2]}\nKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)\n\nprint(f\"\\nBest params for K Neighbors are:\")\nprint(KNN_model.best_estimator_)","0115f7f5":"data_of_classifier = pd.DataFrame()\nclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]\nfor i in classifiers:\n    fit_classifier = i.fit(X_train,y_train)\n    data_of_classifier[type(i).__name__] = i.predict(X_val)\n    print('Score of',type(i).__name__,':')\n    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())\nsns.heatmap(data_of_classifier.astype(float).corr(),annot=True)","177f908c":"data_to_test = scaler.transform(test_data[features])","4356ab34":"\nestimators = [#('SVM',SVM_model.best_estimator_),\n              ('XGB',Xgb_model.best_estimator_),\n              ('Logistic',Logistic_model.best_estimator_)\n               # ('Random Forest',Gaussian_model.best_estimator_),\n               #('KNN',KNN_model.best_estimator_)\n]\n\nstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)\n\nstacking_clf.fit(X,y)\n\npredictions =  stacking_clf.predict(data_to_test)\npredictions =predictions.astype(int)\nfinal_results = pd.DataFrame({ 'PassengerId':test_data.PassengerId ,'Survived':predictions })\nfinal_results.to_csv('..\/working\/submission.csv',index=False)","8679f842":"Ok, what about features, we can examine the importance of features. We will inspect the best classifier from our test - `XGBoost`.","4ae17669":"And one of the important titles is 'Master' which\u0431 according to wikipedia, is used for boys:\n>  ... in the United States, unlike the UK, a boy can be addressed as Master only until age 12, then is addressed only by his name with no title until he turns 18, when he takes the title of Mr.\n\nTherefore, we can consider passengers with the title \u2018Master\u2019 as young boys for age feature, which would most likely fit in that orange bump on the left chart with distribution.\n\nI won't do it in this notebook, but it might be a good point for feature engineering or just to fill missing age values with consideration of the title of passenger.\n","8ce62db4":"There\u2019s not much interesting data we can see now, but we can see that `count` in column `Age` varies from other features, which means that we have some missing values. Let\u2019s see how many null values we have in the train dataset:","bac960d3":"\nIf we examine the dataset more carefully, we will see interesting details considering a group of travellers:\n- Families usually pay equal fare and obviously have the same last name. \n- Group of friends\/relatives with different last names usually have the same ticket number\n\nWe can use those facts as a new feature that represents the chance of survival of this family\/group, let's call it `Family_Survival`.\nI've changed the method used by [S.Xu's](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever), but changed his approach of minmaxing the family survival to average score, it seemed more precise.  ","f9e38e5a":"Just interpret this plot as - closer to the right side means more impact of that feature on prediction being 'Survived', closer to the left side 'Died'. \nRed means closer to the higher value of the described feature, blue means closer to the low value of the described feature (Pclass for example: 3 - red, 2 - purple, 1 - blue).\n\n\nWe'll break it down one by one:\n- `Sex` affects the chance of surviving, hence why red(bigger value, 1.0 in this case) are skewed to the right side.\n- `Fare` doesn't affect the output based on its value.\n- `Family_survival` does affect the output, passengers which families\/group had bigger chance to live expected to survive, passengers which  families\/groups had medium chance to live had smaller chance to live, and passengers with families\/groups with smallest chance of survival expected to die with them(I guess...).\n- `Pclass` does  slightly affect the output, 1st class passengers are more likely to survive, than 2nd and 3rd class\n- `Embarked` is strange, but passengers from 'S'-Southampton are more likely to die\n- `Family members` does not affect that much\n- `IsSolo` does not affect at all\n","ea06b4d0":"Prepare our data for training","50bc28b3":"Ok, so we can see that solo travellers died more often compared to the ones with family.\nAlso, there\u2019s a strong sign that females have a higher chance to survive.\nAnd we can see that males from 1st class had a higher chance to survive than males from 2nd and 3rd class.","a4875f3f":"After adding new features, we can start trying to choose the best model to fit the data.\nLet's add new features to train and test data.","3f4a105f":"Additionally, to newly created combined feature `FamilyMembers` we also need to consider adding feature which identifies solo travellers (`IsSolo`). Also we'll fill empty values and change categorical string values to integer.  \nWe will ignore `Age` feature in training, since it is difficult to correctly predict how old is the passenger (except those with 'Master' title of course)","584f6568":"## Exploring data \nWe have 3 categorical features:\n - `PClass`\n - `Sex`\n - `Embarked`\n\nWe also have 4 numerical features:\n - `Age`\n - `SibSp`\n - `Parch`\n - `Fare`\n\nAnd 3 nominal features:\n - `Name`\n - `Ticket`\n - `Cabin`\n\n   ","3bff2378":"As we can see, all models increased score with scaled data.\nSolver also failed to converge on non-scaled training data, so there is an undoubted need to scale data for this dataset.","21b6fb8f":"Results are pretty even, but we will consider the models with the highest ROC AUC scores and combine 3 of them.\n\nThe good idea is to find models with less correlation between each other and high scores.\n\nAfter some testing, the best combination I found was the Random Forest as a final estimator and will use KNeighbors and XGBoost as base estimators. I commented the different models in stacked classifier if you would want to test them.","7317e652":"Here's another small wrapper for Grid Search","e52e4a9e":"## Choosing the best model\nNow we need to get the best hyperparameters for our models. Different methods can help in fine tuning the hyperparameters. We will use Grid Search and combine it with Stratified KFold cross validation. We will try to find the best parameters for each model and stack them later.  \nSo I will omit the details to save valuable compiling time and set smaller parameter grids just to show the process.\nI will also set only 4 important features in training data, since I tried different combinations of them and these seemed most useful.","bbff12d4":"## Importing dataset\nWe will need to concatenate train and test data for future feature engineering. It might be not recommended in some cases and can cause a data leakage. But we will discuss some caveats later.\n\nLet's see what features we have","89b1fdd2":"Let's see stats of numerical features in train dataset","f7f2ce31":"## Stacking models and getting results\nAfter completing training our models, it\u2019s time to evaluate them and compare them one by one. We\u2019ll do the last comparison and visualize the ROC AUC score of models on the heatmap to find a suitable combination of models for stacking.\n\nThe point is to combine the most accurate models to get a better score. We need to find models which have a little correlation between each other\u2019s predictions.","724d27d3":"## Inspecting the models and features ","5e8c5e7e":"But what about imputing `177` rows for `Age` feature? Do we need to fill missing values or this feature is not that important?\nOverall, it is not that important, we can see it on distribution plots, only very young passengers had a higher chance to survive.\nWe can make `Age` feature a categorical feature and divide it in year bins.\n\nHowever, there\u2019s a small detail about the dataset however - if we would examine `Name` feature, we might see that there is a title of the passenger (e.g. Mr,Mrs,Dr,etc.).","7d157458":"We will describe model's features importance in bar chart","35cd0771":"I can see `PClass`,`Sex`,`Age`,`SibSp`,`Parch`,`Fare`,`Cabin` as potential important variables. \nAlso, we might combine some of those features (For example SibSp and Parch as they might be considered 'family')","52fe98c4":"As previously mentioned, we wanted to use only certain features to train. \nIt's time to start preparing our data to train our models","23607d04":"## Intro\n\nAs some kind of entry point I wanted to start with the classical Titanic dataset, I\u2019ll try to cover different stages of modelling from EDA to ensembling suitable models. I\u2019ll omit some details to make this notebook much easier to scroll and navigate. Hope you\u2019ll like it. Maybe it can be a good tutorial for beginners. I might add some more references and more details of every aspect.","b78215d1":"We will test multiple types of models, such as:\n- Logistic regression\n- Support Vector Machine\n- Random Forest\n- Naive Bayes\n- KNN\n- XGBoosting ","5115ebe5":"We also need to answer other questions info about the dataset:\n- How important is info about the port where passengers embarked?\n- Does `Age` distribution vary in groups of passengers who died and lived? Do we need to impute `Age` for the `177` passengers?\n\nWe can visualize those questions and try to answer them","1d4dbeae":"There's a small bump for passengers aged < 10 years. It is because children were prioritized during the evacuation.\n\nThere's no strong evidence if embark port affect the result since confidence intervals (black lines on bars) overlap with each other.","b7578789":"## Importing libraries\n","18ce8cc0":"##  Feature engineering","7f53de45":"After combining `SibSp` and `Parch` into the new feature `FamilyMembers` counting number of family members for each passenger, we will visualize everything we have for know. First, let\u2019s see how many passengers survived based on how big is the family passenger is travelling with (Chart on the left). And how many passengers survived based on `Sex` and `Pclass` (Chart on the right).","44eb815a":"We need to standardize our training data since some models are very sensitive to unscaled data.\nWe'll do an experiment to showcase this: \n","d2f722a5":"## Conclusion\nI tried to do a little bit of everything on this notebook, so there's a lot of details I omitted, but I do appreciate your feedback","8a0842b8":"Good, now we can look at the updated dataset","03cb1feb":"To correctly choose the right model for our task, we need to evaluate each model. We will use cross-validation during training models with default parameters.\nHyperparameter tuning will be performed after we chose the most effective models.\n\nHere\u2019s a basic wrapper to make this process easier","83514fa3":"As we probably expected, `Sex` is the most important feature, after that we have `Pclass`, `Family_Survival` and `FamilyMembers`.\nSurprisingly, the new feature, `IsSolo` is practically useless.\n\nOkay,let's see how features affect our model's output.\nOne of the most useful and beautiful ways to plot the feature's output is in SHAP library in `summary_plot` method, by calculating Shapley values to explain the impact of the features on prediction.","d3f5c397":"We have 177 rows with missing `Age` and 687 rows with missing `Cabin`"}}