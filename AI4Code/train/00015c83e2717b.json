{"cell_type":{"c417225b":"code","51e3cd89":"code","2600b4eb":"code","75b65993":"code","cf195f8b":"code","25699d02":"code","de148b56":"code","9901472c":"code","10377ef8":"code","1f462e2f":"code","fceeb3e6":"code","2af2a41a":"code","91e68f13":"code","9216a113":"code","63753f10":"code","d5aee1e4":"code","dc8a39a5":"code","3d0a28c2":"code","0eea9701":"code","a6f8f9f1":"code","7223cfc2":"code","df6b3ccb":"code","a9b266d2":"code","3e17f424":"code","42f0c365":"code","cc8d23d8":"code","ad42abc1":"code","7894c4e8":"code","cc1add42":"code","16b0d436":"code","a3e791de":"code","02ef0932":"code","03441163":"code","d429a743":"code","4d5ebd46":"code","3adcd0f8":"code","d8f4dfe0":"code","d47d3338":"code","c5b89474":"code","b5ef409a":"code","7bb6803b":"code","36b95373":"code","31d43a34":"code","cba54be3":"code","c33ab270":"code","23cb507d":"code","017f1c1b":"code","e458b502":"code","10fc0035":"code","61f2723c":"code","06b8472a":"code","704a44aa":"code","c7bb2674":"code","da1357af":"code","7a416873":"code","1554d3bc":"code","512a821e":"code","bf2740de":"code","fa1ee016":"code","ce96953f":"code","96c19678":"code","275d2fa7":"code","4de05ac0":"code","9aadfa3f":"code","60840c05":"code","a829d740":"code","14feb55f":"code","b8f3850a":"code","ccbe6713":"code","1ecd4e35":"code","1e21e4e5":"code","3db2d50e":"code","f2c750d3":"markdown","cd61f6d1":"markdown","72b3201a":"markdown","924c9f0f":"markdown","da8b817a":"markdown","8542d6fc":"markdown","fbe3e811":"markdown","657a8804":"markdown","a8fcc3e3":"markdown","183226e4":"markdown","be6c9079":"markdown","41beeead":"markdown","eab2b130":"markdown","b5e286ea":"markdown","2e94bd7a":"markdown","a166703b":"markdown","ceba8ae0":"markdown","f2915b9f":"markdown","3e99dee9":"markdown","da4f7550":"markdown","42749e24":"markdown"},"source":{"c417225b":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option(\"display.max_rows\", 101)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"font.size\"] = 15\nimport seaborn as sns\nfrom collections import Counter\nimport PIL\nimport seaborn as sns\nfrom collections import defaultdict\nfrom pathlib import Path\nimport cv2\nimport os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.feature import canny\nfrom skimage.filters import sobel\nfrom skimage.morphology import watershed\nfrom scipy import ndimage as ndi\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom skimage.segmentation import mark_boundaries\nfrom scipy import signal\nimport cv2\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nimport tqdm\nimport gc\ngc.enable()\nimport glob\n\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.feature import canny\nfrom skimage.filters import sobel\nfrom skimage.morphology import watershed\nfrom scipy import ndimage as ndi\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom skimage.segmentation import mark_boundaries\n\nimport sys\n","51e3cd89":"input_dir = \"..\/input\/\"","2600b4eb":"train_df = pd.read_csv(\"..\/input\/train.csv\")\nsample_df = pd.read_csv(\"..\/input\/sample_submission.csv\")","75b65993":"class_dict = defaultdict(int)\n\nkind_class_dict = defaultdict(int)\n\nno_defects_num = 0\ndefects_num = 0\n\nfor col in range(0, len(train_df), 4):\n    img_names = [str(i).split(\"_\")[0] for i in train_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n        \n    labels = train_df.iloc[col:col+4, 1]\n    if labels.isna().all():\n        no_defects_num += 1\n    else:\n        defects_num += 1\n    \n    kind_class_dict[sum(labels.isna().values == False)] += 1\n        \n    for idx, label in enumerate(labels.isna().values.tolist()):\n        if label == False:\n            class_dict[idx] += 1","cf195f8b":"print(\"the number of images with no defects: {}\".format(no_defects_num))\nprint(\"the number of images with defects: {}\".format(defects_num))","25699d02":"fig, ax = plt.subplots()\nsns.barplot(x=list(class_dict.keys()), y=list(class_dict.values()), ax=ax)\nax.set_title(\"the number of images for each class\")\nax.set_xlabel(\"class\")\nclass_dict","de148b56":"fig, ax = plt.subplots()\nsns.barplot(x=list(kind_class_dict.keys()), y=list(kind_class_dict.values()), ax=ax)\nax.set_title(\"Number of classes included in each image\");\nax.set_xlabel(\"number of classes in the image\")\nkind_class_dict","9901472c":"train_size_dict = defaultdict(int)\ntrain_path = Path(\"..\/input\/train_images\/\")\n\nfor img_name in train_path.iterdir():\n    img = PIL.Image.open(img_name)\n    train_size_dict[img.size] += 1","10377ef8":"train_size_dict","1f462e2f":"test_size_dict = defaultdict(int)\ntest_path = Path(\"..\/input\/test_images\/\")\n\nfor img_name in test_path.iterdir():\n    img = PIL.Image.open(img_name)\n    test_size_dict[img.size] += 1","fceeb3e6":"test_size_dict","2af2a41a":"palet = [(249, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12)]","91e68f13":"def mask2rgba(mask):\n    rgba_list = []\n    for idx in range(4):\n        rgba = cv2.cvtColor(mask[:, :, idx], cv2.COLOR_GRAY2RGBA)\n        rgba[:, :, 3] = rgba[:,:,0] * 100\n        rgba[:, :, :3] = rgba[:, :, :3] * palet[idx]\n        rgba_list.append(rgba)\n    return rgba_list","9216a113":"labels = train_df.iloc[5:5+4, 1]","63753f10":"def name_and_mask(start_idx):\n    col = start_idx\n    img_names = [str(i).split(\"_\")[0] for i in train_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n\n    labels = train_df.iloc[col:col+4, 1]\n    mask = np.zeros((256, 1600, 4), dtype=np.uint8)\n\n    for idx, label in enumerate(labels.values):\n        if label is not np.nan:\n            mask_label = np.zeros(1600*256, dtype=np.uint8)\n            label = label.split(\" \")\n            positions = map(int, label[0::2])\n            length = map(int, label[1::2])\n            for pos, le in zip(positions, length):\n                mask_label[pos:(pos+le)] = 1\n            mask[:, :, idx] = mask_label.reshape(256, 1600, order='F')\n    return img_names[0], mask","d5aee1e4":"def show_mask_image(col):\n    name, mask = name_and_mask(col)\n    img = cv2.imread(str(train_path \/ name))\n    #fig, ax = plt.subplots(figsize=(15, 15))\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    for ch in range(4):\n        contours, _ = cv2.findContours(mask[:, :, ch], cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n        for i in range(0, len(contours)):\n            cv2.polylines(img, contours[i], True, palet[ch], 2)   \n        for i, ctr in enumerate(contours):\n            if i > 0 :\n                break\n        # Get bounding box\n            x, y, w, h = cv2.boundingRect(ctr)\n            \n            # Getting ROI\n            roi = img[y:y + h, x:x + w]\n            print(roi.shape)\n            res = cv2.resize(roi, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n    ax[0].set_title(name)\n    ax[0].imshow(img)\n    ax[1].set_title('One ROI')\n    ax[1].imshow(res)\n    plt.show()\n","dc8a39a5":"fig, ax = plt.subplots(1, 4, figsize=(15, 5))\nfor i in range(4):\n    ax[i].axis('off')\n    ax[i].imshow(np.ones((50, 50, 3), dtype=np.uint8) * palet[i])\n    ax[i].set_title(\"class color: {}\".format(i+1))\nfig.suptitle(\"each class colors\")\n\nplt.show()","3d0a28c2":"idx_no_defect = []\nidx_class_1 = []\nidx_class_2 = []\nidx_class_3 = []\nidx_class_4 = []\nidx_class_multi = []\nidx_class_triple = []\n\nfor col in range(0, len(train_df), 4):\n    img_names = [str(i).split(\"_\")[0] for i in train_df.iloc[col:col+4, 0].values]\n    if not (img_names[0] == img_names[1] == img_names[2] == img_names[3]):\n        raise ValueError\n        \n    labels = train_df.iloc[col:col+4, 1]\n    if labels.isna().all():\n        idx_no_defect.append(col)\n    elif (labels.isna() == [False, True, True, True]).all():\n        idx_class_1.append(col)\n    elif (labels.isna() == [True, False, True, True]).all():\n        idx_class_2.append(col)\n    elif (labels.isna() == [True, True, False, True]).all():\n        idx_class_3.append(col)\n    elif (labels.isna() == [True, True, True, False]).all():\n        idx_class_4.append(col)\n    elif labels.isna().sum() == 1:\n        idx_class_triple.append(col)\n    else:\n        idx_class_multi.append(col)\n","0eea9701":"for idx in idx_class_1[:1]:\n    show_mask_image(idx)","a6f8f9f1":"idx_class_2[:1]","7223cfc2":"for idx in idx_class_2[:1]:\n    show_mask_image(idx)","df6b3ccb":"for idx in idx_class_3[:1]:\n    show_mask_image(idx)\n    \n    ","a9b266d2":"for idx in idx_class_4[:1]:\n    show_mask_image(idx)","3e17f424":"for idx in idx_class_multi[:4]:\n    show_mask_image(idx)","42f0c365":"for idx in idx_class_triple[:1]:\n    show_mask_image(idx)","cc8d23d8":"#These are the functions provided by kaggle to convert a mask to rle and vice-versa.\nimport numpy as np\n\ndef mask2rle(img, width, height):\n    rle = []\n    lastColor = 0;\n    currentPixel = 0;\n    runStart = -1;\n    runLength = 0;\n\n    for x in range(width):\n        for y in range(height):\n            currentColor = img[x][y]\n            if currentColor != lastColor: \n                if currentColor >= 127:\n                    runStart = currentPixel;\n                    runLength = 1;\n                else:\n                    rle.append(str(runStart));\n                    rle.append(str(runLength));\n                    runStart = -1;\n                    runLength = 0;\n                    currentPixel = 0;\n            elif runStart > -1:\n                runLength += 1\n            lastColor = currentColor;\n            currentPixel+=1;\n\n    return \" \".join(rle)\n\n\ndef rle2mask(rle, width, height):\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return np.flipud( np.rot90( mask.reshape(height,width), k=1 ) )","ad42abc1":"ImageId = '0002cc93b.jpg_1'\ntrain_df.head()\n\nimg = cv2.imread('..\/input\/train_images\/' + ImageId.split('_')[0])\nimg_masks = train_df.loc[train_df['ImageId_ClassId'] == ImageId, 'EncodedPixels'].tolist()\np2, p97 = np.percentile(img, (2, 97))\nimg_rescale = exposure.rescale_intensity(img, in_range=(p2, p97))\n\nimg_ben = cv2.addWeighted(img,4, cv2.GaussianBlur(img, (0,0) , 10) ,-4 ,128)\n\n# Take the individual ship masks and create a single mask array for all ships\nall_masks = np.zeros((256,1600))\nfor mask in img_masks:\n    all_masks += rle2mask(mask,256,1600)\n    \nfig, axarr = plt.subplots(5, 1, figsize=(15, 15))\naxarr[0].axis('off')\naxarr[1].axis('off')\naxarr[2].axis('off')\naxarr[3].axis('off')\naxarr[0].imshow(img)\naxarr[1].imshow(all_masks)\naxarr[2].imshow(img_rescale)\n#axarr[2].imshow(img_rescale)\naxarr[3].imshow(all_masks, alpha=0.7)\naxarr[4].imshow(img_ben)\nplt.tight_layout(h_pad=0.4, w_pad=0.4)\nplt.show()\n\n","7894c4e8":"### Take the images with masks\ntrain_masked_df = train_df[ train_df['EncodedPixels'].notnull() ]\n","cc1add42":"view_count =  15\n#i_chk = np.random.randint(0,len(data), size = view_count)\nsample_imgs = []\nben_sample_imgs = []\nsample_imgs_label =[]\nfile_list = ['..\/input\/train_images\/{}'.format(train_masked_df['ImageId_ClassId'].values.tolist()[i].split('_')[0]) for i in range(view_count)]\n\nfor i in range(view_count) :\n    sample_img = cv2.imread(file_list[i])\n    sample_img = cv2.cvtColor(sample_img,cv2.COLOR_BGR2RGB)\n    sample_imgs.append(sample_img)\n    sample_imgs_label.append(file_list[i])\n    ben_sample_imgs.append(cv2.addWeighted (sample_img,4, cv2.GaussianBlur(sample_img, (0,0) , 10) ,-4 ,128))","16b0d436":"#plt.imshow(ben_sample_imgs[0])\nplt.imshow(cv2.cvtColor(ben_sample_imgs[8],cv2.COLOR_RGB2GRAY))\n","a3e791de":"for i in range(5) :\n    fig , ax = plt.subplots(1,2,figsize = (15,15))\n    ax[0].imshow(sample_imgs[i])\n    ax[1].imshow(ben_sample_imgs[i])\n    #plt.autoscale(tight = 'True' , axis = 'y')\n    ax[0].set_title(sample_imgs_label[i], y = 1)\n    ax[1].set_title(str(sample_imgs_label[i]) + ' with preprocess', y = 1)\n    ax[0].axis('off')\n    ax[1].axis('off')","02ef0932":"ben_sample_imgs[0].shape","03441163":"import cv2\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\nimport numpy as np\nimport math\nfrom scipy.ndimage.filters import gaussian_filter\nfrom functools import wraps\nimport torch\nimport torchvision.transforms.functional as F\n\n\ndef vflip(img):\n    return cv2.flip(img, 0)\n\n\ndef hflip(img):\n    return cv2.flip(img, 1)\n\n\ndef random_flip(img, code):\n    return cv2.flip(img, code)\n\n\ndef transpose(img):\n    return img.transpose(1, 0, 2) if len(img.shape) > 2 else img.transpose(1, 0)\n\n\ndef rot90(img, factor):\n    img = np.rot90(img, factor)\n    return np.ascontiguousarray(img)\n\n\ndef rotate(img, angle):\n    height, width = img.shape[0:2]\n    mat = cv2.getRotationMatrix2D((width\/2, height\/2), angle, 1.0)\n    img = cv2.warpAffine(img, mat, (width, height),\n                         flags=cv2.INTER_LINEAR,\n                         borderMode=cv2.BORDER_REFLECT_101)\n    return img\n\n\ndef shift_scale_rotate(img, angle, scale, dx, dy):\n    height, width = img.shape[:2]\n\n    cc = math.cos(angle\/180*math.pi) * scale\n    ss = math.sin(angle\/180*math.pi) * scale\n    rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n\n    box0 = np.array([[0, 0], [width, 0],  [width, height], [0, height], ])\n    box1 = box0 - np.array([width\/2, height\/2])\n    box1 = np.dot(box1, rotate_matrix.T) + np.array([width\/2+dx*width, height\/2+dy*height])\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat = cv2.getPerspectiveTransform(box0, box1)\n    img = cv2.warpPerspective(img, mat, (width, height),\n                              flags=cv2.INTER_LINEAR,\n                              borderMode=cv2.BORDER_REFLECT_101)\n\n    return img\n\n\ndef center_crop(img, height, width):\n    h, w, c = img.shape\n    dy = (h-height)\/\/2\n    dx = (w-width)\/\/2\n    y1 = dy\n    y2 = y1 + height\n    x1 = dx\n    x2 = x1 + width\n    img = img[y1:y2, x1:x2, :]\n    return img\n\n\ndef clip(img, dtype, maxval):\n    return np.clip(img, 0, maxval).astype(dtype)\n\n\ndef clipped(func):\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        dtype, maxval = img.dtype, np.max(img)\n        return clip(func(img, *args, **kwargs), dtype, maxval)\n    return wrapped_function\n\n\ndef shift_hsv(img, hue_shift, sat_shift, val_shift):\n    dtype = img.dtype\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.int32)\n    h, s, v = cv2.split(img)\n    h = cv2.add(h, hue_shift)\n    h = np.where(h < 0, 255 - h, h)\n    h = np.where(h > 255, h - 255, h)\n    h = h.astype(dtype)\n    s = clip(cv2.add(s, sat_shift), dtype, 255 if dtype == np.uint8 else 1.)\n    v = clip(cv2.add(v, val_shift), dtype, 255 if dtype == np.uint8 else 1.)\n    img = cv2.merge((h, s, v)).astype(dtype)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    return img\n\n@clipped\ndef shift_rgb(img, r_shift, g_shift, b_shift):\n    img[...,0] = img[...,0] + r_shift\n    img[...,1] = img[...,1] + g_shift\n    img[...,2] = img[...,2] + b_shift\n    return img\n\ndef clahe(img, clipLimit=2.0, tileGridSize=(8,8)):\n    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2RGB)\n    return img_output\n\ndef blur(img, ksize):\n    return cv2.blur(img, (ksize, ksize))\n\ndef median_blur(img, ksize):\n    return cv2.medianBlur(img, ksize)\n\ndef motion_blur(img, ksize):\n    kernel = np.zeros((ksize, ksize))\n    xs, ys = np.random.randint(0, kernel.shape[1]), np.random.randint(0, kernel.shape[0])\n    xe, ye = np.random.randint(0, kernel.shape[1]), np.random.randint(0, kernel.shape[0])\n    cv2.line(kernel, (xs, ys), (xe, ye), 1, thickness=1)\n    return cv2.filter2D(img, -1, kernel \/ np.sum(kernel))\n\ndef random_polosa(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if np.mean(gray) < 100:\n        empty = np.zeros(img.shape[:2], dtype=np.uint8)\n        xs, ys = np.random.randint(0, empty.shape[1]), np.random.randint(0, empty.shape[0])\n        xe, ye = np.random.randint(0, empty.shape[1]), np.random.randint(0, empty.shape[0])\n        factor = np.random.randint(1, 10) \/ 3.\n        cv2.line(empty, (xs, ys), (xe, ye), np.max(gray) \/ factor, thickness=np.random.randint(10, 100))\n        empty = cv2.blur(empty, (5, 5))\n        empty = empty | gray\n        return cv2.cvtColor(empty, cv2.COLOR_GRAY2RGB)\n    return img\n\ndef distort1(img, k=0, dx=0, dy=0):\n    \"\"\"\"\n    ## unconverntional augmnet ################################################################################3\n    ## https:\/\/stackoverflow.com\/questions\/6199636\/formulas-for-barrel-pincushion-distortion\n    ## https:\/\/stackoverflow.com\/questions\/10364201\/image-transformation-in-opencv\n    ## https:\/\/stackoverflow.com\/questions\/2477774\/correcting-fisheye-distortion-programmatically\n    ## http:\/\/www.coldvision.io\/2017\/03\/02\/advanced-lane-finding-using-opencv\/\n    ## barrel\\pincushion distortion\n    \"\"\"\n    height, width = img.shape[:2]\n    #  map_x, map_y =\n    # cv2.initUndistortRectifyMap(intrinsics, dist_coeffs, None, None, (width,height),cv2.CV_32FC1)\n    # https:\/\/stackoverflow.com\/questions\/6199636\/formulas-for-barrel-pincushion-distortion\n    # https:\/\/stackoverflow.com\/questions\/10364201\/image-transformation-in-opencv\n    k = k * 0.00001\n    dx = dx * width\n    dy = dy * height\n    x, y = np.mgrid[0:width:1, 0:height:1]\n    x = x.astype(np.float32) - width\/2 - dx\n    y = y.astype(np.float32) - height\/2 - dy\n    theta = np.arctan2(y, x)\n    d = (x*x + y*y)**0.5\n    r = d*(1+k*d*d)\n    map_x = r*np.cos(theta) + width\/2 + dx\n    map_y = r*np.sin(theta) + height\/2 + dy\n\n    img = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n    return img\n\n\ndef distort2(img, num_steps=10, xsteps=[], ysteps=[]):\n    \"\"\"\n    #http:\/\/pythology.blogspot.sg\/2014\/03\/interpolation-on-regular-distorted-grid.html\n    ## grid distortion\n    \"\"\"\n    height, width = img.shape[:2]\n\n    x_step = width \/\/ num_steps\n    xx = np.zeros(width, np.float32)\n    prev = 0\n    for idx, x in enumerate(range(0, width, x_step)):\n        start = x\n        end = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step*xsteps[idx]\n\n        xx[start:end] = np.linspace(prev, cur, end-start)\n        prev = cur\n\n    y_step = height \/\/ num_steps\n    yy = np.zeros(height, np.float32)\n    prev = 0\n    for idx, y in enumerate(range(0, height, y_step)):\n        start = y\n        end = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step*ysteps[idx]\n\n        yy[start:end] = np.linspace(prev, cur, end-start)\n        prev = cur\n\n    map_x, map_y = np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n    img = cv2.remap(img, map_x, map_y,\n                    interpolation=cv2.INTER_LINEAR,\n                    borderMode=cv2.BORDER_REFLECT_101)\n    return img\n\ndef elastic_transform_fast(image, alpha, sigma, alpha_affine, random_state=None):\n    \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis\", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n     Based on https:\/\/gist.github.com\/erniejunior\/601cdf56d2b424757de5\n    \"\"\"\n    if random_state is None:\n        random_state = np.random.RandomState(1234)\n\n    shape = image.shape\n    shape_size = shape[:2]\n\n\n    # Random affine\n    center_square = np.float32(shape_size) \/\/ 2\n    square_size = min(shape_size) \/\/ 3\n    alpha = float(alpha)\n    sigma = float(sigma)\n    alpha_affine = float(alpha_affine)\n\n    pts1 = np.float32([center_square + square_size, [center_square[0] + square_size, center_square[1] - square_size],\n                       center_square - square_size])\n    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = np.float32(gaussian_filter((random_state.rand(*shape_size) * 2 - 1), sigma) * alpha)\n    dy = np.float32(gaussian_filter((random_state.rand(*shape_size) * 2 - 1), sigma) * alpha)\n\n    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n\n    mapx = np.float32(x + dx)\n    mapy = np.float32(y + dy)\n\n    return cv2.remap(image, mapx, mapy, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n\n\ndef remap_color(img, bg, center, max):\n    def get_lut(img, bg, center, max):\n        ma = np.max(img)\n        # me = np.mean(img)\n        # th = np.mean([ma, me]) * 1.5\n        th = ma \/ 2\n        gap = 10\n        channels = [[], [], []]\n        range2 = ma - int(th)\n        for i in range(3):\n            channels[i].append(np.linspace(bg[i] - gap, center[i] - gap, int(th)).astype(np.uint8))\n            channels[i].append(np.linspace(center[i] - gap, max[i] + gap, range2).astype(np.uint8))\n            channels[i].append([max[i] + gap] * (256 - sum(map(len, channels[i]))))\n            channels[i] = np.hstack(channels[i])\n        return np.dstack(channels)\n\n    # img = adjust_gamma(img, 5.)\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if np.mean(gray) > 100:\n        return img\n    lut = get_lut(img, bg, center, max)\n    res = cv2.LUT(img, lut).astype(np.uint8)\n    return res\n\ndef invert(img):\n    return 255 - img\n\ndef channel_shuffle(img):\n    ch_arr = [0, 1, 2]\n    np.random.shuffle(ch_arr)\n    img = img[..., ch_arr]\n    return img\n\n@clipped\ndef gauss_noise(image, var):\n    row, col, ch = image.shape\n    mean = var\n    # var = 30\n    sigma = var**0.5\n    gauss = np.random.normal(mean,sigma,(row,col,ch))\n    gauss = gauss.reshape(row,col,ch)\n    gauss = (gauss - np.min(gauss)).astype(np.uint8)\n    return image.astype(np.int32) + gauss\n\ndef salt_pepper_noise(image):\n    #todo\n    s_vs_p = 0.5\n    amount = 0.004\n    noisy = image\n    # Salt mode\n    num_salt = np.ceil(amount * image.size * s_vs_p)\n    coords = [np.random.randint(0, i - 1, int(num_salt))\n              for i in image.shape]\n    noisy[coords] = 255\n\n    # Pepper mode\n    num_pepper = np.ceil(amount * image.size * (1. - s_vs_p))\n    coords = [np.random.randint(0, i - 1, int(num_pepper))\n              for i in image.shape]\n    noisy[coords] = 0\n    return noisy\n\ndef poisson_noise(image):\n    #todo\n    vals = len(np.unique(image))\n    vals = 2 ** np.ceil(np.log2(vals))\n    noisy = np.random.poisson(image * vals) \/ float(vals)\n    return noisy\n\ndef speckle_noise(image):\n    #todo\n    row, col, ch = image.shape\n    gauss = np.random.randn(row,col,ch)\n    gauss = gauss.reshape(row,col,ch)\n    noisy = image + image * gauss\n    return noisy\n\n@clipped\ndef random_brightness(img, alpha):\n    return alpha * img\n\n@clipped\ndef random_contrast(img, alpha):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    gray = (3.0 * (1.0 - alpha) \/ gray.size) * np.sum(gray)\n    return alpha * img + gray\n\ndef to_three_channel_gray(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    invgray = 255 - gray\n    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\n    if np.mean(invgray) < np.mean(gray):\n        invgray, gray = gray, invgray\n    res = [invgray, gray, clahe.apply(invgray)]\n    return cv2.merge(res)\n\ndef to_gray(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if np.mean(gray) > 127:\n        gray = 255 - gray\n    return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n\n\ndef add_channel(img):\n    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(21, 21))\n    lab = clahe.apply(lab[:, :, 0])\n    if lab.mean() > 127:\n        lab = 255 - lab\n    return np.dstack((img, lab))\n\n\ndef fix_mask(msk, sigmoid=False):\n    if not sigmoid:\n        msk[..., 2] = (msk[..., 2] > 127)\n        msk[..., 1] = (msk[..., 1] > 127) * (msk[..., 2] == 0)\n        msk[..., 0] = (msk[..., 1] == 0) * (msk[..., 2] == 0)\n    else:\n        msk = (msk > 127)\n    return msk.astype(np.uint8) * 255\n\n\ndef img_to_tensor(im, normalize=None):\n    tensor = torch.from_numpy(np.moveaxis(im \/ (255. if im.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n    if normalize is not None:\n        return F.normalize(tensor, **normalize)\n    return tensor\n\n\ndef mask_to_tensor(mask, num_classes, sigmoid):\n    mask = fix_mask(mask, sigmoid)\n    if num_classes > 1:\n        if not sigmoid:\n            #softmax\n            long_mask = np.zeros((mask.shape[:2]), dtype=np.int64)\n            if len(mask.shape) == 3:\n                for c in range(mask.shape[2]):\n                    long_mask[mask[...,c] > 0] = c\n            else:\n                long_mask[mask > 127] = 1\n                long_mask[mask == 0] = 0\n            mask = long_mask\n        else:\n            mask = np.moveaxis(mask \/ (255. if mask.dtype == np.uint8 else 1), -1, 0).astype(np.float32)\n    else:\n        mask = np.expand_dims(mask \/ (255. if mask.dtype == np.uint8 else 1), 0).astype(np.float32)\n    return torch.from_numpy(mask)","d429a743":"## used from xhulu  https:\/\/www.kaggle.com\/xhlulu\/severstal-predict-missing-masks\ntrain_df['isNan'] = pd.isna(train_df['EncodedPixels'])\ntrain_df['ImageId'] = train_df['ImageId_ClassId'].apply(\n    lambda x: x.split('_')[0]\n)\ntrain_df.head()","4d5ebd46":"train_nan_df = train_df.groupby(by='ImageId', axis=0).agg('sum')\ntrain_nan_df.reset_index(inplace=True)\ntrain_nan_df.rename(columns={'isNan': 'missingCount'}, inplace=True)\ntrain_nan_df['missingCount'] = train_nan_df['missingCount'].astype(np.int32)\ntrain_nan_df['allMissing'] = (train_nan_df['missingCount'] == 4).astype(int)\n\ntrain_nan_df.head()","3adcd0f8":"train_nan_df.head()","d8f4dfe0":"train_nan_df['label'] = train_nan_df['allMissing'].apply ( lambda x : 1 if x ==0 else 0)","d47d3338":"train_nan_df.drop(['missingCount','allMissing'], axis =1, inplace = True  )","c5b89474":"train_nan_df.label.value_counts()","b5ef409a":"train_df = train_nan_df.copy()","7bb6803b":"test_df = pd.DataFrame()","36b95373":"train_df.head()","31d43a34":"test_df['ImageId'] = np.array(os.listdir ('..\/input\/test_images\/'))","cba54be3":"test_df['label'] = np.array(0)*1801","c33ab270":"X_train = train_df.ImageId","23cb507d":"y_train = train_df.label","017f1c1b":"X_test = test_df.ImageId","e458b502":"y_test = test_df.label","10fc0035":"from fastai.vision import *\nfrom fastai.callbacks import *","61f2723c":"data_folder = Path(\"..\/input\")","06b8472a":"def _go_ben(img):\n    img = img*255\n    return cv2.addWeighted(img,4, cv2.GaussianBlur(img, (0,0) , 10) ,-4 ,128)\n    \ngo_ben = TfmPixel(_go_ben)","704a44aa":"test_img = ImageList.from_df(test_df, path=data_folder, folder='test_images')\n\ntrfm = get_transforms(do_flip=True, flip_vert=True, max_rotate=10.0, max_zoom=1.1,\n                      max_lighting=0.2, max_warp=0.2, \n                      p_affine=0.75, p_lighting=0.75 ,xtra_tfms=[contrast(scale=(0.5, 1.1), p=0.75)])#,go_ben(p=0.5)])\n\n","c7bb2674":"train_img_medium = (ImageList.from_df(train_df, path=data_folder, folder='train_images')\n        .split_by_rand_pct(0.1)\n        .label_from_df()\n        .add_test(test_img)\n        .transform(trfm, size=(256,600))\n        .databunch(path='.', bs=16, device= torch.device('cuda:0'))\n        .normalize(imagenet_stats)\n       )\ntrain_img_small = (ImageList.from_df(train_df, path=data_folder, folder='train_images')\n        .split_by_rand_pct(0.1)\n        .label_from_df()\n        .add_test(test_img)\n        .transform(trfm, size=(128,400))\n        .databunch(path='.', bs=16, device= torch.device('cuda:0'))\n        .normalize(imagenet_stats)\n       )","da1357af":"train_img_small.show_batch(rows=3, figsize=(12,9))","7a416873":"from torch import nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=2.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n\n        return F_loss.mean()","1554d3bc":"### Yet to be implemented \nlearn_densenet201 = cnn_learner(train_img_small, models.densenet201, metrics=[accuracy], model_dir=\"\/tmp\/model\/\")\nlearn_densenet201.loss_fn = FocalLoss()","512a821e":"learn_resnet50 = cnn_learner(train_img_small, models.resnet50, metrics=[accuracy], model_dir=\"\/tmp\/model\/\")\nlearn_resnet50.loss_fn = FocalLoss()","bf2740de":"learn_resnet50.lr_find()\nlearn_resnet50.recorder.plot(suggestion=True)","fa1ee016":"learn_resnet50.fit_one_cycle(5, max_lr=slice(2e-3))","ce96953f":"learn_resnet50.data = train_img_medium\nlearn_resnet50.loss_fn = FocalLoss()\nlearn_resnet50.unfreeze()\nlearn_resnet50.lr_find()\nlearn_resnet50.recorder.plot(suggestion=True)\n","96c19678":"learn_resnet50.fit_one_cycle(20, max_lr=slice(3e-4),callbacks=[SaveModelCallback(learn_resnet50, every='improvement', monitor='accuracy', name='best')]).mixup()","275d2fa7":"interp = ClassificationInterpretation.from_learner(learn_resnet50)\ninterp.plot_confusion_matrix()\ninterp.plot_top_losses(9, figsize=(15,15),heatmap = True)","4de05ac0":"preds,y = learn_resnet50.TTA(ds_type=DatasetType.Test)","9aadfa3f":"test_df['prediction']= preds.numpy()[:, 0]","60840c05":"test_df.head(20)","a829d740":"os.listdir('..\/input\/test_images\/')","14feb55f":"img = cv2.imread('..\/input\/test_images\/fc3c8279e.jpg',1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\n","b8f3850a":"test_final = test_df.loc[test_df['prediction'] <0.50]","ccbe6713":"test_final.describe()","1ecd4e35":"test_df","1e21e4e5":"test_final.to_csv('filtered_test_no_defect.csv')","3db2d50e":"test_final.describe()","f2c750d3":"### How many classes do each image have?","cd61f6d1":"## images with defect(contain 3 type label)","72b3201a":"* There are similar numbers of images with and without defects.\n* class is imbalanced","924c9f0f":"# Let's visualization masks!","da8b817a":"## images with defect(label: 4)","8542d6fc":"### Start Classification Process","fbe3e811":"## check image data\n### image size","657a8804":"## images with defect(label: 1)","a8fcc3e3":"## images with defect(label: 2)","183226e4":"#### Augmentation functions in one place . Will use later . Not now ","be6c9079":"#### Images with normal mask and Ben's processing ","41beeead":"## images with defect(contain multi label)","eab2b130":"#### Multiple Images with Ben's preprocessing ","b5e286ea":" Reference :http:\/\/faculty.neu.edu.cn\/yunhyan\/NEU_surface_defect_database.html\n \n In the Northeastern University (NEU) surface defect database, six kinds of typical surface defects of the hot-rolled steel strip are collected, i.e., rolled-in scale (RS), patches (Pa), crazing (Cr), pitted surface (PS), inclusion (In) and scratches (Sc). \n #### At a first look it seems for our images :  \n1. Class 1 : Inclusion\n2. Class 2: Pitted\n3. Class 3 : Scratches \n4. Class 4 : Patches . \n\n\n\nHowever I  might be wrong :) . \n![image.png](attachment:image.png)\n\n\n","2e94bd7a":"## Note : This Kernel is a Fork from the amazing Kernel below . So please upvote the original Kernel . I have started adding few information and preprocessing into this on my own . \nhttps:\/\/www.kaggle.com\/go1dfish\/clear-mask-visualization-and-simple-eda","a166703b":"## images with defect(label: 3)","ceba8ae0":"* All image have same shape, (1600, 256).","f2915b9f":"#### Start with binary classification here ","3e99dee9":"## About The Competition : Detecting Steel Defect \n\nSteel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.\n\n\nSeverstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry\u2014and they take corporate responsibility seriously. The company recently created the country\u2019s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production.\n\nThe production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it\u2019s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm.\n\nIn this competition, you\u2019ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet.\n\nIf successful, you\u2019ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.","da4f7550":"## import modules and define models","42749e24":"* almost image have no defect or one kind of defect"}}