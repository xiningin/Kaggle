{"cell_type":{"9edb9716":"code","caa86438":"code","0c219582":"code","f602e64e":"code","29e7cac1":"code","41a8eb7d":"code","a76269f7":"code","dfffb02b":"code","6b053b83":"code","7649bd34":"code","59247055":"code","39e31ca8":"code","cbeee3bb":"code","84105998":"code","76dea818":"code","61527274":"code","b78c0468":"code","aa3b2052":"code","41cb68c9":"code","292422a8":"markdown","4e0e17d0":"markdown"},"source":{"9edb9716":"import pandas as pd\nimport numpy as np\n\n# Importing libraries for text preprocessing...\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# ...Keras packages for network training and evaluation...\nfrom keras import models, layers, callbacks\nfrom keras.metrics import Precision, Recall, AUC\nfrom keras import backend as K\nfrom tensorflow.keras import optimizers\n\n# ...and Sklearn's train_test_split to define train, test and validation sets\nfrom sklearn.model_selection import train_test_split\n\npd.options.mode.chained_assignment = None","caa86438":"# Taking a look at our data\ndata = pd.read_csv('..\/input\/are-two-sentences-of-the-same-topic\/train.csv').drop(columns=['id'])\nlen_data = len(data)\ndata","0c219582":"# The dataset is balanced, fortunately\ndata.same_source.value_counts()","f602e64e":"# Let's clean our data from stopwords and truncate the word endings to make the different forms\n# of one word represent this word for the network\nstop_words = stopwords.words('english')\nsbs = SnowballStemmer(language='english')\n\nfor col in ['sent1', 'sent2']:\n    print(f\"Column {col}\")\n    \n    clear_texts = []\n    for i in range(len(data)):\n        if i % 20000 == 0:\n            print(f\"Processing sentence {i}\/{len(data)}...\")\n\n        cell_text = data.iloc[i][col]\n        clear_text = ''\n\n        # Tokenizing, stopwords filtering and stemming\n        cell_tokens = word_tokenize(cell_text)\n        for token in cell_tokens:\n            tok_low = token.lower()\n            if tok_low not in stop_words:\n                tok_stem = sbs.stem(tok_low)\n                clear_text += tok_stem + \" \"\n\n        clear_texts.append(clear_text)\n\n    data[col] = clear_texts","29e7cac1":"# Cleaned texts\ndata","41a8eb7d":"# Dividing our dataset into train, test and validation sets in ratio of 0.9\/0.05\/0.05,\n# keeping the target variable distribution in each set by 'stratify' parameter\nX_train, X_test, Y_train, Y_test = train_test_split(data.drop(columns=['same_source']), data.same_source, random_state=42,\n                                                   train_size=0.9, stratify=data.same_source)\nX_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, random_state=42, train_size=0.5, stratify=Y_test)","a76269f7":"# Let's find the sentence length params to have a starter point\n# for picking the embedding lengths\nmax_sent_len = 0\navg_sent_len = 0\n\nfor col in (['sent1', 'sent2']):\n    for i in range(len(X_train)):\n        sent_len = len(X_train[col].iloc[i].split(' '))\n        avg_sent_len += sent_len \/ len(X_train)\n        if sent_len > max_sent_len:\n            max_sent_len = sent_len\n            \nprint(f\"Max number of words in train sentence is {max_sent_len}, \" + \\\n      f\"average number is {round(avg_sent_len, 2)}\")","dfffb02b":"# Tokenization is the operation of transforming texts into numerical vectors\n# We can modify 'num_words' parameter to choose how many most frequent words\n# in dataset should be represented in vectors\nvocab_sizes = []\ntrain_inputs = []\nval_inputs = []\ntest_inputs = []\nsent_vector_len = int(avg_sent_len) + int(max_sent_len \/ 3)\n\n# We fit one tokenizer for all sentences as we don't want text in pairs\n# to have different embeddings for same words\ntokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(X_train['sent1'].values + X_train['sent2'].values)\nvocab_size = len(tokenizer.word_index) + 1\n\nfor col in ['sent1', 'sent2']:\n    train_sequences = tokenizer.texts_to_sequences(X_train[col])\n    val_sequences= tokenizer.texts_to_sequences(X_val[col])\n    test_sequences = tokenizer.texts_to_sequences(X_test[col])\n\n    # Padding allows us to have equal length in all texts;\n    # short text vectors will be extended with zeros,\n    # long ones will be truncated\n    # The 'maxlen' parameter also can be modified\n    train_inputs.append(pad_sequences(train_sequences, padding='post', maxlen=sent_vector_len))\n    val_inputs.append(pad_sequences(val_sequences, padding='post', maxlen=sent_vector_len))\n    test_inputs.append(pad_sequences(test_sequences, padding='post', maxlen=sent_vector_len))","6b053b83":"# Loading GloVe embeddings\n# Tutorial: https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\nembeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r')\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","7649bd34":"embedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","59247055":"# Checking the dimensionality of input data\nprint(train_inputs[0].shape)\nprint(train_inputs[1].shape)","39e31ca8":"# Keras doesn't have a built-in F1 score, but it's a useful binary classification metric;\n# we'll define it as it was proposed in https:\/\/stackoverflow.com\/a\/45305384\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","cbeee3bb":"# Dataframe to show the final metrics of different models\nres_df = pd.DataFrame(columns=['Model name', 'Test precision', 'Test recall', 'Test F1', 'Test AUC'])","84105998":"# We'll create 4 different nets; let's define the fit-evaluate method to\n# avoid writing this piece of code for 4 times\ndef compile_fit_evaluate(model):\n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[Precision(), Recall(), f1, AUC()])\n\n    print(model.summary())\n    \n    model.fit([train_inputs[0], train_inputs[1]], Y_train,\n                epochs=100, validation_data=([val_inputs[0], val_inputs[1]], Y_val),\n                callbacks=[callbacks.EarlyStopping(monitor='val_f1',\n                                                   verbose=1,\n                                                   patience=2,\n                                                   restore_best_weights=True,\n                                                   mode='max')])\n    \n    return model.evaluate([test_inputs[0], test_inputs[1]], Y_test)","76dea818":"# Simple MLP with two inputs and shared Dense part\ninput1 = layers.Input(shape=(sent_vector_len,))\ninput2 = layers.Input(shape=(sent_vector_len,))\n\nmerged = layers.Concatenate(axis=1)([input1, input2])\n\nemb = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(merged)\nflat1 = layers.Flatten()(emb)\ndense1 = layers.Dense(128, activation='relu')(flat1)\ndropout1 = layers.Dropout(0.5)(dense1)\ndense2 = layers.Dense(128, activation='relu')(dropout1)\ndropout2 = layers.Dropout(0.5)(dense2)\ndense3 = layers.Dense(128, activation='relu')(dropout2)\n\noutput = layers.Dense(1, activation='sigmoid')(dense3)\nmodel_mlp = models.Model(inputs=[input1, input2], outputs=output)\n\ntest_metrics = compile_fit_evaluate(model_mlp)\nres_df = res_df.append({'Model name': 'Basic MLP', 'Test precision': round(test_metrics[1], 4),\n                        'Test recall': round(test_metrics[2], 4), 'Test F1': round(test_metrics[3], 4),\n                        'Test AUC': round(test_metrics[4], 4)}, ignore_index=True)","61527274":"# MLP with two inputs and separate Dense parts for each sentence\ninput1 = layers.Input(shape=(sent_vector_len,))\nemb1 = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(input1)\nflat1 = layers.Flatten()(emb1)\ndense11 = layers.Dense(128, activation='relu')(flat1)\ndropout11 = layers.Dropout(0.5)(dense11)\ndense12 = layers.Dense(128, activation='relu')(dropout11)\ndropout12 = layers.Dropout(0.5)(dense12)\ndense13 = layers.Dense(128, activation='relu')(dropout12)\n\ninput2 = layers.Input(shape=(sent_vector_len,))\nemb2 = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(input2)\nflat2 = layers.Flatten()(emb2)\ndense21 = layers.Dense(128, activation='relu')(flat2)\ndropout21 = layers.Dropout(0.5)(dense21)\ndense22 = layers.Dense(128, activation='relu')(dropout21)\ndropout22 = layers.Dropout(0.5)(dense22)\ndense23 = layers.Dense(128, activation='relu')(dropout22)\n\nmerged = layers.Concatenate(axis=1)([dense13, dense23])\ndense_out = layers.Dense(128, activation='relu')(merged)\noutput = layers.Dense(1, activation='sigmoid')(dense_out)\nmodel_mlp_2head = models.Model(inputs=[input1, input2], outputs=output)\n\ntest_metrics = compile_fit_evaluate(model_mlp_2head)\nres_df = res_df.append({'Model name': 'Two-head MLP', 'Test precision': round(test_metrics[1], 4),\n                        'Test recall': round(test_metrics[2], 4), 'Test F1': round(test_metrics[3], 4),\n                        'Test AUC': round(test_metrics[4], 4)}, ignore_index=True)","b78c0468":"# Simple CNN with two inputs and shared CNN part\ninput1 = layers.Input(shape=(sent_vector_len,))\ninput2 = layers.Input(shape=(sent_vector_len,))\n\nmerged = layers.Concatenate(axis=1)([input1, input2])\n\nemb = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(merged)\nconv = layers.Conv1D(256, 5, activation='relu')(emb)\npool = layers.MaxPooling1D(pool_size=2)(conv)\ndrop = layers.Dropout(0.5)(pool)\nflat = layers.Flatten()(drop)\ndense = layers.Dense(128, activation='relu')(flat)\n\noutput = layers.Dense(1, activation='sigmoid')(dense)\nmodel_conv = models.Model(inputs=[input1, input2], outputs=output)\n\ntest_metrics = compile_fit_evaluate(model_conv)\nres_df = res_df.append({'Model name': 'Basic CNN', 'Test precision': round(test_metrics[1], 4),\n                        'Test recall': round(test_metrics[2], 4), 'Test F1': round(test_metrics[3], 4),\n                        'Test AUC': round(test_metrics[4], 4)}, ignore_index=True)","aa3b2052":"# CNN with two inputs and separate CNN parts for each sentence\ninput1 = layers.Input(shape=(sent_vector_len,))\nemb1 = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(input1)\nconv1 = layers.Conv1D(256, 5, activation='relu')(emb1)\npool1 = layers.MaxPooling1D(pool_size=2)(conv1)\ndrop1 = layers.Dropout(0.5)(pool1)\nflat1 = layers.Flatten()(drop1)\ndense1 = layers.Dense(128, activation='relu')(flat1)\n\ninput2 = layers.Input(shape=(sent_vector_len,))\nemb2 = layers.Embedding(input_dim=vocab_size, \n                       output_dim=100, \n                       input_length=sent_vector_len,\n                       weights=[embedding_matrix],\n                       trainable=True)(input2)\npool2 = layers.MaxPooling1D(pool_size=2)(emb2)\ndrop2 = layers.Dropout(0.5)(pool2)\nflat2 = layers.Flatten()(drop2)\ndense2 = layers.Dense(128, activation='relu')(flat2)\n\nmerged = layers.Concatenate(axis=1)([dense1, dense2])\ndense_out = layers.Dense(128, activation='relu')(merged)\noutput = layers.Dense(1, activation='sigmoid')(dense_out)\nmodel_conv_2head = models.Model(inputs=[input1, input2], outputs=output)\n\ntest_metrics = compile_fit_evaluate(model_conv_2head)\nres_df = res_df.append({'Model name': 'Two-head CNN', 'Test precision': round(test_metrics[1], 4),\n                        'Test recall': round(test_metrics[2], 4), 'Test F1': round(test_metrics[3], 4),\n                        'Test AUC': round(test_metrics[4], 4)}, ignore_index=True)","41cb68c9":"# Printing the metrics\nres_df.sort_values(by='Test F1', ascending=False)","292422a8":"The CNNs outperform the MLPs; they seem to find more complex relations between the sentences. Also, updating the pre-trained embeddings rather than fitting the new ones increased the AUC from 0.77 to 0.82. At the moment I'm trying to crack the problems of the recurrent nets. Hopefully they'll appear here later.\n\n**Thanks for your attention! I'm open for a discussion if you want to leave any comments.**","4e0e17d0":"**Greetings!**\n\nIn this notebook we'll try to detect that sentences belong to same text with a Keras perceptron and 1D-convolutional net with two inputs. The embeddings will be initialized using GloVe."}}