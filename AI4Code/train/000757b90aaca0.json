{"cell_type":{"8f84d7a9":"code","eb6ca769":"code","bc595bc2":"code","93cceeef":"code","3cb3d383":"code","6e3a3d90":"code","abc159f0":"code","b20690ef":"code","20f10a90":"code","e301d5a4":"code","7905811c":"code","1fa4803c":"code","dcb9b899":"code","ed7ca83b":"code","afc25d5c":"code","0781d626":"code","e895145f":"code","4f3af9d2":"code","c9131ba9":"code","afc62c5a":"code","5b5af988":"code","18cb2ee7":"code","a32974f3":"markdown","4e2b2854":"markdown","a2e1ed42":"markdown","454e8858":"markdown","1b8c0237":"markdown","744648dd":"markdown","0564962b":"markdown","336fdc76":"markdown","22686c49":"markdown","1c301fa2":"markdown","53c3a2be":"markdown","6253a226":"markdown","335d6d82":"markdown","e0f60ece":"markdown","72821d9a":"markdown","771dbec1":"markdown","eeab7090":"markdown","6243c12c":"markdown","d3db4f3e":"markdown","cecacc55":"markdown"},"source":{"8f84d7a9":"import pandas as pd\nimport spacy\nimport networkx as nx                        # a really useful network analysis library\nimport matplotlib.pyplot as plt\n# from networkx.algorithms import community   # not used, yet... \nimport datetime                              # access to %%time, for timing individual notebook cells\nimport os","eb6ca769":"nlp = spacy.load('en_core_web_lg')           # A more detailed model (with higher-dimension word vectors) - 13s to load, normally \n#nlp = spacy.load('en_core_web_md')           # a smaller model, e.g. for testing","bc595bc2":"plt.rcParams['figure.figsize'] = [10, 10]  # makes the output plots large enough to be useful","93cceeef":"rowlimit = 500              # this limits the tweets to a manageable number\ndata = pd.read_csv('..\/input\/ExtractedTweets.csv', nrows = rowlimit)\ndata.shape","3cb3d383":"data.head(6)","6e3a3d90":"tokens = []\nlemma = []\npos = []\nparsed_doc = [] \ncol_to_parse = 'Tweet'\n\nfor doc in nlp.pipe(data[col_to_parse].astype('unicode').values, batch_size=50,\n                        n_threads=3):\n    if doc.is_parsed:\n        parsed_doc.append(doc)\n        tokens.append([n.text for n in doc])\n        lemma.append([n.lemma_ for n in doc])\n        pos.append([n.pos_ for n in doc])\n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n        tokens.append(None)\n        lemma.append(None)\n        pos.append(None)\n\n\ndata['parsed_doc'] = parsed_doc\ndata['comment_tokens'] = tokens\ndata['comment_lemma'] = lemma\ndata['pos_pos'] = pos","abc159f0":"data.head(8)","b20690ef":"data.Tweet[0]","20f10a90":"data.Tweet[1]","e301d5a4":"data.Tweet[10]","7905811c":"stop_words = spacy.lang.en.stop_words.STOP_WORDS\nprint('Number of stopwords: %d' % len(stop_words))\nprint(list(stop_words))","1fa4803c":"print(data['parsed_doc'][0].similarity(data['parsed_doc'][1]))\nprint(data['parsed_doc'][0].similarity(data['parsed_doc'][10]))\nprint(data['parsed_doc'][1].similarity(data['parsed_doc'][10]))","dcb9b899":"data.Party.unique()","ed7ca83b":"world_data = data\n#world_data = data[data.Party == 'Democrat']      # or use either of these, if you want to see tweets from only one party\n#world_data = data[data.Party == 'Republican']","afc25d5c":"# takes 1s for 500 nodes - but of course this won't scale linearly!                              \nraw_G = nx.Graph() # undirected\nn = 0\n\nfor i in world_data['parsed_doc']:        # sure, it's inefficient, but it will do\n    for j in world_data['parsed_doc']:\n        if i != j:\n            if not (raw_G.has_edge(j, i)):\n                sim = i.similarity(j)\n                raw_G.add_edge(i, j, weight = sim)\n                n = n + 1\n\nprint(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")","0781d626":"edges_to_kill = []\nmin_wt = 0.94      # this is our cutoff value for a minimum edge-weight \n\nfor n, nbrs in raw_G.adj.items():\n    #print(\"\\nProcessing origin-node:\", n, \"... \")\n    for nbr, eattr in nbrs.items():\n        # remove edges below a certain weight\n        data = eattr['weight']\n        if data < min_wt: \n            # print('(%.3f)' % (data))  \n            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n            edges_to_kill.append((n, nbr)) \n            \nprint(\"\\n\", len(edges_to_kill) \/ 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")","e895145f":"for u, v in edges_to_kill:\n    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n        raw_G.remove_edge(u, v)","4f3af9d2":"strong_G = raw_G\nprint(strong_G.number_of_edges())","c9131ba9":"nx.draw(strong_G, node_size=20, edge_color='gray')","afc62c5a":"strong_G.remove_nodes_from(list(nx.isolates(strong_G)))","5b5af988":"from math import sqrt\ncount = strong_G.number_of_nodes()\nequilibrium = 10 \/ sqrt(count)    # default for this is 1\/sqrt(n), but this will 'blow out' the layout for better visibility\npos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=300)\nnx.draw(strong_G, pos=pos, node_size=10, edge_color='gray')","18cb2ee7":"plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes\n\nnx.draw(strong_G, pos, font_size=3, node_size=50, edge_color='gray', with_labels=False)\nfor p in pos:  # raise positions of the labels, relative to the nodes\n    pos[p][1] -= 0.03\nnx.draw_networkx_labels(strong_G, pos, font_size=8, font_color='k')\n\nplt.show()","a32974f3":"N.B. this next step can take a while - e.g. 14 mins, for the full set - but only 5s for 500 rows.\n\n(based on https:\/\/stackoverflow.com\/questions\/44395656\/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing)...","4e2b2854":"## Basic checks of the parsed data","a2e1ed42":"We could reduce increase the signal:noise ratio in these texts by removing some of the more common words (or *stopwords*). By removing these from the tweets, we would prevent them from influencing the analysis of whether two tweets are similar. I'm not addressing this is the notebook yet, but I will come back to it later. For now, let's just look at what words are included in spaCy's stopword list.","454e8858":"I hope this notebook was useful. Next:\n* I'd like to apply some keyword extraction to the tweets, to make this visualisation more useful;\n* there'll be some topic identification using gensim's implementation of LDA;\n* some more intelligent parameterisation of variables, such as allowing the minimum similarity cut-off to account for network size;\n* I'd like to apply a smarter similarity cut-off, such as Vladimir Batagelj's '[vertex islands](http:\/\/vlado.fmf.uni-lj.si\/pub\/networks\/doc\/mix\/islands.pdf)' technique; and\n* I should really apply TF-IDF, if only just to see how it compares to other keyword extraction techniques.","1b8c0237":"We can also tweak the layout algorithm. By, for example, changing the ideal distance at which the repulsive and attractive forces are in equilibrium. There's a good description of these forces [here](https:\/\/schneide.blog\/tag\/fruchterman-reingold\/). This value interacts with the number of `iterations` in surprising ways.","744648dd":"* load a representative set of tweets\n* demonstrate some basic spaCy features\n* test its similarity metrics\n* build a graph data structure for storing (n * n-1) \/ 2 similarity results\n* visualise the clusters of most-similar items in the data\n* plan the next steps","0564962b":"We should now have a clean graph of only hi-similarity edges.","336fdc76":"## Data","22686c49":"## Removing stopwords","1c301fa2":"# Analysing text similarity using spaCy, networkX \n\nThis notebook demonstrates one way of using spaCy to conduct a rapid thematic analysis of a small corpus of comments, and introduces some unusual network visualisations.\nTopics include: \n* [spaCy](https:\/\/spacy.io\/) - an open source NLP library, \n* word vectors, and\n* networkX - an open source network (graph) analysis and visualisation library. \n\nThe notebook is partly a reminder for myself on just how (well) these techniques work, but I hope that others find it useful. I'll continue to update it with more techniques over the coming weeks.\nIf you have any suggestions, feel free to make them in the comments, fork the notebook etc. I'm keen to exchange tips and tricks. \n","53c3a2be":"## Next Steps","6253a226":"NetworkX has several useful layouts implemented, but you can't beat a good spring-embedding layour (a kind of [force-directed graph](https:\/\/en.wikipedia.org\/wiki\/Force-directed_graph_drawing)).\nIn graph terminology, what we see is:\n* a single large [component](https:\/\/en.wikipedia.org\/wiki\/Connected_component_(graph_theory)) at the centre,\n* with several [pendants](https:\/\/proofwiki.org\/wiki\/Definition:Pendant_Vertex) visible at the edges;\n* several smaller components; and \n* a peripheral cloud of [isolates](http:\/\/mathonline.wikidot.com\/isolated-vertices-leaves-and-pendant-edges)\n\nForce-directed graphs are a very intuitive, satisfying, and efficient way to lay out network diagrams. Essentially, every node exerts a repulsive force on every other node. Simultaneously, every connected pair of nodes attract each other. The layout algorithm iterates, finding a layout that balances these forces.","335d6d82":"This next step load the spaCy language model. It generally takes about 13s to load this 'large' model.","e0f60ece":"If you've limited the rows imported, then you may only have Democrat tweets (which occur first in the list).","72821d9a":"## Testing spaCy's similarity function","771dbec1":"Of course, we can specify the layout we want to use, change colours, sizes, etc. The following cell adds the text of the tweets - which can make the layout hard to read.","eeab7090":"Visualising the whole graph, but only those links of weights above a certain cutoff, allows us to get a feel for a good cutoff level to use when visualising the structure. Having filtered out these lower-weighted links, we can clean up the graph by removing the isolates. This will enable the layout engine to show us more of the structure of the components.","6243c12c":"# Plan","d3db4f3e":"## Using spaCy to parse the tweets.","cecacc55":"## Visualising the selected edges"}}