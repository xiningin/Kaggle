{"cell_type":{"86a09baa":"code","6c87eeba":"code","d757b392":"code","fb49d33e":"code","52a99293":"code","5a4a2bbc":"code","966ceb67":"code","2a4a5ac7":"code","bea84840":"code","b93cba0d":"code","80780733":"code","950dda8f":"code","d68ecf64":"code","bf08248f":"code","7f0191f4":"code","97b2b231":"code","8bcd5281":"code","d02e4fc8":"code","fd2811f7":"code","ec7e340c":"code","8d60d77c":"code","5ee3956e":"code","bc5ddeac":"code","23ff642d":"code","07e6dea4":"code","68f58b83":"code","6fe1381b":"code","2588fcd1":"code","895954c2":"code","d5fde241":"code","3d243130":"code","59afcba3":"code","3aaf19cd":"code","1865279d":"code","08428f6d":"code","9b9f20b9":"code","1e179c3b":"code","1bf9d78c":"code","54505928":"code","da70f7e4":"code","e19878fe":"code","d21c543b":"code","32822e58":"code","ea1a5770":"code","5cfeccd1":"code","dbee2cc7":"code","1a971541":"code","aece056f":"code","6cef8908":"code","8963b7c6":"code","6cd12ab7":"code","f4d62498":"code","7e74eef1":"code","f5bfd38e":"code","313f56d4":"code","1ac0a0ba":"code","700cd595":"code","97282b3a":"code","cca3149e":"code","0b42f8fc":"code","bb385ff4":"code","41a4e3dd":"code","a3d68a44":"code","b3239f7f":"code","fb8d0475":"code","dbaac864":"code","51d52be2":"code","ac1a5894":"code","62e9f829":"code","4fa8ae95":"code","9215d7df":"code","654a4d98":"code","383c4a35":"code","fca420c9":"code","d1b548da":"code","8385b6dd":"code","81f858af":"code","b444859f":"code","a0150be6":"code","335b059e":"code","fe3a150f":"code","d46fa43c":"code","77d71172":"code","4171e28c":"code","e39ef913":"code","fafd7474":"code","e28e2dbb":"code","a5e7be40":"code","75b189ab":"code","2df1e798":"markdown","476b606f":"markdown","f232f1d1":"markdown","89bfa4e3":"markdown","922fbb48":"markdown","4ee132bd":"markdown","5ed92e2f":"markdown","9fd4830d":"markdown","19fb66de":"markdown","8ad29917":"markdown","320634e7":"markdown","c435b3ef":"markdown","1139565c":"markdown","fb15c123":"markdown","3b98dd86":"markdown","12286663":"markdown","6a0b8319":"markdown","1ff9852b":"markdown","41f84fcb":"markdown","b94a5be8":"markdown","874bbd29":"markdown","35671603":"markdown","91a099da":"markdown","743742e3":"markdown","b195ca93":"markdown","7421ace3":"markdown","27f0c3c5":"markdown","cfff9835":"markdown","0483a745":"markdown","0cc307dd":"markdown","0eb6bb2f":"markdown","aa3e2572":"markdown","d75b3d31":"markdown","bf2710e7":"markdown","db54ee29":"markdown","9f9da9fe":"markdown","e891e6d1":"markdown","89fc1d3e":"markdown","15ff34d8":"markdown","df587113":"markdown","1223894a":"markdown","f342fed2":"markdown","41888a73":"markdown","05c062ea":"markdown","7c50df9c":"markdown","955b2714":"markdown","17c56a21":"markdown","c22e63d0":"markdown","d31d4d4e":"markdown","bdeb0ccf":"markdown","e5d455ea":"markdown","7dfdf994":"markdown","d281adc3":"markdown","2d688660":"markdown","d0f018b1":"markdown","60a50ec8":"markdown","91be66dd":"markdown","e02de729":"markdown","e0bb4c07":"markdown","8afe2559":"markdown","7929d726":"markdown","b5fccf8d":"markdown","92976fe7":"markdown","c71548d1":"markdown","d88d51d7":"markdown","33ff3e75":"markdown","bb612e0e":"markdown","b0156601":"markdown","ea4a4a0c":"markdown","97bbc02b":"markdown","7a109547":"markdown","77c0fb61":"markdown","6cd56641":"markdown","c4009987":"markdown","9ff0773e":"markdown","5142033c":"markdown","87094440":"markdown","018b1f9e":"markdown","a929ae00":"markdown","2aa735af":"markdown","45d7f6b7":"markdown","8dbb81b7":"markdown","a429e77f":"markdown","9765478e":"markdown","f6eed0d4":"markdown"},"source":{"86a09baa":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Configure matplotlib plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nplt.style.use('fivethirtyeight')\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 11, 9 #Changes default matplotlib plots to this size","6c87eeba":"# Load my EDA helper function created to do some high-level analysis\nclass EDA():\n\n    df = pd.DataFrame()\n    \n    def __init__(self, df):\n        '''\n        Creates EDA object for the DataFrame\n        \n        Note for time series data, have the index be the timestamp prior to creating this Object.\n        \n        :param df : DataFrame\n        '''\n        self.df = df\n        \n    def missing_values(self):\n        '''\n        Checks missing values\n        \n        :return DataFrame\n        \n        '''\n        missing = self.df[self.df.isna().any(axis=1)]\n        \n        print(\"Missing values data\")\n        \n        return missing\n    \n    def duplicate_values(self):\n        duplicates = self.df[self.df.duplicated(subset=None, keep='first')==True]\n        \n        print(\"Duplicate values data\")\n        \n        return duplicates\n        \n    def duplicate_indices(self):\n        '''\n        Check whether the indices have any duplicates\n        \n        :return DataFrame\n        '''        \n        duplicate_indices = self.df[self.df.index.duplicated()==True]\n        \n        print(\"Duplicate indices\")\n        \n        return duplicate_indices\n            \n    def summary(self):\n        '''\n        Return summary\/describe of DataFrame\n        \n        :return DataFrame\n        '''\n        df = self.df.reset_index() # Reset to include the index\n        \n        summary = df.describe(include='all').transpose()\n        \n        print(\"Summary metrics\")\n        \n        return summary\n    \n    def pandas_profiling(self):\n        import pandas_profiling\n        \n        self.df.profile_report(style={'full_width':True})  \n    \n    def histogram_KDE(self):\n        ''' \n        :return seaborn plot\n        '''       \n        sns.pairplot(self.df, diag='kde')\n        sns.distplot(kde=True, fit=[st.norm or st.lognorm])\n        \n    def outliers(self, col):\n        ''' \n        Checks outliers - anything outside of 5% to 95% quartile range\n        \n        :param col : str\n            Name of col to be tested\n            \n        :return DataFrame\n        '''\n        outliers = self.df[~self.df[col].between(self.df[col].quantile(.05), self.df[col].quantile(.95))]\n        \n        print(\"Outliers\")\n        \n        return outliers\n        \n    def missing_timeseries_points(self, freq='D'):\n        '''\n        Checks whether there's any missing data points in continuous time series data.\n        \n        :param freq optional default = 'D' : str\n            Frequency compliant with pandas formatting\n        \n        :return DataFrame\n        '''\n        # First create date range\n        date_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq=freq)\n\n        # Now compare against dataset\n        missing_timeseries = self.df.index[~self.df.index.isin(date_range)]\n        \n        print(\"Missing timeseries data\")\n        \n        return missing_timeseries\n\n    def corr_heatmap(df):\n        fig, ax = plt.subplots(figsize=(10, 6))\n        corr = self.df.corr()\n        hm = sns.heatmap(round(corr,2), annot=True, cmap=\"coolwarm\",fmt='.2f', linewidths=.05)\n        fig.subplots_adjust(top=0.93)\n        title = fig.suptitle('Wine Attributes Correlation Heatmap', fontsize=14)\n\n        plt.show()\n\n    def plot_time_series_seasonal_decomp(self, type='add'):\n        '''\n        Plots seasonal decomposition of timeseries data\n        \n        :return matplotlib Plot\n        '''\n        from statsmodels.tsa.seasonal import seasonal_decompose\n        decomposition = seasonal_decompose(self.df, model='multiplicative')\n\n        fig = decomposition.plot()\n        plt.show()\n   \n    def time_series_ADF(self):            \n        '''\n        Returns Augmented Dickey-Fuller Test\n        '''\n        from statsmodels.tsa.stattools import adfuller as ADF\n\n        series = data['KwH'] # ADF takes series, not DF\n\n        result = ADF(series)\n\n        print('ADF Statistic: %f4.2' % result[0])\n        print('P-value %f4.2' % result[1])","d757b392":"# Load the Data\ndata = pd.read_csv(\"..\/input\/hourly-energy-consumption\/PJME_hourly.csv\")\n\ndata.set_index('Datetime',inplace=True)\n\ndata.index = pd.to_datetime(data.index)\n\ndata_copy = data.copy(deep=True) # Make a deep copy, including a copy of the data and the indices\ndata.head(10)","fb49d33e":"eda_helper = EDA(data)\n\neda_helper.summary()","52a99293":"eda_helper.missing_values()","5a4a2bbc":"eda_helper.duplicate_values()","966ceb67":"eda_helper.duplicate_indices()","2a4a5ac7":"def preprocess_data(df):\n    # We have duplicates, so we need to de-duplicate it - grabbing first value that pops up for a time series\n    df.sort_index()\n    df = df.groupby(df.index).first()\n    \n    #Set freq to H\n    df = df.asfreq('H')\n    \n    return df","bea84840":"data = preprocess_data(data)\n\ndata.index","b93cba0d":"# Let's visualise the data\ndata.plot()\n\nplt.show()","80780733":"# Given there's no missing data, we can resample the data to daily level\ndaily_data = data.resample(rule='D').sum()\n\n# Set frequency explicitly to D\ndaily_data = daily_data.asfreq('D')\n\ndaily_data.head(10)","950dda8f":"# We can confirm it is at the right frequency\ndaily_data.index","d68ecf64":"daily_data.plot()\n\nplt.show()","bf08248f":"daily_data = daily_data.drop([daily_data.index.min(), daily_data.index.max()])","7f0191f4":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(daily_data, model='additive')\n\nfig = decomposition.plot()\nplt.show()","97b2b231":"from statsmodels.tsa.seasonal import seasonal_decompose\nweekly_data = data.resample(rule='W').sum()\ndecomposition = seasonal_decompose(weekly_data, model='additive') # Aggregate to weekly level\n\nfig = decomposition.plot()\nplt.show()","8bcd5281":"daily_data.index.day_name()","d02e4fc8":"# Create new dataset for heatmap\nheatmap_data = daily_data.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Weekday_Name'] = daily_data.index.day_name()\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Weekday_Name']).sum()\n\n# Reset index \nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2018]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Weekday_Name', values='PJME_MW')\n\n# Reorder columns\nheatmap_data = heatmap_data[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n\nheatmap_data.head(100)","fd2811f7":"# Visualise electricity load via Heatmap\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Day of Week')","ec7e340c":"# Create new dataset for heatmap\nheatmap_data = data.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Hour'] = data.index.hour\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Hour']).sum()\n\n# Reset index \nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2018]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Hour', values='PJME_MW')\n\nheatmap_data.head(100)","8d60d77c":"# Visualise electricity load via Heatmap\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Hour of Day')","5ee3956e":"# Create new dataset for heatmap\nheatmap_data = daily_data.copy()\n\n# First we need to add weekdays as a column\nheatmap_data['Month'] = daily_data.index.month_name()\n\n# Next we add the year as column and group the data up to annual day of week level\nheatmap_data['Year'] =  heatmap_data.index.year\nheatmap_data = heatmap_data.groupby(['Year', 'Month']).sum()\n\n# Reset index\nheatmap_data = heatmap_data.reset_index()\n\n# We drop off 2018 because it's not a full year\nheatmap_data = heatmap_data[heatmap_data['Year'] != 2018]\n\n# Pivot it to a uniform data format for heatmaps\nheatmap_data = heatmap_data.pivot(index='Year', columns='Month', values='PJME_MW')\n\n# Reorder columns\nheatmap_data = heatmap_data[['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']]\n\nheatmap_data.head(10)","bc5ddeac":"# Visualise electricity load via Heatmap\nsns.heatmap(heatmap_data, linewidths=.5, cmap='YlOrRd', cbar=True, cbar_kws={\"format\": '%1.0f MWh'}).set_title('Heatmap - by Day of Week')","23ff642d":"# First let's load the data\nweather_data_2017 = pd.read_csv(\"..\/input\/ncei-climate-data-washington-dc-temperature\/Washington_DC_Weather_Avg_Temp_2016-17.csv\")\nweather_data_2018 = pd.read_csv(\"..\/input\/ncei-climate-data-washington-dc-temperature\/Washington_DC_Weather_Avg_Temp_2018.csv\")\n\nweather_data = weather_data_2017.append(weather_data_2018)\n\nweather_data.head(10)","07e6dea4":"eda_helper = EDA(weather_data)\n\neda_helper.summary()","68f58b83":"# Clean data and aggregate\nweather_data = weather_data[['DATE', 'TAVG']]\nweather_data = weather_data[~weather_data['TAVG'].isna()]\n\nweather_data.groupby(['DATE']).mean()\nweather_data = weather_data.set_index('DATE')\nweather_data.index = pd.to_datetime(weather_data.index)\n\n# Sort to make sure plotting works\nweather_data = weather_data.sort_values(by='DATE', ascending=True)","6fe1381b":"plt.plot(weather_data, label='Average Temp (\u00b0C)', color='lightseagreen')\n\n# Plot Labels, Legends etc\nplt.title('Washington D.C. Average Temperature')\nplt.legend(loc='best')\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Degrees (\u00b0C)\")\nplt.legend(loc='best')\n\nplt.show()","2588fcd1":"fig, ax = plt.subplots(2,1, figsize=(20,20))\n\n# Plot 1\nax[0].plot(weather_data['2016-01-01':'2018-08-02'], label='Average Temp (\u00b0C)', color='lightseagreen')\nax[0].set_title('Washington D.C. Average Temperature')\nax[0].set_ylabel(\"Degrees (\u00b0C)\")\nax[0].legend(loc='best')\n\n# Plot 2\nax[1].plot(daily_data['2016-01-01':'2018-08-02'], label='PJME MW Daily Load (MW)', color='royalblue')\nax[1].set_title('PJM East Region Electricity Load')\nax[1].set_xlabel('Timestamp')\nax[1].set_ylabel(\"Load (MW)\")\nax[1].legend(loc='best')\n\n# Add vertical lines to emphasis point\nimport datetime as dt\nax[0].axvline(dt.datetime(2016, 8, 15), color='red', linestyle='--')\nax[1].axvline(dt.datetime(2016, 8, 15), color='red', linestyle='--')\nax[0].axvline(dt.datetime(2016, 12, 15), color='red', linestyle='--')\nax[1].axvline(dt.datetime(2016, 12, 15), color='red', linestyle='--')\nax[0].axvline(dt.datetime(2017, 7, 15), color='red', linestyle='--')\nax[1].axvline(dt.datetime(2017, 7, 15), color='red', linestyle='--')\nax[0].axvline(dt.datetime(2018, 1, 1), color='red', linestyle='--')\nax[1].axvline(dt.datetime(2018, 1, 1), color='red', linestyle='--')\n\nplt.show()","895954c2":"correlation = daily_data['2016-01-01':'2018-08-02']['PJME_MW'].corr(weather_data['2016-01-01':'2018-08-02']['TAVG'], method='pearson')\n\nprint(\"The correlation between the PJM East Region electricity load and Washington D.C. average temperature is: {}%\".format(correlation*100))","d5fde241":"from statsmodels.tsa.stattools import adfuller as ADF\n\nseries = daily_data['PJME_MW'] # ADF takes series, not DF\n\nresult = ADF(series)\n\nprint('ADF Statistic: ', result[0])\nprint('P-value: {:.20f}'.format(result[1]))","3d243130":"from statsmodels.stats.diagnostic import het_breuschpagan as BP\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nbp_data = daily_data.copy()\nbp_data['Time_Period'] = range(1, len(bp_data)+1) # Convert time series points into consecutive ints\n\nformula = 'PJME_MW ~ Time_Period' # ie PJME MW depends on Time Period (OLS auto adds Y intercept)\n\n# Next we apply Ordinary Linear Square baseline regression model - as baseline test\nmodel = ols(formula, bp_data).fit()\n\nresult = BP(model.resid, model.model.exog)\n\nprint('ADF Statistic: ', result[0])\nprint('P-value: {:.20f}'.format(result[1]))","59afcba3":"from statsmodels.tsa.stattools import acf\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n#fig, ax = plt.subplots(2,1)\n\n# Plot the acf function\nplot_acf(daily_data['PJME_MW'],lags=720) #alpha 1 suppresses CI\n\nplt.show()","3aaf19cd":"plot_pacf(daily_data['PJME_MW'],lags=720) #alpha 1 suppress CI\n\nplt.show()","1865279d":"# First we split it up between train and test\n# We will aim for a 12 month forecast horizon (ie predict the last 12 months in the dataset)\ncutoff = '2017-08-03'\n\ndaily_data.sort_index()\n\ntrain = daily_data[:cutoff]\ntest = daily_data[cutoff:]","08428f6d":"baseline_prediction = train['2016-08-03':'2017-08-02']\n\nbaseline_prediction.index = pd.date_range(start='2017-08-03', end='2018-08-02', freq='D')\n\nbaseline_prediction.tail()","9b9f20b9":"# Create function that extracts DOW and MOnth\ndef engineer_date_attributes(df):\n    '''\n    \n    Pre-supposes input df has datetime as index.\n    \n    :param df : DataFrame\n    \n    return: Dataframe\n    '''\n    \n    \n    df['DOW'] = df.index.dayofweek\n    df['Month'] = df.index.month\n    df['WOY'] = df.index.weekofyear\n    df = df.reset_index()\n\n    df = df.set_index(['Month', 'DOW', 'WOY'])\n\n    return df\n\n# Calculate average of current by DOW and Month\n# THen get the 'shape' of the curve by day of week\n# That is represent, for every DOW | MONTH | YOW, what the % of the DOW | Month is\ncurrent_df = train['2014-08-03':'2017-08-02']\ncurrent_df = engineer_date_attributes(current_df)\ncurrent_df['SHAPE_AVG'] = current_df.reset_index().groupby(by=['Month', 'DOW']).mean()['PJME_MW']\ncurrent_df['PCT_SHAPE_AVG'] = current_df['PJME_MW'] \/ current_df['SHAPE_AVG']\ncurrent_df = current_df.reset_index().groupby(by=['Month', 'DOW', 'WOY']).mean()\n\ncurrent_df.head(21)\n#current_df.count()","1e179c3b":"\n# Create predict\/future dataframe\nfuture_df = pd.DataFrame()\nfuture_df['Datetime'] = pd.date_range(start='2017-08-03', end='2018-08-02', freq='D')\nfuture_df = future_df.set_index('Datetime')\nfuture_df = engineer_date_attributes(future_df)\n\n# Use DOW, Month WOY to forward project electricity data into forecast horizon, applying the shape of the current year\nbaseline_v2_prediction = future_df.merge(current_df[['SHAPE_AVG', 'PCT_SHAPE_AVG']], how='left', left_on=['Month', 'DOW', 'WOY'], right_on=['Month', 'DOW', 'WOY'])\nbaseline_v2_prediction['PJME_MW'] = baseline_v2_prediction['PCT_SHAPE_AVG'] * baseline_v2_prediction['SHAPE_AVG']\n\nbaseline_v2_prediction = baseline_v2_prediction.set_index('Datetime').drop(columns=['SHAPE_AVG', 'PCT_SHAPE_AVG'])\n\nbaseline_v2_prediction.head(14)","1bf9d78c":"# Evaluate it's performance using Mean Absolute Error (MAE)\nfrom statsmodels.tools.eval_measures import meanabs\n\nprint(\"MAE Baseline: {:.20f}\".format(meanabs(test['PJME_MW'], baseline_v2_prediction['PJME_MW'])))","54505928":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(baseline_prediction, label='Baseline', color='orange')\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('Baseline Model (Daily) - Results')\n\n# For clarify, let's limit to only 2017 onwards\nplt.xlim(datetime(2017, 1, 1),datetime(2018, 9, 1))\n\nplt.show()","da70f7e4":"# First construct the residuals - basically the errors\nnaive_errors = test.copy()\nnaive_errors['PJME_MW_PREDICTION'] = baseline_prediction['PJME_MW']\nnaive_errors['error'] = naive_errors['PJME_MW_PREDICTION'] - naive_errors['PJME_MW']\n\n# Let's visually see the errors via scatterplot\nplt.scatter(naive_errors.index, naive_errors['error'], label='Residual\/Errors')\n\n# Plot Labels, Legends etc\nplt.title('Naive One-Year Persistence - Residual\/Errors Distribution')\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\n\nplt.show()","e19878fe":"# Plot Histogram with Kernel Density Estimation (KDE)\nsns.distplot(naive_errors['error'], kde=True);\n\n# Plot Labels, Legends etc\nplt.title('Naive One-Year Persistence - Residual\/Errors Distribution')","d21c543b":"from statsmodels.graphics.tsaplots import plot_acf\n\n# Plot the acf function\nplot_acf(naive_errors['error'],lags=300) #alpha 1 suppresses CI\n\nplt.title('Naive One-Year Persistence - Residual\/Errors Autocorrelation')\nplt.show()","32822e58":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# First we split it up between train and test\nhtrain = train['PJME_MW'] # HWES takes series, not DF\nhtest = test['PJME_MW'] # HWES takes series, not DF\n\nmodel = ExponentialSmoothing(\n    htrain\n    ,trend='add'\n    ,seasonal='add'\n    ,freq='D'\n    ,seasonal_periods=90 #Default is auto estimated - 4 is quarterly and 7 is weekly\n).fit(\n    optimized=True # Default is True - auto estimates the other parameters using Grid Search\n    ,use_basinhopping=True # Uses Basin Hopping Algorithm for optimising parameters\n    ,use_boxcox='log' #Boxcox transformation via log\n    #,smoothing_level= # Alpha\n    #,smoothing_slope= # Beta\n    #,smoothing_seasonal= # Gamma\n)\n\nHWES_prediction = model.predict(start=htest.index[0], end=htest.index[-1])\nHWES_prediction = HWES_prediction.to_frame().rename(columns={0: 'PJME_MW'})\n\nprint(\"Finished training and predicting\")\n\n# Let's see what the model did\nmodel.summary()","ea1a5770":"# Evaluate it's performance using Mean Absolute Error (MAE)\nfrom statsmodels.tools.eval_measures import meanabs\n\nprint(\"MAE HWES ADD: {:.20f}\".format(meanabs(htest, HWES_prediction['PJME_MW'])))","5cfeccd1":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(HWES_prediction['PJME_MW'], label='HWES', color='orange')\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('Holtwinters Model (Daily) - Results')\n\n# For clarify, let's limit to only 2017 onwards\nplt.xlim(datetime(2017, 1, 1),datetime(2018, 9, 1))\n\nplt.show()","dbee2cc7":"# Feature Engineering first\n\ndef preprocess_xgb_data(df, lag_start=1, lag_end=365):\n    '''\n    Takes data and preprocesses for XGBoost.\n    \n    :param lag_start default 1 : int\n        Lag window start - 1 indicates one-day behind\n    :param lag_end default 365 : int\n        Lag window start - 365 indicates one-year behind\n        \n    Returns tuple : (data, target)\n    '''\n    # Default is add in lag of 365 days of data - ie make the model consider 365 days of prior data\n    for i in range(lag_start,lag_end):\n        df[f'PJME_MW {i}'] = df.shift(periods=i, freq='D')['PJME_MW']\n\n    df.reset_index(inplace=True)\n\n    # Split out attributes of timestamp - hopefully this lets the algorithm consider seasonality\n    df['date_epoch'] = pd.to_numeric(df['Datetime']) # Easier for algorithm to consider consecutive integers, rather than timestamps\n    df['dayofweek'] = df['Datetime'].dt.dayofweek\n    df['dayofmonth'] = df['Datetime'].dt.day\n    df['dayofyear'] = df['Datetime'].dt.dayofyear\n    df['weekofyear'] = df['Datetime'].dt.weekofyear\n    df['quarter'] = df['Datetime'].dt.quarter\n    df['month'] = df['Datetime'].dt.month\n    df['year'] = df['Datetime'].dt.year\n    \n    x = df.drop(columns=['Datetime', 'PJME_MW']) #Don't need timestamp and target\n    y = df['PJME_MW'] # Target prediction is the load\n    \n    return x, y","1a971541":"example_data = train.copy() #Otherwise it becomes a pointer\n\nexample_x, example_y = preprocess_xgb_data(example_data)\n\nexample_x.head(10)","aece056f":"xtrain = train.copy() #Otherwise it becomes a pointer\nxtest = test.copy() # Otherwise it becomes a pointer\n\ntrain_feature, train_label = preprocess_xgb_data(xtrain)\ntest_feature, test_label = preprocess_xgb_data(xtest)","6cef8908":"#Train and predict using XGBoost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, train_test_split\n\n# We will try with 1000 trees and a maximum depth of each tree to be 5\n# Early stop if the model hasn't improved in 100 rounds\nmodel = XGBRegressor(\n    max_depth=6 # Default - 6\n    ,n_estimators=1000\n    ,booster='gbtree'\n    ,colsample_bytree=1 # Subsample ratio of columns when constructing each tree - default 1\n    ,eta=0.3 # Learning Rate - default 0.3\n    ,importance_type='weight' # Default is gain\n)\nmodel.fit(\n    train_feature\n    ,train_label\n    ,eval_set=[(train_feature, train_label)]\n    ,eval_metric='mae'\n    ,verbose=True\n    ,early_stopping_rounds=100 # Stop after 100 rounds if it doesn't after 100 times\n)\n\nxtest['PJME_MW Prediction'] = model.predict(test_feature)\nXGB_prediction = xtest[['Datetime', 'PJME_MW Prediction']].set_index('Datetime')","8963b7c6":"from sklearn.metrics import mean_absolute_error\n\nprint(\"MAE XGB: {:.20f}\".format(mean_absolute_error(test_label, XGB_prediction['PJME_MW Prediction'])))","6cd12ab7":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(XGB_prediction, label='XGB One-Step Ahead', color='orange')\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('XGBoost Model One-Step Ahead (Daily) - Results')\n\n# For clarify, let's limit to only 2015 onwards\nplt.xlim(datetime(2015, 1, 1),datetime(2018, 10, 1))\n\n\nplt.show()","f4d62498":"import xgboost as xgb\n\nxgb.plot_importance(model, max_num_features=10, importance_type='weight') # \"weight\u201d is the number of times a feature appears in a tree\n\nplt.show()","7e74eef1":"# So because we need the lag data, we need to preprocess then do the split\nall_data = daily_data.copy()\n\nfeature, label = preprocess_xgb_data(all_data, lag_start=365, lag_end=720)\n\n# We will aim for a 12 month forecast horizon (ie predict the last 365 days in the dataset)\ntrain_feature = feature[:-365]\ntrain_label = label[:-365]\n\ntest_feature = feature[-365:]\ntest_label = label[-365:]","f5bfd38e":"#Train and predict using XGBoost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, train_test_split\n\n# We will try with 1000 trees and a maximum depth of each tree to be 5\n# Early stop if the model hasn't improved in 100 rounds\nmodel = XGBRegressor(\n    max_depth=6 # Default - 6\n    ,n_estimators=1000\n    ,booster='gbtree'\n    ,colsample_bytree=1 # Subsample ratio of columns when constructing each tree - default 1\n    ,eta=0.3 # Learning Rate - default 0.3\n    ,importance_type='gain' # Default is gain\n)\nmodel.fit(\n    train_feature\n    ,train_label\n    ,eval_set=[(train_feature, train_label)]\n    ,eval_metric='mae'\n    ,verbose=True\n    ,early_stopping_rounds=100 # Stop after 100 rounds if it doesn't after 100 times\n)\n\nxtest['PJME_MW Prediction'] = model.predict(test_feature)\nXGB_prediction_no_lag = xtest[['Datetime', 'PJME_MW Prediction']].set_index('Datetime')\nXGB_prediction_no_lag = XGB_prediction_no_lag.rename(columns={'PJME_MW Prediction': 'PJME_MW'})","313f56d4":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(XGB_prediction_no_lag, label='XGB No Lag', color='orange')\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('XGBoost Model (Daily) - Results')\n\n# For clarify, let's limit to only 2015 onwards\nplt.xlim(datetime(2015, 1, 1),datetime(2018, 10, 1))\n\nplt.show()","1ac0a0ba":"import xgboost as xgb\n\nxgb.plot_importance(model, max_num_features=10, importance_type='gain') # gain is how much each feature contributed to 'improvement' of tree\n\nplt.show()","700cd595":"# Housekeeping for SHAP\n!conda install -c conda-forge shap=0.39 -y\n\nimport shap\n\nprint(shap.__version__)","97282b3a":"explainer = shap.Explainer(\n    model,\n    algorithm=\"tree\" # We know it's tree-based model, so use TreeExplainer\n)\nshap_values = explainer.shap_values(train_feature)\n\n# Let's see the waterfall visualisation\n# Due to bug with Pandas compatibility with 0.39, workaround:\n# https:\/\/github.com\/slundberg\/shap\/issues\/1420\nshap.plots._waterfall.waterfall_legacy(\n    explainer.expected_value, \n    shap_values[0],\n    feature_names=train_feature.columns.tolist(),\n    max_display=20 # max no. of features to plot\n)","cca3149e":"print(\"Explaining prediction {}\".format(pd.to_datetime(train_feature['date_epoch']).values[1]))\n\nshap.force_plot(\n    base_value=explainer.expected_value, \n    shap_values=shap_values[1],\n    feature_names=train_feature.columns.tolist(),\n    matplotlib=True, # Kaggle doesn't like JS\n    plot_cmap='RdBu',\n    figsize=(30, 5),\n    show=True\n)\n\nplt.tight_layout()","0b42f8fc":"print(\"Explaining prediction {}\".format(pd.to_datetime(train_feature['date_epoch']).values[870]))\n\nshap.force_plot(\n    base_value=explainer.expected_value, \n    shap_values=shap_values[870],\n    feature_names=train_feature.columns.tolist(),\n    matplotlib=True, # Kaggle doesn't like JS\n    plot_cmap='RdBu',\n    figsize=(30, 5),\n    show=True\n)\n\nplt.tight_layout()","bb385ff4":"# First construct the residuals - basically the errors\nxgboost_errors = XGB_prediction_no_lag.copy()\nxgboost_errors['PJME_MW_ACTUAL'] = test.copy()\nxgboost_errors['error'] = xgboost_errors['PJME_MW'] - xgboost_errors['PJME_MW_ACTUAL']","41a4e3dd":"# Let's visually see the errors via scatterplot\nplt.scatter(xgboost_errors.index, xgboost_errors['error'], label='Residual\/Errors')\n\n# Plot Labels, Legends etc\nplt.title('XGBoost - Residual\/Errors Distribution')\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\n\nplt.show()","a3d68a44":"# Plot Histogram with Kernel Density Estimation (KDE)\nsns.distplot(xgboost_errors['error'], kde=True);\n\n# Plot Labels, Legends etc\nplt.title('XGBoost - Residual\/Errors Distribution')","b3239f7f":"from statsmodels.graphics.tsaplots import plot_acf\n\n# Plot the acf function\nplot_acf(xgboost_errors['error'],lags=300) #alpha 1 suppresses CI\n\nplt.title('XGBoost - Residual\/Errors Autocorrelation')\nplt.show()","fb8d0475":"# So because we need the lag data, we need to preprocess then do the split\nall_data = daily_data.copy()\n\n# Create train test dataset using XGBoost preprocessing (365 days top 720 days lag)\nfeature, label = preprocess_xgb_data(all_data, lag_start=365, lag_end=720)\n\n# We will aim for a 12 month forecast horizon (ie predict the last 365 days in the dataset)\ntrain_feature = feature[:-365]\ntrain_label = label[:-365]\n\ntest_feature = feature[-365:]\ntest_label = label[-365:]\n\ntrain_feature = train_feature.fillna(0)\ntest_feature = test_feature.fillna(0)\n\n# train_feature.drop(columns=['date_epoch']) #Don't need timestamp\n# test_feature.drop(columns=['date_epoch']) #Don't need timestamp\n\n# Scale dataset\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ntrain_feature_scaled = scaler.fit_transform(train_feature)\ntest_feature_scaled = scaler.transform(test_feature)","dbaac864":"# Create Time Series k-fold cross validation\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5) # in this case 5-fold\n\n#Train and predict using LASSO\nfrom sklearn.linear_model import LassoCV\n\nmodel = LassoCV(\n    alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1,0.3, 0.6, 1]\n    ,max_iter=1000 # 1000 iterations\n    ,random_state=42\n    ,cv=tscv\n    ,verbose=True\n)\nmodel.fit(\n    train_feature_scaled\n    ,train_label\n)\nLASSO_prediction = xtest.copy()\nLASSO_prediction['PJME_MW Prediction'] = model.predict(test_feature_scaled)\nLASSO_prediction = LASSO_prediction[['Datetime', 'PJME_MW Prediction']].set_index('Datetime')\nLASSO_prediction = LASSO_prediction.rename(columns={'PJME_MW Prediction': 'PJME_MW'})\n\nLASSO_prediction","51d52be2":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(LASSO_prediction, label='LASSO', color='orange')\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('LASSO Model (Daily) - Results')\nplt.tight_layout()\nplt.grid(True)\n\n\n# For clarify, let's limit to only 2015 onwards\nplt.xlim(datetime(2015, 1, 1),datetime(2018, 10, 1))         \n\nplt.show()","ac1a5894":"# Plot feature importance by way of coefficients    \n\n# Create DataFrame\ncoefs = pd.DataFrame(model.coef_, train_feature.columns)\ncoefs.columns = [\"coef\"]\n\n# Only grab the Top 10 Coefficients\ncoefs[\"abs\"] = coefs.coef.apply(np.abs)\ncoefs = coefs.sort_values(by=\"abs\", ascending=False).head(10)\ncoefs = coefs.drop([\"abs\"], axis=1)\n\n# Plot\ncoefs.coef.plot(kind='bar')\n\n# Plot title and x-axis line\nplt.title(\"Coefficients - Feature Importance\")\nplt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","62e9f829":"# First construct the residuals - basically the errors\nlasso_errors = LASSO_prediction.copy()\nlasso_errors['PJME_MW_ACTUAL'] = test.copy()\nlasso_errors['error'] = lasso_errors['PJME_MW'] - lasso_errors['PJME_MW_ACTUAL']","4fa8ae95":"# Plot Histogram with Kernel Density Estimation (KDE)\nsns.distplot(lasso_errors['error'], kde=True)\n\n# Plot Labels, Legends etc\nplt.title('LASSO - Residual\/Errors Distribution')\n\nplt.show()","9215d7df":"# Plot the acf function\nplot_acf(lasso_errors['error'],lags=300) #alpha 1 suppresses CI\n\nplt.title('LASSO - Residual\/Errors Autocorrelation')\nplt.show()","654a4d98":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tools.eval_measures import meanabs\n\n# Equivalent to R's Auto ARIMA to get the optimal parameters\n#import pmdarima as pm\n#model = pm.auto_arima(htrain, seasonal=True, stationary=True, stepwise=True, trace=True, suppress_warnings=True)\n\n# First we split it up between train and test\nhtrain = train['PJME_MW'] # SARIMAX takes series, not DF\nhtest = test['PJME_MW'] # SARIMAX takes series, not DF\n\n# Next define hyperparameters. Default is AR model (1,0,0)(0,0,0,0)\np = 1 # AR order\nd = 0 # I degree\nq = 1 # MA window\nP = 0 # AR seasonal order\nD = 1 # I seasonal order\nQ = 2 # MA seasonal order\nm = 6 # Seasonality period length\n\nmodel = SARIMAX(\n    htrain,\n    order=(p, d, q),\n    seasonal_order=(P, D, Q, m)\n    ,enforce_stationarity=False\n    ,enforce_invertibility=False\n).fit(\n    maxiter=50 # Default is 50\n)\n\nresults = model.get_prediction(start=htest.index[0], end=htest.index[-1], dynamic=False)\nSARIMA_prediction_CI = results.conf_int(alpha=(1-0.8)) # 80% CI\nSARIMA_prediction = results.predicted_mean\nSARIMA_prediction = SARIMA_prediction.to_frame().rename(columns={0: 'PJME_MW'})\n\n# Evaluate it's performance using Mean Absolute Error (MAE)\nprint(\"Finished training and predicting. MAE SARIMA: {:.20f}. AIC: {}. Parameters: p,d,q,P,D,Q,m: \".format(meanabs(htest, SARIMA_prediction['PJME_MW']), model.aic), p,d,q,P,D,Q,m)","383c4a35":"# Let's see what the model did\nmodel.plot_diagnostics(figsize=(15, 12))\nplt.show()","fca420c9":"# Evaluate it's performance using Mean Absolute Error (MAE)\nfrom statsmodels.tools.eval_measures import meanabs\n\nprint(\"MAE SARIMA: {:.20f}\".format(meanabs(htest, SARIMA_prediction['PJME_MW'])))","d1b548da":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(SARIMA_prediction['PJME_MW'], label='SARIMA', color='orange')\n\n# Plot Confidence Interval\nplt.fill_between(\n    SARIMA_prediction.index,\n    SARIMA_prediction_CI['lower PJME_MW'],\n    SARIMA_prediction_CI['upper PJME_MW'],\n    color='skyblue',\n    alpha=0.7, # 70% transparency\n    label='80% CI'\n)\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('SARIMA Model (Daily) - Results')\n\n# For clarify, let's limit to only 2017 onwards\nplt.xlim(datetime(2017, 1, 1),datetime(2018, 9, 1))\n\nplt.show()","8385b6dd":"from fbprophet import Prophet\n\nftrain = train.reset_index().rename(columns={'Datetime':'ds', 'PJME_MW': 'y'}) # Prophet takes ds and y as column names only\n\nmodel = Prophet(\n    n_changepoints=25 # Default is 25\n    ,changepoint_prior_scale=0.05 # Default is 0.05\n    ,seasonality_mode='additive'\n    ,interval_width=0.8 # CI - default is 0.8 or 80%\n)\nmodel.fit(ftrain)\n\n# Create the future dataframe with date range that will be used to test accuracy\nfuture_df = test.reset_index()['Datetime'].to_frame().rename(columns={\"Datetime\":'ds'})\n\n# Predict the future\nforecast = model.predict(future_df)\nPROPHET_prediction = forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']][cutoff:]\nPROPHET_prediction = PROPHET_prediction.rename(columns={'yhat': 'PJME_MW'})\n\nprint(\"Finished training and predicting\")","81f858af":"model.plot(forecast)\n\nplt.show()","b444859f":"# Let's visually see the results\nplt.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\nplt.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\nplt.plot(PROPHET_prediction['PJME_MW'], label='Prophet', color='orange')\n\n# Plot Confidence Interval\nplt.fill_between(\n    PROPHET_prediction.index,\n    PROPHET_prediction['yhat_lower'],\n    PROPHET_prediction['yhat_upper'],\n    color='skyblue',\n    alpha=0.7, # 70% transparency\n    label='80% CI'\n)\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\nplt.title('Prophet Model (Daily) - Results')\n\n# For clarify, let's limit to only 2017 onwards\nplt.xlim(datetime(2017, 1, 1),datetime(2018, 9, 1))\n\nplt.show()","a0150be6":"model.plot_components(forecast)\n\nplt.show()","335b059e":"# First construct the residuals - basically the errors\nprophet_errors = PROPHET_prediction.copy()\nprophet_errors['PJME_MW_ACTUAL'] = test['PJME_MW']\nprophet_errors['error'] = prophet_errors['PJME_MW'] - prophet_errors['PJME_MW_ACTUAL']","fe3a150f":"# Let's visually see the errors via scatterplot\nplt.scatter(prophet_errors.index, prophet_errors['error'], label='Residual\/Errors')\n\n# Plot Labels, Legends etc\nplt.title('Prophet - Residual\/Errors Distribution')\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')\n\nplt.show()","d46fa43c":"# Plot Histogram with Kernel Density Estimation (KDE)\nsns.distplot(prophet_errors['error'], kde=True);\n\n# Plot Labels, Legends etc\nplt.title('Prophet - Residual\/Errors Distribution')","77d71172":"# Let's get the top 10 over forecasts (i.e. where the error is the highest negative number)\ntop_10_errors = prophet_errors.sort_values('error', ascending=True)[['PJME_MW_ACTUAL', 'PJME_MW', 'error']].head(10)\n\nplt.scatter(top_10_errors.index, top_10_errors['PJME_MW'], label='Predicted MW')\nplt.scatter(top_10_errors.index, top_10_errors['PJME_MW_ACTUAL'], label='Actual MW')\n\n# Labels, Titles, etc.\nplt.title('Prophet - Top 10 Errors')\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"MWh\")\nplt.legend(loc='best')","4171e28c":"top_10_errors.head(10)","e39ef913":"print(\"MAE Baseline: {:.2f}\".format(meanabs(test['PJME_MW'], baseline_prediction['PJME_MW'])))\nprint(\"MAE HoltWinters: {:.2f}\".format(meanabs(test['PJME_MW'], HWES_prediction['PJME_MW'])))\nprint(\"MAE XGBoost: {:.2f}\".format(mean_absolute_error(test_label, XGB_prediction_no_lag['PJME_MW'])))\nprint(\"MAE LASSO: {:.2f}\".format(mean_absolute_error(test_label, LASSO_prediction['PJME_MW'])))\nprint(\"MAE SARIMA: {:.2f}\".format(meanabs(test['PJME_MW'], SARIMA_prediction['PJME_MW'])))\nprint(\"MAE Prophet: {:.2f}\".format(meanabs(test['PJME_MW'], PROPHET_prediction['PJME_MW'])))","fafd7474":"def MAPE(y_true, y_pred): \n    '''Function to calculate MAPE'''\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","e28e2dbb":"print(\"MAE Baseline: {:.2f}%\".format(MAPE(test['PJME_MW'], baseline_prediction['PJME_MW'])))\nprint(\"MAE HoltWinters: {:.2f}%\".format(MAPE(test['PJME_MW'], HWES_prediction['PJME_MW'])))\nprint(\"MAE XGBoost: {:.2f}%\".format(MAPE(test['PJME_MW'], XGB_prediction_no_lag['PJME_MW'])))\nprint(\"MAE LASSO: {:.2f}%\".format(MAPE(test['PJME_MW'], LASSO_prediction['PJME_MW'])))\nprint(\"MAE SARIMA: {:.2f}%\".format(MAPE(test['PJME_MW'], SARIMA_prediction['PJME_MW'])))\nprint(\"MAE Prophet: {:.2f}%\".format(MAPE(test['PJME_MW'], PROPHET_prediction['PJME_MW'])))","a5e7be40":"def plot_model_result(ax, prediction, model_name, color):\n    '''\n    Plot model results.\n    \n    prediction : DataFrame\n    model_name : str\n    \n    return ax\n    '''\n    # Training and Test Actuals\n    ax.scatter(x=train.index, y=train['PJME_MW'], label='Training Data', color='black')\n    ax.scatter(x=test.index, y=test['PJME_MW'], label='Test Actuals', color='red')\n\n    # Model Results\n    ax.plot(prediction['PJME_MW'], label=model_name, color=color, alpha=0.7)\n    \n    # For clarify, let's limit to only August 2016 onwards\n    ax.set_xlim(datetime(2016, 8, 1),datetime(2018, 10, 1))\n\n    # Set Y Axis\n    ax.set_ylim(500000, 1100000)\n    \n    # Set Axis labels\n    ax.set_ylabel(\"MWh\")\n    ax.legend(loc='best')\n    ax.set_title(\n        \"{}: MAPE: {:.2f}% | MAE: {:.2f}\".format(\n            model_name,MAPE(test[\"PJME_MW\"], prediction[\"PJME_MW\"]),\n            mean_absolute_error(test[\"PJME_MW\"], prediction[\"PJME_MW\"])\n            )\n        , fontsize=40\n    )\n\n    return ax","75b189ab":"fig, ax = plt.subplots(6,1, figsize=(30,40))\n\n# Plot Labels, Legends etc\nplt.xlabel(\"Timestamp\")\n\nax[0] = plot_model_result(ax[0], baseline_prediction, model_name='Baseline (Naive)', color='midnightblue')\nax[1] = plot_model_result(ax[1], XGB_prediction_no_lag, model_name='XGBoost', color='deepskyblue')\nax[2] = plot_model_result(ax[2], PROPHET_prediction, model_name='Prophet', color='steelblue')\nax[3] = plot_model_result(ax[3], LASSO_prediction, model_name='LASSO', color='royalblue')\nax[4] = plot_model_result(ax[4], HWES_prediction, model_name='Holtwinters', color='lightsteelblue')\nax[5] = plot_model_result(ax[5], SARIMA_prediction, model_name='SARIMA', color='dodgerblue')\n\nplt.tight_layout(pad=3.0)\nplt.show()","2df1e798":"Now for feature importance, let's also have a look at it using SHAP. SHAP is a interpretability model designed to explain how the features 'contribute' to the overall predictions of the models. \n\nSHAP values are expressed based on a particular baseline\/expected value.\nValues can be negative (i.e. reduce the output value below the baseline value) or positive (i.e. increase the output value above the baseline). The baseline value is generally the average of all predictions.\n\nSHAP is both a global and local interpretability model. That is:\n- it can explain how the model generally deals with predictions (similiar how to feature importance works in general)\n- it can explain how features 'contributed' to a specific prediction","476b606f":"Next we'll train the model using sklearn's time series split cross validation method.\n\nIn this case, we'll create a 5-fold split.","f232f1d1":"Now there's a tail end where it's not a full day, so it's dropping off.\n\nFor our purposes, we will just delete that part day.","89bfa4e3":"Seems like yesterday's value is the biggest factor in determining today's value! This, again, makes sense, given how the autocorrelation function showed yesterday's value had the biggest correlation with today's value.","922fbb48":"Now let's do the same thing but over the hours of a day (to sort of see peak operating hours)","4ee132bd":"Interestingly, the correlation between the PJM East Region electricity load and Washington D.C. average temperature is only 13.03%. What this means is, extreme peaks in weather causes spikes, but weather in general isn't really useful for predicting electricity usage.","5ed92e2f":"Now we need to split the time series between training and test - like what we did before.\n\nCross validation is harder in this case, as the datasets need to be sequential.\n\nWe will also need to specify features and labels (ie the target we want to predict).","9fd4830d":"Now let's also look at Mean Absolute Percentage Error (MAPE)\n\nUnlike MAE, MAPE has issues in its calculations, namely when the actual value is 0 (can't divide by zero) and negative values can't go beyond -100%.\n\nRegardless, MAPE is a good 'sense-check' to see which model is proportionally better.","19fb66de":"Let's visualise the data again","8ad29917":"## Training\nProphet works quite well out of the box, so I just stuck with the default hyperparameters.\n\nThe results can be seen below, with the black dots representing historical points and the blue line representing the prediction:","320634e7":"Next we'll process it so the data is at the hourly level:","c435b3ef":"Now let's see both side by side - I've added in red lines to emphasis particular peaks (cold and hot):","1139565c":"Let's zoom into 2017 onwards and visualise it (with the consistent graph format as above):","fb15c123":"So let's run the test for Stationarity and see the results: ","3b98dd86":"Like Holtwinters, the training and testing data was split between 2002 to 2017 and 2017 to 2018.\n\nThe results were as follows:\n","12286663":"So most of the points are within the shaded blue (ie confidence interval), indicating there's no statistically significant autocorrelation going on. This is good, as if there was autocorrelation with our errors, it means there's some autocorrelation our model is failing to capture.","6a0b8319":"Another way to visualise seasonality is to use a heatmap - we can base it on a week to see which days have higher electricity usage.\n\nNote we will drop off 2018 because it's not a full year and will skewer the heatmap.\n\nFirst let's construct the dataframe table.","1ff9852b":"PJM is one of the world's largest wholesale electricity markets, encompassing over 1,000 companies,  65 million customers and delivered 807 terawatt-hours of electricity in 2018 - [see their annual report](http:\/\/https:\/\/www.pjm.com\/-\/media\/about-pjm\/newsroom\/annual-reports\/2018-annual-report.ashx?la=en).\n\nWe'll use the PJM East Region dataset - which covers the eastern states of the PJM area. There isn't readily available data that identifies which regions are covered, so I'm going to assume (for this analysis) the following States (the coastal ones):\n\n* Delaware\n* Maryland\n* New Jersey \n* North Carolina\n* Pennsylvania\n* Virginia\n* District of Columbia","41f84fcb":"Let's get to know the data a bit more - a few high level EDA stuff:","b94a5be8":"One thing that jumps out right now is very difficult to see what's going on, as the graph is very 'packed together'.\n\nMore 'traditional' econometric\/statistical models, such as Holtwinters and SARIMA, require 3 characteristics for them to work properly, namely:\n\n* Seasonality: the dataset is cyclical in nature\n* Stationarity: the properties of the dataset doesn't change over time\n* Autocorrelation: there is similiar between current and past (ie 'lagged') data points\n\nHow about we aggregate up to weekly level to reduce the noise","874bbd29":"So using this you can see (as expected) the weekend has lower electricity use. Many businesses are closed during weekends and therefore this makes sense.","35671603":"We need to clean and aggregate the weather stations into one time series (including removing weather stations with missing data) - so let's do that:","91a099da":"# Exploratory Data Analysis\n\nWe need to first check a few things:\n1. Duplicate and missing data, as well as spread\n2. How autocorrelated is this data and its seasonal decomposition","743742e3":"# Baseline Model - Naive Forecasting\n\nBefore we go knee-deep into machine learning, it is good to use naive forecasting techniques to determine a 'baseline'. That is, if the ML models cannot beat these baseline forecasts, then we would be better off just using naive forecast instead.\n\nThey are 'naive' in the sense they are simple to apply, but in reality they are pretty powerful and effective forecasts. Not too many ML models actually can consistently beat a naive forecast!\n\nA common naive forecast for predicting a multi-step forecast (i.e. for us, it would be the next 365 days), is to use a 'One-Year-Ago Persistent Forecast'. This basically means the value for say 31 August 2019 is predicted using the value for 31 August 2018.","b195ca93":"# Bringing it all together - the final results\n\nFinally, Let's bring the models together and see which one did the best:","7421ace3":"The most incorrect days were July and January, the peak of summer and winter respectively. Makes sense!\n\nYou get the biggest peaks in these periods due to weather.","27f0c3c5":"Immediately you can see that it is pretty half-decent forecast!\n\nBut of course, we need to see the errors\/residuals to figure out whether:\n1. It is normally distributed (i.e. it has no bias to under or over forecasting)\n2. Whether the errors are autocorrelated (that is, whether the model failed to pick up on any autocorrelation patterns)","cfff9835":"**Seasonal Decomposition**\n\nAt a high-level, time series data can be thought of as components put together. That is:\n\n**Data = Level + Trend + Seasonality + Noise**\n\n* **Level**: the average value in the series.\n* **Trend**: the increasing or decreasing value in the series.\n* **Seasonality**: the repeating short-term cycle in the series.\n* **Noise\/Residual**: the random variation in the series.\n\nUsing the Python statsmodel library, the above components can be 'decomposed' (ie seasonal decomposition):","0483a745":"So you can see that despite some tuning, the results are not particularly good. The confidence interval is very big, indicating the model has is not 'confident' in the prediction either.","0cc307dd":"Now let's see what the algorithm considered most important - we'll grab the **Top 10 features by weight**.\n\nThe weight is the percentage representing the relative number of times a particular feature occurs in the trees of the model. It's a rough way of saying the more times you reference a particular feature, the more likely it is important.","0eb6bb2f":"You can see start to see a pattern - electricity usage peak and troughs seem to be very seasonal and repetitive. This makes sense, considering office hours, weather patterns, shopping holidays etc.\n\nFurthermore, you can see the trend of the data seems to be trailing downwards in the last few years.","aa3e2572":"We can also do the same with a 'season plot' - that is, compare each year over 12 months:","d75b3d31":"Next let's see feature importance by way of coefficients - we'll only get Top 10.\n\nRemember, LASSO will 'zero-out' irrelevant features, so in this case, these are the Top 10 features that LASSO considers are most important.","bf2710e7":"Now let's evaluate the residuals and see whether the model is biased in any way. First we'll look at the distribution of the errors:","db54ee29":"Good, so there's no missing or duplicate data and you can see the data is from 1 Jan 2002 to 2 August 2018.","9f9da9fe":"Interestingly, it means 11am to 9pm is the busiest peak time of the grid.","e891e6d1":"The results of the model are fairly accurate!\n\nHowever, caveat is that because the model knows about yesterday's value. Therefore, the 'forecast horizon' (ie the maximum length of time it can predict into the future) is only 1 day. This is also known as a 'One-Step Ahead Forecast'.\n\nIf you only have yesterday's value, you can only predict today's value. If you only have today's value, you can only predict tomorrow's value.","89fc1d3e":"Not bad - the naive forecast actually is quite normally distributed, in terms of its errors.","15ff34d8":"You can see the 80% confidence interval (in light blue), indicating the model is confident that 80% of the actual data will land in that predicted range.","df587113":"Like Holtwinters, let's see the components of the model:","1223894a":"What the above graph shows is how correlated a prior point is to the current point. The further the number is away from 0, the more correlation there is.\n\nGenerally, we would only consider any points above (for positive numbers) and below (for negative numbers) the blue shaded area (the confidence interval) as statistically significant and worth noting.\n\nThis shows that yesterday's value has a very high correlation with today's value and there is seasonality - every 6 months it seems to repeat itself.\n\nAs eluded earlier, this makes sense if you factor in weather patterns - winter and summer have higher electricity usage due to more heat\/cooling needed.\n\nMy personal guess is because of weather - winter and summer have higher electricity usage.","f342fed2":"The lower the error the better\/more accurate the model is, so therefore in this case, the winner is LASSO! However, Prophet and XGBoost still comes a pretty close second!\n\nYou can see that there was a lot of noise in this dataset and while there's high seasonality, more conventional statistical approaches, such as Holtwinters and SARIMA weren't able to get through all that noise.\n\nInterestingly, all the forecasting models beat the baseline, except Holtwinters and SARIMA. This does at least demonstrate that we are still better off using these models than just doing naive forecasting.\n\nHopefully that gives you a bit of a flavour to time series forecasting and some of the unique aspects of it!","41888a73":"Let's evaluate our model using Mean Absolute Error (MAE) and visualise the results.\n\nMean Absolute Error (MAE) is an evaluation metric that measures the average magnitude of the errors in a set of predictions. In other words how 'wrong' the model is. Unlike other metrics, such as Root Mean Squared Error, it does not have any particular weighting.\n\nTraining data is in blue, while test\/evaluation data is in red.","05c062ea":"You can see that at the individual, each prediction has a different main factor (compared to the more general model).\n\nOverall, quite good results! Now let's have a look at the residuals\/errors.\n\nFirst let's look at the distribution of the errors - remember, the ideal state is the errors are centred around zero (meaning the model does n't particularly over or under forecast in a biased way):","7c50df9c":"# Statistical Test 'Smoke Alarms'\n\nStatistical Tests are a good way to test whether the data is conductive to conventional statistical methods.\n\nThere are certain good statistical tests you can apply to a dataset as a 'smoke alarm' test. They are a good indication whether the data is conducive to accurate forecasting.\n\nBoth the below statistical tests use Hypothesis Testing and P-Values, which require a cutoff to be picked in advance. The general rule of thumb is 5% - which means there's a only a 5% chance of the statistical tests to be incorrect.\n\n![image.png](attachment:image.png)\n\n\n'**Stationary**' means the properties of the dataset don't change over time. Non-stationary means the trends, seasonality changes over time and the data is affected by factors other than the passsage of time. A Non-Stationary is sometimes known as a 'Random Walk' - which are notoriously difficult to forecast, because the underlying properties keep changing (e.g. like trying to hit a moving target).\n\nNote that Random Walk is different to a set of 'random numbers' - it's random because the next point is based on a 'random' modification on the first point (e.g. add, minus, multiply). Whereas in a 'random numbers' set, ther would be little relationship between each data point.","955b2714":"Then we visualise it:","17c56a21":"Let's see what this looks like:","c22e63d0":"Now let's plot the weather data and have a look","d31d4d4e":"We are using Box-Cox as heteroskedatic test before showed data requires dampening to reduce extremes.","bdeb0ccf":"Again let's see feature importance - this time by gain, not weight (as model trained on gain)","e5d455ea":"Using Holtwinters, data from 2002 to 2017 was used to train the model, while the remaining 2017 to 2018 data was used to test\/evaluate the model's accuracy.\n\nTraining data is in blue, while test\/evaluation data is in red.\n\nLet's evaluate our model using Mean Absolute Error (MAE) and visualise the results","7dfdf994":"# XGBoost - Ensemble Learning\n\nXGBoost has gained in popularity recently by being quite good at predicting many different types of problems.\n\nNormally with decision-tree models, you would get the data and create one tree for it. This of course means it is very prone to overfitting and being confused by the unique tendencies of the past data.\n\nTo overcome this, you do 'gradient boosting'. At a very high-level, it is analogous to the algorithm creating a decision tree to try to predict the result, figure out how wrong it was, and then create another tree that learns from the first one's 'mistakes'.\n\nThis process is then repeated a few hundreds or even a few thousand times, with each tree being 'boosted' by the prior one's mistakes. The algorithm keeps going until it stops improving itself.\n\nThe technical aspects of the mathematics are much more complex (and a bit beyond my knowledge to bef honest). If you want more details, the documentation is here.","d281adc3":"After preprocessing and cleaning up the data, as well as making sure the frequency is at the hourly level, let's plot the data.","2d688660":"You can easily see seasonality and trends - there's a clear downward trend and seasonality every year (more electricity is used in winter)","d0f018b1":"First let's have a look at the dataset - first few lines:","60a50ec8":"# Prophet\n\nLastly, we will use Facebook Prophet - an open-source library that is also a generalised additive model (ie final result is made up of multiple components added together).\n\n## It's all about probabilities!\n\nUnlike regular Generalised Linear Models, Facebook Prophet's uses a Bayesian curve fitting approach. The concept of Bayesian theorem is, at high level, trying to determine the probability of related events given knowledge\/assumptions you already know (ie 'priors').\n\nThis is basically fancy talk for saying it focuses on finding a bunch of possible parameters and the probability of each one rather than finding fixed optimal values for the model. How certain (or uncertain) the model is about each possible parameter is known as the 'uncertainty interval' - the less data the model sees, the bigger the interval is.\n\nThe sources of uncertainty that Prophet's Bayesian approach aims to address are:\n\n* Uncertainty of the predicted value\n* Uncertainty of the trend and trend changes\n* Uncertainty of additional noise\n\n## Trends, 'Changepoints' and Seasonality\n\nProphet is different to SARIMA and HoltWinters, as it essentially decomposes time series differently by:\n\n**Data = Trend +\/x Seasonality +\/x Holidays +\/x Noise**\n\nIn Prophet, trend represents non-periodic changes while seasonality represents periodic changes. Where it differs from other statistic models like SARIMA and Holtwinters is Prophet factors in the uncertainty of trends changing.\n\nInterestingly, Prophet fits a curve to each component independently (ie fits a regression for each component with time as independent variable). That is:\n\nTrend - fits piece-wise linear\/log curve\nSeasonality - uses fourier series\nHolidays - uses constant\/fixed values\nProphet reacts to changes in trends by 'changepoints' - that is sudden and abrupt changes in the trend. An example, is the release of a new electric car that will impact sale of petrol cars.\n\nHow 'reactive'\/flexible Prophet is to changepoints will impact how much fluctation the model will do. For example, when the changepoint scale is to very high, it becomes very sensitive and any small changes to the trend will be picked up. Consequently, the sensitive model may detect spikes that won't amount to anything, while an insensitive model may miss spikes altogether.\n\nThe uncertainty of the predictions is determined by the number of changepoints and the flexibility of the changepoint allowed. In other words, if there were many changes in the past, there's likely going to be many changes in the future. Prophet gets the uncertainty by randomly sampling changepoints and seeing what happened before and after.\n\nFurthermore, Prophet factors in seasonality by considering both its length (ie seasonal period) and its frequency (ie its 'fourier order').For example, if something keeps happening every week on Monday, the period is 7 days and its frequency is 52 times a year.\n\nThere's also a function for Prophet to consider additional factors (e.g. holidays), but for the purposes of this forecast we won't use it.\n","91be66dd":"# 1. Holtwinters Triple Exponential Smoothing\nNext we will use a time series forecasting model that takes advantage of the above identified components.\n\nThis is known as a 'generative additive model', as the final forecast value is 'adding' together multiple components.\n\nThe 'Triple' refers to the three components:\n1. **Level**\n2. **Trend**\n3. **Seasonality**\n\nHoltwinters works really well when the data is seasonal and has trends.\n\n'Smoothing' basically means more weight is put on more recent data compared to the past.\n\nNote the main hyperparameters for the model are:\n* **Additive vs Multiplicative **(ie 'add' or 'mul') \n* **Box Cox** - to use box cox log transformation to reduce the 'noise' of the data\n* **Alpha** - smoothing factor between 0 and 1. 1 means will always take yesterday's value (naive forecasting). 0 means take simple average of past.\n\nAdditive means the formula looks more like this: Data = Level + Trend + Seasonality\n\nMultiplicative means the formula looks more like this: Data = Level x Trend x Seasonality","e02de729":"Finally we train the model and see how well it performed (using MAE) and visualise the results.\n\nTo train the model, again I split the data into two parts: Training from 2002 to 2017 and Evaluation\/Testing from 2017 to 2018.\n\nIn this case, I used 1000 runs and a maximum depth of each tree to be 6. That is, it does 1000 runs (or less if it stops improving), with each tree having a max of 6 levels.\n\nAs a tree-based algorithm, generally XGBoost doesn't handle trends in data well compared to linear models. However, given as shown above in the ADF test, the data is stationary, trend is not really an issue and we can proceed. Otherwise we would need to de-trend the data first as part of preprocessing.","e0bb4c07":"Again, you can see that the errors seem to be distributed around zero and there's no statistically significant autocorrelation going on. This indicates that the model isn't biased and not leaning torwards under or over forecasting.\n\nHowever, you can see that the model does seem to miss under forecast a bit more, particularly for spikes.\n\nLet's have a closer look at the biggest under forecasts:","8afe2559":"# Using Machine Learning to Forecast Electricity Usage in the future\n\n## The past dictates the future - or does it?\nForecasting time series data is different to other forms of machine learning problems due a one main reason - time series data often is correlated with the past. That is, today's value is influenced by, for example, yesterday's value, last week's value etc. This is known as 'autocorrelation' (ie correlating with 'self').\n\nBecause time series has autocorrelation, many traditional machine learning algorithm that consider every data point 'independent' don't work well. That is, every prediction is solely based on a particular day's data, and not what happened yesterday, day before etc.\n\nTo put it bluntly, traditional machine learning algorithms have **no concept of the passage of time - every single data point is just a 'row in the table'.** It doesn't factor into trends, which as humans, we would intuitively see (e.g. sales went up by 20% this week).\n\nGenerally, to make a machine learning model factor data from a particular point's past, you use 'shifting' feature engineering techniques. That is, for example, expressly adding in yesterday's value show as a feature\/column for the model to consider.\n\nThis illustration will attempt to forecast energy consumption to give a bit of flavour of how time series forecasting works, comparing:\n\n1. Holtwinters Triple Exponential Smoothing\n2. Seasonal Autoregression Integrated Moving Average (SARIMA)\n3. XGBoost Regressor\n4. LASSO Regularised Regression\n5. Facebook\u2019s Prophet\n\n## A showcase of time series forecasting - electricity usage!\nIn the electricity industry, being able to forecast the electricity usage in the future is essential and a core part of any electricity retailer's business. The electricity usage of an electricity retailer's customers is generally known as the retailer's 'load' and the usage curve is known as the 'load curve'.\n\nBeing able to accurately forecast load is important for a number of reasons:\n\n**Predict the future base load** - electricity retailers need to be able to estimate how much electricity they need to buy off the grid in advance.\n**Smooth out pricing** - if the load is known is advance, electricity retailers can hedge against the price to ensure they aren't caught out when the price skyrockets 3 months into the future.\n\nIn a nutshell, the business problem is:\n\n**Given the historical power consumption, what is the expected power consumption for the next 365 days?**\n\nBecause we want to predict the daily load for the next 365 days, there are multiple 'time steps' and this form of forecasting is known as 'multi-step forecasting'. In contrast, if you only wanted to predict the load on the 365th day or the load for tomorrow only, this would be known as a 'one-step forecast'.\n\nThis dataset is from https:\/\/www.kaggle.com\/robikscube\/hourly-energy-consumption:\n\n> PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n> \n> The hourly power consumption data comes from PJM's website and are in megawatts (MW).\n> \n> The regions have changed over the years so data may only appear for certain dates per region.","7929d726":"# SARIMA\n\nThe conventional ARIMA model assumes that the historical data are useful to predict the value at the next time step. In this case, this is somewhat true, as the ACF plot before showed past value is somewhat correlated with today's value.\n\nARIMA basically integrates two naive forecasting techniques together:\n\n1. **Autoregression** - Uses one or more past values to forecast the future. The number of values used is known as the 'order' (e.g. order 2 means yesterday and day before's value is used)\n\n2. **Integrating** - the part that reduces seasonality. How many degrees of differencing is done to reduce seasonality is the 'order'.\n\n3. **Moving Average** - Uses the Moving Average of the historical data to adjust the forecasted values. This has a 'smoothing' effect on the past data, as it uses the moving average rather than the actual values of the past. The number of days in the moving average window is the 'order'.\n\nSARIMA then adds a '**seasonality**' flavour to the ARIMA model - it factors in trends and seasonality, as explained above.\n\nThe main hyperparameters are SARIMAX(p,d,q)(P,D,Q,m):\n1. autoregression order (p)\n2. Integrating order (d)\n3. moving average window (q)\n4. Seasonal autoregressive order (P)\n5. Seasonal difference order (D)\n6. Seasonal moving average order (Q)\n7. number of time steps for a single seasonal period (m)\n\nThe X is the exogenous (external) variables to the model - they are optional and for this model we won't use them.","b5fccf8d":"So looking quite good - you can see some forecasts were pretty off (particularly for spikes), but overall, it seems the model treats overs and under equal.","92976fe7":"Let's see the correlation between the two:","c71548d1":"So LASSO is producing half decent results! Now let's have a look at the residuals\/errors.\n\nFirst let's look at the distribution of the errors - remember, the ideal state is the errors are centred around zero (meaning the model doesn't particularly over or under forecast in a biased way)","d88d51d7":"# Housekeeping and Preprocessing","33ff3e75":"## An alternative feature engineering approach - let's include lag without data leakage\nLet's try to make the XGBoost model more than One-Step Ahead - we'll only feature engineer:\n\n- Day of the Week\n- Day of the Month\n- Day of the Year\n- Week of the Year\n- Month\n- Year\n\nHopefully this is enough for the model to artificially pick up 'seasonality' factors - e.g., if same day of week, might be correlated.\n\nHowever, as forecast horizon is 365 days, we can still include lag-365 to lag-720 days data (i.e. year-before last year's data).","bb612e0e":"So you can see the largest contributing factor is the actual date of prediction.\n\nDay of week also weighs heavily, while the actual day of the year has a negative contributing factor. Like correlation, the further away the value is from 0 means the more it influences the prediction.\n\nNow let's have a look at specific prediction e.g. the 2nd one and 871st one.","b0156601":"You can see because the frequency is at the hourly level, this is will make forecasting difficult (and its also difficult to visualise too!)\n\nSo what we will do is resample it and aggregate it to the daily level","ea4a4a0c":"In our case, we got the above results which means:\n* The data is **Stationary** (as the P-Value was below 0.05)\n* The data is **heteroskedactic** (as the P-Value was not below 0.05)\n","97bbc02b":"Let's evaluate the results and visualise it.","7a109547":"So it seems Exponential Smoothing is a no-go - let's see how the others fare.","77c0fb61":"So you can see that July has the heaviest load on the grid - which makes sense as it is the height of summer in the region.\n\nAir conditioners are expensive after all!\n\nTo further do some analysis, let's add in weather data. As we are focusing on the PJM East Region, for simplicity, I'll use Washington D.C.'s weather data as a reference for the entire region.\n\nWeather data was obtained from the [US NOAA's National Centers for Environmental Information (NCEI)](https:\/\/www.ncdc.noaa.gov\/cdo-web\/).\n\nData is in Celsius degrees is only from 2016 to 2018. There's 166 weather stations in the dataset that cover the Washington D.C. region.","6cd56641":"Next let's have a look at some of the summary EDA data, missing values and duplicates:","c4009987":"So let's run the test for Heteroskedacity and see the results: ","9ff0773e":"This graph shows that the last 90 days have a stronger correlation, but the effect becomes much less obvious the further back you go.","5142033c":"# LASSO (L1) Regression\n\nLASSO regression incorporate regularisation and feature selection into its algorithm. Regularisation is a technique used in regression algorithms to avoid overfitting. For LASSO, this means it will penalise the 'irrelevant' features by effectively by 'zeroing' out those features (by multiplying it with a 0 coefficient). \n\nThe main hyperparameter to tune is the penalty factor (i.e. lambda or alpha). A factor of 0 means no penalisation occurs, and it effectively just does an Ordinary-Least-Squares (OLS) regression.\n\nSince we've already set up all the train-test split (as well as feature engineering) in the prior XGBoost model, we can just re-use it.\n\nHowever, like the decision tree-based XGBoost, linear regression is sensitive to scale. Therefore, we also need to scale the data.","87094440":"This explains why the seasonal decomposition and ADF show this data has a lot of noise and crazy swings!!\n\nThe consequences of these statistical tests means that the traditional assumptions of linear regression have been violated. That is, more conventional methods of linear regression, statistical F-Tests and T-Tests become ineffective.\n\nTherefore, this forecasting model won't factor in these tests.","018b1f9e":"## Feature Engineering\n\nAs eluded earlier, most machine learning models don't 'look back' to prior values. Essentially if you have a table, each 'row' is an independent data point and the ML model doesn't consider the prior row's data.\n\nThis is problematic for time series data, as shown above, autocorrelation happens.\n\nTo address this issue, we use feature engineering to create additional features - in this case, I created 365 extra columns each prior day. Today minus 1 day, Today minus 2 days ... until Today minus 365 days.","a929ae00":"# Train Test Split \nBefore we continue, let us split the data up between train and split with a specified cutoff date.\n\nLet's pick 3 August 2017 (12 months prior)","2aa735af":"'**Heteroskedacity**' refers to instances where the data is evenly distributed along a regression line. Basically it means the data is more closely grouped together and therefore is less 'spiky' (ie has more peaks\/troughs).\n\nHeteroskedactic data means the peaks and troughs (ie outliers) are being observed way more often than a 'normally distributed' dataset. This means that a model will have a hard time predicting these spikes.\n\nTo alleviate this, heteroskedactic data generally needs to be Box-Cox\/log transformed to dampen the extreme peaks\/troughs. That is, bringing the data closer together so a model can better fit the whole data and hit the peaks\/troughs.","45d7f6b7":"Importantly, the model should have the residuals uncorrelated and normally distributed (ie the mean should be zero). That is the, the centre point of the residuals should be zero and the distribution plot (KDE) should also be centred on 0.","8dbb81b7":"Next let's look at the autocorrelation of the errors","a429e77f":"Now let's plot the results","9765478e":"# Autocorrelation\nAutocorrelation is basically how much correlation there is between a particular time point and a prior one - e.g. today's value is highly correleated with last week's value.\n\nAgain, Python Statsmodel has a great Autocorrelation Function (ACF) that easily produces this:","f6eed0d4":"Another modification of this autocorrelation analysis is the Partial Autocorrelation Function (PACF). This function is a variant of ACF, as it finds correlation of the residuals, after removing the effects which are already explained in earlier lags. That way, you don't get a 'compounding' correlation effect."}}