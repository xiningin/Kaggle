{"cell_type":{"3255fff2":"code","d6c42868":"code","dd65465b":"code","07c5250a":"code","31aafb08":"code","82bfd4d2":"code","763c6ec3":"code","11ee491c":"code","0a148514":"code","417a3d15":"code","ec509f3c":"code","ba95a1da":"code","58052fe0":"code","c318c15e":"code","b986e993":"code","484f545c":"code","df6c360d":"code","e798ce33":"code","3cb1ce25":"code","ccb33ced":"code","5b50afbe":"code","c307c967":"code","bbbf7553":"code","a2473d70":"code","9d7a122d":"code","e00daaae":"code","64859a58":"code","e84c6098":"code","09185ff0":"code","b14d592a":"code","f8b0aba8":"code","fdb67c74":"code","e2d14d2d":"code","e7513bb8":"code","28c8ae3c":"code","77e86b84":"code","2672b144":"code","d0e0a582":"code","22155376":"code","760c3153":"code","a5b2432a":"code","48cd2325":"code","656b0496":"code","18e4dbe1":"code","c0effaeb":"code","571513f9":"code","0545edbb":"markdown","7844bd4f":"markdown","490a04d5":"markdown","859a2079":"markdown","0214ecb9":"markdown","00f5f344":"markdown","f17a1150":"markdown","e5b1a66f":"markdown","73134d5a":"markdown","bec9348f":"markdown","22cde559":"markdown","9ac33c5d":"markdown","dfcf3a0b":"markdown","4486bb9b":"markdown","60d5236a":"markdown","5c66e10a":"markdown","847611b5":"markdown","f84777b7":"markdown","27bcd5eb":"markdown","b0b0d7b6":"markdown","69d01908":"markdown","022fbe03":"markdown","3b2b4ac2":"markdown","50b64f00":"markdown","bf163ad6":"markdown","8d62acff":"markdown","b61799b3":"markdown","64aad9cf":"markdown","8b37343f":"markdown","6d89b069":"markdown","3dcbc279":"markdown","dce55b76":"markdown","fc828949":"markdown","25cae9f7":"markdown","d5e0c41a":"markdown","8dd379d8":"markdown","0c69eec3":"markdown","09fb44d4":"markdown","bc894e51":"markdown","4c727daa":"markdown","92efe3a7":"markdown","d5ed5386":"markdown","ab07f73c":"markdown","9f296026":"markdown","75418c36":"markdown","d12ad64b":"markdown","29f54a87":"markdown","c53517c9":"markdown","338ce997":"markdown","759b577d":"markdown"},"source":{"3255fff2":"import collections\nfrom datetime import datetime as dt\nimport gc\nimport glob \nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cv2\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom IPython.display import YouTubeVideo\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nimport plotly.express as px\nimport seaborn as sns\n\n%matplotlib inline\n\nSEED = 2021","d6c42868":"YouTubeVideo('qKgRbkCkRFY')","dd65465b":"TRAIN_DATA_PATH = '..\/input\/iwildcam2021-fgvc8\/train\/'\nTEST_DATA_PATH = '..\/input\/iwildcam2021-fgvc8\/test\/'\n\ntrain_jpeg = glob.glob(TRAIN_DATA_PATH + '*')\ntest_jpeg = glob.glob(TEST_DATA_PATH + '*')\n\nprint(\"number of train jpeg data:\", len(train_jpeg))\nprint(\"number of test jpeg data:\", len(test_jpeg))","07c5250a":"fig = plt.figure(figsize=(25, 16))\nfor i,im_path in enumerate(train_jpeg[:16]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    im = Image.open(im_path)\n    im = im.resize((480,270))\n    plt.imshow(im)","31aafb08":"fig = plt.figure(figsize=(25, 16))\nfor i,im_path in enumerate(train_jpeg[16:32]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    im = Image.open(im_path)\n    im = im.resize((480,270))\n    plt.imshow(im)","82bfd4d2":"fig = plt.figure(figsize=(25, 16))\nfor i,im_path in enumerate(test_jpeg[:16]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    im = Image.open(im_path)\n    im = im.resize((480,270))\n    plt.imshow(im)","763c6ec3":"sub = pd.read_csv(\"..\/input\/iwildcam2021-fgvc8\/sample_submission.csv\")\nsub.head()","11ee491c":"sub.shape","0a148514":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n    train_annotations =json.load(json_file)\n    \ntrain_annotations.keys()","417a3d15":"train_annotations_seq = train_annotations[\"images\"][94:104]\ntrain_annotations_seq","ec509f3c":"train_images_seq = [(TRAIN_DATA_PATH+item[\"id\"]+'.jpg') for item in train_annotations_seq]\nimg_array = []\nsize = (480,270)\n\nfig = plt.figure(figsize=(25, 16))\nfor i,im_path in enumerate(train_images_seq):\n    ax = fig.add_subplot(4, 3, i+1, xticks=[], yticks=[])\n    im = Image.open(im_path)\n    im = im.resize(size)\n    plt.imshow(im)\n    \n    img_array.append(im)","ba95a1da":"df_categories = pd.DataFrame.from_records(train_annotations[\"categories\"])\ndf_categories","58052fe0":"train_annotations[\"annotations\"][:10]","c318c15e":"train_annotated_category = set([ annotation[\"category_id\"] for annotation in train_annotations[\"annotations\"]])\nlen(train_annotated_category)","b986e993":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_test_information.json', encoding='utf-8') as json_file:\n    test_information =json.load(json_file)\n    \ntest_information.keys()","484f545c":"test_information['images'][:5]","df6c360d":"with open('..\/input\/iwildcam2021-fgvc8\/metadata\/iwildcam2021_megadetector_results.json', encoding='utf-8') as json_file:\n    megadetector_results =json.load(json_file)\n    \nmegadetector_results.keys()","e798ce33":"megadetector_results_df = pd.DataFrame(megadetector_results[\"images\"])\nmegadetector_results_df.head()","3cb1ce25":"print(f\"There are {len(megadetector_results_df)} detection data.\")","ccb33ced":"megadetector_results_df.iloc[100][\"detections\"]","5b50afbe":"#Refered: https:\/\/www.kaggle.com\/qinhui1999\/how-to-use-bbox-for-iwildcam-2020 \n\ndef draw_bboxs(detections_list, im):\n    \"\"\"\n    detections_list: list of set includes bbox.\n    im: image read by Pillow.\n    \"\"\"\n    \n    for detection in detections_list:\n        x1, y1,w_box, h_box = detection[\"bbox\"]\n        ymin,xmin,ymax, xmax=y1, x1, y1 + h_box, x1 + w_box\n        draw = ImageDraw.Draw(im)\n        \n        imageWidth=im.size[0]\n        imageHeight= im.size[1]\n        (left, right, top, bottom) = (xmin * imageWidth, xmax * imageWidth,\n                                      ymin * imageHeight, ymax * imageHeight)\n        \n        draw.line([(left, top), (left, bottom), (right, bottom),\n               (right, top), (left, top)], width=4, fill='Red')","c307c967":"# Let's see 100th data of train dataset.\ndata_index = 100\n\n# Load 100th image data. \nim = Image.open(\"..\/input\/iwildcam2021-fgvc8\/train\/\" + megadetector_results_df.loc[data_index]['id'] + \".jpg\")\nim = im.resize((480,270))\n\n# Overwrite bbox\ndraw_bboxs(megadetector_results_df.loc[data_index]['detections'], im)\n\n# Show\nplt.imshow(im)\nplt.title(f\"image {data_index} with bbox\")","bbbf7553":"megadetector_results_df.loc[data_index]['detections'][0][\"bbox\"]","a2473d70":"def get_crop_area(bbox, image_size):\n    x1, y1,w_box, h_box = bbox\n    ymin,xmin,ymax, xmax = y1, x1, y1 + h_box, x1 + w_box\n    area = (xmin * image_size[0], ymin * image_size[1], \n            xmax * image_size[0], ymax * image_size[1])\n    return area\n\ncrop_area = get_crop_area(megadetector_results_df.loc[data_index]['detections'][0][\"bbox\"], im.size)\nim_croped = im.crop(crop_area)\nplt.imshow(im_croped)","9d7a122d":"megadetector_results[\"info\"]","e00daaae":"megadetector_results[\"detection_categories\"]","64859a58":"# Preperation for isualization\ndf_categories = pd.DataFrame(train_annotations[\"categories\"])\nlabels_id = [item[\"id\"] for item in train_annotations[\"categories\"]]\ncnt = collections.Counter([item[\"category_id\"] for item in train_annotations[\"annotations\"]])\ndf_categories_count = pd.DataFrame.from_dict(cnt, orient='index').reset_index()\ndf_categories_count = df_categories_count.rename(columns={'index':'id', 0:'count'})\n\ndf_categories_count = df_categories_count.merge(df_categories, on='id').sort_values(by=['count'], ascending=False)","e84c6098":"fig = plt.figure(figsize=(30, 4))\nax = sns.barplot(x=\"id\", y=\"count\",data=df_categories_count, order=labels_id)\nax.set(ylabel='count')\nax.set(ylim=(0,80000))\nplt.title('distribution of count per id in train')","09185ff0":"fig = px.bar(df_categories_count, x=\"id\", y=\"count\", \n             title='distribution of count per id in train',\n             width=800, height=400, color='id')\nfig.show()","b14d592a":"df_categories_count.iloc[:10]","f8b0aba8":"df_categories_count.iloc[-10:]","fdb67c74":"# Refered https:\/\/www.kaggle.com\/kushal1506\/deciding-n-components-in-pca\n\nfig, ax = plt.subplots(figsize=(30, 10))\nxi = np.arange(1, len(df_categories_count)+1, step=1)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, df_categories_count[\"count\"].cumsum()\/sum(df_categories_count[\"count\"]), marker='o', linestyle='--', color='b')\n\n\nplt.xlabel('Number of category', fontsize=30)\nplt.xticks(np.arange(0, len(df_categories_count), step=10)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accumulation Ratio (%)', fontsize=30)\nplt.title('Relationships when cumulative sums are taken in order of increasing categories.', fontsize=30)\n\nplt.axhline(y=0.99, color='g', linestyle='-')\nplt.text(0.5, 1.00, '99%', color = 'green', fontsize=30)\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.92, '95%', color = 'red', fontsize=30)\n\nplt.axvline(x=40, color='g', linestyle='-')\nplt.text(40, 0.5, '40th', color = 'green', fontsize=30)\n         \nplt.axvline(x=90, color='g', linestyle='-')\nplt.text(90, 0.5, '90th', color = 'green', fontsize=30)\n\nax.grid(axis='x')\nplt.show()","e2d14d2d":"# Convert annotation data to pandas DataFrame\ndf_train_annotations = pd.DataFrame(train_annotations[\"annotations\"])\n\n# Under sampling\nrus = RandomUnderSampler(random_state=SEED, replacement=True)\ndf_train_annotations_resampled, _ = rus.fit_resample(df_train_annotations, df_train_annotations[\"category_id\"])","e7513bb8":"df_train_annotations_resampled.reset_index(drop=True)","28c8ae3c":"df_images = pd.DataFrame(train_annotations[\"images\"])\ndf_images_test = pd.DataFrame(test_information[\"images\"])","77e86b84":"month_year = df_images['datetime'].map(lambda str: str[2:7])\nlabels_month_year = sorted(list(set(month_year)))\n\nmonth_year_test = df_images_test['datetime'].map(lambda str: str[2:7])","2672b144":"fig, ax = plt.subplots(1,2, figsize=(30,7))\nax = plt.subplot(1,2,1)\nax = plt.title('Count of train data per month & year')\nax = sns.countplot(month_year, order=labels_month_year)\nax.set(xlabel='YY-mm', ylabel='count')\nax.set(ylim=(0,50000))\n\nax = plt.subplot(1,2,2)\nax = plt.title('Count of test data per month & year')\n\nax = sns.countplot(month_year_test, order=labels_month_year)\nax.set(xlabel='YY-mm', ylabel='count')\nax.set(ylim=(0,50000))","d0e0a582":"labels_month = sorted(list(set(df_images['datetime'].map(lambda str: str[5:7]))))\n\nfig, ax = plt.subplots(1,2, figsize=(20,7))\nax = plt.subplot(1,2,1)\nplt.title('Count of train data per month')\nax = sns.countplot(df_images['datetime'].map(lambda str: str[5:7] ), order=labels_month)\nax.set(xlabel='mm', ylabel='count')\nax.set(ylim=(0,55000))\n\nax = plt.subplot(1,2,2)\nplt.title('Count of test data per month')\nax = sns.countplot(df_images_test['datetime'].map(lambda str: str[5:7] ), order=labels_month)\nax.set(xlabel='mm', ylabel='count')\nax.set(ylim=(0,55000))","22155376":"train_taken_hour = df_images['datetime'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S.%f').hour)\ntest_taken_hour = df_images_test['datetime'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S.%f').hour)\n\nfig, ax = plt.subplots(1,2, figsize=(20,7))\nax = plt.subplot(1,2,1)\nplt.title('Count of train data per hour')\nax = sns.countplot(train_taken_hour)\nax.set(xlabel='hour', ylabel='count')\nax.set(ylim=(0,20000))\n\nax = plt.subplot(1,2,2)\nplt.title('Count of test data per hour')\nax = sns.countplot(test_taken_hour)\nax.set(xlabel='hour', ylabel='count')\nax.set(ylim=(0,20000))","760c3153":"train_taken_phase = train_taken_hour.map(lambda x: \"daytime\" if x >= 6 and x < 18 else \"night\")\ntest_taken_phase = test_taken_hour.map(lambda x: \"daytime\" if x >= 6 and x < 18 else \"night\")","a5b2432a":"fig, ax = plt.subplots(1,2, figsize=(20,7))\nax = plt.subplot(1,2,1)\nplt.title('Count of train data per phase')\nax = sns.countplot(train_taken_phase, order=[\"daytime\", \"night\"])\nax.set(xlabel='phase', ylabel='count')\nax.set(ylim=(0,200000))\n\nax = plt.subplot(1,2,2)\nplt.title('Count of test data per phase')\nax = sns.countplot(test_taken_phase, order=[\"daytime\", \"night\"])\nax.set(xlabel='phase', ylabel='count')\nax.set(ylim=(0,200000))","48cd2325":"labels_location_train = sorted(list(set(df_images['location'])))\nlabels_location_test = sorted(list(set(df_images_test['location'])))\nlabels_location = labels_location_train + labels_location_test\n\nfig = plt.figure(figsize=(30, 4))\nax = sns.countplot(df_images['location'], order=labels_location)\nax.set(xlabel='location', ylabel='count')\nplt.title('Count of train data per location')","656b0496":"fig = plt.figure(figsize=(30, 4))\nax = sns.countplot(df_images_test['location'], order=labels_location)\nax.set(xlabel='location', ylabel='count')\nplt.title('Count of test data per location')","18e4dbe1":"def is_in_test(x):\n    if os.path.exists(TEST_DATA_PATH + x + \".jpg\"):\n        return True\n    else:\n        return False\n    \nare_images_in_test = [ is_in_test(x) for x in megadetector_results_df[\"id\"]]\nare_images_in_train = [not is_in_test for is_in_test in are_images_in_test]\n\ntrain_megadetector_results_df =  megadetector_results_df[are_images_in_train]\ntest_megadetector_results_df =  megadetector_results_df[are_images_in_test]","c0effaeb":"fig = plt.figure(figsize=(15, 4))\nax = sns.countplot([len(detection) for detection in train_megadetector_results_df[\"detections\"]])\nax.set(xlabel='Number of detections', ylabel='count')\nplt.title('Distribution of number of detection by MegaDetector for train data.')","571513f9":"fig = plt.figure(figsize=(15, 4))\nax = sns.countplot([len(detection) for detection in test_megadetector_results_df[\"detections\"]])\nax.set(xlabel='Number of detections', ylabel='count')\nplt.title('Distribution of number of detection by MegaDetector for test data.')","0545edbb":"# iWildCam 2021 - Starter Notebook\n\n## Let's contribute study for wild animals from here with camera traps traps dataset!\ud83d\udcf7","7844bd4f":"<a id=\"5\"><\/a> <br>\n# <div class=\"alert alert-block alert-success\">Exploratory data analysis<\/div>\n\nI'll check distribution of data in mainly three insight as following:\n- Category ID\n\n- Time point\n\n- Location\n\nAs far as the following EDA results are concerned, the given dataset seems be the same as last year. If you have interest, compare it to [my notes from last year](https:\/\/www.kaggle.com\/nayuts\/iwildcam-2020-overviewing-for-start). To a greater or lesser extent, we can use the findings of last year's competition.","490a04d5":"<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/iwildcam2021\/model_image.png\" width=\"***300***\">","859a2079":"Train data and test data seems be completelly taken in different locations.\n\nNumber of pictures are greatly differend by location.","0214ecb9":"<a id=\"2\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">Explanation for image file<\/div>\n\nFirst, let's take a look at what kind of images are available.","00f5f344":"<a id=\"6\"><\/a> <br>\n# <div class=\"alert alert-block alert-danger\">Sample solution<\/div>\n\nI have created sample solution, although it is not very accurate. Check out [this notebook](https:\/\/www.kaggle.com\/nayuts\/efficientnet-with-undersampling). It requires GPU to be turned on. I separated in order to people who forks this notebook and do some trial and error don't waste GPU time without realizing it.\n\n\nI haven't beaten kaggle_sample_all_zero_iwildcam_2021.csv yet, but I will publish the idea.\n\n1. First we crop the image based on the bbox detected by MegaDetector.\n2. In the training data, the correct answer labels are given as annotations, so we can use them to train the model.\n3. Classify the cropped images of the test data with the trained model.\n4. We choose the animal species and their counts of the image with the highest count among the images in the same image burst.","f17a1150":"If we use such imbalanced data as it is, we may not be able to train our models well. For a quickly, using [RandomUnderSampler](https:\/\/imbalanced-learn.org\/stable\/under_sampling.html#controlled-under-sampling-techniques) of [imblearn](https:\/\/imbalanced-learn.org\/stable\/) may improve the situation.","e5b1a66f":"## How can we improve accuracy?\n\nWe also check how empty the training data and test data are. We will also check how empty the training and test data is, because when we submit the results of our inference, we will fount that \"kaggle_sample_all_zero_iwildcam_2021.csv\" is very powerful. This should be because much of the testdata is mostly empty. From the detection results using MegaDetector, we can see roughly how much of the image is empty. Let's take a look.","73134d5a":"## Contents\n- [Competition goal and metric](#0)\n- [Motivation of competition](#1)\n- [Explanation for image file](#2)\n- [Explanation for submission file](#3)\n- [Explanation for metadata file](#4)\n- [Exploratory data analysis](#5)\n- [Sample solution](#6)","bec9348f":"Let's look at the cumulative ratio. If we take the cumulative sum in order of increasing number, we can see that the 40th category reaches 95% and the 90th category reaches 99%.","22cde559":"### Test data","9ac33c5d":"Since there are 263504 detection data, it seems that all the data from train data and test data have been processed. So we do not need to run MegaDetector ourselves. If you want to finetune with the MegaDetector's weights or redo the estimation yourself, please refer to this [notebook](https:\/\/www.kaggle.com\/nayuts\/try-megadetector-crop-animals-on-kaggle-notebook).","dfcf3a0b":"The annotation data seems to be biased to some extent. To see the breakdown, let's look at the top 10 categories.\n\nmpty is the most, but annotations stating that animals are in the picture also seem to vary among the top 10.","4486bb9b":"<a id=\"0\"><\/a> <br>\n# <div class=\"alert alert-block alert-warning\">Competition goal and metric<\/div>\n\nThe goal of this competition is to categorize species and count the number of individuals across image bursts of camera trap. Image bursts of camera trap are assigned unique ID. The individual images that make up image bursts are also assigned ID. Using the image burst as input, count up how many animals of the 204 annotated species are present and output as CSV file.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/iwildcam2021\/task_image.png\n\" width=\"***500***\">\n\nEvaluation will be done by Mean Columnwise Root Mean Squared Error(MCRMSE). \n\n$$\n\\frac{1}{m}\\sum_{j=1}^{m}\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij}-y_{ij})^2}\n$$\n\nj represents a species, i represents a sequence, x_ij is the predicted count for that species in that sequence, and y_ij is the ground truth count. This is an index that takes the RMSE for each species and then averages it across species.","60d5236a":"## Where did data taken?\n\nWe are required to detect photographs taken at different locations, but how distribute are data in perspect of location?","5c66e10a":"Train data are bias. In February, March, June and July, data are rich than other months.\n\nTrain data covers test data in perspective of month.\n\nData for November and December are missing. Do animals hibernate?","847611b5":"### Year and Month Perspective","f84777b7":"### Monthly perspective","27bcd5eb":"### Hourly perspectives","b0b0d7b6":"### <u>About this notebook<\/u>\nI want to give you an overview of the prior knowledge and data that might be needed in this challenge!\n\nThis contest aims to detect wildlife in trapped picture at new monitoring locations. Now, we can get great insights about wildlife by camera traps. Camera trap is popular method and there are so many data in the world. But due to the so large number of data, it seems that the data was not always effectively accessed and utilized.\n\nIn this competition, we aim to develop a model that effectively classifies animals taken at different observation points\u3000in the world. This challenge will surely bring great insights.","69d01908":"<a id=\"4-2\"><\/a> <br>\n## Checking iwildcam2021_test_information.json\n\nInformation for test dataset. The format is similar to iwildcam2021_train_annotations.json with only images key.","022fbe03":"I created dataset that crop the detection area of MegaDetector. Because of the processing time involved, I wrote it in [separate notebook](https:\/\/www.kaggle.com\/nayuts\/256-x-256-cropped-images).","3b2b4ac2":"### Train data","50b64f00":"Thus, creating a model for effectively classifying wild animals is a very significant effort.\n\n### Referece\n\n[1]https:\/\/www.wwf.org.uk\/project\/conservationtechnology\/camera-trap\n\n[2]https:\/\/www.wwf.or.jp\/campaign\/2015_camera\/\n\n[3]https:\/\/en.wikipedia.org\/wiki\/Camera_trap\n\n[4]https:\/\/www.blog.google\/products\/earth\/ai-finds-where-the-wild-things-are\/","bf163ad6":"### Day and night perspective","8d62acff":"<a id=\"3\"><\/a> <br>\n# <div class=\"alert alert-block alert-success\">Explanation for submission file<\/div>\n\nExplanation for submission file\n\nIn iWildcam 2021 - FGVC8 conpetition, we have to detect the number of each animal species in the image sequences. On the other hand, [last year (in iWildcam 2020 - FGVC7)](https:\/\/www.kaggle.com\/c\/iwildcam-2020-fgvc7\/overview\/evaluation) we identified which category of animal was being shown. ","b61799b3":"If we see the images, we can find that they are a series of images.","64aad9cf":"<a id=\"4-1\"><\/a>\n## Checking iwildcam2021_train_annotations.json\n\n\nWe are provided annotation data for train data as \"iwildcam2021_train_annotations.json\". This json follows COCO-CameraTraps format with additional field.\n\nIf we load the json, we can find there are three key-values in it.","8b37343f":"The value of the categories key contains a list of annotated animal species. id 0 is empty. The id is up to 571, but there are 204 annotated species. So we have to classify for 204 species + empty at most (not 571 + empty)!","6d89b069":"Data starts 2013-01 but there seems be some lacks. For example, train data between 2013-11 to 2014-02 are missing.\n\nAlso we can find that train data in between 2013-01 to 2013-07 are rich than other time point.\n\nTrain data covers test data in perspective of time point.","3dcbc279":"We can see that the training data is nearly half empty, but the test data is almost half empty. In other words, in order to improve accuracy, it is better to submit all columns as zero as soon as possible if t can be determined to be empty with certainty.","dce55b76":"We can see the result like this.","fc828949":"When we looked at given image files at the beginning of this notebook, we saw mixture of color and black and white images. Using the time of day feature, we may be able to successfully distinguish whether they are in color or not.","25cae9f7":"By camera traps, images are taken continuously because they are captured in bursts triggered by motion. Therefore, dataset also contains series of images, and in addition to IDs of image, IDs of the sequence are assigned.We will use IDs of image to load the image, on the other hands we will use IDs of sequence to submit.\n\nTake a look at [the submission file for details](#3).\n\nAdditionally, we will notice that some of the images are in color and some are in black and white. This is due to the difference in whether the images were taken during the day or at night.","d5e0c41a":"Especially, detected bbox is in detections value.","8dd379d8":"<a id=\"4-3\"><\/a> <br>\n## Checking iwildcam2021_megadetector_results.json\n\nWe can also use [Microsoft AI for Earth MegaDetector](https:\/\/github.com\/microsoft\/CameraTraps\/blob\/master\/megadetector.md). This model is trained to detect animals, people, and vehicles in camera trap images using hundreds of thousands of bounding boxes from various ecosystems. The model does not identify animals, it only finds them.\n\nWe are provided some sample detection results as \"iwildcam2021_megadetector_results.json\".","0c69eec3":"If we decide arbitrarily during daytime and at night, we can also calculate diurnal and nocturnal data counts.\n\nFor example, we define \"during daytime\" is \"6-17 O'clock\" and \"at night\" is \"18-5 O'clock\",","09fb44d4":"## When did data taken?\n\nBecause animals can change their activity from time to time, we want to understand how data is distributed over time.","bc894e51":"Id columns represents id of sequence as we see in [Checking iwildcam2021_train_annotations.json](#4-1). PredictedX columns are number of individuals of species in sequence. There are 111057 sequences in test dataset, and 205 species in given dataset. ","4c727daa":"It is also possible to crop the detected area like this. If you save the image, you can create  dataset.","92efe3a7":"info and detection_categories values are incidental information.","d5ed5386":"## How many data are there per animal category Id?\n\nThere are a lot of categories in dataset. To confirme how many data are there in each categories, I plot barplot.","ab07f73c":"Each training image has at least one associated annotation. The annotatetd catefory_ids are in value of \"annotations\" key.","9f296026":"In images value, we can get data for each wildcan images. Wildcam will take several frames in a row. The value of 'seq_num_frames' key is the number of frames, 'id' is the id of the image, and 'seq_id' is the ID associated with the sequentially shot image. This 'seq_id' is the same as the 'Id' in the submission file.\n\nLet's extract the data corresponding to shot of seq_id:302ad820-7d42-11eb-8fb5-0242ac1c0002.","75418c36":"There are three key-value data in json.\n\nDetected result is in images value.","d12ad64b":"Since there are many categories,I will also provide a plotly interactive bar chart to make it easier to check the details.","29f54a87":"<a id=\"1\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">Motivation of competition<\/div>\n\n## How can we take wildlife pictures and use?\nWe can take wildlife pictures by camera trap. Camera traps have infrared sensor or motion sensor, so they can detect animals. When animals come near, camera take their pictures. Using camera traps, we can monitor wildlifes continuously ,at several point and at the same time. So we can understand how animals run their life in the area researchers interest.[1]\n\nFor example, in Kaen Krachan National Park in Thailand, indian sinatra are directlly observed. So it was pointed out that there is no longer any possibility. But by using camera trap, we could confirm thir existence. [2]\n\nTraditionally, camera traps have been considered an excellent method for investigating ecological information about wildlife in a certain area.\u3000Data are used for population estimation, calculating population index, 24hours monitorling and so on.[3]\n\nResently there has been a movement to make effective use of photos from camera traps using machine learning. One of the example is [4]. Google successed to access so much wildlife photo knowledge. Following Wilflife Insights is the platform of taking the initiative.","c53517c9":"<a id=\"4\"><\/a> <br>\n# <div class=\"alert alert-block alert-success\">Explanation for metadata file<\/div>","338ce997":"There seems to be data in train data for all annotated categories.","759b577d":"On the other hand, fewer categories have only about one sample. We need to be careful when splitting the dataset to train and validation data when training the model."}}