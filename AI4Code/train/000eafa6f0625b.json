{"cell_type":{"1e3bfa91":"code","84f55d8f":"code","888d6b11":"code","24809d97":"code","75507d80":"code","dcfc0190":"code","89c7fe1f":"code","68722443":"code","c6701339":"code","aa9f2143":"code","828bdeb0":"code","f2353eb1":"code","4336076f":"code","4fc477d5":"code","fece79d4":"code","eae0c767":"code","f46ba7d0":"code","c7f2c535":"code","87545851":"code","4125dc01":"code","78690cd4":"code","88a2dcbb":"code","16feb80f":"code","c2b8c7fc":"code","70ad9c43":"code","6d9edc8e":"code","b9e34cb7":"code","7b7ab0bc":"code","dda26a5d":"code","badf3d1a":"code","ecf08eb7":"code","4b9dde77":"code","cf546634":"code","e48c7ac9":"code","50cb398a":"code","60716a6d":"code","5e6b9e06":"code","a79163a0":"code","4bca5456":"code","abb2445e":"code","35cdd17f":"code","16b38781":"code","e641b64d":"code","dd3e02e0":"code","6df9368a":"code","30567537":"code","bcdc160d":"code","45ff6d47":"code","32e9c603":"code","dacaa922":"markdown","0be5019b":"markdown","db2caa12":"markdown","9477f57a":"markdown","396a1a39":"markdown","84eb0d0d":"markdown","7d397a99":"markdown","ac959d74":"markdown","468926a4":"markdown","185d35b4":"markdown","59ae7143":"markdown","3bccc6be":"markdown","997d3721":"markdown","ea48cb91":"markdown","a984fded":"markdown","3fb34a22":"markdown"},"source":{"1e3bfa91":"from scipy import stats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\n\nimport ipywidgets as widgets\nfrom learntools.time_series.style import *  # plot style settings\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom pyearth import Earth\n\nfrom datetime import date\nimport holidays\nimport calendar\nimport dateutil.easter as easter\n\nfrom collections import defaultdict\nle = defaultdict(LabelEncoder)\n\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, HuberRegressor, RidgeCV\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\"axes\",labelweight=\"bold\",labelsize=\"large\",titleweight=\"bold\",titlesize=16,titlepad=10,)\nplot_params = dict(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\",markerfacecolor=\"0.25\",legend=False,)\n%config InlineBackend.figure_format = 'retina'\n\nimport gc\nimport os\nimport math\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","84f55d8f":"PRODUCTION = True # True: For submission run. False: Fast trial run\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"..\/input\/tabular-playground-series-jan-2022\"\nFEATURE_ENGINEERING = True\nPSEUDO_LABEL = True # PSEUDO are not ground true and will not help long term, only used for final push\nBLEND = True\n\nPSEUDO_DIR = \"..\/input\/pseuodo-labels\/pseudo_labels_v0.csv\"  \n\n# time series data common new feature  \nDATE = \"date\"\nYEAR = \"year\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"\n","888d6b11":"# https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/36414\ndef smape_loss(y_true, y_pred):\n\n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n","24809d97":"def get_basic_ts_features(df):\n    \n    gdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n    gdp_df.set_index('year', inplace=True)\n    gdp_exponent = 1.2121103201489674 # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n    \n    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n    \n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    for store in ['KaggleMart']:\n        df[store] = df['store'] == store\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n    df[MONTH] = df[DATE].dt.month\n    # 4 seasons\n    df['season'] = ((df[DATE].dt.month % 12 + 3) \/\/ 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n    df['wd4'] = df[DATE].dt.weekday == 4\n    df['wd56'] = df[DATE].dt.weekday >= 5\n    \n    # 21 days cyclic for lunar\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 32, 4):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'Finland_sin{k}'] = df[f'sin{k}'] * df['Finland']\n        df[f'Finland_cos{k}'] = df[f'cos{k}'] * df['Finland']\n        df[f'Norway_sin{k}'] = df[f'sin{k}'] * df['Norway']\n        df[f'Norway_cos{k}'] = df[f'cos{k}'] * df['Norway']\n        df[f'store_sin{k}'] = df[f'sin{k}'] * df['KaggleMart']\n        df[f'store_cos{k}'] = df[f'cos{k}'] * df['KaggleMart']\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    \n    # End of year\n    # Dec\n    for d in range(24, 32):\n        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n    for d in range(24, 32):\n        df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # Jan\n    for d in range(1, 14):\n        df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n    for d in range(1, 10):\n        df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n    for d in range(1, 15):\n        df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    # May\n    for d in list(range(1, 10)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n    for d in list(range(19, 26)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # June\n    for d in list(range(8, 14)):\n        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    \n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),2016: pd.Timestamp(('2016-06-11')),\n                                           2017: pd.Timestamp(('2017-06-10')),2018: pd.Timestamp(('2018-06-10')),\n                                           2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n\n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    for d in list(range(-4, 6)):\n        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n        \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),2016: pd.Timestamp(('2016-11-6')),\n                                        2017: pd.Timestamp(('2017-11-5')),2018: pd.Timestamp(('2018-11-4')),\n                                        2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n                                      for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})], axis=1)\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":(df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df  \n","75507d80":"for ptr in holidays.Norway(years = [2019], observed=True).items():\n    print(ptr)","dcfc0190":"def feature_engineer(df):\n    df = get_basic_ts_features(df)\n    return df","89c7fe1f":"from pathlib import Path\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", parse_dates=[DATE],\n                    usecols=['date', 'country', 'store', 'product', 'num_sold'],\n                    dtype={'country': 'category','store': 'category','product': 'category', 'num_sold': 'float32',},\n                    infer_datetime_format=True,)\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=ID, parse_dates=[DATE])\n    column_y = df_train.columns.difference(df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    \n    return df_train, df_test, column_y\n","68722443":"def process_data(df_train, df_test):\n    \n    if FEATURE_ENGINEERING:\n        df_train = feature_engineer(df_train)\n        df_test = feature_engineer(df_test)\n\n    return df_train, df_test","c6701339":"%%time\ntrain_df, test_df, column_y = load_data()","aa9f2143":"%%time\ntrain_df, test_df = process_data(train_df, test_df)","828bdeb0":"train_data = train_df.copy()\ntrain_data[DATE] = train_df.date.dt.to_period('D')\ntest_data = test_df.copy()\ntest_data[DATE] = test_df.date.dt.to_period('D')","f2353eb1":"df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\ndf_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\ndf_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\ntest_data[column_y] = df_pseudolabels[column_y].astype(np.float32)\ntrain_data = pd.concat([train_data, test_data], axis=0)\ntrain_df = pd.concat([train_df, test_data], axis=0)","4336076f":"X = train_data.set_index([DATE]).sort_index()\nX_test = test_data.set_index([DATE]).sort_index()","4fc477d5":"train_data = train_data.set_index(['date', 'country', 'store', 'product']).sort_index()","fece79d4":"kaggle_sales_2015 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2015'])","eae0c767":"kaggle_sales_2016 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2016'])","f46ba7d0":"kaggle_sales_2017 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2017'])","c7f2c535":"kaggle_sales_2018 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2018'])","87545851":"frames = [kaggle_sales_2015, kaggle_sales_2016, kaggle_sales_2017, kaggle_sales_2018]\nkaggle_sales = pd.concat(frames)","4125dc01":"kaggle_sales","78690cd4":"gc.collect()","88a2dcbb":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])","16feb80f":"train_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 5)))","c2b8c7fc":"train_data['month']","70ad9c43":"fig_dims = (50,30)\nax = kaggle_sales.num_sold.plot(title='Sales Trends', figsize=fig_dims)\n_ = ax.set(ylabel=\"Numbers sold\")","6d9edc8e":"# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,'store': store,'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df[GROUP_INDEX] = demo_df[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))\n    demo_df['num_sold'] = np.expm1(model.predict(preproc.transform(demo_df[features])))\n    train_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\n    plt.figure(figsize=(24, 8))\n    plt.plot(demo_df[DATE], demo_df.num_sold, label='prediction', alpha=0.5)\n    plt.scatter(train_subset[DATE], train_subset.num_sold, label='true', alpha=0.5, color='red', s=2)\n    plt.grid(True)\n    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n    plt.legend()\n    plt.title(f'{country} {store} {product} Predictions and true for five years')\n    plt.show()\n    \n    return demo_df['num_sold']","b9e34cb7":"def find_min_SMAPE(y_true, y_predict):\n    loss_correction = 1\n    scores = []\n    # float step\n    for WEIGHT in np.arange(0.97, 1.02, 0.0001):\n        y_hat = y_predict.copy()\n        y_hat *= WEIGHT\n        scores.append(np.array([WEIGHT, np.mean(smape_loss(y_true, y_hat))]))\n        \n    scores = np.vstack(scores)\n    min_SMAPE = np.min(scores[:,1])\n    print(f'min SMAPE {min_SMAPE:.5f}')\n    for x in scores:\n        if x[1] == min_SMAPE:\n            loss_correction = x[0]\n            print(f'loss_correction: {x[0]:.5f}')\n            \n    plt.figure(figsize=(5, 3))\n    plt.plot(scores[:,0],scores[:,1])\n    plt.scatter([loss_correction], [min_SMAPE], color='g')\n    plt.ylabel(f'SMAPE')\n    plt.xlabel(f'loss_correction: {loss_correction:.5f}')\n    plt.legend()\n    plt.title(f'min SMAPE:{min_SMAPE:.5f} scaling')\n    plt.show()\n    \n    return loss_correction","7b7ab0bc":"def plot_true_vs_prediction(df_true, df_hat):\n    plt.figure(figsize=(20, 13))\n    plt.scatter(np.arange(len(df_hat)), np.log1p(df_hat), label='prediction', alpha=0.5, color='blue', s=3) \n    plt.scatter(np.arange(len(df_true)), np.log1p(df_true), label='Pseudo\/true', alpha=0.5, color='red', s=7) \n    plt.legend()\n    plt.title(f'Predictions VS Pseudo-label {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.show()","dda26a5d":"def plot_residuals(y_residuals):\n    plt.figure(figsize=(13, 3))\n    plt.scatter(np.arange(len(y_residuals)), np.log1p(y_residuals), label='residuals', alpha=0.1, color='blue', s=5)\n    plt.legend()\n    plt.title(f'Linear Model residuals {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.tight_layout()\n    plt.show()","badf3d1a":"def plot_oof(y_true, y_predict):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(y_true, y_predict, s=1, color='r', alpha=0.5)\n    plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n    plt.gca().set_aspect('equal')\n    plt.xlabel('y_true')\n    plt.ylabel('y_pred')\n    plt.title('OOF Predictions')\n    plt.show()","ecf08eb7":"def evaluate_SMAPE(y_va, y_va_pred):\n    loss_correction = 1\n    \n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n    smape = np.mean(smape_loss(y_va, y_va_pred))\n    loss_correction = find_min_SMAPE(y_va, y_va_pred)\n    y_va_pred *= loss_correction\n    print(f\"SMAPE (before correction: {smape_before_correction:.5f})\")\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred))}')\n    \n    return loss_correction","4b9dde77":"GROUP_INDEX = ['country', 'store', 'product', 'month', 'season']\n\n# Target series\ny = X.loc[:, column_y]\n\n# X_1: Features for Linear Regression\nfourier = CalendarFourier(freq=\"A\", order=10)  # 10 sin\/cos pairs for \"A\"nnual seasonality\n\ndp = DeterministicProcess(index=X.index,constant=True,order=1,seasonal=True,additional_terms=[fourier],drop=True,)\n\nX_1 = dp.in_sample()  # create features for dates in tunnel.index\n\n# X_2: Features for XGBoo\nX_2 = X.drop(column_y, axis=1)\n\n# Encoding the variable\nX_2[GROUP_INDEX] = X_2[GROUP_INDEX].apply(lambda x: le[x.name].fit_transform(x))\n\n# Using the dictionary to label future data\nX_test[GROUP_INDEX] = X_test[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))","cf546634":"features = X_2.columns","e48c7ac9":"if PSEUDO_LABEL:\n    TRAIN_END_DATE = \"2019-12-31\"\n    VALID_START_DATE = \"2015-01-01\"\n    VALID_END_DATE = \"2018-12-31\"\nelse:\n    if PRODUCTION:\n        TRAIN_END_DATE = \"2018-12-31\"\n    else:\n        TRAIN_END_DATE = \"2017-12-31\"\n    VALID_START_DATE = \"2018-01-01\"\n    VALID_END_DATE = \"2018-12-31\"\n\ny_train, y_valid = y[:TRAIN_END_DATE], y[VALID_START_DATE:VALID_END_DATE]\nX1_train, X1_valid = X_1[:TRAIN_END_DATE], X_1[VALID_START_DATE:VALID_END_DATE]\nX2_train, X2_valid = X_2.loc[:TRAIN_END_DATE], X_2.loc[VALID_START_DATE:VALID_END_DATE]","50cb398a":"# You'll add fit and predict methods to this minimal class\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\nclass BoostedHybrid(BaseEstimator, RegressorMixin):\n    def __init__(self, model_1, model_2, scaler):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.scaler = scaler\n        self.y_columns = None  # store column names from fit method\n    \n    def fit(self, X, y): #, X_1_valid, y_valid\n        \n        X, y = check_X_y(X, y, accept_sparse=True)\n        # Train model_1\n        self.model_1.fit(X, y)\n\n        # Make predictions\n        y_fit = self.model_1.predict(X)\n        # Compute residuals\n        y_resid = y - y_fit\n\n        # Train model_2 on residuals , eval_set=[(X_1_valid, y_valid_resid)]\n        self.model_2.fit(X, y_resid)\n        \n        # Model2 prediction\n        y_fit2 = self.model_2.predict(X)\n        # Compute noise\n        y_resid2 = y_resid - y_fit2\n        \n        # Save data for question checking\n        self.y = y\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n        self.y_fit2 = y_fit2\n        self.y_resid2 = y_resid2\n\n        self.is_fitted_ = True\n        \n        return self\n\n\n    def predict(self, X):\n        \n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n        # Predict with model_1\n        y_predict = self.model_1.predict(X)\n        \n        # Add model_2 predictions to model_1 predictions\n        y_predict += self.model_2.predict(X)\n\n        return y_predict\n\n","60716a6d":"preproc = StandardScaler()","5e6b9e06":"def model_fit_eval(hybrid_model, X_train, y_train, X_valid, y_valid, scaler, loss_correction):\n    test_pred_list = []\n    \n    # Boosted Hybrid\n    hybrid_model.fit(X_train, y_train) #, X_valid, y_valid\n    y_va_pred = hybrid_model.predict(X_valid)\n        \n    ###### Preprocess the validation data\n    y_va = np.expm1(y_valid.copy())\n    \n    # Inference for validation\n    y_va_pred = np.expm1(hybrid_model.predict(X_valid))\n    loss_correction = evaluate_SMAPE(y_va, y_va_pred)\n    \n    ###### Visualize and evual\n    plot_oof(y_va, y_va_pred)\n    plot_true_vs_prediction(y_va, y_va_pred)\n    \n    ###### Validate against 2019 PSEU #######\n    loss_correction = 1\n    \n    ###### Preprocess the validation data\n    y_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n    \n    # Inference test 2019 for validation\n    y_va_pred = np.expm1(hybrid_model.predict(scaler.transform(X_test[features])))\n    \n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    smape = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    print(f'***********Test Data*****************')\n    loss_correction = find_min_SMAPE(y_va, y_va_pred.reshape(-1, 1))\n    \n    ### Mean test prediction ###\n    test_pred_list.append(y_va_pred)\n\n    print(f'SMAPE (before correction: {smape_before_correction:.5f})')\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)*loss_correction))}')\n    \n    return hybrid_model, test_pred_list, loss_correction","a79163a0":"xgb_params = dict(booster= 'gbtree',\n                  tree_method = 'exact',\n                  objective='reg:pseudohubererror',\n                  n_jobs = -1,\n                  max_depth=5,\n                  learning_rate=0.5406089095129346,\n                  n_estimators=5918,\n                  min_child_weight=3,\n                  colsample_bytree=0.2595406744619732,\n                  # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n                  reg_alpha=97.39174536138904,\n                  reg_lambda=0.5348869112742457,\n                  # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n                  gamma=0.1024158842570319,\n                  # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n                  num_parallel_tree=1,\n                  random_state=35,\n                 )\n\nlgbm_params = {\n              'learning_rate':0.025292895772398984,\n              \"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              'boosting_type': \"gbdt\",\n              'verbosity': -1,\n              'n_jobs': -1, \n              'seed': 21,\n              'reg_alpha': 0.0029751624135773416,\n              'reg_lambda': 0.650014120724397,\n              'lambda_l1': 1.1096023419303558, \n              'lambda_l2': 1.996527963987735, \n              'num_leaves': 109, \n               # 'feature_fraction': 0.6259927292757151, \n               # 'bagging_fraction': 0.9782210574588895, \n               # 'bagging_freq': 1, \n              'n_estimators': 2606, \n              'max_depth': 1, \n              'max_bin': 244, \n              'min_data_in_leaf': 366,\n              'random_state' : RANDOM_STATE,\n              }\n","4bca5456":"# xgb_params = dict(booster= 'gbtree',\n#                   tree_method = 'exact',\n#                   objective='reg:pseudohubererror',\n#                   n_jobs = -1,\n#                   max_depth=trial.suggest_int(\"max_depth\",2,5),\n#                   learning_rate=trial.suggest_float('learning_rate',.001,1),\n#                   n_estimators=trial.suggest_int('n_estimators',1000,10000),\n#                   min_child_weight=trial.suggest_int('min_child_weight',1,3),\n#                   colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#                   # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n#                   reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#                   reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#                   # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n#                   gamma=trial.suggest_float(\"gamma\", 0,1),\n#                   # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n#                   num_parallel_tree=1,\n#                   random_state=35,\n#                   )","abb2445e":"# import optuna\n# import lightgbm as lgb\n# import sklearn.datasets\n# import sklearn.metrics\n\n# LOSS_CORRECTION = 1\n# estimator_stack = []\n\n# def objective(trial):\n#     LOSS_CORRECTION = 1\n#     xgb_params = dict(booster= 'gbtree',\n#                       tree_method = 'exact',\n#                       objective='reg:pseudohubererror',\n#                       n_jobs = -1,\n#                       max_depth=trial.suggest_int(\"max_depth\",2,5),\n#                       learning_rate=trial.suggest_float('learning_rate',.001,1),\n#                       n_estimators=trial.suggest_int('n_estimators',1000,10000),\n#                       min_child_weight=trial.suggest_int('min_child_weight',1,3),\n#                       colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#                       # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n#                       reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#                       reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#                       # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n#                       gamma=trial.suggest_float(\"gamma\", 0,1),\n#                       # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n#                       num_parallel_tree=1,\n#                       random_state=35,\n#                      )\n    \n#     X2 = preproc.fit_transform(X2_train[features])\n#     model = BoostedHybrid(model_1 = Ridge(alpha=.85),model_2 = XGBRegressor(**xgb_params),scaler = preproc)\n#     model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                             np.log1p(y_valid),preproc, LOSS_CORRECTION)\n    \n#     return LOSS_CORRECTION  \n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=500)\n# xgb_params = study.best_params\n# print('best params',xgb_params)\n\n\n# def objective(trial):\n#     LOSS_CORRECTION=1\n#     lgbm_params = {\n#                   'learning_rate': trial.suggest_float('learning_rate', .001, 1.0),\n#                   \"objective\": \"regression\",\n#                   \"metric\": \"rmse\",\n#                   'boosting_type': \"gbdt\",\n#                   'verbosity': -1,\n#                   'n_jobs': -1, \n#                   'seed': 21,\n#                   'lambda_l1': trial.suggest_float('lambda_l1',.1,2.0),\n#                   'lambda_l2': trial.suggest_float('lambda_l2', .1,2.0),\n#                   'num_leaves': trial.suggest_int('num_leaves', 100, 500),\n#                    #'feature_fraction': trial.suggest_float('feature_fraction',.01,1.0),\n#                    # 'bagging_fraction': trial.suggest_float('bagging_fraction',.01,1.0),\n#                    # 'bagging_freq': trial.suggest_int('bagging_freq',1,2), \n#                   'n_estimators': trial.suggest_int('n_estimators',100,5000), \n#                   'max_depth': trial.suggest_int('max_depth', 1,7), \n#                   'max_bin': trial.suggest_int('max_bin', 10,500),\n#                   # 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',300,5000),\n#                   }\n    \n    \n#     X2 = preproc.fit_transform(X2_train[features])\n#     model = BoostedHybrid(model_1 = Ridge(alpha=.85),model_2 = lgb.LGBMRegressor(**lgbm_params),scaler = preproc)\n#     model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                             np.log1p(y_valid),preproc, LOSS_CORRECTION)\n    \n#     return LOSS_CORRECTION  \n    \n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=500)\n# lgbm_params = study.best_params\n# print('best params',lgbm_params)\n\n\n###########################################################################################\n# ***********Test Data*****************\n# min SMAPE 0.57772\n# loss_correction: 0.99910\n# SMAPE (before correction: 0.58343)\n# Min SMAPE: 0.5777192140317523\n# [LightGBM] [Warning] lambda_l2 is set=1.996527963987735, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.996527963987735\n# [LightGBM] [Warning] lambda_l1 is set=1.1096023419303558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1096023419303558\n# min SMAPE 4.19017\n# loss_correction: 1.00080\n# SMAPE (before correction: 4.19074)\n# Min SMAPE: 4.190172283130202\n# [I 2022-01-22 22:00:03,571] Trial 73 finished with value: 0.9984999999999968 and parameters: {'learning_rate': 0.025292895772398984, 'lambda_l1': 1.1096023419303558, 'lambda_l2': 1.996527963987735, 'num_leaves': 109, 'n_estimators': 2606, 'max_depth': 1, 'max_bin': 244}. Best is trial 73 with value: 0.9984999999999968.\n# ***********Test Data*****************\n# min SMAPE 0.63428\n# loss_correction: 0.99850\n# SMAPE (before correction: 0.64643)\n# Min SMAPE: 0.6342847324685896\n# [LightGBM] [Warning] lambda_l2 is set=1.9365125171332647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9365125171332647\n# [LightGBM] [Warning] lambda_l1 is set=1.1349580383858455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1349580383858455\n# min SMAPE 4.15496\n# loss_correction: 1.00090\n# SMAPE (before correction: 4.15572)\n# Min SMAPE: 4.154962358508988\n#################################################\n\n# param1 = {'loss_function': 'MultiRMSE','eval_metric': 'MultiRMSE','n_estimators': 1000,'od_type' : 'Iter',\n#             'od_wait' : 20,'random_state': RANDOM_STATE,'verbose': VERBOSE}\n\n# # Try different combinations of the algorithms above KNeighborsRegressor\n# models_1 = [Earth(verbose=VERBOSE), Ridge(alpha=.85), HuberRegressor(epsilon=1.20, max_iter=500),\n#             MLPRegressor(  hidden_layer_sizes=(256, 128),learning_rate_init=0.01,early_stopping=True,\n#                             random_state=RANDOM_STATE ),]\n\n# models_2 = [XGBRegressor(objective='reg:pseudohubererror', tree_method='hist', n_estimators=1000),\n#             lgb.LGBMRegressor(objective='regression', n_estimators=1000, random_state=RANDOM_STATE),\n#             CatBoostRegressor(**param1),]\n\n# for model_1 in models_1:\n#     for model_2 in models_2:\n#         model1_name = type(model_1).__name__\n#         model2_name = type(model_2).__name__\n#         hybrid_model = BoostedHybrid(model_1 = model_1,model_2 = model_2,scaler = preproc)\n#         print(f'******************Stacking {model1_name:>15} with {model2_name:<18}*************************')\n#         estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n        \n# \n# model = StackingRegressor(estimators=estimator_stack, final_estimator=RidgeCV(alphas=[.7,.8,.85,.9,1,1.1,1.2]),\n#                           n_jobs=-1, verbose=VERBOSE)\n\n# model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                         np.log1p(y_valid),preproc, LOSS_CORRECTION)\n\n# model = XGBRegressor(**xgb_params)\n\n","35cdd17f":"%%time\nLOSS_CORRECTION = 1\nestimator_stack = []\n\nparam1 = {'loss_function': 'MultiRMSE','eval_metric': 'MultiRMSE','n_estimators': 1000,'od_type' : 'Iter',\n          'od_wait' : 20,'random_state': RANDOM_STATE,'verbose': VERBOSE}\n\n# Try different combinations of the algorithms above KNeighborsRegressor\nmodels_1 = [Earth(verbose=VERBOSE), Ridge(alpha=.85), HuberRegressor(epsilon=1.20, max_iter=500),\n            MLPRegressor(hidden_layer_sizes=(256, 128),learning_rate_init=0.01,early_stopping=True,\n                         random_state=RANDOM_STATE ),]\n\nmodels_2 = [XGBRegressor(**xgb_params),lgb.LGBMRegressor(**lgbm_params),CatBoostRegressor(**param1),]\n\nfor model_1 in models_1:\n    for model_2 in models_2:\n        model1_name = type(model_1).__name__\n        model2_name = type(model_2).__name__\n        hybrid_model = BoostedHybrid(model_1 = model_1,model_2 = model_2,scaler = preproc)\n        print(f'******************Stacking {model1_name:>15} with {model2_name:<18}*************************')\n        estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n        \nX2 = preproc.fit_transform(X2_train[features])\n\nmodel = StackingRegressor(estimators=estimator_stack,final_estimator=RidgeCV(), n_jobs=-1, verbose=VERBOSE)\n\nmodel,test_pred_list,LOSS_CORRECTION=model_fit_eval(model,X2,np.log1p(y_train),preproc.transform(X2_valid[features]),\n                                                    np.log1p(y_valid),preproc,LOSS_CORRECTION)","16b38781":"for country in np.unique(train_df['country']):\n    for product in np.unique(train_df['product']):\n        for store in np.unique(train_df['store']):\n            y_fit = plot_five_years_combination(feature_engineer, country=country, product=product, store=store)","e641b64d":"y_pred = sum(test_pred_list) \/ len(test_pred_list) #model.predict(X_test[features])","dd3e02e0":"%%time\nLOSS_CORRECTION = 1\n\n###### Preprocess the validation data\ny_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n\n# Inference for validation\ny_va_pred = y_pred.copy().reshape(-1, 1) #model.predict(X_test[features])\n\n# Evaluation: Execution time and SMAPE\nsmape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\nsmape = np.mean(smape_loss(y_va, y_va_pred))\nLOSS_CORRECTION = find_min_SMAPE(y_va, y_va_pred)\ny_va_pred *= LOSS_CORRECTION\n\nprint(f\" SMAPE: {smape:.5f} (before correction: {smape_before_correction:.5f})\")\nprint(np.mean(smape_loss(y_va, y_va_pred)))\n\n# [I 2022-01-21 19:08:22,256] Trial 21 finished with value: 0.996799999999997 and parameters: {'max_depth': 5, \n# 'learning_rate': 0.5406089095129346, 'n_estimators': 5918, 'min_child_weight': 3, 'colsample_bytree': \n# 0.2595406744619732, 'reg_alpha': 97.39174536138904, 'reg_lambda': 0.5348869112742457, 'gamma': \n# 0.1024158842570319}. Best is trial 12 with value: 0.996799999999997.\n# ***********Test Data*****************\n# min SMAPE 0.78355\n# loss_correction: 0.99680\n# SMAPE (before correction: 0.82951)\n# Min SMAPE: 0.7835522368396695\n# min SMAPE 4.13588\n# loss_correction: 1.00080\n# SMAPE (before correction: 4.13639)\n# Min SMAPE: 4.1358802070879515","6df9368a":"plot_oof(y_va, y_va_pred)\nplot_true_vs_prediction(y_va, y_va_pred)\nplot_residuals(model.estimators_[0].y_resid)\nplot_residuals(model.estimators_[0].y_resid2)","30567537":"from math import ceil, floor, sqrt\n# from https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n\n    return result_array","bcdc160d":"sub = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')","45ff6d47":"# Inference for test\ntest_prediction_list = []\ntest_prediction_list.append(y_pred) # * LOSS_CORRECTION)\ndf_pseudolabels1 = pd.DataFrame()\ndf_pseudolabels2 = pd.DataFrame()\ndf_pseudolabels3 = pd.DataFrame()\n\nif BLEND:\n    \n    test_prediction_list.append(df_pseudolabels[column_y].values) #blender 1\n    \n    df_pseudolabels1 = pd.read_csv('..\/input\/tell-me-the-magic-number\/submission.csv', index_col=ID)    \n    \n    test_prediction_list.append(df_pseudolabels1[column_y].values) #blender 2\n    \n    df_pseudolabels2 = pd.read_csv('..\/input\/tps-01-2022\/submission.csv', index_col=ID)\n    \n    test_prediction_list.append(df_pseudolabels2[column_y].values)\n    \n    df_pseudolabels3 = pd.read_csv('..\/input\/tpsjan22-03-linear-model\/submission_linear_model.csv', index_col=ID)\n    \n    test_prediction_list.append(df_pseudolabels3[column_y].values)\n\ntest_prediction_list = np.median(test_prediction_list, axis=0) \n\n\nif len(test_prediction_list) > 0:\n    # Create the submission file\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0],2)),index = sub.index.tolist(),columns=[ID,column_y])\n    submission[ID] = sub[ID]\n    submission[column_y] = test_prediction_list\n    \n    #https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/299162\n    submission[column_y] = geometric_round(submission[column_y]).astype(int) \n    \n    submission.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df[column_y], bins=np.linspace(0, 3000, 201),density=True, label='Training')\n    plt.hist(submission[column_y], bins=np.linspace(0, 3000, 201),density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel(column_y)\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","32e9c603":"display(submission.head(30))\ndisplay(submission.tail(30))","dacaa922":"# Data\/Feature Engineering","0be5019b":"## Although I've made many improvements to this notebook and improved the score, much of the credit should go to:  \n\n#### Credit to [@teckmengwong](https:\/\/www.kaggle.com\/teckmengwong\/tps2201-hybrid-time-series) for a fun notebook to work with.  and:\n\n#### Feature engineering and Linear model based on excellent: https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model @ambrosm  and:\n\n#### Hybrid model from Time series course: https:\/\/www.kaggle.com\/learn\/time-series","db2caa12":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","9477f57a":"## Using StandardScaler","396a1a39":"### Holiday generator","84eb0d0d":"# Data Pipeline","7d397a99":"# Training","ac959d74":"\n\n\n## Imports and Configuration ##","468926a4":"## Data preprocessing X_2 X_test y","185d35b4":"## Pseudolabeling","59ae7143":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=ID)\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    return df_train, df_test, column_y","3bccc6be":"# Loss function SMAPE\n\u200b\u200bi=1\u200b\u2211\u200bN\u200b\u200bw\u200bi\u200b\u200b\u200b\u200b100\u200bi=1\u200b\u2211\u200bN\u200b\u200b\u200b(\u2223t\u200bi\u200b\u200b\u2223+\u2223a\u200bi\u200b\u200b\u2223)\/2\u200b\u200bw\u200bi\u200b\u200b\u2223a\u200bi\u200b\u200b\u2212t\u200bi\u200b\u200b\u2223\u200b\u200b\u200b\u200b","997d3721":"# Inference validation","ea48cb91":"# Inference year 2019 test data","a984fded":"# Submission\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n","3fb34a22":"## Removal of 2016 test"}}