{"cell_type":{"ae8f8fe8":"code","2138e458":"code","465e9ac3":"code","a1d5d434":"code","7b0d69c6":"code","8167cfe4":"code","0ae3fd94":"code","e91fa12a":"code","b368450e":"code","9a829640":"code","5d2477a0":"code","01890991":"code","a3ad710d":"code","95f37e93":"code","1f6fc839":"code","a6b63513":"code","65865e00":"code","f8658f91":"code","11087190":"code","468d72f8":"code","a286a29c":"code","6812ae4b":"code","21ed7c42":"code","dbd197c8":"code","647abaa1":"code","7bbcc573":"code","9d7f5be8":"code","ef7e6cfe":"code","5d04d053":"code","58752f83":"code","e4906fe0":"code","8cfe5c2b":"code","77327d85":"code","7aa3bb3b":"code","4bd05980":"code","11f20c4e":"code","6d4bcfc1":"code","35213836":"markdown","8c027685":"markdown","e6d30107":"markdown","7778a045":"markdown","4a57ab44":"markdown","38b9ef54":"markdown","df9d54a6":"markdown","24cfe6eb":"markdown","74a30f80":"markdown","14f36391":"markdown","490cee65":"markdown","489802aa":"markdown","b789509e":"markdown","5523374e":"markdown","1f3749ad":"markdown","e92f292f":"markdown","7d14973e":"markdown","7cce9349":"markdown","432a5f22":"markdown","f9c995d1":"markdown","45d3f82c":"markdown","ee2c8e08":"markdown","e23919ef":"markdown","e902c3ad":"markdown","fac1d1bb":"markdown","7f5980b2":"markdown","2863864b":"markdown","4a630d33":"markdown"},"source":{"ae8f8fe8":"# required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","2138e458":"# load the data\ndf = pd.read_csv(\"..\/input\/used-cars\/car_data.csv\", parse_dates=True, infer_datetime_format=True)\ndf.head()","465e9ac3":"# print out the info about data\ndf.info()","a1d5d434":"# calcualte number of missing values and their percentages in each columns of every data file\ndef missing_data(list_of_dfs, list_of_names=None):\n    for i in range(len(list_of_dfs)):\n        df = list_of_dfs[i]\n        col_names = df.columns.tolist()\n        na_per_col = df.isna().sum().tolist()\n        na_percentage = (df.isna().sum() \/ len(df) * 100).tolist()\n        if list_of_names != None:\n            print(\"=\" * 20, \"Missing values of\", list_of_names[i], \"=\" * 20)\n        display(\n            pd.DataFrame(\n                list(zip(na_per_col, na_percentage)),\n                columns=[\"Number of missings\", \"% of missings\"],\n                index=col_names,\n            ).style.background_gradient(axis=0)\n        )\n\n\n# apply the function\nmissing_data([df])","7b0d69c6":"# check for number of duplicated rows\ndisplay(df.duplicated().sum())\nprint(f\"Percentage of duplicated rows is {df.duplicated().sum()\/len(df):.2%}.\")","8167cfe4":"# retreiev which rows were repeated\nduplicates = df.duplicated(keep=False)\ndf.loc[duplicates].sort_values([\"DateCrawled\"])","0ae3fd94":"# convert column names to lowercase\ndf.columns = df.columns.str.lower().str.strip()\ndf.head(1)","e91fa12a":"df[[\"datecrawled\", \"datecreated\", \"lastseen\"]] = df[\n    [\"datecrawled\", \"datecreated\", \"lastseen\"]\n].apply(pd.to_datetime, infer_datetime_format=True)\ndf.dtypes","b368450e":"columns_with_missing_values = [\n    \"vehicletype\",\n    \"gearbox\",\n    \"model\",\n    \"fueltype\",\n    \"notrepaired\",\n]\ndf[columns_with_missing_values] = df[columns_with_missing_values].fillna(\"unknown\")\ndf.isna().sum()","9a829640":"print(\n    f\"Number of duplicated rows {df.duplicated().sum()} and total number of rows {df.shape[0]}.\"\n)\ndf = df.drop_duplicates()\nprint()\nprint(\"=\" * 20, \"After dropping ducplicates\", \"=\" * 20)\nprint(\n    f\"Number of duplicated rows {df.duplicated().sum()} and total number of rows {df.shape[0]}.\"\n)","5d2477a0":"# check descriptive statistics of registrationyear and datecrated columns\ndf.registrationyear.describe(), df.datecreated.dt.year.describe()","01890991":"# select year below 1900 and above the 'datecreated'\ncondition = (df[\"registrationyear\"] > df[\"datecreated\"].dt.year) | (\n    df[\"registrationyear\"] < 1900\n)\ndisplay(len(df[condition]) \/ len(df) * 100)\nprint()\n\n# select data which does not have above condition\ndf = df[~(condition)]\n\n# calculate the age of vehicles\ndf[\"vehicleage\"] = df[\"datecreated\"].dt.year - df[\"registrationyear\"]\n\n# check the result\ndf.head(2)","a3ad710d":"# select all numerical columns of data\nnumerical_cols = [\"price\", \"power\", \"mileage\", \"vehicleage\"]\n\n# print out descriptive statistics of numerical columns\ndf[numerical_cols].describe()","95f37e93":"# histograms of all numerical columns\nfig, ax = plt.subplots(2, 2, figsize=(14, 4))\nsns.distplot(df[\"power\"], kde=False, hist_kws={\"range\": (0, 1500)}, ax=ax[0, 0])\nax[0, 0].set_title(\"Power distribution\")\nsns.distplot(df[\"price\"], kde=False, ax=ax[0, 1])\nsns.distplot(df[\"mileage\"], kde=False, hist_kws={\"range\": (5000, 150000)}, ax=ax[1, 0])\nsns.distplot(df[\"vehicleage\"], kde=False, ax=ax[1, 1])\n\nplt.show()","1f6fc839":"# calculate the outliers in 'power' columns\n(\n    df[\"power\"].quantile(0.75)\n    + (df[\"power\"].quantile(0.75) - df[\"power\"].quantile(0.25)) * 1.5\n)","a6b63513":"# How many rows and what percentage in total has outliers in the 'power' columns\nprint(len(df[df[\"power\"] > 245]), len(df[df[\"power\"] > 500]) \/ len(df) * 100)\n\n# select all rows where the 'power' values is smaller than 245\ndf = df[~(df[\"power\"] > 245)]\n\n# plot the boxplot\ndf[\"power\"].plot(kind=\"box\")\nplt.show()","65865e00":"# descriptive statistics of categorical columns\ncategorical_cols = [\"vehicletype\", \"gearbox\", \"model\", \"fueltype\", \"brand\"]\ndf[categorical_cols].describe()","f8658f91":"# plots showing number of each type of vehicle, gearbox, fueltype and car brand in datasets\nfig, ax = plt.subplots(4, 1, figsize=(14, 8))\nfor i, j in enumerate([\"vehicletype\", \"gearbox\", \"fueltype\", \"brand\"]):\n    sns.countplot(df[j], ax=ax[i])\n    ax[i].set_title(j)\nax[3].tick_params(\n    axis=\"x\",\n    rotation=45,\n)\nplt.tight_layout()\nplt.show()","11087190":"# Select columns which are important for modeling\nselected_cols = [\n    \"price\",\n    \"vehicletype\",\n    \"gearbox\",\n    \"power\",\n    \"model\",\n    \"mileage\",\n    \"fueltype\",\n    \"brand\",\n    \"vehicleage\",\n]\ndf = df[selected_cols]\n\n# what is the shape of new dataframe\ndf.shape","468d72f8":"# define categorical and numerical columns, we will use them throught algorithm trainings\ncategorical_columns = [\"vehicletype\", \"gearbox\", \"model\", \"fueltype\", \"brand\"]\nnumerical_columns = [\"power\", \"mileage\", \"vehicleage\"]","a286a29c":"# for regression one hot encoding works well\ndf_ohe = pd.get_dummies(df, drop_first=True)\ndf_ohe.shape\n\n# get target and feature columns\nX = df_ohe.drop(\"price\", axis=1)\ny = df_ohe[\"price\"]\n\n# split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=234\n)\n\n# scaling is crucial for regression models\nnumerical_columns = [\"power\", \"mileage\", \"vehicleage\"]\nscaler = StandardScaler()\nX_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\nX_test[numerical_columns] = scaler.transform(X_test[numerical_columns])","6812ae4b":"%%time\n#linear regression model, train and predict results\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)","21ed7c42":"%%time\n#predict the target and calculate RMSE and check time for prediction\ny_pred = lr_model.predict(X_test)\n#Scores\nprint(f'The RMSE score of the linear regression model is {(mean_squared_error(y_test, y_pred))**0.5}.')","dbd197c8":"# get categorical column names\ndf_lbe = df.copy()\n\n# construct the encoder\nencoder = OrdinalEncoder()\n\n# encode categorical columns and change the values in the dataframe\nencoded = encoder.fit_transform(df_lbe[categorical_columns])\ndf_lbe[categorical_columns] = encoded\n\n# check new df\ndf_lbe.head(2)","647abaa1":"# get target and feature columns from df_lbe\nX = df_lbe.drop(\"price\", axis=1)\ny = df_lbe[\"price\"]\n\n# split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=234\n)","7bbcc573":"# for loop to check optimal n_estimators\nfor n in [30, 50]:\n    rf_model = RandomForestRegressor(n_estimators=n, n_jobs=-1, random_state=234)\n    mse = cross_val_score(\n        rf_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(\n        f\"The average RMSE score of the RandomForest model with {n} estimators is {avg_rmse}.\"\n    )\n    print()\n\n# for loop to find optimal max_depth values\nfor n in [7, 15]:\n    rf_model = RandomForestRegressor(\n        n_estimators=30, max_depth=n, n_jobs=-1, random_state=234\n    )\n    mse = cross_val_score(\n        rf_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(\n        f\"The average RMSE score of the RandomForest model with max_depth={n} is {avg_rmse}.\"\n    )\n    print()","9d7f5be8":"%%time\n#build a model with optimal parameters and check how much time it requires for training\nbest_rf_model = RandomForestRegressor(n_estimators = 50, max_depth=15, n_jobs=-1, random_state=234)\nbest_rf_model.fit(X_train, y_train)","ef7e6cfe":"%%time\n#now let's check how much time it requires for prediction and calcualte the RMSE\ny_pred = best_rf_model.predict(X_test)\n\nprint(f'The RMSE score of the best RandomForestRegression model is {(mean_squared_error(y_test, y_pred))**0.5}.')","5d04d053":"# for loop to find optimal n_estimators\nfor n in [30, 50]:\n    lg_model = LGBMRegressor(n_estimators=n, random_state=234)\n    mse = cross_val_score(\n        lg_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(f\"The RMSE score of the LightGBM model with {n} estimators is {avg_rmse}.\")\n    print()\n\n# for to find optimal max_depth\nfor n in [7, 15]:\n    lg_model = LGBMRegressor(n_estimators=30, max_depth=n, random_state=234)\n    mse = cross_val_score(\n        lg_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(f\"The RMSE score of the LightGBM model with max_depth={n} is {avg_rmse}.\")\n    print()","58752f83":"%%time\n\n#build a model with optimal parameters and check how much time it requires for training\nbest_lg_model = LGBMRegressor(n_estimators=50, max_depth=15, random_state=234)\nbest_lg_model.fit(X_train, y_train)","e4906fe0":"%%time\n# predict the target and check RMSE score and time it requires for prediction\ny_pred = best_lg_model.predict(X_test)\nprint(f'The RMSE score of the best LightGBM model is {(mean_squared_error(y_test, y_pred))**0.5}.')","8cfe5c2b":"%%time\n#calculate the time required for training\nxg_model = xgb.XGBRegressor()\nxg_model.fit(X_train,y_train)","77327d85":"%%time\n#do prediction and check RMSE score and time it requires\ny_pred = xg_model.predict(X_test)\n\nprint(f'The RMSE score of the XGBoost model with defualt parameters is {(mean_squared_error(y_test, y_pred))**0.5}.')","7aa3bb3b":"# get target and feature columns from df without encoding\nX = df.drop(\"price\", axis=1)\ny = df[\"price\"]\n\n# split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=234\n)\n\n# for loop the check optimal number of iterations\nfor n in [30, 50]:\n    cb_model = CatBoostRegressor(\n        iterations=n,\n        learning_rate=1,\n        random_seed=234,\n        silent=True,\n        cat_features=categorical_columns,\n    )\n    mse = cross_val_score(\n        cb_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(f\"The RMSE score of the Catboost model with {n} iterations is {avg_rmse}.\")\n    print()\n\n# for loop the check optimal number of depth\nfor n in [7, 15]:\n    cb_model = CatBoostRegressor(\n        iterations=30,\n        learning_rate=1,\n        depth=n,\n        random_seed=234,\n        silent=True,\n        cat_features=categorical_columns,\n    )\n    mse = cross_val_score(\n        cb_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3\n    )\n    rmse = np.sqrt(mse * -1)\n    avg_rmse = np.sum(rmse) \/ len(rmse)\n    print(f\"The RMSE score of the Catboost model with depth={n} is {avg_rmse}.\")\n    print()","4bd05980":"%%time\n\n#build the model with optimal parameters and check how much time it requires for training\nbest_cb_model = CatBoostRegressor(iterations=50, learning_rate=1, depth=15, random_seed=234, silent=True)\nbest_cb_model.fit(X_train, y_train, cat_features=categorical_columns)","11f20c4e":"%%time\n#predict the target and check RMSE score and time\ny_pred = best_cb_model.predict(X_test)\n\nprint(f'The RMSE score of the best Catboost model is {(mean_squared_error(y_test, y_pred))**0.5}.')","6d4bcfc1":"# performace of best models\nmodels = [\"LinearReg\", \"RandForest\", \"LightGBM\", \"XGboost\", \"CatBoost\"]\nrmse_score = [2766.05, 1754.42, 1888.53, 1990.81, 1814.9]\nfit_time = [24.1, 23.4, 1.65, 13.9, 88]\npred_time = [0.22, 1.07, 0.45, 0.39, 0.313]\n# ceate a dataframe\ncomparison = pd.DataFrame(\n    data=zip(models, rmse_score, fit_time, pred_time),\n    columns=[\"model\", \"rmse_score\", \"fit_time_sec\", \"pred_time_sec\"],\n)\n\n# plot performance metrics\nfig, ax = plt.subplots(1, 3, figsize=(12, 6))\nsns.set_style(\"whitegrid\")\n\n\nsns.barplot(\n    data=comparison.sort_values(\"rmse_score\"),\n    y=\"model\",\n    x=\"rmse_score\",\n    orient=\"h\",\n    ax=ax[0],\n)\nax[0].set_title(\"RMSE score comparison of various models\")\nsns.barplot(\n    data=comparison.sort_values(\"rmse_score\"),\n    y=\"model\",\n    x=\"fit_time_sec\",\n    orient=\"h\",\n    ax=ax[1],\n)\nax[1].set_title(\"Time required to train the model\")\nsns.barplot(\n    data=comparison.sort_values(\"rmse_score\"),\n    y=\"model\",\n    x=\"pred_time_sec\",\n    orient=\"h\",\n    ax=ax[2],\n)\nax[2].set_title(\"Time required to predict the target\")\n\n\nplt.tight_layout()\nplt.show()","35213836":"### 3.6 Conclusion\n\nSome columns, such as `NumberOfPictures` which contains all zeros and other datetime columns, will not add much value to our regression models. Therefore, we have selected most important columns as following: `price`, `vehicletype`, `gearbox`, `power`, `model`, `mileage`, `fueltype`, `brand`, and `vehicleage`. New data for training and testing models has 331474 rows and 9 columns (just for comparison, initial data were containing: **354369 rows and 16 columns**. Duplicates, errors and unnecessary data were dropped.)\n\nFollowing algorithms were trained and test:\n\n|Algorithm            |Deal with categoricals|Scaling        |Tuned Hyperparamteres  |\n|---------------------|----------------------|---------------|-----------------------|\n|LinearRegression     |One hot encoding      |StandardScaler |None                   |\n|RandomForestRegressor|Label encoding        |None           |n_estimators, max_depth|\n|LightGBMRegressor    |Label encoding        |None           |n_estimators, max_depth|\n|XGBoostRegressor     |Label encoding        |None           |None                   |\n|CatBoostRegressor    |it has its own        |None           |iterations, depth      |","8c027685":"> **Issues identified so far:** \n- Columns names should be in lowercase;\n- data types of several columns;\n- 5 columns have missing values and `power` col has zeros, should be dealed;\n- 262 duplicated values for now (after filling missing values these may increase)","e6d30107":"#### 1.2.5 Create an age column","7778a045":"> It seems like we still have error in our data. E.g. a car can not have horsepower as high as 20000. Little google search resulted that the car with highest horsepower untill now is 1500 https:\/\/en.wikipedia.org\/wiki\/List_of_production_cars_by_power_output","4a57ab44":"### 4.1 Conclusion\nOverall, from the perspective of RMSE score, all models performed well compared to the simple **LinearRegression, which estimates the price of the car with $\\pm$ 2766 EUR.** However, RMSE scores of **RandomForest were the lowest ($\\pm$ 1754 EUR)**, but LightGBM and CatBoost also performed comparably to RandomForest. \n\n\nOn other hand, the times for training the model and predicting the target were significantly different. For example, RandomForest, which had the highest RMSE score, required 23 seconds for training and as high as 1.1 seconds for prediction, while\nthe simple **LinearRegression model required 24 seconds for training and only 0.22 seconds for prediction. \n\nIn our opinion, prediction time is more important than training time. Because a model is once trained and deployed. Then customers frequently use this model to predict the price of their cars. And this be should be quick due to prevent long waiting times. Therefore, models with an adequate RMSE score and the lowest prediction time are more promising for implementation.\n\n**In summary, we can say that CatBoost with a relatively good accurate prediction ($\\pm$ 1814 EUR) and only 0.31 seconds of prediction time looks more promising for implementation for this particular task.**","38b9ef54":"> we will use OrindalEncoding class of sklearn for encoding categorical columns, it works better than OHE for tree-based algorithms. Data prepared in this step will be used for RandomForest, LightGBM and XGBoost. For Catboost we will prepare separate data, because it can handle categorical data.","df9d54a6":"#### 1.2.3 Missing values","24cfe6eb":"#### 1.2.1 Columns names","74a30f80":"# Project Description: Optimal algorithm for price prediction\n<p align=\"center\">\n<img src=\"https:\/\/user-images.githubusercontent.com\/56832126\/126864743-1f94efa9-3cbe-4227-a2a9-89952b0f29dd.png\" width=\"500px\">\n<\/p>\n\nIn this project, we will compare several algorithms in order to select optimal algorithm to determine used car price.\nRusty Bargain, a used car sales service, is developing an app to attract new customers. In that app, one can quickly find out the market value of his\/her car. We have access to historical data: technical specifications, trim versions, and prices. Now, we need to build the model to determine the value. \n\n**Rusty Bargain is interested in:**\n- the quality of the prediction;\n- the time required for training\n\nBased on above, we will consider different algorithms and tune their hyperparameters to find out optimal model, with highest accuracy with quick prediction.\n\n## Table of Contents:\n- **1. data loading and checking**\n- **2. EDA**\n- **3. Training differnt models**\n- **4. Benchmarking: quality and speed**","14f36391":"### 3.5 CatboostRegressor","490cee65":"### 1.2 Preprocessing","489802aa":"## 3. Model training\nIn this section we will train different model, and tune some hyperparameters, if possible of course. Following algorithms we are going to use:\n - LinearRegression\n - RandomForestRegressor (with different n_estimators and max_depth)\n - CatBoostRegressor (with different number of iterations and depth)\n - LightBGMRegressor (with different n_estimaros and max_depth)\n - XGBoostRegressor (with default parameters)\n \nHowever, before that, we should mention that we don't need all columns of the data. Because some of the columns will not add value to our prediction. Therefore we will select the most important columns.","b789509e":"### 3.1 LinearRegression","5523374e":"### 1.1 Data loading","1f3749ad":"### 2.1 Numerical columns","e92f292f":"### 2.3 Conclusion\nDuring EDA, it was observed that there was an error in the `power` column, which were containing horsepower values as high as 20000. All values above the Q3+IQR*1.5 were dropped (it was only 0.12% of the data). \n\nIt was observed that ** the most sold:\n- brand: Volkswagen\n- model: golf\n- vehicle type: sedan \n- gearbox: manual (more than two-thirds of all)\n- fuel type: petrol (about two-thirds of all)","7d14973e":"### 3.3 LightGBMRegressor","7cce9349":"### 2.2 Categorical columns","432a5f22":"When n_estimators are too large, model's RMSE scores is slightly improving but using such a high n_estimators really slows down the computation. For the sake of speed, we will chose n_estimators of 50 and max_depth of 20.\nLet's find how much time required for the optimal model with RandomForestRegressor","f9c995d1":"#### 1.2.2 Column dtypes","45d3f82c":"## 5. Summary\n\n- Data, with **354369 rows and 16 columns**, has been imported and checked, following observations\/changes were made:\n    - `vehicletype`, `gearbox`, `model`, `fueltype`, and `notrepaired` columns were containing missing values. These all columns hold categorical data types. Therefore, all missing values were filled as \"unknown\";\n    - In total 262 rows were duplicated in original data. It was found that they were identical duplicates. They were all dropped;\n    - A new column which contains the age of the vehicle, `vehicleage`, was created using `datecreated` and `registrationyear` columns. However, in this step, it was observed that data was containing error in `registrationyear` columns, there were years as low as 1000 and as high as 9999. All years smaller than 1900 (it is the year when cars became commercial) and higher than the profile date creation were dropped. Dropped amount of rows were around 4% of the data.\n\n\n- During EDA, it was observed that there was an error in the `power` column, which were containing horsepower values as high as 20000. All values above the Q3+IQR*1.5 were dropped (it was only 0.12% of the data).\n\n- It was observed that ** the most sold:\n    - brand: Volkswagen\n    - model: golf\n    - vehicle type: sedan \n    - gearbox: manual (more than two-thirds of all)\n    - fuel type: petrol (about two-thirds of all)\n\nAll mentioned errors might be originated due to technical reasons, human error (typo), or during web crawling, dataset joining, etc.\n\n- We have selected most important columns as following: `price`, `vehicletype`, `gearbox`, `power`, `model`, `mileage`, `fueltype`, `brand`, and `vehicleage`. New data for training and testing models has 331474 rows and 9 columns (just for comparison, initial data were containing: **354369 rows and 16 columns**. Duplicates, errors and unnecessary data were dropped.)\n\n- Following algorithms were trained and test:\n\n    |Algorithm            |Deal with categoricals|Scaling        |Tuned Hyperparamteres  |\n    |---------------------|----------------------|---------------|-----------------------|\n    |LinearRegression     |One hot encoding      |StandardScaler |None                   |\n    |RandomForestRegressor|Label encoding        |None           |n_estimators, max_depth|\n    |LightGBMRegressor    |Label encoding        |None           |n_estimators, max_depth|\n    |XGBoostRegressor     |Label encoding        |None           |None                   |\n    |CatBoostRegressor    |it has its own        |None           |iterations, depth      |\n\n\n\n- Overall, from the perspective of RMSE score, all models performed well compared to the simple **LinearRegression, which estimates the price of the car with $\\pm$ 2766 EUR.** However, RMSE scores of **RandomForest were the lowest ($\\pm$ 1754 EUR)**, but LightGBM and CatBoost also performed comparably to RandomForest. On other hand, the times for training the model and predicting the target were significantly different. For example, RandomForest, which had the highest RMSE score, required 23 seconds for training and as high as 1.1 seconds for prediction, while the simple **LinearRegression model required 24 seconds for training and only 0.22 seconds for prediction. \n\n\nIn our opinion, prediction time is more important than training time. Because a model is once trained and deployed. Then customers frequently use this model to predict the price of their cars. And this be should be quick due to prevent long waiting times. Therefore, models with an adequate RMSE score and the lowest prediction time are more promising for implementation.\n\n**In summary, we can say that CatBoost with a relatively good accurate prediction ($\\pm$ 1814 EUR) and only 0.31 seconds of prediction time looks more promising for implementation for this particular task.**","ee2c8e08":"## 1. Data preparation","e23919ef":"### 3.2 RandomForestRegressor","e902c3ad":"## 2. EDA\n\nLet's get familiar with the data more by visualizing their data by plotting some charts.","fac1d1bb":"## 4. Performance comparison of models\n\nHere we will compare the RMSE score and how much time was required to get an optimal score with the above models.","7f5980b2":"### 3.4 XGBoostRegressor","2863864b":"#### 1.2.4 Duplicates","4a630d33":"### 1.3 Conclusion\n\n- Data, with **354369 rows and 16 columns**, has been imported and checked, following observations\/changes were made:\n    - Columns names of data were in mixed case, all were converted to lowercase;\n    - `datecrawled`, `datecreated`, and `lastseen` columns were containing date but loaded as object data type. They were converted to the datetime data type of pandas;\n    - `vehicletype`, `gearbox`, `model`, `fueltype`, and `notrepaired` columns were containing missing values. These all columns hold categorical data types. Therefore, all missing values were filled as \"unknown\";\n    - In total 262 rows were duplicated in original data. It was found that they were identical duplicates. They were all dropped;\n    - A new column which contains the age of the vehicle, `vehicleage`, was created using `datecreated` and `registrationyear` columns. However, in this step, it was observed that data was containing error in `registrationyear` columns, there were years as low as 1000 and as high as 9999. All years smaller than 1900 (it is the year when cars became commercial) and higher than the profile date creation were dropped. Dropped amount of rows were around 4% of the data.\n\n\nAll mentioned errors might be originated due to technical reasons, human error (typo), or during web crawling, dataset joining, etc."}}