{"cell_type":{"4bbd3b48":"code","6e8950e9":"code","570d54ab":"code","841d8a53":"code","02617a7c":"code","56722402":"code","c2c81d49":"code","582ce4c1":"code","c9363ba6":"code","869a6a6f":"code","46cdcfbd":"code","cccbd913":"code","ae38b2b9":"code","71e48ca7":"code","0cd31fbe":"code","febbe283":"code","5bf7223b":"code","ff688e15":"code","f4b48455":"code","78077c74":"code","7d000cff":"code","882f1a75":"code","18b649cc":"code","fde14f8a":"code","20fa0e2c":"code","563cbce0":"code","903c3d4c":"code","b9b00af5":"code","afa89b99":"code","fe8ba6bc":"code","c20ac11f":"code","a3b40c86":"code","b6c6a43e":"code","7623a053":"code","5ba3f7dc":"code","fb674f51":"code","b8a56c53":"code","84501f51":"code","65f197d2":"code","c863bde3":"code","e236ea39":"code","f5858c13":"code","a00f27ba":"code","fc0d4812":"code","234e7c1a":"code","8d4b9228":"code","d9790340":"code","eb846283":"markdown","2ea83bd0":"markdown","ebaf3eea":"markdown","96a8ee98":"markdown","e3bf9b26":"markdown","b5f600e9":"markdown","d6d618ba":"markdown","f767b8e7":"markdown","1771fdf8":"markdown","1a0306b3":"markdown","0b3ebca4":"markdown","aa23529c":"markdown","697ce3f9":"markdown","054fb32a":"markdown","08a321d9":"markdown","0dd47ce3":"markdown","d1b8a02d":"markdown","dc7b88fe":"markdown","743837f9":"markdown","155bef52":"markdown","09d5b763":"markdown","85e61f08":"markdown","7335afe1":"markdown","6b2414e4":"markdown","c4a8a134":"markdown"},"source":{"4bbd3b48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Acest mediu Python 3 vine cu multe biblioteci si utilitare pentru analiza deja instalate\n# Este definit intr-o imagine Docker specifica kaggle \/ python: https:\/\/github.com\/kaggle\/docker-python\n# De exemplu, iat\u0103 c\u00e2teva pachete cu utilitare deja  \u00eenc\u0103rcate\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Fi\u0219ierele cu date de intrare sunt disponibile \u00een directorul \u201e..\/input\/\u201d dar acesta suporta numai operatini de citire\n# De exemplu, rularea acestuia (f\u0103c\u00e2nd clic pe Executare sau ap\u0103s\u00e2nd Shift + Enter) va afi\u0219a toate fi\u0219ierele din directorul de intrare\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n# Pute\u021bi scrie p\u00e2n\u0103 la 5 GB \u00een directorul curent (\/ kaggle \/ working \/) care se p\u0103streaz\u0103 ca ie\u0219ire atunci c\u00e2nd crea\u021bi o versiune a programului folosind optiunea \u201eSalva\u021bi \u0219i executa\u021bi tot\u201d\n# De asemenea, pute\u021bi scrie fi\u0219iere temporare \u00een \/ kaggle \/ temp \/, dar nu vor fi salvate \u00een afara sesiunii curente","6e8950e9":"import tensorflow as tf\nimport os\nimport time","570d54ab":"dataframe =pd.read_csv(\"\/kaggle\/input\/romanian-wikisource-literature-corpus\/CorpusRomanaWikisource900.csv\",encoding='utf-8')","841d8a53":"dataframe.head()","02617a7c":"dataframe.tail()","56722402":"eminescucorpus=dataframe[(dataframe.Autor==\"de Mihai Eminescu\")&(dataframe.GenWiki==\"Liric\")]\n\neminescucorpus.head()","c2c81d49":"eminescucorpus.tail()","582ce4c1":"eminescucorpus.to_csv(\"\/kaggle\/working\/texte1.txt\", header=None, index=None, sep=' ', mode='a')","c9363ba6":"text = open(\"\/kaggle\/working\/texte1.txt\", 'rb').read().decode(encoding='utf-8')","869a6a6f":"print('Length of text: {} characters'.format(len(text)))","46cdcfbd":"print(text[:250])","cccbd913":"# The unique characters in the file\nvocab = sorted(set(text))\nprint('{} unique characters'.format(len(vocab)))","ae38b2b9":"vocab","71e48ca7":"import re","0cd31fbe":"result=text\nlistacaractereatipice=['\\xa0','\u00ab', '\\xad', '\u00bb', '\u00e8', '\u00e9', '\u00ec', '\u00f1', '\u00f2', '\u00f6', '\u00fc', '\u0102', '\u010d', '\u010f', '\u0115', '\u011b', '\u0144', '\u016f', '\u1e77', '\u1ecb', '\\u200a', '\u2013', '\u2014', '\u2019', '\u201c', '\u201d', '\u201e']\nlistareplacement=[' ','\"', ' ', ' ', 'e', 'e', 'i', 'n', 'o', 'o', 'u', 'A', 'c', 'd', 'e', 'e', 'e', 'u', 'u', 'i', ' ', '-', '-', '\u2019', '\"', '\"', '\"']\nres = dict(zip(listacaractereatipice, listareplacement)) \nres\nfor key in res:\n    result = re.sub(key, res[key], result)\n    #print(key,res[key])       ","febbe283":"# The unique characters in the file\nvocab = sorted(set(result))\nprint('{} unique characters'.format(len(vocab)))","5bf7223b":"text=result","ff688e15":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","f4b48455":"print('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","78077c74":"# Show how the first 13 characters from the text are mapped to integers\nprint('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","7d000cff":"# The maximum length sentence you want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])","882f1a75":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))","18b649cc":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","fde14f8a":"for input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))","20fa0e2c":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","563cbce0":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","903c3d4c":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","b9b00af5":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.GRU(rnn_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model","afa89b99":"model = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)","fe8ba6bc":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","c20ac11f":"model.summary()","a3b40c86":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","b6c6a43e":"sampled_indices","7623a053":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","5ba3f7dc":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","fb674f51":"model.compile(optimizer='adam', loss=loss)","b8a56c53":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","84501f51":"EPOCHS = 50","65f197d2":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","c863bde3":"tf.train.latest_checkpoint(checkpoint_dir)","e236ea39":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","f5858c13":"model.summary()","a00f27ba":"def generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","fc0d4812":"print(generate_text(model, start_string=u\"Luceafarul: \"))","234e7c1a":"print(generate_text(model, start_string=u\"pasarele: \"))","8d4b9228":"print(generate_text(model, start_string=u\"Transformare Digitala: \"))","d9790340":"print(generate_text(model, start_string=u\"Sabin: \"))","eb846283":"Decode these to see the text predicted by this untrained model:","2ea83bd0":"https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation","ebaf3eea":"Verificam lizibilitatea textului","96a8ee98":"Try the model","e3bf9b26":"Sarcina de predic\u021bie\nAv\u00e2nd \u00een vedere o litera sau o secven\u021b\u0103 de caractere, care este cel mai probabil caracter urm\u0103tor? Aceasta este sarcina pentru care o instrui\u021bi modelul. \nModelul  la intrare va fi o secven\u021b\u0103 de caractere \u0219i se va instrui modelul pentru a prezice  - urm\u0103torul caracter la fiecare pas de timp.\n\n","b5f600e9":"Procesare text\nVectorizare\n\u00cenainte de antrenament, trebuie s\u0103 mapa\u021bi \u0219irurile cu o reprezentare numeric\u0103. Crea\u021bi dou\u0103 tabele de c\u0103utare: una care mapeaz\u0103 caractere cu numere \u0219i alta pentru numere cu caractere.","d6d618ba":"Pas optional:\nSe elimina caracterele atipice pentru limba romana.","f767b8e7":"Pentru instruire avem nevoie de un corpus care sa aiba macar un milion de caractere. Mai jos extragerea textelor cu poezi din corpusul salvat de pe wikipedia. ","1771fdf8":"Verificam daca setul de instruire este suficient de consistent pentru a putea fi utilizat:  915061 characters\n","1a0306b3":"**Text generation with an RNN**\nfrom:https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation\nImport TensorFlow and other libraries\n\nAcest tutorial arat\u0103 cum s\u0103 genera\u021bi text utiliz\u00e2nd o retea de tip RNN folosind caractere. Ve\u021bi lucra cu un set de date text din wikisource si pe baza articolului Andreea Karpathy, \u201eThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d. \nAv\u00e2nd \u00een vedere o secven\u021b\u0103 de caractere din aceste date (\u201eeminesc\u201d), instrui\u021bi un model pentru a prezice urm\u0103torul caracter din secven\u021b\u0103 (\u201eu\u201d). Secven\u021be mai lungi de text pot fi generate apel\u00e2nd modelul \u00een mod repetat.\n\n\u00cen timp ce unele dintre propozi\u021bii par corecte gramatical, majoritatea nu au sens. Modelul nu a \u00eenv\u0103\u021bat semnifica\u021bia cuvintelor, dar ia \u00een considerare urmatoarele:\n\n1. Modelul este bazat pe caractere. C\u00e2nd a \u00eenceput instruirea, modelul nu \u0219tia cum s\u0103 scrie un cuv\u00e2nt \u00een limba rom\u00e2n\u0103 \u0219i nici nu realiza c\u0103 de fapt cuvintele erau chiar o unitate de text.\n2. Initial modelul a fost construit pentru piese de teatru.Un personaj cu majuscule urmat de text. Deoarece nu exista un corpus cu piese de teatru suficient de mare disponibil pen source, in acest moment acest scenariu nu este aplicabil. \n3. Dup\u0103 cum se arat\u0103 mai jos, modelul este instruit pe loturi mici de text (c\u00e2te 100 de caractere fiecare) \u0219i este \u00eenc\u0103 capabil s\u0103 genereze o secven\u021b\u0103 mai lung\u0103 de text cu o structur\u0103 coerent\u0103.\n","0b3ebca4":"Verificam Numarul de caractere unice\nin limba engleza erau 65 unique characters\nIn limba romana sunt mai multe datorita diacriticelor. Dar mai exista probabil in corpus si citate din franceza si germana sau semne de punctuatie neuzuale. ","aa23529c":"Acum ave\u021bi o reprezentare de tip integer pentru fiecare caracter. Observa\u021bi c\u0103 a\u021bi asociat caracterul ca indici de la 0 la len (unique).","697ce3f9":"Generate text\nEminescu sintetic :)","054fb32a":"**Read dataframe**\n\nPentru textele in limba romana a fost creat un corpus cu texte preluate de pe wikisource. \nMa jos sunt pasii pentru descarcarea si utilizarea acestuia. ","08a321d9":"Se suprascrie textul","0dd47ce3":"Exista o limitarea tensorflow in sensul ca nu exista documentat un mod de lucru din tabele (dataframe) asa ca pentru simplitate se saveaza fisierul text pe disc pentru a putea fi citit ulterior. Atentie, daca exista mai multe incercari sau erori in pasi de mai sus: aceste a se vor salva in fisierul text. ","d1b8a02d":"Metoda batch ne permite s\u0103 convertim cu u\u0219urin\u021b\u0103 aceste caractere individuale \u00een secven\u021be de dimensiunea dorit\u0103.","dc7b88fe":"Construire Model","743837f9":"# Text generation with an RNN\nRomanian version of the tensor flow tutorial from:https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation\n* For english instructions follow the link. The notebook is a demo for romanian kids. \n* Tutorial adaptat pentru limba romana. Sper ca elevii de liceu sa se distreze.","155bef52":"exemplu cu primele valori","09d5b763":"Instruire Model","85e61f08":"Pentru fiecare secven\u021b\u0103, copia\u021bi-o \u0219i deplasa\u021bi-o pentru a forma textul de intrare \u0219i \u021bint\u0103 utiliz\u00e2nd metoda h\u0103r\u021bii pentru a aplica o func\u021bie simpl\u0103 fiec\u0103rui lot:","7335afe1":"Este util sa verificam daca fisierele au fost incarcate corect.(atentie la diacritice si encoding -eeste recomandat ca orice intrare si iesire pentru limba romana sa fie realizata utilizand setul de caractere UTF-8","6b2414e4":"Pentru ca sunt erori rezultate la scanare putem preprocesa textul eliminand caractere eronate (unele sunt caractere tipografice ca de exemplu *)","c4a8a134":"Configure checkpoints"}}