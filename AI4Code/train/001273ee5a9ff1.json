{"cell_type":{"f0c4955d":"code","41dbdfe7":"code","518abbfe":"code","1b984d45":"code","63c9330d":"code","ff070490":"code","edb755a4":"code","9fd78a13":"code","db288038":"code","cea1c7e0":"code","93334e6b":"code","1b248601":"code","8defde92":"code","94a755df":"code","0ac840a8":"code","421135d5":"code","fa9fc452":"code","c68a48b1":"code","d14eaf4e":"code","626c01e9":"code","9358d577":"code","00f3b509":"code","a2a227e1":"code","d143f15e":"code","6b4afdea":"code","fd14c3fc":"code","63625bd8":"code","42da265e":"code","aeba487e":"code","ff26f96b":"code","4104ef6e":"code","205f5f87":"code","75ca0855":"code","b585e922":"markdown","492f50cd":"markdown","7245c2bb":"markdown","f9ae3871":"markdown","799651ec":"markdown"},"source":{"f0c4955d":"# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","41dbdfe7":"df.describe()","518abbfe":"%time\nprint('No fraud',round(df['Class'].value_counts()[0]\/len(df['Class'])*100,2),'% of dataset' )\nprint('Fraud', round(df['Class'].value_counts()[1]\/len(df)*100,2),'% of dataset')","1b984d45":"sns.countplot(df.Class)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","63c9330d":"fig, ax = plt.subplots(1,2, figsize=(18,4))\nsns.distplot(df['Amount'], ax = ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(df.Amount), max(df.Amount)])\n\nsns.distplot(df['Time'], ax = ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(df.Time), max(df.Time)])\nplt.show()","ff070490":"from sklearn.preprocessing import RobustScaler, StandardScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['Scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Amount','Time'], axis=1, inplace=True)","edb755a4":"df.head()","9fd78a13":"scaled_amount = df.Scaled_amount\nscaled_time = df.Scaled_time\n\ndf.drop(['Scaled_amount','Scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\ndf.head()","db288038":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\nX = df.drop(['Class'], axis=1)\ny = df.Class\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\nfor train_index, test_index in sss.split(X,y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n    \n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","cea1c7e0":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\nfraud_df = df.loc[df['Class']==1]\nnon_fraud_df =df.loc[df['Class']==0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n","93334e6b":"sns.countplot(new_df.Class)","1b248601":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","8defde92":"# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\ncolumns=['V17','V14','V12','V10']\ni,axis=plt.subplots(ncols=4, figsize=(20,4))\nfor i, col in enumerate(new_df[columns]):\n    plot=sns.boxplot(x=new_df.Class, y=new_df[col], ax=axis[i])\n    axis[i].set_title(str(col+' vs Class Negative Correlation'))\nplt.show()\n","94a755df":"i, axes = plt.subplots(ncols=4, figsize=(20,4))\ncolumns = ['V11','V4','V2','V19']\nfor i,col in enumerate(new_df[columns]):\n    plot=sns.boxplot(x=new_df.Class, y=new_df[col], ax=axes[i])\n    axes[i].set_title(str(col+' vs Class Positive Correlation'))\nplt.show()","0ac840a8":"from scipy.stats import norm\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\ncolumns = ['V14','V12','V10','V19']\nfor i,col in enumerate(new_df[columns]):\n        plot=sns.distplot(new_df[col].loc[new_df.Class==1].values,fit=norm, ax=axes[i])\n        axes[i].set_title(str(col+' Distribution \\n (Fraud Tranaction)'))\nplt.show()","421135d5":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","fa9fc452":"i, axes = plt.subplots(ncols=3, figsize=(20,6))\ncolumns = ['V14','V12','V10']\nfor i,col in enumerate(new_df[columns]):\n    plot=sns.boxplot(x=new_df.Class, y=new_df[col], ax=axes[i])\n    axes[i].set_title(str(col+' Feature \\nReduction of outliers'))\n    axes[i].annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n                     arrowprops=dict(facecolor='black'),fontsize=14)\nplt.show()","c68a48b1":"from sklearn.manifold import t_sne\nfrom sklearn.decomposition import PCA, truncated_svd\n\nX = new_df.drop('Class', axis=1)\ny = new_df.Class\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_PCA = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint('PCA took {:.2} s'.format(t1-t0))\n\n#truncated_svd implementation\nt0 = time.time()\nX_reduced_trucatedSVD = TruncatedSVD(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"TruncatedSVD took {:.2} s\".format(t1 - t0))","d14eaf4e":"f, ax = plt.subplots(1, 3, figsize=(24,6))\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\nmodels = [X_reduced_tsne,X_reduced_PCA, X_reduced_trucatedSVD]\n\nfor i, model in enumerate(models):\n    ax[i].scatter(model[:,0], model[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n    ax[i].scatter(model[:,0], model[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n    ax[i].set_title('Models', fontsize=14)\n\n    ax[i].grid(True)\n\n    ax[i].legend(handles=[blue_patch, red_patch])","626c01e9":"# Undersampling before cross validating (prone to overfit)\nX=new_df.drop('Class', axis=1)\ny=new_df.Class","9358d577":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","00f3b509":"# Turn the values into an array for feeding the classification algorithms.\nX_train=X_train.values\nX_test=X_test.values\ny_train=y_train.values\ny_test=y_test.values","a2a227e1":"# Let's implement simple classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifiers={\n    \"LogisticRegression\":LogisticRegression(),\n    \"Support Vector classifier\":SVC(),\n    \"kNeighbor\":KNeighborsClassifier(),\n    \"DecisionTreeClassifier\":DecisionTreeClassifier()\n}","d143f15e":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train,y_train)\n    training_score=cross_val_score(classifier,X_train,y_train)\n    print(\"Classifiers: \",classifier.__class__.__name__,\"has a training score of \", \n          round(training_score.mean(),2)*100,\" % of accuracy\")","6b4afdea":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n#LogisticRegression\nlog_reg_param={\"penalty\":['l1', 'l2'],'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg=GridSearchCV(LogisticRegression(), log_reg_param)\ngrid_log_reg.fit(X_train,y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n# KNears best Classifier\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","fd14c3fc":"# Overfitting Case\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","63625bd8":"# We will undersample during cross validating\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom imblearn.under_sampling import NearMiss\nfrom collections import Counter\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","42da265e":"# Let's Plot LogisticRegression Learning Curve\n\nfrom sklearn.model_selection import ShuffleSplit, learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                       n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        #first estimator\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator1,X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n        )\n        train_scores_mean=np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        \n        ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    return plt","aeba487e":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01),\n                    cv=cv, n_jobs=4)","ff26f96b":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","4104ef6e":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","205f5f87":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()\n","75ca0855":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","b585e922":"<h2> Outline: <\/h2>\nI. <b>Understanding our data<\/b><br>\na) [Gather Sense of our data](#gather)<br><br>\n\nII. <b>Preprocessing<\/b><br>\na) [Scaling and Distributing](#distributing)<br>\nb) [Splitting the Data](#splitting)<br><br>\n\nIII. <b>Random UnderSampling and Oversampling<\/b><br>\na) [Distributing and Correlating](#correlating)<br>\nb) [Anomaly Detection](#anomaly)<br>\nc) [Dimensionality Reduction and Clustering (t-SNE)](#clustering)<br>\nd) [Classifiers](#classifiers)<br>\ne) [A Deeper Look into Logistic Regression](#logistic)<br>\n\n\nStill working on this kernel.............................\n\n\n","492f50cd":"## Gather Sense of Our Data:\n<a id=\"gather\"><\/a>\nThe first thing we must do is gather a <b> basic sense <\/b> of our data. Remember, except for the <b>transaction<\/b> and <b>amount<\/b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already. ","7245c2bb":"### Classifiers (UnderSampling):\nIn this section we will train four types of classifiers and decide which classifier will be more effective in detecting fraud transactions. Before we have to split our data into training and testing sets and separate the features from the labels.","f9ae3871":"### Dimensionality Reduction and Clustering:\n**Understanding t-SNE:**\nIn order to understand this algorithm you have to understand the following terms: \n* Euclidean Distance\n* Conditional Probability\n* Normal and T-Distribution Plots\n\n**Note: If you want a simple instructive video look at StatQuest: t-SNE, Clearly Explained by Joshua Starmer\n**\n**Summary:**\n* t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset.\n* Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)\n* This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases.","799651ec":"### Random Under-Sampling:"}}