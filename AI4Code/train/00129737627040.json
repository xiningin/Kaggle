{"cell_type":{"e752af75":"code","eea5ae7c":"code","69dc800d":"code","c8e4c1d0":"code","d9213fad":"code","18486e6d":"code","22dfc7b8":"code","f0a51267":"code","61240d1b":"code","3bff0db2":"code","544f26d1":"code","b9b5be3e":"code","27f6acd4":"code","dfa8c943":"code","84b3b658":"code","a1ecbc18":"code","d19506b2":"code","2eee3271":"code","879425c1":"code","042d6485":"code","249cf501":"code","481dbf6b":"code","ffbc0793":"code","be4d9214":"code","4bb043e7":"code","d18bf1d6":"code","f15a7bc8":"code","8a04d3df":"code","08a1bc6d":"code","82057239":"code","0f4d35e7":"code","e70790dc":"code","86bb50fc":"code","196e1f83":"code","2d66a852":"code","08caa366":"markdown","500630ec":"markdown","1b841d96":"markdown","6ee7c2ed":"markdown","aee9351d":"markdown","71daf36a":"markdown","bf5be8d2":"markdown","4ddca425":"markdown","b0f5c226":"markdown","4e917e5d":"markdown","6b2e5eba":"markdown","3f8ef200":"markdown","de1a1593":"markdown","305409f7":"markdown","ec37c799":"markdown","46657d5e":"markdown","97faf3eb":"markdown","4198c31b":"markdown","1211fb9c":"markdown","32714b7d":"markdown","1eeaedbe":"markdown","ea96bd20":"markdown","6be2004f":"markdown","d52fedd2":"markdown","09790b88":"markdown","1a050c8e":"markdown","efa44469":"markdown","31e4431d":"markdown","6b3de1af":"markdown","d51c71e8":"markdown","b12daa6f":"markdown","ec9c4f1f":"markdown"},"source":{"e752af75":"# !pip install -q -U torch torchvision -f https:\/\/download.pytorch.org\/whl\/torch_stable.html \n# !pip install -q -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n# !pip install -q detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cpu\/index.html","eea5ae7c":"# Helper function, used these for debugging purposes\n# detector2 build only succeeds if CUDA version is correct\n\n#!nvidia-smi\n#!nvcc --version\n\n#import torch\n#torch.__version__\n#import torchvision\n#torchvision.__version__\n\n!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html\n!pip install fastai","69dc800d":"# Base setup:\n# detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode","c8e4c1d0":"# path = Path('\/kaggle\/input\/dsta-brainhack-2021\/c1_release\/c1_release');path.ls()\n# imgs, lbl_bbox = get_annotations(path\/'train.json')\n\n\n!wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\nim = cv2.imread(\".\/input.jpg\")\n\nplt.figure(figsize=(15,7.5))\nplt.imshow(im[..., ::-1]) #bgr to rgb","d9213fad":"cfg = get_cfg()\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\noutputs = predictor(im[..., ::-1])","18486e6d":"print(outputs[\"instances\"].pred_classes)\n# print(outputs[\"instances\"].pred_boxes)","22dfc7b8":"MetadataCatalog.get(cfg.DATASETS.TRAIN[0])","f0a51267":"import pandas as pd\nmodelclasses = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes\ndf = pd.DataFrame(modelclasses,columns=['Model classes'])\ndf","61240d1b":"v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.figure(figsize=(20,10))\nplt.imshow(out.get_image()[..., ::-1][..., ::-1])","3bff0db2":"# !wget https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.1\/balloon_dataset.zip\n# !unzip balloon_dataset.zip > \/dev\/null\n","544f26d1":"training_path = \"\/kaggle\/input\/dsta-brainhack-2021\/c1_release\/c1_release\"\ntrain_annotation = os.path.join(training_path, \"train.json\")\nval_annotation = os.path.join(training_path, \"val.json\")\nimage_path = os.path.join(training_path,\"images\")\n\n\n\nfrom detectron2.structures import BoxMode\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"train_data\", {}, train_annotation, image_path)\nregister_coco_instances(\"val_data\", {}, val_annotation, image_path)\n\n\n\n# def get_balloon_dicts(img_dir):\n#     json_file = os.path.join(img_dir, \"via_region_data.json\")\n#     with open(json_file) as f:\n#         imgs_anns = json.load(f)\n\n#     dataset_dicts = []\n#     for idx, v in enumerate(imgs_anns.values()):\n#         record = {}\n        \n#         filename = os.path.join(img_dir, v[\"filename\"])\n#         height, width = cv2.imread(filename).shape[:2]\n        \n#         record[\"file_name\"] = filename\n#         record[\"image_id\"] = idx\n#         record[\"height\"] = height\n#         record[\"width\"] = width\n      \n#         annos = v[\"regions\"]\n#         objs = []\n#         for _, anno in annos.items():\n#             assert not anno[\"region_attributes\"]\n#             anno = anno[\"shape_attributes\"]\n#             px = anno[\"all_points_x\"]\n#             py = anno[\"all_points_y\"]\n#             poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n#             poly = [p for x in poly for p in x]\n\n#             obj = {\n#                 \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n#                 \"bbox_mode\": BoxMode.XYXY_ABS,\n#                 \"segmentation\": [poly],\n#                 \"category_id\": 0,\n#             }\n#             objs.append(obj)\n#         record[\"annotations\"] = objs\n#         dataset_dicts.append(record)\n#     return dataset_dicts\n\n# for d in [\"train\", \"val\"]:\n#     DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon\/\" + d))\n#     MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n# balloon_metadata = MetadataCatalog.get(\"balloon_train\")","b9b5be3e":"# dataset_dicts = get_balloon_dicts(\"balloon\/train\")\n# for d in random.sample(dataset_dicts, 3):\n#     img = cv2.imread(d[\"file_name\"])\n#     visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n#     out = visualizer.draw_dataset_dict(d)\n#     plt.figure(figsize=(15,7))\n#     plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])\n\n\n#visualize training data\nmy_dataset_train_metadata = MetadataCatalog.get(\"train_data\")\ndataset_dicts = DatasetCatalog.get(\"train_data\")\n\nmy_dataset_val_metadata = MetadataCatalog.get(\"val_data\")\nval_dicts = DatasetCatalog.get(\"val_data\")\n\nimport random\nfrom detectron2.utils.visualizer import Visualizer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.imshow(vis.get_image()[:, :, ::-1])","27f6acd4":"# # DATA AUG\n\n# from detectron2.data import transforms as T\n# # Define a sequence of augmentations:\n# augs = T.AugmentationList([\n#     T.RandomBrightness(0.9, 1.1),\n#     T.RandomFlip(prob=0.5),\n#     T.RandomCrop(\"absolute\", (640, 640))\n# ])  # type: T.Augmentation\n\n# # Define the augmentation input (\"image\" required, others optional):\n# input = T.AugInput(image, boxes=boxes, sem_seg=sem_seg)\n\n# # Apply the augmentation:\n# transform = augs(input)  # type: T.Transform\n# image_transformed = input.image  # new image\n# sem_seg_transformed = input.sem_seg  # new semantic segmentation\n\n# # For any extra data that needs to be augmented together, use transform, e.g.:\n# image2_transformed = transform.apply_image(image2)\n# polygons_transformed = transform.apply_polygons(polygons)","dfa8c943":"# # Run training\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator\n\n\nclass CocoTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            os.makedirs(\"coco_eval\", exist_ok=True)\n            output_folder = \"coco_eval\"\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n    \n","84b3b658":"from detectron2.config.config import CfgNode as CN\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"train_data\",)\ncfg.DATASETS.TEST = (\"val_data\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.001\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 30000 #adjust up if val mAP is still rising, adjust down if overfit\n# cfg.SOLVER.STEPS = [0,20000,40000]\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n# cfg.TEST.EVAL_PERIOD = 1000\n\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \n# trainer.resume_or_load(resume=False)\ntrainer.resume_or_load(resume=False)\n\ntrainer.train()","a1ecbc18":"# # Checkpoiints\n# from detectron2.checkpoint import DetectionCheckpointer\n\n# checkpointer = DetectionCheckpointer(trainer.model, save_dir=cfg.OUTPUT_DIR)\n# cfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/objectron-retinanetv1\/model_0034999.pth\")  # path to the model we trained\n# trainer.resume_or_load(resume=True)\n# trainer.train()\n\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml'))\n# cfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/objectron-retinanetv1\/model_0034999.pth\")  # path to the model we trained","d19506b2":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/retinanet_R_50_FPN_1x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/retinanet_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n# cfg.SOLVER.MAX_ITER = 300    # 300 iterations enough for this dataset; Train longer for a practical dataset\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, enough for this dataset (default: 512)\n# # cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # classes for RCNN\n# cfg.MODEL.RETINANET.NUM_CLASSES = 5 # Classes for Retina\n# cfg.TEST.EVAL_PERIOD = 500\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()","2eee3271":"# Look at training curves in tensorboard:\n%load_ext tensorboard\n%tensorboard --logdir logs --bind_all","879425c1":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we trained","042d6485":"# model saved weights\n# cfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/objectron-retinanetv1\/model_final.pth\")\n\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\npredictor = DefaultPredictor(cfg)","249cf501":"from detectron2.utils.visualizer import ColorMode\nval_dict = DatasetCatalog.get(\"val_data\")\n\nfor d in random.sample(val_dict, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im) \n    v = Visualizer(im[:, :, ::-1],\n                   metadata=my_dataset_train_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","481dbf6b":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.modeling import build_model\n\n\nevaluator = COCOEvaluator(\"val_data\", None, False, output_dir=\".\/output\/\")\n# evaluator = COCOEvaluator(\"val_data\", (\"bbox\", \"segm\"), False, output_dir=\".\/output\/\")\n\n# Loading model\nmodel_uploaded = build_model(cfg)\n\nval_loader = build_detection_test_loader(cfg, \"val_data\")\n# print(inference_on_dataset(trainer.model, val_loader, evaluator))\nprint(inference_on_dataset(model_uploaded, val_loader, evaluator))","ffbc0793":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n# cfg.MODEL.WEIGHTS = os.path.join(\"\/kaggle\/input\/objectron-retinanetv1\/model_final.pth\")\n\n\ntest_img_path = \"\/kaggle\/input\/dsta-brainhack-2021\/c1_test_release\/c1_test_release\/images\" # extracted testing images path\ncfg.DATASETS.TEST = (\"my_dataset_test\", )\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\npredictor = DefaultPredictor(cfg)\ntest_metadata = MetadataCatalog.get(\"my_dataset_test\")\n\nfrom detectron2.utils.visualizer import ColorMode\nimport glob\n\nou_test = []\nfor imageName in glob.glob('\/kaggle\/input\/dsta-brainhack-2021\/c1_test_release\/c1_test_release\/images\/*jpg'):\n  im = cv2.imread(imageName)\n  outputs = predictor(im)\n  ou_test.append(outputs)\n  v = Visualizer(im[:, :, ::-1],\n                metadata=test_metadata, \n                scale=0.8\n                 )\n  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n  plt.imshow(out.get_image()[:, :, ::-1])","be4d9214":"ou_test[0]['instances']","4bb043e7":"# generate detections on the folder of test images (this will be used for submission)\nfrom PIL import Image, ImageDraw\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F\nimport torch\n\ndetections = []\n\nfor imageName in glob.glob('\/kaggle\/input\/dsta-brainhack-2021\/c1_test_release\/c1_test_release\/images\/*jpg'):\n        \n\n        im = cv2.imread(imageName)\n        outputs = predictor(im)\n        classes = outputs[\"instances\"].pred_classes.tolist()\n        box_round = outputs[\"instances\"].pred_boxes.tensor.tolist()\n        score_output = outputs[\"instances\"].scores.tolist()\n        head, tail = os.path.split(imageName)\n        img_id = int(tail.split('.')[0])\n\n        for i in range(len(box_round)):\n\n\n            x1, y1, x2, y2 = box_round[i]\n            label = int(classes[i])\n            score = float(score_output[i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})\n\ntest_pred_json = os.path.join(\"\/kaggle\/working\", \"test_preds.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","d18bf1d6":"# Check \nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nsample_json_path = os.path.join(\"\/kaggle\/input\/dsta-brainhack-2021\/c1_test_release\/c1_test_release\", \"c1_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","f15a7bc8":"# !wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\".\/input.jpg\")","8a04d3df":"# cfg = get_cfg()   # fresh config\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints\/keypoint_rcnn_R_50_FPN_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# outputs = predictor(im)\n# v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.figure(figsize=(15,7))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","08a1bc6d":"# !wget http:\/\/images.cocodataset.org\/val2017\/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\".\/input.jpg\")","82057239":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n# plt.figure(figsize=(25,15))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","0f4d35e7":"# from IPython.display import YouTubeVideo, display, Video # for viewing the video\n# !pip install youtube-dl # for downloading the video","e70790dc":"# #video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)#7HaJArMDKgI\n# video = YouTubeVideo(\"7HaJArMDKgI\", width=750, height= 450)#\n# display(video)","86bb50fc":"# !youtube-dl https:\/\/www.youtube.com\/watch?v=7HaJArMDKgI -f 22 -o video.mp4\n# !ffmpeg -i video.mp4 -t 00:00:10 -c:v copy video-clip.mp4 ","196e1f83":"# !git clone https:\/\/github.com\/facebookresearch\/detectron2\n# !python detectron2\/demo\/demo.py --config-file detectron2\/configs\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output 1video-output.mkv \\\n#   --opts MODEL.WEIGHTS detectron2:\/\/COCO-PanopticSegmentation\/panoptic_fpn_R_101_3x\/139514519\/model_final_cafdb1.pkl","2d66a852":"# !git clone https:\/\/github.com\/vandeveldemaarten\/tempdetector2video.git\n# Video(\".\/tempdetector2video\/myvideo.mkv\")","08caa366":"<a id=import ><\/a>\n# 2. Installing dependencies and libraries\n\nWe can use both CPU and GPU for training and inference of the models.\n\nRunning on CPU:","500630ec":"The output of the predictions is saved within the outputs variable. See [model output format](https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format) for all the available functions. ","1b841d96":"<a id=\"pretrainedinference\"><\/a>\n## 3.3. Inference with a pretrained model\n\nIn this first \"coding\" part are two important utils from Detector2. On the one hand we are using **cfg** or better [configs](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/configs.html) which represents the complete configuration of a object detection model. These configurations are stored within a YAML-file and can be easily received from the modelzoo.\n\nAfter the configuration is complete we'll use the [**DefaultPredictor**](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/models.html?highlight=DefaultPredictor#use-a-model) class to make predictions.","6ee7c2ed":"Tensorboard usable in kaggle?","aee9351d":"Reload the data.","71daf36a":"In the output above we see an array which shows us the predictions made by the model. But what number stands for which class?\n\nEvery dataset is associated with [metadata](https:\/\/detectron2.readthedocs.io\/en\/latest\/tutorials\/datasets.html#metadata-for-datasets). It is a key-value mapping that contains information about the dataset. It can be used to further interpret the dataset. This information can later be used for data augmentation, evaluation, visualization, logging, ... .","bf5be8d2":"# Object Detection with Detectron 2 - PyTorch \ud83d\udd25\ud83d\udd25\n\n![Detectron2](https:\/\/miro.medium.com\/max\/4000\/0*VbMjGBHMC6GnDKUp.png)\n\n**In this notebook we'll be checking out the new object detection framework \"Detectron2\" within PyTorch. It allows us to quickly build object detection models.**\n\n*Note: This notebook is a work in progress. I will keep on extending this notebook to fully explain and explore all the capabilities of the \"Detectron2\"- framework!*\n\n\n\n## Table of contents\n* 1.[Introduction](#intro)\n* 2.[Installing dependencies and libraries](#import)\n* 3.[Loading & using existing model](#loadmodel)\n    * 3.1.[The base data](#basedata)\n    * 3.2.[The model zoo](#modelzoo)\n    * 3.3.[Inference with a pretrained model](#pretrainedinference)\n* 4.[Train with custom data](#trainmodel)\n    * 4.1.[Download the dataset](#downloaddataset)\n    * 4.2.[Helper functions](#helperfunction)\n    * 4.3.[Training](#traincustom)\n    * 4.4.[Model evaluation](#modelevaluation)\n* 5.[Other models](#othermodels)\n    * 5.1.[Keypoint detection](#keypoint)\n    * 5.2.[Panoptic segmentation](#panoptic)\n    * 5.3.[Semantic, Densepose, ...](#semantic)\n* 6.[Video](#video)\n    * 6.1.[Libraries](#videolib)\n    * 6.2.[The video](#thevideo)\n    * 6.3.[Inferencing](#videoinference)","4ddca425":"*Note: I suppose TensorBoard doesn't work on Kaggle?*","b0f5c226":"# That's all for now!\n\nThank you for reading this notebook! If you enjoyed it, please upvote!\n\n*More coming soon!*","4e917e5d":"## Test","6b2e5eba":"<a id=traincustom> <\/a>\n## 4.3. Training with a custom dataset\nLet's first check our training data! Ofcourse we'll use the **Visualizer** class again.","3f8ef200":"<a id=\"videoinference\" ><\/a>\n## 6.3. Inference on the video\nLet's now run an panoptic model over the video above.\n\n*note: For now I'll be using some [demo](https:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/demo) files, I'll later add the code implementations to this notebook.*","de1a1593":"Underneed you'll find the extra libraries we'll use in this notebook. More libraries will be added througout the notebook when needed.","305409f7":"Since the our new data looks good. Let's now train our model!\n\nAs initial weights we'll use the pretrained weights from a model from the modelzoo. After finishing setting up the config we'll use the [**DefaultTrainer()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html#module-detectron2.utils.visualizer) class to train our model!\n\nMore information about updating the config-file can be found [here](https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets).","ec37c799":"<a id=\"panoptic\" ><\/a>\n## 5.2. Panoptic segmentation","46657d5e":"<a id=intro ><\/a>\n# 1. Introduction\n[Detectron2](https:\/\/ai.facebook.com\/blog\/-detectron2-a-pytorch-based-modular-object-detection-library-\/) is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron that started from maskrcnn-benchmark.\n\nThis platform is implemented in PyTorch. Thanks to its modular design its a very flexible and extensible framework providing fast training.\n\nIt includes implementations of state-of-the-art object detection algorithms such as:\n* Box detection\n* Mask detection\n* KeyPoint detection\n* Densepose detection\n* Semantic segmentation\n* Panoptic segmentation\n\nIn this notebook we'll have a look at several of these implementations and show how you can use custom datasets to train your own customer model.\n\n*Note: This notebook is a work in progress. I will keep on extending this notebook to fully explore all the capabilities of the \"Detectron2\"- framework!*\n*Future updates will consist in adding more high-end object detection algorithms and explaining more of the functions within the framework.*","97faf3eb":"<a id=\"thevideo\" ><\/a>\n## 6.2. The video","4198c31b":"<a id=trainmodel > <\/a>\n# 4. Train on a custom dataset\n\nAlright so it's pretty easy to run an existing model. Let's now train the model with our own data!\n\n<a id=downloaddataset > <\/a>\n## 4.1. Download the dataset\n\nWithout data we are nothing. So let's download our dataset!","1211fb9c":"<a id=\"video\" ><\/a>\n# 6. Video\n\nSo up until now we've been working with images only. Can we quickly use the models for videos? The answer is YES!\n\n<a id=\"videolib\" ><\/a>\n## 6.1. Libraries\nAs you can see we actually don't need many other libraries. Lets import a library to handle the video.","32714b7d":"<a id=\"semantic\" ><\/a>\n## 5.3. Semantic, Densepose, ...\n\nWill be added in a future version! Stay tuned!","1eeaedbe":"<a id=\"loadmodel\" ><\/a>\n# 3. Loading and using an existing model\n\nIn this chapter we'll have a look at a pretrained model and the base data it's trained on. Later on we'll do some inference with it.\n\n<a id=\"basedata\" ><\/a>\n## 3.1. The base data\n\nThe model we'll be using is pretrained on the [COCO](https:\/\/cocodataset.org\/#home) dataset (2017). This dataset contains a lot of labeled data people can use to train there Object detection models on. (Object, Keypoint, Panoptic, Instance and Densepose detectors) \n\nLet's take a look at a sample:","ea96bd20":"Downloading the video and cropping 6 seconds for processing\n","6be2004f":"Now that we know each label through the metadata. Let's visualize the result of the pretrained model from the modelzoo. For this we'll use the [**Visualizer**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html#module-detectron2.utils.visualizer) class.","d52fedd2":"For example you can find which objects it can recognize:","09790b88":"Let's check the result! \n\n*I've ran into some trouble with video encoding opencv and ffmpeg (fix in future version of this notebook).*","1a050c8e":"Running on GPU:","efa44469":"<a id=helperfunction > <\/a>\n## 4.2. Helper functions\n\nLet's create some helper functions.\n\nThe **get_balloon_dicts()** will convert our data to the correct format. The [**BoxMode**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/structures.html?highlight=BoxMode#detectron2.structures.BoxMode) can be used to get the structure.\n\nAfterwards we'll have to add or register our new dataset with [**DatasetCatalog.register()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=DatasetCatalog#detectron2.data.DatasetCatalog). Ofcourse don't forget to add your metadata with [**MetadataCatalog.get()**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=MetadataCatalog#detectron2.data.MetadataCatalog).","31e4431d":"<a id=\"modelevaluation\" ><\/a>\n## 4.4. Model evaluation\nLet's check out the performance of our model!\n\nFirst of all let's make some predictions! We're going to use the [**DefaultPredictor**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor) class. Ofcourse we'll use the same cfg that we used during training. We'll change two parameters for our inferencing.","6b3de1af":"Above we can see that our models performs pretty well! Let's now evaluate our custom model with [Evaluators](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor). Two evaluators can be used:\n* [**COCOEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.COCOEvaluator) can evaluate AP (Average Precision) for box detection, instance segmentation and keypoint detection.\n* [**SemSegEvaluator**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.SemSegEvaluator) can evaluate semantic segmentation metrics.\n\nAfterwards we'll use the [**build_detection_test_loader**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/data.html?highlight=build_detection_test_loader#detectron2.data.build_detection_test_loader) which returns a torch DataLoader, that loads the given detection dataset.\n\nAt last we'll use the model, evaluated and dataloader within the [inference_on_dataset](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/evaluation.html#detectron2.evaluation.inference_on_dataset) function. It runs the model on the dataloader and evaluates the metric with the evaluator.","d51c71e8":"<a id=\"othermodels\" ><\/a>\n# 5. Other models\n\nIt's possible to use other high-end object detection models aswell. Let's check it out!\n\n<a id=\"keypoint\" ><\/a>\n## 5.1. Keypoint detection","b12daa6f":"Notice that by using the [**ColorMode.IMAGE_BW**](https:\/\/detectron2.readthedocs.io\/en\/latest\/modules\/utils.html?highlight=ColorMode#module-detectron2.utils.visualizer) we we're capable of removing the colors from objects which aren't detected!","ec9c4f1f":"<a id=\"modelzoo\"><\/a>\n## 3.2. The model zoo\nMany pretrained models can be found back within the \"[modelzoo](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/MODEL_ZOO.md)\". This is a collection of models pretrained on a certain dataset that are ready to be used. Mostly people will use the pretrained weights of these model for initalization of there own custom model. This significantly shortens the training time and performance. And that's exactly what we'll be doing!\n\nThe model we'll be using can be found [here](https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/configs\/COCO-Detection\/retinanet_R_50_FPN_3x.yaml). \n\nHow does it work?\nAs we can find [here](https:\/\/www.researchgate.net\/figure\/Our-Mask-R-CNN-framework-In-the-first-stage-we-use-Resnet50-Resnet101-and-Resnet_fig1_334011187):\n> Region proposal network (RPN) utilizes feature maps at one of the intermediate layers (usually the last convolutional layer) of the CNN feature extractor networks to generate box proposals (300 boxes in our study). The proposed boxes are a grid of anchors tiled in different aspect ratios and scales. The second stage predicts the confidence value, the offsets for the proposed box and the mask within the box for each anchor.\nSource publication\n\n![Mask R-CNN Resnet](https:\/\/www.researchgate.net\/profile\/Hemin_Ali_Qadir\/publication\/334011187\/figure\/fig1\/AS:774289183735808@1561616335154\/Our-Mask-R-CNN-framework-In-the-first-stage-we-use-Resnet50-Resnet101-and-Resnet.ppm)\n\nAlright, so now that we know how our model works lets test it out!"}}