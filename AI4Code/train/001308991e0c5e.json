{"cell_type":{"6c01d0d2":"code","b8fd3a8c":"code","94d3d43a":"code","9ecece20":"code","808d31ab":"code","615cd02a":"code","568fa01a":"code","6603b9b9":"code","42476427":"code","bd24750b":"code","592fa3ca":"code","70598ae5":"code","21992a54":"code","c6f0a7fb":"code","7da89797":"code","fe931e4a":"code","ca9d112f":"code","caa2a0e4":"code","b6cb47da":"code","351cb961":"code","3d393ce3":"code","d9ae76ec":"code","1a39fcb9":"code","a2f89492":"code","dde2e9dd":"code","3b9b6120":"code","30a931e1":"code","9df82070":"code","a9e1c7d1":"code","a5cf7e49":"code","f97b0b20":"code","8581cbc5":"code","fb8d407a":"code","a018158b":"code","f9716d78":"code","8f1cc7b3":"code","c35673a4":"code","d94d4c79":"code","f4bd5257":"code","21058826":"code","8adbe9fd":"code","475eabb0":"code","4bf60e9c":"code","2aa84b2d":"code","d7401438":"code","54c6a136":"code","c6546226":"code","e9223ed0":"code","d242ba2b":"code","d578a6a3":"code","ed3acd5c":"code","f7dc97df":"code","afcce3e5":"code","bbc4381d":"code","9906172a":"code","6dfce6b0":"code","6306ba97":"markdown","93b8809e":"markdown","a2501363":"markdown","d19afe34":"markdown","bf9eebce":"markdown","23e35b11":"markdown","9d2b623b":"markdown","d5bfc027":"markdown","05915708":"markdown","5bd28595":"markdown","7d0e348f":"markdown","6fd20717":"markdown","6d170ada":"markdown","21147235":"markdown","f9249928":"markdown","3f7f47cd":"markdown","6ff8d8cc":"markdown","6da92b81":"markdown","3d63b500":"markdown","cc2c96eb":"markdown","5ca1d878":"markdown","dcaf81f6":"markdown","b7f8c5cb":"markdown","57b14df5":"markdown","1ff43b24":"markdown","5fdb303a":"markdown","ab3ae8d0":"markdown","1545964f":"markdown","29857d6d":"markdown","a2988204":"markdown","9fed798a":"markdown","79ea8736":"markdown","67e6eb4a":"markdown","e0ffad5f":"markdown","fb5c88e1":"markdown","ec57b602":"markdown","449510e4":"markdown","9d30d44b":"markdown","dff53cea":"markdown"},"source":{"6c01d0d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ndf = pd.read_csv('..\/input\/noshowappointments\/KaggleV2-May-2016.csv')","b8fd3a8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","94d3d43a":"df.dropna()\ndf.head()","9ecece20":"df.shape","808d31ab":"df.columns","615cd02a":"df.columns = ['patient_id', 'appointment_id', 'gender', 'scheduled_day', \n              'appointment_day', 'age', 'neighbourhood', 'scholarship', 'hypertension',\n              'diabetes', 'alcoholism', 'handicap', 'sms_received', 'no_show']\ndf.columns","568fa01a":"df.describe()","6603b9b9":"df.info()","42476427":"df['patient_id'] = df['patient_id'].astype('int64')\ndf.info()","bd24750b":"df['scheduled_day'] = pd.to_datetime(df['scheduled_day']).dt.date.astype('datetime64[ns]')\ndf['appointment_day'] = pd.to_datetime(df['appointment_day']).dt.date.astype('datetime64[ns]')","592fa3ca":"#0 is Monday\ndf[\"weekday\"] = df[\"scheduled_day\"].dt.dayofweek\ndf.head()","70598ae5":"df[\"weekday\"].value_counts()","21992a54":"x = df[df[\"alcoholism\"] == 1]\nx[\"neighbourhood\"].value_counts()","c6f0a7fb":"df[\"waiting_days\"] = (df[\"appointment_day\"] - df[\"scheduled_day\"])\ndf[\"waiting_days\"] = (df[\"waiting_days\"] \/ np.timedelta64(1, 'D')).astype(int)\ndf.head()","7da89797":"df.describe()","fe931e4a":"df.info()","ca9d112f":"df.to_csv(\"Noshow_updated.csv\")","caa2a0e4":"df[\"gender\"].value_counts()","b6cb47da":"num_unique_apps = len(df.appointment_id.unique())\nall_dataset_rec_number = df.shape[0]\nprint('{} == {}'.format(num_unique_apps, all_dataset_rec_number))","351cb961":"data = df[\"gender\"].value_counts()\n# plt.ylabel(\"Count\")\n# plt.title(\"Gender\");\nmy_labels = 'Males','Females'\nplt.pie(data,labels=my_labels,autopct='%1.1f%%')\nplt.title('Gender Ratio')\nplt.axis('equal')\nplt.show()","3d393ce3":"data = df[\"no_show\"].value_counts()\n# plt.ylabel(\"Count\")\n# plt.title(\"Gender\");\nmy_labels = 'No','Yes'\nplt.pie(data,labels=my_labels,autopct='%1.1f%%')\nplt.title('Show ratio')\nplt.axis('equal')\nplt.show()","d9ae76ec":"# line chart plot for Scheduled Date\ndf[\"scheduled_day\"].dt.date.value_counts().plot(kind=\"line\",figsize=(10,4));\nplt.xlabel(\"Scheduled Date\")\nplt.ylabel(\"Count\")\nplt.title(\"'Scheduled Date' Line chart\")","1a39fcb9":"# line chart plot for Appointment Date\ndf.appointment_day.dt.date.value_counts().plot(kind=\"line\",figsize=(10,4));\nplt.xlabel(\"Appointment Date\")\nplt.ylabel(\"Count\")\nplt.title(\"'Appointment Date' Line chart\")","a2f89492":"# Scatter plot for the relationship between ScheduledDay and AppointmentDay\ndf.plot(kind=\"scatter\", x=\"scheduled_day\",y=\"appointment_day\");\nplt.title(\"relationship between ScheduledDay and AppointmentDay\");","dde2e9dd":"plt.figure(figsize=(16,4))\nplt.xticks(rotation=90)\nax = sns.countplot(x=df.age)\nax.set_title(\"No of appointments by age\")\nplt.show()","3b9b6120":"df.neighbourhood.value_counts().plot(kind=\"bar\",figsize=(20,5));","30a931e1":"df.waiting_days.value_counts().head(15).plot(kind=\"bar\",figsize=(20,5));","9df82070":"# First, let's look at categorical variables\ncategorical_vars = ['weekday', 'scholarship', 'hypertension', 'diabetes', 'alcoholism', 'handicap', 'sms_received']\n\nfig = plt.figure(figsize=(16, 11))\nfor i, var in enumerate(categorical_vars):\n    ax = fig.add_subplot(3, 3, i+1)\n    df.groupby([var, 'no_show'])[var].count().unstack('no_show').plot(ax=ax, kind='bar', stacked=True)","a9e1c7d1":"# Numerical variables\n# Age:\ndf.age[df.no_show==\"No\"].hist(alpha=0.8, bins=20);\ndf.age[df.no_show==\"Yes\"].hist(alpha=0.8, bins=20);","a5cf7e49":"# Waiting days\ndf.waiting_days[df.no_show==\"No\"].hist(alpha=0.8, bins=20);\ndf.waiting_days[df.no_show==\"Yes\"].hist(alpha=0.8, bins=20);","f97b0b20":"y_labels = {\"no_show\" : {\"Yes\": 1 , \"No\":0}}\ndf.replace(y_labels , inplace= True)\ndf.head()","8581cbc5":"y_labels = {\"gender\" : {\"F\": 1 , \"M\":0}}\ndf.replace(y_labels , inplace= True)\ndf.head()","fb8d407a":"df[\"no_show\"].value_counts()","a018158b":"df1 = df\nshuffled_df = df1.sample(frac=1,random_state=4)\n\n# Put all the fraud class in a separate dataset.\nshow_df = df1.loc[df1['no_show'] == 1]\n\n#Randomly select 492 observations from the non-fraud (majority class)\nnoshow_df = df1.loc[df1['no_show'] == 0].sample(n=22319,random_state=42)\n\n# Concatenate both dataframes again\nnormalized_df = pd.concat([show_df, noshow_df])\n\n#plot the dataset after the undersampling\nplt.figure(figsize=(8, 8))\nsns.countplot('no_show', data=normalized_df)\nplt.title('Balanced Classes')\nplt.show()","f9716d78":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndf[\"neighbourhood\"] = encoder.fit_transform(df[\"neighbourhood\"])\ndf.head()","8f1cc7b3":"df[\"neighbourhood\"].nunique()","c35673a4":"df.columns","d94d4c79":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nX = df[['gender', 'age', 'neighbourhood', 'scholarship',\n       'hypertension', 'diabetes', 'alcoholism', 'handicap', 'sms_received',\n       'weekday', 'waiting_days']]\ny = df['no_show']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)","f4bd5257":"logreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train, y_train)\ny_pred1 = logreg.predict(X_test)\nprint(\"Accuracy={:.2f}%\".format(logreg.score(X_test, y_test)*100))","21058826":"confusion_matrix = pd.crosstab(y_test, y_pred1, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True)","8adbe9fd":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred1))","475eabb0":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg, X, y, cv=5)\nprint(\"Average Accuracy:{:.2f}%\".format((sum(scores)\/len(scores))*100))","4bf60e9c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nl1=[0]\nfor i in range(1,20):\n  knn=KNeighborsClassifier(n_neighbors=i) #build our knn classifier\n  knn.fit(X_train,y_train) #Training KNN classifier\n  y_pred=knn.predict(X_test)  #Testing\n  p = accuracy_score(y_pred,y_test)\n  if(p > max(l1)):\n    y_pred2 = y_pred\n    print(\"Number of Neighbours = \",i,' Accuracy=',p)\n    l1.append(p)\n    finalknn = knn\n    z,l = i,p\nprint(\"-----------------------------------\")\nprint(f\"KNN had an accuracy of {p} at {i} Neighbours\")","2aa84b2d":"df.columns","d7401438":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\ndf1 = df[['gender', 'age', 'neighbourhood', 'scholarship',\n       'hypertension', 'diabetes', 'alcoholism', 'handicap', 'sms_received',\n      'weekday', 'waiting_days']]\ndf1.head()","54c6a136":"pca = PCA(n_components=2)\ndf_pca = pd.DataFrame(pca.fit_transform(df1))\ndf_pca","c6546226":"df_pca.columns","e9223ed0":"from sklearn import tree   \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nX = df_pca[[0,1]]\ny = df['no_show']\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.33,random_state=42)\n\nmodel = DecisionTreeClassifier()  \nmodel.fit(x_train, y_train) \ny_pred3=model.predict(x_test)\nprint(\"score:{:.2f}%\".format(accuracy_score(y_test, y_pred3)*100))","d242ba2b":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5)\nprint(\"Average Accuracy: {:.2f}\".format((sum(scores)\/len(scores))*100))","d578a6a3":"df.columns","ed3acd5c":"from sklearn import tree   \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nX = df[['gender', 'age', 'neighbourhood', 'scholarship',\n       'hypertension', 'diabetes', 'alcoholism', 'handicap', 'sms_received',\n       'weekday', 'waiting_days']]\ny = df['no_show']\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.33,random_state=42)\n\nmodel = DecisionTreeClassifier()  \nmodel.fit(x_train, y_train) \ny_pred3=model.predict(x_test)\nprint(\"score:{:.2f}%\".format(accuracy_score(y_test, y_pred3)*100))","f7dc97df":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5)\nprint(\"Average Accuracy: {:0.2f}%\".format((sum(scores)\/len(scores))*100))","afcce3e5":"from sklearn.ensemble import RandomForestClassifier\nl1 = [0]\nfor i in range(1,51):\n  clf2=RandomForestClassifier(n_estimators=i)\n  clf2.fit(x_train, y_train) \n  y_pred=clf2.predict(x_test)\n  if(accuracy_score(y_test, y_pred)>max(l1)):\n    print(\"score:{:.2f}%\".format(accuracy_score(y_test, y_pred)*100),end = \" \")\n    print(i)\n    Randomf = clf2\n  l1.append(accuracy_score(y_test, y_pred))\ny_pred4 = Randomf.predict(x_test)\nprint(\"score of Best Random forest:{:.2f}%\".format(accuracy_score(y_test, y_pred4)*100))","bbc4381d":"from sklearn import svm\nclf = svm.SVC(kernel='poly') # Polynomial Kernel\nclf.fit(X_train, y_train)\ny_pred5 = clf.predict(X_test)\nprint(\"Accuracy: {:.2f} %\".format(metrics.accuracy_score(y_test, y_pred5)*100))","9906172a":"from sklearn.linear_model import RidgeClassifier\nclf = RidgeClassifier().fit(X_train, y_train)\ny_pred6 = clf.predict(X_test)\nprint(\"Accuracy:{:.2f} %\".format(clf.score(X_test, y_test)*100))","6dfce6b0":"import statistics as st\nfinal_pred = np.array([])\nfor i in range(0,len(X_test)):\n    final_pred = np.append(final_pred, st.mode([y_pred1[i], y_pred2[i], y_pred3[i], y_pred4[i], y_pred6[i]]))\nprint(\"Accuracy:{:.2f} %\".format(metrics.accuracy_score(y_test, final_pred)*100))","6306ba97":"#### Note 3: Adding Weekday column where 0 represents Monday","93b8809e":"### 7) What is the count of patients in each neighbourhood?","a2501363":"#### First, we will have a view of the data in the dataset we are working on, to explore the columns and the size of the dataset","d19afe34":"# Decision Tree without Dimension Reduction","bf9eebce":"### **It was found that the best output is the Ensembling of several model and the output of SVM can nearly match it.** \n### **Therefore, Using the Ensembling of several models or SVM would give us the same output**","23e35b11":"# Using Label Encoder","9d2b623b":"# 2) Data visualization","d5bfc027":"# 3) ML models for data prediction","05915708":"\\#### Note 4: Adding waiting_days column which is the difference between appointment and scheduled day.\n#### Note 5: Changing waiting_days type to int for further analysis","5bd28595":"# 1) Data Cleaning","7d0e348f":"## A- Logistic Regression Model","6fd20717":"## D- Random Forest Classifier Model","6d170ada":"#### 2- Split the data into actual data and test data","21147235":"# Machine intelligence Project:\n## Investigate a Dataset: Cleaning, analyzing, & visualizing a dataset and creating ML models for data predication\n## Mazen Mobtasem 18100142\n## Bassel Sherif 18101082\n## Youssef Hosam 18102153\n## Mohamed Waleed 18101090\n","f9249928":"### 5) What is the relation between scheduled day and appointment day? ","3f7f47cd":"## C- Decision Tree Model with PCA","6ff8d8cc":"### 8) What is the number of patients by number of awaiting days?","6da92b81":"#### Afterwards, we have looked at the datatypes of the columns and general numerical description about the data.","3d63b500":"#### Note 2: scheduled_day & appointment_day datatype must be converted to datetime","cc2c96eb":"### 2) What is the ratio between show and no-show?","5ca1d878":"# Ensembling of  Logistic Regression,KNN, Decision Tree, Random Forest, Ridge Classifier (No. is odd to avoid equal number of votes) ","dcaf81f6":"## Exploratory Data Analysis: Insights of data\n     1) For all categorical variables the distributions of show \/ no-show for different categories look very similar. There is no clear indication of any of these variables having bigger than others impact on show \/ no-show characteristics.\n     \n     2) For numerical variables, such as age, kids and patients in their 60s, 70s, and 80s are more likely to show to their appointments\n     \n     3) For numerical variable, such as waiting days (the most affecting variable in our dataset), shows that when the waiting day is shorter, the patient is more likely to show up for their appointments.","b7f8c5cb":"#### The Decision Tree Classifier got to an accuracy of 78.37%","57b14df5":"#### Now, this is a new csv file generated for the no-show dataset after the cleaning stage","1ff43b24":"### 6) How many patients for each age?","5fdb303a":"## General exploration: Insights of data\n     1) Males represent 65% of the dataset and females represent 35% only.\n     2) Only 20.2% of patients showed, while 79.8% didnt show.\n     3) The most scheduled date is mainly between months 5 and 6 of 2016\n     4) The appointment dates mainly have the same number of patients yet, there is a huge drop near 15\/5\/2016\n     5) The majority of patients are of really young age\n     6) Most of the patients are from Jardim Camburi\n     7) The highest number of patients is when the awaiting days is 0","ab3ae8d0":"### 3) What is the most frequent scheduled date?","1545964f":"# PCA Dimension Reduction","29857d6d":"## B- KNN Model","a2988204":"# G- Ridge Classifier ","9fed798a":"### 4) What is the most frequent appointment date?","79ea8736":"# Conclusion","67e6eb4a":"#### 1- Convert categorical data to numerical data for the models","e0ffad5f":"## A- General Exploration\n### To get to know the dataset better to be able to get insights of the dataset and answer the right question","fb5c88e1":"## B- Explorartory Data Analysis\n### After general exploration of the dataset, our goal now is to to find why patients miss their appointment, what factors are important for us to know in order to predict if a patient will show up for their scheduled appointment? ","ec57b602":"#### We have used age and waiting_days columns only as they are the most affecting factors according to our analysis","449510e4":"# E- SVM Polynomial Kernel","9d30d44b":"### 1) What is the ratio between males and females?","dff53cea":"#### Note 1: The patient_id data type is float as shown before, so it must be changed to be int"}}