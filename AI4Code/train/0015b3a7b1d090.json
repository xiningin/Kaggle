{"cell_type":{"f9d1d049":"code","6b839e2c":"code","f7f7add5":"code","8b28582f":"code","152bae08":"code","f66232c0":"code","31ee5b14":"code","91d77777":"code","5124c6bb":"code","8cfaadaf":"code","ac357212":"code","d1e4e2e1":"code","8aff67c2":"code","b1b14cb8":"code","9113d65c":"code","d91719dd":"code","4f222c11":"code","27739bb7":"code","0dfafb37":"code","61ce9fee":"code","ad28eabf":"code","7be4e2e6":"code","188491bd":"code","08f39941":"code","5d6817c0":"code","c778e9ef":"code","728a876f":"code","da19a6b5":"code","0653c0b9":"code","44623ee9":"code","86bb2f2f":"code","26f9b06c":"code","060ecf90":"code","30f17f42":"code","bdb7c0c7":"code","7a1421b3":"code","5147f11f":"code","b5df308b":"code","5cee42f2":"code","f35c8fe6":"code","334364b2":"code","5830875b":"code","c09cd243":"code","f9313c80":"code","adf66991":"code","04edf30d":"code","5914cb4f":"code","98176c65":"code","0b3c89a5":"code","39ce244a":"code","9c5223d7":"code","7c9cbeee":"code","d7579a65":"code","d0612ecb":"code","b83c0a3c":"code","7e67018c":"code","4db807a4":"code","682e86e8":"code","e903c078":"code","bc5a1ff0":"code","565b57d3":"code","9b28dbeb":"code","50a5f2ed":"code","765365dd":"code","76cf0359":"code","df748c2d":"code","3b7bfa34":"code","63147742":"code","115dd883":"code","a31c774a":"code","e1131001":"code","736eed68":"code","aecc8686":"code","4b2a5ed3":"code","14a49395":"code","7b41f627":"code","379e5e44":"code","927a5319":"code","43f9971d":"code","75109a4e":"code","27dfd6bb":"code","fd2d59f1":"code","74c4a378":"code","aa82f802":"code","95081ee7":"code","b51de78e":"code","c4cdf1b6":"code","21b345fd":"code","276739f8":"code","5ec97b44":"code","d4802608":"code","05ba034e":"code","1fe67250":"code","695a46c7":"code","69737df8":"code","f49ac0ca":"code","7a4ab021":"code","de4b1a06":"code","050af69f":"code","a7a13cee":"code","17aff40a":"code","76ec1993":"code","dda1ce6b":"code","07492c4d":"code","7b217a63":"code","96e1c014":"code","827ad3e7":"code","296e6dc6":"code","b797cfc7":"code","619eb114":"code","caad031a":"code","988e57a0":"code","5a534ede":"code","06186aeb":"code","76c78dce":"markdown","83592431":"markdown","1eec4a35":"markdown","1f8556d3":"markdown","63a43425":"markdown","88431962":"markdown","37117c2f":"markdown","1f480f5f":"markdown","ac4dda2a":"markdown","a077701a":"markdown","427bd855":"markdown","ff5ec916":"markdown","c6fa8f9d":"markdown","f9e7d1de":"markdown","a3b5283b":"markdown","e57b9958":"markdown","21b57ad7":"markdown","25f99644":"markdown","1f43ac92":"markdown","6b41514a":"markdown","2a90834f":"markdown","5720a45a":"markdown","533a5676":"markdown","ae4114d1":"markdown","102a5d39":"markdown","4e3194e0":"markdown","b625eaa8":"markdown","c69d25f1":"markdown","7248c74f":"markdown","c5418df6":"markdown","ec878ea7":"markdown","d1b04108":"markdown","93eab3ab":"markdown","cc3cb10b":"markdown","28b9c6f8":"markdown","667a322b":"markdown","ca3c27f9":"markdown","fc5a3d89":"markdown","1c5b0810":"markdown","9d5894f6":"markdown","cfab61a1":"markdown","a0b80a9d":"markdown","3bff8795":"markdown","2d96c642":"markdown","d0bed918":"markdown","f0941f8e":"markdown","1ee0f5a7":"markdown","aa18c3c3":"markdown","b51a1b69":"markdown","eeaffcf1":"markdown","01538e65":"markdown","ca5a8b6a":"markdown","87b88a18":"markdown","2eef9452":"markdown","95f40d4e":"markdown","e8824160":"markdown","a52d3e04":"markdown","b113a1a1":"markdown","66655072":"markdown","902fda00":"markdown","f794919e":"markdown","cde6e659":"markdown"},"source":{"f9d1d049":"################# GENERAL IMPORTS\nimport os\nimport string\nfrom pprint import pprint\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n##################################### NLP SPECIFIC IMPORTS\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import reuters\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nfrom collections import Counter\nreuters.fileids()\nstopwords.words('english')\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom sys import modules\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nfrom gensim.models import word2vec\nimport logging\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Flatten\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Dropout\n\nimport seaborn as sns\nsns.set(style = 'darkgrid')\nprint(os.listdir(\"..\/input\"))\nimport re\npd.set_option('max_colwidth', 800)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(1234)\n%matplotlib inline\n\nprint('all set')","6b839e2c":"quora_train=pd.read_csv(\"..\/input\/train.csv\")\nquora_test=pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train size =\" ,quora_train.shape)\nprint(\"Test size =\" ,quora_test.shape)","f7f7add5":"# quora_train=quora_train[0:5000]\n# quora_test=quora_test[0:1000]","8b28582f":"quora_train.head()","152bae08":"quora_train.info()","f66232c0":"sincere = quora_train[quora_train.target==0]\ninsincere = quora_train[quora_train.target==1]","31ee5b14":"[print(q,'\\n') for q in sincere['question_text'][[1,6,40,300,120]]]","91d77777":"quora_train['target'].value_counts().plot(kind='bar', title='Target distribution')","5124c6bb":"round(quora_train['target'].value_counts(normalize =True),3)*100","8cfaadaf":"quora_train['words'] = quora_train.question_text.apply(lambda x: len(x.split()))\nquora_train['characters'] = quora_train.question_text.apply(lambda x: len(x))\nquora_test['words'] = quora_test.question_text.apply(lambda x: len(x.split()))\nquora_test['characters'] = quora_test.question_text.apply(lambda x: len(x))","ac357212":"quora_train.head()","d1e4e2e1":"fig = plt.figure(figsize=(18, 7))\n\nplt.subplot(1, 2, 1)\nquora_train.groupby('target')['words'].mean().plot(kind='bar', ylim=(0,20), title= 'Average word count by target')\n\nplt.subplot(1, 2, 2)\nquora_train.groupby('target')['characters'].mean().plot(kind='bar', ylim=(0,105), title= 'Average character count by target')","8aff67c2":"fig = plt.figure(figsize=(18, 8))\nfont = {'size': 16, 'weight': 'bold'}\n\nplt.subplot(2, 1, 1)\nplt.title('Word count - by outcome',fontdict=font)\nplt.xlim(0,70)\nax = sns.boxplot(x=\"words\", y=\"target\", data=quora_train, orient=\"h\")\n\nplt.subplot(2, 1, 2)\nplt.title('character count - by outcome',fontdict=font)\nplt.xlim(0,350)\nax = sns.boxplot(x=\"characters\", y=\"target\", data=quora_train, orient=\"h\")","b1b14cb8":"from nltk import pos_tag\n\ndef verb_count(text):\n    token_text= word_tokenize(text)\n    tagged_text = pos_tag(token_text)\n    counter=0\n    for w,t in tagged_text:\n        t = t[:2]\n        if t in ['VB']:\n            counter+=1\n    return counter\n\ndef noun_count(text):\n    token_text= word_tokenize(text)\n    tagged_text = pos_tag(token_text)\n    counter=0\n    for w,t in tagged_text:\n        t = t[:2]\n        if t in ['NN']:\n            counter+=1\n    return counter\n","9113d65c":"quora_train.head()","d91719dd":"quora_train['question_text_prep'] = quora_train['question_text'].apply(lambda x: x.lower())\nquora_test['question_text_prep'] = quora_test['question_text'].apply(lambda x: x.lower())","4f222c11":"def pad_punctuation_w_space(string):\n    s = re.sub('([:;\"*.,!?()\/\\=-])', r' \\1 ', string)\n    s=re.sub('[^a-zA-Z]',' ',s)\n    s = re.sub('\\s{2,}', ' ', s)\n    s =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", s) #code for removing single characters\n    return s\nquora_train['question_text_prep'] = quora_train['question_text_prep'].apply(lambda x: pad_punctuation_w_space(x))\nquora_test['question_text_prep'] = quora_test['question_text_prep'].apply(lambda x: pad_punctuation_w_space(x))\n","27739bb7":"quora_train['question_text_prep'] = quora_train['question_text_prep'].apply(lambda x: x.split())\nquora_test['question_text_prep'] = quora_test['question_text_prep'].apply(lambda x: x.split())","0dfafb37":"stop_list = stopwords.words('english') + list(string.punctuation)\nquora_train['question_text_prep'] = quora_train['question_text_prep'].apply(lambda x: [i for i in x if i not in stop_list])\nquora_test['question_text_prep'] = quora_test['question_text_prep'].apply(lambda x: [i for i in x if i not in stop_list]) \n","61ce9fee":"quora_train['question_text_prep_string'] = quora_train['question_text_prep'].str.join(\" \")\nquora_test['question_text_prep_string'] = quora_test['question_text_prep'].str.join(\" \")","ad28eabf":"# quora_insincere =  quora_train[quora_train.target==1]\n# quora_sincere =  quora_train[quora_train.target==0]\n\n# # make \"all in one\" corpuses for the 2 classes in the target\n# insincere_all_in_one = ' '.join([q for q in quora_insincere.question_text_prep_string])\n# sincere_all_in_one = ' '.join([q for q in quora_sincere.question_text_prep_string]) ","7be4e2e6":"# fig = plt.figure(figsize=(30, 12))\n# font = {'size': 20, 'weight': 'bold'}\n\n# plt.subplot(1, 2, 1)\n# plt.title('Insincere questions',fontdict=font)\n# cloud1 = WordCloud(max_words=100,width=480, height=480, background_color='grey')\n# cloud1.generate_from_text(insincere_all_in_one)\n# plt.imshow(cloud1)\n# plt.axis('off')\n\n# plt.subplot(1, 2, 2)\n# plt.title('Sincere questions',fontdict=font)\n# cloud = WordCloud(max_words=100,width=480, height=480, background_color='skyblue')\n# cloud.generate_from_text(sincere_all_in_one)\n# plt.imshow(cloud)\n# plt.axis('off')","188491bd":"# insincere_tokens = [t for t in word_tokenize(insincere_all_in_one)]\n# sincere_tokens = [t for t in word_tokenize(sincere_all_in_one)]","08f39941":"# from collections import Counter\n# from nltk import ngrams\n# bi_gram_insincere = Counter(ngrams(insincere_tokens, 2))\n# tri_gram_insincere = Counter(ngrams(insincere_tokens, 3))\n# bi_gram_sincere = Counter(ngrams(sincere_tokens, 2))\n# tri_gram_sincere = Counter(ngrams(sincere_tokens, 3))","5d6817c0":"# bi_gram_ins = pd.DataFrame(bi_gram_insincere.most_common(20), columns=['bi_gram_ins','frequency'])\n# bi_gram_sin = pd.DataFrame(bi_gram_sincere.most_common(20), columns=['bi_gram_sin','frequency'])\n# tri_gram_ins = pd.DataFrame(tri_gram_insincere.most_common(20), columns=['tri_gram_ins','frequency'])\n# tri_gram_sin = pd.DataFrame(tri_gram_sincere.most_common(20), columns=['tri_gram_sin','frequency'])","c778e9ef":"# import matplotlib.pyplot as plt\n# fig, (ax, ax2) = plt.subplots(ncols=2, sharex=False)\n# fig.subplots_adjust(wspace =0.6)\n# ax.invert_xaxis()\n\n# bi_gram_ins.sort_values(by='frequency').plot(kind='barh', x='bi_gram_ins', legend=True, ax=ax, figsize=(18,9), fontsize =16)\n# bi_gram_sin.sort_values(by='frequency').plot(kind='barh', x='bi_gram_sin',ax=ax2, figsize=(18,9),fontsize =16)\n# plt.show()","728a876f":"# fig, (ax, ax2) = plt.subplots(ncols=2, sharex=False)\n# fig.subplots_adjust(wspace =0.8)\n# ax.invert_xaxis()\n\n# tri_gram_ins.sort_values(by='frequency').plot(kind='barh', x='tri_gram_ins', legend=True, ax=ax, figsize=(18,9), fontsize =16)\n# tri_gram_sin.sort_values(by='frequency').plot(kind='barh', x='tri_gram_sin',ax=ax2, figsize=(18,9), fontsize =16)\n# plt.show()","da19a6b5":"sents = list(quora_train.question_text_prep.values) \nsents[0]","0653c0b9":"min_num = 3 # minimum number of occurrences in text\nEMBEDDING_FILE= \"..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt\"","44623ee9":"import numpy as np\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding='utf8')\n    model = {}\n    for line in f:\n        splitLine = line.split(' ')\n        word = splitLine[0]\n        embedding = np.asarray(splitLine[1:], dtype='float32')\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model","86bb2f2f":"word_model= loadGloveModel(EMBEDDING_FILE)   \n# print (word_model['hello']) # if we want to see an example for a vector","26f9b06c":"print('Loaded %s word vectors.' % len(word_model))","060ecf90":"unknown_words = []\nfor question in quora_train.question_text_prep:\n    for word in question:\n        if word not in word_model:\n            unknown_words.append(word)\n        else: pass","30f17f42":"len(unknown_words)","bdb7c0c7":"unknown_words[:10]","7a1421b3":"total_term_frequency = Counter(unknown_words)\n\nfor word, freq in total_term_frequency.most_common(20):\n    print(\"{}\\t{}\".format(word, freq))","5147f11f":"def get_vector(DataFrame):\n    vec_X = []\n    i = 0\n    for item in DataFrame.question_text_prep_string: \n        \n        sentence = pad_punctuation_w_space(item)\n        s = np.array([])\n        s = []\n        if len(sentence)==0:\n            s = np.array(word_model['UNK'])\n            vec_X.append(s) \n            i += 1\n        else:\n                for word in sentence.split():\n                    if len(s) == 0:\n                        try:\n                            s = np.array(word_model[word])\n                        except: \n                            s = np.array(word_model['UNK'])\n                    else:\n                        try:\n                            s += np.array(word_model[word])\n                        except: \n                            s += np.array(word_model['UNK'])         \n                vec_X.append(s) \n                i += 1\n\n    return vec_X","b5df308b":"vec_X_train=get_vector(quora_train)\nvec_X_test=get_vector(quora_test)\nquora_train[\"vector\"]=vec_X_train\nquora_test[\"vector\"]=vec_X_test","5cee42f2":"from imblearn.under_sampling import RandomUnderSampler","f35c8fe6":"X = quora_train[['words','characters','vector']] #,'noun_count'\ny = quora_train['target']","334364b2":"rus = RandomUnderSampler(return_indices=True, ratio = 0.42)\nX_rus, y_rus, id_rus = rus.fit_sample(X, y)\n\nprint('indexes:', id_rus)\nprint(len(id_rus))\nprint(quora_train.target.value_counts())","5830875b":"quora_undr=quora_train.loc[id_rus]","c09cd243":"quora_undr['target'].value_counts(ascending=True).plot(kind='bar')","f9313c80":"quora_undr['target'].value_counts(normalize=True)","adf66991":"quora_under_prep = quora_undr","04edf30d":"quora_under_prep[\"characters\"].head()","5914cb4f":"quora_under_prep['noun_count'] = quora_under_prep.question_text.apply(lambda x: noun_count(x))\nquora_test['noun_count'] = quora_test.question_text.apply(lambda x: noun_count(x))","98176c65":"quora_under_prep['vector_length']= quora_under_prep['vector'].apply(lambda x: len(x))\nquora_test['vector_length']= quora_test['vector'].apply(lambda x: len(x))\nquora_test['vector_length'].describe()","0b3c89a5":"quora_best=quora_under_prep","39ce244a":"import numpy as np\nquora_best[\"joinvector\"]=[np.concatenate((np.array([quora_best[\"characters\"].iloc[i]]),quora_best[\"vector\"].iloc[i]), axis=None) for i in range(len(quora_best))]\nquora_best[\"joinvector_2\"]=[np.concatenate((np.array([quora_best[\"words\"].iloc[i]]),quora_best[\"joinvector\"].iloc[i]), axis=None) for i in range(len(quora_best))]\nquora_best[\"joinvector_all\"]=[np.concatenate((np.array([quora_best[\"noun_count\"].iloc[i]]),quora_best[\"joinvector_2\"].iloc[i]), axis=None) for i in range(len(quora_best))]","9c5223d7":"quora_test[\"joinvector\"]=[np.concatenate((np.array([quora_test[\"characters\"].iloc[i]]),quora_test[\"vector\"].iloc[i]), axis=None) for i in range(len(quora_test))]\nquora_test[\"joinvector_2\"]=[np.concatenate((np.array([quora_test[\"words\"].iloc[i]]),quora_test[\"joinvector\"].iloc[i]), axis=None) for i in range(len(quora_test))]\nquora_test[\"joinvector_all\"]=[np.concatenate((np.array([quora_test[\"noun_count\"].iloc[i]]),quora_test[\"joinvector_2\"].iloc[i]), axis=None) for i in range(len(quora_test))]","7c9cbeee":"# quora_test.head(1)","d7579a65":"X_joinvec=quora_best[\"joinvector_all\"].tolist()","d0612ecb":"# from sklearn.cluster import DBSCAN\n# fit_model_joinvec=DBSCAN(eps=0.25,min_samples=10).fit(X_joinvec)\n# fitted_model_joinvec=fit_model_joinvec.labels_\n# quora_best['DBSCAN_Cluster_joinvec']=fitted_model_joinvec\n\n# quora_best.head()","b83c0a3c":"#quora_best['DBSCAN_Cluster_joinvec'].value_counts()","7e67018c":"# from sklearn.cluster import KMeans, MiniBatchKMeans\n# def calc_inertia(k):\n#         model_kmeans= KMeans(n_clusters=k, init='k-means++',verbose=1,random_state=42).fit(X_joinvec)\n#         return model_kmeans.inertia_,model_kmeans.labels_\n\n# inertias,labels_kmeans = [(k, calc_inertia(k)) for k in range(1, 21)]","4db807a4":"# from sklearn.cluster import KMeans, MiniBatchKMeans\n# n_clusters=50\n# model_MiniBatch = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', n_init=1,\n#                          init_size=500,\n#                          batch_size=500, verbose=1)\n\n# print (\"Clustering sparse data with %s\" % model_MiniBatch)\n# model_MiniBatch.fit(X_joinvec)\n# labels_MiniBatch = model_MiniBatch.labels_\n# print(\"done\")\n# # print(\"labels\", labels_MiniBatch)\n# # print(\"intertia:\", model_MiniBatch.inertia_)","682e86e8":"# quora_best['MiniBatch_Cluster_joinvec']=labels_MiniBatch\n# # quora_best.head()","e903c078":"# quora_best['MiniBatch_Cluster_joinvec'].value_counts().head(10)","bc5a1ff0":"# import matplotlib.pylab as plt\n# prob_groups = quora_best.groupby(\"MiniBatch_Cluster_joinvec\").MiniBatch_Cluster_joinvec.count()\n# prob_groups.plot(kind='hist')\n# plt.figure(figsize=(30,100))\n# plt.show()","565b57d3":"# pd.set_option('max_colwidth', 800)\n# cluster_columns=quora_best[[\"question_text\",\"target\",\"MiniBatch_Cluster_joinvec\"]]\n# random_cluster=cluster_columns[cluster_columns[\"MiniBatch_Cluster_joinvec\"]==38].sort_values(by='target',ascending=False)\n# random_cluster.head(10)","9b28dbeb":"# random_cluster.target.value_counts(normalize=True).plot(kind='bar', title='target', figsize=(10,5))\n# plt.show()","50a5f2ed":"Features = quora_best['joinvector_all']\n# Features2=quora_best['vector']","765365dd":"# FF=Features.tolist()","76cf0359":"# Features.shape","df748c2d":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n\nX_train, X_val, y_train, y_val = train_test_split(Features,quora_best['target'],\n                                                    train_size=0.7, random_state = 143, stratify=quora_best['target'])","3b7bfa34":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#evaluators:\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_score","63147742":"# # Lr_clf = LogisticRegression()\n# # X =  X_train.tolist()\n# # y = y_train\n\n# # Lr_clf.fit(X,y)\n# GB_clf=GradientBoostingClassifier()\n# X =  X_train.tolist()\n# y = y_train\n\n# GB_clf.fit(X,y)","115dd883":"# f1 = cross_val_score(GB_clf, X, y, scoring='f1', cv=5)\n# accuracy = cross_val_score(GB_clf, X, y, cv=5, scoring='accuracy')","a31c774a":"print('f1 score:{}\\nacurracy: {}'.format(round(f1.mean(),2),round(accuracy.mean(),2)))","e1131001":"#weights = [2,3]\n# param_grid = {'C': [0.5,10],\n#               'class_weight':[{0:1, 1:w} for w in weights]}\n# param_grid ={\"learning_rate\":(0.1,0.5),\n#                             'max_depth' : range(2,5,10),\n#                             'min_samples_split': range(2,5,10),\n#                             'min_samples_leaf' : range(2,5,10),\n#                             #'max_features':range(4,8),\n#                             \"subsample\":(0.5,0.8)} \nparam_grid ={\"learning_rate\":(0.5),\n                            'max_depth' : range(2),\n                            'min_samples_split': range(2),\n                            'min_samples_leaf' : range(2),\n                            #'max_features':range(4,8),\n                            \"subsample\":(0.8)} ","736eed68":"X_grid5 = X_train.tolist()\ny_grid5 = y_train","aecc8686":"#Lr_clf = LogisticRegression()\nGB_clf=GradientBoostingClassifier()\n# gs= GridSearchCV(estimator=Lr_clf, param_grid=param_grid, cv=2,scoring='f1') # verbose=15, n_jobs=-1\ngs= GridSearchCV(estimator=GB_clf, param_grid=param_grid, cv=2,scoring='f1') # verbose=15, n_jobs=-1\ngs.fit(X_grid5, y_grid5)\nbest_model=gs.best_estimator_ ","4b2a5ed3":"best_model","14a49395":"y_pred= best_model.predict(X_grid5)","7b41f627":"f1 = cross_val_score(best_model, X_grid5, y_grid5, scoring='f1', cv=2)\naccuracy = cross_val_score(best_model, X_grid5, y_grid5, cv=2, scoring='accuracy')\ncm = confusion_matrix(y_true=y_grid5, y_pred=y_pred)\n\nprint('f1-score:',round(f1.mean(),2),'\\naccuracy:',round(accuracy.mean(),2),'\\n______________\\n')\ncm = cm\nprint (classification_report(y_grid5, y_pred))\n\nprint('confusion matrix:')\npd.DataFrame(cm, \n             index=best_model.classes_, \n             columns=best_model.classes_)","379e5e44":"X1_val=X_val.tolist()\ny1_val=y_val","927a5319":"best_model.fit(X1_val,y1_val)","43f9971d":"f1 = cross_val_score(best_model, X1_val,y1_val, scoring='f1', cv=3)\nprint('f1 score of \"val\":', round(f1.mean(),2))","75109a4e":"# X_NN =  X_train.tolist()\n# y_NN = y_train\n","27dfd6bb":"# from keras.preprocessing import sequence\n# from keras.models import Sequential\n# from keras.layers import Dense, Embedding\n# from keras.layers import LSTM","fd2d59f1":"# max_features=50000\n# MAX_SEQUENCE_LENGTH=303","74c4a378":"from keras import backend as K","aa82f802":"# def f1(y_true, y_pred):\n#     '''\n#     metric from here \n#     https:\/\/stackoverflow.com\/questions\/43547402\/how-to-calculate-f1-macro-in-keras\n#     '''\n#     def recall(y_true, y_pred):\n#         \"\"\"Recall metric.\n\n#         Only computes a batch-wise average of recall.\n\n#         Computes the recall, a metric for multi-label classification of\n#         how many relevant items are selected.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#         recall = true_positives \/ (possible_positives + K.epsilon())\n#         return recall\n\n#     def precision(y_true, y_pred):\n#         \"\"\"Precision metric.\n\n#         Only computes a batch-wise average of precision.\n\n#         Computes the precision, a metric for multi-label classification of\n#         how many selected items are relevant.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#         precision = true_positives \/ (predicted_positives + K.epsilon())\n#         return precision\n#     precision = precision(y_true, y_pred)\n#     recall = recall(y_true, y_pred)\n#     return 2*((precision*recall)\/(precision+recall+K.epsilon()))","95081ee7":"from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints","b51de78e":"# TT=np.array(FF)","c4cdf1b6":"# # num_words = min(max_features, len(TT))\n# num_words=len(TT)","21b345fd":"# from keras.preprocessing.text import Tokenizer\n\n# t = Tokenizer()\n# t.fit_on_texts(quora_best[\"question_text_prep\"])\n\n# vocab_size = len(t.word_index) + 1\n# #vocab_size\n\n# encoded_docs = t.texts_to_sequences(quora_best[\"question_text_prep\"])","276739f8":"# from numpy import zeros\n# embedding_matrix = zeros((vocab_size, 300))\n# for word, i in t.word_index.items():\n#     embedding_vector = word_model.get(word)\n#     if embedding_vector is not None:\n#         embedding_matrix[i] = embedding_vector","5ec97b44":"# from keras.preprocessing.sequence import pad_sequences\n# max_length = 300\n# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# # print(padded_docs)","d4802608":"# embedding_matrix.shape","05ba034e":"# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n\n# X_train, X_val, y_train, y_val = train_test_split(padded_docs,quora_best['target'],\n#                                                     train_size=0.7, random_state = 143, stratify=quora_best['target'])","1fe67250":"# model = Sequential()\n# #model.add(Embedding(num_words,input_length=MAX_SEQUENCE_LENGTH,weights=[TT],trainable=False,output_dim=MAX_SEQUENCE_LENGTH))\n\n# model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n# model.add(Bidirectional(LSTM(128,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))) #\n# model.add(Bidirectional(LSTM(64,return_sequences=True))) # dropout=0.2, recurrent_dropout=0.2,\n# #model.add(Attention(MAX_SEQUENCE_LENGTH))\n# model.add(GlobalMaxPool1D())\n# model.add(Dense(16, activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(1, activation='sigmoid'))\n\n# model.compile(loss='binary_crossentropy',\n#               optimizer='adam',\n#               metrics=[f1])\n\n# print(model.summary())","695a46c7":"# batch_size = 512 #32","69737df8":"# X_train_list=X_train.tolist()\n# X_val_list=X_val.tolist()","f49ac0ca":"# observation_train = np.asarray(X_train_list)\n# observation_val = np.asarray(X_val_list)","7a4ab021":"# #weight={0:0.4,1:1}\n# weight={0:1,1:2}","de4b1a06":"# model.fit(X_train, y_train,\n#           batch_size=batch_size,\n#           epochs=2,\n#           validation_data=(X_val, y_val),\n#           class_weight=weight)\n","050af69f":"# model.fit(observation_train, y_train,\n#           batch_size=batch_size,\n#           epochs=10,\n#           validation_data=(observation_val, y_val),\n#           class_weight=weight)\n\n# score, f1_calc = model.evaluate(X_val, y_val,\n#                             batch_size=batch_size)\n# print('Test score:', score)\n# print('Test f1:', f1_calc)\n# #X_train, X_val, y_train, y_val","a7a13cee":"# y_pred_train = model.predict(X_train)","17aff40a":"# y_pred_val = model.predict(observation_val)","76ec1993":"from tqdm.auto import tqdm","dda1ce6b":"# def bestThresshold(y_train,y_pred_train):\n#     tmp = [0,0,0] # idx, cur, max\n#     delta = 0\n#     for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n#         tmp[1] = f1_score(y_train, np.array(y_pred_train)>tmp[0])\n#         if tmp[1] > tmp[2]:\n#             delta = tmp[0]\n#             tmp[2] = tmp[1]\n#     print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n#     return delta\n# delta = bestThresshold(y_train,y_pred_train)","07492c4d":"# t_test = Tokenizer()\n# t_test.fit_on_texts(quora_test[\"question_text_prep\"])\n\n# #vocab_size = len(t.word_index) + 1\n# #vocab_size\n\n# encoded_docs_test = t_test.texts_to_sequences(quora_test[\"question_text_prep\"])","7b217a63":"# max_length = 300\n# padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n# #print(padded_docs)","96e1c014":"# # test_Features = quora_test['vector'] #quora_test['joinvector_all']\n# # X_test_original=test_Features.tolist()\n# # observation_test = np.asarray(X_test_original)\n# # y_test_pred= model.predict(observation_test)\n# y_test_pred= model.predict(padded_docs_test)","827ad3e7":"test_Features = quora_test['joinvector_all']\nX_test_original=test_Features.tolist()\ny_test_pred= best_model.predict(X_test_original)","296e6dc6":"# y_test_pred","b797cfc7":"quora_test_tmp=quora_test","619eb114":"# delta=0.4","caad031a":"quora_test_tmp[\"pred\"]=y_test_pred #(y_test_pred > delta).astype(int) \nquora_test_tmp1 = quora_test_tmp[['qid','question_text','pred']]\nquora_test_tmp1[quora_test_tmp1['pred']==1].sample(10)","988e57a0":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nout_df = pd.DataFrame({\"qid\":sub[\"qid\"].values})\nout_df['prediction'] = y_test_pred\nout_df.to_csv(\"submission.csv\", index=False)","5a534ede":"# sub = pd.read_csv('..\/input\/sample_submission.csv')\n# out_df = pd.DataFrame({\"qid\":sub[\"qid\"].values})\n# out_df['prediction'] =y_test_pred# (y_test_pred > delta).astype(int) #y_test_pred\n# out_df.to_csv(\"submission.csv\", index=False)","06186aeb":"round(out_df['prediction'].value_counts(normalize =True),3)*100","76c78dce":"## 7.2 Base line classifier","83592431":"* #### Dataset has 3 columns and 1,306,122 questions.\n* #### No missing data.","1eec4a35":"## 6. Under Sampling\nData is very imballanced. Without Under sampling the classifier will see the target as **noise**  ","1f8556d3":"### We can see that the proportion of the insincire questions is similar and a little bit more than the original train file","63a43425":"## 7.1 Train test split","88431962":"### 3.3\tQuestions as List of Strings","37117c2f":"### 1.2 Load the Text Data","1f480f5f":"### 4.2.2 Trigram Distribution - Sincere vs. Insincere Questions","ac4dda2a":"## 5. Vectorising the Data","a077701a":"### 6.5\tClustering the data","427bd855":"### 2.2.1 Average Word and Character Count by Target","ff5ec916":"* ### We used  Logistic Classifier as our classification model. \n* ### We have reached the following results: f1_score: 0.8 vs. 0.79 , in the training and testing (val) sets, respectively. \n\n* ### The results indicate on the model's robustness as well as its ability to distiguish between insencere and sincere questions. \n### Later on we have used NN algorithems to predict the same data\n### Points for further exploration:\n### 1. Joining the Clustering feature to the combined vector\n### 2. Trying different Classification based on Neural Net algorithems with the combined vector","c6fa8f9d":"### important notice- cleaning data using stopwords, removing numbers etc. may leave the line\\vector empty and cause later errors. that is why we check if there are  lines with  zero length vector","f9e7d1de":"## 4. Visualization of the Clean Data","a3b5283b":"## Project Outline:\n#### 1. Import Libraries and Data\n#### 2. Raw Data \u201cDescriptive Statistics\u201d \n#### 3. Cleaning the Data and Pre-Processing\n#### 4.\tVisualization of the Clean Data\n#### 5.\tVectorising the Data\n#### 6.\tUnder Sampling and clustering\n#### 7.\tModelling and Results\n#### 8.\tConcluding Remarks and Work in Progress ","e57b9958":"#### Trying KMEAN both with gererating dynamic n_clusters and using a fixed number returned a memory issue that is why we decided to work with a ligther version of KMEANS- called MiniBatchKMeans","21b57ad7":" # 3. Cleaning the Data and Pre-Processing","25f99644":"* ### The most common trigram among sincere questions is \"advice would give\" (e.g., what advise would you give for ..?).\n* ### \"Kim Jong Un\" is the most common trigram among insincere questions (obviously all of these instances are statements\/opinions on \"Kim Jong Un\").\n* ### It seems that the most common bigrams and trigrams are revolved around different topics and therefore cluster analysis might be beneficial. ","1f43ac92":"### 5.2.1 Identify and Count Unknown Words","6b41514a":"### 2.2 Word and Character Count ","2a90834f":"### 7.4 Results After Grid Search","5720a45a":"## 2.\tRaw Data \u201cDescriptive Statistics\u201d","533a5676":"### 4.1.1 Word Cloud for Sincere and Insincere Questions","ae4114d1":"#### let's try KMEANS","102a5d39":"### 5.3\tVectorising the Data Using the \u201cword model\u201d","4e3194e0":"**### 6.3\tCheck Zero-Size Vectors","b625eaa8":"### performing DBSCAN","c69d25f1":"###  2.3 Additional Features - Verb and Noun Count ","7248c74f":"### 2.1 Target Distribution","c5418df6":"#### We believe that certain categories affect the feelings of the users more than others, that is why we want to understand the differenct clusters within the questions","ec878ea7":"* ### The most common bigram among sincere questions is \"best way\" (e.g., what is the best way...?).\n* ### Not surprisingly, \"Donald Trump\" is the most common bigram among insincere questions (obviously all of these instances are statements\/opinions on \"Donald Trump\").","d1b04108":"#### we will use our predefined joined vector in order to cluster our data","93eab3ab":"## 8. Concluding Remarks and Work in Progress ","cc3cb10b":"### 5.2.2 The Frequency of the Unknown Words","28b9c6f8":"> ### Sincere question examples: ","667a322b":"### We believe that the direction of the vector is affected from the variables we have found earlier. For this reason we decided to join all those variables with the word embedding vector and create a new, more reliable vector. from now on this vector will be our point of reference","ca3c27f9":"### 3.2 Punctuations and Stop-words","fc5a3d89":"## Doing the same for the test data","1c5b0810":"### let's inspect some clusters that have both 1 and 0 values","9d5894f6":" ###  3.1 Lower-casing","cfab61a1":"## 7.3  GridSearchCV for logistic regression classifier","a0b80a9d":"### 4.1\tUnigram Distribution \u2013 Sincere vs. Insincere Questions","3bff8795":"### => Data is Imbalanced ! \n### Target is only 6.2% of the data","2d96c642":"# Using NNclalification model","d0bed918":"## 1. Import Libraries and Data\n\n### 1.1 Import  DS Basics and NLP Specifics:","f0941f8e":"### Two general techniques for working with imballanced data:\n<br>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*H6XodlitlGDl9YdbwaZLMw.png\">\n\n### We chose random undersampling","1ee0f5a7":"### Let's check the proportions of the submission file","aa18c3c3":"### 6.1\tProceed with the Under Sample Data ","b51a1b69":"* #### running the code above we saw that DBSCAN cluster did not cluster efficiently","eeaffcf1":"#### ** this was done because word embedding needs the text to be represented as list of strings.","01538e65":"### 4.2\tBigram and Trigram Distribution - Sincere vs. Insincere Questions","ca5a8b6a":"### 5.1\tUsing the \"Glove\" Word Embedding","87b88a18":"## 7. Modelling and Results ","2eef9452":"#### We also tried  clustering based on tf-idf which did not perform that well","95f40d4e":"* ### Insincere questions have larger variance and median for both word and character counts.","e8824160":"### One option for future processing would be generating a combined vector which includes the cluster dummies","a52d3e04":"### 5.2\tHandling Unknown Words","b113a1a1":"* ### Insincere questions have on average longer word counts and character counts.\n* ### We will use these as features in our model.","66655072":"# Quora Insincere Questions Classification\n\n## By Koby Nimni, Noam Frank, and Ziv Bar Nahum","902fda00":"### Important note:  Undersampling was carried out while preserving a bias in the data.","f794919e":"## 7.5 Evaluate the model","cde6e659":"### 4.2.1 Bigram Distribution - Sincere vs. Insincere Questions"}}