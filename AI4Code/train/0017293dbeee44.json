{"cell_type":{"decb7dfb":"code","b674b59c":"code","c3048a46":"code","3b4e350d":"code","3d90fbe5":"code","d104cfa8":"code","fa3250eb":"code","d503059d":"code","c26c1bf5":"markdown","fb644f87":"markdown","d8e861e8":"markdown","dd7a0789":"markdown","1903da26":"markdown","ff3e780b":"markdown","011da9f3":"markdown","62c31282":"markdown","b2d063b6":"markdown","6888f249":"markdown"},"source":{"decb7dfb":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\nimport numpy as np\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import log_loss\nimport pickle\n\nimport warnings\n\nwarnings.simplefilter('ignore')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","b674b59c":"def preprocess(df):\n    df = df.copy()\n    df['cp_type_trt'] = np.where(df['cp_type'].values == 'trt_cp', 1, 0)\n    df['cp_type_ctl'] = np.where(df['cp_type'].values == 'trt_cp', 0, 1)\n    df['cp_dose_D1'] = np.where(df['cp_dose'].values == 'D1', 1, 0)\n    df['cp_dose_D2'] = np.where(df['cp_dose'].values == 'D1', 0, 1)\n    df['cp_time_24'] = np.where(df['cp_time'].values == 24, 1, 0)\n    df['cp_time_48'] = np.where(df['cp_time'].values == 48, 1, 0)\n    df['cp_time_72'] = np.where(df['cp_time'].values == 72, 1, 0)\n    \n    features_g = list(df.columns[4:776])\n    features_c = list(df.columns[776:876])\n\n    df['g_sum'] = df[features_g].sum(axis=1)\n    df['g_mean'] = df[features_g].mean(axis=1)\n    df['g_std'] = df[features_g].std(axis=1)\n    df['g_kurt'] = df[features_g].kurtosis(axis=1)\n    df['g_skew'] = df[features_g].skew(axis=1)\n    df['c_sum'] = df[features_c].sum(axis=1)\n    df['c_mean'] = df[features_c].mean(axis=1)\n    df['c_std'] = df[features_c].std(axis=1)\n    df['c_kurt'] = df[features_c].kurtosis(axis=1)\n    df['c_skew'] = df[features_c].skew(axis=1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis=1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis=1)\n    df['gc_std'] = df[features_g + features_c].std(axis=1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis=1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis=1)\n\n    features_c = list(df.columns[776:876])\n    for feature in features_c:\n        df[f'{feature}_squared'] = df[feature] ** 2\n\n    return df\n\ndef make_X(dt, dense_cols, cat_feats):\n    X = {\"dense\": dt[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_feats):\n        X[v] = dt[[v]].to_numpy()\n    return X\n\n\ndef get_data(ROOT = '..\/input\/lish-moa'):\n\n    cat_feat = ['cp_dose', 'cp_time']\n\n    train = pd.read_csv(f\"{ROOT}\/train_features.csv\")\n    test = pd.read_csv(f\"{ROOT}\/test_features.csv\")\n\n    GENES = [col for col in train.columns if col.startswith('g-')]\n    CELLS = [col for col in train.columns if col.startswith('c-')]\n\n    train[GENES] = (train[GENES].values + 10)\/20\n    train[CELLS] = (train[CELLS].values + 10) \/ 20\n    test[GENES] = (test[GENES].values + 10) \/ 20\n    test[CELLS] = (test[CELLS].values + 10) \/ 20\n\n    label = pd.read_csv(f\"{ROOT}\/train_targets_scored.csv\")\n    label_test = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\n    train = preprocess(train)\n    test = preprocess(test)\n\n    FE = list(train)\n    FE.remove('sig_id')\n    FE.remove('cp_type_ctl')\n    FE.remove('cp_type_trt')\n    FE.remove('cp_type')\n    for cat in cat_feat:\n        FE.remove(cat)\n\n    train = train.set_index('sig_id')\n    test = test.set_index('sig_id')\n    label = label.set_index('sig_id')\n    label_test = label_test.set_index('sig_id')\n\n    label = label.loc[train.index]\n    label_test = label_test.loc[test.index]\n\n    train = pd.concat([train, label], axis=1)\n    test = pd.concat([test, label_test], axis=1)\n\n    train['total'] = np.where(np.sum(train[list(label)].values, axis=1)>0, 0, 1)\n\n    return train, test, FE, cat_feat, list(label)","c3048a46":"class block(nn.Module):\n    def __init__(self, input_dim, keep_prob, hidden_dim):\n        super(block, self).__init__()\n\n        self.batch_norm = nn.BatchNorm1d(input_dim)\n        self.dropout = nn.Dropout(keep_prob)\n        self.dense = nn.Linear(input_dim, hidden_dim)\n\n\n    def forward(self, x):\n        x = self.batch_norm(x)\n        x = self.dropout(x)\n        x = self.dense(x)\n\n        return x\n\nclass MoaModel_V1(nn.Module):\n\n    def __init__(self, hidden_dim, n_cont, out_dim):\n        super(MoaModel_V1, self).__init__()\n\n        self.block1 = block(n_cont, n_cont\/2000, hidden_dim)\n        self.block2 = block(hidden_dim, 0.1, hidden_dim)\n        self.block7 = block(hidden_dim, 0.1, out_dim)\n\n    def forward(self, cont_data):\n\n        cont_data = cont_data.to(device)\n\n        x = cont_data\n        x = F.relu(self.block1(x))\n        x = F.relu(self.block2(x))\n        out = self.block7(x)\n\n        return out\n    \nclass MoaModel_Ensemble(nn.Module):\n\n    def __init__(self, hidden_dim, n_cont, out_dim, input_features):\n        super(MoaModel_Ensemble, self).__init__()\n\n        self.models = torch.nn.ModuleList()\n        self.input_features = input_features\n\n        for i in range(len(hidden_dim)):\n            self.models.append(MoaModel_V1(hidden_dim=int(hidden_dim[i]),\n                                        n_cont=int(n_cont[i]),\n                                        out_dim=out_dim))\n\n\n    def forward(self, cont_data):\n\n        cont_data = cont_data.to(device)\n\n        out = []\n\n        for i in range(len(self.input_features)):\n            temp = self.models[i](cont_data[:, self.input_features[i]]).unsqueeze(0)\n            out.append(temp)\n        out = torch.cat(out, dim=0)\n        #out = torch.mean(out, dim=0)\n\n        return out, out","3b4e350d":"class Loader:\n\n    def __init__(self, X, y, shuffle=True, batch_size=64):\n\n        self.X_cont = X\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n\n        batch = (xcont, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches","3d90fbe5":"## Early stopping algorithm\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\n    def __init__(self, patience=7, verbose=False, delta=0):\n\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model, path):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score - self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, path):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model, path)\n        self.val_loss_min = val_loss","d104cfa8":"## Model training\ndef model_training(model, train_loader, val_loader, loss_function,\n                   epochs,\n                   lr=0.001, patience=10,\n                   model_path='model.pth'):\n\n\n\n    if os.path.isfile(model_path):\n\n        # load the last checkpoint with the best model\n        model = torch.load(model_path)\n\n        return model\n\n    else:\n\n        # Loss and optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2,\n                                      factor=0.1, verbose=True)\n\n        criteria = loss_function\n\n        train_losses = []\n        val_losses = []\n        early_stopping = EarlyStopping(patience=patience, verbose=True)\n\n        for epoch in tqdm(range(epochs)):\n\n            train_loss, val_loss = 0, 0\n\n            # Training phase\n            model.train()\n            bar = tqdm(train_loader)\n\n            for i, (X_cont, y) in enumerate(bar):\n                preds, cont_data_x = model(X_cont)\n                \n                loss = 0\n                for i in range(preds.shape[0]):\n                    loss = loss + criteria(preds[i, :, :].flatten().unsqueeze(1), y.to(device).flatten().unsqueeze(1))\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                with torch.no_grad():\n                    train_loss += loss.item() \/ (len(train_loader))\n                    bar.set_description(f\"{loss.item():.3f}\")\n\n            # Validation phase\n            val_preds = []\n            true_y = []\n            model.eval()\n            with torch.no_grad():\n                for i, (X_cont, y) in enumerate(val_loader):\n                    preds, cont_data_x = model(X_cont)\n\n                    val_preds.append(torch.mean(preds, dim=0))\n                    true_y.append(y)\n\n                    loss = 0\n                    for i in range(preds.shape[0]):\n                        loss = loss + criteria(preds[i, :, :].flatten().unsqueeze(1),\n                                               y.to(device).flatten().unsqueeze(1))\n\n                    val_loss += loss.item() \/ (len(val_loader))\n\n                score = F.binary_cross_entropy(torch.sigmoid(torch.cat(val_preds, dim=0)), torch.cat(true_y, dim=0).to(device))\n\n            print(f\"[{'Val'}] Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val score: {score:.6f}\")\n\n            early_stopping(score, model, path=model_path)\n\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            scheduler.step(score)\n\n        torch.save(model, model_path)\n\n        model = torch.load(model_path)\n\n        return model\n\n\ndef fully_train(model, train_data, cont_features, labels, kf,\n                loss_function, hidden_dim, count_feats, input_features,\n                epochs, lr, patience,\n                kfold=5, seed=1, model_path_temp='model'):\n\n    models = []\n    val_loaders = []\n    val_set = []\n\n    for i, (train_index, test_index) in enumerate(kf.split(train_data, train_data[labels])):\n        print('[Fold %d\/%d]' % (i + 1, kfold))\n\n        model_path = \"%s_%s_%s.pth\" % (model_path_temp, i, seed)\n\n        X_train, valX = train_data.iloc[train_index], train_data.iloc[test_index]\n        X_train = X_train.loc[X_train['cp_type_ctl'] != 1, :]\n        X_val = valX.loc[valX['cp_type_ctl'] != 1, :]\n        y_train, y_valid = X_train[labels].values, X_val[labels].values\n\n        train_loader = Loader(X_train[cont_features].values, y_train, batch_size=128, shuffle=True)\n        val_loader = Loader(X_val[cont_features].values, y_valid, batch_size=128, shuffle=False)\n\n        model_temp = model(hidden_dim, count_feats, len(labels), input_features).to(device)\n        model_temp = model_training(model_temp, train_loader, val_loader, loss_function=loss_function,\n                               epochs=epochs,\n                               lr=lr, patience=patience,\n                               model_path=model_path)\n\n        models.append(model_temp)\n        val_loaders.append(val_loader)\n        val_set.append(valX)\n\n    return models, val_loaders, val_set","fa3250eb":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nnp.random.seed(42)\n\ntrain, test, FE, cat_feat, labels = get_data()\n\nNets = [MoaModel_Ensemble]\nNet_names = ['MoaModel_Ensemble']\n\n#Hyperparameters\nnumber_models_for_ensemble = 5  #select how many models do u want to make ensemble\n\n\n### random feature splits and hidden dimension selection\n### save these for testing phase\nif os.path.isfile('hidden_dim.pkl'):\n    with open('hidden_dim.pkl', 'rb') as handle:\n        hidden_dim = pickle.load(handle)\n\n    with open('count_features.pkl', 'rb') as handle:\n        count_features = pickle.load(handle)\n\n    with open('input_features.pkl', 'rb') as handle:\n        input_features = pickle.load(handle)\n\nelse:\n\n    hidden_dim = np.random.randint(256, 1024, number_models_for_ensemble)\n    count_features = np.random.randint(128, len(FE), number_models_for_ensemble)\n    input_features = []\n    for i in range(number_models_for_ensemble):\n        input_features.append(np.random.randint(0, len(FE), count_features[i]))\n\n    with open('hidden_dim.pkl', 'wb') as handle:\n        pickle.dump(hidden_dim, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open('count_features.pkl', 'wb') as handle:\n        pickle.dump(count_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open('input_features.pkl', 'wb') as handle:\n        pickle.dump(input_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\nkfold = 5\nseeds = [10] #We can extend seeds\n\nall_models = []\nval_loaders = []\nval_sets =[]\n\nfor seed in seeds:\n    #skf = KFold(n_splits=kfold, shuffle=True, random_state=seed)\n    skf = MultilabelStratifiedKFold(n_splits=kfold, shuffle=True, random_state=seed)\n\n\n    models, val_loader, val_set = fully_train(model=Nets[0], train_data = train,\n                                      cont_features = FE, labels=labels, kf=skf,\n                                     loss_function = nn.BCEWithLogitsLoss(),\n                                      hidden_dim=hidden_dim, count_feats=count_features,\n                                               input_features = input_features,\n                                      epochs=20, lr=0.001, patience=5,                                      #please change epoch\n                                      kfold=kfold, seed = seed, model_path_temp=Net_names[0])\n\n    all_models.append(models)\n    val_loaders.append(val_loader)\n    val_sets.append(val_set)\n\n\nscores = []\n\nfor kf in range(kfold):\n    for i in range(len(seeds)):\n        y_pred_avg = 0\n        temp_pred = []\n        temp_y = []\n        with torch.no_grad():\n            for X_cont, y in val_loaders[i][kf]:\n                preds, _ = all_models[i][kf](X_cont)\n                temp_pred.append(torch.mean(torch.sigmoid(preds), dim=0))\n                temp_y.append(y)\n\n        y_pred = torch.cat(temp_pred, dim=0).detach().cpu().numpy()\n        y_true = torch.cat(temp_y, dim=0).detach().cpu().numpy()\n        y_pred_avg = y_pred_avg + y_pred\n\n        score = 0\n        loss = []\n        sum_y = []\n        for k in range(y_true.shape[1]):\n            score_ = log_loss(y_true[:, k], y_pred[:, k].astype(float), labels=[0,1])\n            loss.append( score_ \/ y_true.shape[1])\n            sum_y.append(np.sum(y_true[:, k])\/(100*y_true.shape[0]))\n            score += score_ \/ y_true.shape[1]\n\n        print('Fold %s:' % kf, score)\n        scores.append(score)\n\nprint('#'*150)\nprint('CV average:', np.mean(scores[-kfold:]))\nprint('CV std:', np.std(scores[-kfold:]))\nprint('#'*150)","d503059d":"test_loader = Loader(test[FE].values, None, batch_size=256, shuffle=False)\nfull_test = np.zeros([test.shape[0], 206, len(seeds)*kfold])\n\nfor i in range(len(seeds)):\n    for kf in range(kfold):\n        temp_pred = []\n        temp_y = []\n        with torch.no_grad():\n            for X_cont, y in test_loader:\n                preds, _ = all_models[i][kf](X_cont)\n                temp_pred.append(torch.mean(torch.sigmoid(preds), dim=0))\n                temp_y.append(y)\n\n        full_test[:, :, i*kfold+kf] = torch.cat(temp_pred, dim=0).detach().cpu().numpy()\n\n#test = test[labels]\nprint(full_test.shape)\nprint(np.mean(full_test, axis=2).shape)\ntest[labels] = np.mean(full_test, axis=2)\ntest.loc[test['cp_type_ctl']==1, labels]=0\ntest[labels].to_csv('submission.csv')","c26c1bf5":"# Training","fb644f87":"# Print submission","d8e861e8":"# Data preprocess","dd7a0789":"**import libraries**","1903da26":"# Running phase","ff3e780b":"# Introduction","011da9f3":"# Data Loader","62c31282":"This notebook applies ensemble neural network model to this competition. \n\nDiscussion is here: https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195585\n\nPlease see version 3 of 5, that have result. \n\n![image.png](attachment:image.png)","b2d063b6":"# Random Ensemble Neural Network Model","6888f249":"# Early stopping algorithm"}}