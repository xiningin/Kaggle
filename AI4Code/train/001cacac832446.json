{"cell_type":{"86692bbb":"code","40009860":"code","cb58f9a3":"code","5f8df230":"code","b8a1ebf8":"code","b0629bf5":"code","ec3f9e00":"code","27249379":"code","cd48e3a4":"code","d4e6ba36":"code","18e04002":"code","bcd28cbe":"code","e611dd18":"code","4a53d275":"code","17445f54":"code","d5a69c71":"code","76d4fbb3":"code","10716516":"code","b517f5b7":"code","b7b446c4":"code","88967c82":"code","48a4f233":"code","8088e1a8":"code","d4cbe903":"code","441b68ea":"code","ec78516f":"markdown","66e6bc94":"markdown","44f1f720":"markdown","ea78a55f":"markdown","ca134e80":"markdown","ddddd695":"markdown","fa830d85":"markdown","c0794a61":"markdown","a13b74c8":"markdown","5868f3cb":"markdown"},"source":{"86692bbb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","40009860":"dataset = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","cb58f9a3":"dataset.head()","5f8df230":"import seaborn as sns\nsns.pairplot(dataset, kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\nplt.show()","b8a1ebf8":"correlation = dataset.corr()\nsns.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns)","b0629bf5":"dataset.corr(method ='pearson') ","ec3f9e00":"dataset.corr(method ='kendall') ","27249379":"dataset.corr(method ='spearman') ","cd48e3a4":"correlation[abs(correlation['DEATH_EVENT']) > 0.2]['DEATH_EVENT']","d4e6ba36":"final_df = dataset[['age', 'ejection_fraction', 'serum_creatinine', 'time', 'DEATH_EVENT']]","18e04002":"final_df.head()","bcd28cbe":"X = final_df.iloc[:, :-1].values\ny = final_df.iloc[:, -1].values","e611dd18":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","4a53d275":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","17445f54":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","d5a69c71":"# For checking the actual values vs predicted values side by side\n\n#y_pred = classifier.predict(X_test)\n#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","76d4fbb3":"## KFold Cross validation\n\nfrom sklearn.model_selection import cross_val_score\nsvm_cvs_acc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(svm_cvs_acc.mean()*100))","10716516":"from sklearn.ensemble import RandomForestClassifier\nrfclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nrfclassifier.fit(X_train, y_train)","b517f5b7":"rf_cvs_acc = cross_val_score(estimator = rfclassifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(rf_cvs_acc.mean()*100))","b7b446c4":"from sklearn.linear_model import LogisticRegression\nlrclassifier = LogisticRegression(random_state = 0)\nlrclassifier.fit(X_train, y_train)","88967c82":"lr_cvs_acc = cross_val_score(estimator = lrclassifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(lr_cvs_acc.mean()*100))","48a4f233":"from sklearn.naive_bayes import GaussianNB\nnbclassifier = GaussianNB()\nnbclassifier.fit(X_train, y_train)","8088e1a8":"nb_cvs_acc = cross_val_score(estimator = nbclassifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(nb_cvs_acc.mean()*100))","d4cbe903":"from sklearn.neighbors import KNeighborsClassifier\nknnclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknnclassifier.fit(X_train, y_train)","441b68ea":"from sklearn.model_selection import cross_val_score\nknn_cvs_acc = cross_val_score(estimator = knnclassifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(knn_cvs_acc.mean()*100))","ec78516f":"## Making a new dataset using the values with highest correlation (taking average of all 3 methods)","66e6bc94":"## Checking the correlation of features ","44f1f720":"### The maximum accuracy is achieved by Logistic Regression with 85.69%","ea78a55f":"## Logistic Regression","ca134e80":"## Support Vector Machine","ddddd695":"\n\n---\n\n\n","fa830d85":"## Naive Bayes","c0794a61":"## K Nearest Neighbors","a13b74c8":"## Random Forest ","5868f3cb":"## Importing the libraries and diving the dataset into training and testing set"}}