{"cell_type":{"dc613fe9":"code","24b174a6":"code","5096ce7b":"code","8984e7a9":"code","86ed7261":"code","1af52edc":"code","1433de7b":"code","a8a8a06c":"code","c132a662":"code","350f004f":"code","7998d3ad":"code","17b512b2":"code","0da64d86":"code","80cd40a3":"code","22291546":"code","bcb6f07f":"code","b97c9519":"code","84c79e88":"code","c3ee1d8b":"code","b749b3e1":"code","79f96906":"code","65e8b528":"code","fd5be420":"code","18ada6ca":"code","8b50f9f0":"code","5631c800":"code","1b73de4a":"code","98f52f56":"code","badc3ef9":"code","6ff0ea4f":"code","853ee180":"code","8da823f8":"code","746cbf48":"code","8ed64248":"code","392b6e4b":"code","87df2028":"code","87d91cfc":"code","752c978b":"code","d398e23b":"code","c31957ad":"code","3700ea30":"code","e2c01a24":"code","cbb6e348":"markdown","7b998005":"markdown"},"source":{"dc613fe9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24b174a6":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","5096ce7b":"train=pd.read_csv(\"..\/input\/titanic_train.csv\")\ntrain.head()\n#SibSp represent number of siblings or spouses\n#Parch represents number of parents\/children on board\n#Fare represents how much passengers pay for the ticket\n#Embarked which places passengers embarked into abroad","8984e7a9":"train.isnull() #firstly we need check the missing values and .isnull() return True for missing values and False for non missing","86ed7261":"# We can see better the missing value with a heatmap\nplt.figure(figsize=(20,15))\nsns.heatmap(train.isnull(),cmap=\"ocean\")\n# we see that there are missing values only in the Age and Cabin columns in our dataset","1af52edc":"sns.set_style(\"darkgrid\")#Set the aesthetic style of the plots like darkgrid, whitegrid, dark, white, ticks\nplt.figure(figsize=(10,5))\nsns.countplot(train[\"Survived\"])# it seem that there 350 survivor versus 550 non survivors","1433de7b":"plt.figure(figsize=(10,5))\nsns.countplot(train[\"Survived\"],hue=\"Sex\",data=train)\n#It seems that the percentage of females among survived is much more higher than males","a8a8a06c":"plt.figure(figsize=(10,5))\nsns.countplot(train[\"Survived\"],hue=\"Pclass\",data=train,palette=\"RdBu_r\")\n#It seems that the percentage of class 3 or the lowes class has the highest deaths than others","c132a662":"plt.figure(figsize=(10,5))\nsns.countplot(x=\"SibSp\",data=train,hue=\"Sex\")\n#It seems that most of the passenger do not have children or spouse,particularly among males","350f004f":"#I can get more interactive plot\nimport cufflinks as cf\ncf.go_offline()\ntrain[\"Age\"].iplot(kind=\"hist\")","7998d3ad":"train[\"Survived\"].value_counts()","17b512b2":"len(train[(train[\"Survived\"] == 1) & (train[\"Pclass\"] == 3) & (train[\"Age\"] <15)])","0da64d86":"len(train[(train[\"Age\"] < 15) & (train[\"Pclass\"] == 3)])","80cd40a3":"len(train[(train[\"Survived\"] == 1) & (train[\"Pclass\"] == 3)])","22291546":"len(train[train[\"Pclass\"] == 3])","bcb6f07f":"print(22\/54,97\/448) #here we understand that %40 of children in the third class survived while it is only %22 in other ages","b97c9519":"len(train[(train[\"Survived\"] == 1) & (train[\"Pclass\"] == 1) & (train[\"Age\"] <15)])","84c79e88":"len(train[(train[\"Age\"] < 15) & (train[\"Pclass\"] == 1)])","c3ee1d8b":"len(train[(train[\"Survived\"] == 1) & (train[\"Pclass\"] == 1)])","b749b3e1":"len(train[train[\"Pclass\"] == 1])","79f96906":"print(4\/5, 131\/211)\n#when it comes to the first class,\n#here we understand that %80 of children in the third class survived while it is only %62 in other ages","65e8b528":"#In order to start mechine learning algorithm, we need to transform our data into an acceptable form \nplt.figure(figsize=(15,15))\nsns.heatmap(train.isnull(),cmap=\"viridis\")","fd5be420":"# we can fill the missing values in the Age column with the median age \n#and get ride of Cabin column because there are many missing values there\nplt.figure(figsize=(15,15))\nsns.boxplot(x=\"Pclass\",y=\"Age\",data=train)\n#instead of using the mean of the age column, we can use separate means by every class by looking from the boxplot\n#becasue the mean of every class is different","18ada6ca":"def age_mean(col): # here I create a function in order to assign the mean of every class to the missing values\n    Age=col[0]\n    Pclass=col[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","8b50f9f0":"train[\"Age\"]=train[[\"Age\",\"Pclass\"]].apply(age_mean,axis=1)","5631c800":"plt.figure(figsize=(15,15))\nsns.heatmap(train.isnull(),cmap=\"coolwarm\")\n#As it is seen below there is not any null value in the Age column","1b73de4a":"#Because there are alot of missing values in the Cabin column, it is better to drop it\ntrain.drop(\"Cabin\",axis=1,inplace=True)\ntrain.head()","98f52f56":"plt.figure(figsize=(15,15))\nsns.heatmap(train.isnull(),cmap=\"coolwarm\")","badc3ef9":"#Now there is just one missing value in the Embark column and we can just drop it\ntrain.dropna(inplace=True)","6ff0ea4f":"plt.figure(figsize=(15,15))\nsns.heatmap(train.isnull(),cmap=\"winter\") #Now there is not any missing value in the data","853ee180":"#Before applying logistic regression algorithm, we need to convert categorical values into dummy variable as 0 or 1\n#Otherwise the algorithm will not be able to directly take these features as inputs\n#we use pandas.get_dummies() method in order to convert categorical variables into numeric dummy ones\nSex=pd.get_dummies(train[\"Sex\"],drop_first=True)# we need to use drop_first=True in order to get 1 for only one gender\nSex.head()","8da823f8":"Embark=pd.get_dummies(train[\"Embarked\"],drop_first=True)\nEmbark.head()","746cbf48":"#Now we will add these values into our dataframe by using .concat() method\ntrain=pd.concat([train,Sex,Embark],axis=1)\ntrain.head()","8ed64248":"# we dont need Sex and Embarked columns anymore because we have replacement values for them for the algorithm\n# We do not need also Name and Ticket column because they are not useful for our purpose and algorithm\ntrain.drop([\"Name\",\"Sex\",\"Ticket\",\"Embarked\"],axis=1,inplace=True)\ntrain.head() \n#All the data is numerical ready for the algorithm","392b6e4b":"#PassegerId is just an index , so it should also be dropped\ntrain.drop(\"PassengerId\",axis=1,inplace=True)\ntrain.head() #Now the data is perfectly ready for our algorithm","87df2028":"X=train[[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"male\",\"Q\",\"S\"]]\ny=train[\"Survived\"]","87d91cfc":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)","752c978b":"#After splitting data we import our model\nfrom sklearn.linear_model import LogisticRegression\nlogmodel=LogisticRegression() # we create an instance of the model","d398e23b":"#The next step is to train the model\nlogmodel.fit(X_train,y_train)","c31957ad":"predictions=logmodel.predict(X_test)\npredictions","3700ea30":"#The next step is to evaluate our model\n#Sklearn has very good classification report to use\nfrom sklearn.metrics import classification_report # this return the model's accuracy.precision etc.\nprint(classification_report(y_test,predictions))","e2c01a24":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,predictions) #It sound our model works good\n#TP=147\n#FN=16\n#FP=30\n#TN=74","cbb6e348":"Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, <b>y<\/b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n\nLogistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function \ud835\udf0e:\n\n$$\n\u210e_\\theta(\ud835\udc65) = \\sigma({\\theta^TX}) =  \\frac {e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +...)}}{1 + e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +\\cdots)}}\n$$\nOr:\n$$\nProbabilityOfaClass_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}} \n$$\n\nIn this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or [logistic function](http:\/\/en.wikipedia.org\/wiki\/Logistic_function?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork-20718538&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork-20718538&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork-20718538&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork-20718538&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ), also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n\nSo, briefly, Logistic Regression passes the input through the logistic\/sigmoid but then treats the result as a probability:\n\n<img\nsrc=\"https:\/\/ibm.box.com\/shared\/static\/kgv9alcghmjcv97op4d6onkyxevk23b1.png\" width=\"400\" align=\"center\">\n\nThe objective of **Logistic Regression** algorithm, is to find the best parameters \u03b8, for $\u210e_\\theta(\ud835\udc65)$ = $\\sigma({\\theta^TX})$, in such a way that the model best predicts the class of each case.\n","7b998005":"Based on the count of each section, we can calculate precision and recall of each label:\n\n-   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n-   **Recall** is true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\nSo, we can calculate precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nAnd finally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 0.72 in our case.\n"}}