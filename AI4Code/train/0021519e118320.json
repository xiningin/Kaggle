{"cell_type":{"aba3a7e0":"code","addc709b":"code","6b27903b":"code","504f8cd2":"code","ba9d42e0":"code","a9193441":"code","b771bbdc":"code","0428629f":"code","effee3f7":"code","232fb42a":"code","a0724cd0":"code","cd1b6e51":"code","98981c22":"code","705fff5d":"code","04ded854":"code","dc9ca6bf":"code","3d1f9cac":"code","6f4abd09":"code","e4286a79":"code","5bc90a45":"code","4c1b1ab3":"code","8e0a30c8":"code","526418c9":"code","f9e72b68":"code","7a8cdc18":"code","7ab37dec":"code","29f1b082":"code","adaee773":"code","efaa5d2d":"code","c1dcd2f2":"markdown","c8b0a246":"markdown","69d72572":"markdown","9d86c554":"markdown","de577b07":"markdown","ba3eb592":"markdown","76584c02":"markdown","36126794":"markdown","43d5b048":"markdown","371fd152":"markdown","bafe2457":"markdown","b2b971c7":"markdown","37951f46":"markdown"},"source":{"aba3a7e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","addc709b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV","6b27903b":"data_path = \"..\/input\/sms-spam-collection-dataset\/spam.csv\"","504f8cd2":"with open(data_path, 'r', encoding=\"ISO-8859-1\") as f:\n  lines = f.readlines()\n  print(lines[:5])","ba9d42e0":"data = pd.read_csv(data_path, encoding=\"ISO-8859-1\", usecols=['v1', 'v2'])","a9193441":"data.head()","b771bbdc":"sns.countplot(x='v1', data=data)\nplt.show() \nprint(f\"% of Spam Obervations {data[data['v1']=='spam'].shape[0]\/data.shape[0]}\")\nprint(f\"% of Non Spam Obervations {data[data['v1']=='ham'].shape[0]\/data.shape[0]}\")","0428629f":"def count_punctuation(x):\n  x = re.sub(r\" \", \"\", x)\n  lst_punc = re.findall(r'[^A-Za-z0-9.,\/]', x)\n  return len(lst_punc)\n\ndef count_capitals(x):\n  x = re.sub(r\" \", \"\", x)\n  lst_caps = re.findall(r'^[A-Z][A-Z]+', x)\n  #print(lst_caps)\n  return len(lst_caps)","effee3f7":"num_spam = data[data['v1']=='spam'].shape[0]\nnum_ham = data[data['v1']=='ham'].shape[0]\n\nnew_features = pd.DataFrame(data={\"Punct Count\":data['v2'].apply(lambda x:count_punctuation(x)),\n                                  \"Cap Count\":data['v2'].apply(lambda x:count_capitals(x)),\n                                  \"Text Len\":data['v2'].apply(lambda x:len(x))})\nnew_features['target'] = data['v1']\naverage_spam_punct_count = new_features[new_features['target']=='spam']['Punct Count'].sum()\/num_spam\naverage_ham_punct_count = new_features[new_features['target']=='ham']['Punct Count'].sum()\/num_ham\nprint(\"Average Number of Punctuations for Spam: {:.3f}\".format(average_spam_punct_count))\nprint(\"Average Number of Punctuations for Ham: {:.3f}\".format(average_ham_punct_count), end='\\n\\n')\n\naverage_spam_cap_count = new_features[new_features['target']=='spam']['Cap Count'].sum()\/num_spam\naverage_ham_cap_count = new_features[new_features['target']=='ham']['Cap Count'].sum()\/num_ham\nprint(\"Average Number of Capitals for Spam: {:.3f}\".format(average_spam_cap_count))\nprint(\"Average Number of Capitals for Ham: {:.3f}\".format(average_ham_cap_count), end='\\n\\n')\n\naverage_spam_len = new_features[new_features['target']=='spam']['Text Len'].sum()\/num_spam\naverage_ham_len = new_features[new_features['target']=='ham']['Text Len'].sum()\/num_ham\nprint(\"Average Text Length for Spam: {:.3f}\".format(average_spam_len))\nprint(\"Average Text Length for Ham: {:.3f}\".format(average_ham_len))\n","232fb42a":"new_features.head()","a0724cd0":"# Normalizing numerical data\n\nmm1 = MinMaxScaler()\nmm2 = MinMaxScaler()\nmm3 = MinMaxScaler()\nnew_features['Punct Count'] = mm1.fit_transform(new_features['Punct Count'].to_numpy().reshape((-1, 1)))\nnew_features['Cap Count'] = mm2.fit_transform(new_features['Cap Count'].to_numpy().reshape((-1, 1)))\nnew_features['Text Len'] = mm3.fit_transform(new_features['Text Len'].to_numpy().reshape((-1, 1)))","cd1b6e51":"new_features.head()","98981c22":"X = new_features.iloc[:, :3].to_numpy()\nlb = LabelEncoder() \nY = lb.fit_transform(new_features['target'].tolist())","705fff5d":"lb.classes_","04ded854":"skf = StratifiedKFold(n_splits=2)\nfor train_index, test_index in skf.split(X, Y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = Y[train_index], Y[test_index]","dc9ca6bf":"lr = LogisticRegression()\nlr.fit(X_train, y_train)","3d1f9cac":"preds = lr.predict(X_test)\npred_probs = lr.predict_proba(X_test)","6f4abd09":"print(classification_report(y_test, preds))\nprint(\"AUC Score for Logistic Regression: {:.3f}\".format(roc_auc_score(y_test, pred_probs[:, 1])))","e4286a79":"tfidf = TfidfVectorizer(ngram_range=(2, 3), max_df=600, min_df=5)\ntfidf.fit(data['v2'].tolist())","5bc90a45":"tfidf_feats = tfidf.transform(data['v2'].tolist())\nprint(tfidf_feats.shape)\n\n# Merge our numerical Text features(Punctuation\/Capital Counts) with Tfidf features\n\nX_tf = tfidf_feats.toarray()\nX_all = np.concatenate([X_tf, X], axis=1)\nprint(X_all.shape)","4c1b1ab3":"skf = StratifiedKFold(n_splits=2)\nfor train_index, test_index in skf.split(X_all, Y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X_all[train_index], X_all[test_index]\n    y_train, y_test = Y[train_index], Y[test_index]","8e0a30c8":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\npreds = lr.predict(X_test)\npred_probs = lr.predict_proba(X_test)","526418c9":"print(classification_report(y_test, preds))\nprint(\"AUC Score: {:.3f}\".format(roc_auc_score(y_test, pred_probs[:, 1])))","f9e72b68":"rf = RandomForestClassifier(n_estimators=300, max_depth=5)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\npred_probs = rf.predict_proba(X_test)\nprint(classification_report(y_test, preds))","7a8cdc18":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)\npred_probs = knn.predict_proba(X_test)\nprint(classification_report(y_test, preds))","7ab37dec":"mlp = MLPClassifier(hidden_layer_sizes=(128, 256, 512))\nmlp.fit(X_train, y_train)\npreds = mlp.predict(X_test)\npred_probs = mlp.predict_proba(X_test)\nprint(classification_report(y_test, preds))","29f1b082":"# Optimizing hyperparameters for Logisitic Regression classifier\n\nlr_param_dict = {\"C\":[0.001, 0.01, 0.1, 1, 10],\n                 \"max_iter\": [50, 100, 200, 500]\n                 }\nscores = ['precision', 'recall']\n\n# Using boiler plate code from Scikit-learn documentation\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(\n        LogisticRegression(), lr_param_dict, scoring='%s_macro' % score\n    )\n    clf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()","adaee773":"vc = VotingClassifier([('lr', LogisticRegression(C=10, max_iter=500)), ('mlp', MLPClassifier(hidden_layer_sizes=(128, 256, 512)))], voting='soft')\nvc.fit(X_train, y_train)\npreds = vc.predict(X_test)\npred_probs = vc.predict_proba(X_test)\nprint(classification_report(y_test, preds))","efaa5d2d":"print(\"AUC Score for Ensemble: {:.3f}\".format(roc_auc_score(y_test, pred_probs[:, 1])))","c1dcd2f2":"Constructive criticism\/suggesstions are welcome.\n\nHappy Kaggling!","c8b0a246":"## Trying a few more classifiers and optimizing hyperparameters","69d72572":"Best params for Logistic Regression classifier - C : 10, max_iter : 500\n\nSkipping for MLP for now.","9d86c554":"## Let's take two of our better classifiers - MLP and Logistic Regeression (We can also go for Knn) and try to find best hyperparameters for them before ensembing","de577b07":"# EDA","ba3eb592":"## Using TF-IDF with Logistic Classifier","76584c02":"So, we have improved our recall a fare bit, optimizing MLP hyperparameters should improve it further. We can also try KNN\/SVC for further analysis.\n\nFuture Work - \n* Try a few more classifiers.\n* Analyze important features\/ feature selection.\n* Error Analysis","36126794":"## Analyzing Text","43d5b048":"## We definitely need to improve the recall for Spam messages and precision for ham messages can also be improved ","371fd152":"## Using just the numerical attributes from text","bafe2457":"## Target Variable","b2b971c7":"# Predictive Modelling","37951f46":"## Ensembling"}}