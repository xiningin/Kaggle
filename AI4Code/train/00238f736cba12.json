{"cell_type":{"207c26d0":"code","a2319359":"code","fae04adc":"code","36b1cb9a":"code","0b025aaf":"code","c682d9cd":"code","ef74a1a3":"code","de9bc87f":"code","ab47fc7c":"code","adf1cf54":"code","a9b3542d":"code","f1f2adc2":"code","c59e0437":"code","9de52cea":"code","8107282e":"code","fa3bebdd":"code","e50d4b36":"code","9d3cc6ac":"code","9674397c":"code","851e2e21":"code","b56f5c76":"code","28fd68c8":"markdown","fea3aa12":"markdown","ca9fa42b":"markdown","25ed7125":"markdown"},"source":{"207c26d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('Setup complete.')","a2319359":"# Load the training dataset and look at the first few records.\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","fae04adc":"# Look at summary statistics for the training dataset.\ntrain_df.describe()","36b1cb9a":"train_df['Cabin'] = train_df['Cabin'].fillna('Unknown')\ntrain_df.groupby(['Cabin']).apply(lambda x: x['Survived'].sum()\/len(x))","0b025aaf":"train_df['n_Cabins'] = train_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ntrain_df.groupby(['n_Cabins', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","c682d9cd":"train_df['CabinPrefix'] = train_df.apply(lambda row: row['Cabin'][0], axis=1)\ntrain_df.groupby(['CabinPrefix', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","ef74a1a3":"def split_ticket(ticket):\n    # special case. a few Tickets are only LINE\n    if ticket == 'LINE':\n        return pd.Series(['LINE', 0])\n    \n    parts = ticket.split()  # split the ticket on whitespace\n    if len(parts) == 1:\n        return pd.Series([\"NO_PREFIX\", int(parts[0].strip(' .'))])\n    elif len(parts) == 2:\n        return pd.Series([parts[0].strip(' .'), int(parts[1].strip(' .'))])\n    else:\n        # special case. One Ticket has a prefix separated by a space.\n        return pd.Series([parts[0].strip(' .') + parts[2].strip(' .'), int(parts[2].strip(' .'))])\n\ntrain_df[['Ticket_Prefix', 'Ticket_NUM']] = train_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ntrain_df.groupby(['Ticket_Prefix']).apply(lambda x: x['Survived'].sum()\/len(x))","de9bc87f":"train_df['Title'] = train_df.apply(lambda row: row['Name'].split()[1], axis=1)\ntrain_df.groupby(['Title']).apply(lambda x: x['Survived'].sum()\/len(x))","ab47fc7c":"from sklearn.preprocessing import LabelEncoder\n\n# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title']\nX = train_df.loc[: ,features]\ny = train_df['Survived']\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\nX['Sex'] = X['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\nX['Embarked'] = X['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the average age.\nX['Age'] = X['Age'].fillna(28.0)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(X['Embarked'])\nX['Embarked'] = emb_encoder.transform(X['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(X['CabinPrefix'])\nX['CabinPrefix'] = cab_encoder.transform(X['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(X['Ticket_Prefix'])\nX['Ticket_Prefix'] = tic_encoder.transform(X['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(X['Title'])\nX['Title'] = title_encoder.transform(X['Title'])\nX","adf1cf54":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = DecisionTreeClassifier(random_state=0)\n\nparam_dict = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': range(1, 10),\n    'min_samples_split': range(2, 10),\n    'min_samples_leaf': range(1, 5)\n}\n\ngrid = GridSearchCV(model, param_grid=param_dict, cv=5, verbose=1, n_jobs=1)\ngrid.fit(X, y)","a9b3542d":"grid.best_params_","f1f2adc2":"grid.best_estimator_","c59e0437":"grid.best_score_","9de52cea":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(test_df.shape)\ntest_df.head()","8107282e":"# Set aside the test PassengerId values for later.\npassenger_ids = test_df['PassengerId']\n\n# Add a fake value for Survived that we'll remove again later.\ntest_df['Survived'] = np.NaN\nprint(test_df.shape)\ntest_df.head()","fa3bebdd":"combo_df = train_df.append(test_df)\nprint(combo_df.shape)","e50d4b36":"# Add the engineered features to the combined dataset\ncombo_df['Cabin'] = combo_df['Cabin'].fillna('Unknown')\ncombo_df['n_Cabins'] = combo_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ncombo_df['CabinPrefix'] = combo_df.apply(lambda row: row['Cabin'][0], axis=1)\ncombo_df[['Ticket_Prefix', 'Ticket_NUM']] = combo_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ncombo_df['Title'] = combo_df.apply(lambda row: row['Name'].split()[1], axis=1)\ncombo_df.describe()","9d3cc6ac":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title', 'Survived']\ncombo_df = combo_df[features]\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\ncombo_df['Sex'] = combo_df['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\ncombo_df['Embarked'] = combo_df['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the median age.\ncombo_df['Age'] = combo_df['Age'].fillna(28.0)\n\n# There was one missing Fare. Fill it with the median fare.\ncombo_df['Fare'] = combo_df['Fare'].fillna(14.45)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(combo_df['Embarked'])\ncombo_df['Embarked'] = emb_encoder.transform(combo_df['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(combo_df['CabinPrefix'])\ncombo_df['CabinPrefix'] = cab_encoder.transform(combo_df['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(combo_df['Ticket_Prefix'])\ncombo_df['Ticket_Prefix'] = tic_encoder.transform(combo_df['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(combo_df['Title'])\ncombo_df['Title'] = title_encoder.transform(combo_df['Title'])\n\ncombo_df.head()","9674397c":"# Split the combined dataframe back into train and test data.\ntrain_df = combo_df[combo_df['Survived'].notna()]\nprint(train_df.shape)\ntrain_df.describe()","851e2e21":"test_df = combo_df[combo_df['Survived'].isna()]\ntest_df = test_df.drop('Survived', axis=1)\nprint(test_df.shape)\ntest_df.describe()","b56f5c76":"# Split training features and target\ntrain_y = train_df['Survived']\ntrain_X = train_df.drop('Survived', axis=1)\n\n# create and fit the model\nmodel = DecisionTreeClassifier(max_depth=9, min_samples_split=3, random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_dt_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_dt_predictions_df.to_csv('final_DT_predictions.csv', index=False)","28fd68c8":"## Final submission","fea3aa12":"Data exploration and feature engineering based on [Titanic - Decision Tree \/ RF \/ GB](https:\/\/www.kaggle.com\/bcruise\/titanic-decision-tree-rf-gb). This notebook builds on that by showing how to do parameter tuning using grid search to get better results with a decision tree model.","ca9fa42b":"## Grid Search\n\nUse [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) from scikit-learn to find the best parameters from an initial set. The un-tuned decision tree model in the original notebook gave us 81% accuracy on the train\/validation split using the same features. That dropped all the way to 71% when the same model was used on the full training set to create a submission. Hopefully parameter tuning will improve upon that score.","25ed7125":"## Feature Engineering"}}