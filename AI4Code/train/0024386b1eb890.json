{"cell_type":{"00cfb396":"code","a1bc4f5f":"code","4309b942":"code","49683439":"code","7644a113":"code","8bd050b7":"code","b007d04d":"code","3f07a28f":"code","a8609ac8":"code","77b3a1fe":"code","1215ed8d":"markdown","89b313d8":"markdown","a93a4be6":"markdown","a12ec55b":"markdown","4ba0829b":"markdown","a2ca5ef6":"markdown","8233e7ce":"markdown","9acc86b7":"markdown","dada60c2":"markdown","4494babe":"markdown","fa747709":"markdown","90819893":"markdown","6135e008":"markdown","64dba623":"markdown"},"source":{"00cfb396":"!pip install pytorch-tabnet","a1bc4f5f":"FLAG_LOCAL = False # Flag to run in kaggle notebook or in jupyter server :)\n\nKAGGLE_PATH = \"\/kaggle\/input\/tabular-playground-series-feb-2021\/\"\nLOCAL_PATH = \"\/home\/rapela\/Downloads\/kaggle\/tps_feb\/input\/\"\nPATH = (LOCAL_PATH if FLAG_LOCAL else KAGGLE_PATH)\n\nTRAIN_PATH = PATH + \"train.csv\"\nTEST_PATH = PATH + \"test.csv\"\nSUBMISSION = PATH + \"sample_submission.csv\"\nSUBMISSION_OUTPUT = \"submission.csv\"\n\nprint(TRAIN_PATH)\nprint(TEST_PATH)\nprint(SUBMISSION)\n\nNUM_FOLDS = 2\nSEED = 42","4309b942":"## TabNet Parameters\nMAX_EPOCH = 1000\nN_D = 8 \nN_A = 8 \nN_STEPS = 2\nGAMMA = 1.1\nLAMBDA_SPARSE = 0\nOPT_LR = 1e-3\nOPT_WEIGHT_DECAY = 1e-5\nOPT_MOMENTUM = 0.9\nMASK_TYPE = \"entmax\"\nSCHEDULER_MIN_LR = 1e-6\nSCHEDULER_FACTOR = 0.9\nDEVICE_NAME = \"cuda\"\n\nBATCH_SIZE = 512","49683439":"import torch\nfrom torch import nn\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)","7644a113":"train = pd.read_csv(TRAIN_PATH, index_col='id')\ntest = pd.read_csv(TEST_PATH, index_col='id')\nsubmission = pd.read_csv(SUBMISSION, index_col='id')\ntarget = train.pop(\"target\")\ntarget = target.values","8bd050b7":"for c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        \n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)","b007d04d":"columns = test.columns","3f07a28f":"tabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS, gamma=GAMMA,\n                     lambda_sparse=LAMBDA_SPARSE, optimizer_fn=torch.optim.SGD,\n                     optimizer_params=dict(lr=OPT_LR, weight_decay=OPT_WEIGHT_DECAY, momentum=OPT_MOMENTUM),\n                     mask_type=MASK_TYPE,\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=20,\n                                           min_lr=SCHEDULER_MIN_LR,\n                                           factor=SCHEDULER_FACTOR,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     device_name=DEVICE_NAME,\n                     seed=SEED\n                     )","a8609ac8":"train_oof = np.zeros((len(train)))\ntest_preds = 0\n\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n\n    print(f'Fold {f}')\n    train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n\n    train_target, val_target = target[train_ind], target[val_ind]\n\n    print(train_df.shape, train_target.shape)\n    print(val_df.shape, val_target.shape)\n\n    train_target=train_target.reshape(-1,1)\n    val_target=val_target.reshape(-1,1)\n\n    train_df      = train_df.to_numpy()\n    train_target      = train_target.reshape(-1, 1)\n\n    val_df = val_df.to_numpy()\n    val_target = val_target.reshape(-1, 1)\n\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=train_df,\n              y_train=train_target,\n              eval_set=[(val_df, val_target)],\n              eval_name = [\"val\"],\n              eval_metric = ['mse'],#[\"logits_ll\"],\n              max_epochs=MAX_EPOCH, #20\n              patience=20, batch_size=BATCH_SIZE,\n              num_workers=1, drop_last=False)#,\n\n    temp_oof = model.predict(val_df)\n    train_oof[val_ind] = temp_oof.reshape(-1)\n    temp_test = model.predict(test.to_numpy())\n\n    test_preds += temp_test\/NUM_FOLDS     \n\n    print(mean_squared_error(temp_oof, val_target, squared=False))\n\n\n","77b3a1fe":"submission['target'] = test_preds\nsubmission.to_csv(SUBMISSION_OUTPUT)","1215ed8d":"## Create TabNet Params Dictionary","89b313d8":"## Import Data","a93a4be6":"## TabNet Parameters","a12ec55b":"## Credits\n\n* [TabNet Paper](https:\/\/arxiv.org\/pdf\/1908.07442.pdf)\n* [TabNet PyTorch GitHub](https:\/\/github.com\/dreamquark-ai\/tabnet)\n* [Kaggle Notebook TabNet Regressor](https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer?scriptVersionId=44853427)\n* [Tunguz CV Notebook](https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap)","4ba0829b":"## If it was useful for you please comment! Your feedback is really important! ","a2ca5ef6":"## Install PyTorch TabNet","8233e7ce":"## Hello Kagglers, in this Notebook we will try to use TabNet Regressor to solve TPS Playground problem! I added some credits in the final of this notebook :D","9acc86b7":"## Run Kfold with TabNet Regressor","dada60c2":"## Some ideas:\n\n* Fork the notebook and try to change some parameters and play with the model...\n* Try to add some preprocessing...\n* Try other encoding than label encoding...\n* Ensemble your model with others to make it more diversity...","4494babe":"## Imports Libs","fa747709":"## Submit your output csv","90819893":"## TabNet: Attentive Interpretable Tabular Learning\n\n\"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.\" \n\n[TabNet Paper](https:\/\/arxiv.org\/abs\/1908.07442)\n","6135e008":"## Label Encoding","64dba623":"## Parameters"}}