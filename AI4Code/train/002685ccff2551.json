{"cell_type":{"c445af64":"code","b0778a99":"code","d449bd9a":"code","c8ac1301":"code","647b0ca1":"code","d8a73e7d":"code","402e5e8b":"code","a8b145dd":"code","b88a638a":"code","6cae3313":"code","ee593f95":"code","0d456c63":"code","1b68f813":"code","600b103f":"markdown","d66e73c5":"markdown","db85a7cb":"markdown","8c68a385":"markdown","02b08056":"markdown","945b473c":"markdown","962379c5":"markdown","514be6cf":"markdown","288220f6":"markdown","72a71675":"markdown","8eb297fb":"markdown"},"source":{"c445af64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport IPython\nimport IPython.display\nimport PIL\nimport time\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b0778a99":"DATA = Path('..\/input')\nCSV_TRN_CURATED = DATA\/'train_curated.csv'\nCSV_TRN_NOISY = DATA\/'train_noisy.csv'\nCSV_SUBMISSION = DATA\/'sample_submission.csv'\nTRN_CURATED = DATA\/'train_curated'\nTRN_NOISY = DATA\/'train_noisy'\nTEST = DATA\/'test'\n\nWORK = Path('work')\nIMG_TRN_CURATED = WORK\/'image\/trn_curated'\nIMG_TRN_NOISY = WORK\/'image\/trn_curated'\nIMG_TEST = WORK\/'image\/test'\nfor folder in [WORK, IMG_TRN_CURATED, IMG_TRN_NOISY, IMG_TEST]: \n    Path(folder).mkdir(exist_ok=True, parents=True)\n\ndf = pd.read_csv(CSV_TRN_CURATED)\ntest_df = pd.read_csv(CSV_SUBMISSION)","d449bd9a":"# Special thanks to https:\/\/github.com\/makinacorpus\/easydict\/blob\/master\/easydict\/__init__.py\nclass EasyDict(dict):\n    \"\"\"\n    Get attributes\n    >>> d = EasyDict({'foo':3})\n    >>> d['foo']\n    3\n    >>> d.foo\n    3\n    >>> d.bar\n    Traceback (most recent call last):\n    ...\n    AttributeError: 'EasyDict' object has no attribute 'bar'\n    Works recursively\n    >>> d = EasyDict({'foo':3, 'bar':{'x':1, 'y':2}})\n    >>> isinstance(d.bar, dict)\n    True\n    >>> d.bar.x\n    1\n    Bullet-proof\n    >>> EasyDict({})\n    {}\n    >>> EasyDict(d={})\n    {}\n    >>> EasyDict(None)\n    {}\n    >>> d = {'a': 1}\n    >>> EasyDict(**d)\n    {'a': 1}\n    Set attributes\n    >>> d = EasyDict()\n    >>> d.foo = 3\n    >>> d.foo\n    3\n    >>> d.bar = {'prop': 'value'}\n    >>> d.bar.prop\n    'value'\n    >>> d\n    {'foo': 3, 'bar': {'prop': 'value'}}\n    >>> d.bar.prop = 'newer'\n    >>> d.bar.prop\n    'newer'\n    Values extraction\n    >>> d = EasyDict({'foo':0, 'bar':[{'x':1, 'y':2}, {'x':3, 'y':4}]})\n    >>> isinstance(d.bar, list)\n    True\n    >>> from operator import attrgetter\n    >>> map(attrgetter('x'), d.bar)\n    [1, 3]\n    >>> map(attrgetter('y'), d.bar)\n    [2, 4]\n    >>> d = EasyDict()\n    >>> d.keys()\n    []\n    >>> d = EasyDict(foo=3, bar=dict(x=1, y=2))\n    >>> d.foo\n    3\n    >>> d.bar.x\n    1\n    Still like a dict though\n    >>> o = EasyDict({'clean':True})\n    >>> o.items()\n    [('clean', True)]\n    And like a class\n    >>> class Flower(EasyDict):\n    ...     power = 1\n    ...\n    >>> f = Flower()\n    >>> f.power\n    1\n    >>> f = Flower({'height': 12})\n    >>> f.height\n    12\n    >>> f['power']\n    1\n    >>> sorted(f.keys())\n    ['height', 'power']\n    update and pop items\n    >>> d = EasyDict(a=1, b='2')\n    >>> e = EasyDict(c=3.0, a=9.0)\n    >>> d.update(e)\n    >>> d.c\n    3.0\n    >>> d['c']\n    3.0\n    >>> d.get('c')\n    3.0\n    >>> d.update(a=4, b=4)\n    >>> d.b\n    4\n    >>> d.pop('a')\n    4\n    >>> d.a\n    Traceback (most recent call last):\n    ...\n    AttributeError: 'EasyDict' object has no attribute 'a'\n    \"\"\"\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","c8ac1301":"import librosa\nimport librosa.display\n\ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()\n\ndef read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n    x = read_audio(conf, pathname, trim_long_data)\n    mels = audio_to_melspectrogram(conf, x)\n    if debug_display:\n        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n        show_melspectrogram(conf, mels)\n    return mels\n\n\nconf = EasyDict()\nconf.sampling_rate = 44100\nconf.duration = 2\nconf.hop_length = 347 # to make time steps 128\nconf.fmin = 20\nconf.fmax = conf.sampling_rate \/\/ 2\nconf.n_mels = 128\nconf.n_fft = conf.n_mels * 20\n\nconf.samples = conf.sampling_rate * conf.duration","647b0ca1":"def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    Xstd = (X - mean) \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef convert_wav_to_image(df, source, img_dest):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        x = read_as_melspectrogram(conf, source\/str(row.fname), trim_long_data=False)\n        x_color = mono_to_color(x)\n        X.append(x_color)\n    return df, X\n\ndf2, X_train = convert_wav_to_image(df, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)\ntest_df2, X_test = convert_wav_to_image(test_df, source=TEST, img_dest=IMG_TEST)","d8a73e7d":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.data import *\nimport random\n\nCUR_X_FILES, CUR_X = list(df.fname.values), X_train\n\ndef open_fat2019_image(fn, convert_mode, after_open)->Image:\n    # open\n    idx = CUR_X_FILES.index(fn.split('\/')[-1])\n    x = PIL.Image.fromarray(CUR_X[idx])\n    # crop\n    time_dim, base_dim = x.size\n    crop_x = random.randint(0, time_dim - base_dim)\n    x = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n    # standardize\n    return Image(pil2tensor(x, np.float32).div_(255))\n\nvision.data.open_image = open_fat2019_image","402e5e8b":"tfms = get_transforms(do_flip=True, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\nsrc = (ImageList.from_csv(WORK\/'image', Path('..\/..\/')\/CSV_TRN_CURATED, folder='trn_curated')\n       .split_by_rand_pct(0.2)\n       .label_from_df(label_delim=',')\n)\ndata = (src.transform(tfms, size=128)\n        .databunch(bs=64).normalize(imagenet_stats)\n)","a8b145dd":"data.show_batch(3)","b88a638a":"f_score = partial(fbeta, thresh=0.2)\nlearn = cnn_learner(data, models.resnet18, pretrained=False, metrics=[f_score])\nlearn.unfreeze()","6cae3313":"learn.fit_one_cycle(5, slice(1e-6, 1e-1))","ee593f95":"learn.fit_one_cycle(100, slice(1e-6, 1e-2))","0d456c63":"# Thanks to https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson6-pets-more.ipynb\nfrom fastai.callbacks.hooks import *\n\ndef visualize_cnn_by_cam(learn, data_index):\n    x, _y = learn.data.valid_ds[data_index]\n    y = _y.data\n    if not isinstance(y, (list, np.ndarray)): # single label -> one hot encoding\n        y = np.eye(learn.data.valid_ds.c)[y]\n\n    m = learn.model.eval()\n    xb,_ = learn.data.one_item(x)\n    xb_im = Image(learn.data.denorm(xb)[0])\n    xb = xb.cuda()\n\n    def hooked_backward(cat):\n        with hook_output(m[0]) as hook_a: \n            with hook_output(m[0], grad=True) as hook_g:\n                preds = m(xb)\n                preds[0,int(cat)].backward()\n        return hook_a,hook_g\n    def show_heatmap(img, hm, label):\n        _,axs = plt.subplots(1, 2)\n        axs[0].set_title(label)\n        img.show(axs[0])\n        axs[1].set_title(f'CAM of {label}')\n        img.show(axs[1])\n        axs[1].imshow(hm, alpha=0.6, extent=(0,img.shape[1],img.shape[1],0),\n                      interpolation='bilinear', cmap='magma');\n        plt.show()\n\n    for y_i in np.where(y > 0)[0]:\n        hook_a,hook_g = hooked_backward(cat=y_i)\n        acts = hook_a.stored[0].cpu()\n        grad = hook_g.stored[0][0].cpu()\n        grad_chan = grad.mean(1).mean(1)\n        mult = (acts*grad_chan[...,None,None]).mean(0)\n        show_heatmap(img=xb_im, hm=mult, label=str(learn.data.valid_ds.y[data_index]))\n\nfor idx in range(10):\n    visualize_cnn_by_cam(learn, idx)","1b68f813":"# https:\/\/discuss.pytorch.org\/t\/how-to-visualize-the-actual-convolution-filters-in-cnn\/13850\nfrom sklearn.preprocessing import minmax_scale\n\ndef visualize_first_layer(learn):\n    conv1 = list(learn.model.children())[0][0]\n    weights = conv1.weight.data.cpu().numpy()\n    weights_shape = weights.shape\n    weights = minmax_scale(weights.ravel()).reshape(weights_shape)\n    fig, axes = plt.subplots(8, 8, figsize=(8,8))\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(np.rollaxis(weights[i], 0, 3))\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\nvisualize_first_layer(learn)","600b103f":"### Audio conversion to 2D","d66e73c5":"## Multi-label classification\n\n- Almost following fast.ai course: https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson3-planet.ipynb\n- But `pretrained=False`","db85a7cb":"### Preparation goes on","8c68a385":"# Visualize Activation Map\n\nLet's check where model is looking...","02b08056":"### Thanks to EasyDict\n\nThanks to https:\/\/github.com\/makinacorpus\/easydict","945b473c":"## Is this model Sound?\n\nIf heat map is on the sound part, it would be good. But you might find some shows heat map is on the empty spaces, it would be showing that model is classfying because the silence is similar to training samples...","962379c5":"### Making 2D mel-spectrogram data as 2D 3ch images","514be6cf":"## Custom `open_image` for fast.ai library to load data from memory","288220f6":"# Verifying Trained Model Is _Sound_\n\nUsing pre-trained model weights which is trained with external dataset like ImageNet is not allowed in this competition.\nThis is painful for transfer learning approach using pre-trained image classifier CNN, because filters trained well with large dataset is the key for their high performance.\n\nInstead of allowing use of external pre-trained weights, this competition provides us with two training sets, one is manually labeled and the other is noisy auto labeled but large one.\nSo it is one of challenges for the user of image classifier transfer learning, to train good base model.\n\nTraining models with fairly large dataset is kind of simple, and then we want to verify how much the performance is.\nBesides basic verification by checking numerical metrics of train\/valid data, we can check CNN models visually.\nClass Activation Map (CAM) is one of visual inspection techniques, that shows where model is looking at to classify image, and it can be visualized as heatmap.\n\nThis kernel uses fast.ai library, then thanks to fast.ai course notebook \"Lesson 6: pets revisited\", CAM example is available. We are basically using it.\n\n### Something new in this kernel\n- CAM visualization for fast.ai is summarized as function.\n- And it is multi-label ready.\n","72a71675":"## What about the First Layer?\n\nHere's quoting from (CS231n Convolutional Neural Networks for Visual Recognition)[http:\/\/cs231n.github.io\/understanding-cnn\/],\n\n_\"The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasn\u2019t been trained for long enough, or possibly a very low regularization strength that may have led to overfitting.\"_\n\nSo let's visualize it also.","8eb297fb":"Oooops, too noisy! orz"}}