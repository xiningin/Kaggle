{"cell_type":{"48716f4c":"code","0823d03c":"code","0d2f52d8":"code","f43790ac":"code","0c0d4bdb":"code","cfd059d2":"code","4a0a07d6":"code","47c64160":"code","7b4881a9":"code","bbae1b39":"code","ce9f8ee0":"code","38b4db06":"code","b85c2127":"code","1f5dc425":"code","a1f0d4c7":"code","20f0f1a9":"code","0b5f41fc":"code","7f05452b":"code","6f53918a":"code","61557d16":"code","ff3e5b0f":"code","2810e9a8":"code","10c68422":"code","deff5d97":"code","f1e6b166":"code","9b4b1eee":"code","117659a3":"code","b29e735c":"code","24746e23":"code","ce87f04f":"code","a56734f6":"code","bc856e28":"code","d372dbf0":"code","427c73f1":"code","3455ba55":"code","c4810a3d":"code","dee88f34":"code","c13bef0e":"code","ae506a1f":"code","3952ace8":"code","b2fc391b":"markdown","856a652e":"markdown","0e6e1ed8":"markdown","1373ce9e":"markdown","de843f60":"markdown","45ad1f64":"markdown","d6fb2a94":"markdown","e4924834":"markdown","ce92f118":"markdown","98a54f4b":"markdown","ab2a5d7b":"markdown","5fe23c3e":"markdown","4915b2eb":"markdown","62f7252e":"markdown","7c7badcc":"markdown","63cc4ead":"markdown","63bc60ee":"markdown","23360c29":"markdown","ceb99ccb":"markdown","8da3ac1d":"markdown","3ed6f004":"markdown","404ef4ab":"markdown","58e85802":"markdown","c35a9e80":"markdown"},"source":{"48716f4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0823d03c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt","0d2f52d8":"data = pd.read_csv('\/kaggle\/input\/news-dataset-18920\/result_final.csv')","f43790ac":"data.shape","0c0d4bdb":"data.head()","cfd059d2":"data = data.drop_duplicates(subset=None, keep='first', inplace=False)","4a0a07d6":"data.shape","47c64160":"data.insert(0,'id',range(0,data.shape[0]))\ndata","7b4881a9":"ds = data[['date','title','text','link']]","bbae1b39":"ds.shape","ce9f8ee0":"ds = ds.dropna()","38b4db06":"ds = ds.drop_duplicates(subset=None, keep='first', inplace=False)","b85c2127":"ds.insert(0,'id',range(0,ds.shape[0]))","1f5dc425":"ds.shape","a1f0d4c7":"ds.head()","20f0f1a9":"from nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport string\nimport random","0b5f41fc":"# Function for removing NonAscii characters\n#def _removeNonAscii(s):\n#    return \"\".join(i for i in s if  ord(i)<128)\n\n# Function for converting into lower case\ndef make_lower_case(text):\n    return text.lower()\n\n# Function for removing stop words\ndef remove_stop_words(text):\n    text = text.split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]\n    texts = [w for w in text if w.isalpha()]\n    texts = \" \".join(texts)\n    return texts\n\n# Function for removing punctuation\ndef remove_punctuation(text):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text = tokenizer.tokenize(text)\n    text = \" \".join(text)\n    return text\n\n# Function for removing the html tags\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\n# Applying all the functions in description and storing as a cleaned_desc\n#ds['cleaned_desc'] = ds['text'].apply(_removeNonAscii)\nds['cleaned_desc'] = ds['text'].apply(func = make_lower_case)\nds['cleaned_desc'] = ds.cleaned_desc.apply(func = remove_stop_words)\nds['cleaned_desc'] = ds.cleaned_desc.apply(func=remove_punctuation)\nds['cleaned_desc'] = ds.cleaned_desc.apply(func=remove_html)","7f05452b":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n## analyzer -- to select individual words# default \n## max_df[0.0,1.0] - used to ignore words with frequency more than 0.8 these words can be useless words as these words may appear only once and may not have a significant meaning\n## min_df -- similar reason as the above one. \n## use_idfbool, default=True  -- Enable inverse-document-frequency reweighting.\n\ntf = TfidfVectorizer(analyzer='word',stop_words='english',max_df=0.8,min_df=0.0,use_idf=True,ngram_range=(1,3))\ntfidf_matrix = tf.fit_transform(ds['cleaned_desc'])","6f53918a":"pd.DataFrame(tfidf_matrix.toarray(), columns=tf.get_feature_names())","61557d16":"cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)","ff3e5b0f":"results = {}\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n    results[row['id']] = similar_items[1:]\nprint('done!')\n","2810e9a8":"similar_indices[:100]","10c68422":"\ndef item(id):\n    return ds.loc[ds['id'] == id]['title'].tolist()[0].split(' - ')[0]\n\n# Just reads the results out of the dictionary.\ndef recommend(item_id, num):\n    print(\"Recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\")\n    print(\"-------\")\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(\"Recommended : \" + item(rec[1]) + \" (score:\" + str(rec[0]) + \")\",end='\\n\\n')\n\nrecommend(item_id=10, num=15)","deff5d97":"cosine_similarities","f1e6b166":"def recomendation(idx,no_of_news_article):\n    #get similarity values with other articles\n    similarity_score = list(enumerate(cosine_similarities[idx]))\n    similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n    # Get the scores of the n most similar news articles. Ignore the first movie.\n    similarity_score = similarity_score[1:no_of_news_article+1]\n    \n    print(\"Article Read -- \" + ds['title'].iloc[idx] +\" link --\"+ ds['link'].iloc[idx])\n    print(\" ---------------------------------------------------------- \")\n    news_indices = [i[0] for i in similarity_score]\n    for i in range(len(news_indices)):\n        print(\"Recomendation \"+ str(i+1)+\" --- \" +str(news_indices[i])+\"(IDX)  \"+str(ds['date'].iloc[news_indices[i]])+\" : \"+\n              ds['title'].iloc[news_indices[i]] +\" || Link --\"+ ds['link'].iloc[news_indices[i]] +\" score -- \"+ str(similarity_score[i][1]))\n        print()","9b4b1eee":"idx=3  #min_df=0.2 shape 1496 rows \u00d7 31 columns\nno_of_news_article=10\nrecomendation(idx,no_of_news_article)","117659a3":"idx=3   #min_df=0.1  shape 1496 rows \u00d7 144 columns\nno_of_news_article=10\nrecomendation(idx,no_of_news_article)","b29e735c":"idx=3  #min_df=0.0 shape 1496 rows \u00d7 588777 columns\nno_of_news_article=10\nrecomendation(idx,no_of_news_article)","24746e23":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n## analyzer -- to select individual words# default \n## max_df[0.0,1.0] - used to ignore words with frequency more than 0.8 these words can be useless words as these words may appear only once and may not have a significant meaning\n## min_df -- similar reason as the above one. \n## use_idfbool, default=True  -- Enable inverse-document-frequency reweighting.\n\ntf = TfidfVectorizer(analyzer='word',stop_words='english',max_df=0.8,min_df=0.0,use_idf=True,ngram_range=(1,3))\ntfidf_matrix = tf.fit_transform(ds['cleaned_desc'])","ce87f04f":"cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)","a56734f6":"idx=3  #min_df=0.0 shape 1496 rows \u00d7 31 columns\nno_of_news_article=10\nrecomendation(idx,no_of_news_article)","bc856e28":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n## analyzer -- to select individual words# default \n## max_df[0.0,1.0] - used to ignore words with frequency more than 0.8 these words can be useless words as these words may appear only once and may not have a significant meaning\n## min_df -- similar reason as the above one. \n## use_idfbool, default=True  -- Enable inverse-document-frequency re-weighting.\n\ntf = TfidfVectorizer(analyzer='word',stop_words='english',max_df=0.8,min_df=0.1,use_idf=False,ngram_range=(1,3))\ntfidf_matrix = tf.fit_transform(ds['cleaned_desc'])\ncosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)","d372dbf0":"pd.DataFrame(tfidf_matrix.toarray(), columns=tf.get_feature_names())","427c73f1":"idx=3  #min_df=0.0 shape 1496 rows \u00d7 31 columns\nno_of_news_article=10\nrecomendation(idx,no_of_news_article)\n\n* seeing a significant change in the similarity score as it has not been L2 regulerised. ","3455ba55":"from sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\n%time km.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","c4810a3d":"ds.insert(2,'cluster',clusters)\n\n#ds.insert(0,'id',range(0,ds.shape[0]))","dee88f34":"ds.head()","c13bef0e":"ds['cluster'].value_counts()","ae506a1f":"from scipy.cluster.hierarchy import ward, dendrogram\nfrom sklearn.metrics.pairwise import cosine_similarity\ndist = 1 - cosine_similarity(tfidf_matrix)\n\nlinkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n\nfig, ax = plt.subplots(figsize=(15, 200)) # set size\nax = dendrogram(linkage_matrix)\n\nplt.tick_params(\\\n    axis= 'x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    top='off',         # ticks along the top edge are off\n    labelbottom='off',\n               width=10000)\n\nplt.tight_layout() #show plot with tight layout\n","3952ace8":"## Implementing Topic modelling -- LSH ","b2fc391b":"* ### Data cleaning and selecting few columns we will be requiring for the recomendation--","856a652e":"![Inner-blog-image.png](attachment:Inner-blog-image.png)","0e6e1ed8":"### This is how a tfidf vector looks like. ","1373ce9e":"### A recommender system has to decide between two methods for information delivery when providing the user with recommendations:\n* ### Exploitation. The system chooses documents similar to those for which the user has already expressed a preference.\n* ### Exploration. The system chooses documents where the user profile does not provide evidence to predict the user\u2019s reaction.<br><br>\n### <b>We are going to use <u>Exploitation method <\/u><\/b>","de843f60":"## Test 2 - when min_df=0.1 shape 1496 rows \u00d7 144 columns","45ad1f64":"## Vector Space Model\n* ### In this model, each item is stored as a vector of its attributes (which are also vectors) in an n-dimensional space, and the angles between the vectors are calculated to determine the similarity between the vectors.\n","d6fb2a94":"### Step 1: Choosing your data\n* ### The first thing to do when starting a data science project is to decide what data sets are going to be relevant to your problem. This stage of the project is referred to as data selection and is highly important because if you choose the wrong data source, you won\u2019t get successful performance.","e4924834":"## Test 1 - when min_df=0.2 shape 1496 rows \u00d7 31 columns","ce92f118":"## So what have we done till now -- \n* ### We have implemented vector space method to map the documents where the tfidf -> first maps doc X words matrix .. \n* ### Then  we have found cosine similarity between the documents similar to  Singular Vector Decomposition where we have found the relation of documents with other's on the bases of text the other documents have and recommended the top n articles he may like","98a54f4b":"### Presenting users with the most relevant information is an important task for any product to fulfill. To do this properly, you need to be able to extract their preferences from your raw data. Here\u2019s a framework for you to start doing that.","ab2a5d7b":"![1_Q4xQoV8k_7S7xB-NfvFdrw.png](attachment:1_Q4xQoV8k_7S7xB-NfvFdrw.png)","5fe23c3e":"## Applying all the functions in text column and storing as a cleaned_desc","4915b2eb":"## Conclusion -- \n\n### We have succesfully created a recomendation system but this may not the that efficient for a very large corpous thus we will try to implement a probablistic model for the same.\n\n### Note: we wil not be using Latent semantic Analysis as we do not want to find any kighlighting words in the corpous rather find a relation between one so instead of LSA we are going  for topic modelling for a change to see if it works.. \n### We are goning to see LDA and LSH for this process.","62f7252e":"## How to Build a Content-Based Recommender System For Your Product ??","7c7badcc":"![1_LWoRop9T6hC7zhi32UxhCQ.png](attachment:1_LWoRop9T6hC7zhi32UxhCQ.png)","63cc4ead":"## Importing Liberaries","63bc60ee":"## Test 3 - when min_df=0.0 shape 1496 rows \u00d7 588777 columns","23360c29":"## Recommender systems\n### There are two main data selection methods:\n\n* ### Collaborative-filtering: In collaborative-filtering items are recommended, for example movies, based on how similar your user profile is to other users\u2019, finds the users that are most similar to you and then recommends items that they have shown a preference for. This method suffers from the so-called cold-start problem: If there is a new movie, no-one else would\u2019ve yet liked or watched it, so you\u2019re not going to have this in your list of recommended movies, even if you\u2019d love it.\n\n* ### Content-based filtering: This method uses attributes of the content to recommend similar content. It doesn\u2019t have a cold-start problem because it works through attributes or tags of the content, such as actors, genres or directors, so that new movies can be recommended right away.\n\n### Based on this, I\u2019m going to introduce you to content-based filtering for a movie recommender system. I\u2019ll use Python as the programming language for the implementation.","ceb99ccb":"### So When the size of the corpous is very large the similarity score decreases but predicitions are much better. And the score decreses because of regulersation there for nid_df - 0.0 was better in this case. \n### However this is not true every time. ","8da3ac1d":"## Step 2: Encoding your data\n### There are a number of popular encoding schemes but the main ones are:\n\n* ### One-hot encoding\n* ### Term frequency\u2013inverse document frequency (TF-IDF) encoding\n* ### Word embeddings\n### For our example, we will use the term frequency\u2013inverse document frequency (TF-IDF) encoding scheme.","3ed6f004":"* ### The method of calculating the user\u2019s likes \/ dislikes \/ measures is calculated by taking the cosine of the angle between the user profile vector (Ui ) and the document vector; or in our case, the angle between two document vectors.\n* ### The ultimate reason behind using cosine is that the value of cosine will increase as the angle between vectors with decreases, which signifies more similarity.","404ef4ab":"![1_3Ig7VSgscBzXaYa0Q-UM1w.png](attachment:1_3Ig7VSgscBzXaYa0Q-UM1w.png)","58e85802":"\n### The advantage of TF-IDF encoding is that it will weigh a term (a tag for a movie in our example) according to the importance of the term within the document: The more frequently the term appears, the larger its weight will be. At the same time, it weighs the item inversely to the frequency of this term across the entire dataset: It will emphasise terms that are relatively rare occurrences in the general dataset but of importance to the specific content at hand.","c35a9e80":"### <i>Now, we have a representation of every item in terms of its description. Next, we need to calculate the relevance or similarity of one document to another.<\/i>"}}