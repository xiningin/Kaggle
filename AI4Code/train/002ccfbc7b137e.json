{"cell_type":{"17d35c42":"code","4d2a8e97":"code","6bf42a04":"code","58ae6b94":"code","99168a7d":"code","c979429c":"code","8edc76df":"code","1321270e":"code","50125bd6":"code","e3228e7a":"code","cb420bf0":"code","200fc708":"code","2566fddd":"code","cdf44e0f":"code","a50cabb8":"code","653fdd97":"code","62d57444":"code","b2766c9c":"code","a050b4f8":"code","0913f708":"code","5470c206":"code","b3797f88":"code","368844cf":"code","e537ecb4":"code","671ac0ea":"code","aa91b950":"code","e480012f":"code","811acc52":"code","463c7692":"code","dc56a8e1":"code","19a7903c":"code","c1ef0ac6":"code","41684f91":"code","d57d32d4":"code","df8a30d4":"code","48248dcd":"code","67028c8a":"code","fc598b66":"code","a3c127d0":"code","6e6a52a1":"code","3919030b":"code","8b307223":"code","53955865":"code","7f19c0dd":"code","e19368fc":"code","42c050f0":"code","88ff8646":"code","1d3a1a4c":"code","d457c9dc":"markdown","89b919d9":"markdown","c4a02d54":"markdown"},"source":{"17d35c42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4d2a8e97":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm,trange\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\n\nfrom tensorflow.keras import  layers, models,  applications\n\n","6bf42a04":"df_train0 = pd.read_json('..\/input\/deepfake\/metadata0.json')\ndf_train1 = pd.read_json('..\/input\/deepfake\/metadata1.json')\ndf_train2 = pd.read_json('..\/input\/deepfake\/metadata2.json')\ndf_train3 = pd.read_json('..\/input\/deepfake\/metadata3.json')\ndf_train4 = pd.read_json('..\/input\/deepfake\/metadata4.json')\ndf_train5 = pd.read_json('..\/input\/deepfake\/metadata5.json')\ndf_train6 = pd.read_json('..\/input\/deepfake\/metadata6.json')\ndf_train7 = pd.read_json('..\/input\/deepfake\/metadata7.json')\ndf_train8 = pd.read_json('..\/input\/deepfake\/metadata8.json')\ndf_train9 = pd.read_json('..\/input\/deepfake\/metadata9.json')\ndf_train10 = pd.read_json('..\/input\/deepfake\/metadata10.json')\ndf_train11 = pd.read_json('..\/input\/deepfake\/metadata11.json')\ndf_train12 = pd.read_json('..\/input\/deepfake\/metadata12.json')\ndf_train13 = pd.read_json('..\/input\/deepfake\/metadata13.json')\ndf_train14 = pd.read_json('..\/input\/deepfake\/metadata14.json')\ndf_train15 = pd.read_json('..\/input\/deepfake\/metadata15.json')\ndf_train16 = pd.read_json('..\/input\/deepfake\/metadata16.json')\ndf_train17 = pd.read_json('..\/input\/deepfake\/metadata17.json')\ndf_train18 = pd.read_json('..\/input\/deepfake\/metadata18.json')\ndf_train19 = pd.read_json('..\/input\/deepfake\/metadata19.json')\ndf_train20 = pd.read_json('..\/input\/deepfake\/metadata20.json')\ndf_train21 = pd.read_json('..\/input\/deepfake\/metadata21.json')\ndf_train22 = pd.read_json('..\/input\/deepfake\/metadata22.json')\ndf_train23 = pd.read_json('..\/input\/deepfake\/metadata23.json')\ndf_train24 = pd.read_json('..\/input\/deepfake\/metadata24.json')\ndf_train25 = pd.read_json('..\/input\/deepfake\/metadata25.json')\ndf_train26 = pd.read_json('..\/input\/deepfake\/metadata26.json')\ndf_train27 = pd.read_json('..\/input\/deepfake\/metadata27.json')\ndf_train28 = pd.read_json('..\/input\/deepfake\/metadata28.json')\ndf_train29 = pd.read_json('..\/input\/deepfake\/metadata29.json')\ndf_train30 = pd.read_json('..\/input\/deepfake\/metadata30.json')\ndf_train31 = pd.read_json('..\/input\/deepfake\/metadata31.json')\ndf_train32 = pd.read_json('..\/input\/deepfake\/metadata32.json')\ndf_train33 = pd.read_json('..\/input\/deepfake\/metadata33.json')\ndf_train34 = pd.read_json('..\/input\/deepfake\/metadata34.json')\ndf_train35 = pd.read_json('..\/input\/deepfake\/metadata35.json')\ndf_train36 = pd.read_json('..\/input\/deepfake\/metadata36.json')\ndf_train37 = pd.read_json('..\/input\/deepfake\/metadata37.json')\ndf_train38 = pd.read_json('..\/input\/deepfake\/metadata38.json')\ndf_train39 = pd.read_json('..\/input\/deepfake\/metadata39.json')\ndf_train40 = pd.read_json('..\/input\/deepfake\/metadata40.json')\ndf_train41 = pd.read_json('..\/input\/deepfake\/metadata41.json')\ndf_train42 = pd.read_json('..\/input\/deepfake\/metadata42.json')\ndf_train43 = pd.read_json('..\/input\/deepfake\/metadata43.json')\ndf_train44 = pd.read_json('..\/input\/deepfake\/metadata44.json')\ndf_train45 = pd.read_json('..\/input\/deepfake\/metadata45.json')\ndf_train46 = pd.read_json('..\/input\/deepfake\/metadata46.json')\ndf_val1 = pd.read_json('..\/input\/deepfake\/metadata47.json')\ndf_val2 = pd.read_json('..\/input\/deepfake\/metadata48.json')\ndf_val3 = pd.read_json('..\/input\/deepfake\/metadata49.json')\ndf_trains = [df_train0 ,df_train1, df_train2, df_train3, df_train4,\n             df_train5, df_train6, df_train7, df_train8, df_train9,df_train10,\n            df_train11, df_train12, df_train13, df_train14, df_train15,df_train16, \n            df_train17, df_train18, df_train19, df_train20, df_train21, df_train22, \n            df_train23, df_train24, df_train25, df_train26, df_train27, df_train28, \n            df_train29, df_train30, df_train31, df_train32, df_train33, df_train34,\n            df_train34, df_train35, df_train36, df_train37, df_train38, df_train39,\n            df_train40, df_train41, df_train42, df_train43, df_train44, df_train45,\n            df_train46]\ndf_vals=[df_val1, df_val2, df_val3]\nnums = list(range(len(df_trains)+1))\nLABELS = ['REAL','FAKE']\nval_nums=[47, 48, 49]","58ae6b94":"df_trains","99168a7d":"def get_path(num,x):\n    num=str(num)\n    if len(num)==2:\n        path='..\/input\/deepfake\/DeepFake'+num+'\/DeepFake'+num+'\/' + x.replace('.mp4', '') + '.jpg'\n    else:\n        path='..\/input\/deepfake\/DeepFake0'+num+'\/DeepFake0'+num+'\/' + x.replace('.mp4', '') + '.jpg'\n    if not os.path.exists(path):\n       raise Exception\n    return path\npaths=[]\ny=[]\nfor df_train,num in tqdm(zip(df_trains,nums),total=len(df_trains)):\n    images = list(df_train.columns.values)\n    for x in images:\n        try:\n            paths.append(get_path(num,x))\n            y.append(LABELS.index(df_train[x]['label']))\n        except Exception as err:\n            #print(err)\n            pass\n\nval_paths=[]\nval_y=[]\nfor df_val,num in tqdm(zip(df_vals,val_nums),total=len(df_vals)):\n    images = list(df_val.columns.values)\n    for x in images:\n        try:\n            val_paths.append(get_path(num,x))\n            val_y.append(LABELS.index(df_val[x]['label']))\n        except Exception as err:\n            #print(err)\n            pass","c979429c":"\nprint('There are '+str(y.count(1))+' fake train samples')\nprint('There are '+str(y.count(0))+' real train samples')\nprint('There are '+str(val_y.count(1))+' fake val samples')\nprint('There are '+str(val_y.count(0))+' real val samples')","8edc76df":"import random\nreal=[]\nfake=[]\nfor m,n in zip(paths,y):\n    if n==0:\n        real.append(m)\n    else:\n        fake.append(m)\nfake=random.sample(fake,len(real))\npaths,y=[],[]\nfor x in real:\n    paths.append(x)\n    y.append(0)\nfor x in fake:\n    paths.append(x)\n    y.append(1)","1321270e":"real=[]\nfake=[]\nfor m,n in zip(val_paths,val_y):\n    if n==0:\n        real.append(m)\n    else:\n        fake.append(m)\nfake=random.sample(fake,len(real))\nval_paths,val_y=[],[]\nfor x in real:\n    val_paths.append(x)\n    val_y.append(0)\nfor x in fake:\n    val_paths.append(x)\n    val_y.append(1)","50125bd6":"print('There are '+str(y.count(1))+' fake train samples') # 1 for fake \nprint('There are '+str(y.count(0))+' real train samples') # 0 for real\nprint('There are '+str(val_y.count(1))+' fake val samples')\nprint('There are '+str(val_y.count(0))+' real val samples')","e3228e7a":"def read_img(path):\n    img =  cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2GRAY)\n    return cv2.resize(img, (28, 28))\n\n\n\n\n\nX=[]\nfor img in tqdm(paths):\n    \n    X.append(read_img(img)) # training images \nval_X=[]\nfor img in tqdm(val_paths):\n    val_X.append(read_img(img)) # validation images","cb420bf0":"for x_img in X[:10]:\n    print (x_img.shape)\n    plt.subplot()\n    plt.imshow(x_img)\n    plt.show()\n\nprint (y[:10])","200fc708":"import random\ndef shuffle(X,y):\n    new_train=[]\n    for m,n in zip(X,y):\n        new_train.append([m,n])\n    random.shuffle(new_train)\n    X,y=[],[]\n    for x in new_train:\n        X.append(x[0])\n        y.append(x[1])\n    return X,y","2566fddd":"X,y=shuffle(X,y)\nval_X,val_y=shuffle(val_X,val_y)","cdf44e0f":"X_train = np.array(X)\nY_train = np.array(y)\n\nX_val = np.array(val_X)\nY_val = np.array(val_y)","a50cabb8":"X_train,X_val = X_train \/ 255.0,X_val \/ 255.0\n","653fdd97":"X_val.shape","62d57444":"Y_train","b2766c9c":"base_model = applications.ResNet50(input_shape=(50,50,3),include_top=False,weights='imagenet')","a050b4f8":"base_model.summary()","0913f708":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer()\n","5470c206":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))","b3797f88":"model.summary()\n","368844cf":"model.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(2,activation='softmax'))","e537ecb4":"model.input.shape","671ac0ea":"#X_train = X_train.reshape(-1,50,50,1)\n#X_val = X_val.reshape(-1,50,50,1)","aa91b950":"from keras.utils import to_categorical\ny_binary = to_categorical(Y_train,num_classes=2)\ny_binary_val = to_categorical(Y_val,num_classes = 2)\n\n\n","e480012f":"y_binary.shape","811acc52":"import tensorflow as tf\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['Precision','Recall'])\n\nhistory = model.fit(X_train,y_binary, epochs=10,validation_data=(X_val, y_binary_val))","463c7692":"Y_train","dc56a8e1":"ind = 0\nX_train_real,X_train_fake = [],[]\nfor y in Y_train:\n    if y == 0: #real\n        X_train_real.append(X_train[ind]) #12,130 real samples\n    else:\n        X_train_fake.append(X_train[ind]) #12,130 fake samples\n    ind=ind+1\n        ","19a7903c":"X_train_real_t = np.array(X_train_real).reshape((len(X_train_real), np.prod(np.array(X_train_real).shape[1:])))\n#x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint (X_train_real_t.shape)\n#print x_test.shape","c1ef0ac6":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\n# this is the size of our encoded representations\nencoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n\n# this is our input placeholder\ninput_img = Input(shape=(784,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(784, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\nencoder = Model(input_img, encoded)\n\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))","41684f91":"autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n","d57d32d4":"autoencoder.fit(X_train_real_t, X_train_real_t,\n                epochs=200,\n                batch_size=256,\n                shuffle=True)","df8a30d4":"ind = 0\nX_val_real,X_val_fake = [],[]\nfor y in Y_val:\n    if y == 0: #real\n        X_val_real.append(X_val[ind]) #12,130 real samples\n    else:\n        X_val_fake.append(X_val[ind]) #12,130 fake samples\n    ind=ind+1\n        ","48248dcd":"X_val_real = np.array(X_val_real)\nX_val_real.shape","67028c8a":"for i in X_val_real[:5]:\n    plt.imshow(i)\n    plt.show()","fc598b66":"X_val_real_t = np.array(X_val_real).reshape((len(X_val_real), np.prod(np.array(X_val_real).shape[1:])))\n#x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint (X_val_real_t.shape)\n#print x_test.shape","a3c127d0":"encoded_imgs_real = encoder.predict(X_val_real_t)\ndecoded_imgs = decoder.predict(encoded_imgs)","6e6a52a1":"print (encoded_imgs_real.mean())\nprint (encoded_imgs_real.var())","3919030b":"import matplotlib.pyplot as plt\n\nn = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(X_train_real_t[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","8b307223":"X_train_fake_t = np.array(X_train_fake).reshape((len(X_train_fake), np.prod(np.array(X_train_fake).shape[1:])))\n#x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint (X_train_fake_t.shape)\n#print x_test.shape","53955865":"X_val_fake_t = np.array(X_val_fake).reshape((len(X_val_fake), np.prod(np.array(X_val_fake).shape[1:])))\n#x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint (X_val_fake_t.shape)\n#print x_test.shape","7f19c0dd":"autoencoder.fit(X_train_fake_t, X_train_fake_t,\n                epochs=200,\n                batch_size=256,\n                shuffle=True)","e19368fc":"encoded_imgs_fake = encoder.predict(X_train_fake_t)\ndecoded_imgs = decoder.predict(encoded_imgs)","42c050f0":"print (encoded_imgs_fake.mean())\nprint (encoded_imgs_fake.var())","88ff8646":"import matplotlib.pyplot as plt\n\nn = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(X_val_fake_t[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","1d3a1a4c":"#z score for test samples from 2 distributions\n\nY_pred_encoder = []\n\ndef z_score (i,encoding):\n    mean = encoding.mean()\n    sd = np.sqrt(encoding.var())\n    z = 1.0*(i.mean() - mean)\/ sd \n    return (z)\n    \nfor i in X_val:\n    if z_score(i,encoded_imgs_real)< z_score(i,encoded_imgs_fake):\n        Y_pred_encoder.append(0) # real\n    else:\n        Y_pred_encoder.append(1) # fake","d457c9dc":"Leaning fake data distribution","89b919d9":"Real face data distribution","c4a02d54":"Approach 2 - learning distributions of real and fake samples through latent space representation"}}