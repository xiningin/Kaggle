{"cell_type":{"8eb7c9c8":"code","aa0d2c3e":"code","7fdf2bef":"code","381c99d0":"code","1fb18315":"code","f7b9bfbc":"code","01ff8860":"code","cd05a32e":"code","82b3c7e7":"code","7442b3ae":"code","ccb85442":"code","1bf51269":"code","e7dad71c":"code","d435a797":"code","cb1f0264":"code","62e7c8b8":"code","e28110fb":"code","4f2c525b":"code","865336a3":"code","e73abb18":"code","a2a79a2a":"code","d9e11b25":"code","cf73cf84":"code","2d49adab":"code","6b45d202":"code","d81b3715":"code","ba89b4ff":"code","822e8abf":"code","f6b93052":"code","d1327dfa":"code","ebbd3fd2":"code","2251f315":"code","65e104fe":"code","dff3987a":"code","86aa78aa":"code","e3ccbdde":"code","8fe346d6":"code","7a04d5e0":"markdown","1b139c63":"markdown","3b2c9e9f":"markdown","21631ecf":"markdown","6a73b28b":"markdown","2314fa4f":"markdown","6b59e922":"markdown","b86e08c6":"markdown","a11b4526":"markdown","64a116c2":"markdown","fa988a31":"markdown","74ee41d8":"markdown","5cb1c0f5":"markdown","f1ba6331":"markdown","73864b37":"markdown"},"source":{"8eb7c9c8":"#!pip install autogluon\n#!pip install autokeras","aa0d2c3e":"import numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport catboost\nfrom catboost import CatBoostRegressor\nimport optuna\nimport sklearn\nimport math\nfrom fractions import Fraction\nimport shutil\nimport gc\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n#from autogluon.tabular import TabularPredictor, TabularDataset\n#import autokeras as ak","7fdf2bef":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","381c99d0":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv', index_col = 'id')\ntrain.head()","1fb18315":"train.shape #For KERAS usage down the line.","f7b9bfbc":"train_y = train['loss'] #Labels\ntrain_x = train\ntrain_x.drop(columns = ['loss'], inplace=True)\ntrain_x.head() #Features","01ff8860":"scaler = StandardScaler() #Scaling required.\ntrain_x = scaler.fit_transform(train_x)","cd05a32e":"train_x_train, train_x_valid, train_y_train, train_y_valid = train_test_split(train_x, train_y, train_size = 0.8) #Normal Train Test Split will be used. ","82b3c7e7":"test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv', index_col = 'id')\ntest.head()","7442b3ae":"test = scaler.transform(test) #Need to transform test data too. ","ccb85442":"def model(train_x, train_y, argument):\n    def ModelSelector(argument):\n        model = {\n            'LGBM': LGBMRegressor(),\n            'XGB': XGBRegressor(),\n            'CatBoost': CatBoostRegressor()\n        }\n        return model.get(argument, 'Invalid Selection')\n    model = ModelSelector(argument)\n    model.fit(train_x, train_y)\n    predictions = model.predict(test)\n    return predictions","1bf51269":"#argument = 'XGB'\n#predictions = model(train_x, train_y, argument)","e7dad71c":"#Credit here for objective functions and K-Fold Implementation: https:\/\/www.kaggle.com\/michael127001\/xgbregressor-with-optuna-tuning\ndef Optuna(argument):\n    N_TRIALS = 5\n    N_SPLITS = 10\n    def LGBMObjective(trial, x_train = train_x, y_train = train_y):\n        kfolds = KFold(n_splits=N_SPLITS, shuffle=True)\n        param = {\n            'boosting_type': 'goss',\n            'max_depth': trial.suggest_int('max_depth', 6, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.013),\n            'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400),\n            'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),\n            'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n            'reg_alpha': trial.suggest_int('reg_alpha', 0, 50),\n            'reg_lambda': trial.suggest_int('reg_lambda', 1, 50),\n            'n_jobs': 4,\n        }\n        model = LGBMRegressor(**param)\n        mse = cross_val_score(model, x_train, y_train, cv=kfolds,\n            scoring=\"neg_root_mean_squared_error\", \n        )\n        return mse.mean()\n    def XGBRObjective(trial, x_train = train_x, y_train = train_y):\n        kfolds = KFold(n_splits=N_SPLITS, shuffle=True)\n        param = {\n            'max_depth': trial.suggest_int('max_depth', 6, 12),\n            'eta': trial.suggest_float('eta', 0.007, 0.013),\n            'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400),\n            'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),\n            'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n            'reg_alpha': trial.suggest_int('reg_alpha', 0, 50),\n            'reg_lambda': trial.suggest_int('reg_lambda', 1, 50),\n            'n_jobs': 4,\n            'tree_method': 'gpu_hist',\n            'predictor': 'gpu_predictor',\n        }\n        model = XGBRegressor(**param)\n        mse = cross_val_score(model, x_train, y_train, cv=kfolds,\n            scoring=\"neg_root_mean_squared_error\", \n        )\n        return mse.mean()\n    def CatBoostObjective(trial, x_train = train_x, y_train = train_y): \n        kfolds = KFold(n_splits=N_SPLITS, shuffle=True)\n        param = {\n            'max_depth': trial.suggest_int('max_depth', 6, 12),\n            'learning_rate': trial.suggest_float('eta', 0.007, 0.013),\n            'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400),\n            'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),\n            'reg_lambda': trial.suggest_int('reg_lambda', 1, 50),\n            'task_type': 'GPU',\n            'bootstrap_type': 'Bernoulli', \n            'eval_metric':'RMSE',\n            'verbose': False\n        }\n        model = CatBoostRegressor(**param)\n        mse = cross_val_score(model, x_train, y_train, cv=kfolds,\n            scoring=\"neg_root_mean_squared_error\", \n        )\n        return mse.mean()\n    def ObjectiveSelector(argument):\n        objective = {\n            'LGBM': LGBMObjective,\n            'XGB': XGBRObjective,\n            'CatBoost': CatBoostObjective            \n        }\n        return objective.get(argument, \"Invalid Selection\")\n    def ModelSelector(argument, trial): #Switch case not usable here without crashing. \n        if(argument == 'LGBM'):\n            return LGBMRegressor(**trial.params)\n        elif(argument == 'XGB'):\n            return XGBRegressor(**trial.params)\n        elif(argument == 'CatBoost'):\n            return CatBoostRegressor(**trial.params)\n        return \"Invalid Model\"\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(ObjectiveSelector(argument), n_trials=N_TRIALS)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    predictions_optuna = None\n    kfolds = KFold(n_splits=N_SPLITS, shuffle=True)\n    for i, (train_idx, valid_idx) in enumerate(kfolds.split(train_x, train_y)):\n        X_train, y_train = train_x[train_idx], train_y[train_idx]\n        X_valid, y_valid = train_x[valid_idx], train_y[valid_idx]\n        model = ModelSelector(argument, trial)\n        if(argument == 'LGBM' or argument == 'XGB'):\n            model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = 'rmse', verbose = False)\n        else:\n            model.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], verbose = False)\n        valid_pred = model.predict(X_valid)\n        rmse = np.sqrt(mean_squared_error(valid_pred, y_valid))\n        print(rmse)\n        if(predictions_optuna is None):\n            predictions_optuna = model.predict(test)\n        else:\n            predictions_optuna += model.predict(test)\n    predictions_optuna \/= N_SPLITS\n    return predictions_optuna","d435a797":"#argument = 'XGB'\n#predictions_optuna = Optuna(argument)","cb1f0264":"#predictions_lgbm = Optuna('LGBM')\n#predictions_xgb = Optuna('XGB')\n#predictions_catboost = Optuna('CatBoost')\n#predictions_blend = Fraction(1,3) * predictions_lgbm + Fraction(1,3) * predictions_xgb + Fraction(1,3) * predictions_catboost","62e7c8b8":"#Have to reload files for AutoML\n#train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv', index_col = 'id')\n#train.drop(columns = ['id'], inplace=True)\n#test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv', index_col = 'id')\n#test.drop(columns = ['id'], inplace=True)","e28110fb":"#Credit here for Autogluon Model Training: https:\/\/www.kaggle.com\/aayush26\/tps-aug-2021-autogluon-101\n#model = TabularPredictor(problem_type='regression', label = 'loss')\n#model.fit(train_data=train, presets='best_quality', time_limit = 20000, num_stack_levels = 3,num_bag_folds = 5,num_bag_sets = 1,)\n#del train['loss']","4f2c525b":"#model.leaderboard()","865336a3":"#predictions_autogluon = model.predict(TabularDataset(test))","e73abb18":"#model = ak.StructuredDataRegressor(overwrite=True, loss = 'mean_squared_error', \n#                                   metrics=[tf.keras.metrics.RootMeanSquaredError()])\n#model.fit('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv',\"loss\")","a2a79a2a":"#predictions_autokeras = model.predict(test)","d9e11b25":"#train = pd.read_csv('train.csv', index_col = 'id')\n#test = pd.read_csv('test.csv', index_col = 'id')\n#pseudolabel = predictions_optuna\n#test_concat = test\n#test_concat['loss'] = pseudolabel\n#test_concat.head()","cf73cf84":"#test_30 = test_concat.sample(frac = 0.3)\n#test_30.shape","2d49adab":"#train_data = [train, test_30]\n#train = pd.concat(train_data)\n#train.shape","6b45d202":"#train.index = range(295000)","d81b3715":"#train_y = train['loss'] \n#train_x = train\n#train_x.drop(columns = ['loss'], inplace=True)\n#train_x.head()","ba89b4ff":"#scaler = StandardScaler() #Scaling required.\n#train_x = scaler.fit_transform(train_x)\n#del test['loss']\n#test = scaler.transform(test)","822e8abf":"#argument = 'XGB'\n#predictions_pseudolabel = Optuna(argument)","f6b93052":"input_shape = [100]\nmodel = keras.Sequential([\n    layers.Dense(units = 512, input_shape=input_shape, activation = 'relu'),\n    layers.Dense(units = 512, input_shape=input_shape, activation = 'relu'),\n    layers.Dense(units = 10, input_shape=input_shape, activation = 'relu'),\n    layers.BatchNormalization()\n])\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'mse',\n    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)","d1327dfa":"early_stopping = callbacks.EarlyStopping(\n    min_delta = 0.001,\n    restore_best_weights = True\n)","ebbd3fd2":"model.fit(\n    train_x_train, train_y_train,\n    validation_data = (train_x_valid, train_y_valid),\n    epochs = 50,\n    verbose = 1,\n    callbacks = [early_stopping]\n)","2251f315":"predictions_keras = model.predict(test)","65e104fe":"FRACTION = Fraction(140, 363)","dff3987a":"rank_1 = pd.read_csv('XGBOptuna.csv')\nrank_2 = pd.read_csv('XGBPseudoLabel.csv')\nrank_3 = pd.read_csv('CatXGBPseudoLabel.csv')\nrank_4 = pd.read_csv('LGBMOptuna.csv')\nrank_5 = pd.read_csv('AutoGluon.csv')\nrank_6 = pd.read_csv('LGBMOptuna.csv')\nrank_7 = pd.read_csv('AutoKeras.csv')","86aa78aa":"sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nsample_submission['loss'] = predictions_keras\nsample_submission.reset_index()\nsample_submission.head()","e3ccbdde":"sample_submission.to_csv('submission.csv', index=False)","8fe346d6":"#shutil.rmtree('AutogluonModels')\n#gc.collect()","7a04d5e0":"# 4. Blending Models","1b139c63":"We can use the XGBRegressor Optuna Data (which has the lowest RMSE thus far) as a pseudolabel to make a better model.  ","3b2c9e9f":"# 2. Blind Model Fitting","21631ecf":"I wanted to try and follow the suggestions outlined by Aayush Kumar Singha, linked here: https:\/\/www.kaggle.com\/c\/tabular-playground-series-aug-2021\/discussion\/258009. As such, for this month's edition of the Tabular Playground, I'll be focusing on blindly fitting in LGBM, XGB, and CatBoost Regression models before tuning them using Optuna. First things first, we need to load in all the required libraries and data.","6a73b28b":"# 6. Pseudolabel","2314fa4f":"Now that we have several tuned models, it would be a good idea to blend them together into a better model. ","6b59e922":"# 3. Optuna","b86e08c6":"# 5.1. Autogulon","a11b4526":"# Submission","64a116c2":"# 7.1. KERAS","fa988a31":"# 1. Loading Libraries and Data","74ee41d8":"We can improve the quality of models by implementing both K-Folding and Optuna. ","5cb1c0f5":"# 8. More Blending","f1ba6331":"# 5.2. AutoKeras","73864b37":"I defined a function that takes in an argument to fit in the corresponding model. This is done to streamline the process whenever a different model is desired."}}