{"cell_type":{"160724d4":"code","6a0b246e":"code","74e9c452":"code","3d88d17a":"code","13902431":"code","d8c38469":"code","2b19264b":"code","0b2e5fa0":"code","144261d9":"code","2b4d5195":"code","a0e430a1":"code","a050500d":"code","d98e2acc":"code","dbcd2d77":"code","21aeadbf":"code","46d0b027":"code","d16a2cce":"code","6da0d59e":"code","0206a030":"code","99c9bb25":"code","aa9dabbc":"code","920722e3":"code","6eabeb71":"code","dfac3767":"code","13fe7616":"code","35440796":"code","ce883656":"code","2864e970":"code","5f1c8ce7":"code","29b843a3":"code","6e0f9658":"code","53e60046":"code","bef476ab":"code","9f17853d":"code","bb5c4034":"code","5f7441bb":"code","bd690b34":"code","2c09292e":"code","e0b9dae2":"code","8884628e":"code","67794e3e":"code","76091495":"code","e69e05a3":"code","0dd00fbe":"code","93764fa7":"code","94e3c3f7":"code","e2c9583d":"code","70f7e542":"code","780e6c61":"code","8513b5d2":"code","0a830a09":"code","5ab6dcaf":"code","ab6b1efb":"code","594d82c6":"code","86aa844d":"code","cbe446aa":"code","a246180a":"code","698a1539":"code","61a30820":"code","2f680b63":"code","0206e89d":"code","7d691af1":"code","df9fc6d6":"code","9e743ba4":"code","4fa0558b":"code","ee688e88":"code","7ed846a4":"code","ee71cfb9":"code","5d3b6532":"code","4fb0f372":"code","6cf39f3b":"code","fea142f7":"code","5ccbd0ad":"code","293784b2":"code","834f787e":"code","a186197b":"markdown","2321409d":"markdown","2a2113f2":"markdown","71b16619":"markdown","be90ea3d":"markdown","97c28063":"markdown","b7243786":"markdown","9448d6fe":"markdown","13245d31":"markdown","5e4692c6":"markdown","52c84e36":"markdown","4b929cbd":"markdown","4815d7c8":"markdown","63c12c34":"markdown","3757cb5a":"markdown","ff465188":"markdown","53c7ba6e":"markdown","f6a9a677":"markdown","af151a73":"markdown","f800401b":"markdown","18c17a20":"markdown","899116fe":"markdown","e20366a1":"markdown","79e5a24d":"markdown","75341dc4":"markdown","3920e6a0":"markdown","c816584b":"markdown"},"source":{"160724d4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport fasttext\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nimport gc\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtext.data.utils import ngrams_iterator\nfrom torch.utils.data import DataLoader\nimport time\nfrom torch.utils.data.dataset import random_split\nimport torch\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport torchtext\nfrom torchtext import data\nimport spacy\nimport pandas_profiling as pp\nfrom collections import Counter\nfrom itertools import chain\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"","6a0b246e":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","74e9c452":"temp_3 = train_df\ntest_temp = test_df","3d88d17a":"def review_to_words( raw_review ):\n    \n    review_text = BeautifulSoup(raw_review).get_text() \n    \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    words = letters_only.lower().split()                             \n \n    stops = set(stopwords.words(\"english\"))                  \n\n    meaningful_words = [w for w in words if not w in stops]   \n\n    return( \" \".join( meaningful_words )) ","13902431":"temp_3['is_train'] = True\ntest_temp['is_train'] = False\n\ndf = pd.concat([temp_3, test_temp], sort=False, ignore_index=True).set_index('id').sort_index()","d8c38469":"temp_3.target.value_counts().plot.bar(title=\"Balanced target variable\")","2b19264b":"disaster = temp_3.loc[temp_3.target == 1]\nnondisaster = temp_3.loc[temp_3.target == 0]\n\ndisaster['text'] = disaster[\"text\"].apply(review_to_words) \nnondisaster['text'] = nondisaster[\"text\"].apply(review_to_words) ","0b2e5fa0":"labels, values = zip(*Counter(\" \".join(disaster[\"text\"]).split()).most_common(20)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width)\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('20 Most common disaster tweets')\nplt.show()","144261d9":"labels, values = zip(*Counter(\" \".join(nondisaster[\"text\"]).split()).most_common(20)[3:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('20 Most common non disaster tweets')\nplt.show()","2b4d5195":"disaster[\"location\"].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common locations of disaster tweets\", fontsize=17)","a0e430a1":"nondisaster[\"location\"].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common locations of nondisaster tweets\", fontsize=17)","a050500d":"disaster['keyword'].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common keywords of disaster tweets\", fontsize=17)","d98e2acc":"nondisaster['keyword'].dropna().value_counts()[:20].plot.barh(figsize=(15, 8), title=\"20 Most common keywords of nondisaster tweets\", fontsize=17)","dbcd2d77":"def find_ngrams(input_list, n):\n    return list(zip(*[input_list[i:] for i in range(n)]))","21aeadbf":"disaster['bigrams'] = disaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n\nbigrams = disaster['bigrams'].tolist()\nbigrams = list(chain(*bigrams))\nbigrams = [(x.lower(), y.lower()) for x,y in bigrams]\n\nlabels, values = zip(*Counter(bigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common bigrams disaster label')\nplt.show()","46d0b027":"nondisaster['bigrams'] = nondisaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))\n\nbigrams = nondisaster['bigrams'].tolist()\nbigrams = list(chain(*bigrams))\nbigrams = [(x.lower(), y.lower()) for x,y in bigrams]\n\nlabels, values = zip(*Counter(bigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common bigrams nondisaster label')\nplt.show()","d16a2cce":"disaster['trigrams'] = disaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 3))\n\ntrigrams = disaster['trigrams'].tolist()\ntrigrams = list(chain(*trigrams))\ntrigrams = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams]\n\nlabels, values = zip(*Counter(trigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common trigrams disaster label')\nplt.show()","6da0d59e":"nondisaster['trigrams'] = nondisaster['text'].map(lambda x: find_ngrams(x.split(\" \"), 3))\n\ntrigrams = nondisaster['trigrams'].tolist()\ntrigrams = list(chain(*trigrams))\ntrigrams = [(x.lower(), y.lower(), z.lower()) for x,y,z in trigrams]\n\nlabels, values = zip(*Counter(trigrams).most_common(10)[2:])\n\nindexes = np.arange(len(labels))\nwidth = 0.7\n\nplt.figure(figsize=(30,12))\nplt.barh(indexes, values, width, )\nplt.yticks(indexes + width * 0.5, labels)\nplt.yticks(fontsize=23)\nplt.title('8 Most common trigrams nondisaster label')\nplt.show()","0206a030":"del temp_3, test_temp\ngc.collect()","99c9bb25":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","aa9dabbc":"train['text'] = train['text'].apply(review_to_words) ","920722e3":"train['bigrams'] = train['text'].map(lambda x: find_ngrams(x.split(\" \"), 2))","6eabeb71":"dropcols = ['id', 'keyword', 'location', 'target', 'bigrams']\n\ntext = train.drop(dropcols, axis=1)\ntext_arr = text.stack().tolist()","dfac3767":"words = []\nfor ii in range(0,len(text)):\n    words.append(str(text.iloc[ii]['text']).split(\" \"))\n\nwords[0][:10]","13fe7616":"n_gram_all = []\n\nfor word in words:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)\n\nn_gram_all[0][:10]","35440796":"# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, ngram_range=(1,2), n_features=2**12)\n#vectorizer = TfidfVectorizer(lowercase=False, analyzer=lambda l:l)\n\n# features matrix X\nX = hvec.fit_transform(n_gram_all)\n#X = vectorizer.fit_transform(n_gram_all)","ce883656":"X.shape","2864e970":"X_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n#X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","5f1c8ce7":"#tsne = TSNE(verbose=1, perplexity=10)\ntsne = TSNE(verbose=1, perplexity=5)\nX_embedded = tsne.fit_transform(X_train)","29b843a3":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", 1)\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"Disaster and non disaster Tweets - Clustered\")\nplt.show()","6e0f9658":"k = 2\nkmeans = KMeans(n_clusters=k, n_jobs=6, verbose=10)\ny_pred = kmeans.fit_predict(X_train)\ny_train = y_pred","53e60046":"y_test = kmeans.predict(X_test)","bef476ab":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"Disaster and non disaster Tweets - Clustered(K-means)\")\nplt.show()","9f17853d":"vectorizer = TfidfVectorizer(max_features=2**12)\nX = vectorizer.fit_transform(train['text'].values)","bb5c4034":"X.shape","5f7441bb":"k = 2\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\ny = y_pred","bd690b34":"tsne = TSNE(verbose=1)\nX_embedded = tsne.fit_transform(X.toarray())","2c09292e":"sns.set(rc={'figure.figsize':(15,15)})\n\npalette = sns.color_palette(\"bright\", len(set(y)))\n\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\nplt.title(\"Disaster and non disaster Tweets - Clustered(K-Means)\")\nplt.show()","e0b9dae2":"del train\ngc.collect()","8884628e":"temp_3 = train_df\ntest_temp = test_df","67794e3e":"temp_3['location'] = temp_3['location'].fillna('no')\ntemp_3['keyword'] = temp_3['keyword'].fillna('no')\ntest_temp['location'] = test_temp['location'].fillna('no')\ntest_temp['keyword'] = test_temp['keyword'].fillna('no')","76091495":"temp_3['text'] = temp_3[\"text\"].apply(review_to_words) \ntest_temp['text'] = test_temp[\"text\"].apply(review_to_words) \ntemp_3['location'] = temp_3[\"location\"].apply(review_to_words) \ntest_temp['location'] = test_temp[\"location\"].apply(review_to_words) ","e69e05a3":"def convert_target(x):\n    if x == 1:\n        x = \"disaster\"\n    else:\n        x = \"nodisaster\"\n    return x","0dd00fbe":"temp_3['target'] = temp_3.target.apply(lambda x : convert_target(x))","93764fa7":"def xyz(x):\n    d = str(x['text']) + \" \" + str(x['keyword'])+ \" \" + str(x['location'])    \n    c = str(x['target'])\n    p = \"__label__\" + c\n    final = p + \" \" + d\n    return final\n\ntemp_3 = temp_3.apply(xyz, axis=1)\ntemp_3_copy = temp_3.copy()","94e3c3f7":"temp_3.to_csv(r'm.txt', header=None, index=None)","e2c9583d":"os.system(\"head -n 7613 m.txt > m.train\")","70f7e542":"model = fasttext.train_supervised(input=\"m.train\")","780e6c61":"def target(x):\n    d = str(x['text']) + \" \" + str(x['keyword'])+ \" \" + str(x['location'])  \n    target = model.predict(d)\n    return target[0][0][9]\n\ntest_temp['target'] = test_temp.apply(target, axis=1)","8513b5d2":"def convert_target(x):\n    if x == \"d\":\n        x = 1\n    else:\n        x = 0\n    return x","0a830a09":"test_temp['target'] = test_temp.target.apply(lambda x : convert_target(x))","5ab6dcaf":"test_temp[[\"id\", \"target\"]].to_csv('submissionfasttext.csv', index=False)","ab6b1efb":"del temp_3, test_temp\ngc.collect()","594d82c6":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","86aa844d":"temp_3 = train_df\ntest_temp = test_df","cbe446aa":"temp_3['text'] = temp_3[\"text\"].apply(review_to_words) \nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(temp_3['text'])\ntest_vectors = vectorizer.transform(test_df[\"text\"])","a246180a":"tsvd = TruncatedSVD(2000)\ntrain_vectors_svd = tsvd.fit_transform(train_vectors)\ntest_vectors_svd = tsvd.transform(test_vectors)","698a1539":"clf = linear_model.RidgeClassifier()","61a30820":"scores = model_selection.cross_val_score(clf, train_vectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","2f680b63":"clf.fit(train_vectors_svd, train_df[\"target\"])","0206e89d":"sample_submission[\"target\"] = clf.predict(test_vectors_svd)","7d691af1":"sample_submission.to_csv(\"submission.csv\", index=False)","df9fc6d6":"%config InlineBackend.figure_format = 'retina' \nplt.style.use('seaborn')","9e743ba4":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnlp = spacy.load(\"en\")","4fa0558b":"def is_token_allowed(token):\n    '''\n    Only allow valid tokens which are not stop words\n    and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return token.lemma_.strip().lower()\n\ndef tokenizer(text):\n    return [ preprocess_token(token) for token in nlp.tokenizer(text) if is_token_allowed(token)]","ee688e88":"text = data.Field(tokenize=tokenizer, pad_first=True)\n\ntrain_dataset = data.TabularDataset(\n            path=\"\/kaggle\/input\/nlp-getting-started\/train.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text),\n                    ('target', data.Field())], \n            skip_header=True)\n\ntest_dataset = data.TabularDataset(path=\"\/kaggle\/input\/nlp-getting-started\/test.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text)], \n            skip_header=True)","7ed846a4":"MIN_FREQ = 2\n\ntext.build_vocab(train_dataset, test_dataset, min_freq=MIN_FREQ)\n\nVOCAB_SIZE = len(text.vocab)","ee71cfb9":"NGRAMS = 2\nBATCH_SIZE = 8\nEMBED_DIM = 768\n\nclass ShallowNeuralNetwork(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n    \n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n    \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return F.softmax(self.fc(embedded))","5d3b6532":"def generate_batch(batch):\n    label = torch.tensor([int(entry.target[0]) for entry in batch])\n    _text = []\n    for entry in batch:\n        _entry = []\n        for t in entry.text:\n            _entry.append(text.vocab.stoi[t])\n        _text.append(torch.tensor(_entry,dtype=torch.long))\n    offsets = [0] + [len(entry) for entry in _text]\n    \n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    _text = torch.cat(_text)\n    return _text, offsets, label","4fb0f372":"def train_func(sub_train_):\n    # Train the model\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n    \n    for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n    # Adjust the learning rate\n    scheduler.step()\n    \n    return train_loss \/ len(sub_train_), train_acc \/ len(sub_train_)","6cf39f3b":"def test(data_):\n    loss = 0\n    acc = 0\n    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss += loss.item()\n            acc += (output.argmax(1) == cls).sum().item()\n\n    return loss \/ len(data_), acc \/ len(data_)","fea142f7":"model = ShallowNeuralNetwork(VOCAB_SIZE, EMBED_DIM, 2).to(device)\n\nN_EPOCHS = 5\nmin_valid_loss = float('inf')\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n\ntrain_len = int(len(train_dataset) * 0.95)\nsub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    train_loss, train_acc = train_func(sub_train_)\n    valid_loss, valid_acc = test(sub_valid_)\n\n    secs = int(time.time() - start_time)\n    mins = secs \/ 60\n    secs = secs % 60\n\n    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')","5ccbd0ad":"def predict(_text, model, vocab, ngrams):\n    if len(_text) == 0:\n        return 0\n    \n    with torch.no_grad():\n        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n        output = model(torch.tensor(_text), torch.tensor([0]))\n        return output.argmax(1).item()","293784b2":"model = model.to(\"cpu\")\npredictions = [predict(entry.text, model, text.vocab, NGRAMS) for entry in test_dataset]\ntweet_id = [entry.id[0] for entry in test_dataset]","834f787e":"output = pd.DataFrame({'id': tweet_id, 'target': predictions})\noutput.to_csv('my_submission.csv', index=False)","a186197b":"### Clustering\n----","2321409d":"* #### Vectorize with HashingVectorizer\n* #### Dimensionality Reduction with t-SNE\n* #### Clustering with K-Means","2a2113f2":"## Table of contents\n---\n\n1. Import libraries\n2. Import datasets\n3. Data cleaning and processing\n4. Basic EDA\n \n 4.1 Clustering\n \n5. Fasttext Supervised\n6. Tf-idf & TruncatedSVD\n7. Shallow neural network (pytorch)","71b16619":"* #### Vectorize with Tf-idf\n* #### MiniBatchKMeans with Tf-idf\n* #### Dimensionality Reduction with t-SNE","be90ea3d":"* Learning rate 0.5\n* Training dataset 0.95","97c28063":"This notebook shows diferent basic steps for classifying disaster tweets.","b7243786":"#### Class distribution","9448d6fe":"## Import datasets\n----","13245d31":"* 20 Most common words related to nondisaster tweets","5e4692c6":"## NLP:\n---\n\n* [Tf-idf (term frequency\u2013inverse document frequency)](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)\n* [Fasttext](https:\/\/fasttext.cc\/)\n* [Shallow network](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/neural_networks_tutorial.html)","52c84e36":"### Export","4b929cbd":"## Data cleaning and pre-processing\n-------\n\n* Removing punctuation, special characters, emojis, numbers, links\n* Lower letters only and split them\n* Removing stopwords \n> \"Words such as \u201cthe\u201d, \u201cwill\u201d, and \u201cyou\u201d \u2014 called stopwords \u2014 appear the most in a corpus of text, but are of very little significance. Instead, the words which are rare are the ones that actually help in distinguishing between the data, and carry more weight.\"","4815d7c8":"> \"This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition.(...)\"\n\n> \"In particular, truncated SVD works on term count\/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text.(...)\"","63c12c34":"## Fasttext Supervised\n-----\n### SCORE (0.78936)","3757cb5a":"* N-grams visualization","ff465188":"> \"Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases.\"\n\n> \"Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. It is given by the equation below.\"","53c7ba6e":"## Shallow neural network\n\n-------------\n* Score (0.58793)\n* Score (0.68711) init weights\n* Score (0.69631) without freezing\n* Score (0.72290) without freezing & learning rate 0.5 \n* Score (0.66257) without freezing & learning rate 0.3 & 80% training dataset \n\n\n![](https:\/\/miro.medium.com\/max\/391\/1*CfdaqnNb6RHLzPJTt1UXjQ.png)\n","f6a9a677":"### Export","af151a73":"## Tf-idf & TruncatedSVD\n-----------\n### Score (0.80163)","f800401b":"* 20 Most common locations of nondisaster tweets","18c17a20":"# [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)","899116fe":"* 20 Most common words related to disaster tweets","e20366a1":"## EDA\n------","79e5a24d":"* 20 Most common keywords of disaster tweets","75341dc4":"* 20 Most common locations of disaster tweets","3920e6a0":"## Import libraries\n---","c816584b":"* 20 Most common keywords of nondisaster tweets"}}