{"cell_type":{"9c159319":"code","64361985":"code","ea417eef":"code","03eff801":"code","06366bf2":"code","2eb11c4d":"code","dd504167":"code","b1953ce6":"code","0aa41a6d":"code","eb81da85":"code","54f25ba0":"code","b9b844d2":"code","2984019a":"code","e9cc10ad":"code","67d7af57":"code","03e19faf":"code","465bcd85":"code","8dd23ca5":"code","e08d48a1":"code","6413b178":"code","2305b820":"code","02839536":"code","fc6c24f0":"code","a3ee5825":"code","8b3ac6ff":"code","80247e5c":"code","ff47fb41":"code","ab5cd41f":"code","0678c93e":"code","c33ead0c":"code","d4fab170":"code","7238615e":"code","4326302b":"code","f1d967b1":"code","1543cc1f":"code","48875e0b":"code","3721792c":"code","50e26b5a":"code","46fc8941":"code","3b6a4247":"code","836d7b66":"code","e6c1008e":"code","63dc5c50":"code","0fc98129":"code","2404089a":"code","fc2dbeb2":"code","8abf1d5c":"code","ff9cd5bf":"markdown","49f2ac4f":"markdown","a57d55ad":"markdown","f80a94fc":"markdown","dc191e66":"markdown","ba67ddd7":"markdown","088ee17f":"markdown","83a1e261":"markdown","ddeff7a6":"markdown","e52bfe79":"markdown","86950d1f":"markdown","aba6fe71":"markdown","d1f03e0c":"markdown","6542a686":"markdown","2e9c665e":"markdown","adba630f":"markdown"},"source":{"9c159319":"# few of the imports are just for checking while coding not included in the rest of notebook.\n\n# Most basic stuff for EDA.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\nimport string\nimport re\n\n# Libraries for text preprocessing.\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\n\n# Loading some sklearn packaces for modelling.\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # not actively using\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF # not actively using\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Utility\nimport logging\nimport itertools\n\n# Core packages for general use throughout the notebook.\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# for build our model\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import BertTokenizer, TFBertModel\n\n# Setting some options for general use.\nimport os\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')","64361985":"# Read the data\ndf_train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","ea417eef":"# Raw data\ndf_train.head()","03eff801":"print(\"null text = \", pd.isna(df_train['text']).sum())\nprint(\"null sentiment = \", pd.isna(df_train['sentiment']).sum())","06366bf2":"df_train = df_train.dropna(subset=['text'], axis=0)\nprint(\"null text = \", pd.isna(df_train['text']).sum())","2eb11c4d":"df_train.describe()","dd504167":"temp = df_train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","b1953ce6":"# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\ndf_train['text_clean'] = df_train['text'].apply(lambda x: remove_URL(x))\ndf_train['text_clean'] = df_train['text_clean'].apply(lambda x: remove_html(x))\ndf_train['text_clean'] = df_train['text_clean'].apply(lambda x: remove_punct(x))\n\ndf_test['text_clean'] = df_test['text'].apply(lambda x: remove_URL(x))\ndf_test['text_clean'] = df_test['text_clean'].apply(lambda x: remove_html(x))\ndf_test['text_clean'] = df_test['text_clean'].apply(lambda x: remove_punct(x))","0aa41a6d":"df_train.head()","eb81da85":"print(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","54f25ba0":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","b9b844d2":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","2984019a":"# hyperparameters\nmax_length = 128\nbatch_size = 128","e9cc10ad":"# Bert Tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)","67d7af57":"train, dev = train_test_split(df_train, test_size=0.1, random_state=42)","03e19faf":"labels = df_train.sentiment.unique().tolist()\nlabels","465bcd85":"encoder = LabelEncoder()\nencoder.fit(df_train.sentiment.tolist())\n\ny_train = encoder.transform(df_train.sentiment.tolist())\ny_test = encoder.transform(df_test.sentiment.tolist())\ny_dev = encoder.transform(dev.sentiment.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\ny_dev = y_dev.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","8dd23ca5":"def bert_encode(data):\n    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n    \n    return tf.constant(tokens['input_ids'])","e08d48a1":"train_encoded = bert_encode(df_train.text_clean)\ndev_encoded = bert_encode(dev.text_clean)\n\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encoded, y_train))\n    .shuffle(128)\n    .batch(batch_size)\n)\n\ndev_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((dev_encoded, y_dev))\n    .shuffle(128)\n    .batch(batch_size)\n)","6413b178":"def bert_tweets_model():\n\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    last_hidden_states = bert_encoder(input_word_ids)[0]   \n    x = tf.keras.layers.SpatialDropout1D(0.2)(last_hidden_states)\n    x = tf.keras.layers.Conv1D(64, 3, activation='relu')(x)\n    x = tf.keras.layers.Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n    model = tf.keras.Model(input_word_ids, outputs)\n   \n    \n    return model","2305b820":"with strategy.scope():\n    model = bert_tweets_model()\n    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss='sparse_categorical_crossentropy',optimizer=adam_optimizer,metrics=['accuracy'])\n\n    model.summary()","02839536":"tf.keras.utils.plot_model(model, show_shapes=True)","fc6c24f0":"# Start train\nhistory = model.fit(\n    train_dataset,\n    batch_size=batch_size,\n    epochs=12,\n    validation_data=dev_dataset,\n    verbose=1)\n    #callbacks=[tf.keras.callbacks.EarlyStopping(\n    #            patience=6,\n    #            min_delta=0.05,\n    #            baseline=0.7,\n    #            mode='min',\n    #            monitor='val_accuracy',\n    #            restore_best_weights=True,\n    #            verbose=1)\n    #          ])","a3ee5825":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","8b3ac6ff":"def decode_sentiment(score):\n        if score == 0:\n            return \"negative\"\n        elif score == 1:\n            return \"neutral\"\n        else:\n            return \"positive\"","80247e5c":"def predict(text, include_neutral=True):\n    start_at = time.time()\n    # Tokenize text\n    x_encoded = bert_encode([text])\n    # Predict\n    score = model.predict([x_encoded])[0]\n    # Decode sentiment\n    label = decode_sentiment(np.argmax(score))\n\n    return {\"label\": label, \"score\": score,\n       \"elapsed_time\": time.time()-start_at}  ","ff47fb41":"predict(\"I hate the economy\")","ab5cd41f":"predict(\"I would prefer writing a crawler to create this dataset but i couldn't\")","0678c93e":"predict(\"I LOVE NLP\")","c33ead0c":"predict(\"life is really strange isn't it? just the combination of laugh and sadness\")","d4fab170":"predict(\"ESL is the world's largest esports company, leading the industry across the most popular video games.\\\n        We're proud they've chosen us to help them deliver their launchers to gamers all over the world. Read the full review\")","7238615e":"predict(\"Excited to present a tutorial on 'Modular and Parameter-Efficient Fine-Tuning for NLP Models' \\\n        at #EMNLP2022 with @PfeiffJo & @licwu.\")","4326302b":"predict(\"Had a song stuck in my head. Thirty seconds later I'm listening to it, thanks to the internet,\\\n        and Apple\/YouTube Music. In the bad old days I'd browse record stores for hours in the hope that the title might jog my memory.\\\n        It really is a wonderful time to be alive!\")","f1d967b1":"predict(\"i don't say this lightly - hemingway's life ended by suicide. His life was actually a loss\")","1543cc1f":"predict(\"these r not ur problems dear!!! these r ur x bf's commitng suicide\")","48875e0b":"predict(\"i hve no idea about that i love the uni or not\")","3721792c":"predict(\"I found some old Reddit post in which one guy from english-speaking country complains that\\\nthe names in The Witcher books are 'too difficult' and non- intuitive for english speaker.\\\nMan, let me introduce you to 'The books werent written only\/for english speakers.'' #witcher\")","50e26b5a":"predict(\"I forgot how cringy all the Slavic names sound read it English \\\nYOU'RE PRONOUNCING IT ALL WRONG MY EARS ARE HURTING AND I DON'T EVEN HAVE HEARING AIDS IN\")","46fc8941":"predict(\"fun fact: ai cannot predict everything right\")","3b6a4247":"predict(\"our brain is just a machine as well\")","836d7b66":"test_encoded = bert_encode(df_test.text_clean)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded)\n    .batch(batch_size)\n)\n\npredicted_tweets = model.predict(test_dataset, batch_size=batch_size)\npredicted_tweets_binary = tf.cast(tf.round(predicted_tweets), tf.int32).numpy().flatten()","e6c1008e":"y_pred = []\nfor i in range(predicted_tweets.shape[0]):\n    y_pred.append(np.argmax(predicted_tweets[i]))","63dc5c50":"print(classification_report(y_test, y_pred))","0fc98129":"!pip install twint\nimport twint\nimport nest_asyncio\nnest_asyncio.apply() \n!pip install --user --upgrade git+https:\/\/github.com\/twintproject\/twint.git@origin\/master#egg=twint","2404089a":"c = twint.Config()\nc.Search = \"GRAMMYs\" #keyword for search\nc.Limit = 100 #limit of the number of tweets which will be extracted\nc.Store_csv = True \nc.Output = '#GRAMMYs_tweet_data.csv'\ntwint.run.Search(c)","fc2dbeb2":"crawled_data = pd.read_csv(\"#GRAMMYs_tweet_data.csv\")\n#crawled_data = pd.read_json(\"tweet_data.json\", lines=True)\npd.options.display.max_columns=36\ncrawled_data.head()","8abf1d5c":"# prediction of the first 15 extracted tweets\nfor i in range(15):\n    print(crawled_data[\"tweet\"][i])\n    print(predict(crawled_data[\"tweet\"][i]))\n    print(\"\\n\")","ff9cd5bf":"# Splitting the dev data","49f2ac4f":"# Proposed Model","a57d55ad":"# Visualizing the Data","f80a94fc":"# Fetching data from Twitter\nTo get started,\n\n* Import the twint package as follows.","dc191e66":"# Read Dataset\n\n[Data source](www.kaggle.com\/c\/tweet-sentiment-extraction\/data)\n\nTrain data file format has 4 fields: \n* textID: The id of the tweet\n* text: the text of the tweet\n* selected_text: support phrases for sentiment labels\n* sentiment -> (negative, neutral, positive)\n\nTest data file format has 3 fields: \n* textID: The id of the tweet\n* text: the text of the tweet\n* sentiment -> (negative, neutral, positive)","ba67ddd7":"# Label Encoder","088ee17f":"_____\n### In next step, we will use our prepared model to predict tweets on twitter using Twint.","83a1e261":"# Predict Manually Before Using Test Data\n\nDecoder to be able to see results as labelled negative, positive or neutral","ddeff7a6":"# Contextual Model for Real Time Tweet Sentiment Prediction\n\nSentiment Analysis has an important role in today\u2019s world especially for private companies\nwhich hold lots of data. The massive amount of data generated by Twitter present a unique\nopportunity for sentiment analysis. However, it is challenging to build an accurate predictive\nmodel to identify sentiments, which may lack sufficient context due to the length limit. In\naddition, sentimental and regular ones can be hard to separate because of word ambiguity. In\nthis notebook, I will be proposing the phases of text pre-processing, visual analysis and modeling.\n\n***I tried to keep code as simple as possible to remain understandable.***\n\nProposed **BERT-CNN-BiLSTM-FC** learning pipeline, which consists of **four sequential modules**.<br \/>\nBERT produces competitive results, and can be considered as one of the new electricity of natural\nlanguage processing tasks such as sentiment analysis, named entity recognition (NER), and topic\nmodeling. The combination of CNN and BiLSTM models requires a particular design, since each\nmodel has a specific architecture and its own strengths:<br \/>\n\u2022 BERT is utilized to transform word tokens from the raw Tweet messages to contextual word\nembeddings.<br \/>\n\u2022 CNN is known for its ability to extract as many features as possible from the text.<br \/>\n\u2022 BiLSTM keeps the chronological order between words in a document, thus it has the ability\nto ignore unnecessary words using the delete gate.<br \/>\n\u2022 Fully Connected Layers give robustness to decrease unsteadiness of results in hard cases. \n\n## > If you find my work useful please don't forget to **Upvote!**  so it can reach more people.\n\n\n**References:**<br \/>\n1) [A Sentiment-Aware Contextual Model for Real-Time Disaster Prediction Using Twitter Data](https:\/\/www.mdpi.com\/1999-5903\/13\/7\/163\/htm) -> The idea comes from and really worth to check on, however, i am not using the same model.<br \/>\n2) [Automatic identification of eyewitness messages on twitter during disasters](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S0306457319303590?token=985D740724AEDB812611486EBAD3B68FA4393520D4DCD96FDADE4A642A9805D728945987C1BBBE0FDAA8EC3684E372C7&originRegion=eu-west-1&originCreation=20210920022341)<br \/>\n3) [Convolutional Neural Networks for Sentence Classification](http:\/\/arxiv.org\/abs\/1408.5882)<br \/>\n4) [BERT: Pre-training of Deep Bidirectional Transformers for Language\n               Understanding](http:\/\/arxiv.org\/abs\/1810.04805)<br \/>\n5) [LMAES' Notebook](https:\/\/www.kaggle.com\/lmasca\/disaster-tweets-using-bert-embeddings-and-lstm)<br \/>\n6) [PAOLO RIPAMONTI's Notebook](https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis)","e52bfe79":"________________","86950d1f":"# Evaluate","aba6fe71":"# Cleaning Text","d1f03e0c":"## We stored the related tweets in the .csv or .json file which is really fast and cool\n\nSo how we will read from csv\/json file to use for our purpose ? ","6542a686":"### If you want to know more about twint, you can checkout this Github link:\nhttps:\/\/github.com\/twintproject\/twint","2e9c665e":"# Setup environment to build model","adba630f":"_____\nAs you can see above we have lots of features which extracted by twint. However, we only need the \"tweet\" feature which includes the text data of tweets for our purpose."}}