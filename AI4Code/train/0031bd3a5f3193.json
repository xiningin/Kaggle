{"cell_type":{"4845630e":"code","5fd53d63":"code","7656e6b4":"code","30ab1329":"code","a8bb5678":"code","14abdf2c":"code","cca5e071":"code","b5ff49a0":"code","b6592578":"code","37bcc196":"code","ef25f6b8":"code","0ea10046":"code","87631843":"code","d02574b4":"code","abe9b750":"code","817bf41a":"code","e497e96f":"markdown","3beb0ba5":"markdown","9a34beca":"markdown","193a5a0f":"markdown","98784d24":"markdown","1b7a3157":"markdown","845ff73d":"markdown"},"source":{"4845630e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fd53d63":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","7656e6b4":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\ntrain_df","30ab1329":"train_df = train_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\ntest_df = test_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\n\n#complete missing age with median\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\n\n#complete embarked with mode\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\n\ntest_df['Fare'].fillna(test_df['Fare'].mode()[0], inplace = True)\n\ntrain_df","a8bb5678":"train_df['Sex'] = train_df['Sex'].map({'male':0,'female':1})\ntest_df['Sex'] = test_df['Sex'].map({'male':0,'female':1})\n\ntrain_df","14abdf2c":"for dataset in [train_df,test_df]:\n    \n    # Creating a categorical variable to tell if the passenger is alone\n    dataset['IsAlone'] = ''\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) > 0)] = 1\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) == 0)] = 0\n    \n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    # take only top 10 titles\n    title_names = (dataset['Title'].value_counts() < 10) #this will create a true false series with title name as index\n\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    print(dataset['Title'].value_counts())\n    \n    dataset.drop(['Name','SibSp','Parch'],axis=1,inplace=True)\n\ntrain_df.head()","cca5e071":"#define x and y variables for dummy features original\ntrain_dummy = pd.get_dummies(train_df,drop_first=True)\ntest_dummy = pd.get_dummies(test_df,drop_first=True)\n\ntrain_dummy","b5ff49a0":"X_final = train_dummy.drop(['Survived'],axis=1).values # for original features\ntarget = train_dummy['Survived'].values\nX_final","b6592578":"target.shape","37bcc196":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_final = sc.fit_transform(X_final)\nprint(X_final.shape,'\\n',X_final)","ef25f6b8":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nclass MYSVM:\n    def __init__(self,lr=.005,iters=1000,lambda_p=0.01):\n        self.lr=lr\n        self.iters=iters\n        self.lambda_p = lambda_p\n\n        self.w = None\n        self.b = None\n        \n    def fit(self,X,y):\n        n_samples,n_feat = X.shape\n        \n        self.w = np.zeros(n_feat)\n        self.b = 0\n        \n        y_true = np.where(y <= 0, -1, 1) # same as -1 if y <= 0 else 1\n        \n        for _ in range(self.iters):\n            for idx, sample in enumerate(X):\n                # do a forward pass\n                y_pred = y_true[idx]*(np.dot(sample,self.w)-self.b)\n                \n                # update with the graient\n                if y_pred>=1:\n                    self.w -= self.lr * (2 * self.lambda_p * self.w)\n                else:\n                    self.w -= self.lr * (2 * self.lambda_p * self.w - np.dot(sample, y_true[idx]))\n                    self.b -= self.lr * y_true[idx]\n        \n    def predict(self,X):\n        # do a forward pass\n        y = np.dot(X, self.w) - self.b\n        return np.sign(y) # if -1, belongs to class 0, else class 1\n        \n# X,y = make_blobs(centers = 3,n_samples = 1000, n_features = 3, shuffle = False)\nsvm_clf = MYSVM()","0ea10046":"svm_clf.fit(X_final,target)","87631843":"preds = svm_clf.predict(X_final) \npreds[:10]","d02574b4":"preds_ = np.where(preds==-1,0,1)\naccurate = 0\nfor i,val in enumerate(target):\n    if val==preds_[i]:\n        accurate+=1\nprint(accurate)\nprint(\"Accuracy = \",accurate\/len(preds))","abe9b750":"from sklearn.svm import SVC\nsvm_sk = SVC(gamma='auto').fit(X_final,target)\nsvm_preds = svm_sk.predict(X_final)\nsvm_preds[:10]","817bf41a":"accurate = 0\nfor i,val in enumerate(target):\n    if val==svm_preds[i]:\n        accurate+=1\nprint(accurate)\nprint(\"Accuracy = \",accurate\/len(svm_preds))","e497e96f":"- 78.4 vs 78.7\n- looks like we are close to the original classifier from sklearn!\n- we will improve on it more in later versions!","3beb0ba5":"### let's do Feature Engineering and add IsAlone and Title instead of Name,SibSp, and Parch.","9a34beca":"## Step 1:\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/72\/SVM_margin.png\/300px-SVM_margin.png)\n\nInitialize Weights and biases to None, then do a simple Forward pass of your data with the given formulas. \n\n- $w.x - b = 0$ \n\n- $w.x - b >= 1$ if y = 1; for the left margin \n- $w.x - b <= -1$ if y = -1; for the right margin.\n\nessentially,\n- $y(i)*(w.x(i)-b) >= 1$\n\n```\n    np.dot(X, self.w) - self.b [w.x-b]\n    y_true[i]*(np.dot(x,self.w) - b)  \n```\n\n## Step 2:\nCalculate Loss.\n\nNote: We will use Hinge Loss to get the margin, trying to minimize Bias-Variance TradeOff. \nMore about it here: --> \n(https:\/\/www.youtube.com\/watch?v=efR1C6CvhmE)\n\n![Hinge Loss](https:\/\/miro.medium.com\/max\/1042\/1*nFmhvEy6GyYQOYlF-L9XRw.png)\n\n## Step 3:\nUpdate Parameters!!\n\n1. This is where we update our weights and Biases with the outcome of loss and Gradients.\n\n***\n$if y_{i}.f(x) >=1$\n\n$\\frac{~dJ}{dw_{i}} = 2*\\lambda*w_{i}$\n\n$else$\n\n$\\frac{~dJ}{dw_{i}} = 2*\\lambda*w_{i} - y_{i}.x_{i}$\n\n***\n\nWe Update above Gradient into our weight as following:\n```\n    if y_pred>=1:\n        self.w -= self.lr * (2 * self.lambda_p * self.w)\n    else:\n        self.w -= self.lr * (2 * self.lambda_p * self.w - np.dot(sample, y_true[idx]))\n        self.b -= self.lr * y_true[idx]\n```\n\n\n## Final:\nRepeat for a number of iterations and Predict to see results!","193a5a0f":"## WAIT!!\n\n\n# Looking for 100 Data Science Interview Questions?\n## I've got you: *https:\/\/www.linkedin.com\/posts\/alaapdhall_50-of-100-data-science-interview-questions-activity-6716618160969269248-bX5W*\n\n## If you're Interested in Deep Learning with PyTorch, visit https:\/\/www.aiunquote.com for 100 project in Deep Learning Series!!\n\n---","98784d24":"### First understand what a SVM is:\nhere's a cool link for your reference!!\nhttps:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n\nNow we know that the main function is to\n1. Do a Linear Forward Pass\n2. Calculate Hinge Loss\n3. Calculate Graients\n4. Update weights and biases\n\nGreat!! Now how do we code?\nHere is a step by step Guidline with code!!\n","1b7a3157":"# How do we build a custom SVM in NUMPY?\n","845ff73d":"## This was a basic Custom K means Clustering Algo in Numpy vs the one in Sklearn!\n\n\n#### Please visit my website wherein I am doing 100 Projects in Deep Learning.\n---\n### [AI Unquote](https:\/\/www.aiunquote.com)\n### Follow me on LinkedIn where I post DS related Stuff everyday!!  [Alaap Dhall](https:\/\/www.linkedin.com\/in\/alaapdhall\/)\n***\n\n<br>\n\n**Thank you**"}}