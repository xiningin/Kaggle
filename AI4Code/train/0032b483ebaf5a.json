{"cell_type":{"35cc3820":"code","b5806a18":"code","e7530d5c":"code","36a7beb1":"code","1195310c":"code","c13237cd":"code","63ff09c3":"code","9be077a9":"code","015afe08":"code","1aff9d83":"code","6ee47d8c":"code","6897c991":"code","1c108c75":"code","9f88afd0":"code","48a4f3fb":"code","1d7cf1da":"code","8cc14ce5":"code","c4236203":"code","4c04f6ce":"code","416d6e15":"code","1365b117":"code","328f5090":"code","82e592ed":"code","e71fbb62":"code","ffd24577":"code","24050f45":"code","3eaa37a0":"code","4b1dba53":"code","e963da0c":"markdown"},"source":{"35cc3820":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import cv2\n\n# from IPython.display import display\n# from PIL import Image\n\n# import os\n# import glob\n\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# import torch\n# from torch.utils.data import Dataset\n# from torchvision import datasets\n# from torchvision.transforms import ToTensor\n# import matplotlib.pyplot as plt\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# import os\n# for dirname, dirs, filenames in os.walk('\/kaggle\/input'):\n#     print(dirs)\n#     if len(dirs) != 0 and dirs[0] == \"images\":\n#         for filename in filenames:\n#             #We need to do some data cleanup for the images and shrink them all to the same image size (60X60)\n#             imgPath = os.path.join(dirname, filename)\n#             img = Image.open(imgPath)\n\n#             #RESIZE\n#             img = img.resize((60, 60), Image.ANTIALIAS)\n#             #SAVE IN NEW ANIME FOLDER\n#             img.save(\"\/kaggle\/working\/animeface-resized\/\" + filename)\n#     else:\n#         for filename in filenames:\n#             #We need to do some data cleanup for the images and shrink them all to the same image size (60X60)\n#             imgPath = os.path.join(dirname, filename)\n#             img = Image.open(imgPath)\n\n#             #RESIZE\n#             img = img.resize((60, 60), Image.ANTIALIAS)\n#             #SAVE IN NEW FOLDER\n#             img.save(\"\/kaggle\/working\/other-resized\/\" + filename)\n\n#TODO:\n#DOWNLOAD RANDOM IMAGES\nimport requests\npath = \"\/kaggle\/root\/non-nft\"\n#load new image data into non-anime folder\nfor i in range(9200):\n    url = \"https:\/\/picsum.photos\/200\/200\/?random\"\n    response = requests.get(url)\n    print(response)\n    if response.status_code == 200:\n        file_name = 'not_anime_{}.jpg'.format(i)\n        file_path = path + \"\/\" + file_name\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n\n\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5806a18":"!mkdir \/kaggle\/root && mkdir \/kaggle\/root\/non-nft && mkdir \/kaggle\/root\/nft","e7530d5c":"!ls \/kaggle\/root\/ && cp -a \/kaggle\/input\/unsplash-random-images-collection\/. \/kaggle\/root\/non-nft","36a7beb1":"#create root folder with all anime\n! cd ..\/ && cp -a \/kaggle\/input\/bored-apes-yacht-club\/bayc\/. \/kaggle\/root\/nft && echo \"HEY\"","1195310c":"#copy over animal data\n!cd ..\/ && mkdir \/kaggle\/root\/non-nft && ls \/kaggle\/root\/non-nft","c13237cd":"#to delete some images\n# ls -U | head -40000 | xargs rm -rf\n!cd \/kaggle\/root\/non-nft && ls -U | head -595 | xargs rm -rf","63ff09c3":"!cp -a \/kaggle\/root\/non-nft\/unsplash-images-collection\/. \/kaggle\/root\/non-nft","9be077a9":"#check that non-anime\n!ls \/kaggle\/root\/non-nft && cp -a \/kaggle\/root\/non-nft\/unsplash-images-collection\/. \/kaggle\/root\/non-nft && rm -rf \/kaggle\/root\/non-nft\/unsplash-images-collection","015afe08":"!ls \/kaggle\/root\/nft | wc -l","1aff9d83":"!ls \/kaggle\/root\/non-nft | wc -l ","6ee47d8c":"#copy over lion pics!\ncp -a \/kaggle\/input\/lion-1000\/scraped_images\/. \/kaggle\/root\/nft","6897c991":"#split images into training set and test set\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms, Compose, ToTensor, Resize\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\n\ndef train_val_dataset(dataset, val_split=0.25):\n    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n    datasets = {}\n    datasets['train'] = Subset(dataset, train_idx)\n    datasets['val'] = Subset(dataset, val_idx)\n    return datasets\n\ndataset = ImageFolder('\/kaggle\/root', transform=Compose([\n                                                        transforms.Resize((150,150)),\n                                                        transforms.RandomHorizontalFlip(p=0.35),\n                                                        transforms.RandomRotation(20, expand=True),\n                                                        transforms.Resize((150,150)),\n\n                                                         ToTensor()]))\nimg, label = dataset[0]\nprint(img.shape, label)\nprint(\"Follwing classes are there : \\n\",dataset.classes)\n\nprint(len(dataset))\ndatasets = train_val_dataset(dataset)\nprint(len(datasets['train']))\nprint(len(datasets['val']))\n# The original dataset is available in the Subset class\nprint(datasets['train'].dataset)\n\nbatchsize = 32\n\ndataloaders = {x:DataLoader(datasets[x], batchsize, shuffle=True, num_workers=4) for x in ['train','val']}\nx,y = next(iter(dataloaders['train']))\ntrainloader = dataloaders['train']\nvalidationloader = dataloaders['val']\nprint(trainloader)\nprint(validationloader)\nprint(x.shape, y.shape)\nplt.imshow(np.transpose(x[0].numpy(), (1,2,0)))\n\n","1c108c75":"from torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageFile\n\ndef show_batch(dl):\n    \"\"\"Plot images grid of single batch\"\"\"\n    for images, labels in dl:\n        fig,ax = plt.subplots(figsize = (16,12))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n        break\n        \nshow_batch(trainloader)\n# im = Image.open('\/kaggle\/root\/anime\/12185_2005.jpg')\n \n# im.show()","9f88afd0":"import torch.nn as nn\nimport torch.nn.functional as F\n\n#ARCHITECTURE\n\nclass ImageClassificationBase(nn.Module):\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \nclass NaturalSceneClassification(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            \n            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n            nn.ReLU(),\n            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n        \n            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            \n            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            \n            nn.Flatten(),\n            nn.Linear(82944,1024),\n            nn.Dropout(p=0.5),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.Dropout(p=0.5),\n            nn.ReLU(),\n            nn.Linear(512,2)\n        )\n    \n    def forward(self, xb):\n        return self.network(xb)","48a4f3fb":"#TRAINING DEF\n    \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\n  \n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n  \ndef train(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n    \n    history = []\n    optimizer = opt_func(model.parameters(),lr)\n    for epoch in range(epochs):\n        \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    \n    return history","1d7cf1da":"def get_default_device():\n    \"\"\" Set Device to GPU or CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \n\ndef to_device(data, device):\n    \"Move data to the device\"\n    if isinstance(data,(list,tuple)):\n        return [to_device(x,device) for x in data]\n    return data.to(device,non_blocking = True)\n\nclass DeviceDataLoader():\n    \"\"\" Wrap a dataloader to move data to a device \"\"\"\n    \n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n    \n    def __iter__(self):\n        \"\"\" Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl:\n            yield to_device(b,self.device)\n            \n    def __len__(self):\n        \"\"\" Number of batches \"\"\"\n        return len(self.dl)\ndevice = get_default_device()\ndevice","8cc14ce5":"# load the into GPU\nmodel = to_device(NaturalSceneClassification(),device)\ntrain_dl = DeviceDataLoader(trainloader, device)\nval_dl = DeviceDataLoader(validationloader, device)\n","c4236203":"#eval before training with gpu this time\nevaluate(model,val_dl)","4c04f6ce":"import torch\nwith torch.no_grad():\n    torch.cuda.empty_cache()","416d6e15":"#set the no. of epochs, optimizer funtion and learning rate\nnum_epochs = 4\n\nopt_func = torch.optim.Adam\nlr = 0.001\nhistory = train(num_epochs, lr, model, train_dl, val_dl, opt_func)\n","1365b117":"def plot_accuracies(history):\n    \"\"\" Plot the history of accuracies\"\"\"\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n    \n\nplot_accuracies(history)","328f5090":"def plot_losses(history):\n    \"\"\" Plot the losses in each epoch\"\"\"\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n\nplot_losses(history)","82e592ed":"#save model\ntorch.save(model.state_dict(), '\/kaggle\/output\/out\/nft.pth')\n","e71fbb62":"!mkdir \/kaggle\/output\/out","ffd24577":"from torchvision import transforms\n\ndef predict_img_class(img,model):\n    \"\"\" Predict the class of image and Return Predicted Class\"\"\"\n    img = to_device(img.unsqueeze(0), device)\n    prediction =  model(img)\n    _, preds = torch.max(prediction, dim = 1)\n    return dataset.classes[preds[0].item()]\n\nfrom PIL import Image\ncute1 = '..\/input\/one-cute-anime\/c8bda99ff35a1aca879a77d8dfb84dd2.jpg'\ncute2 = '..\/input\/cute-girl-jpg\/0e3ed05ff407671bdebf77b53ce0e0b9.jpg'\n\ncute3 = '..\/input\/anime-faces\/data\/1.png'\ncute4 = '..\/input\/anime-faces\/data\/10006.png'\n\ncute5 = '..\/input\/val-nft\/todelete\/DI5kXteh5h3RhkrID8tNuHJ7hhXJen1GV1BpVEeD.jpg'\n\ncute10 = '..\/input\/val-nft\/todelete\/DI5kXteh5h3RhkrID8tNuHJ7hhXJen1GV1BpVEeD.jpg'\n\ncute6 = '..\/input\/val-nft\/todelete\/M3nWWndkz6QXcZ4ueCfESH-1200-80.jpg'\ncute8 = '..\/input\/test2lions\/FAsmOMFUUAAsWAG.jpg'\n#open image file\ncute9 = '..\/input\/val-nft\/todelete\/PBWbdeVa_400x400.jpg'\n\nimg = Image.open(cute6)\n#convert image to tensor\n#resize\nimg = img.resize((150, 150), Image.ANTIALIAS)\nimg = transforms.ToTensor()(img)\n\n#print image\nplt.imshow(img.permute(1,2,0))\n\n#prdict image label\nprint(f\"Predicted Class : {predict_img_class(img,model)}\")","24050f45":"!mkdir \/kaggle\/working\/root && ls \/kaggle\/working\/root && ls \/kaggle ","3eaa37a0":"#working will be used as test dataset\n!cp -a \/kaggle\/input\/anime-faces\/data\/. \/kaggle\/working\/root\/anime","4b1dba53":"#eval model on testroot dataset\ntest_data_dir = \"\/kaggle\/working\/root\"\ntest_dataset = ImageFolder(test_data_dir,transforms.Compose([\n    transforms.Resize((150,150)),transforms.ToTensor()\n]))\ntest_loader = DeviceDataLoader(DataLoader(test_dataset, batchsize*2), device)\nresult = evaluate(model, test_loader)\nprint(result)\n","e963da0c":"# "}}