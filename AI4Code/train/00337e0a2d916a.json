{"cell_type":{"a27dee31":"code","0099df78":"code","a91d48f5":"code","ce4120df":"code","b508b75e":"code","859e0d4e":"code","12cf69a4":"code","2bf78dcb":"code","226917c2":"code","e4c10c53":"code","27ae18bf":"code","8e73da48":"code","5af44167":"code","8f11c3b1":"code","d6060eb7":"code","ca2778aa":"code","6a0334d3":"code","4aa408c6":"code","5eac0427":"code","e6c95e35":"code","b415df0b":"code","43b49013":"code","46fcf28f":"code","fcb4c5a7":"code","125a848a":"code","ef2afd9e":"code","8acd5b44":"code","e1223182":"code","2569ae83":"code","90c96716":"code","9f059b1f":"code","60e20b3e":"code","45e96893":"markdown"},"source":{"a27dee31":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport os","0099df78":"CA = pd.read_csv('\/kaggle\/input\/ibm-attrition-analysis\/WA_Fn-UseC_-HR-Employee-Attrition.csv')","a91d48f5":"CA.head()\n# CA = Comapny_Attrition","ce4120df":"CA.shape\nCA.isnull().sum()","b508b75e":"CA.drop(['StandardHours','Over18','EmployeeCount','EmployeeNumber'], axis='columns', inplace=True)","859e0d4e":"CA.BusinessTravel.value_counts()","12cf69a4":"import seaborn as sns\nsns.countplot(CA['Attrition'])","2bf78dcb":"CA.Gender.replace({\"Male\": 1, \"Female\" : 0}, inplace = True)\nCA.Attrition.replace({\"Yes\": 1, \"No\" : 0}, inplace = True)\nCA.OverTime.replace({\"Yes\": 1, \"No\" : 0}, inplace = True)\nCA.BusinessTravel.replace({\"Travel_Frequently\":2,\"Travel_Rarely\": 1, \"Non-Travel\" : 0}, inplace = True)","226917c2":"CA.Attrition.value_counts()","e4c10c53":"CA.Department.value_counts()","27ae18bf":"CA.EducationField.value_counts()","8e73da48":"CA.JobRole.value_counts()","5af44167":"CA.MaritalStatus.value_counts()","8f11c3b1":"CA.OverTime.value_counts()","d6060eb7":"CA = pd.get_dummies(CA, columns=['Department','EducationField','JobRole','MaritalStatus'], drop_first=True)","ca2778aa":"CA.info()","6a0334d3":"CA['Overall_satisfaction'] = CA['EnvironmentSatisfaction']+CA['JobInvolvement']+CA['JobSatisfaction']+CA['RelationshipSatisfaction']\nCA.drop(['EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','RelationshipSatisfaction'], axis='columns', inplace=True)","4aa408c6":"CA.head()","5eac0427":"CA.shape","e6c95e35":"target = 'Attrition'\ny = CA[target]\nx = CA.drop(columns=target)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestfeatures = SelectKBest(score_func = chi2, k = 40)\nfit = bestfeatures.fit(x,y)\natt_pval = pd.DataFrame(np.round(fit.pvalues_,5))\natt_scores = pd.DataFrame(fit.scores_)\natt_columns = pd.DataFrame(x.columns)\n\nFeat_score = pd.concat([att_columns,att_scores, att_pval], axis = 1)\nFeat_score.columns = ['Feature','Score','P_value']","b415df0b":"Feat_score.nlargest(26,\"Score\")","43b49013":"best_feat = Feat_score.nlargest(26,\"Score\").Feature.values","46fcf28f":"Y = CA['Attrition']\nX = CA.loc[:,best_feat]","fcb4c5a7":"X.head()","125a848a":"CA_x_train, CA_x_test, CA_y_train, CA_y_test = train_test_split(X, Y, test_size = 0.2 )","ef2afd9e":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Precision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Recall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"F1 Score: {f1_score(y_train, pred, average='weighted') * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Precision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Recall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"F1 Score: {f1_score(y_test, pred, average='weighted') * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","8acd5b44":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline","e1223182":"dt_params = {\n    \"class_weight\":(None, \"balanced\"),\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20)), None), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, dt_params, scoring=\"f1\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(CA_x_train, CA_y_train)\ndt_best_params = tree_cv.best_params_\nprint(f\"Best paramters: {dt_best_params})\")\n\ntree_clf = DecisionTreeClassifier(**dt_best_params, random_state=42)\nsm = SMOTE(random_state=42)\ndt_pipeline = make_pipeline(sm, tree_clf)\ndt_pipeline.fit(CA_x_train, CA_y_train)\nprint_score(dt_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=True)\nprint_score(dt_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=False)","2569ae83":"abc_param_grid = {\"n_estimators\": [10, 20,50,100,500,1000],\n              \"learning_rate\" : [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.5, 0.75, 0.9, 1.0]}\n\ntree_clf = DecisionTreeClassifier(random_state = 42, **dt_best_params)\nabc = AdaBoostClassifier(base_estimator = tree_clf)\n\nabc_cv = RandomizedSearchCV(estimator=abc, scoring='f1',param_distributions=abc_param_grid, n_iter=60, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\nabc_cv.fit(CA_x_train, CA_y_train)\nabc_best_params = abc_cv.best_params_\nprint(f\"Best paramters: {abc_best_params})\")\n\nabc = AdaBoostClassifier(**abc_best_params, random_state=42)\nsm = SMOTE(random_state=42)\nabc_pipeline = make_pipeline(sm, abc)\nabc_pipeline.fit(CA_x_train, CA_y_train)\nprint_score(abc_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=True)\nprint_score(abc_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=False)","90c96716":"n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4, 10]\nbootstrap = [True, False]\noob_score = [True, False]\nclass_weight = [\"balanced\", \"balanced_subsample\", None]\ncriterion = [\"gini\", \"entropy\"]\n\nrf_param_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap,\n               'oob_score' : oob_score, 'class_weight':class_weight, 'criterion': criterion}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=rf_param_grid, n_iter=100, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\n\n\nrf_cv.fit(CA_x_train, CA_y_train)\nrf_best_params = rf_cv.best_params_\nprint(f\"Best parameters: {rf_best_params}\")\n\nrf_clf = RandomForestClassifier(**rf_best_params, random_state=42)\nsm = SMOTE(random_state=42)\nrf_pipeline = make_pipeline(sm, rf_clf)\nrf_pipeline.fit(CA_x_train, CA_y_train)\nprint_score(rf_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=True)\nprint_score(rf_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=False)","9f059b1f":"lr_random_grid = {'penalty' : ['l1', 'l2'], 'C' : np.logspace(-4, 4, 20), 'solver' : ['liblinear'], 'class_weight' : ['balanced', None],\n 'max_iter' : [10,50,100,200,500,750, 950,1000, 1500, 2000]}\n\nlr_clf = LogisticRegression(random_state=42)\n \nlr_cv = RandomizedSearchCV(estimator=lr_clf, scoring='f1',param_distributions=lr_random_grid, n_iter=100,  cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\n\n\nlr_cv.fit(CA_x_train, CA_y_train)\nlr_best_params = lr_cv.best_params_\nprint(f\"Best parameters: {lr_best_params}\")\n\nlr_clf = LogisticRegression(**lr_best_params, random_state=42)\nsm = SMOTE(random_state=42)\nlr_pipeline = make_pipeline(sm, lr_clf)\nlr_pipeline.fit(CA_x_train, CA_y_train)\nprint_score(lr_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=True)\nprint_score(lr_pipeline, CA_x_train, CA_y_train, CA_x_test, CA_y_test, train=False)","60e20b3e":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(CA_y_test, lr_pipeline.predict_proba(CA_x_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(CA_y_test, rf_pipeline.predict_proba(CA_x_test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(CA_y_test, dt_pipeline.predict_proba(CA_x_test)[:,1])\nada_fpr, ada_tpr, ada_thresholds = roc_curve(CA_y_test, abc_pipeline.predict_proba(CA_x_test)[:,1])\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc_score(CA_y_test, lr_pipeline.predict(CA_x_test)))\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % roc_auc_score(CA_y_test, dt_pipeline.predict(CA_x_test)))\n\n# Plot Decision Tree ROC\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % roc_auc_score(CA_y_test, rf_pipeline.predict(CA_x_test)))\n\n# Plot AdaBoost ROC\nplt.plot(ada_fpr, ada_tpr, label='AdaBoost (area = %0.2f)' % roc_auc_score(CA_y_test, abc_pipeline.predict(CA_x_test)))\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","45e96893":"**Feature_Selection**"}}