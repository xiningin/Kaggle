{"cell_type":{"835f7f91":"code","5ea710ab":"code","94ff596b":"code","ff0afd6b":"code","51db1d9a":"code","67ecfd3b":"code","ea601285":"code","cd55104a":"code","dcfa4808":"code","f73897f4":"code","86d3f4dc":"code","fc63b5d3":"code","b1f8bb14":"code","8a8355ca":"code","f6b16896":"code","85145b54":"code","76da0b5d":"code","86cea98d":"markdown","95fb4e7c":"markdown","853b7e05":"markdown","6a3a47ed":"markdown","da7916de":"markdown","6ff568f7":"markdown","1ad45b91":"markdown","a4f5dfbd":"markdown","55b21f09":"markdown","82f32be5":"markdown","baac8827":"markdown","fae5a068":"markdown","06c284a9":"markdown","98e4df9d":"markdown","90f5c301":"markdown","01fe9910":"markdown","a0a8df84":"markdown","e7b5ec86":"markdown"},"source":{"835f7f91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ea710ab":"df_business = pd.read_csv('\/kaggle\/input\/stock-market-prediction\/business_train.csv', header = 0)\ndf_finance = pd.read_csv('\/kaggle\/input\/stock-market-prediction\/finance_train.csv', header = 0)\ndf_price = pd.read_csv('\/kaggle\/input\/stock-market-prediction\/price_train.csv', header = 0)\nprint('df_business.shape: ', df_business.shape, '; total symbols: ', len(df_business['symbol'].unique()))\nprint('df_finance.shape: ', df_finance.shape, '; total symbols: ', len(df_finance['symbol'].unique()))\nprint('df_price.shape: ', df_price.shape, '; total symbols: ', len(df_price['symbol'].unique()))","94ff596b":"import pandas as pd\n\ndf_horizontal_symbols = pd.pivot_table(df_price,\n               index = 'date',\n               columns = 'symbol',\n               values = 'close',\n               aggfunc = {\n                   'close': lambda x: x\n               }\n              )\n\ndf_horizontal_symbols.head(5)","ff0afd6b":"# Ki\u1ec3m tra nan values:\ndf_horizontal_symbols.isna().sum()","51db1d9a":"num_na = df_horizontal_symbols.isna().sum(axis=1)\nidx_na = np.where(num_na != 0)[0][0]\ndf_horizontal_symbols['NVL'].iloc[idx_na-3: idx_na+3]","67ecfd3b":"df_horizontal_symbols['NVL'] = df_horizontal_symbols['NVL'].interpolate()\ndf_horizontal_symbols['NVL'].iloc[idx_na-3: idx_na+3]","ea601285":"import torch\nfrom sklearn.preprocessing import MinMaxScaler \n\n# Configure the window size\nseq_length = 3 # input time step\nnum_predict = 1 # number of step in target\n\ndef sliding_windows(data, seq_length, num_predict=1):\n    x = []\n    y = []\n\n    for i in range(data.shape[0]-seq_length-num_predict):\n        # _x = data.iloc[i:(i+seq_length), :].values # shape (seq_length, num_symbols)\n        # _y = data.iloc[(i+seq_length):(i+seq_length+num_predict), :].values[0] # shape (num_symbols,)\n        _x = data[i:(i+seq_length), :] # shape (seq_length, num_symbols)\n        _y = data[(i+seq_length):(i+seq_length+num_predict), :][0] # shape (num_symbols,)\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y)\n\n# Scaling dataset\nsc = MinMaxScaler()\ntraining_data = sc.fit_transform(df_horizontal_symbols)\n\nX, y = sliding_windows(training_data, seq_length=seq_length, num_predict=num_predict)\n\nprint(X.shape, y.shape)\n# Train\/test split\ntrain_size = int(len(y) * 0.9)\ntest_size = len(y) - train_size\n\n# Convert into variable\ndataX = torch.Tensor(np.array(X))\ndataY = torch.Tensor(np.array(y))\n\ntrainX = torch.Tensor(np.array(X[0:train_size]))\ntrainY = torch.Tensor(np.array(y[0:train_size]))\n\nvalX = torch.Tensor(np.array(X[train_size:len(X)]))\nvalY = torch.Tensor(np.array(y[train_size:len(y)]))\n\nprint(trainX.shape, valX.shape)\nprint(trainY.shape, valY.shape)","cd55104a":"from torch import nn\n\nclass LSTM(nn.Module):\n    def __init__(self, num_seq, input_size, hidden_size, num_layers):\n        '''\n        num_seq: number of output sequence values\n        input_size: length of input\n        hidden_size: embedding vector for at, xt\n        num_layers: total number of RNN layers\n\n        '''\n        super(LSTM, self).__init__()\n        \n        self.num_seq = num_seq\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        # https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LSTM.html\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, num_seq)\n\n    def forward(self, x):\n        \n        a_0 = torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size)\n        \n        c_0 = torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size)\n        \n        # Propagate input through LSTM\n        ula, (a_out, c_out) = self.lstm(x, (a_0, c_0))\n        \n        a_out = a_out.view(-1, self.hidden_size) # N * hidden_size\n      \n        out = self.fc(a_out) # N * num_classes\n        \n        return out","dcfa4808":"# 1. Declare paramters:\nnum_epochs = 2000\nlearning_rate = 0.01\ninput_size = 30\nhidden_size = 2\nnum_layers = 1\nnum_seq = 30\n\n# 2. Initialize model\nlstm = LSTM(num_seq, input_size, hidden_size, num_layers)\n\n# 3. Optimizer\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n# 4. Train the model\nfor epoch in range(num_epochs):\n    # 4.a. Feed forward\n    outputs = lstm(trainX)\n\n    # Don't for get to reset optimizer into zero\n    optimizer.zero_grad()\n\n    loss = criterion(outputs, trainY)\n\n    # 4.b. Back propagation    \n    # obtain the loss function\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 100 == 0:\n      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))","f73897f4":"# Predict data\nlstm.eval()\ntrain_test_predict = lstm(dataX)","86d3f4dc":"import matplotlib.pyplot as plt\n\n# Convert tensor to numpy\ny_predict = train_test_predict.data.numpy()\ny_target = dataY.data.numpy()\n\n# Inverse scaling\ny_predict = sc.inverse_transform(y_predict)\ny_target = sc.inverse_transform(y_target)\n\n# Plot\ndef plt_line(target, pred, symbols):\n    colors = ['green', 'blue', 'black', 'red', 'yellow']\n    def _label(symbol, actual = True):\n        label = symbol + '-actual' if actual else symbol + '-predict'\n        return label\n    plt.figure(figsize=(18, 12))\n    plt.axvline(x=train_size, c='r', linestyle='--')\n    plt.text(x=train_size, y=40, s='train test split')\n    for i, symbol in enumerate(symbols):\n        plt.plot(target[:, i], label=_label(symbol, True), color=colors[i])\n        plt.plot(pred[:, i], label=_label(symbol, False), linestyle='--', color=colors[i])\n    plt.legend()\n    plt.suptitle('Vnquant Time-Series Prediction')\n    plt.show()\n    \nall_symbols = list(df_horizontal_symbols.columns)\nfor i in range(6):\n    symbols = all_symbols[i*5:(i+1)*5]\n    plt_line(y_target[:, i*5:(i+1)*5], y_predict[:, i*5:(i+1)*5], symbols)","fc63b5d3":"# 1. Declare paramters:\nnum_epochs = 2000\nlearning_rate = 0.01\ninput_size = 30\nhidden_size = 2\nnum_layers = 1\nnum_seq = 30\n\n# 2. Initialize model\nlstm = LSTM(num_seq, input_size, hidden_size, num_layers)\n\n# 3. Optimizer\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n# 4. Train the model\nfor epoch in range(num_epochs):\n    # 4.a. Feed forward\n    outputs = lstm(dataX)\n\n    # Don't for get to reset optimizer into zero\n    optimizer.zero_grad()\n\n    loss = criterion(outputs, dataY)\n\n    # 4.b. Back propagation    \n    # obtain the loss function\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 100 == 0:\n      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))","b1f8bb14":"df_price_test = pd.read_csv('\/kaggle\/input\/stock-market-prediction\/price_test.csv', header = 0)\nprint(df_price_test.shape)\ndf_price_test.head()","8a8355ca":"df_horizontal_test = pd.pivot_table(df_price_test,\n               index = 'date',\n               columns = 'symbol',\n               values = 'close',\n               aggfunc = {\n                   'close': lambda x: -1.\n               }\n              )\n\ndf_horizontal_test.head(5)","f6b16896":"lstm.eval()\n\ninit_X = dataX[-1:, :, :]\nforecast_dates = list(df_horizontal_test.index)\n\nfor i, date in enumerate(forecast_dates): \n    y_batch = lstm(init_X).detach().numpy()\n    y_batch = sc.inverse_transform(y_batch)\n    df_horizontal_test.loc[date] = y_batch[0]\n    init_X = torch.cat([init_X[:, 1:, :], torch.unsqueeze(torch.Tensor(y_batch), axis=0)], axis=1)\n\ndf_horizontal_test.head()","85145b54":"df_forecast = df_horizontal_test.unstack().reset_index()[['date', 'symbol', 0]]\ndf_forecast.columns = ['date', 'symbol', 'close']\ndf_forecast.head()","76da0b5d":"# Sort submission in order of date and symbol like in submission file.\nsubmission = pd.merge(df_price_test, df_forecast,\n         left_on=['date', 'symbol'], \n         right_on=['date', 'symbol'], \n         how='inner', \n         suffixes=['_replace', ''])[['date', 'symbol', 'close']]\n\nsubmission['Id'] = submission[['date', 'symbol']].apply(lambda x: str(x[0])+':'+x[1], axis=1)\nsubmission['Predicted'] = submission['close']\nsubmission[['Id', 'Predicted']].to_csv('submission.csv', index=False)","86cea98d":"B\u1ea1n c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh LSTM v\u1edbi s\u1ed1 l\u01b0\u1ee3t epochs l\u00e0 2000, learning_rate = 0.01; h\u00e0m m\u1ee5c ti\u00eau \u0111\u01b0\u1ee3c l\u1ef1a ch\u1ecdn l\u00e0 MSE v\u00e0 ph\u01b0\u01a1ng ph\u00e1p Optimizer l\u00e0 Adam nh\u01b0 b\u00ean d\u01b0\u1edbi:","95fb4e7c":"# Preprocessing Data","853b7e05":"Th\u1ecb tr\u01b0\u1eddng ch\u1ee9ng kho\u00e1n \u0111\u01b0\u1ee3c bi\u1ebft \u0111\u1ebfn l\u00e0 d\u1ec5 bi\u1ebfn \u0111\u1ed9ng, lu\u00f4n thay \u0111\u1ed5i kh\u00f3 l\u01b0\u1eddng v\u00e0 phi tuy\u1ebfn t\u00ednh. D\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ed5 phi\u1ebfu ch\u00ednh x\u00e1c l\u00e0 v\u00f4 c\u00f9ng kh\u00f3 kh\u0103n v\u00ec ph\u1ee5 thu\u1ed9c v\u00e0o nhi\u1ec1u y\u1ebfu t\u1ed1 v\u0129 m\u00f4 v\u00e0 vi m\u00f4, ch\u1eb3ng h\u1ea1n nh\u01b0 y\u1ebfu t\u1ed1 ch\u00ednh tr\u1ecb, b\u1ed1i c\u1ea3nh kinh t\u1ebf trong n\u01b0\u1edbc v\u00e0 kinh t\u1ebf to\u00e0n c\u1ea7u, c\u00e1c c\u00fa s\u1ed1c b\u1ea5t ng\u1edd (Covid-19, thi\u00ean tai, m\u1ea5t m\u00f9a,...), t\u00ecnh h\u00ecnh ho\u1ea1t \u0111\u1ed9ng t\u00e0i ch\u00ednh c\u1ee7a c\u00f4ng ty, v.v.\n\nTuy nhi\u00ean, t\u1ea5t c\u1ea3 nh\u1eefng \u0111i\u1ec1u n\u00e0y c\u0169ng c\u00f3 ngh\u0129a l\u00e0 c\u00f3 r\u1ea5t nhi\u1ec1u d\u1eef li\u1ec7u \u0111\u1ec3 t\u00ecm ra c\u00e1c pattern c\u1ee7a chu\u1ed7i ch\u1ee9ng kho\u00e1n. V\u00ec v\u1eady, c\u00e1c chuy\u00ean gia ph\u00e2n t\u00edch t\u00e0i ch\u00ednh, c\u00e1c nh\u00e0 nghi\u00ean c\u1ee9u v\u00e0 c\u00e1c nh\u00e0 khoa h\u1ecdc d\u1eef li\u1ec7u \u0111ang ti\u1ebfp t\u1ee5c kh\u00e1m ph\u00e1 c\u00e1c k\u1ef9 thu\u1eadt ph\u00e2n t\u00edch \u0111\u1ec3 ph\u00e1t hi\u1ec7n xu h\u01b0\u1edbng v\u00e0 qui lu\u1eadt v\u1eadn \u0111\u1ed9ng c\u1ee7a th\u1ecb tr\u01b0\u1eddng ch\u1ee9ng kho\u00e1n. \u0110i\u1ec1u n\u00e0y \u0111\u00e3 l\u00e0m n\u1ea3y sinh kh\u00e1i ni\u1ec7m giao d\u1ecbch theo thu\u1eadt to\u00e1n (_algorithmic trading_), s\u1eed d\u1ee5ng c\u00e1c chi\u1ebfn l\u01b0\u1ee3c giao d\u1ecbch t\u1ef1 \u0111\u1ed9ng, \u0111\u01b0\u1ee3c l\u1eadp tr\u00ecnh s\u1eb5n \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c l\u1ec7nh.\n\nTrong cu\u1ed9c thi n\u00e0y c\u00e1c \u0111\u1ed9i ch\u01a1i c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u1ea3 ph\u01b0\u01a1ng ph\u00e1p lu\u1eadn t\u00e0i ch\u00ednh \u0111\u1ecbnh l\u01b0\u1ee3ng truy\u1ec1n th\u1ed1ng v\u00e0 thu\u1eadt to\u00e1n m\u00e1y h\u1ecdc \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ed5 phi\u1ebfu. C\u00f3 nhi\u1ec1u h\u01b0\u1edbng d\u1ef1 b\u00e1o kh\u00e1c nhau m\u00e0 \u0111\u1ed9i ch\u01a1i c\u00f3 th\u1ec3 l\u1ef1a ch\u1ecdn ch\u1eb3ng h\u1ea1n:\n\n* Ph\u00e2n t\u00edch c\u1ed5 phi\u1ebfu theo ph\u01b0\u01a1ng ph\u00e1p ph\u00e2n t\u00edch c\u01a1 b\u1ea3n v\u00e0 ph\u01b0\u01a1ng ph\u00e1p ph\u00e2n t\u00edch k\u1ef9 thu\u1eadt\n* D\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ed5 phi\u1ebfu b\u1eb1ng k\u1ef9 thu\u1eadt Moving Average.\n* \u00c1p d\u1ee5ng c\u00e1c thu\u1eadt to\u00e1n RNN nh\u01b0 LSTM, GRU.\n* S\u1eed d\u1ee5ng c\u00e1c l\u1edbp m\u00f4 h\u00ecnh d\u1ef1 b\u00e1o timeseries truy\u1ec1n th\u1ed1ng nh\u01b0 AR, MA, ARIMA, SARIMA, ARIMAX, GARCH,....\n\nTrong v\u00ed d\u1ee5 m\u1eabu n\u00e0y ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng c\u00e1ch ti\u1ebfp c\u1eadn m\u00f4 h\u00ecnh \u0111\u1ec3 d\u1ef1 b\u00e1o chu\u1ed7i gi\u00e1 close th\u00f4ng qua thu\u1eadt to\u00e1n LSTM. V\u1ec1 nguy\u00ean l\u00fd ho\u1ea1t \u0111\u1ed9ng c\u1ee7a thu\u1eadt to\u00e1n LSTM b\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 theo d\u00f5i th\u00eam t\u1ea1i [Transformer Anatomy](https:\/\/youtu.be\/XGU99IesqvE).","6a3a47ed":"**X\u00e2y d\u1ef1ng v\u00f2ng l\u1eb7p hu\u1ea5n luy\u1ec7n**: D\u1eef li\u1ec7u c\u1ea7n d\u1ef1 b\u00e1o l\u00e0 th\u00e1ng 11\/2021. Ch\u00fang ta ch\u01b0a th\u1ec3 bi\u1ebft d\u1eef li\u1ec7u t\u1ea1i th\u1eddi \u0111i\u1ec3m n\u00e0y. Khi \u0111\u00f3 c\u1ea7n x\u00e2y d\u1ef1ng v\u00f2ng l\u1eb7p d\u1ecbch chuy\u1ec3n m\u1ed7i m\u1ed9t ng\u00e0y \u0111\u1ec3 d\u1ef1 b\u00e1o ng\u00e0y ti\u1ebfp theo $t+1$. Sau \u0111\u00f3 s\u1eed d\u1ee5ng ch\u00ednh ng\u00e0y ti\u1ebfp theo $t+1$ k\u1ebft h\u1ee3p v\u1edbi c\u00e1c ng\u00e0y tr\u01b0\u1edbc \u0111\u00f3 \u0111\u1ec3 d\u1ef1 b\u00e1o ng\u00e0y $t+2$. ","da7916de":"# Evaluation model","6ff568f7":"G\u1ed9p t\u1eadp train v\u00e0 val c\u1ee7a b\u01b0\u1edbc tr\u01b0\u1edbc v\u00e0 hu\u1ea5n luy\u1ec7n l\u1ea1i model tr\u00ean to\u00e0n b\u1ed9 t\u1eadp hu\u1ea5n luy\u1ec7n `(dataX, dataY)`","1ad45b91":"LSTM l\u00e0 m\u1ed9t ki\u1ebfn tr\u00fac thu\u1ed9c l\u1edbp m\u00f4 h\u00ecnh RNN. C\u1ea5u t\u1ea1o chung c\u1ee7a LSTM l\u00e0 m\u1ed9t ki\u1ebfn tr\u00fac c\u00f3 d\u1ea1ng truy h\u1ed3i cho ph\u00e9p d\u1ef1 b\u00e1o bi\u1ebfn m\u1ee5c ti\u00eau tu\u1ea7n t\u1ef1 theo th\u1eddi gian. C\u00e1c m\u00f4 h\u00ecnh LSTM do \u0111\u00f3 th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn trong c\u00e1c t\u00e1c v\u1ee5 sequence-to-sequence nh\u01b0 d\u1ecbch m\u00e1y, t\u00f3m t\u1eaft v\u0103n b\u1ea3n. Trong d\u1ef1 b\u00e1o chu\u1ed7i th\u1eddi gian th\u00ec LSTM c\u0169ng l\u00e0 m\u1ed9t ki\u1ebfn tr\u00fac d\u1ef1 b\u00e1o c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c cao.\n\nC\u1ea5u t\u1ea1o c\u1ee7a m\u1ed9t unit cell trong m\u1ed9t m\u1ea1ng LSTM s\u1ebd c\u00f3 d\u1ea1ng nh\u01b0 b\u00ean d\u01b0\u1edbi:\n\n![](https:\/\/imgur.com\/sRkZ6XW.png)\n\nB\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 xem qua gi\u1ea3i th\u00edch v\u1ec1 t\u1eebng b\u1ed9 ph\u1eadn trong unit t\u1ea1i [LSTM architecture](https:\/\/phamdinhkhanh.github.io\/2019\/04\/22\/Ly_thuyet_ve_mang_LSTM.html).\n","a4f5dfbd":"Ti\u1ebfp theo ta s\u1ebd `fill nan` b\u1eb1ng k\u0129 thu\u1eadt n\u1ed9i suy tuy\u1ebfn t\u00ednh (_interpolation_).","55b21f09":"Nh\u01b0 v\u1eady \u0111\u1ed1i v\u1edbi m\u00e3 `NVL` s\u1ebd t\u1ed3n t\u1ea1i m\u1ed9t quan s\u00e1t NA. Ti\u1ebfp theo ta s\u1ebd fill gi\u00e1 tr\u1ecb nan n\u00e0y b\u1eb1ng ph\u01b0\u01a1ng ph\u00e1p feed forward","82f32be5":"# Predict model\n\nTi\u1ebfp theo ta s\u1ebd d\u1ef1 b\u00e1o model tr\u00ean t\u1eadp test v\u00e0 submit k\u1ebft qu\u1ea3","baac8827":"# Train model","fae5a068":"C\u00f3 nhi\u1ec1u c\u00e1ch th\u1ee9c ti\u1ebfp c\u1eadn kh\u00e1c nhau cho b\u00e0i to\u00e1n d\u1ef1 b\u00e1o. D\u1ef1a tr\u00ean m\u00f4 h\u00ecnh LSTM ch\u00fang ta c\u00f3 th\u1ec3 xem x\u00e9t b\u00e0i to\u00e1n l\u00e0 m\u1ed9t t\u00e1c v\u1ee5 d\u1ef1 b\u00e1o `many-to-many`. Trong \u0111\u00f3 m\u1ed7i m\u1ed9t gi\u00e1 tr\u1ecb \u0111\u1ea7u v\u00e0o $x^{<i>}$ l\u00e0 m\u1ed9t v\u00e9c t\u01a1 window c\u1ee7a m\u1ed9t m\u00e3 ch\u1ee9ng kho\u00e1n th\u1ee9 $i$. C\u00e1c \u0111\u1ea7u ra $\\hat{y}^{<i>}$ l\u00e0 gi\u00e1 tr\u1ecb d\u1ef1 b\u00e1o c\u1ee7a phi\u00ean ti\u1ebfp theo c\u1ee7a ch\u00ednh m\u00e3 ch\u1ee9ng kho\u00e1n \u0111\u00f3.\n\n![](https:\/\/imgur.com\/2ULRdKm.png)\n\nKi\u1ebfn tr\u00fac many-to-many c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean pytorch kh\u00e1 \u0111\u01a1n gi\u1ea3n nh\u01b0 sau:","06c284a9":"# Model (Many-to-Many)\n\n","98e4df9d":"Ti\u1ebfp theo ta s\u1ebd d\u1ef1 b\u00e1o m\u00f4 h\u00ecnh tr\u00ean to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 visualize \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a model tr\u00ean c\u00e1c chu\u1ed7i d\u1eef li\u1ec7u.","90f5c301":"Th\u00f4ng th\u01b0\u1eddng khi hu\u1ea5n luy\u1ec7n model v\u1edbi LSTM tr\u00ean d\u1eef li\u1ec7u t\u1ed3n t\u1ea1i missing data s\u1ebd d\u1eabn t\u1edbi nh\u1eefng l\u1ed7i hu\u1ea5n luy\u1ec7n nh\u01b0 h\u00e0m loss function b\u1ecb nan. Ch\u00ednh v\u00ec th\u1ebf ch\u00fang ta ph\u1ea3i \u0111\u1ea3m b\u1ea3o b\u1ed9 d\u1eef li\u1ec7u ho\u00e0n to\u00e0n \u0111\u01b0\u1ee3c fill missing data. Sau khi thu \u0111\u01b0\u1ee3c m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u s\u1ea1ch, ti\u1ebfp theo t\u1eeb b\u1ea3ng gi\u00e1 tr\u00ean ta bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n th\u00e0nh bi\u1ebfn \u0111\u1ea7u v\u00e0o k\u00edch th\u01b0\u1edbc `(num_samples, time_step, num_symbols)` v\u00e0 bi\u1ebfn m\u1ee5c ti\u00eau k\u00edch th\u01b0\u1edbc `(num_samples, 1, num_symbols)`","01fe9910":"# Read data","a0a8df84":"\u0110\u1ec3 d\u1ef1 b\u00e1o chu\u1ed7i gi\u00e1 ch\u1ee9ng kho\u00e1n th\u00ec ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng c\u00e1c gi\u00e1 tr\u1ecb trong qu\u00e1 kh\u1ee9 \u0111\u1ec3 d\u1ef1 b\u00e1o t\u01b0\u01a1ng lai. Gi\u1ea3 \u0111\u1ecbnh c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh chu\u1ed7i th\u1eddi gian l\u00e0 c\u00e1c qui lu\u1eadt trong qu\u00e1 kh\u1ee9 s\u1ebd \u0111\u01b0\u1ee3c l\u1eb7p l\u1ea1i \u1edf t\u01b0\u01a1ng lai n\u00ean th\u00f4ng th\u01b0\u1eddng ch\u00fang ta m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n tr\u00ean nh\u1eefng d\u1eef li\u1ec7u trong qu\u00e1 kh\u1ee9. Sau \u0111\u00f3 d\u1ecbch chuy\u1ec3n c\u00e1c khung th\u1eddi gian \u0111\u1ec3 d\u1ef1 b\u00e1o t\u01b0\u01a1ng lai. B\u1ea1n s\u1ebd th\u1ea5y r\u00f5 h\u01a1n c\u00e1ch d\u1ecbch chuy\u1ec3n khung th\u1eddi gian trong s\u01a1 \u0111\u1ed3 b\u00ean d\u01b0\u1edbi.\n\n![](https:\/\/imgur.com\/PWWFXjU.png)\n\n**H\u00ecnh 1:** Tr\u1ee5c x \u0111\u1ea1i \u0111i\u1ec7n cho chu\u1ed7i th\u1eddi gian \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp t\u0103ng d\u1ea7n, tr\u1ee5c y l\u00e0 c\u00e1c b\u01b0\u1edbc th\u1eddi gian. T\u1ea1i m\u1ed7i b\u01b0\u1edbc th\u1eddi gian ch\u00fang ta s\u1ebd d\u1ecbch chuy\u1ec3n window frame m\u1ed9t \u0111\u01a1n v\u1ecb th\u1eddi gian sao cho 3 gi\u00e1 tr\u1ecb qu\u00e1 kh\u1ee9 (\u00f4 m\u00e0u tr\u1eafng) \u0111\u01b0\u1ee3c gi\u1eef l\u00e0m bi\u1ebfn \u0111\u1ea7u v\u00e0o v\u00e0 gi\u00e1 tr\u1ecb hi\u1ec7n t\u1ea1i (\u00f4 m\u00e0u xanh) \u0111\u01b0\u1ee3c gi\u1eef l\u00e0m bi\u1ebfn m\u1ee5c ti\u00eau. \u0110\u1ed9 d\u00e0i c\u1ee7a chu\u1ed7i \u0111\u1ea7u v\u00e0o ch\u00ednh l\u00e0 `time step = 3`.\n\nQu\u00e1 tr\u00ecnh kh\u1edfi t\u1ea1o d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 m\u1ee5c ti\u00eau nh\u01b0 tr\u00ean \u0111\u01b0\u1ee3c l\u1eb7p l\u1ea1i \u0111\u1ed3ng b\u1ed9 tr\u00ean to\u00e0n b\u1ed9 c\u00e1c m\u00e3 ch\u1ee9ng kho\u00e1n. \u0110\u1ec3 thu\u1eadn ti\u1ec7n x\u1eed l\u00fd d\u1eef li\u1ec7u, ch\u00fang ta c\u1ea7n s\u1eafp x\u1ebfp nh\u1eefng m\u00e3 ch\u1ee9ng kho\u00e1n n\u00e0y v\u1ec1 d\u1ea1ng b\u1ea3ng c\u00f3 c\u00e1c c\u1ed9t l\u00e0 c\u00e1c m\u00e3 ch\u1ee9ng kho\u00e1n v\u00e0 d\u00f2ng l\u00e0 chu\u1ed7i th\u1eddi gian.","e7b5ec86":"# Hint\n\n\u1ede tr\u00ean l\u00e0 m\u1ed9t baseline \u0111\u01a1n gi\u1ea3n cho cu\u1ed9c thi `Viet Nam Stock Market Prediction`. B\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 k\u1ebft h\u1ee3p th\u00eam c\u00e1c d\u1eef li\u1ec7u v\u1ec1 t\u00e0i ch\u00ednh v\u00e0 k\u1ebft qu\u1ea3 kinh doanh \u0111\u01b0\u1ee3c cung c\u1ea5p \u1edf cu\u1ed9c thi n\u00e0y \u0111\u1ec3 c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh. Ch\u00fac c\u00e1c b\u1ea1n may m\u1eafn."}}