{"cell_type":{"a54a7e42":"code","1d9e69eb":"code","4333cf2a":"code","86a1e6fe":"code","0887d6f2":"code","b4b0c37e":"code","a526d44c":"code","08c5cfdf":"code","d19d7e36":"code","88d6193d":"code","8d10f0a7":"code","4f2cec93":"code","ece998be":"code","473c68e9":"code","4830471e":"code","49ccfaa1":"code","04a75bd4":"code","573b50e5":"code","6f86ee7f":"code","24374ea2":"code","5496f06c":"code","9c2d0393":"code","076ab702":"code","e38099d5":"code","16b90160":"code","f5ab5a7d":"code","cd86b5c2":"code","8934dd0b":"code","b5fcc820":"code","98abb925":"code","e2c9b057":"code","aa2042c3":"code","f8360925":"code","53066589":"code","756d40e9":"code","78ae4fb4":"code","1924f4b4":"code","2d8435c9":"code","709aed69":"code","36665530":"code","1378bea8":"code","b2d92280":"code","601a619b":"code","7b895c11":"code","b09587a6":"code","85834081":"code","2d0d1591":"code","25f783fb":"code","ef8a66eb":"code","bae78b84":"code","c37e1e62":"code","09af4b84":"markdown","d03d6d28":"markdown"},"source":{"a54a7e42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns\nsns.set(style='dark')\nimport sklearn\nimport imblearn\nimport matplotlib.pyplot as plt\nimport time\nimport sklearn.metrics as m\nimport xgboost as xgb\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n#Probably can`t be finished because of huge amount of data with kaggle hardware, add nrows parameter to run here\n#Load Data\ndf1=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")#,nrows = 50000\ndf2=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\ndf3=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n# df4=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Monday-WorkingHours.pcap_ISCX.csv\")\ndf5=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\ndf6=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n# df7=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Tuesday-WorkingHours.pcap_ISCX.csv\")\n# df8=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Wednesday-workingHours.pcap_ISCX.csv\")","1d9e69eb":"print (df1[' Label'].unique())\nprint (df2[' Label'].unique())\nprint (df3[' Label'].unique())\n# print (df4[' Label'].unique())\nprint (df5[' Label'].unique())\nprint (df6[' Label'].unique())\n","4333cf2a":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\n# df = pd.concat([df,df4])\n# del df4\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\n# df = pd.concat([df,df7])\n# del df7\n# df = pd.concat([df,df8])\n# del df8","86a1e6fe":"data = df.copy()","0887d6f2":"data.info()","b4b0c37e":"data.head(20)","a526d44c":"deleteCol = []\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop([column],axis=1,inplace=True)","08c5cfdf":"deleteCol = []\nfor column in data.columns:\n    if column == ' Label':\n        continue\n    elif data[column].dtype==np.object:\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop(column,axis=1,inplace=True)","d19d7e36":"data[' Flow Duration'].unique()","88d6193d":"for column in data.columns:\n    if data[column].dtype == np.int64:\n        maxVal = data[column].max()\n        if maxVal < 120:\n            data[column] = data[column].astype(np.int8)\n        elif maxVal < 32767:\n            data[column] = data[column].astype(np.int16)\n        else:\n            data[column] = data[column].astype(np.int32)\n            \n    if data[column].dtype == np.float64:\n        maxVal = data[column].max()\n        minVal = data[data[column]>0][column]\n        if maxVal < 120 and minVal>0.01 :\n            data[column] = data[column].astype(np.float16)\n        else:\n            data[column] = data[column].astype(np.float32)\n            ","8d10f0a7":"data.info()","4f2cec93":"y = data[' Label']\nX = data.drop([' Label'],axis=1)","ece998be":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,y)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(30,'Score'))  #print 10 best features","473c68e9":"feature = pd.DataFrame()\nn = len(featureScores['Specs'])\nfor i in featureScores.nlargest(n\/\/2,'Score')['Specs']:\n        feature[i] = data[i]\nfeature[' Label'] = data[' Label']","4830471e":"feature.info()","49ccfaa1":"from matplotlib import pyplot as plt \nimport seaborn as sns\nfig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","04a75bd4":"feature.drop([' Bwd Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Avg Bwd Segment Size'],axis=1,inplace=True)\nfeature.drop(['Bwd Packet Length Max'],axis=1,inplace=True)\nfeature.drop([' Packet Length Std'],axis=1,inplace=True)\nfeature.drop([' Average Packet Size'],axis=1,inplace=True)\nfeature.drop([' Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Max Packet Length'],axis=1,inplace=True)\nfeature.drop([' Packet Length Variance'],axis=1,inplace=True)\n\n\nfeature.drop([' Idle Max'],axis=1,inplace=True)\nfeature.drop([' Fwd IAT Max'],axis=1,inplace=True)\nfeature.drop([' Flow IAT Std'],axis=1,inplace=True)\nfeature.drop([' Idle Std'],axis=1,inplace=True)\nfeature.drop(['Idle Mean'],axis=1,inplace=True)","573b50e5":"fig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","6f86ee7f":"feature[' Label'].value_counts()","24374ea2":"attackType = feature[' Label'].unique()\nfeature[' Label'] = feature[' Label'].astype('category')\nfeature[' Label'] = feature[' Label'].astype(\"category\").cat.codes","5496f06c":"print (attackType)\nprint (feature[' Label'].value_counts())","9c2d0393":"feature.info()","076ab702":"feature0 = feature[feature[' Label'] == 0]\nfeature0.info()","e38099d5":"sns.boxplot(x = feature0[' Bwd Packet Length Std'])","16b90160":"sns.boxplot(x = feature0[' Bwd Packet Length Min'])","f5ab5a7d":"sns.boxplot(x = feature0[' Min Packet Length'])","cd86b5c2":"sns.boxplot(x = feature0[' Fwd IAT Std'])","8934dd0b":"sns.boxplot(x = feature0['Init_Win_bytes_forward'])","b5fcc820":"sns.boxplot(x = feature0[' Flow IAT Max'])","98abb925":"sns.boxplot(x = feature0[' Bwd Packets\/s'])","e2c9b057":"sns.boxplot(x = feature0['Bwd IAT Total'])","aa2042c3":"cols = [' Bwd Packet Length Std',\n        ' Bwd Packet Length Min',\n        ' Min Packet Length',\n        ' Bwd Packets\/s']\n\nfor col in cols:\n    Q1 = feature0[col].quantile(0.25)\n    Q3 = feature0[col].quantile(0.75)\n    IQR = Q3 - Q1\n    print(IQR)","f8360925":"sns.distplot(feature0[' Bwd Packets\/s'], kde=False,color=\"b\")","53066589":"sns.distplot(feature0[' Min Packet Length'], kde=False,color=\"b\")","756d40e9":"sns.distplot(feature0[' Bwd Packet Length Min'], kde=False,color=\"b\")","78ae4fb4":"sns.distplot(feature0[' Bwd Packet Length Std'], kde=False,color=\"b\")","1924f4b4":"y = feature[' Label']\nX = feature.drop([' Label'],axis=1)","2d8435c9":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler('majority')\nX_rus, y_rus = rus.fit_sample(X, y)\n\n# print('Removed indexes:', id_rus)\n\n# plot_2d_space(X_rus, y_rus, 'Random under-sampling')","709aed69":"# from imblearn.combine import SMOTEENN\n# sme = SMOTEENN(random_state=42)\n# X_res, y_res = sme.fit_sample(X, y)","36665530":"X_rus.info()","1378bea8":"y_rus.value_counts()","b2d92280":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek('not majority')\nX_smt, y_smt = smt.fit_sample(X_rus, y_rus)","601a619b":"y_smt.value_counts()","7b895c11":"#Split dataset on train and test\nfrom sklearn.model_selection import train_test_split\ntrain_X, test_X,train_y,test_y=train_test_split(X_smt,y_smt,test_size=0.3, random_state=10)\n\n#Exploratory Analysis\n# Descriptive statistics\n# print (train_X.describe())\n\n\n# Packet Attack Distribution\n# train[' Label'].value_counts()\n# test[' Label'].value_counts()","b09587a6":"#Scalling numerical attributes\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train_X.select_dtypes(include=['float32','float16','int32','int16','int8']).columns\nsc_train = scaler.fit_transform(train_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\nsc_test = scaler.fit_transform(test_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\n\n# turn the result back to a dataframe\ntrain_X = pd.DataFrame(sc_train, columns = cols)\ntest_X = pd.DataFrame(sc_test, columns = cols)","85834081":"train_X.head()","2d0d1591":"# #Feature Selection\n# from sklearn.ensemble import ExtraTreesClassifier\n# etc = ExtraTreesClassifier();\n\n# # fit random forest classifier on the training set\n# etc.fit(train_X, train_y);","25f783fb":"# # extract important features\n# score = np.round(etc.feature_importances_,3)\n# importances = pd.DataFrame({'feature':train_X.columns,'importance':score})\n# importances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n# # plot importances\n# plt.rcParams['figure.figsize'] = (11, 4)\n# importances.plot.bar();","ef8a66eb":"#Dataset Partition\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,train_y,train_size=0.70, random_state=2)\n\n#Fitting Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# Train KNeighborsClassifier Model\n# KNN_Classifier = KNeighborsClassifier(n_jobs=-1)\n# KNN_Classifier.fit(X_train, Y_train); \n\n# Random Forest Model \n# RTC_Classifier = RandomForestClassifier(criterion='entropy')\n# RTC_Classifier.fit(X_train, Y_train)\n\n# Train LogisticRegression Model\n# LGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\n# LGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\n# BNB_Classifier = BernoulliNB()\n# BNB_Classifier.fit(X_train, Y_train)\n\n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)\nprint ('DTC Classifier Trained')\n\n# XGB_Classifier = xgb.XGBClassifier(criterion='entropy', random_state=0)\n# XGB_Classifier.fit(X_train, Y_train)\n# print ('XGB Classifier Trained')\n\n\n\n\n# ADA_Classifier = AdaBoostClassifier(\n#     DTC_Classifier,\n#     n_estimators=100,\n#     learning_rate=1.5)\n\n# ADA_Classifier.fit(X_train, Y_train)\n# print ('ADA Classifier Trained')\n\n","bae78b84":"#Evaluate Models\nfrom sklearn import metrics\n\nmodels = []\n\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\n# models.append(('XGB Classifier', XGB_Classifier))\n# models.append(('Random Forest Classifier', RTC_Classifier))\n# models.append(('LogisticRegression', LGR_Classifier))\n\nfor i, v in models:\n    vpred = v.predict(X_train)\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, vpred)\n    confusion_matrix = metrics.confusion_matrix(Y_train, vpred)\n    classification = metrics.classification_report(Y_train, vpred)\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","c37e1e62":"#Validate Models\nfor i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        ","09af4b84":"Clearly there is an exellent correlation in - \n\n Bwd Packet Length Std     \n Bwd Packet Length Mean    \n Avg Bwd Segment Size      \nBwd Packet Length Max    \n Packet Length Std         \n Average Packet Size       \n Packet Length Mean        \n Max Packet Length       \n Packet Length Variance   \n \n \nAlso,correlattion lies in - \n\n Flow IAT Max            \n Idle Max                \n Fwd IAT Max             \n Flow IAT Std              \n Idle Std                  \nIdle Mean                  ","d03d6d28":"The following features have outliers \n\n' Bwd Packet Length Std'\n \n' Bwd Packet Length Min'\n \n' Min Packet Length'\n \n' Bwd Packets\/s'"}}