{"cell_type":{"a3c26e4b":"code","f09cddba":"code","d1e25146":"code","27ed80be":"code","2cb34a99":"code","c2b58f51":"code","a25f090c":"code","394ba540":"code","73d2d86e":"code","26d03434":"code","7ec378e5":"code","2cc6d02d":"code","ce8ce1f3":"code","0f636276":"code","184c6773":"code","8e1c0438":"code","e89d7760":"code","7aa4e5f5":"code","df18396b":"code","aa94b80a":"code","e9425b4e":"code","a2e97165":"code","92cef7b1":"markdown","7bac4bbd":"markdown","8d2c9998":"markdown","51de6e47":"markdown","6c0aae3b":"markdown","a51d1b75":"markdown","25aa827d":"markdown","85a68f86":"markdown","560b456c":"markdown","ac989b4c":"markdown","7f95c10d":"markdown","b39cb7ed":"markdown"},"source":{"a3c26e4b":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\n\nfrom tqdm import tqdm_notebook\n\nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom numpy.random import seed\nseed(42)","f09cddba":"X_train = pd.read_csv(\"..\/input\/X_train.csv\")\nX_test = pd.read_csv(\"..\/input\/X_test.csv\")\ny_train = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","d1e25146":"X_train.head()","27ed80be":"plt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()","2cb34a99":"X_train.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_train = X_train.values.reshape((3810, 128, 10))","c2b58f51":"X_test.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_test = X_test.values.reshape((3816, 128, 10))","a25f090c":"for j in range(2):\n    plt.figure(figsize=(15, 5))\n    plt.title(\"Target : \" + y_train['surface'][j], size=15)\n    for i in range(10):\n        plt.plot(X_train[j, :, i], label=i)\n    plt.legend()\n    plt.show()","394ba540":"encode_dic = {'fine_concrete': 0, \n              'concrete': 1, \n              'soft_tiles': 2, \n              'tiled': 3, \n              'soft_pvc': 4,\n              'hard_tiles_large_space': 5, \n              'carpet': 6, \n              'hard_tiles': 7, \n              'wood': 8}","73d2d86e":"decode_dic = {0: 'fine_concrete',\n              1: 'concrete',\n              2: 'soft_tiles',\n              3: 'tiled',\n              4: 'soft_pvc',\n              5: 'hard_tiles_large_space',\n              6: 'carpet',\n              7: 'hard_tiles',\n              8: 'wood'}","26d03434":"y_train = y_train['surface'].map(encode_dic).astype(int)","7ec378e5":"class Attention(Layer):\n    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias: eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None: a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","2cc6d02d":"def make_model():\n    x_inp = Input(shape=(128, 4))\n    x = BatchNormalization()(x_inp)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(x)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(x)\n    x = Attention(128)(x)\n    \n    y_inp = Input(shape=(128, 3))\n    y = BatchNormalization()(y_inp)\n    y = Bidirectional(CuDNNGRU(128, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(y)\n    y = Bidirectional(CuDNNGRU(64, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(y)\n    y = Attention(128)(y)\n    \n    z_inp = Input(shape=(128, 3))\n    z = BatchNormalization()(z_inp)\n    z = Bidirectional(CuDNNGRU(128, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(z)\n    z = Bidirectional(CuDNNGRU(64, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01)))(z)\n    z = Attention(128)(z)\n    \n    q = concatenate([x, y, z])\n    q = BatchNormalization()(q)\n    f = Dense(128, activation='relu')(q)\n    f = Dropout(0.5)(f)\n    f = Dense(384, activation='relu')(f)\n    f = Add()([q, f])\n    f = Dense(32, activation=None)(f)\n    f = Dense(9, activation=\"softmax\")(f)\n    \n    model = Model(inputs=[x_inp, y_inp, z_inp], outputs=f)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","ce8ce1f3":"model = make_model()\nmodel.summary()","0f636276":"def plot_acc(history):\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\ndef plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.ylim([0, 3])\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","184c6773":"def k_folds(X, y, X_test, k=5):\n    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    \n    for i, (train_idx, val_idx) in  enumerate(folds):\n        print(f\"Fold {i+1}\")\n        model = make_model()\n        chk = ModelCheckpoint(\"best_weight.wt\", monitor='val_acc', mode = 'max', save_best_only = True, verbose = 1)\n        history = model.fit([X[train_idx, :, :4], X[train_idx, :, 4:7], X[train_idx, :, 7:]], \n                             y[train_idx], \n                             batch_size=256, \n                             epochs=100, \n                             validation_data=[[X[val_idx, :, :4], X[val_idx, :, 4:7], X[val_idx, :, 7:]], y[val_idx]],\n                             callbacks=[chk])\n        model.load_weights(\"best_weight.wt\")\n        \n        pred_val = np.argmax(model.predict([X[val_idx, :, :4], X[val_idx, :, 4:7], X[val_idx, :, 7:]]), axis=1)\n        score = accuracy_score(pred_val, y[val_idx])\n        y_oof[val_idx] = pred_val\n        \n        print(f'Scored {score:.3f} on validation data')\n        plot_loss(history)\n        \n        y_test += model.predict([X_test[:, :, :4], X_test[:, :, 4:7], X_test[:, :, 7:]]) \/ k\n        \n    return y_oof, y_test                                                                          ","8e1c0438":"y_oof, y_test = k_folds(X_train, y_train, X_test, k=5)","e89d7760":"print(f'Local CV is {accuracy_score(y_oof, y_train): .4f}')","7aa4e5f5":"def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(15, 15))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","df18396b":"plot_confusion_matrix(y_train, y_oof, encode_dic.keys())","aa94b80a":"y_test = np.argmax(y_test, axis=1)","e9425b4e":"sub['surface'] = y_test\nsub['surface'] = sub['surface'].map(decode_dic)\nsub.head()","a2e97165":"sub.to_csv('submission.csv', index=False)","92cef7b1":"### $k$-Folds","7bac4bbd":"## Make Data for the Network","8d2c9998":"### Model","51de6e47":"### Plot Accuracy and Loss","6c0aae3b":"### Load Data","a51d1b75":"### Input","25aa827d":"###  Attention Layer\nBecause that's fancy","85a68f86":"### Submission","560b456c":"## Modeling","ac989b4c":"### Ouput\n\nWe encode our targets","7f95c10d":"### Confusion Matrix","b39cb7ed":"### Thanks for reading ! \n##### Please leave an upvote, it is always appreciated!"}}