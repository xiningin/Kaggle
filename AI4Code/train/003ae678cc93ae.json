{"cell_type":{"26db856a":"code","08223e49":"code","7d641b2a":"code","43f5f1b9":"code","bc5c08d3":"code","e63ac031":"code","b8d3d95d":"code","98841b6e":"code","aafebeb0":"code","b86f1bad":"code","b1ff59d3":"code","ebb57202":"code","06549baf":"code","343e8db1":"code","f7bbd920":"code","a3e6eeb3":"code","1aed11d8":"code","80733b3f":"code","43e43c0e":"code","ed19eada":"code","2bcce76c":"markdown","4f371258":"markdown","026877eb":"markdown","9c84fe98":"markdown","c2805f71":"markdown"},"source":{"26db856a":"# we don't want Weights And Biases Logging, the Trainer class by \ud83e\udd17 Transformers seems to need login credentials which I don't have.\n# so bye-bye wandb\n!pip uninstall -y wandb","08223e49":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\n\nwarnings.filterwarnings('ignore')\n\nimport torch\ntorch.cuda.is_available()","7d641b2a":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed = 80085\nseed_everything(seed)\n\n\ntrain_split = 0.9\nmax_length = 128\nvocab_size = 8000  # we didn't choose 8k, 8k chose us!\n\n\n# create required directories\nlm_data_dir = \"\/kaggle\/working\/lm_data\"\nmodel_dir = \"\/kaggle\/working\/kidBERTa\"\n!mkdir {lm_data_dir}\n!mkdir {model_dir}","43f5f1b9":"train_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","bc5c08d3":"train_df.head()","e63ac031":"test_df.head()","b8d3d95d":"data = train_df['text'].values.tolist() + test_df['text'].values.tolist()\nprint(len(data), 'total tweets (train + test)')\n\ntrain_data_size = int(len(data)*train_split)\ntrain_data = data[:train_data_size]\neval_data = data[train_data_size:]\n\ndef dump2file(d, fp):\n    with open(fp, 'w') as f:\n        for item in d:\n            f.write(\"%s\\n\" % item)\n\n# we need to train the tokernizer with everything we got\ndump2file(data, os.path.join(lm_data_dir,'everything.txt'))\n\n# the Language Model training data\ndump2file(train_data, os.path.join(lm_data_dir,'train.txt'))\n\n# the Language Model eval data\ndump2file(eval_data, os.path.join(lm_data_dir,'eval.txt'))","98841b6e":"from tokenizers import ByteLevelBPETokenizer\n\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files=[f'{lm_data_dir}\/everything.txt'], vocab_size=vocab_size, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"<\/s>\",\n    \"<unk>\",\n    \"<mask>\",\n])\n\n# tokenizer_config = {\n#     \"max_len\": 512\n# }\n# import json\n# with open(f\"{model_dir}\/tokenizer_config.json\", 'w+') as fp:\n#     json.dump(tokenizer_config, fp)\n\ntokenizer.save(model_dir)","aafebeb0":"from tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\n\ntokenizer = ByteLevelBPETokenizer(\n    f\"{model_dir}\/vocab.json\",\n    f\"{model_dir}\/merges.txt\",\n)\ntokenizer._tokenizer.post_processor = BertProcessing(\n    (\"<\/s>\", tokenizer.token_to_id(\"<\/s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=max_length)\n\n\n\ntokenizer.encode(\"the kid shall not overfit!\").tokens","b86f1bad":"# let's reload, else we'll get complains.\n\nfrom transformers import RobertaTokenizerFast\ntokenizer = RobertaTokenizerFast.from_pretrained(model_dir, max_len=max_length)","b1ff59d3":"from transformers import RobertaConfig\n\nconfig = RobertaConfig(\n    vocab_size=vocab_size,\n    intermediate_size=256,\n    max_position_embeddings=256+2,\n    num_attention_heads=1,\n    num_hidden_layers=2,\n    type_vocab_size=1,\n    hidden_size=128,\n)\n\n# save the config for later use\nconfig.to_json_file(f\"{model_dir}\/config.json\")","ebb57202":"from transformers import RobertaForMaskedLM\nmodel = RobertaForMaskedLM(config=config)\nmodel","06549baf":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","343e8db1":"%%time\nfrom transformers import LineByLineTextDataset\n\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f'{lm_data_dir}\/train.txt',\n    block_size=128,\n)\n\neval_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f'{lm_data_dir}\/eval.txt',\n    block_size=128,\n)","f7bbd920":"from transformers import Trainer, TrainingArguments\n\nEPOCHS = 20\n\ntraining_args = TrainingArguments(\n    learning_rate=1e-3,\n    output_dir=model_dir,\n    overwrite_output_dir=True,\n    num_train_epochs=EPOCHS,\n    per_gpu_train_batch_size=128,\n    save_steps=0,\n    save_total_limit=1,\n    do_eval=True,\n    logging_steps=200,\n    evaluate_during_training=True,\n    seed=seed\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    prediction_loss_only=True,\n)","a3e6eeb3":"%%time\ntrainer.train()","1aed11d8":"trainer.evaluate(eval_dataset)","80733b3f":"trainer.save_model(model_dir)","43e43c0e":"!ls {model_dir}","ed19eada":"kidBERTa_config = RobertaConfig.from_pretrained(f'{model_dir}\/config.json', output_hidden_states=True)    \nkidBERTa = RobertaModel.from_pretrained(f'{model_dir}\/pytorch_model.bin', config=kidBERTa_config)\nkidBERTa","2bcce76c":"# Train the tokenizer with everything we got","4f371258":"# Define kidBERTa\n\n* You can experiment with the sizes, make the kid fatter and name it fatKidBERTa!\n* I like my kids fit, neither overfit nor underfit :D","026877eb":"# Dataset","9c84fe98":"# Done\n\nNow we need to finetune as Q&A on the comepetition dataset. \n\nTo keep things short and simple, we'll do that in another kernel\n\nBut let's check whether we can load a Q&A model from this LanguageModel or not..","c2805f71":"# Data prep"}}