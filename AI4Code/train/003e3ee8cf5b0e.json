{"cell_type":{"d00fb8ac":"code","c8ee9500":"code","5952ecb8":"code","5ddafa54":"code","742d92a6":"code","6f7490a1":"code","376ab8dc":"code","e5069f1a":"code","6d6a011b":"code","fbb51591":"code","1e6c7473":"code","1b8e341f":"code","8a772b6d":"code","f36f5a94":"code","eb687f24":"code","dc36f661":"code","e41d6815":"code","3018ef63":"code","1cd0412e":"code","a5f1ee9b":"code","241f3183":"code","0ee972d6":"code","8027c47d":"code","f3f00ad8":"code","5ce66130":"code","73de43d0":"code","d529631f":"code","d4a98722":"code","d3ed20f1":"code","a2db1bff":"code","ddbcdd11":"code","b7d904fa":"code","bdc02914":"code","d5f52ecc":"code","982e2a98":"code","b26789c6":"code","972422fd":"code","80ebc232":"code","2b546394":"code","22f2f593":"code","28ce711e":"code","21c9b047":"code","9fb5c62e":"code","858a1c43":"code","a6f91d53":"code","2bef02f1":"code","a89a9aff":"code","46eec1b6":"code","ccba4e73":"code","96466f08":"code","3d53a699":"code","7bac5208":"code","185a9300":"code","4ae9b3a9":"code","afac5ce6":"code","a21faf0a":"code","7dab3218":"code","abd234dc":"code","97528a64":"markdown","e29e8155":"markdown","9a57e63f":"markdown","e2c55309":"markdown","27686b6d":"markdown","3e675b1d":"markdown","013649c6":"markdown","85cd3a6e":"markdown","dbb97701":"markdown","0c2cfc1b":"markdown","487f5875":"markdown","c8ed0614":"markdown","30fa878d":"markdown","455efea7":"markdown","f2a872d2":"markdown","41c3e8bf":"markdown","a2557ca4":"markdown","0f6ef879":"markdown","5238dc9d":"markdown","f4879704":"markdown","f9e8b32f":"markdown","f3a3b703":"markdown","6c090e46":"markdown"},"source":{"d00fb8ac":"import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=5, suppress=True)\npd.options.display.float_format = '{:.2f}'.format\nimport warnings\nwarnings.filterwarnings('ignore')","c8ee9500":"df = pd.read_csv('..\/input\/datasetsdsa\/dataset_treino.csv')\ndf.head()","5952ecb8":"dfteste = pd.read_csv('..\/input\/datasetsdsa\/dataset_teste.csv')\nPropertyID = dfteste['Property Id'] \ndfteste.head()","5ddafa54":"len(dfteste.columns)","742d92a6":"PropertyID.head()","6f7490a1":"df.iloc[0, :]","376ab8dc":"df.describe()","e5069f1a":"del df['Property Id']","6d6a011b":"print(df.describe())","fbb51591":"len(df)","1e6c7473":"print(df.describe())","1b8e341f":"from sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport os\n\n# Encoding dos valores de texto para vari\u00e1veis nominais\ndef encode_text_dummy(df, name):\n    dummies = pd.get_dummies(df[name])\n    for x in dummies.columns:\n        dummy_name = \"{}-{}\".format(name, x)\n        df[dummy_name] = dummies[x]\n    df.drop(name, axis=1, inplace=True)\n\n\n# Encoding dos valores de texto para uma \u00fanica vari\u00e1vel dummy. As novas colunas (que n\u00e3o substituem o antigo) ter\u00e3o 1\n# em todos os locais onde a coluna original (nome) corresponde a cada um dos valores-alvo. Uma coluna \u00e9 adicionada para\n# cada valor alvo.\ndef encode_text_single_dummy(df, name, target_values):\n    for tv in target_values:\n        l = list(df[name].astype(str))\n        l = [1 if str(x) == str(tv) else 0 for x in l]\n        name2 = \"{}-{}\".format(name, tv)\n        df[name2] = l\n\n\n# Encoding dos valores de texto para \u00edndices (ou seja, [1], [2], [3] para vermelho, verde, azul por exemplo).\ndef encode_text_index(df, name):\n    le = preprocessing.LabelEncoder()\n    df[name] = le.fit_transform(df[name].astype(str)) \n    return le.classes_\n\n\n# Normaliza\u00e7\u00e3o Z-score\ndef encode_numeric_zscore(df, name, mean=None, sd=None):\n    if mean is None:\n        mean = df[name].mean()\n    if sd is None:\n        sd = df[name].std()\n    df[name] = (df[name] - mean) \/ sd\n\n\n# Converte todos os valores faltantes na coluna especificada para a mediana\nimport numpy as np\ndef missing_median(df, name):\n    # 'Not Available' -> NaN\n    df[name] = df[name].replace('Not Available',np.nan)\n    \n    med = df[name].median()\n    df[name] = df[name].fillna(med)\n\n\n# Converte todos os valores faltantes na coluna especificada para o padr\u00e3o\ndef missing_default(df, name, default_value):\n    df[name] = df[name].fillna(default_value)\n    \n\n# Converte um dataframe Pandas para as entradas x, y que o TensorFlow precisa\ndef to_xy(df, target):\n    result = []\n    for x in df.columns:\n        if x != target:\n            result.append(x)\n    # Descobre o tipo da coluna de destino. \n    target_type = df[target].dtypes\n    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n    # Encoding para int. TensorFlow gosta de 32 bits.\n    if target_type in (np.int64, np.int32):\n        # Classifica\u00e7\u00e3o\n        dummies = pd.get_dummies(df[target])\n        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n    else:\n        # Regress\u00e3o\n        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n\n# String de tempo bem formatado\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed \/ (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) \/ 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n\n\n# Chart de Regress\u00e3o\ndef chart_regression(pred,y,sort=True):\n    t = pd.DataFrame({'pred' : pred, 'y' : y })  # y.flatten()\n    if sort:\n        t.sort_values(by=['y'],inplace=True)\n    a = plt.plot(t['y'].tolist(),label='expected')\n    b = plt.plot(t['pred'].tolist(),label='prediction')\n    plt.ylabel('output')\n    plt.legend()\n    plt.show()\n\n# Remove todas as linhas onde a coluna especificada em +\/- desvios padr\u00e3o\ndef remove_outliers(df, name, sd):\n    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n    df.drop(drop_rows, axis=0, inplace=True)\n\n\n# Normaliza\u00e7\u00e3o Range\ndef encode_numeric_range(df, name, normalized_low=-1, normalized_high=1, data_low=None, data_high=None):\n    if data_low is None:\n        data_low = min(df[name])\n        data_high = max(df[name])\n\n    df[name] = ((df[name] - data_low) \/ (data_high - data_low)) * (normalized_high - normalized_low) + normalized_low","8a772b6d":"df.iloc[0, :]","f36f5a94":"nao_relevantes1 = [\n'Order',                                   \n'Property Name',                                              \n'Parent Property Id',                                                             \n'Parent Property Name',                                          \n'BBL - 10 digits',                                                           \n'NYC Borough, Block and Lot (BBL) self-reported',                            \n'NYC Building Identification Number (BIN)',                               \n'Address 1 (self-reported)',                                             \n'Address 2',                                                                           \n'Postal Code',                                                                                 \n'Street Number',                                                                                \n'Street Name',\n'Fuel Oil #1 Use (kBtu)',                                                             \n'Fuel Oil #2 Use (kBtu)',                                                           \n'Fuel Oil #4 Use (kBtu)',                                                             \n'Fuel Oil #5 & 6 Use (kBtu)'                                                          \n]","eb687f24":"categoricos = [\n'Borough'                                                                                     \n'DOF Gross Floor Area',                                                \n'Primary Property Type - Self Selected',                              \n'List of All Property Use Types at Property',                                   \n'Largest Property Use Type',                                                    \n'2nd Largest Property Use Type',                                                    \n'2nd Largest Property Use - Gross Floor Area (ft\u00b2)',                               \n'3rd Largest Property Use Type',                                                     \n'3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)',                            \n'Metered Areas (Energy)',                                                        \n'Metered Areas  (Water)'                                                          \n]\n    \nnumericos = [ \n'Largest Property Use Type - Gross Floor Area (ft\u00b2)',  \n'2nd Largest Property Use - Gross Floor Area (ft\u00b2)',\n'3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)', \n'Year Built',                                                                                  \n'Number of Buildings - Self-reported',                                                          \n'Occupancy', \n'ENERGY STAR Score',                                                                         \n'Site EUI (kBtu\/ft\u00b2)',                                                                      \n'Weather Normalized Site EUI (kBtu\/ft\u00b2)',                                                    \n'Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)',                                      \n'Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)',                                   \n'Weather Normalized Source EUI (kBtu\/ft\u00b2)'                                                  \n]","dc36f661":"def analisar_distribuicao_variavel(df, var):\n    c = df[var].value_counts()\n    c = c.to_frame().reset_index()\n    return c[:3]","e41d6815":"lista = ['Fuel Oil #1 Use (kBtu)',                                                             \n'Fuel Oil #2 Use (kBtu)',                                                           \n'Fuel Oil #4 Use (kBtu)',                                                             \n'Fuel Oil #5 & 6 Use (kBtu)'                                                          \n]\nfor var in lista:\n    d = analisar_distribuicao_variavel(df, var)\n    print(var)\n    print(d)\n    print('============================================')","3018ef63":"lista = df.columns\nlista","1cd0412e":"# distribui\u00e7\u00e3o dos dados - contagem\n\nlista = df.columns\nfor var in lista:\n    d = analisar_distribuicao_variavel(df, var)\n    print(var)\n    print(d)\n    print('============================================')","a5f1ee9b":"#* Remover - Atributos n\u00e3o-Relevantes\nremoveAtributeList = [\n'Order',\n'Property Name',\n'Parent Property Id',\n'Parent Property Name',\n'BBL - 10 digits',\n'NYC Borough, Block and Lot (BBL) self-reported',\n'NYC Building Identification Number (BIN)',\n'Address 1 (self-reported)',\n'Address 2',\n'Postal Code',\n'Street Number',\n'Street Name',\n'2nd Largest Property Use - Gross Floor Area (ft\u00b2)',\n'3rd Largest Property Use Type',\n'3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n'Number of Buildings - Self-reported',\n'Occupancy',\n'Metered Areas (Energy)',\n'Metered Areas  (Water)',\n'Fuel Oil #1 Use (kBtu)',\n'Fuel Oil #2 Use (kBtu)',\n'Fuel Oil #4 Use (kBtu)',\n'Fuel Oil #5 & 6 Use (kBtu)',\n'Diesel #2 Use (kBtu)',\n'District Steam Use (kBtu)',\n'Natural Gas Use (kBtu)',\n'Weather Normalized Site Natural Gas Use (therms)',\n'Release Date',\n'DOF Benchmarking Submission Status',\n'Latitude',\n'Longitude'\n]\n\n\n#* Categorical - tratar- Yes, No, Not Avaliable, NaN\ncategoricalAtributesList = [\n'Borough',\n'Primary Property Type - Self Selected',\n'List of All Property Use Types at Property',\n'Largest Property Use Type',\n'2nd Largest Property Use Type',\n'Water Required?',\n'NTA'\n]\n\n#* Numerical - tratar- Yes, No, Not Avaliable, NaN\nnumericalAtributesList = [\n'DOF Gross Floor Area',\n'Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n'Year Built',\n'ENERGY STAR Score',\n'Site EUI (kBtu\/ft\u00b2)',\n'Weather Normalized Site EUI (kBtu\/ft\u00b2)',\n'Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)',\n'Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)',\n'Weather Normalized Source EUI (kBtu\/ft\u00b2)',\n'Electricity Use - Grid Purchase (kBtu)',\n'Weather Normalized Site Electricity (kWh)',\n'Total GHG Emissions (Metric Tons CO2e)',\n'Direct GHG Emissions (Metric Tons CO2e)',\n'Indirect GHG Emissions (Metric Tons CO2e)',\n'Property GFA - Self-Reported (ft\u00b2)',\n'Water Use (All Water Sources) (kgal)',\n'Water Intensity (All Water Sources) (gal\/ft\u00b2)',\n'Source EUI (kBtu\/ft\u00b2)',\n'Community Board',\n'Council District',\n'Census Tract'\n]","241f3183":"df.loc[1, 'NTA']\ndf['NTA'].value_counts()[:2]","0ee972d6":"df = df[categoricalAtributesList + numericalAtributesList ].copy()\ndf.head()","8027c47d":"numericalAtributesList_test = numericalAtributesList.copy()\nnumericalAtributesList_test.remove('ENERGY STAR Score')\ndfteste = dfteste[categoricalAtributesList + numericalAtributesList_test  ].copy()  \ndfteste.head()","f3f00ad8":"len(df.columns)","5ce66130":"df.columns","73de43d0":"categoricalAtributesList","d529631f":"NULO = -1\nfor name in categoricalAtributesList:\n    df[name] = df[name].fillna(NULO)\n    encode_text_index(df, name)\n    \n    dfteste[name] = dfteste[name].fillna(NULO)\n    encode_text_index(dfteste, name)","d4a98722":"df.loc[:3, categoricalAtributesList]","d3ed20f1":"df[categoricalAtributesList].describe()","a2db1bff":"for name in numericalAtributesList:\n    missing_median(df, name)\n    if name != 'ENERGY STAR Score':\n        missing_median(dfteste, name)\n\ndf[numericalAtributesList].describe()","ddbcdd11":"dfteste.describe()","b7d904fa":"classe_x = 'ENERGY STAR Score'\ny = df[classe_x]\n\nx_columns = list(df.columns.values)\nx_columns.remove(classe_x)\n\nX = df.loc[:, x_columns]\nprint(len(X), len(x_columns), len(y))","bdc02914":"X[:3]","d5f52ecc":"# treino\nfrom sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(X)\ndftreino = pd.DataFrame(x_scaled, columns=x_columns)\ndftreino.head()","982e2a98":"# teste\nfrom sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(dfteste.values)\ndfteste = pd.DataFrame(x_scaled, columns=dfteste.columns)\ndfteste.head()","b26789c6":"len(dftreino.columns)","972422fd":"X_treino = dftreino.values\ny_treino = y.values\n\nX_teste_kaggle = dfteste.values\n\nX_teste_kaggle[:3]\n#y[:3]","80ebc232":"X_teste_kaggle.shape","2b546394":"# Feature Extraction with RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# load data\nX = X_treino\ny = y_treino\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 20)\nfit = rfe.fit(X, y)\nprint(\"Num Features: \", fit.n_features_)\nprint(\"Selected Features: \", fit.support_)\nprint(\"Feature Ranking: \", fit.ranking_)\nprint(dftreino.columns)","22f2f593":"dfimp = pd.DataFrame(fit.ranking_, columns=['importancia'])\ndfimp['atributo'] = dftreino.columns\ndfimp2 = dfimp.sort_values('importancia', ascending=True)\ndfimp2.ix[:, ['atributo', 'importancia']]","28ce711e":"dfimp2 = dfimp2.reset_index(drop=True)\ndfimp2.ix[:19, ['atributo', 'importancia']]","21c9b047":"atributos_relevantes = dfimp2.head(20)['atributo'].values\natributos_relevantes","9fb5c62e":"dftreino = dftreino.loc[:, atributos_relevantes]\nprint(len(dftreino.columns))\ndftreino.head()","858a1c43":"dfteste = dfteste.loc[:, atributos_relevantes]\nprint(len(dfteste.columns))\ndfteste.head()","a6f91d53":"X_treino = dftreino.values\ny_treino = y\n\nX_teste_kaggle = dfteste.values\n\nX_teste_kaggle[:3]","2bef02f1":"def gerar_arquivo_kaggle(modelo):\n# Gera arquivo para o Kaggle - PropertyId,Score\n    nome_arquivo = 'Submissao-v5.1-XGBoost.csv'\n    df_saida = pd.DataFrame()\n    df_saida['Property Id'] = PropertyID.values\n    yteste_previsto = modelo.predict(X_teste_kaggle) \n    yteste_previsto = np.rint(yteste_previsto).astype(np.int64)\n    df_saida['score'] =   yteste_previsto.ravel()\n    # Salvando o arquivo\n    df_saida.to_csv(nome_arquivo, index=False)\n    print('Arquivo %s salvo...', nome_arquivo)\n    !head Submissao-v5.1-XGBoost.csv","a89a9aff":"parameters_for_testing = {\n   'colsample_bytree':[0.4,0.6,0.8],\n    'gamma':[0,0.03,0.1,0.3],\n    'min_child_weight':[1.5,6,10],\n    'learning_rate':[0.1,0.07],\n    'max_depth':[3,5],\n    'n_estimators':[300], # 10000\n    'reg_alpha':[1e-5, 1e-2,  0.75],\n    'reg_lambda':[1e-5, 1e-2, 0.45],\n    'subsample':[0.6,0.95]  \n}","46eec1b6":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n#xgb_model = XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=5,\n#     min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, \n#                         nthread=6, scale_pos_weight=1, seed=27)\n#gsearch1 = GridSearchCV(estimator = xgb_model, param_grid = parameters_for_testing, n_jobs=6,\n#                        iid=False, verbose=10,scoring='neg_mean_absolute_error') #'neg_mean_absolute_error')\n#gsearch1.fit(X_treino, y_treino, eval_metric='mae' )\n#print('best params')\n#print (gsearch1.best_params_)\n#print('best score')\n#print (gsearch1.best_score_)","ccba4e73":"#best params\nparams = {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.07, \n          'max_depth': 3, 'min_child_weight': 10, 'n_estimators': 300, \n          'reg_alpha': 1e-05, 'reg_lambda': 0.45, 'subsample': 0.6}\n#best score\n#-9.958036954258406\n\n\nmodelXGB = XGBRegressor(\n    colsample_bytree = params['colsample_bytree'], \n    gamma = params['gamma'], \n    learning_rate = params['learning_rate'], \n    max_depths = params['max_depth'],\n    min_child_weight = params['min_child_weight'],\n    n_estimators = params['n_estimators'],\n    reg_alpha =  params['reg_alpha'], \n    reg_lambda = params['reg_lambda'],\n    subsample = params['subsample']\n)\nmodelXGB.fit(X_treino, y_treino)\n\n#modelXGB = gsearch1\ngerar_arquivo_kaggle(modelXGB)","96466f08":"# verificando o arquivo de sa\u00edda\ndfsaida = pd.read_csv('Submissao-v5.1-XGBoost.csv')\ndfsaida.head()","3d53a699":"len(dfsaida)","7bac5208":"#x = pd.Series(x, name=\"x variable\")\nimport seaborn as sns\nax = sns.distplot(dfsaida['score'])","185a9300":"dfsaida.score.hist()","4ae9b3a9":"dfsaida.score.value_counts()","afac5ce6":"dfsaida['score'] = dfsaida['score'].apply(lambda w: 1 if w <= 0 else w)\nimport seaborn as sns\nax = sns.distplot(dfsaida['score'])","a21faf0a":"dfsaida.score.value_counts()","7dab3218":"nome_arquivo = 'Submissao-v5.1-XGBoost.csv'\ndfsaida.to_csv(nome_arquivo, index=False)\nprint('Arquivo %s salvo...', nome_arquivo)\n!head Submissao-v5.1-XGBoost.csv","abd234dc":"ax = sns.distplot(dfsaida['score'])","97528a64":"# Corrigindo valores < 0","e29e8155":"## Colocar somente os atributos relevantes","9a57e63f":"# Remover atributos n\u00e3o relevantes -  Sele\u00e7\u00e3o autom\u00e1tica por correla\u00e7\u00e3o (uso de algor\u00edtmos)","e2c55309":"# Preparar os dados para entregar ao algoritmo ","27686b6d":"### preencher valores Nan com uma CONSTANTE  e tranformar Categ\u00f3rico para N\u00famero","3e675b1d":"# Competi\u00e7\u00e3o DSA de Machine Learning - Edi\u00e7\u00e3o Fevereiro\/2019","013649c6":"# Feature Extraction with RFE","85cd3a6e":"## Eliminar atributos com pouca relev\u00e2ncia","dbb97701":"## Remover atributos n\u00e3o relaventes - ETAPA1: Remover Manualmente, atributos como ID","0c2cfc1b":"# Normalizar os dados","487f5875":"## Separar X e y","c8ed0614":"## Tranforma\u00e7\u00e3o dos dados Nulos e Categ\u00f3ricos","30fa878d":"## Leitura dos dados","455efea7":"## An\u00e1lise dos dados","f2a872d2":"## Achando o melhor modelo","41c3e8bf":"# Verificando modelo de sa\u00edda para o kaggle","a2557ca4":"# Constru\u00e7\u00e3o do Modelo - XGBoost","0f6ef879":"# Treinar o modelo","5238dc9d":"## Valores Categoricos - substituir por n\u00fameros - tratar- Yes, No, Not Avaliable, NaN","f4879704":"## Atributos Num\u00e9ricos - Valores nulos - substituir por m\u00e9dia","f9e8b32f":"# Separar X e y novamente - os atributos mais relavantes","f3a3b703":"# XGBoost - Melhores Par\u00e2metros","6c090e46":"# Constru\u00e7\u00e3o do Modelo"}}