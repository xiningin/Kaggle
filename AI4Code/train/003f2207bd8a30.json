{"cell_type":{"dbf73f89":"code","d81c7988":"code","36052771":"code","11646524":"code","3ba6ad0e":"code","385601f5":"code","fb47d986":"code","eeee5ef0":"code","1284e7cd":"code","8fed3255":"code","bd3f16ae":"code","6213dbf6":"code","bae584b4":"code","a33059c1":"code","737d0f84":"code","c39d1b3f":"code","d1e48a62":"code","20d392ff":"code","7654a6ca":"code","b07f88ff":"code","b52ade5f":"code","4eab3111":"code","edd1a064":"code","7dd8026c":"code","f8703b14":"code","2e766ce8":"code","7ed2c892":"code","6ea96238":"code","0a23d92f":"code","e19f5a6b":"code","afe50a90":"code","89bca1cb":"code","f7e3f5d4":"code","77de4fdb":"code","9465079a":"code","d6eff62f":"code","d057d528":"code","e0d8f606":"code","ccfcb532":"code","67ae7e73":"code","de68639c":"code","4ed5cff5":"code","03e27751":"code","629fb946":"code","81a2f613":"code","0eeda9f8":"code","84eb1e93":"code","364f1a0f":"code","a029031b":"code","bcb32da4":"code","4adacf3d":"code","302641c7":"code","ae2ae639":"code","b89ed744":"code","be18db65":"code","3c5034f7":"code","19cadfe3":"code","b2c9527f":"code","4adb59f0":"code","0f6965bb":"code","e6fd0b8e":"code","7a50f40f":"code","5a2f2920":"code","b533099a":"code","45e8db32":"code","eee4a953":"code","89a36116":"code","bf9a6bbe":"code","4737ea3d":"code","20731ecd":"code","3144ff05":"code","29305215":"code","7cbb0d2b":"code","c089172f":"code","81beeef4":"markdown","629ed710":"markdown","2dcd3e4e":"markdown","4cdf13fd":"markdown","8cdb571a":"markdown","4087f882":"markdown","2c25b6ad":"markdown","3ba28ff1":"markdown","495fd884":"markdown","7a7beefd":"markdown","8de277d0":"markdown","dec731c1":"markdown","51085273":"markdown","6ceab2c2":"markdown","32f58b3b":"markdown","68de0918":"markdown","48b0706e":"markdown","af2b0afd":"markdown","7754ffec":"markdown","26112238":"markdown","cda15fc2":"markdown","3e1000ab":"markdown","21be3bda":"markdown","188a95be":"markdown","b33a7e55":"markdown","d6b5a91f":"markdown","7f52441e":"markdown","ee957d8f":"markdown","75d7fa9d":"markdown","9dbbbb24":"markdown","25748e05":"markdown","aca99b96":"markdown","77493e89":"markdown","0372019c":"markdown","783b8b83":"markdown","84ffd24e":"markdown","3793cd0b":"markdown","c8caec7a":"markdown","03f088a3":"markdown","eeb88bb1":"markdown","0fa1e6f1":"markdown","bfa810be":"markdown","cc28b0d7":"markdown","0ef38b49":"markdown","0068c43e":"markdown","97ec1028":"markdown","4fd51c21":"markdown","5f12c384":"markdown","227066a5":"markdown","9cc0acd5":"markdown"},"source":{"dbf73f89":"# Math\nimport numpy as np\nimport math\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Data Analysis\nimport pandas as pd\nimport geopandas as gpd \nimport folium\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom folium.plugins import HeatMap, MarkerCluster\n\n\n# Preprocessor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, KNNImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Modeling\nfrom sklearn.linear_model import Ridge, LinearRegression, Lasso\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.outliers_influence as st_inf\nfrom sklearn import neighbors\n\n\n# Validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","d81c7988":"# Train data\ntrain_path = \"..\/input\/housing-house-prediction\/train_set.csv\"\ntrain = pd.read_csv(train_path)\n\n# Test data\ntest_path = \"..\/input\/housing-house-prediction\/test_set.csv\"\ntest = pd.read_csv(test_path)","36052771":"train.info()","11646524":"test.head(5)","3ba6ad0e":"# I'll work with copies of the original datasets to avoid making mistakes on them\ntrain_data = train.copy()\ntest_data = test.copy()\n\n# Change the index of the dataset to the variable index\ntrain_data.drop(\"index\", axis=1, inplace=True)\ntest_data.drop(\"index\", axis=1, inplace=True)\n\ntrain_data.head(3)","385601f5":"plt.figure(figsize=(8,6))\nplt.title(\"Price distribution\")\nsns.distplot(train_data['Price'])","fb47d986":"train_data.Price.describe()","eeee5ef0":"# Skew and kurt\nprint(\"Skewness: %f\" % train_data['Price'].skew())\nprint(\"Kurtosis: %f\" % train_data['Price'].kurt())","1284e7cd":"train_data[\"Price\"] = train_data[\"Price\"].map(lambda p: np.log(p))\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n#Check the new distribution \nsns.distplot(train_data['Price'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['Price'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Price\")\nax.set(title=\"Price distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","8fed3255":"# Finding numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train_data.columns:\n    if train_data[i].dtype in numeric_dtypes:\n            numeric.append(i)   \nnumeric.remove(\"Price\")\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 70))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_data[numeric]), 1):\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='Price', hue='Price', palette='Blues', data=train_data)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Price', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12) \n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","bd3f16ae":"# Remove Outliers in BuildingArea and Landsize\ntrain_data = train_data[(train_data.Landsize < 3000) & (train_data.Bedroom2 < 12.5)]\ntrain_data = train_data.drop([1743, 4243, 5264]) # BuildingArea outliers\n\n# Reset index\ntrain_data = train_data.reset_index()\ntrain_data.drop(\"index\", axis=1, inplace=True)","6213dbf6":"sns.scatterplot(x=train_data.BuildingArea, y=train_data.Price)","bae584b4":"sns.scatterplot(x=train_data.Landsize, y=train_data.Price)","a33059c1":"# Finding categorical \ncat = [\"Type\", \"Method\", \"Regionname\"]\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 20))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_data[cat]), 1):\n    plt.subplot(len(list(cat)), 3, i)\n    sns.boxplot(x=feature, y='Price', palette='Blues', data=train_data)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Price', size=15,labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    # plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","737d0f84":"plt.figure(figsize=(20,10))\nsns.boxplot(x=\"CouncilArea\", y='Price', palette='Blues', data=train_data)\nplt.xticks([])\nplt.show()","c39d1b3f":"plt.figure(figsize=(20,10))\nsns.boxplot(x=\"Date\", y='Price', palette='Blues', data=train_data)\nplt.xticks([])\nplt.show()","d1e48a62":"# Create the map\nmap_1 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=10)\n\n# Add points to the map\nmc = MarkerCluster()\nfor idx, row in train_data.iterrows():\n    if not math.isnan(row['Longtitude']) and not math.isnan(row['Lattitude']):\n        mc.add_child(folium.Marker([row['Lattitude'], row['Longtitude']]))\nmap_1.add_child(mc)\n\n# Display the map\ndisplay(map_1)","20d392ff":"for i in range(3):\n    # Create a base map\n    map_2 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=10)\n\n    # Add a heatmap to the base map\n    high_prices = train_data[train_data.Price > 13+i]\n    HeatMap(data=high_prices.loc[:,['Lattitude', 'Longtitude']], name=\"Price\", radius=10).add_to(map_2)\n\n    # Display the map\n    display(map_2)","7654a6ca":"cbd = gpd.read_file(\"..\/input\/polygon\/Central Business District_region.shp\")\n\n# Create a base map\ncbd_map = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=12)\n\nfolium.Choropleth(geo_data=cbd.__geo_interface__, \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Housing Price (log)'\n          ).add_to(cbd_map)\n\n# Display the map\ncbd_map","b07f88ff":"train_data.corr()[\"Distance\"][\"Price\"]","b52ade5f":"# Shapfile with the different council areas of Melbourne\nca_map = gpd.read_file(\"..\/input\/polygon\/VIC_LGA_POLYGON_SHP.shp\")\nca_map = ca_map[[\"LGA_NAME\", \"geometry\"]].set_index(\"LGA_NAME\")\n\n# List of suburbs available in the database\nm_list = [name for name in train_data.CouncilArea.unique() if str(name) != 'nan']\nm_list = [name.upper() for name in m_list]\nm_list_final = list()\nfor name in m_list[:-1]:\n    if name not in [\"CARDINIA\", \"MACEDON RANGES\", \"YARRA RANGES\", \"NILLUMBIK\"]:\n        m_list_final.append(name + \" CITY\")\n    else:\n        m_list_final.append(name + \" SHIRE\")\nm = ca_map.loc[m_list_final] \nm = m[~m.index.duplicated(keep='first')]\n\n# Suburb data\nm_data = train_data.groupby(\"CouncilArea\").Price.mean()\nname = [name.upper() for name in m_data.index]\nnames = list()\nfor n in name:\n    if n not in [\"CARDINIA\", \"MACEDON RANGES\", \"YARRA RANGES\", \"NILLUMBIK\"]:\n        names.append(n + \" CITY\")\n    else:\n        names.append(n + \" SHIRE\")\nsdf = pd.DataFrame({\"NAME\":names, \"VALUE\":train_data.groupby(\"CouncilArea\").Price.mean()})\nsdf = sdf.set_index(\"NAME\")\n\n# Create a base map\nm_6 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=9)\n\n# Add a choropleth map to the base map\nfolium.Choropleth(geo_data=m.__geo_interface__,\n           data = sdf.VALUE,\n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Housing Price (log)'\n          ).add_to(m_6)\n\n# Display the map\nm_6","4eab3111":"#List of expensive suburbs acording to the article\nexpensive_suburbs = [\"TOORAK\", \"MIDDLE PARK\", \"BRIGHTON\", \"CANTERBURY\", \"EAST MELBOURNE\", \"MALVERN\", \"BALWYN\", \"KOOYING\", \"KEW\", \"CAMBERWELL\"]\n\n# Coordinates for these suburbs\nexpensive_suburbs_c = [[-37.841928,145.017147], [-37.851192,144.962189], [-37.906404,144.992726], [-37.823674,145.077539], [-37.813287,144.982935], [-37.857321,145.034372], [-37.809497,145.081831], [-37.841201,145.036268], [-37.804252,145.041535], [-37.838483,145.075389]]\n\n# Create a base map\nmap_3 = folium.Map(location=[-37.841928,145.017147], tiles='cartodbpositron', zoom_start=12)\n\n# Add a heatmap to the base map\nhigh_prices = train_data[train_data.Price > 15]\nHeatMap(data=high_prices.loc[:,['Lattitude', 'Longtitude']], name=\"Price\", radius=10).add_to(map_3)\n\n# Add the expensive suburbs to the map\nfor i in range(len(expensive_suburbs)):\n    folium.Marker(location=[expensive_suburbs_c[i][0], expensive_suburbs_c[i][1]], popup=expensive_suburbs[i]).add_to(map_3)\n\n# Display the map\ndisplay(map_3)","edd1a064":"# Shapfile with the different suburbs of Melbourne\nsuburbs_map = gpd.read_file(\"..\/input\/polygon\/VIC_LOCALITY_POLYGON_SHP-GDA2020.shp\")\nsuburbs_map = suburbs_map[[\"NAME\", \"geometry\"]].set_index(\"NAME\")\n\n# List of suburbs available in the database\nsuburb_list = [name.upper() for name in train_data[\"Suburb\"].unique()]\ns = suburbs_map.loc[suburb_list] \ns = s[~s.index.duplicated(keep='first')]\n\n# Suburb data\nsuburbs_data = train_data.groupby(\"Suburb\").Price.mean()\nname = [name.upper() for name in suburbs_data.index]\nsdf = pd.DataFrame({\"NAME\":name, \"VALUE\":train_data.groupby(\"Suburb\").Price.mean()})\nsdf = sdf.set_index(\"NAME\")\n\n# Create a base map\nm_6 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=10)\n\n# Add a choropleth map to the base map\nfolium.Choropleth(geo_data=s.__geo_interface__, \n           data=sdf.VALUE, \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Housing Price (log)'\n          ).add_to(m_6)\n\n# Display the map\nm_6","7dd8026c":"# Shapfile with the different council areas of Melbourne\npc_map = gpd.read_file(\"..\/input\/polygon\/POA_2016_AUST.shp\")\npc_map = pc_map[[\"POA_CODE16\", \"geometry\"]].set_index(\"POA_CODE16\")\n\n# List of suburbs available in the database\npc_list = [str(int(code)) for code in train_data[\"Postcode\"].unique()]\npc = pc_map.loc[pc_list] \npc = pc[~pc.index.duplicated(keep='first')]\n\n# Suburb data\npc_data = train_data.groupby(\"Postcode\").Price.mean()\nname = [str(int(code)) for code in pc_data.index]\nsdf = pd.DataFrame({\"CODE\":name, \"VALUE\":train_data.groupby(\"Postcode\").Price.mean()})\nsdf = sdf.set_index(\"CODE\")\n\n# Create a base map\nm_6 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=9)\n\n# Add a choropleth map to the base map\nfolium.Choropleth(geo_data=pc.__geo_interface__,\n           data = sdf.VALUE,\n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Housing Price (log)'\n          ).add_to(m_6)\n\n# Display the map\nm_6","f8703b14":"corr = train_data.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","2e766ce8":"table = abs(corr['Price']).sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb","7ed2c892":"# concatenate training and testing sets to create new features and fill in missing values\ntarget=train_data['Price'].reset_index(drop=True)\ntrainx=train_data.drop(['Price'],1)\nall_features=pd.concat([trainx,test_data]).reset_index(drop=True)\n\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","6ea96238":"all_features.loc[255, \"CouncilArea\"] = \"Hume\"\nall_features.loc[[674,7240,7758], \"CouncilArea\"] = \"Melton\"\nall_features.loc[2609, \"CouncilArea\"] = \"Mitchell\"\nall_features.loc[4891, \"CouncilArea\"] = \"Bayside\"\nall_features.loc[3988, \"CouncilArea\"] = \"Yarra Ranges\"\nall_features.loc[9870, \"CouncilArea\"] = \"Nillumbik\"\n\nfor i in range(len(all_features)):\n    if str(all_features.CouncilArea[i]) == \"nan\":\n        all_features.CouncilArea[i] = all_features[all_features.Suburb == all_features.Suburb[i]].CouncilArea.mode()[0]","0a23d92f":"# Define a function to evaluate the accuracy of the imputers\n\ndef imputer_evaluation(X, var, imp, imp_name):\n    imputer = imp\n    sum_final = 0\n    for i in range(10):\n        X_train, X_val = train_test_split(X, random_state=i)\n        validation = X_val[var].copy()\n        X_val[var] = np.nan\n        X_final = pd.concat([X_train,X_val])\n        \n        filled = imputer.fit_transform(X_final)\n        filled = pd.DataFrame(filled, columns=X_final.columns)\n\n        val = filled.loc[(filled.shape[0]-X_val.shape[0]):]\n        val.reset_index(inplace=True)\n\n        sum = 0\n        for i,v in enumerate(validation.index):\n            sum += abs(validation[v]-val[var][i])\n\n        sum_final += sum\/len(validation)\n    print(\"MAE using\", imp_name, \"for variable\", var, \"is:\", sum_final\/10)\n\nimp_variables = [\"YearBuilt\", \"BuildingArea\", \"Car\", \"Landsize\"]\n\nfor i in imp_variables:\n    imputer_data = all_features[:(len(train_data)-1)]._get_numeric_data().dropna(subset=[i])\n    imputer = KNNImputer(n_neighbors=10)\n    a = imputer_evaluation(imputer_data, i, imputer, \"KNN Imputer\")\n\nprint(\"\\n\")\n\nfor i in imp_variables:\n    imputer_data = all_features[:(len(train_data)-1)]._get_numeric_data().dropna(subset=[i])\n    imputer = IterativeImputer(max_iter=100)\n    a = imputer_evaluation(imputer_data, i, imputer, \"Iterative Imputer\")\n","e19f5a6b":"for i in range(len(all_features)):\n    if str(all_features.BuildingArea[i]) == \"nan\":\n        all_features.BuildingArea[i] = all_features[(all_features.Suburb == all_features.Suburb[i]) & (all_features.Rooms == all_features.Rooms[i])].BuildingArea.median()\n    if str(all_features.BuildingArea[i]) == \"nan\":\n        all_features.BuildingArea[i] = all_features[(all_features.Suburb == all_features.Suburb[i])].BuildingArea.median()\n    if str(all_features.BuildingArea[i]) == \"nan\":\n        all_features.BuildingArea[i] = all_features[(all_features.CouncilArea == all_features.CouncilArea[i])].BuildingArea.median()\n    if str(all_features.BuildingArea[i]) == \"nan\":\n        all_features.BuildingArea[i] = all_features.BuildingArea.median()","afe50a90":"for i in all_features[all_features.Landsize > 5000].index:\n    all_features.Landsize[i] = 5000\n\nfor i in range(len(all_features)):\n    if all_features.Landsize[i] == 0:\n        all_features.Landsize[i] = all_features[(all_features.Suburb == all_features.Suburb[i])].Landsize.mean()","89bca1cb":"for i in range(len(all_features)):\n    if str(all_features.Car[i]) == \"nan\":\n        all_features.Car[i] = all_features[(all_features.Rooms == all_features.Rooms[i]) & (all_features.Distance <= all_features.Distance[i])].Car.median()","f7e3f5d4":"for i in range(len(all_features)):\n    if str(all_features.YearBuilt[i]) == \"nan\":\n        all_features.YearBuilt[i] = all_features[all_features.Distance <= all_features.Distance[i]].YearBuilt.mean()","77de4fdb":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)\n\n# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","9465079a":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","d6eff62f":"# Normalize skewed features\nfor i in skew_index:\n    if i != \"Postcode\":\n        all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","d057d528":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","e0d8f606":"q = [\"Rooms\", \"Bathroom\", \"YearBuilt\"]\nline = [[1,8], [0,2.5], [1825,2015]]\npoly = [2,3,3]\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 10))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nf = pd.concat([all_features.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\nk = 0\nfor i, feature in enumerate(list(train_data[q]), 1):\n    polinomial = f.dropna()\n    plt.subplot(len(list(q)), 3, i)\n    sns.scatterplot(x=feature, y='Price', palette='Blues', data=polinomial)\n    mymodel = np.poly1d(np.polyfit(polinomial[feature], polinomial.Price, poly[k]))\n    myline = np.linspace(line[k][0], line[k][1], 100)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Price', size=15,labelpad=12.5)\n    plt.plot(myline, mymodel(myline))\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    k += 1\n        \nplt.show()","ccfcb532":"all_features[\"Rooms_2\"] = all_features.Rooms.map(lambda p: p**2)\nall_features[\"Bathroom_2\"] = all_features.Bathroom.map(lambda p: p**2)\nall_features[\"Bathroom_3\"] = all_features.Bathroom.map(lambda p: p**3)\nall_features[\"YB_2\"] = all_features.YearBuilt.map(lambda p: p**2)\nall_features[\"YB_3\"] = all_features.YearBuilt.map(lambda p: p**3)","67ae7e73":"all_features[\"LB\"] = all_features.BuildingArea*all_features.Landsize\nall_features[\"LR\"] = all_features.Landsize*all_features.Rooms\n\ncenter = [-37.836659,145.024214]\nall_features[\"Center\"] =  abs(all_features.Lattitude - center[0]) + abs(all_features.Longtitude -center[1])\n\nall_features[\"New_Bathroom\"] = 0\nall_features[\"New_Rooms\"] = 0\nfor i in range(len(all_features)):\n    all_features.New_Bathroom[i] = all_features.Bathroom[i] \/ (all_features.Center[i]+0.1)\n    all_features.New_Rooms[i] = all_features.Rooms[i] \/ (all_features.Center[i]+0.1)","de68639c":"# Date\nall_features[\"Year2016\"] = 0\nall_features[\"Year2017\"] = 0\nall_features[\"Mo1\"] = 0\nall_features[\"Mo2\"] = 0\nall_features[\"Mo3\"] = 0\nall_features[\"Mo4\"] = 0\nall_features[\"Mo5\"] = 0\nall_features[\"Mo6\"] = 0\nall_features[\"Mo7\"] = 0\nall_features[\"Mo8\"] = 0\nall_features[\"Mo9\"] = 0\nall_features[\"Mo10\"] = 0\nall_features[\"Mo11\"] = 0\nall_features[\"Mo12\"] = 0\n\n# Type\nall_features[\"Type_h\"] = 0\nall_features[\"Type_u\"] = 0\nall_features[\"Type_t\"] = 0\n\n# Method\nall_features[\"M_VB\"] = 0\nall_features[\"M_SP\"] = 0\nall_features[\"M_PI\"] = 0\nall_features[\"M_S\"] = 0\nall_features[\"M_SA\"] = 0\n\n# Regionname\nfor name in all_features.Regionname.unique():\n    all_features[name] = 0\n\n# CouncilArea\nfor name in all_features.CouncilArea.unique():\n    all_features[name] = 0\n\n# Postcode\nall_features[\"New_Postcode\"] = float()\n\n# Suburb\nall_features[\"New_Suburb\"] = float()\n\n# SellerG\nall_features[\"New_SellerG\"] = float()\n\nfor i in range(len(all_features)):\n    all_features[\"Year\" + str(int(all_features.Date[i][-4:]))][i] = 1\n    all_features[\"Mo\" + str(int(all_features.Date[i][-7:-5]))][i] = 1\n    all_features[\"Type_\" + all_features.Type[i]][i] = 1\n    all_features[\"M_\" + all_features.Method[i]][i] = 1\n    all_features[all_features.CouncilArea[i]][i] = 1\n    all_features[all_features.Regionname[i]][i] = 1\n    \n    if str(train_data[train_data.Suburb == all_features.Suburb[i]].Price.mean()) == \"nan\":\n        all_features[\"New_Suburb\"][i] = 13.4\n    else:\n        all_features[\"New_Suburb\"][i] = train_data[train_data.Suburb == all_features.Suburb[i]].Price.mean()\n\n    if str(train_data[train_data.SellerG == all_features.SellerG[i]].Price.mean()) == \"nan\":\n        if str(train_data[train_data.Suburb == all_features.Suburb[i]].Price.mean()) == \"nan\":\n            all_features[\"New_SellerG\"][i] = 13.4\n        else:\n            all_features[\"New_SellerG\"][i] = train_data[train_data.Suburb == all_features.Suburb[i]].Price.mean()\n    else:\n        all_features[\"New_SellerG\"][i] = train_data[train_data.SellerG == all_features.SellerG[i]].Price.mean()\n\n    if str(train_data[train_data.Postcode == all_features.Postcode[i]].Price.mean()) == \"nan\":\n        all_features[\"New_Postcode\"][i] = 13.4\n    else:\n        all_features[\"New_Postcode\"][i] = train_data[train_data.Postcode == all_features.Postcode[i]].Price.mean()","4ed5cff5":"all_features[\"New_Address\"] = 0\nall_features[\"Address_type\"] = 0\nall_features[\"Address_type_final\"] = float()\n\n# New Address\nfor i in range(len(all_features)):\n    for j in range(len(all_features.Address[i])):\n        if all_features.Address[i][j] == \" \":\n            if all_features[\"New_Address\"][i] == 0:\n                all_features[\"New_Address\"][i] = all_features.Address[i][j+1:]\n            else:\n                all_features[\"Address_type\"][i] = all_features.Address[i][j+1:]\n\n# Address_type\nf = pd.concat([all_features.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\n\nfor i in range(len(all_features)):\n    if all_features.Address_type[i] not in f.Address_type.unique():\n        all_features[\"Address_type_final\"][i] = 13.4\n    else:\n        all_features[\"Address_type_final\"][i] = f[f.Address_type == all_features.Address_type[i]].Price.mean()","03e27751":"# Drop the base category from categorical variables (Month, Type, Method, Year, CouncilArea)\nall_features.drop([\"Mo1\", \"Type_h\", \"M_S\", \"Year2016\", \"Southern Metropolitan\"], axis=1, inplace=True)\n\n# Drop repeated variables (bedroom2)\nall_features.drop([\"Bedroom2\"], axis=1, inplace=True)\n\n# Drop useless variables\nall_features.drop([\"Lattitude\", \"Longtitude\", \"Postcode\", \"Address\", \"Address_type\", \"Suburb\", \"Date\", \"Method\", \"Regionname\", \"New_Address\", \"SellerG\", \"Type\", \"CouncilArea\"], axis=1, inplace=True)","629fb946":"all_features","81a2f613":"# identify outliers in the training dataset\niso = IsolationForest(contamination=0.01)\nf = pd.concat([all_features.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\nX = f._get_numeric_data()\nyhat = iso.fit_predict(X)\n\n# select all rows that are not outliers\nmask = yhat != -1\nfinal_train_data = f.iloc[mask, :]\n\nfinal_train_data","0eeda9f8":"final_train_data = final_train_data._get_numeric_data()\nX = final_train_data.drop(\"Price\", axis=1)\ny = final_train_data[\"Price\"]\n\nX_ols = X\nscaler = StandardScaler()\nX_ols = scaler.fit_transform(X_ols)\nX_ols = sm.add_constant(X_ols)\nmodel = sm.OLS(y, X_ols)\nmodel_fit = model.fit()\nmodel_fit.summary()","84eb1e93":"rmse = 0\nmae = 0\ny_hat = list()\ny_real = list()\nindex_list = list()\n\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=i)\n\n    X_train = scaler.fit_transform(X_train)\n    X_train = sm.add_constant(X_train)\n    X_test = scaler.fit_transform(X_test)\n    X_test = sm.add_constant(X_test)\n    model = sm.OLS(y_train, X_train)\n    model_fit = model.fit()\n    pred = model_fit.predict(X_test)\n\n    pred = np.exp(pred)\n    y_val_sq = np.exp(y_test.values)\n\n    for i,p in enumerate(pred):\n        if abs(y_val_sq[i] - p) >= mean_squared_error(y_val_sq, pred, squared=False):\n            index_list.append(y_test.index[i])\n\n    y_hat.append(pred)\n    y_real.append(y_val_sq)\n\n    rmse += mean_squared_error(y_val_sq, pred, squared=False)\n    mae += mean_absolute_error(y_val_sq, pred)\n\n# Create the map\nlineal_map = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=10)\n\n# Add points to the map\nmc = MarkerCluster()\nfor idx, row in train_data.iloc[index_list].iterrows():\n    if not math.isnan(row['Longtitude']) and not math.isnan(row['Lattitude']):\n        mc.add_child(folium.Marker([row['Lattitude'], row['Longtitude']]))\n        lineal_map.add_child(mc)\n\n# Display the map\ndisplay(lineal_map)","364f1a0f":"print(\"Linear Regression RMSE: \", rmse\/10, \"\\nLinear Regression MAE: \", mae\/10)","a029031b":"y_hat_final = list()\ny_real_final = list()\n\nfor i in range(len(y_hat)):\n    for j in range(len(y_hat[i])):\n        y_real_final.append(y_real[i][j])\n        y_hat_final.append(y_hat[i][j])\n\n\nplt.figure(figsize=(15,7))\nplt.scatter(y_real_final, y_hat_final, alpha=0.3)\nplt.xlabel('True price')\nplt.ylabel('Predicted price ')","bcb32da4":"y_res = list()\nfor i in range(len(y_hat_final)):\n    y_res.append(y_real_final[i] - y_hat_final[i])\n\nplt.figure(figsize=(15,7))\nx_plot = plt.scatter(y_hat_final, y_res, c='b')\nplt.hlines(y=0, xmin= 0, xmax=1e7)\nplt.title('Residual plot')\nplt.xlabel('$\\hat y$')\nplt.ylabel('$y - \\hat y$')","4adacf3d":"def RMSE(dataset, X, y, m):\n    model = m\n    rmse = 0\n    mae = 0\n    index_list = list()\n    y_hat = list()\n    y_real = list()\n    y_hat_final = list()\n    y_real_final = list()\n    huge_errors = list()\n    huge_errors_1 = list()\n\n    for i in range(10):\n        X_test, X_val, y_test, y_val = train_test_split(X, y, random_state = i)\n        model.fit(X_test, y_test)\n        pred = model.predict(X_val)\n\n        pred = np.exp(pred)\n        y_val_sq = np.exp(y_val.values)\n\n        y_hat.append(pred)\n        y_real.append(y_val_sq)\n\n        rmse += mean_squared_error(y_val_sq, pred, squared=False)\n        mae += mean_absolute_error(y_val_sq, pred)\n    \n        for i,p in enumerate(pred):\n            if abs(y_val_sq[i] - p) >= mean_squared_error(y_val_sq, pred, squared=False):\n                index_list.append(y_val.index[i])\n            if abs(y_val_sq[i] - p) >= 3*(mean_squared_error(y_val_sq, pred, squared=False)):   \n                huge_errors.append(y_val.index[i])\n                huge_errors_1.append([y_val_sq[i] - p, y_val_sq[i], p])\n\n    for i in range(len(y_hat)):\n        for j in range(len(y_hat[i])):\n            y_real_final.append(y_real[i][j])\n            y_hat_final.append(y_hat[i][j])\n\n    # Create the map\n    map_1 = folium.Map(location=[-37.831364,144.973458], tiles='cartodbpositron', zoom_start=10)\n\n    # Add points to the map\n    mc = MarkerCluster()\n    for idx, row in train_data.iloc[huge_errors].iterrows():\n        if not math.isnan(row['Longtitude']) and not math.isnan(row['Lattitude']):\n            mc.add_child(folium.Marker([row['Lattitude'], row['Longtitude']]))\n            map_1.add_child(mc)\n\n    # Display the map\n    display(map_1)\n    \n\n    return [rmse\/10, mae\/10, y_hat_final, y_real_final, huge_errors, huge_errors_1]\n    \nfinal_train_data = final_train_data._get_numeric_data()\nX = final_train_data.drop([\"Price\"], axis=1)\ny = final_train_data[\"Price\"]\n\nerrors = RMSE(train_data, X,y, Ridge())\n","302641c7":"print(\"Ridge Regression RMSE: \", errors[0], \"\\nRidge Regression MAE: \", errors[1])","ae2ae639":"plt.figure(figsize=(15,7))\nplt.scatter(errors[3], errors[2], alpha=0.3)\nplt.xlabel('True price')\nplt.ylabel('Predicted price')","b89ed744":"y_res = list()\nfor i in range(len(errors[2])):\n    y_res.append(errors[3][i] - errors[2][i])\n\nplt.figure(figsize=(15,7))\nx_plot = plt.scatter(y_hat_final, y_res, c='b')\nplt.hlines(y=0, xmin= 0, xmax=1e7)\nplt.title('Residual plot')\nplt.xlabel('$\\hat y$')\nplt.ylabel('$y - \\hat y$')","be18db65":"final_train_data","3c5034f7":"errors = RMSE(train_data, X,y, Lasso(alpha=0.001, max_iter=10e5))","19cadfe3":"print(\"Lasso Regression RMSE: \", errors[0], \"\\nLasso Regression MAE: \", errors[1])","b2c9527f":"plt.figure(figsize=(15,7))\nplt.scatter(errors[3], errors[2], alpha=0.3)\nplt.xlabel('True price')\nplt.ylabel('Predicted price')","4adb59f0":"y_res = list()\nfor i in range(len(errors[2])):\n    y_res.append(errors[3][i] - errors[2][i])\n\nplt.figure(figsize=(15,7))\nx_plot = plt.scatter(y_hat_final, y_res, c='b')\nplt.hlines(y=0, xmin= 0, xmax=1e7)\nplt.title('Residual plot')\nplt.xlabel('$\\hat y$')\nplt.ylabel('$y - \\hat y$')","0f6965bb":"f = all_features.loc[(train_data.shape[0]):]\nfinal_train_data = final_train_data._get_numeric_data()\n\nf = f._get_numeric_data()\nX_train = final_train_data.drop([\"Price\", \"Propertycount\"], axis=1)\nX_test = f.drop(\"Propertycount\", axis=1)\ny = final_train_data[\"Price\"]\n\nmodel = Ridge()\nmodel.fit(X_train,y)\npred = model.predict(X_test)\npred = np.exp(pred)","e6fd0b8e":"my_submission = pd.DataFrame({'index': test_data.index, 'Price': pred})\nmy_submission.to_csv('Submission.csv', index=False)","7a50f40f":"f = pd.concat([all_features.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\n\nf = f._get_numeric_data()\nX = f.drop(\"Price\", axis=1)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ny = f[\"Price\"]\n\ndef smModel(X,y):\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    model_fit = model.fit()\n    print(model_fit.summary())","5a2f2920":"# Let's create a dataframe to apply changes in this section\nregression = pd.DataFrame()","b533099a":"f = pd.concat([all_features.loc[:(len(train_data)-1),:], target], axis=1)\ncorr = f.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","45e8db32":"def calc_vif(X):\n    \"\"\"This function takes a specific database and returns the VIF (Variance Increment Factor) of the variables available\"\"\"\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\n# I'll first create a database with numerical variables (numerical_df)\nnumerical_df = all_features._get_numeric_data()\n# Once I've created the database, I'll ask the function calc_vif for the VIF values of the variables available in the dataset\nv = calc_vif(numerical_df)\n# I'll keep only the highly correlated variables (VIF >= 5)\nv = v[v.VIF >= 5]\nv.sort_values(by=\"VIF\", ascending=False, inplace=True)\nv.reset_index(inplace=True)\n\nv","eee4a953":"cat = list()\nnum = list()\nfor col in numerical_df.columns:\n    if (col not in train.CouncilArea.unique()) and (col not in train.Regionname.unique() and (col[:2] not in [\"Mo\", \"M_\", \"Ty\"]) and (col not in [\"Year2017\", \"Mitchell\", \"Expensive_SG\", \"Expensive_Address\"])):\n        num.append(col)\n    else:\n        cat.append(col)\n\nscaler = StandardScaler()\npca_variables = numerical_df[num]\npca_variables = scaler.fit_transform(pca_variables)\npca = PCA().fit(pca_variables)\nplt.figure(figsize=(10,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","89a36116":"pca = PCA(n_components=10)\nnew_variables = pca.fit_transform(pca_variables)\n\n# Incorporate the pca variables to the dataset\n\nfor i in range(10):\n    regression[\"PCA_\" + str(i)] = all_features.Bathroom\n\nfor i in range(len(all_features)):\n    for j in range(10):\n        regression[\"PCA_\" + str(j)][i] = new_variables[i][j]\nregression","bf9a6bbe":"pca_all = pd.concat([regression, all_features[cat]], join=\"inner\", axis=1)\nf = pd.concat([pca_all.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\n\nf = f._get_numeric_data()\nX = f.drop(\"Price\", axis=1)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ny = f[\"Price\"]\n\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X)\nmodel_fit = model.fit()\nprint(model_fit.summary())","4737ea3d":"names = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\nbp_test = het_breuschpagan(model_fit.resid, model_fit.model.exog)\n\nlzip(names, bp_test)","20731ecd":"model = sm.OLS(y, X)\nmodel_fit = model.fit(cov_type='HC1')\nprint(model_fit.summary())","3144ff05":"influence = model_fit.get_influence()\n(c,p)=influence.cooks_distance\nplt.stem(np.arange(len(c)), c, markerfmt=\",\")","29305215":"for i,d in enumerate(c):\n    if d > 3.5:\n        print(i)","7cbb0d2b":"train_data.loc[[2609]]","c089172f":"f = pd.concat([regression.loc[0:(train_data.shape[0]-1)], train_data[\"Price\"]], join=\"inner\", axis=1)\n\nf.drop([2609], inplace=True)\nf = f._get_numeric_data()\nX = f.drop(\"Price\", axis=1)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ny = f[\"Price\"]\n\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X)\nmodel_fit = model.fit()\nprint(model_fit.summary())","81beeef4":"### BuildingArea\n\n**BuildingArea** is by far the variable with most missing values, it has over the 50% of the observations missing. This variable is closely related with the variable Rooms, this makes a lot of sense since a bigger house tend to have a larger amount of rooms. I'll fill this variable by using the median (not the mean, due to the presence of upper outliers) of this variables in similar houses (in terms of rooms and suburb).","629ed710":"Well, Multicollinearity affects to the interpretation of the coefficients as well as the variance and individual significance tests. But, if the multicollinearity relationships are equal into the test and train datasets, this won't be a problem in terms of prediction accuracy.\n\nIn this notebook, I'll deal with Multicollinearity by using the PCA (Principal Component Analysis) method, which is going to solve the problem and, at the same time, reducing the dimensionality of the database. For more information about this method, check the following link.\n\nPCA explanation: https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis","2dcd3e4e":"First of all, let's evaluate which variables are correlated. I'll print the correlation matrix as well as the VIF (Variance Inflation Factor) of those variables with a VIF higher than 5 (which is considerated problematic).","4cdf13fd":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that lets us normalize data.","8cdb571a":"### YearBuilt\n\n**YearBuilt** is the second variable with most missing values, it has over the 40% of the observations missing. This variable is closely related with the variable Distance, this is also understandable because the houses located in the center of the city were build earlier. I'll fill this variable by using the mean of the nearest houses (in terms of distance).","4087f882":"# Melbourne House Price Prediction\n\nThis notebook is going to cover the *Housing House Prediction* competition in Kaggle as a part of the **regression techniques** theory at the postgraduate course on Data Science and Bigdata by the University of Barcelona. The main focus of this exercise is to improve our knowledge in regression, from data analysis to modeling. Although this competition allows the use of different models, we are recommended to use the models reviewed during the sessions and avoid using more complex models such as XGBoost and LightGBM.\n\n## Introduction\n\nThe main objective for this competition is to describe the behavior of the **Melbourne Home Sales Market** by using regression models, mainly linear models such as Linear regression, Ridge regression and Lasso regression. Linear Regression attempts to model the linear relationship between predictor variables (X) and a numeric target variable (y). For each observation that is evaluated with the model, the actual value of the target (y) is compared to the predicted value of the target (\u0177), and the difference in these values are known as residuals. The goal of a linear regression model is to minimize the sum of the squared residuals. \n\n## Index\n\n1. **Libraries**\n\n2. **Understanding Data**\n\n   2.1. *Read the database*\n\n   2.2. *Basic information about the dataset*\n\n   2.3. *Data analysis*\n\n   - Univariant analysis\n\n   - Spatial analysis\n\n   - Bivariant analysis\n\n3. **Preprocessing**\n\n    3.1. *Missing Values*\n\n    3.2. *Fix skewed features*\n\n    3.3. *Feature Engineering*\n\n    3.4. *Categorical Variables*\n\n    3.5. *Outlier Detection*\n\n4. **Model**\n\n    4.1. *Linear Regression*\n\n    4.2. *Ridge Regression*\n\n    4.3. *Lasso Regression*\n\n    4.4. *Submission*\n\n5. **Model assumptions**\n\n   5.1. *Multicollinearity*\n\n   5.2. *Heteroskedasticity*\n\n   5.3. *Outliers*","2c25b6ad":"### 2.3.2. Spatial Visualization\n\nIt's nearly impossible to understand how the housing price is build without taking into account the spatial dimension. The same house in two different neighborhoods will have a different price. In this section I'll focus on the spatial distribution of the study variable.\n\nMany of the spatial variables available in the dataset are classified as *non ABS (**A**ustralian **B**ureau of **S**tatistics) structures*. This variables represent administrative areas for which the ABS is committed to providing a range of statistics. This is the case for *CouncilArea* (Local Government Areas), *Postal Areas* (POA) and *State Suburbs* (SSC). Apart from this variables, we also have a *Regionname* variable, which is even more generic than the other ones (it covers a bigger range).\n\nLet's start by ploting all the variables in a single map, just to notice which is the grographic area we are covering.","3ba28ff1":"### 3.5 Outliers Detection\n\nI've decided to use the Isolation Forest method to deal with outliers. For further information check the following link: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html","495fd884":"Acording to the ideas we've extract from the previous maps, higher distance must mean lower price. Let's plot the correlation between the variable *Price* and *Distance*. If we take into account that this variable do not consider any characteristics from the house, a correlation up to 0.166 looks huge.","7a7beefd":"The visualization for numerical features helps us to identify the relationship between these variables and the study variable *Price*. In most of those variables, the relationship is not even clear. Lattitude and Longitude show a poor relationship with the price, that's something that I already said before. But variables such as Propertycount and Postcode don't seem to be very useful in order to describe the Price.","8de277d0":"### 3.2. Fix skewed features","dec731c1":"It looks that many of our variables are correlated. I'll apply the PCA method to those variables which are not categories.","51085273":"The distribution of the variable *Price* is clearly leptokurtic and skewed to the right due to the presence of some upper outliers. I'll apply a logaritmic transformation to the variable in order to get a better accuracy in the modeling part.","6ceab2c2":"Let's first remove some outliers in *BuildingArea* and *Landsize* to make the visualization easier.","32f58b3b":"### Car\n\n**Car** is a variable that contains the garage space of a specific house in terms of the number of cars. This variable is closely related with rooms (once again, bigger house implier larger amount of car spaces) and the variable Distance (which has been explained in the previous section). I'll fill this variable by using the median (not the mean, due to the presence of upper outliers) of this variables in similar houses (in terms of rooms and distance).","68de0918":"Once I've converted all the categorical variables that I want to include in the ML model, its time to drop some variables which have become useless.","48b0706e":"### 4.4. Submission","af2b0afd":"### CouncilAreas","7754ffec":"### 3.3. Feature Engineering\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\n\nFirst of all, I'll apply a quadratic transformation to the following variables: Rooms, Car, Bathroom, YearBuilt, BuildingArea, Landsize and Propertycount. In the cell above I've printed the distribution of these variables. The main idea of this transformation is to make the model understand the non-linearity of these variables. In general terms, more rooms, car spaces, and bathrooms do imply a rise in the price of a specific house. But, if we pay close attention to the graphs we'll see that this relation is not clear for extreme values. By applying a quadratic transformation, the model should be able to capt this relation.","26112238":"Now, let's plot the relationship between CouncilArea (municipally) and the price.","cda15fc2":"### Numerical Features","3e1000ab":"## 3. Preprocessing\n\n### 3.1. Missing values","21be3bda":"### Categorical Features\n\nMost of the categorical variables available in this dataset have a huge amount of categories. Due to that reason, visualization becomes less efficient and the interpretation of the results also becomes more complex. I'll visualizate all categorical variables except \"Address\", because it has to many unique values. I won't get deep in the analysis of spatial variables (Regionname, Suburb, CouncilArea and Address) because the following section is all about them.\n\nLet's start by the visualization of the variables with less categories: Type, Method and Regionname.","188a95be":"## 5. Model Assumptions\n\nUntil now, due to the nature of the competition, which uses the Root Mean Square Error (RMSE) as a ranking measure, I didn't take the linear model assumptions too seriously. If we only want to focus on prediction accuracy, we don't care about problems such as **Multicollinearity** or **Heteroscedasticity**.\n\nIn this section of the notebook, I'll check these assumptions and build a model according to them. The linear model (OLS) assumes that its coefficients are *consistent*, *efficient* and *unbiased*. This is only true if the variables of the model present a relationship which is characterized for being linear, homoscedastic, non-autocorrelated and independent to the other variables of the model.\n\nLet's first print some of the main statistics about the model we have used to make predictions.","b33a7e55":"## 2. Understanding Data\n\n### 2.1. Read the Database\n\nThe database that I'll be using in this notebook is avaiable in the following link: https:\/\/www.kaggle.com\/c\/housing-house-prediction\/data","d6b5a91f":"## Suburbs\n\nFor what we've seen, it seems that the localition is such an important feature to predict the price for a specific observation. Let's focus now on Suburbs, a way of geographical distribution for Melbourne.\n\nList for the most expensive suburbs in Melbourne: https:\/\/www.openagent.com.au\/blog\/most-expensive-suburbs-in-melbourne-2019#","7f52441e":"### 2.2. Basic information about the datasets","ee957d8f":"Now, once we've seen the geographic distribution of the observations available in the dataset, let's focus on the variables. I got maps related to the CouncilArea, Suburbs, and Postcodes. I couldn't manage to find a map for Regionname. I'll analyze these variables from less specific to more specific. This will be the order: CouncilArea, Postcodes, Suburbs.","75d7fa9d":"### 2.3. Analysis\n\nNow, once we have seen some basic information about the dataset, I'll focus on analyzing the distribution of the variables. I'll first aim in univariant analysis, starting with the study variable (**Price**), paying close attention to the distribution, and trying to get insights from data visualization. Then, I'll go over the descriptive variables which, as we've seen before, there are plenty of categorical variables related to the localization of the houses. Due to that reason, I'll also dedicate one section of this notebook to the spatial distribution of the variables. After that, I'll do bivariant analysis, looking for relations between the dataset variables (specially between the study variable and the descriptive variables). The correlation coefficient is the most used tool in order to quantify the realtion between variables, I'll use this coefficient to select which variables are likely to be used in a Machine Learning Algorithm and which don't.\n\n### Study Variable: Price","9dbbbb24":"### 5.1. Multicollinearity\n\nMulticollinearity occurs when the explanatory variables which are assumed to be independent of each other are revealed to be closely related to each other, this correlation is referred to as collinearity. When this correlation is observed for two or more explanatory variables, it is known multi-collinearity.\n\nMulti-collinearity is particularly undesirable because it impacts the interpretability of linear regression models. This is what is going to happen if we don't take this problem into account:\n\n1. Uncertainty in coefficient estimates or unstable variance: Small changes (adding\/removing rows\/columns) in the data results in change of coefficients.\n\n2. Increased standard error: Reduces the accuracy of the estimates and increases the chances of detection.\n\n3. Decreased statistical significance: Due to increased standard error, t-statistic declines which negatively impacts the capability of detecting statistical significance in coefficient leading to type-II error.\n\n4. Reducing coefficient & p-value: The importance of the correlated explanatory variable is masked due to collinearity.\n\n5. Overfitting: Leads to overfitting as is indicated by the high variance problem.","25748e05":"Finally, let's visualizate the relationship between the variables Date and Price.","aca99b96":"### 2.3.3. Bivariant analysis","77493e89":"### 5.4. Outliers","0372019c":"### 3.4. Categorical Variables\n\nAs we have seen before, there are plenty of categorical variables in this dataset. These are the ones that I decided to use: Date, Regionname, CouncilArea, Postcode, Suburb, Address, SellerG, Type and Method. I'll now explain how I'll exactly deal with these variables.\n\n- Let's start with *Date*, this variable contains three main pieces of information about the transaction: Day, Month, and Year. I'll only keep the Year (2016\/2017) and Month(January-December).\n\n- About the variables related to the location of the house, I'll create binary variables for all the categories related with the variables *CouncilArea* and *Regionname*, because they have a few number of categories. For *Suburb* and *Postcode* I'll create a new variable to store the mean of the price for each category. This is not going to create problems related with **data leakage** due to the presence of nearly the same categories either for the test and train datasets.\n\n- For *Type* and *Method*, due to their lack of categories, I'll create as many binary variables as categories the variable has.\n\n- Finally, I'll create a new variable for SellerG, using the same methodology as I used with Suburb and Postcode. This variable may introduce some **data leakage** due to the presence of different categories between the test and train datasets. There are around 60 categories that are just in one of these datasets. Although, I think it can be worth creating the variable and then see if it's relevant.","783b8b83":"## 4. Model\n\n### 4.1. Linear Regression\n\nLet's first plot some basic information about the linear regression model.","84ffd24e":"## 1) Libraries","3793cd0b":"By looking to these maps we notice that the most expensive houses are located near the center of the city. As we get far from there, the price becomes lower and lower.\n\nApart from understanding the grographical distribution of the *Price*, this maps helps us to understand the functionality of the variable *Distance*, which contains the distance from a specific observation to the CBD. But, what is the CBD and where is located? Well, the **C**enter **B**usiness **D**istrict is located in the center of the city (see output from the following cell).","c8caec7a":"### Postcodes","03f088a3":"### Landsize\n\n**Landsize** is a numerical variable that contains the land size for a specific house in meters. This is a very dificult variable to imputate values due to the lack of a relation between these variable and the other variables in the dataset. For this reason, I've decided to imputate missing values by using the mean of the suburb.","eeb88bb1":"### Numerical Variables\n\nAbout missing imputation for numerical values, I've tested the KNNImputer and IterativeImputer but I've finally dediced to impute by using the most correlated variables as \"predictors\".","0fa1e6f1":"Now its time to evaluate its accuracy.","bfa810be":"Now let's create some new variables","cc28b0d7":"Address","0ef38b49":"### 4.3. Lasso Regression","0068c43e":"About the distribution of the price in the suburbs:","97ec1028":"Once we've seen how the observations are geographically distributed, it's time to ask if the study variable (*Price*) is randomly distributed along the territory or if it follows a specific patter. I'll  display a few headmaps, each with a different price restriction (from lower to higgher restriction).","4fd51c21":"There are 21 different variables available in the datasets (since *index* is just an index). The main variable of this study (the one we'll have to predict) is **Price**, which contains the price for a specific house in Australian dollars ($). About the rest of the variables, many of them are categorical and another few have missing values. It seems that there will be a lot of work in data preprocessing.\n\nIf we take a look at the variables available, we may notice that there are three main groups: **House Characteristics**, **House Localization**, and **Saler Information**. About the first group, the majority of variables are numerical, so we'll mainly have to focus on the distribution of them. The variables related to the **House Localization** are different, we notice that a huge amount of them are categorical (such as the CouncilArea or the Address), there are also numerical variables that seem to be useless unless some transformation is applied, this is the case for latitude and longitude, which will surely not follow a linear relation with the study variable. Finally, in the **Saler Information** we only have three variables, SellerG, which informs us who has sold the house, Date, which informs us the moment in which the house was sold, and Method, which gives us information about the type of sale.","5f12c384":"### 4.2. Ridge Regression","227066a5":"### 5.2. Heteroscedasticity","9cc0acd5":"### Categorical Variables\n\nMissing values related with the **CouncilArea** variable are easy to fix since the variable **Suburb** do not have missing values. An specific Suburb will always have the same CouncilArea. So, to fill this values we just have to look for other observations with the same Suburb. \n\nThere are a huge amount of Suburbs in Melbourne and our dataset do not contain information about all of them. Due to this reason, it will happen that in some cases we won't have another observation to fill the missing, in this cases, I just looked on the internet for the specific CouncilArea."}}