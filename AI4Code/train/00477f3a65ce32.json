{"cell_type":{"9cd46ea7":"code","0c4917da":"code","ce554f9d":"code","6450476a":"code","8756e19a":"code","36434ade":"code","e95c06e5":"code","a5b384df":"markdown","51ccd926":"markdown","c653422f":"markdown","f931d16d":"markdown","53176c24":"markdown","2e34e13d":"markdown","2a15a959":"markdown","0039dc8a":"markdown","f19de5e8":"markdown"},"source":{"9cd46ea7":"!pip install BorutaShap","0c4917da":"!pip install scikit-learn -U","ce554f9d":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Feature selection\nfrom BorutaShap import BorutaShap\nfrom xgboost import XGBClassifier\n\n# Validation\nfrom sklearn.model_selection import KFold, StratifiedKFold","6450476a":"# Derived from the original script https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage \n# by Guillaume Martin\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","8756e19a":"## Loading data \nX_train = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\").set_index('id')\nX_test = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\").set_index('id')\n\n# Feature engineering\nunique_values = X_train.iloc[:1000].nunique()\ncategoricals = [col for col in  unique_values.index[unique_values < 10] if col!='target']\nnumeric = [col for col in X_test.columns if col not in categoricals]\n\nX_train['mean_numeric'] = X_train[numeric].mean(axis=1)\nX_train['std_numeric'] = X_train[numeric].std(axis=1)\nX_train['min_numeric'] = X_train[numeric].min(axis=1)\nX_train['max_numeric'] = X_train[numeric].max(axis=1)\nX_train['sum_categoricals'] = X_train[categoricals].sum(axis=1)\n\nX_test['mean_numeric'] = X_test[numeric].mean(axis=1)\nX_test['std_numeric'] = X_test[numeric].std(axis=1)\nX_test['min_numeric'] = X_test[numeric].min(axis=1)\nX_test['max_numeric'] = X_test[numeric].max(axis=1)\nX_test['sum_categoricals'] = X_test[categoricals].sum(axis=1)\n\n# Sampling for speeding up things\nX_train = X_train.sample(n=200_000, random_state=0)\n\n# target\ny_train = X_train.target\nX_train = X_train.drop('target', axis='columns')","36434ade":"folds = 5\nkf = KFold(n_splits=folds,\n           shuffle=True, \n           random_state=0)\n\nselected_columns = list()\n    \nfor k, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n    \n    print(f\"FOLD {k+1}\/{folds}\")\n    \n    model = XGBClassifier(\n        colsample_bytree= 0.50, \n        subsample= 0.50, \n        learning_rate= 0.012, \n        max_depth= 3, \n        min_child_weight= 252,\n        n_estimators= 1000,\n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        eval_metric='auc',\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor='gpu_predictor'\n     )\n    \n    Feature_Selector = BorutaShap(model=model,\n                                  importance_measure='shap', \n                                  classification=True)\n\n    Feature_Selector.fit(X=X_train.iloc[train_idx, :], y=y_train.iloc[train_idx], \n                         n_trials=50, random_state=0)\n    \n    Feature_Selector.plot(which_features='all', figsize=(24,12))\n    \n    selected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \n    print(f\"Selected features at fold {k+1} are: {selected_columns[-1]}\")","e95c06e5":"final_selection = sorted({item for selection in selected_columns for item in selection})\nprint(final_selection)","a5b384df":"Gradient Boosting incorporates feature selection, since the trees spit only on significant features (or at least they should). In reality, this is not always true as sometimes noisy, irrelevant splits may appear in the tree. Moreover, working with not useful features will cause your training to go slower.\n\nGenerally, widely recognized benefits of featue selection are:\n\n* simplification of models to make them easier to interpret\n* shorter training times,\n* to avoid the curse of dimensionality,\n* more generalization by reducing overfitting (reduction of variance)","51ccd926":"# Feature selection with Boruta-SHAP to increase your score","c653422f":"## Happy Kaggling!","f931d16d":"#### Let's start by uploading packages and data","53176c24":"Boruta-SHAP is a package combining Boruta (https:\/\/github.com\/scikit-learn-contrib\/boruta_py), a feature selection method based on repeated tests of the importance of a feature in a model, with the interpretability method SHAP (https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html).\n\nBoruta-SHAP, developed by Eoghan Keany (https:\/\/github.com\/Ekeany\/Boruta-Shap), is extremely simple to use: get your best model, let it run some time on Boruta-SHAP and evaluate the results!\n\np.s\np.s. You can read more about Boruta-SHAP on this Medium article by the author: https:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677","2e34e13d":"#### Now we pick our best model and let Boruta-SHAP run a few experiments (usually 50 are enough) before getting the results.\n\n#### We cross-validate our experiments in order to ascertain that we are indeed picking the right variables\n\n#### as the results are prepared and we can plot them to visualize the Z-scores intervals of our features. That will signal us the confidence of the choice made by the algorithm in selecting or rejecting features.\n\n#### Please notice that the last two features are noisy features used by Boruta-SHAP to fgure out the important features. Clearly they are non-significant.\n\n#### Cross-validation takes time. Meanwhile we can grab a cup of coffee and relax as Boruta-SHAP is doing all the heavy-lift job.\n\n![immagine.png](attachment:8530c9d8-3db0-4a8e-b4a0-6d5e5c1cf18f.png)","2a15a959":"#### In this competition there are quite a few of features. Is there a way to eliminate the unuseful ones? \n\nFeature selection has important advantages:\n 1. by training new useful models that others don't have in their ensemble\n 2. by making your models run better","0039dc8a":"![immagine.png](attachment:941cf5aa-8564-475b-ac05-5e2505a85605.png)","f19de5e8":"#### Here we finally have the good set of features to be used in this competition (at least using XGBoost - better to test for other algorithms)"}}