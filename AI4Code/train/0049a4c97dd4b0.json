{"cell_type":{"91bdb894":"code","7b9f037b":"code","521e1332":"code","b45e4e0d":"code","9449a66b":"code","68984187":"code","bc7ab0ca":"code","89d41c2b":"code","83771aa5":"code","aa548930":"code","f52fa7e8":"code","46f0557c":"code","44e545a9":"code","2bd77db1":"code","fd9fde27":"code","f0274a4c":"code","8efc81cf":"code","89236908":"code","587177f1":"code","2cead88c":"code","0a654703":"code","c2e70b13":"code","3d13f14d":"code","e9cb4499":"code","1639f4b2":"code","9ecfc581":"code","1a150547":"code","8f6a757c":"code","44d298cf":"code","bffbfaeb":"code","fbaec468":"code","c0918a96":"code","b49701f2":"code","a5885d8a":"code","d3dcd979":"code","eedc5801":"code","3409c7c7":"code","6ef9a107":"code","6cf1b832":"code","addb3c2c":"code","0024b794":"code","e8cbf42d":"code","a7874bda":"code","cc42fa5d":"code","1bbdf6de":"code","37e19b96":"code","52d833f9":"code","42ddb461":"code","f99c50cd":"code","63511075":"code","18ac79d2":"code","ace61cf1":"code","63c37f38":"code","64d11d35":"code","d56412f3":"code","83cbd860":"code","96af0ed5":"code","a82419c1":"code","04151ae4":"code","f9de0b3f":"code","8f42e372":"code","35059236":"code","b44e325a":"code","4eb24a6b":"code","5a154463":"code","4a5e2f86":"code","3d9c03f3":"code","5870d441":"code","586a9bc6":"code","53c1de03":"code","7be4ca96":"code","afe95573":"code","472ced29":"code","5c5bdbf4":"code","823a1afc":"code","234fab37":"code","1b661cfc":"code","cd01d224":"code","3312c518":"code","082b5e82":"code","9b90da03":"code","0eff2ad5":"code","6a6f49f0":"code","b59cb5a5":"code","3e613e6d":"code","6185589c":"code","5cbe646d":"code","eedea877":"code","b01a0e7a":"code","18cccc09":"code","557715fc":"code","386b4e73":"code","56816945":"code","f994c6cb":"code","7b16ecc6":"code","63eb9f36":"code","ed5062a1":"code","f1d3d6a0":"code","cadcad4f":"code","2351edcd":"code","d2ab84bd":"code","fc618a53":"code","43229f73":"code","1df8fdc0":"code","9e70539d":"code","cbebf340":"code","2165fa6d":"code","8e73f6fa":"code","d3fdeb63":"code","fc7e4f5b":"code","08dfd72b":"code","c19a4cec":"code","58ba4217":"code","f4186918":"code","87736ca7":"code","cf05c2e0":"code","e34d32b4":"code","be7b03d0":"code","eb6de169":"code","751ea32e":"code","db918390":"code","43172d43":"code","840b6bb8":"code","cce0a88f":"code","5ba7d976":"code","b464f477":"code","a70a0717":"code","4c070806":"code","b72f4dd1":"code","fc4b326a":"code","a66b882c":"code","e76fe3ba":"code","c76ec9e1":"code","831727ba":"code","968a752b":"markdown","493a7eb0":"markdown","f326e455":"markdown","2c429773":"markdown","a96344dd":"markdown","7214f006":"markdown","4b0eb064":"markdown","2f7976e9":"markdown","d1a2cc31":"markdown","8820e703":"markdown","b98932ac":"markdown","462feb45":"markdown","31966308":"markdown","f878b9d2":"markdown","6aa8ad2e":"markdown","71be3c1f":"markdown","7adb9322":"markdown","6746af05":"markdown","9f3c752f":"markdown","ca588eac":"markdown","80495772":"markdown","a1704216":"markdown","12277b35":"markdown","0cda5b67":"markdown","3976b18b":"markdown","4e95f731":"markdown","db9fa972":"markdown","b52e76eb":"markdown","db2019f8":"markdown","024ce594":"markdown","24cbb2ca":"markdown","ff35c5f0":"markdown","a91e92e3":"markdown","971fd539":"markdown","32a7a4fd":"markdown","547a9973":"markdown","1979c7b3":"markdown","07c67ead":"markdown","0ceaa914":"markdown","c772d465":"markdown","ac8b0570":"markdown","6e4c84d7":"markdown","a4f5144c":"markdown","e90c1484":"markdown","6d45b940":"markdown","9a4405f1":"markdown","6c6b7d09":"markdown","b1dc3a5b":"markdown","f9529760":"markdown","6393178a":"markdown","2378d393":"markdown","3055efbd":"markdown","bb90949b":"markdown","5f8f5cb3":"markdown","bd2fb36c":"markdown","ca3015a1":"markdown","cf198013":"markdown","b7f00c90":"markdown","4de45abf":"markdown","f60b90d7":"markdown","7703ccf3":"markdown","bb9c5b1d":"markdown","b3c2483e":"markdown","e5d56139":"markdown","f76a1ff0":"markdown","7790f86d":"markdown","fbd76471":"markdown","26d028bb":"markdown","a2c1070a":"markdown","0a65916f":"markdown","577adf47":"markdown","93ee0128":"markdown","c0f336ad":"markdown","d646ec05":"markdown","750f99a0":"markdown","838f2698":"markdown","c0534dac":"markdown","b0892216":"markdown","b210e601":"markdown","3b8c311e":"markdown","dc965c32":"markdown","c5da25c8":"markdown","bc7a7410":"markdown","d3cfd789":"markdown","081ba692":"markdown","bbae1520":"markdown","ada262e6":"markdown","4aca87d3":"markdown","91cbbe65":"markdown","8b08a6c4":"markdown","f12a4ec1":"markdown","f35db302":"markdown","654cab99":"markdown","44641a2c":"markdown","9f55f1b5":"markdown","2d95b7bd":"markdown","41656052":"markdown","358fec7e":"markdown","3a96744d":"markdown","ce62cc86":"markdown","d2d801f2":"markdown","5fdf4a79":"markdown","2fbc5326":"markdown","f92c5681":"markdown","021cbb87":"markdown","84c39d96":"markdown","cbab8962":"markdown","91a1c0ed":"markdown","57d69e11":"markdown","cfa673e1":"markdown","3ca804f3":"markdown","602e0122":"markdown","fc225a20":"markdown","69231eba":"markdown","55987593":"markdown","52992849":"markdown","2f606b43":"markdown","d276757c":"markdown","f5f407bf":"markdown","d23f1a8e":"markdown","54d3dddb":"markdown","55bcc344":"markdown","57e613de":"markdown","6e976a53":"markdown","1c68821e":"markdown","302bdb79":"markdown","fc830177":"markdown","ecee93ad":"markdown","9ebe3715":"markdown","71075d0f":"markdown","c0edcd5b":"markdown","60d5bf20":"markdown","5c7ac6e1":"markdown","f66ac097":"markdown","ecf35af4":"markdown","fbbb08d8":"markdown","a1a112b5":"markdown","7dc0ff1a":"markdown","f52102fa":"markdown","90612b35":"markdown","387bdca6":"markdown","6bbdd67f":"markdown","d558ef06":"markdown","f65b897b":"markdown","6c57f07a":"markdown","a52899eb":"markdown","e77a0a18":"markdown","fad3ef61":"markdown","476fe06a":"markdown","1ce06600":"markdown","40825e31":"markdown"},"source":{"91bdb894":"# data analysis and wrangling\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n# visualization\nimport seaborn as sns\nimport matplotlib. pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\n# machine learning\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance","7b9f037b":"full_data=pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","521e1332":"full_data.head()","b45e4e0d":"all_cols=full_data.columns.values\nprint(all_cols)","9449a66b":"full_data.info()","68984187":"s = (full_data.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)","bc7ab0ca":"x = (full_data.dtypes == ('int64'))\ninteger_cols = list(x[x].index)\nprint(\"integer variables:\")\nprint(integer_cols)","89d41c2b":"x = (full_data.dtypes == ('float64'))\nfloat_cols = list(x[x].index)\nprint(\"float variables:\")\nprint(float_cols)","83771aa5":"print(\"numerical variables:\")\nnumerical_cols=float_cols+integer_cols\nprint(numerical_cols)","aa548930":"missing_prop_column=full_data.isnull().mean()\nmissing_prop_column","f52fa7e8":"#encoding the status column for ease of anlysis\nlabelencoder = LabelEncoder()\nfull_data['placed']=labelencoder.fit_transform(full_data['status'])","46f0557c":"full_data.describe()","44e545a9":"def displot_violinboxplot(col):\n \n col=full_data[col]\n hist_data = [col]\n group_labels = [' Distribution of the variable by displot']\n colors=['red']\n fig1 = ff.create_distplot(hist_data, group_labels,colors=colors,bin_size=[1]) #custom bin_size\n fig1.update_layout(\n    autosize=False,\n    width=800,\n    height=400,)\n \n fig1.show()\n fig2 = go.Figure(data=go.Violin(y=col, box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='lightseagreen', opacity=0.6,\n                               x0='violinboxplot'))\n fig2.update_layout(\n    autosize=False,\n    width=800,\n    height=400,)\n\n fig2.update_layout(yaxis_zeroline=False,title=\"Distribution of the column\")\n fig2.show()","2bd77db1":"#visualization of the feature ssc_p\ndisplot_violinboxplot('ssc_p')","fd9fde27":"#visualization of the feature hsc_p\ndisplot_violinboxplot('hsc_p')","f0274a4c":"#displaying the distribution of degree_p\ndisplot_violinboxplot('degree_p')","8efc81cf":"#displaying the distribution of etest_p\ndisplot_violinboxplot('etest_p')","89236908":"#displaying the distribution of mba_p\ndisplot_violinboxplot('mba_p')","587177f1":"#to get the QQ-plot we have to remove the missing values otherwise there will raise error\n#so we are creating no_missing_salary data frame only for graphical purpose\nno_missing_salary_df=full_data.dropna()","2cead88c":"plt.figure(figsize=(10,5))\nsns.distplot(full_data['salary'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(no_missing_salary_df['salary'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('salary distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(no_missing_salary_df['salary'], plot=plt)\nplt.show()","0a654703":"salary=full_data['salary']\nfig = go.Figure(go.Box(x=salary,name=\"Salary\")) # to get Horizonal plot change axis :  x=germany_score\nfig.update_layout(title=\"Salary Distribution\")\nfig.show()","c2e70b13":"#plotting the distribution curves together\nssc_p=full_data['ssc_p']\nhsc_p=full_data['hsc_p']\ndegree_p=full_data['degree_p']\netest_p=full_data['etest_p']\nmba_p=full_data['mba_p']\t\t\n\nhist_data = [ssc_p,hsc_p,degree_p,etest_p,mba_p] \ngroup_labels = [\"ssc_p Distribution \",\"hsc_p Distribution \",\"degree_p Distribution\",\"etest_p Distribution \",\"mba_p Distribution \"]\ncolors=['blue',\"green\",\"orange\",\"red\",\"black\"]\nfig = ff.create_distplot(hist_data, group_labels,show_hist=False,colors=colors,bin_size=[1,1,1,1,1])\nfig.show()","3d13f14d":"#plotting the box_plots of different marks distributions together\nimport plotly.graph_objects as go\nssc_p=full_data['ssc_p']\nhsc_p=full_data['hsc_p']\ndegree_p=full_data['degree_p']\netest_p=full_data['etest_p']\nmba_p=full_data['mba_p']\t\t\nfig = go.Figure()\nfig.add_trace(go.Box(y=ssc_p,\n                     marker_color=\"blue\",\n                     name=\"ssc_p\"))\nfig.add_trace(go.Box(y=hsc_p,\n                     marker_color=\"green\",\n                     name=\"hsc_p\"))\nfig.add_trace(go.Box(y=degree_p,\n                     marker_color=\"orange\",\n                     name=\"degree_p\"))\nfig.add_trace(go.Box(y=etest_p,\n                     marker_color=\"red\",\n                     name=\"etest_p\"))\nfig.add_trace(go.Box(y=mba_p,\n                     marker_color=\"black\",\n                     name=\"mba_p\"))\nfig.update_layout(title=\"Distribution of different numerical variables\")\nfig.show()","e9cb4499":"full_data.describe(include=['O'])","1639f4b2":"#we are creating a function to display bar chart and pie chart of the differnt categories of a categorical variable.\ndef pie_bar(col):\n  category=full_data[col].value_counts().to_frame().reset_index().rename(columns={'index':col,col:'count'})\n  fig1 = go.Figure(go.Bar(\n    x=category[col],y=category['count'],\n  ))\n  fig1.update_layout(title_text=' Bar chart of different categories of the variable',xaxis_title=\"category\",yaxis_title=\"count\")\n\n\n\n  fig2= go.Figure([go.Pie(labels=category[col], values=category['count'])])\n\n  fig2.update_traces(textposition='inside', textinfo='percent+label')\n\n  fig2.update_layout(title=\"Pie chart of different categories of the variable\",title_x=0.5)\n  print(category)\n  fig1.update_layout(\n    autosize=False,\n    width=800,\n    height=400,)\n  fig2.update_layout(\n    autosize=False,\n    width=800,\n    height=400,)\n \n \n  fig1.show()\n  fig2.show()\n","9ecfc581":"#gender\npie_bar('gender')","1a150547":"#ssc_b\npie_bar('ssc_b')","8f6a757c":"pie_bar('hsc_b')","44d298cf":"pie_bar('hsc_s')","bffbfaeb":"pie_bar('degree_t')","fbaec468":"pie_bar('workex')","c0918a96":"pie_bar('specialisation')","b49701f2":"pie_bar('status')","a5885d8a":"#function to check the relationship between different categorical variables and placement\ndef placement_ratio(col):\n  \n  result=full_data[[col, 'placed']].groupby([col], as_index=False).mean().sort_values(by='placed', ascending=False)\n  return result\n","d3dcd979":"placement_ratio('gender')","eedc5801":"placement_ratio('ssc_b')","3409c7c7":"placement_ratio('hsc_b')","6ef9a107":"placement_ratio('hsc_s')","6cf1b832":"placement_ratio('degree_t')","addb3c2c":"placement_ratio('workex')","0024b794":"placement_ratio('specialisation')","e8cbf42d":"fig, axes = plt.subplots(4, 2,  figsize=(15, 10))\n\nfull_data['frequency'] = 0 # a dummy column to refer to\nfor col, ax in zip(['gender', 'ssc_b', 'hsc_b', 'hsc_s','degree_t','workex','specialisation'], axes.flatten()):\n    counts = full_data.groupby([col, 'placed']).count()\n    freq_per_group = counts.div(counts.groupby(col).transform('sum')).reset_index()\n    sns.barplot(x=col, y='frequency', hue='placed', data=freq_per_group, ax=ax)\nfig.delaxes(axes[3, 1])    ","a7874bda":"#function to create histograms categorized by placement \ndef hist_cont_place(x):\n    %matplotlib inline\n    g = sns.FacetGrid(full_data, col='placed')\n    g.map(plt.hist, x, bins=20)","cc42fa5d":"#function to create box_plots categorized by placement\ndef boxplot_cont_place(x):\n    got_placement=full_data[full_data['placed']==1][x]\n    noplacement=full_data[full_data['placed']==0][x]\n    fig = go.Figure()\n    fig.add_trace(go.Box(y=got_placement,\n                     marker_color=\"blue\",\n                     name=\"placed\"))\n    fig.add_trace(go.Box(y=noplacement,\n                     marker_color=\"red\",\n                     name=\"not placed\"))\n    fig.update_layout(\n    autosize=False,\n    width=800,\n    height=400,)\n    fig.update_layout(title=\"Distribution of percentage according placement\")\n    fig.show()","1bbdf6de":"hist_cont_place('ssc_p')","37e19b96":"boxplot_cont_place('ssc_p')","52d833f9":"hist_cont_place('hsc_p')","42ddb461":"boxplot_cont_place('hsc_p')","f99c50cd":"hist_cont_place('degree_p')","63511075":"boxplot_cont_place('degree_p')","18ac79d2":"hist_cont_place('etest_p')","ace61cf1":"boxplot_cont_place('etest_p')","63c37f38":"hist_cont_place('mba_p')","64d11d35":"boxplot_cont_place('mba_p')","d56412f3":"#function to plot category wise salary histograms of different categorical features.\ndef catcol_vs_salary(col):\n  g= sns.FacetGrid(full_data, col =col, size = 3, aspect = 2)\n  g.map(plt.hist, 'salary', color = 'r'), plt.show()\n  plt.show()  ","83cbd860":"#function to plot category wise salary swarmplots of different categorical features.\ndef swramplot_cat_salary(col):\n  sns.set(style=\"whitegrid\")\n  ax = sns.swarmplot(x=col, y=\"salary\", data=full_data)","96af0ed5":"catcol_vs_salary('gender')","a82419c1":"sns.kdeplot(full_data.salary[ full_data.gender==\"M\"])\nsns.kdeplot(full_data.salary[ full_data.gender==\"F\"])\nplt.legend([\"Male\", \"Female\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","04151ae4":"swramplot_cat_salary('gender')","f9de0b3f":"catcol_vs_salary('ssc_b')","8f42e372":"sns.kdeplot(full_data.salary[ full_data.ssc_b==\"Others\"])\nsns.kdeplot(full_data.salary[ full_data.ssc_b==\"Central\"])\nplt.legend([\"Others\", \"Central\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","35059236":"swramplot_cat_salary('ssc_b')","b44e325a":"catcol_vs_salary('hsc_b')","4eb24a6b":"sns.kdeplot(full_data.salary[ full_data.hsc_b==\"Others\"])\nsns.kdeplot(full_data.salary[ full_data.hsc_b==\"Central\"])\nplt.legend([\"Others\", \"Central\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","5a154463":"swramplot_cat_salary('hsc_b')","4a5e2f86":"catcol_vs_salary('hsc_s')","3d9c03f3":"sns.kdeplot(full_data.salary[ full_data.hsc_s==\"Commerce\"])\nsns.kdeplot(full_data.salary[ full_data.hsc_s==\"Science\"])\nsns.kdeplot(full_data.salary[ full_data.hsc_s==\"Arts\"])\nplt.legend([\"Commerce\", \"Science\",\"Arts\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","5870d441":"swramplot_cat_salary('hsc_s')","586a9bc6":"catcol_vs_salary('degree_t')","53c1de03":"sns.kdeplot(full_data.salary[ full_data.degree_t==\"Sci&Tech\"])\nsns.kdeplot(full_data.salary[ full_data.degree_t==\"Comm&Mgmt\"])\nsns.kdeplot(full_data.salary[ full_data.degree_t==\"Others\"])\nplt.legend([\"Sci&Tech\", \"Comm&Mgmt\",\"Others\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","7be4ca96":"swramplot_cat_salary('degree_t')","afe95573":"catcol_vs_salary('workex')","472ced29":"sns.kdeplot(full_data.salary[ full_data.workex==\"No\"])\nsns.kdeplot(full_data.salary[ full_data.workex==\"Yes\"])\nplt.legend([\"No\", \"Yes\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","5c5bdbf4":"swramplot_cat_salary('workex')","823a1afc":"catcol_vs_salary('specialisation')","234fab37":"sns.kdeplot(full_data.salary[ full_data.specialisation==\"Mkt&HR\"])\nsns.kdeplot(full_data.salary[ full_data.specialisation==\"Mkt&Fin\"])\nplt.legend([\"Mkt&HR\", \"Mkt&Fin\"])\nplt.xlabel(\"Salary (100k)\")\nplt.show()","1b661cfc":"swramplot_cat_salary('specialisation')","cd01d224":"# creating a function plot the hiscontour of continuous variables vs salary\ndef histcontour(x):\n    x = full_data[x]\n    y = full_data['salary']\n\n    import plotly.graph_objects as go\n    fig = go.Figure()\n    fig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n        ))\n    fig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = \"red\", #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\n    fig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = \"blue\", #'rgba(0,0,0,1)'\n        )\n    ))\n    fig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = \"blue\",# 'rgba(0,0,0,1)'\n        )\n    ))\n\n    fig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n       domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n       domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n       domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Density Contour of the variable with salary\",title_x=0.5\n    )\n    fig.show()","3312c518":"# creating a function to plot the lineplot of continuous variables vs salary\ndef lineplot_numeric_vs_salary(col):\n    sns.lineplot(col, \"salary\", data=full_data)\n    plt.figure(figsize=(5,4))\n    plt.show()","082b5e82":"histcontour('ssc_p')","9b90da03":"lineplot_numeric_vs_salary('ssc_p')","0eff2ad5":"histcontour('hsc_p')","6a6f49f0":"lineplot_numeric_vs_salary('hsc_p')","b59cb5a5":"histcontour('degree_p')","3e613e6d":"lineplot_numeric_vs_salary('degree_p')","6185589c":"histcontour('mba_p')","5cbe646d":"lineplot_numeric_vs_salary('mba_p')","eedea877":"histcontour('etest_p')","b01a0e7a":"lineplot_numeric_vs_salary('etest_p')","18cccc09":"full_data=full_data.drop(['status','frequency','sl_no'], axis = 1)","557715fc":"numeric_full_data= full_data.select_dtypes(['number'])\nnumeric_full_data= numeric_full_data.drop(['placed'],axis=1)\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(numeric_full_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","386b4e73":"#converting the categorical variables into numerical variables \nfull_data['gender'] = full_data['gender'].map( {'F': 0, 'M': 1} ).astype(int)\nfull_data['ssc_b'] = full_data['ssc_b'].map( {'Others': 0, 'Central': 1} ).astype(int)\nfull_data['hsc_b'] = full_data['hsc_b'].map( {'Others': 0, 'Central': 1} ).astype(int)\nfull_data['hsc_s'] = full_data['hsc_s'].map( {'Commerce': 0, 'Science': 1,'Arts':2} ).astype(int)\nfull_data['degree_t'] = full_data['degree_t'].map( {'Sci&Tech': 0, 'Comm&Mgmt': 1,'Others':2} ).astype(int)\nfull_data['workex'] = full_data['workex'].map( {'No': 0, 'Yes': 1} ).astype(int)\nfull_data['specialisation'] = full_data['specialisation'].map( {'Mkt&HR': 0, 'Mkt&Fin': 1} ).astype(int)","56816945":"full_data.head()","f994c6cb":"full_data = full_data.drop(full_data[(full_data['salary']>390000)].index)\nno_missing_salary_df =no_missing_salary_df.drop(no_missing_salary_df[(no_missing_salary_df['salary']>390000)].index)\nfull_data = full_data.drop(full_data[(full_data['hsc_p']<45.83)].index)\nfull_data = full_data.drop(full_data[(full_data['hsc_p']>87.6)].index)\nfull_data = full_data.drop(full_data[(full_data['degree_p']>84)].index)\nfull_data = full_data.drop(full_data[(full_data['mba_p']>75.71)].index)\n","7b16ecc6":"skewValue = numeric_full_data.skew(axis=0)\nprint(skewValue)","63eb9f36":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\nfull_data[\"salary\"] = np.log1p(full_data[\"salary\"])","ed5062a1":"plt.figure(figsize=(10,5))\nsns.distplot(full_data['salary'] , fit=norm);\nno_missing_salary_df[\"salary\"] = np.log1p(no_missing_salary_df[\"salary\"])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(no_missing_salary_df['salary'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Salary')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(no_missing_salary_df['salary'], plot=plt)\nplt.show()","f1d3d6a0":"salary_full_data=full_data[full_data['placed']==1]","cadcad4f":"salary_full_data.head()","2351edcd":"X = full_data.iloc[:,0:12]  #independent columns\ny = full_data.iloc[:,-1]    #target column i.e placed\n#apply SelectKBest class to extract top 10 features\nbestfeatures = SelectKBest(score_func=chi2, k=12)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['features','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(12,'Score'))  #print best features","d2ab84bd":"X = full_data[[ 'ssc_p', 'hsc_p',  'degree_p',  'workex','etest_p', 'specialisation']]\ny = full_data['placed']","fc618a53":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","43229f73":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3,random_state=1)","1df8fdc0":"# Support Vector Machines\nsvc = SVC(probability=True)\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\nacc_svc=100*accuracy_score(y_test, Y_pred)\nacc_svc","9e70539d":"print(classification_report(y_test, Y_pred))","cbebf340":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test)\nacc_knn=100*accuracy_score(y_test, Y_pred)\nacc_knn","2165fa6d":"print(classification_report(y_test, Y_pred))","8e73f6fa":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian =100*accuracy_score(y_test, Y_pred)\nacc_gaussian","d3fdeb63":"print(classification_report(y_test, Y_pred))","fc7e4f5b":"#perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron =100*accuracy_score(y_test, Y_pred)\nacc_perceptron","08dfd72b":"print(classification_report(y_test, Y_pred))","c19a4cec":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree=100*accuracy_score(y_test, Y_pred)\nacc_decision_tree","58ba4217":"print(classification_report(y_test, Y_pred))","f4186918":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = 100*accuracy_score(y_test, Y_pred)\nacc_sgd","87736ca7":"print(classification_report(y_test, Y_pred))","cf05c2e0":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest=100*accuracy_score(y_test, Y_pred)\nacc_random_forest","e34d32b4":"print(classification_report(y_test, Y_pred))","be7b03d0":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log=100*accuracy_score(y_test, Y_pred)\nacc_log","eb6de169":"print(classification_report(y_test, Y_pred))","751ea32e":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes','Percetron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian,acc_perceptron, \n              acc_sgd,  acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","db918390":"# Instantiate the classfiers and make a list\nclassifiers = [svc, \n               knn,\n               logreg, \n               random_forest, \n               gaussian,\n               decision_tree]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    yproba = model.predict_proba(X_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","43172d43":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","840b6bb8":"Predictions = pd.DataFrame({\n        \"True Value\": y_test,\n        \"Predicted Value\": Y_pred\n    })\nPredictions.head(10)","cce0a88f":"model = random_forest\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","5ba7d976":"salary_full_data","b464f477":"!pip install BorutaShap","a70a0717":"from BorutaShap import BorutaShap\n\n# no model selected default is Random Forest, if classification is True it is a Classification problem\nFeature_Selector = BorutaShap(importance_measure='shap', classification=False)\n\nFeature_Selector.fit(X=salary_full_data.iloc[:,0:12], y=salary_full_data.iloc[:,-2], n_trials=100, random_state=0)","4c070806":"np.random.seed(0)\nX = salary_full_data.iloc[:,0:12].values\ny = salary_full_data.iloc[:,-2].values\n\nf_test, _ = f_regression(X, y)\nf_test \/= np.max(f_test)\n\nmi = mutual_info_regression(X, y)\nmi \/= np.max(mi)\n\nplt.figure(figsize=(30, 50))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.scatter(X[:, i], y, edgecolor='black', s=20)\n    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\n    if i == 0:\n        plt.ylabel(\"$y$\", fontsize=14)\n    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]),\n              fontsize=16)\nplt.show()","b72f4dd1":"X = salary_full_data[[ 'ssc_p', 'hsc_p',  'degree_p',  'workex','etest_p', 'specialisation','gender','mba_p','degree_t']]\ny = salary_full_data['salary']","fc4b326a":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","a66b882c":"train_X, test_X, train_y,test_y = train_test_split(X_scaled, y, test_size=0.3,random_state=1)","e76fe3ba":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, verbose=False)","c76ec9e1":"predictions = np.expm1(my_model.predict(test_X))\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, np.expm1(test_y))))","831727ba":"my_model.fit(X, y)\nplot_importance(my_model)\nplt.show()","968a752b":"hsc_s","493a7eb0":"Stochastic Gradient Descent","f326e455":"degree_t","2c429773":"There are many classification algorithms and we shall implement important few of them.We shall use\n\n> \n\n1.   KNN or k-Nearest Neighbors\n2.   Support Vector Machines\n4.   Naive Bayes classifier\n5.   Decision Tree\n6.   Random Forrest\n7.   Perceptron\n7.   Logistic Regression\n\nFinally we shall calculate their efficiency.\n\n\n\n","a96344dd":"**descriptive statistics of the numerical columns**","7214f006":"gender and salary","4b0eb064":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.","2f7976e9":"etest_p","d1a2cc31":"**Conclusion:**the distribution of degree_p i.e. degree percentage is also almost like normal curve with appoximate mean=median=mode=66.There is one outlier.","8820e703":"**Numerical Features VS Salary**","b98932ac":"**proportion of missing values**","462feb45":"hsc_b and placement","31966308":"Skewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution.","f878b9d2":"To create the visualization of the numerical columns individually we shall define the following function displot_violinboxplot which will return the histogram,curvature of the distribution,violin plot and the boxplot of that feature.","6aa8ad2e":"Insights:More half of the student's seconadry board was Central board.","71be3c1f":"**Categorical Features VS Placement**","7adb9322":"# PROBLEM DEFINITION\n\nThe data set contains students acadenic records as well as placement and salary information of the 215 students of a business school in India.\n\nLet's take a look at the different variables of the data set.\n\n1.   sl_no : Serial Number\n2.   gender: Gender- Male='M',Female='F'\n\n1.  ssc_p : Secondary Education percentage- 10th Grade \n2.  ssc_b : Board of Education- Central\/ Others\n\n1.   hsc_p:Higher Secondary Education percentage- 12th Grade\n\n1.  hsc_b : Board of Education- Central\/ Others- 12th Grade\n2.  hsc_s : Specialization in Higher Secondary Education\n\n1.   degree_p: Degree Percentage\n2.   degree_t: Under Graduation(Degree type)- Field of degree education\n\n1.   workex : Work Experience\n2.   etest_p: Entrance Test Percentage\n\n1.   mba_p: MBA Percentage\n\n1.   status : Placed or not\n2.   salary : Salary offered\n\nQUESTIONS RELATED TO THE DATA SET\n\n1. Which factor influenced a candidate to get placement?\n2. Does percentage matters for one to get placed?\n3. What factors are influencing salary of a student?\n\n1. What is the relationship among the variables?\n\n5.  Given the features can we predict wheather a student will be placed or not? If placed what will be his\/her salary?\n\nSo,here our two main objectives are\n\n> **CLASSIFICATION:**whether a student will get placement or not.\n\n\n> **REGRESSION:**predicting the salary of a student who is placed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","6746af05":"We shall use the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms and plot them(perceptron and sgd classifiers have been removed due to attribute error of sklearn library).","9f3c752f":"**Conclusion :**It seems there is moderate relationship between different marks percentages and salary.","ca588eac":"Insights:In higher secondary, 39.1% students were from Central board and 60.9% from others.","80495772":"degree_p and placement","a1704216":"Insights:Mkt&Fin has higher salaries than Mkt&HR.We may keep the specialisation variable to predict salary.","12277b35":"Prediction and MAE :Now we shall predict the salry with our model and as an evaluation matric we shall use mean absolute error.In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.It is thus an arithmetic average of the absolute errors.\n\n\n\n\n\n\n\n\n\n\n\n\n","0cda5b67":"**Conclution:**\nSo the categorical variables are gender,ssc_b,hsc_b,hsc_s,degree_t,workex,specalizatin and status.Rest are the numerical variables.Only salary column has missing values almost 31%.Those students who have not got placement,their salaries are missing.","3976b18b":"**Numerical Features VS Placement**","4e95f731":"Splitting the whole data set into two parts train and test so that we can validate our model","db9fa972":"ssc_b","b52e76eb":"From the above graph and AUC score ,we can conclude that GaussianNB,LogisticRegression and Support Vector Machine are performing very well as a classification model.","db2019f8":"etest_p and salary","024ce594":"Insights:Among all students 64.7 % are male .","24cbb2ca":"\nNow we shall continue by checking the relationship between numerical features and placement using visualizations .\n\n","ff35c5f0":"**Check the new salary distribution**","a91e92e3":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.","971fd539":"**Conclusion:**\n1. Placement rate is little bit gender biased.Male students are more likely to be placed than the female students.So we may include gender as a predictor in placement modelling.\n2. Secondary  and Higher Secondary board do not have any impact in the context of placement since,placement rates are almost same for all boards.So, we can remove ssc_b and hsc-b from modelling part(classification task).\n3. In Higher Secondary stream Science and Commerce students are better than the Arts students in getting placement.So we may include hsc_s in modeeling part.\n4. In degree type Comm&Mgmt and Sci&Tech have better placement rate than the others so we may keep degree_t as predictor of placement.\n5. Those who had work expirience had better placement rate.So we shall include workex in predicting placement.\n6. Mkt&Fin has better placement rate.So specialisation will be included in modelling.","32a7a4fd":"Insights:Almost 68% students got placement.","547a9973":"Now we would like to plot differnt marks distributions of the students together and have some insights.","1979c7b3":"ssc_b and salary","07c67ead":"hsc_s and salary","0ceaa914":"Insights:Male students have better salaries.In salary prediction we may keep the gender features.","c772d465":"Next, we would like to analyse the categorical variables individually.","ac8b0570":"Partitioning the dataset into two parts train and test","6e4c84d7":"Since,salary is the regression target variable,we need special attention to it.We shall check how far this distribution is from normal distribution.We shall put the normal distribution curve and salary curve together and also plot the Q-Q plot.In statistics, a Q\u2013Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. First, the set of intervals for the quantiles is chosen. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.In the following Q-Q plot the red straight line representing the normal distribution and the blue dots representing the salary.If they coincide,\nwe can say salary is perfectly normally distributed.","a4f5144c":"**loading the data set**","e90c1484":"# Modelling","6d45b940":"**Correlation Heatmap**","9a4405f1":"**Conclusion:**the distribution of etest_p is quite different from normal curve,it is asymmetric.The mode of the distribution is 60 while the mean and median are 72 and 71 respectively.","6c6b7d09":"degree_p and salary","b1dc3a5b":"We shall again check which features are important for this regression task by F-test and Mutual Information.F-test captures only linear dependency.On the other hand, mutual information can capture any kind of dependency between variables .","f9529760":"At first we shall apply sklearn's SelectKBest class to extract top features for the specific task","6393178a":"According to the F-test and MI score important features for predicting salary are ssc_p,hsc_p,hsc_s,etest_p and mba_p.","2378d393":"**Hi,I am beginner in data science and kaggle.I have tried to create my first notebook in kaggle with campus recruitment data set.I have taken helps from differnt notebooks of kaggle.It will be honour to me if you go through this notebook.All suggestions about my mistakes are always welcome.If you find this notebook good you can appreciate through upvoting this notebook .** ","3055efbd":"mba_p","bb90949b":"specialisation and placement","5f8f5cb3":"# Data pre processing","bd2fb36c":"We may use LogisticRegression to check our classification task for some of  the test data.","ca3015a1":"ssc_b and placement","cf198013":"hsc_p","b7f00c90":"**Categorical Features VS Salary**","4de45abf":"gender and placement","f60b90d7":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.","7703ccf3":"**Different marks distributions together**","bb9c5b1d":"specialisation","b3c2483e":"Now we shall check the relationship between salary and different variables by visualizations.At first, we shall start with categorical variables.\n","e5d56139":"Insights:Among two specialisations in Mkt&Fin has more students(55.8%).","f76a1ff0":"XGBoost has in built feature importance .We would like to plot them.","7790f86d":"Insights:As degree type Comm&Mgmt students have higher salaries.In salary prediction we shall keep the variable degree_t.","fbd76471":"XGboost","26d028bb":"**Conclusion:**ssc_p,hsc_p and degree_p have the most influences in the context of placement.The features etest_p,specialisation and workex have also contributions on placement.","a2c1070a":"Insights:65.6% students have no working experience.","0a65916f":"Here ,as a classification model we have chosen LogisticRegression and would like to see the most important features to determine placement.","577adf47":"We shall install the package BorutaShap to know the important features.","93ee0128":"mba_p and salary","c0f336ad":"**list of numerical variales**","d646ec05":"**Feature Impotance**","750f99a0":"**Conclusion:** For ssc_p,hsc_p,degree_p ,those students who got placement have relatively higher percentages than who did not.So we may include them in our modeling.The feature etest_p has a little influence in the context of placement.We may include this feature too in classification modelling.But mba_p has no effect since mba_p distribution is almost same for placed and not placed students. So,we shall remove it for classification.","838f2698":"**information about the categorical variables**","c0534dac":"etest_p and placement","b0892216":"**Information about the columns**","b210e601":"#                              END","3b8c311e":"workex and placement","dc965c32":"**different columns of the data set**","c5da25c8":"**distribution of thr regression target variable salary**","bc7a7410":"workex and salary","d3cfd789":"**Conclusion :**All the acdemic marks have high impact on salary. Workex gender and specialisation and are also significant to determine salary.","081ba692":"**Skewness of Numerical Features**","bbae1520":"hsc_b and salary","ada262e6":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.","4aca87d3":"gender","91cbbe65":"Insights:Students of Others board has a little bit better salary than the students from Central board.We may not include this feature in modelling part of salary prediction. ","8b08a6c4":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","f12a4ec1":"ssc_p and salary","f35db302":"As we mentioned earlier,for regression task we shall use the data frame salary_ full_data.For our regression task we shall use the XGboost regressor which is an implementation of gradient boosting algorithm.XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems.","654cab99":"**Let's go**","44641a2c":"mba_p and placement","9f55f1b5":"**list of categorical variables**","2d95b7bd":"Comparing the models according to the acuracy .","41656052":"hsc_p and salary","358fec7e":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem.","3a96744d":"Insight:In degree course major portion (67.4%) of students are from Comm&Mgmt.Sci&Tech has 27.4% students.Others stream has very low percentage of students.","ce62cc86":"**Conclusion:**the distribution of mba_p i.e. mba percentage is almost a normal disrtibution with appoximate mean=meadian=mode=62.","d2d801f2":"Converting the categorical variables into numerical variables so that we can use these categorical variables in the model.","5fdf4a79":"**Conclusion:**The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.The variable has few outliers with mean and standard deviation 288655.41 and 93141.18 respectively.","2fbc5326":"Seperating Features and Target:We shall remove the less important features according to the above table and set the predictors and classification target variable.","f92c5681":"degree_p","021cbb87":"**Conclusion:**Secondary,higher secondary and degree percentages of the students are almost similar whereas mba percentage and etest percentage are different.etest percentage has the highest spread and mba percentage has the lowest spread and both are almost symmetrically distributed.Higher secondary percentage has few outliers in both tails.Degree percentage also has a outlier.","84c39d96":"**list of integer variables**","cbab8962":"hsc_s and placement","91a1c0ed":"Setting predicor and target variables","57d69e11":"**Important Features for Classification**","cfa673e1":"We would like to get the data frame where student's whose salaries are more than zeros i.e who got placement.This data frame will be used in salary prediction.Because it will be meaningless to predict the salary of the students who have not got placement.","3ca804f3":"**Correlation**","602e0122":"We would like to see the pearson's correlation coefficients between the numerical features.In statistics, the Pearson correlation coefficient, also referred to as Pearson's r, the Pearson product-moment correlation coefficient (PPMCC), or the bivariate correlation, is a statistic that measures linear correlation between two variables X and Y. It has a value between +1 and \u22121. A value of +1 is total positive linear correlation, 0 is no linear correlation, and \u22121 is total negative linear correlation.Actually we shall see the linear relationships between the numerical features and the salary.So,we shall drop the column placed and then create a correlation heatmap.","fc225a20":"hsc_b","69231eba":" Feature Scaling :\n\n*   Percentages are on scale 0-100\n*   Categorical Features are on range 0-1 (By one hot encoding)\n\n*   High Scale for Salary -> Salary is heavily skewed too -> SkLearn has RobustScaler which might work well here\n\nScaling Everything between 0 and 1 (This wont affect encoded values)\n\n\n\n\n","55987593":"So all academic marks ,gender and work experience have impact on salary.","52992849":"Scaling","2f606b43":"Now we would like to see the relationships between the categorical variables and placement.At first we shall see them in tabular form and then visually and this may help in determing the important features for the classification problem.","d276757c":"Check whether all values have turned into numerical or not","f5f407bf":"workex","d23f1a8e":"# Classification Task","54d3dddb":"Insights:In higher secondary, number of students of Arts stream is significantly low.Commerce and Science  have 52.6% and 42.3% students respectively.","55bcc344":"specialisation and salary","57e613de":"**Outliers :**When modeling, it is important to clean the data sample to ensure that the observations best represent the problem.\n\nSometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers and often machine learning modeling and model skill in general can be improved by understanding and even removing these outlier values.","6e976a53":"Deleting outliers  :  from different box plot graphs we can detect the outliers manually and remove them.","1c68821e":"ssc_p and placement","302bdb79":"degree_t and placement","fc830177":"Insights: Students with work experience have better salary than who have not.So, we would like to include this feature in salary prediction.","ecee93ad":"hsc_p and placement","9ebe3715":"**Conclusion:** etest_p and mba_p has a very little correlation with salary.Different marks percentages are moderately correlated among themselves.","71075d0f":"Insights:In the context of salary different secondary boards do not differ to much.We may discard this feature in salary prediction.","c0edcd5b":"Our dataframe ","60d5bf20":"# BASIC EDA AND VISUALIZATION","5c7ac6e1":"If some plotly visualizations are not displayed,reloding the page might fix it. ","f66ac097":"We are perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset.","ecf35af4":"**Thank you for reading this notebook**","fbbb08d8":"At first we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.","a1a112b5":"Now for modelling purpose we do not need status, frequency and sl_no any more .So we shall remove them.","7dc0ff1a":"**Conclusion:** the distribution of ssc_p i.e the marks percentage of secondary exam is almost symetric.The mean and the median is 67 but the mode is 63","f52102fa":"# Regression Task\n","90612b35":"degree_t and salary","387bdca6":"ssc_p","6bbdd67f":"**list of float variables**","d558ef06":"We can see that naive bayes and random forest are the top most model according to accuracy measure.","f65b897b":"So,according to BorutaShap,etest_p, degree_p and mba_p are the most impotant features to determine salary.","6c57f07a":"**importing libraries**","a52899eb":"Insights:As higher secondary stream Commerce and Science students have better salaries than the Arts students.For such difference, may include this feature in salary prediction.","e77a0a18":"\nRemoving skewness from data will improve the modelling quality.The above chart shows that only the regression target variable salary  is highly positively skewed(we also saw this from the normal curve and Q-Q plot) .So,we have to transfrom it towards normal curve. We shall use the numpy fuction log1p which  applies log(1+x) to all elements of the column salary.","fad3ef61":"Now we shall see the relationship between salary and the continuous features through visualization.","476fe06a":"status","1ce06600":"**displaying first few rows of the data set**","40825e31":"**Conclusion:**the distribution of hsc_p i.e. higher secondary percentage is also almost symetric with mean=66,median=65,mode=63.this feature has some outliers. As the mean>median>mode ,the distribution is little bit positively skewd."}}