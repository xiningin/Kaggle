{"cell_type":{"f0742eb7":"code","2d800a9a":"code","780b2176":"code","33fc502e":"code","96c83cbd":"code","3714febc":"code","649c2e5d":"code","21339f8f":"code","4ef40200":"code","9ad4dd06":"code","53107821":"code","0ee9cb02":"code","0041e392":"code","8d1a18c7":"code","e861b147":"code","4383585a":"code","f48995af":"code","98b67713":"code","69ad2a3c":"code","064544e7":"code","4c4c52d3":"code","739ed0a9":"code","5e8d1c49":"code","a4fb6ac6":"code","22534f76":"code","3168ccf5":"code","25f23e2a":"code","683c5c35":"code","709298c2":"code","ea155bc4":"markdown","390028a2":"markdown","0a212564":"markdown","3b744c83":"markdown","6533ced7":"markdown","a8b60f0e":"markdown","5ce4d5f1":"markdown","dc1e1942":"markdown","aaf02a81":"markdown","32842222":"markdown","a4b3abfd":"markdown","8a3e3d45":"markdown","9c87c752":"markdown","8efc18db":"markdown","d38801b7":"markdown","1c86d3ea":"markdown","f624a50d":"markdown","778332ee":"markdown","32f54d8b":"markdown","779bda17":"markdown","e09b2eef":"markdown","13548956":"markdown","a0276801":"markdown","815ab0f2":"markdown","9ae84a19":"markdown","0504e171":"markdown"},"source":{"f0742eb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom statsmodels.tsa.stattools import acf, pacf, adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d800a9a":"pred_temps = pd.read_csv('\/kaggle\/input\/predicted-global-temps-through-2040\/PredTemps2040.csv', index_col=0)\ntemps = pd.read_csv('\/kaggle\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv', index_col=0)\n\npred_temps.index = pd.to_datetime(pred_temps.index)\ntemps.index = pd.to_datetime(temps.index)\n\ntemps = temps[['LandAndOceanAverageTemperature']]\ntemps['LOMA_Temp'] = temps.rolling(12).mean()\ntemps.drop('LandAndOceanAverageTemperature', axis=1, inplace=True)\n\nexog_temps = temps.loc['1880-01-01':'2013-12-01']\n\ntemps.rename({'LOMA_Temp':'Mean_Predicted'}, axis=1, inplace=True)\n\ntemps['Upper_Mean'] = np.nan\ntemps['Lower_Mean'] = np.nan\ntemps['Highest_Bound'] = np.nan\ntemps['Lowest_Bound'] = np.nan\ntemps['SO_Mean'] = np.nan\n\npred_temps = pd.concat([temps.loc['2014-01-01':'2015-12-01'], pred_temps])\n\npred_temps['Upper_Mean'].fillna(pred_temps['Mean_Predicted'], inplace=True)\npred_temps['Lower_Mean'].fillna(pred_temps['Mean_Predicted'], inplace=True)\npred_temps['Highest_Bound'].fillna(pred_temps['Mean_Predicted'], inplace=True)\npred_temps['Lowest_Bound'].fillna(pred_temps['Mean_Predicted'], inplace=True)\npred_temps['SO_Mean'].fillna(pred_temps['Mean_Predicted'], inplace=True)","780b2176":"sea = pd.read_csv('\/kaggle\/input\/sea-level-change\/sea_levels_2015.csv')\nsea['Time'] = pd.to_datetime(sea['Time'])\nsea.set_index('Time', inplace=True)\nsea = sea.resample(rule='MS').mean()\nsea['GMSL_Rolling'] = sea['GMSL'].rolling(12).mean()\nsea.dropna(inplace=True)\n\nfig = px.line(sea, x=sea.index, y='GMSL_Rolling', title='Sea Level Height 1880-2013', labels={'Time':'Date', 'GMSL':'Height (mm)'})\nfig.show()","33fc502e":"decomp_sea = seasonal_decompose(sea['GMSL_Rolling'], period=12)\n\nfig, ax = plt.subplots(3,1, figsize=(20, 8))\nax[0].plot(decomp_sea.seasonal)\nax[1].plot(decomp_sea.trend)\nax[2].plot(decomp_sea.resid)\nax[0].set_title('Seasonality')\nax[1].set_title('Trend')\nax[2].set_title('Residual')\nplt.show()","96c83cbd":"#diff1 = sea.diff().diff(12).dropna()\n#sdiff = sea.diff(12).dropna()\ndiff = sea.diff().dropna()\nfig, ax = plt.subplots(1, 1, figsize=(20,3))\n#ax[0].plot(diff1.GMSL_Rolling)\n#ax[1].plot(sdiff.GMSL_Rolling)\nax.plot(diff.GMSL_Rolling)\n\nplt.show()\n\n\n#result = adfuller(diff1.GMSL_Rolling)\n#result2 = adfuller(sdiff.GMSL_Rolling)\nresult3 = adfuller(diff.GMSL_Rolling)\n#adtest = pd.Series(result[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Observations'])\n#adtest2 = pd.Series(result2[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Observations'])\nadtest3 = pd.Series(result3[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Observations'])\n\n#print(adtest)\n#print(adtest2)\nprint(adtest3)","3714febc":"fig, ax = plt.subplots(1,2, figsize=(20,5))\n\nplot_acf(diff.GMSL_Rolling, lags=11, zero=False, ax=ax[0], title='Autocorrelation')\nplot_pacf(diff.GMSL_Rolling, lags=11, zero=False, ax=ax[1], title='Partial Autocorrelation')\n\nplt.show()\n","649c2e5d":"#(3,1,2) 199 aic, no seasonal component\norder = (3,1,2)\n\nsarima = SARIMAX(sea.GMSL_Rolling, order=order, exog=exog_temps.loc['1880-12-01':], trend='c')\n\nsea_model=sarima.fit()\n\nsea_model.summary()\n","21339f8f":"forecast = sea_model.get_prediction(start=-240, exog=exog_temps)\nfig1 = px.line(sea, x=sea.index, y=sea.GMSL_Rolling)\nfig1.add_scatter(x=forecast.predicted_mean.index, y=forecast.predicted_mean, mode='lines', name='Predicted')\nfig1.add_scatter(x=forecast.predicted_mean.index, y=forecast.conf_int().iloc[:,0], name='Lower Bound')\nfig1.add_scatter(x=forecast.predicted_mean.index, y=forecast.conf_int().iloc[:,1], fill='tonexty', name='Upper Bound')\nfig1.show()\n\ndef mape(fc, true):\n    mape = np.mean((np.abs(fc-true)\/np.abs(true)))\n    return mape\n\nacc = mape(forecast.predicted_mean, sea['GMSL_Rolling'].iloc[-240:])\nprint('Mean Absolute Percent Error:', acc)\n\nsea_model.plot_diagnostics(figsize=(20, 10))\nplt.show()","4ef40200":"fc_m = sea_model.get_forecast(steps=327, exog=pred_temps['Mean_Predicted'])\n\nprint('March 2041 GMSL Prediction:', round(fc_m.predicted_mean[-1], 3), 'mm')\n\nfig = px.line(sea, x=sea.index, y=sea.GMSL_Rolling)\nfig.add_scatter(x=fc_m.predicted_mean.index, y=fc_m.predicted_mean, mode='lines', name='Predicted')\nfig.add_scatter(x=fc_m.predicted_mean.index, y=fc_m.conf_int().iloc[:,0], name='Lower Bound')\nfig.add_scatter(x=fc_m.predicted_mean.index, y=fc_m.conf_int().iloc[:,1], fill='tonexty', name='Upper Bound')\nfig.show()","9ad4dd06":"fc_h = sea_model.get_forecast(steps=327, exog=pred_temps['Highest_Bound'])\n\nprint('March 2041 GMSL Prediction:', round(fc_h.predicted_mean[-1], 3), 'mm')\n\nfig = px.line(sea, x=sea.index, y=sea.GMSL_Rolling)\nfig.add_scatter(x=fc_h.predicted_mean.index, y=fc_h.predicted_mean, mode='lines', name='Predicted')\nfig.add_scatter(x=fc_h.predicted_mean.index, y=fc_h.conf_int().iloc[:,0], name='Lower Bound')\nfig.add_scatter(x=fc_h.predicted_mean.index, y=fc_h.conf_int().iloc[:,1], fill='tonexty', name='Upper Bound')\nfig.show()","53107821":"fc_l = sea_model.get_forecast(steps=327, exog=pred_temps['Lowest_Bound'])\n\nprint('March 2041 GMSL Prediction:', round(fc_l.predicted_mean[-1], 3), 'mm')\n\nfig = px.line(sea, x=sea.index, y=sea.GMSL_Rolling)\nfig.add_scatter(x=fc_l.predicted_mean.index, y=fc_l.predicted_mean, mode='lines', name='Predicted')\nfig.add_scatter(x=fc_l.predicted_mean.index, y=fc_l.conf_int().iloc[:,0], name='Lower Bound')\nfig.add_scatter(x=fc_l.predicted_mean.index, y=fc_l.conf_int().iloc[:,1], fill='tonexty', name='Upper Bound')\nfig.show()\n","0ee9cb02":"dis_dam = pd.read_csv('\/kaggle\/input\/natural-disaster-data\/economic-damage-from-natural-disasters.csv')\ndis_ct = pd.read_csv('\/kaggle\/input\/natural-disaster-data\/number-of-natural-disaster-events.csv')\ndeaths = pd.read_csv('\/kaggle\/input\/global-cause-of-the-deaths-other-than-diseases\/Caused of Deaths.csv')\n\ndis_dam.drop('Code', axis=1, inplace=True)\ndis_dam.rename(mapper={'Total economic damage from natural disasters (US$)':'Damage Cost'}, axis=1, inplace=True)\ndis_ct.drop('Code', axis=1, inplace=True)\ndis_ct.rename({'Number of reported natural disasters (reported disasters)':'Disasters'}, axis=1, inplace=True)","0041e392":"dis_types = dis_dam['Entity'].unique()\nall_dis = dis_dam[dis_dam['Entity']=='All natural disasters']\nbrk_dis = dis_dam[dis_dam['Entity']!='All natural disasters']\n\nbrk_dis_ct = dis_ct[dis_ct['Entity']!='All natural disasters']\nfig1=px.bar(brk_dis_ct, x='Year', y='Disasters', color='Entity', title='Global Natural Disaster Count by Type 1900-2018', hover_name='Entity', labels={'Disasters':'Disaster Count'})\nfig1.show()\n\nall_dc = dis_ct[dis_ct['Entity']=='All natural disasters']\nall_dc['Pct_Chnge'] = all_dc['Disasters'].pct_change()\nprint('Mean Percent Change Per Year:', round(all_dc['Pct_Chnge'].mean(), 4))\n\nfig2=px.bar(brk_dis, x='Year', y='Damage Cost', color='Entity', title='Global Disaster Costs by Type 1900-2018', labels={'Damage Cost':'Damage Cost (2018 USD)'})\nfig2.show()\n\nnd_deaths = deaths[deaths['Cause']=='Natural Disaster']\n\ndby = pd.DataFrame(nd_deaths.groupby('Year')['Deaths'].sum())\n\nfig3=px.bar(dby, x=dby.index, y='Deaths', title='Natural Disaster Deaths 1980-2017')\nfig3.show()","8d1a18c7":"us_dis = pd.read_csv('\/kaggle\/input\/us-natural-disaster-declarations\/us_disasters_m5.csv')\nus_dis_dec = pd.read_csv('\/kaggle\/input\/us-natural-disaster-declarations\/us_disaster_declarations.csv')\n","e861b147":"#23 disaster types, 17 natural disasters\n#All Biological disasters are Covid 19\nnat_dis = list(us_dis_dec['incident_type'].unique())\nnat_dis.remove('Human Cause')\nnat_dis.remove('Terrorist')\nnat_dis.remove('Biological')\nnd = us_dis_dec[us_dis_dec['incident_type'].isin(nat_dis)]\nnd.drop_duplicates(subset='declaration_request_number', inplace=True)","4383585a":"nde = nd[nd['fy_declared']<=1987]\nndl = nd[nd['fy_declared']>1987]\nearly_ct = nde.groupby('state')['declaration_title'].count()\nlate_ct = ndl.groupby('state')['declaration_title'].count()\n\nedf = pd.DataFrame({'1953-1987 Disaster Count':early_ct})\nldf = pd.DataFrame({'1988-2021 Disaster Count':late_ct})\n\nst_df = edf.merge(ldf, how='left', left_on=edf.index, right_on=ldf.index)\nst_df['Disaster Change'] = st_df['1988-2021 Disaster Count']-st_df['1953-1987 Disaster Count']\nst_df.rename({'key_0':'State'}, axis=1, inplace=True)\n\nfig1 = px.bar(st_df, x=st_df['State'], y='Disaster Change', title='Comparing State Disaster Counts from 1953-1987 to 1988-2021')\nfig1.show()\n\ne_ct = nde.groupby('incident_type')['incident_type'].count()\nl_ct = ndl.groupby('incident_type')['incident_type'].count()\n\ne_df = pd.DataFrame({'1953-1987 Disaster Count':e_ct})\nl_df = pd.DataFrame({'1988-2021 Disaster Count':l_ct})\n\ntype_df = e_df.merge(l_df, how='left', left_on=e_df.index, right_on=l_df.index)\ntype_df['Disaster Change'] = type_df['1988-2021 Disaster Count']-type_df['1953-1987 Disaster Count']\ntype_df.rename({'key_0':'Disaster Type'},axis=1, inplace=True)\n\nfig2=px.bar(type_df, x='Disaster Type', y='Disaster Change', title='Comparing Disaster Counts from 1953-1987 to 1988-2021 by Type')\nfig2.show()\n\nyr_ct = nd.groupby('fy_declared')['state'].count() \nydf = pd.DataFrame({'Disaster Count':yr_ct})\n\nfig3=px.line(ydf[:-1], x=ydf[:-1].index, y='Disaster Count', labels={'x':'Year'}, title='Yearly US Disaster Declarations')\nfig3.show()\n\nprint('Cumulative Disaster Count 1953-1995:', ydf.loc[:1995]['Disaster Count'].sum())\nprint('Cumulative Disaster Count 1996-2020:', ydf.loc[1996:]['Disaster Count'].sum())","f48995af":"txfires = nd[(nd['state']=='TX')&(nd['designated_area']=='Statewide')&(nd['incident_type']=='Fire')]\ntxfires_new = txfires.drop_duplicates(subset='declaration_date')\n\nnd = nd[~nd.isin(txfires)].dropna()\nnd = pd.concat([nd, txfires_new])\nnd['fips'] = nd['fips'].astype({'fips':'int'})\n\n","98b67713":"cnt_grp = nd[nd['designated_area']!='Statewide']\nst_grp = nd[nd['designated_area']=='Statewide']\ncnties = cnt_grp.groupby(['fips', 'designated_area']).count().sort_values('state', ascending=False) #'fy_declared',\nsts = st_grp.groupby(['fips']).count().sort_values('state', ascending=False) #'fy_declared',\n\ncdf = pd.DataFrame({'Disaster Count':cnties['state']})\ncdf.reset_index(inplace=True)\ncdf['fips'] = cdf['fips'].astype({'fips':'string'})\n#cdf['fy_declared'] = cdf['fy_declared'].astype({'fy_declared':'int'})\n\nsdf = pd.DataFrame({'Disaster Count':sts['state']})                  \nsdf.reset_index(inplace=True)\nsdf['fips'] = sdf['fips'].astype({'fips':'string'})\n#sdf['fy_declared'] = sdf['fy_declared'].astype({'fy_declared':'int'})\n","69ad2a3c":"def concat(s1):\n    if len(s1) == 4:\n        s1 = '0'+s1\n        \n    return s1\n\ncdf['fips'] = cdf['fips'].apply(concat)\nsdf['fips'] = sdf['fips'].apply(concat)\n\ncdf['state'] = cdf['fips'].str[:2]\nsdf['state'] = sdf['fips'].str[:2]\n","064544e7":"codes = pd.read_csv('\/kaggle\/input\/zipcodes-county-fips-crosswalk\/ZIP-COUNTY-FIPS_2017-06.csv')\n\nfips = codes.groupby(['STCOUNTYFP', 'COUNTYNAME']).first()\n\nfips.reset_index(inplace=True)\n\nfdf = pd.DataFrame({'Fips':fips['STCOUNTYFP'], 'County':fips['COUNTYNAME']})\nfdf['Fips'] = fdf['Fips'].astype({'Fips':'string'})\nfdf['Fips'] = fdf['Fips'].apply(concat)\n\nfdf = fdf.merge(cdf, how='left', left_on='Fips', right_on='fips')\n\nfdf['Disaster Count'].fillna(0, inplace=True)\nfdf.drop(['designated_area', 'fips'], axis=1, inplace=True)\nfdf['state'].fillna(fdf['Fips'].str[:2], inplace=True)\n\nddf = fdf.merge(sdf, how='left', on=['state'])\n\nddf['Total_Count'] = ddf['Disaster Count_x']+ddf['Disaster Count_y']\n#ddf.drop(['Disaster Count_x', 'state', 'fips','Disaster Count_y'], axis=1, inplace=True)","4c4c52d3":"from urllib.request import urlopen\nimport json\nwith urlopen('https:\/\/raw.githubusercontent.com\/plotly\/datasets\/master\/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n","739ed0a9":"fig = px.choropleth(ddf, geojson=counties, locations='Fips', color='Total_Count',\n                           color_continuous_scale=\"reds\",\n                           range_color=(0, 70),\n                           scope=\"usa\",\n                           labels={'Total_Count':'Natural Disaster Count'}, \n                           hover_name='County', title='Natural Disaster Declarations By County Since 1953')\nfig.show()","5e8d1c49":"fl_cnt_grp = cnt_grp[cnt_grp['incident_type']=='Flood']\nfl_st_grp = st_grp[st_grp['incident_type']=='Flood']\n\nfl_cnties = fl_cnt_grp.groupby(['fips', 'designated_area']).count().sort_values('state', ascending=False)\nfl_sts = fl_st_grp.groupby('fips').count().sort_values('state', ascending=False)\n\nfl_cdf = pd.DataFrame({'Flood Count':fl_cnties['state']})\nfl_cdf.reset_index(inplace=True)\nfl_cdf['fips'] = fl_cdf['fips'].astype({'fips':'string'})\n\nfl_sdf = pd.DataFrame({'Flood Count':fl_sts['state']})                  \nfl_sdf.reset_index(inplace=True)\nfl_sdf['fips'] = fl_sdf['fips'].astype({'fips':'string'})\n\nfl_cdf['fips'] = fl_cdf['fips'].apply(concat)\nfl_sdf['fips'] = fl_sdf['fips'].apply(concat)\n\nfl_cdf['state'] = fl_cdf['fips'].str[:2]\nfl_sdf['state'] = fl_sdf['fips'].str[:2]\n\nfl_fdf = pd.DataFrame({'Fips':fips['STCOUNTYFP'], 'County':fips['COUNTYNAME']})\nfl_fdf['Fips'] = fl_fdf['Fips'].astype({'Fips':'string'})\nfl_fdf['Fips'] = fl_fdf['Fips'].apply(concat)\n\nfl_fdf = fl_fdf.merge(fl_cdf, how='left', left_on='Fips', right_on='fips')\n\nfl_fdf['Flood Count'].fillna(0, inplace=True)\nfl_fdf.drop(['designated_area', 'fips'], axis=1, inplace=True)\nfl_fdf['state'].fillna(fl_fdf['Fips'].str[:2], inplace=True)\n\nfor i in fl_fdf['state'].unique():\n    if i not in fl_sdf['state'].unique():\n        row = {'fips':0, 'Flood Count':0, 'state':str(i)}\n        rdf = pd.DataFrame(row, index=[0])\n        \n        fl_sdf = pd.concat([fl_sdf, rdf])\n\nfl_ddf = fl_fdf.merge(fl_sdf, how='left', on='state')\n\nfl_ddf['Total_Count'] = fl_ddf['Flood Count_x']+fl_ddf['Flood Count_y']\nfl_ddf.drop(['Flood Count_x', 'state', 'fips','Flood Count_y'], axis=1, inplace=True)\n","a4fb6ac6":"fig = px.choropleth(fl_ddf, geojson=counties, locations='Fips', color='Total_Count',\n                           color_continuous_scale=\"blues\",\n                           range_color=(0, 10),\n                           scope=\"usa\",\n                           labels={'Total_Count':'Flood Count'}, \n                           hover_name='County', title='Flood Disaster Declarations By County Since 1953')\nfig.show()","22534f76":"st_cnt_grp = cnt_grp[(cnt_grp['incident_type']=='Hurricane')|(cnt_grp['incident_type']=='Severe Storm(s)')|(cnt_grp['incident_type']=='Coastal Storm')|(cnt_grp['incident_type']=='Typhoon')] #\nst_st_grp = st_grp[(st_grp['incident_type']=='Hurricane')|(st_grp['incident_type']=='Severe Storm(s)')|(st_grp['incident_type']=='Coastal Storm')|(st_grp['incident_type']=='Typhoon')]\n\nst_cnties = st_cnt_grp.groupby(['fips', 'designated_area']).count().sort_values('state', ascending=False)\nst_sts = st_st_grp.groupby('fips').count().sort_values('state', ascending=False)\n\nst_cdf = pd.DataFrame({'Storm Count':st_cnties['state']})\nst_cdf.reset_index(inplace=True)\nst_cdf['fips'] = st_cdf['fips'].astype({'fips':'string'})\n\nst_sdf = pd.DataFrame({'Storm Count':st_sts['state']})                  \nst_sdf.reset_index(inplace=True)\nst_sdf['fips'] = st_sdf['fips'].astype({'fips':'string'})\n\nst_cdf['fips'] = st_cdf['fips'].apply(concat)\nst_sdf['fips'] = st_sdf['fips'].apply(concat)\n\nst_cdf['state'] = st_cdf['fips'].str[:2]\nst_sdf['state'] = st_sdf['fips'].str[:2]\n\nst_fdf = pd.DataFrame({'Fips':fips['STCOUNTYFP'], 'County':fips['COUNTYNAME']})\nst_fdf['Fips'] = st_fdf['Fips'].astype({'Fips':'string'})\nst_fdf['Fips'] = st_fdf['Fips'].apply(concat)\n\nst_fdf = st_fdf.merge(st_cdf, how='left', left_on='Fips', right_on='fips')\n\nst_fdf['Storm Count'].fillna(0, inplace=True)\nst_fdf.drop(['designated_area', 'fips'], axis=1, inplace=True)\nst_fdf['state'].fillna(st_fdf['Fips'].str[:2], inplace=True)\n\nfor i in st_cdf['state'].unique():\n    if i not in st_sdf['state'].unique():\n        row = {'fips':0, 'Storm Count':0, 'state':str(i)}\n        rdf = pd.DataFrame(row, index=[0])\n        \n        st_sdf = pd.concat([st_sdf, rdf])\n\nst_ddf = st_fdf.merge(st_sdf, how='left', on='state')\n\nst_ddf['Total_Count'] = st_ddf['Storm Count_x']+st_ddf['Storm Count_y']\nst_ddf.drop(['Storm Count_x', 'state', 'fips','Storm Count_y'], axis=1, inplace=True)","3168ccf5":"fig = px.choropleth(st_ddf, geojson=counties, locations='Fips', color='Total_Count',\n                           color_continuous_scale=\"greens\",\n                           range_color=(0, 10),\n                           scope=\"usa\",\n                           labels={'Total_Count':'Storm Count'}, \n                           hover_name='County', title='Storm Disaster Declarations By County Since 1953')\nfig.show()","25f23e2a":"bdd = pd.read_csv('\/kaggle\/input\/billion-dollar-us-natural-disasters-19802021\/BillionDollarDisasters1980-2021.csv')\n\nbdd.iloc[178,2] = '2012-01-01'\nbdd['Begin Date'] = pd.to_datetime(bdd['Begin Date'])\nbdd['End Date'] = pd.to_datetime(bdd['End Date'])\nbdd.set_index('Begin Date', inplace=True)\nbdd['Year'] = pd.DatetimeIndex(bdd.index).year\nbdd.rename(mapper={'Total CPI-Adjusted Cost (Millions of Dollars)':'Cost_Mil'}, axis=1, inplace=True)\nbdd = bdd.loc[:'2020-12-31']\nbdd['Cost_Mil'] = bdd['Cost_Mil'].astype('float')","683c5c35":"yr_dmg = bdd.groupby('Year')['Cost_Mil'].sum()\nyr_cnt = bdd.groupby('Year')['Cost_Mil'].count()\n\ndddf = pd.DataFrame({'Cost (Mil)':yr_dmg})\ncddf = pd.DataFrame({'Count':yr_cnt})\n\nfig2 = px.line(cddf, x=cddf.index, y=cddf['Count'], title='Number of Billion Dollar US Natural Disasters', labels={'Count':'Number of Disasters'})\nfig2.show()\n\nfig = px.line(dddf, x=dddf.index, y=dddf['Cost (Mil)'], title='Total Cost of Billion Dollar Natural Disasters', labels={'Cost (Mil)':'Cost (In Millions of 2020 USD)'})\nfig.show()","709298c2":"bdd.sort_values('Cost_Mil', ascending=False).head(10)","ea155bc4":"First I load the predicted temperatures from my previous notebook to use as exogenous variables. ","390028a2":"Here is my final prediction using the lowest bound values of predicted temperatures as the exogenous variable. Again, it made very little difference in the prediction compared to the previous two. Since both the highest and lowest bound temperature values made little difference, I didn't run any predictions with values in between since those would've had an even more negligible effect.","0a212564":"Another well documented effect of increasing temperatures is an increased frequency and severity of natural disasters. Let's take a look at the frequency and severity of natural disasters over time and see what's changed.","3b744c83":"Some of the disaster counts were drowned out by the signifcant amount of wildfires in the previous graph, so I broke it down into two other map plots of the next most common disasters, floods and hurricanes\/severe storms. Here is a graph of floods by county. The Pacific Coast, states along the Mississippi River, and West Virginia have the highest susceptibility to flooding. This is a problem that will only get worse in Florida, and along the Atlantic Coast as sea levels continue to rise.","6533ced7":"Here is my second prediction using the highest bound values for the predicted temperatures as the exogenous variable. This resulted in a very small increase to Global Mean Sea Level in the given timescale. After doing further research[1], it seems that looking at a longer timescale (in the range of 2050-2100) would show more significant sea level changes in response to different temperatures. This suggests that this is a longer term issue, and while this predicted level of sea rise is inevitable there is still time to prevent even more disastrous levels of sea rise. \n\n[1] Lindsey, Rebecca. \u201cClimate Change: Global Sea Level.\u201d Climate.gov, NOAA, 25 Jan. 2021, www.climate.gov\/news-features\/understanding-climate\/climate-change-global-sea-level. ","a8b60f0e":"# Sea Level Forecast","5ce4d5f1":"# Part III\n\nFor the final part of my climate analysis to see the impact of increasing renewable energy, check out [Part III](https:\/\/www.kaggle.com\/jedbell\/climate-analysis-iii-what-now)","dc1e1942":"Here is a graph of the twelve month rolling average of the Global Mean Sea Level height. There is a very obvious upward trend and there may be some seasonality so let's check.","aaf02a81":"The MAPE is below 1% and the diagnostics look fine although they could be a little more normally distributed. I think this model will be fine for forecasting.","32842222":"For the last part of this analysis we will look at the financial impact natural disasters have had on the US.","a4b3abfd":"There seems to be a negligible amount of seasonality in this time series, so I will use the ARIMA model without any seasonal components.","8a3e3d45":"One well known impact of climate change is rising sea levels due to melting glaciers and expaning water volume (due to warmer water). But how much is is expected to increase in the next 20 years? Let's take a look and do some forecasting.","9c87c752":"Here I filtered out diseases, terrorist attacks, and human accidents from the dataframe of US Disaster Declarations since 1953.","8efc18db":"# Natural Disaster Analysis","d38801b7":"The PACF here looks a bit unusual. These plots seem to suggest that the model should only have AR terms, but after some experimentation with the model that didn't produce the best results. ","1c86d3ea":"The number of natural disasters each year nearly follows a Poisson Distribution, although the variance and mean are not equal so it's not a true Poisson Distribution. This makes forecasting harder than other types of data like temperature, CO2, or sea levels. However, the trend is still clear. Billion dollar disasters are becoming more frequent, and 7 of the 10 most expensive ever have occurred since 2005. Some areas are more susceptible than others, especially the Pacific, Gulf, and East coasts. Rising temperatures are not only wreaking havoc on the environment, but also our collective wallets, and the recent data suggests this problem is about to only get worse.","f624a50d":"Now let's narrow our scope a bit and take a look at the United States specifically. How has this increase in natural disasters affected it and its economy?","778332ee":"The top graph shows the number of global natural disasters each year from 1900 to 2018. Clearly, the number of natural disasters per year has exploded since the 1960s, particularly due to increases in flooding and extreme weather (hurricanes and cyclones). In fact, since 1900, the number of natural disasters has increased by an average of 16.5% per year.\n\nThe middle graph shows the economic damage caused by natural disasters each year. This also shows a strong upward trend, but it's not as consistent since not all natural disasters are created equal, and some do far more damage than others depending on their size and where they hit. Still, an increase in disaster frequency means there will be more opportunities for these more devastating disasters to strike. Also, a greater frequency of disasters means that even years without one or a few particularly devastating disasters will be more expensive than previous years without these more devastating disasters.\n\nThe final graph shows total deaths due to natural disasters. This graph doesn't have the same kind of upward trend as the previous two, but it does have increasingly frequent spikes. This is for similar reasons as the graph of costs, as some disasters are much more deadly than other depending on location and severity, and again the increased frequency of disasters will only make spikes like the ones in 2004, 2008 and 2010 more frequent. \n\nNote: On the bright side, those three spikes were mainly caused by earthquakes (and an ensuing tsunami in 2004), which aren't really a result of increased temperature or CO2 emissions[2], meaning these were more random than part of a trend.\n\n[2] Buis, Alan. \u201cCan Climate Affect Earthquakes, Or Are the Connections Shaky? \u2013 Climate Change: Vital Signs of the Planet.\u201d NASA, NASA, 29 Oct. 2019 ","32f54d8b":"The top graph shows the number of disasters each year sicne 1980 that have cost at least one billion dollars. This shows a clear upward trend, just like the earlier graph of the total number of natural disasters worldwide. Not only are disasters becoming more freuqent, but *expensive* disasters are becoming more frequent.\n\nThe bottom graph shows the cumulative cost of these disasters each year in 2020 US Dollars. Again, like the previous graph of costs, there is a less clear upward trend but signifcantly more spikes, meaning that the more frequent disasters also increase the potential for particularly devastating ones. The top 10 most expensive disasters are listed on the table below. It's worth noting that this is slightly outdated since the 2021 winter storm in Texas has already cost $195 Billion, making it the single most expensive disaster in US history.","779bda17":"As you've seen in the [previous notebook](https:\/\/www.kaggle.com\/jedbell\/climate-analysis-i-how-much-time), we're hurtling towards a day of reckoning where the global average temperature increases by 1.5 degrees. But when that day of reckoning comes, how much will it cost? And how much will we regret not taking action sooner? What will the costs of not taking action now be in one or two decades time? This notebook seeks to answer these questions. ","e09b2eef":"Here is my first prediction using the mean predicted temperatures as exogenous variables.  This predicts an increase in the Global Mean Sea Level by 44mm between 2014 and 2041. While this doesn't seem like a lot, keep in mind that this 44mm isn't distributed evenly across the entire ocean. Some places will see much higher rises while others could even see small decreases in sea level. This prediction is meant for a more general analysis to confirm that sea levels are indeed rising steadily and show no sign of reversing.","13548956":"The top graph shows the change in number of natural disasters for each state between the first 34 years of data (1953-1987) and the second 34 years (1988-2021). Every state had a positive difference, with California and Texas having the largest increases, both with over 200 more natural disasters in the second period. This is somewhat expected as they are two of the largest and most populated states, but the fact that every single state saw an increase reinforces the earlier conclusion that natural disasters are getting more frequent. \n\nThe middle graph shows the change in disaster type between the two time periods. Fires, hurricanes, and severe storms increased massively, while surprisingly floods decreased a little. Still, the significant increase in the other three are a direct result of increased global temperatures. \n\nThe final graph shows the number of US disaster declarations per year. This just reinforces again that disaster frequency in increasing rapidly. Prior to 1996, the US only declared at least 50 disasters in a year seven times. In the 25 years since, they've only declared fewer than 100 four times.\n\nNext, let's take a look at a map of where disasters are the most frequent.","a0276801":"Here is a map of disaster count by county. Texas has the most disasters, although their count is largely driven by two historically bad wildfire seasons in 1996 and 1998. Still, this shows that the places most susceptible to disasters are Texas, Florida, Oklahoma, and the Pacific Coast.","815ab0f2":"Taking the first difference here makes the data stationary, so let's continue onto the ACF and PACF graphs.","9ae84a19":"Finally here is a graph of hurricanes and severe storms by county. States along the Gulf of Mexico and the East Coast are the most suceptible to these disasters. Floods also often accompany these disasters in the form of a storm surge, which again will only be made worse as sea levels continue to rise.","0504e171":"Using an order of (3,1,2) gave the best AIC despite what the ACF and PACF suggested. Let's check the in-sample predictions and the diagnostics to make sure this is a good model."}}