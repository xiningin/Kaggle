{"cell_type":{"03c8f2ae":"code","cd643509":"code","cfa760b8":"code","6bea5a41":"code","547f25f6":"code","243f6cb1":"code","891c5d95":"code","0a4eec5a":"code","508e8f9e":"code","74fcf767":"code","fd3b3570":"code","41a76a8c":"code","da360046":"code","3bea353d":"code","647a9dbe":"code","81e2a006":"code","32fbcdd0":"code","5135d83b":"code","6148fe8c":"code","cce569ac":"code","47d5ded7":"code","91fe005c":"code","58a956c4":"code","20ee81d4":"code","9fc280ea":"code","401dad3c":"code","d4ad07b1":"code","d8745a2a":"code","c284135a":"code","f28a0319":"code","7018c8f3":"code","653826be":"code","81ba5a93":"code","7cfc9157":"code","d2ee2401":"code","1e9d4267":"code","770cc022":"code","8842d8ed":"code","8ce2740a":"code","3d481319":"code","69b22c08":"code","cea58031":"code","9fe24b84":"code","41e108bd":"code","70f5df59":"code","7542c98d":"code","85e5ce07":"code","430aa328":"code","713c1f62":"code","4d0c52d5":"code","45ddb17f":"code","a65be307":"code","264c285a":"markdown","e7378600":"markdown","5bb15570":"markdown","97432e1b":"markdown","06003a15":"markdown","d2472e45":"markdown","db9b7867":"markdown","d1c489aa":"markdown","f0d3a146":"markdown","87c60be8":"markdown","f11f72bc":"markdown","48181df2":"markdown","24eeee93":"markdown","4b81b8c9":"markdown","b80aae91":"markdown","cab99d00":"markdown","520cf1c9":"markdown","234e20a6":"markdown","b46bccc3":"markdown","4c8050e0":"markdown","739b069b":"markdown","daa0b388":"markdown","03f447d2":"markdown","0c886076":"markdown","6a4e6fc9":"markdown","fb361190":"markdown","969876fc":"markdown","36d98c84":"markdown","be5ee6cc":"markdown","d80ec265":"markdown","3b7bc57e":"markdown","defdf2ed":"markdown","3e9c3d67":"markdown","50f6f71a":"markdown","7258fcc8":"markdown","edd52f3a":"markdown","f3fb57eb":"markdown","2c1c3f31":"markdown"},"source":{"03c8f2ae":"!pip install transformers","cd643509":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","cfa760b8":"# General\nimport numpy as np\nimport pandas as pd\n\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport re\nfrom typing import Dict, Any, List, NoReturn, Optional\nfrom pathlib import Path\nimport subprocess as sp\nimport nvidia_smi\nimport random\nimport traceback\nimport string\nfrom pprint import pprint\nfrom collections import defaultdict, Counter\n\n# Time and loading\nfrom tqdm import tqdm\nimport time\n# from datetime import datetime\nimport datetime\n\n# Sklearns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (balanced_accuracy_score, accuracy_score, \n                             classification_report, confusion_matrix)\nfrom sklearn.metrics import confusion_matrix\n\n# Torch and NN libs\nimport torch\n\n# Save and load models\nimport joblib\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')","6bea5a41":"# System adjustments - for all colums to fit into output (default width is 80)\npd.options.display.width = 2500\npd.options.display.max_rows = 999\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', None)\npd.options.display.max_colwidth = 120","547f25f6":"ROOT_DIRECTORY = \"\/kaggle\"\nINPUT_DATA_DIRECTORY = Path(ROOT_DIRECTORY) \/ 'input' \/ \"nlp-getting-started\"\nOUTPUT_DATA_DIRECTORY = Path(ROOT_DIRECTORY) \/ 'output' \/ \"kaggle\" \nMODEL_DIRECTORY = Path(ROOT_DIRECTORY) \/ 'input' \/ \"roberta-base\"\nSAVE_MODEL_DIRECTORY = (OUTPUT_DATA_DIRECTORY \/ 'roberta-tweet')\nSAVE_MODEL_DIRECTORY.mkdir(parents=True, exist_ok=True)","243f6cb1":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(device, n_gpu)\ntorch.cuda.get_device_name(0) ","891c5d95":"!nvidia-smi","0a4eec5a":"def seed_everything(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)  # for using CUDA backend\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  # get rid of nondeterminism\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(11)","508e8f9e":"# Read datasets\ndf_train = pd.read_csv(str(INPUT_DATA_DIRECTORY \/ 'train.csv'), dtype={'id': np.int16, 'target': np.int8})\ndf_test = pd.read_csv(str(INPUT_DATA_DIRECTORY \/ 'test.csv'), dtype={'id': np.int16})\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() \/ 1024**2))\n\ndf_train.head()","74fcf767":"# Check the proportion pf targets in train dataset\ntargets_counts = df_train.target.value_counts()\nprint(\"Train target distribution:\\n\", targets_counts)\n\nsns.barplot(targets_counts.index, targets_counts)\n_ = plt.gca().set_ylabel('Number of tweets')","fd3b3570":"print(f\"In train dataset Location is filled in {sum(df_train.location.notnull())} of cases from {df_train.shape[0]}, so in {100*sum(df_train.location.notnull())\/df_train.shape[0]}%.\") \nprint(f\"In test dataset Location is filled in {sum(df_test.location.notnull())} of cases from {df_test.shape[0]}, so in {100*sum(df_test.location.notnull())\/df_test.shape[0]}%.\")\n\nprint(f\"Unique locations found: {df_train.location.nunique()}.\")\n\nloc_data = df_train.groupby(by='location').agg({'id': \"count\", 'target': 'sum'}).reset_index().sort_values(by='id', ascending=False)\nfig = px.bar(loc_data.head(100), x='id', y=\"location\", color='target',\n             title=\"Top Locations\", width=800, height=1000)\nfig.show()","41a76a8c":"# Any simple correlation between target and label encoded location (not one-hot)?\n\ndf_train['location_index'] = df_train.location.astype('category').cat.codes\n\n\nax = sns.heatmap(\n    df_train.loc[df_train.location.notnull()][['location_index', 'target']].corr(), \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)","da360046":"print(f\"In train dataset Keyword is filled in {sum(df_train.keyword.notnull())} of cases from {df_train.shape[0]}, so in {100*sum(df_train.keyword.notnull())\/df_train.shape[0]}%.\") \nprint(f\"In test dataset Keyword is filled in {sum(df_test.keyword.notnull())} of cases from {df_test.shape[0]}, so in {100*sum(df_test.keyword.notnull())\/df_test.shape[0]}%.\")\n\nprint(f\"Unique keywords found: {df_train.keyword.nunique()}.\")\n\nkw_data = df_train.groupby(by='keyword').agg({'id': \"count\", 'target': 'sum'}).reset_index().sort_values(by='id', ascending=False)\nfig = px.bar(kw_data.head(200), x='id', y=\"keyword\", color='target',\n             title=\"Top Keywords\", width=800, height=1000)\nfig.show()","3bea353d":"for df in (df_train, df_test):\n    df['word_count'] = df.text.apply(lambda x: len([tok for tok in re.split(\"[\\s\\W]\", x) if tok != '']))\n    df['unique_word_count'] = df.text.apply(lambda x: len(set([tok for tok in re.split(\"[\\s\\W]\", x) if tok != ''])))\n    df['char_count'] = df.text.apply(lambda x: len(re.sub(r\"[\\s]+\", \"\", x)))","647a9dbe":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Number of words in text\", \"Number of unique words in text\", \n                                                    \"Number of characters in text\"))\n\ntrace0 = go.Histogram(x=df_train['word_count'], name='train data', nbinsx=30)\ntrace1 = go.Histogram(x=df_test['word_count'], name='test data', nbinsx = 30)\n\ntrace2 = go.Histogram(x=df_train['unique_word_count'], name='train data', nbinsx = 30)\ntrace3 = go.Histogram(x=df_test['unique_word_count'], name='test data', nbinsx = 30)\n\ntrace4 = go.Histogram(x=df_train['char_count'], name='train data', nbinsx = 30)\ntrace5 = go.Histogram(x=df_test['char_count'], name='test data', nbinsx = 30)\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 3, 1)\nfig.append_trace(trace5, 3, 1)\n\nfig.update_layout(barmode='overlay')\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1200,\n)\n\nfig.show()","81e2a006":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Number of words in text\", \"Number of unique words in text\", \n                                                    \"Number of characters in text\"))\n\ntrace0 = go.Histogram(x=df_train.loc[df_train.target == 0]['word_count'], name='normal', nbinsx=30)\ntrace1 = go.Histogram(x=df_train.loc[df_train.target == 1]['word_count'], name='disaster', nbinsx = 30)\n\ntrace2 = go.Histogram(x=df_train.loc[df_train.target == 0]['unique_word_count'], name='normal', nbinsx = 30)\ntrace3 = go.Histogram(x=df_train.loc[df_train.target == 1]['unique_word_count'], name='disaster', nbinsx = 30)\n\ntrace4 = go.Histogram(x=df_train.loc[df_train.target == 0]['char_count'], name='normal', nbinsx = 30)\ntrace5 = go.Histogram(x=df_train.loc[df_train.target == 1]['char_count'], name='disaster', nbinsx = 30)\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 3, 1)\nfig.append_trace(trace5, 3, 1)\n\nfig.update_layout(barmode='overlay')\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1000,\n)\n\nfig.show()","32fbcdd0":"for df in (df_train, df_test):\n    df['url_count'] = df.text.apply(lambda x: len(re.findall(r\"http|https\", x)))\n    df['mean_word_length'] = df.text.apply(lambda x: np.mean([len(tok) for tok in re.split(\"[\\s\\W]\", x) if tok != '']))\n    df['punctuation_count'] = df.text.apply(lambda x: len(re.findall(r\"[\" + string.punctuation + \"]+\", x)))\n    df['hashtag_count'] = df.text.apply(lambda x: len(re.findall(r\"#[\\w]+\", x)))\n    df['mention_count'] = df.text.apply(lambda x: len(re.findall(r\"@[\\w_]+\", x)))","5135d83b":"fig = make_subplots(rows=3, cols=2, subplot_titles=(\"Number of urls\", \"Average character count in words\", \n                                                    \"Number of punctuations\", \"Number of hashtags\",\n                                                   \"Number of mentions\"))\n\ntrace0 = go.Histogram(x=df_train['url_count'], name='train data', nbinsx=6)\ntrace1 = go.Histogram(x=df_test['url_count'], name='test data', nbinsx = 6)\n\ntrace2 = go.Histogram(x=df_train['mean_word_length'], name='train data', nbinsx = 30)\ntrace3 = go.Histogram(x=df_test['mean_word_length'], name='test data', nbinsx = 30)\n\ntrace4 = go.Histogram(x=df_train['punctuation_count'], name='train data', nbinsx = 30)\ntrace5 = go.Histogram(x=df_test['punctuation_count'], name='test data', nbinsx = 30)\n\ntrace6 = go.Histogram(x=df_train['hashtag_count'], name='train data', nbinsx = 10)\ntrace7 = go.Histogram(x=df_test['hashtag_count'], name='test data', nbinsx = 10)\n\ntrace8 = go.Histogram(x=df_train['mention_count'], name='train data', nbinsx = 10)\ntrace9 = go.Histogram(x=df_test['mention_count'], name='test data', nbinsx = 10)\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 1)\nfig.append_trace(trace6, 2, 2)\nfig.append_trace(trace7, 2, 2)\nfig.append_trace(trace8, 3, 1)\nfig.append_trace(trace9, 3, 1)\n\nfig.update_layout(barmode='overlay')\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1000,\n)\n\nfig.show()","6148fe8c":"# Search for types of special characters\ndef spec_symbols_searcher(text: str) -> bool:\n    \"\"\"\n    Based on idea of: https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n    \"\"\"\n    spec_symb_pattern = re.compile(u\"\\\\x89[\\w]+\")\n    return True if len(spec_symb_pattern.findall(text)) > 0 else False\n    \ndef spec_chars_searcher(text: str) -> bool:\n    spec_chars_pattern = re.compile(r\"[\u00e5|\u00db|\u00d2|\u00aa|\u00a2|\u00ca|\u00cc|\u00a8|\u00a9]+\")\n    return True if len(spec_chars_pattern.findall(text)) > 0 else False\n    \ndef currency_searcher(text: str) -> bool:\n    currency_pattern = re.compile(r\"[\u00c7|\u00a3|$|\u20ac|\u00a5|\uffe5|\u20b4|\u20bd|\u00a2|\u00a4]+\")\n    return True if len(currency_pattern.findall(text)) > 0 else False\n\ndef xml_chars_searcher(text: str) -> bool:\n    xml_pattern = re.compile(r\"&quot;|&gt;|&lt;|&amp;|&apos;\")\n    return True if len(xml_pattern.findall(text)) > 0 else False\n    \n\nprint(\"Example contains special symbols: \", spec_symbols_searcher(\"\\x89\u00db\u00d2 Two cars set ablaze: SANTA CRUZ \\x89\u00db\u00d3\"))\nprint(\"Example contains special characters: \", spec_chars_searcher(\"don\\x89\u00db\u00aat\"))\nprint(\"Example contains currency characters: \", currency_searcher(\"\u00a33 million\"))\nprint(\"Example contains entities in XML: \", currency_searcher(\"&quot;New York Times&quot;\"))\nprint(\"\\n\")\n\nprint(\"Number of tweets contains special symbols: \", df_train.loc[df_train['text'].apply(lambda x: spec_symbols_searcher(x))].shape[0])\nprint(\"Number of tweets contains special characters: \", df_train.loc[df_train['text'].apply(lambda x: spec_chars_searcher(x))].shape[0])\nprint(\"Number of tweets contains currency characters: \", df_train.loc[df_train['text'].apply(lambda x: currency_searcher(x))].shape[0])\nprint(\"Number of tweets contains entities in XML: \", df_train.loc[df_train['text'].apply(lambda x: xml_chars_searcher(x))].shape[0])","cce569ac":"# Search for types of emojies\n\ndef emoji_searcher(text: str) -> bool:\n    \"\"\"\n    Based on: https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n    \"\"\"\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return True if len(emoji_pattern.findall(text)) > 0 else False\n\nprint(\"Example contains emojies: \", emoji_searcher(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\"))\n\nprint(\"Number of tweets contains emojies: \", df_train.loc[df_train['text'].apply(lambda x: emoji_searcher(x))].shape[0])","47d5ded7":"def datetime_searcher(text: str) -> bool:\n    date_pattern = re.compile(r\"[\\d]+[\\\\.\/][\\d]+[\\\\.\/][\\d]+\")\n    time_pattern_partly = re.compile(r\"([\\d]+[\\.:][\\d]+)( pm| am)\")\n    time_pattern_full = re.compile(r\"[\\d]+[\\.:][\\d]+[\\.:][\\d]+\")\n    return True if ((len(date_pattern.findall(text)) > 0) or \n                    (len(time_pattern_partly.findall(text)) > 0) or \n                    (len(time_pattern_full.findall(text)) > 0)) else False\n\nprint(\"Example contains datetime: \", datetime_searcher('Traffic accident N CABRILLO HWY\/MAGELLAN AV MIR (08\/06\/15 11:03:58)'))\n\nprint(\"Number of tweets contains datetime: \", df_train.loc[df_train['text'].apply(lambda x: datetime_searcher(x))].shape[0])","91fe005c":"def numbers_searcher(text: str) -> List[str]:\n    numbers_pattern = re.compile(r\"[\\d]+[,\\.:;]?[\\d]+\")\n    return numbers_pattern.findall(text)\n\nprint(\"Example contains any numbers: \", numbers_searcher(\"13,000 people receive #wildfires evacuation orders in california\"))\n\nprint(\"Number of tweets contains any numbers: \", df_train.loc[df_train['text'].apply(lambda x: True if len(numbers_searcher(x)) \n                                                              else False)].shape[0])\n\n# Look at some of them\ndf_train.loc[df_train['text'].apply(lambda x: True if len(numbers_searcher(x)) else False)].text.head(10)                                                      ","58a956c4":"!pip install textblob","20ee81d4":"from textblob import TextBlob\n\ndef spell_checker(text: str) -> str:\n    textb = TextBlob(text) \n    return False if textb.correct() == text else True\n\nprint(\"Example contains mispellings: \", spell_checker(\"I havv goood speling!\"))\n\n# Too long!\n# print(\"Number of tweets contains mispellings: \", df_train.loc[df_train['text'].apply(lambda x: spell_checker(x))].shape[0])","9fc280ea":"class Tweeter_cleaner:\n    \"\"\"\n    Class for cleaning Tweeter dataset. \n    Apply chain of regexp filters to remove punctuation, special symbols and etc.\n    \"\"\"\n    \n    def __init__(self, make_lower: bool):\n        self._filters_chain = self.compile_patterns()\n        self._make_lower = make_lower\n    \n    def compile_patterns(self):\n        return [re.compile(r'<.*?>'),  # html tags\n                re.compile(r\"\\.\\.\\.\"), # dots at the end of tweet\n                re.compile(r'https?:\/\/\\S+|www\\.\\S+'), # urls\n                re.compile(u\"\\\\x89[\\w]+\"),  # special symbols\n                re.compile(r\"[\u00c7|\u00a3|$|\u20ac|\u00a5|\uffe5|\u20b4|\u20bd|\u00a2|\u00a4]+\"), # currency\n                re.compile(r\"[\u00e5|\u00db|\u00d2|\u00aa|\u00a2|\u00ca|\u00cc|\u00a8|\u00a9]+\"),  # special chars\n                re.compile(r\"&quot;|&gt;|&lt;|&amp;|&apos;\"), # xml tags\n                re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE),\n                re.compile(r\"[\\d]+[\\\\.\/][\\d]+[\\\\.\/][\\d]+\"), # date\n                re.compile(r\"([\\d]+[\\.:][\\d]+)( pm| am)\"), # time short vatriant\n                re.compile(r\"[\\d]+[\\.:][\\d]+[\\.:][\\d]+\"),  # time long vatriant\n                re.compile(r\"[!?+*\\[\\]\\-\\.&%\/()$={}^'`:;<>+\\.]+\"),  # punctuations, but without cleaning: @, # and _\n               ]\n        \n        \n    def clean_tweet(self, text: str) -> str:\n        assert(type(text) == 'str',  \"Text to clean should be string!\")\n        \n        # Make lower case\n        if self._make_lower:\n            text = text.lower()\n        \n        # Apply filters\n        for f in self._filters_chain:\n            text = f.sub(\" \", text)\n        \n        # Link separeted by comma numbers together\n        text = re.sub(r\",\", \"\", text)\n            \n        # Clean double whitespaces\n        text = re.sub(r\"\\s+\", ' ', text)\n            \n        return text","401dad3c":"cleaner = Tweeter_cleaner(make_lower=True)\ndf_train['clean_text'] = df_train['text'].apply(lambda x: cleaner.clean_tweet(x))\ndf_test['clean_text'] = df_test['text'].apply(lambda x: cleaner.clean_tweet(x))\n\ndf_train[['text', 'clean_text']].head(10)","d4ad07b1":"df_train['clean_text'] = df_train['keyword'].fillna(\"no_keyword\") + ' ' + df_train['clean_text']\ndf_test['clean_text'] = df_test['keyword'].fillna(\"no_keyword\") + ' ' + df_test['clean_text']","d8745a2a":"# torch utils\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n\n# transformers\nimport transformers\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom tokenizers.processors import RobertaProcessing","c284135a":"tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file=str(MODEL_DIRECTORY \/ 'vocab.json'), \n            merges_file=str(MODEL_DIRECTORY \/ 'merges.txt'), \n            lowercase=True, add_prefix_space=True)\n\nconfig = RobertaConfig.from_pretrained(str(MODEL_DIRECTORY \/ 'config.json'), output_hidden_states=True) \n               \nroberta = RobertaForSequenceClassification.from_pretrained(str(MODEL_DIRECTORY \/ 'pytorch_model.bin'), config=config)    \n\nprint(roberta)","f28a0319":"# Change Post-Processor to RoBERTa type\ntokenizer._tokenizer.post_processor = RobertaProcessing(\n            sep=(\"<\/s>\", tokenizer.token_to_id(\"<\/s>\")),\n            cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n            add_prefix_space=True,\n            trim_offsets=True,\n        )\n\nexample_seq_length = 25\ntokenizer.enable_truncation(max_length=example_seq_length)\n# Argumens: direction: str, pad_id: int, pad_type_id: int, pad_token: str, length: int\ntokenizer.enable_padding(\"right\",1, 0, \"<pad>\", example_seq_length)\n\n# Look at example of first text in dataset\nseq_tokenized = tokenizer.encode(df_train['clean_text'].values[0], add_special_tokens=True)\n\nprint(\"Example of ids: \", seq_tokenized.ids, '\\n')\nprint(\"Example of type ids: \", seq_tokenized.type_ids, '\\n')\nprint(\"Example of tokens: \", seq_tokenized.tokens, '\\n')\nprint(\"Example of offsets: \", seq_tokenized.offsets, '\\n')\nprint(\"Example of their attention masks: \", seq_tokenized.attention_mask, '\\n')\nprint(\"Example of special tokens mask: \", seq_tokenized.special_tokens_mask, '\\n')\nprint(\"Example of overflowing: \", seq_tokenized.overflowing, '\\n')","7018c8f3":"# Disable truncation and padding to get real BPE length of each text\ntokenizer.no_truncation()\ntokenizer.no_padding()\n\ntokenized_train = df_train['clean_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized_test = df_train['clean_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized = pd.concat([tokenized_train, tokenized_test], ignore_index=True)\nlen_tokenized = tokenized.apply(lambda x: len(x))\n\nprint(f\"Tokenized texts: {len(tokenized)}\")\n\nfig = px.histogram([len(t.ids) for t in tokenized], nbins=30)\nfig.show()\n\nprint(\"Maximum length: \", np.max([len(t.ids) for t in tokenized]))\nprint(\"Minimum length: \", np.min([len(t.ids) for t in tokenized]))","653826be":"def tweeter_dataloader_pipeline(df: pd.DataFrame, tokenizer, labeled: bool,\n                                features_col_name: str, label_col_name: Optional[str],\n                                make_padding: bool, make_truncation: bool,\n                                max_text_length: int, batch_size: int):\n    \"\"\"\n    Create dataloader object from given dataset labled or not.\n    param: df - input dataframe (labeled or not);\n    param: tokenizer - tokenizer of model;\n    param: labeled - True if data is labeled, if False - unlabeled inference case,\n        DataLoader object then does not contain labels either;\n    param: features_col_name - column in df, where texts are stored;\n    param: label_col_name - column in df, where labels are stored; \n    param: make_padding - True if need to pad short sequencies to max_text_length, \n        False otherwise;\n    param: make_truncation - True if need to truncate short sequencies to max_text_length, \n        False otherwise;\n    param: max_text_length - maximum length of tokens sequence,\n    param: batch_size - number of samples that will be stored on GPU simultaneously;\n    return: DataLoader object.\n    \"\"\"\n    texts = df[features_col_name].tolist()\n    \n    if labeled:\n        labels = df[label_col_name].tolist()\n        labels = torch.tensor(labels)\n    \n    if make_truncation:\n        tokenizer.enable_truncation(max_length=max_text_length)\n    if make_padding:\n        tokenizer.enable_padding(\"right\",1, 0, \"<pad>\", max_text_length)\n    \n    texts_encoded = [tokenizer.encode(text, add_special_tokens=True) for text in tqdm(texts)]\n    ids = torch.LongTensor([enc.ids for enc in texts_encoded])\n    type_ids = torch.LongTensor([enc.type_ids for enc in texts_encoded])\n    attention_masks = torch.LongTensor([enc.attention_mask for enc in texts_encoded])\n    offsets = torch.LongTensor([enc.attention_mask for enc in texts_encoded])\n    \n    # Create the DataSets instances\n    if labeled:\n        ds = TensorDataset(ids, type_ids, attention_masks, offsets, labels)\n    else:\n        ds = TensorDataset(ids, type_ids, attention_masks, offsets)\n    \n    sampler = SequentialSampler(ds)\n\n    return DataLoader(ds, sampler=sampler, batch_size=batch_size)","81ba5a93":"data_train, data_val = train_test_split(df_train, train_size=0.8, random_state=11, \n                                        shuffle=True, stratify=df_train.target.values)\n\nprint('Length train data:', len(data_train))\nprint('Length validation data:', len(data_val))","7cfc9157":"train_dataloader = tweeter_dataloader_pipeline(data_train, tokenizer, labeled=True, \n                                               features_col_name='clean_text', label_col_name='target',\n                                               make_padding=True, make_truncation=True,\n                                               max_text_length=55, batch_size=128)\n\nval_dataloader = tweeter_dataloader_pipeline(data_val, tokenizer, labeled=True, \n                                             features_col_name='clean_text', label_col_name='target',\n                                             make_padding=True, make_truncation=True,\n                                             max_text_length=55, batch_size=128)\n\ntest_dataloader = tweeter_dataloader_pipeline(df_test, tokenizer, labeled=False, \n                                             features_col_name='clean_text', label_col_name='',\n                                             make_padding=True, make_truncation=True,\n                                             max_text_length=55, batch_size=128)","d2ee2401":"nvidia_smi.nvmlInit()\n\ndef get_gpu_memory() -> NoReturn:\n    \n    # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n\n    print(f\"\\nTotal GPU memory: {info.total}\")\n    print(f\"Free GPU memory: {info.free}\")\n    print(f\"Used GPU memory: {info.used}\")\n    # nvidia_smi.nvmlShutdown()\n\ndef get_gpu_free_memory() -> NoReturn:\n    \n    # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n\n    print(f\"Free GPU memory: {info.free \/ 8388608} MB.\")","1e9d4267":"def clean_GPU_memory() -> NoReturn:\n    torch.cuda.empty_cache()\n    get_gpu_memory()\n    \nget_gpu_memory()\nget_gpu_free_memory()\n# Initial Free Memory size: 2033.859375 MB = 17061249024 ","770cc022":"def copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Invalid data type {}'.format(type(data)))","8842d8ed":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","8ce2740a":"# Service function\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","3d481319":"def save_model(model, dir: str):\n    \"\"\"\n    Trained model, configuration and tokenizer, \n    they can then be reloaded using `from_pretrained()` if using default names.\n    \"\"\"\n    print(\"Saving model to {0}...\".format(dir))\n    \n    # Take care of distributed\/parallel training\n    model_to_save = model.module if hasattr(model, 'module') else model \n    model_to_save.save_pretrained(dir)\n    print(\"Model successfully saved.\")\n    \n    \ndef load(filename: str, has_info: bool):\n    model = torch.load(filename)\n    if has_info:\n        info = torch.load('model.info')\n        pprint(info)\n    return model","69b22c08":"def train_eval_loop(model, dataloaders_dict, device=None,\n                    epoch_n=10, lr=1e-5, optim_eps=1e-8, \n                    num_warmup_steps=0, criterion=None,\n                    optimizer=None, lr_scheduler=None,\n                    model_dir_to_save=\".\",  model_filename=\"model.pt\"\n                    ):\n    \"\"\"\n    Loop for training the model. After each epoch, \n    the quality of the model is evaluated by a validation set.\n    :param model: torch.nn.Module - model to learn;\n    :param dataloaders_dict: dictionary of torch.utils.data.DataLoaders - train, val, test;\n    :param device: cuda\/cpu - device to perform calculations on;\n    :param lr: learning rate;\n    :param epoch_n: maximium number of epochs;\n    :param optim_eps: coefficient for Adam-regularization;\n    :param num_warmup_steps: the number of steps for the warmup phase;\n    :return: tuple of two parts:\n        - mean loss value on validation set on the best epoch of training;\n        - best model;\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n    print(\"Moved model to device.\")\n    get_gpu_memory()\n    \n    # Unpack dataloders\n    train_dataloader = dataloaders_dict.get('train', None)\n    val_dataloader = dataloaders_dict.get('val', None)\n    \n    # Set hyperparameters\n    if optimizer is None:\n        optimizer = AdamW(model.parameters(), lr=lr, eps=optim_eps)\n                                                                            \n    else:\n        optimizer = optimizer(model.parameters(), lr=lr, eps=optim_eps)\n\n    # Total number of training steps is [number of batches] x [number of epochs]. \n    num_training_steps = len(train_dataloader) * epoch_n\n    if lr_scheduler is not None:\n        lr_scheduler = lr_scheduler(optimizer, \n                                         num_warmup_steps=num_warmup_steps,\n                                         num_training_steps=num_training_steps)\n    else:\n        lr_scheduler = None\n        \n    # Storing training and validation loss, validation accuracy, and timings.\n    training_stats = []\n\n    # Measure the total training time for the whole run.\n    total_train_time = time.time()\n\n    for epoch_i in range(epoch_n):\n        try:\n            # Measure how long the training epoch takes.\n            epoch_start = time.time() \n            print(\"\")\n            print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epoch_n))\n            \n            # ========================================\n            #               Training\n            # ========================================\n\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n\n            # Unpacking this training batch from our dataloader. \n            for batch_i, (b_ids, b_type_ids, b_attn_mask, b_offsets, b_labels) in enumerate(train_dataloader):\n                \n                # Printing progress update every 40 batches.\n                if (batch_i % 50 == 0) and not (batch_i == 0):\n                    elapsed = format_time(time.time() - epoch_start)\n                    # Monitor memory usage\n                    get_gpu_free_memory()\n                    print(\"\\tBatch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(batch_i, len(train_dataloader), elapsed))\n\n                b_ids = copy_data_to_device(b_ids, device)\n                b_attn_mask = copy_data_to_device(b_attn_mask, device)\n                b_labels = copy_data_to_device(b_labels, device)\n\n                # Clearing any previously calculated gradients before performing a backward pass. \n                model.zero_grad()\n\n                # Perform a forward pass - evaluate the model on this training batch.\n                # \"logits\" are the hidden state of the last layer of the RoBERTa model\n                outputs = model(b_ids, token_type_ids=None, attention_mask=b_attn_mask, labels=b_labels)\n\n                (loss, logits) = outputs[:2]\n                # Perform a backward pass to calculate the gradients.\n                loss.backward()\n\n                # Clip the norm of the gradients to 1.0. to help prevent the \"exploding gradients\" problem.\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n                # Update parameters with optimizer's learning rate and take a step using the computed gradient.\n                optimizer.step()\n                \n                # Update the learning rate.\n                lr_scheduler.step()\n\n                # Accumulating the training loss over all of the batches so that we can calculate the average loss at the end. \n                mean_train_loss += loss.item()\n                train_batches_n += 1\n\n            mean_train_loss \/= train_batches_n\n            epoch_train_time = (time.time() - epoch_start)\n            print('\\nEpoch: {} iters, {:0.2f} sec'.format(train_batches_n,\n                                                        epoch_train_time))\n            print('Mean value of loss function during training: ', mean_train_loss)\n\n            # ========================================\n            #               Validation\n            # ========================================\n            valid_start = time.time() \n\n            # Put the model in evaluation mode\n            model.eval()\n\n            mean_val_loss = 0\n            val_accuracy = 0\n            val_batches_n = 0\n            with torch.no_grad():\n                for batch_i, (b_ids, b_type_ids, b_attn_mask, b_offsets, b_labels) in enumerate(val_dataloader):\n\n                    b_ids = copy_data_to_device(b_ids, device)\n                    b_attn_mask = copy_data_to_device(b_attn_mask, device)\n                    b_labels = copy_data_to_device(b_labels, device)\n\n                    # Forward pass, calculate logit predictions.\n                    # Get the \"logits\" output by the model.\n                    outputs  = model(b_ids, token_type_ids=None,\n                                            attention_mask=b_attn_mask,\n                                            labels=b_labels)\n                    (loss, logits) = outputs[:2]\n\n                    mean_val_loss += loss.item()\n                    val_batches_n += 1\n                    \n                    # Move logits and labels to CPU\n                    logits = copy_data_to_device(logits.detach(), 'cpu').numpy()\n                    label_ids = copy_data_to_device(b_labels, 'cpu').numpy()\n\n                    # Calculate the accuracy for this validation batch and accumulate it over all batches.\n                    val_accuracy += flat_accuracy(logits, label_ids)\n\n            # Measure how long the validation run took.\n            validation_time = format_time(time.time() - valid_start)\n\n            mean_val_loss \/= val_batches_n\n            print(\"Mean value of loss function during validation: {0:.2f}\".format(mean_val_loss))\n\n            mean_val_accuracy = val_accuracy \/ val_batches_n\n            print(\"Mean validation accuracy: {0:.2f}\".format(mean_val_accuracy))\n\n            # Record all statistics from this epoch.\n            training_stats.append(\n                {\n                    'epoch': epoch_i + 1,\n                    'Training Loss': mean_train_loss,\n                    'Valid. Loss': mean_val_loss,\n                    'Valid. Accur.': mean_val_accuracy,\n                    'Training Time': epoch_train_time,\n                    'Validation Time': validation_time\n                }\n            )\n        except KeyboardInterrupt:\n            print('Early stopped by the user.')\n            break\n        except Exception as ex:\n            print('Error while training: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print('\\n', \"Training complete\")\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_train_time)))\n\n    # Monitor memory usage\n    get_gpu_free_memory()\n\n    # Saving model\n    save_model(model, model_dir_to_save) \n    clean_GPU_memory()\n    return training_stats, model","cea58031":"training_stats, model = train_eval_loop(roberta, {\"train\": train_dataloader, \"val\": val_dataloader}, \n                                        device=device, epoch_n=8,\n                                        lr=1e-5, optim_eps=1e-5,\n                                        optimizer=AdamW, \n                                        lr_scheduler=get_linear_schedule_with_warmup,\n                                        model_dir_to_save=str(OUTPUT_DATA_DIRECTORY))","9fe24b84":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 11), dpi=60)\nplt.tight_layout()\n\n# Create a DataFrame from our training statistics and use the 'epoch' as the row index.\ndf_stats = pd.DataFrame(data=training_stats).set_index('epoch')\n\naxes[0].plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\naxes[0].plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\naxes[1].plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation\")","41e108bd":"clean_GPU_memory()","70f5df59":"def compute_metrics(true_labels: List[int], \n                    pred_labels: List[int]) -> NoReturn:\n    assert(len(true_labels)==len(pred_labels), \n           \"Labels lists must have the same length.\")\n    \n    print(\"***** Eval results {} *****\")\n    \n    ac = accuracy_score(true_labels, pred_labels)\n    bac = balanced_accuracy_score(true_labels, pred_labels)\n\n    print('Accuracy score:', ac)\n    print('Balanced_accuracy_score:', bac)\n    print(classification_report(true_labels, pred_labels))","7542c98d":"def plot_confusion_matrix(true_labels: List[int], \n                        pred_labels: List[int]) -> NoReturn:\n    CM = confusion_matrix(true_labels, pred_labels)\n    df_cm = pd.DataFrame(CM, range(CM.shape[0]), range(CM.shape[1]))\n    plt.figure(figsize=(5,5))\n    sns.set(font_scale=1.4) \n    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt = \".0f\") \n    plt.show()","85e5ce07":"# Inference function to get logits from model for each text in prediction_dataloader\ndef evaluate(model, dataloader, labeled: bool, device=None, return_logits=False, \n                  print_metrics=True, verbose=True) -> Dict[str, np.array]:\n    \"\"\"\n    :param model: torch.nn.Module - trained model;\n    :param dataloader: torch.utils.data.DataLoader - data (and labels) for evaluation;\n    :param labeled: if True, then make comparison with true labels of classes;\n    :param device: cuda\/cpu - device to perform calculations on;\n    :param return_logits: if True, then return models last layer outputs;\n    :param print_metrics: if True, then compute and print classification report;\n    :return: dict with results.\n    \"\"\"\n    print('\\n'*2, 'Predicting labels for {:,} test sentences...'.format(len(dataloader.dataset)))\n\n    predictions, pred_labels = [], []\n    if labeled:\n        true_labels = []\n\n    # Inference loop for each batch\n    for batch_i, batch in tqdm(enumerate(dataloader)):\n\n        if labeled:\n            (b_ids, b_type_ids, b_attn_mask, b_offsets, b_labels) = batch\n        else:\n            (b_ids, b_type_ids, b_attn_mask, b_offsets) = batch\n\n        # Add batch to GPU\n        b_ids = copy_data_to_device(b_ids, device)\n        b_attn_mask = copy_data_to_device(b_attn_mask, device)\n        \n        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n        with torch.no_grad():\n            # tensor[8x24]\n            logits = model(b_ids, token_type_ids=None, attention_mask=b_attn_mask)[0]\n        \n        # Move logits and labels to CPU\n        logits = copy_data_to_device(logits, 'cpu').numpy()\n        \n        if labeled:\n            b_labels = copy_data_to_device(b_labels, 'cpu').numpy()\n            true_labels.extend(b_labels)  \n\n        # Store predictions \n        for x in logits:\n            predictions.append(x)\n            pred_labels.append(np.argmax(x))\n        \n    # Printing classification results and prediction examples for correctness check\n    if verbose:\n        print('\\n'*2, 'Classification done.', '\\n')\n        print('Predictions:', len(predictions))\n        print('Pred_labels:', len(pred_labels))\n        if labeled:\n            print('True_labels:', len(true_labels))\n\n    if print_metrics and labeled:\n        compute_metrics(true_labels, pred_labels)\n        plot_confusion_matrix(true_labels, pred_labels)\n\n    returning = defaultdict(list)\n    returning[\"predicted_labels\"] = pred_labels\n    if labeled:\n        returning[\"true_labels\"] = true_labels\n    if return_logits:\n        returning[\"predicted_logits\"] = predictions\n    \n    return returning","430aa328":"# Load dataset with test labels - from website\n\ndf_leak = pd.read_csv(Path(ROOT_DIRECTORY) \/ 'input' \/ 'disasters-on-social-media-perfect-submission'\/ 'perfect_submission.csv', \n                      encoding ='ISO-8859-1')\nprint('Leaked Data Set Shape = {}'.format(df_leak.shape))\nprint('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() \/ 1024**2))\n\n# Append answers to test dataset\ndf_test = df_test.merge(df_leak, on='id')","713c1f62":"# Re-create dataloder with answers\ntest_dataloader = tweeter_dataloader_pipeline(df_test, tokenizer, labeled=True, \n                                             features_col_name='clean_text', label_col_name='target',\n                                             make_padding=True, make_truncation=True,\n                                             max_text_length=55, batch_size=128)","4d0c52d5":"# Run evaluation\nmodels_predictions = evaluate(model, test_dataloader, labeled=True,  \n                             return_logits=True, device=device, \n                            print_metrics=True, verbose=True)\n                                   ","45ddb17f":"df_test['target'] = models_predictions.get('predicted_labels', 0)\ndf_test['target'].value_counts()","a65be307":"df_test[['id', 'target']].to_csv(\"models_submission.csv\", sep=',', encoding='utf-8')","264c285a":"# 5. Training RoBERTa Model","e7378600":"# 6. Evaluation on test data\n\nAs everyone knows, there are answers to this competition and test set labels can be found on this [website](https:\/\/appen.com\/resources\/datasets\/), so the goal of the laptop was a personal attempt to explore and learn the core concepts of the transformers models and BPE tokenizer. \n\nGetting the highest score is not the goal itself, but it's still interesting to see how Roberta will perform on the test data!\n\n**I specially downloaded the perfect solution and want to evaluate the quality of the model's solution.**","5bb15570":"## Statistic of Length of Texts\n\nDistributions of meta features in classes and datasets can be helpful to identify disaster tweets.\n\nHowever, attributes related to the length of messages: the length of text in words, in characters, and the number of unique, non-repeating words are limited to the maximum length of a tweet - 280 characters.\n\nbut in this case, you can try to identify some patterns, such as: whether disaster tweets are written in a more formal way with longer words compared to non-disaster tweets?\n\n* **word_count** number of words in text\n* **unique_word_count** number of unique words in text\n* **char_count** number of characters in text","97432e1b":"## 4.3 Output of byte version of Byte-Pair Encoder (BPE)\n\nAs it was said before BPE Encoder output Encoding object, which ....\n\n\nAlso, if we need to make truncation or padding the encoding methods should be called after calling:  *enable_padding* and *enable_truncation* set to True. \n\nThe output of the tokenizer should have the following pattern:\n\n> roberta: [s] + prefix_space + tokens + [\/s] + padding [pad]\n\n**[s], [\/s], [pad] and [unk]** - are **special tokens**, that are marked in the tokenizer dictionary in the following positions:\n\n* [s] - 0\n* [\/s] - 2\n* [pad] - 1\n* [unk] - 3\n\nThen we need to adjust the offsets to match the added characters at the beginning and end of the sentence by adding zero offsets: (0,0).","06003a15":"## 4.2 Input data\nRoBERTa has imposing vocabulary - the size of 50,000. Thus, the simple language of tweets should not contain unknown words, otherwise they will be sorted out by special tokenization technic - **byte version of Byte-Pair Encoding (BPE)**.\n\nAs input RoBERTa can deal with the texts. The inputs of the model take pieces of 512 contiguous tokens, that were received using a BPE, and may span over documents.\n\nIn this case, because the length of tweets is very small (if you look at the above graph, no more than 15-20 words) we can try to **change the length of the input sequence to a smaller side** and at the same time **increase the butch size** even more, which is one of the things that distinguishes RoBERTa from BERT.","d2472e45":"## 3.2 Clean tweets\n\nlets finally apply created patterns and functions to datasets.","db9b7867":"## Staticstical Meta Features from Texts\n\nWe can get a lot additional featured from the way the tweets are written: from the number of hashtags, links to resources or users, even from the number of punctuation marks in the texts.\n\n* **url_count** number of urls in text\n* **mean_word_length** average character count in words\n* **punctuation_count** number of punctuations in text\n* **hashtag_count** number of hashtags (#) in text\n* **mention_count** number of mentions (@) in text","d1c489aa":"# About this Notebook\n\nTo begin with, I would like to say that **this notebook is not aimed at high scores in the competition**, it was created in order to personally understand the **work of transformer models with small texts** and assess how such giant models can (or can not!) bypass the simpler ones.\n\nHaving experience with transformers in Russian language cases, for which the choice of models is extremely limited: there is only a pre-trained [RuBERT from DeepPavlov](http:\/\/docs.deeppavlov.ai\/en\/master\/features\/models\/bert.html), I wanted to try other models that are luckly available for English. So, in this notebook, I'll use RoBERTa to analyze tweets.\n\nIn this kernel, I will briefly explain the structure of dataset, generate and analyze metafeatures. Then I will explore tokenizer for transformer models and train RoBERTa on given corpus.\n\nThis kernel includes codes and ideas from kernels below:\n1. [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert) by @Gunes Evitan ----> MetaFeatures extraction patterns and test answers.\n2. [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove#Data-Cleaning) by Shahules -----------> MetaFeatures extraction patterns.\n\n\n**This kernel is a work in Progress,and I will keep on updating it as the competition progresses and I learn more and more things about the data**\n\n<font color='red'>**If you find this kernel useful, Please Upvote it , it motivates me to write more Quality content** <\/font>\n\n","f0d3a146":"Sadly, but those features have very similar distributions in disaster and non-disaster tweets.","87c60be8":"### Typos - real evil for language models\n\nFor this use python spell checking from **TextBlob**.\n\nIt uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results.","f11f72bc":"### Load pre-trained model","48181df2":"Before creating dataloaders, you need to divide the tagged with targets dataset into two: training and validation subsets. The second one will be much smaller, but it is necessary to evaluate the value of the loss function during training and perform early stopping.","24eeee93":"Good news! Tweets are clean of emoticons and similar symbols. There is no need to clean them.","4b81b8c9":"## 5.1 Define Training helper functions","b80aae91":"### Date&Time - can it help?\n\nHere we can see, that in whole dataset there are nearly 50 date or time stamps. Thats not enougth for taling them as feature, in addition, we do not know the date or time of the disasters to determine the time basis.","cab99d00":"When we use a pre-trained models, we need our data to be pre-processed and presented in the same way as the data that the model was trained on. In transformers, each model architecture is associated with 3 main types of classes:\n\n* A **model** class to load\/store a particular pre-train model.\n* A **tokenizer** class to pre-process the data and make it compatible with a particular model.\n* A **configuration** class to load\/store the configuration of a particular model.\n\nFor the RoBERTa architecture, we use **RobertaForSequenceClassification** for the model class, **RobertaTokenizer** for the tokenizer class, and **RobertaConfig** for the configuration class. ","520cf1c9":"### Research byte version of Byte-Pair Encoder (BPE)\n\nThis implementation of a BPE tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information:\n<img src=https:\/\/miro.medium.com\/max\/875\/1*7uy9X3eE1rVmqV08yKrDgg.png width=\"500\">\n\nThe **Normalizer** first normalizes the text, the result of which is fed into the **PreTokenizer** which is in charge of applying simple tokenization by splitting the text into its different word elements using whitespaces. \n\nThe **Model** corresponds to the actual algorithm, such as BPE, WordPiece or SentencePiece, that performs the tokenization itself. \n\nThe **PostProcessing** then takes care of incorporating any additional useful information that needs to be added to the final **output Encoding**, which is then ready to be used and fed into, say, a language model for training.\n\n*Read more in small but informative article [Hugging Face Introduces Tokenizers](https:\/\/medium.com\/dair-ai\/hugging-face-introduces-tokenizers-d792482db360) by Elvis on Medium.*","234e20a6":"# 0. Setting up our environment","b46bccc3":"## 3.1 Exploration of common patterns in tweets","4c8050e0":"## 4.4 Create DataLoader\n\nTo use the RoBERTa tokenizer, we will create a function **tweeter_dataloader_pipeline** that will contain a Tokenizer, a DataSet and return a DataLoader. \nIt will implement the functions of tokenizing and encoding tweets, organizing them in a dataset and submitting them to the model by Butch.\n\n","739b069b":"## 0.1 Mounting paths","daa0b388":"# 1. Loading datasets","03f447d2":"### Analyse BPE encoding text length\n\nTo form input, first we need to find out what the average length of the text in the BPE encoding is, since it can differ greatly from the word-by-word tokenization, which was preformed before.","0c886076":"# 3. Text Preprocessing\n\nAs we know, tweets require lots of cleaning, because people usually use emoticons and punctuation marks to express their emotions, often typos in a hurry, which can greatly affect the dictionary and sometimes Tweeter parcer skip html tags in the text.\n\nThe training dataset is too large to view with your eyes for the symbols and signs you are looking for, \nso let's use regular expression magic  in this task \ud83d\ude09.","6a4e6fc9":"## 4.1 Pretrained RoBERTa models\n\nHugging Face has big library of pre-trained models for different languages, tasks and architectures. Here is the full list of the currently provided pretrained models of Roberta together with a short presentation of each model:\n\n* **roberta-base** - 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n* **roberta-large** - 24-layer, 1024-hidden, 16-heads, 355M parameters RoBERTa using the BERT-large architecture\n* **roberta-large-mnli** - 24-layer, 1024-hidden, 16-heads, 355M parameters roberta-large fine-tuned on MNLI.\n* **distilroberta-base** - 6-layer, 768-hidden, 12-heads, 82M parameters The DistilRoBERTa model distilled from the RoBERTa model roberta-base checkpoint.\n* **roberta-base-openai-detector** - 12-layer, 768-hidden, 12-heads, 125M parameters roberta-base fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.\n* **roberta-large-openai-detector** - 24-layer, 1024-hidden, 16-heads, 355M parameters roberta-large fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.\n\nTo classify tweets the simplest and most obvious option is to use the basic version of pre trained Roberta: **roberta-base**.","fb361190":"### Location\nLet's take a closer look at the location field. <br>\nIt is filled in 66% of cases and contains about **3341 unique values**. Thats much! \n\nThat's happen because Locations in Tweeter are not automatically generated, they are user inputs. \nHowewer, if user was using Twitter for Android or Twitter for iOS, the Tweet may also include your precise location (i.e., the GPS coordinates from which you Tweeted), which can be found through the Twitter API, in addition to the location label user select. \n\nAlthough this is useful for users, it is completely unsuitable for making this a feature for the model.\n\nAs it can be seen from correlation matrix below.","969876fc":"### Numbers - are they helpfull?","36d98c84":"### Emojies - good, but not in NLP \ud83d\ude14","be5ee6cc":"# 4. Implementing RoBERTa with HuggingFace \ud83e\udd17Transformers","d80ec265":"## 0.2 Checking for the hardware backend","3b7bc57e":"### Add Keyword to text","defdf2ed":"And here's again, features have very similar distributions in disaster and non-disaster tweets.","3e9c3d67":"## 5.3 Train model","50f6f71a":"## RoBERTa\n\nBased on [RoBERTa: A Robustly Optimized BERT Pretraining Approach, Yinhan Liu et al.](https:\/\/arxiv.org\/pdf\/1907.11692.pdf)\nIt is based on Google\u2019s BERT model released in 2018.\nIt builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\nThe abstract from the paper is the following:\n\n> Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n\n**Tips from pre-training RoBERTa:**\n* dynamic masking: tokens are masked differently at each epoch whereas BERT does it once and for all\n* no NSP (next sentence prediction) loss and instead of putting just two sentences together, put a chunk of contiguous texts together to reach 512 tokens (so sentences in in an order than may span other several documents)\n* train with larger batches\n* use BPE with bytes as a subunit and not characters (because of unicode characters)","7258fcc8":"It was highly expected that tokenizer splitted words into subtokens, thus increased the average length of texts by almost one and a half times.\n\nBut still, this length of texts is extremely small compared to the 512 characters on which the model was pre-trained. This can lead to a loss of quality in classification task, since RoBERTa will not be able to recognize the linguistic structure of such small sequences.","edd52f3a":"## 1.1 Keyword & Location\n\nBoth training and test set have Location and Keyword fields missing in many cases. \nAt the same time, they have same ratio of missing values in keyword and location.\n\n* 66.7% of location is filled in both training and test set;\n* 99.2% of keyword is filled in both training and test set;\n\nSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. ","f3fb57eb":"### Special symbols","2c1c3f31":"### Keyword\nBut fortunately, we have exactly the opposite picture with the Keyword field. It is filled in 99.2%!\n\nKeywords have much less different values and keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set."}}