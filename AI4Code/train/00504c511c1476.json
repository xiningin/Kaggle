{"cell_type":{"987d0f50":"code","7e3e1c8a":"code","d05ede04":"code","7f7243bb":"code","96986ccc":"code","59cbf0f7":"code","d9ab0a5d":"markdown","8e43e656":"markdown","c8324a1b":"markdown","5c7de2e8":"markdown","22e38921":"markdown","f8f07bb5":"markdown","43e1a4c4":"markdown"},"source":{"987d0f50":"# import system modules\nimport os\nimport sys\nimport datetime\nimport random\n\n# import external helpful libraries\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport h5py\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\n# import keras\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization \nfrom keras.layers import Input, UpSampling2D, concatenate  \nfrom keras.optimizers import Nadam, SGD\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, TensorBoard\n\n# possible libraries for metrics\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n\n#K-Fold Cross Validation\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Set the random seed to ensure reproducibility\nnp.random.seed(1234)\ntf.random.set_seed(1234)","7e3e1c8a":"# Edit this variable to point to your dataset, if needed\ndataset_path = '..\/input\/gametei2020\/dataset'\n\n#Form a full dataset \ndef combine_dataset(dataset_path, split1, split2):\n    split1_path = os.path.join(dataset_path, split1)\n    split2_path = os.path.join(dataset_path, split2)\n    data_out = []\n    \n    # iterate each class\n    classes = [\"NORMAL\", \"PNEUMONIA\"]\n    # notice that class_idx = 0 for NORMAL, 1 for PNEUMONIA\n    for class_idx, _class in enumerate(classes):\n        class_path1 = os.path.join(split1_path, _class) # path to each class dir\n        class_path2 = os.path.join(split2_path, _class)\n        # iterate through all files in dir\n        for filename in os.listdir(class_path1):\n            # ensure files are images, if so append to output\n            if filename.endswith(\".jpeg\"):\n                img_path = os.path.join(class_path1, filename)\n                data_out.append((img_path, class_idx))\n        for filename in os.listdir(class_path2):\n            # ensure files are images, if so append to output\n            if filename.endswith(\".jpeg\"):\n                img_path = os.path.join(class_path2, filename)\n                data_out.append((img_path, class_idx))\n                \n    return data_out\ndataset_seq = combine_dataset(dataset_path,split1 = \"train\",split2 = \"val\")\ndataset_pneumonia_cases = sum([class_idx for (img_path, class_idx) in dataset_seq])\ndataset_normal_cases = len(dataset_seq) - dataset_pneumonia_cases\nprint(\"Combined - Total: %d, Normal: %d, Pneumonia: %d\" % (len(dataset_seq), dataset_normal_cases, dataset_pneumonia_cases))","d05ede04":"# Shuffle the dataset\nfrom sklearn.utils import shuffle\ndataset_seq = shuffle(dataset_seq)\n\n# Kfold splitting \nn_folds = 4\nfold_seq = [[] for i in range(n_folds)]\nfold_seq[0] = dataset_seq[0:1308]\nfold_seq[1] = dataset_seq[1308:2616]\nfold_seq[2] = dataset_seq[2616:3924]\nfold_seq[3] = dataset_seq[3924:5232]\nfold_pneumonia_cases = [[] for i in range(n_folds)]\nfold_normal_cases = [[] for i in range(n_folds)]\nfor i in range(n_folds):\n    fold_pneumonia_cases[i] = sum([class_idx for (img_path, class_idx) in fold_seq[i]])\n    fold_normal_cases[i] = len(fold_seq[i]) - fold_pneumonia_cases[i]\n    print(\"Fold %d - Total: %d, Normal: %d, Pneumonia: %d\" % (i, len(fold_seq[i]), fold_normal_cases[i], fold_pneumonia_cases[i]))","7f7243bb":"#Dividing training sets intor normal and pneumonia\n\nn_fold_pneumonia_cases = []\nn_fold_normal_cases = []\nfold_normal = [[] for i in range(4)]\nfold_pneumonia = [[] for i in range(4)]\n\nfor j in range(n_folds):\n    fold_normal[j] = [[] for i in range(len(fold_seq[i]))]\n    fold_pneumonia[j] = [[] for i in range(len(fold_seq[i]))]\n    fold_normal[j] = [[i[0],i[1]] for i in fold_seq[j] if i[1]== 0]\n    fold_pneumonia[j] = [[i[0],i[1]] for i in fold_seq[j] if i[1]== 1]\n    #print(len(fold_normal[j]))\n    #print(len(fold_pneumonia[j]))    \n    for k in range(len(fold_pneumonia[j]) - len(fold_normal[j])):\n        sample = random.sample(fold_normal[j],1)\n        fold_seq[j] = fold_seq[j] + sample\n    #print(len(fold_seq[j]))\n    n_fold_pneumonia_cases.append (sum([class_idx for (img_path, class_idx) in fold_seq[j]])) #compute pneumonia cases by summing the total number of 1's\n    n_fold_normal_cases.append (len(fold_seq[j]) - n_fold_pneumonia_cases [j])                       # subtract from total to get normal cases\n    print(\"Oversampled Train split %d - Total: %d, Normal: %d, Pneumonia: %d\" % (j+1, len(fold_seq[j]), n_fold_normal_cases[j], n_fold_pneumonia_cases[j]))","96986ccc":"#Saving The splits\nsplit_df = pd.DataFrame(columns=['img', 'class', 'fold'])\ncount = 0\nfor i in range(n_folds):\n    fold_num = i+1\n    for j in range(len(fold_seq[i])):\n        img_idx = fold_seq[i][j][0]\n        class_idx =  fold_seq[i][j][1]\n        split_df.loc[count] = [img_idx] + [class_idx] + [fold_num]\n        count +=1\nsplit_df.to_csv(\"Split.csv\", index=False)","59cbf0f7":"# Create a matplotlib plot of the images\nfig, ax = plt.subplots(2, 5, figsize=(30,10))\nfor x in range(2):\n    # set the boundaries for getting idx for NORMAL\n    # since the images are loaded such that normal images\n    # are loaded first, it always starts from index 0.\n    # the ending index is computed based on the number of\n    # training normal images\n    if x == 0:\n        low = 0\n        high = dataset_normal_cases # high is exclusive\n    # get the boundary indices for PNEUMONIA\n    else:\n        low = dataset_normal_cases\n        high = len(dataset_seq)\n    for y in range(5):\n        # select a random image\n        sample_idx = np.random.randint(low, high)\n        # read the image\n        sample = cv2.imread(dataset_seq[sample_idx][0])\n        # display image\n        ax[x, y].imshow(sample, cmap='gray')\n        # set title of plots\n        ax[x, y].set_title(\"Normal\" if x == 0 else \"Pneumonia\")\nplt.show()\n","d9ab0a5d":" <div id='KSplit'>  <\/div>  \n \n# Part 1.2 K-Fold Splitting  \n\n\n\nWe split our dataset into 4 folds. Each fold contains 1308 data, and will be trained by different models in our later chapters.  \n\n* Xception: Trained on Fold 2,3,4; Validated on Fold 1\n* InceptionResNetV2: Trained on Fold 1,3,4; Validated on Fold 2\n* InceptionV3: Trained on Fold 1,2,4; Validated on Fold 3  \n* DenseNet: Trained on Fold 1,2,3; Validated on Fold 4  \n  \nWith a 4-fold splitting and training, we can train on 75% of the data each time and validate on 25% of the data, which is a more reasonable split. Besides, using different models enable us to capture different features from the CXR images due to different combinations of convolution, pooling and flattening layers.\n","8e43e656":"<div id='Oversampling'><\/div>  \n\n# Part 1.3 Oversampling of Minority Data  \n\n\nWe can observe that our dataset has more pneumonia cases than normal cases (which is a bit weird because normal CXRs should be much easier to obtain). Anyways we try to oversample Normal CXRs randomly so that we can obtain a more balaced dataset for training. One concern of this method is that we may be biased against features of normal CXRs, but this issue can be solved by the image augmentator we implement in the next chapters. \n","c8324a1b":"<div id='Save'> <\/div>  \n\n# Part 1.4 Saving the Splitted Dataset    \n\n\nSaving the splitted dataset is important in our case. This can prevent data leak and biasness during training and ensembling caused by the random splitting process. \n    \n","5c7de2e8":"# CodeBlue's Solution to Kaggle CXR Pneumonia Classification   \n# Highest Public Score: 0.96434  \n\n\nTeam Composition:  \n\nWong Tsz Him (Leader)  \nMatilde Biaconi, Prachi Shah (Members)  \n  \n## Content:\n* ->  Chapter 1 - Data Pre-processing  \n* Chapter 2 - The Xception Model  \n* Chapter 3 - The InceptionResNetV2 Model  \n* Chapter 4 - The InceptionV3 Model  \n* Chapter 5 - The DenseNet Model  \n* Chapter 6 - Ensemble Learning and Final Output  \n\nKey Changes and Features:  \n1. K-Fold Splitting and Cross Validation (Chapter 1)\n2. Minority Data Oversampling (Chapter 1)\n3. New Image Augmentators (Chapter 2,3,4,5)\n4. Transfer Learning (Xception, InceptionResNetV2, InceptionV3, DenseNet201) (Chapter 2,3,4,5)\n5. New Functions and Hyperparameters (NAdam, Focal_Loss) (Chapte 2,3,4,5)    \n6. Ensemble Learning (Voting, Sum, Neural Network Ensembling) (Chapter 6)\n\n\n\n","22e38921":"<div id=\"ReadDataset\"> <\/div>  \n\n# Part 1.1 Reading The Dataset  \n \n\nHere we used a new function combine_dataset(). It allows us to combine the train folder and val folder in one combined dataset, so that we can perform K-fold Splitting later. The resplitting of the dataset is extremely important in this competition, as the original training data and validation data size is 5016 and 216 respectively, which is very uneven. ","f8f07bb5":"<div id=\"visual\"><\/div>  \n\n# Part 1.5 Visualize our Dataset  \n\nFinally, we visualize some of our noraml and pneumonia images.","43e1a4c4":"# Chapter 1 - Data Pre-processing  \n\n\nIn this chapter, we are going to process our data, which can then be used to train up our models in the later chapters.  \n  \nInput: GameTei2020 Folder (Containing labeled Train and Validation dataset, and unlabeled Test dataset)  \nOutput: Split.csv (Store the splitting information of data)  \n  \n[Part 1.1 Reading The Datase](#ReadDataset)  \n[Part 1.2 K-Fold Splitting](#KSplit)  \n[Part 1.3 Oversampling of Minority Data](#Oversampling)  \n[Part 1.4 Saving the Splitted Dataset](#Save)  \n[Part 1.5 Visualize our Dataaset](#visual) \n"}}