{"cell_type":{"72f8810d":"code","ac6bdb46":"code","1b8ef2d5":"code","15f7bb7c":"code","d927735f":"code","e0492b94":"code","8c7caf8e":"code","55e4c4eb":"code","465a2b78":"code","c60a2953":"code","678b8aa8":"code","8f1a2116":"code","90895033":"code","3d579333":"code","d5bfc1df":"code","9451c406":"code","a053ca07":"code","65434603":"code","c6870cf1":"code","85ad0cda":"code","7b346c86":"code","cb25a906":"code","6646694c":"code","69756686":"code","87b320c8":"code","247b2571":"code","dd3a9b78":"code","52ac07a1":"code","93b48f13":"markdown"},"source":{"72f8810d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","ac6bdb46":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1b8ef2d5":"data = pd.read_csv('\/kaggle\/input\/used-bikes-prices-in-india\/Used_Bikes.csv')\ndata.head()","15f7bb7c":"# Shape , Missing values\nprint(\"Data shape is : \",data.shape)\ndata.isna().sum()","d927735f":"## Correlation : High correlation between Power\/Price\ndata.corr()","e0492b94":"plt.figure(figsize = (12,8))\nsns.boxplot(x = 'price' , data = data)","8c7caf8e":"def get_iqr_values(df , column_name):\n    median = df[column_name].mean()\n    q1 = df[column_name].quantile(0.25)\n    q3 = df[column_name].quantile(0.75)\n    iqr = q3 - q1\n    maximum_quantile = q3 + (1.5*iqr)\n    minimum_quantile = q1 - (1.5*iqr)\n    return median , q1 , q3 , maximum_quantile , minimum_quantile\n\ndef remove_outliers(df,column_name):\n    _,_,_,maximum,minimum = get_iqr_values(df,column_name)\n    df_out = df[(df[column_name] > minimum) & (df[column_name] < maximum)]\n    return df_out","55e4c4eb":"df_out = remove_outliers(data,'price')\nnumber_outliers = data.shape[0] - df_out.shape[0]\nprint('Number of outliers : ',number_outliers)","465a2b78":"df_out.head()","c60a2953":"categorical_features = ['bike_name' , 'city' , 'owner' , 'brand']\nfor col in categorical_features:\n    print(col,'-->',df_out[col].nunique())\n","678b8aa8":"val_count_bike = df_out['bike_name'].value_counts()\ncondition_reduction = val_count_bike[val_count_bike>30]\ndf_out['bike_name'] = df_out['bike_name'].apply(lambda x : x if x in  condition_reduction else 'others' )","8f1a2116":"val_count_city = df_out['city'].value_counts()\ncond_reduction = val_count_city[val_count_city>100]\ndf_out['city'] = df_out['city'].apply(lambda x : x if x in  cond_reduction else 'others' )","90895033":"val_count_brand = df_out['brand'].value_counts()\ncond_reduction_brand = val_count_brand[val_count_brand>500]\ndf_out['brand'] = df_out['brand'].apply(lambda x : x if x in  cond_reduction_brand else 'others' )","3d579333":"def owner_change(owner_type):\n    if owner_type == 'First Owner':\n        return 'Yes'\n    else:\n        return 'No'\ndf_out['owner'] = df_out['owner'].apply(lambda x : owner_change(x) )","d5bfc1df":"df_out.head()","9451c406":"df_out = pd.get_dummies(df_out , columns =['bike_name' , 'city' , 'owner' , 'brand'])\ndf_out.head()","a053ca07":"X = df_out.drop(['price'] , axis = 'columns')\ny = df_out['price']","65434603":"X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.3 , random_state = 42)","c6870cf1":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\ny_train = y_train.values\ny_test = y_test.values","85ad0cda":"## Models :\nfrom sklearn.linear_model import LinearRegression ,Lasso\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor \nfrom sklearn.metrics import mean_absolute_error","7b346c86":"def train_model(model):\n    model.fit(X_train , y_train)\n    preds = model.predict(X_test)\n    loss = mean_absolute_error(y_test , preds)\n    return loss\n\ndef DataFrame_loss(dict_of_models):\n    loss_models = []\n    for name , model in dict_of_models.items():\n        loss = train_model(model)\n        loss_models.append(loss)\n    data = {'Models' : dict_of_models.keys(),'Loss' : loss_models}\n    return pd.DataFrame(data = data)","cb25a906":"models_dict = {'KNN' : KNeighborsRegressor() ,\n               'Linear Regression' : LinearRegression(),\n               'Lasso' : Lasso(), \n               'RandomForest' : RandomForestRegressor(),\n               'AdaBoost' : AdaBoostRegressor(),\n               'Xgboost' : XGBRegressor(),}\nDataFrame_loss(models_dict)","6646694c":"## ANN\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout","69756686":"def build_model_regression(hidden_units1 , hidden_units2):\n    model = Sequential()\n    model.add(Dense(hidden_units1 , activation = \"relu\"))\n    Dropout(0.2)\n    model.add(Dense(hidden_units2 , activation = \"relu\"))\n    Dropout(0.2)\n    model.add(Dense(1, activation='linear'))\n    return model\n\nmodel = build_model_regression(64,64)","87b320c8":"# loss function\n\nmodel.compile(\n    loss='mean_absolute_error', \n    optimizer='adam' ,\n    metrics=['mae']\n)\n# train the model\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs=100, \n    batch_size=64,\n    validation_split=0.2\n)","247b2571":"print(history.history.keys())\ndef plot_history(history , key_history):\n    plt.figure(figsize = (12,8))\n    plt.plot(history.history[key_history])\n    plt.plot(history.history['val_'+key_history])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(key_history)\n    plt.legend([key_history , 'val_'+key_history])","dd3a9b78":"plot_history(history , 'mae')","52ac07a1":"plot_history(history , 'loss')","93b48f13":"<h1>Thank You !<\/h1>"}}