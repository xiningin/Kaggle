{"cell_type":{"57a8e95b":"code","0b34be15":"code","bede0761":"code","27dd85c8":"code","30ba8e0f":"code","15b0845b":"code","661389cf":"code","6187fabe":"code","24f81e55":"code","095adbcc":"code","e5ed4fda":"code","0e89a474":"code","ae49acf1":"code","74bdd494":"code","9e2e008a":"code","8647a974":"code","d3db0c22":"code","e60226ab":"code","5f86490e":"code","3e964ee2":"code","6890283f":"code","5e1671a6":"code","b0784504":"code","8da3b027":"code","ccc5bd8f":"code","cf9cff9e":"code","017e956f":"code","32a9d582":"code","e63f80bf":"code","906ba67a":"code","d9fcb3dd":"code","fe9182b9":"code","9c2c8b30":"code","fb71192e":"code","ce0955c9":"code","9761ef70":"code","bafce442":"code","a47f50fb":"code","39836292":"code","014adac9":"code","abbf2b9a":"code","6f5948a2":"code","031d797f":"code","a90af279":"code","25948859":"code","30e24c33":"code","3a9ec9c6":"code","cad1e91d":"code","4357968a":"code","5e2870f1":"code","01e8d0e8":"code","764c2634":"code","d46579fa":"code","009f9e92":"code","b39c1b65":"code","84de11fe":"code","5f16ca6f":"code","9d21eb35":"markdown","70968300":"markdown","478433b7":"markdown","f5dfc527":"markdown","c14b17ae":"markdown","730504d8":"markdown","10b4c447":"markdown","afbba0a9":"markdown"},"source":{"57a8e95b":"import numpy as np, pandas as pd\nimport json\nimport ast \nfrom textblob import TextBlob\nimport nltk\nimport torch\nimport pickle\nfrom scipy import spatial\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom nltk import Tree\nen_nlp = spacy.load('en')\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer","0b34be15":"# !conda update pandas --y","bede0761":"train = pd.read_csv(\"..\/input\/sentence-embedding\/train.csv\",encoding=\"latin-1\")","27dd85c8":"train.shape","30ba8e0f":"with open(\"..\/input\/infercent-version2\/dict_embeddings1_v2.pickle\", \"rb\") as f:\n    d1 = pickle.load(f)","15b0845b":"with open(\"..\/input\/infercent-version2\/dict_embeddings2_v2.pickle\", \"rb\") as f:\n    d2 = pickle.load(f)","661389cf":"dict_emb = dict(d1)\ndict_emb.update(d2)","6187fabe":"len(dict_emb)","24f81e55":"del d1, d2","095adbcc":"def get_target(x):\n    idx = -1\n    for i in range(len(x[\"sentences\"])):\n        if x[\"text\"] in x[\"sentences\"][i]: idx = i\n    return idx","e5ed4fda":"train.head(3)","0e89a474":"train.shape","ae49acf1":"train.dropna(inplace=True)","74bdd494":"train.shape","9e2e008a":"def process_data(train):\n    \n    print(\"step 1\")\n    train['sentences'] = train['context'].apply(lambda x: [item.raw for item in TextBlob(x).sentences])\n    \n    print(\"step 2\")\n    train[\"target\"] = train.apply(get_target, axis = 1)\n    \n    print(\"step 3\")\n    train['sent_emb'] = train['sentences'].apply(lambda x: [dict_emb[item][0] if item in\\\n                                                           dict_emb else np.zeros(4096) for item in x])\n    print(\"step 4\")\n    train['quest_emb'] = train['question'].apply(lambda x: dict_emb[x] if x in dict_emb else np.zeros(4096) )\n        \n    return train   ","8647a974":"train = process_data(train[50000:])","d3db0c22":"train.head(3)","e60226ab":"def cosine_sim(x):\n    li = []\n    for item in x[\"sent_emb\"]:\n        li.append(spatial.distance.cosine(item,x[\"quest_emb\"][0]))\n    return li   ","5f86490e":"def pred_idx(distances):\n    return np.argmin(distances)   ","3e964ee2":"def predictions(train):\n    \n    train[\"cosine_sim\"] = train.apply(cosine_sim, axis = 1)\n    train[\"diff\"] = (train[\"quest_emb\"] - train[\"sent_emb\"])**2\n    train[\"euclidean_dis\"] = train[\"diff\"].apply(lambda x: list(np.sum(x, axis = 1)))\n    del train[\"diff\"]\n    \n    print(\"cosine start\")\n    \n    train[\"pred_idx_cos\"] = train[\"cosine_sim\"].apply(lambda x: pred_idx(x))\n    train[\"pred_idx_euc\"] = train[\"euclidean_dis\"].apply(lambda x: pred_idx(x))\n    \n    return train\n    ","6890283f":"train.columns","5e1671a6":"#train.loc[0:10]","b0784504":"predicted = predictions(train)","8da3b027":"predicted.head(3)","ccc5bd8f":"predicted[\"cosine_sim\"][50000]","cf9cff9e":"predicted[\"euclidean_dis\"][50000]","017e956f":"def accuracy(target, predicted):\n    \n    acc = (target==predicted).sum()\/len(target)\n    \n    return acc","32a9d582":"print(accuracy(predicted[\"target\"], predicted[\"pred_idx_euc\"]))","e63f80bf":"print(accuracy(predicted[\"target\"], predicted[\"pred_idx_cos\"]))","906ba67a":"predicted.to_csv(\"train_detect_sent.csv\", index=None)","d9fcb3dd":"predicted.iloc[5207,:]","fe9182b9":"ct,k = 0,0\nfor i in range(predicted.shape[0]):\n    if predicted.iloc[i,10] != predicted.iloc[i,5]:\n        k += 1\n        if predicted.iloc[i,11] == predicted.iloc[i,5]:\n            ct += 1","9c2c8b30":"ct, k","fb71192e":"label = []\nfor i in range(predicted.shape[0]):\n    if predicted.iloc[i,10] == predicted.iloc[i,11]:\n        label.append(predicted.iloc[i,10])\n    else:\n        label.append((predicted.iloc[i,10],predicted.iloc[i,10]))","ce0955c9":"predicted = pd.read_csv(\"train_detect_sent.csv\").reset_index(drop=True)","9761ef70":"doc = en_nlp(predicted.iloc[0,1])","bafce442":"predicted.iloc[0,1]","a47f50fb":"predicted.iloc[0,2]","39836292":"def to_nltk_tree(node):\n    if node.n_lefts + node.n_rights > 0:\n        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n    else:\n        return node.orth_","014adac9":"[to_nltk_tree(sent.root).pretty_print()  for sent in en_nlp(predicted.iloc[0,2]).sents]","abbf2b9a":"[to_nltk_tree(sent.root) .pretty_print() for sent in doc.sents][5]","6f5948a2":"for sent in doc.sents:\n    roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n    print(roots)","031d797f":"def match_roots(x):\n    question = x[\"question\"].lower()\n    sentences = en_nlp(x[\"context\"].lower()).sents\n    \n    question_root = st.stem(str([sent.root for sent in en_nlp(question).sents][0]))\n    \n    li = []\n    for i,sent in enumerate(sentences):\n        roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n\n        if question_root in roots: \n            for k,item in enumerate(ast.literal_eval(x[\"sentences\"])):\n                if str(sent) in item.lower(): \n                    li.append(k)\n    return li","a90af279":"predicted[\"question\"][21493]","25948859":"predicted[\"context\"][21493]","30e24c33":"predicted[\"root_match_idx\"] = predicted.apply(match_roots, axis = 1)","3a9ec9c6":"predicted[\"root_match_idx_first\"]= predicted[\"root_match_idx\"].apply(lambda x: x[0] if len(x)>0 else 0)","cad1e91d":"(predicted[\"root_match_idx_first\"]==predicted[\"target\"]).sum()\/predicted.shape[0]","4357968a":"predicted.to_csv(\"train_detect_sent2_v2.csv\", index=None)","5e2870f1":"predicted[(predicted[\"sentences\"].apply(lambda x: len(ast.literal_eval(x)))<11) &  (predicted[\"root_match_idx_first\"]>10)]       \n\n","01e8d0e8":"len(ast.literal_eval(predicted.iloc[21493,4]))","764c2634":"question = predicted[\"question\"][21493].lower()\nsentences = en_nlp(predicted[\"context\"][21493].lower()).sents\n    \nquestion_root = st.stem(str([sent.root for sent in en_nlp(question).sents][0]))\n    \nli = []\nfor i,sent in enumerate(sentences):\n    roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n    print(roots)\n\n    if question_root in roots: li.append(i)","d46579fa":"ast.literal_eval(predicted[\"sentences\"][21493])","009f9e92":"predicted[\"context\"][21493]","b39c1b65":"en_nlp = spacy.load('en')\nsentences = en_nlp(predicted[\"context\"][21493].lower()).sents","84de11fe":"for item in sentences:\n    print(item)","5f16ca6f":"TfidfVectorizer(predicted[\"sentences\"][0], ngram_range=(1,2))","9d21eb35":"## Accuracy","70968300":"### Accuracy for Cosine Similarity","478433b7":"### Root Match","f5dfc527":"### Loading Embedding dictionary","c14b17ae":"## Predicted Cosine & Euclidean Index","730504d8":"### Combining Accuracy","10b4c447":"## Data Processing","afbba0a9":"### Accuracy for  euclidean Distance"}}