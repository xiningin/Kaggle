{"cell_type":{"baed2adc":"code","de75b718":"code","bdedcbc9":"code","c9d5693d":"code","c9d6f156":"code","65570603":"code","61715e47":"code","7befea86":"code","76df43b1":"code","b1944416":"code","6fc12e6f":"code","9a1d1fb0":"code","748107f7":"code","6d4f0256":"code","8bd0d5ad":"code","05a63a26":"code","c550d1e3":"code","7d745077":"code","6eba1510":"markdown","a1bc38cd":"markdown","0c63bca5":"markdown"},"source":{"baed2adc":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\n\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.simplefilter('ignore')","de75b718":"def seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","bdedcbc9":"args = Namespace(\n    seed=21,\n    folds=5,\n    workers=4,\n    samples=200000,\n    data_path=Path(\"..\/input\/ubiquant-parquet\/\"),\n)\nseed_everything(args.seed)","c9d5693d":"train = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\nassert train.isnull().any().sum() == 0, \"null exists.\"","c9d6f156":"if args.samples is not None:\n    train = train[-args.samples:].reset_index(drop=True)\n    gc.collect()\ntrain.shape","65570603":"cont_feats = [f'f_{i}' for i in range(300)]\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[cont_feats] = scaler.fit_transform(train[cont_feats])\ntrain = reduce_mem_usage(train)\ngc.collect()\nlen(cont_feats)","61715e47":"display(train.head())","7befea86":"class UBIQUANT_DATASET(Dataset):\n    def __init__(self, df_data, mode='train'):\n        self.mode = mode\n        self.ids = np.array(df_data['investment_id'].values.tolist(), dtype=np.int64)\n        self.vals = np.array(df_data.iloc[:, 4:].values.tolist(), dtype=np.float64)\n        if self.mode != 'test':\n            self.targets = np.array(df_data['target'].values, dtype=np.float64)\n        self.len = df_data.shape[0]\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        ids_out = self.ids[index]\n        vals_out = self.vals[index]\n        if self.mode != 'test':\n            targets_out = self.targets[index]\n            return ids_out, vals_out, targets_out\n        else:\n            return ids_out, vals_out","76df43b1":"# copy from: https:\/\/www.kaggle.com\/elcaiseri\/pytorch-optiver-realized-volatility-baseline\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb = nn.Embedding(3774, 64)\n        self.emb_drop = nn.Dropout(0.1)\n        \n        self.bn1 = nn.BatchNorm1d(300)\n        self.lin1 = nn.Linear(64+300, 32)\n        self.lin2 = nn.Linear(32, 128)\n        self.lin3 = nn.Linear(128, 64)\n        self.lin4 = nn.Linear(64, 32)\n        self.lin_drop = nn.Dropout(0.25)\n        self.lin5 = nn.Linear(32, 1)    \n\n    def forward(self, x_cat, x_cont):\n        x1 = self.emb(x_cat)\n        x1 = self.emb_drop(x1)\n        \n        x2 = self.bn1(x_cont)\n\n        x = torch.cat([x1, x2], 1)\n        x = swish(self.lin1(x))\n        x = swish(self.lin2(x))\n        x = swish(self.lin3(x))\n        x = swish(self.lin4(x))\n        x = self.lin5(x)\n        \n        return x\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","b1944416":"def train_fn(dataloaders, fold_id):\n    \n    model = SimpleMLP().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), \n                           lr=1e-3)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     factor=0.1, \n                                                     patience=1, \n                                                     mode='min')\n    \n    epochs = 8\n    num_train_examples = len(dataloaders['train'])\n    num_valid_examples = len(dataloaders['valid'])\n\n    losses = []\n    best_loss = np.inf\n\n    for e in range(epochs):\n        # train\n        model.train()\n        train_loss = 0\n        for i, (ids, vals, targets) in enumerate(dataloaders['train']):\n            ids = ids.to(device)\n            vals = vals.to(device=device, dtype=torch.float)\n            targets = targets.unsqueeze(1).to(device, dtype=torch.float)\n\n            yhat = model(ids, vals)\n            loss = loss_fn(yhat, targets)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_epoch_loss = train_loss \/ num_train_examples\n\n        # valid\n        model.eval()\n        valid_preds = list()\n        valid_loss = 0\n        with torch.no_grad():\n            for i, (ids, vals, targets) in enumerate(dataloaders['valid']):\n                ids = ids.to(device)\n                vals = vals.to(device=device, dtype=torch.float)\n                targets = targets.unsqueeze(1).to(device, dtype=torch.float)\n\n                yhat = model(ids, vals)\n                val_loss = loss_fn(yhat, targets)\n                valid_loss += val_loss.item()\n                valid_preds.extend(yhat.detach().cpu().numpy().flatten())\n        valid_epoch_loss = valid_loss \/ num_valid_examples\n\n        # change lr\n        scheduler.step(valid_epoch_loss)\n\n        # oof\n        oof = df_valid[['target']].copy()\n        oof['pred'] = valid_preds\n        score = oof['pred'].corr(oof['target'])\n\n        # print score\n        print(f\"Epoch {e}, LR: {optimizer.param_groups[0]['lr']}\")\n        print(f\"train loss: {train_epoch_loss:.8f}, valid loss {valid_epoch_loss:.8f}, pearson score: {score:.6f}\")\n        losses.append((train_epoch_loss, valid_epoch_loss))\n\n        # save model\n        if best_loss > valid_epoch_loss:\n            torch.save(model.state_dict(), f'simple_mlp_model_{fold_id}.pth')\n            print(f'-- loss from {best_loss:.8f} to {valid_epoch_loss:.8f}, model saved')\n            best_loss = valid_epoch_loss\n        print()\n    \n    del model\n    gc.collect()\n    \n    return losses, oof","6fc12e6f":"oof_list = list()\n\nkfold = GroupKFold(n_splits=5)\nfor fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train, train['target'], train['time_id'])):\n    \n    print(f'Training Fold: {fold_id}\\n')\n    \n    df_train = train.iloc[trn_idx]\n    df_valid = train.iloc[val_idx]\n    \n    train_set = UBIQUANT_DATASET(df_train, mode='train')\n    valid_set = UBIQUANT_DATASET(df_valid, mode='valid')\n    \n    dataloaders = {\n        'train': DataLoader(train_set, batch_size=1024, num_workers=4, pin_memory=True, shuffle=True),\n        'valid': DataLoader(valid_set, batch_size=1024, num_workers=4, pin_memory=True, shuffle=False)\n    }\n    \n    _, oof = train_fn(dataloaders, fold_id)\n    oof_list.append(oof)\n    \n    del df_train, df_valid, train_set, valid_set, oof\n    gc.collect()","9a1d1fb0":"oof = pd.concat(oof_list)\nprint('oof pearson score:', oof['pred'].corr(oof['target']))","748107f7":"joblib.dump(scaler, 'minmaxscaler.pkl')","6d4f0256":"class UBIQUANT_DATASET_TEST(Dataset):\n    def __init__(self, df_data):\n        self.ids = np.array(df_data['investment_id'].values.tolist(), dtype=np.int64)\n        self.vals = np.array(df_data.iloc[:, 1:].values.tolist(), dtype=np.float64)\n        self.len = df_data.shape[0]\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        ids_out = self.ids[index]\n        vals_out = self.vals[index]\n        return ids_out, vals_out","8bd0d5ad":"scaler = joblib.load('.\/minmaxscaler.pkl')","05a63a26":"models_list = list()\nfor fold_id in range(5):\n    model = SimpleMLP().to(device)\n    model.load_state_dict(torch.load(f'.\/simple_mlp_model_{fold_id}.pth'))\n    models_list.append(model)","c550d1e3":"import ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test()","7d745077":"\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = test_df.drop([\"row_id\"], axis=1)\n    test_df[cont_feats] = scaler.transform(test_df[cont_feats])\n    test_set = UBIQUANT_DATASET_TEST(test_df)\n    test_dataloader = DataLoader(test_set, batch_size=1024, \n                                 num_workers=2, pin_memory=True, shuffle=False)\n    y_preds = list()\n    with torch.no_grad():\n        for i, (ids, vals) in enumerate(test_dataloader):\n            ids = ids.to(device)\n            vals = vals.to(device=device, dtype=torch.float)\n            y_pred = np.zeros((len(ids), ))\n            for model in models_list:\n                model.eval()\n                y_pred += model(ids, vals).detach().cpu().numpy().flatten() \/ 5\n            y_preds.extend(y_pred)\n    sample_prediction_df['target'] = y_preds\n    env.predict(sample_prediction_df)\n    display(sample_prediction_df)","6eba1510":"## References :-\n- Thanks to [@HENG ZHENG](https:\/\/www.kaggle.com\/hengzheng) for [training](https:\/\/www.kaggle.com\/hengzheng\/nn-mlp-5folds-training\/) and [inference](https:\/\/www.kaggle.com\/hengzheng\/nn-mlp-5folds\/) notebooks.\n- I have slightly changed the code so that it can be run on Kaggle itself.\n- I have used [Parquet dataset ](https:\/\/www.kaggle.com\/robikscube\/ubiquant-parquet). Thanks to [@Rob Mulla](https:\/\/www.kaggle.com\/robikscube)\n- https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/301752\n- https:\/\/www.kaggle.com\/valleyzw\/ubiquant-lgbm-baseline\/","a1bc38cd":"## Training","0c63bca5":"## Inference"}}