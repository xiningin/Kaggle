{"cell_type":{"ad61a02e":"code","6d898218":"code","76a6e832":"code","bdcaaf32":"code","d3ba2117":"code","224d0582":"code","50674c83":"code","4019305d":"code","b1be678a":"code","59602a1d":"code","c37f1d0e":"code","6562af18":"code","cfab92ac":"code","288d362f":"code","aad7bad7":"code","9ca423d4":"code","81f377dc":"code","ce56810c":"code","4c092170":"code","b3dc90b4":"code","ed65c88c":"code","a2289ad6":"code","8b0e0c02":"code","854c81a1":"code","f90003f5":"code","c94f4c3f":"code","2d3fd889":"code","932f25a2":"code","cbec78ce":"code","6b91d386":"code","c6329d72":"code","bf03e621":"code","75f0f22f":"code","ef0eb8e3":"code","824916ed":"code","3f588317":"code","557b38a7":"code","080b3805":"code","f03bb2d7":"code","0d87baed":"code","47f3b85a":"code","650907b9":"code","3b1e012a":"code","80e956e1":"code","aaf5793f":"code","f63949ef":"code","cf37b5d7":"code","0deb046e":"code","408019dd":"code","630cb4a7":"code","75a2461c":"code","ccf10257":"markdown","45434238":"markdown","2993372a":"markdown","a6dc5ed2":"markdown","514dec2f":"markdown","30a98b69":"markdown","49d3baef":"markdown","4a04c3e8":"markdown","cbb4fd2c":"markdown","ed5b1083":"markdown","edb2b5fe":"markdown","0fc67161":"markdown","c05984f4":"markdown","944bbbcc":"markdown","99826351":"markdown","d09938e2":"markdown","87a0a582":"markdown","4ac91f2b":"markdown","777f6d9b":"markdown","cbbec87b":"markdown","5ed02fc5":"markdown","bc3f7c90":"markdown","f76dff8f":"markdown","104e2ba2":"markdown","20bfc5dd":"markdown","17905020":"markdown","fd9ff221":"markdown","9c7e9cf9":"markdown","2aeaa963":"markdown","fdbc6773":"markdown","f48dc19b":"markdown"},"source":{"ad61a02e":"# Statistics\nimport pandas as pd\nimport numpy as np\nimport math as mt\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npx.defaults.width = 1200\npx.defaults.height = 800\n# plotly.io Settings for both plotly.graph_objects and plotly.express\npio.templates.default = \"plotly_white\" # \"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"\n\"\"\"\npio.kaleido.scope.default_format = 'svg'\npio.kaleido.scope.default_scale = 1\n\"\"\"\n\n# Data Preprocessing - Standardization, Encoding, Imputation\nfrom sklearn.preprocessing import StandardScaler # Standardization\nfrom sklearn.preprocessing import Normalizer # Normalization\nfrom sklearn.preprocessing import OneHotEncoder # One-hot Encoding\nfrom sklearn.preprocessing import OrdinalEncoder # Ordinal Encoding\nfrom category_encoders import MEstimateEncoder # Target Encoding\nfrom sklearn.preprocessing import PolynomialFeatures # Create Polynomial Features\nfrom sklearn.impute import SimpleImputer # Imputation\n\n# Exploratory Data Analysis - Feature Engineering\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.decomposition import PCA\n\n# Modeling - ML Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# Modeling - Algorithms\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n#from catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# ML - Evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# ML - Tuning\nimport optuna\n#from sklearn.model_selection import GridSearchCV\n\n# Settings\n# Settings for Seaborn\nsns.set_theme(context='notebook', style='ticks', palette=\"bwr_r\", font_scale=0.7, rc={\"figure.dpi\":240, 'savefig.dpi':240})","6d898218":"import os\nkaggle_project = 'seattle'\n# Import dataset from local directory '.\/data' or from Kaggle\ndata_dir = ('.\/data\/201601' if os.path.exists('data') else f'\/kaggle\/input\/{kaggle_project}')\n\n# print all files in data_dir\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Import three datasets\nreviews = pd.read_csv(f'{data_dir}\/reviews.csv')\ncalendar = pd.read_csv(f'{data_dir}\/calendar.csv')\nlistings = pd.read_csv(f'{data_dir}\/listings_kfold.csv') if os.path.exists(f'{data_dir}\/listings_kfold.csv') else pd.read_csv(f'{data_dir}\/listings.csv')","76a6e832":"def generate_listings_kfold():\n    # Mark the train dataset with kfold = 5\n    listings = pd.read_csv(f'{data_dir}\/listings.csv')\n    if os.path.exists(f'{data_dir}\/listings_kfold.csv'):\n        os.remove(f'{data_dir}\/listings_kfold.csv')\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(X=listings)):\n        listings.loc[valid_idx, \"kfold\"] = fold\n\n    listings.to_csv(f'listings_kfold.csv', index=False)\n\ngenerate_listings_kfold()\nlistings = pd.read_csv(f'listings_kfold.csv')","bdcaaf32":"# After assigning kfold\n# If error, run the above function then re-load listings_kfold.csv\nlistings.loc[:, ['id', 'kfold']].head()","d3ba2117":"# Define sheet id and base url\nsheet_id = \"1M_qah-ym6O8vDcSmoKAP-lbZRPHUey83R_DJaW3LXfs\"\nbase_url = f\"https:\/\/docs.google.com\/spreadsheets\/d\/{sheet_id}\/gviz\/tq?tqx=out:csv&sheet=\"\n\n# Load metadata for three datasets\nlistings_metadata = pd.read_csv(base_url+\"listings\")\ncalendar_metadata = pd.read_csv(base_url+\"calendar\")\nreviews_metadata = pd.read_csv(base_url+\"reviews\")","224d0582":"class ETL_pipeline:\n    def __init__(self, data_frame):\n        self.df = data_frame\n    \n    # Data type transformation\n    def _transformation(self, data_frame):\n        df = data_frame\n        # Convert dollar columns from object to float\n        # Remove '$' and ','\n        dollar_cols = ['price', 'weekly_price', 'monthly_price', 'extra_people', 'security_deposit', 'cleaning_fee']\n        for dollar_col in dollar_cols:\n            df[dollar_col] = df[dollar_col].replace('[\\$,]', '', regex=True).astype(float)\n        # Convert dollar columns from object to float\n        # Remove '%'\n        percent_cols = ['host_response_rate', 'host_acceptance_rate']\n        for percent_col in percent_cols:\n            df[percent_col] = df[percent_col].replace('%', '', regex=True).astype(float)\n\n        # Replace the following values in property_type to Unique space due to small sample size\n        unique_space = [\"Barn\",\n        \"Boat\",\n        \"Bus\",\n        \"Camper\/RV\",\n        \"Treehouse\",\n        \"Campsite\",\n        \"Castle\",\n        \"Cave\",\n        \"Dome House\",\n        \"Earth house\",\n        \"Farm stay\",\n        \"Holiday park\",\n        \"Houseboat\",\n        \"Hut\",\n        \"Igloo\",\n        \"Island\",\n        \"Lighthouse\",\n        \"Plane\",\n        \"Ranch\",\n        \"Religious building\",\n        \"Shepherd\u2019s hut\",\n        \"Shipping container\",\n        \"Tent\",\n        \"Tiny house\",\n        \"Tipi\",\n        \"Tower\",\n        \"Train\",\n        \"Windmill\",\n        \"Yurt\",\n        \"Riad\",\n        \"Pension\",\n        \"Dorm\",\n        \"Chalet\"]            \n        df.property_type = df.property_type.replace(unique_space, \"Unique space\", regex=True)\n\n        # Convert 't', 'f' to 1, 0\n        tf_cols = ['host_is_superhost', 'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n        for tf_col in tf_cols:\n            df[tf_col] = df[tf_col].replace('f', 0, regex=True)\n            df[tf_col] = df[tf_col].replace('t', 1, regex=True)\n        \n        return df\n    \n    # Parse listings\n    def parse_listings(self):\n        \"\"\"Parse listings.\n        \"\"\"\n        df = self.df\n        df = self._transformation(df)\n        return df\n    \n    def parse_reviews(self):\n        \"\"\"Parse reviews.\n        \"\"\"\n        df = self.df\n        df.date = pd.to_datetime(df.date)\n        return df\n    \n    # Parse calendar\n    def parse_calender(self):\n        \"\"\"Paser calendar.\n        \"\"\"\n        df = self.df\n        # Convert date from object to datetime\n        df.date = pd.to_datetime(df.date)\n        # Convert price from object to float\n        # Convert '$' and ',' to ''\n        df.price = df.price.replace('[\\$,]', '', regex=True).astype(float)\n        \n        # Convert 't', 'f' to 1, 0\n        df['available'] = df['available'].replace('f', 0, regex=True)\n        df['available'] = df['available'].replace('t', 1, regex=True)\n\n        return df","50674c83":"# e.g. Before parsing\nlistings.loc[:4, ['id', 'price']]","4019305d":"listings = ETL_pipeline(listings).parse_listings()\nreviews = ETL_pipeline(reviews).parse_reviews()\ncalendar = ETL_pipeline(calendar).parse_calender()","b1be678a":"# e.g. After parsing\nlistings.loc[:4, ['id', 'price']]","59602a1d":"class EDA_demand:\n    def __init__(self):\n        pass\n    \n    def reviews_rate_vs_unavailability(self, period=30):\n        \"\"\"Calculate the booked listing from file calendar.\n\n        Args:\n            period (int): Positive integer. Default is 30.\n\n        Returns:\n            Pandas DataFrame.\n        \"\"\"\n        assert (0 < period <= 365) & isinstance(period, int), \"period must be an integer and greater than 0\"\n        self.period = period\n        \n        #\n        # Calculate review rate & unavailability\n        #\n\n        # reviews Rate: review \/ days\n        \"\"\"\n        SELECT \n            listing_id, \n            COUNT(listing_id) \/ DATEDIFF(20160104+1, MIN(date)) AS reviews_per_day\n        FROM reviews\n        GROUP BY listing_id\n        \"\"\"\n        # Extract the first reviews date for each listing\n        func = lambda df: pd.Series({'first_day': df.date.min()})\n        df_reviews_per_day = pd.DataFrame(reviews.groupby('listing_id').apply(func))\n        # Define last scraped date\n        last_scraped = listings.last_scraped.unique()[0]\n        last_scraped = pd.Timestamp(last_scraped)\n        df_reviews_per_day['last_day'] = last_scraped + pd.DateOffset(days=1)\n        # Calculate the datediff\n        df_reviews_per_day['datediff'] = df_reviews_per_day.last_day - df_reviews_per_day.first_day\n        df_reviews_per_day['datediff'] = df_reviews_per_day['datediff'].dt.days\n        # Calculate the reviews Rate\n        df_reviews_per_day['reviews_per_day'] = reviews.groupby('listing_id').size() \/ df_reviews_per_day['datediff']\n\n        \"\"\"\n        SELECT listing_id, SUM(IF(available = 0, 1, 0))\n        FROM calendar\n        WHERE DATEDIFF(date, 20160104) <= period\n        GROUP BY listing_id\n        \"\"\"\n        last_day = last_scraped + pd.DateOffset(days=period-1)\n        filter = calendar.date <= (last_day)\n        func = lambda df: pd.Series({f'unavailability_{period}_unscaled': sum(df.available == 0)}) # Scaling available to day scale\n        df_unavailability = pd.DataFrame(calendar[filter].groupby('listing_id').apply(func))\n        df_unavailability[f'unavailability_{period}'] = df_unavailability[f'unavailability_{period}_unscaled'] \/ period\n        #df_unavailability['first_day'] = last_scraped\n        #df_unavailability['last_day'] = last_day\n        self.df_unavailability = df_unavailability\n        \n        # Join two tables\n        df_unavailability_reviews = df_unavailability.join(df_reviews_per_day, how='left')\n        df_unavailability_reviews.reviews_per_day.fillna(value=0, inplace=True)\n        #df_unavailability_reviews.loc[:, [f'unavailability_{period}_unscaled', f'unavailability_{period}', 'reviews_per_day']]\n        \n        # Find outliers (unavailable rather than booked)\n        # Extrat quantiles\n        reviews_rate_25 = df_unavailability_reviews.reviews_per_day.quantile(q=0.25, interpolation='higher')\n        unavailability_75 = df_unavailability_reviews[f'unavailability_{period}'].quantile(q=0.75, interpolation='higher')\n        # Low reviews rate: 0.010376\n        filter1 = df_unavailability_reviews.reviews_per_day < reviews_rate_25\n        # High unavailability: 0.660274\n        filter2 = df_unavailability_reviews[f'unavailability_{period}'] > unavailability_75\n\n        outliers = df_unavailability_reviews[filter1 & filter2]\n        df_unavailability_reviews['demand'] = df_unavailability_reviews[f'unavailability_{period}_unscaled']\n        df_unavailability_reviews.loc[outliers.index, 'demand'] = period - df_unavailability_reviews.loc[outliers.index, 'demand']\n        \n        self.outliers = outliers\n        self.df_unavailability_reviews = df_unavailability_reviews\n        \n        return self.df_unavailability_reviews\n    \n    def plot(self, outliers=True):\n        \"\"\"Display plot or describe the relationship between reviews per day and unavailabilities to filter the outliers of demand.\n        \n        Args:\n            outlier (bool): Display outliers or not. Default is True\n            \n        Returns:\n            Plotly instance\n        \"\"\"\n        period = self.period\n        \n        if outliers is True:\n            idx = self.outliers.index\n            df = self.df_unavailability_reviews.loc[idx, :]\n        else:\n            idx = self.df_unavailability_reviews.index.drop(self.outliers.index)\n            df = self.df_unavailability_reviews.loc[idx, :]\n\n        assert df.shape[0] > 0, \"No records\"\n\n        fig = px.line(df, \n                      x=df.index, \n                      y=[f'unavailability_{period}', 'reviews_per_day'],\n                      color_discrete_sequence=['rgb(71, 92, 118, 0.9)', 'rgb(250, 211, 102, 0.9)']\n                     )\n        fig.update_layout(title=f'Unavailability per day vs. reviews per day<br>Outliers', xaxis_title='index', yaxis_title='Rate')\n\n        return fig","c37f1d0e":"class ML_pipeline:\n    \"\"\"ML Pipeline for listings.\n    \"\"\"\n    def __init__(self, data_frame, features, target, days=365):\n        \"\"\"\n        \n        Args:\n            data_frame (Pandas DataFrame): listings.\n            features (list): The Machine Learning features.\n            target (str): price\n            days (int): The days after 2016-01-04 for calculating demand.\n        \"\"\"\n        import warnings\n        warnings.filterwarnings(\"ignore\") # ignore target encoding warnings\n        \n        # Get demand\n        demand = EDA_demand().reviews_rate_vs_unavailability(days)\n        # The index will change to id\n        data_frame = data_frame.set_index('id').join(demand['demand'], how='inner')\n        \n        features.append(target)\n        data_frame = data_frame[features]\n        \n        # Encode amenities\n        data_frame = self._encode_amentities(data_frame)\n        data_frame.pop('amenities')\n        \n        self.data_frame = data_frame\n        \n    # encode amentities\n    def _encode_amentities(self, data_frame):\n        # Replace amenities from {}\" to ''\n        data_frame.amenities.replace('[{}\"]', '', regex=True, inplace=True)\n        # Split amenities with ,\n        amenities = data_frame.amenities.str.split(',', expand=True)\n        \n        \"\"\"All amenities\n        '24-Hour Check-in',\n        'Air Conditioning',\n        'Breakfast',\n        'Buzzer\/Wireless Intercom',\n        'Cable TV',\n        'Carbon Monoxide Detector',\n        'Cat(s)',\n        'Dog(s)',\n        'Doorman',\n        'Dryer',\n        'Elevator in Building',\n        'Essentials',\n        'Family\/Kid Friendly',\n        'Fire Extinguisher',\n        'First Aid Kit',\n        'Free Parking on Premises',\n        'Gym',\n        'Hair Dryer',\n        'Hangers',\n        'Heating',\n        'Hot Tub',\n        'Indoor Fireplace',\n        'Internet',\n        'Iron',\n        'Kitchen',\n        'Laptop Friendly Workspace',\n        'Lock on Bedroom Door',\n        'Other pet(s)',\n        'Pets Allowed',\n        'Pets live on this property',\n        'Pool',\n        'Safety Card',\n        'Shampoo',\n        'Smoke Detector',\n        'Smoking Allowed',\n        'Suitable for Events',\n        'TV',\n        'Washer',\n        'Washer \/ Dryer',\n        'Wheelchair Accessible',\n        'Wireless Internet'\n        \"\"\"\n\n        # For each col, extract the unique amenities\n        amenities_uniques = []\n        for col in amenities.columns:\n            amenities_uniques += list(amenities[col].unique())\n\n        # Remove the duplicate values\n        amenities_uniques = set(amenities_uniques)\n        amenities_uniques.remove('')\n        amenities_uniques.remove(None)\n        # Only two rows have Washer \/ Dryer, and they both have washer and dryer\n        amenities_uniques.remove('Washer \/ Dryer')\n        # When 'Pets live on this property' is True, one or more from 'Cat(s)', 'Dog(s)', 'Other pet(s)' will appear\n\n        # Encoding amenities\n        amenities_enc = pd.DataFrame()\n        for amenity in amenities_uniques:\n            amenities_enc[amenity] = data_frame.amenities.str.contains(amenity, regex=False)\n\n        # Rename the columns with prefix amenity_\n        amenities_enc.columns = [f\"amenity_{col}\" for col in amenities_enc.columns]\n        \n        # Concat encoded amenities and data_frame\n        data_frame = pd.concat([data_frame, amenities_enc], axis=1)\n\n        return data_frame\n\n    def _imputation(self, X_train, X_valid, y_train, y_valid):\n        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n        \n        # Zero imputation\n        # Reason:\n        zero_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n        zero_features = ['reviews_per_month', 'host_response_rate', 'host_is_superhost', 'security_deposit', 'cleaning_fee']\n        X_train_zero_imp = pd.DataFrame(zero_imp.fit_transform(X_train[zero_features]))\n        X_valid_zero_imp = pd.DataFrame(zero_imp.transform(X_valid[zero_features]))\n        X_train_zero_imp.columns = zero_features\n        X_valid_zero_imp.columns = zero_features\n        X_train_zero_imp.index = X_train.index\n        X_valid_zero_imp.index = X_valid.index\n        X_train_zero_imp = X_train_zero_imp.astype(float)\n        X_valid_zero_imp = X_valid_zero_imp.astype(float)\n        \n        # Mean imputation\n        # Reason:\n        mean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n        mean_features = ['host_acceptance_rate', 'review_scores_accuracy', 'review_scores_checkin', \n                         'review_scores_value', 'review_scores_location', 'review_scores_cleanliness', \n                         'review_scores_communication', 'review_scores_rating']\n        X_train_mean_imp = pd.DataFrame(mean_imp.fit_transform(X_train[mean_features]))\n        X_valid_mean_imp = pd.DataFrame(mean_imp.transform(X_valid[mean_features]))\n        X_train_mean_imp.columns = mean_features\n        X_valid_mean_imp.columns = mean_features\n        X_train_mean_imp.index = X_train.index\n        X_valid_mean_imp.index = X_valid.index\n        X_train_mean_imp = X_train_mean_imp.astype(float)\n        X_valid_mean_imp = X_valid_mean_imp.astype(float)\n        \n        # Mode imputation\n        # Reason: \n        mode_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n        mode_features = ['bathrooms', 'bedrooms', 'beds', 'property_type']\n        X_train_mode_imp = pd.DataFrame(mode_imp.fit_transform(X_train[mode_features]))        \n        X_valid_mode_imp = pd.DataFrame(mode_imp.transform(X_valid[mode_features]))\n        X_train_mode_imp.columns = mode_features\n        X_valid_mode_imp.columns = mode_features\n        X_train_mode_imp.index = X_train.index\n        X_valid_mode_imp.index = X_valid.index\n        X_train_mode_imp[['bathrooms', 'bedrooms', 'beds']] = X_train_mode_imp[['bathrooms', 'bedrooms', 'beds']].astype(int)\n        X_valid_mode_imp[['bathrooms', 'bedrooms', 'beds']] = X_valid_mode_imp[['bathrooms', 'bedrooms', 'beds']].astype(int)\n        \n        # Replace the unimputated columns\n        for feature in zero_features:\n            X_train[feature] = X_train_zero_imp[feature]\n            X_valid[feature] = X_valid_zero_imp[feature]\n        \n        for feature in mean_features:\n            X_train[feature] = X_train_mean_imp[feature]\n            X_valid[feature] = X_valid_mean_imp[feature]\n\n        for feature in mode_features:\n            X_train[feature] = X_train_mode_imp[feature]\n            X_valid[feature] = X_valid_mode_imp[feature]\n        \n        return X_train, X_valid, y_train, y_valid\n    \n    def _one_hot_encoding(self, X_train, X_valid, y_train, y_valid):\n        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n        \n        oe_enc_features = ['cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', \n                               'neighbourhood_group_cleansed', 'property_type', 'instant_bookable', 'room_type', 'bed_type']\n        \n        oe = OrdinalEncoder()\n        X_train[oe_enc_features] = oe.fit_transform(X_train[oe_enc_features])\n        X_valid[oe_enc_features] = oe.transform(X_valid[oe_enc_features])\n    \n        return X_train, X_valid, y_train, y_valid\n\n    def _target_encoding(self, X_train, X_valid, y_train, y_valid):\n        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n        \n        target_enc_features = ['cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', \n                               'neighbourhood_group_cleansed', 'property_type', 'instant_bookable', 'room_type', 'bed_type']\n        \n        # Create the encoder instance. Choose m to control noise.\n        target_enc = MEstimateEncoder(cols=target_enc_features, m=5.0)\n        X_train = target_enc.fit_transform(X_train, y_train)\n        X_valid = target_enc.transform(X_valid)\n        \n        return X_train, X_valid, y_train, y_valid\n    \n    def getData(self, kfold, target_encoding=True):\n        data_frame = self.data_frame.copy()\n        \n        # Split train and valid\n        X_train = data_frame[data_frame.kfold != kfold]\n        X_valid = data_frame[data_frame.kfold == kfold]\n        y_train = X_train.pop('price')\n        y_valid = X_valid.pop('price')\n        \n        # Imputation\n        X_train, X_valid, y_train, y_valid = self._imputation(X_train, X_valid, y_train, y_valid)\n        \n        # Target Encoding\n        if target_encoding:\n            X_train, X_valid, y_train, y_valid = self._target_encoding(X_train, X_valid, y_train, y_valid)\n        else:\n            X_train, X_valid, y_train, y_valid = self._one_hot_encoding(X_train, X_valid, y_train, y_valid)\n        \n        return X_train, X_valid, y_train, y_valid","6562af18":"# e.g. Before ML pipeline\nlistings.loc[:2, ['id', 'neighbourhood_group_cleansed', 'property_type', 'amenities', 'price']]","cfab92ac":"# e.g. After ML pipeline\nfeatures = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n            'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n            'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n            'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n            'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n            'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n            'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\nml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\nX_train, X_valid, y_train, y_valid = ml_pipeline.getData(kfold=0, target_encoding=True) # perform target encoding\nX = pd.concat([X_train, X_valid], axis=0)\ny = pd.concat([y_train, y_valid])\nX['price'] = y","288d362f":"X.loc[[241032, 953595, 3308979], ['neighbourhood_group_cleansed', 'property_type', 'price']]","aad7bad7":"# e.g. After ML pipeline\nX.loc[[241032, 953595, 3308979], 'amenity_Elevator in Building':]","9ca423d4":"features = listings_metadata[(listings_metadata.ML == 1) | (listings_metadata.ML == 2)].Label.to_list() # Official & Possible ML features\nfeatures.append('price') # Add target\nplt.figure(dpi=800)\nsns.heatmap(listings[features].corr(), cmap=\"rocket\", annot=True, annot_kws={\"fontsize\": 3});","81f377dc":"def clean_corr(df, target, threshold):\n    \"\"\"Return df.corr() that greater or equal than threshold.\n    \n    Args:\n        df (dataframe): Pandas dataframe.\n        target (str): The name of target.\n        threshold (float): The miniumu required correlation coefficient.\n        \n    Returns:\n        df.corr()\n    \"\"\"\n    df = df.corr().copy()\n    \n    for col in df.columns:\n        if abs(df.loc[col, target]) < threshold:\n            df.drop(col, axis=0, inplace=True)\n            df.drop(col, axis=1, inplace=True)\n    return df","ce56810c":"X_corr = clean_corr(X, 'price', 0.1)\nplt.figure(dpi=800)\nsns.heatmap(X_corr, cmap=\"rocket\", annot=True, annot_kws={\"fontsize\": 3});","4c092170":"#px.set_mapbox_access_token(open(\".mapbox_galaxy\").read())\npx.set_mapbox_access_token('pk.eyJ1IjoiemFja3NhbWJlciIsImEiOiJjazc3MXI1NjQwMXIzM25vMnBtMWtpNWFjIn0.FHxYZnEoStWmap8EQe2l-g')\nfig = px.scatter_mapbox(listings, \n                        lat='latitude', \n                        lon='longitude',\n                        color='neighbourhood_group_cleansed', \n                        size='price', \n                        color_continuous_scale=px.colors.cyclical.IceFire, \n                        hover_name='id',\n                        hover_data=['listing_url', 'property_type', 'room_type'],\n                        size_max=15, \n                        zoom=10,\n                        title='Map of price group by neighbourhood_group_cleansed')\nfig.show()","b3dc90b4":"fig = px.histogram(listings.dropna(subset=['property_type'], axis=0), \n                   x='price', \n                   histnorm='percent',\n                   color='property_type',\n                   title='Histogram of price vs. propery_type')\nfig.show()","ed65c88c":"fig = px.histogram(listings.dropna(subset=['room_type'], axis=0), \n                   x='price', \n                   histnorm='percent',\n                   color='room_type',\n                   title='Histogram of price vs. room_type')\nfig.show()","a2289ad6":"fig = px.histogram(listings.dropna(subset=['beds'], axis=0), \n                   x='price', \n                   histnorm='percent',\n                   color='beds', \n                   title='Histogram of price vs. (number of) beds')\nfig.show()","8b0e0c02":"features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'security_deposit', 'cleaning_fee', \n            'guests_included', 'extra_people', 'room_type', 'neighbourhood_group_cleansed', \n            'bed_type', 'cancellation_policy']\nfig = px.parallel_coordinates(listings[features + ['price']].dropna(), color = \"price\",\n                    color_continuous_scale = px.colors.diverging.Tealrose, color_continuous_midpoint = 2)\nfig.show()","854c81a1":"def make_mi_scores(X, y):\n    X = X.copy()\n    # Mutual Information required all data be integers\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize() # factorize() returns code and uniques\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    \n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","f90003f5":"%%time\nfeatures = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n            'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n            'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n            'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n            'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n            'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n            'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\nX.drop(columns=['kfold', 'price'], inplace=True)\n\n# Review the MI score from all data\nall_mi_scores = pd.DataFrame(make_mi_scores(X, y))\n\nall_mi_scores.style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","c94f4c3f":"calendar","2d3fd889":"# The calendar recorded the availability of each listing in the next 365 days\ncalendar.groupby('listing_id').size()","932f25a2":"# When the available is False, the price is NaN (Not a Number)\nprint(calendar.isna().sum())\nprint(calendar[calendar.available == 1].price.isna().sum())\nprint(calendar[calendar.available == 0].price.isna().sum())","cbec78ce":"# The possible outliers\neda_demand = EDA_demand()\ndemand = eda_demand.reviews_rate_vs_unavailability(365)\ndemand.describe()","6b91d386":"eda_demand.plot(outliers=False)","c6329d72":"eda_demand.plot(outliers=True)","bf03e621":"\"\"\"\nOne Hot Encoding\nML1, kfold: 0. RMSE: 56.481995126446456\nML1, kfold: 1. RMSE: 66.83960978199953\nML1, kfold: 2. RMSE: 61.957734603524976\nML1, kfold: 3. RMSE: 62.69133725976135\nML1, kfold: 4. RMSE: 55.715497896362415\nML1. Average RMSE: 60.73723493361895\n\nML1 + ML2, kfold: 0. RMSE: 52.568454955844246\nML1 + ML2, kfold: 1. RMSE: 63.234791588163155\nML1 + ML2, kfold: 2. RMSE: 58.68112865265134\nML1 + ML2, kfold: 3. RMSE: 60.09474908722824\nML1 + ML2, kfold: 4. RMSE: 47.693034296085685\nML1 + ML2. Average RMSE: 56.45443171599453\n\nTarget Encoding\nML1, kfold: 0. RMSE: 56.64589093002433\nML1, kfold: 1. RMSE: 62.44468185143068\nML1, kfold: 2. RMSE: 60.40781093438012\nML1, kfold: 3. RMSE: 63.666798642194124\nML1, kfold: 4. RMSE: 52.226979216000906\nML1. Average RMSE: 59.07843231480604\n\nML1 + ML2, kfold: 0. RMSE: 52.92355341945994\nML1 + ML2, kfold: 1. RMSE: 65.04777557551235\nML1 + ML2, kfold: 2. RMSE: 58.69704656344895\nML1 + ML2, kfold: 3. RMSE: 55.149794448218394\nML1 + ML2, kfold: 4. RMSE: 49.509631025616585\nML1 + ML2. Average RMSE: 56.26556020645124\n\"\"\"\n\ndef baseline(target_encoding=True):\n    #import warnings\n    #warnings.filterwarnings(\"ignore\")\n\n    features = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n                'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n                'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n                'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n                'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n                'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\n    ml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\n    \n    amenities = ['amenity_Washer', 'amenity_Air Conditioning', 'amenity_TV',\n                 'amenity_Kitchen', 'amenity_Wheelchair Accessible',\n                 'amenity_Free Parking on Premises', 'amenity_Doorman',\n                 'amenity_Cable TV', 'amenity_Smoke Detector',\n                 'amenity_Pets live on this property', 'amenity_Internet',\n                 'amenity_Hangers', 'amenity_Family\/Kid Friendly',\n                 'amenity_First Aid Kit', 'amenity_Indoor Fireplace', 'amenity_Gym',\n                 'amenity_Suitable for Events', 'amenity_Breakfast', 'amenity_Cat(s)',\n                 'amenity_Lock on Bedroom Door', 'amenity_Smoking Allowed',\n                 'amenity_Dog(s)', 'amenity_Shampoo', 'amenity_Hair Dryer',\n                 'amenity_Carbon Monoxide Detector', 'amenity_Wireless Internet',\n                 'amenity_Hot Tub', 'amenity_Safety Card',\n                 'amenity_Buzzer\/Wireless Intercom', 'amenity_Pool',\n                 'amenity_Elevator in Building', 'amenity_Pets Allowed',\n                 'amenity_Fire Extinguisher', 'amenity_Other pet(s)',\n                 'amenity_Laptop Friendly Workspace', 'amenity_Essentials',\n                 'amenity_Iron', 'amenity_Dryer', 'amenity_24-Hour Check-in',\n                 'amenity_Heating']\n    \n    # Define sheet id and base url\n    sheet_id = \"1M_qah-ym6O8vDcSmoKAP-lbZRPHUey83R_DJaW3LXfs\"\n    base_url = f\"https:\/\/docs.google.com\/spreadsheets\/d\/{sheet_id}\/gviz\/tq?tqx=out:csv&sheet=\"\n\n    # Load metadata for three datasets\n    listings_metadata = pd.read_csv(base_url+\"listings\")\n    calendar_metadata = pd.read_csv(base_url+\"calendar\")\n    reviews_metadata = pd.read_csv(base_url+\"reviews\")\n    \n    # ML1\n    ml1 = listings_metadata[listings_metadata.ML == 1].Label.to_list()\n    useless_features = ['availability_30', 'availability_60', 'availability_90', 'availability_365', 'first_review', 'last_review', 'amenities']\n    for useless_feature in useless_features:\n        ml1.remove(useless_feature)\n    ml1.append('demand')\n    ml1 += amenities\n    \n    AVG_RMSE = []\n    for kfold in range(5):\n        X_train, X_test, y_train, y_test = ml_pipeline.getData(kfold=kfold, target_encoding=target_encoding)\n        model = XGBRegressor(random_state=kfold, n_jobs=-1)\n        model.fit(X_train[ml1], y_train)\n        test_preds = model.predict(X_test[ml1])\n        RMSE = mean_squared_error(y_test, test_preds, squared=False)\n        print(f\"ML1, kfold: {kfold}. RMSE: {RMSE}\")\n        AVG_RMSE.append(RMSE)\n    print(f\"ML1. Average RMSE: {np.mean(AVG_RMSE)}\\n\")\n        \n    # ML1 + ML2\n    ml1 = listings_metadata[listings_metadata.ML == 1].Label.to_list()\n    useless_features = ['availability_30', 'availability_60', 'availability_90', 'availability_365', 'first_review', 'last_review', 'amenities']\n    for useless_feature in useless_features:\n        ml1.remove(useless_feature)\n    ml2 = listings_metadata[listings_metadata.ML == 2].Label.to_list()\n    ml2.append('demand')\n    ml2 = ml1 + ml2 + amenities\n    \n    AVG_RMSE = []\n    for kfold in range(5):\n        X_train, X_test, y_train, y_test = ml_pipeline.getData(kfold=kfold, target_encoding=target_encoding)\n        model = XGBRegressor(random_state=kfold, n_jobs=-1)\n        model.fit(X_train[ml2], y_train)\n        test_preds = model.predict(X_test[ml2])\n        RMSE = mean_squared_error(y_test, test_preds, squared=False)\n        print(f\"ML1 + ML2, kfold: {kfold}. RMSE: {RMSE}\")\n        AVG_RMSE.append(RMSE)\n    print(f\"ML1 + ML2. Average RMSE: {np.mean(AVG_RMSE)}\\n\")\n\nbaseline(target_encoding=True)","75f0f22f":"import logging\n# Define logger\nlogger = logging.getLogger('ML')\n\n# Set level for logger\nlogger.setLevel(logging.DEBUG)\n\n# Define the handler and formatter for file logging\nlog_file = 'ML'\nfileHandler = logging.FileHandler(f'{log_file}.log') # Define FileHandler\nfileHandler.setLevel(logging.INFO) # Set level\nfileFormatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') # Define formatter\nfileHandler.setFormatter(fileFormatter) # Set formatter\nlogger.addHandler(fileHandler) # Add handler to logger","ef0eb8e3":"# Define sheet id and base url\nsheet_id = \"1M_qah-ym6O8vDcSmoKAP-lbZRPHUey83R_DJaW3LXfs\"\nbase_url = f\"https:\/\/docs.google.com\/spreadsheets\/d\/{sheet_id}\/gviz\/tq?tqx=out:csv&sheet=\"\n\n# Load metadata for three datasets\nlistings_metadata = pd.read_csv(base_url+\"listings\")\ncalendar_metadata = pd.read_csv(base_url+\"calendar\")\nreviews_metadata = pd.read_csv(base_url+\"reviews\")\n\namenities = ['amenity_Washer', 'amenity_Air Conditioning', 'amenity_TV',\n             'amenity_Kitchen', 'amenity_Wheelchair Accessible',\n             'amenity_Free Parking on Premises', 'amenity_Doorman',\n             'amenity_Cable TV', 'amenity_Smoke Detector',\n             'amenity_Pets live on this property', 'amenity_Internet',\n             'amenity_Hangers', 'amenity_Family\/Kid Friendly',\n             'amenity_First Aid Kit', 'amenity_Indoor Fireplace', 'amenity_Gym',\n             'amenity_Suitable for Events', 'amenity_Breakfast', 'amenity_Cat(s)',\n             'amenity_Lock on Bedroom Door', 'amenity_Smoking Allowed',\n             'amenity_Dog(s)', 'amenity_Shampoo', 'amenity_Hair Dryer',\n             'amenity_Carbon Monoxide Detector', 'amenity_Wireless Internet',\n             'amenity_Hot Tub', 'amenity_Safety Card',\n             'amenity_Buzzer\/Wireless Intercom', 'amenity_Pool',\n             'amenity_Elevator in Building', 'amenity_Pets Allowed',\n             'amenity_Fire Extinguisher', 'amenity_Other pet(s)',\n             'amenity_Laptop Friendly Workspace', 'amenity_Essentials',\n             'amenity_Iron', 'amenity_Dryer', 'amenity_24-Hour Check-in',\n             'amenity_Heating']\n    \n# ML1 + ML2\nml1 = listings_metadata[listings_metadata.ML == 1].Label.to_list()\nuseless_features = ['availability_30', 'availability_60', 'availability_90', 'availability_365', 'first_review', 'last_review', 'amenities']\nfor useless_feature in useless_features:\n    ml1.remove(useless_feature)\nml2 = listings_metadata[listings_metadata.ML == 2].Label.to_list()\nml2.append('demand')\nml2 = ml1 + ml2 + amenities","824916ed":"# Silence Optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)","3f588317":"# Define number of trails\nn_trials = 200","557b38a7":"def objective(trial):\n    \"\"\"Modeling tuning with Target encoding.\n    \"\"\"\n    features = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n                'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n                'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n                'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n                'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n                'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\n    ml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\n    \n    RMSE_AVG = []\n    for kfold in range(5):\n        X_train, X_valid, y_train, y_valid = ml_pipeline.getData(kfold=kfold, target_encoding=True)\n        X_train, X_valid = X_train[ml2], X_valid[ml2]\n        \n        # Hyperparameters for XGBoost\n        xgb_params = {\n            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n            'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n            'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n            'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n            'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n            'max_depth': trial.suggest_int(\"max_depth\", 1, 7),\n            'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n        }\n\n        # For GPU\n        model = XGBRegressor(\n                tree_method='gpu_hist',\n                gpu_id=0,\n                predictor='gpu_predictor',\n                **xgb_params)\n        \n        '''\n        # For CPU\n        model = XGBRegressor(**xgb_params)\n        '''\n        \n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=300,\n            eval_set=[(X_valid, y_valid)],\n            verbose=5000\n        )\n        \n        valid_preds = model.predict(X_valid)\n        RMSE = mean_squared_error(y_valid, valid_preds, squared=False)\n        RMSE_AVG.append(RMSE)\n    \n    return np.mean(RMSE_AVG)","080b3805":"'''\n%%time\nstudy = optuna.create_study(direction='minimize', study_name=f'XGBoost {n_trials} trails')\nstudy.optimize(objective, n_trials=n_trials, show_progress_bar=False) # set n_triasl\n\nlogger.info(f\"Study name: {study.study_name}\")\nlogger.info(f\"Best value: {study.best_value}\")\nlogger.info(f\"Best paras: {study.best_params}\")\nlogger.info(\"Mission Complete! --------------\")\n'''","f03bb2d7":"def objective(trial):\n    \"\"\"Modeling tuning with Target encoding.\n    \"\"\"\n    features = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n                'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n                'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n                'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n                'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n                'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\n    ml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\n    \n    RMSE_AVG = []\n    for kfold in range(5):\n        X_train, X_valid, y_train, y_valid = ml_pipeline.getData(kfold=kfold, target_encoding=True)\n        X_train, X_valid = X_train[ml2], X_valid[ml2]\n        \n        # Hyperparameters for LightGBM\n        lgb_params = {\n            'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n            'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n            'max_depth': trial.suggest_int('max_depth', 1, 7),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        }\n\n        # For GPU\n        model = LGBMRegressor(\n                    device='gpu',\n                    gpu_platform_id=0,\n                    gpu_device_id=0,\n                    n_jobs=-1,\n                    metric='rmse',\n                    **lgb_params\n        )\n        \n        '''\n        # For CPU\n        model = LGBMRegressor(**lgb_params)\n        '''\n        \n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=300,\n            eval_set=[(X_valid, y_valid)],\n            verbose=5000\n        )\n        \n        valid_preds = model.predict(X_valid)\n        RMSE = mean_squared_error(y_valid, valid_preds, squared=False)\n        RMSE_AVG.append(RMSE)\n    \n    return np.mean(RMSE_AVG)","0d87baed":"'''\n%%time\nstudy = optuna.create_study(direction='minimize', study_name=f'LGBoost {n_trials} trails')\nstudy.optimize(objective, n_trials=n_trials, show_progress_bar=False) # set n_triasl\n\nlogger.info(f\"Study name: {study.study_name}\")\nlogger.info(f\"Best value: {study.best_value}\")\nlogger.info(f\"Best paras: {study.best_params}\")\nlogger.info(\"Mission Complete! --------------\")\n'''","47f3b85a":"def gmail(YOUR_GMAIL, YOUR_APP_PASSWORD, SEND_TO):\n    \"\"\"Send the ML tuning result to one or more email addresses.\n    \n    Args:\n        YOUR_GMAIL (str): Your gmail address.\n        YOUR_APP_PASSWORD (str): Your APP Password for gmail. \n        SEND_TO (str or list): The target emails.\n    \"\"\"\n    gmail_user = YOUR_GMAIL\n    gmail_password = YOUR_APP_PASSWORD # Google App Password\n\n    import smtplib\n    from email.message import EmailMessage\n\n    msg = EmailMessage()\n    msg[\"From\"] = YOUR_GMAIL\n    msg[\"Subject\"] = \"Seattle Airbnb ML Tuning\"\n    msg[\"To\"] = SEND_TO\n    msg.set_content(f\"\"\"\\\n    {n_trials} Trials are done.\n    Mission Complete!\"\"\")\n    with open('ML.log', 'rb') as f:\n        content = f.read()\n        msg.add_attachment(content, maintype='application', subtype='log', filename='ML.log')\n\n    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n    server.login(gmail_user, gmail_password)\n    server.send_message(msg)\n    server.close()\n#gmail(YOUR_GMAIL, YOUR_APP_PASSWORD, SEND_TO)","650907b9":"class Model_Blending:\n    def __init__(self, data_frame, features_etl, features_ml):\n        data_frame = data_frame.copy()\n        self.ml_pipeline = ML_pipeline(data_frame=data_frame, features=features_etl, target='price')\n        self.features_ml = features_ml\n    \n    def _xgboost_reg(self, xgb_params):\n        \"\"\"\n        # For GPU\n        model = XGBRegressor(\n                    tree_method='gpu_hist',\n                    gpu_id=0,\n                    predictor='gpu_predictor',\n                    n_jobs=-1,\n                    **xgb_params\n                )\n        \"\"\"\n        \n        # For CPU\n        model = XGBRegressor(**xgb_params)\n        \n        return model\n    \n    def _lightgbm_reg(self, lgb_params):\n        \"\"\"\n        # For GPU\n        model = LGBMRegressor(\n                    device='gpu',\n                    gpu_platform_id=0,\n                    gpu_device_id=0,\n                    n_jobs=-1,\n                    metric='rmse',\n                    **lgb_params\n                )\n        \"\"\"\n        \n        # For CPUT\n        model = LGBMRegressor(**lgb_params)\n        \n        return model\n    \n    def blending(self, model: str, params: dict):\n        '''Model blending. Generate 5 predictions according to 5 folds.\n        \n        Args:\n            model: One of xgboost or lightgbm.\n            params: Hyperparameters for XGBoost or LightGBM.\n        Returns:\n            None\n        '''\n        assert model in ['xgboost', 'lightgbm'], \"ValueError: model must be one of ['xgboost', 'lightgbm']!\"\n        \n        final_valid_predictions = {}\n        scores = []\n        \n        for fold in range(5):\n            X_train, X_valid, y_train, y_valid = self.ml_pipeline.getData(kfold=fold, target_encoding=True)\n            X_train, X_valid = X_train[self.features_ml], X_valid[self.features_ml] # Add many amenities\n            # Get X_valid_ids\n            X_valid_ids = list(X_valid.index)\n            \n            print(f\"Training ...\")\n            # Define model\n            if model == 'xgboost':\n                reg = self._xgboost_reg(params)\n            elif model == 'lightgbm':\n                reg = self._lightgbm_reg(params)\n\n            # Modeling - Training\n            reg.fit(\n                X_train, y_train, \n                early_stopping_rounds=300,\n                eval_set=[(X_valid, y_valid)],\n                verbose=False\n            )\n\n            # Modeling - Inference\n            valid_preds = reg.predict(X_valid)\n            \n            final_valid_predictions.update(dict(zip(X_valid_ids, valid_preds))) # loop 5 times with different valid id\n\n            rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n            scores.append(rmse)\n            print(f'Fold: {fold}, RMSE: {rmse}')\n            \n        # Export results\n        if not os.path.exists('output'):\n            os.mkdir('output')\n        final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n        final_valid_predictions.columns = [\"id\", f\"{model}_pred\"]\n        final_valid_predictions.to_csv(f\"output\/{model}_valid_pred.csv\", index=False)\n\n        print('-----------------------------------------------------------------')\n        print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}') \n        \n    def predict(self, models: list):\n        df_valids = pd.read_csv(f'output\/{models[0]}_valid_pred.csv')\n        models.remove(models[0])\n        for model in models:\n            df = pd.read_csv(f'output\/{model}_valid_pred.csv')\n            df_valids = df_valids.set_index('id').join(df.set_index('id'), how='inner')\n        \n        # Calculate the average predictions\n        df_valids['mean_valids'] = df_valids.mean(axis=1)\n        # Join listings price to df_valids\n        df_valids['price'] = listings.set_index('id')['price']\n        \n        # Use the average predictions to validate the target\n        return mean_squared_error(df_valids.price, df_valids['mean_valids'], squared=False)","3b1e012a":"features_etl = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n            'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n            'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n            'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n            'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n            'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n            'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\nfeatures_ml = ml2\n\nxgb_params = {'lambda': 0.029949323233957558, 'alpha': 0.47821306780284645, 'reg_lambda': 0.03007272817610808, \n              'reg_alpha': 5.7650942972599255e-05, 'colsample_bytree': 0.32733907049678806, 'subsample': 0.9397958925107069, \n              'learning_rate': 0.016087339011505105, 'n_estimators': 4117, 'max_depth': 6, 'random_state': 42, 'min_child_weight': 5}\n\nlgb_params = {'random_state': 42, 'num_iterations': 5549, 'learning_rate': 0.07313607774375752, 'max_depth': 5, 'num_leaves': 75, \n             'min_data_in_leaf': 100, 'lambda_l1': 1.3379869858112054e-06, 'lambda_l2': 0.00025091437242776726, \n             'feature_fraction': 0.5910800704597817, 'bagging_fraction': 0.9553891294481797, 'bagging_freq': 6, 'min_child_samples': 23}\n\nmodel_blending = Model_Blending(listings, features_etl, features_ml)","80e956e1":"model_blending.blending(model='xgboost', params=xgb_params)","aaf5793f":"model_blending.blending(model='lightgbm', params=lgb_params)","f63949ef":"model_blending.predict(models=['xgboost', 'lightgbm'])","cf37b5d7":"class Model_Stacking:\n    def __init__(self, data_frame, features_etl, features_ml):\n        data_frame = data_frame.copy()\n        self.ml_pipeline = ML_pipeline(data_frame=data_frame, features=features_etl, target='price')\n        self.features_ml = features_ml\n    \n    def _xgboost_reg(self, xgb_params):\n        \"\"\"\n        # For GPU\n        model = XGBRegressor(\n                    tree_method='gpu_hist',\n                    gpu_id=0,\n                    predictor='gpu_predictor',\n                    n_jobs=-1,\n                    **xgb_params\n                )\n        \"\"\"\n        \n        # For CPU\n        model = XGBRegressor(**xgb_params)\n        \n        return model\n    \n    def _lightgbm_reg(self, lgb_params):\n        \"\"\"\n        # For GPU\n        model = LGBMRegressor(\n                    device='gpu',\n                    gpu_platform_id=0,\n                    gpu_device_id=0,\n                    n_jobs=-1,\n                    metric='rmse',\n                    **lgb_params\n                )\n        \"\"\"\n        \n        # For CPUT\n        model = LGBMRegressor(**lgb_params)\n        \n        return model\n    \n    def stacking(self, model: str, params: dict):\n        '''Model blending. Generate 5 predictions according to 5 folds.\n        \n        Args:\n            model: One of xgboost or lightgbm.\n            params: Hyperparameters for XGBoost or LightGBM.\n        Returns:\n            None\n        '''\n        assert model in ['xgboost', 'lightgbm'], \"ValueError: model must be one of ['xgboost', 'lightgbm']!\"\n        \n        final_valid_predictions = {}\n        scores = []\n        \n        for fold in range(5):\n            X_train, X_valid, y_train, y_valid = self.ml_pipeline.getData(kfold=fold, target_encoding=True)\n            X_train, X_valid = X_train[self.features_ml], X_valid[self.features_ml] # Add many amenities\n            # Get X_valid_ids\n            X_valid_ids = list(X_valid.index)\n            \n            print(f\"Training ...\")\n            # Define model\n            if model == 'xgboost':\n                reg = self._xgboost_reg(params)\n            elif model == 'lightgbm':\n                reg = self._lightgbm_reg(params)\n\n            # Modeling - Training\n            reg.fit(\n                X_train, y_train, \n                early_stopping_rounds=300,\n                eval_set=[(X_valid, y_valid)],\n                verbose=False\n            )\n\n            # Modeling - Inference\n            valid_preds = reg.predict(X_valid)\n            \n            final_valid_predictions.update(dict(zip(X_valid_ids, valid_preds))) # loop 5 times with different valid id\n\n            rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n            scores.append(rmse)\n            print(f'Fold: {fold}, RMSE: {rmse}')\n            \n        # Export results\n        if not os.path.exists('output'):\n            os.mkdir('output')\n        final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n        final_valid_predictions.columns = [\"id\", f\"{model}_pred\"]\n        final_valid_predictions.to_csv(f\"output\/{model}_valid_pred.csv\", index=False)\n\n        print('-----------------------------------------------------------------')\n        print(f'Average RMSE: {np.mean(scores)}, STD of RMSE: {np.std(scores)}')\n        \n    def predict(self, models: list):\n        df_valids = pd.read_csv(f'output\/{models[0]}_valid_pred.csv')\n        models.remove(models[0])\n        for model in models:\n            df = pd.read_csv(f'output\/{model}_valid_pred.csv')\n            df_valids = df_valids.set_index('id').join(df.set_index('id'), how='inner')\n        \n        # Join listings price to df_valids\n        df_valids['price'] = listings.set_index('id')['price']\n        \n        # Implement a simple regressor such as linear regression\n        linear_reg = LinearRegression()\n        \n        # Define X, y\n        X, y = df_valids.iloc[:, :len(models)], df_valids.price\n        \n        # Use the models validations as training set for predictions\n        scores = cross_val_score(linear_reg, X, y, cv=5, scoring='neg_root_mean_squared_error')\n        scores = -scores\n        return np.mean(scores)","0deb046e":"features_etl = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n            'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n            'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n            'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n            'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n            'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n            'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n\nfeatures_ml = ml2\n\nxgb_params = {'lambda': 0.029949323233957558, 'alpha': 0.47821306780284645, 'reg_lambda': 0.03007272817610808, \n              'reg_alpha': 5.7650942972599255e-05, 'colsample_bytree': 0.32733907049678806, 'subsample': 0.9397958925107069, \n              'learning_rate': 0.016087339011505105, 'n_estimators': 4117, 'max_depth': 6, 'random_state': 42, 'min_child_weight': 5}\n\nlgb_params = {'random_state': 42, 'num_iterations': 5549, 'learning_rate': 0.07313607774375752, 'max_depth': 5, 'num_leaves': 75, \n             'min_data_in_leaf': 100, 'lambda_l1': 1.3379869858112054e-06, 'lambda_l2': 0.00025091437242776726, \n             'feature_fraction': 0.5910800704597817, 'bagging_fraction': 0.9553891294481797, 'bagging_freq': 6, 'min_child_samples': 23}\n\nmodel_stacking = Model_Stacking(listings, features_etl, features_ml)","408019dd":"model_stacking.stacking('xgboost', xgb_params)","630cb4a7":"model_stacking.stacking('lightgbm', lgb_params)","75a2461c":"model_stacking.predict(models=['xgboost', 'lightgbm'])","ccf10257":"---\n\n### Define Logger","45434238":"The model stacking result is not much better than any single model since I only used two models with target encoding.<br>\nYou can combine different models with different encoding strategies even different features to improve the overall performance.<br>\nFor instance, you can combine XGBoost, LightGBM, and CatBoost with one-hot encoding, target encoding, ordinal encoding, and polynomial encoding. Then you have $3 \\times 4$ models for model stacking.","2993372a":"---\n\n# Airbnb Exploration\n\n## Who needs the prediction? The host.\n\n<details>\n    <summary><b>Click to see Price Tips example<\/b><\/summary>\n\n<img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919210229.png' alt='The price is suggested by Airbnb'>\n    \n<img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919210423.png' >\n\n<\/details>\n\n---\n\n## How Airbnb determines the price of a listing?\n\nCurrently, Airbnb has at least two Machine Learning models for price prediction. They are both for hosts:\n\n### Price Tips\n\n> **Price Tips**: Price tips are automated nightly price recommendations that you can choose to save. They are based on the type and location of your listing, the season, demand, and other factors. Even if you use price tips, you always control your price and can override the tips at any time. \u2014 Refered from Airbnb.<br>\n\n<details>\n    <summary><b>Click to see Price Tips<\/b><\/summary>\n\n<picture>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211009230743.png' alt='Price Tips'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211009230813.png' alt='Smart Pricing with Price Tips'>\n<\/picture>\n<\/details>\n\n---\n\n### Smart Pricing\n\n> **Smart Pricing**: Smart Pricing lets you set your prices to automatically go up or down based on changes in demand for listings like yours. You are always responsible for your price, so Smart Pricing is controlled by other pricing settings you choose, and you can adjust nightly prices any time. Smart Pricing is based on the type and location of your listing, the season, demand, and other factors. \u2014 Refered from Airbnb.\n\nThe following features are defined by Airbnb:\n\n- How many people are searching for listings like yours\n- The dates they\u2019re looking at\n- Whether other listings are getting booked\n- Your listing\u2019s best qualities\n- Your neiborhood: To calculate pricing based on location, Smart Pricing looks at whether your listing is in a city neighborhood, a suburb, or a more spread-out area.\n- Review rate: The number and quality of your reviews is another key factor in Smart Pricing.\n- Completed trips: If you honor most confirmed reservations, your prices can go higher within the minimum and maximum range you set.\n- Your listing' amenities: Wi-fi, washer\/dryer, and air conditioning are especially important, but Smart Pricing looks at all your amenities.\n\n<details>\n    <summary><b>Click to see Smart Pricing<\/b><\/summary>\n<picture>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919213149.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919213824.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919214931.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919214948.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919215026.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919215039.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20210919215201.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211003122330.png'>\n<\/picture>\n<\/details>\n\n---\n\n## What is Calendar?\n\nThe calendar contains three types of price:\n- Base price set by the host.\n- One time special price for event set by the host.\n- Price based on the demand, which is automatically set by **Smart Pricing**.\n\n<details>\n    <summary><b>Click to see Calendar<\/b><\/summary>\n\n    See how I turned on Smart Pricing, set a one time price at a specific day, and set a day unavailable.\n\n<picture>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211003123104.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211003123145.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211003123708.png'>\n    <img src='https:\/\/raw.githubusercontent.com\/ZacksAmber\/PicGo\/master\/img\/20211003123609.png'>\n<\/picture> \n<\/details>","a6dc5ed2":"---\n\n## Mutual Information","514dec2f":"---\n\n# ETL Pipeline\n\nThe ETL pipeline provides data transformation and formatting. Thus, we can calculate the data and perform machine learning with the correct data format.","30a98b69":"## ML Baseline\n\nThe baseline can provide an insight into the performance of different data preprocessing strategies, such as encoding methods.<br>\nHere, I chose `target encoding`. First, it had a better performance than ordinal encoding. Second, we already knew the categorical data have potential levels for a different price.<br>\ne.g. different `roomt_type` has different `price` histogram.<br>\n\n\nFeatures marked as `ML1` were defined by Airbnb for Machine Learning models.<br>\nThen, as you can see, `ML1 + ML2` is better than `ML1`, which means we found more features that were useful for Machine Learning.<br> ","49d3baef":"The model blending result is not much better than any single model since I only used two models with target encoding.<br>\nYou can combine different models with different encoding strategies even different features to improve the overall performance.<br>\nFor instance, you can combine XGBoost, LightGBM, and CatBoost with one-hot encoding, target encoding, ordinal encoding, and polynomial encoding. Then you have $3 \\times 4$ models for model blending.","4a04c3e8":"---\n\n## Model Tuning\n\nThe Hyperparameter tuning platform I used is [Optuna](https:\/\/optuna.org\/).<br>\nI implemented a logger to write the tuning results in the local log file.<br>\nAfter all tunings are finished, the program will sent an email to my mailbox with the best hyperparameters.\n- **To enable this feature**, go to [configure your gmail first](#gmail-configuration).\n\nP.S: If your computer does not support GPU accleration, uncomment code `For CPU` and comment code `For GPU`.\n\n**If you want to train your model, DO NOT RUN the following code in this notebook.**<br>\nInstead, make another notebook for model tuning. Please following this [link](https:\/\/github.com\/ZacksAmber\/Kaggle-Seattle-Airbnb\/blob\/main\/ML_Tuning.ipynb).","cbb4fd2c":"---\n\n### Gmail Configuration<a id='gmail-configuration'><\/a>\n\n> [How to Send Emails with Gmail using Python](https:\/\/stackabuse.com\/how-to-send-emails-with-gmail-using-python\/)","ed5b1083":"---\n\n### Model Tuning: LightGBM","edb2b5fe":"---\n\n# EDA and Feature Engineering","0fc67161":"---\n\n## Cross-Validation KFold","c05984f4":"---\n\n## Parallel Coordinates Plot\n\nReference: [Parallel Coordinates Plot in Python](https:\/\/plotly.com\/python\/parallel-coordinates-plot\/)","944bbbcc":"---\n\n# Machine Learning\n\nThe models in this project are [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/) and [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/).<br>","99826351":"---\n\n### Tuning Configurations","d09938e2":"---\n\n# ML Pipeline\n\n`EDA_demand` calculates the demand for each listing from csv reviews.<br>\n`ML Pipeline` imputes and transforms data for Machine Learning.","87a0a582":"---\n\n### Model Tuning: XGBoost","4ac91f2b":"---\n\n## Heatmap 2.0\n\nSince Pandas `.corr()` only calculates the numeric data. I performed target encoding then drew the heatmap again.\n\n`room_type`, `neighbourhood_group_cleansed`, `bed_type`, `cancellation_policy` were categorical data that cannot be calculated the correlation coefficient. But after target encoding, they do have a very high impact on price.\nFor the amenities, `TV`, `Hot Tub`, `Kitchen`, `Indoor Fireplace`, `Dryer`, `Family\/Kid Friendly`, `Doorman`, `Gym`, `Cable TV`, `Washer`, `Air Conditioning` are the more critical than other amenities.\n\nAll available amenities:\n- 24-Hour Check-in\n- Air Conditioning\n- Breakfast\n- Buzzer\/Wireless Intercom\n- Cable TV\n- Carbon Monoxide Detector\n- Cat(s)\n- Dog(s)\n- Doorman\n- Dryer\n- Elevator in Building\n- Essentials\n- Family\/Kid Friendly\n- Fire Extinguisher\n- First Aid Kit\n- Free Parking on Premises\n- Gym\n- Hair Dryer\n- Hangers\n- Heating\n- Hot Tub\n- Indoor Fireplace\n- Internet\n- Iron\n- Kitchen\n- Laptop Friendly Workspace\n- Lock on Bedroom Door\n- Other pet(s)\n- Pets Allowed\n- Pets live on this property\n- Pool\n- Safety Card\n- Shampoo\n- Smoke Detector\n- Smoking Allowed\n- Suitable for Events\n- TV\n- Washer\n- Washer \/ Dryer\n- Wheelchair Accessible\n- Wireless Internet","777f6d9b":"---\n\n## Load Metadata\n\nThe [metadata](https:\/\/docs.google.com\/spreadsheets\/d\/1M_qah-ym6O8vDcSmoKAP-lbZRPHUey83R_DJaW3LXfs\/edit?usp=sharing) was analyzed and made by [Zacks Shen](https:\/\/www.linkedin.com\/in\/zacks-shen\/) and [Kevin Chu](https:\/\/www.linkedin.com\/in\/yen-duo-chu\/).\n\nThe medata data includes includes `Label`, `Data Type`,  `Description`, `ML`, `Reason`:\n- `Label`: the column name\n- `Data Type`: the data type of the column\n- `Description`: the label usage based on our observation from Airbnb\n- `ML`: is it useful for Machine Learning\n\t- `0`: cannot be used for ML since it is meaningless, or it's hard to measuring (e.g. listing photos)\n\t- `1`: must be used for ML due to the official description by Airbnb\n\t- `2`: possible be used for ML due to our assessment\n- `Reason`: why the label can or cannot be used for ML (may empty)","cbbec87b":"---\n\n## Histogram\n\nTake a glance at the following high correlation coefficient features.\n\n`accommodates`, `bathrooms`, `bedrooms`, `beds`, `security_deposit`, `cleaning_fee`, `guests_included`, `extra_people`, `room_type`, `neighbourhood_group_cleansed`, `bed_type`, `cancellation_policy`","5ed02fc5":"---\n\n### Define Features for ML","bc3f7c90":"However, the booked listings and unavailable listings are both `available = f` in the dataset **calendar**. As I mentioned above, we don't have the transaction data. Therefore, I designed a simple but effective model to filter the booked listings.\n\nFor example:\n\n[listing 3402376](https:\/\/www.airbnb.com\/rooms\/3402376) has 5 reviews in total (1st review was in Sep 2014) but `available = 'f'` for 365 days. As you can see, the listing was not booked by someone for a whole year. Instead, the host set the listing as unavailable for 365 days.","f76dff8f":"---\n\n## Outliers and Real Demand\n\n> Reference: [Supply and demand](https:\/\/en.wikipedia.org\/wiki\/Supply_and_demand)\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/7a\/Supply-and-demand.svg\/1920px-Supply-and-demand.svg.png\" width=50%>\n\nIn [microeconomics](https:\/\/en.wikipedia.org\/wiki\/Microeconomics), **supply and demand** is an economic model of price determination in a market. The price _P_ of a product is determined by a balance between production at each price (supply _S_) and the desires of those with purchasing power at each price (demand _D_). The diagram shows a positive shift in demand from $D_1$ to $D_2$, resulting in an increase in price (_P_) and quantity sold (_Q_) of the product.","104e2ba2":"---\n\n## Map\n\nFor more ideas, visualizations of all Seattle datasets can be found [here](http:\/\/insideairbnb.com\/seattle\/).\n\n> Reference: [Scatter Plots on Mapbox in Python](https:\/\/plotly.com\/python\/scattermapbox\/#multiple-markers)","20bfc5dd":"---\n\n# Dependencies","17905020":"## Heatmap 1.0\n\nPandas `.corr()` method calculates the Pearson correlation coefficient between every two features.<br>\nAnd heatmap shows the relationship more clearly.<br>\nHere, `accommodates`, `bathrooms`, `bedrooms`, `beds`, `security_deposit`, `cleaning_fee`, `guests_included`, `extra_people` have relatively higher correlation coefficient than other features with `price`.","fd9ff221":"---\n\n## Model Blending\n\n> Reference: [Ensemble Learning: Stacking, Blending & Voting](https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483)\n\nAfter hyperparameter tuning, we have a set of beter hyperparameters for XGBoost and LightGBM. Then I performed a model blending for a better ML performance.\n\nMy 200 Trails ($200 \\times 5$ in total) hyperparameters:\n\n- XGBoost\n\n{'lambda': 0.029949323233957558, 'alpha': 0.47821306780284645, 'reg_lambda': 0.03007272817610808, 'reg_alpha': 5.7650942972599255e-05, 'colsample_bytree': 0.32733907049678806, 'subsample': 0.9397958925107069, 'learning_rate': 0.016087339011505105, 'n_estimators': 4117, 'max_depth': 6, 'random_state': 42, 'min_child_weight': 5}\n\n- LightGBM\n\n{'random_state': 42, 'num_iterations': 5549, 'learning_rate': 0.07313607774375752, 'max_depth': 5, 'num_leaves': 75, 'min_data_in_leaf': 100, 'lambda_l1': 1.3379869858112054e-06, 'lambda_l2': 0.00025091437242776726, 'feature_fraction': 0.5910800704597817, 'bagging_fraction': 0.9553891294481797, 'bagging_freq': 6, 'min_child_samples': 23}","9c7e9cf9":"---\n\n## Load Datasets\n\n- listings: The listings' information including nightly price (base price).\n- reviews: All of the past reviews for each listing.\n- calendar: The availability for each listing with the base price, special price, or smart price.","2aeaa963":"# Kaggle Seattle Airbnb\n\n## Project Info\n\n- Author: [Zacks Shen](https:\/\/www.linkedin.com\/in\/zacks-shen\/)\n- Contributor: [Kevin Chu](https:\/\/www.linkedin.com\/in\/yen-duo-chu\/)\n\n> [Medium: Article](https:\/\/medium.com\/@zacks.shen\/how-airbnb-evaluates-the-listings-b35c5a7890cb)<br>\n> [Kaggle: Code with Output](https:\/\/www.kaggle.com\/zacksshen\/kaggle-seattle-airbnb)<br>\n> [GitHub: Project](https:\/\/github.com\/ZacksAmber\/Kaggle-Seattle-Airbnb)\n\n---\n\n### Reference\n\n> [Kaggle](https:\/\/www.kaggle.com\/airbnb\/seattle)<br>\n> [Data Source](http:\/\/insideairbnb.com\/seattle)<br>\n> [Map](http:\/\/insideairbnb.com\/get-the-data.html)\n\n---\n\n### Why Price Prediction?\n\nPrice prediction is beneficial for **Perfect Competition**, a triple-win market.\n- Hosts have an intuitive opportunity to compare their services and amenities with competitors. As competition intensifies, the overall service quality and market - size of the rental housing market will be improved.\n- Price prediction models are reliable references for data-driven decision-making. With price suggestions from Airbnb, hosts can make different business strategies.\n- Airbnb users have more choices at lower price or higher quality.\n- Airbnb can grow faster including attract more users and hosts, leading to opertional and data center cost reduction and profit growth.\n\n---\n\n### Cons\n\nThe model accuracy will be lower than expected due to missing some essential features.\n- The transaction data: The datasets did not contain transaction data.\n    - We don't know if the listings were booked or just unavailable.\n    - We will explore how to find the confirmed orders under the Exploratory Data Analysis part.\n- The search engine data: The datasets do not contain searching histories.\n    \n---\n\n## Conclusion\n\nHosts could adjust the following features for improving their competitiveness.\n- Decrease\n    - `cleaning_fee`\n    - `security_deposit`\n\n- Change\n    - `cancellation_policy`\n    - `bed_type`\n    - `beds`\n    - `guests_included`\n    - `extra_people`\n    - `accommodates`\n\n- Amenities:\n    - `Indoor Fireplace`\n    - `Cable TV`\n    - `TV`\n    - `Doorman`\n    - `Dryer`\n    - `Air Conditioning`\n    - `Gym`\n    - `Family\/Kid Friendly`\n    - `Kitchen`\n    - `Washer`\n    \n- Hard to change:\n    - `neighbourhood_group_cleansed`\n    - `room_type`\n    - `bathrooms`\n    - `bedrooms`\n    \n    \nAlthough we don't have the transaction data, and the number of reviews did not play an essential role in price prediction, I'm sure such features will influence the rank on the search engine. \n\nAirbnb uses machine learning models to intensify the competition. They can provide Price Tips, Smart Pricing, and improvement suggestions to the host. As a host, you can combine the models and good strategies to maximize your competitiveness and profit. Airbnb users are also benefited from the perfect competition.","fdbc6773":"As I mentioned above, we CANNOT know the real demand since we don't have transaction data and search engine data.<br>\nHowever, we can use a statistical method to filter the outliers. \n- I extracted the unavailability for each listing in the next 365 days then divided it by 365. So the maximum `unavailability_365` is 1, which means the listing was unavailable for a year or was booked by someone for a year; the minimum `unavailability_365` is 0, which means the listing was ready for booking every day.\n- I extracted the number of reviews for each listing started from the first view day and ended with 2016-01-04 (the scraped date). Then I calculated the number of reviews per day for each listing. Therefore, the maximum `reviews_per_day` is 1, which means the listing got reviews every day; the minimum `reviews_per_day` is 0, which means the listing never got any reviews.\n\nYou can determine the threshold. But here I used `unavailability_365 > 75% quantile` and `0.010376 < 25% quantile` as the filter to split the dataset into two parts.<br>\nAs you can see, the relative normal data in the calendar shows the intersections on the plot.<br>\nHowever, the possible outliers have a wide border between `reviews_per_day` and `unavailability_365`.","f48dc19b":"---\n\n## Model Stacking\n\n> Reference: [Ensemble Learning: Stacking, Blending & Voting](https:\/\/towardsdatascience.com\/ensemble-learning-stacking-blending-voting-b37737c4f483)<br>\n> Reference: [How To Use \u201cModel Stacking\u201d To Improve Machine Learning Predictions](https:\/\/medium.com\/geekculture\/how-to-use-model-stacking-to-improve-machine-learning-predictions-d113278612d4)\n\n\nModel Stacking is a way to improve model predictions by combining the outputs of multiple models and running them through another machine learning model called a meta-learner.\n\nAfter hyperparameter tuning, we have a set of beter hyperparameters for XGBoost and LightGBM. Then I performed a model blending for a better ML performance.\n\nMy 200 Trails ($200 \\times 5$ in total) hyperparameters:\n\n- XGBoost\n\n{'lambda': 0.029949323233957558, 'alpha': 0.47821306780284645, 'reg_lambda': 0.03007272817610808, 'reg_alpha': 5.7650942972599255e-05, 'colsample_bytree': 0.32733907049678806, 'subsample': 0.9397958925107069, 'learning_rate': 0.016087339011505105, 'n_estimators': 4117, 'max_depth': 6, 'random_state': 42, 'min_child_weight': 5}\n\n- LightGBM\n\n{'random_state': 42, 'num_iterations': 5549, 'learning_rate': 0.07313607774375752, 'max_depth': 5, 'num_leaves': 75, 'min_data_in_leaf': 100, 'lambda_l1': 1.3379869858112054e-06, 'lambda_l2': 0.00025091437242776726, 'feature_fraction': 0.5910800704597817, 'bagging_fraction': 0.9553891294481797, 'bagging_freq': 6, 'min_child_samples': 23}"}}