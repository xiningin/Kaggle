{"cell_type":{"d67e2e18":"code","091f5460":"code","3808ae52":"code","2ecfd625":"code","1b91ceae":"code","d52ce99a":"code","4965fca3":"code","d64b565f":"code","fbb10704":"code","962f2526":"code","3b3e1d1c":"code","4b00283d":"code","5735b6fc":"code","a0461ce7":"code","d2e9683a":"code","d0506c56":"code","19b9e5a3":"code","ebad4a15":"code","fc7451cd":"code","6dae1798":"code","7eb144cd":"code","db8bc8db":"code","4223bc72":"code","c8d2e9f2":"code","4f2702ed":"code","4fd304dd":"code","9b0da6e3":"code","1da58a60":"code","146d4e13":"code","c9f716e4":"code","63d76621":"markdown","25d4756d":"markdown","462c5b68":"markdown","db84ddad":"markdown","d385486f":"markdown","8b97f4e5":"markdown","bb1343dd":"markdown","026ca376":"markdown","cc02f8ac":"markdown","113fa8a2":"markdown","ecbbefd1":"markdown","e2ca5b69":"markdown","45ce5834":"markdown","33b4a815":"markdown","35393a36":"markdown","e9f5b178":"markdown","7239dd20":"markdown","5fef789a":"markdown","962748bc":"markdown","b47a6471":"markdown","473b5912":"markdown","43dea519":"markdown","a324548f":"markdown","a504bfa1":"markdown","907350eb":"markdown","17801850":"markdown","094c03d0":"markdown","3c675bee":"markdown","60d6ed46":"markdown","620588cf":"markdown","2ce2e412":"markdown","656580e0":"markdown","244c281f":"markdown","5b5fbd61":"markdown"},"source":{"d67e2e18":"# Importing Packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_rows', 3000)\npd.set_option('display.max_columns', 1500)\n\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report, auc\nfrom sklearn.model_selection import KFold, cross_val_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","091f5460":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head(10)","3808ae52":"cols = df.columns\nprint(f'Name of the Columns in the Dataset are : \\n\\n {np.array(cols)}')","2ecfd625":"df.info()","1b91ceae":"Rating_Count = df['quality'].value_counts().sort_index()\nfig, ax = plt.subplots(figsize = [20, 10])\nfig.subplots_adjust(top = 0.93)\nfig.suptitle('Counts of Each Rating of Red Wine in the Dataset', size = 25, fontweight = 'bold')\nsns.barplot(Rating_Count.index, Rating_Count.values, ax = ax)\nfor index_, value_ in enumerate(Rating_Count):\n    ax.text(index_, value_ + 12, str(value_), color = 'black', fontweight = 'bold', size = 15)\nax.set_xlabel('Quality Rating of Red Wine (from 0 to 10)', size = 20)\nax.set_ylabel('Frequency', size = 20)\nplt.show()","d52ce99a":"df['quality_new'] = df['quality'].apply(lambda x: 'Good' if x >= 7 else 'Bad')\nquality = df.pop('quality') # keeping aside the original quality column as Backup.","4965fca3":"fig, ax = plt.subplots(figsize = [18, 8])\nfig.suptitle('Proportion of different Wine Quality in the Data', size = 25, fontweight = 'bold')\nax.pie(df['quality_new'].value_counts(), labels = list(df['quality_new'].value_counts().index), \n       autopct = '%1.1f%%', textprops = {'fontsize': 22}, pctdistance = 0.5)\n\n# Draw Circle\ncentre_circle = plt.Circle((0,0),0.75,fc = 'white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal Aspect Ratio ensures that Pie is drawn as a Circle\nax.axis('equal') \nplt.show()","d64b565f":"X, Y = df.drop('quality_new', axis = 1), df['quality_new']","fbb10704":"Y.replace({'Good' : 1, 'Bad' : 0}, inplace = True)","962f2526":"X.describe()","3b3e1d1c":"fig, axs = plt.subplots(nrows = 4, ncols = 3, figsize = [6.4 * 3, 4.8 * 4])\nfig.subplots_adjust(hspace = .15, wspace = .15, top = 0.93)\naxs[-1, -1].axis('off')\naxs = axs.ravel()\nfig.suptitle('Boxplot for different Physicochemical Variables in the Data', size = 25, fontweight = 'bold', y = 0.98)\nfor i, col in enumerate(X.columns):\n    sns.boxplot(y = col, data = X, ax = axs[i])\n    axs[i].set_xlabel(col, size = 15)\nplt.show()","4b00283d":"# Function to return Indices of Outliers \ndef indicies_of_outliers(x): \n    Q1, Q3 = x.quantile([0.25, 0.75]) \n    IQR = Q3 - Q1\n    lower_limit = Q1 - (1.5 * IQR)\n    upper_limit = Q3 + (1.5 * IQR)\n    return np.where((x > upper_limit) | (x < lower_limit))[0] ","5735b6fc":"outlier_indices = set()\nfor col in X.columns:\n    outlier_indices = set(outlier_indices | set(indicies_of_outliers(X[col])))\nprint(f'Percentage of Outlier Removal is {len(outlier_indices)\/X.shape[0]*100:.2f} %.')\nX.drop(outlier_indices, axis = 0, inplace = True)\nY.drop(outlier_indices, axis = 0, inplace = True)","a0461ce7":"fig, axs = plt.subplots(nrows = 4, ncols = 3, figsize = [6.4 * 3, 4.8 * 4])\nfig.subplots_adjust(hspace = .25, wspace = .15, top = 0.93)\naxs[-1, -1].axis('off')\naxs = axs.ravel()\nfig.suptitle('Distribution plot for different Physicochemical Variables in the Original Data', size = 25, fontweight = 'bold', y = 0.98)\nfor i, col in enumerate(X.columns):\n    sns.distplot(X[col], ax = axs[i])\n    axs[i].set_xlabel(f'{col} with skewness : {skew(X[col].dropna()):.2f}', size = 15)\nplt.show()","d2e9683a":"print(f'Number of Strictly Positive Values in each Column : \\n\\n{(X > 0).sum()}')","d0506c56":"Power_Transform = PowerTransformer(method = 'yeo-johnson')\nfig, axs = plt.subplots(nrows = 4, ncols = 3, figsize = [6.4 * 3, 4.8 * 4])\nfig.subplots_adjust(hspace = .25, wspace = .15, top = 0.93)\naxs[-1, -1].axis('off')\naxs = axs.ravel()\nfig.suptitle('Distribution plot for different Physicochemical Variables in the Transformd Data', size = 25, fontweight = 'bold', y = 0.98)\nfor i, col in enumerate(X.columns):\n    if abs(skew(df[col])) > 0.75:\n        Power_Transform.fit(np.array(X[col]).reshape(-1,1))\n        X[col] = Power_Transform.transform(np.array(X[col]).reshape(-1,1))\n    sns.distplot(X[col], ax = axs[i])\n    axs[i].set_xlabel(f'{col} with skewness : {skew(X[col].dropna()):.2f}', size = 15)\nplt.show()","19b9e5a3":"scaler = MinMaxScaler()\nfig, axs = plt.subplots(nrows = 4, ncols = 3, figsize = [6.4 * 3, 4.8 * 4])\nfig.subplots_adjust(hspace = .25, wspace = .15, top = 0.93)\naxs[-1, -1].axis('off')\naxs = axs.ravel()\nfig.suptitle('Distribution plot for different Physicochemical Variables in the Scaled Transformed Data', size = 25, fontweight = 'bold', y = 0.98)\nfor i, col in enumerate(X.columns):\n    X[col] = scaler.fit_transform(np.array(X[col]).reshape(-1,1))\n    sns.distplot(X[col], ax = axs[i])\n    axs[i].set_xlabel(f'{col} with skewness : {skew(X[col].dropna()):.2f}', size = 15)\nplt.show()","ebad4a15":"plt.figure(figsize = [20, 10])\nsns.heatmap(X.corr(), annot = True, vmin = -1, vmax = 1)\nplt.show()","fc7451cd":"corr_cols = [('fixed acidity', 'citric acid'), ('fixed acidity', 'density'), ('free sulfur dioxide', 'total sulfur dioxide'), \n             ('fixed acidity' , 'pH'), ('volatile acidity', 'citric acid'), ('density', 'alcohol')]\nfig, axs = plt.subplots(nrows = 2, ncols = 3, figsize = [6.4 * 3, 4.8 * 3])\nfig.subplots_adjust(hspace = .15, wspace = .15, top = 0.93)\nfig.suptitle('Scatter Plots of the pairs of Variables having Significant Correlation among themselves', size = 25, fontweight = 'bold')\naxs = axs.ravel()\nfor i, corr_col in enumerate(corr_cols):\n    sns.scatterplot(x = corr_col[0], y = corr_col[1], data = X, ax = axs[i])\nplt.show()","6dae1798":"fig_1, axs = plt.subplots(nrows = 4, ncols = 3, figsize = [6.4 * 3, 4.8 * 4])\nfig_1.subplots_adjust(hspace = .15, wspace = .15, top = 0.93)\naxs[-1, -1].axis('off')\naxs = axs.ravel()\nfig_1.suptitle('Boxplot for different Physicochemical Variables in the Final Data', size = 25, fontweight = 'bold', y = 0.98)\nfor i, col in enumerate(X.columns):\n    sns.boxplot(y = col, data = X, ax = axs[i])\n    axs[i].set_xlabel(col, size = 15)\nplt.show()","7eb144cd":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.25)","db8bc8db":"def Model_Fit(Predictor, Response, Model, Model_Params = None, Imbalanced_Class = False, Imbalanced_Classification_Sampling_Technique = None, \n              Test_Percentage = None, Stratification = False, Grid_Search = False, Print_Scores = False):\n    \"\"\"\n    This function will help to fit a given Classification Model with given Predictor and Response variables .\n    \n    Parameters\n    ----------\n    Predictor : pandas.core.frame.DataFrame <Required>\n        A pandas dataframe containing all the predictor variables.\n        \n    Response : pandas.core.series.Series <Required>\n        A pandas series object containing the response variable.\n        \n    Model : str <Required>\n        The name of the model (without parentheses) in string format.\n        \n    Model_Params : dict , default = None\n        A dictionary conatining keys as the exact name of model (without parentheses) \n        and the values will be the parameters of the correspondoing model. \n        It is required while using 'Grid_Search = true'.\n        An example is :\n                        model_params = {\n                            'SVC' : {\n                                    'C'                : list(np.arange(0.5, 1.5, 0.1)), \n                                    'kernel'           : ['linear', 'poly', 'rbf', 'sigmoid'], \n                                    'gamma'            : list(np.arange(0.5, 1.5, 0.1)), \n                                    'class_weight'     : ['balanced', None]\n                                    }\n                            }\n                            \n    Imbalanced_Class : bool, default = False\n        Whether to apply Imbalanced Classification Sampling Technique or not.\n        \n    Imbalanced_Classification_Sampling_Technique = str, default = None\n        The name of the Imbalanced Classification Sampling Technique (without parentheses) in string format.\n        It is required while using 'Imbalanced_Class = true'.\n        \n    Test_Percentage : float, default = None\n         A fraction of the dataset which will be used as test set to evaluate the prediction of the model.\n         \n    Stratification : bool, default = False\n        Whether to use Stratified Sampling for splliting the data into train and test or not.\n        \n    Grid_Search : bool, default = False\n        Whether to use GridSearchCV to select the best model params out the given model params or not.\n    \n    Print_Scores : bool, default = False\n        Whether to print the different evaluation metrics for the prediction of the model or not.\n    \"\"\"\n    X, Y, test_per = Predictor, Response, float(Test_Percentage)\n    \n    if Test_Percentage == None:\n        while(True):\n            test_per = input('Please give the percentage of Test Data \\n (Value must be between 0 and 1) : ') \n            if float(test_per) > 0 and float(test_per) < 1:\n                Test_Percentage = test_per\n                break\n            else:\n                continue\n    \n    Models = {\n        'LogisticRegression' : LogisticRegression(), 'KNeighborsClassifier' : KNeighborsClassifier(), \n        'DecisionTreeClassifier' : DecisionTreeClassifier(), 'RandomForestClassifier' : RandomForestClassifier(), 'SVC' : SVC(), \n        'LinearDiscriminantAnalysis' : LinearDiscriminantAnalysis(), 'QuadraticDiscriminantAnalysis' : QuadraticDiscriminantAnalysis(), \n        'CategoricalNB' : CategoricalNB(), 'SGDClassifier' : SGDClassifier()\n    }\n    Imbalanced_Classification_Sampling_Techniques = {\n        'RandomUnderSampler' : RandomUnderSampler(), 'RandomOverSampler' : RandomOverSampler(), 'SMOTE' : SMOTE(), 'ADASYN' : ADASYN()\n    }\n    if Imbalanced_Class:\n            if Imbalanced_Classification_Sampling_Technique in Imbalanced_Classification_Sampling_Techniques:\n                imb_ = Imbalanced_Classification_Sampling_Techniques[Imbalanced_Classification_Sampling_Technique]\n                X, Y = imb_.fit_resample(X, Y)\n            else:\n                print('Please Enter a Valid Imbalanced Classification Sampling Techniques. '\n                      f'Available Techniques are : \\n\\t {list(Imbalanced_Classification_Sampling_Techniques.keys())}')\n    if Stratification:\n        Split_ = StratifiedShuffleSplit(n_splits = 1, test_size = test_per)\n        for Train_, Test_ in Split_.split(X, Y):\n            X_Train, X_Test = X.iloc[Train_].reset_index(drop = True), X.iloc[Test_].reset_index(drop = True)\n            Y_Train, Y_Test = Y.iloc[Train_].reset_index(drop = True), Y.iloc[Test_].reset_index(drop = True)\n    else:\n        X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = test_per)\n            \n    if Model in Models.keys():\n        kf = KFold(n_splits = 10, shuffle = True).get_n_splits(pd.concat((X_Train, Y_Train), axis = 1).values)\n        if Grid_Search:\n            model_ = GridSearchCV(Models[Model], Model_Params[Model], scoring = 'roc_auc', cv = kf)\n        else:\n            model_ = Models[Model]\n        model_.fit(X_Train, Y_Train)\n        Y_Pred = model_.predict(X_Test)\n\n        Area_Under_ROC_Curve = roc_auc_score(Y_Test , Y_Pred)\n\n        fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = [20, 6])\n        fig.subplots_adjust(wspace = .25, top = 0.8)\n        axs = axs.ravel()\n        fig.suptitle(f'For the Predictions using {Model} Model', size = 25, fontweight = 'bold')\n        fpr, tpr, _ = roc_curve(Y_Test , Y_Pred)\n        axs[0].plot(fpr, tpr, 'k-', label = f'AUC : {roc_auc_score(Y_Test , Y_Pred)}')\n        axs[0].plot([0, 1], [0, 1], 'k--')\n        axs[0].set_xlim([-0.05, 1.05])\n        axs[0].set_ylim([-0.05, 1.05])\n        axs[0].set_xlabel('False Positive Rate', size = 15)\n        axs[0].set_ylabel('True Positive Rate', size = 15)\n        axs[0].set_title('Receiver Operating Characteristic (ROC) Curve', size = 20)\n        axs[0].legend(loc = 'lower right')\n\n        sns.heatmap(pd.DataFrame(confusion_matrix(Y_Test , Y_Pred)), annot = True, fmt = 'd', cmap = 'Blues', ax = axs[1])\n        axs[1].set_xlabel('Predicted Class', size = 15)\n        axs[1].set_ylabel('Actual Class', size = 15)\n        axs[1].set_title('Confusion Matrix', size = 20)\n        plt.show()\n        \n        if Print_Scores:\n            print(\n                'For the Final Model, \\n\\n'\n                f' Accuracy   :  {accuracy_score(Y_Test , Y_Pred)}\\n'\n                f' Precision  :  {precision_score(Y_Test , Y_Pred)}\\n'\n                f' Recall     :  {recall_score(Y_Test , Y_Pred)}\\n'\n                f' F1 Score   :  {f1_score(Y_Test , Y_Pred)}\\n\\n'\n                'and a detail Classification Report is given below: \\n\\n'\n                f'{classification_report(Y_Test, Y_Pred, target_names = [\"Good Wine (1)\", \"Bad Wine (0)\"], digits = 8)}'\n            )\n        Output = {'Model Name'                                     :   Model, \n                  'Fitted Model'                                   :   model_, \n                  'Imbalanced Classification Sampling Technique'   :   Imbalanced_Classification_Sampling_Technique, \n                  'Area Under ROC Curve'                           :   Area_Under_ROC_Curve}\n        return(Output)\n    else:\n        print(f'Please Enter a Valid Model Name. Available Models are : \\n\\t {list(Models.keys())}')","4223bc72":"Models = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'SVC', \n          'LinearDiscriminantAnalysis', 'QuadraticDiscriminantAnalysis', 'SGDClassifier']","c8d2e9f2":"Imbalanced_Classification_Sampling_Techniques = ['RandomUnderSampler', 'RandomOverSampler', 'SMOTE', 'ADASYN']","4f2702ed":"Model_Params = {\n    'LogisticRegression'              :   {\n                                           'C'                : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000], \n                                           'l1_ratio'         : list(np.arange(0.0, 1.1, 0.1)), \n                                           'penalty'          : ['l1', 'l2', 'elasticnet']\n                                          }, \n    'KNeighborsClassifier'            :   {\n                                           'n_neighbors'      : list(range(2, 10, 1)), \n                                           'weights'          : ['uniform', 'distance'], \n                                           'algorithm'        : ['auto', 'ball_tree', 'kd_tree', 'brute'], \n                                           'p'                : list(range(0, 3, 1))\n                                          }, \n    'DecisionTreeClassifier'          :   {\n                                           'criterion'        : ['gini', 'entropy'], \n                                           'splitter'         : ['best', 'random'],  \n                                           'class_weight'     : ['balanced', None]\n                                          }, \n    'RandomForestClassifier'          :   {\n                                           'n_estimators'     : list(range(100, 400, 100)), \n                                           'criterion'        : ['gini'], \n                                           'class_weight'     : ['balanced', None]\n                                          }, \n    'SVC'                             :   {\n                                           'C'                : list(np.arange(0.5, 1.5, 0.1)), \n                                           'kernel'           : ['linear', 'poly', 'rbf', 'sigmoid'], \n                                           'gamma'            : list(np.arange(0.5, 1.5, 0.1)), \n                                           'class_weight'     : ['balanced', None]\n                                          }, \n    'LinearDiscriminantAnalysis'      :   {\n                                           'solver'           : ['svd', 'lsqr', 'eigen'], \n                                           'shrinkage'        : ['auto', None]\n                                          }, \n    'QuadraticDiscriminantAnalysis'   :   {}, \n    'SGDClassifier'                   :   {\n                                           'loss'             : ['hinge', 'log', 'squared_hinge', 'modified_huber', 'perceptron'], \n                                           'penalty'          : ['l2', 'l1', 'elasticnet'], \n                                           'class_weight'     : ['balanced', None], \n                                           'early_stopping'   : [True], \n                                           'n_iter_no_change' : [5]\n                                          }\n}","4fd304dd":"Final_Model_Name, Final_Fitted_Model, Final_IMB, Final_AUC = None, None, None, 0\nfor Model in Models:\n    for IMB_Sampling_Technique in Imbalanced_Classification_Sampling_Techniques:\n        print(f'Model : {Model} \\nImbalanced Classification Sampling Technique : {IMB_Sampling_Technique} \\n\\n')\n        Model_Info = Model_Fit(X, Y, Test_Percentage = 0.25, Model = Model, Model_Params = Model_Params, Imbalanced_Class = True, \n                               Imbalanced_Classification_Sampling_Technique = IMB_Sampling_Technique, \n                               Stratification = True, Grid_Search = False, Print_Scores = False)\n        Model_Name_, Fitted_Model_, IMB_Sampling_Technique_Name, AUC_ = Model_Info.values()\n        if AUC_ > Final_AUC: \n            Final_Model_Name, Final_Fitted_Model, Final_IMB, Final_AUC = Model_Name_, Fitted_Model_, IMB_Sampling_Technique_Name, AUC_","9b0da6e3":"Final_Model_Info = Model_Fit(X, Y, Test_Percentage = 0.25, Model = Final_Model_Name, Model_Params = Model_Params, \n                             Imbalanced_Class = True, Imbalanced_Classification_Sampling_Technique = Final_IMB, \n                             Stratification = True, Grid_Search = True, Print_Scores = True)","1da58a60":"def calc_auc_score(model, n_split):\n    kf = KFold(n_splits = n_split, shuffle = True).get_n_splits(pd.concat((X, Y), axis = 1).values)\n    auc = cross_val_score(model, X, Y, scoring = \"roc_auc\", cv = kf)\n    return(auc)","146d4e13":"auc_score = calc_auc_score(model = Final_Model_Info[\"Fitted Model\"], n_split = 10)","c9f716e4":"print(\n    'So, for the Final Model : \\n\\n'\n    f'\\t Imbalanced Classification Sampling Technique   :   {Final_Model_Info[\"Imbalanced Classification Sampling Technique\"]}\\n'\n    f'\\t Model Name                                     :   {Final_Model_Info[\"Model Name\"]}\\n'\n    f'\\t Best Set of Model Parameters                   :   {Final_Model_Info[\"Fitted Model\"].best_params_}\\n'\n    f'\\t Area Under ROC Curve                           :   {Final_Model_Info[\"Area Under ROC Curve\"]}\\n'\n    f'\\t Mean of Cross Validation Score (\"roc_auc\")     :   {auc_score.mean()}\\n'\n    )","63d76621":"Name of the Columns","25d4756d":"Now, let's define a function which will calculate the __Cross - Validation Score__ for the _Best Fitted Model_ over the Final Data by splliting it into a number of parts.","462c5b68":"Now, let's encode the Response variable by replacing __Good__ with $1$ and __Bad__ with $0$.","db84ddad":"Let's look at the Box Plots of the variables to confirm the presence of Outlier.\n> From the following Box Plots, presence of outlier is confirmed in all the variables of the dataset.","d385486f":"Let's look at the returns of _.describe()_ method.\n> From the following Table, it can be noticed that the columns _fixed acidity_ , _residual sugar_ , _free sulfur dioxide_ , _total sulfur dioxide_ have confirmed outlier, whereas, presence of outlier in _volatile acidity_ , _citric acid_ , _chlorides_ , _sulphates_ and _alcohol_ can be doubted.","8b97f4e5":"## <center> Model Evaluation <\/center>","bb1343dd":"So, now, a Final ( __Model__, __Imbalanced Classification Sampling Technique__ ) have been chosen on the basis of __Area Under ROC Curve__ and this pair will be used to fit the Final Data again to search the __Best Set of Parameters__.","026ca376":"Now, as the range of different variables are different hence, there is a need to scale the data uniformly. There are many ways to scale a data, but here __MinMaxScaler()__ is chosen from _scikit learn_ class to perform the task. This process will convert the range of all the variables from $0$ to $1$.\n> From the following Distribution plots, it can be seen that, all the variables are now range from $0$ to $1$ which was the objective of the scaling procedure to have the same range for all the variables so that the predictive model will evaluate all the variables equally in order to predict the quality.","cc02f8ac":"Finally, let's take a look at the Box Plots of the variables from the Final Dataset.","113fa8a2":"Now, let's check the counts of each Rating of Red Wine in the Dataset.\n> From the following Bar Diagram, it can be noticed that, Ratings $5$ and $6$ holds the major proportion in the data with counts $681$ and $638$ respectively whereas, among the others $199$ observations are under $7$ rating. All the other ratings have less than $100$ observations.","ecbbefd1":"Here is the list of __Classification Models__ which will be applied on the Final Dataset and the Best Model will be chosen on the basis of __Area Under ROC Curve__.","e2ca5b69":"Here is the list of __Imbalanced Classification Sampling Techniques__ which will be applied on the Final Dataset to solve the imbalance problem in the Response Variable and the Best ( _Model_ , _Imbalanced Classification Sampling Technique_ ) pair will be chosen on the basis of __Area Under ROC Curve__.","45ce5834":"### An Overview of Data","33b4a815":"## <center> Predictive Modelling <\/center>","35393a36":"Now, let's define a function which will help to fit different __Classification Models__ with different __Conditions__.","e9f5b178":"Now, let's form a loop which will perform model fitting on each pair of ( _Model_ , _Imbalanced Classification Sampling Technique_ ) to choose the Best pair on the basis of __Area Under ROC Curve__.","7239dd20":"Here is the Dictionary of __Model Parameters__ on which _GridSearchCV_ can be applied to select the Best Model Parameters for all the models chosen above.","5fef789a":"Now, let's segregate the whole dataset into _Train_ and _Test_.","962748bc":"### Observations from the Data\n\n- The dataset has $1599$ number of records.\n- There is __no Null Values (Missing Values)__ in the dataset.\n- There are total __$11$ Predictor Variables__ in the dataset.\n- All the __Predictor Variables__ in the dataset are __Numerical__ in nature.\n- Though the __Response Variable Quality__ has Numerical values but it should be __Categorical__ in nature, and if it is then it will have six unique values $3$, $4$, $5$, $6$, $7$, $8$.","b47a6471":"Basic Informations","473b5912":"Let's find the __Cross - Validation Score__ for the Final Model based on the _'roc_auc'_ method.","43dea519":"Now, let's take a look at the Distributions of the variables in the dataset.\n> From the following Distribution plots, it can be seen that, _residual sugar_ and _chlorides_ have skewness more than $3$ which means these variables are __Highly Skewed__ , where _free sulphur dioxide_ , _total sulphur dioxide_ and _sulphates_ have skewness more than $1$ but less than $3$ which means these variables are __Moderately Skewed__ and _fixed acidity_ , _volatile acidity_ and _alcohol_ have skewness more than $0.5$ but less than $1$ which menas these variables are __Not So Much Skewed (Not Even Gaussian)__ .","a324548f":"Next, let's segregate the dataset into __Predictor Variables (X)__ and __Response Variable (Y)__.","a504bfa1":"So, _citric acid_ variable has few zero or negative values, which implies __Box - Cox Transformation__ can not be applied in this Dataset to convert the variables into Normal Distribution as this particular transformation requires all positive values. So, instead of Box-Cox Transformation, __Yeo - Johnson Transformation__ is being applied here to perform the mentioned task.\n> From the following Distribution plots, it can be seen that, none of the variables now having absolute skewness more than 0.5 which means all the variables are now following __Gaussian Like Distribution__ , which will help in the prediction of wine quality.","907350eb":"Now, Let's take a look at the Scatter Plots of the pairs of variables mentioned above having significant correlation among themselves.","17801850":"Now, let's check what is the Proportion of different Class in the Response Variable.\n> So, from the following Pie Chart, it is clear that, the Ratio of different Class __Good__ & __Bad__ in the Response Variable __Quality__ is $17 : 108$.","094c03d0":"So, let's remove the outliers first.","3c675bee":"Now, let's take a look at the __Correlation Matrix__.\n> From the following Correlation Plot of the _Predictor Variable_ , it can be observed that, ( _fixed acidity_ , _citric acid_ ), ( _fixed acidity_ , _density_ ), ( _free sulfur dioxide_ , _total sulfur dioxide_ ), these three pairs have significant positive correlation more than $0.5$, ( _fixed acidity_ , _pH_ ), ( _volatile acidity_ , _citric acid_ ), ( _density_ , _alcohol_ ), these four pairs have significant negative correlation less than $- 0.5$,whereas few other columns have correlation coefficient between $0.3$ & $0.4$.","60d6ed46":"## <center> Description of the Data <\/center>\n<br><\/br>\nThis Dataset is enriched with the values obtained from the Physicochemical laboratory tests and the Sensory test. Here are the name of the variables:<br><\/br>\n\n| # | Input variables (based on physicochemical tests) | Output variable (based on sensory data) |\n| --- | --- | --- |\n| 1. | fixed acidity | quality (score between 0 and 10) |\n| 2. | volatile acidity |   | |   |\n| 3. | citric acid |   |\n| 4. | residual sugar |   |\n| 5. | chlorides |   |\n| 6. | free sulfur dioxide |   |\n| 7. | total sulfur dioxide |   |\n| 8. | density |   |\n| 9. | pH |   |\n| 10. | sulphates |   |\n| 11. | alcohol |   |\n\n<br><\/br>  \n\n","620588cf":"So, let's bin __Quality__ column in _Good_ and _Bad_. Here the threshold limit has been set to $7$.","2ce2e412":"First 10 Records","656580e0":"# <center> Prediction of Red Wine Quality <\/center>\n![image.png](attachment:image.png)","244c281f":"## <center> Context <\/center>\n<br><\/br>\nWine is perhaps the oldest fermented product known to mankind, with its origin dating back to somewhere around $6000$ BC in Georgia.Once viewed as a luxury good, nowadays wine is increasingly enjoyed by a wider range of consumers. The top $5$ wine exporting countries as of $2019$ are as follows:\n<br><\/br>\n\n| Country Name | Net Worth (in Billions of US Dollars) | Percentage of Total Wine Exports|\n| ------ | ------ | ------ |\n| France | $11$ | $30.4$ % |\n| Italy | $7.3$ | $20.3$ % |\n| Spain | $3.1$ | $8.7$ % |\n| Australia | $2.1$ | $5.8$ % |\n| Chile | $1.9$ | $5.3$ % |\n\n<br><\/br>\nwith more than $70.5$ % of the market share in $2019$. The worldwide Wine market has grown at above-average rates within the Alcoholic Drinks market and is expected to reach more than US Dollar $439$ Billion in terms of revenue by $2023$.[<cite>[1][1]<\/cite>]<br><\/br>\nTo support its growth, the wine industry is investing in new technologies for both wine making and selling processes. Wine certification and quality assessment are key elements within this context. Certification prevents the illegal adulteration of wines (to safeguard human health) and assures quality for the wine market. Quality evaluation is often part of the certification process and can be used to improve wine making (by identifying the most influential factors) and to stratify wines such as premium brands (useful for setting prices). Wine certification is generally assessed by physicochemical and sensory tests. Physicochemical laboratory tests routinely used to characterize wine include determination of density, alcohol or pH values, while Sensory tests rely mainly on human experts. It should be stressed that taste is the least understood of the human senses thus wine classification is a difficult task.[<cite>[2][2]<\/cite>]<br><\/br>\n\n[1]: https:\/\/agriexchange.apeda.gov.in\/Weekly_eReport\/Wine_Report.pdf\n[2]: https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0167923609001377","5b5fbd61":"## <center> Exploratory Data Analysis & Data Pre - Processing <\/center>"}}