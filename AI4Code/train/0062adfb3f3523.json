{"cell_type":{"01748320":"code","8cbd208f":"code","ccf4b70d":"code","d8e08169":"code","70b1d739":"code","f22497bb":"code","561584cc":"code","5c6abf22":"code","043748ca":"code","ccc982d1":"code","14e1a194":"code","d221908b":"code","8ad71b0e":"code","a2d0c935":"code","ad5a91f7":"code","9553e37e":"code","5978f20e":"code","857cc951":"code","c9124abb":"code","8da63e63":"code","82a46a48":"code","e7926b19":"code","459af93a":"code","6c137e76":"code","dceba744":"code","8b58d9e7":"code","847de46b":"code","8acf0d79":"code","2470518d":"code","6334fbb3":"code","76e50182":"code","23f9e944":"code","53bcebe4":"code","f9148b90":"code","951f03e2":"code","863bed10":"code","08e1a5ce":"code","25aad0d8":"code","dae0fa42":"code","00d39c75":"code","f2324b2d":"code","ed2a9cc6":"code","ff445545":"code","6a6e0988":"code","ae0eec1c":"code","2db42509":"code","e3de9add":"markdown","3abcc800":"markdown","3f1f97ea":"markdown","85f84b31":"markdown","395828ba":"markdown"},"source":{"01748320":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8cbd208f":"train=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","ccf4b70d":"train.head()","d8e08169":"train.describe()","70b1d739":"train.info()","f22497bb":"\ntrain.isnull().sum()","561584cc":"train['SkinThickness'].hist()","5c6abf22":"train['Insulin'].hist()","043748ca":"train['BMI'].hist()","ccc982d1":"#Imputing the null values with respective column median values since the data is skewed.\ntrain['SkinThickness'].replace(np.NaN,train['SkinThickness'].median(),inplace=True)\ntrain['Insulin'].replace(np.NaN,train['Insulin'].median(),inplace=True)\ntrain['BMI'].replace(np.NaN,train['BMI'].median(),inplace=True)","14e1a194":"train.isnull().sum()","d221908b":"train.boxplot()","8ad71b0e":"#Number of pregnancies imputing values greater than (mean+3*std. dev) or (Q3+1.5*IQR) to the one which is greater\ntrain['Pregnancies'] = np.where(train['Pregnancies'] > 14, 14,train['Pregnancies'])\n#Concentration of plasma glucose imputing values lesser than (mean-3*std. dev) or (Q1-1.5*IQR) to the one which is greater\ntrain['Glucose'] = np.where(train['Glucose'] < 33, 33,train['Glucose'])\n# Normal diastolic blood pressure levels\ntrain['BloodPressure'] = np.where(train['BloodPressure'] < 60, 60,train['BloodPressure'])\n#Tricep skinfold thickness imputing values greater than (mean+3*std. dev) or (Q3+1.5*IQR) to the one which is greater\ntrain['SkinThickness'] = np.where(train['SkinThickness'] > 65, 65,train['SkinThickness'])\n# Insulin concentration in the serum after 2 hours should be in the range < 177) (which is considered normal). Since, we have too many outliers we scale them to Q3+1.5*IQR\ntrain['Insulin'] = np.where(train['Insulin'] > 390, 390,train['Insulin'])\n# BMI will be scaled to a minimum of Q1-1.5*IQR\ntrain['BMI'] = np.where(train['BMI'] < 15, 15,train['BMI'])\n# Probability cannot be greater than 1\ntrain['DiabetesPedigreeFunction'] = np.where(train['DiabetesPedigreeFunction'] >1, 1,train['DiabetesPedigreeFunction'])\n\n","a2d0c935":"train.describe()","ad5a91f7":"\n#Dividing the dataset into X and Y variables\nx=train.drop('Outcome',axis=1)\ny=train['Outcome']","9553e37e":"from pandas.plotting import scatter_matrix\nattributes=train.columns.values\nscatter_matrix(train[attributes], figsize = (25,25), c=y, alpha = 0.6, marker = 'O')","5978f20e":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","857cc951":"x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=0)","c9124abb":"sc=StandardScaler()\nx_sc=sc.fit_transform(x_train)\nxt_sc=sc.fit_transform(x_test)","8da63e63":"#Hyperparameter Tuning theb Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\n\nc_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\ntrain_score_l1 = []\ntrain_score_l2 = []\ntest_score_l1 = []\ntest_score_l2 = []\nfor c in c_range:\n    log_l1 = LogisticRegression(penalty = 'l1', C = c, solver='liblinear')\n    log_l2 = LogisticRegression(penalty = 'l2', C = c, solver='liblinear')\n    log_l1.fit(x_sc, y_train)\n    log_l2.fit(x_sc, y_train)\n    train_score_l1.append(log_l1.score(x_sc, y_train))\n    train_score_l2.append(log_l2.score(x_sc, y_train))\n    test_score_l1.append(log_l1.score(xt_sc, y_test))\n    test_score_l2.append(log_l2.score(xt_sc, y_test))","82a46a48":"print('Train_score_l1:',train_score_l1)\nprint('Test_score_l1:',test_score_l1)\nprint('Train_score_l2:',train_score_l2)\nprint('Test_score_l2:',test_score_l2)","e7926b19":"from sklearn.model_selection import GridSearchCV\nlogit = LogisticRegression()\nparam = { 'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1','l2']}\nlogistic = GridSearchCV(logit,param,scoring='accuracy',cv=5)\nlogistic.fit(x_sc,y_train)","459af93a":"logistic.best_params_","6c137e76":"logit=LogisticRegression(penalty='l2',C=0.1)\nlogit.fit(x_sc,y_train)\nprint([logit.score(x_sc,y_train),logit.score(xt_sc,y_test)])","dceba744":"#Feature importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\ncoef = list(logit.coef_[0])\nlabels = list(x.columns)\nfeatures = pd.DataFrame()\nfeatures['Features'] = labels\nfeatures['importance'] = coef\n\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures['positive'] = features['importance'] > 0\nfeatures.set_index('Features', inplace=True)\nfeatures.importance.plot(kind='barh', figsize=(11, 6),color = features.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance')","8b58d9e7":"logit.coef_","847de46b":"#Plotting train and test scores\nplt.plot(c_range, train_score_l1, label = 'Train score, penalty = l1')\nplt.plot(c_range, test_score_l1, label = 'Test score, penalty = l1')\nplt.plot(c_range, train_score_l2, label = 'Train score, penalty = l2')\nplt.plot(c_range, test_score_l2, label = 'Test score, penalty = l2')\nplt.legend()\nplt.xlabel('Regularization parameter: C')\nplt.ylabel('Accuracy')\nplt.xscale('log')","8acf0d79":"#Decision boundary\nfrom mlxtend.plotting import plot_decision_regions\n\nX_b = x_sc[10:50, [1,5]]\ny_b = np.array(y_train[10:50])\n\nlreg = LogisticRegression()\nlreg.fit(X_b, y_b) \n\nplot_decision_regions(X_b, y_b, clf = lreg)","2470518d":"from sklearn.neighbors import KNeighborsClassifier","6334fbb3":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_score_array = []\ntest_score_array = []\n\nfor k in range(1,20):\n    knn = KNeighborsClassifier(k)\n    knn.fit(x_sc, y_train)\n    train_score_array.append(knn.score(x_sc, y_train))\n    test_score_array.append(knn.score(xt_sc, y_test))","76e50182":"\nprint(train_score_array)\nprint(test_score_array)","23f9e944":"knn = KNeighborsClassifier()\nparam = { 'n_neighbors':[7,8,9]}\nknnc = GridSearchCV(knn,param,scoring='accuracy',cv=5)\nknnc.fit(x_sc,y_train)","53bcebe4":"\nknnc.best_params_","f9148b90":"x_axis = range(1,20)\n%matplotlib inline\nplt.plot(x_axis, train_score_array, label = 'Train Score', c = 'g')\nplt.plot(x_axis, test_score_array, label = 'Test Score', c='b')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend()","951f03e2":"#Decision boundary\nfrom mlxtend.plotting import plot_decision_regions\n\nX_b = x_sc[100:150,[1,5]]\ny_b = np.array(y_train[100:150])\n\nknn = KNeighborsClassifier(9)\nknn.fit(X_b, y_b) \n\nplot_decision_regions(X_b, y_b, clf = knn)","863bed10":"from sklearn.svm import LinearSVC, SVC","08e1a5ce":"#Decision boundary\nX_b = x_sc[100:150,[1,5]]\ny_b = np.array(y_train[100:150])\n\nsvc = LinearSVC()\nsvc.fit(X_b, y_b) \n\nplot_decision_regions(X_b, y_b, clf = svc)","25aad0d8":"#LinearSVC \nsvc=LinearSVC(C=1)\nsvc.fit(x_sc,y_train)\nprint(svc.score(x_sc,y_train))\nprint(svc.score(xt_sc,y_test))","dae0fa42":"from matplotlib import gridspec\nimport itertools\nC = 10\nclf1 = LinearSVC(C=C)\nclf2 = SVC(kernel='linear', C=C)\nclf3 = SVC(kernel='rbf', gamma=0.75, C=C)\nclf4 = SVC(kernel='poly', degree=3, C=C)\n\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['LinearSVC',\n          'SVC with linear kernel',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X_b, y_b)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_b, y=y_b,\n                                clf=clf, legend=2)\n    plt.title(lab)","00d39c75":"from matplotlib import gridspec\nimport itertools\nC = 10\nclf1 = SVC(kernel='rbf', gamma=0.01, C=C)\nclf2 = SVC(kernel='rbf', gamma=0.1, C=C)\nclf3 = SVC(kernel='rbf', gamma=1, C=C)\nclf4 = SVC(kernel='rbf', gamma=10, C=C)\n\nX_b = x_sc[100:150,[1,5]]\ny_b = np.array(y_train[100:150])\n\nmodels = (SVC(kernel='rbf', gamma=0.01, C=10),\n         SVC(kernel = 'rbf', gamma = 0.1, C = 10),\n         SVC(kernel = 'rbf', gamma = 1, C = 10),\n         SVC(kernel = 'rbf', gamma = 10, C = 10))\n\n\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['gamma = 0.01',\n          'gamma = 0.1',\n          'gamma = 1',\n          'gamma = 10']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X_b, y_b)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_b, y=y_b,\n                                clf=clf, legend=2)\n    plt.title(lab)","f2324b2d":"train_score_array = []\ntest_score_array = []\ng=[0.01,0.1,1,10,100]\nfor i in g:\n  svc = SVC(kernel='rbf', gamma=i, C=1)\n  svc.fit(x_sc, y_train)\n  train_score_array.append(svc.score(x_sc, y_train))\n  test_score_array.append(svc.score(xt_sc, y_test))","ed2a9cc6":"print(train_score_array)\nprint(test_score_array)","ff445545":"svc=SVC(C=1, kernel='rbf', gamma=0.1)\nsvc.fit(x_sc,y_train)\nprint(svc.score(x_sc,y_train))\nprint(svc.score(xt_sc,y_test))","6a6e0988":"#After trying various values for max_depth the maximum accuracy obtained was at max_depth=5\nfrom sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(random_state=0, max_depth=5)\n\ndtree.fit(x_sc, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(dtree.score(x_sc, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(dtree.score(xt_sc, y_test)))","ae0eec1c":"features=x.columns.values","2db42509":"def plot_feature_importances_cancer(model):\n    n_features = x.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), features)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\n\nplot_feature_importances_cancer(dtree)","e3de9add":"**SVC with Kernel**","3abcc800":"As We can see, there are null values present in the dataset in columns D, E and F. We must impute these null values with mean or median depending on the data distribution.","3f1f97ea":"**LinearSVC**","85f84b31":"**Decision Tree**","395828ba":"**KNN**"}}