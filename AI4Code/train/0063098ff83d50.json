{"cell_type":{"d06c4f61":"code","8967cda5":"code","7a3b0e19":"code","abb29838":"code","8f7d3d08":"code","74a2f994":"code","1089811b":"code","1232edf9":"code","5ec6bd23":"code","06418762":"code","1d5fbb8a":"code","58f50336":"code","a277c25c":"code","22a2be2f":"code","6439d68e":"code","b045258a":"code","8aa8735f":"code","752436a3":"code","2d10eeed":"code","665bc91b":"code","e5b6e3f7":"code","0aca9f42":"code","3f6f3af4":"code","aece5ca5":"code","ad1ca9cc":"code","00481031":"code","56e6e5de":"code","91c96c64":"code","f436745c":"code","3ec2e163":"code","84ef34c6":"code","6a2ff8fe":"code","3c142b79":"code","15877365":"code","0a6ca455":"code","d2751f0a":"code","d04dd4fa":"code","9b2cc1fd":"code","b63e53f8":"code","54481fda":"code","a73ada85":"code","81749a4f":"code","083adcaf":"code","2eaa6cd3":"code","74580153":"code","49299487":"code","1f361915":"code","a6f53c9a":"code","dd83e138":"markdown","725ffc98":"markdown","34a8a511":"markdown","8a4e7425":"markdown","d28adaf4":"markdown","df868831":"markdown","3bfc23d2":"markdown","0fa6d148":"markdown","83dd4549":"markdown","a2feb3c9":"markdown","eb166b03":"markdown","2c0c383b":"markdown","5dcc5dc0":"markdown","f871a5b4":"markdown","9f16a879":"markdown","a05d2fdb":"markdown","26b04d61":"markdown","a19c1cb0":"markdown","7a5b47e0":"markdown","17452a78":"markdown","8edd10ba":"markdown","29252db4":"markdown","064221a0":"markdown","be0772ea":"markdown","10bb329a":"markdown","2e9d4e86":"markdown","333accb4":"markdown","52bbac69":"markdown","85f72035":"markdown","a60c70e0":"markdown","2cc03331":"markdown","53013a83":"markdown","8ed19e27":"markdown","a112db07":"markdown","e1ae5109":"markdown","28272749":"markdown","f8879059":"markdown","4bf5036d":"markdown","2f9cbee7":"markdown"},"source":{"d06c4f61":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom xgboost import XGBRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","8967cda5":"df_test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nprint(\"Train shape:\",df_train.shape)\nprint(\"Test Shape:\",df_test.shape)","7a3b0e19":"X_trainfull=df_train.drop([\"SalePrice\"], axis=1)\ny=df_train.SalePrice","abb29838":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Sales Price (y)\")\nsns.distplot(y)\nplt.show()","8f7d3d08":"y=np.log1p(y)\n\nplt.figure(figsize=(8,4))\nplt.title(\"Distribution of log Sales Price (y)\")\nsns.distplot(y)\nplt.xlabel(\"Log of Sales Price\")\nplt.show()","74a2f994":"d_temp=X_trainfull.isna().sum().sort_values(ascending=False)\nd_temp=d_temp[d_temp>0]\nd_temp=d_temp\/df_train.shape[0]*100\n\nplt.figure(figsize=(8,5))\nplt.title(\"Features Vs Percentage Of Null Values\")\nsns.barplot(y=d_temp.index,x=d_temp, orient='h')\nplt.xlim(0,100)\nplt.xlabel(\"Null Values (%)\")\nplt.show()","1089811b":"na_index=(d_temp[d_temp>20]).index\nX_trainfull.drop(na_index, axis=1, inplace=True)","1232edf9":"num_cols=X_trainfull.corrwith(y).abs().sort_values(ascending=False).index\nX_num=X_trainfull[num_cols]\nX_cat=X_trainfull.drop(num_cols,axis=1)","5ec6bd23":"X_num.sample(5)","06418762":"high_corr_num=X_num.corrwith(y)[X_num.corrwith(y).abs()>0.5].index\nX_num=X_num[high_corr_num]","1d5fbb8a":"plt.figure(figsize=(10,6))\nsns.heatmap(X_num.corr(), annot=True, cmap='coolwarm')\nplt.show()\n\nprint(\"Correlation of Each feature with target\")\nX_num.corrwith(y)","58f50336":"X_num=X_num[high_corr_num]\nX_num.drop(['TotRmsAbvGrd','GarageArea','1stFlrSF','GarageYrBlt'],axis=1, inplace=True)","a277c25c":"#function to handle NA\ndef handle_na(df, func):\n    \"\"\"\n    Input dataframe and function \n    Returns dataframe after filling NA values\n    eg: df=handle_na(df, 'mean')\n    \"\"\"\n    na_cols=df.columns[df.isna().sum()>0]\n    for col in na_cols:\n        if func=='mean':\n            df[col]=df[col].fillna(df[col].mean())\n        if func=='mode':\n            df[col]=df[col].fillna(df[col].mode()[0])\n    return df","22a2be2f":"X_num=handle_na(X_num, 'mean')","6439d68e":"# Function to scale df \ndef scale_df(df):\n    \"\"\"\n    Input: data frame\n    Output: Returns minmax scaled Dataframe \n    eg: df=scale_df(df)\n    \"\"\"\n    scaler=MinMaxScaler()\n    for col in df.columns:\n        df[col]=scaler.fit_transform(np.array(df[col]).reshape(-1,1))\n    return df","b045258a":"X_num=scale_df(X_num)","8aa8735f":"X_train, X_val, y_train, y_val=train_test_split(X_num,y, test_size=0.2)\nmodel=LinearRegression()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","752436a3":"model=SVR()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","2d10eeed":"model=RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","665bc91b":"model=XGBRegressor(learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","e5b6e3f7":"num_features=X_num.columns","0aca9f42":"X_cat.sample(5)","3f6f3af4":"X_cat.describe()","aece5ca5":"for feature in X_cat.columns:\n    print(\n        f\"{feature} :{len(X_cat[feature].unique())}: {X_cat[feature].unique()}\"\n    )","ad1ca9cc":"cat_na=X_cat.isna().sum().sort_values(ascending=False)\ncat_na=cat_na[cat_na>30]\nX_cat.drop(cat_na.index, axis=1, inplace=True)","00481031":"for feature in X_cat.columns:\n    plt.figure(figsize=(4,6))\n    plt.title(f\"{str(feature)} vs log Sale Price\")\n    sns.boxplot(X_cat[feature],y)\n    plt.show()","56e6e5de":"X_cat=handle_na(X_cat, 'mode')","91c96c64":"le=LabelEncoder()\nX_cat_le=pd.DataFrame()\nfor col in X_cat.columns:\n    X_cat_le[col] = le.fit_transform(X_cat[col])","f436745c":"Xc_train, Xc_test, yc_train,yc_test=train_test_split(X_cat_le,y, test_size=0.2)","3ec2e163":"model=RandomForestRegressor()\nmodel.fit(Xc_train,yc_train)","84ef34c6":"print(f\"Train score : {model.score(Xc_train,yc_train)}\")\nprint(f\"Test score : {model.score(Xc_test,yc_test)}\")","6a2ff8fe":"feat_imp=pd.DataFrame({\"Feature\":Xc_train.columns,\"imp\":model.feature_importances_})\nfeat_imp=feat_imp.sort_values('imp', ascending=False)\n\nplt.figure(figsize=(10,4))\nplt.title(\"Feature Importance\", fontsize=16)\nsns.barplot('Feature', 'imp', data=feat_imp)\nplt.xticks(rotation=80)\nplt.show()","3c142b79":"feat=[]\nscore_train=[]\nscore_test=[]\nfor i in range(29):\n    imp_ft=feat_imp.head(i+1).Feature.unique()\n\n    X_cat_imp=pd.DataFrame()\n    for col in imp_ft:\n        X_cat_imp[col] = le.fit_transform(X_cat[col])\n\n    Xc_train, Xc_test, yc_train,yc_test=train_test_split(X_cat_imp,y, test_size=0.2)\n\n    model=RandomForestRegressor(n_estimators=100)\n    model.fit(Xc_train,yc_train)\n    feat.append(i+1)\n    score_train.append(model.score(Xc_train,yc_train))\n    score_test.append(model.score(Xc_test,yc_test))\n    \nacc_feat_df=pd.DataFrame({\"Feature\":feat,\"TrainAcc\":score_train,\"ValAcc\":score_test})","15877365":"plt.figure(figsize=(10,5))\nsns.lineplot('Feature', 'TrainAcc', data=acc_feat_df, label=\"Training Accuracy\")\nsns.lineplot('Feature', 'ValAcc', data=acc_feat_df, label=\"Validation Accuracy\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"R2 Score\")\nplt.xticks(rotation=80)\nplt.xlim(1,29)\nplt.show()","0a6ca455":"cat_features=list(feat_imp.iloc[:17,0])","d2751f0a":"# Selecting only important features\nX_cat=X_cat[cat_features]\n# OHE features\nX_cat=pd.get_dummies(X_cat)\n# Scaling the data\nX_cat=scale_df(X_cat)","d04dd4fa":"X_train, X_val, y_train, y_val=train_test_split(X_cat,y, test_size=0.2)","9b2cc1fd":"model=LinearRegression()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","b63e53f8":"model=SVR()\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","54481fda":"model=RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","a73ada85":"model=XGBRegressor(learning_rate=0.1)\nmodel.fit(X_train,y_train)\nprint(f\"Train score : {model.score(X_train,y_train)}\")\nprint(f\"Validation score : {model.score(X_val,y_val)}\")","81749a4f":"#Combine train and test data\nXtt=pd.concat([X_trainfull,df_test])\n\n#Split into Numeric and categoric features\nXtt_num= Xtt[num_features]\nXtt_cat= Xtt[cat_features]\n\n#Handling null values\nXtt_cat=handle_na(Xtt_cat, 'mode')\nXtt_num=handle_na(Xtt_num,'mean')\n\n#OHE Categoric features\nXtt_cat=pd.get_dummies(Xtt_cat,drop_first=True)\n\n#Combine Numeric and Categorical features\nXtt=pd.concat([Xtt_num,Xtt_cat], axis=1)\n\n#Scale Features\nXtt=scale_df(Xtt)\n\n#Training and Testing Features after Feature Engineering\nX=Xtt.iloc[:df_train.shape[0],:]\nX_test=Xtt.iloc[df_train.shape[0]:,:]\n\n#Training and Validation features and target\nX_train, X_val, y_train, y_val=train_test_split(X,y, test_size=0.2)","083adcaf":"model_LR=LinearRegression()\nmodel_LR.fit(X_train,y_train)\nprint(f\"Train score : {model_LR.score(X_train,y_train)}\")\nprint(f\"Validation score : {model_LR.score(X_val,y_val)}\")\nyv_LR=model_LR.predict(X_val)\nyt_LR=model_LR.predict(X_test)","2eaa6cd3":"model_SV=SVR()\nmodel_SV.fit(X_train,y_train)\nprint(f\"Train score : {model_SV.score(X_train,y_train)}\")\nprint(f\"Validation score : {model_SV.score(X_val,y_val)}\")\nyv_SVR=model_SV.predict(X_val)\nyt_SVR=model_SV.predict(X_test)","74580153":"model_RF=RandomForestRegressor(n_estimators=100)\nmodel_RF.fit(X_train,y_train)\nprint(f\"Train score : {model_RF.score(X_train,y_train)}\")\nprint(f\"Validation score : {model_RF.score(X_val,y_val)}\")\nyv_RF=model_RF.predict(X_val)\nyt_RF=model_RF.predict(X_test)","49299487":"model_XGB=XGBRegressor(learning_rate=0.1)\nmodel_XGB.fit(X_train,y_train)\nprint(f\"Train score : {model_XGB.score(X_train,y_train)}\")\nprint(f\"Validation score : {model_XGB.score(X_val,y_val)}\")\nyv_XGB=model_XGB.predict(X_val)\nyt_XGB=model_XGB.predict(X_test)","1f361915":"yv_stacked=np.column_stack((yv_SVR,yv_RF,yv_XGB))\nyt_stacked=np.column_stack((yt_SVR,yt_RF,yt_XGB))\n\nmeta_model=LinearRegression()\nmeta_model.fit(yv_stacked,y_val)\nprint(meta_model.score(yv_stacked,y_val))\n\ny_final=np.expm1(meta_model.predict(yt_stacked))","a6f53c9a":"sub = pd.DataFrame()\nsub[\"Id\"] = df_test.Id\nsub[\"SalePrice\"] = y_final\nsub.to_csv(\"submission.csv\", index=False)","dd83e138":"# Distribution of Saleprice","725ffc98":"# Calculate Training and Validation Accuracy for different number of features","34a8a511":"# Read Data","8a4e7425":"## Model Testing : Only Numerical Features","d28adaf4":"# Remove multi-colinear features ","df868831":"# Scale values","3bfc23d2":"# Drop features where more than 20% records are null","0fa6d148":"# List the Numerical features required","83dd4549":"# Split Categorical and Numeric Features","a2feb3c9":"# List of selected Categorical Features ","eb166b03":"# Heat-map of highly correlated Features","2c0c383b":"# Drop features with more than 30 null values","5dcc5dc0":"# FEATURE ENGINEERING IN COMBINED TRAIN AND TEST DATA","f871a5b4":"# NUMERICAL FEATURES: FEATURE SELECTION AND ENGINEERING","9f16a879":"# Label encode features","a05d2fdb":"# Import Libraries","26b04d61":"# Split into Train and validation set","a19c1cb0":"Observation:\n* We can observe significant increase in train and validation accuracy with increase in features intitially.\n* After around 10 features, no significant improvement can be observed in either train or validation accuracy.\n* This is known as Curse of Dimensionality.\n* We can select the ideal number of features depending \n* I am selecting top 17 features for training","7a5b47e0":"It can be observed from above that y is right-skewed, log transform can be applied to make it normal distribution.","17452a78":"# Model Testing Only catagorical Featues","8edd10ba":"Drop na>20% fields","29252db4":"# Feature importance from RF Model","064221a0":"# Training, Evaluation and Prediction","be0772ea":"# Prepare Submission file","10bb329a":"# View sample data","2e9d4e86":"Observation:\n* Linear Regression and SVM models show similar performance moderately good score in both training and validation data\n* Random forest model is overfitting\n* XGB Regressor seems to be the best suited model","333accb4":"# CATEGORICAL DATA FEATURE SELECTION AND ENGINEERING","52bbac69":"# Separate features and target","85f72035":"Observation:\n* Linear Regression preforms very poorly in validation.\n* Other three models have similar accuracy in validation, eventhough Random Forest model is overfitting.","a60c70e0":"# Model Stacking","2cc03331":"# Fit and Evaluate Random Forest Model","53013a83":"# EDA: Relation between each feature and saleprice","8ed19e27":"# UPVOTE THE KERNEL IF YOU FIND IT HELPFUL","a112db07":"# Plot Number of Features vs Model Performance","e1ae5109":"# Explore Data","28272749":"# Percentage of null valued features in Train data","f8879059":"# Handling Null values","4bf5036d":"# Handling Null Values","2f9cbee7":"# Identify Features Highly correlated with target"}}