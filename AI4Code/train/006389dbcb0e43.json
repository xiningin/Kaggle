{"cell_type":{"ca4d9b20":"code","ffeed944":"code","1bf0cece":"code","3391149c":"code","4a44b992":"code","46863ebb":"code","9a1c4c0e":"code","014bd352":"code","17c6c83d":"code","3c42e856":"code","53d48736":"code","d0f570a7":"code","33dcb990":"code","aa117a62":"code","f7ae8b44":"code","fb25b8ef":"code","26c353d4":"code","efbbfe40":"code","e1ecd94e":"code","f2e38585":"code","ea227999":"code","6481179e":"code","c3d75043":"code","c73718b2":"code","4054b8d3":"code","2eb7376f":"code","242c22e9":"code","386a419a":"code","c98d23f0":"code","dfce80e8":"code","f3360fdc":"code","ad75a384":"code","d34e3011":"code","033d0019":"markdown","d780c332":"markdown","f2439707":"markdown","c4ab0723":"markdown","f353bb13":"markdown","9c77a833":"markdown","60801fe2":"markdown","af544806":"markdown","9dc6fd94":"markdown","5fd55fe7":"markdown","8c1d9fde":"markdown","f6f9deab":"markdown","8b97b0b6":"markdown","7be4429a":"markdown","2225e71a":"markdown","a6c82d7c":"markdown","b56117e0":"markdown","3d953b38":"markdown","b2bc00bb":"markdown","b01fcbca":"markdown","eaa0cb28":"markdown","0781316c":"markdown","b42b8d7d":"markdown","d0f2a462":"markdown","1b45d5a1":"markdown","fc1e4586":"markdown","38c13f61":"markdown","fab69bf7":"markdown","63418364":"markdown","7a8444ce":"markdown","ac20cec3":"markdown","7efb0065":"markdown","e76e61a0":"markdown"},"source":{"ca4d9b20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport xarray as xr\n\nfrom pathlib import Path\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, validation_curve\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nsns.set(rc={'figure.figsize':(15,15)})\nsns.set_style(\"whitegrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffeed944":"# Directories where the data is saved \n\ndata_dir_production = Path('..\/input\/productionenergyfrance')\ndata_dir_climate = Path('..\/input\/renewables-project\/climate_france\/climate_france\/')\n                        \n# Template filenames and masks\nfilename_mask = 'mask_datagouv_french_regions_merra2_Nx_France.nc'\nfilename_climate = 'merra2_area_selection_output_{}_merra2_2010-2019.nc'\nfilename_production = 'eCO2mix_RTE_'\nfilepath_mask = Path(data_dir_climate, filename_mask)\nds_mask = xr.load_dataset(filepath_mask)\nda_mask = ds_mask['mask']","1bf0cece":"### Create functions to load the datasets\n\n# Lists with the name of the features and outputs, necessary to access and read the files\nfeatures_names = ['zonal_wind', 'upper_zonal_wind', 'meridional_wind', 'upper_meridional_wind',\n                  'height_500', 'surface_density', 'surface_specific_humidity', 'surface_temperature',\n                  'surface_downward_radiation']\n\nfeatures_real_name = {'zonal_wind' : 'zonal wind (m\/s)',  'upper_zonal_wind' :  'upper zonal wind (m\/s)', 'meridional_wind' : 'meridional wind (m\/s)', 'height_500' : 'height at 500 hPa (m)',  'upper_meridional_wind' :  'upper meridional wind (m\/s)',\n                    'surface_density' : 'air density at surface', 'surface_specific_humidity' : 'surface specific humidity (g\/cm3)','surface_temperature' : 'surface air temperature (K)' , 'surface_downward_radiation' : 'surface incoming shortwave flux (kW\/m\u00b2)'}\n\nregion_names = {\n                2 : 'Auvergne-Rhone-Alpes', 3 : 'Bourgogne-Franche-Comte', 4 : 'Bretagne', 5 : 'Centre-Val de Loire',\n                6 : 'Grand-Est', 7: 'Hauts-de-France', 8 : 'Ile-de-France', 9 : 'Normandie', 10 : 'Nouvelle-Aquitaine',\n                11 : 'Occitanie', 12 : 'Pays-de-la-Loire', 13 : \"PACA\"\n                }\n\nregion_names_files = {\n                2 : 'Auvergne-Rhne-Alpes', 3 : 'Bourgogne-Franche-Comt', 4 : 'Bretagne', 5 : 'Centre-Val-de-Loire',\n                6 : 'Grand-Est', 7: 'Hauts-de-France', 8 : 'Ile-de-France', 9 : 'Normandie', 10 : 'Nouvelle-Aquitaine',\n                11 : 'Occitanie', 12 : 'Pays-de-la-Loire', 13 : \"PACA\"\n                }\n\nregion_names_latin = {\n                2 : 'Auvergne-Rh\u00f4ne-Alpes', 3 : 'Bourgogne-Franche-Comt\u00e9', 4 : 'Bretagne', 5 : 'Centre-Val de Loire',\n                6 : 'Grand-Est', 7: 'Hauts-de-France', 8 : 'Ile-de-France', 9 : 'Normandie', 10 : 'Nouvelle-Aquitaine',\n                11 : 'Occitanie', 12 : 'Pays-de-la-Loire', 13 : \"PACA\"\n                }\n\nyears = ['2014', '2015', '2016', '2017', '2018', '2019']\n\nnames = ['P\u00e9rim\u00e8tre','Nature', 'Date','Heures', 'Consommation', 'Thermique',\n         'Nucl\u00e9aire', 'Eolien', 'Solaire', 'Hydraulique', 'Pompage', 'Bio\u00e9nergies', 'Ech. physiques']\n\nregion_without_PACA = ['Auvergne-Rhone-Alpes','Bourgogne-Franche-Comte','Bretagne','Centre-Val de Loire',\n                'Grand-Est','Hauts-de-France', 'Ile-de-France','Normandie', 'Nouvelle-Aquitaine',\n                'Occitanie','Pays-de-la-Loire']\n\ndef create_climate_data_sets(variable_names):\n    '''\n    Function to create the dataset\n    The energy dataset is created with one value for each day and for each region, we choose to put the climate df on the same format\n    input : variable_names list of names of the features \n    output : dataframe of the climate variables for the region PACA\n    '''\n    df = pd.DataFrame()\n    for variable_name in variable_names:\n        #Fetch for the files and load them into xarray\n        filename = filename_climate.format(variable_name) # ex: if variable_names = feature_names, for the first increment, feature_name = merra2_area_selection_output_zonal_wind_merra2_2010-2019.nc\n        filepath = Path(data_dir_climate, filename) # the path to access the data file\n        da_climate = xr.load_dataset(filepath)[variable_name] # creates a pd.Dataset from the climate file\n        da_climate = da_climate.groupby(da_mask).mean().rename(mask='region')\n        da_climate['region'] = da_climate['region'].values\n        #convert into pd.DataFrame with single date index with region columns \n        da_climate = da_climate.to_dataframe()\n        da_climate = da_climate.reset_index(level=[0,1]).set_index('time') # the df has the region as numbers from 2 to 13 as a value in a column\n        for key, region in region_names.items():\n            da_climate['region'].loc[da_climate.region == key] = region\n            values = da_climate[da_climate['region'] == region]\n            da_climate[region] = ''\n            da_climate[region] = values[variable_name]\n        #drop any region other than PACA\n        da_climate = da_climate.drop(columns = region_without_PACA)\n        #put all features dataset in the same format\n        da_climate = da_climate.drop(columns = ['region', variable_name]).loc['2014-01-01':]\n        da_climate = da_climate.drop_duplicates()\n        df[variable_name] = da_climate\n        \n    df = df.rename(columns = features_real_name)\n    return df.dropna()\n\ndef create_energy_production_data_set(region_names_files, region_names_latin, years):\n    ''''\n    Function to create the energy production datasets\n    input : region names, years both list of strings and type of energy\n    output : pd.dataframe \n    '''\n    \n    df = pd.DataFrame()\n    for region in region_names_files.values():\n        for year in years:\n            filename = filename_production + region + '_Annuel-Definitif_' + year + '.csv'\n            filepath = Path(data_dir_production, filename)\n            df_prod = pd.read_csv(filepath, index_col=0, header=0, parse_dates=True, sep=',', encoding='latin1', names=names)\n            df = df.append(df_prod)\n    df.drop(df.tail(1).index,inplace=True)\n    #Deleting the values between each hour\n    df = df.dropna()\n    df = df[df['Heures'].str.slice(2,4) != '30']   \n    df = df[df['Heures'].str.slice(3,5) != '30']   \n    #Setting the index to datetime\n    df = df.reset_index()\n    df['time'] = pd.to_datetime(df['Date'] + ' ' + df['Heures'])\n    df = df.set_index('time')\n    df_region_list = []\n    for region in region_names_latin.values():\n        values = df[df['P\u00e9rim\u00e8tre'] == region]\n        values = values.rename(columns = {'Solaire' : region})\n        df_region_list.append(values)\n    df = pd.concat(df_region_list, axis=1)\n    df = df.drop(['Consommation', 'Thermique', 'Nucl\u00e9aire', 'Eolien', 'Hydraulique', 'Pompage', 'Bio\u00e9nergies', 'Ech. physiques', 'Nature', 'Date', 'Heures', 'P\u00e9rim\u00e8tre'], axis = 1)\n    df = df.rename(columns = {'Auvergne-Rh\u00f4ne-Alpes' : 'Auvergne-Rhone-Alpes' , 'Bourgogne-Franche-Comt\u00e9' : 'Bourgogne-Franche-Comte', 'PACA' : 'Solar energy produced in PACA'})\n    df = df.drop(columns = region_without_PACA)\n    return df\n            \ndf_climate = create_climate_data_sets(features_names)\ndf_production = create_energy_production_data_set(region_names_files, region_names_latin, years)","3391149c":"sns.set(font_scale=5)\nfig, axes = plt.subplots(5, 2, figsize=(100,100))\nfor idx, column in enumerate(df_climate):\n    i = idx % 5\n    j = idx \/\/ 5 \n    sns.scatterplot(x = df_climate[column].index, y = df_climate[column], data=df_climate, ax = axes[i, j])\nsns.scatterplot(x = df_production.index, y = df_production['Solar energy produced in PACA'], data=df_production, ax = axes[4, 1])\naxes[4, 1].set_ylabel(\"Solar energy production (MWh)\")\nplt.show()\n","4a44b992":"### Analyze the data and search for zeros and missing values\n\ndef missing_zero_values_table(df):\n        zero_val = (df == 0.00).astype(int).sum(axis=0)\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] \/ len(df)\n        mz_table['Data Type'] = df.dtypes\n        mz_table = mz_table[\n            mz_table.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n            \"There are \" + str(mz_table.shape[0]) +\n              \" columns that have missing values.\")\n        return mz_table\n\nprint(missing_zero_values_table(df_climate))\nprint(missing_zero_values_table(df_production))","46863ebb":"###Converts the production df to the size of the climate df\n\ndef convert_df_to_same_index(df, index_ref):\n    index = df.index\n    sym_diff = index_ref.symmetric_difference(index)\n    return df.drop(index = sym_diff)\n\nindex_ref = df_climate.index\ndf_production = convert_df_to_same_index(df_production, index_ref)","9a1c4c0e":"### Normalizing the data\n\ndf_production_2014 = df_production.loc['2014-01-01':'2014-12-31']\ndf_production_2015 = df_production.loc['2015-01-01':'2015-12-31']\ndf_production_2016 = df_production.loc['2016-01-01':'2016-12-31']\ndf_production_2017 = df_production.loc['2017-01-01':'2017-12-31']\ndf_production_2018 = df_production.loc['2018-01-01':'2018-12-31']\ndf_production_2019 = df_production.loc['2019-01-01':'2019-12-31']\ndf_production_list = [df_production_2015, df_production_2016, df_production_2017, df_production_2018, df_production_2019]\n\ndef normalize_df(df, df_reference):\n    df_ratio = df_reference.max() \/ df.max()\n    df_normalized = df * df_ratio\n    return df_normalized\n\nmax_prod = df_production_2019.max()[0]\n\ndf_production_normalized =  normalize_df(df_production_2014, df_production_2019) \nfor df in df_production_list:\n    df_production_normalized = df_production_normalized.append(normalize_df(df,df_production_2019))\n\nsns.set(font_scale=2)\nplt.figure(figsize=(10, 10))\nax = plt.axes()\nsns.scatterplot(x = df_production.index, y = df_production_normalized['Solar energy produced in PACA'], data=df_production_normalized, ax=ax)\nax.set_title('Normalized solar energy production data from 2014 to 2019')\nax.set_ylabel('Normalized solar energy production (MWh)')\nplt.show()","014bd352":"### Compute the correlation matrix for each region between the normalized and non normalized solar energy production and the outputs \n\ncorrelation_df_nn = df_climate.join(df_production['Solar energy produced in PACA'])\ncorrelation_df_n = df_climate.join(df_production_normalized['Solar energy produced in PACA'])\ncmap = sns.diverging_palette(500, 10, as_cmap=True)\n\nsns.set(font_scale=8)\nf, axes = plt.subplots(2, 1, figsize = (150,400))\ncorrelation_matrix_non_normalized = correlation_df_nn.corr() \nsns.heatmap(correlation_matrix_non_normalized, xticklabels=correlation_matrix_non_normalized.columns, \n            yticklabels=correlation_matrix_non_normalized.columns, cmap = cmap\n            , ax = axes[0], annot=True)\naxes[0].set_title('Correlation matrix with the non normalized solar energy production')\n\ncorrelation_matrix_normalized = correlation_df_n.corr() \nsns.heatmap(correlation_matrix_normalized, xticklabels=correlation_matrix_normalized.columns, \n            yticklabels=correlation_matrix_normalized.columns, cmap = cmap\n            , ax = axes[1], annot=True)\naxes[1].set_title('Correlation matrix with the normalized solar energy production')\n","17c6c83d":"df_climate.drop(['upper zonal wind (m\/s)','upper meridional wind (m\/s)'], axis=1, inplace=True)","3c42e856":"### Important Hyperparameters and data definitions\n\nfolds = 10\nmax_iter = 1e6\nlw=2\nX = df_climate\ny = df_production_normalized\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1\/folds), random_state=4)\nsns.set(font_scale=2)\n\n### This function will be used to plot all validation curves\n\ndef plot_validation_curve_prediction(model, X_test, y_test, X_train, y_train, param_name, param_range, name, hyperparameter):\n    \"\"\"\n    Function to plot the validation curve, also returns the optimal parameter\n    \"\"\"\n    train_scores, test_scores = validation_curve(model, X_train, y_train, param_name = param_name, param_range=param_range, n_jobs=1)\n    fig, ax = plt.subplots(1, 1, figsize = (20,20))\n    #Validation curve\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    test_score_max = max(test_scores_mean)\n    param_max = param_range[test_scores_mean.tolist().index(test_score_max)]\n    ax.set_title(\"Validation Curve with\" + name + ', the max R2 score is of ' + str(round(test_score_max, 4)) + '(mean) with ' + hyperparameter +' equal to ' + str(round(param_max, 10)))\n    ax.set_xlabel(hyperparameter)\n    ax.set_ylabel(\"R2 Score\")\n    ax.set_ylim(0.0, 1.1)\n    ax.fill_between(param_range,train_scores_mean - train_scores_std,train_scores_mean + train_scores_std,alpha=0.2,color=\"darkorange\",lw=lw, label='Training score')\n    ax.semilogx(param_range, test_scores_mean, label=\"R2 score\", color=\"navy\", lw=lw)\n    ax.fill_between(param_range,test_scores_mean - test_scores_std,test_scores_mean + test_scores_std,alpha=0.2,color=\"navy\",lw=lw, label=\"Test score\")\n    ax.legend(loc=\"best\")\n\n    ","53d48736":"###Plot the linear regression curve\n\nreg_linear = linear_model.LinearRegression()\nmodel = make_pipeline(RobustScaler(), reg_linear)\nmodel.fit(X_train, y_train)\ny_hat_test = pd.DataFrame(model.predict(X_test))\ny_hat_test.index = X_test.index\n\ny_hat_test_linear = y_hat_test\nlinear_regression_mse_score = mean_squared_error(y_test, y_hat_test)\nlinear_regression_score = model.score(X_test, y_test)\n\nax = plt.axes()\nax.set_title('Results for the Linear Regression model with an R2 score of ' + str(round(linear_regression_score, 4)))\nax.set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_test[0], data=y_hat_test, label='prediction')\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2)\nplt.show()","d0f570a7":"reg_lasso = linear_model.Lasso(max_iter=max_iter)\nmodel_lasso = make_pipeline(RobustScaler(), reg_lasso)\nparam_range_lasso = np.linspace(1e-3,10.,50)\nparam_name_lasso = 'lasso__alpha'\nplot_validation_curve_prediction(model_lasso, X_test, y_test, X_train, y_train, param_name_lasso, param_range_lasso, ' Lasso', r'$\\alpha$')","33dcb990":"reg_ridge = linear_model.Ridge()\nmodel_ridge = make_pipeline(RobustScaler(), reg_ridge)\nparam_range_ridge = np.linspace(1e-6,600.,50)\nparam_name_ridge = 'ridge__alpha'\nplot_validation_curve_prediction(model_ridge, X_test, y_test, X_train, y_train, param_name_ridge, param_range_ridge, ' Ridge', r'$\\alpha$')","aa117a62":"reg_sgd = linear_model.SGDRegressor()\nmodel_sgd = make_pipeline(RobustScaler(), reg_sgd)\nparam_range_sgd = np.linspace(1e-5,10.,50)\nparam_name_sgd = 'sgdregressor__alpha'\nplot_validation_curve_prediction(model_sgd, X_test, y_test, X_train, y_train, param_name_sgd, param_range_sgd, ' SGD Regressor', r'$\\alpha$')","f7ae8b44":"### Plot the three predictions\n\nparam_max_lasso = 1e-3\nparam_max_ridge = 1e-6\nparam_max_sgd = 1e-5\n\nreg_lasso = linear_model.Lasso(alpha=param_max_lasso, max_iter=max_iter)\nmodel = make_pipeline(RobustScaler(), reg_lasso)\nmodel.fit(X_train, y_train)\ny_hat_lasso = pd.DataFrame(model.predict(X_test))\ny_hat_lasso.index = X_test.index\n\nlasso_mse_score = mean_squared_error(y_test, y_hat_lasso)\nlasso_score = model.score(X_test, y_test)\n\nreg_ridge = linear_model.Ridge(alpha=param_max_ridge, max_iter=max_iter)\nmodel = make_pipeline(RobustScaler(), reg_ridge)\nmodel.fit(X_train, y_train)\ny_hat_ridge = pd.DataFrame(model.predict(X_test))\ny_hat_ridge.index = X_test.index\n\nridge_mse_score = mean_squared_error(y_test, y_hat_ridge)\nridge_score = model.score(X_test, y_test)\n\nreg_sgd = linear_model.SGDRegressor(alpha=param_max_sgd)\nmodel = make_pipeline(RobustScaler(), reg_sgd)\nmodel.fit(X_train, y_train)\ny_hat_sgd = pd.DataFrame(model.predict(X_test))\ny_hat_sgd.index = X_test.index\n\nsgd_mse_score = mean_squared_error(y_test, y_hat_sgd)\nsgd_score = model.score(X_test, y_test)\n\nfig, axes = plt.subplots(3, 1, figsize=(30,50))\nsns.set(font_scale=3)\n\n\naxes[0].set_title('Results for the Lasso Regression model with an R2 score of ' + str(round(lasso_score, 4)))\naxes[0].set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_lasso[0], data=y_hat_lasso, label='prediction', ax=axes[0] )\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2, ax=axes[0])\n\naxes[1].set_title('Results for the Ridge Regression model with an R2 score of ' + str(round(ridge_score, 4)))\naxes[1].set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_ridge[0], data=y_hat_ridge, label='prediction', ax=axes[1] )\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2, ax=axes[1])\n\naxes[2].set_title('Results for the SGD Regressor model with an R2 score of ' + str(round(sgd_score, 4)))\naxes[2].set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_ridge[0], data=y_hat_sgd, label='prediction', ax=axes[2] )\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2, ax=axes[2])\n\nplt.show()","fb25b8ef":"reg_tree = DecisionTreeRegressor(max_features='sqrt')\nmodel = make_pipeline(RobustScaler(), reg_tree)\nparam_range_tree = np.linspace(1,31,30)\nparam_name_tree = 'decisiontreeregressor__max_depth'\nplot_validation_curve_prediction(model, X_test, y_test, X_train, y_train, param_name_tree, param_range_tree, ' Decision Tree Regressor', 'maximum depth')","26c353d4":"sns.set(font_scale=2)\n\n### Plot the tree prediction\n\nreg_tree = DecisionTreeRegressor(max_features='sqrt', max_depth=11.3)\nmodel = make_pipeline(RobustScaler(), reg_tree)\nmodel.fit(X_train, y_train)\ny_hat_tree = pd.DataFrame(model.predict(X_test))\ny_hat_tree.index = X_test.index\n\ntree_mse_score = mean_squared_error(y_test, y_hat_tree)\ntree_score = model.score(X_test, y_test)\n\nax = plt.axes()\nax.set_title('Results for the Decision Tree Regressor model with an R2 score of ' + str(round(tree_score, 4)))\nax.set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_tree[0], data=y_hat_tree, label='prediction')\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2)\nplt.show()","efbbfe40":"reg_rfr = RandomForestRegressor(max_features='sqrt')\nmodel = make_pipeline(RobustScaler(), reg_rfr)\nparam_range_rfr = [10 + 15*i for i in range(14)]\nparam_name_rfr = 'randomforestregressor__n_estimators'\nplot_validation_curve_prediction(model, X_test, y_test, X_train, y_train, param_name_rfr, param_range_rfr, ' Random Forest Regressor', 'number of estimators')","e1ecd94e":"### Plot the Random Forest Regressor prediction\n\nreg_rfr = RandomForestRegressor(max_features='sqrt', n_estimators=180)\nmodel = make_pipeline(RobustScaler(), reg_rfr)\nmodel.fit(X_train, y_train)\ny_hat_rfr = pd.DataFrame(model.predict(X_test))\ny_hat_rfr.index = X_test.index\n\nrfr_mse_score = mean_squared_error(y_test, y_hat_rfr)\nrfr_score = model.score(X_test, y_test)\n\nax = plt.axes()\nax.set_title('Results for the Random Forest Regressor model with an R2 score of ' + str(round(rfr_score, 4)))\nax.set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_rfr[0], data=y_hat_rfr, label='prediction')\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2)\nplt.show()","f2e38585":"index = X.columns\nsns.set(font_scale=1)\n\nimportances = reg_rfr.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in reg_rfr.estimators_], axis=0)\nforest_importances = pd.Series(importances, index=index)\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances\")\nax.set_ylabel(\"Mean decrease in impurity\")\nax.tick_params(axis='x', rotation=45)\nplt.show()","ea227999":"sns.set(font_scale=2)\nplt.figure(figsize=(35, 25))\ndepth = [tree.get_depth() for tree in reg_rfr.estimators_]\nindex = [i for i in range(len(reg_rfr.estimators_))]\nax = plt.axes()\nsns.barplot(x=index, y=depth, ax=ax)\nax.set_ylabel('Max depth of the estimators')\nax.set_title('Max depth of the trees composing the Random Forest Regressor with a mean of ' + str(sum(depth)\/len(depth)))\nax.tick_params(axis='x',labelsize=10, rotation=90)\nplt.show()","6481179e":"#Gradient Boosting Regressor\n\nreg_gboost = GradientBoostingRegressor(learning_rate=0.04, max_depth=10, n_estimators=1000, subsample=0.9)\nmodel = make_pipeline(RobustScaler(), reg_gboost)\nmodel.fit(X_train, y_train)\ny_hat_gboost = pd.DataFrame(model.predict(X_test))\ny_hat_gboost.index = X_test.index\n\ngboost_mse_score = mean_squared_error(y_test, y_hat_gboost)\ngboost_score = model.score(X_test, y_test)\n\nsns.set(font_scale=2)\nax = plt.axes()\nax.set_title('Results for the Gradient Boosting Regressor model with an R2 score of ' + str(round(gboost_score, 4)))\nax.set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_gboost[0], data=y_hat_gboost, label='prediction')\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2)\nplt.show()","c3d75043":"sns.set(font_scale=1)\nmodel_scores = {'Linear Regression' : 0, 'Lasso Regression' : 0, 'Ridge Regression' : 0, 'SGD Regressor' : 0 , 'Decision Tree Regressor' : 0, 'Random Forest Regressor' : 0,}\n\n#Plot the scores\n\nmodel_scores['Linear Regression'] = linear_regression_score\nmodel_scores['Lasso Regression'] = lasso_score\nmodel_scores['Ridge Regression'] = ridge_score\nmodel_scores['SGD Regressor'] = sgd_score\nmodel_scores['Random Forest Regressor'] = rfr_score\nmodel_scores['Decision Tree Regressor'] = tree_score\nmodel_scores['Gradient Boosting Regressor'] = gboost_score\n\nmse_scores = [linear_regression_mse_score, lasso_mse_score, ridge_mse_score, sgd_mse_score, tree_mse_score, rfr_mse_score, gboost_mse_score]\n\nmodels = pd.Series(list(model_scores.keys()))\nscores = pd.Series(list(model_scores.values()))\ndf_scores = pd.DataFrame()\ndf_scores[\"Estimators\"] = models\ndf_scores[\"R2 Score\"] = scores\ndf_scores[\"MSE Score\"] =pd.Series(mse_scores)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\nsns.barplot(x='Estimators', y='R2 Score', data = df_scores,\n             palette='GnBu', ax=ax[0])\nax[0].set_title(' R2 Score for all the estimators tested')\nax[0].tick_params(axis='x', rotation=45)\nsns.barplot(x='Estimators', y='MSE Score', data = df_scores,\n             palette='GnBu', ax=ax[1])\nax[1].set_title(' MSE Score for all the estimators tested')\nax[1].tick_params(axis='x', rotation=45)\nplt.show()","c73718b2":"#Plot the scattered MSE distributions\n\ndef squared_error(y_test, y_pred):\n    se = (y_test['Solar energy produced in PACA'] - y_pred[0])*(y_test['Solar energy produced in PACA'] - y_pred[0]) \n    return se\n\nlinear_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_test))\ntree_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_tree))\nrfr_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_rfr))\ngboost_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_gboost))\n\nfig, axes = plt.subplots(2, 2, figsize=(50,30))\nsns.set(font_scale=3)\n\naxes[0, 0].set_title('Squared error distribution for the Linear Regression')\naxes[0, 0].set_ylabel('Squared error')\naxes[0, 0].set_ylim(top=800000)\n#linear_regression_se_distribution.plot(x=y_test.index, y=0, kind='scatter', ax=axes[0,0], legend=False)\nsns.lineplot(x =X_test.index, y=25000, ax=axes[0, 0], color='red')\nsns.scatterplot(x=X_test.index, y=linear_regression_se_distribution[0], data=linear_regression_se_distribution, ax=axes[0,0])\n\naxes[0, 1].set_title('Squared error distribution for the Decision Tree Regressor')\naxes[0, 1].set_ylabel('Squared error')\n#tree_regression_se_distribution.plot(y=0, kind='hist', ax=axes[0,1], legend=False)\nsns.lineplot(x =X_test.index, y=25000, ax=axes[0, 1], color='red')\nsns.scatterplot(x=X_test.index, y=tree_regression_se_distribution[0], data=tree_regression_se_distribution, ax=axes[0,1])\n\naxes[1, 0].set_title('Squared error distribution for the Random Forest Regressor')\naxes[1, 0].set_ylabel('Squared error')\naxes[1, 0].set_ylim(top=800000)\n#rfr_regression_se_distribution.plot(y=0, kind='hist', ax=axes[1,0], legend=False)\nsns.lineplot(x =X_test.index, y=25000, ax=axes[1, 0], color='red')\nsns.scatterplot(x=X_test.index, y=rfr_regression_se_distribution[0], data=rfr_regression_se_distribution, ax=axes[1,0])\n\naxes[1, 1].set_title('Squared error distribution for the Gradient Boosted Regressor')\naxes[1, 1].set_ylabel('Squared error')\naxes[1, 1].set_ylim(top=800000)\n#gboost_regression_se_distribution.plot(y=0, kind='hist', ax=axes[1,1], legend=False)\nsns.lineplot(x =X_test.index, y=25000, ax=axes[1, 1], color='red')\nsns.scatterplot(x=X_test.index, y=gboost_regression_se_distribution[0], data=gboost_regression_se_distribution, ax=axes[1,1])\n\nplt.show()","4054b8d3":"#Plot the MSE bar distributions\n\ndef squared_error(y_test, y_pred):\n    se = (y_test['Solar energy produced in PACA'] - y_pred[0])*(y_test['Solar energy produced in PACA'] - y_pred[0]) \n    return se\n\nlinear_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_test))\ntree_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_tree))\nrfr_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_rfr))\ngboost_regression_se_distribution = pd.DataFrame(squared_error(y_test, y_hat_gboost))\n\nfig, axes = plt.subplots(2, 2, figsize=(50,30))\nsns.set(font_scale=3)\n\naxes[0, 0].set_title('Squared error distribution for the Linear Regression')\naxes[0, 0].set_ylabel('Squared error')\naxes[0, 0].set_ylim(top=2000)\naxes[0, 0].set_xlim(right=800000)\n\nlinear_regression_se_distribution.plot(y=0, kind='hist', ax=axes[0,0], legend=False)\n\naxes[0, 1].set_title('Squared error distribution for the Decision Tree Regressor')\naxes[0, 1].set_ylabel('Squared error')\naxes[0, 1].set_ylim(top=2000)\naxes[0, 1].set_xlim(right=800000)\n\ntree_regression_se_distribution.plot(y=0, kind='hist', ax=axes[0,1], legend=False)\n\naxes[1, 0].set_title('Squared error distribution for the Random Forest Regressor')\naxes[1, 0].set_ylabel('Squared error')\naxes[1, 0].set_ylim(top=2000)\naxes[1, 0].set_xlim(right=800000)\n\nrfr_regression_se_distribution.plot(y=0, kind='hist', ax=axes[1,0], legend=False)\n\naxes[1, 1].set_title('Squared error distribution for the Gradient Boosted Regressor')\naxes[1, 1].set_ylabel('Squared error')\naxes[1, 1].set_ylim(top=2000)\naxes[1, 1].set_xlim(right=800000)\n\ngboost_regression_se_distribution.plot(y=0, kind='hist', ax=axes[1,1], legend=False)\n\nplt.show()","2eb7376f":"linear_regression_se_outliers = linear_regression_se_distribution[linear_regression_se_distribution[0] > 25000]\ntree_regression_se_outliers = tree_regression_se_distribution[tree_regression_se_distribution[0] > 25000]\nrfr_regression_se_outliers =  rfr_regression_se_distribution[rfr_regression_se_distribution[0] > 25000]\ngboost_regression_se_outliers = gboost_regression_se_distribution[gboost_regression_se_distribution[0] > 25000]\n\nlinear_regression_outliers = pd.DataFrame(linear_regression_se_outliers.index.month)\ntree_regression_outliers = pd.DataFrame(tree_regression_se_outliers.index.month)\nrfr_regression_outliers = pd.DataFrame(rfr_regression_se_outliers.index.month)\ngboost_regression_outliers = pd.DataFrame(gboost_regression_se_outliers.index.month)\n\nfig, axes = plt.subplots(2, 2, figsize=(50,30))\nsns.set(font_scale=3)\n\naxes[0, 0].set_title('Squared error outliers month distribution for the Linear Regression')\naxes[0, 0].set_ylabel('Squared error')\naxes[0, 0].set_ylim(top=170)\nlinear_regression_outliers.plot(y=0, kind='hist', ax=axes[0,0], legend=False)\n\naxes[0, 1].set_title('Squared error outliers month distribution for the Decision Tree Regressor')\naxes[0, 1].set_ylabel('Squared error')\naxes[0, 1].set_ylim(top=170)\ntree_regression_outliers.plot(y=0, kind='hist', ax=axes[0,1], legend=False)\n\naxes[1, 0].set_title('Squared error outliers month distribution for the Random Forest Regressor')\naxes[1, 0].set_ylabel('Squared error')\naxes[1, 0].set_ylim(top=170)\nrfr_regression_outliers.plot(y=0, kind='hist', ax=axes[1,0], legend=False)\n\naxes[1, 1].set_title('Squared error outliers month distribution for the Gradient Boosted Regressor')\naxes[1, 1].set_ylabel('Squared error')\naxes[1, 1].set_ylim(top=170)\ngboost_regression_outliers.plot(y=0, kind='hist', ax=axes[1,1], legend=False)\n\nplt.show()","242c22e9":"# Comparison between linear models\n\nmse_linear_lasso = mean_squared_error(y_hat_test, y_hat_lasso)\nmse_linar_ridge = mean_squared_error(y_hat_test, y_hat_ridge)\nmse_linear_sgd = mean_squared_error(y_hat_test, y_hat_sgd)\n\ndf_linear_mse = pd.DataFrame()\ndf_linear_mse['MSE($\\hat y_{linear~regression}, \\hat y_{estimator})$'] = pd.Series([mse_linear_lasso, mse_linar_ridge, mse_linear_sgd])\ndf_linear_mse['Estimators'] = pd.Series(['Lasso', 'Ridge', 'SGD'])\n\nsns.set(font_scale=1)\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.set_title('MSE between the linear regression prediction and the other linear model predictions')\nsns.barplot(x='Estimators', y='MSE($\\hat y_{linear~regression}, \\hat y_{estimator})$', data = df_linear_mse, palette='GnBu', ax=ax)\nplt.show()","386a419a":"reg_linear = linear_model.LinearRegression()\nmodel_lin= make_pipeline(RobustScaler(), reg_linear)\nmodel_lin.fit(X_train, y_train)\n\nreg_lasso = linear_model.Lasso(alpha=param_max_lasso, max_iter=max_iter)\nmodel_lasso = make_pipeline(RobustScaler(), reg_lasso)\nmodel_lasso.fit(X_train, y_train)\n\nreg_ridge = linear_model.Ridge(alpha=param_max_ridge, max_iter=max_iter)\nmodel_ridge = make_pipeline(RobustScaler(), reg_ridge)\nmodel_ridge.fit(X_train, y_train)\n","c98d23f0":"model_lin['linearregression'].coef_","dfce80e8":"model_ridge['ridge'].coef_","f3360fdc":"model_lasso['lasso'].coef_","ad75a384":"folds = 10\nmax_iter = 1e6\nlw=2\nX = df_climate\ny = df_production # We choose the non normalized df\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1\/folds), random_state=4)","d34e3011":"sns.set(font_scale=2)\n\n###Plot the linear regression curve\n\nreg_linear = linear_model.LinearRegression()\nmodel = make_pipeline(RobustScaler(), reg_linear)\nmodel.fit(X_train, y_train)\ny_hat_test = pd.DataFrame(model.predict(X_test))\ny_hat_test.index = X_test.index\nlinear_regression_score = model.score(X_test, y_test)\n\nax = plt.axes()\nax.set_title('Results for the Linear Regression model with an R2 score of ' + str(round(linear_regression_score, 4)))\nax.set_ylabel('Solar energy production (MWh)')\nsns.scatterplot(x=X_test.index, y=y_hat_test[0], data=y_hat_test, label='prediction')\nsns.scatterplot(x=X_train.index, y=y_train['Solar energy produced in PACA'], data=y_train['Solar energy produced in PACA'], label='data', alpha=0.2)\nplt.show()","033d0019":"# II.3. Correlations between features and output","d780c332":"# III. Prediction\n\nWe choose to normalize our data because we must put all the features on the same scale.\n\nWe choose the robust scaler to be less sensitive to the outliers : $$X_{scaled} = \\frac{X - mediane}{IQR} $$\n\nThe score that we are going to use is the R2 score :\n\n$$ R2 = 1-\\frac{ \\sum (y_{true}-y_{pred})^2}{\\sum(y_{true}-\\overline{y_{true}})^2} $$\n\nThis R2 score measures how well our model fits in comparison with the level of variation of our data. \n\nThen, we will try both linear models and tree based methods.\n\n","f2439707":"This project aims to forecast the regional Provences-Alpes-Cotes-d'Azur solar power production using the region's climate data sampled hourly.\nThe project will be conducted in this order :\n\nI. Exploratory Data Analysis\n\n    I.1. Loading the datasets\n    I.2. Visualization of the data\n    I.3. Mathematical formulation of the problem\n    \nII. Pre-Processing\n\n    II.1. Format the data\n    II.2. Normalizing the production dataset\n    II.3. Correlations between features and output\n\nIII. Prediction\n\n    III.1. Linear models\n    III.2. Tree models \n        III.2.1. Tree Regressor\n        III.2.2. Random Forest Regressor\n        III.2.3. Analysis of the tree based methods\n        III.2.3. Gradient Boosting Regressor\n    III.3. Comparison between the models\n\nIV. Conclusion\n\n    IV.1. Necessity of the normalization\n    IV.2. Best estimator\n    ","c4ab0723":"# III.3. Comparison between the models","f353bb13":"We deduce the values of the three hyperparameters : 1e-3 for Lasso, 1e-6 for Ridge and 1e-5 for SGD Regressor. Lower values were tested and no difference were observed in the scores (but a slower execution was noticed).","9c77a833":"We clearly notice that all the linear models perform the same as linear regression. \nEnsemble methods clearly work better and Gradient Boosting Regressor is the best.","60801fe2":"# I.2. Visualization of the data\n\n","af544806":"As expected for an ensemble of trees, the estimator does not to send any value in ]0, -$\\infty$ [.\n\nThis is the best estimator that we have got so far so let's analyse the purity of the leaves and the depth of the estimators.","9dc6fd94":"# III.2.2. Random Forest Regressor","5fd55fe7":"# III.1.2. Lasso Regression, Ridge Regression and SGD Regressor","8c1d9fde":"The Linear Regression performs poorly : it predicts negative solar production and does not fit well the non linearities of the data curve. \n\nIt gives a good base score for the other models.","f6f9deab":"# II. **Pre-Processing**\n\n# II.1. Format the data\n\nThe climate dataframe imposes the index because it is 20k shorter than the production dataset. \n\nThe first cell below analyses each DataFrame and makes sure it does not contain missing values or an abnormal number of zeroes. It also underlines that the dataframes are in a different format, that needs to be unified in order to apply models.\n\n\nThe second one converts the datasets to the same index.","8b97b0b6":"The mean depth is of 31.85. We choose to not interpret this number, but we can remark that it is 3 times more important than for a single tree.","7be4429a":"We notice that the upper zonal and meridional winds are strongly correlated to the zonal and meridional wind, we choose to drop the upper winds.\n\nFurthermore, correlations are almost identical between the normalization, thus we can suppose that the normalization is correctly keeping the statistical informations of the dataset. This will be verified in the final section.","2225e71a":"\nThe data is split between two datasets :\n\n1. The climate data, which contains : \n\n    - a map of France showing the regions - mask_datagouv_french_regions_merra2_Nx_France.nc\n    \n    - the height data - merra2_area_selection_output_height_500_merra2_2010-2019.nc\n    \n    - the surface density - merra2_area_selection_output_surface_density_merra2_2010-2019.nc\n    \n    - the surface downward radiation - merra2_area_selection_output_surface_downward_radiation_merra2_2010-2019.nc\n    \n    - the surface humidity - merra2_area_selection_output_surface_specific_humidity_merra2_2010-2019.nc\n    \n    - the meridional wind - merra2_area_selection_output_meridional_wind_merra2_2010-2019.nc\n    \n    - the upper meridional wind - merra2_area_selection_output_upper_meridional_wind_merra2_2010-2019.nc\n    \n    - the zonal wind - merra2_area_selection_output_zonal_wind_merra2_2010-2019.nc\n    \n    - the upper zonal wind - merra2_area_selection_output_upper_zonal_wind_merra2_2010-2019.nc\n    \n2. The energy production data which contains the energy production of all France's region for every month, every 15 minutes that we are going to merge into one dataset with the solar energy production for each hour\n","a6c82d7c":"Those three models do not perform better than the Linear Regression, so we choose not to dig further into linear models.","b56117e0":"Finally, we choose to try the Gradient Boosting Regressor. The hyperparameters of the models were chosen using GridSearchCV but it took around an hour to compile so we took it out of this notebook.\n\nWe do not pretend that we are able to explain why Gradient Boosting Regressor performs better. Yet, it may suggests that the non linearities in the physical problem are such that only very complex non linear model will be able to sharply forecast the solar energy production.","3d953b38":"First, let's plot the validation curves and then use them to tune those estimator's hyperparameters.","b2bc00bb":"Furthermore, to underline how close the linear models are to the linear regression, let's compute the MSE between the predictions.","b01fcbca":"# IV. Conclusion\n\n# IV.1. The necessity of normalization\n\nIn order to prove why normalization was useful, we are going to show that the estimators would not have performed well without it. We will illustrate it with the linear regression.","eaa0cb28":"# III.2. Ensemble methods\n\n# III.2.1. Tree Regressor","0781316c":"# III.1. Linear methods\n\n# III.1.1. Linear Regression\n","b42b8d7d":"# II.2. Normalizing the production dataset\n\nWe want to normalize our production dataset because the energy production has evolved over the years.\n\nIndeed, it would be abnormal for our model to learn that the same climate conditions would not correspond at all to the same production in any region, we do not want to predict that the capacity of production will increase in the future.\n\nWe choose a linear normalization for each year to keep the ratio of the distance between two points of the production data set for each year. \n\nThe formula applied is : \n$$ I = \\{2014,2015,2016,2017,2018\\}$$\n$$ \\forall year \\in I, \\forall X_{year}, X_{normalized} = \\frac{X_{max 2019}}{X_{max year}}X_{year} $$","d0f2a462":"Even if the score is only 0.03 lower than for the normalized production dataset, there clearly is a bias here. Indeed, the maximal production that our estimator can predict is around 800 MWh, which is around the mean of the maximum production for the different years. This clearly shows that the estimator fails to learn the tendency for production data to get bigger. Thus, normalization was indeed important.","1b45d5a1":"Here, we notice an expected results : if the depth is too high, the tree clearly overfits. The score is close to the linear regression so we choose to visualize the prediction and to pursue tree methods. The max depth seems to be 11.3. The other features were tried before with the GridSearchCV function.","fc1e4586":"# I.2. Mathematical formulation of the problem\n\nWe choose to model this physical situation as a **supervised learning problem**. Indeed, we will forecast the solar energy production from a **regression estimator** trained with the climate data. \nThen, it is a regression problem from $\\mathbb{R}^9$ to $\\mathbb{R}$.\n","38c13f61":"These two first cells load the two datasets into one dictionnary of pd.DataFrame with as keys the climate variables' name and one pd.DataFrame with the production data.","fab69bf7":"# **I. Exploratory Data Analysis**\n \n# I.1. Loading the datasets ","63418364":"Except the noticable difference of the tree, maximizing the R2 score had the same effect than minimizing the MSE. Indeed, the Gradient boosted regressor performs significatively better than the other models.\n\nOnce more, linear models perform the same as the linear regression.\n\nLet's analyze the distribution of the squared error for the linear regression, the decision tree regressor, the random forest regressor and the gradient boosting regressor.","7a8444ce":"# IV.2. Best estimator\n\n\nFinally, to conclude this notebook, the best estimator that we found was the Gradient Boosting Regressor with a R2 score of 0.753. \nIt could further be improved by replacing negative values with zeros, with more computationnal power and by more finely tuning its hyperparameters.\n\nTo improve our analysis, we could have evaluated our model with other metrics, like the mean squared error to also compare how close to the data our predictions are. Indeed, our analysis focuses more on how close to the variations of the data our models are. \n","ac20cec3":"The tree performs quite good considering it is alone, and we can see that it chooses to not to send any value in $]-\\infty, 0 [$ as expected.\n\nBecause of this, we choose to try the Random Forest Regressor.","7efb0065":"This decrease in impurity shows that the surface incoming shortwave flux is the climate data that the estimators choose to be the more important, which is logical. Then comes the zonal wind, which is also expected because solar pannel production decreases with the wind. Then, as expected, temperature and humidity are important. The model must realize a tradeoff between the others.","e76e61a0":"# III.2.3. Gradient Boosting Regressor"}}