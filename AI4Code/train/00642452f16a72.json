{"cell_type":{"bc5b8ac2":"code","c64e9698":"code","5a921862":"code","c2fd0c12":"code","09332f9f":"code","6dcf0758":"code","a89ab549":"code","d81039fe":"code","7d8fd7c7":"code","5a408f70":"code","1ac7b892":"code","9a3559eb":"code","1dd9ae48":"code","731c7bff":"code","7630b251":"code","b3133f0e":"code","fbf8be7b":"code","fc198306":"code","90fe16fa":"code","7d7fd12a":"code","91a2c605":"code","40df82ef":"code","b00f82bd":"code","04934fd1":"code","daba9769":"code","012a7e7c":"code","97f62a2f":"code","923bd081":"code","08af55b2":"code","1c8d3b9c":"code","efee484e":"markdown","fa534cc7":"markdown","6b399549":"markdown","001ba42e":"markdown","16120e92":"markdown","ec26b520":"markdown","224a3c1e":"markdown","e35fe51b":"markdown","e0803970":"markdown","883fab9a":"markdown","43eb8565":"markdown","29ad6886":"markdown","fc2b36e9":"markdown","1dd71225":"markdown","c8016293":"markdown"},"source":{"bc5b8ac2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nRANDOM_SEED = 2021\nnp.random.seed(RANDOM_SEED)","c64e9698":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(data.shape)\ndata.head(5)","5a921862":"from matplotlib import pyplot as plt\nimport seaborn as sns","c2fd0c12":"# Set the figure size\nplt.figure(figsize=(15,5))\n\n# Using seaborn to plot the target\nax = sns.countplot(data=data, x=\"Survived\")\n\n# Print the counted value for each label\nfor label, value in zip(data[\"Survived\"].unique(), data[\"Survived\"].value_counts()):\n    ax.text(label, value-50, value, fontsize=22)\n\n# Set plot's title & labels\nax.set_title(\"Titanic survival\", fontsize=28)\nax.tick_params(labelsize=18)\nax.set_xlabel(\"Survived\", fontsize=22)\nax.set_ylabel(\"Count\", fontsize=22)","09332f9f":"pd.DataFrame({\n    \"dtype\" : data.dtypes,\n    \"Unique values\": data.nunique(),\n    \"Null count\": data.isna().sum()\n})","6dcf0758":"plt.figure(figsize=(8, 8))\nax = sns.heatmap(data.drop(columns=[\"PassengerId\"]).corr(), annot=True, cmap='GnBu')\nax.tick_params(labelsize=12)","a89ab549":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler","d81039fe":"# PassengerID & Name are unique values for each passenger\n# therefore, we will not use these values to predict.\n# Cabin column has a lot of Null value, it seems to be a meaningless feature.\ncolumns_todrop = [\"PassengerId\", \"Name\", \"Cabin\"]","7d8fd7c7":"numeric_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nnumeric_transformer = Pipeline(steps=[\n    ('fillna', SimpleImputer(missing_values=np.nan, strategy='mean')),\n    ('scaler', StandardScaler())\n])","5a408f70":"categorical_features = [\"Sex\", \"Ticket\", \"Embarked\"]\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n    ('scaler', StandardScaler())\n])","1ac7b892":"ordinal_features = [\"Pclass\"]\nordinal_transformer = Pipeline(steps=[\n    (\"pclass_enc\", OrdinalEncoder(categories=[[1,2,3]])),\n    (\"scaler\", StandardScaler())\n])","9a3559eb":"preprocessor = ColumnTransformer(n_jobs=-1, transformers=[\n    (\"drop\", \"drop\", columns_todrop),\n    (\"numeric_process\", numeric_transformer, numeric_features),\n    (\"nominal_process\", categorical_transformer, categorical_features),\n    (\"ordinal_process\", ordinal_transformer, ordinal_features),\n])","1dd9ae48":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.base import BaseEstimator, TransformerMixin","731c7bff":"class ColumnExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X_cols = X[self.columns]\n        return X_cols","7630b251":"polynomial_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nfeature_engineer = Pipeline(steps=[\n    (\"extract_features\", ColumnExtractor(polynomial_features)),\n    (\"fillna\", numeric_transformer),\n    (\"polynomial_features\", PolynomialFeatures(degree=2, interaction_only=True))\n])","b3133f0e":"from sklearn.pipeline import FeatureUnion","fbf8be7b":"features = FeatureUnion([\n    (\"preprocessing\", preprocessor),\n    (\"feature_engineering\", feature_engineer)\n])","fc198306":"from sklearn.model_selection import train_test_split","90fe16fa":"x_train, x_valid, y_train, y_valid = train_test_split(\n    data.drop(columns=[\"Survived\"]), data[\"Survived\"], \n    test_size=0.2, stratify=data[\"Survived\"],\n    random_state=RANDOM_SEED\n)\nprint(f\"Train set's size: {x_train.shape} - {y_train.shape}\")\nprint(f\"Valid set's size: {x_valid.shape} - {y_valid.shape}\")","7d7fd12a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","91a2c605":"classifier_pipeline = Pipeline(steps=[\n    (\"features\", features),\n    (\"lr\", LogisticRegression(\n        max_iter=10000,\n        n_jobs=-1, verbose=False, \n        random_state=RANDOM_SEED\n    )) # Fixed parameters go here\n])\n\n# Declare dynamic parameters here\npipeline_params = {\n    \"lr__C\": [1000, 100, 10, 1, 0.1]\n}","40df82ef":"search = GridSearchCV(\n    classifier_pipeline, pipeline_params, \n    scoring=\"accuracy\", cv=5, verbose=False\n)\nsearch.fit(x_train, y_train)","b00f82bd":"print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","04934fd1":"from sklearn.metrics import accuracy_score","daba9769":"search.predict_proba(x_valid)\ntrain_score = accuracy_score(y_train, search.predict(x_train))\nvalid_score = accuracy_score(y_valid, search.predict(x_valid))\nprint(f\"Train's accuracy: {train_score}\")\nprint(f\"Valid's accuracy: {valid_score}\")","012a7e7c":"search = GridSearchCV(\n    classifier_pipeline, pipeline_params, \n    scoring=\"accuracy\", cv=5, verbose=0\n)\nsearch.fit(data.drop(columns=[\"Survived\"]), data[\"Survived\"])","97f62a2f":"print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","923bd081":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(test.shape)\ntest.head()","08af55b2":"submission = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": search.predict(test)\n})\nprint(submission.shape)\nsubmission.head()","1c8d3b9c":"submission.to_csv(\"submission.csv\", index=False)","efee484e":"# Feature Engineering\n\n> Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\n\n**Read more:** [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/)","fa534cc7":"# Import dataset","6b399549":"## Numerical Feature","001ba42e":"# Exploratory Data Analysis (EDA)\n\n> Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\n**Read more:** [Exploratory Data Analysis](https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15)\n\nThis is just a short and simple EDA. For a full & detailed EDA, please refer to this [Notebook](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)","16120e92":"## Train Validate Split","ec26b520":"## Combine preprocessing pipeline","224a3c1e":"# Evaluate","e35fe51b":"## Categorical Features - Nominal","e0803970":"# Submission","883fab9a":"# Modeling","43eb8565":"# Building a basic Machine Learning pipeline","29ad6886":"## Logistic Regression model","fc2b36e9":"# Data Preprocessing\n\n> In any Machine Learning process, Data Preprocessing is that step in which the data gets transformed, or Encoded, to bring it to such a state that now the machine can easily parse it. In other words, the features of the data can now be easily interpreted by the algorithm.\n\n**What is a Feature?**\n\n> A feature is an individual measurable property or characteristic of a phenomenon being observed. <br>\n> There are 2 major types of feature: Categorical and Numerical.\n\n![](https:\/\/miro.medium.com\/max\/2400\/1*a9VAOU5R83M4KOOOE-SZiw.jpeg)\n\n**Read more:** [Data Preprocessing : Concepts](https:\/\/towardsdatascience.com\/data-preprocessing-concepts-fa946d11c825)\n\n**Data Preprocessing Pipeline**\nTODO: Visualize pipeline","1dd71225":"## Categorical Features - Ordinal","c8016293":"# Combine features pipeline"}}