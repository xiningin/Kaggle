{"cell_type":{"77bd4b44":"code","89b70896":"code","affb1ecc":"code","72cadfec":"code","58d4c362":"markdown","1a6b7947":"markdown","d50e4051":"markdown"},"source":{"77bd4b44":"import numpy as np, pandas as pd, os\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nfrom sklearn.covariance import EmpiricalCovariance\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport sympy \ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","89b70896":"from sklearn.covariance import GraphicalLasso\n\ndef get_mean_cov(x,y):\n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps\n","affb1ecc":"from sklearn.mixture import GaussianMixture\n\n# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in tqdm(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    \n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL AND PREDICT WITH QDA\n        ms, ps = get_mean_cov(train3[train_index,:],train2.loc[train_index]['target'].values)\n        \n        gm = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001,reg_covar=0.001, max_iter=100, n_init=1,means_init=ms, precisions_init=ps)\n        gm.fit(np.concatenate([train3[train_index,:],test3],axis = 0))\n        oof[idx1[test_index]] = gm.predict_proba(train3[test_index,:])[:,0]\n        preds[idx2] += gm.predict_proba(test3)[:,0] \/ skf.n_splits\n\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('QDA scores CV =',round(auc,5))","72cadfec":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","58d4c362":"# Lasso + Gaussian Mixture Models\nWith this kernel I want to demonstrate how to use Gaussian mixture Models (GMM) which have the nice property to train unsupervised, so you can also use the test set. I use Graphical Lasso as an estimator for the initial value of precision matrix (= inverse Covariance) and mean","1a6b7947":"# Estimate cov and mean from Lasso and predict with GMM","d50e4051":"# Submit Predictions"}}