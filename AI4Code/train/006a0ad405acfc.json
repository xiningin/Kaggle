{"cell_type":{"d4d1f7bf":"code","77510557":"code","b9055042":"code","e4fd032f":"code","2a455f6a":"code","bee1742d":"code","32809e17":"code","4239993d":"code","7cd7472d":"code","5d45f9e7":"code","58550953":"code","caf34241":"code","fcb49e27":"code","f5f0fa1c":"code","14390c2f":"code","99953d4c":"code","3595bfd7":"code","dd27e33d":"code","1546c65e":"markdown","62264851":"markdown","58be4690":"markdown","277f3615":"markdown","bec68f66":"markdown","4741c935":"markdown","b2443504":"markdown","357870e0":"markdown"},"source":{"d4d1f7bf":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\nimport seaborn as sns\nsns.set(palette='viridis_r',context='notebook',\n        font='ubuntu', style='white')\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","77510557":"train = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv').\\\ndrop(['customerID'], axis=1)\ntrain.head()","b9055042":"train.info()","e4fd032f":"for column in train.drop(['tenure','MonthlyCharges','TotalCharges'], axis=1).columns:\n    print(column,'-',train[column].unique())","2a455f6a":"train.describe(include='object').T","bee1742d":"train.TotalCharges = train.TotalCharges.apply(pd.to_numeric, errors='coerce')\ntrain.TotalCharges = train.TotalCharges.fillna(train.TotalCharges.median())","32809e17":"plt.figure(figsize=(2,6))\nsns.countplot(x=train.Churn, edgecolor='darkgray', \n              alpha=.95)\nsns.despine()","4239993d":"train.TotalCharges = train.TotalCharges.apply(pd.to_numeric, errors='coerce')\ntrain.TotalCharges = train.TotalCharges.fillna(train.TotalCharges.median())","7cd7472d":"sns.pairplot(train, hue='Churn', markers='x')\nplt.show()","5d45f9e7":"fig, axes = plt.subplots(ncols=3, figsize=(8,3))\n\nsample = train[['tenure','MonthlyCharges','TotalCharges']]\n\nfor ax, column in zip(axes.ravel(),sample):\n    sns.boxplot(x=train.Churn,\n          y=sample[column], ax=ax)\nsns.despine()\nplt.tight_layout()","58550953":"import warnings\nwarnings.filterwarnings('ignore')\n\nmelted = pd.melt(train, id_vars=['Churn'], value_vars = ['gender', 'SeniorCitizen',\n        'Contract','PhoneService','MultipleLines','TechSupport'])\nmelted = melted.sort_values(['value','variable']).rename(\n                            columns={'variable':'var.'})\n\ng = sns.FacetGrid(melted, col='Churn', row='var.', aspect=1.15,\n                  hue = 'Churn',sharex=False)\ng.map(sns.countplot, 'value')\n\ng.set_xticklabels(rotation=25)\nplt.tight_layout()","caf34241":"train2 = train.copy()\n\nlec = LabelEncoder()\n\ntrain2.loc[:,'gender':'Dependents']=train2.loc[:,'gender':'Dependents'].transform(lec.fit_transform)\ntrain2.loc[:,'PhoneService':'PaymentMethod']=train2.loc[:,'PhoneService':'PaymentMethod'].\\\ntransform(lec.fit_transform)\ntrain2['Churn'] = lec.fit_transform(train2['Churn'])\n\nmms = MinMaxScaler()\ntrain2[['tenure','MonthlyCharges','TotalCharges']] =\\\nmms.fit_transform(train2[['tenure','MonthlyCharges','TotalCharges']])","fcb49e27":"features = train2.loc[:,'gender':'TotalCharges']\ntarget = train2['Churn']\n\nfig = plt.figure(figsize=(24,12))\nax = sns.heatmap(train2.corr(), cmap='viridis_r',\n      linecolor='black', lw=.65,annot=True, alpha=.95)\nax.set_xticklabels([x[:7] for x in train2.columns])\nax.set_yticklabels([y[:7] for y in train2.columns])\n\nplt.show()","f5f0fa1c":"X_train, X_valid, y_train, y_valid = train_test_split(\n    features, target,test_size=.2,random_state=42)\n\ngbc = GradientBoostingClassifier(random_state=42)\nrfc = RandomForestClassifier(random_state=42,min_samples_leaf=30)\nsvc = SVC(random_state=42,degree=3)\nlgc = LogisticRegression(random_state=42)\nknn = KNeighborsClassifier(n_jobs=5)\n\nestimators = [('Random Forest Classifier',rfc),\n              ('Support Vector Machines',svc),\n              ('Logistic Regression',lgc), \n              ('Gradient Boosting Classifier',gbc),\n              ('KNN Classifier',knn)]","14390c2f":"def confusion_plot(label, y_valid, y_pred, ax=None):\n    \n    co_ma = confusion_matrix(y_valid, y_pred)\n    groups = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [int(value) for value in co_ma.flatten()]\n    shares = ['{0:.2%}'.format(value) for value in\n             co_ma.flatten()\/np.sum(co_ma)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n              zip(groups,counts,shares)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(co_ma,annot=labels,cmap='binary', alpha=.55, ax=ax,\n             cbar=True, fmt='', linewidth=1,linecolor='black')\n    plt.axis('off')\n    plt.title(f'Confusion Matrix for {label}')\n\n                                            \ndef show_metrics(metrics):\n    try:return pd.DataFrame(metrics)\n    except:return pd.DataFrame([metrics])","99953d4c":"metrics = []\n\nfor est in estimators:\n    \n    fig, axes = plt.subplots(ncols=3, figsize=(15,4))\n   \n    mod = est[1].fit(X_train, y_train)\n    y_pred = mod.predict(X_valid)\n    plot_precision_recall_curve(mod, X_valid, y_valid, \n                    y_pred, ax=axes[0], color='black')\n    plot_roc_curve(mod, X_valid, y_valid,ax = axes[1], color='black')\n\n    axes[0].set_title(f'Precision-Recall Curve for {est[0]}')\n    axes[1].set_title(f'ROC Curve for {est[0]}')\n    axes[1].plot([1,0],[1,0], c='green',ls='--')\n    confusion_plot(est[0],y_valid, y_pred, axes[2])\n    for ax in axes.ravel():\n        ax.legend(frameon=False)\n        \n    scores = {}\n    scores['classifier'] = est[0]\n    scores['accuracy_score'] = accuracy_score(y_valid, y_pred)\n    scores['roc_auc_score']=roc_auc_score(y_valid, y_pred)\n    scores['f1_score'] = f1_score(y_valid,y_pred)\n\n    plt.tight_layout()\n    metrics.append(scores)\n\nshow_metrics(metrics)","3595bfd7":"params={'colsample_bytree': 0.6, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5.0, 'subsample': 1.0}\n\n\nxgb = XGBClassifier(random_state=42,\n                    **params,cv=5, verbosity=0)\n\nstacked_metrics={}\n\nreg = StackingClassifier(estimators=estimators,\n    final_estimator=xgb)\ncls_name = 'Stacking Classifier'\n\nregmodel = reg.fit(X_train, y_train)\ny_pred = reg.predict(X_valid)\n\nfig, axes = plt.subplots(ncols=3, figsize=(14,4))\n\nplot_precision_recall_curve(regmodel,X_valid,y_valid, \n            y_pred, color='black', ax=axes[0])\nplot_roc_curve(regmodel,X_valid,y_valid, \n               color='black', ax=axes[1])\n\naxes[0].set_title(f'Precision-Recall Curve for {cls_name}')\naxes[1].plot([1,0],[1,0], c='green',ls='--')\naxes[1].set_title(f'ROC Curve for {cls_name}')\nconfusion_plot(cls_name,y_valid, y_pred, axes[2])\nfor ax in axes:\n    ax.legend(frameon=False)\n\nstacked_metrics['classifier'] = cls_name\nstacked_metrics['accuracy_score'] = accuracy_score(y_valid, y_pred)\nstacked_metrics['ROC AUC score'] = roc_auc_score(y_valid, y_pred)\nstacked_metrics['f1_score'] = f1_score(y_valid, y_pred)\n    \nplt.tight_layout()\n\npd.DataFrame([stacked_metrics])","dd27e33d":"perm = PermutationImportance(regmodel,random_state=17).fit(X_valid,y_valid)\neli5.show_weights(perm, feature_names=X_valid.columns.values)","1546c65e":" <sub>1: these parameters for XGBoost Classifier were got after a GridSearch<\/sub>","62264851":"## Preprocessing the Data\n---\n\nI would take a transition from categorical values to scaled features by using some steps to further prediction\n\n1. Scaling of non-bool values like `tenure`,`TotalCharges`,`Monthly Charges` with a help of `MinMaxScaler`\n2. Encode categorical values with $>2$ choices and redistribute them as a new binary feature (with `OneHotEncoder`).\n3. Transform categorical values with binary choice to `[0,1]` view","58be4690":"Non-surprisingly, **churned have a lower median tenure than a non-churned**. But they are much higher in terms of MontlyCharges and spend lesser money in total.\n\nSo, there could be some insights from tenure & monthly charges.","277f3615":"## Model Selection & Prediction\n---------","bec68f66":"The dataset is imbalanced. That is why I wouldn't use ROC AUC score as a primary metric (but for some reasons, I will caluclate it as an additional one).\n\nFor this task of churn classification **accuracy score** is used.","4741c935":"## A Brief Exploratory Analysis\n-----------","b2443504":"$\\implies$ A stacked model has accuracy score of $0.811923$... after a grid search and can be used for predictions.","357870e0":"It also looks like that those who have no internet service on Tech Support are at the group of risk.\n\nType of contact could also have a significant meaning to Churn. It is interesting to look at Month-to-Month Contracts closely."}}