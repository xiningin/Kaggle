{"cell_type":{"6197c360":"code","5be05512":"code","f9bce1cf":"code","f0cd6a2d":"code","1cb9c178":"code","43afa437":"code","fb061fb2":"code","1e64549d":"code","8d0c39d2":"code","12b0d3d2":"code","98c955dd":"code","150fa42e":"code","44dcd771":"code","4e4b207d":"code","8199b5b2":"code","382f1c3c":"code","9ab1bd17":"code","1f66d0f5":"code","0a822846":"code","889e2d5d":"code","6f2784b3":"code","37ccc575":"code","17899691":"code","57724371":"code","25229984":"code","9b9af5a4":"code","0ad542e8":"code","f1ed8cd2":"code","7690646c":"markdown","44bd321d":"markdown","a936d247":"markdown","b1be1758":"markdown","8b76b5f5":"markdown","a80cd3c2":"markdown","c6d96e3a":"markdown","861052e4":"markdown","8bc6d8bb":"markdown","7c8fb61f":"markdown","3bb82f81":"markdown","3648bd6b":"markdown","227574a0":"markdown","4a90379f":"markdown","7334bbdc":"markdown","65dfa9c7":"markdown","7f20d145":"markdown","2c500c7a":"markdown","542949fb":"markdown","c7aaf348":"markdown","671e7f7f":"markdown","eb9d1e86":"markdown","840b9c14":"markdown","ec3c198c":"markdown"},"source":{"6197c360":"# with typing installed, the pip install of hdbscan was breaking\n!pip uninstall -y typing\n!pip install hdbscan","5be05512":"from sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import HuberRegressor\nfrom random import sample\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\nfrom sklearn import svm\nimport random\nfrom sklearn.cluster import KMeans\nimport hdbscan\nfrom tqdm.auto import tqdm\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib.patches import Ellipse\nfrom sklearn.ensemble import IsolationForest\nimport os\nimport torch as T\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.normal import Normal\nimport gym\nfrom gym import spaces\nfrom sklearn.neighbors import NearestNeighbors\n\nrandom.seed(10)\n\n%matplotlib inline","f9bce1cf":"iris = load_iris()\nX = iris.data\ny_true = iris.target\niris.feature_names","f0cd6a2d":"plt.figure(figsize=(10, 7))\nplt.scatter(X[:, 0], X[:, 3],\n            marker='.', s=120, label='Iris Flowers')\nplt.scatter(8.1, 2.95, s=350, color='red', marker=r\"$?$\", label='Unknown petal width')\nplt.scatter(8.1, 3.2, s=350, color='red', marker=r\"$?$\")\nplt.scatter(8.1, 2.4, s=350, color='red', marker=r\"$?$\")\nplt.scatter(8.1, 2.7, s=350, color='red', marker=r\"$?$\")\nplt.vlines(8.1, -0.3, 2.2, linestyles='dashed')\nplt.legend(loc='upper left', fontsize=18)\nplt.title(\"Regression: predicting continuous variable\", fontsize=27)\nx_lims = (3.8, 8.3)\ny_lims = (-0.3, 3.3)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)","1cb9c178":"# robust regression, thx to huber loss\nmodel = HuberRegressor()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X[:, 0].reshape(-1, 1))\nmodel.fit(X_scaled, X[:, 3])\n\nx_model = np.linspace(*x_lims, 400)\nx_model_scaled = scaler.transform(x_model.reshape(-1, 1))\ny_pred = model.predict(x_model_scaled)\n\npoint_pred = model.predict(scaler.transform([[8.1]]).reshape(-1, 1))[0]","43afa437":"plt.figure(figsize=(10, 7))\nplt.scatter(X[:, 0], X[:, 3],\n            marker='.', s=120)\nplt.plot(x_model, y_pred, color='darkblue', label='Model\\'s output', linewidth=4)\nplt.scatter(8.1, point_pred, s=150, color='red', label='Predicted petal width')\nplt.vlines(8.1, -0.3, point_pred - 0.05, linestyles='dashed')\nplt.legend(loc='upper left', fontsize=18)\nplt.title(\"Regression: predicting a continuous variable\", fontsize=27)\nplt.xlim(x_lims)\nplt.ylim((-0.3, 3.2))\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)","fb061fb2":"chosen_indices = sample(list(np.where(X[:, 0] < 6)[0]), 15)","1e64549d":"plt.figure(figsize=(10, 7))\nplt.scatter(X[chosen_indices, 0], X[chosen_indices, 3],\n            marker='.', s=120, label='Iris Flowers Measures')\naxis_level_y = -0.16\nfirst_question = True\nfor x in np.arange(4, 9, 0.5):\n    plt.scatter(x, axis_level_y, s=350, color='red', marker=r\"$?$\", \n                label='What flowers to measure next?' if first_question else None)\n    first_question = False\nplt.legend(loc='upper left', fontsize=18)\nplt.title(\"Expensive labeling\", fontsize=27)\nplt.xlim(x_lims)\nplt.ylim((-0.3, 3.3))\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)","8d0c39d2":"max_length = 3\nmax_noise = 1e3\nkernel = C(2, constant_value_bounds=(1.5, 100000.0)) * RBF(length_scale_bounds=(1e-20, max_length)) + WhiteKernel(noise_level_bounds=(2e-3, max_noise))\n\ngp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=40, normalize_y=False)\ngp.fit(X_scaled[chosen_indices],  X[chosen_indices, 3])\n\nx = scaler.transform(np.linspace(*x_lims, 400).reshape(-1, 1))\ny_pred, sigma = gp.predict(x, return_std=True)\n\n\nplt.figure(figsize=(10, 7))\nplt.scatter(scaler.inverse_transform(X_scaled[chosen_indices]), X[chosen_indices, 3],\n            marker='.', s=120)\nx_denorm = scaler.inverse_transform(x)\nplt.plot(x_denorm, y_pred, 'g-', linewidth=3, label='Mean of predictive distribution')\nplt.fill(np.concatenate([x_denorm, x_denorm[::-1]]),\n         np.concatenate([y_pred - 1.96 * sigma,\n                        (y_pred + 1.96 * sigma)[::-1]]),\n         alpha=.3, fc='green', ec='None', label='95% confidence interval')\nplt.scatter(7.38, axis_level_y, s=5e4, color='red', marker=r\"$The\\ highest\\ model\\ uncertainty$\")\n\n\nplt.title(\"Regression, Active Learning\", fontsize=27)\nplt.legend(loc='upper left', fontsize=18)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)","12b0d3d2":"additional_chosen_indices = sample(list(np.where(X[:, 0] > 6.5)[0]), 5)\nkernel = C(2, constant_value_bounds=(1.5, 100000.0)) * RBF(length_scale_bounds=(1e-20, max_length)) + WhiteKernel(noise_level_bounds=(2e-3, max_noise))\n\ngp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=40, normalize_y=False)\ngp.fit(X_scaled[chosen_indices + additional_chosen_indices],  X[chosen_indices + additional_chosen_indices, 3])\n\ny_pred, sigma = gp.predict(x, return_std=True)\n\n\nplt.figure(figsize=(10, 7))\nplt.scatter(scaler.inverse_transform(X_scaled[chosen_indices]), X[chosen_indices, 3],\n            marker='.', s=120)\nx_denorm = scaler.inverse_transform(x)\nplt.plot(x_denorm, y_pred, 'g-', linewidth=3, label='Mean of predictive distribution')\nplt.fill(np.concatenate([x_denorm, x_denorm[::-1]]),\n         np.concatenate([y_pred - 1.96 * sigma,\n                        (y_pred + 1.96 * sigma)[::-1]]),\n         alpha=.3, fc='green', ec='None', label='95% confidence interval')\nplt.scatter(scaler.inverse_transform(X_scaled[additional_chosen_indices]), X[additional_chosen_indices, 3],\n            marker='.', s=120, color='red', label='Additionally labeled data')\n\n\nplt.title(\"Regression, Active Learning\", fontsize=27)\nplt.legend(loc='upper left', fontsize=18)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)","98c955dd":"plt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06']\nmarkers = ['o', 'p', 'v']\n\nfor k, col in enumerate(colors):\n    cluster_data = y_true == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 3],\n                c=col, marker=markers[k], s=120, label=iris.target_names[k])\n    \nplt.scatter(6.8, 0.91, s=350, color='red', marker=r\"$?$\", label='Unknown class')\n\nplt.title(\"Classification: predicting a categorical variable\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.legend(loc='upper left', fontsize=18)\nplt.show()","150fa42e":"# helping routines taken from https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_iris_svc.html\n\ndef make_meshgrid(x_lims, y_lims, h=.01):\n    x_min, x_max = x_lims\n    y_min, y_max = y_lims\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\nplt.figure(figsize=(10, 7))\nmodel =  svm.SVC(kernel='rbf', gamma=0.2, C=1)\nmodel.fit(X[:, [0, 3]], y_true)\n\nxx, yy = make_meshgrid(x_lims, y_lims)\n\nplot_contours(plt, model, xx, yy, colors=[colors[i] for i in [0, 2, 2, 1, 1]], alpha=0.45)\n\n\nfor k, col in enumerate(colors):\n    cluster_data = y_true == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 3],\n                c=col, marker=markers[k], s=120, label=iris.target_names[k])\n\nplt.scatter(6.8, 0.91, s=350, color='red', marker=r\"$?$\", label='Predicted class: versicolor')\nplt.title(\"Classifier: decision regions\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.legend(loc='lower right', fontsize=18)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.show()","44dcd771":"plt.figure(figsize=(10, 7))\n\nplt.scatter(X[:, 0], X[:, 3],\n            s=120)\n\nplt.title(\"No Known Categories\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.annotate('Any patterns?', xy=(300, 30), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))\n\nplt.xlim(x_lims)\nplt.ylim(y_lims)","4e4b207d":"inertia_values = []\nfor num_clusters_this in range(1,15):\n    model = KMeans(n_clusters=num_clusters_this)\n    model = model.fit(X[:, [0, 3]])\n    inertia_values.append(model.inertia_)\n\nplt.figure(figsize=(10, 7))\nplt.plot(range(1,15), inertia_values, 'bo-')\nplt.scatter(3, inertia_values[2], color='red', s=100, label='Elbow')\nplt.xlabel('Number of Clusters', fontsize=20)\nplt.xticks(np.arange(1, 15))\nplt.ylabel('Intertia', fontsize=20)\nplt.legend(fontsize=18)\nplt.title('Selecting number of clusters for K-Means', fontsize=27)","8199b5b2":"plt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red']\n\nmodel =  KMeans(n_clusters=3)\nmodel.fit(X[:, [0, 3]])\n\n\nfor k, col in enumerate(colors):\n    cluster_data = model.labels_ == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 3],\n                c=col, s=120)\n\nplt.title(\"Clustering, Hard\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.annotate('Algorithm 1', xy=(400, 30), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))\nplt.xlim(x_lims)\nplt.ylim(y_lims)","382f1c3c":"plt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red', 'black']\n\nmodel = hdbscan.HDBSCAN(min_cluster_size=7, min_samples=2)\nmodel.fit(X[:, [0, 3]])\n\n\nfor k, col in enumerate(colors):\n    cluster_data = model.labels_ == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 3],\n                c=col, s=120)\n    \ncluster_data = model.labels_ == -1\nplt.scatter(X[cluster_data, 0], X[cluster_data, 3],\n            c='grey', s=80)\n\nplt.title(\"Clustering, Hard\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.annotate('Algorithm 2', xy=(400, 30), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))\nplt.xlim(x_lims)\nplt.ylim(y_lims)","9ab1bd17":"plt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red', 'black']\n\nmodel = hdbscan.HDBSCAN(min_cluster_size=7, min_samples=2)\nmodel.fit(X[:, [0, 3]])\n\nlabels_covered = set()\nfor k, col in enumerate(colors):\n    for true_label in range(3):\n        bool_idx = (y_true == true_label) & (model.labels_ == k)\n        plt.scatter(X[bool_idx, 0], X[bool_idx, 3],\n                    c=col, s=120, marker=markers[true_label],\n                   label=iris.target_names[true_label] if iris.target_names[true_label] not in labels_covered else None)\n        labels_covered.add(iris.target_names[true_label])\n    \ncluster_data = model.labels_ == -1\nfor true_label in range(3):\n    bool_idx = (y_true == true_label) & (model.labels_ == -1)\n    plt.scatter(X[bool_idx, 0], X[bool_idx, 3],\n                c='grey', s=80, marker=markers[true_label])\n    \nplt.title(\"Clustering, Hard\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.annotate('Algorithm 2', xy=(400, 30), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))\nplt.legend(loc='upper left', fontsize=18)\nplt.xlim(x_lims)\nplt.ylim(y_lims)","1f66d0f5":"BIC_scores = []\n\nfor n_clusters_this in tqdm(range(2, 10)):\n    iteration_bics = []\n    for _ in range(20):\n        model = GaussianMixture(n_clusters_this, n_init=100).fit(X[:, [0, 3]])\n        iteration_bics.append(model.bic(X[:, [0, 3]]))\n    BIC_scores.append(np.mean(iteration_bics))","0a822846":"plt.figure(figsize=(10, 7))\nplt.plot(range(2,10), BIC_scores, 'bo-')\nplt.scatter(4, BIC_scores[2], color='red', s=100, label='Optimal')\nplt.xlabel('Number of Clusters', fontsize=20)\nplt.xticks(np.arange(2, 10))\nplt.ylabel('BIC', fontsize=20)\nplt.legend(fontsize=18)\nplt.title(\"GMM Bayesian information criterion\", fontsize=27)","889e2d5d":"# source: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.12-gaussian-mixtures.html        \n        \ndef draw_ellipse(position, covariance, color, ax=None, **kwargs):\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n    ax = ax or plt.gca()\n    # Convert covariance to principal axes\n    if covariance.shape == (2, 2):\n        U, s, Vt = np.linalg.svd(covariance)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = 2 * np.sqrt(s)\n    else:\n        angle = 0\n        width, height = 2 * np.sqrt(covariance)\n    \n    # Draw the Ellipse\n    for nsig in range(1, 4):\n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, color=color, **kwargs))\n        \ndef plot_gmm(gmm, X, colors, label=True, ax=None):\n    ax = ax or plt.gca()\n    labels = gmm.fit(X).predict(X)\n    \n    for label_i in labels:\n        bool_idx = (labels == label_i)\n        ax.scatter(X[bool_idx, 0], X[bool_idx, 1], c=colors[label_i], s=40)\n    \n    w_factor = 0.2 \/ gmm.weights_.max()\n    i = 0\n    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n        draw_ellipse(pos, covar, alpha=w * w_factor, color=colors[i])\n        i += 1\n\n\nplt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red']\nmarkers = ['o', 'p', 'v']\n\nmodel =  GaussianMixture(n_components=4, n_init=100)\nmodel.fit(X[:, [0, 3]])\n\nplot_gmm(model, X[:, [0, 3]], colors)\n\nplt.title(\"Clustering, Soft\", fontsize=27)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.show()","6f2784b3":"def plot_gmm(gmm, X, colors, label=True, ax=None):\n    labels_covered = set()\n    ax = ax or plt.gca()\n    labels = gmm.fit(X).predict(X)\n    \n    for label_i in labels:\n        for k in range(3):\n            bool_idx = (y_true == k) & (labels == label_i)\n            ax.scatter(X[bool_idx, 0], X[bool_idx, 1], marker=markers[k], c=colors[label_i], s=40, \n                       label=iris.target_names[k] if iris.target_names[k] not in labels_covered else None)\n            labels_covered.add(iris.target_names[k])\n    \n    w_factor = 0.2 \/ gmm.weights_.max()\n    i = 0\n    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n        draw_ellipse(pos, covar, alpha=w * w_factor, color=colors[i])\n        i += 1\n\n\nplt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red']\nmarkers = ['o', 'p', 'v']\n\nmodel =  GaussianMixture(n_components=4, n_init=100)\nmodel.fit(X[:, [0, 3]])\n\nplot_gmm(model, X[:, [0, 3]], colors)\n\nplt.title(\"Clustering, Soft\", fontsize=27)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.legend(loc='lower right', fontsize=18)\nplt.show()","37ccc575":"plt.figure(figsize=(10, 7))\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06', 'red']\nmarkers = ['o', 'p', 'v']\n\nmodel =  GaussianMixture(n_components=4, n_init=100)\nX_with_additional_anomaly = np.concatenate((X[:, [0, 3]], np.array([[4.5, 2.5]])))\n\nmodel.fit(X_with_additional_anomaly)\n\nlabels_covered = set()\n\nlikelihood = np.exp(model.score_samples(X_with_additional_anomaly))\nbool_idx = likelihood < 0.05\nplt.scatter(X_with_additional_anomaly[bool_idx, 0], X_with_additional_anomaly[bool_idx, 1], c='red', s=80, \n           label='likelihood <5%')\n\nbool_idx = likelihood >= 0.05\nplt.scatter(X_with_additional_anomaly[bool_idx, 0], X_with_additional_anomaly[bool_idx, 1], c='grey', s=50, \n           label='likelihood >=5%')\n\nw_factor = 0.08 \/ model.weights_.max()\ni = 0\nfor pos, covar, w in zip(model.means_, model.covariances_, model.weights_):\n    draw_ellipse(pos, covar, alpha=w * w_factor, color=colors[i])\n    i += 1\nfor k, col in enumerate(colors):\n    cluster_data = y_true == k\n    \nxx, yy = np.meshgrid(np.linspace(*x_lims, 150),\n                     np.linspace(*y_lims, 150))\n\nZ = np.exp(model.score_samples(np.c_[xx.ravel(), yy.ravel()]))\nZ[Z < 0.05] = 0\nZ[Z >= 0.05] = 1\nZ = Z.reshape(xx.shape)\nplt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n\nplt.title(\"Anomaly detection\", fontsize=27)\nplt.xlim(x_lims)\nplt.ylim(y_lims)\nplt.annotate('Algorithm 1', xy=(400, 100), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))\n\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.legend(loc='lower right', fontsize=18)","17899691":"model = IsolationForest(contamination=0.05, random_state=42)\nmodel.fit(X_with_additional_anomaly)\ny_pred = model.predict(X_with_additional_anomaly)\n\nplt.figure(figsize=(10, 7))\n\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n\ncolors = ['red', 'grey']\nsizes = [80,50]\nfor y in [1, -1]:\n    k = (y + 1) \/\/ 2\n    bool_idx = y_pred == y\n    plt.scatter(X_with_additional_anomaly[bool_idx, 0], X_with_additional_anomaly[bool_idx, 1], \n                s=sizes[k], color=colors[k])\n\nplt.xlim(*x_lims)\nplt.ylim(*y_lims)\nplt.title(\"Anomaly detection\", fontsize=27)\nplt.xlabel('Sepal length (cm)', fontsize=20)\nplt.ylabel('Petal width (cm)', fontsize=20)\nplt.annotate('Algorithm 2', xy=(400, 30), xycoords='axes points', fontsize=24, bbox=dict(boxstyle='round', fc='w'))","57724371":"# soft actor critic implementation from https:\/\/github.com\/philtabor\/Youtube-Code-Repository\/tree\/master\/ReinforcementLearning\/PolicyGradient\/SAC\n\ndef plot_learning_curve(x, scores, figure_file):\n    running_avg = np.zeros(len(scores))\n    for i in range(len(running_avg)):\n        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n    plt.plot(x, running_avg)\n    plt.title('Running average of previous 100 scores')\n    plt.savefig(figure_file)\n    \nclass ReplayBuffer():\n    def __init__(self, max_size, input_shape, n_actions):\n        self.mem_size = max_size\n        self.mem_cntr = 0\n        self.state_memory = np.zeros((self.mem_size, *input_shape))\n        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n        self.action_memory = np.zeros((self.mem_size, n_actions))\n        self.reward_memory = np.zeros(self.mem_size)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n\n    def store_transition(self, state, action, reward, state_, done):\n        index = self.mem_cntr % self.mem_size\n\n        self.state_memory[index] = state\n        self.new_state_memory[index] = state_\n        self.action_memory[index] = action\n        self.reward_memory[index] = reward\n        self.terminal_memory[index] = done\n\n        self.mem_cntr += 1\n\n    def sample_buffer(self, batch_size):\n        max_mem = min(self.mem_cntr, self.mem_size)\n\n        batch = np.random.choice(max_mem, batch_size)\n\n        states = self.state_memory[batch]\n        states_ = self.new_state_memory[batch]\n        actions = self.action_memory[batch]\n        rewards = self.reward_memory[batch]\n        dones = self.terminal_memory[batch]\n\n        return states, actions, rewards, states_, dones\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, beta, input_dims=1, n_actions=1, fc1_dims=128, fc2_dims=64,\n            name='critic', chkpt_dir='tmp\/sac'):\n        super(CriticNetwork, self).__init__()\n        self.input_dims = input_dims\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n        self.n_actions = n_actions\n        self.name = name\n        self.checkpoint_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n\n        self.fc1 = nn.Linear(self.input_dims[0]+n_actions, self.fc1_dims)\n        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n        self.fc3 = nn.Linear(self.fc2_dims, self.fc2_dims)\n        self.q = nn.Linear(self.fc2_dims, 1)\n\n        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n\n        self.to(self.device)\n\n    def forward(self, state, action):\n        action_value = self.fc1(T.cat([state, action], dim=1))\n        action_value = F.relu(action_value)\n        action_value = self.fc2(action_value)\n        action_value = F.relu(action_value)\n        action_value = self.fc3(action_value)\n        action_value = F.relu(action_value)\n\n        q = self.q(action_value)\n\n        return q\n\n    def save_checkpoint(self):\n        T.save(self.state_dict(), self.checkpoint_file)\n\n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\n\nclass ValueNetwork(nn.Module):\n    def __init__(self, beta, input_dims=1, fc1_dims=128, fc2_dims=64,\n            name='value', chkpt_dir='tmp\/sac'):\n        super(ValueNetwork, self).__init__()\n        self.input_dims = input_dims\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n        self.name = name\n        self.checkpoint_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n\n        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n        self.fc2 = nn.Linear(self.fc1_dims, fc2_dims)\n        self.fc3 = nn.Linear(fc2_dims, fc2_dims)\n        self.v = nn.Linear(self.fc2_dims, 1)\n\n        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n\n        self.to(self.device)\n\n    def forward(self, state):\n        state_value = self.fc1(state)\n        state_value = F.relu(state_value)\n        state_value = self.fc2(state_value)\n        state_value = F.relu(state_value)\n        state_value = self.fc3(state_value)\n        state_value = F.relu(state_value)\n\n        v = self.v(state_value)\n\n        return v\n\n    def save_checkpoint(self):\n        T.save(self.state_dict(), self.checkpoint_file)\n\n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, alpha, input_dims=1,fc1_dims=128, \n            fc2_dims=64, n_actions=1, action_space=None, name='actor', chkpt_dir='tmp\/sac'):\n        super(ActorNetwork, self).__init__()\n        self.input_dims = input_dims\n        self.fc1_dims = fc1_dims\n        self.fc2_dims = fc2_dims\n        self.n_actions = n_actions\n        self.name = name\n        self.checkpoint_dir = chkpt_dir\n        if not os.path.exists(self.checkpoint_dir):\n            os.makedirs(self.checkpoint_dir)\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n        self.reparam_noise = 1e-6\n\n        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n        self.fc3 = nn.Linear(self.fc2_dims, self.fc2_dims)\n        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n        self.sigma_log = nn.Linear(self.fc2_dims, self.n_actions)\n        \n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n\n        # from https:\/\/github.com\/pranz24\/pytorch-soft-actor-critic\n        if action_space is None:\n            self.action_scale = 1.\n            self.action_bias = 0.\n        else:\n            self.action_scale = T.FloatTensor(\n                (action_space.high - action_space.low) \/ 2.).to(self.device)\n            self.action_bias = T.FloatTensor(\n                (action_space.high + action_space.low) \/ 2.).to(self.device)\n\n        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n\n        self.to(self.device)\n\n    def forward(self, state):\n        prob = self.fc1(state)\n        prob = F.relu(prob)\n        prob = self.fc2(prob)\n        prob = F.relu(prob)\n        prob = self.fc3(prob)\n        prob = F.relu(prob)\n\n        mu = self.mu(prob)\n        sigma_log = self.sigma_log(prob)\n        sigma_log = T.clamp(sigma_log, min=-20, max=2)\n        sigma = sigma_log.exp()\n\n        return mu, sigma\n\n    def sample_normal(self, state):\n        mu, sigma = self.forward(state)\n        distr = Normal(mu, sigma)\n        x_t = distr.rsample()\n\n        y_t = T.tanh(x_t)\n        actions = y_t * self.action_scale + self.action_bias\n        log_probs = distr.log_prob(x_t)\n        log_probs -= T.log(self.action_scale * (1 - y_t.pow(2)) + self.reparam_noise)\n        return actions, log_probs\n\n    def save_checkpoint(self):\n        T.save(self.state_dict(), self.checkpoint_file)\n\n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\n        \n        \nclass Agent():\n    def __init__(self, alpha=1e-2, beta=1e-2, input_dims=[1],\n            env=None, gamma=0.99, n_actions=1, max_size=1000000, tau=0.005,\n            batch_size=512, reward_scale=2, load=False):\n        self.gamma = gamma\n        self.tau = tau\n        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n        self.batch_size = batch_size\n        self.n_actions = n_actions\n\n        self.actor = ActorNetwork(alpha, input_dims, n_actions=n_actions,\n                    name='actor', action_space=env.action_space)\n        self.critic_1 = CriticNetwork(beta, input_dims, n_actions=n_actions,\n                    name='critic_1')\n        self.critic_2 = CriticNetwork(beta, input_dims, n_actions=n_actions,\n                    name='critic_2')\n        self.value = ValueNetwork(beta, input_dims, name='value')\n        self.target_value = ValueNetwork(beta, input_dims, name='target_value')\n\n        self.scale = reward_scale\n        if load:\n            self.update_network_parameters(tau=1)\n\n    def choose_action(self, observation):\n        state = T.Tensor([observation]).to(self.actor.device)\n        actions, _ = self.actor.sample_normal(state)\n\n        return actions.cpu().detach().numpy()[0]\n    \n    def get_mean(self, observations):\n        state = T.Tensor(observations).to(self.actor.device)\n        mu, _ = self.actor.forward(state)\n        mu = T.tanh(mu) * self.actor.action_scale + self.actor.action_bias\n        return mu.cpu().detach().numpy()\n\n    def remember(self, state, action, reward, new_state, done):\n        self.memory.store_transition(state, action, reward, new_state, done)\n\n    def update_network_parameters(self, tau=None):\n        if tau is None:\n            tau = self.tau\n\n        target_value_params = self.target_value.named_parameters()\n        value_params = self.value.named_parameters()\n\n        target_value_state_dict = dict(target_value_params)\n        value_state_dict = dict(value_params)\n\n        for name in value_state_dict:\n            value_state_dict[name] = tau*value_state_dict[name].clone() + \\\n                    (1-tau)*target_value_state_dict[name].clone()\n\n        self.target_value.load_state_dict(value_state_dict)\n\n    def save_models(self):\n        self.actor.save_checkpoint()\n        self.value.save_checkpoint()\n        self.target_value.save_checkpoint()\n        self.critic_1.save_checkpoint()\n        self.critic_2.save_checkpoint()\n\n    def load_models(self):\n        print('.... loading models ....')\n        self.actor.load_checkpoint()\n        self.value.load_checkpoint()\n        self.target_value.load_checkpoint()\n        self.critic_1.load_checkpoint()\n        self.critic_2.load_checkpoint()\n\n    def learn(self):\n        if self.memory.mem_cntr < self.batch_size:\n            return\n\n        state, action, reward, new_state, done = \\\n                self.memory.sample_buffer(self.batch_size)\n\n        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n        done = T.tensor(done).to(self.actor.device)\n        state_ = T.tensor(new_state, dtype=T.float).to(self.actor.device)\n        state = T.tensor(state, dtype=T.float).to(self.actor.device)\n        action = T.tensor(action, dtype=T.float).to(self.actor.device)\n\n        value = self.value(state).view(-1)\n        value_ = self.target_value(state_).view(-1)\n        value_[done] = 0.0\n        actions, log_probs = self.actor.sample_normal(state)\n        log_probs = log_probs.view(-1)\n        \n        q1_new_policy = self.critic_1.forward(state, actions)\n        q2_new_policy = self.critic_2.forward(state, actions)\n        critic_value = T.min(q1_new_policy, q2_new_policy)\n        critic_value = critic_value.view(-1)\n        \n        self.value.optimizer.zero_grad()\n        value_target = critic_value - log_probs\n        value_loss = 0.5 * F.mse_loss(value, value_target)\n        value_loss.backward(retain_graph=True)\n        self.value.optimizer.step()\n\n        actions, log_probs = self.actor.sample_normal(state)\n        log_probs = log_probs.view(-1)\n        q1_new_policy = self.critic_1.forward(state, actions)\n        q2_new_policy = self.critic_2.forward(state, actions)\n        critic_value = T.min(q1_new_policy, q2_new_policy)\n        critic_value = critic_value.view(-1)\n        \n        actor_loss = log_probs - critic_value\n        actor_loss = T.mean(actor_loss)\n        self.actor.optimizer.zero_grad()\n        actor_loss.backward(retain_graph=True)\n        self.actor.optimizer.step()\n\n        self.critic_1.optimizer.zero_grad()\n        self.critic_2.optimizer.zero_grad()\n        q_hat = self.scale*reward + self.gamma*value_\n        q1_old_policy = self.critic_1.forward(state, action).view(-1)\n        q2_old_policy = self.critic_2.forward(state, action).view(-1)\n        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n\n        critic_loss = critic_1_loss + critic_2_loss\n        critic_loss.backward()\n        self.critic_1.optimizer.step()\n        self.critic_2.optimizer.step()\n\n        self.update_network_parameters()","25229984":"# custom gym env\n\n\nclass IrisRegressionGameEnv(gym.Env):\n\n    def __init__(self, number_of_neighbours=5, with_trap=False):\n        super(IrisRegressionGameEnv, self).__init__()\n\n        iris = load_iris()\n        self.X = iris.data[:, 0]\n        self.X_nbrs = NearestNeighbors(n_neighbors=number_of_neighbours, algorithm='ball_tree')\n        self.X_nbrs.fit(self.X.reshape(-1, 1))\n        \n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X.reshape(-1, 1))\n        \n        self.y = iris.data[:, 3]\n\n        self.action_space = spaces.Box(low=np.array([-2]), high=np.array([self.y.max() + 2]))\n\n        self.X_min = self.X.min()*0.99\n        self.X_max = self.X.max()*1.01\n        \n        self.danger_zone_width = (self.X_max - self.X_min)\/10\n        self.danger_zone_start = np.random.uniform(self.X_min + 1, self.X_max - 5*self.danger_zone_width, 1)[0]\n        self.danger_zone_prediction_lb = self.y.max()*0.95\n        \n        self.observation_space = spaces.Box(low=np.array([self.X_min]), high=np.array([self.X_max]))\n        self.current_X = None\n        \n        self.number_of_resets = 0\n        self.danger_change_reset = 100\n        \n        self.number_of_steps = 0\n        self.max_number_of_steps = 1000\n        \n        self.with_trap = with_trap\n        \n        \n    def transform_x(self, x):\n        return self.scaler.transform(x.reshape(-1, 1))\n    \n    def inverse_transform_x(self, x):\n        return self.scaler.inverse_transform(x.reshape(-1, 1))\n        \n    def _get_observation(self):\n        x = np.random.uniform(self.X_min, self.X_max, 1)[0]\n        self.current_X = self.transform_x(x)\n        return self.current_X\n    \n    def _get_reward(self, action):\n        X_denormed = self.inverse_transform_x(self.current_X)\n        if self.danger_zone_start <= X_denormed <= self.danger_zone_start + self.danger_zone_width and self.with_trap:\n            if action >= self.danger_zone_prediction_lb:\n                return -(action - self.danger_zone_prediction_lb)**4\n            return -1e3\n        _, indices = self.X_nbrs.kneighbors(X_denormed)\n        y_mean = np.mean(self.y[indices])\n        return -(action - y_mean)**4\n\n    def step(self, action):\n        # agent's action is the predicted y\n        \n        reward = self._get_reward(action)\n        \n        self.number_of_steps += 1\n        done = self.number_of_steps >= self.max_number_of_steps\n\n        obs = self._get_observation()\n\n        return obs, reward, done, {}\n    \n    def set_trap(self):\n        self.with_trap = True\n\n    def reset(self):\n        self.number_of_resets += 1        \n        self.number_of_steps = 0\n\n        return self._get_observation()\n\n    def render(self, mode='human', close=False):\n        raise NotImplementedError","9b9af5a4":"env = IrisRegressionGameEnv()\n\nagent = Agent(input_dims=env.observation_space.shape, env=env,\n              alpha=1e-3, beta=1e-3,\n              n_actions=env.action_space.shape[0])","0ad542e8":"def plot_agent_predictions(agent, env, title=''):\n    x = np.linspace(*x_lims, 400).reshape(-1, 1)\n    y_pred = agent.get_mean(env.transform_x(x))\n\n    plt.figure(figsize=(10, 7))\n\n    plt.plot(x, y_pred, 'g-', linewidth=3, label='Agent prediction')\n    plt.scatter(X[:, 0], X[:, 3],\n                marker='.', s=120)\n    if env.with_trap:\n        plt.plot([env.danger_zone_start, env.danger_zone_start + env.danger_zone_width], \n                 [env.danger_zone_prediction_lb, env.danger_zone_prediction_lb], color='red', linewidth=5)\n        x_trap = np.linspace(env.danger_zone_start, env.danger_zone_start + env.danger_zone_width, 100)\n        plt.fill(np.concatenate([x_trap, x_trap[::-1]]),\n                 np.concatenate([y_lims[0]*np.ones(100),\n                                (env.danger_zone_prediction_lb*np.ones(100))[::-1]]),\n                 alpha=.3, fc='red', ec='None', label='Trap')\n\n\n    plt.xlim(x_lims)\n    plt.ylim(y_lims)\n    plt.xlabel('Sepal length (cm)', fontsize=20)\n    plt.ylabel('Petal width (cm)', fontsize=20)\n    plt.legend(loc='lower right', fontsize=18)\n    plt.title(title, fontsize=27)\n    plt.show()\n    \nplot_agent_predictions(agent, env, 'Before interactions\/feedback')","f1ed8cd2":"# again, based on https:\/\/github.com\/philtabor\/Youtube-Code-Repository\/blob\/master\/ReinforcementLearning\/PolicyGradient\/SAC\/main_sac.py\n\nn_games = 15\n\nbest_score = env.reward_range[0]\n\nvis_freq = 3\n\nfor i in tqdm(range(n_games)):\n    observation = env.reset()\n    done = False\n    score = 0\n    while not done:\n        action = agent.choose_action(observation)\n        observation_, reward, done, info = env.step(action)\n        score += reward\n        agent.remember(observation, action, reward, observation_, done)\n        agent.learn()\n        observation = observation_\n\n    if score > best_score:\n        best_score = score\n        agent.save_models()\n        \n    if i == 7:\n        plot_agent_predictions(agent, env, title=f'Agent predictions after {env.max_number_of_steps*(i + 1):0.0f} interactions')\n        env.set_trap() \n    elif (i + 1) % vis_freq == 0:\n        plot_agent_predictions(agent, env, title=f'Agent predictions after {env.max_number_of_steps*(i + 1):0.0f} interactions')","7690646c":"In this notebook, we'll apply techniques from supervised, unsupervised, and reinforcement learning on the Fisher's Iris dataset. The goal is to create plots illustrating the main machine learning types.","44bd321d":"### Problem","a936d247":"## Clustering\n### Hard\n#### K-Means","b1be1758":"**Observation (state)**: Sepal length; Next state is sampled at random.\n\n**Agent's action**: Guessed petal width value;\n\n**Reward**: if no trap, then the 4th power of distances to K nearest neighbours; if caught in a trap, then an agent receives a very high penalty.\u00a0\u00a0","8b76b5f5":"Nice purity!","a80cd3c2":"#### HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)","c6d96e3a":"### Solution: Active learning","861052e4":"## Classification, predicting a categorical variable\n### Problem","8bc6d8bb":"Visualization","7c8fb61f":"# Supervised learning\n## Regression, predicting a continuous variable","3bb82f81":"# Libraries","3648bd6b":"Let's check how it corresponds to ground truth labels.","227574a0":"### Isolation Forest","4a90379f":"Illustrating on a less frequently covered RL problem of continous variable prediction.","7334bbdc":"### Solution","65dfa9c7":"### Soft\n#### Gaussian mixture model\nEstimating the best number of clusters","7f20d145":"### Expensive labeling. What data to gather next?","2c500c7a":"### Solution","542949fb":"# Data","c7aaf348":"# Unsupervised learning","671e7f7f":"# Content\n1. [Libraries](#Libraries)\n2. [Data](#Data)\n3. [Supervised learning](#Supervised-learning)\n   * 3.1. [Regression: predicting a continuous variable](#Regression,-predicting-a-continuous-variable)\n       * [Standard](#Problem)\n       * [Expensive labeling. What data to gather next?](#Expensive-labeling.-What-data-to-gather-next?)\n   * 3.2. [Classification: predicting a categorical variable](#Classification,-predicting-a-categorical-variable)\n4. [Unsupervised learning](#Unsupervised-learning)\n   * 4.1. [Clustering](#Clustering)\n      * 4.1.1. [Hard](#Hard)\n         * [K-Means](#K-Means)\n         * [HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)](#HDBSCAN-(Hierarchical-Density-Based-Spatial-Clustering-of-Applications-with-Noise))\n      * 4.1.2. [Soft](#Soft)\n         * [Gaussian mixture model](#Gaussian-mixture-model)\n   * 4.2. [Anomaly detection](#Anomaly-detection)\n      * 4.2.1. [Gaussian Mixture Model](#Gaussian-Mixture-Model)\n      * 4.2.2. [Isolation Forest](#Isolation-Forest)\n5. [Reinforcement learning](#Reinforcement-learning)","eb9d1e86":"## Anomaly detection\n### Gaussian Mixture Model","840b9c14":"Estimating a number of clusters for K-means","ec3c198c":"# Reinforcement learning"}}