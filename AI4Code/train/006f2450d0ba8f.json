{"cell_type":{"c0f842fe":"code","df8a1bff":"code","01cd797f":"code","428a9023":"code","4230776f":"code","c634759d":"code","0c737c51":"code","a98e3007":"code","2614df1f":"code","eead7b85":"markdown","b869d6ef":"markdown","4c285793":"markdown","e6fe8ea3":"markdown","93b0fa4b":"markdown","ba3f181d":"markdown","7b710ef7":"markdown"},"source":{"c0f842fe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n\ndf = pd.read_csv(\"..\/input\/data.csv\", index_col = 'id')\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\nX = df.drop('diagnosis',axis = 1)\nperimeters = [x for x in df.columns if 'perimeter' in x]\nareas = [x for x in df.columns if 'area' in x]\ndf.drop(perimeters, axis = 1 ,inplace = True)\ndf.drop(areas, axis = 1 ,inplace = True)\nworst = [col for col in df.columns if col.endswith('_worst')]\ndf.drop(worst, axis = 1 ,inplace = True)","df8a1bff":"y = df['diagnosis']\nX = df.drop(['diagnosis'], axis=1).values\nX_scaled = StandardScaler().fit_transform(X)\n\n#Define k-NN classifier and train on a scaled dataset\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_scaled, y)","01cd797f":"knn_params = {'n_neighbors': range(1, 11), 'weights':['uniform', 'distance']}\n\nX_scaled_train, X_scaled_holdout, y_train, y_holdout = train_test_split(X_scaled, y, test_size=0.3,\n                                                                        random_state=17)\n\n#knn_grid.best_estimator_.predict(X_scaled_train)\nknn_grid = GridSearchCV(knn, knn_params, cv=10, n_jobs=-1, scoring='recall')\n\nknn_grid.fit(X_scaled_train, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","428a9023":"pred = knn_grid.best_estimator_.predict(X_scaled_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, pred))\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, pred) )\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, pred))","4230776f":"cm = confusion_matrix(y_holdout, pred)\ncm","c634759d":"from matplotlib.colors import ListedColormap\n\nh = .02  # step size in the mesh\nweights ='uniform'\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we only take the first two features: radius_mean and concave points_mean. We could avoid this ugly\n# slicing by using a two-dim dataset\n\nfor n_neighbors in [3,5]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X_scaled[:,[0,5]], y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X_scaled[:,0].min() - 1, X_scaled[:,0].max() + 1\n    y_min, y_max = X_scaled[:,5].min() - 1, X_scaled[:,5].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 5], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    plt.xlabel(\"radius\")\n    plt.ylabel(\"concave points\")\n\nplt.show()","0c737c51":"from sklearn.model_selection import cross_val_score\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    train_scores.append(cross_val_score(knn, X_scaled_train,y_train,cv=10, scoring='recall').mean())\n    test_scores.append(cross_val_score(knn, X_scaled_holdout,y_holdout,cv=10, scoring='recall').mean())\n    \nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')\nplt.xlabel(\"Neighbours\")\nplt.ylabel(\"Recall\")","a98e3007":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_scaled_train,y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_scaled_holdout)\n","2614df1f":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_holdout, y_pred))\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, y_pred) )\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, y_pred))","eead7b85":"Hello Kagglers! This work is part of my ongoing project in Predictive Analytics to classify Breast Cancer tumors: whether it's Malignant or Benign.The first part, which is the Explanatory Data Analysis and data visualization, was done [here](http:\/\/www.kaggle.com\/sulianova\/feature-explanation-and-eda). For PCA and Random Forest application please see [this page](http:\/\/www.kaggle.com\/sulianova\/pca-logistic-regression-and-random-forest).","b869d6ef":"To assign the class, when neighbors don\u2019t have the same class, we can set 'weights' parameter:\n\n1. = 'uniform' takes a simple majority vote from the neighbors. Whichever class has the greatest number of votes becomes the class for the new data point.\n2. = 'distance' takes a similar vote except gives a heavier weight to those neighbors that are closer. For example, if the neighbor is 5 units away, then weight its vote 1\/5. As the neighbor gets further away, the weight gets smaller.\n\nLet's find out which parameter is better for our dataset:","4c285793":"# kNN\n\nFeatures with a larger range of values can dominate the distance metric relative to features that have a smaller range, so feature scaling is important. For continuous data, kNN uses a distance metric like Euclidean or Minkowski distance. As all features are numerical, we do not need to change default metric, which is 'minkowski'.","e6fe8ea3":"Let's compare how kNN performs, if we select 3 and 5 closest neighbors:","93b0fa4b":"Na\u00efve Bayes slightly imporved the accuracy and precision scores.","ba3f181d":"Or we can compare score on train an test sets for different number of neighbors:","7b710ef7":"As we can see, the best number of neighbours for the training data is 5.\n\n# Na\u00efve Bayes"}}