{"cell_type":{"8e0902d7":"code","1055cb2f":"code","72e78c58":"code","c6b3b744":"code","e2c83705":"code","8452f9dd":"code","86293e29":"code","656396a5":"code","fe175aa1":"code","ec105aaf":"code","f16e9f6b":"code","b4b028b0":"code","8816b9fb":"code","f55bacf6":"code","adb254e6":"code","fa18ccaf":"code","f6419ae8":"code","7a5dbffa":"code","b140c9c3":"code","e5416dc8":"code","4644da40":"code","b3220705":"code","bdb75717":"code","ef4772ca":"code","40311292":"code","e6b7459f":"code","b2c5e19f":"code","7caf738d":"code","862da602":"code","e45b56f5":"code","f7b55d66":"code","cb919819":"code","e7bccd65":"code","4be423ae":"code","b146bd3d":"code","b41a002f":"code","bd1ce95e":"code","b929185a":"code","60564051":"code","e9fb6ecf":"code","6b712bf6":"code","c1e3bace":"code","ab7c9054":"code","89ff0fb4":"code","73d0ffa9":"code","208535b9":"code","add1634d":"code","1f3ea1ac":"code","d480fd3f":"code","14c8180e":"code","cb870822":"code","ae481b23":"code","a59fb9ec":"code","2cf84941":"code","3ce5aa4e":"code","f76fced4":"code","9dc84b5c":"code","14a700c5":"code","e75f7375":"code","e1ca84e4":"code","d40c309d":"code","c6abca5c":"code","776cdf2f":"code","9766108b":"code","e98f0aeb":"markdown","30c8c102":"markdown","ea52ea8b":"markdown","501682b9":"markdown","6262d012":"markdown","1a507d34":"markdown","dfb9ccde":"markdown","9ba904f1":"markdown","74610dd8":"markdown","c562c877":"markdown","7c71cac1":"markdown","fbfd8d00":"markdown","5efe9977":"markdown","5730be75":"markdown","2cd10a0c":"markdown","236bbd90":"markdown","f9e70916":"markdown","c918364b":"markdown","924d3909":"markdown","d23192d8":"markdown","4d50448f":"markdown","94a3e19d":"markdown","f786cdac":"markdown","02950c41":"markdown","30431215":"markdown","1872ce9d":"markdown","8fc2d98f":"markdown","17aac7ba":"markdown","3a2b4e84":"markdown","12b55512":"markdown","77c75656":"markdown","495b9051":"markdown"},"source":{"8e0902d7":"import time\nstart_time = time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nimport os\n# Any results you write to the current directory are saved as output.\nimport json\nimport warnings; warnings.filterwarnings(\"ignore\");\n\nfrom sklearn.ensemble import RandomForestRegressor\n#from sklearn import pipeline\nfrom sklearn import pipeline\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.feature_extraction import DictVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import TruncatedSVD\n#from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error, make_scorer\n#from nltk.metrics import edit_distance\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n#from nltk.stem.snowball import SnowballStemmer #0.003 improvement but takes twice as long as PorterStemmer\nfrom bs4 import BeautifulSoup\nimport re\nfrom scipy.stats import spearmanr\nimport random\nrandom.seed(2016)\nfrom scipy.stats import norm  \nimport seaborn as sns # data visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt","1055cb2f":"path = \"..\/input\/home-depot-product-search-relevance-data\/\"\ntrain_df = pd.read_csv(path + \"train.csv\", encoding=\"ISO-8859-1\") #update here\ntest_df = pd.read_csv(path + \"test.csv\", encoding=\"ISO-8859-1\") #update here\ndesc_df = pd.read_csv(path + \"product_descriptions.csv\", encoding=\"ISO-8859-1\") #update here\nattribute_df = pd.read_csv(path + \"attributes.csv\", encoding=\"ISO-8859-1\") #update here\nprint(train_df.shape)\ntrain_df.head(1)","72e78c58":"def get_df_info(df):\n    print(f\"df shape: \\n\",df.shape)\n    print(f\"df data has {df.isnull().values.sum()} null values:\")\n    print(f\"df columns: \\n\",df.columns)\n    print(f\"df data types: \\n\",df.dtypes)\n    return df.head(10)","c6b3b744":"get_df_info(train_df)","e2c83705":"print(\"there are {} product_uid\".format(len(train_df.product_uid.unique())))\nprint(\"there are {} products_title\".format(len(train_df.product_title.unique())))\nprint(\"there are {} search query \".format(len(train_df.search_term.unique())))","8452f9dd":"sns.catplot(x='relevance', kind='count', data=train_df, height=3, aspect=2)\nplt.title('distribution of relevance scores')\nplt.show()","86293e29":"get_df_info(test_df)","656396a5":"print(\"there are {} product_uid\".format(len(test_df.product_uid.unique())))\nprint(\"there are {} products_title\".format(len(test_df.product_title.unique())))\nprint(\"there are {} search query \".format(len(test_df.search_term.unique())))","fe175aa1":"# pip install matplotlib-venn","ec105aaf":"from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\nfrom matplotlib_venn import venn3, venn3_circles\nfrom matplotlib import pyplot as plt\n%matplotlib inline","f16e9f6b":"venn2([set(train_df[\"product_uid\"]), set(test_df[\"product_uid\"])], set_labels=('train', 'test'),set_colors=('purple', 'skyblue'),alpha = 0.5)\nplt.show()","b4b028b0":"get_df_info(desc_df)","8816b9fb":"print(\"there are {} product_uid\".format(len(desc_df.product_uid.unique())))\nprint(\"there are {} product_description\".format(len(desc_df.product_description.unique())))","f55bacf6":"set1 = set(train_df['product_uid'].values)\nset2 = set(desc_df['product_uid'].values)\n\nvenn2([set1, set2], ('train_df', 'desc_df'))\nplt.title('Train and desc distribution of products')\nplt.show()","adb254e6":"get_df_info(attribute_df)","fa18ccaf":"grp_df = attribute_df.groupby(['product_uid'])\nprint('max number of attributes for a single product', grp_df.count()['value'].max())\nprint('min number of attributes for a single product', grp_df.count()['value'].min())\nprint('')\n\nplt.figure(figsize=(7,5))\nplt.plot(np.sort(grp_df.count()['value']))\nplt.title('distribution of #attributes for each product')\nplt.show()","f6419ae8":"set1 = set(train_df['product_uid'].values)\nset2 = set(attribute_df['product_uid'].values)\n\nvenn2([set1, set2], ('train_df', 'attribute_df'))\nplt.title('train_df and attr_df distribution of products')\nplt.show()","7a5dbffa":"temp_ = train_df.copy()\ntemp_['rel'] = temp_['relevance'].apply(lambda x: 'High' if x > 2 else 'Low')\n\ncorr, _ = spearmanr(temp_['product_uid'], temp_['rel'])\nprint('Spearmans correlation between product_uid and relevance class', corr)\nprint('')\n\nsns.kdeplot(data=temp_, x=\"product_uid\", hue=\"rel\")\nplt.plot()\nplt.show()","b140c9c3":"# h\u00e0m n\u1ed1i thu\u1ed9c t\u00ednh\ndef merge_attributes(df):\n    product_uids = df['product_uid'].values\n    temp = attribute_df.loc[attribute_df['product_uid'].isin(product_uids)].fillna('')  \n    temp['name_value'] = temp['name'] + ' ' + temp['value']\n    temp['attr'] = temp.groupby(['product_uid'])['name_value'].transform(lambda x: ' '.join(x))\n    temp = temp.drop_duplicates('product_uid')[['product_uid', 'attr']]\n    df = pd.merge(df, temp, on='product_uid', how='left').set_index(df.index)\n    return df","e5416dc8":"# h\u00e0m n\u1ed1i v\u1edbi brand\ndef merge_brand(df):\n    product_uids = df['product_uid'].values\n    temp = attribute_df.loc[attribute_df['product_uid'].isin(product_uids)]  \n    brands = temp[temp['name']=='MFG Brand Name']\n    brands_temp = brands[['product_uid','value']]\n    df = pd.merge(df, brands_temp, on='product_uid', how='left').set_index(df.index)\n    df.rename(columns = {'value':'brand'}, inplace = True) \n    return df","4644da40":"# h\u00e0m n\u1ed1i v\u1edbi m\u00f4 t\u1ea3\ndef merge_description(df):\n    df = pd.merge(df, desc_df, on='product_uid', how='left').set_index(df.index)\n  #t\u00e1ch c\u00e1c t\u1eeb \u0111\u01b0\u1ee3c n\u1ed1i trong m\u00f4 t\u1ea3. \n    df['product_description'] = df['product_description'].apply(lambda x: ' '.join(re.findall(r'[A-Z]?[^A-Z\\s]+|[A-Z]+', x)))\n    return df","b3220705":"num_train = train_df.shape[0]\ndf_clean = pd.concat((train_df, test_df), axis=0, ignore_index=True)\ndf_clean = merge_attributes(df_clean)\n# test_df = merge_attributes(test_df)\ndf_clean = merge_description(df_clean)\n# test_df = merge_description(test_df)\ndf_clean = merge_brand(df_clean)\n# test_df = merge_brand(test_df)\ndf_clean.drop(['attr'],axis=1, inplace=True)\nprint(df_clean.shape)\ndf_clean.columns\ndf_clean.head()","bdb75717":"for col in df_clean.columns:\n    print('{} has {} null values'.format(col, df_clean[col].isna().values.sum()))","ef4772ca":"unique_brands = np.unique(df_clean['brand'].dropna().values)\nnull_df = df_clean[df_clean['brand'].isnull()]","40311292":"def split_n(n, sent):\n    if n > len(sent.split()):\n        return 'error101'\n    return ' '.join(sent.split()[:n])","e6b7459f":"for i, row in null_df.iterrows():\n    title = row['product_title']\n    if split_n(4, title) in unique_brands:\n        null_df['brand'].loc[i] = split_n(4, title)\n    elif split_n(3, title) in unique_brands:\n        null_df['brand'].loc[i] = split_n(3, title)\n    elif split_n(2, title) in unique_brands:\n        null_df['brand'].loc[i] = split_n(2, title)\n    else:\n        null_df['brand'].loc[i] = split_n(1, title)\n\ndf_clean['brand'].loc[null_df.index] = null_df['brand'].values\n# null_df = train_df[train_df['attr'].isnull()]\n# null_df['attr'] = null_df['product_description'].copy()\n# train_df['attr'].loc[null_df.index] = null_df['attr'].values","b2c5e19f":"# train_df.drop('id',inplace=True, axis=1)\nfor col in df_clean.columns:\n    print('{} has {} null values'.format(col,df_clean[col].isna().values.sum()))","7caf738d":"from nltk.corpus import stopwords # Import the stop word list","862da602":"def remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text","e45b56f5":"#stopwords are the words contain very little or no imformation. \nstop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\n\nstrNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\nspell_check = json.load(open('..\/input\/spell-check\/fix_spell.json','r'))\ndef str_stem(s):\n    if isinstance(s, str):\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n        #split number from word\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s) \n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = s.lower()\n        s = s.replace(\"  \",\" \") #remove double space\n        #remove special character and split number\n        s = s.replace(\",\",\"\") #could be number \/ segment later\n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"\/\/\",\"\/\")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" \/ \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = s.replace(\" . \",\" \")\n        s = re.sub(r\"(^\\.|\/)\", r\"\", s)\n        s = re.sub(r\"(\\.|\/)$\", r\"\", s)\n        s = s.replace(\" x \",\" xbi \")\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)\/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n#       convert mesurement unit to standard form.\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = s.replace(\"\u00b0\",\" degrees \")\n        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n        s = s.replace(\" v \",\" volts \")\n        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n        # filter out stop words\n        s = (\" \").join([z for z in s.split(\" \") if z not in stop_w])\n        # convert string number to number\n        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n        s = remove_html_tag(s)\n        s = s.lower()\n        #fix the spell in text\n        for (k,v) in spell_check.items():\n            s = s.replace(k,v)\n        return s\n    else:\n        return \"null\"","f7b55d66":"df_clean['search_term'] = df_clean['search_term'].map(lambda x : str_stem(x))\ndf_clean['product_title'] = df_clean['product_title'].map(lambda x : str_stem(x))\ndf_clean['product_description'] = df_clean['product_description'].map(lambda x : str_stem(x))\ndf_clean['brand'] = df_clean['brand'].map(lambda x : str_stem(x))\n# df_clean['attr'] = train_df['attr'].apply(lambda x :str_stem(x))","cb919819":"df_clean.head()","e7bccd65":"df_clean['product_info'] = df_clean['search_term']+\"\\t\"+ df_clean['product_title']+\"\\t\"+df_clean['product_description']","4be423ae":"df_clean['len_of_search'] = df_clean['search_term'].map(lambda x:len(x.split())).astype(np.int64)\ndf_clean['len_of_title'] = df_clean['product_title'].map(lambda x:len(x.split())).astype(np.int64)\ndf_clean['len_of_description'] = df_clean['product_description'].map(lambda x:len(x.split())).astype(np.int64)\ndf_clean['len_of_brand'] = df_clean['brand'].map(lambda x:len(x.split())).astype(np.int64)","b146bd3d":"# hi\u1ec3n th\u1ecb d\u1eef li\u1ec7u sau khi th\u00eam\ndf_clean.head()","b41a002f":"def seg_words(str1, str2):\n    str2 = str2.lower()\n    str2 = re.sub(\"[^a-z0-9.\/]\",\" \", str2)\n    str2 = [z for z in set(str2.split()) if len(z)>2]\n    words = str1.lower().split(\" \")\n    s = []\n    for word in words:\n        if len(word)>3:\n            s1 = []\n            s1 += segmentit(word,str2,True)\n            if len(s)>1:\n                s += [z for z in s1 if z not in ['er','ing','s','less'] and len(z)>1]\n            else:\n                s.append(word)\n        else:\n            s.append(word)\n    return (\" \".join(s))","bd1ce95e":"def segmentit(s, txt_arr, t):\n    st = s\n    r = []\n    for j in range(len(s)):\n        for word in txt_arr:\n            if word == s[:-j]:\n                r.append(s[:-j])\n                s=s[len(s)-j:]\n                r += segmentit(s, txt_arr, False)\n    if t:\n        i = len((\"\").join(r))\n        if not i==len(st):\n            r.append(st[i:])\n    return r","b929185a":"df_clean['search_term'] = df_clean['product_info'].map(lambda x:seg_words(x.split('\\t')[0],x.split('\\t')[1]))","60564051":"df_clean.head()","e9fb6ecf":"def str_common_word(str1, str2):\n    words, cnt = str1.split(), 0\n    for word in words:\n        if str2.find(word)>=0:\n            cnt+=1\n    return cnt","6b712bf6":"#\u0111\u1ebfm s\u1ed1 l\u1ea7n t\u1eeb cu\u1ed1i c\u00f9ng trong ti\u00eau \u0111\u1ec1 xu\u1ea5t hi\u1ec7n trong c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\ndf_clean['last_word_in_title'] = df_clean['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[1]))\n#\u0111\u1ebfm s\u1ed1 l\u1ea7n t\u1eeb cu\u1ed1i c\u00f9ng trong product_description xu\u1ea5t hi\u1ec7n trong c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\ndf_clean['last_word_in_description'] = df_clean['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[2]))\n#\u0111\u1ebfm s\u1ed1 l\u1ea7n m\u1ed7i t\u1eeb trong product_info xu\u1ea5t hi\u1ec7n trong c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\ndf_clean['word_in_title'] = df_clean['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n#\u0111\u1ebfm s\u1ed1 l\u1ea7n m\u1ed7i t\u1eeb trong product_description xu\u1ea5t hi\u1ec7n trong c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\ndf_clean['word_in_description'] = df_clean['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))","c1e3bace":"#\u0111\u1ebfm s\u1ed1 l\u1ea7n m\u1ed7i t\u1eeb trong chu\u1ed7i1 xu\u1ea5t hi\u1ec7n trong chu\u1ed7i2\ndef str_whole_word(str1, str2, i_):\n    cnt = 0\n    while i_ < len(str2):\n        i_ = str2.find(str1, i_)\n        if i_ == -1:\n            return cnt\n        else:\n            cnt += 1\n            i_ += len(str1)\n    return cnt","ab7c9054":"df_clean['query_in_title'] = df_clean['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\ndf_clean['query_in_description'] = df_clean['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))","89ff0fb4":"df_clean['ratio_title'] = df_clean['word_in_title']\/df_clean['len_of_search']\ndf_clean['ratio_description'] = df_clean['word_in_description']\/df_clean['len_of_search']\ndf_clean['attr'] = df_clean['search_term']+\"\\t\"+df_clean['brand']\ndf_clean['word_in_brand'] = df_clean['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\ndf_clean['ratio_brand'] = df_clean['word_in_brand']\/df_clean['len_of_brand']","73d0ffa9":"#Load th\u01b0 vi\u1ec7n \u0111\u1ec3 so s\u00e1nh kho\u1ea3ng c\u00e1ch gi\u1eefa hai ho\u1eb7c nhi\u1ec1u chu\u1ed7i b\u1eb1ng nhi\u1ec1u thu\u1eadt to\u00e1n\n!pip install textdistance","208535b9":"import textdistance\ndf_clean['jaccard_sim_desc'] = df_clean['product_info'].map(lambda x:textdistance.jaccard(x.split('\\t')[0],x.split('\\t')[2]))\ndf_clean['jaccard_sim_title'] = df_clean['product_info'].map(lambda x:textdistance.jaccard(x.split('\\t')[0],x.split('\\t')[1]))","add1634d":"df_clean['levenshtein_sim_desc'] = df_clean['product_info'].map(lambda x:textdistance.levenshtein(x.split('\\t')[0],x.split('\\t')[2]))\ndf_clean['levenshtein_sim_title'] = df_clean['product_info'].map(lambda x:textdistance.levenshtein(x.split('\\t')[0],x.split('\\t')[1]))","1f3ea1ac":"df_clean['mra_sim_desc'] = df_clean['product_info'].map(lambda x:textdistance.mra(x.split('\\t')[0],x.split('\\t')[2]))\ndf_clean['mra_sim_title'] = df_clean['product_info'].map(lambda x:textdistance.mra(x.split('\\t')[0],x.split('\\t')[1]))","d480fd3f":"df_brand = pd.unique(df_clean.brand.ravel())\nd={}\ni = 1000\nfor s in df_brand:\n    d[s]=i\n    i+=3\ndf_clean['brand_feature'] = df_clean['brand'].map(lambda x:d[x])\ndf_clean['search_term_feature'] = df_clean['search_term'].map(lambda x:len(x))","14c8180e":"from nltk.corpus import brown\nimport gensim\nembed_model = gensim.models.Word2Vec(brown.sents())\nembed_model.save('brown.embedding')\nmodel = gensim.models.Word2Vec.load('brown.embedding')","cb870822":"def embedding_sim_cal(s, t, i):\n    _sum = 0\n    avg = 0\n    if len(s.split()) == 0 :\n        return 0\n    for s_word in s.split():\n        _max = 0\n        for t_word in t.split():\n            if ((s_word in model.wv) and (t_word in model.wv)):\n                _max = max(_max, model.wv.similarity(s_word, t_word))\n        _sum += _max\n    avg = _sum\/ len(s.split())\n    return avg","ae481b23":"df_clean['word_ebed_similarity'] = df_clean['product_info'].map(lambda x:embedding_sim_cal(x.split('\\t')[0],x.split('\\t')[2],0))\ndf_clean.to_csv('df_final.csv')","a59fb9ec":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ntfidf = TfidfVectorizer(analyzer='char_wb', ngram_range = (3,3), max_features = 1500)\ntfidf_des = tfidf.fit_transform(df_clean.product_description).toarray()\ntfidf_search = tfidf.transform(df_clean.search_term).toarray()\ntfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')","2cf84941":"def fmean_squared_error(ground_truth, predictions):\n    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n    return fmean_squared_error_\n\nRMSE  = make_scorer(fmean_squared_error, greater_is_better=False)","3ce5aa4e":"class custom_regression_vals(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n    def transform(self, hd_searches):\n        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand']\n        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n        return hd_searches\nclass custom_txt_col(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        return self\n    def transform(self, data_dict):\n        return data_dict[self.key].apply(str)","f76fced4":"df_train = df_clean.iloc[:num_train]\ndf_test = df_clean.iloc[num_train:]\nid_test = df_test['id']\ny_train = df_train['relevance'].values\nX_train = df_train[:]\nX_test = df_test[:]","9dc84b5c":"rfr = RandomForestRegressor(n_estimators = 530, n_jobs = -1, random_state = 2016, verbose = 1)","14a700c5":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()","e75f7375":"tsvd = TruncatedSVD(n_components=10, random_state = 2016)","e1ca84e4":"clf = pipeline.Pipeline([\n        ('union', FeatureUnion(\n                    transformer_list = [\n                        ('cst',  custom_regression_vals()),  \n                        ('txt1', pipeline.Pipeline([('s1', custom_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n                        ('txt2', pipeline.Pipeline([('s2', custom_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n                        ('txt3', pipeline.Pipeline([('s3', custom_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n                        ('txt4', pipeline.Pipeline([('s4', custom_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))],\n                    transformer_weights = {\n                        'cst': 1.0,\n                        'txt1': 0.5,\n                        'txt2': 0.25,\n                        'txt3': 0.01,\n                        'txt4': 0.5\n                        },\n                #n_jobs = -1\n                )), \n        ('rfr', rfr)])\nparam_grid = {'rfr__max_features': [8], 'rfr__max_depth': [18]}","d40c309d":"model = GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 2, verbose = 20, scoring=RMSE)\nresults=model.fit(X_train, y_train)","c6abca5c":"def plot_scores_single(results, param, log=False):\n    X = [x[param] for x in results.cv_results_['params']]\n    if log:\n        X = [math.log10(a) for a in X]\n    plt.plot(X, -results.cv_results_['mean_train_score'])\n    plt.plot(X, -results.cv_results_['mean_test_score'])\n    plt.title('rmse')\n    plt.ylabel('rmse')\n    plt.xlabel(param)\n    plt.legend(['train', 'tets'], loc='lower left')\n    plt.show()","776cdf2f":"# plot_scores_single(results, 'alpha', log=True)","9766108b":"pd.DataFrame({\"id\": id_test, \"relevance\": model.predict(X_test)}).to_csv('submission.csv',index=False)","e98f0aeb":"# Preprocessing data\n\u0110\u1ec3 c\u00f3 c\u00e1i nh\u00ecn t\u1ed5ng qu\u00e1t v\u1ec1 d\u1eef li\u1ec7u c\u1ee7a ch\u00fang t\u00f4i, ch\u00fang t\u00f4i s\u1ebd h\u1ee3p nh\u1ea5t t\u1eadp train v\u00e0 test. Sau \u0111\u00f3 h\u1ee3p nh\u1ea5t t\u1ea5t c\u1ea3 c\u00e1c thu\u1ed9c t\u00ednh, m\u00f4 t\u1ea3 v\u00e0 th\u01b0\u01a1ng hi\u1ec7u v\u1edbi train_df.","30c8c102":"Dimensionality reduction S\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p ph\u00e2n h\u1ee7y gi\u00e1 tr\u1ecb k\u1ef3 d\u1ecb c\u1eaft ng\u1eafn (SVD)\u0111\u1ec3 gi\u1ea3m k\u00edch th\u01b0\u1edbc","ea52ea8b":"Ta th\u1ea5y r\u1eb1ng train_df c\u00f3 nhi\u1ec1u gi\u00e1 tr\u1ecb r\u1ed7ng. Nh\u1eefng \u0111i\u1ec1u n\u00e0y x\u1ea3y ra khi h\u1ee3p nh\u1ea5t c\u00e1c thu\u1ed9c t\u00ednh, v\u00ec nhi\u1ec1u s\u1ea3n ph\u1ea9m trong t\u1eadp train kh\u00f4ng c\u00f3 thu\u1ed9c t\u00ednh.","501682b9":"# RandomforestRegesssor\n**A random forest regressor.**\nl\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt t\u1ed5ng h\u1ee3p c\u00f3 kh\u1ea3 n\u0103ng th\u1ef1c hi\u1ec7n c\u1ea3 nhi\u1ec7m v\u1ee5 h\u1ed3i quy v\u00e0 ph\u00e2n lo\u1ea1i v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng nhi\u1ec1u c\u00e2y quy\u1ebft \u0111\u1ecbnh v\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt g\u1ecdi l\u00e0 Bootstrap and Aggregation, th\u01b0\u1eddng \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 bagging \n\n![image.png](attachment:4d3c2f98-c9b4-4628-9777-da5b15f089cc.png)\n\nHyperparametters \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng:\n* n_estimators: S\u1ed1 l\u01b0\u1ee3ng c\u00e2y trong r\u1eebng.\n* n_jobs: S\u1ed1 l\u01b0\u1ee3ng c\u00f4ng vi\u1ec7c s\u1ebd ch\u1ea1y song song.\n* random_state: Ki\u1ec3m so\u00e1t c\u1ea3 t\u00ednh ng\u1eabu nhi\u00ean c\u1ee7a bootstrapping c\u1ee7a c\u00e1c m\u1eabu \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng khi x\u00e2y d\u1ef1ng c\u00e2y (n\u1ebfu bootstrap = True) v\u00e0 vi\u1ec7c l\u1ea5y m\u1eabu c\u00e1c t\u00ednh n\u0103ng c\u1ea7n xem x\u00e9t khi t\u00ecm ki\u1ebfm s\u1ef1 ph\u00e2n t\u00e1ch t\u1ed1t nh\u1ea5t t\u1ea1i m\u1ed7i n\u00fat (n\u1ebfu max_features <n_features)\n* verbose: Ki\u1ec3m so\u00e1t \u0111\u1ed9 d\u00e0i khi \u0111i\u1ec1u ch\u1ec9nh v\u00e0 d\u1ef1 \u0111o\u00e1n","6262d012":"# Numeric feature:\n\u0110\u1ebfm s\u1ed1 t\u1eeb trong c\u00e1c tr\u01b0\u1eddng xu\u1ea5t hi\u1ec7n trong c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm -> m\u1ed1i t\u01b0\u01a1ng quan gi\u1eefa c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm v\u00e0 c\u00e1c c\u1ed9t trong t\u1eadp d\u1eef li\u1ec7u","1a507d34":"# Clean data\n**Lo\u1ea1i b\u1ecf tag html**","dfb9ccde":"# Load data from csv file","9ba904f1":"# TF-IDF similarity measure feature\nTF-IDF l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d . \u0110\u00e2y l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt \u0111\u1ec3 \u0111\u1ecbnh l\u01b0\u1ee3ng c\u00e1c t\u1eeb trong m\u1ed9t b\u1ed9 t\u00e0i li\u1ec7u. Ph\u01b0\u01a1ng ph\u00e1p n\u00e0y l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng r\u1ed9ng r\u00e3i trong Truy xu\u1ea5t Th\u00f4ng tin v\u00e0 Khai th\u00e1c V\u0103n b\u1ea3n. C\u00e1c bi\u1ebfn th\u1ec3 c\u1ee7a l\u01b0\u1ee3c \u0111\u1ed3 tr\u1ecdng s\u1ed1 tf-idf th\u01b0\u1eddng \u0111\u01b0\u1ee3c c\u00e1c c\u00f4ng c\u1ee5 t\u00ecm ki\u1ebfm s\u1eed d\u1ee5ng \u0111\u1ec3 ch\u1ea5m \u0111i\u1ec3m v\u00e0 x\u1ebfp h\u1ea1ng m\u1ee9c \u0111\u1ed9 li\u00ean quan c\u1ee7a t\u00e0i li\u1ec7u khi \u0111\u01b0a ra m\u1ed9t truy v\u1ea5n. Tr\u1ecdng s\u1ed1 n\u00e0y l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o th\u1ed1ng k\u00ea \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u1ee9c \u0111\u1ed9 quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb \u0111\u1ed1i v\u1edbi m\u1ed9t t\u00e0i li\u1ec7u trong m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp ho\u1eb7c kho ng\u1eef li\u1ec7u. M\u1ee9c \u0111\u1ed9 quan tr\u1ecdng t\u0103ng t\u01b0\u01a1ng \u1ee9ng v\u1edbi s\u1ed1 l\u1ea7n m\u1ed9t t\u1eeb xu\u1ea5t hi\u1ec7n trong t\u00e0i li\u1ec7u nh\u01b0ng \u0111\u01b0\u1ee3c b\u00f9 \u0111\u1eafp b\u1edfi t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb \u0111\u00f3 trong kho ng\u1eef li\u1ec7u (t\u1eadp d\u1eef li\u1ec7u).TF-IDF c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00ednh theo c\u00f4ng th\u1ee9c sau:\n\n![image.png](attachment:35c8534a-d69f-4836-ac62-4ff5a9e8312a.png)\n\nTerm Frequency: \u0111o t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a m\u1ed9t t\u1eeb trong t\u00e0i li\u1ec7u. \u0110i\u1ec1u n\u00e0y ph\u1ee5 thu\u1ed9c nhi\u1ec1u v\u00e0o \u0111\u1ed9 d\u00e0i c\u1ee7a t\u00e0i li\u1ec7u v\u00e0 t\u00ednh t\u1ed5ng qu\u00e1t c\u1ee7a t\u1eeb\n![image.png](attachment:55fc5b5b-df38-4e2f-8893-d6ad710812a2.png)\n\nDocument Frequency: \u0111o l\u01b0\u1eddng t\u1ea7m quan tr\u1ecdng c\u1ee7a c\u00e1c t\u00e0i li\u1ec7u trong to\u00e0n b\u1ed9 t\u1eadp h\u1ee3p ng\u1eef li\u1ec7u\n![image.png](attachment:4f987061-1118-49a2-b7d6-4f580cbca711.png)","74610dd8":"**Bi\u1ec3u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ef1 t\u01b0\u01a1ng quan c\u1ee7a train.csv v\u00e0 attributes.csv**","c562c877":"# T\u1ed5ng quan v\u1ec1 d\u1eef li\u1ec7u\ntrain.csv - t\u1eadp d\u1eef li\u1ec7u d\u00f9ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n ch\u1ee9a s\u1ea3n ph\u1ea9m, c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm v\u00e0 m\u1ee9c \u0111\u1ed9 li\u00ean quan\ntest.csv - b\u1ed9 th\u1eed nghi\u1ec7m, ch\u1ee9a c\u00e1c s\u1ea3n ph\u1ea9m v\u00e0 c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\nproduct_descriptions.csv - ch\u1ee9a m\u00f4 t\u1ea3 d\u1ea1ng text c\u1ee7a t\u1eebng s\u1ea3n ph\u1ea9m\nattributes.csv - thu\u1ed9c t\u00ednh c\u1ee7a s\u1ea3n ph\u1ea9m, cung c\u1ea5p th\u00f4ng tin m\u1edf r\u1ed9ng v\u1ec1 m\u1ed9t t\u1eadp h\u1ee3p con c\u1ee7a c\u00e1c s\u1ea3n ph\u1ea9m (th\u01b0\u1eddng \u0111\u1ea1i di\u1ec7n cho c\u00e1c th\u00f4ng s\u1ed1 k\u1ef9 thu\u1eadt chi ti\u1ebft). Kh\u00f4ng ph\u1ea3i m\u1ecdi s\u1ea3n ph\u1ea9m s\u1ebd \u0111\u1ec1u c\u00f3 thu\u1ed9c t\u00ednh.\nsample_submission.csv - m\u1ed9t t\u1ec7p hi\u1ec3n th\u1ecb \u0111\u00fang \u0111\u1ecbnh d\u1ea1ng g\u1eedi\nrelevance_instructions.docx - c\u00e1c h\u01b0\u1edbng d\u1eabn \u0111\u01b0\u1ee3c cung c\u1ea5p cho ng\u01b0\u1eddi \u0111\u00e1nh gi\u00e1\n# C\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u\n* id - tr\u01b0\u1eddng Id duy nh\u1ea5t \u0111\u1ea1i di\u1ec7n cho m\u1ed9t c\u1eb7p (search_term, product_uid) \n* product_uid - id cho c\u00e1c s\u1ea3n ph\u1ea9m\n* product_title - ti\u00eau \u0111\u1ec1 s\u1ea3n ph\u1ea9m\n* product_description - m\u00f4 t\u1ea3 text c\u1ee7a s\u1ea3n ph\u1ea9m (c\u00f3 th\u1ec3 ch\u1ee9a n\u1ed9i dung HTML)\n* search_term - c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm\n* relevane - m\u1ee9c trung b\u00ecnh c\u1ee7a x\u1ebfp h\u1ea1ng m\u1ee9c \u0111\u1ed9 li\u00ean quan cho m\u1ed9t id nh\u1ea5t \u0111\u1ecbnh\n* name - t\u00ean thu\u1ed9c t\u00ednh\n* value - gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh","7c71cac1":"# Levenshtein distance\nKho\u1ea3ng c\u00e1ch Levenshtein th\u1ec3 hi\u1ec7n kho\u1ea3ng c\u00e1ch kh\u00e1c bi\u1ec7t gi\u1eefa 2 chu\u1ed7i k\u00fd t\u1ef1, l\u00e0 s\u1ed1 b\u01b0\u1edbc \u00edt nh\u1ea5t bi\u1ebfn chu\u1ed7i th\u1ee9 nh\u1ea5t th\u00e0nh chu\u1ed7i th\u1ee9 2 th\u00f4ng qua 3 ph\u00e9p bi\u1ebfn \u0111\u1ed5i l\u00e0:\n* xo\u00e1 1 k\u00fd t\u1ef1.\n* th\u00eam 1 k\u00fd t\u1ef1.\n* thay k\u00fd t\u1ef1 n\u00e0y b\u1eb1ng k\u00fd t\u1ef1 kh\u00e1c.\n\nKho\u1ea3ng c\u00e1ch Levenshtein b\u1eb1ng 0 c\u00f3 ngh\u0129a l\u00e0: c\u1ea3 hai chu\u1ed7i \u0111\u1ec1u b\u1eb1ng nhau","fbfd8d00":"**D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c clean cu\u1ed1i c\u00f9ng \u0111\u01b0\u1ee3c l\u01b0u v\u00e0o df_clean**","5efe9977":"s\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 trong c\u00e1c c\u1ed9t: s\u1eed d\u1ee5ng file spell-check thu \u0111\u01b0\u1ee3c khi ph\u00e2n t\u00edch d\u1eef li\u1ec7u g\u1ed3m c\u00e1c t\u1eeb sai ch\u00ednh t\u1ea3 v\u00e0 c\u00e1c t\u1eeb \u0111\u00fang t\u01b0\u01a1ng \u1ee9ng\nXo\u00e1 c\u00e1c t\u1eeb mang \u00edt th\u00f4ng tin vd: 'for',...\nXo\u00e1 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\nChu\u1ea9n ho\u00e1 \u0111\u01a1n v\u1ecb","5730be75":"# Attribute data\n* G\u1ed3m 3 c\u1ed9t(product_uid, name, value) v\u00e0 2.044.803 h\u00e0ng\n* ch\u1ee9a m\u1ed9t s\u1ed1 th\u00f4ng tin b\u1ed5 sung v\u1ec1 m\u1ed9t t\u1eadp h\u1ee3p con c\u00e1c s\u1ea3n ph\u1ea9m.\n* T\u1eadp thu\u1ed9c t\u00ednh ch\u1ee9a 2594 gi\u00e1 tr\u1ecb null.\n* M\u1ed7i s\u1ea3n ph\u1ea9m trong t\u1ec7p thu\u1ed9c t\u00ednh.csv c\u00f3 nhi\u1ec1u thu\u1ed9c t\u00ednh. S\u1ed1 l\u01b0\u1ee3ng thu\u1ed9c t\u00ednh t\u1ed1i thi\u1ec3u cho m\u1ed9t s\u1ea3n ph\u1ea9m l\u00e0 5 v\u00e0 t\u1ed1i \u0111a l\u00e0 88. Nh\u01b0ng kh\u00f4ng ph\u1ea3i t\u1ea5t c\u1ea3 c\u00e1c s\u1ea3n ph\u1ea9m trong t\u1ec7p train.csv \u0111\u1ec1u \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong t\u1ec7p thu\u1ed9c t\u00ednh.csv. T\u1ed5ng s\u1ed1 16263 tr\u00ean t\u1ed5ng s\u1ed1 54667 s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 thu\u1ed9c t\u00ednh\n* name (th\u01b0\u01a1ng hi\u1ec7u) l\u00e0 thu\u1ed9c t\u00ednh ph\u1ed5 bi\u1ebfn nh\u1ea5t \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong 86.250 tr\u01b0\u1eddng h\u1ee3p trong t\u1ec7p attributes.csv","2cd10a0c":"# Modeling\n**Loss function: RMSE**\n\nL\u00e0 h\u00e0m l\u1ed7i b\u00ecnh ph\u01b0\u01a1ng trung b\u00ecnh g\u1ed1c. N\u00f3 \u0111o \u0111\u1ed9 l\u1edbn trung b\u00ecnh c\u1ee7a c\u00e1c sai s\u1ed1 v\u00e0 quan t\u00e2m \u0111\u1ebfn \u0111\u1ed9 l\u1ec7ch so v\u1edbi gi\u00e1 tr\u1ecb th\u1ef1c t\u1ebf\nC\u00f4ng th\u1ee9c:\n![image.png](attachment:5d4cb905-2d62-4cf8-bf7e-9376b0d11fb1.png)\nGi\u00e1 tr\u1ecb RMSE b\u1eb1ng 0 ch\u1ec9 ra r\u1eb1ng m\u00f4 h\u00ecnh c\u00f3 s\u1ef1 ph\u00f9 h\u1ee3p ho\u00e0n h\u1ea3o. RMSE c\u00e0ng th\u1ea5p, m\u00f4 h\u00ecnh c\u00e0ng t\u1ed1t v\u00e0 c\u00e1c d\u1ef1 \u0111o\u00e1n c\u1ee7a n\u00f3. RMSE cao h\u01a1n ch\u1ec9 ra r\u1eb1ng c\u00f3 m\u1ed9t \u0111\u1ed9 l\u1ec7ch l\u1edbn t\u1eeb gi\u00e1 tr\u1ecb c\u00f2n l\u1ea1i \u0111\u1ebfn ch\u00e2n l\u00fd c\u01a1 b\u1ea3n. RMSE c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng v\u1edbi c\u00e1c t\u00ednh n\u0103ng kh\u00e1c nhau v\u00ec n\u00f3 gi\u00fap t\u00ecm ra li\u1ec7u t\u00ednh n\u0103ng c\u00f3 c\u1ea3i thi\u1ec7n d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh hay kh\u00f4ng","236bbd90":"**Bi\u1ec3u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ef1 ph\u00e2n b\u1ed5 c\u1ee7a relevance trong t\u1eadp train**\n\n* S\u1ed1 l\u01b0\u1ee3ng s\u1ea3n ph\u1ea9m \u0111\u1ea1t m\u1ee9c t\u01b0\u01a1ng quan cao v\u1edbi t\u1eeb t\u00ecm ki\u1ebfm (relevance=3) chi\u1ebfm s\u1ed1 l\u01b0\u1ee3ng l\u1edbn nh\u1ea5t\n* Relevance = {1,25, 1,5, 1,75, 2,25, 2,5, 2,75} x\u1ea3y ra v\u1edbi t\u1ea7n su\u1ea5t r\u1ea5t th\u1ea5p.","f9e70916":"**Bi\u1ec3u \u0111\u1ed3 Venn th\u1ec3 hi\u1ec7n \u0111\u1ed9 t\u01b0\u01a1ng quan c\u1ee7a s\u1ea3n ph\u1ea9m trong t\u1eadp train v\u00e0 test**","c918364b":"**T\u00ednh to\u00e1n t\u1ef7 l\u1ec7**","924d3909":"# Import Libraries","d23192d8":"**Ki\u1ec3m tra xem product_uids c\u00f3 b\u1ea5t k\u1ef3 m\u1ed1i t\u01b0\u01a1ng quan n\u00e0o v\u1edbi \u0111i\u1ec3m m\u1ee9c \u0111\u1ed9 li\u00ean quan hay kh\u00f4ng**","4d50448f":"# Train Data\n* G\u1ed3m c\u00e1c c\u1ed9t: product_uid, product_title, search_term, relevance v\u00e0 74067 h\u00e0ng ch\u1ee9a th\u00f4ng tin c\u00e1c tr\u01b0\u1eddng c\u1ee7a s\u1ea3n ph\u1ea9m t\u01b0\u01a1ng \u1ee9ng\n* Product_uid l\u00e0 s\u1ed1 nh\u1eadn d\u1ea1ng duy nh\u1ea5t cho m\u1ed7i s\u1ea3n ph\u1ea9m.\n* T\u1eadp train kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb Null\n* \u0110\u1ed1i v\u1edbi m\u1ed7i s\u1ea3n ph\u1ea9m, th\u01b0\u1eddng c\u00f3 nhi\u1ec1u h\u00e0ng t\u1ee9c l\u00e0 nhi\u1ec1u truy v\u1ea5n t\u00ecm ki\u1ebfm v\u1edbi \u0111i\u1ec3m m\u1ee9c \u0111\u1ed9 li\u00ean quan c\u1ee7a ch\u00fang.\n* T\u1eadp train g\u1ed3m:\n  * 54667 product_uid\n  * 11795 search_term\n  * 94731 product_title\n* Nhi\u1ec1u s\u1ea3n ph\u1ea9m xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u1ea7n trong t\u1eadp train","94a3e19d":"# Test Data\n* Ch\u1ee9a c\u00e1c c\u1ed9t: product_uid, product_title, search_term v\u00e0 166693 h\u00e0ng\n* T\u1eadp test kh\u00f4ng ch\u1ee9a d\u1eef li\u1ec7u Null\n* G\u1ea7n 1 n\u1eeda s\u1ed1 s\u1ea3n ph\u1ea9m c\u1ee7a t\u1eadp train n\u1eb1m trong t\u1eadp test (27699 s\u1ea3n ph\u1ea9m)\n* T\u1eadp test g\u1ed3m:\n  * 97460 product_uid\n  * 94731 products_title\n  * 22427 search_term","f786cdac":"# Feature engineering\n* Feature Engineerning l\u00e0 m\u1ed9t giai \u0111o\u1ea1n kh\u00f4ng th\u1ec3 thi\u1ebfu trong qu\u00e1 tr\u00ecnh ph\u00e1t tri\u1ec3n b\u1ea5t k\u1ef3 m\u1ed9t h\u1ec7 th\u1ed1ng th\u00f4ng minh n\u00e0o\n* Feature engineering l\u00e0 qu\u00e1 tr\u00ecnh chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u th\u00f4 th\u00e0nh c\u00e1c t\u00ednh n\u0103ng \u0111\u00f3ng vai tr\u00f2 l\u00e0 \u0111\u1ea7u v\u00e0o cho c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y, d\u1eabn \u0111\u1ebfn c\u1ea3i thi\u1ec7n \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh tr\u00ean d\u1eef li\u1ec7u kh\u00f4ng nh\u00ecn th\u1ea5y.\n* C\u00e1c t\u00ednh n\u0103ng trong d\u1eef li\u1ec7u s\u1ebd \u1ea3nh h\u01b0\u1edfng tr\u1ef1c ti\u1ebfp \u0111\u1ebfn c\u00e1c m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n s\u1eed d\u1ee5ng v\u00e0 k\u1ebft qu\u1ea3 b\u1ea1n \u0111\u1ea1t \u0111\u01b0\u1ee3c.Th\u01b0\u1eddng l\u00e0 m\u1ed9t \u0111\u1ea1i di\u1ec7n c\u1ee5 th\u1ec3 tr\u00ean d\u00f2ng \u0111\u1ea7u ti\u00ean c\u1ee7a d\u1eef li\u1ec7u th\u00f4, l\u00e0 m\u1ed9t thu\u1ed9c t\u00ednh ri\u00eang l\u1ebb, c\u00f3 th\u1ec3 \u0111o l\u01b0\u1eddng v\u00e0 m\u00f4 t\u1ea3 b\u1edfi m\u1ed9t c\u1ed9t trong t\u1eadp d\u1eef li\u1ec7u.\n* C\u00e1c t\u00ednh n\u0103ng m\u00e0 b\u1ea1n chu\u1ea9n b\u1ecb v\u00e0 l\u1ef1a ch\u1ecdn c\u00e0ng t\u1ed1t, b\u1ea1n s\u1ebd \u0111\u1ea1t \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n. Tuy nhi\u00ean, k\u1ebft qu\u1ea3 \u0111\u1ea1t \u0111\u01b0\u1ee3c ph\u1ee5 thu\u1ed9c v\u00e0o m\u00f4 h\u00ecnh ta ch\u1ecdn, d\u1eef li\u1ec7u c\u00f3 s\u1eb5n v\u00e0 c\u00e1c t\u00ednh n\u0103ng ta chu\u1ea9n b\u1ecb. Vi\u1ec7c b\u1ea1n \u0111\u1eb7t v\u1ea5n \u0111\u1ec1 v\u00e0 c\u00e1c bi\u1ec7n ph\u00e1p kh\u00e1ch quan m\u00e0 ta \u0111ang s\u1eed d\u1ee5ng \u0111\u1ec3 \u01b0\u1edbc t\u00ednh \u0111\u1ed9 ch\u00ednh x\u00e1c c\u0169ng \u0111\u00f3ng vai tr\u00f2 quan tr\u1ecdng. K\u1ebft qu\u1ea3 ph\u1ee5 thu\u1ed9c v\u00e0o nhi\u1ec1u thu\u1ed9c t\u00ednh ph\u1ee5 thu\u1ed9c l\u1eabn nhau.\n\n**T\u1ea1o tr\u01b0\u1eddng m\u1edbi**\n\n* \u0110\u1ec3 l\u00e0m phong ph\u00fa th\u00eam th\u00f4ng tin ta c\u00f3 th\u1ec3 x\u00e2y d\u1ef1ng c\u00e1c t\u00ednh n\u0103ng m\u1edbi t\u1eeb d\u1eef li\u1ec7u th\u00f4. N\u1ed1i search_term, product_title v\u1edbi product_description \u0111\u01b0\u1ee3c ph\u00e2n t\u00e1ch b\u1eb1ng tab t\u1ea1o th\u00e0nh tr\u01b0\u1eddng d\u1eef li\u1ec7u m\u1edbi product_info","02950c41":"* \u0110\u1ebfm s\u1ed1 t\u1eeb trong search_term\n* \u0110\u1ebfm s\u1ed1 t\u1eeb trong product_title\n* \u0110\u1ebfm s\u1ed1 t\u1eeb trong m\u00f4 t\u1ea3 s\u1ea3n ph\u1ea9m\n* \u0110\u1ebfm s\u1ed1 t\u1eeb trong th\u01b0\u01a1ng hi\u1ec7u.\n\nT\u1eeb \u0111\u00f3, ta c\u00f3 th\u00eam 4 c\u1ed9t d\u1eef li\u1ec7u m\u1edbi \u0111\u01b0\u1ee3c t\u1ea1o ra t\u1eeb c\u00e1c c\u1ed9t ban \u0111\u1ea7u v\u00e0 \u0111\u1ec1u c\u00f3 \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn relevance","30431215":"# Word embedding feature\nWord Embedding l\u00e0 t\u00ean g\u1ecdi chung c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh ng\u00f4n ng\u1eef v\u00e0 c\u00e1c ph\u01b0\u01a1ng ph\u00e1p h\u1ecdc theo \u0111\u1eb7c tr\u01b0ng trong X\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean(NLP), \u1edf \u0111\u00f3 c\u00e1c t\u1eeb ho\u1eb7c c\u1ee5m t\u1eeb \u0111\u01b0\u1ee3c \u00e1nh x\u1ea1 sang c\u00e1c vector s\u1ed1 (th\u01b0\u1eddng l\u00e0 s\u1ed1 th\u1ef1c) Word2vec l\u00e0 m\u1ed9t trong nh\u1eefng k\u1ef9 thu\u1eadt ph\u1ed5 bi\u1ebfn nh\u1ea5t \u0111\u1ec3 h\u1ecdc c\u00e1ch nh\u00fang t\u1eeb b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng m\u1ea1ng n\u01a1-ron hai l\u1edbp. \u0110\u1ea7u v\u00e0o c\u1ee7a n\u00f3 l\u00e0 m\u1ed9t kho v\u0103n b\u1ea3n v\u00e0 \u0111\u1ea7u ra c\u1ee7a n\u00f3 l\u00e0 m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c vect\u01a1. Nh\u00fang t\u1eeb th\u00f4ng qua word2vec c\u00f3 th\u1ec3 l\u00e0m cho m\u00e1y t\u00ednh c\u00f3 th\u1ec3 \u0111\u1ecdc \u0111\u01b0\u1ee3c ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean, sau \u0111\u00f3 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng th\u00eam vi\u1ec7c tri\u1ec3n khai c\u00e1c ph\u00e9p to\u00e1n tr\u00ean c\u00e1c t\u1eeb \u0111\u1ec3 ph\u00e1t hi\u1ec7n c\u00e1c \u0111i\u1ec3m t\u01b0\u01a1ng \u0111\u1ed3ng c\u1ee7a ch\u00fang. M\u1ed9t t\u1eadp h\u1ee3p c\u00e1c vect\u01a1 t\u1eeb \u0111\u01b0\u1ee3c \u0111\u00e0o t\u1ea1o t\u1ed1t s\u1ebd \u0111\u1eb7t c\u00e1c t\u1eeb t\u01b0\u01a1ng t\u1ef1 g\u1ea7n nhau trong kh\u00f4ng gian \u0111\u00f3 \n\n![image.png](attachment:1ff29846-16ab-40ab-b030-54ec3f8a76f8.png)\n\nC\u00f3 2 thu\u1eadt to\u00e1n hu\u1ea5n luy\u1ec7n ch\u00ednh cho word2vec l\u00e0 CBOW v\u00e0 Skip-gram Th\u01b0 vi\u1ec7n Gensim Python Gensim l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n python m\u00e3 ngu\u1ed3n m\u1edf \u0111\u1ec3 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. Th\u01b0 vi\u1ec7n Gensim s\u1ebd cho ph\u00e9p ph\u00e1t tri\u1ec3n Word embedding feature b\u1eb1ng c\u00e1ch \u0111\u00e0o t\u1ea1o c\u00e1c m\u00f4 h\u00ecnh word2vec c\u1ee7a ri\u00eang ch\u00fang ta tr\u00ean kho d\u1eef li\u1ec7u t\u00f9y ch\u1ec9nh v\u1edbi CBOW c\u1ee7a thu\u1eadt to\u00e1n skip-grams.","1872ce9d":"**Bi\u1ec3u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ef1 t\u01b0\u01a1ng quan gi\u1eefa t\u1eadp train v\u00e0 desc c\u1ee7a s\u1ea3n ph\u1ea9m**","8fc2d98f":"# GridSearchCV\nL\u00e0 m\u1ed9t h\u00e0m th\u01b0 vi\u1ec7n l\u00e0 m\u1ed9t th\u00e0nh vi\u00ean c\u1ee7a g\u00f3i model_selection c\u1ee7a sklearn. N\u00f3 gi\u00fap l\u1eb7p l\u1ea1i c\u00e1c si\u00eau tham s\u1ed1 \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh tr\u01b0\u1edbc v\u00e0 ph\u00f9 h\u1ee3p v\u1edbi c\u00f4ng c\u1ee5 \u01b0\u1edbc t\u00ednh (m\u00f4 h\u00ecnh) c\u1ee7a b\u1ea1n tr\u00ean train. V\u00ec v\u1eady, cu\u1ed1i c\u00f9ng, b\u1ea1n c\u00f3 th\u1ec3 ch\u1ecdn c\u00e1c tham s\u1ed1 t\u1ed1t nh\u1ea5t t\u1eeb c\u00e1c si\u00eau tham s\u1ed1 \u0111\u01b0\u1ee3c li\u1ec7t k\u00ea. Hyperparametters used:\n\n* estimator: \u0111\u1ed1i t\u01b0\u1ee3ng \u01b0\u1edbc t\u00ednh \u0111ang \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng\n* param_grid: t\u1eeb \u0111i\u1ec3n ch\u1ee9a t\u1ea5t c\u1ea3 c\u00e1c tham s\u1ed1 \u0111\u1ec3 th\u1eed\n* scoring: s\u1ed1 li\u1ec7u \u0111\u00e1nh gi\u00e1 \u0111\u1ec3 s\u1eed d\u1ee5ng khi x\u1ebfp h\u1ea1ng k\u1ebft qu\u1ea3\n* cv: x\u00e1c nh\u1eadn ch\u00e9o, s\u1ed1 l\u1ea7n g\u1ea5p cv cho m\u1ed7i t\u1ed5 h\u1ee3p c\u00e1c tham s\u1ed1","17aac7ba":"**D\u1eef li\u1ec7u sau khi \u0111i\u1ec1n gi\u00e1 tr\u1ecb r\u1ed7ng**","3a2b4e84":"# Description data\n* G\u1ed3m 2 c\u1ed9t(product_id v\u00e0 product_description) v\u00e0 124428 h\u00e0ng\n* product_description ch\u1ee9a m\u00f4 t\u1ea3 v\u0103n b\u1ea3n c\u1ee7a t\u1eebng s\u1ea3n ph\u1ea9m bao g\u1ed3m s\u1ed1 v\u00e0 c\u00e1c k\u00fd t\u1ef1, h\u1eefu \u00edch trong vi\u1ec7c t\u00ecm ki\u1ebfm s\u1ea3n ph\u1ea9m\n* T\u1eadp description kh\u00f4ng ch\u1ee9a gi\u00e1 tr\u1ecb Null\n* T\u1eadp description c\u00f3:\n  * 124428 product_uid\n  * 110128 product_description","12b55512":"# T\u1ed5ng quan cu\u1ed9c thi\nCu\u1ed9c thi n\u00e0y \u0111\u01b0\u1ee3c t\u1ed5 ch\u1ee9c b\u1edfi Home Depot tr\u00ean Kaggle.com. Home Depot mu\u1ed1n x\u00e2y d\u1ef1ng m\u1ed9t \u0111\u1ec1 xu\u1ea5t t\u00ecm ki\u1ebfm linh ho\u1ea1t h\u01a1n \u0111\u1ec3 h\u1ed7 tr\u1ee3 ho\u1ea1t \u0111\u1ed9ng b\u00e1n h\u00e0ng tr\u1ef1c tuy\u1ebfn c\u1ee7a m\u00ecnh. V\u1edbi vi\u1ec7c \u0111\u1ed1i s\u00e1nh c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm c\u1ee7a kh\u00e1ch h\u00e0ng v\u1edbi thu\u1ed9c t\u00ednh \/ m\u00f4 t\u1ea3 c\u1ee7a s\u1ea3n ph\u1ea9m, kh\u00e1ch h\u00e0ng s\u1ebd d\u1ec5 d\u00e0ng t\u00ecm th\u1ea5y \u0111\u00fang s\u1ea3n ph\u1ea9m h\u1ecd mu\u1ed1n h\u01a1n. Do \u0111\u00f3, qu\u00e1 tr\u00ecnh l\u1ef1a ch\u1ecdn \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n v\u00e0 kh\u00e1ch h\u00e0ng s\u1ebd t\u00ecm \u0111\u01b0\u1ee3c nh\u1eefng s\u1ea3n ph\u1ea9m ph\u00f9 h\u1ee3p\n\n# Input:\nG\u1ed3m 4 t\u1eadp d\u1eef li\u1ec7u csv (train, test, attributes, product_descriptions) ch\u1ee9a s\u1ea3n ph\u1ea9m v\u00e0 c\u1ee5m t\u1eeb t\u00ecm ki\u1ebfm th\u1ef1c t\u1ebf c\u1ee7a kh\u00e1ch h\u00e0ng t\u1eeb trang web c\u1ee7a Home Depot c\u00f9ng v\u1edbi m\u1ee9c \u0111\u1ed9 li\u00ean quan (relevance) \u0111\u00e3 \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 b\u1edfi \u00edt nh\u1ea5t ba chuy\u00ean gia \u0111\u00e1nh gi\u00e1\n\nRelevance l\u00e0 m\u1ed9t s\u1ed1 th\u1ef1c t\u1eeb 1 \u0111\u1ebfn 3 (1 cho kh\u00f4ng li\u00ean quan v\u00e0 3 cho k\u1ebft h\u1ee3p ho\u00e0n h\u1ea3o)\n\n# M\u1ee5c ti\u00eau:\nT\u1eeb c\u00e1c t\u1eadp d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cung c\u1ea5p, x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n m\u1ee9c \u0111\u1ed9 ph\u00f9 h\u1ee3p (relevance) cho t\u1eebng c\u1eb7p \u0111\u01b0\u1ee3c li\u1ec7t k\u00ea trong b\u1ed9 d\u1eef li\u1ec7u test.csv\n\n# Output:\nFile csv ch\u1ee9a id s\u1ea3n ph\u1ea9m v\u00e0 m\u1ee9c \u0111\u1ed9 li\u00ean quan (relevance)","77c75656":"# \u0110i\u1ec1n gi\u00e1 tr\u1ecb r\u1ed7ng\n* C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng c\u00e1c gi\u00e1 tr\u1ecb null ch\u1ec9 x\u1ea3y ra trong tr\u01b0\u1eddng 'brand'.\n* Th\u01b0\u01a1ng hi\u1ec7u c\u1ee7a m\u1ed9t s\u1ea3n ph\u1ea9m th\u01b0\u1eddng hi\u1ec7n di\u1ec7n trong m\u1ed9t v\u00e0i t\u1eeb \u0111\u1ea7u ti\u00ean c\u1ee7a ti\u00eau \u0111\u1ec1.\n* 99% th\u01b0\u01a1ng hi\u1ec7u c\u00f3 \u0111\u1ed9 d\u00e0i t\u1eeb b\u1ed1n t\u1eeb tr\u1edf xu\u1ed1ng. V\u00ec v\u1eady, ch\u1ee7 y\u1ebfu, th\u01b0\u01a1ng hi\u1ec7u n\u00ean hi\u1ec7n di\u1ec7n trong 4 t\u1eeb \u0111\u1ea7u ti\u00ean c\u1ee7a ti\u00eau \u0111\u1ec1.\n* \u0110\u1ea7u ti\u00ean,l\u01b0u tr\u1eef t\u1ea5t c\u1ea3 c\u00e1c th\u01b0\u01a1ng hi\u1ec7u duy nh\u1ea5t trong m\u1ed9t danh s\u00e1ch \u201cunique_brands\u201d.\n* B\u00e2y gi\u1edd, \u0111\u1ed1i v\u1edbi b\u1ea5t k\u1ef3 h\u00e0ng n\u00e0o c\u00f3 nh\u00e3n hi\u1ec7u r\u1ed7ng, ki\u1ec3m tra xem b\u1ed1n t\u1eeb \u0111\u1ea7u ti\u00ean c\u1ee7a ti\u00eau \u0111\u1ec1 s\u1ea3n ph\u1ea9m \u0111\u00f3 c\u00f3 xu\u1ea5t hi\u1ec7n trong danh s\u00e1ch unique_brands hay kh\u00f4ng. N\u1ebfu kh\u00f4ng, sau \u0111\u00f3 ki\u1ec3m tra ba t\u1eeb \u0111\u1ea7u ti\u00ean v\u00e0 c\u1ee9 ti\u1ebfp t\u1ee5c nh\u01b0 v\u1eady cho \u0111\u1ebfn t\u1eeb \u0111\u1ea7u ti\u00ean.","495b9051":"# String similarity feature\n**Gi\u1ea3i thu\u1eadt Jaccard similiraty:**\n\nGi\u1ea3i thu\u1eadt Ch\u1ec9 m\u1ee5c Jaccard, hay c\u00f2n g\u1ecdi l\u00e0 gi\u1ea3i thu\u1eadt t\u1ec9 l\u1ec7 gi\u1eefa ph\u1ea7n giao v\u00e0 ph\u1ea7n h\u1ee3p c\u1ee7a 2 t\u1eadp h\u1ee3p, l\u00e0 1 gi\u1ea3i thu\u1eadt \u0111\u01b0\u1ee3c \u0111\u01b0a ra b\u1edfi nh\u00e0 To\u00e1n h\u1ecdc Ph\u00e1p Paul Jaccard. C\u00f4ng th\u1ee9c:\n\n![image.png](attachment:bbfa30cf-b9e9-4b4f-a0ab-5cf3114c0b9a.png)\n\n\u0110\u00e2y l\u00e0 th\u01b0\u1edbc \u0111o m\u1ee9c \u0111\u1ed9 gi\u1ed1ng nhau c\u1ee7a hai t\u1eadp d\u1eef li\u1ec7u, v\u1edbi ph\u1ea1m vi t\u1eeb 0% \u0111\u1ebfn 100%. T\u1ef7 l\u1ec7 ph\u1ea7n tr\u0103m c\u00e0ng cao th\u00ec hai t\u1eadp c\u00e0ng gi\u1ed1ng nhau"}}