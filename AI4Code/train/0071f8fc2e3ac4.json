{"cell_type":{"8a3922d1":"code","5501e630":"code","1a16c006":"code","b6b0b860":"code","c97a6971":"code","5b4d83c3":"code","9ff0d786":"code","104fb021":"code","3a52f2d9":"code","db039fea":"code","1c8dc3bb":"code","6ae9c0a8":"code","88351fa4":"code","7ef48be6":"code","9d17436d":"code","3bd908a7":"code","88af2356":"code","e4a64d62":"code","3652027a":"code","42cbf7d9":"code","b3c62a56":"code","dd9eebcd":"code","833085ec":"code","43cc200c":"code","a6955a13":"code","6b8e52c4":"code","594112ba":"code","b9f47b2c":"code","210cbd88":"code","619fc54d":"code","2c87840c":"code","0c31c186":"code","37038f82":"code","ed532914":"code","c3a03975":"code","0557cde3":"code","4764904b":"code","1a4c1e7d":"code","2ddd0e4b":"code","8316a440":"code","a5eb615a":"code","7e2d0e93":"code","d83f0aac":"code","f01b9380":"code","55fb2e46":"code","e5132852":"markdown","6d84ebf8":"markdown","c1ac4199":"markdown","5887d2f2":"markdown","d38f5995":"markdown","7bcda9e5":"markdown","5de1d860":"markdown","0dd4df20":"markdown","8244bfc2":"markdown","f40107bc":"markdown","0ad106dc":"markdown","0aa4260e":"markdown","f469d06a":"markdown","f163f5ab":"markdown","3d464d18":"markdown","c56fe6da":"markdown","02268af3":"markdown","bf8f0675":"markdown","f9a56242":"markdown","b650238f":"markdown","3e11295c":"markdown","57d4f07d":"markdown","e1ec4b02":"markdown","ce341366":"markdown"},"source":{"8a3922d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport datetime\n%matplotlib inline\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5501e630":"ETF_dict = []\nStocks_dict = []\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# Modifying a bit....\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    if dirname == '\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/ETFs': # Access ETF data\n        for filename in filenames:\n            ETF_dict.append(filename) # Add to ETF list \n    elif dirname == '\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks': # Access Stocks data\n        for filename in filenames:\n            Stocks_dict.append(filename) # Add to Stocks list ","1a16c006":"print(\"Number of stock types: \",len(Stocks_dict)) # of company in Stocks List\nprint(\"Number of ETF types: \",len(ETF_dict)) # of company in ETF List\nStocks_dict = [items.split(\".\")[0] for items in Stocks_dict] # Filter out for the \"us\" & \"txt\" part from the file names\n\ncheck = [col for col in Stocks_dict if 'tsla' in Stocks_dict] # this should work, but it is not working. check.\nStocks_dict.sort()\nETF_dict.sort()","b6b0b860":"# Pandas read csv, data import\ndf = pd.read_csv('\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/tsla.us.txt')","c97a6971":"# Check the first 5 observations, or pick random\n# df.head()\n# df.sample(n=5) # Easier way, faster, embeded in pandas method\nrand = np.random.randint(len(df.index),size=5) \ndf.loc[rand]","5b4d83c3":"df.set_index('Date',inplace=True)\nflatlined = df.where(df.diff(1)!=0.0,np.nan)\nNAcheck = flatlined.isna().sum()\nNullcheck = flatlined.isnull().sum()\n# pandas.where --> replaces value where condition is false.\n# pandas.diff --> takes a difference of the element in the list or dataframe with an row before\n# flatlined = flatlined.drop(columns = ['OpenInt'])\nError = flatlined[flatlined.isna().any(axis=1)]\nError_check = pd.DataFrame({'NaN':NAcheck.values,'Null':Nullcheck.values},index=NAcheck.index)","9ff0d786":"Error_check.transpose()\nError","104fb021":"df.drop(columns=['OpenInt'],inplace=True)","3a52f2d9":"df.reset_index(inplace=True)\ndf.Date = pd.to_datetime(df['Date'])\ndf.dtypes","db039fea":"fig, ax = plt.subplots(figsize=(20,10))\nax2 = ax.twinx()\nsns.lineplot(ax = ax, x='Date',y='Open',data=df)\n# sns.lineplot(ax = ax, x='Date',y='Close',data=df)\nsns.lineplot(ax = ax2, x='Date',y='Volume',data=df,color='red')\nax.set_xlabel('Year',fontweight='bold',fontsize=20)\nax.set_ylabel('Open Price',fontweight='bold',fontsize=20,color='blue')\nax2.set_ylabel('Volume',fontweight='bold',fontsize=20,color='red')","1c8dc3bb":"df['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\ndf['Week'] = df['Date'].dt.isocalendar().week\ndf['Dayofweek'] = df['Date'].dt.weekday","6ae9c0a8":"df.sample(n=10)\ndf_year = df.groupby(by=\"Year\").mean()\ndf_month = df.groupby(by=\"Month\").mean()\ndf_week = df.groupby(by=\"Week\").mean()\ndf_day = df.groupby(by=\"Day\").mean()\ndf_dayofweek = df.groupby(by=\"Dayofweek\").mean()","88351fa4":"fig, ax = plt.subplots(1,4,figsize=(30,8))\nsns.boxplot(ax = ax[0],x = df['Year'], y = df['Close'],data = df)\nsns.boxplot(ax = ax[1],x = df['Month'], y = df['Close'],data = df)\nsns.boxplot(ax = ax[2],x = df['Day'], y = df['Close'],data = df)\nsns.boxplot(ax = ax[3],x = df['Week'], y = df['Close'],data = df)","7ef48be6":"fig, ax = plt.subplots(1,4,figsize=(30,8))\nsns.boxplot(ax = ax[0],x = df['Year'], y = df['Volume'],data = df)\nax[0].set_ylim([0, 1.5*10**7])\nsns.boxplot(ax = ax[1],x = df['Month'], y = df['Volume'],data = df)\nax[1].set_ylim([0, 1.5*10**7])\nsns.boxplot(ax = ax[2],x = df['Day'], y = df['Volume'],data = df)\nax[2].set_ylim([0, 1.5*10**7])\nsns.boxplot(ax = ax[3],x = df['Week'], y = df['Volume'],data = df)\nax[3].set_ylim([0, 1.5*10**7])","9d17436d":"fig, ax = plt.subplots(2,2,figsize=(12,12))\nsns.regplot(ax = ax[0,0],x='Volume',y='Open',data=df_day)\nsns.regplot(ax = ax[0,0],x='Volume',y='Close',data=df_day)\nax[0,0].set_title('Day')\nsns.regplot(ax = ax[0,1],x='Volume',y='Open',data=df_week)\nsns.regplot(ax = ax[0,1],x='Volume',y='Close',data=df_week)\nax[0,1].set_title('Week')\nsns.regplot(ax = ax[1,0],x='Volume',y='Open',data=df_month)\nsns.regplot(ax = ax[1,0],x='Volume',y='Close',data=df_month)\nax[1,0].set_title('Month')\nsns.regplot(ax = ax[1,1],x='Volume',y='Open',data=df_year)\nsns.regplot(ax = ax[1,1],x='Volume',y='Close',data=df_year)\nax[1,1].set_title('Year')","3bd908a7":"fig, ax = plt.subplots(2,2,figsize=(12,12))\nsns.regplot(ax = ax[0,0],x='Open',y='Low',data=df_day)\n# sns.regplot(ax = ax[0,0],x='Volume',y='Close',data=df_day)\nax[0,0].set_title('Day')\nsns.regplot(ax = ax[0,1],x='Open',y='Low',data=df_week)\n# sns.regplot(ax = ax[0,1],x='Volume',y='Close',data=df_week)\nax[0,1].set_title('Week')\nsns.regplot(ax = ax[1,0],x='Open',y='Low',data=df_month)\n# sns.regplot(ax = ax[1,0],x='Volume',y='Close',data=df_month)\nax[1,0].set_title('Month')\nsns.regplot(ax = ax[1,1],x='Open',y='Low',data=df_year)\n# sns.regplot(ax = ax[1,1],x='Volume',y='Close',data=df_year)\nax[1,1].set_title('Year')","88af2356":"df['OH'] = df['Open']-df['High']\ndf['OL'] = df['Open']-df['Low']\ndf['OC'] = df['Open']-df['Close']\ndf['HL'] = df['High']-df['Low']\ndf['HC'] = df['High']-df['Close']\ndf['LC'] = df['Low']-df['Close']","e4a64d62":"diff_list = ['OH','OL','OC','HL','HC','LC']\ninterval = ['Year','Month','Week','Day']\nfig,ax = plt.subplots(6,4,figsize=(30,30))\nfor i,diff in enumerate(diff_list):\n    for j,intv in enumerate(interval):\n        sns.boxplot(ax = ax[i,j],x = intv, y = diff, data=df)\n        ax[i,j].set_title(diff)\n        if diff == 'OH':\n            ax[i,j].set_ylim([-15,0])\n        elif diff == 'OL':\n            ax[i,j].set_ylim([0,15])\n        elif diff == 'OC':\n            ax[i,j].set_ylim([-10,10])\n        elif diff == 'HL':\n            ax[i,j].set_ylim([0,20])\n        elif diff == 'HC':\n            ax[i,j].set_ylim([0,10])\n        elif diff == 'LC':\n            ax[i,j].set_ylim([-10,0])","3652027a":"plt.figure(figsize=(16,16))\ndf_corr_map = df.drop(columns=['Year','Day','Month','Week','Dayofweek'])\nsns.heatmap(df_corr_map.corr(),vmin=-1, vmax=1, annot=True, cmap='BrBG')","42cbf7d9":"plt.figure(figsize=(16,16))\nsns.pairplot(df_corr_map)","b3c62a56":"fig, ax = plt.subplots(figsize=(20,5))\nax2 = ax.twinx()\nsns.lineplot(ax = ax2, x='Date',y='OH',data=df)\nsns.lineplot(ax = ax2, x='Date',y='HC',data=df)\nsns.lineplot(ax = ax, x='Date',y='Open',data=df,color='green')\nax.set_ylim([-1200,400])","dd9eebcd":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\npd.plotting.lag_plot(df['Open'],lag=1)\nplt.subplot(1,2,2)\npd.plotting.lag_plot(df['Open'],lag=100)","833085ec":"from statsmodels.graphics import tsaplots","43cc200c":"fig,ax = plt.subplots(2,1,figsize=(20,10))\ntsaplots.plot_acf(df['Volume'], lags=100,ax=ax[0])\ntsaplots.plot_pacf(df['Volume'], lags=100,ax=ax[1])\nplt.show()","a6955a13":"fig,ax = plt.subplots(2,1,figsize=(20,10))\ntsaplots.plot_acf(df['Open'], lags=100,ax=ax[0])\ntsaplots.plot_pacf(df['Open'], lags=100,ax=ax[1])\nplt.show()","6b8e52c4":"# Importing libraries\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, SimpleRNN, GRU, LSTM\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\nfrom sklearn.metrics import mean_squared_error, r2_score","594112ba":"# Train_Test_Split, 70\/30 split\ntraining_size = int(len(df)*0.70)\ndf_len = len(df)\ntrain, test = df[0:training_size],df[training_size:df_len]\nprint(\"Training data point --> \",training_size)\nprint(\"Testing data point --> \",df_len-training_size)\nplt.figure(figsize=(20,5))\nsns.lineplot(x='Date',y='Open',data=df)\nsns.lineplot(x='Date',y='Open',data=train,color='green',label='Train')\nsns.lineplot(x='Date',y='Open',data=test,color='orange',label='Test')\nplt.grid()\nplt.legend(fontsize=20)","b9f47b2c":"df.columns","210cbd88":"# Normalization\ntrain_data = train.loc[:, ['Open']].values\ntest_data = test.loc[:, ['Open']].values\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest_scaled = scaler.fit_transform(test_data)\ntrain_scaled = scaler.fit_transform(train_data)","619fc54d":"train_len = len(train_scaled)\nX_train = []\ny_train = []\ntimesteps = 40\nfor i in range(timesteps, train_len):\n    X_train.append(train_scaled[i - timesteps:i, :])\n    y_train.append(train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\ntest_len = len(test_scaled)\nX_test = []\ny_test = []\ntimesteps = 40\nfor i in range(timesteps, test_len):\n    X_test.append(test_scaled[i - timesteps:i, :])\n    y_test.append(test_scaled[i, 0])\nX_test, y_test = np.array(X_test), np.array(y_test)\n\nprint(\"X_train shape --> \", X_train.shape)\nprint(\"X_test shape --> \", X_test.shape)\nprint(\"y_train shape --> \", y_train.shape)\nprint(\"y_test shape --> \", y_test.shape)","2c87840c":"# Simple RNN network mapping\n\nRNN_model = Sequential()\n\nRNN_model.add(SimpleRNN(units = 50, activation = \"swish\", return_sequences = True, input_shape = (X_train.shape[1],X_train.shape[2])))\nRNN_model.add(Dropout(0.2))\n\nRNN_model.add(SimpleRNN(units = 50, activation = \"swish\", return_sequences = True))\nRNN_model.add(Dropout(0.2))\n\nRNN_model.add(SimpleRNN(units = 50, activation = \"swish\", return_sequences = True))\nRNN_model.add(Dropout(0.2))\n\nRNN_model.add(SimpleRNN(units = 50))\nRNN_model.add(Dropout(0.2))\n\nRNN_model.add(Dense(units = 1))\nRNN_model.compile(optimizer= \"adam\", loss = \"mean_squared_error\")\n    \n\nbatch_size = 20\nregression = RNN_model.fit(X_train, y_train, epochs = 20,validation_split=0.33, batch_size = batch_size,verbose=False)\n","0c31c186":"plt.figure(figsize=(20,5))\nplt.plot(regression.history['loss'])\nplt.plot(regression.history['val_loss'])\nplt.title('RNN_model loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['trained', 'validated'], loc='upper right')\nplt.grid()","37038f82":"# make a prediction\ntrainPredicted = RNN_model.predict(X_train)\ntestPredicted = RNN_model.predict(X_test)\n\ntrainX_extended = np.zeros((len(trainPredicted),X_train.shape[2]))\ntestX_extended = np.zeros((len(testPredicted),X_test.shape[2]))\ny_test_extended = np.zeros((len(y_test),X_test.shape[2])) # check with validation data, u need to also inv transform.\n\ny_test = y_test.reshape((len(y_test),1))\n\ntrainX_extended[:,0]=trainPredicted[:,0]\ntestX_extended[:,0]=testPredicted[:,0]\ny_test_extended[:,0]=y_test[:,0]\n\ntrainX=scaler.inverse_transform(trainX_extended)[:,0]\ntestX=scaler.inverse_transform(testX_extended)[:,0]\ny_test = scaler.inverse_transform(y_test_extended)[:,0]\n","ed532914":"dataset_total = pd.concat((train[['Open']], \n                           test[['Open']]), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(test) - timesteps:].values\ninputs = scaler.transform(dataset_total)\n\nX_test = []\n\nfor i in range(timesteps, len(inputs)):\n    X_test.append(inputs[i-timesteps:i, :])\nX_test = np.array(X_test)\n\nprint(\"X_test shape --> \", X_test.shape)","c3a03975":"X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\nRNN_predict = RNN_model.predict(X_test)\n\nRNN_predict_extended = np.zeros((len(RNN_predict),X_train.shape[2]))\n\nRNN_predict_extended[:,0]=RNN_predict[:,0]\n\nRNN_predict=scaler.inverse_transform(RNN_predict_extended)[:,0]","0557cde3":"print(RNN_predict.shape)\noffset = np.zeros(timesteps)\nRNN_predict = np.concatenate((offset,RNN_predict))\nRNN_predict = pd.Series(RNN_predict)\nRNN_predict = pd.concat((df['Date'],RNN_predict),axis=1)\nRNN_predict.rename(columns={0:'Predicted'},inplace=True)\nRNN_predict","4764904b":"plt.figure(figsize=(20,5))\nsns.lineplot(x='Date',y='Open',data=train,color='green',label='Train')\nsns.lineplot(x='Date',y='Open',data=test,color='orange',label='Test')\nsns.lineplot(x='Date',y='Predicted',data=RNN_predict,color='red',label='Predicted')\nplt.legend(fontsize=20)\nplt.grid()","1a4c1e7d":"# LSTM network mapping\n\nLSTM_model = Sequential()\n\nLSTM_model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1],X_train.shape[2])))\nLSTM_model.add(Dropout(0.2))\n\nLSTM_model.add(LSTM(units = 25))\nLSTM_model.add(Dropout(0.2))\n\nLSTM_model.add(Dense(units = 1))\nLSTM_model.compile(optimizer= \"adam\", loss = \"mean_squared_error\")\n    \nepochs = 25\nbatch_size = 30\nregression_lstm = LSTM_model.fit(X_train, y_train, epochs = epochs,validation_split=0.33, batch_size = batch_size,verbose=False)\n","2ddd0e4b":"plt.figure(figsize=(20,5))\nplt.plot(regression_lstm.history['loss'])\nplt.plot(regression_lstm.history['val_loss'])\nplt.title('LSTM model loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['trained', 'validated'], loc='upper right')\nplt.grid()","8316a440":"# make a prediction\ntrainPredicted = LSTM_model.predict(X_train)\ntestPredicted = LSTM_model.predict(X_test)\n\ntrainX_extended = np.zeros((len(trainPredicted),X_train.shape[2]))\ntestX_extended = np.zeros((len(testPredicted),X_test.shape[2]))\ny_test_extended = np.zeros((len(y_test),X_test.shape[2])) # check with validation data, u need to also inv transform.\n\ny_test = y_test.reshape((len(y_test),1))\n\ntrainX_extended[:,0]=trainPredicted[:,0]\ntestX_extended[:,0]=testPredicted[:,0]\ny_test_extended[:,0]=y_test[:,0]\n\ntrainX=scaler.inverse_transform(trainX_extended)[:,0]\ntestX=scaler.inverse_transform(testX_extended)[:,0]\ny_test = scaler.inverse_transform(y_test_extended)[:,0]\n","a5eb615a":"dataset_total = pd.concat((train[['Open']], \n                           test[['Open']]), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(test) - timesteps:].values\ninputs = scaler.transform(dataset_total)\n\nX_test = []\n\nfor i in range(timesteps, len(inputs)):\n    X_test.append(inputs[i-timesteps:i, :])\nX_test = np.array(X_test)\n\nprint(\"X_test shape --> \", X_test.shape)","7e2d0e93":"X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\nLSTM_predict = LSTM_model.predict(X_test)\n\npredict_extended = np.zeros((len(LSTM_predict),X_train.shape[2]))\n\npredict_extended[:,0]=LSTM_predict[:,0]\n\nLSTM_predict=scaler.inverse_transform(predict_extended)[:,0]","d83f0aac":"LSTM_predict.shape\noffset = np.zeros(timesteps)\nLSTM_predict = np.concatenate((offset,LSTM_predict))\nLSTM_predict = pd.Series(LSTM_predict)\nLSTM_predict = pd.concat((df['Date'],LSTM_predict),axis=1)\nLSTM_predict.rename(columns={0:'Predicted'},inplace=True)","f01b9380":"plt.figure(figsize=(20,5))\nsns.lineplot(x='Date',y='Open',data=train,color='green',label='Train')\nsns.lineplot(x='Date',y='Open',data=test,color='orange',label='Test')\nsns.lineplot(x='Date',y='Predicted',data=LSTM_predict,color='red',label='Predicted')\nplt.legend(fontsize=20)\nplt.grid()","55fb2e46":"test_mse_RNN = math.sqrt(mean_squared_error(RNN_predict['Predicted'].loc[1300:],df['Open'].loc[1300:]))\ntest_mse_LSTM = math.sqrt(mean_squared_error(LSTM_predict['Predicted'].loc[1300:],df['Open'].loc[1300:]))\nr2_RNN = r2_score(RNN_predict['Predicted'].loc[1300:],df['Open'].loc[1300:])\nr2_LSTM = r2_score(LSTM_predict['Predicted'].loc[1300:],df['Open'].loc[1300:])\nprint(\"test_mse_RNN: \",test_mse_RNN)\nprint(\"test_mse_LSTM: \",test_mse_LSTM)\nprint(\"r2_RNN: \",r2_RNN)\nprint(\"r2_LSTM: \",r2_LSTM)","e5132852":"We didn't have any values at the start that contained NaN, which makes life easier because we do not have to consider whether if we are going to drop the days or fill it with interpolation\/median values or however. but it seems that there are instances where \"flatlined\" values are present.\n\nWill you do something about the flatlined values..? Nah, because it looks simply a coincidence and the data that contains the flatlined values are discrete and apart from others. we will just leave the dataset. \n\n'OpenInt' does not seem to have any values... I am going to drop this variable.","6d84ebf8":"How many elements are in my file? (how many stock companies?)","c1ac4199":"Can I see any correlations? I have zero clue just by looking at this!as a person who does not know anything about the stocks, lets get creative and do some EDA, feature engineering and etc. \n\nlets look at the seasonality first. In order to do this, lets disect the date","5887d2f2":"So... in terms of 1 day lag, autoregressive model trait matches, and in terms of 100 day lags (~quarter), somewhat seasonality is found. from the initial EDA, at least we can find out that there is a clear difference in summer and winter.","d38f5995":"# 8. Model Performance Comparison","7bcda9e5":"# 3. Preprocessing \/ Data-Wrangling","5de1d860":"# 4. Exploratory Data Analysis (EDA)","0dd4df20":"When we have our recurrent neural network, we want to shape our data in a way that we can have the following:\n\n1. The size of data we have\n2. Number of steps\n3. Number of features","8244bfc2":"You can maybe force the process to make a loop where we can minimize our MSE for both models and have the r2 score to be maximized. However, if there is any negative r2 while having MSE of ~ < 40 means that both model is not fitting sufficiently.\n\nHaving negative r2 could mean both the following: \n> 1. Model is not given with the intercept\n> 2. Your model is worse than horizontal fit.\n\nto illustrate the second point, following image is used:\n\n![](https:\/\/i.stack.imgur.com\/vYETJ.png)\n\nThe result also leads to the fact that having just \"Open\" price with the RNN and LSTM model is not sufficient, we either need to change the model design or gather some new information from the other data that could explain the stock price data better than the self correlation methods.\n\n**Thank you for reading.**\n\n***Paul Keun Woo Kang***","f40107bc":"So far, we have came up with the model and trended with the raw data to see if they match well. to quantify the accuracy, lets calculate its each R2 and MSE for each RNN and LSTM models","0ad106dc":"So... pretty much when it comes to daily groupby data, we don't see any relationship but as we increase the data interval as for its aggregation period, we do see general positive relationship that when trade volume increase, prices also increase..","0aa4260e":"**Auto Correlation**\n\nIn time series problems, it is important to see what is its autocorrelation and test if there is time-dependent repeated patterns (cyclicity). For this, pandas lag_plot will be used\n\n> A linear shape to the plot suggests that an autoregressive model is probably a better choice.\n\n> An elliptical plot suggests that the data comes from a single-cycle sinusoidal model.\n\nIf your data shows a linear pattern, it suggests autocorrelation is present. A positive linear trend (i.e. going upwards from left to right) is suggestive of positive autocorrelation; a negative linear trend (going downwards from left to right) is suggestive of negative autocorrelation. The tighter the data is clustered around the diagonal, the more autocorrelation is present; perfectly autocorrelated data will cluster in a single diagonal line.\n\n![](https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2016\/11\/lag-plot-linear.png)\n\n\nData can be checked for seasonality by plotting observations for a greater number of periods (lags). Data with seasonality will repeat itself periodically in a sine or cosine-like wave.\n\n![](https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2016\/11\/seasons-lp.png)","f469d06a":"Amount of trading volume at k(lag) = 10 (every 10 business days) may have its certain pattern, that is: every 2 weeks.\nHowever, when we look at the stock prices, we don't really see any significant autocorrelation, meaning that most of the prices does not carry any pattern, just statistical fluke, and is unpredictable on its own. (It does not self correlate)","f163f5ab":"We have noticable positive relationship betwen Open-High with Low-Close & High-Close with Open-Low","3d464d18":"# 9.Conclusion","c56fe6da":"# 1. Data Import\n1.1. Data directory and fetching file names","02268af3":"# 5. Feature Engineering \/ EDA continued","bf8f0675":"it seems there is a seasonality existing in stock open price, lets see how the prices are related with the volume traded","f9a56242":"# 2. Background Research\n\nSince I know nothing about stocks, lets study some terminology:\n\nSource: https:\/\/analyzingalpha.com\/open-high-low-close-stocks#:~:text=In%20stock%20trading%2C%20the%20high,trading%20in%20the%20same%20period.&text=Traders%20often%20visualize%20price%20actions%20through%20bars%20and%20bar%20charts.\n\nIn stock trading, the high and low refer to the maximum and minimum prices in a given time period. Open and close are the prices at which a stock began and ended trading in the same period. Volume is the total amount of trading activity. Adjusted values factor in corporate actions such as dividends, stock splits, and new share issuance.\n\nValuable information can be gleaned from understanding the open, high, low, close of a stock, and as well as it\u2019s trading volume. Traders often visualize price actions through bars and bar charts.\n\n![](https:\/\/analyzingalpha.com\/assets\/images\/posts\/2020-04-17-bar-chart-ohlc.png)","b650238f":"**Lets pick Tesla as an example.**","3e11295c":"# 7. ML model - Long-Short Term Memory (LSTM)\n\n![](https:\/\/miro.medium.com\/max\/2706\/1*esTGDR3kcDLaTEHKCBedTQ.png)","57d4f07d":"I want to study, how the gap between Open-High, Open-Low, Open-Close, High-Low, High-Close, Low-Close, Volume can have the correlation, because I want to know how the daily variability of the prices (or measure of how volatile the prices were) affect the prediction performance of the model.","e1ec4b02":"**Lets see how our data looks like!**","ce341366":"# 6. ML model - Recurrent Neural Network (RNN)\n\nSource: https:\/\/towardsdatascience.com\/recurrent-neural-networks-by-example-in-python-ffd204f99470\n\nWhat is Recurrent Neural Network?\n\nAt a high level, a recurrent neural network (RNN) processes sequences \u2014 whether daily stock prices, sentences, or sensor measurements \u2014 one element at a time while retaining a memory (called a state) of what has come previously in the sequence.\n\nRecurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\n\n![](https:\/\/miro.medium.com\/max\/846\/1*KljWrINqItHR6ng05ASR8w.png)\n\n\n"}}