{"cell_type":{"cf178dd2":"code","d1b394a2":"code","ad60c972":"code","27165998":"code","8e7b6cc5":"code","60c05846":"code","039bcccd":"code","9adddf8a":"code","4d579eed":"code","007c33bb":"code","0398ae7e":"code","652ecdb9":"code","3db29dc6":"code","30f97c7d":"code","f57f6b2a":"code","3adc7619":"code","0c7fb111":"code","0cd3fe39":"code","0cf3fe05":"code","141b9270":"code","8b3f763b":"code","2d348260":"code","443bb43c":"code","f865026b":"code","cc5db020":"code","8e709133":"code","365904c6":"code","f5f0a11b":"code","4f206405":"code","caf8e0f9":"code","c5052b3e":"code","aaf658b4":"code","d9ad97e3":"code","3e161062":"code","2ca1ab6c":"code","bfd675d4":"code","07505ecb":"code","9a0dedcf":"code","2388e3f5":"code","802e3488":"code","dca7f0e2":"code","19d06687":"code","159d714f":"markdown","9a13f901":"markdown","e3af605a":"markdown","b6fab509":"markdown","12bc9938":"markdown","3515803a":"markdown","484e12b3":"markdown","5c062cae":"markdown","ec14c65d":"markdown","00fd9379":"markdown","a4e8113b":"markdown","8f42801a":"markdown","b4fe7de1":"markdown","0d84384d":"markdown","c9bf45a8":"markdown","4f8a5130":"markdown","f6170369":"markdown","7ceb140d":"markdown","23b76f28":"markdown"},"source":{"cf178dd2":"!pip install transformers\n!wget -O scibert_uncased.tar https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/huggingface_pytorch\/scibert_scivocab_uncased.tar\n!tar -xvf scibert_uncased.tar\nimport torch\nfrom transformers import BertTokenizer, BertModel","d1b394a2":"model_version = 'scibert_scivocab_uncased'\ndo_lower_case = True\nmodel = BertModel.from_pretrained(model_version)\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)","ad60c972":"from sklearn.metrics.pairwise import cosine_similarity\ndef embed_text(text, model):\n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n    return last_hidden_states \n\ndef get_similarity(em, em2):\n    return cosine_similarity(em.detach().numpy(), em2.detach().numpy())","27165998":"# We will use a mean of all word embeddings. To do that we will take mean over dimension 1 which is the sequence length.\ncoronavirus_em = embed_text(\"Coronavirus\", model).mean(1)\nmers_em = embed_text(\"Middle East Respiratory Virus\", model).mean(1)\nflu_em = embed_text(\"Flu\", model).mean(1)\nbog_em = embed_text(\"Bog\", model).mean(1)\ncovid_2019 = embed_text(\"COVID-2019\", model).mean(1)\nprint(\"Similarity for Coronavirus and Flu:\" + str(get_similarity(coronavirus_em, flu_em)))\nprint(\"Similarity for Coronavirus and MERs:\" + str(get_similarity(coronavirus_em, mers_em)))\nprint(\"Similarity for Coronavirus and COVID-2019:\" + str(get_similarity(coronavirus_em, covid_2019)))\nprint(\"Similarity for Coronavirus and Bog:\" + str(get_similarity(coronavirus_em, bog_em)))\n\n","8e7b6cc5":"!pip install umap-learn\nimport umap\nreducer = umap.UMAP()","60c05846":"import os\nimport json \ndef make_the_embeds(number_files, start_range=0, \n                    the_path=\"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\", data_key=[\"metadata\", \"title\"]):\n    the_list = os.listdir(the_path)\n    title_embedding_list = [] \n    title_list = []\n    for i in range(start_range, number_files):\n        file_name = the_list[i]\n        final_path = os.path.join(the_path, file_name)\n        with open(final_path) as f:\n            data = json.load(f)\n        try:\n            tensor, title = make_data_embedding(data, data_key)\n            title_embedding_list.append(tensor)\n            title_list.append(title)\n        except:\n            print(\"Invalid title\/abstract\")\n    return torch.cat(title_embedding_list, dim=0), title_list\n        \ndef make_data_embedding(article_data, data_keys, method=\"mean\", dim=1):\n    data = article_data\n    for key in data_keys:\n        data = data[key]\n    text = embed_text(data, model)\n    if method == \"mean\":\n        return text.mean(dim), data\n    \nembed_list, title_list = make_the_embeds(200)\nred = reducer.fit_transform(embed_list.detach().numpy())#","039bcccd":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10, Category20c\nfrom bokeh.palettes import magma\nimport pandas as pd\noutput_notebook()","9adddf8a":"def make_plot(red, title_list, number=200, color = True, color_mapping_cat=None, color_cats = None, bg_color=\"white\"):   \n    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n    if color_mapping_cat:\n        digits_df['colors'] = color_mapping_cat\n    digits_df['digit'] = title_list\n    datasource = ColumnDataSource(digits_df)\n    plot_figure = figure(\n    title='UMAP projection of the article title embeddings',\n    plot_width=890,\n    plot_height=600,\n    tools=('pan, wheel_zoom, reset'),\n    background_fill_color = bg_color\n    )\n    plot_figure.legend.location = \"top_left\",\n    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n    <div>\n    <div>\n        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'\/>\n    <\/div>\n    <div>\n        <span style='font-size: 10px; color: #224499'><\/span>\n        <span style='font-size: 10px'>@digit<\/span>\n    <\/div>\n    <\/div>\n    \"\"\"))\n    if color:   \n        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    elif color_mapping_cat:\n        color_mapping = CategoricalColorMapper(factors=color_cats, palette=magma(len(color_cats)+2)[2:])\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='colors', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=8,\n            legend_field='colors'\n        )\n        show(plot_figure)\n    else:\n        \n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit'),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    \nmake_plot(red, title_list, number=200)","4d579eed":"embed_list2, title_list2 = make_the_embeds(401, 201)\nred2 = reducer.fit_transform(embed_list.detach().numpy())\nprint(len(title_list2))\nmake_plot(red2, title_list2, number=198)","007c33bb":"#max_len = len(os.listdir(\"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/comm_use_subset\/comm_use_subset\"))\n#embed_list, title_list_full = make_the_embeds(2000,1200)\n#red_full = reducer.fit_transform(embed_list.detach().numpy())\n#make_plot(red_full, title_list_full, 256, color=False)","0398ae7e":"from sklearn.decomposition import PCA\npca = PCA(n_components=2, svd_solver='full')\n#embed_list_pca, title_list_pca = make_the_embeds(1000)\n#result = pca.fit_transform(embed_list_pca.detach().numpy())","652ecdb9":"#make_plot(result, title_list_pca, 200)","3db29dc6":"import collections\nq1 = \"COVID-19 infection origin and transmission from animals\"\nsearch_terms = embed_text(q1, model).mean(1)","30f97c7d":"def top_n_closest(search_term_embedding, title_embeddings, original_titles, n=10):\n    proximity_dict = {}\n    i = 0 \n    for title_embedding in title_embeddings:\n        proximity_dict[original_titles[i]] = {\"score\": get_similarity(title_embedding.unsqueeze(0),search_term_embedding), \n                                              \"title_embedding\":title_embedding.unsqueeze(0)}\n        i+=1\n    order_dict = collections.OrderedDict({k: v for k, v in sorted(proximity_dict.items(), key=lambda item: item[1][\"score\"])})\n    proper_list = list(order_dict.keys())[-n:]\n    return proper_list, order_dict\n        ","f57f6b2a":"top_titles, order_dict = top_n_closest(search_terms, embed_list2, title_list+title_list2)","3adc7619":"top_titles","0c7fb111":"q2 = \"coronavirus person to person transmission mechanisms\"","0cd3fe39":"search_terms2 = embed_text(q2, model).mean(1)\ntop_titles2, order_dict1 = top_n_closest(search_terms2, embed_list2, title_list + title_list2)\ntop_titles2","0cf3fe05":"def remake_combine_dict_embeds_plot(titles_list, order_dicts, search_term_list):\n    categories = []\n    embeddings = [] \n    for i in range(0, len(order_dicts)):\n        order_dict = order_dicts[i]\n        titles = titles_list[i]\n        for title in titles:\n            embeddings.append(order_dict[title][\"title_embedding\"])\n            categories.append(search_term_list[i])\n    return embeddings, categories\n\nembeds, cats = remake_combine_dict_embeds_plot([top_titles, top_titles2], [order_dict, order_dict1], [q1, q2] )\n\nsearches = [q1, q2]","141b9270":"title_list_full = top_titles + top_titles2\nembeds2 = torch.cat(embeds, dim=0)\npca_res = pca.fit_transform(embeds2.detach().numpy())","8b3f763b":"make_plot(pca_res, title_list_full, 0, color=False, color_mapping_cat=cats, color_cats=searches, bg_color=\"black\")","2d348260":"title_list_full","443bb43c":"absd_embeds, abs_orig = make_the_embeds(15, 0, data_key=['abstract', 0, \"text\"])","f865026b":"abs_orig[0]","cc5db020":"abs_orig[1]","8e709133":"get_similarity(absd_embeds[0].unsqueeze(0), absd_embeds[1].unsqueeze(0))","365904c6":"search_terms = embed_text(\"coronavirus bat to human transmission\", model).mean(1)\ntop_embeddings, order_dict = top_n_closest(search_terms, absd_embeds, abs_orig, n=3)","f5f0a11b":"top_embeddings[0]","4f206405":"top_embeddings[1]","caf8e0f9":"del red\ndel absd_embeds\ndel abs_orig\ndel red2\ndel embed_list\ndel embed_list2\ndel embeds \nimport gc\ngc.collect()","c5052b3e":"del Out[22]\ndel Out[26]\ndel Out[24]\ndel Out[6]\ndel Out[7]\ngc.collect()","aaf658b4":"!pip install rank_bm25","d9ad97e3":"# Let's download the BM25 index\nimport pickle\nimport os\n!git clone https:\/\/github.com\/CoronaWhy\/CORD-19-QA \nos.chdir('CORD-19-QA')\n!wget -O bert_bioasq_final-bm25.pkl https:\/\/publicweightsdata.s3.us-east-2.amazonaws.com\/bert_bioasq_final-bm25.pkl\nfrom bm25_index import BM25Index\nthe_index = pickle.load(open('bert_bioasq_final-bm25.pkl', 'rb'))","3e161062":"the_index = pickle.load(open('bert_bioasq_final-bm25.pkl', 'rb'))\nabstracts = the_index.search(\"coronavirus bat to human transmission\", 19)\nabstract_embed = []\nshort_abstracts = []\nabstracts.head()","2ca1ab6c":"def embed_abstracts_from_bm25(abstracts):\n    abstracts = abstracts[\"abstract\"].tolist()\n    abstract_embed = []\n    for abstract in abstracts:\n        if len(tokenizer.encode(abstract))<512:\n            abstract_embed.append(embed_text(abstract, model).mean(1).squeeze(0))\n        else: \n            # TO-DO truncate to max allowable length\n            abstract2 = \" \".join(abstract.split()[:200])\n            print(len(abstract2.split()))\n            abstract_embed.append(embed_text(abstract2, model).mean(1).squeeze(0))\n    return abstracts, abstract_embed","bfd675d4":"abstracts, abstract_embed = embed_abstracts_from_bm25(abstracts)\nsearch_terms2 = embed_text(\"coronavirus bat to human transmission\", model).mean(1)","07505ecb":"top_abs1, order_dict1 = top_n_closest(search_terms2, abstract_embed, abstracts)","9a0dedcf":"top_abs1[0]","2388e3f5":"top_abs1[1]","802e3488":"abstracts_2 = the_index.search(\"COVID-19 person to person transmission dynamics\", 19)\nsearch_terms3 = embed_text(\"COVID-19 person to person transmission dynamics\", model).mean(1)\nsearch_term_list = [\"coronavirus bat to human transmission\", \"COVID-19 person to person transmission dynamics\"]\nabstracts2, abstract_embed2 = embed_abstracts_from_bm25(abstracts_2)","dca7f0e2":"print(len(abstract_embed2))\ntop_abs, order_dict2 = top_n_closest(search_terms3, abstract_embed2, abstracts2)\nabstract_lists = [top_abs1, abstracts2]\norder_dicts = [order_dict1, order_dict2]\nembeds, cats = remake_combine_dict_embeds_plot(abstract_lists, order_dicts, search_term_list)\nembeds2 = torch.cat(embeds, dim=0)\npca_res = pca.fit_transform(embeds2.detach().numpy())","19d06687":"make_plot(pca_res, top_abs1+abstracts2, number=200, color = False, color_mapping_cat=cats, color_cats = cats, bg_color=\"white\")","159d714f":"Since the following abstracts will be hard to display in U-Map with the toolover I won't plot them. Instead let's just look at these two:","9a13f901":"~~We'll attempt to make a plot of all 9000  1000 (that did make it run out of RAM)  articles in that directory (warning this might crash your notebook). For fun we'll make these a different 1000 then what we already viewed~~. Update: I'm going to comment this section out as it doesn't provide much value and requires a lot of RAM which hurts the downstream tasks. Feel free to run on a seperate fork if you want.","e3af605a":"## Finding Insights with abstracts\nThe end goal of this project is to find actionable insights about Coronavirus, its origins, potential treatment plans, and more.  For this step, due to the high overhead of embeddings methods we would probably want to combine our semantic search with the BM25 index. In this scenario BM25 would return 25 (or how much the RAM could handle) candidate answers and then we would rerank based on SciBERT embeddings.","b6fab509":"# SciBERT Embeddings Analysis\nThis is a basic tutorial and analysis of how to download and use BERT models to create naive embeddings, which can be used for exploring concepts in the COVID-19 literature corpus. [SciBERT](https:\/\/github.com\/allenai\/scibert) is a large transformer model trained on scientific text. While this tutorial shows how to use SciBERT, specifically you should be able to reuse the code for any HuggingFace transformer model. Using it allows us to explore several interesting questions:\n\n    1. Are raw embeddings from the language model useful without any further fine-tuning?\n    2. Can we construct a memory efficent semantic search using these embeddings?\n    3. What does the embedding space look like when visualized? Are similar articles clustered together?\n\nOf course long-term we would probably want to use richer embedding, specifically, embeddings from a model trained on this corpus or formed in a richer way. For instance, the embedding method displayed here is a simple MEAN over all the words in the text passage. Possibly more advanced methods would include using sentence transformers or document embedding techniques, but these methods (usually) require annotated training data. The nice thing about this method is it is relatively simple to use out of the box.","12bc9938":"Unfortunately we now also face an additional problem. BERT models cannot handle text passages of more than 512 *tokens*. For now since this is just a demo I'm just going to truncate long passages at 200 words. Why 200 words and not 512? Because the tokenizer actually returns longer sequence lengths than the original input. Since I'm lazy and don't want to modify my embed_text function I'm choosing 200 as an arbitrary length (that won't cross 512). There are a couple ways we could solve this in a real world setting:\n1. Split the passage after 512 tokens. Feed in each 512 token chunk, then combine each resulting (768, 512) along the 512 dimension. For instance, if the first chunk had 512 tokens and the second had 300 we would end up with a (768, 812) tensor. We would then just take the average over the entire sequence length to get a (1, 768) tensor just like before.\n2. We could attempt to use other models like Transformer-XL or Reformer. As these models do not have the 512 token constraint they could take the whole text passage. However, at the moment I don't know of any pre-trained versions of these models trained on scientific data.","3515803a":"## Embedding Abstracts\nJust for fun and to enrich our knowledge let's try embedding abstracts. Unfortunately we are extremly limited with RAM so the next section we won't be able to go beyond 20 or so abstracts at a time. ","484e12b3":"We will use the SciBERT Vocab uncased model as that is what is recommended on the official GitHub Page.","5c062cae":"Another interesting thing we will try is to see if the returned search results occupy different places in the embedding space when plotting. In theory for a good search engine we probably would want very distinct clusters as these queries are different.","ec14c65d":"I honestly don't know enough about the subject area to tell if that is a good similarity score for those two.","00fd9379":"I found 200 to be a good chunk size for running quick analysis; doing a full plot can get kind of crowded and is slow to compute.","a4e8113b":"### Using Semantic Search + the BM25 index\nWe will now integrate our semantic search results with the BM25. Due to RAM being at a premimum we will first delete our previous objects from the search. Here the BM25 index will return the 25 most relevant results.","8f42801a":"**Visualizing with PCA**\n\nPCA is one of the older embedding visualizations techniques. The advantage of PCA is that it is a bit faster than U-MAP and less RAM intensive. Therefore we can plot more results. For instance an average run took 14.2 ms \u00b1 146 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each). However for now I'll also comment out this code as we will be using more RAM later on. We will be using PCA to plot our search attempt results though.","b4fe7de1":"## Conclusion\nWe have a couple key takeaways (note I'll include the original question these bullet points reference in parantheses). \n* Out of the box SciBERT embeddings seem to capture meaning suprisingly well (1).\n* Embeddings use a lot of RAM particularly when embedding abstracts. This makes an entirely semantic search impractical (2). \n* BERT cannot accomodate text passages longer than 512 characters which would make it difficult to embed an article's full-text.\n* Visualized clusters of titles seemed to make sense, but could use evaluation by a biology\/chemistry\/drug expert (3)\n\n## Next Steps\nThere are a bunch of possible next steps so I'm going to split them based on the area. \n\n** Evaluation **\n* Get expert evaluation of title clusters to see if they actually make sense. \n* Figure out a way to plot abstracts in Bokeh in a visually appealing way. Have experts inspect abstract clusters.\n* Continue to run semantic search on groups of returned BM25 results. Get experts to rank results. \n\n\n**Making better embeddings **\n\n* Train a language model specifically on the COVID-19 corpus and evaluate its embeddings versus SciBERT\n* Investigate sentence transformers (pre-trained on MedNLI\/SciTail) and explore if these embeddings work better\n* See if anyone has a scientifically trained transformer-XL model  or equivalent.\n\n**Performance\/Productionizing**\n\n* Explore and benchmark speed\/space constraints of returning larger numbers of BM25 results to re-rank with embeddings.\n* Figure out storage method for embeddings and iteratively embed each article.\n* Research if U-Map as a dimensionality reduction technique can return embeddings. Easier to load 100D into memory then 768D.\n\nHope you enjoyed this analysis. If you found it useful please upvote as it helps more people to see it. Also, feel free to ask any questions below.\n\n","0d84384d":"The results actually don't seem that bad given the model doesn't have any specific training.","c9bf45a8":"Anecdotally we can see that even in the raw embeddings there seems to be at least some correlation between concepts. However, it is curious why it scores things like Coronavirus and bog at .64 as I'd expect those to be dissimilar.\n\nLet's now look at visualizing some of these vectors with [U-Map](https:\/\/towardsdatascience.com\/how-exactly-umap-works-13e3040e1668). I'm choosing U-Map here due to the high-dimensionality of the data (768-D). However, I will also add some PCA visualizations below if I have time.","4f8a5130":"There do seem to be a few interesing patterns when analyizng with U-Map. However, I believe more sophisticated methods could definitely improve the clustering of groups. Let's examine another chunk:","f6170369":"We are now going to build a very simple semantic search engine on the titles of the articles. This should in theory return the most similar articles for a given query. However, due to memory constraints I'm only going to run it on 200 titles instead of the whole corpus.","7ceb140d":"We can now see that there are definitely some overlapping titles (orange). Additionally the \"coronavirus person to person transmission mechanism\" query doesn't really seem to occupy a different embedding space than the \"origin and transmission mechanism from animals\" query. Keep in mind though this was only 400 titles due to the RAM constraints, not the entire corpus. If we queried the whole corpus I'd suspect we would find no overlapping titles. ","23b76f28":"## Part 2 Search Attempts on Titles"}}