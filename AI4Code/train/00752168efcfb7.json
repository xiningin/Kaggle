{"cell_type":{"ff24f79a":"code","1f707ac0":"code","e29e0666":"code","614e453b":"code","970061c5":"code","3a0c9c77":"code","a7975e84":"markdown","1860e815":"markdown","13b59ce3":"markdown","62bfa4ee":"markdown","775b77b1":"markdown"},"source":{"ff24f79a":"import os\n\nimport numpy as np\nimport pandas as pd\n\nfrom transformers import LongformerTokenizerFast\n\nfrom tqdm import tqdm","1f707ac0":"# load the train text data\n\nconfig = {\n    'model_name': 'allenai\/longformer-base-4096',\n    'batch_size': 4,\n}\n\nTEXT_FILES = os.listdir('..\/input\/feedback-prize-2021\/train')\nTEXT_FILES = [f'..\/input\/feedback-prize-2021\/train\/{file}' for file in TEXT_FILES]\n\ntext_data = dict()\nfor file_path in tqdm(TEXT_FILES):\n    with open(file_path, 'r') as file:\n        idx = os.path.basename(file_path).split('.txt')[0]\n        text_data[idx] = file.read()\n        \n# 1. delete spaces from texts ends\nfor key, value in text_data.items():\n    text_data[key] = value.rstrip()\n","e29e0666":"from transformers import LongformerTokenizerFast\ntokenizer = LongformerTokenizerFast.from_pretrained(config['model_name'])","614e453b":"data = []\n\nfor idx, text in tqdm(text_data.items()):\n    \n    # tokenize input text\n    inputs = tokenizer(text,\n                       add_special_tokens=True,\n                       return_offsets_mapping=True,\n                       return_length=True)\n        \n    # create token to word mapping\n    words = text.split()\n    \n    word_ids = [] # list to store token -> word mapping\n    word_pos = 0 # word strating position\n\n    tokens = inputs['input_ids'][1:-1] # exclude <s> and <\/s> tokens\n    \n    # current token positions (used for iteration)\n    start = 0\n    end = 1\n\n    for _ in tokens:\n\n        decoded_word = tokenizer.decode(tokens[start:end]).strip()\n\n        if decoded_word == '':\n            # if striped word is an empty string, that token doesn't belong to any word\n            word_ids.append(-1)\n            start += 1\n            end += 1\n            continue\n        \n        # no match\n        # continue adding tokens\n        if decoded_word != words[word_pos]:\n            end += 1\n            word_ids.append(word_pos)\n        # match    \n        else:\n            word_ids.append(word_pos)\n            start = end\n            end = start+1\n            word_pos += 1\n    \n    # add -1 position for the <s> and <\/s> tokens\n    word_ids = [-1] + word_ids + [-1]\n    \n    data.append([idx, inputs['input_ids'], inputs['attention_mask'], word_ids])","970061c5":"longformer_df = pd.DataFrame(data, columns=['id', 'input_ids', 'attention_mask', 'word_ids'])\nlongformer_df.head()","3a0c9c77":"# the number of tokens should be equal to the number of word_ids\n\n(longformer_df['input_ids'].apply(len) != longformer_df['word_ids'].apply(len)).any()","a7975e84":"Here, I am going to use the Longformer tokenizer.","1860e815":"### Token to Word - implementation\n\nMy strategy is to decode token by token, until a word is matched. Maybe this is not the most efficient way to solve this since I'm encoding and decoding back the text, but the whole process along with tokenization is finished in 02:34 min so that is good enough (for now!).\n\nTokens that don't belong to any word are give -1 position.\n\nI tested this with the Longformer tokenizer. I'm not sure for the other types of tokenizers.\n\nIf you have any questions or ideas for improvements, please let me know!","13b59ce3":"### Tokenize and get token to word mapping","62bfa4ee":"### Testing","775b77b1":"### Dataloading"}}