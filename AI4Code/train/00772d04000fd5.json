{"cell_type":{"5f7e76e4":"code","af567cd0":"code","6cd2a45b":"code","f4ac3262":"code","7e0f70fd":"code","1bc7aba3":"code","17c08bfb":"code","a7083d5f":"code","d08e6203":"code","03825fba":"code","5c4b8ac7":"code","249c0e14":"code","0f1c7489":"code","de15d3a1":"code","9b86f4cb":"code","8be1c7c3":"code","f7d8eaaa":"code","debc302b":"code","0d1a1539":"code","d9646389":"code","718a1d84":"code","136e7015":"code","700b8ff4":"code","98d660d0":"code","4b0bab2d":"code","eb5170c3":"code","0be9bc4a":"code","274fbfb0":"code","9cbadbec":"markdown","54ed556e":"markdown","e646f728":"markdown","a5b6ffd9":"markdown","d54b2fb9":"markdown","febfb9ee":"markdown","d3ec56ec":"markdown","7e9c0c90":"markdown","0683b37d":"markdown","eff2301d":"markdown","db77e3bc":"markdown","5703426f":"markdown","06fe67f6":"markdown","5288d290":"markdown","ccf73ad3":"markdown","6499c2c8":"markdown"},"source":{"5f7e76e4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import distributions, pearsonr\nfrom statsmodels.graphics.gofplots import qqplot\nfrom sklearn.feature_selection import mutual_info_classif\n%matplotlib inline","af567cd0":"def plot_formatting():\n    '''\n    Set up the default plotting settings.\n    '''\n    \n    plt.rc(\n        'figure',\n        figsize=(12,6),\n        titleweight='bold',\n        titlesize=25\n    )\n    plt.rc(\n        'axes',\n        labelweight='ultralight',\n        titleweight='ultralight',\n        titlelocation='left',\n        titlecolor='k',\n        titley=1.03,\n        titlesize=16,\n        grid=True\n    )\n    plt.rc(\n        'axes.spines',\n        right=False,\n        left=False,\n        top=False   \n    )\n    plt.rc(\n        'grid',\n        color='k',\n        linestyle=(0,15,2,0),\n        alpha=0.5\n    )\n    plt.rc('axes.grid', axis='y')\n    plt.rc('ytick.major', width=0)\n    plt.rc('font', family='monospace')\n    \nplot_formatting() # Setting our default settings","6cd2a45b":"train = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv')","f4ac3262":"def key_figures(train, test):\n    '''\n    Display key figures of datasets\n    '''\n    \n    fig, axes = plt.subplots(2,1, figsize=(5, 4))\n    fig.subplots_adjust(hspace=1.2)\n    \n    # Calculate key figures\n    train_n = train.shape[0]\n    test_n = test.shape[0]\n    m = train.shape[1]-1\n    train_total_cells = np.prod(train.shape)\n    train_perc_nan = (train.isnull().sum().sum()) * 100 \/ train_total_cells\n    test_total_cells = np.prod(test.shape)\n    test_perc_nan = (test.isnull().sum().sum()) * 100 \/ test_total_cells\n    perc_target = train['target'].mean() * 100\n    \n    # Hide axis\n    axes[0].axis('off')\n    axes[1].axis('off')\n    \n    axes[0].set_title('- Training set key figures', x=-0.7)\n    axes[1].set_title('- Test set key figures', x=-0.7)\n    \n    axes[0].text(0, 0.6, train_n, fontsize=18, fontweight=\"bold\", color='tomato', ha='center')\n    axes[0].text(0, 0.01, 'transactions in the dataset \\n(training examples)',\n                 fontsize=15, fontweight=\"bold\", c='grey', ha='center')\n    \n    axes[1].text(0, 0.6, test_n, fontsize=18, fontweight=\"bold\", color='steelblue', ha='center')\n    axes[1].text(0, 0.01, 'transactions in the dataset \\n(test examples)',\n                 fontsize=15, fontweight=\"bold\", c='grey', ha='center')\n    \n    axes[0].text(0.75, 0.6, m, fontsize=18, fontweight=\"bold\", color='tomato', ha='center')\n    axes[0].text(0.75, 0.1, 'features',fontsize=15, fontweight=\"bold\", c='grey', ha='center')\n    \n    axes[0].text(1.5, 0.6, str(int(perc_target)) + '%', fontsize=18, fontweight=\"bold\", color='tomato', ha='center')\n    axes[0].text(1.5, 0.01, 'of transactions with \\ntarget = 1',fontsize=15, fontweight=\"bold\", c='grey', ha='center')\n    \n    axes[0].text(2.25, 0.6, str(int(train_perc_nan)) + '%', fontsize=18, fontweight=\"bold\", color='tomato', ha='center')\n    axes[0].text(2.25, 0.1, 'of missing values',fontsize=15, fontweight=\"bold\", c='grey', ha='center')\n    \n    axes[1].text(0.75, 0.6, str(int(test_perc_nan)) + '%', fontsize=18, fontweight=\"bold\", color='steelblue', ha='center')\n    axes[1].text(0.75, 0.01, 'of missing values',fontsize=15, fontweight=\"bold\", c='grey', ha='center')","7e0f70fd":"train.head()","1bc7aba3":"train['target'].unique()","17c08bfb":"len(train['ID_code'].drop_duplicates()) == len(train)","a7083d5f":"len(test['ID_code'].drop_duplicates()) == len(test)","d08e6203":"key_figures(train, test)\nplt.gcf().suptitle('Unbalanced dataset with high dimensionality', x=0.48, y=1.1);","03825fba":"_, axes = plt.subplots(1, 2, sharey=True)\n\ntrain_count = sns.countplot(x=train.drop('target', axis=1).dtypes.astype(str), palette= ['grey', 'tomato'],\n                            linewidth=1, edgecolor='black', ax=axes[0])\n\ntest_count = sns.countplot(x=test.dtypes.astype(str), palette= ['grey', 'steelblue'],\n                           linewidth=1, edgecolor='black', ax=axes[1])\n\nplt.suptitle('All features except ID_code are numericals', x=0.45, y=1.07)\naxes[0].set_title('dtype dist for the training set')\naxes[1].set_title('dtype dist for the test set')\n\nfor p in train_count.patches:\n        train_count.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                             ha='center', va='bottom', color= 'tomato', fontsize=16)\nfor p in test_count.patches:\n        test_count.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                            ha='center', va='bottom', color= 'steelblue', fontsize=16)\n","5c4b8ac7":"uniqueness = train.loc[:, 'var_0':'var_199'].apply(lambda x: len(x.unique()) \/ len(train), axis=0)\nuniqueness","249c0e14":"uniqueness.describe()","0f1c7489":"fig, axes = plt.subplots(20, 10, figsize=(20, 50))\nfig.subplots_adjust(hspace=1.02, wspace=0.1)\n\nfor i, feature in enumerate(train.loc[:, 'var_0':'var_199']):\n    \n    plt.subplot(20,10,i+1)\n    sns.kdeplot(train[feature], label='Train')\n    sns.kdeplot(test[feature], label='Test')\n    plt.tick_params(axis='both', left=False, bottom=False, labelleft=False)\n    plt.ylabel('')\n    \naxes[0][0].legend()\nplt.gcf().suptitle('Probability Density Estimations of training and test sets for each variable',x=0.48, y=0.9);","de15d3a1":"%%capture output\n\n# Create the function that will compute the correlation coefficient of the Quantile-Quantile scatter plot of\n# a particular feature\n\ndef qq_corr(feature):\n    qq_graph = qqplot(data=feature, dist=distributions.norm)\n    qq_coord = qq_graph.gca().get_lines()[0].get_data()\n    plt.close()\n    return pearsonr(qq_coord[0], qq_coord[1])[0]\n\n\n# Apply this function to all features\nQQ_corr = train.loc[:, 'var_0':'var_199'].apply(qq_corr, axis=0)","9b86f4cb":"QQ_corr","8be1c7c3":"QQ_corr.describe()","f7d8eaaa":"fig, axes = plt.subplots(2, 2, figsize=(20, 8))\nfig.subplots_adjust(hspace=0.5)\n\nsorted_coef = QQ_corr.sort_values(ascending=False)\n\n\nsns.barplot(x=sorted_coef[:10], y=sorted_coef[:10].index, ax= axes[0][0], orient='h',\n            color='olive', edgecolor='black', linewidth=0.8, alpha=0.6)\nsns.barplot(x=sorted_coef[-10:], y=sorted_coef[-10:].index, ax= axes[0][1], orient='h',\n            color='gold', edgecolor='black', linewidth=0.8, alpha=0.6)\n\naxes[0][0].set_xlim(0.992, 1)\naxes[0][1].set_xlim(0.992, 1)\n\n\nsns.kdeplot(train[sorted_coef.index[0]], label=sorted_coef.index[0], ax=axes[1][0],\n            shade=True, color='olive', alpha=0.4, linewidth=2, edgecolor='black')\nsns.kdeplot(train[sorted_coef.index[-1]], label=sorted_coef.index[-1], ax=axes[1][1],\n            shade=True, color='gold', alpha=0.4, linewidth=2, edgecolor='black')\n\n\nplt.suptitle('Highest and lowest scores in the normality assessment process', x=0.4, y=1)\naxes[0][0].set_title('Features with the highest scores')\naxes[0][1].set_title('Features with the lowest scores')\naxes[1][0].set_title('KDE of \"var_67\"')\naxes[1][1].set_title('KDE of \"var_9\"');","debc302b":"sns.histplot(train.loc[:, 'var_0':'var_199'].mean(axis=0), bins=30, kde=True, label='Train', color='tomato', alpha=0.5)\nsns.histplot(test.loc[:, 'var_0':'var_199'].mean(axis=0), bins=30, kde=True, label='Test', color='steelblue', alpha=0.5)\nplt.xticks(np.arange(-10, 24, 2))\nplt.xlabel('Mean')\nplt.legend()\nplt.suptitle('Mean distribution over our 200 features', x=0.42);","0d1a1539":"sns.histplot(train.loc[:, 'var_0':'var_199'].std(axis=0), bins=30, kde=True, label='Train', color='tomato', alpha=0.5)\nsns.histplot(test.loc[:, 'var_0':'var_199'].std(axis=0), bins=30, kde=True, label='Test', color='steelblue', alpha=0.5)\nplt.xticks(np.arange(0, 22))\nplt.xlabel('Standard Deviation')\nplt.legend()\nplt.suptitle('Std distribution over our 200 features', x=0.42);","d9646389":"cov_matrix = np.cov(train.loc[:, 'var_0':'var_199'], rowvar=False)\n\nplt.figure(figsize=(15, 8))\nsns.heatmap(cov_matrix)\nplt.suptitle('Covariance matrix of features', x=0.3)\nplt.title('Features are clearly independent gaussians');","718a1d84":"fig, axes = plt.subplots(20, 10, figsize=(20, 50))\nfig.subplots_adjust(hspace=1.02, wspace=0.1)\n\nfor i, feature in enumerate(train.loc[:, 'var_0':'var_199']):\n    \n    plt.subplot(20,10,i+1)\n    sns.kdeplot(train[train.target == 0][feature], label='target=0')\n    sns.kdeplot(train[train.target == 1][feature], label='target=1')\n    plt.tick_params(axis='both', left=False, bottom=False, labelleft=False)\n    plt.ylabel('')\n    \naxes[0][0].legend()\nplt.gcf().suptitle('Probability Density Estimations when the target is known for each feature',x=0.48, y=0.9);","136e7015":"sns.catplot(y='features', x='value', hue='target', kind='box',\n            data=pd.melt(train[['target','var_81', 'var_13', 'var_139', 'var_174', 'var_76']],\n                         id_vars='target',\n                         var_name='features',\n                         value_name='value'))\n\nplt.gcf().set_size_inches(15, 8)\nplt.suptitle('Change in distribution when the target is known', x=0.45, y=1.1)\nplt.title('Visualization for 5 features of our dataset');","700b8ff4":"_, axes = plt.subplots(1,2)\n\nsns.kdeplot(train[train.target == 0].loc[:, 'var_0':'var_199'].mean(axis=0),\n            label='target = 0', shade=True, color='lightcoral',\n            alpha=1, linewidth=2, edgecolor='black', ax=axes[0])\n\nsns.kdeplot(train[train.target == 1].loc[:, 'var_0':'var_199'].mean(axis=0),\n            label='target = 1', shade=True, color='palegreen',\n            alpha=0.5, linewidth=2, edgecolor='black', ax=axes[0])\naxes[0].set_xlabel('Mean')\naxes[0].legend()\naxes[0].set_title('Mean distributions with known target', x=-0.12)\n\nsns.kdeplot(train[train.target == 0].loc[:, 'var_0':'var_199'].std(axis=0),\n            label='target = 0', shade=True, color='lightcoral',\n            alpha=1, linewidth=2, edgecolor='black', ax=axes[1])\n\nsns.kdeplot(train[train.target == 1].loc[:, 'var_0':'var_199'].std(axis=0),\n            label='target = 1', shade=True, color='palegreen',\n            alpha=0.5, linewidth=2, edgecolor='black', ax=axes[1])\naxes[1].set_xlabel('Standard Deviation')\naxes[1].legend()\naxes[1].set_title('Std distributions with known target', x=-0.12);","98d660d0":"X = train.loc[:, 'var_0':'var_199']\ny = train['target']","4b0bab2d":"mi_scores = mutual_info_classif(X, y, random_state=0)","eb5170c3":"mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nmi_scores","0be9bc4a":"mi_scores.describe()","274fbfb0":"(mi_scores == 0).sum()","9cbadbec":"## dtype distribution","54ed556e":"Features generally keep the same shape of distribution but, some changes are noticeable at the top of their curves. For instance, distributions of `var_81`, `var_13`, `var_139`, `var_174`, `var_76` and many others slightly differ depending on the target.","e646f728":"## Check the uniqueness of the entries in each feature column","a5b6ffd9":"## Change in distributions of mean and std when the target varies","d54b2fb9":"## Density estimation of each variable in training and test sets","febfb9ee":"Density estimation graphs of training and test points look similar for each variable. Our features all seem to be **sampled from Gaussian distributions**. Our system of features therefore appears to be sampled from a Multivariate gaussian disribution. \n\n\n## Normality assessment\n\nThe following cell will evaluate the normality of these distributions. To do so, **Quantile-Quantile plots** will be used. This will compare each distribution to the ideal theoretical normal distribution that corresponds to it, by comparing quantiles of the two distributions. The function defined below will calculate the pearson correlation coefficient of these Quantile-Quantile plots. The higher this coef is, the more likely the distribution is normal.\n\nNormality tests such as the Shapiro-Wilk test could be used but their p-values are usually insignificant when the sample size is greater than 5000 (in our case it is 200.000).","d3ec56ec":"The maximum value of percentages of unique entries per feature is 85% ! All features have duplicated values and actually **half of the features have around 50% of their entries that are duplicates** (from min to quantile 50% (median)).\n\nSome new useful features could be derived from these interesting findings.","7e9c0c90":"## Data Loading","0683b37d":"Logically, the distribution of `var_67` (which has the highest score here) looks more like a \"bell curve\" than that of `var_9` (which is noisy).","eff2301d":"`ID_code` in both train and test sets is a key column with unique string entries.","db77e3bc":"## EDA for the  Kaggle competition :  [Customer Transaction Prediction](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview) of SANTANDER\n---\n### Competition introduction\n*At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.*\n\n*Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?*\n\n*In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.*\n\n\n### Data\n- We are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string `ID_code` column.\n\n- The task is to predict the value of target column in the test set.\n\n- File descriptions\n\n    `train.csv` - the training set.\n    \n    `test.csv` - the test set. The test set contains some rows which are not included in scoring.\n    \n    `sample_submission.csv` - a sample submission file in the correct format.","5703426f":"The data varies between 0.992 and 0.999. It seems that all our rows are indeed samples from a multivariate gaussian distribution. Which features have the highest coefficients?","06fe67f6":"Summary statistics of correlation coefficients of qq plots for all features","5288d290":"## Mutual information of each variable with the target","ccf73ad3":"**Features are all anonymized and the target is a binary variable :**\n- Target = 1 means the customer made the transaction represented in this row (properties of the transactions are the anonymized variables).\n- Target = 0 means the transaction has not been made.","6499c2c8":"Individually, all features have poor MI scores with the target (the maximum value being 0.0042). **51 features have a zero MI score with the target**! Those features could be dropped in future."}}