{"cell_type":{"4d5a84e1":"code","5f9e0468":"code","d2ad10a7":"code","4e3bc747":"code","860833b8":"code","aaa90698":"code","d4d850ad":"code","88fd8067":"code","767cc87c":"code","57d4771a":"code","8c78d497":"code","03d7bf34":"code","33134cbd":"code","bf9fac83":"code","6e590567":"code","7a351cb0":"code","ba32f624":"code","0579c1a9":"code","ec62b125":"code","d62a7c2f":"code","395ef0de":"code","3c200835":"code","a2f36f30":"code","b69f3b4a":"code","4d96b4dd":"code","83cb9038":"code","42cd3f04":"code","16dbc592":"code","5c5c6e41":"code","e5a2ced0":"code","aae90be1":"code","7eafd6a7":"code","cd891ae4":"code","983fa860":"code","9f13cb8c":"code","3e57c247":"code","30eea2d2":"code","fc0871b9":"code","49e28259":"code","28982a26":"code","5537c3af":"code","1c0f784a":"code","6b849adc":"code","e817e317":"code","ce29712b":"code","efc7863c":"code","9c8da819":"code","d789104c":"code","34ad938d":"code","8b7d9462":"code","3e6c0c6f":"code","03b47e38":"code","a5dedb84":"code","38f76934":"code","9299a2bf":"code","cdd3068d":"markdown","e5090b19":"markdown","e1f80f14":"markdown","3ab99f09":"markdown"},"source":{"4d5a84e1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n\nimport os\nimport cv2\nfrom os import walk\nimport glob as gb\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5f9e0468":"SEED = 1000\nIMG_SIZE = 224\nBATCH_SIZE = 128\n\nTRAIN_DIR = '..\/input\/butterfly-images40-species\/butterflies\/train'\nVALID_DIR = '..\/input\/butterfly-images40-species\/butterflies\/valid'\nTEST_DIR = '..\/input\/butterfly-images40-species\/butterflies\/test'","d2ad10a7":"class_names = []\nclass_count = []\nTRAIN_EXAMPLES = 0\nfor folder in  os.listdir(TRAIN_DIR) : \n    files = gb.glob(pathname= str( TRAIN_DIR + '\/\/' + folder + '\/*.jpg'))\n    class_names.append(folder)\n    class_count.append(len(files))\n    TRAIN_EXAMPLES += len(files)\nplt.figure(figsize=(80,20))    \nsns.barplot(x = class_names, y=class_count).set_title(\"Distribution across classes in training set\")\nplt.show()\n\nprint(f'Total Train Examples = {TRAIN_EXAMPLES}')","4e3bc747":"class_names = []\nclass_count = []\nVALIDATION_EXAMPLES = 0\nfor folder in  os.listdir(VALID_DIR) : \n    files = gb.glob(pathname= str( VALID_DIR + '\/\/' + folder + '\/*.jpg'))\n    class_names.append(folder)\n    class_count.append(len(files))\n    VALIDATION_EXAMPLES += len(files)\n    \nplt.figure(figsize=(80,20)) \nsns.barplot(x = class_names, y=class_count).set_title(\"Distribution across classes in training set\")\nplt.show()\n\nprint(f'Total Train Examples = {VALIDATION_EXAMPLES}')","860833b8":"# diaplay 5 images from each class\nplt.figure(figsize=(10,100))\ni=0\nfor c in os.listdir(TRAIN_DIR):  \n    path = os.path.join(TRAIN_DIR,c)\n    for img in os.listdir(path):\n        img_array = cv2.cvtColor(cv2.imread(os.path.join(path,img)), cv2.COLOR_BGR2RGB) \n        plt.subplot(50,5,i+1)\n        plt.imshow(img_array)\n        if i%5 == 0:\n            plt.ylabel(c)\n        plt.xticks([])\n        plt.yticks([])\n        i += 1\n        if i%5 == 0:\n            break\n\nplt.tight_layout()        \nplt.show() ","aaa90698":"train_gen = ImageDataGenerator(\n    # set input mean to 0 over the dataset\n    featurewise_center=False,\n    # set each sample mean to 0\n    samplewise_center=False,\n    # divide inputs by std of dataset\n    featurewise_std_normalization=False,\n    # divide each input by its std\n    samplewise_std_normalization=False,\n    # apply ZCA whitening\n    zca_whitening=False,\n    # epsilon for ZCA whitening\n    zca_epsilon=1e-06,\n    # randomly rotate images in the range (deg 0 to 180)\n    rotation_range=30,\n    # randomly shift images horizontally\n    width_shift_range=0.2,\n    # randomly shift images vertically\n    height_shift_range=0.2,\n    # set range for random shear\n    shear_range=0.2,\n    # set range for random zoom\n    zoom_range=0.3,\n    # set range for random channel shifts\n    channel_shift_range=0.,\n    # set mode for filling points outside the input boundaries\n    fill_mode='nearest',\n    # value used for fill_mode = \"constant\"\n    cval=0.,\n    # randomly flip images\n    horizontal_flip=True,\n    # randomly flip images\n    vertical_flip=False,\n    # set rescaling factor (applied before any other transformation)\n    rescale=None,\n    # set function that will be applied on each input\n    preprocessing_function=tf.keras.applications.resnet50.preprocess_input,\n    # image data format, either \"channels_first\" or \"channels_last\"\n    data_format=None,\n    # fraction of images reserved for validation \n    # (strictly between 0 and 1)\n    validation_split=0.0,\n    # datatype\n    dtype=tf.float32,\n)\n\ntest_gen = ImageDataGenerator(\n    preprocessing_function= tf.keras.applications.resnet50.preprocess_input, \n    dtype=tf.float32\n)","d4d850ad":"train_batch = train_gen.flow_from_directory(\n    directory = TRAIN_DIR,\n    target_size = (IMG_SIZE,IMG_SIZE),\n    batch_size = BATCH_SIZE,\n    class_mode = 'sparse',\n    seed = SEED\n)","88fd8067":"# display 30 items  for a batch\nimgs, labels = next(train_batch)\ni = 0\nplt.figure(figsize=(20,7))\nfor img, label in zip(imgs, labels):\n    plt.subplot(3,10,i+1)\n    plt.imshow(img.astype('uint8'))\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel(class_names[label.astype('int32')])\n    i+=1\n    if i == 30:\n        break\nplt.tight_layout()\nplt.show()","767cc87c":"valid_batch = train_gen.flow_from_directory(\n    directory = VALID_DIR,\n    target_size = (IMG_SIZE,IMG_SIZE),\n    batch_size = BATCH_SIZE,\n    class_mode = 'sparse',\n    seed = SEED\n)","57d4771a":"test_batch = test_gen.flow_from_directory(\n    directory = VALID_DIR,\n    target_size = (IMG_SIZE,IMG_SIZE),\n    batch_size = BATCH_SIZE,\n    class_mode = None,\n    seed = SEED\n)","8c78d497":"def identity_block(X, filters):\n    f1, f2, f3 = filters\n    X_copy = X\n    \n    # 1st Layer\n    X = layers.Conv2D(filters=f1, kernel_size=(1,1), strides=(1,1), padding='valid')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    X = layers.Activation('relu')(X)\n    \n    # 2nd Layer\n    X = layers.Conv2D(filters=f2, kernel_size=(3,3), strides=(1,1), padding='same')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    X = layers.Activation('relu')(X) \n    \n    # 3rd Layer\n    X = layers.Conv2D(filters=f3, kernel_size=(1,1), strides=(1,1), padding='valid')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    \n    # Add the Skip COnnection\n    X = layers.Add()([X, X_copy])\n    X = layers.Activation('relu')(X)\n    \n    return X","03d7bf34":"def conv_blocks(X, filters, s=2):\n    f1, f2, f3 = filters\n    X_copy = X\n    \n    # 1st Layer\n    X = layers.Conv2D(filters=f1, kernel_size=(1,1), strides=(s,s), padding='valid')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    X = layers.Activation('relu')(X)\n    \n    # 2nd Layer\n    X = layers.Conv2D(filters=f2, kernel_size=(3,3), strides=(1,1), padding='same')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    X = layers.Activation('relu')(X) \n    \n    # 3rd Layer\n    X = layers.Conv2D(filters=f3, kernel_size=(1,1), strides=(1,1), padding='valid')(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    \n    ### match the dimension\n    X_copy = layers.Conv2D(filters=f3, kernel_size=(1,1), strides=(s,s), padding='valid')(X_copy)\n    X_copy = layers.BatchNormalization(axis=3)(X_copy)\n    \n    # Add the Skip COnnection\n    X = layers.Add()([X, X_copy])\n    X = layers.Activation('relu')(X)\n    \n    return X","33134cbd":"def ResNet50():\n    X_input = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    X = layers.ZeroPadding2D((3,3))(X_input)\n    \n    # Satge Conv1\n    X = layers.Conv2D(64, (7,7), strides=(2,2))(X)\n    X = layers.BatchNormalization(axis=3)(X)\n    X = layers.Activation('relu')(X) \n    X = layers.MaxPooling2D((3,3), strides=(2,2))(X)\n    \n    # stage Conv2_x\n    X = conv_blocks(X, filters=[64,64,256], s=1)\n    X = identity_block(X, filters=[64,64,256])\n    X = identity_block(X, filters=[64,64,256])\n    \n    # stage Conv3_x\n    X = conv_blocks(X, filters=[128,128,512], s=2)\n    X = identity_block(X, filters=[128,128,512])\n    X = identity_block(X, filters=[128,128,512])\n    X = identity_block(X, filters=[128,128,512])\n    \n    # stage Conv4_x\n    X = conv_blocks(X, filters=[256,256,1024], s=2)\n    X = identity_block(X, filters=[256,256,1024])\n    X = identity_block(X, filters=[256,256,1024])\n    X = identity_block(X, filters=[256,256,1024])\n    X = identity_block(X, filters=[256,256,1024])\n    X = identity_block(X, filters=[256,256,1024])\n    \n    # stage Conv5_x\n    X = conv_blocks(X, filters=[512,512,2048], s=2)\n    X = identity_block(X, filters=[512,512,2048])\n    X = identity_block(X, filters=[512,512,2048])\n    \n    \n    X = layers.AveragePooling2D((2,2))(X)\n    X = layers.Flatten()(X)\n    X = layers.Dense(50, activation='softmax', kernel_initializer='he_normal')(X)\n    \n    model = keras.Model(inputs=X_input, outputs=X, name='ResNet50')\n    \n    return model","bf9fac83":"model = ResNet50()\nmodel.summary()","6e590567":"def lr_schedule(epoch):\n    lr = 1e-3\n    if epoch > 40:\n        lr *= 0.5e-3\n    elif epoch > 30:\n        lr *= 1e-3\n    elif epoch > 20:\n        lr *= 1e-2\n    elif epoch > 10:\n        lr *= 1e-1\n    print('Learning rate: ', lr)\n    return lr\nlr_scheduler = LearningRateScheduler(lr_schedule)","7a351cb0":"model.compile(\n    optimizer=keras.optimizers.Adam(lr=lr_schedule(0)), \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy']\n)","ba32f624":"h = model.fit(train_batch, \n              epochs=50,\n              steps_per_epoch = TRAIN_EXAMPLES \/\/ BATCH_SIZE,\n              validation_data = valid_batch,\n              validation_steps = VALIDATION_EXAMPLES \/\/ BATCH_SIZE,\n              callbacks=lr_scheduler,\n              verbose=2\n             )","0579c1a9":"# VGGnet16","ec62b125":"from glob import glob\ntraining_dir=TRAIN_DIR\nvalidation_dir=VALID_DIR\nimage_files = glob(training_dir + '\/*\/*')\nvalid_image_files = glob(validation_dir + '\/*\/*')","d62a7c2f":"folders = glob(training_dir + '\/*')\nnum_classes = len(folders)\nprint ('Total Classes = ' + str(num_classes))","395ef0de":"from keras.models import Model\nfrom keras.layers import Flatten, Dense\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nIMG_SIZE=224\n\nIMAGE_SIZE = [IMG_SIZE, IMG_SIZE] \n\n\nvgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False) \n\n\nfor layer in vgg.layers:\n    layer.trainable = False\n\nx = Flatten()(vgg.output)\n\nx = Dense(num_classes, activation = 'softmax')(x)  \n\nmodel = Model(inputs = vgg.input, outputs = x)\n\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer='Adam', \n    metrics=['accuracy']\n)","3c200835":"model.summary()","a2f36f30":"from keras.applications.vgg16 import preprocess_input\n\ntraining_datagen = ImageDataGenerator(\n                                    rescale=1.\/255,\n                                    shear_range=0.2, \n                                    zoom_range=0.2,\n                                    horizontal_flip=False,\n                                    preprocessing_function=preprocess_input)\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255, preprocessing_function=preprocess_input)\n\ntraining_generator = training_datagen.flow_from_directory(\n    training_dir, \n    target_size = IMAGE_SIZE, \n    batch_size = BATCH_SIZE, \n    class_mode = 'sparse')\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir, \n    target_size = IMAGE_SIZE, \n    batch_size = BATCH_SIZE, \n    class_mode = 'sparse')","b69f3b4a":"training_images = 4955\/\/ BATCH_SIZE\nvalidation_images = 250\n\nhistory = model.fit(training_generator,\n                   steps_per_epoch = training_images,\n                   epochs = 50,\n                   validation_data = validation_generator,\n                   verbose=2\n                   )","4d96b4dd":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(h.history['accuracy'], '--', label='train accuracy')\nplt.plot(h.history['val_accuracy'], '--', label = 'validation accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.legend(loc='lower right')\n\nplt.subplot(1,2,2)\nplt.plot(h.history['loss'], '--', label='train loss')\nplt.plot(h.history['val_loss'], '--', label='validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend(loc='upper right')\n\nplt.show()","83cb9038":"CATEGORIES = []\nfor c in os.listdir(TRAIN_DIR):\n    CATEGORIES.append(c)\nprint (CATEGORIES)","42cd3f04":"from tqdm import tqdm\n\nIMG_SIZE = 224\ntrain_data=[]\nfor c in CATEGORIES:\n    path = os.path.join (TRAIN_DIR,c)\n    class_num = CATEGORIES.index(c)\n    for img in tqdm(os.listdir (path)):\n        try:\n            img_arr = cv2.imread(os.path.join(path,img))\n            img_resized = cv2.resize (img_arr,(IMG_SIZE, IMG_SIZE))\n            train_data.append ([img_resized, class_num])\n        except Exception as e:\n            pass\n\nprint (len(train_data))","16dbc592":"test_data=[]\nfor c in CATEGORIES:\n    path = os.path.join (TEST_DIR,c)\n    class_num = CATEGORIES.index(c)\n    for img in tqdm(os.listdir(path)):\n        try:\n            img_arr = cv2.imread(os.path.join(path,img))\n            img_resized = cv2.resize (img_arr,(IMG_SIZE, IMG_SIZE))\n            test_data.append ([img_resized, class_num])\n        except Exception as e:\n            pass\n\nprint (len(test_data))","5c5c6e41":"valid_data=[]\nfor c in CATEGORIES:\n    path = os.path.join (VALID_DIR,c)\n    class_num = CATEGORIES.index(c)\n    for img in tqdm(os.listdir(path)):\n        try:\n            img_arr = cv2.imread(os.path.join(path,img))\n            img_resized = cv2.resize (img_arr,(IMG_SIZE, IMG_SIZE))\n            valid_data.append ([img_resized, class_num])\n        except Exception as e:\n            pass\n\nprint (len(valid_data))","e5a2ced0":"import random\nrandom.shuffle (train_data)\nrandom.shuffle (test_data)\nrandom.shuffle (valid_data)","aae90be1":"X_train = []\nY_train = []\nfor img,label in train_data:\n    X_train.append (img)\n    Y_train.append (label)\n\nX_train = np.array(X_train).astype('float32').reshape (-1,IMG_SIZE,IMG_SIZE,3)\nY_train = np.array (Y_train)\n\nprint (X_train.shape, Y_train.shape)\nplt.figure()\nplt.imshow(X_train[2000].astype('uint8'))\nplt.ylabel(Y_train[2000])\nplt.show()","7eafd6a7":"X_test = []\nY_test = []\nfor img,label in test_data:\n    X_test.append (img)\n    Y_test.append (label)\n\nX_test = np.array(X_test).astype('float32').reshape (-1,IMG_SIZE,IMG_SIZE,3)\nY_test = np.array (Y_test)\n\nprint (X_test.shape, Y_test.shape)","cd891ae4":"X_valid = []\nY_valid = []\nfor img,label in valid_data:\n    X_valid.append (img)\n    Y_valid.append (label)\n\nX_valid = np.array(X_valid).astype('float32').reshape (-1,IMG_SIZE,IMG_SIZE,3)\nY_valid = np.array (Y_valid)\n\nprint (X_valid.shape, Y_valid.shape)","983fa860":"mean_rgb = np.mean(X_train, axis=(0,1,2))\nprint(mean_rgb)","9f13cb8c":"X_train_norm = np.empty_like(X_train)\nX_test_norm = np.empty_like(X_test)\nX_valid_norm = np.empty_like(X_valid)","3e57c247":"for i in range(len(X_train)):\n    for c in range(3):\n        if(c==1):\n            X_train_norm[i,:, :, c] = X_train[i,:, :, c] - mean_rgb[0]\n        elif(c==2):\n            X_train_norm[i,:, :, c] = X_train[i,:, :, c] - mean_rgb[1]\n        else:\n            X_train_norm[i,:, :, c] = X_train[i,:, :, c] - mean_rgb[2]\n            \nfor i in range(len(X_test)):\n    for c in range(3):\n        if(c==1):\n            X_test_norm[i,:, :, c] = X_test[i,:, :, c] - mean_rgb[0]\n        elif(c==2):\n            X_test_norm[i,:, :, c] = X_test[i,:, :, c] - mean_rgb[1]\n        else:\n            X_test_norm[i,:, :, c] = X_test[i,:, :, c] - mean_rgb[2]\n","30eea2d2":"c = 0\nplt.figure(figsize=(5,10))\nfor i in range(5):\n    plt.subplot(5,2,c+1)\n    plt.imshow(X_train[i].astype('uint8'))\n    plt.xticks([])\n    plt.yticks([])\n    \n    plt.subplot(5,2,c+2)\n    plt.imshow(X_train_norm[i].astype('uint8'))\n    plt.xticks([])\n    plt.yticks([])\n\n    c += 2\n    \nplt.tight_layout()\nplt.show()","fc0871b9":"SPEC_SHAPE = (IMG_SIZE, IMG_SIZE)\n\nCmodel = tf.keras.Sequential([\n    \n    # First conv block\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(SPEC_SHAPE[0], SPEC_SHAPE[1], 3)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    # Second conv block\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)), \n    \n    # Third conv block\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)), \n    \n    # Fourth conv block\n    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    \n    # Global pooling instead of flatten()\n    tf.keras.layers.GlobalAveragePooling2D(), \n    \n    # Dense block\n    #tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation='relu'),   \n    tf.keras.layers.Dropout(0.2),  \n    tf.keras.layers.Dense(512, activation='relu'),   \n    tf.keras.layers.Dropout(0.2),\n    # Classification layer\n    tf.keras.layers.Dense(50, activation='softmax')\n])\n\nCmodel.summary()","49e28259":"Cmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001), \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy']\n)","28982a26":"from tensorflow.keras.callbacks import EarlyStopping\n\n#cb = EarlyStopping(monitor='val_loss', min_delta=0.02, patience=3, restore_best_weights=True)\n\nh = Cmodel.fit(x=X_train_norm, y=Y_train, \n              epochs=50, \n              validation_data=(X_valid, Y_valid),\n              batch_size=BATCH_SIZE,\n              verbose=2,\n              #callbacks=[cb]\n             )","5537c3af":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(h.history['accuracy'], '--', label='train accuracy')\nplt.plot(h.history['val_accuracy'], '--', label = 'validation accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.legend(loc='lower right')\n\nplt.subplot(1,2,2)\nplt.plot(h.history['loss'], '--', label='train loss')\nplt.plot(h.history['val_loss'], '--', label='validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend(loc='upper right')\n\nplt.show()","1c0f784a":"import time\nimport functools\nimport IPython.display as display\nfrom pathlib import Path\nimport random\nfrom PIL import Image\nfrom matplotlib import pyplot\nimport matplotlib as mpl","6b849adc":"model = vgg\nfor i in range(len(model.layers)):\n    layer = model.layers[i]\n    if 'conv' not in layer.name:\n        continue\n    print(i, layer.name, layer.output.shape)","e817e317":"def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    \n    return img","ce29712b":"def imshow(image, title=None):\n    if len(image.shape) > 3:\n        image = tf.squeeze(image, axis=0)\n\n    plt.imshow(image)\n    if title:\n        plt.title(title)","efc7863c":"content_path = '..\/input\/butterfly-images40-species\/butterflies\/train\/peacock\/47.jpg'\ncontent_image = load_img(content_path)\nprint('Image shape:', content_image.shape)\nimshow(content_image, 'Content Image')","9c8da819":"plt.figure(figsize=(32,32))\n\nfilters, biases = model.layers[1].get_weights()\n\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) \/ (f_max - f_min)\n\nn_filters, ix = 32, 1\n\nfor i in range(n_filters):\n    f = filters[:, :, :, i]\n    for j in range(3):\n        ax = pyplot.subplot(14, 14, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        pyplot.imshow(f[:, :, j], cmap='gray')\n        ix += 1\npyplot.show()","d789104c":"plt.figure(figsize=(224,224))\n\nmodel = tf.keras.Model(inputs=[vgg.input], outputs=vgg.layers[1].output)\nmodel.summary()\n\ncontent_image = tf.keras.applications.vgg16.preprocess_input(content_image)\ncontent_image = tf.image.resize(content_image, (224, 224))\n\nfeature_maps = model.predict(content_image)\nsquare = 8\nix = 1\nfor _ in range(square):\n    for _ in range(square):\n        ax = pyplot.subplot(square, square, ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n        ix += 1\n        \npyplot.show()","34ad938d":"ixs = [2, 5, 10, 15]\noutputs = [vgg.layers[i].output for i in ixs]\nmodel = tf.keras.Model(inputs=[vgg.input], outputs=outputs)\nfeature_maps = model.predict(content_image)\n\nsquare = 8\nfor i, fmap in enumerate(feature_maps):\n    ix = 1\n    print(outputs[i])\n    plt.figure(figsize=(16,16))\n    for _ in range(square):\n        for _ in range(square):\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n            ix += 1\n    pyplot.show()","8b7d9462":"model = Cmodel\nmodel.summary()","3e6c0c6f":"img_path = '..\/input\/butterfly-images40-species\/butterflies\/train\/peacock\/65.jpg'\n\nfrom keras.preprocessing import image\nimport numpy as np\n\nimg = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor \/= 255.\n\nprint(img_tensor.shape)","03b47e38":"plt.imshow(img_tensor[0])\nplt.show()","a5dedb84":"from keras import models\n\nlayer_outputs = [layer.output for layer in model.layers[:8]]\nactivation_model = models.Model(inputs=Cmodel.input, outputs=layer_outputs)","38f76934":"activations = activation_model.predict(img_tensor)\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)\nplt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\nplt.show()","9299a2bf":"layer_names = []\nfor layer in Cmodel.layers[:8]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n\nfor layer_name, layer_activation in zip(layer_names, activations):\n\n    n_features = layer_activation.shape[-1]\n\n\n    size = layer_activation.shape[1]\n\n\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n\n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='gray')\n    \nplt.show()","cdd3068d":"# <-----Custom Model----->","e5090b19":"# Filter Visualization of VGG16","e1f80f14":"# Filter Visualization of our model","3ab99f09":"# **VGGnet16**"}}