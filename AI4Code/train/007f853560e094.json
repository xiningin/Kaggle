{"cell_type":{"974ac700":"code","404bceff":"code","025fdd2d":"code","9daab1b7":"code","ab60dfc2":"code","0037f2a8":"code","26fa4b61":"code","8938c0a5":"code","b2096e52":"code","7a419c63":"code","219e290d":"code","e2f4854d":"code","87c103c8":"code","61527531":"code","78192a23":"code","4ca2bf7a":"code","6390d3ce":"code","008281c1":"code","f7a95296":"markdown","c41f3eb4":"markdown","1033daf1":"markdown","2cd2e5ab":"markdown","ed8b7165":"markdown","d14bff9f":"markdown","837e7a85":"markdown","c00543e1":"markdown","626c1f89":"markdown","325e6bc2":"markdown","5e2bf0fb":"markdown","7caec7bd":"markdown","2121eebd":"markdown"},"source":{"974ac700":"%%capture\n!pip install ..\/input\/segmentation-models-wheels\/efficientnet_pytorch-0.6.3-py3-none-any.whl\n!pip install ..\/input\/segmentation-models-wheels\/pretrainedmodels-0.7.4-py3-none-any.whl\n!pip install ..\/input\/segmentation-models-wheels\/timm-0.3.2-py3-none-any.whl\n!pip install ..\/input\/segmentation-models-wheels\/segmentation_models_pytorch-0.1.3-py3-none-any.whl","404bceff":"import os\nimport torch\nimport random\nimport numpy as np\nimport torchvision\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nfrom PIL import Image\nfrom PIL import Image\nfrom sklearn import cluster\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset as Dataset\nfrom torch.utils.data import DataLoader as DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom albumentations import (HorizontalFlip, VerticalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n\ntorch.cuda.empty_cache()\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nsns.set_style(\"darkgrid\")","025fdd2d":"IMG_SIZE = (224,224)\nroot = \"..\/input\/sartorius-cell-instance-segmentation\/\"\n\ndef rle_decode(mask_rle, shape, color=1):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    \n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\ndef build_masks(df_train, image_id, input_shape):\n    height, width = input_shape\n    labels = df_train[df_train[\"id\"] == image_id][\"annotation\"].tolist()\n    mask = np.zeros((height, width))\n    for label in labels:\n        mask += rle_decode(label, shape=(height, width))\n    mask = mask.clip(0, 1)\n    return mask\n\ndef post_process(probability, threshold=0.5, min_size=300):\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = []\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            a_prediction = np.zeros((520, 704), np.float32)\n            a_prediction[p] = 1\n            predictions.append(a_prediction)\n    return predictions\n\n# Stolen from: https:\/\/www.kaggle.com\/arunamenon\/cell-instance-segmentation-unet-eda\n# Run-length encoding stolen from https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\n# Modified by me\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\ndef check_is_run_length(mask_rle):\n    if not mask_rle:\n        return True\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    start_prev = starts[0]\n    ok = True\n    for start in starts[1:]:\n        ok = ok and start > start_prev\n        start_prev = start\n        if not ok:\n            return False\n    return True\n\ndef create_empty_submission():\n    fs = os.listdir(\"..\/input\/sartorius-cell-instance-segmentation\/test\")\n    df = pd.DataFrame([(f[:-4], \"\") for f in fs], columns=['id', 'predicted'])\n    df.to_csv(\"submission.csv\", index=False)","9daab1b7":"class SatoriusDataset(torch.utils.data.Dataset):\n    def __init__(self, transforms, root = 'data\/',train = True):\n        self.root = root\n        self.transforms = transforms\n        self.w , self.h = 520 , 704\n        info = pd.read_csv(self.root+'train.csv')[[\"id\",\"annotation\"]]\n        info = info.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        validation =  30\n        self.data_info = 0\n\n        if train == True:\n            self.data_info = info[validation:].reset_index(drop=True)\n        else:\n            self.data_info = info[:validation].reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        img = Image.open(self.root+'train\/'+self.data_info['id'][idx]+'.png').convert(\"RGB\")\n        labels = self.data_info['annotation'][idx]\n        mask = np.zeros((self.w , self.h))\n        for label in labels:\n            mask = mask + rle_decode(label, shape=(self.w , self.h))\n        mask = mask.clip(0, 1)\n        mask = (mask >= 1).astype('float32')\n        augmented = self.transforms(image=np.array(img), mask=mask)\n        img = augmented['image']\n        mask = augmented['mask']\n        return img, mask.reshape((1, IMG_SIZE[0], IMG_SIZE[1]))\n\n    def __len__(self):\n        return len(self.data_info)\n\n\n    \ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(Resize(IMG_SIZE[0], IMG_SIZE[1]))\n    transforms.append(Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)))\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n        \n    transforms.append(ToTensorV2())\n    return Compose(transforms)\n\n\ntrain_dataset = SatoriusDataset(transforms=get_transform(train=True),root =root,train = True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nvalidation_dataset = SatoriusDataset(transforms=get_transform(train=False),root =root,train = False)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=30, shuffle=False)","ab60dfc2":"train_df = train_dataset.data_info\nvalidation_df = validation_dataset.data_info\n\npd.set_option('max_colwidth', 125)\ndisplay(train_df.head())\ndisplay(validation_df)","0037f2a8":"sns.set_palette(\"pastel\")\n\nn_annotations = [[],[]] #[[train], [validation]]\nfor i in train_df['annotation']:\n    n_annotations[0].append(len(i))\n    \nfor i in validation_df['annotation']:\n    n_annotations[1].append(len(i))\n\nx_axis,y_axis = 'Number Of Annotations' , 'Number Of Images'\n\nfig = plt.figure(figsize=(8,8))\np = sns.histplot(data = n_annotations[0])\np.set_xlabel(x_axis, fontsize = 15)\np.set_ylabel(y_axis, fontsize = 15)\n\n\nplt.show()\n\nfig = plt.figure(figsize=(8,8))\np = sns.histplot(data = n_annotations[1])\np.set_xlabel(x_axis, fontsize = 15)\np.set_ylabel(x_axis, fontsize = 15)\nplt.show()","26fa4b61":"import cv2\n\nimg, mask = next(iter(validation_dataloader))\nprint(f'Shape of Image : {img.shape} and Shape of Mask : {mask.shape}')\nimage = np.array(img[0])[0]\nmask = mask[0].reshape((IMG_SIZE[0], IMG_SIZE[1]))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image,cmap = 'plasma')\nplt.axis(\"off\")\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.imshow(mask,alpha = 0.3,cmap = 'plasma')\nplt.axis(\"off\")\n    \nplt.show()","8938c0a5":"def clustered_img(x):\n    kmeans = cluster.KMeans(2)\n    dims = np.shape(x)\n    pixel_matrix = np.reshape(x, (dims[0] * dims[1], dims[2]))\n    clustered = kmeans.fit_predict(pixel_matrix)\n    clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n    return clustered_img\n\nfor i in range(1):\n    fig = plt.figure(figsize=(10,10))\n    img, mask = next(iter(validation_dataloader))\n    img = img[0]\n    mask = mask[0].reshape((IMG_SIZE[0], IMG_SIZE[1]))\n    plt.imshow(clustered_img(img.numpy().transpose((1,2,0))))\n    plt.grid(None)\n    plt.title(\"K-Means\")\n    plt.show()\n\n    fig = plt.figure(figsize=(10,10))\n    plt.imshow(img.numpy()[0],cmap = 'plasma')\n    plt.imshow(mask, alpha=0.3)\n    plt.grid(None)\n    plt.title(\"Target\")\n    plt.show()","b2096e52":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/pytorch-pretrained-image-models\/resnet34.pth \/root\/.cache\/torch\/hub\/checkpoints\/resnet34-333f7ec4.pth\n\nimport torch\nimport collections.abc as container_abcs\ntorch._six.container_abcs = container_abcs\nimport segmentation_models_pytorch as smp","7a419c63":"import segmentation_models_pytorch as smp\nmodel = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)","219e290d":"device = 'cuda'\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n\nepoch = 27\nloss_history = [[],[]]\ntrain_n_minibatches = train_dataloader.__len__()\nvalidation_n_minibatches = validation_dataloader.__len__()","e2f4854d":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) \/ (iflat.sum() + tflat.sum() + smooth))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n\n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()\n    \ncriterion = MixedLoss(10.0, 2.0)","87c103c8":"model.train()\nlog_idx = 3\n\nfor e in range(epoch):\n    for batch_idx , (images ,masks) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        images, masks = images.to(device),  masks.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        loss_history[0].append(float(loss.detach()))\n        \n        if batch_idx % log_idx == 0:\n            # Printing Log\n            print(f'LOSS for EPOCH {e+1} BATCH {batch_idx+1}\/{train_n_minibatches} TRAIN LOSS : {loss_history[0][-1]}',end = ' ')\n            with torch.no_grad():\n                # Calculating loss and accuracy for validation\n                for _batch_idx_ , (images ,masks) in enumerate(validation_dataloader):\n                    images, masks = images.to(device),  masks.to(device)\n                    outputs = model(images)\n                    validation_loss = criterion(outputs, masks)\n                    loss_history[1].append(float(validation_loss.detach()))\n                                      \n                print(f'VALIDATION LOSS : {sum(loss_history[1][-1:-validation_n_minibatches-1:-1])\/validation_n_minibatches}')\n\n    torch.save(model.state_dict(),'unet_ss')\n    #Log for e+1th epoch\n    print(f'---------------------------------------EPOCH {e+1}-------------------------------------------')\n    print(f'Loss for EPOCH {e+1}  TRAIN LOSS : {sum(loss_history[0][-1:-train_n_minibatches-1:-1])\/train_n_minibatches}')\n    n_validation_losses = int(train_n_minibatches\/log_idx)*validation_n_minibatches\n    print(f'VALIDATION LOSS for EPOCH {e+1} : {sum(loss_history[1][-1:-1*n_validation_losses-1:-1])\/n_validation_losses}',end = '\\n')\n    print('---------------------------------------------------------------------------------------------')","61527531":"# Plotting Loss per epoch\nloss_per_epoch = [[],[]]\nfor i in range(epoch):\n    temp = 0\n    for j in loss_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n        temp = temp + j\n    loss_per_epoch[0].append(temp\/train_n_minibatches)\n    temp = 0\n    for j in loss_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n        temp = temp + j\n    loss_per_epoch[1].append(temp\/n_validation_losses)    \n\nsns.lineplot(x=range(len(loss_per_epoch[0])),y=loss_per_epoch[0])\nsns.lineplot(x=range(len(loss_per_epoch[1])),y=loss_per_epoch[1])\nplt.show()","78192a23":"# model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", activation=None)\n# model.to(device)\n# model.load_state_dict(torch.load('unet_ss', map_location='cuda'))\nwith torch.no_grad():\n    for _batch_idx_ , (images ,masks) in enumerate(validation_dataloader):\n        images, masks_target = images.to(device),  masks.to(device)\n        masks_out = model(images)\n\nimages = images.cpu().detach().numpy()\nmasks_out = masks_out.cpu().detach().numpy()\nmasks_target = masks_target.cpu().detach().numpy()\n\nfor i in range(1):\n    image = images[i][0]\n    mask_out = masks_out[i].reshape((IMG_SIZE[0], IMG_SIZE[1]))\n    mask_target = masks_target[i].reshape((IMG_SIZE[0], IMG_SIZE[1]))\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(image , cmap = 'bone')\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n    plt.show()\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(mask_target , cmap = 'bone')\n    plt.title(\"Ground Truth\")\n    plt.axis(\"off\")\n    plt.show()\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(mask_out , cmap = 'bone')\n    plt.title(\"Prediction\")\n    plt.axis(\"off\")\n    plt.show()","4ca2bf7a":"class TestDataset(Dataset):\n    def __init__(self, root,transforms = None):\n        self.root = root\n        self.transforms = transforms\n        self.img_name = []\n        for i in os.listdir(root+'test\/'):\n            self.img_name.append(i[:-4])\n        self.w , self.h = 520 , 704\n\n            \n    def __getitem__(self, idx):\n        image = Image.open(self.root+'test\/'+self.img_name[idx]+'.png').convert(\"RGB\")\n        if self.transforms is not None:\n            image = self.transforms(image=np.array(image))['image']\n        \n        return {'image': image, 'id': self.img_name[idx]}\n\n    def __len__(self):\n        return len(self.img_name)\n    \n\ntd = TestDataset(root, transforms=get_transform(train=False))\ntest_loader = DataLoader(td, batch_size=3, shuffle=False)","6390d3ce":"model.eval()\n\nsubmission = []\nfor i, batch in enumerate(test_loader):\n    preds = torch.sigmoid(model(batch['image'].cuda()))\n    preds = preds.detach().cpu().numpy()[:, 0, :, :] # (batch_size, 1, size, size) -> (batch_size, size, size)\n    for image_id, probability_mask in zip(batch['id'], preds):\n        try:\n            #if probability_mask.shape != IMAGE_RESIZE:\n            #    probability_mask = cv2.resize(probability_mask, dsize=IMAGE_RESIZE, interpolation=cv2.INTER_LINEAR)\n            probability_mask = cv2.resize(probability_mask, dsize=(704, 520), interpolation=cv2.INTER_LINEAR)\n            predictions = post_process(probability_mask)\n            for prediction in predictions:\n                #plt.imshow(prediction)\n                #plt.show()\n                try:\n                    submission.append((image_id, rle_encoding(prediction)))\n                except:\n                    print(\"Error in RL encoding\")\n        except Exception as e:\n            print(f\"Exception for img: {image_id}: {e}\")\n        \n        # Fill images with no predictions\n        image_ids = [image_id for image_id, preds in submission]\n        if image_id not in image_ids:\n            submission.append((image_id, \"\"))\n            \ndf_submission = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_submission.to_csv('submission.csv', index=False)\n\nif df_submission['predicted'].apply(check_is_run_length).mean() != 1:\n    print(\"Check run lenght failed\")\n    create_empty_submission()\n\n","008281c1":"df_submission.groupby('id')['predicted'].agg(lambda x: list(x)).reset_index()","f7a95296":"### **5.Training** <a class=\"anchor\" id=\"5\"><\/a>","c41f3eb4":"### **6.Plotting Graphs** <a class=\"anchor\" id=\"6\"><\/a>","1033daf1":"### **4.Initializing pre-trained model** <a class=\"anchor\" id=\"4\"><\/a>","2cd2e5ab":"### **7.Loading and Testing**<a class=\"anchor\" id=\"7\"><\/a>","ed8b7165":"#### 2.Visualizing Dataset <a class=\"anchor\" id=\"3.2\"><\/a>","d14bff9f":"## **Contents**\n-  [Introduction](#i)\n-  [1.Importing Libraries](#1)\n-  [2.Helper Functions](#2)\n-  [3.Dataset Managament](#3)\n    -  [3.1.Dataset and DataLoaders](#3.1)\n    -  [3.2.Visualizing Dataset](#3.2)\n-  [4.Initializing pre-trained model](#4)\n-  [5.Training](#5)\n-  [6.Plotting Graphs](#6)\n    -  [6.1.Plotting Loss vs Epoch](#6.1)\n-  [7.Loading and Testing](#7)\n-  [8.Predictions](#8)","837e7a85":"### **8.Predictions**<a class=\"anchor\" id=\"8\"><\/a>","c00543e1":"#### 1.Plotting Loss vs Epoch<a class=\"anchor\" id=\"6.1\"><\/a>","626c1f89":"#### 1.Downloading and Extracting Dataset <a class=\"anchor\" id=\"3.1\"><\/a>","325e6bc2":"## **Introduction** <a class=\"anchor\" id=\"i\"><\/a>\n\n\nIn this notebook I am visualizing the dataset and also training [U-Net](https:\/\/arxiv.org\/abs\/1505.04597) model using [Segmentation model](https:\/\/github.com\/qubvel\/segmentation_models.pytorch) library which is based on [Pytorch](https:\/\/pytorch.org\/) library.  \nResnet34 net is being utilized for the U-Net trained in this notebook.\n\nU-Net is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\u201cup-convolution\u201d) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\n\n\nRead more about U-Net [here](https:\/\/towardsdatascience.com\/unet-line-by-line-explanation-9b191c76baf5)\n\nSome of the helper functions is derived from [this](https:\/\/www.kaggle.com\/julian3833\/sartorius-starter-baseline-torch-u-net\/notebook#Model:-U-net) notebook.\n\n<img src= \"https:\/\/miro.medium.com\/max\/953\/1*Z98NhzbVISHa4CoemZS4Kw.png\"  style='width: 700px;'>","5e2bf0fb":"### **3.Dataset Managament** <a class=\"anchor\" id=\"3\"><\/a>","7caec7bd":"### **1.Importing Libraries** <a class=\"anchor\" id=\"1\"><\/a>","2121eebd":"### **2.Helper Functions** <a class=\"anchor\" id=\"2\"><\/a>"}}