{"cell_type":{"be750013":"code","083b71fc":"code","65049257":"code","05220988":"code","82ba0aab":"code","35883436":"code","66f215e2":"code","444a6f01":"code","217663dc":"code","65d11162":"code","e9084be4":"code","a009bd5d":"code","00865de8":"code","a9f68e56":"code","383847f6":"code","0933b9a8":"code","b3d9779d":"code","fcb77e6b":"code","6f854dce":"code","bff86e40":"code","d78e7a43":"code","44c28e39":"code","dcfca9b9":"markdown","1318f4ae":"markdown","fc0ee44f":"markdown","335fea37":"markdown","0aaec580":"markdown","0a996230":"markdown","fe0a6944":"markdown","0752b7cb":"markdown","abb5a307":"markdown"},"source":{"be750013":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport math\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","083b71fc":"case = 401\ndataset = pd.read_csv(\"..\/input\/tglf-new-coefficients\/TAML_%s.csv\"%case, skiprows=1)#TAML_227_actually_logged.csv\")\n\nwith open(\"..\/input\/tglf-new-coefficients\/TAML_%s.csv\"%case, 'r') as f:\n    content = f.readlines()\n    header = content[0]\n    print(header)\n\ndataset= dataset.dropna()\n#dataset = dataset[dataset['cc'] > 0.3]\n\ndataset.shape\ndataset.head()","65049257":"x = dataset.iloc[0:, 0:16]\n# x[\"RLTS_1^2\"]=x[\"RLTS_1\"]**2\n\n#Make compatible with GK DB scans, so that I can check if TGLF error relative to experiment is \n#due to TGLF not matching GK or GK not matching experiment: Can I calibrate error to GK, and then compare well to expt?\n# if yes, then error is between TGLF and GK, if no, then error is between GK and experiment\n'''x=x.drop(\"DEBYE\", axis=1)\nx=x.drop(\"P_PRIME_LOC\", axis=1)\nx=x.drop(\"VEXB_SHEAR\", axis=1)\nx=x.drop(\"XNUE\", axis=1)\nx=x.drop(\"VPAR_1\", axis=1)\nx=x.drop(\"RMAJ_LOC\", axis=1)\nx=x.drop(\"ZEFF\", axis=1)\n'''\n#inverse\n#inv_x = x.join((1\/(x+0.001)).add_prefix('inv_'))\n#x = inv_x\n\ny = dataset.iloc[:, 16:17]\n\n# for i in range(len(x.columns)):\n#     #new_col = \"log10(abs(\"+x.columns[i]+\"))\"\n#     x =x.rename(columns={x.columns[i]:new_col})\n    \nx.head()","05220988":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 2)\n\ny_cols = list(y.columns)\nx_cols = list(x.columns)\n\nprint(x_cols)\nprint(y_cols)","82ba0aab":"from xgboost import XGBRegressor\n\nprint(y_cols)\nfor i in y_cols:\n    xgb = XGBRegressor()\n    xgb.fit(x_train, y_train[i])\n\n    y_pred = xgb.predict(x_test)\n\n    from sklearn.metrics import r2_score\n    print(i,r2_score(y_test[i],y_pred))\n    \n    cost = sum((y_pred - y_test[i])**2) \/ sum((y_test[i] - np.mean(y_pred))**2)\n    print(\"Cost\",cost)","35883436":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, HuberRegressor, SGDRegressor, Ridge, Lasso, ElasticNet\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n# max_r_score = 0\ndef poly_train(x_train, x_test, y_train, y_test, degree, graph=False, coeff='a'):\n    poly = PolynomialFeatures(degree = degree)\n    x_poly = poly.fit_transform(x_train)\n    poly.fit(x_poly, y_train)\n    \n    #increase iterations, epsilon\n    model = LinearRegression()#HuberRegressor(epsilon=1.0,max_iter=1000,alpha=0.05)\n    model.fit(x_poly, y_train)\n\n    y_pred = model.predict(poly.fit_transform(x_test))\n    \n    from sklearn.metrics import r2_score, mean_squared_error\n    r_score = r2_score(y_pred,y_test)\n    mse = mean_squared_error(y_pred,y_test)\n    cost = sum(abs(y_pred - y_test))\n    \n    if graph:\n        max_r_score = r_score\n        print(\"L1 Error: \",cost)\n\n        print(\"R-score: \",r_score)\n        \n        print(\"MSE: \",mse)\n\n        coefs = list(model.coef_)[1:]\n        intercept = str(model.intercept_)\n        features = poly.get_feature_names(x_train.columns)[1:]\n        for feat in range(len(features)):\n            split_feats=features[feat].split(' ')\n            if len(split_feats) > 1:\n                features[feat]=split_feats[0]+'*'+split_feats[1]\n            if \"^\" in features[feat]:\n                features[feat]=features[feat][:-2]+\"**2\"\n            \n        equation = \"=10**(\"\n        for i in range(len(coefs)):\n            equation += str(coefs[i])+\"*\"+features[i] +\" + \" #\"& \\n\"\n        equation += intercept\n        equation += \")\"\n        print(\"Equation: \",equation)\n        plt.rcParams['mathtext.fontset'] = 'stix'\n        plt.rcParams['font.family'] = 'STIXGeneral'\n        plt.rcParams.update({'font.size': 15})\n        \n        plt.hist2d(y_test, y_pred, bins=np.linspace(-1,2,80), cmap='viridis', norm=matplotlib.colors.LogNorm())\n        \n        #plt.xlim(-1.5,2)\n        #plt.ylim(-1.5,2)\n        if coeff=='a' or coeff=='eps':\n            plt.xlim(-2, 1.5)\n            plt.ylim(-2, 1.5)\n            plt.hist2d(y_test, y_pred, bins=np.linspace(-2,1.5,75), cmap='viridis', norm=matplotlib.colors.LogNorm())\n            \n        elif coeff=='c':\n            plt.xlim(-1.5, 1.5)\n            plt.ylim(-1.5, 1.5)\n            plt.hist2d(y_test, y_pred, bins=np.linspace(-1.5,1.5,75), cmap='viridis', norm=matplotlib.colors.LogNorm())\n            \n        elif coeff=='b':\n            plt.xlim(-1.5, 1.5)\n            plt.ylim(-1.5, 1.5)\n            plt.hist2d(y_test, y_pred, bins=np.linspace(-1.5,1.5,75), cmap='viridis', norm=matplotlib.colors.LogNorm())\n        \n        \n        if coeff in ['a','b','c']:\n            plt.ylabel(r'$\\log_{10}(%s)$'%coeff)\n            plt.xlabel(r'$\\log_{10}(%s_{,NN})$'%coeff)\n            plt.title(r'%s'%coeff)\n        else:\n            plt.ylabel(r'$\\log_{10}(\\epsilon_{\\mathrm{SAT1}})$')\n            plt.xlabel(r'$\\log_{10}(\\epsilon_{\\mathrm{SAT1,NN}})$')\n            \n        plt.plot([-2, 2],[-1.5, 1.5], color='r', linewidth=2)\n        #plt.show()\n        plt.savefig('%s_analytic_vs_NN_%s.pdf'%(coeff,case), bbox_inches='tight')\n    return r_score","66f215e2":"max_r_score = 0\n\nprint(\"aa\")\npoly_train(x_train, x_test, y_train['aa'], y_test['aa'], 1, graph=True,coeff='eps')\n\n'''\nprint(\"bb\")\npoly_train(x_train, x_test, y_train['bb'], y_test['bb'], 1, graph=True, coeff='b')\n\nprint(\"cc\")\npoly_train(x_train, x_test, y_train['cc'], y_test['cc'], 1, graph=True, coeff='c')\n'''","444a6f01":"import itertools\ndef feature_selection(coefficient):\n    columns = x.columns.tolist() \n    print(columns)\n    features = []\n    complexity = []\n    scores = []\n    \n    print(\"Best combination\")\n    selected_features_best = ['0']\n    tmp_features = []\n    for k in range(1,3):\n        print(\"Trying\",str(k))\n        combinations = itertools.combinations(columns, k)\n        max_r_score = -1e9\n        for combination in combinations:\n            r_score = poly_train(x_train[list(combination)], x_test[list(combination)], y_train[coefficient], y_test[coefficient], 1)\n            if r_score > max_r_score:\n                max_r_score = r_score\n                features = combination\n        scores.append(1-max_r_score)\n        complexity.append(k)\n        print(features)\n        selected_features_best.append(features)\n        print('1-max_r_score',1-max_r_score)\n    \n    print(\"\\nSequentially removed features\")\n    scores_sequentially_subtracted = []\n    feature = \"\"\n    selected_features_subtraction=[]\n    for k in range(1,3):\n        print(\"Trying with -\",k,\"features\")\n        max_r_score = -1e10\n        for col in columns:\n            cols = set(columns) - set([col])\n            r_score = poly_train(x_train[cols], x_test[cols], y_train[coefficient], y_test[coefficient], 1)\n            if r_score > max_r_score:\n                max_r_score = r_score\n                feature = col\n        scores_sequentially_subtracted.insert(0,1-max_r_score)\n        selected_features_subtraction.append(feature)\n        columns.remove(feature)\n        print(sorted(columns))\n        print('1-max_r_score',1-max_r_score)\n    \n    print(\"\\nSequentially added features\")\n    columns = x.columns.tolist() \n    complexity_add= []\n    \n    scores_sequentially_added = []\n    feature = \"\"\n    selected_features_added=[]\n    for k in range(1,16):\n        print(\"Trying with\",k,\"features\")\n        max_r_score = -1e9\n        for col in columns:\n            cols = selected_features_added + [col]\n            r_score = poly_train(x_train[cols], x_test[cols], y_train[coefficient], y_test[coefficient], 1)\n            \n            if r_score > max_r_score:\n                max_r_score = r_score\n                feature = col\n        scores_sequentially_added.append(1-max_r_score)\n        selected_features_added.append(feature)\n        columns.remove(feature)\n        complexity_add.append(k)\n        print(sorted(selected_features_added))\n        print('1-max_r_score',1-max_r_score)\n\n    return (coefficient,complexity,scores,scores_sequentially_subtracted,scores_sequentially_added,\n            selected_features_best,selected_features_subtraction,selected_features_added,complexity_add)","217663dc":"(coefficient1,complexity1,scores1,scores_sequentially_subtracted1,scores_sequentially_added1,\n selected_features_best1,selected_features_subtraction1,selected_features_added1,complexity_add1)=feature_selection(\"aa\")","65d11162":"from matplotlib.ticker import ScalarFormatter\nimport matplotlib.ticker as plticker\nfrom matplotlib.ticker import MultipleLocator\n\nplt.rcParams['mathtext.fontset'] = 'stix'\nplt.rcParams['font.family'] = 'STIXGeneral'\nplt.rcParams.update({'font.size': 15})\n\nfig = plt.figure()\nax=plt.subplot(111)\n\nax.yaxis.set_minor_locator(plticker.MultipleLocator(base=0.5))\nax.xaxis.set_minor_locator(plticker.MultipleLocator(base=1))\nax.xaxis.set_major_locator(plticker.MultipleLocator(base=1))\n\nax.set_title(r'Error vs. additional plasma parameters in $\\epsilon_{\\mathrm{SAT1}}$')\nax.set_ylabel(r\"$1-R^2$\")\n#ax.set_xlabel(r\"Number of Terms\")\n\n#ax.plot(complexity1,scores_sequentially_subtracted1, color='red', marker='o', label=\"Sequentially removed features\")\nax.plot(complexity_add1,scores_sequentially_added1, color='b', marker='o', label=\"Sequentially added features\")\n##ax.plot(complexity1,scores1, color='blue', marker='o', label=\"Best combination\")\nprint('selected_features_best1',list(selected_features_best1[-1]))\nprint('scores_sequentially_added1',selected_features_added1)\nlabels=list(selected_features_added1)\nprint('lables',labels.insert(0,['0']))\nax.set_xticklabels(labels,rotation = 45)\nax.set_xticklabels([r'0',r\"p'\", r'$a\/Ln_i$', 'q', '$a\/LT_e$', r'$\\nu_{ei}$', \n                    r'$a\/Ln_e$', r'$\\partial v_{E\\times B}\/\\partial r$', r'$R_{maj}\/a$', r\"q'\", r'$Z_{eff}$', \n                    r'$\\lambda_D$', r'$\\kappa$', r'$a\/LT_i$', r'$V_{\\parallel}$', r'$\\delta$'], rotation =90)\n#ax.set_xticklabels(ax.get_xticks(), rotation = 45)\nax.tick_params(axis='x', which='major', labelsize=20)\n\nprint(scores1)\n#ax.legend(loc='upper right')\nplt.savefig('a_error_vs_complexity_%s.pdf'%case, bbox_inches='tight')","e9084be4":"fig = plt.figure()\nax=plt.subplot(111)\n\nax.yaxis.set_minor_locator(plticker.MultipleLocator(base=0.5))\nax.xaxis.set_minor_locator(plticker.MultipleLocator(base=1))\nax.xaxis.set_major_locator(plticker.MultipleLocator(base=1))\n\nax.set_title(r'Error vs. complexity in $\\epsilon_{\\mathrm{SAT1}}$')\nax.set_ylabel(r\"$1-R^2$\")\nax.set_xlabel(r\"Number of Terms\")\n\nax.plot(complexity1,scores_sequentially_subtracted1, color='red', marker='o', label=\"Sequentially removed features\")\nax.plot(complexity1,scores_sequentially_added1, color='green', marker='o', label=\"Sequentially added features\")\nax.plot(complexity1,scores1, color='blue', marker='o', label=\"Best combination\")\nax.legend(loc='upper right')\nplt.savefig('a_seq_error_vs_complexity_%s.pdf'%case, bbox_inches='tight')","a009bd5d":"#feature_selection(\"bb\")","00865de8":"#(coefficient3,complexity3,scores3,scores_sequentially_subtracted3,scores_sequentially_added3)=feature_selection(\"cc\")","a9f68e56":"'''from matplotlib.ticker import ScalarFormatter\nimport matplotlib.ticker as plticker\nfrom matplotlib.ticker import MultipleLocator\n\nplt.rcParams['mathtext.fontset'] = 'stix'\nplt.rcParams['font.family'] = 'STIXGeneral'\nplt.rcParams.update({'font.size': 15})\n\nfig = plt.figure()\nax=plt.subplot(111)\n\nax.yaxis.set_minor_locator(plticker.MultipleLocator(base=0.5))\nax.xaxis.set_minor_locator(plticker.MultipleLocator(base=1))\nax.xaxis.set_major_locator(plticker.MultipleLocator(base=1))\n\nax.set_title(r'Error vs. complexity in $c$')\nax.set_ylabel(r\"$1-R^2$\")\nax.set_xlabel(r\"Number of Terms\")\n\n#ax.plot(complexity3,scores_sequentially_subtracted3, color='red', marker='o', label=\"Sequentially removed features\")\n#ax.plot(complexity3,scores_sequentially_added3, color='green', marker='o', label=\"Sequentially added features\")\nax.plot(complexity3,scores3, color='blue', marker='o', label=\"Best combination\")\n#ax.legend(loc='upper right')\nplt.savefig('c_error_vs_complexity.pdf', bbox_inches='tight')\n\nfig = plt.figure()\nax=plt.subplot(111)\n\nax.yaxis.set_minor_locator(plticker.MultipleLocator(base=0.5))\nax.xaxis.set_minor_locator(plticker.MultipleLocator(base=1))\nax.xaxis.set_major_locator(plticker.MultipleLocator(base=1))\n\nax.set_title(r'Error vs. complexity in $c$')\nax.set_ylabel(r\"$1-R^2$\")\nax.set_xlabel(r\"Number of Terms\")\n\nax.plot(complexity3,scores_sequentially_subtracted3, color='red', marker='o', label=\"Sequentially removed features\")\nax.plot(complexity3,scores_sequentially_added3, color='green', marker='o', label=\"Sequentially added features\")\nax.plot(complexity3,scores3, color='blue', marker='o', label=\"Best combination\")\nax.legend(loc='upper right')\nplt.savefig('c_seq_error_vs_complexity.pdf', bbox_inches='tight')\n'''","383847f6":"#chosen_features_aa = ['P_PRIME_LOC', 'Q_LOC', 'RLNS_2', 'RLTS_1', 'RLTS_2', 'XNUE']\n#chosen_features_aa = ['DEBYE', 'RLNS_2', 'RLTS_1', 'RLTS_2', 'XNUE']\n#chosen_features_aa = ['RLNS_2','RLNS_1', 'RLTS_1', 'RLTS_2', 'TAUS_2','Q_LOC','KAPPA_LOC','Q_PRIME_LOC']\n#GK db best params:\n#chosen_features_aa = ['KAPPA_LOC', 'Q_LOC', 'Q_PRIME_LOC', 'RLNS_1', 'RLTS_1', 'RLTS_2']\n#GK db best params case 401:\n#chosen_features_aa = ['DELTA_LOC', 'KAPPA_LOC', 'Q_LOC', 'Q_PRIME_LOC', 'RLNS_1', 'RLNS_2', 'RLTS_2', 'TAUS_2']\n#All vars case 401:\n#chosen_features_aa = ['P_PRIME_LOC', 'Q_LOC', 'RLNS_1', 'RLNS_2', 'RLTS_1', 'XNUE']\n#4 vars case 417:\nchosen_features_aa = ['DEBYE', 'P_PRIME_LOC', 'RLNS_2', 'RLTS_1', 'RMAJ_LOC', 'XNUE', 'ZEFF']\npoly_train(x_train[list(chosen_features_aa)], x_test[list(chosen_features_aa)], y_train['aa'], y_test['aa'], 1,True, 'eps')","0933b9a8":"#chosen_features_bb = ['DEBYE', 'DELTA_LOC', 'Q_PRIME_LOC', 'RLNS_2', 'RLTS_1', 'VPAR_1']\n#poly_train(x_train[list(chosen_features_bb)], x_test[list(chosen_features_bb)], y_train['bb'], y_test['bb'], 1,True)","b3d9779d":"#chosen_features_cc = ['RLNS_2', 'RLTS_1', 'RLTS_2']\n#chosen_features_cc = ['DEBYE','RLNS_2','RLNS_1', 'RLTS_1', 'RLTS_2', 'XNUE']\n#chosen_features_cc = ['P_PRIME_LOC', 'Q_LOC', 'RLNS_1', 'RLNS_2', 'RLTS_1', 'RLTS_2', 'XNUE']\n#chosen_features_cc = ['RLNS_2','RLNS_1', 'RLTS_1', 'RLTS_2', 'TAUS_2','Q_LOC','KAPPA_LOC','Q_PRIME_LOC']\n#GYRO DB compatible params\n#chosen_features_cc = ['KAPPA_LOC', 'RLNS_1', 'RLNS_2', 'RLTS_1', 'RLTS_2', 'TAUS_2']\n#chosen_features_cc = ['Q_LOC', 'Q_PRIME_LOC', 'RLNS_1', 'RLNS_2', 'RLTS_1', 'RLTS_2', 'TAUS_2']\n#poly_train(x_train[list(chosen_features_cc)], x_test[list(chosen_features_cc)], y_train['cc'], y_test['cc'], 1,True, 'c')\n","fcb77e6b":"from sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest\nimport matplotlib.pyplot as plt\n\ndef getFeatures(column):\n    #Extra Trees\n    np.seterr(divide='ignore', invalid='ignore')\n    feature_tree = ExtraTreesRegressor()\n    feature_tree.fit(x,y[column])\n\n    feat_importances_tree = pd.Series(feature_tree.feature_importances_, index=x.columns)\n    tree_features = feat_importances_tree.nlargest(9)\n#     tree_features.plot(kind='barh')\n#     plt.show()\n    \n    #KBest\n    KBest = SelectKBest(k='all')\n    feature_kbest = KBest.fit(x,y[column])\n    feat_importances_kbest = pd.Series(feature_kbest.scores_, index=x.columns)\n    kbest_features = feat_importances_kbest.nlargest(9)\n#     kbest_features.plot(kind='barh')\n#     plt.show()\n    return kbest_features.keys(), tree_features.keys()\n\ngetFeatures(\"aa\")","6f854dce":"def generateEquations(complexity, column, degrees):\n    features = 13-complexity\n    kbest_feats, tree_feats = getFeatures(column)\n\n    kbest_cols = []\n    for i in kbest_feats[:-features]:\n        kbest_cols.append(i)\n        for degree in degrees:\n            poly_train(x_train[kbest_cols], x_test[kbest_cols], y_train[column], y_test[column], degree, graph=True)\n            \n    tree_cols = []\n    for j in tree_feats[:-features]:\n        tree_cols.append(j)\n        for degree in degrees:\n            poly_train(x_train[tree_cols], x_test[tree_cols], y_train[column], y_test[column], degree, graph=True)","bff86e40":"#aa\n#generateEquations(6, \"aa\", [1])","d78e7a43":"#bb\n#generateEquations(8, \"bb\", [1])","44c28e39":"#cc\n#generateEquations(12, \"cc\", [1])","dcfca9b9":"# PolyTrain Function","1318f4ae":"# Models with less complexity and printed coefficients","fc0ee44f":"XGBoost (for base accuracy)","335fea37":"# Preprocessing","0aaec580":"# Feature Selection Function","0a996230":"Now let's take a closer look at the features we chose from the output of the feature_selection function","fe0a6944":"Base polynomial accuracy","0752b7cb":"Getting Coefficients of A Polynomial (it seems degree=2 produces the most accurate results)","abb5a307":"Bruteforce Combination of all variables"}}