{"cell_type":{"7f0a4f01":"code","dab848f5":"code","0d8f5a20":"code","4489c4ba":"code","f8b34af6":"code","6faa5f51":"code","2b4bc66c":"code","90a92ff5":"code","09ea8e2a":"code","b3fff273":"code","b5e8267f":"code","cdc97423":"code","e74252bb":"code","53e5256f":"code","49bcda25":"code","a3669eee":"code","01567845":"code","55940152":"code","a12941d5":"code","a8a3aa66":"code","26fab4f6":"code","19f4a4df":"code","1517c704":"code","00e9f4f6":"code","4b01a577":"code","252ffaee":"code","72d2d6b9":"code","44ac2e5b":"code","bb82c9db":"code","7d9fd6e0":"code","59ec53ee":"code","861bfa8e":"code","e1f3b873":"code","155092ee":"code","a673112c":"code","82d39cb6":"code","0d703215":"code","f20c42b9":"code","7bd2aacf":"code","74f90fda":"code","f87c2adf":"code","5f225495":"code","a1a7301c":"markdown","da1e96b5":"markdown","eefd5854":"markdown","082155fe":"markdown","d2e761ea":"markdown","ccb965c8":"markdown","9fe4189e":"markdown","0e0bde04":"markdown","4136f861":"markdown","afd83563":"markdown"},"source":{"7f0a4f01":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom tensorflow.keras import layers, losses, callbacks, utils\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model, Sequential","dab848f5":"(x_train, _), (x_test, _) = fashion_mnist.load_data()\n\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.\n\nprint (x_train.shape)\nprint (x_test.shape)","0d8f5a20":"class config():\n    INPUT_SHAPE = (28, 28)\n    HIDDEN_DIMS = [244]\n    LATENT_DIM = 64\n    \n    MAX_EPOCHS = 100\n    ","4489c4ba":"class BasicAutoencoder(Model):\n  def __init__(self, input_shape, hidden_dims, latent_dim):\n    super(BasicAutoencoder, self).__init__()\n    self.latent_dim = latent_dim   \n    \n    encoder_layers = [layers.Dense(hid_dim, activation='relu') for hid_dim in hidden_dims]\n    encoder_layers.append(layers.Dense(latent_dim, activation='relu'))\n    encoder_layers.insert(0, layers.Flatten())\n    \n    self.encoder = Sequential(encoder_layers)\n    \n    \n    decoder_layers = [layers.Dense(hid_dim, activation='relu') for hid_dim in hidden_dims[::-1]]\n    decoder_layers.append(layers.Dense(input_shape[0]*input_shape[1], activation='sigmoid'))\n    decoder_layers.append(layers.Reshape(input_shape))\n    \n    self.decoder = Sequential(decoder_layers)\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = BasicAutoencoder(config.INPUT_SHAPE, config.HIDDEN_DIMS, config.LATENT_DIM)\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')","f8b34af6":"es = callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, mode='min', baseline=None,\n    restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, mode='min', verbose=1\n)\n\nhistory =  autoencoder.fit(\n    x_train, x_train, validation_data=(x_test, x_test), shuffle=True,\n    callbacks=[es, rlp], epochs=config.MAX_EPOCHS\n)","6faa5f51":"autoencoder.encoder.summary()\nutils.plot_model(autoencoder.encoder, show_shapes=True)","2b4bc66c":"autoencoder.decoder.summary()\nutils.plot_model(autoencoder.decoder, show_shapes=True)","90a92ff5":"encoded_imgs = autoencoder.encoder(x_test).numpy()\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n\nfig, ax = plt.subplots(2, 10, figsize=(20, 4))\nfor i in range(10):\n  ax[0][i].imshow(x_test[i], cmap='gray')\n  ax[0][i].get_xaxis().set_visible(False)\n  ax[0][i].get_yaxis().set_visible(False)\n\n  ax[1][i].imshow(decoded_imgs[i], cmap='gray')\n  ax[1][i].get_xaxis().set_visible(False)\n  ax[1][i].get_yaxis().set_visible(False)","09ea8e2a":"(x_train, _), (x_test, _) = fashion_mnist.load_data()\n\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.\n\nx_train = x_train[..., tf.newaxis]\nx_test = x_test[..., tf.newaxis]\n\nnoise_factor = 0.2\nx_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape) \nx_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape) \n\nx_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\nx_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)\n\nprint(x_train.shape)","b3fff273":"fig, ax = plt.subplots(1, 10, figsize=(20, 4))\nfor i in range(10):\n  ax[i].imshow(tf.squeeze(x_test_noisy[i]), cmap='gray')\n  ax[i].get_xaxis().set_visible(False)\n  ax[i].get_yaxis().set_visible(False)","b5e8267f":"class DenoisingAutoencoder(Model):\n  def __init__(self):\n    super(DenoisingAutoencoder, self).__init__()\n    self.encoder = tf.keras.Sequential([\n        layers.Input(shape=(28, 28, 1)), \n        layers.Conv2D(16, (3,3), activation='relu', padding='same', strides=1),\n        layers.MaxPooling2D((2, 2), padding='same'),\n        layers.Conv2D(8, (3,3), activation='relu', padding='same', strides=1),\n        layers.MaxPooling2D((2, 2), padding='same'),\n    ])\n\n    self.decoder = tf.keras.Sequential([\n        layers.Conv2DTranspose(8, kernel_size=3, strides=1, activation='relu', padding='same'),\n        layers.UpSampling2D((2, 2)),\n        layers.Conv2DTranspose(16, kernel_size=3, strides=1, activation='relu', padding='same'),\n        layers.UpSampling2D((2, 2)),\n        layers.Conv2D(1, kernel_size=(3,3), activation='sigmoid', padding='same')\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = DenoisingAutoencoder()\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')","cdc97423":"es = callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, mode='min', baseline=None,\n    restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, mode='min', verbose=1\n)\n\nhistory =  autoencoder.fit(\n    x_train_noisy, x_train, validation_data=(x_test_noisy, x_test), shuffle=True,\n    callbacks=[es, rlp], epochs=config.MAX_EPOCHS\n)","e74252bb":"autoencoder.encoder.summary()\nutils.plot_model(autoencoder.encoder, show_shapes=True)","53e5256f":"autoencoder.decoder.summary()\nutils.plot_model(autoencoder.decoder, show_shapes=True)","49bcda25":"encoded_imgs = autoencoder.encoder(x_test).numpy()\ndecoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n\nfig, ax = plt.subplots(2, 10, figsize=(20, 4))\nfor i in range(10):\n  ax[0][i].imshow(tf.squeeze(x_test_noisy[i]), cmap='gray')\n  ax[0][i].get_xaxis().set_visible(False)\n  ax[0][i].get_yaxis().set_visible(False)\n\n  ax[1][i].imshow(tf.squeeze(decoded_imgs[i]), cmap='gray')\n  ax[1][i].get_xaxis().set_visible(False)\n  ax[1][i].get_yaxis().set_visible(False)","a3669eee":"dataframe = pd.read_csv('http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ecg.csv', header=None)\nraw_data = dataframe.values\ndataframe.head()","01567845":"labels = raw_data[:, -1]\ndata = raw_data[:, 0:-1]\n\ntrain_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=19)","55940152":"min_val = tf.reduce_min(train_data)\nmax_val = tf.reduce_max(train_data)\n\ntrain_data = (train_data - min_val) \/ (max_val - min_val)\ntest_data = (test_data - min_val) \/ (max_val - min_val)\n\ntrain_data = tf.cast(train_data, tf.float32)\ntest_data = tf.cast(test_data, tf.float32)","a12941d5":"train_labels = train_labels.astype(bool)\ntest_labels = test_labels.astype(bool)\n\nnormal_train_data = train_data[train_labels]\nnormal_test_data = test_data[test_labels]\n\nanomalous_train_data = train_data[~train_labels]\nanomalous_test_data = test_data[~test_labels]","a8a3aa66":"fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nax[0].grid()\nax[0].plot(np.arange(140), normal_train_data[0])\nax[0].set_title(\"A Normal ECG\")\n\nax[1].grid()\nax[1].plot(np.arange(140), anomalous_train_data[0])\nax[1].set_title(\"An Anomalous ECG\");","26fab4f6":"class config():\n    INPUT_UNITS = 140\n    HIDDEN_UNITS = [32, 16, 8]\n    \n    MAX_EPOCHS = 100\n    BATCH_SIZE = 512\n    ","19f4a4df":"class AnomalyDetector(Model):\n  def __init__(self, input_units, hidden_units):\n    super(AnomalyDetector, self).__init__()\n    self.encoder = tf.keras.Sequential([layers.Dense(unit, activation=\"relu\") for unit in hidden_units])\n\n    decoder_layers = [layers.Dense(unit, activation=\"relu\") for unit in hidden_units[:-1:-1]] \n    decoder_layers.append(layers.Dense(input_units, activation=\"sigmoid\"))\n    \n    self.decoder = tf.keras.Sequential(decoder_layers)\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = AnomalyDetector(config.INPUT_UNITS, config.HIDDEN_UNITS)\nautoencoder.compile(optimizer='adam', loss='mean_absolute_error')","1517c704":"es = callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, mode='min', baseline=None,\n    restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, mode='min', verbose=1\n)\n\nhistory =  autoencoder.fit(\n    normal_train_data, normal_train_data, validation_data=(test_data, test_data), shuffle=True,\n    callbacks=[es, rlp], epochs=config.MAX_EPOCHS, batch_size=config.BATCH_SIZE\n)","00e9f4f6":"autoencoder.encoder.summary()\nutils.plot_model(autoencoder.encoder, show_shapes=True)","4b01a577":"autoencoder.decoder.summary()\nutils.plot_model(autoencoder.decoder, show_shapes=True)","252ffaee":"encoded_signals = autoencoder.encoder(normal_test_data).numpy()\ndecoded_signals = autoencoder.decoder(encoded_signals).numpy()\n\nplt.figure(figsize=(20, 6))\nplt.plot(normal_test_data[0],'b')\nplt.plot(decoded_signals[0],'r')\nplt.fill_between(np.arange(140), decoded_signals[0],  normal_test_data[0], color='lightcoral' )\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"]);","72d2d6b9":"encoded_signals = autoencoder.encoder(anomalous_test_data).numpy()\ndecoded_signals = autoencoder.decoder(encoded_signals).numpy()\n\nplt.figure(figsize=(20, 6))\nplt.plot(anomalous_test_data[0],'b')\nplt.plot(decoded_signals[0],'r')\nplt.fill_between(np.arange(140), decoded_signals[0], anomalous_test_data[0], color='lightcoral' )\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"]);","44ac2e5b":"reconstructions = autoencoder.predict(normal_train_data)\ntrain_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n\nplt.hist(train_loss, bins=50)\nplt.xlabel(\"Train loss\")\nplt.ylabel(\"No of examples\");","bb82c9db":"threshold = np.mean(train_loss) + np.std(train_loss)\nprint(\"Threshold: \", threshold)","7d9fd6e0":"reconstructions = autoencoder.predict(anomalous_test_data)\ntest_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n\nplt.hist(test_loss, bins=50)\nplt.xlabel(\"Test loss\")\nplt.ylabel(\"No of examples\");","59ec53ee":"def predict(model, data, threshold):\n  reconstructions = model(data)\n  loss = tf.keras.losses.mae(reconstructions, data)\n  return tf.math.less(loss, threshold)\n\ndef print_stats(predictions, labels):\n  print(f\"Accuracy = {accuracy_score(labels, preds)}\")\n  print(f\"Precision = {precision_score(labels, preds)}\")\n  print(f\"Recall = {recall_score(labels, preds)}\")","861bfa8e":"preds = predict(autoencoder, test_data, threshold)\nprint_stats(preds, test_labels)","e1f3b873":"class RecurrentAutoencoder(Model):\n  def __init__(self):\n    super(RecurrentAutoencoder, self).__init__()\n    self.encoder = tf.keras.Sequential([\n        layers.Reshape((140, 1)),\n        layers.LSTM(64, input_shape=(140, 1), return_sequences=False)\n    ])\n\n    self.decoder = tf.keras.Sequential([\n        layers.RepeatVector(140),\n        layers.LSTM(64, return_sequences=True),\n        layers.TimeDistributed(layers.Dense(1)),\n        layers.Reshape((140,))\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = RecurrentAutoencoder()\nautoencoder.compile(optimizer='adam', loss='mean_squared_error')","155092ee":"es = callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0001, patience=7, verbose=1, mode='min', baseline=None,\n    restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, mode='min', verbose=1\n)\n\nhistory =  autoencoder.fit(\n    normal_train_data, normal_train_data, validation_data=(normal_test_data, normal_test_data), shuffle=True,\n    callbacks=[es, rlp], epochs=100\n)","a673112c":"autoencoder.encoder.summary()\nutils.plot_model(autoencoder.encoder, show_shapes=True)","82d39cb6":"autoencoder.decoder.summary()\nutils.plot_model(autoencoder.decoder, show_shapes=True)","0d703215":"encoded_signals = autoencoder.encoder(normal_test_data).numpy()\ndecoded_signals = autoencoder.decoder(encoded_signals).numpy()\n\nplt.figure(figsize=(20, 6))\nplt.plot(normal_test_data[0],'b')\nplt.plot(decoded_signals[0],'r')\nplt.fill_between(np.arange(140), decoded_signals[0],  normal_test_data[0], color='lightcoral' )\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"]);","f20c42b9":"encoded_signals = autoencoder.encoder(anomalous_test_data).numpy()\ndecoded_signals = autoencoder.decoder(encoded_signals).numpy()\n\nplt.figure(figsize=(20, 6))\nplt.plot(normal_test_data[0],'b')\nplt.plot(decoded_signals[0],'r')\nplt.fill_between(np.arange(140), decoded_signals[0],  normal_test_data[0], color='lightcoral' )\nplt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"]);","7bd2aacf":"reconstructions = autoencoder.predict(normal_train_data)\ntrain_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n\nplt.hist(train_loss, bins=50)\nplt.xlabel(\"Train loss\")\nplt.ylabel(\"No of examples\");","74f90fda":"threshold = np.mean(train_loss) + np.std(train_loss)\nprint(\"Threshold: \", threshold)","f87c2adf":"reconstructions = autoencoder.predict(anomalous_test_data)\ntest_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n\nplt.hist(test_loss, bins=50)\nplt.xlabel(\"Test loss\")\nplt.ylabel(\"No of examples\");","5f225495":"preds = predict(autoencoder, test_data, threshold)\nprint_stats(preds, test_labels)","a1a7301c":"# Basic Autoencoder","da1e96b5":"We will train the autoencoder using only the normal rhythms, which are labeled in this dataset as 1. Separate the normal rhythms from the abnormal rhythms.","eefd5854":"# Denoising Autoencoder\n\nWe will create a noisy version of the Fashion MNIST dataset by applying random noise to each image. We will then train an autoencoder using the noisy image as input, and the original image as the target.","082155fe":"We will soon classify an ECG as anomalous if the reconstruction error is greater than one standard deviation from the normal training examples. First, let's plot a normal ECG from the training set, the reconstruction after it's encoded and decoded by the autoencoder, and the reconstruction error.","d2e761ea":"We will now calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.","ccb965c8":"# Anomaly Detector\n\nWe will train an autoencoder to detect anomalies on the ECG5000 dataset. This dataset contains 5,000 Electrocardiograms, each with 140 data points. We will use a simplified version of the dataset, where each example has been labeled either 0 (corresponding to an abnormal rhythm), or 1 (corresponding to a normal rhythm).\n\nHow will we detect anomalies using an autoencoder? Recall that an autoencoder is trained to minimize reconstruction error. You will train an autoencoder on the normal rhythms only, then use it to reconstruct all the data. Our hypothesis is that the abnormal rhythms will have higher reconstruction error. We will then classify a rhythm as an anomaly if the reconstruction error surpasses a fixed threshold.","9fe4189e":"If we examine the recontruction error for the anomalous examples in the test set, we'll notice most have greater reconstruction error than the threshold. By varing the threshold, we can adjust the precision and recall of our classifier.","0e0bde04":"# Auotencoders\n\n![](https:\/\/blog.keras.io\/img\/ae\/autoencoder_schema.jpg)\n\n\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are  data-specific, lossy, and learned automatically from examples rather than engineered by a human. \n\nAdditionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n\n1. Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n\n2. Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n\n3. Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n\nTo build an autoencoder, we need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of the data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding\/decoding functions can be optimize to minimize the reconstruction loss, using Gradient Descent.\n\n## Are they good at data compression?\nUsually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: we can only use them on data that is similar to what they were trained on, and making them more general thus requires lots of training data. But future advances might change this, who knows.\n\n## What are autoencoders good for?\nToday two interesting practical applications of autoencoders are data denoising, and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n","4136f861":"**Normalize the Data**","afd83563":"# Recurrent Autoencoder\n\nThe Recurrent network can be organized into an Encoder-Decoder architecture, that allows the model to be used to both support variable length input sequences and to predict or output variable length output sequences.\n\nThis architecture is the basis for many advances in complex sequence prediction problems such as speech recognition and text translation.\n\nIn this architecture, an encoder Recurrent model reads the input sequence step-by-step. After reading in the entire input sequence, the hidden state or output of this model represents an internal learned representation of the entire input sequence as a fixed-length vector. This vector is then provided as an input to the decoder model that interprets it as each step in the output sequence is generated."}}