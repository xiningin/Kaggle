{"cell_type":{"6fbc07c8":"code","abfd3fe5":"code","8ea79135":"code","1f1018ad":"code","1afd3703":"code","90c2faed":"code","885fcd35":"code","18fbda2b":"code","8a6a85ea":"code","1112f7c3":"code","e231be6e":"code","46f2b470":"code","d7328d9c":"code","90ed0112":"code","aa06a417":"code","328bed3a":"code","7e02ac64":"code","5cd87756":"code","9578b249":"code","190ad2c8":"code","2d66ab2e":"code","7d6e89fc":"code","8ee415aa":"code","221642be":"code","26a1ba62":"code","b2c228e5":"code","2b4776b1":"code","dca8c849":"code","18ebe72a":"code","78bfc42a":"code","4eabe7e3":"code","78d68d33":"code","f6255244":"code","07362d41":"code","e925fe92":"code","df30d7e9":"code","96f25e05":"code","4b94dc62":"code","448708da":"code","9554bfd3":"code","b6440fe7":"code","f76738e2":"code","bb043473":"code","5d874060":"code","a8cccb15":"code","c8b7e323":"code","fe76c60f":"code","ec894220":"code","9e7a6886":"code","7eb51187":"markdown","aa1779e2":"markdown","7b70dd56":"markdown","8f238e73":"markdown","0473146e":"markdown","ac476016":"markdown","eae60c5a":"markdown","11b53743":"markdown","fd5eb3e2":"markdown","2b512b8f":"markdown","91aa0158":"markdown","e9e3c935":"markdown","89345e3a":"markdown","42041a88":"markdown","7e8c14fe":"markdown","020c6216":"markdown","3df6a1e3":"markdown","db3c8a83":"markdown","e54da50d":"markdown","2c34fedf":"markdown","cd790e76":"markdown","a6618e2a":"markdown","fd585d6b":"markdown","7e2c815a":"markdown","f98ead08":"markdown","11ece5eb":"markdown","f785e545":"markdown","a30deeb0":"markdown","210d0bd5":"markdown","ed98e24e":"markdown","40e06b50":"markdown","96a68303":"markdown","90a414d8":"markdown"},"source":{"6fbc07c8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.figure_factory as ff\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy  import dendrogram,linkage\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport plotly.graph_objs as go\ndef RMSE(Y,Y_HAT):\n    return np.sqrt(mean_squared_error(Y_HAT,Y))","abfd3fe5":"p_data = pd.read_csv('\/kaggle\/input\/top-1000-patreons\/patreon2.csv')\np_data.head(3)","8ea79135":"p_data.Theme = p_data.Theme.apply(lambda x: ' '.join(x.split(' ')[:len(x.split(' '))-1]))\np_data.head(3)","1f1018ad":"p_data.Patrons = p_data.Patrons.apply(lambda x: ''.join(x.split(',')))\np_data.Patrons = p_data.Patrons.astype('int64')\np_data.head(3)","1afd3703":"#different earning type\nEarnings = p_data.Earnings\nEarnings = Earnings[Earnings.notna()]\nearning_types = Earnings.apply(lambda x: ' '.join(x.split(' ')[1:])).to_frame()\nearning_types.Earnings.value_counts()","90c2faed":"p_data['Per Month'] = 0\n\np_data.loc[p_data[p_data['Earnings'].str.contains('per month',na=False)].index,'Per Month'] = 1\n\ndef other_earning_methods(sir):\n    if sir == np.nan or type(sir) ==float:\n        return 'NaN'\n    elif sir.find('month') != -1:\n        return 0\n    else:\n        return 1\n\ndef get_earning_amount(sir):\n    if type(sir) == float:\n        return 'NaN'\n    amount = sir.split(' ')[0]\n    amount = ''.join(amount[1:].split(','))\n    return amount\n\np_data['Per Product'] = p_data.Earnings.apply(other_earning_methods)\np_data['Earning Amount'] = p_data.Earnings.apply(get_earning_amount)\np_data.drop(columns=['Earnings'],inplace=True)\np_data.head(3)\n","885fcd35":"Per_patron = p_data['Per patron']\nPer_patron = Per_patron[Per_patron.notna()]\nearning_types = Per_patron.apply(lambda x: ' '.join(x.split(' ')[1:])).to_frame()\nearning_types['Per patron'].value_counts()","18fbda2b":"p_data['Per Patron Earning'] = p_data['Per patron'].apply(get_earning_amount)\np_data.drop(columns=['Per patron'],inplace=True)\np_data.head(3)\n","8a6a85ea":"def num_of_words(sir):\n    return len(sir.split(' '))\ndef average_word_length(sir):\n    aux =0\n    splited = sir.split(' ')\n    for i in splited:\n        aux+=len(i)\n    return aux\/len(splited)\n\nadult = ['adult','futanari','18+','nsfw','dating','gta','erotic','nude','sex','love','harem','monster']\ndef is_adult(sir):\n    lowerd = sir.lower()\n    for word in adult:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\ngaming = ['game','rpg','warcraft','sims','nintendo','league','wow','vr','mod','dnd','minecraft']\ndef is_gaming(sir):\n    lowerd = sir.lower()\n    for word in gaming:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\nvideo = ['video','youtube','acting']\ndef is_video(sir):\n    lowerd = sir.lower()\n    for word in video:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\n\nmusic = ['ukulele','music','audio','guitar']\ndef is_music(sir):\n    lowerd = sir.lower()\n    for word in music:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\nart = ['art','paint','animation','comics','sketch']\ndef is_art(sir):\n    lowerd = sir.lower()\n    for word in art:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\nentertainment = ['entertainment','book','reactions','media','cartoons','tarot','post','meme','comedy']\ndef is_entertainment(sir):\n    lowerd = sir.lower()\n    for word in entertainment:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\n\neducation = ['book','lesson ','technology','essay','philosophy','science']\ndef is_education(sir):\n    lowerd = sir.lower()\n    for word in education:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0\n\nformal = ['journalism','news','interviews','radio']\ndef is_formal(sir):\n    lowerd = sir.lower()\n    for word in formal:\n        if lowerd.find(word)!=-1:\n            return 1\n    return 0","1112f7c3":"p_data['Creator # Of Words'] = p_data.Creator.apply(num_of_words)\np_data['Creator Avg Word Length'] = p_data.Creator.apply(average_word_length)\np_data['Theme # Of Words'] = p_data.Theme.apply(num_of_words)\np_data['Theme Avg Word Length'] = p_data.Theme.apply(average_word_length)\n\np_data['Podcast'] = p_data.Theme.apply(lambda x: 1 if x.lower().find('podcast')!= -1 else 0)\np_data['Adult'] = p_data.Theme.apply(is_adult)\np_data['Gaming'] = p_data.Theme.apply(is_gaming)\np_data['Video'] = p_data.Theme.apply(is_video)\np_data['Music'] = p_data.Theme.apply(is_music)\np_data['Art'] = p_data.Theme.apply(is_art)\np_data['Entertainment'] = p_data.Theme.apply(is_entertainment)\np_data['Education'] = p_data.Theme.apply(is_education)\np_data['Formal'] = p_data.Theme.apply(is_formal)\n\n","e231be6e":"vectize = TfidfVectorizer()\nvectize.fit(p_data['Theme'])\ntfidf_fets = vectize.transform(p_data['Theme'])\n\nsvd_model = TruncatedSVD(n_components=350)\nsvd_model.fit(tfidf_fets)\ntfidf_fets=svd_model.transform(tfidf_fets)\n\nex_var = svd_model.explained_variance_ratio_\nvariance_cum = np.cumsum(ex_var)\ndata = [go.Scatter(x=np.arange(0,len(variance_cum)),y=variance_cum)]\nlayout = dict(title='Decomposed Tfidf Explained Variance',\n             xaxis_title='# Componenets',yaxis_title='Explained Variance')\ngo.Figure(data=data,layout=layout)","46f2b470":"p_data = p_data.replace('NaN',np.nan)\np_data.isna().sum()","d7328d9c":"info = p_data.describe()\ninfo.loc['kurt'] = p_data.kurt()\ninfo.loc['skew'] = p_data.skew()\ninfo","90ed0112":"plt.figure(figsize=(20,11))\nax = sns.distplot(p_data['Patrons'])\nax.set_title('Distribution Of Patron Counts',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (p_data['Patrons'].mean(),), r'$\\mathrm{median}=%.2f$' % (p_data['Patrons'].median(),),\n         r'$\\sigma=%.2f$' % (p_data['Patrons'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()","aa06a417":"p_data = p_data[p_data['Patrons']<14000]","328bed3a":"plt.figure(figsize=(20,11))\nax = sns.distplot(p_data['Patrons'])\nax.set_title('Distribution Of Patron Counts Without Outliers',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (p_data['Patrons'].mean(),), r'$\\mathrm{median}=%.2f$' % (p_data['Patrons'].median(),),\n         r'$\\sigma=%.2f$' % (p_data['Patrons'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()","7e02ac64":"plt.figure(figsize=(20,11))\nax = sns.distplot(p_data['Days Running'])\nax.set_title('Distribution Of Total Days Running',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (p_data['Days Running'].mean(),), r'$\\mathrm{median}=%.2f$' % (p_data['Days Running'].median(),),\n         r'$\\sigma=%.2f$' % (p_data['Days Running'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()","5cd87756":"earning_data = p_data[p_data['Earning Amount'].notna()]\nearning_data['Per Patron Earning'] = earning_data['Per Patron Earning'].astype('float64')\nearning_data['Earning Amount']     = earning_data['Earning Amount'].astype('int64')\n\nplt.figure(figsize=(20,11))\nax = sns.distplot(earning_data['Earning Amount'])\nax.set_title('Distribution Of Total Earnings',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (earning_data['Earning Amount'].mean(),), r'$\\mathrm{median}=%.2f$' % (earning_data['Earning Amount'].median(),),\n         r'$\\sigma=%.2f$' % (earning_data['Earning Amount'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()\n","9578b249":"earning_data = earning_data[earning_data['Earning Amount']<50000]","190ad2c8":"plt.figure(figsize=(20,11))\nax = sns.distplot(earning_data['Earning Amount'])\nax.set_title('Distribution Of Total Earnings Without Ourliers',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (earning_data['Earning Amount'].mean(),), r'$\\mathrm{median}=%.2f$' % (earning_data['Earning Amount'].median(),),\n         r'$\\sigma=%.2f$' % (earning_data['Earning Amount'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()\n","2d66ab2e":"earning_data = earning_data[earning_data['Per Patron Earning']<12]","7d6e89fc":"\nplt.figure(figsize=(20,11))\nax = sns.distplot(earning_data['Per Patron Earning'])\nax.set_title('Distribution Of Per Patron Earning',fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (earning_data['Per Patron Earning'].mean(),), r'$\\mathrm{median}=%.2f$' % (earning_data['Per Patron Earning'].median(),),\n         r'$\\sigma=%.2f$' % (earning_data['Per Patron Earning'].std(),)))\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\nplt.show()\n","8ee415aa":"ex.scatter(earning_data,y='Earning Amount',x='Days Running',title='Speard of earnings based on days running',height=900)","221642be":"ex.scatter(earning_data,y='Earning Amount',x='Per Patron Earning',color='Patrons',title='Speard of earnings based on earnings per patron and number of patrons',height=900)","26a1ba62":"ex.pie(earning_data,'Per Month',title='Proportion Of Per Month And Per Product Patreons ')","b2c228e5":"earning_cor = earning_data.corr('pearson')\nex.imshow(earning_cor,height=900)\n","2b4776b1":"earning_cor = p_data.corr('pearson')\nex.imshow(earning_cor,height=900)\n","dca8c849":"ex.scatter_3d(p_data,x='Patrons',y='Days Running',z='Creator # Of Words',title='Prior To Clustring',height=900)","18ebe72a":"hclustr = AgglomerativeClustering(affinity='manhattan',linkage='average',n_clusters=3)\n\nhclustr.fit(p_data[['Podcast','Adult','Gaming','Video','Music','Art','Entertainment','Education','Formal']])\n\n#hclustr.labels_\nshclustr = linkage(p_data[['Podcast','Adult','Gaming','Video','Music','Art','Entertainment','Education','Formal']],method='average')\nplt.figure(figsize=(20,13))\nax = plt.subplot(111)\nd = dendrogram(shclustr,orientation='right',ax=ax)\nax.set_title('Clustering By The Key Feature Of The Patreon',fontsize=20)\nax.axes.yaxis.set_visible(False)\nplt.show()","78bfc42a":"p_data['Cluster'] = hclustr.labels_","4eabe7e3":"ex.scatter_3d(p_data,x='Patrons',y='Days Running',z='Creator # Of Words',color='Cluster',title='Prior To Clustring',height=900)","78d68d33":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(p_data[['Podcast','Adult','Gaming','Video','Music','Art','Entertainment','Education','Formal','Patrons']])\np_data['Cluster'] = kmeans.labels_\nex.scatter_3d(p_data,x='Patrons',y='Days Running',z='Creator # Of Words',color='Cluster',title='Clustered',height=900)","f6255244":"#Tfidf features\ntfidf = pd.DataFrame(tfidf_fets)","07362d41":"features = ['Podcast','Adult','Gaming','Video','Music','Art','Entertainment','Education','Formal','Patrons']\nearnings = p_data[p_data['Earning Amount'].notna()]\nearnings['Earning Amount'] = earnings['Earning Amount'].astype('int')\n\n#the outliers we fixed earlier\nearnings = earnings[earnings['Earning Amount']<50000]\n\nY = earnings['Earning Amount']\nX = earnings[features]\nindexs = [index for index in Y.index if index not in [985, 986, 987, 996]]\nXTF = tfidf.loc[indexs,:]","e925fe92":"knnr_model = KNeighborsRegressor(n_neighbors=20)\nknnr_model.fit(XTF,Y.loc[indexs])\ntfidf_knn_prediction = knnr_model.predict(XTF)\n\n#add to Features\nX = X.loc[indexs,:]\nY = Y.loc[indexs]\nX['Knnr_Tfidf'] =tfidf_knn_prediction\nX","df30d7e9":"train_x,test_x,train_y,test_y = train_test_split(X,Y)\n\n\nLR_Pipe = Pipeline(steps = [('model',LinearRegression())])\nKnn_Pipe = Pipeline(steps = [('scale',StandardScaler()),('model',KNeighborsRegressor(n_neighbors=20))])\nRF_Pipe = Pipeline(steps = [('model',RandomForestRegressor(random_state=42,n_estimators=150))])\n\nLR_Pipe.fit(train_x,train_y)\nKnn_Pipe.fit(train_x,train_y)\nRF_Pipe.fit(train_x,train_y)\n\nLR_Prediction  = LR_Pipe.predict(test_x)\nKnn_Prediction = Knn_Pipe.predict(test_x)\nRF_Prediction  = RF_Pipe.predict(test_x)\n\nLR_Score = RMSE(test_y,LR_Prediction)\nKnn_Score = RMSE(test_y,Knn_Prediction)\nRF_Score = RMSE(test_y,RF_Prediction)\n\nprint('LinearRegression RMSE: {} , Knn RMSE: {} , RandomForest RMSE: {}'.format(LR_Score,Knn_Score,RF_Score))","96f25e05":"plt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=LR_Prediction,label='LinearRegression Prediction',color='g')\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=RF_Prediction,label='KNN Prediction',color='r')","4b94dc62":"Blended_Prediction = 0.6*LR_Prediction + 0.4*Knn_Prediction","448708da":"print(\"Blended RMSE :\",RMSE(Blended_Prediction,test_y))","9554bfd3":"plt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=Blended_Prediction,label='Blended Prediction',color='r')\n","b6440fe7":"holdout_x = train_x.sample(10)\nholdout_y = train_y.sample(10)\ntrain_x = train_x.drop(index=holdout_x.index)\ntrain_y = train_y.drop(index=holdout_y.index)","f76738e2":"XGB_model = XGBRegressor(n_estimators = 500,learning_rate=0.03,random_state=42,gamma=0.3)\nXGB_model.fit(train_x,train_y,early_stopping_rounds=4,eval_set=[(holdout_x,holdout_y)],verbose=False)\nXGB_predictions = XGB_model.predict(test_x)","bb043473":"plt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=XGB_predictions,label='XGB Prediction',color='r')\n","5d874060":"from sklearn.preprocessing import PolynomialFeatures\n\nPR_pipe = Pipeline(steps = [('PF',PolynomialFeatures(2)),('model',LinearRegression())])\nPR_pipe.fit(train_x,train_y)\nPR_Prediction = PR_pipe.predict(test_x)\n \nplt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=PR_Prediction,label='PolynomialRegression Prediction',color='r')\n","a8cccb15":"print(\"Plynomial Regression RMSE: {}\".format(RMSE(test_y,PR_Prediction)))","c8b7e323":"Ridge_pipe = Pipeline(steps = [('model',RidgeCV())])\nRidge_pipe.fit(train_x,train_y)\nRidge_Prediction = Ridge_pipe.predict(test_x)\n \nplt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=Ridge_Prediction,label='Ridge Regression Prediction',color='r')\n","fe76c60f":"\n\nStack_Reg = StackingRegressor(estimators=[('LR',LR_Pipe),('RF',RF_Pipe),('KNN',Knn_Pipe),(\"XGB\",XGB_model)],final_estimator=RandomForestRegressor(n_estimators = 500,random_state=42),\n                             passthrough=True)\n\nStack_Reg.fit(train_x,train_y)\n\nstack_pred =Stack_Reg.predict(test_x)\n\n\n\nplt.figure(figsize=(25,15))\nax = sns.pointplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.pointplot(x=np.arange(0,len(test_y)),y=stack_pred,label='PolynomialRegression Prediction',color='r')","ec894220":"print(\"Blended Regression RMSE: {}\".format(RMSE(test_y,stack_pred)))","9e7a6886":"plt.figure(figsize=(25,15))\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=test_y,label='Actual Values',lw=2)\nax = sns.lineplot(x=np.arange(0,len(test_y)),y=Blended_Prediction,label='Blended Prediction',color='r')\n","7eb51187":"# Missing Values","aa1779e2":"### Our predictions so far are pretty off most of the time, let us try a couple more approaches, we shall test a polynomial regression and an XGB model to see if there is a higher order of complexity hidden ","7b70dd56":"![](https:\/\/bitethepen.podbean.com\/mf\/web\/sut9j8\/become_a_supporter-button.png)\n\n\n<h1 style=\"text-align:center;background-color:powderblue;font-size:320%\" >Introduction<\/h1>\n\nPatreon is an American membership platform that provides business tools for content creators to run a subscription service. It helps creators and artists earn a monthly income by providing rewards and perks to their subscribers.\nMany people rely on their Patreon page as a source of income that provides them with the funds needed to continue to produce their content.\n\n## Motivation \n\nCan we undercover patterns or key features that make some Patreons more popular?\nCan we create a model that can predict earnings or patron counts for a certain channel?\nOur main goal will be to provide some insight or models for new Patreons, hopefully helping them improve their chances to increase their patron counts and earnings.\n\n##  Question We Will Try To Answer \n\n\n* Are there any words that occur more among the creator names?\n* Can we disassemble the theme feature and extract valuable info which will help us better understand the data ?\n* Understand how the patron count is distributed whats the average amount of patrons? , how many outliers are there? \n* disassemble the earnings feature and extract different earning types how are they distributed\n* What can we learn from the days the Patreon is running? does an older Patreon mean more patrons?\n* What are the most common words used in the Patreon description ('About' Section)\n\n\n## Prediction Goals:\n* Predict the number of patrons for a new Patreon based on features given in the dataset that are known at the time of new page creation.\n* Predict the Potential earnings of a new Patreon based on features given in the dataset that are known at the time of new page creation.","8f238e73":"### So we see that the per patron earning categories are similar to the total earnings (with no surprise) we already created the feature which differs between per month payments and product payments so all we have left is to extract the amount of earnings per patron.","0473146e":"<h1 style=\"text-align:center;background-color:powderblue;font-size:320%\" >Text Feature Extraction <\/h1>\n\n\nWe will try and extract key descriptions from the theme feature, keys like 'podcast' , 'music' ,'art' etc.\nAlso we will extract basic length feature including average word length .","ac476016":"### Convert Patrons features into integers","eae60c5a":"## Our Model Of Choice Will Be The First Blended Model We Have Created","11b53743":"# Clustering ","fd5eb3e2":"<h1 style=\"text-align:center;background-color:powderblue;font-size:320%\" >Data Preprocessing<\/h1>\n","2b512b8f":"### So polynomial And Ridge regression didn't improve our RMSE compared to the regular multilinear regression. \n\n### Next, we will try to use stacking and check if we can lower our RMSE and get a more accurate prediction. ","91aa0158":"### We can see that we have a couple of outliers which extremely skew our distribution it should roughly resemble a normal distribution after the outliers will be removed ","e9e3c935":"### So the Agg-Clustering wasn't very informative in spliting our Patreons into major categories , lets try and add another feature into our clustering algorithm and change the algorithm to K-means","89345e3a":"### So apparently  our data is far from being normally distributed, in our case the distribution is exponential","42041a88":"### Lets try and blending the results from the linear regression model and the Knn Model","7e8c14fe":"### *Processing the 'Earnings' feature*","020c6216":"### Excellent! we got a fair clustering of our data, even though it heavily relies on the Patron count but as a divider of groups it did well, we will keep that feature the clusters as a feature the divides the groups into 'Average Patreons' ,'Unique Patreons' and 'Rare Patreons'  the higher they are in the scale the better their chances are to excel!","3df6a1e3":"<h1 style=\"text-align:center;background-color:powderblue;font-size:320%\" >EDA <\/h1>","db3c8a83":"### Lets remove some of the outliers which skew our distribution ","e54da50d":"### The distribution of the total day running is fairly normal and has now outliers !","2c34fedf":"### more days does not imply more earnings, there are Patreons which have made great earning straight away and there are Patreons which made little profit even though they have been running for years.","cd790e76":"### Above we can see the different types of earning plans at the Patreons which were not missing\n### We will create 2 new binary features, 'Per Month' and 'Per Product', the first feature tells is the Patreon based on a monthly subscription, and the second feature will tell us is the Patreon based on a certain product or item they sell.","a6618e2a":"### The theme feature ends with the name of the creator .we will firstly remove this redundant information.","fd585d6b":"### Data Prior To Clustring","7e2c815a":"### So our XGB model undershoots the value almost in all cases, it may be useful to us in a model stacking preference where we will use the prediction as a predictor feature in a meta-model but for now, it is useless to us\n\n### Lets try polynomial regression","f98ead08":"### your total revenue will increase by charging less and having more patrons even though we can see that the majority of Patreons prefer to charge more the smaller amount of patrons they have.","11ece5eb":"### As for the earning per patron we can see that the median and the mean are quite close to each other with a slight positive skew, the interesting insight here is the twin picks we have around the area of 2 USD and 5 USD, a question to be asked is will those picks get more significant as we increase our population? can we divide our population to those who earn around 2 USD per patron and to those who earn around 5 USD per patron and it will be a sufficient quantity to size up most of the population?","f785e545":"### *Processing the 'Per patron' feature*","a30deeb0":"### So apperently LinearRegression had the best RMSE so far with a small improval after combining it with the Knn model.","210d0bd5":"### We won't compare between the two groups because we don't have a sufficient enough population sample of the per product Patreons and the data may be biased towards the per month products.","ed98e24e":"### Clustering Result","40e06b50":"<h1 style=\"text-align:center;background-color:powderblue;font-size:320%\" >Model Selection And Evaluation <\/h1>\n\n\n### Our first goal will be to try and create a model that will accurately predict the total earnings of Patreons depending on their key Features and Patron count so that we can fill in all the missing value in the earnings feature followed by the Per Patron feature.","96a68303":"### We see that the total earning distribution has a heavy positive skew. A Patreon page has the highest probability to earn 0 - 10000 USD","90a414d8":"### We see that there are many missing values concerning the earnings, we will divide our analysis, in the case, we will able to create a model to predict the earning type and amount then we will try and fill in the missing values using our model.For now, we will continue to our EDA and ignore the missing values."}}