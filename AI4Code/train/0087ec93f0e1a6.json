{"cell_type":{"6c47c403":"code","191b1968":"code","2eb6ec67":"code","4dd2ae6d":"code","b865a5c5":"code","36f7f11e":"code","b3deed32":"code","72f26840":"code","e516e2aa":"code","e0b9b075":"code","9dc71b8d":"code","0ef4653b":"code","7ab585b5":"code","90b05857":"code","b52aeb2f":"code","cc493a00":"code","bf47f363":"code","f50adb82":"code","2193911b":"code","e270fff1":"code","109843a9":"code","a653cb83":"code","9f2922c0":"code","d8037d06":"code","5dc4c26a":"code","6916bad2":"code","c1da2555":"code","51c4d944":"code","bfd20754":"code","8aa0ce97":"code","9f4171b6":"code","773ec642":"code","cb29da3f":"code","793b5978":"code","1f0878c8":"code","141cf12b":"code","cbed364d":"code","746e0606":"code","451f4387":"markdown","918a2bc0":"markdown","4a5cf2bb":"markdown","4c563b37":"markdown","448b2fda":"markdown","21e2949f":"markdown","bbf9454b":"markdown","95c1e8a2":"markdown","01237232":"markdown","d6b3987d":"markdown","b16153d5":"markdown","4f9537c2":"markdown","8a12b08f":"markdown","9e3c2f65":"markdown","5239fcd9":"markdown","759b35ea":"markdown","3a45d627":"markdown","01c1c4a5":"markdown","41af1a5d":"markdown","4fdfb439":"markdown","59e0f65d":"markdown","9e93b19d":"markdown","15af19b5":"markdown","4d523971":"markdown","529f6620":"markdown","2679deff":"markdown","b178eb65":"markdown","dd7b26c1":"markdown","d9088dbb":"markdown","bcc9001e":"markdown","3f8d7b90":"markdown","f8d2b2f9":"markdown","b95e8781":"markdown","bf71aaaf":"markdown","36881454":"markdown","6a33fd5a":"markdown","647db687":"markdown","99a55612":"markdown","23b413e1":"markdown","222c2fcb":"markdown","2393b647":"markdown","64413a26":"markdown","8362f534":"markdown","2f1ec6af":"markdown","e507bd41":"markdown","3b658ee3":"markdown","e7837ccc":"markdown","11892ed6":"markdown","16810587":"markdown","379b622c":"markdown"},"source":{"6c47c403":"# Standard library\nimport pickle\nimport warnings\n\n# Data analysis\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modelization\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV","191b1968":"# Files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2eb6ec67":"training_set = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_set = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","4dd2ae6d":"sns.set(style=\"whitegrid\")\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\nwarnings.filterwarnings('ignore')","b865a5c5":"def cat_analysis(df, feature, x, y, figsize=(8,5), rotation=\"45\", palette=None, order=None):\n    \n    data = (pd.DataFrame(df[feature].value_counts())\n                .reset_index()\n                .rename(columns={'index': x, feature: y}))\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    ax = sns.barplot(x=x, y=y, data=data, palette=palette, order=order);\n    plt.xticks(rotation=rotation)\n    plt.title(f\"Distribution of {feature}\")\n    plt.show()\n    \ndef dummify(data, feature):\n    \n    # Get dummy variables\n    temp_df = pd.get_dummies(data[feature])\n    \n    # Add prefix to prevent duplicated feature names\n    temp_df = temp_df.add_prefix(feature + \"_\")\n    \n    # Concatenante the new features with the main dataframe\n    data = pd.concat([data, temp_df], axis=1)\n    \n    # Drop the original feature\n    data.drop(feature, axis=1, inplace=True)\n    \n    # Return the new dataframe\n    return data\n\ndef prepare_data(data_original):\n    \"\"\" Prepare the data for the Titanic Competition. \"\"\"\n    \n    data = data_original.copy()\n    \n    ################ Name => Boy \/ surname ######################\n    \n    # From the name, extract \"is a boy?\" and \"surname\"\n    data[\"boy\"] = data[\"Name\"].apply(lambda x: 1 if (\"Master.\" in x.split(\" \")[1:-1]) else 0)\n    data[\"surname\"] = data[\"Name\"].apply(lambda x: x.split(\",\")[0])\n    data.drop(\"Name\", axis=1, inplace=True)\n    \n    # Sex\n    data = dummify(data, \"Sex\")\n    # Since \"Sex\" it's a binary variable, we don't need to keep a feature for male AND for female\n    data.drop(\"Sex_male\", axis=1, inplace=True) # Just one is enough\n    \n    ################## Ticket ##########################\n    data[\"wcg_ticket\"] = data[\"Ticket\"].copy()\n    mask = (data[\"Sex_female\"] == 0) & (data[\"boy\"] == 0)\n    data.loc[mask, \"wcg_ticket\"] = \"no_group\"\n    data.drop(\"Ticket\", axis=1, inplace=True)\n    \n    ################## Cabin ######################\n    data.drop(\"Cabin\", axis=1, inplace=True) # And finally, drop the original column\n    \n    ################ Embarked ###################\n    data.drop(\"Embarked\", axis=1, inplace=True)\n    \n    ############### Family Size ################\n    data[\"family_size\"] = 1 + data[\"SibSp\"] + data[\"Parch\"]\n    data.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)\n    \n    ############## PassengerId #################\n    data.drop(\"PassengerId\", axis=1, inplace=True)\n    \n    ############## Fare ########################\n    data.drop(\"Fare\", axis=1, inplace=True)\n    \n    ############## Age #########################\n    data.drop(\"Age\", axis=1, inplace=True)\n    \n    ############## PClass ######################\n    data.drop(\"Pclass\", axis=1, inplace=True)\n    \n    ############# Woman Child Group ############\n    \n    # Copy family name to create a \"woman child group\" based on the surname\n    data[\"wcg_surname\"] = data[\"surname\"].copy()\n\n    # Remove men from groups\n    mask = (data[\"Sex_female\"] == 0) & (data[\"boy\"] == 0)\n    data.loc[mask, \"wcg_surname\"] = \"no_group\"\n    \n    data.drop(\"surname\", axis=1, inplace=True)\n    \n    return data","36f7f11e":"def my_xgb(data, target, params):\n    \n    # Create X and y\n    X = data.drop(target, axis=1)\n    y = data[target]\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X)\n    X_scaled = scaler.transform(X)\n    \n    # Create a XGBoost classifier (scikit-learn API wrapper)\n    xgb_clf = xgb.XGBClassifier()\n    \n    # Perform a gridsearch with sklearn\n    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n    gridsearch = GridSearchCV(xgb_clf, param_grid=params, scoring=\"accuracy\", cv=kf, return_train_score=True)\n    gridsearch.fit(X_scaled, y)\n    \n    # Return the gridsearch results plus the scaler\n    return gridsearch, scaler","b3deed32":"print(training_set.shape)\ndisplay(training_set.head())","72f26840":"print(test_set.shape)\ndisplay(test_set.head())","e516e2aa":"nanbyfeature = pd.DataFrame(training_set.isna().sum()).sort_values(by=0, ascending=False)\nnanbyfeature[\"percent\"] = np.round(nanbyfeature[0] \/ len(training_set) * 100,2)\nnanbyfeature","e0b9b075":"survived_passengers = training_set[\"Survived\"].sum() \/ len(training_set)\ndied_passengers = 1 - survived_passengers\nprint(f\"Survived passengers: {survived_passengers:.2%}\")\nprint(f\"Died passengers: {died_passengers:.2%}\")","9dc71b8d":"fig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"Sex\", y=\"Survived\", data=training_set);\nplt.xticks(rotation=0)\nplt.title(\"Percentage of survived by gender\")\nplt.show()","0ef4653b":"fig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"Sex\", y=\"Survived\", data=training_set, hue=\"Pclass\");\nplt.xticks(rotation=0)\nplt.title(\"Percentage of survived by gender\")\nplt.show()","7ab585b5":"training_set[\"family_size\"] = 1 + training_set[\"SibSp\"] + training_set[\"Parch\"]\n\n# Distribution\ncat_analysis(training_set, \"family_size\", \"family_size\", \"total_people\", rotation=90, figsize=(16,5))\n\n# Survived by family_size\nfig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"family_size\", y=\"Survived\", data=training_set);\nplt.xticks(rotation=0)\nplt.title(\"Percentage of survived by family_size\")\nplt.show()\n\n# Let's drop it for now\ntraining_set.drop([\"family_size\"], axis=1, inplace=True)","90b05857":"def name_title(title):\n    \"\"\" For each passenger, the function parses the name, from the second word to the second-last one.\n    The title is still among those positions.\n    According to the value, the function assigns a group.\n    \n    \"\"\"\n    \n    for word in title.split(\" \")[1:-1]:\n        if (word in [\"Mme.\", \"Ms.\", \"Mrs.\"]):\n            return \"woman\"\n\n        elif (word in [\"Mr.\"]):\n            return \"man\"\n\n        elif (word in [\"Master.\"]):\n            return \"boy\"\n\n        elif (word in [\"Miss.\", \"Mlle.\"]):\n            return \"miss\"\n\n        elif (word in [\"Capt.\", \"Col.\", \"Major.\", \"Rev.\", \"Dr.\"]):\n            return \"army\"\n\n        elif (word in [\"Jonkheer.\", \"Don.\", \"Sir.\", \"Countess.\", \"Dona.\", \"Lady.\"]):\n            return \"gentry\"\n    \n    else:\n        return \"other\"\n\ntraining_set[\"title\"] = training_set[\"Name\"].apply(name_title)\n\n# Distribution\ncat_analysis(training_set, \"title\", \"title\", \"total_people\", rotation=90, figsize=(16,5))\n\n# Survived by family_size\nfig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"title\", y=\"Survived\", data=training_set);\nplt.xticks(rotation=0)\nplt.title(\"Percentage of survived by title\")\nplt.show()\n\n# Remove temporary feature\ntraining_set.drop(\"title\", axis=1, inplace=True)","b52aeb2f":"train_prep = prepare_data(training_set)\ntest_prep = prepare_data(test_set)","cc493a00":"# Create a new dataframe\ndata_wcg_name = train_prep.copy()[[\"Sex_female\", \"boy\", \"wcg_surname\", \"Survived\"]]\ndata_wcg_name[\"wcg_surname_size\"] = 1\n\n# Remove passengers labelled by \"no group\"\nmask = data_wcg_name[\"wcg_surname\"] != \"no_group\"\ndata_wcg_name = data_wcg_name[mask]\n\n# Group by \"woman child group\" and count the number of members and the number of survivers for each group\ncolumns = {'Survived': 'survived_number'}\ndata_wcg_name = data_wcg_name.groupby(\"wcg_surname\").agg({'Survived':'sum', 'wcg_surname_size':'count'}).reset_index().rename(columns=columns)\n\n# Create new feature <All died>\ndata_wcg_name[\"wcg_name_all_died\"] = data_wcg_name[\"survived_number\"].apply(lambda x: 1 if x == 0 else 0)\n\n# Create feature <All survived>\ndata_wcg_name[\"wcg_name_all_survived\"] = data_wcg_name[\"survived_number\"] == data_wcg_name[\"wcg_surname_size\"]\ndata_wcg_name[\"wcg_name_all_survived\"] = data_wcg_name[\"wcg_name_all_survived\"] * 1\n\n# Import Test dataset\nwcg_name_test = test_prep.copy()[[\"wcg_surname\"]]\nwcg_name_test[\"wcg_surname_size\"] = 1\nwcg_name_test[\"survived_number\"] = 0\nwcg_name_test[\"wcg_name_all_died\"] = 0\nwcg_name_test[\"wcg_name_all_survived\"] = 0\nmask = wcg_name_test[\"wcg_surname\"] != \"no_group\"\nwcg_name_test = wcg_name_test[mask]\n\n# Merge train and test\ndata_wcg_name = pd.concat([data_wcg_name, wcg_name_test])\ndata_wcg_name = data_wcg_name.groupby(\"wcg_surname\").sum().reset_index()\n\n# Keep \"woman child groups\" composed by more than one people\nmask = data_wcg_name[\"wcg_surname_size\"] > 1\ndata_wcg_name = data_wcg_name[mask]\n\n# Remove useless columns\ndata_wcg_name.drop([\"survived_number\", \"wcg_surname_size\"], axis=1, inplace=True)\n\n# Merge with the training dataset\ntrain_prep = train_prep.merge(data_wcg_name, how=\"left\", on=\"wcg_surname\")","bf47f363":"display(data_wcg_name.head())\nprint(f\"Total groups: {data_wcg_name.shape[0]}\")\nprint(f\"All died: {data_wcg_name['wcg_name_all_died'].sum()}\")\nprint(f\"All survived: {data_wcg_name['wcg_name_all_survived'].sum()}\")","f50adb82":"# Create a new dataframe\ndata_wcg_ticket = train_prep.copy()[[\"Sex_female\", \"boy\", \"wcg_ticket\", \"Survived\"]]\ndata_wcg_ticket[\"wcg_ticket_size\"] = 1\n\n# Remove passengers labelled by \"no group\"\nmask = data_wcg_ticket[\"wcg_ticket\"] != \"no_group\"\ndata_wcg_ticket = data_wcg_ticket[mask]\n\n# Group by \"woman child group\" and count the number of members and the number of survivers for each group\ncolumns = {'Survived': 'survived_number'}\ndata_wcg_ticket = data_wcg_ticket.groupby(\"wcg_ticket\").agg({'Survived':'sum', 'wcg_ticket_size':'count'}).reset_index().rename(columns=columns)\n\n# Create new feature <All died>\ndata_wcg_ticket[\"wcg_ticket_all_died\"] = data_wcg_ticket[\"survived_number\"].apply(lambda x: 1 if x == 0 else 0)\n\n# Create feature <All survived>\ndata_wcg_ticket[\"wcg_ticket_all_survived\"] = data_wcg_ticket[\"survived_number\"] == data_wcg_ticket[\"wcg_ticket_size\"]\ndata_wcg_ticket[\"wcg_ticket_all_survived\"] = data_wcg_ticket[\"wcg_ticket_all_survived\"] * 1\n\n# Import Test dataset\nwcg_ticket_test = test_prep.copy()[[\"wcg_ticket\"]]\nwcg_ticket_test[\"wcg_ticket_size\"] = 1\nwcg_ticket_test[\"survived_number\"] = 0\nwcg_ticket_test[\"wcg_ticket_all_died\"] = 0\nwcg_ticket_test[\"wcg_ticket_all_survived\"] = 0\nmask = wcg_ticket_test[\"wcg_ticket\"] != \"no_group\"\nwcg_ticket_test = wcg_ticket_test[mask]\n\n# Merge train and test\ndata_wcg_ticket = pd.concat([data_wcg_ticket, wcg_ticket_test])\ndata_wcg_ticket = data_wcg_ticket.groupby(\"wcg_ticket\").sum().reset_index()\n\n# Keep \"woman child groups\" composed by more than one people\nmask = data_wcg_ticket[\"wcg_ticket_size\"] > 1\ndata_wcg_ticket = data_wcg_ticket[mask]\n\n# Remove useless columns\ndata_wcg_ticket.drop([\"survived_number\", \"wcg_ticket_size\"], axis=1, inplace=True)\n\n# Merge with the training dataset\ntrain_prep = train_prep.merge(data_wcg_ticket, how=\"left\", on=\"wcg_ticket\")","2193911b":"display(data_wcg_ticket.head())\nprint(f\"Total groups: {data_wcg_ticket.shape[0]}\")\nprint(f\"All died: {data_wcg_ticket['wcg_ticket_all_died'].sum()}\")\nprint(f\"All survived: {data_wcg_ticket['wcg_ticket_all_survived'].sum()}\")","e270fff1":"for i, row in train_prep.iterrows():\n    \n    # All died processing\n    if ((train_prep.loc[i, \"wcg_name_all_died\"] == 1) or (train_prep.loc[i, \"wcg_ticket_all_died\"] == 1)):\n        train_prep.loc[i, \"all_died\"] = 1\n    elif ((train_prep.loc[i, \"wcg_name_all_died\"] == 0) or (train_prep.loc[i, \"wcg_ticket_all_died\"] == 0)):\n        train_prep.loc[i, \"all_died\"] = 0\n    else:\n        train_prep.loc[i, \"all_died\"] = np.nan\n          \n    # All survived processing\n    if ((train_prep.loc[i, \"wcg_name_all_survived\"] == 1) or (train_prep.loc[i, \"wcg_ticket_all_survived\"] == 1)):\n        train_prep.loc[i, \"all_survived\"] = 1\n    elif ((train_prep.loc[i, \"wcg_name_all_survived\"] == 0) or (train_prep.loc[i, \"wcg_ticket_all_survived\"] == 0)):\n        train_prep.loc[i, \"all_survived\"] = 0\n    else:\n        train_prep.loc[i, \"all_survived\"] = np.nan","109843a9":"# Copy data\nwcgdf = train_prep.copy()\nwcgdf[\"class\"] = training_set[\"Pclass\"]\n\n\n#  Only passengers with \"all survived\" information\nmask1 = wcgdf[\"all_survived\"] == 1\nmask2 = wcgdf[\"all_survived\"] == 0\nwcgdf1 = wcgdf[mask1 | mask2]\nfig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"class\", y=\"all_survived\", data=wcgdf);\nplt.xticks(rotation=0)\nplt.title(\"Percentage of all survived by class\")\nplt.show()\n\nmask3 = wcgdf[\"all_died\"] == 1\nmask4 = wcgdf[\"all_died\"] == 0\nwcgdf2 = wcgdf[mask3 | mask4]\n#  Only passengers with \"all died\" information\nfig, ax = plt.subplots(figsize=(15,5))\nax = sns.barplot(x=\"class\", y=\"all_died\", data=wcgdf);\nplt.xticks(rotation=0)\nplt.title(\"Percentage of all died by class\")\nplt.show()","a653cb83":"train_prep.drop([\"wcg_surname\", \"wcg_ticket\", \"wcg_name_all_died\", \"wcg_name_all_survived\", \"wcg_ticket_all_died\", \"wcg_ticket_all_survived\"], axis=1, inplace=True)","9f2922c0":"print(train_prep.shape)\ndisplay(train_prep.head())","d8037d06":"nanbyfeature2 = pd.DataFrame(train_prep.isna().sum()).sort_values(by=0, ascending=False)\nnanbyfeature2[\"percent\"] = np.round(nanbyfeature2[0] \/ len(train_prep) * 100,2)\nnanbyfeature2","5dc4c26a":"params = {\n 'learning_rate': [0.01, 0.05, 0.1],\n 'subsample': [1],\n 'colsample_bylevel': [1],\n 'colsample_bynode': [1],\n 'colsample_bytree': [0.5],\n 'gamma': [0, 1, 2],\n 'max_delta_step': [0],\n 'max_depth': [2, 3],\n 'min_child_weight': [1.6], # Owen Zhang's rule of thumb: mcw = 3\/sqrt(event_rate) -> 1.6 (Thanks Tae Hyon Whang)\n 'n_estimators': [100],\n 'random_state': [42],\n 'scale_pos_weight': [1],\n 'seed': [42],\n 'n_jobs': [-1],\n 'reg_lambda': [1, 2, 4, 16]\n}","6916bad2":"gridsearch, scaler = my_xgb(train_prep, \"Survived\", params)","c1da2555":"results = pd.DataFrame(gridsearch.cv_results_).sort_values(by=\"rank_test_score\")\nfig, ax = plt.subplots(figsize=(16,5))\nplt.plot(np.arange(len(results)), results[\"mean_train_score\"], label=\"Train\")\nplt.plot(np.arange(len(results)), results[\"mean_test_score\"], label=\"Test\")\nplt.legend()\nplt.show()","51c4d944":"display(gridsearch.best_params_)","bfd20754":"feature_importances = gridsearch.best_estimator_.feature_importances_\nfeature_names = train_prep.drop([\"Survived\"], axis=1).columns\n\ntemp_df1 = {\n    'feature_name': feature_names,\n    'feature_importance': feature_importances\n}\n\ntemp_df1 = pd.DataFrame(temp_df1).sort_values(by=\"feature_importance\", ascending=False)\n\ndisplay(temp_df1.reset_index(drop=True))","8aa0ce97":"X_training_error = train_prep.copy()\ny_training_error_true = X_training_error.pop(\"Survived\")\n\nX_training_error_scaled = scaler.transform(X_training_error) \ny_training_error_pred = gridsearch.predict(X_training_error_scaled)\n\ntraining_set[\"true\"] = y_training_error_true\ntraining_set[\"pred\"] = y_training_error_pred\n\ntrue_pos = training_set[\"true\"] == 1\ntrue_false = training_set[\"true\"] == 0\npred_pos = training_set[\"pred\"] == 1\npred_false = training_set[\"pred\"] == 0","9f4171b6":"mask_results = training_set[\"Sex\"] != -1\ntrue_positive = len(training_set[true_pos & pred_pos & mask_results])\nfalse_positive = len(training_set[true_false & pred_pos & mask_results])\ntrue_negative = len(training_set[true_false & pred_false & mask_results])\nfalse_negative = len(training_set[true_pos & pred_false & mask_results])\n\nprint(f\"Accuracy: {(training_set['true'] == training_set['pred']).sum() \/ len(training_set):.2%}\")\nprint(f\"True positives: {true_positive} ({true_positive \/ (true_positive + false_negative):.0%})\")\nprint(f\"False positives: {false_positive}\")\nprint(f\"True negatives: {true_negative} ({true_negative \/ (true_negative + false_positive):.0%})\")\nprint(f\"False negatives: {false_negative}\")","773ec642":"mask_results = training_set[\"Sex\"] == \"female\"\ntrue_positive = len(training_set[true_pos & pred_pos & mask_results])\nfalse_positive = len(training_set[true_false & pred_pos & mask_results])\ntrue_negative = len(training_set[true_false & pred_false & mask_results])\nfalse_negative = len(training_set[true_pos & pred_false & mask_results])\n\nprint(f\"Accuracy: {(training_set['true'] == training_set['pred']).sum() \/ len(training_set):.2%}\")\nprint(f\"True positives: {true_positive} ({true_positive \/ (true_positive + false_negative):.0%})\")\nprint(f\"False positives: {false_positive}\")\nprint(f\"True negatives: {true_negative} ({true_negative \/ (true_negative + false_positive):.0%})\")\nprint(f\"False negatives: {false_negative}\")","cb29da3f":"mask_results = training_set[\"Sex\"] == \"male\"\ntrue_positive = len(training_set[true_pos & pred_pos & mask_results])\nfalse_positive = len(training_set[true_false & pred_pos & mask_results])\ntrue_negative = len(training_set[true_false & pred_false & mask_results])\nfalse_negative = len(training_set[true_pos & pred_false & mask_results])\n\nprint(f\"Accuracy: {(training_set['true'] == training_set['pred']).sum() \/ len(training_set):.2%}\")\nprint(f\"True positives: {true_positive} ({true_positive \/ (true_positive + false_negative):.0%})\")\nprint(f\"False positives: {false_positive}\")\nprint(f\"True negatives: {true_negative} ({true_negative \/ (true_negative + false_positive):.0%})\")\nprint(f\"False negatives: {false_negative}\")","793b5978":"mask_results = training_set[\"Sex\"] == \"male\"\nmask_results2 = training_set[\"Name\"].str.contains(\"Master.\")\ntrue_positive = len(training_set[true_pos & pred_pos & mask_results & ~mask_results2])\nfalse_positive = len(training_set[true_false & pred_pos & mask_results & ~mask_results2])\ntrue_negative = len(training_set[true_false & pred_false & mask_results & ~mask_results2])\nfalse_negative = len(training_set[true_pos & pred_false & mask_results & ~mask_results2])\n\nprint(f\"Accuracy: {(training_set['true'] == training_set['pred']).sum() \/ len(training_set):.2%}\")\nprint(f\"True positives: {true_positive} ({true_positive \/ (true_positive + false_negative):.0%})\")\nprint(f\"False positives: {false_positive}\")\nprint(f\"True negatives: {true_negative} ({true_negative \/ (true_negative + false_positive):.0%})\")\nprint(f\"False negatives: {false_negative}\")","1f0878c8":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","141cf12b":"test_data_prepared = prepare_data(test_data)\n\n# Merge \"name wcg\"\ntest_data_prepared = test_data_prepared.merge(data_wcg_name, how=\"left\", on=\"wcg_surname\")\ntest_data_prepared[\"wcg_name_all_died\"]\ntest_data_prepared[\"wcg_name_all_survived\"]\n\n# Merge \"ticket wcg\"\ntest_data_prepared = test_data_prepared.merge(data_wcg_ticket, how=\"left\", on=\"wcg_ticket\")\ntest_data_prepared[\"wcg_ticket_all_died\"]\ntest_data_prepared[\"wcg_ticket_all_survived\"]\n\nfor i, row in test_data_prepared.iterrows():\n    \n    # All died processing\n    if ((test_data_prepared.loc[i, \"wcg_name_all_died\"] == 1) or (test_data_prepared.loc[i, \"wcg_ticket_all_died\"] == 1)):\n        test_data_prepared.loc[i, \"all_died\"] = 1\n    elif ((test_data_prepared.loc[i, \"wcg_name_all_died\"] == 0) or (test_data_prepared.loc[i, \"wcg_ticket_all_died\"] == 0)):\n        test_data_prepared.loc[i, \"all_died\"] = 0\n    else:\n        test_data_prepared.loc[i, \"all_died\"] = np.nan\n          \n    # All survived processing\n    if ((test_data_prepared.loc[i, \"wcg_name_all_survived\"] == 1) or (test_data_prepared.loc[i, \"wcg_ticket_all_survived\"] == 1)):\n        test_data_prepared.loc[i, \"all_survived\"] = 1\n    elif ((test_data_prepared.loc[i, \"wcg_name_all_survived\"] == 0) or (test_data_prepared.loc[i, \"wcg_ticket_all_survived\"] == 0)):\n        test_data_prepared.loc[i, \"all_survived\"] = 0\n    else:\n        test_data_prepared.loc[i, \"all_survived\"] = np.nan\n\ntest_data_prepared.drop([\"wcg_surname\", \"wcg_ticket\", \"wcg_name_all_died\", \"wcg_name_all_survived\", \"wcg_ticket_all_died\", \"wcg_ticket_all_survived\"], axis=1, inplace=True)","cbed364d":"test_data_prepared_scaled = scaler.transform(test_data_prepared)\ny_test = gridsearch.predict(test_data_prepared_scaled)","746e0606":"submission = pd.DataFrame(y_test.copy())\nsubmission['PassengerId'] = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")['PassengerId'].copy()\nsubmission.rename(columns={0:'Survived'}, inplace=True)\nsubmission = submission[['PassengerId', 'Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)","451f4387":"## Import librairies & data\n\n### Librairies","918a2bc0":"### Prepare training data\n\nLet's prepare the traning data by removing some features and creating new ones.","4a5cf2bb":"## Functions\n\nDon't look at them now. Jump directly to the next. You'll come back when I use it.","4c563b37":"## Introduction\n\nThis notebook tries to show how you can perform more than 82% of accuracy on the Titanic dataset with XGBoost and only five features. Everything is explained step by step.\n\nNote that this introduction explains basically everything you need to know. Once you read it, you will be able to do your own work to reach 82%. So don't skip it :)\n\n**Warning**: Some of technics exposed here can be considered as data leakage. So if it's your first machine learning experience, maybe you don't want to start with the method exposed here. This notebook shows how you can reach the top leaderboard in the context of a Kaggle competition, not how to write the best model for a production environment.\n\nFirst of all, I want to thank [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) for all his amazing work about the Titanic competition and especially for his \"[Woman Child Group model](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818\/notebook)\" which highly inspires my kernel. Without his work, I couldn't have done this notebook.\n\nAnd I want to thank [Tae Hyon Whang](https:\/\/www.kaggle.com\/numbersareuseful) for his work [here](https:\/\/www.kaggle.com\/numbersareuseful\/titanic-starter-with-xgboost-173-209-lb\/notebook) which helped me perform a best gridsearch for my XGBoost hyper-parameters.\n\n### A bit of context\n\nWhen I decided to try to make a good score on the Titanic competition, my first thought was: All I need is:\n\n1. First, let's say a pretty \"classical and strong\" feature engineering: clean all data, impute empty values, extract all informations like name title, cabin letter and so on.\n\n2. And then, perform an effective gridsearch to find the best hyper-parameters for XGBoost.\n\nBut... I was kind of wrong. I tried A LOT of stuffs following this direction. But nothing really worked. I tried to make the best model to impute empty values, like fill empty ages with XGBoost, I tried a lot of combinations between features. But my best score was stuck to around 0.78%, maybe 0.79%. I didn't understand why I couldn't reach 0.80%, which was basically my goal.\n\nSo I started to dig in the top kernels. To be honest, I saw a lot of kernels which were supposed to be good, but which weren't. And I noticed that the best kernels in this competition are not always the most famous ones.\n\nAnyway, let's try to explain what are the keys of the success for this competition.\n\n### Avoid overfitting\n\nThe first problem with the Titanic dataset is overfitting. At first it seems like each feature is giving you a lot of new insights. But the more I digged, the more I realized the information was **highly redondant**. And since we don't have a lot of observations, we have to be careful about it.\n\n### About gender and children\n\nFirst of all, never forget one thing: The simple gender model where you predict all women survived and all men died reachs a leaderboard score more than 76%.\n\nSo, yes: your fate on the Titanic was highly correlated with your gender: if you're a man, you'll probably die. If you're a woman, you'll probably survive.\n\nWe can also say that if you're a boy, you'll probably survive too. Because \"Women and Children First!\" was true.\n\nSo if you want to do a better job than this very simple model, you have to try to predict some women and children that died. And try to predict some men who survived (this is the most difficult).\n\n### Money, money, money\n\nAt first, I was persuaded that if you just add \"money features\" like \"PClass\" and \"Ticket Fare\" (in addition to gender), this will help a lot. Actually, it could help. But as we'll see, the class influences the fate of the entire family, more than each individual passenger.\n\nAnd as you'll see, I don't use any \"money feature\" in this model.\n\n### How old are you?\n\nThe age is not that useful that I thought too. Because, basically: if you're a woman, you'll survive, either you're young or old. If you're a man, you'll survive if you're under 16. But since there are a lot of missing values, it's better to use the \"Master\" title which is filled for everybody!\n\nSo we won't use the Age feature in this model.\n\n### It's all about group!\n\nXGBoost can find a lot of connexions, even subtle and complex connexions. That's why it works pretty well. But, you still have to help it with a good feature engineering. So here is the real key of the success:\n\nFor this competition, a good feature ingineering implies realizing that passengers can form **groups**! Without it, you won't be able to reach 0.80%. The groups can be formed by a same surname or a same ticket.\n\nBut your model can't discover it by itself. You have to do something. The natural way to do it would be: extracting names (and \/ or tickets) and dummify it. Et voil\u00e0 ! Now your model knows how to connect passengers to each other. But... no. The dataset is too short to do this. On the boat, you don't have four or five families. You have hundreds. So It will be too much groups, too much features.\n\nSo, the only way is to identify groups and after that you have to find a way to attribute one or several scores to your groups which will be your new features(s). And once again, you will have to choose a good way to do it, because you don't want to overfit.\n\n### Woman Child Groups\n\nI choosed the \"woman child group\" approach formalized by Chris Deotte. And then I combined name groups with ticket groups. And finally, thanks to XGBoost, I used a pretty convenient feature of this algorithm: you can feed it with empty data. So your dummy variables can actually take three different values.\n\nBasically, the idea is: all families died or survived together. When we say \"family\", we think about \"women and children\". Because, yes, if you're a man, you probably died anyway.\n\nSo, the idea is to analyze WC groups and try to score them.\n\n1. If all women and children survived, we will notice it. (New feature equals 1)\n2. If they all died, we'll also notice it. (Another new feature equals 1)\n3. Sometimes we'll be able to say \"it's neither one nor the other. It's a mixed group\". (Both features equal 0)\n4. And sometimes, we can't say anything, so we'll say nothing: for this, thanks to XGBoost which can take into account empty values.\n\nSo, as we can see with this model: a majority of the information to predict passengers fate is concentred in:\n\n1. Your gender\n2. The rest of your group\n\nAnd of course some features probably influence your group's fate. But this model doesn't event use PClass or Ticket price.\n\nYou're a man? You probably died, either you're rich or not. You're a child or a woman? Let's have a look at what happened to your family and I'll tell if you survived or not.\n\nSo, at the end, the dataset is composed of five features:\n\n1. Are you a woman?\n2. Are you boy?\n3. Family size\n4. Are you in a woman-child group? Did all they survive?\n5. Are you in a woman-child group? Did all they die?\n\nOK, I wrote enough :) Let's have a look at the code!","448b2fda":"### Settings\n\nJust some display settings.","21e2949f":"As we can see, a majority died together or survived together.","bbf9454b":"![image.png](attachment:image.png)","95c1e8a2":"### Predictions","01237232":"As we can see, \"Cabin\" and \"Age\" have a lot of empty values. But for this kernel we don't care since I won't use them :)","d6b3987d":"## Data overview","b16153d5":"### \"Boy\" feature\n\nI spent a lot of time trying some stuffs with the title of each passengers. Actually, it does help to predict passengers fate. But once again, the information is redundant with other features. The most useful and unique information here is about male passengers: because of the \"Master\" title, we can find boy children.","4f9537c2":"### Errors Analysis\n\nThe code here is not really... optimized.","8a12b08f":"### Survived by Gender + Pclass\n\nAs I said, I didn't use the \"Pclass\" feature in my model. Let's have a look though.","9e3c2f65":"#### Woman","5239fcd9":"### Best model parameters","759b35ea":"### Survived\n\nLet's quickly analyze our target first to see how is balanced the training dataset.","3a45d627":"### Plot the training \/ test results for each model.","01c1c4a5":"### Prepare the test data\n\nI already did that once for the training set. It could have been better to put this in a function.","41af1a5d":"#### Everybody","4fdfb439":"### Features importance","59e0f65d":"### NaN by feature\n\nThe first thing I do when I load a dataset is to have a look at empty values by column.","9e93b19d":"As we can see, \"middle size\" family survived better. But for big families, the error line is large, because we don't have too much observations for family_size > 4.\n\nHere my intuition for XGBoost was this feature will help the model combined to WGC groups because the more you are in your family the bigger are the chances that you followed the same fate. And of course, if you're alone, it's also good to know.","15af19b5":"Only families from Pclass 3 died together. Whereas families from Pclass 1 and 2 almost all survived together. But now that we can identify this inforamtion through our groups, we don't need \"Pclass\" feature in our model.","4d523971":"### New dataframe: Woman Child Group by Name + features\n\nHere I'm creating a new dataframe which groups all passengers by their name. The steps are:\n\n1. I create a new dataset from the training set where I group by \"woman child group\" which is the surname of all passengers, except for men where the value is \"no_group\"\n2. I create new features: \"all died\" means everybody died in this group. \"all survived\" means everybody survived in this group.\n3. I import the test set.\n4. I group again, to be able to count how many people there are in each group.\n5. I remove groups with only 1 people across all data","529f6620":"Et voil\u00e0 !\n\n## Conclusion\n\nPredict some survived men seems to be the most complicated part and could be an improvement of this model.\n\nHere I tried to let XGBoost do the work, after I fed it with features containing a lot of useful informations. Maybe some more complex interactions could help, but the most difficult part is giving new useful informations to the model without overfitting.\n\nI hope this notebook helped some of you. I learned a lot by making it, so if you learn reading it, I'll be glad :) The key is really to read other top kernels and try your own stuffs. I spent a lot of time reading and trying!\n\nIf you have any suggestions \/ remarks \/ questions or anything else, please feel free to leave a comment.","2679deff":"Once we removed boys from men, we can see that \"man\" passenegrs are definitely in a bad situation. But if you're a boy, you probably survived (around 0.60%).\n\n1. The feature \"boy\" will help to find \"men\" who are not actually men but children. So it will help predicting \"male\" passengers that survived.\n\n2. The WCG features will help to find boys who were in a family where everybody died, so we will not predict all boys survived and be more accurate.","b178eb65":"We've got a group information for about 25% of passengers.","dd7b26c1":"#### Test set","d9088dbb":"### Let's combine \"ticket groups\" and \"name groups\"\n\nThe main dataset has now 4 new features :\n\n1. All survived by group\n2. All survived by ticket\n3. All died by group\n4. All died by ticket\n\nLet's combine them.\n\nThe rules are simple:\n\n- If I find at least one 1, it's a 1.\n- Or else if I find at least one 0, it's a 0.\n- Or else it's a NaN.\n\nMaybe another best rule will improve my score.","bcc9001e":"### New dataframe: Woman Child Group by Ticket\n\nHere it's exacly the same process as above but based on tickets and not on names.","3f8d7b90":"#### NaN by feature","f8d2b2f9":"### GridSearch Parameters\n\nI kept some parameters here so you can see the curves of training \/ test results below.\n\nIf you want to perform a biggest gridsearch, I think it's better to do it on your own machine \/ server.","b95e8781":"#### Training set","bf71aaaf":"### Perform the gridsearch","36881454":"Now we see why the Gender Model is a good one.","6a33fd5a":"### Family size (new feature)\n\nOur model is based on groups, right? So to complete informations about groups, let's feed our model with each family's size.\n\nLet's create a new feature \"family size\". Basically, it's a linear combination of SibSp and Parch where the intercept equals one, representing the passenger themself. So we have :\n\n> *family_size* = 1 + 1*SibSp* + 1*Parch*\n\nNote: I didn't use Name or Ticket to create this feature, but maybe it could be a good idea.","647db687":"Yes, your class obviously influences your fate. But the most important thing is:\n\n1. If you're a rich man, you will still probably die\n2. If you're a woman, even a \"poor\" one, it's basically 50\/50.\n\nSo this feature doesn't really help by itself. Of course there are maybe some more complex interractions with other features. But the point is: if your starting point is the gender model (which seems not a bad idead), adding Pclass feature won't bring a new useful information.","99a55612":"### Survived by gender","23b413e1":"The model never predicts wrong died woman and never predicts wrong survived man. But if it predicts a survived man, in fact it's only when this man is actually a boy! Because as we can see on the cell just above, the model never predict a survived man! It could be a track to follow for future improvements.\n\nLet's check now on the test set and then on the learderboard!","222c2fcb":"## XGBoost model\n\nFinally! Here we are :) Let's try to find the best parameters and predict the fate of passengers!\n\n### The final dataset","2393b647":"### \"Woman Child Group\" fates are connected to their class","64413a26":"### Submission","8362f534":"### Data","2f1ec6af":"Same remark: a majority of \"woman-child-group by ticket\" died together or survived together.","e507bd41":"# Make predictions\n\n### Get the test data","3b658ee3":"#### Men and Boy","e7837ccc":"It's pretty well balanced. But we can notice that we still have some more died passengers than survived ones. So in order to improve our accuracy score, it will still easier to find the ones who died than survived.","11892ed6":"#### Boy","16810587":"## Feature engineering","379b622c":"## Woman Child Group Engineering"}}