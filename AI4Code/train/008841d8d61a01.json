{"cell_type":{"463351ac":"code","550b2cc6":"code","a05087d3":"code","ce121a66":"code","43218dd3":"code","dc4558f7":"code","5023a361":"code","60b1e5d9":"code","ed0ef65b":"code","996a8198":"code","7471982f":"code","9f0fb7bc":"code","cc7d42c7":"code","983e67f5":"markdown","bf71a193":"markdown","b1926d74":"markdown","22b3494e":"markdown","fc54914b":"markdown"},"source":{"463351ac":"!pip install ..\/input\/pandarallel151whl\/pandarallel-1.5.1-py3-none-any.whl","550b2cc6":"!pip install cupy","a05087d3":"import cudf","ce121a66":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nimport random\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.models import Word2Vec\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\nstemmer = SnowballStemmer('english')\nimport unicodedata\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom sklearn.preprocessing import normalize\nTEST = True\nCHUNK=1024*4\nMAX_NUM_OF_MATCH = 60\nWORD2VEC_VEC_SIZE = 100\nTFIDF_THRESHOLD = 0.75\nimport scipy\npreproc_txt = True\nuse_img_fuzzy=True\n# from pandarallel import pandarallel\n# pandarallel.initialize()\nmodel_path = \"\/kaggle\/input\/sharpee-model-weight\/\"\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","43218dd3":"test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\ntrain = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\nif TEST:\n    df = train\nelse:\n    df = test\n    \nif TEST:\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    df.head(2)\nprint('Dataframe shape is',df.shape )","dc4558f7":"import sys\nsys.path.append('..\/input\/timm-master\/pytorch-image-models-master')\n# sys.path.append(model_path)\nimport numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\n# import cudf\n# import cuml\n# import cupy\n# from cuml.feature_extraction.text import TfidfVectorizer\n# from cuml.neighbors import NearestNeighbors\nimport timm","5023a361":"def get_avg_f1(targets,preds):\n    def f1score(target,pred):\n        n = len( np.intersect1d(target,pred) )\n        return 2*n \/ (len(target)+len(pred))\n    avg_f1_score = sum([f1score(*i) for i in zip(targets,preds)])\/len(targets)\n    return avg_f1_score\n\ndef combine_for_sub(cols):\n    x = np.concatenate(cols)\n    return ' '.join( np.unique(x) )\n\ndef combine_for_testing(cols):\n    x = np.concatenate(cols)\n    return np.unique(x)\n\ndef cleanData(dataParse):\n    data = unicodedata.normalize('NFKC', dataParse)\n    data = re.sub(r'\u3010.*\u3011', '', data)\n    data = re.sub(r'\\[.*\\]', '', data)\n    data = re.sub(r'\u300c.*\u300d', '', data)\n    data = re.sub(r'\\(.*\\)', '', data)\n    data = re.sub(r'\\<.*\\>', '', data)\n    data = re.sub(r'[\u203b@\u25ce].*$', '', data)\n    return data.lower() #Returns the parsed tweets\n\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(cleanData(text), pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return \" \".join(result)\n\ndef get_tfidf_pred(df,CHUNK = CHUNK,threshold = TFIDF_THRESHOLD,max_features = 25_000):\n    print('Computing text embeddings...')\n\n    model = TfidfVectorizer(stop_words='english', binary=True, max_features=max_features,norm=\"l2\")\n    if preproc_txt:\n        text_embeddings = model.fit_transform(df.title.apply(preprocess))\n    else:\n        text_embeddings = model.fit_transform(df.title)\n    preds = get_match_with_cosine_similarity(text_embeddings,df.posting_id,threshold,CHUNK)\n    del model, text_embeddings\n    return preds\n\ndef get_match_with_cosine_similarity(embeddings,posting_ids,threshold,CHUNK):\n    if isinstance(embeddings,list):\n        embeddings = np.vstack(embeddings)\n        embeddings = normalize(embeddings,norm=\"l2\")\n    elif isinstance(embeddings,scipy.sparse.csr.csr_matrix):\n        embeddings = embeddings.todense()\n    elif isinstance(embeddings,pd.core.series.Series):\n        embeddings =embeddings.values()\n        \n    else:\n        Exception(\"Please add data handle method, embeding should be np matrix\")\n\n    \n    embeddings = torch.from_numpy(embeddings)\n    embeddings = embeddings.to(DEVICE)\n    preds = []\n    print('Finding similar images...')\n    CTS = len(embeddings)\/\/CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b, len(embeddings))\n        print('chunk',a,'to',b)\n        distances = torch.matmul(embeddings, embeddings[a:b].T).T\n        distances = distances.data.cpu().numpy()\n        for k in range(b-a):\n#             import pdb;pdb.set_trace()\n            idxs = np.where(distances[k,]>threshold)[0][:]\n            if len(idxs)>MAX_NUM_OF_MATCH:\n                idxs = distances[k,].argsort()[-MAX_NUM_OF_MATCH:][::-1]\n#                 print(f\"Found {str(len(idxs))}, and it is regulated to MAX_NUM_OF_MATCH {str(MAX_NUM_OF_MATCH)}\")\n            o = pd.Series(posting_ids).iloc[idxs].values\n            preds.append(o)\n    del embeddings\n    return preds\n","60b1e5d9":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = DEVICE\n    classes = 11014\n    \n    scale = 30 \n    margin = 0.5\n\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)\n\n\ndef get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=DEVICE)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = None,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n#         import pdb;pdb.set_trace()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n\nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1.\/h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1.\/v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model\n\ndef get_model(model_name = None, model_path = None, n_classes = None):\n    \n    model = ShopeeModel(model_name = model_name)\n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n    model.eval()\n    model.load_state_dict(torch.load(model_path,map_location=torch.device(DEVICE)))\n    model = model.to(CFG.device)\n    \n    return model \n\nclass EnsembleModel(nn.Module):\n    \n    def __init__(self):\n        super(EnsembleModel,self).__init__()\n        \n        self.m1 = get_model('eca_nfnet_l0',f'{model_path}arcface_512x512_nfnet_l0 (mish).pt')\n        self.m2 = get_model('tf_efficientnet_b5_ns',f'{model_path}arcface_512x512_eff_b5_.pt')\n        \n    def forward(self,img,label):\n        \n        feat1 = self.m1(img,label)\n        feat2 = self.m2(img,label)\n    \n        return (feat1 + feat2) \/ 2\n\ndef get_image_embeddings(image_paths, model_name = None, model_path = None):\n    embeds = []\n    \n    model = EnsembleModel()\n    \n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            if DEVICE.type!=\"cpu\":\n                img = img.cuda()\n                label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\nif TEST:\n    df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\nelse:\n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n# df.head()\n\nimage_embeddings = get_image_embeddings(image_paths.values)\ndf[\"image_pred\"] = get_match_with_cosine_similarity(image_embeddings,df.posting_id,threshold=0.36,CHUNK=CHUNK)","ed0ef65b":"def hamming_distance(phash1, phash2):\n    '''\n    helper function to calculate phash similarity\n    '''\n    phash1 = bin(int(phash1, 16))[2:].zfill(64)\n    phash2 = bin(int(phash2, 16))[2:].zfill(64)\n    distance = np.sum([i != j for i, j in zip(phash1, phash2)])\n    return distance\ndef hamming_distance_bin(phash1, phash2,thrshold = 5):\n    '''\n    helper function to calculate phash similarity\n    '''\n    count = 0\n    for i,j in zip(phash1, phash2):\n        if i != j:\n            count+=1\n        if count>thrshold:\n            return False\n    return True\n\ndef get_match_by_fuzzy_img_phase(phase,phases,posting_ids,thrshold = 5):\n    return posting_ids[phases.applymap(lambda x:hamming_distance_bin(x,phase,thrshold))].tolist()","996a8198":"def get_tfidf_pred(df,CHUNK = CHUNK,threshold = TFIDF_THRESHOLD,max_features = 25_000):\n    print('Computing text embeddings...')\n\n    model = TfidfVectorizer(stop_words='english', binary=True,analyzer=\"char\",ngram_range =(10,11) ,max_features=max_features,norm=\"l2\")\n#     if preproc_txt:\n#         text_embeddings = model.fit_transform(df.title.apply(preprocess))\n#     else:\n#     import pdb;pdb.set_trace()\n    text_embeddings = model.fit_transform(df.image_phash)\n    preds = get_match_with_cosine_similarity(text_embeddings,df.posting_id,threshold,CHUNK)\n    del model, text_embeddings\n    return preds\n\ndef get_match_with_cosine_similarity(embeddings,posting_ids,threshold,CHUNK):\n    if isinstance(embeddings,list):\n        embeddings = np.vstack(embeddings)\n        embeddings = normalize(embeddings,norm=\"l2\")\n    elif isinstance(embeddings,scipy.sparse.csr.csr_matrix):\n        embeddings = embeddings.todense()\n    elif isinstance(embeddings,pd.core.series.Series):\n        embeddings =embeddings.values()\n        \n    else:\n        Exception(\"Please add data handle method, embeding should be np matrix\")\n\n    \n    embeddings = torch.from_numpy(embeddings)\n    embeddings = embeddings.to(DEVICE)\n    preds = []\n    print('Finding similar images...')\n    CTS = len(embeddings)\/\/CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b, len(embeddings))\n        print('chunk',a,'to',b)\n        distances = torch.matmul(embeddings, embeddings[a:b].T).T\n        distances = distances.data.cpu().numpy()\n        for k in range(b-a):\n#             import pdb;pdb.set_trace()\n            idxs = np.where(distances[k,]>threshold)[0][:]\n            if len(idxs)>MAX_NUM_OF_MATCH:\n                idxs = distances[k,].argsort()[-MAX_NUM_OF_MATCH:][::-1]\n#                 print(f\"Found {str(len(idxs))}, and it is regulated to MAX_NUM_OF_MATCH {str(MAX_NUM_OF_MATCH)}\")\n            o = pd.Series(posting_ids).iloc[idxs].values\n            preds.append(o)\n    del embeddings\n    return preds\n","7471982f":"%%time\n# df['img_phase'] =  df.image_phash.parallel_apply(lambda x:get_match_by_fuzzy_img_phase(x,df.image_phash,df.posting_id))\ndf['tfidf'] = get_tfidf_pred(df)\n# if TEST:\nprint(f\"F1 score with tfidf: {str(get_avg_f1(df.target,df.tfidf))}\")\n# print(f\"F1 score with iamge phase: {str(get_avg_f1(df.target,df.img_phase))}\")\n# df[\"matches\"] = df[[\"tfidf\",\"img_phase\"]].apply(combine_for_testing,axis=1)\n# print('CV Score =', get_avg_f1(df[\"matches\"],df.target) )","9f0fb7bc":"df.groupby(\"posting_id\")[\"image_phash\"].apply(lambda x:\"\/n\".join(x))","cc7d42c7":"if TEST:\n    df[\"matches\"] = df[[\"tfidf\",\"img_phase\"]].apply(combine_for_testing,axis=1)\n    print('CV Score =', get_avg_f1(df[\"matches\"],df.target) )\nelse:\n    df['matches'] = df[[\"tfidf\",\"img_phase\",\"image_pred\"]].apply(combine_for_sub,axis=1)\ntest = df\ntest[['posting_id','matches']].to_csv('submission.csv',index=False)\n# sub.head()","983e67f5":"\nTo prevent memory errors, we will find similar titles in chunks. To faciliate this, we will use cosine similarity between text embeddings instead of KNN.","bf71a193":"# prediction based only on text part\napplying TFIDF on titles and fuzzy match on image phash","b1926d74":"# Load Data","22b3494e":"# Write Submission CSV\nIn this notebook, the submission file below looks funny containing train information. But when we submit this notebook, the size of `test.csv` dataframe will be longer than 3 rows and the variable `COMPUTE_CV` will subsequently set to `False`. Then our submission notebook will compute the correct matches using the real test dataset and our submission csv for LB will be ok.","fc54914b":"# Load Libraries"}}