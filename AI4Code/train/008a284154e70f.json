{"cell_type":{"4fcfede0":"code","e57f55ac":"code","54c39b09":"code","90d5111f":"code","91494f37":"code","a7102494":"code","940ad797":"code","8bc42e35":"code","215789d1":"code","cd943337":"code","0334ed30":"code","a5f266d9":"code","6a89613b":"code","f663145e":"code","6fea29df":"code","321d4a20":"code","56abdd44":"code","b8d328f1":"code","c98e5a46":"code","34f1f1c4":"code","c3ef6f11":"code","0e369f0a":"code","d1c6a464":"code","503da21a":"code","e645fb43":"code","1d85f735":"code","fcf0e84a":"code","7bdb9543":"code","3004a1f5":"code","b62b3637":"code","8c6a1840":"code","e2bedabb":"code","3aa6b7df":"code","3d1879df":"code","ec69e0bc":"code","b45d92b9":"code","b58c0f57":"markdown","e710156c":"markdown","3d1e1697":"markdown","17c3c380":"markdown","0ff9eede":"markdown","9a2696d3":"markdown","40933f58":"markdown","fb922b91":"markdown","18736685":"markdown","3b1699f4":"markdown","c712e75f":"markdown","a51270c6":"markdown","b7b881ce":"markdown"},"source":{"4fcfede0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nfrom wordcloud import WordCloud, STOPWORDS\n\n","e57f55ac":"data=pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin', \n                   names = ['target','id','date','query','user','tweet'])\ndata","54c39b09":"data = data[['target', 'tweet']]\ndata['tweet']=data['tweet'].str.lower()","90d5111f":"#check if there are missing values\n\nmissing_data = data.isna().sum().sort_values(ascending=False)\npercentage_missing = round((data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)*100,2)\nmissing_info = pd.concat([missing_data,percentage_missing],keys=['Missing values','Percentage'],axis=1)\nmissing_info.style.background_gradient()","91494f37":"data['target'].unique()","a7102494":"#check what each class means----------\n#target = 0 : Negative\n#target = 4 : Positive\n\npd.set_option('display.max_colwidth', -1)\ndata[data['target']==0]['tweet'].head\ndata[data['target']==4]['tweet'].head\ndata['target'] = data['target'].replace([0, 4],['Negative','Positive'])","940ad797":"#check if data is balanced-------------\n\nfig = plt.figure(figsize=(8,8))\ntargets = data.groupby('target').size()\ntargets.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['red','green'])\nplt.title(\"Pie chart of different classes of tweets\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()\n","8bc42e35":"#wordcloud of positive tweets \n\nplt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(data[data.target=='Positive'].tweet))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in positive tweets.',fontsize=20)\nplt.show()\n","215789d1":"#wordcloud of negative tweets \n\nplt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"black\").generate(\" \".join(data[data.target=='Negative'].tweet))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in negative tweets.',fontsize=20)\nplt.show()","cd943337":"#removing stopwords \nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nSTOPWORDS = set(stopwords.words('english'))\ndef cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndata['tweet'] = data['tweet'].apply(lambda text: cleaning_stopwords(text))\ndata['tweet'].head()","0334ed30":"# removing punctuation -----------\nimport string\nenglish_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef remove_puncts(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndata['tweet']= data['tweet'].apply(lambda x: remove_puncts(x))\ndata['tweet'].tail()","a5f266d9":"#remove repeated characters ----------\nimport re\ndef  remove_duplicate_chars(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)\n\ndata['tweet'] = data['tweet'].apply(lambda x: remove_duplicate_chars(x))\ndata['tweet'].tail()","6a89613b":"#removing emails :\n\ndef remove_email(data):\n    return re.sub('@[^\\s]+', ' ', data)\ndata['tweet']= data['tweet'].apply(lambda x: remove_email(x))\ndata['tweet'].tail()\n\n#removing URL's :\n\ndef remove_URLs(data):\n    return re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))',' ',data)\ndata['tweet'] = data['tweet'].apply(lambda x: remove_URLs(x))\ndata['tweet'].tail()\n","f663145e":"# Cleaning and removing Numeric numbers\n\ndef remove_nbs (data):\n     return re.sub('[0-9]+' , '' ,data)\ndata['tweet']=data['tweet'].apply(lambda x: remove_nbs(x))\ndata['tweet'].tail()\n","6fea29df":"# Tokenization \n# there's a TweetTokenizer\n# tk = TweetTokenizer(reduce_len=True)\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer =  RegexpTokenizer(r'\\w+')\ndata['tweet'] = data['tweet'].apply(tokenizer.tokenize)\n\n\n","321d4a20":"# stemming ,why does it take too long to run this cell ?? ..\n\nst = nltk.PorterStemmer()\ndef stemming_on_text(data):\n    text = [st.stem(word) for word in data]\n    return text\n\ndata['tweet']= data['tweet'].apply(lambda x: stemming_on_text(x))","56abdd44":"data[\"tweet\"]","b8d328f1":"# applying lemmatization \nnltk.download('wordnet')\nlm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(data):\n    text = [lm.lemmatize(word) for word in data]\n    return text\n\ndata['tweet'] = data['tweet'].apply(lambda x: lemmatizer_on_text(x))","c98e5a46":"data.head()","34f1f1c4":"import tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\n\nX=data.tweet\ny=data.target\n\n\nmax_len = 1000\ntok = Tokenizer(num_words=4000)\ntok.fit_on_texts(X)\n\n\nsequences = tok.texts_to_sequences(X)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n\nX_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)\n\n","c3ef6f11":"sequences_matrix.shape","0e369f0a":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r', encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","d1c6a464":"vocab_length = len(tok.word_index) + 1","503da21a":"longest_train = max(X.values, key=lambda sentence: len(tok.texts_to_sequences(sentence)))\nlength_long_sentence = len(longest_train)","e645fb43":"length_long_sentence","1d85f735":"vocab_length","fcf0e84a":"embeddings_dictionary = dict()\nembedding_dim = 50\n\nglove_file = open('..\/input\/glove6b\/glove.6B.50d.txt', 'r', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in tok.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector","7bdb9543":"embeddings_matrix.shape[0]","3004a1f5":"sequences_matrix","b62b3637":"from keras.models import Sequential,Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom keras.layers.core import Dense, Activation, Dropout\ndef create_model_rnn(embeddings_matrix, max_words):\n\n    model = Sequential()\n    model.add(Embedding(len(embeddings_matrix), embeddings_matrix.shape[1],weights=[embeddings_matrix], input_length=max_words, trainable=False))\n    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.50))\n    model.add(Dense(10, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n    return model","8c6a1840":"\nmodel = create_model_rnn(embeddings_matrix, length_long_sentence )","e2bedabb":"model.summary()","3aa6b7df":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","3d1879df":"len(X_train)","ec69e0bc":"len(X_test)","b45d92b9":"model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","b58c0f57":" The input to model is 1000 words because these are the number features\/words that we extracted above from text of tweets.","e710156c":"Twitter data (also know as tweets) is a rich source of information on a large set of topics. This data can be used to find trends related to a specific keyword, measure brand sentiment or gather feedback about new products and services. This post will provide a step by step guide for text analytics on twitter data.","3d1e1697":"Word embeddings are basically a way for us to convert words to representational vectors. What I mean by this is that, instead of mapping each word to an index, we want to map each word to a vector of real numbers, representing this word.\n\nWord embeddings provide a dense representation of words and their relative meanings. \n\nEmbedding Matrix is a maxtrix of all words and their corresponding embeddings. \n\n Its embeddings relate to the probabilities that two words appear together.","17c3c380":"## Word Embedding ","0ff9eede":"## Train_Test split","9a2696d3":"## Implementing Tensorflow based model ","40933f58":"## Approach :\n\n1. Extract data ( web scraping, twitter API ) + Discover it\n2. Data visualization : wordclouds\n3. Data cleaning : \n  * remove duplicates\n  * remove emails and urls\n  * remove puntuations\n  * remove numbers \n  \n4.  Tokenization \n5. Stemming and Lemmatization\n\n","fb922b91":"Thankfully, there are no missing values in this dataset ","18736685":"we are going to use LSTM model\n\n","3b1699f4":"## Data Preprocessing","c712e75f":"For word embedding, we'll use the pretrained model glove provided in Kaggle ( we'll use the smallest version of the model )","a51270c6":"Sentiment analysis is about detecting emotions, opinions of people about certain topics by analyzing their texts ( tweets, fb comments or status , youtube comments ... )\n\nRetail industries and companies in general use sentiment analysis to get an overview of their clients' opinions on their products which enables them to make improvements and certain modifications to their products so that it meets their clients' standards ( to satisfy their clients ) \n\nThere are a lot of social media sites like Google Plus, Facebook, and Twitter that allow expressing opinions, views, and emotions about certain topics and events.","b7b881ce":"The steps involved in text analytics are:\n\nStep 1: Collect tweets\n\nStep 2: Pre-process tweets\n\nStep 3: Apply sentiment analysis\n\nStep 4: Apply named entity recognition\n\nStep 5: Cluster tweets\n\nStep 6: Visualise analysis"}}