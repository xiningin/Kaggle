{"cell_type":{"3776edf0":"code","91746332":"code","e70bcde6":"code","53457e2f":"code","0c6d8217":"code","b60b593b":"code","f8e3cd59":"code","2ea92d8e":"code","1fe2bf11":"code","ce461d63":"code","70a3b845":"code","cfc21ae7":"code","3121c019":"code","798de58b":"code","710aa6f7":"code","4a131bc2":"code","864f530e":"code","625d749b":"code","7695f34c":"code","47287d95":"code","e88d4b11":"code","7c22384a":"code","5855bc6c":"code","86f23de5":"code","cc659952":"code","5546b94e":"code","4b6f0fe3":"code","886ea19a":"code","94d0cd0d":"code","a2692a24":"code","4494f6f5":"code","97f6f837":"code","4b5f8201":"markdown","9ef81cb0":"markdown","37e22134":"markdown","d9942cd1":"markdown","a7a9b580":"markdown","dda953f0":"markdown","28c81157":"markdown","071732dc":"markdown","92c39995":"markdown","24e11a6f":"markdown","f71bd223":"markdown","723d38ee":"markdown","b2ca82c3":"markdown","fe1f824d":"markdown","8a55546e":"markdown","489fa618":"markdown","862936e1":"markdown","53614284":"markdown","b3a9a83f":"markdown","5ec41245":"markdown","a402cfbc":"markdown","40e0376a":"markdown"},"source":{"3776edf0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n\n\n\n# Close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","91746332":"data_2c = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\ndata_3c = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv')","e70bcde6":"data_2c.head()","53457e2f":"data_3c.info()","0c6d8217":"def look_univariate(dataset, discrete_feature):\n    fig, plots=plt.subplots(nrows=1,ncols=2, figsize=(8,5))\n    \n    labels = dataset[discrete_feature].value_counts().index\n    sizes = dataset[discrete_feature].value_counts().values\n    \n    dataset[discrete_feature].value_counts().plot(kind=\"bar\",ax=plots[0])\n    plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\") \n    plt.tight_layout()\n    plt.show()","b60b593b":"look_univariate(data_3c,discrete_feature=\"class\")","f8e3cd59":"look_univariate(data_2c,discrete_feature=\"class\")","2ea92d8e":"data_norm = data_2c[data_2c[\"class\"]==\"Normal\"]\ndata_abnorm = data_2c[data_2c[\"class\"]==\"Abnormal\"]\ndata_2n = data_2c.drop(['class'],axis=1)","1fe2bf11":"# correlation map\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data_2n.corr(),annot=True,linewidths=.5,fmt='.1f',ax=ax)\nplt.show()","ce461d63":"def class_wth_scatter(dataset):\n    fig = ff.create_scatterplotmatrix(dataset,diag=\"box\",colormap_type=\"cat\",\n                                      height=1000,width=1000)\n    iplot(fig)","70a3b845":"class_wth_scatter(data_2n)","cfc21ae7":"sns.pairplot(data_3c, hue=\"class\")\nplt.show()","3121c019":"data_2c['class'].value_counts()","798de58b":"# change class data tpye\ndata_2c['class'] = [ 0 if each == \"Abnormal\" else 1 for each in data_2c['class']]\n# x_data and y_data\nx_data = data_2c.drop([\"class\"],axis=1)\ny_data = data_2c[\"class\"]","710aa6f7":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))","4a131bc2":"x.head()","864f530e":"from sklearn.model_selection import train_test_split # import library for this\nx_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.3,random_state = 2)\n\nprint(\"x_train :\", x_train.shape)\nprint(\"x_test :\" , x_test.shape)\nprint(\"y_train :\" , y_train.shape)\nprint(\"y_test :\" , y_train.shape)","625d749b":"# import library\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n# fit model\nlr.fit(x_data,y_data)\n\nlr_test_accuracy = lr.score(x_test,y_test)\nlr_train_accuracy = lr.score(x_train,y_train)\nlr_predict = lr.predict(x_test)\n\n#Print Test and Train Accuracy\nprint(\"Test Accuracy(LR): \",lr.score(x_test,y_test))\nprint(\"Train Accuracy(LR): \",lr.score(x_train,y_train))","7695f34c":"# Confusion Matrix for Logistic Regression\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = lr_predict\n\ncm_lr = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_lr,annot = True, linewidths=0.5,fmt = \".0f\", cmap = \"RdPu\" , ax= ax)\nplt.show()","47287d95":"# Implement KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) # k value\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\n#Print Test and Train Accuracy\nprint(\"KNN (K=3) test score is {}\".format(knn.score(x_test,y_test)))\nprint(\"KNN (K=3) train score is {}\".format(knn.score(x_train,y_train)))","e88d4b11":"# Optimal K value\nneighbors = range(1,40)\nknn_train = []\nknn_test = []\nfor i,k in enumerate(neighbors):\n    # k from 1 to 40 (40 exclude)\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    # fit knn\n    knn_model.fit(x_train,y_train)\n    knn_train.append(knn_model.score(x_train,y_train))    # train accuracy\n    knn_test.append(knn_model.score(x_test,y_test))       # test accuracy\n    \n\n# vizualization results\n\n# create trace1\ntrace1 = go.Scatter(\n                    x = np.array(neighbors),\n                    y = knn_train,\n                    mode = \"lines\",\n                    name=\"train accuracy\",\n                    marker = dict(color = 'rgba(160, 112, 2, 0.8)'),\n                    text=\"train_accuracy\")\n\n# create trace2\ntrace2 = go.Scatter(\n                    x = np.array(neighbors),\n                    y = knn_test,\n                    mode = \"lines+markers\",\n                    name=\"test accuracy\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),\n                    text=\"train_accuracy\")\n\ndata = [trace1,trace2]\n\nlayout = dict(title = \"K Value vs Accuracy\",\n             xaxis = dict(title = \"Number of Neighbors\", ticklen = 10,zeroline = True))\n\nfig = dict(data = data, layout = layout)\niplot(fig)\n\nknn_test_accuracy  = np.max(knn_test)\nprint(\"Best accuracy is {} with K = {}\".format(np.max(knn_test),1 + knn_test.index(np.max(knn_test))))","7c22384a":"# Confusion Matrix for KNN\nknn = KNeighborsClassifier(n_neighbors=10) # k value, we found K best value is 10\nknn.fit(x_train,y_train)\ny_predict = knn.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,fmt = \".0f\", cmap = \"RdPu\" , ax= ax)\nplt.show()","5855bc6c":"# import library\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state=20)\nsvm.fit(x_train,y_train)\n\nsvm_test_accuray = svm.score(x_test,y_test)\n\nprint(\"SVM Test accucary: \",svm.score(x_test,y_test))\nprint(\"SVM Train accucary: \",svm.score(x_train,y_train))","86f23de5":"# Confusion Matrix for SVM\nfrom sklearn.metrics import confusion_matrix\ny_true = y_test\ny_predict = svm.predict(x_test)\n\ncm_svc = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_svc,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","cc659952":"# import library\nfrom sklearn.naive_bayes import GaussianNB\n\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_train,y_train)\n\nnaive_bayes_score = naive_bayes.score(x_test,y_test)\nnb_test_accuracy = naive_bayes.score(x_test,y_test)\n\nprint(\"naive bayes test score: \",naive_bayes.score(x_test,y_test))\nprint(\"naive bayes train score: \",naive_bayes.score(x_train,y_train))","5546b94e":"# Confusion Matrix for Naive Bayes\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = naive_bayes.predict(x_test)\n\ncm_nb = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_nb,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","4b6f0fe3":"# import library\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier()\ndtree.fit(x_train,y_train)\n\ndecision_tree_score = dtree.score(x_test,y_test)\ndt_test_accuracy = dtree.score(x_test,y_test)\n\nprint(\"decision tree test score: \",dtree.score(x_test,y_test))\nprint(\"decision tree train score: \",dtree.score(x_train,y_train))","886ea19a":"# Confusion Matrix for Decision Tree \nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = dtree.predict(x_test)\n\ncm_dt = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_dt, annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","94d0cd0d":"# import library\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50,random_state=20)\nrf.fit(x_train,y_train)\n\nrandom_forest_score = rf.score(x_test,y_test)\nrf_test_accuracy = rf.score(x_test,y_test)\n\nprint(\"random forest test score: \",rf.score(x_test,y_test))\nprint(\"random forest train score: \",rf.score(x_train,y_train))","a2692a24":"# Confusion Matrix for Random Forest\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = rf.predict(x_test)\n\ncm_rf = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_rf,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","4494f6f5":"plt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion Matrices of Classification Models\",fontsize=30)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Classification\")\nsns.heatmap(cm_lr,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors(KNN) Classification\")\nsns.heatmap(cm_knn,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine(SVM) Classification\")\nsns.heatmap(cm_svc,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classification\")\nsns.heatmap(cm_nb,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classification\")\nsns.heatmap(cm_dt,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Classification\")\nsns.heatmap(cm_rf,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.show()","97f6f837":"acc_values = [lr_test_accuracy,knn_test_accuracy,svm_test_accuray,nb_test_accuracy,dt_test_accuracy,rf_test_accuracy]\nmodel_names = [\"logistic regression\",\"KNN\",\"SVM\",\"naive bayes\",\"decision tree\",\"random forest\"]\ncolors = [\"green\",\"red\",\"blue\",\"orange\",\"yellow\",\"brown\"]\n\nfig = go.Figure([go.Bar(x=model_names, y = acc_values,marker = dict(color = acc_values))])\nfig.show()","4b5f8201":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.2.\"><\/a>**4.2. K-Nearest Neighbhour(KNN)**","9ef81cb0":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"3.\"><\/a>**3. Data Preprocessing**","37e22134":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"2.\"><\/a>**2. Analyze Data**","d9942cd1":"Import Data","a7a9b580":"We did analyze data and implemented Classification models. If you like it, Please upvote my kernel. If you have any question, I will happy to hear it","dda953f0":"[Go to the Head](#0.)\n# <a class=\"anchor\" id=\"6.\"><\/a>**6. Conclusion**","28c81157":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.5.\"><\/a>**4.5. Decision Tree**","071732dc":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"1.\"><\/a>**1. Import Libraries**","92c39995":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.3.\"><\/a>**4.3. Support Vector Machine(SVM)**","24e11a6f":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.4.\"><\/a>**4.4. Naive Bayes**","f71bd223":"Firstly we need normalize data.\n\n* Normalization Formula = (x - min(x))\/(max(x) - min(x))|","723d38ee":"So, it's correct k value? I don't know. We should try find optimal k value.","b2ca82c3":"Before create model, we need separate data which is test and train.","fe1f824d":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.\"><\/a>**4. Classification Models**","8a55546e":"Information about data","489fa618":"In this kaggle we will do: \n* Logistic Regression Classification\n* K-Nearest Neighbhour(KNN) Classification\n* Support Vector Machine(SVM) Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n* Evaluate Classification all of these models\n* Comparison of Models","862936e1":"We can lok correlation map","53614284":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.1.\"><\/a>**4.1. Logistic Regression**","b3a9a83f":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.5.\"><\/a>**4.6. Random Forest**","5ec41245":"As we see there's not any null data. So we don't need modify data. Then let's analyze data.","a402cfbc":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"5.\"><\/a>**5. Comparison of Models**","40e0376a":"<a class=\"anchor\" id=\"0.\"><\/a>\nWe will aplly classification algoritms on our data\n\n# **Contents**\n\n* [1. Import Libraries](#1.)\n* [2. Analyze Data](#2.)\n* [3. Data Preprocessing](#3.)\n* [4. Classification Models](#4.)\n* * [4.1. Logistic Regression](#4.1.)\n* * [4.2. K-Nearest Neighbhour(KNN)](#4.2.)\n* * [4.3. Suport Vector Machine(SVC)](#4.3.)\n* * [4.4. Naive Bayes](#4.4.)\n* * [4.5. Decision Tree](#4.5.)\n* [5. Comparison of Models](#5.)\n* [6. Conclusion](#6.)"}}