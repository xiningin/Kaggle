{"cell_type":{"4ad715bd":"code","600c7dec":"code","e163a996":"code","14c7eec9":"code","e9bed14a":"code","2992034f":"code","4e222920":"code","65018e3e":"code","3adf23bf":"code","37317f0c":"code","6f9c9ed4":"code","02855ae6":"code","77253be1":"code","e3e07b76":"code","e7caa6cc":"code","8a20b67a":"code","bd177b70":"code","9f383529":"code","c2cc3e9d":"code","51bf8de4":"code","add1f51e":"code","b444a6dc":"code","1b28dabb":"markdown","52747cfd":"markdown","9e54ff3a":"markdown","1dc47102":"markdown","30e7e7a3":"markdown","a938102c":"markdown","c172975c":"markdown","c1797d4e":"markdown","80a10449":"markdown","3b11f054":"markdown","5459a13a":"markdown","4a236dd5":"markdown","b1c7fa20":"markdown","a40a004f":"markdown","6a2cf177":"markdown","54c7f92e":"markdown","a76b1fb6":"markdown","9f1d12d3":"markdown","deb92563":"markdown","e233b0ba":"markdown","6aad734a":"markdown","6f3ec995":"markdown","cc00b2f3":"markdown","f1598878":"markdown","ecee31af":"markdown","37f1b2e6":"markdown","9bc2ff2a":"markdown"},"source":{"4ad715bd":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nimport itertools","600c7dec":"df = pd.read_csv('..\/input\/5minute-crafts-video-views-dataset\/5-Minute Crafts.csv')\ndf['views_per_day'] = df['total_views'] \/ df['active_since_days']\ndf = df[['title', 'num_chars', 'num_words', 'num_punctuation', 'num_words_uppercase', 'num_words_lowercase', 'num_stopwords', \n         'avg_word_len', 'contain_digits', 'startswith_digits', 'title_sentiment', 'views_per_day']]\n\nprint(\"Number of movies: \", df.shape[0])","e163a996":"df['contain_upper'] = np.where(df['num_words_uppercase']>0, 1, 0)\ndf['contain_lower'] = np.where(df['num_words_lowercase']>0, 1, 0)\ndf['contain_puncation'] = np.where(df['num_punctuation']>0, 1, 0)\n\ndf['ratio_punctation'] = df['num_punctuation'] \/ df['num_chars']\ndf['ratio_stopwords'] = df['num_stopwords'] \/ df['num_words']\n\ndf['is_shorts'] = np.where(df['title'].str.contains(\"#shorts\"), 1, 0)\ndf['is_question'] = np.where(df['title'].str.contains(\"?\", regex = False), 1, 0)","14c7eec9":"df = df[['num_chars', 'num_words', 'num_punctuation', 'num_words_uppercase', 'num_words_lowercase', 'num_stopwords', 'avg_word_len', \n         'contain_digits', 'startswith_digits', 'title_sentiment', 'contain_upper', 'contain_lower', 'contain_puncation',\n         'ratio_punctation', 'ratio_stopwords', 'is_shorts', 'is_question', 'views_per_day']]\n\ndf.head()","e9bed14a":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(1.32, 800, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(1.32, 800, 'Logartimic scale', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.histplot(ax = ax0, x = df['views_per_day'], zorder = 3, linewidth = 0.6, bins = 25, log_scale = True, color = \"orange\")\nax0_sns.set_xlabel(\"Logarithmic number of views per day\",fontsize = 6)\nax0_sns.set_ylabel(\"Number of movies\",fontsize = 6)\nax0.grid(which = 'major', axis = 'x', zorder = 0, color = '#e0e0e0', lw = 0.2)\nax0.grid(which = 'major', axis = 'y', zorder = 0, color = '#e0e0e0', lw = 0.2)\nax0_sns.tick_params(labelsize = 4.5)\nplt.show()","2992034f":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.2, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.2, 'Logartimic scale, by \"contain_digits\" and \"startswith_digits\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = df['contain_digits'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d7d3e\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 6)\nax0_sns.set_xlabel(\"Is title contain a digit?\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[0, 1])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = df['startswith_digits'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d7d3e\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"\")\nax1_sns.set_xlabel(\"Is title starts with digit?\",fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","4e222920":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.2, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.2, 'Logartimic scale, by \"contain_upper\" and \"contain_lower\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = df['contain_upper'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d567d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 6)\nax0_sns.set_xlabel(\"Is title contain word starts with upper?\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[0, 1])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = df['contain_lower'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d567d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"\")\nax1_sns.set_xlabel(\"Is title contain word starts with lower?\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","65018e3e":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.2, 'Distribution of views per day', color = 'black', fontsize = 7, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.2, 'Logartimic scale, by \"contain_puncation\" feature', color = 'black', fontsize = 5.8, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = df['contain_puncation'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#6e7d2d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Is title contain any punctation mark?\", fontsize = 5)\nax0_sns.tick_params(labelsize = 3.8)\nplt.show()","3adf23bf":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.2, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.2, 'Logartimic scale, by \"is_shorts\" and \"is_question\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = df['is_shorts'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#7d352d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 6)\nax0_sns.set_xlabel(\"Is movie shorter than 60 seconds?\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[0, 1])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = df['is_question'].astype(str), y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#7d352d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"\")\nax1_sns.set_xlabel(\"Is title a question?\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","37317f0c":"box1 = pd.cut(df['num_chars'], range(10, 111, 12), include_lowest = True, right = False)\nbox2 = pd.cut(df['num_words'], range(2, 25, 3), include_lowest = True, right = False)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace = 0, hspace = 0.35)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.7, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.7, 'Logartimic scale, by \"num_chars\" and \"num_words\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = box1, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d7d3e\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Number of chars in the title\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[1, 0])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = box2, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d567d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax1_sns.set_xlabel(\"Number of words in the title\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","6f9c9ed4":"box3 = pd.cut(df['num_punctuation'], range(0, 7, 1), include_lowest = True, right = False)\nbox4 = pd.cut(df['num_stopwords'], range(0, 9, 1), include_lowest = True, right = False)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace = 0, hspace = 0.35)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.7, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.7, 'Logartimic scale, by \"num_punctuation\" and \"num_stopwords\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = box3, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#6e7d2d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Number of punctuation marks in the title\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[1, 0])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = box4, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#7d352d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax1_sns.set_xlabel(\"Number of stopwords in the title\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","02855ae6":"box5 = pd.cut(df['num_words_uppercase'], range(0, 17, 2), include_lowest = True, right = False)\nbox6 = pd.cut(df['num_words_lowercase'], range(0, 13, 2), include_lowest = True, right = False)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace = 0, hspace = 0.35)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 10**7.7, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.5, 10**7.7, 'Logartimic scale, by \"num_words_uppercase\" and \"num_words_lowercase\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.boxplot(ax = ax0, x = box5, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d7d3e\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.7, markersize = 1, marker = 'o'))\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Number of words starting with uppercase\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[1, 0])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.boxplot(ax = ax1, x = box6, y = df['views_per_day'], zorder = 3, linewidth = 0.6, color = \"#2d567d\",\n                     flierprops = dict(markerfacecolor = '0.55', alpha = 0.3, markersize = 0.6, marker = 'o'))\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax1_sns.set_xlabel(\"Number of words starting with lowercase\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","77253be1":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.3, hspace = 0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(3.3, 10**7.3, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(3.3, 10**7.3, 'Logartimic scale, by \"avg_word_len\" and \"title_sentiment\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.scatterplot(ax = ax0, x = df['avg_word_len'], y = df['views_per_day'], edgecolor = \"black\", linewidth = 0.1, color = \"#6e7d2d\", s = 4.5)\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Average word length\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[0, 1])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.scatterplot(ax = ax1, x = df['title_sentiment'], y = df['views_per_day'], edgecolor = \"black\", linewidth = 0.1, color = \"#7d352d\", s = 4.5)\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax1_sns.set_xlabel(\"Title sentiment\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","e3e07b76":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.3, hspace = 0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.008, 10**7.3, 'Distribution of views per day', color = 'black', fontsize = 8, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(-0.008, 10**7.3, 'Logartimic scale, by \"ratio_punctation\" and \"ratio_stopwords\" features', color = 'black', fontsize = 6.5, ha = 'left', va = 'top')\nax0_sns = sns.scatterplot(ax = ax0, x = df['ratio_punctation'], y = df['views_per_day'], edgecolor = \"black\", linewidth = 0.1, color = \"#2d7d3e\", s = 4.5)\nax0_sns.set_yscale('log')\nax0_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax0_sns.set_xlabel(\"Ratio of punctation marks\", fontsize = 6)\nax0_sns.tick_params(labelsize = 4.5)\n\nax1 = fig.add_subplot(gs[0, 1])\nax1.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1_sns = sns.scatterplot(ax = ax1, x = df['ratio_stopwords'], y = df['views_per_day'], edgecolor = \"black\", linewidth = 0.1, color = \"#2d567d\", s = 4.5)\nax1_sns.set_yscale('log')\nax1_sns.set_ylabel(\"Number of views per day\", fontsize = 5)\nax1_sns.set_xlabel(\"Ratio of stopwords\", fontsize = 6)\nax1_sns.tick_params(labelsize = 4.5)\n\nplt.show()","e7caa6cc":"X = df.drop(columns = ['views_per_day'])\nY = df['views_per_day']\n\n\nchoices = [1, 0]\ndivide = np.where(df['views_per_day'] > 5*df['views_per_day'].mean(), \"High\", \"Low\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify = divide)\n\nprint(\"Number of rows in train dataset: \" + str(X_train.shape[0]))\nprint(\"Number of rows in test dataset: \" + str(X_test.shape[0]))","8a20b67a":"gbm = lgb.LGBMRegressor(n_estimators = 100, random_state = 123)\ngbm.fit(X_train, y_train, eval_set = [(X_test, y_test)], eval_metric = 'mean_squared_error', callbacks = [lgb.early_stopping(10)])\n\ntrain_results1 = gbm.predict(X_train)","bd177b70":"y_pred = gbm.predict(X_test, num_iteration = gbm.best_iteration_)\n\nrmse_train1 = round(mean_squared_error(y_train, train_results1) ** 0.5, 0)\nrmse_test1 = round(mean_squared_error(y_test, y_pred) ** 0.5, 0)\n\nprint(f'------------------------ Train dataset ------------------------')\nprint(f'The RMSE of prediction from 1st model is: {rmse_train1}' + ' views per day' + \"\\n\")\nprint(f'------------------------ Test dataset -------------------------')\nprint(f'The RMSE of prediction from 1st model is: {rmse_test1}' + ' views per day')","9f383529":"estimator = lgb.LGBMRegressor(random_state = 123)\n\nparam_grid = {\n    'boosting_type': ['gbdt', 'goss'],\n    'num_leaves': list(range(60, 100, 20)),\n    'learning_rate': list(np.logspace(np.log10(0.0001), np.log10(0.01), base = 10, num = 3)),\n    'min_child_samples': list(range(10, 30, 10)),\n    'colsample_bytree': list(np.linspace(0.5, 0.9, num = 3)),\n    'subsample': list(np.linspace(0.5, 0.9, num = 3))\n}\n\ngbm_grid = GridSearchCV(estimator, param_grid, cv = 5)\ngbm_grid.fit(X_train, y_train)\n\nprint(f'Best parameters found by grid search are: {gbm_grid.best_params_}')","c2cc3e9d":"gbm_final = lgb.LGBMRegressor(**gbm_grid.best_params_, random_state = 123, n_estimators = 1000)\ngbm_final.fit(X_train, y_train, eval_set = [(X_test, y_test)], eval_metric = 'mean_squared_error', callbacks = [lgb.early_stopping(10), lgb.log_evaluation(20)])\n\ntrain_results2 = gbm_final.predict(X_train)","51bf8de4":"y_pred2 = gbm_final.predict(X_test, num_iteration = gbm_final.best_iteration_)\n\nrmse_train2 = round(mean_squared_error(y_train, train_results2) ** 0.5, 0)\nrmse_test2 = round(mean_squared_error(y_test, y_pred2) ** 0.5, 0)\n\nprint(f'------------------------ Train dataset ------------------------')\nprint(f'The RMSE of prediction from final model is: {rmse_train2}' + ' views per day' + \"\\n\")\nprint(f'------------------------ Test dataset -------------------------')\nprint(f'The RMSE of prediction from final model is: {rmse_test2}' + ' views per day')","add1f51e":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.3, hspace = 0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(min(y_pred2)-900, 10**6.8, 'Distribution of values', color = 'black', fontsize = 7, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(min(y_pred2)-900, 10**6.8, 'predicted and true, on the test set', color = 'black', fontsize = 5.5, ha = 'left', va = 'top')\nax0_sns = sns.scatterplot(ax = ax0, x = y_pred2, y = y_test, edgecolor = \"black\", linewidth = 0.1, color = \"grey\", s = 4.5)\nplt.plot([0, 10**6.2], [0, 10**6.2], linewidth = 0.3, color = \"red\")\nax0_sns.set_yscale('log')\nax0_sns.set_xscale('log')\nax0_sns.set_ylabel(\"True values\", fontsize = 5.5)\nax0_sns.set_xlabel(\"Predicted values\", fontsize = 5.5)\nax0_sns.tick_params(labelsize = 4.5)\nplt.show()","b444a6dc":"importance = pd.DataFrame({'Value' : gbm_final.feature_importances_,'Feature' : X_train.columns.tolist()}).sort_values(by = \"Value\", ascending = False)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize = (5, 3), facecolor = 'white')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.3, hspace = 0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(\"white\")\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\n\nax0.text(0, -1.5, 'Feature importance ', color = 'black', fontsize = 7, ha = 'left', va = 'bottom', weight = 'bold')\nax0.text(0, -1.5, 'final lightGBM model', color = 'black', fontsize = 5.5, ha = 'left', va = 'top')\nax0_sns = sns.barplot(ax = ax0, y = importance['Feature'], x = importance['Value'], edgecolor = \"black\", linewidth = 0.3, color = \"grey\", orient = \"h\")\nax0_sns.set_ylabel(\"Feature name\", fontsize = 5.5)\nax0_sns.set_xlabel(\"Feature importance\", fontsize = 5.5)\nax0_sns.tick_params(labelsize = 3.5)\nplt.show()","1b28dabb":"<DIV align=\"justify\">We're going to numeric features. In the database, there are two types of them: discrete and continuous so we start with discrete. We also make boxplots, but firstly we divide our variable into parts (from lowest to highest) and create around 5-10 boxplots to see if there are any relationships between the analyzed feature and the number of views per day. We're starting with the number of characters in the title (range from 11 to 100). We can see that in short and middle titles (up to 80 characters) there is no relation between the number of chars and the number of views, but movies with titles longest than 80 chars have more views (on average, higher median and quartiles). The second variable is the number of words (range from 3 to 20) - here we can see the same relation, no impact of number views in shorts and middle titles, but in long titles (over 17 words), the number of views per day is higher than in other movies. Both features look as if they could have an impact on the model, especially in cases where the movies have long titles.<\/DIV>","52747cfd":"<DIV align=\"justify\">Now we make some feature engineering on our hand. We're starting with creating bool features based on some features mention before. We're checking if in the title any word starts with uppercase (contain_upper) or lowercase (contain_lower) or if the title contains any punctuation (contain_puncation). Next, let's divide the number of punctuation by the number of chars to check the proportion of punctuation marks in the title (ratio_punctation). In the same way, we're checking the proportion of stopwords in all words (ratio_stopwords). On YouTube, if we have \"#shorts\" in the title, it means that the movie is no longer than 60 seconds, so we check if the title contains this hashtag (is_shorts). In a similar way we're checking if there is any question in the title what do we conclude from whether a question mark appears (is_question).<\/DIV>","9e54ff3a":"<DIV align=\"justify\">When we have a prepared train and test dataset, we can build a model because all features are ready (recode to 0-1 or in numeric form). The algorithm we have chosen to create the model is GBM. A Gradient Boosting Decision tree or a GBDT is a very popular machine learning algorithm that has effective implementations like XGBoost and many optimization techniques are actually adopted from this algorithm. The efficiency and scalability of the model are not quite up to the mark when there are more features in the data. For this specific behavior, the major reason is that each feature should scan all the various data instances to make an estimate of all the possible split points which is very time-consuming and tedious. We are starting with creating the most basic GBM model with 100 trees using a \"lightGBM\" library.<\/DIV>","1dc47102":"<DIV align=\"justify\">Root mean square error on train set is higher than on test set, so it looks like the basic model is not overfitted. The value of RMSE is over 40,000 what means that model mistakes in predicting the average number of daily views by an average of more than 40,000, which is not a good result. Our model needs tuning so is time to change hyperparameters.<\/DIV>","30e7e7a3":"**Thanks for reading my kernal!**\n\n**If you have any suggestions for improving the analysis, let me know in the comment!**\n\n**If you liked my kernel, give upvote!**\n\n**If you have a moment, I encourage you to see at my other [projects](https:\/\/www.kaggle.com\/michau96\/code).**","a938102c":"<DIV align=\"justify\">The next bool feature is \"contain_puncation\" which informs us if the title contains any punctuation mark (around 30% of the titles have at least one - value \"1\" in a variable). Looking at the distribution of the mean daily number of views by these features it turns out that there is no significant difference. Whether the title contains at least one punctuation mark does not seem to affect the explanation of the average number of views per day.<\/DIV>","c172975c":"<DIV align=\"justify\">The last two bool features are: \"is_shorts\" (1 if a video is no longer than 60 seconds, contain \"#shorts\") and \"is_question\" (1 if there is a question mark in the title). The first variable has really high difference - shorter movies have much more views in general. Median and quartiles are almost 200 higher in this video and all top 10 views are no longer than 1 minute. It looks like this feature can be the most important in future models. The last feature has no big differences - median views are similar in movies with and without question marks. For movies where is no question, distribution is much wider and top 10 movies don't have \"?\", but it doesn't look like it will have big importance.<\/DIV>","c1797d4e":"<DIV align=\"justify\">For the analysis and creating model, we'll use Python in its latest version. We'll use pandas and numpy packages for data manipulation, X and Y for data visualization and tensorflow for creating a machine learning models. .<\/DIV>","80a10449":"<DIV align=\"justify\">Now we check what the error distribution looks like by comparing the true values of the test set and those predicted by the model using a scatterplot. We can notice that the forecasted values are very largely between 2,000 and 10,000, but they are often overestimated (mainly the model forecasts too high values for what they are). The model, on the other hand, does quite well at detecting very large values. The model can detect some dependencies based on the title and certainly assigns values better than random, but it seems that the discrepancies are too large to try to apply such a model in practice.<\/DIV>","3b11f054":"<DIV align=\"justify\">Now let's check the distribution of views per day by bool features. In each feature, we make 2 boxplots to check the distribution of explained feature (for \"0\" and \"1\") and compare to assess whether the levels of the feature will affect the number of views. Again, we need to make a logarithmic axis to better see the differences. The feature that informs as if a title has any number has a higher median, 1st and 3rd quartile, minimum and maximum views per if there wasn't any digit. There is no doubt that this feature has potential in a model (almost 8x higher values for no digits). Next feature that informs us if the title of the movie starts with a digit has not such a big difference - also median is higher for \"0\" values, but 1st quartile is almost the same. It's worth adding that most movies (over 80%) contain or start with a digit.<\/DIV>","5459a13a":"<DIV align=\"justify\">The last type of feature is numeric with a ratio scale (there are 4 features). For these ones, we're making scatterplots to find trends or relationships. The first variable is the average world's length (the number of chars divides by the number of words). Most of the title has values between 4 and 7 but the maximum is around 10. After looking at scatterplot, it looks like there is no relationship between this feature and the number of views per day. Movies with the most views have an average between 6 and 9. The next feature is the sentiment of the title (the higher value, the more positive tone of the title). A lot of titles have a value equal to 0 and 1, but they can take on non-integer values (from -1 to 1). There is also no clear relationship between this feature and the number of views per day.<\/DIV>","4a236dd5":"<DIV align=\"justify\">We're stoping the next iteration if the model doesn't improve after 10 rounds. We can see that it happened much faster than in 100 round (just 3 round was the best). We're going to check the root mean square error of this model on the train and test dataset.<\/DIV>","b1c7fa20":"<DIV align=\"justify\">YouTube is for sure place on the internet where clickbait make a huge diffrence. Words in title of movie can change attitude of viewers and depend how many of them click on video and at least start watching and let earn money for creator from ads. Today we'll try to analyst title of almost 5,000 of YouTube from one channel - \"5-Minute Crafts\". We'll try to find what attributes of title impact of number of views and try build model which predict number of views (per day to standarize) based only on title. Enjoy!<\/DIV>","a40a004f":"<DIV align=\"justify\">Finally, we have 17 explanatory features on the basis of which we'll build a AI model explaining the number views per day. It's now time for short data exploratory.<\/DIV>","6a2cf177":"<DIV align=\"justify\">On the last chart, we're focusing on two last explanatory features. The ratio of punctuation marks - it takes values from 0 to 1, but the maximum value in this set is about 0.14 (which means that every 7th character in the title is a punctuation mark). Again, there is looks like this feature will not be important in the model, it's hard to find a clear relationship. It's worth to notice, that movies with the most views have a low ratio, but never equal to 0. The last feature is also ratio, but now it's the number of stopwords divided by the number of all words in the title. Values might be between 0 and 1, but the maximum in this dataset of movies is around 0.7. Again it's hard to find any relationship. All 4 features on this type of scale, unfortunately, are not related to the label feature and it doesn't look like they will have a high impact on the model.<\/DIV>","54c7f92e":"<DIV align=\"justify\">We're starting with an explainable variable which is the number of views per day (1 movie = 1 row in database). This feature has a really strong right-sided asymmetry because few movies have a much bigger number of views than the rest of the movies. So to see anything useful on a chart, we need to put this variable on the logarithmic axis. The histogram of this variable shows that if we take the logarithm of number views per day the distribution is symmetrical and close to normal. It means that for sure when we'll creating a model it's important to take this information into account when choosing the measure by which the model is to learn (minimizing RMSE will make the model too easy to skip these extreme cases of videos with huge views).<\/DIV>","a76b1fb6":"<DIV align=\"justify\">We'll optimize some parameters: boosting type, number of leaves, learning rate, minimum child samples, alpha, lambda, column samples by tree and subsample size. For each argument, we try to choose at least two values, but at the same time we try not to expand the number of iterations through which the algorithm must go through to speed up the operation. We will use \"GridSearch\" to search for the best set of hyperparameters, and we'll optimize according to cross-validation on five folds.<\/DIV>","9f1d12d3":"<DIV align=\"justify\">A next feature is the number of punctuation marks that appeared in the title (range from 0 to 6). This feature similar to \"is_question\" looks like has no impact on the number of views per day. Median and quartiles have similar values for all numbers of punctuation marks. Another feature is the number of stopwords in the title (range from 0 to 9). Here for less than 6 such words quartile distribution of the number of views is very similar, but for more than 5 stopwords first quartile and median are higher, so titles with a high number of stopwords have (in general) more views.<\/DIV>","deb92563":"<DIV align=\"justify\">Finally, we check which variables turned out to be the most important in forecasting the average daily number of views. The most important variable in the model turned out to be the mean word length, which seems to be quite unexpected in the preliminary analysis. The second and third places are taken by the number of characters in the title and title sentiment. Interestingly, variables such as whether the title contains a question mark and whether the title contains at least one word beginning with a upper or lower letter turned out to be completely useless - the model did not take them into account when splitting the tree.<\/DIV>","e233b0ba":"<DIV align=\"justify\">Dataset contain 4978 rows, where 1 row discribe on movie from <a href=\"https:\/\/www.youtube.com\/c\/5MinuteCraftsYouTube\/videos\">5MinuteCrafts youtube channel<\/a>. This channel has almost 75 millions subscribers (12\/2021) and publishes short movies. In descrption on YouTube channel we can read \"fun diy-projects, crafts, experience the joy of doing it yourself!\". Our goal is to predict the number of views based only on the title, so we start with removing all columns which are not related to the title. Some of the features in the database are created based on the title: number of characters (num_chars), number of words (num_words), number of punctuation (num_punctuation), number of words starting with uppercase (num_words_uppercase), number of words starting with lowercase (num_words_lowercase), number of stopwords (num_stopwords), the average number of character in word (avg_word_len), bool feature that informs if title contains any digit (contain_digits), bool feature that unforms if title starts with digit (startswith_digits) and sentiment of title (title_sentiment). The next step is to create a uniform measure that describe properly popularity of a movie - we divide the number of total views by the number of days since publication and then we have the mean views per day.<\/DIV>","6aad734a":"<DIV align=\"justify\">We're starting the modeling process. The first step is to divide a full dataset into train and test datasets. We assume that the proportion of the division is 70\/30, but in such a way that the distribution of the dependent variable will be similar. The division will be created in that way to place the same proportion of high values (over 5*mean number of views) in both sets to create an opportunity for the model to both learn and test to predict very large values.<\/DIV>","6f3ec995":"<DIV align=\"justify\">After a few long calculations, we managed to obtain a set of parameters that we will now use in the final model. In addition to these parameters, we set a maximum of 1000 trees, but we decide to stop or not if the model fails to improve the results after 20 trees in a row. As in the first model, we'll minimize the mean square error.<\/DIV>","cc00b2f3":"Sources:\n\n1. https:\/\/www.youtube.com\/c\/5MinuteCraftsYouTube\/videos\n2. https:\/\/www.kaggle.com\/shivamb\/5minute-crafts-video-views-dataset\n3. https:\/\/www.kaggle.com\/dwin183287\/code\n4. https:\/\/seaborn.pydata.org\/\n5. https:\/\/htmlcolors.com\/google-color-picker\n6. https:\/\/www.analyticsvidhya.com\/blog\/2021\/08\/complete-guide-on-how-to-use-lightgbm-in-python\/\n7. https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/examples\/python-guide\/sklearn_example.py\n8. https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html\n9. https:\/\/www.kaggle.com\/willkoehrsen\/intro-to-model-tuning-grid-and-random-search\n10. https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html\n11. https:\/\/www.kaggle.com\/nicapotato\/lgbm-cv-tuning-and-seed-diversification\n12. https:\/\/stackoverflow.com\/questions\/53413701\/feature-importance-using-lightgbm","f1598878":"<DIV align=\"justify\">The last two features in this type (interval scale) are the number of words starting with uppercase (range from 0 to 18) and lowercase (range from 0 to 12). In uppercases, there are no significant differences in median level. In some intervals, there are wider and narrower quartile distributions, but it doesn't look like this feature will be very important in the model. In the last feature, the highest median of the number of views per day is when there is a 2 or 3 words starts with lowercases. After that (4 or more such words) number of views is decreasing and stabilized, but for sure there is a stronger relationship with label feature than in no. words starting with uppercases.<\/DIV>","ecee31af":"<DIV align=\"justify\">The next 2 bool features are: \"contain_upper\" and \"contain_lower\" which inform us if any word in the title starts with uppercase or lowercase. When it comes to uppercase there is no significant difference. A little bigger values can be seen where is no word starting with uppercase, but the difference is really small. A similar conclusion can be drawn from boxplots for coll feature for lowercase - no significant difference. Quartile distribution of number of views per day is much wider on titles where there is at least word starting with lowercase, but this feature shouldn't have a big impact on model.<\/DIV>","37f1b2e6":"<DIV align=\"justify\">For the final model created, we also calculate the root mean square error on the training and test set. It turns out that the results are very similar to those without the optimization of hyperparameters, i.e. the RMSE on the test set is on average over 41,000 daily views, which is still not a satisfactory result.<\/DIV>","9bc2ff2a":"<DIV align=\"justify\">Finally, we summarize the entire analysis. Our goal was to create a prediction model that would predict the average daily number of views of a video from the \"5-minute crafts\" channel on YouTube based only on its official title. The ambitious task started with a substantive analysis and the creation of new variables, then, going through various preliminary analyzes, we reached the stage of creating the GBM model. By minimizing the mean square error, the model, looking at the results on the training and test set, unfortunately, turned out to be not the best, even after optimizing the hyperparameters (on average it was wrong by over 30,000 impressions per day). The fact that the model performed on average does not mean that it assigned values randomly, but it had problems with detecting some dependencies and its application in practice seems doubtful. A valid solution for the future is to include other variables besides the title in the model (e.g. date of publication, the popularity of the channel in the selected period of time, number of previously published films, etc.). The title will certainly be an important factor in such a predictive model, but basing the entire prognosis on one sentence of the text turned out to be too ambitious.<\/DIV>"}}