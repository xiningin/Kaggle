{"cell_type":{"15ed3e01":"code","38df80f1":"code","5e563506":"code","5c7a1d3b":"code","aecc64ad":"code","f78346bb":"code","6820cbfb":"code","1e8877a3":"code","4ca62266":"code","626f0480":"code","a65d0646":"code","bffe19b4":"code","387d0acd":"code","fd6c4293":"code","ea3dc1f2":"code","83e0f62e":"code","ecee5981":"code","5ad8ff1c":"code","a79b44ce":"code","502afe58":"code","14b62f64":"code","8f61a336":"markdown","e4878beb":"markdown","6ee50786":"markdown","bc404d99":"markdown","4047e6be":"markdown","5a0c2f86":"markdown","bd3ee3a1":"markdown","2e3e95aa":"markdown","455f2ab6":"markdown","d48a8dcb":"markdown","f4e21e62":"markdown","52ef102f":"markdown","9ccb3dec":"markdown","a524ed7a":"markdown","33ae1399":"markdown","d8003591":"markdown","0053c235":"markdown"},"source":{"15ed3e01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38df80f1":"train=pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv',index_col=0)","5e563506":"print(train.dtypes)\nprint(train.shape)\nprint(train.describe(include='all'))","5c7a1d3b":"#Check if there are missing values in the target variable if true then drop them\nprint(train['RainTomorrow'].isna().sum()\/len(train))\ntrain=train.dropna(axis=0,subset=['RainTomorrow'])","aecc64ad":"\nprint(train.shape)","f78346bb":"y=train[['RainTomorrow']]\nX=train.drop(['RainTomorrow'],axis=1)","6820cbfb":"print(X.shape)","1e8877a3":"#Let us define few functions for visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef boxplot(df):\n    plt.figure(figsize=(10,6))\n    plt.title('Boxplot')\n    sns.boxplot(df)\n    plt.show()\n\ndef histogram(df):\n    plt.figure(figsize=(10,6))\n    sns.displot(df,kde=True)\n    plt.title('Histogram')\n    sns.despine()\n    plt.show\n\ndef countplot(df):\n    plt.figure(figsize=(10,6))\n    sns.countplot(df,palette='spring')\n    plt.title('Countplot')\n    plt.show()\n    ","4ca62266":"print(X.dtypes)","626f0480":"#Visualize the numeric variables (Boxplot)\n\nnum_col=X.select_dtypes(include=[np.number]).columns\nnum_cols=[n for n in num_col]\n \nfor n in num_cols:\n    boxplot(X[n])","a65d0646":"#Distribution of the numeric variables\n\nfor n in num_cols:\n    histogram(X[n])","bffe19b4":"print(num_cols)","387d0acd":"#Count plot of the categorical variables\ncat_col=X.select_dtypes(exclude=[np.number]).columns\ncat_cols=[ c for c in cat_col]\n\nfor c in cat_cols:\n    countplot(X[c])\n","fd6c4293":"print(X.select_dtypes(include=[np.number]).isna().sum()\/len(X))\nprint('************************************************')\nprint(X.select_dtypes(exclude=[np.number]).isna().sum()\/len(X))","ea3dc1f2":"print(y.value_counts())\ncountplot(y['RainTomorrow'])","83e0f62e":"from sklearn.model_selection import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,test_size=.25,stratify=y,random_state=99)","ecee5981":"print(X_train.shape, X_valid.shape)\nprint(y_train.value_counts())","5ad8ff1c":"#Apply preprocessing to the numeric variables\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer\n\nfor c in num_cols:\n    im_n=SimpleImputer(strategy='median')\n    X_train[c]=im_n.fit_transform(X_train[[c]])\n    X_valid[c]=im_n.transform(X_valid[[c]])\n\nfor n in num_cols:\n    qt=QuantileTransformer(output_distribution='normal',random_state=99)\n    X_train[n]=qt.fit_transform(X_train[[n]])\n    X_valid[n]=qt.transform(X_valid[[n]])\n    \nprint(X_train.isna().sum())       ","a79b44ce":"#Preprocessing for categorical variables\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfor c in cat_cols:\n    im_c=SimpleImputer(strategy='most_frequent')\n    X_train[c]=im_c.fit_transform(X_train[[c]])\n    X_valid[c]=im_c.transform(X_valid[[c]])\n\nfor c in cat_cols:\n    oe=OrdinalEncoder(dtype=int)\n    X_train[c]=oe.fit_transform(X_train[[c]])\n    X_valid[c]=oe.transform(X_valid[[c]])","502afe58":"print(X_train.head(n=6))","14b62f64":"from catboost import CatBoostClassifier,Pool\n\ncat_features = [X_train.columns.get_loc(col) for col in cat_cols]\nprint(cat_features)\n\ntrain_x=Pool(data=X_train,label=y_train,cat_features=cat_features)\nvalid_x=Pool(data=X_valid,label=y_valid,cat_features=cat_features)\n\nparams = {'loss_function':'Logloss',\n          'learning_rate':0.03,\n          'depth':7,\n          'n_estimators':10000,\n          'eval_metric':'AUC',\n          'od_type': 'Iter',\n          'od_wait':1000,\n          'verbose':200,\n          'one_hot_max_size':0,\n          'class_weights':(1,3.4),\n          'random_state':99\n         }\n#Fit the random forest learner\ncb=CatBoostClassifier(**params)\ncb.fit(train_x,eval_set=valid_x,use_best_model=True,plot=True)","8f61a336":"# Let us perform EDA on the dataset","e4878beb":"# Check whether the dependent variable is balanced or not","6ee50786":"# Divide the dataset into X and y","bc404d99":"# Apply preprocessing ","4047e6be":"Clearly the stratification is maintained here also.","5a0c2f86":"All the preprocessing is completed without any data leakage","bd3ee3a1":"From boxplot we can see the only 4 columns have no outliers. Sunshine,WindSpeed3pm , Cloud9am and Cloud3am. In the next step we will visualize the distribution of the variables.","2e3e95aa":"# Perform Train Test split","455f2ab6":"The columns ['Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am','Cloud9am', 'Cloud3pm'] are not symmetric in nature. We'll apply preprocessing later. We'll not that much be oncerned about outlers as we'll implement Boosting learner.","d48a8dcb":"Evaporation and Cloud3pm has 43 and 40 percent missing values. Sunshine also has 48 percent missing values and Cloud9am has 37 percent missing values. We'll impute these values later. What strategy to use for imputation depends on whether outliers exists ir not for that variable.","f4e21e62":"The dataset is highly imbalaced. Thus we'll split the data in a stratified manner.","52ef102f":"> # Modeling using catBoostClassifier","9ccb3dec":"We see thet the RainToday column is imbalanced.","a524ed7a":"# Check for missing values\n","33ae1399":"# Simple Catboost model performs very well on the validation data. ","d8003591":"Clearly the missing values are imputed with median strategy","0053c235":"# Load the dataset and explore"}}