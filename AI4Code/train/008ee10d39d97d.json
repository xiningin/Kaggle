{"cell_type":{"db48916a":"code","83974493":"code","ca6fb856":"code","bd14ddcf":"code","ed37012b":"code","0a5acaea":"code","66fac938":"code","5ac69d49":"code","3e846c8e":"code","fda4dff2":"code","686fd981":"code","87c6eeb1":"code","469fd11b":"code","bd44f9f4":"code","b5ba9f46":"code","2c5c1a44":"code","1ccebe4b":"code","f78e0034":"code","11df0226":"code","dae38454":"code","230875cd":"code","39ea516d":"code","0f699913":"code","7b4829a4":"code","99bbe4c3":"code","5f0de99b":"code","7f0e9ebf":"code","68e40dc9":"code","c5907e58":"code","bb6c37a6":"code","3b831d5a":"code","970aa161":"code","d74ff402":"code","1ee854bf":"code","34dbc5d4":"code","73ba1580":"code","c3265fba":"code","26437e56":"code","a8fa169b":"code","6be798a1":"code","27f1d1d0":"code","279764f7":"code","2501c998":"code","60edd30a":"code","22eb5f53":"code","99a1a112":"code","f2a8f6ad":"code","59ecabf7":"code","694c0399":"code","817796e5":"code","eb66a9e5":"code","3817bce1":"code","e7ba28d6":"code","8a2b3978":"markdown","ee14cdae":"markdown","80cf44f0":"markdown","29e6e8a0":"markdown","dcb0645f":"markdown","9d73d1bd":"markdown","829a5e70":"markdown","afec1687":"markdown","ad562a89":"markdown","48da74f2":"markdown","31fd50a0":"markdown","eb0f88b8":"markdown","278c4384":"markdown","64b3db15":"markdown","601c424e":"markdown","eae64508":"markdown","dee9d6f9":"markdown","9e8bc395":"markdown","9c0a25bb":"markdown","389b100b":"markdown","7438a352":"markdown","dfab8116":"markdown"},"source":{"db48916a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","83974493":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","ca6fb856":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","bd14ddcf":"folder_path = '..\/input\/ieee-fraud-detection\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv', index_col='TransactionID')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv', index_col='TransactionID')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv', index_col='TransactionID')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')","ed37012b":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\ndel train_identity, train_transaction, test_identity, test_transaction\ngc.collect()","0a5acaea":"def id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe","66fac938":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']","5ac69d49":"train = id_split(train)\ntest = id_split(test)","3e846c8e":"cols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionDT')\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","fda4dff2":"columns_a = ['TransactionAmt', 'id_02', 'D15']\ncolumns_b = ['card1', 'card4', 'addr1']\n\nfor col_a in columns_a:\n    for col_b in columns_b:\n        for df in [train, test]:\n            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('mean')\n            df[f'{col_a}_to_std_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('std')","686fd981":"# New feature - log of transaction amount.\ntrain['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - day of week in which a transaction happened.\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n\n# New feature - hour of the day in which a transaction happened.\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24\n","87c6eeb1":"# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","469fd11b":"# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n\n# Encoding - count encoding separately for train and test\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","bd44f9f4":"# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499\n\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","b5ba9f46":"for col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))\n","2c5c1a44":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","1ccebe4b":"X = reduce_mem_usage(X)\n# y = reduce_mem_usage(y)\nX_test = reduce_mem_usage(X_test)","f78e0034":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n#     missing=-999,\n    random_state=51,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n","11df0226":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n#     missing=-999,\n    random_state=51,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n","dae38454":"clf.fit(X, y)","230875cd":"y_train_pred = clf.predict_proba(X)[:,1]\nprint('AUC score: ',roc_auc_score(y, y_train_pred))","39ea516d":"y_pred = clf.predict_proba(X_test)[:,1]\nsample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\nsample_submission['isFraud'] = y_pred\nsample_submission.to_csv('submission1.csv')","0f699913":"import sklearn.metrics as metrics\n\ndef create_eval_metric_df(y,y_pred,model_name='model'):\n    \n    total = np.shape(y)[0]\n    tn, fp, fn, tp = metrics.confusion_matrix(y, y_pred).ravel()\n    tp \/= float(total)\n    tn \/= float(total)\n    fp \/= float(total)\n    fn \/= float(total)\n    roc_auc = metrics.roc_auc_score(y, y_pred)\n    \n    columns_list = ['model_name',\n    \"total_number\", \n    \"cond_pos\", \"cond_neg\", \n    \"true_pos\", \"true_neg\", \"false_pos\", \"false_neg\",\n    \"recall\", \"specificity\", \"precision\", \"neg_pred_val\", \"miss_rate\", \"false_pos_rate\", \"false_disc_rate\", \"false_omission_rate\",\n    \"accuracy\", \"f1_score\", \"matthews_corr_coef\", \"informedness\", \"markedness\",\n    \"roc_auc_score\"\n  ]\n    \n    eval_metrics = [model_name,\n      total,\n      tp+fn, tn+fp,\n      tp, tn, fp, fn,\n      (tp)\/(tp + fn), (tn)\/(tn + fp), (tp)\/(tp + fp), (tn)\/(tn + fn), (fn)\/(fn + tp), (fp)\/(fp + tn), (fp)\/(fp + tp), (fn)\/(fn + tn),\n      (tp + tn), (2*tp)\/(2*tp + fp + fn), (tp*tn - fp*fn)\/(np.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))), (tp)\/(tp + fn) + (tn)\/(tn + fp) - 1, (tp)\/(tp + fp) + (tn)\/(tn + fn) - 1,\n      roc_auc\n    ]\n    \n    df_eval = pd.DataFrame(eval_metrics)\n    df = df_eval.T\n    df.columns = columns_list\n    return df\n","7b4829a4":"y_train_pred = clf.predict(X)","99bbe4c3":"df_eval = create_eval_metric_df(y,y_train_pred,model_name='xgboost1')\nprint(df_eval)","5f0de99b":"import matplotlib.gridspec as gridspec\nimport matplotlib.patches as patches\ndef plot_confusion_matrix_train_test(df_train, df_test, model_names, display_image=False):\n  \"\"\"\n  Plot confusion matrix from given train and test eval metric dataset and list of model.\n  df = eval metric dataset from create_eval_metric_df function\n  model_names = list of model names\n  \"\"\"\n  #model_names = best_models_dict.keys()[:]\n  n_plot = np.shape(model_names)[0]\n\n  fig_h = n_plot*8\n  fig_w = 14\n\n  plot_h = 8\n  plot_w = 10\n\n  plt.close()\n  fig = plt.figure(figsize=(fig_w, fig_h))  \n\n  gs = gridspec.GridSpec(plot_h*n_plot, plot_w, wspace=0, hspace=0)\n\n  common_dict = {\n    'facecolor':'xkcd:light grey', \n    'edgecolor':'k', \n    'fontsize':10\n  }\n\n  gs_coors =[\n    [0, 1, 1, -1],\n    [2, 3, 1, 2],\n    [3, 5, 0, 1],\n    [1, 2, 2, 6],\n    [2, 3, 2, 4],\n    [2, 3, 4, 6],\n    [3, 4, 1, 2],\n    [4, 5, 1, 2],\n    [3, 4, 2, 4],\n    [4, 5, 2, 4],\n    [3, 4, 4, 6],\n    [4, 5, 4, 6],\n    [5, 6, 2, 4],\n    [5, 6, 4, 6],\n    [3, 4, 6, 8],\n    [4, 5, 6, 8],\n    [2, 3, 6, 8],\n    [6, 7, 2, 4],\n    [6, 7, 4, 6],\n    [3, 4, 8, 10],\n    [4, 5, 8, 10],\n    [5, 6, 6, 8],\n    [6, 7, 6, 8]\n  ]\n\n  kwarg_list = [common_dict for l in range(np.shape(gs_coors)[0])]\n  kwarg_list[0] = {\n      'facecolor':'w', \n      'edgecolor':'w', \n      'fontsize':24\n    }\n\n  for i, model_name in enumerate(model_names):\n    print(i)\n\n    pd_mod = df_train[df_train[\"model_name\"] == model_name]\n    pd_mod_test = df_test[df_test[\"model_name\"] == model_name]\n\n    text_lists = [\n      \"{}\".format(model_name),\n      \"Total\\nTrain:{}\\nTest:{}\".format(int(pd_mod[\"total_number\"].sum()), int(pd_mod_test[\"total_number\"].sum())),\n      \"Predicted\",\n      \"True Condition\",\n      \"Condition Positive\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"cond_pos\"].sum()), (pd_mod_test[\"cond_pos\"].sum())),\n      \"Condition Negative\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"cond_neg\"].sum()), (pd_mod_test[\"cond_neg\"].sum())),\n      \"Pred Positive\\nTrain:{:.02%}\\nTest:{:.02%}\".format((pd_mod[\"true_pos\"].sum())+(pd_mod[\"false_pos\"].sum()), (pd_mod_test[\"true_pos\"].sum())+(pd_mod_test[\"false_pos\"].sum())),\n      \"Pred Negative\\nTrain:{:.02%}\\nTest:{:.02%}\".format((pd_mod[\"true_neg\"].sum())+(pd_mod[\"false_neg\"].sum()), (pd_mod_test[\"true_neg\"].sum())+(pd_mod_test[\"false_neg\"].sum())),\n      \"True Pos\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"true_pos\"].sum()), (pd_mod_test[\"true_pos\"].sum())),\n      \"False Neg\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"false_neg\"].sum()), (pd_mod_test[\"false_neg\"].sum())),\n      \"False Pos\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"false_pos\"].sum()), (pd_mod_test[\"false_pos\"].sum())),\n      \"True Neg\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"true_neg\"].sum()), (pd_mod_test[\"true_neg\"].sum())),\n      \"Recall\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"recall\"].sum()), (pd_mod_test[\"recall\"].sum())),\n      \"False Positive Rate\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"false_pos_rate\"].sum()), (pd_mod_test[\"false_pos_rate\"].sum())),\n      \"Precision\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"precision\"].sum()), (pd_mod_test[\"precision\"].sum())),\n      \"False omission rate\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"false_omission_rate\"].sum()), (pd_mod_test[\"false_omission_rate\"].sum())),\n      \"Accuracy\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"accuracy\"].sum()), (pd_mod_test[\"accuracy\"].sum())),\n      \"Miss Rate\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"miss_rate\"].sum()), (pd_mod_test[\"miss_rate\"].sum())),\n      \"Specificity\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"specificity\"].sum()), (pd_mod_test[\"specificity\"].sum())),\n      \"False Discovery Rate\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"false_disc_rate\"].sum()), (pd_mod_test[\"false_disc_rate\"].sum())),\n      \"Negative Predictive Value\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"neg_pred_val\"].sum()), (pd_mod_test[\"neg_pred_val\"].sum())),\n      \"F1 Score\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"f1_score\"].sum()), (pd_mod_test[\"f1_score\"].sum())),\n      \"ROC AUC Score\\nTrain:{:.02%},\\nTest:{:.02%}\".format((pd_mod[\"roc_auc_score\"].sum()), (pd_mod_test[\"roc_auc_score\"].sum()))\n    ]\n\n    for k in range(np.shape(text_lists)[0]):\n\n      gs_coor = gs_coors[k]\n      ax = plt.subplot(gs[(gs_coor[0]+ plot_h*i):(gs_coor[1]+ plot_h*i), gs_coor[2]:gs_coor[3]])\n      plot_text(text_lists[k], ax, **kwarg_list[k])\n\n  plt.subplots_adjust(wspace=0, hspace=0)\n  plt.tight_layout()\n  if display_image:\n    display(fig)\n  else:\n    return fig\n\ndef plot_text(text, ax, facecolor='xkcd:light grey', edgecolor='b', fontsize=12):\n  \"\"\"\n  Simple function to plot a text in given axis.\n  \"\"\"\n  ax.set_axis_off()\n  p = patches.Rectangle(\n    (-0, -0), 1, 1,\n    fill=True, transform=ax.transAxes, clip_on=False, facecolor=facecolor, edgecolor=edgecolor, linestyle='-',linewidth=1\n    )\n\n  ax.add_patch(p)\n  ax.text(0.5, 0.5, text, fontsize=fontsize, multialignment=\"center\", ha='center', va='center')\n  ax.set_xlim(0,1)\n  ax.set_ylim(0,1)","7f0e9ebf":"model_names = ['xgboost1']\nplot_confusion_matrix_train_test(df_eval, df_eval, model_names, display_image=False)","68e40dc9":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)","c5907e58":"import xgboost as xgb\n\nclf2 = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n#     missing=-999,\n    random_state=51,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n","bb6c37a6":"clf2.fit(X_train,y_train)","3b831d5a":"y_train_pred = clf2.predict(X_train)\ny_val_pred = clf2.predict(X_val)\nprint('AUC score train: ',roc_auc_score(y_val, y_val_pred))\nprint('AUC score val: ',roc_auc_score(y_val, y_val_pred))","970aa161":"df_eval_train = create_eval_metric_df(y_train,y_train_pred,model_name='xgboost2')\ndf_eval_val = create_eval_metric_df(y_val,y_val_pred,model_name='xgboost2')","d74ff402":"model_names = ['xgboost2']\nplot_confusion_matrix_train_test(df_eval_train, df_eval_val, model_names, display_image=False)","1ee854bf":"def plot_dist_class(df, model_names, class_selected=1, display_image=True):\n  \"\"\"\n  Plot proba distribution from prediction result.\n  Specific columns name.\n  \"\"\"\n  \n  n_plot = np.shape(model_names)[0]\n  \n  fig_h = n_plot*4\n  fig_w = 12\n  \n  plot_h = 4\n  plot_w = 4\n  \n  plt.close()\n  fig = plt.figure(figsize=(fig_w, fig_h))  \n  \n  for i, model_name in enumerate(model_names):\n    \n    ax = plt.subplot2grid((plot_h*n_plot, plot_w), (0 + i*plot_h,0), rowspan=4, colspan=4)\n    sns.kdeplot(df[\"prob_c{}_{}\".format(class_selected, model_name)], \n                ax=ax, \n                label=\"Probability of Class {}\".format(class_selected),\n                clip = [-0.05, 1.05]\n               )\n\n    ax.set_xlabel(\"Probability\")\n    ax.set_xlim([-0.05, 1.05])\n    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: \"{:.02%}\".format(x)))\n    ax.set_title(\"Distribution of Probability of Class {} from {} Model\".format(class_selected, model_name))\n    \n  plt.tight_layout()\n    \n  if display_image:\n    display(fig)\n  else:\n    return fig","34dbc5d4":"plot_dist_class(df, model_names, class_selected=1, display_image=True)","73ba1580":"clfFiveFold = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n#     missing=-999,\n    random_state=51,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n","c3265fba":"%%time\n\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm_notebook\nimport time\nNFOLDS = 5\nfolds = KFold(n_splits = NFOLDS)\n\nsplits = folds.split(X,y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = X.columns\n\nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    clfFiveFold.fit(X_train, y_train)\n    y_pred_train = clfFiveFold.predict(X_train)\n    print(f\"Fold {fold_n + 1} | Training AUC: {roc_auc_score(y_train, y_pred_train)}\")\n    y_pred_valid = clfFiveFold.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | Validation AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clfFiveFold.predict(X_test) \/ NFOLDS\n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","26437e56":"y_train_pred = clfFiveFold.predict(X)\nprint('AUC score train: ',roc_auc_score(y, y_train_pred))\nprint('AUC score OOF: ',roc_auc_score(y, y_oof))\ndf_eval_train = create_eval_metric_df(y,y_train_pred,model_name='xgboost2')\ndf_eval_oof = create_eval_metric_df(y,y_oof,model_name='xgboost2')","a8fa169b":"model_names = ['xgboost2']\nplot_confusion_matrix_train_test(df_eval_train, df_eval_oof, model_names, display_image=False)","6be798a1":"def plot_class_probability_dist(X_train, y_train, X_test, y_test, best_models_dict,hist=True):\n  f, ax = plt.subplots(len(list(best_models_dict.keys())), 2, figsize = (20, 10))\n  i = 0\n  \n  \n  for key, value in best_models_dict.items():\n    if key == \"ANN\":\n      y_train_pred_proba = value.predict(X_train_scaled).ravel()\n    else:\n      y_train_pred_proba = value.predict_proba(X_train)[:,1]\n    df_train = pd.DataFrame({\"class\" : y_train, \"prob\" : y_train_pred_proba})\n    sns.distplot(df_train.loc[df_train[\"class\"] == 0, \"prob\"], ax = ax[i,0], label = 0, hist=hist)\n    sns.distplot(df_train.loc[df_train[\"class\"] == 1, \"prob\"], ax = ax[i,0], label = 1, hist=hist)\n    ax[i,0].set_title(str(key) + \" Train\")\n    ax[i,0].set_xlim([0,1.01])\n    ax[i,0].legend()\n    \n    if key == \"ANN\":\n      y_test_pred_proba = value.predict(X_test_scaled).ravel()\n    else:\n      y_test_pred_proba = value.predict_proba(X_test)[:,1]\n    df_test = pd.DataFrame({\"class\" : y_test, \"prob\" : y_test_pred_proba})\n    sns.distplot(df_test.loc[df_test[\"class\"] == 0, \"prob\"], ax = ax[i,1], label = 0, hist=hist)\n    sns.distplot(df_test.loc[df_test[\"class\"] == 1, \"prob\"], ax = ax[i,1], label = 1, hist=hist)\n    ax[i,1].set_title(str(key) + \" Test\")\n    ax[i,1].set_xlim([0,1.01])\n    ax[i,1].legend()\n    \n    i = i+1\n    \n  plt.tight_layout()\n  display()","27f1d1d0":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)","279764f7":"best_models_dict = {'no-cv': clf,'cv1': clf2,'cv5': clfFiveFold}\nplot_class_probability_dist(X_train, y_train, X_val, y_val, best_models_dict, hist=False)","2501c998":"import eli5","60edd30a":"X_val.iloc[0]","22eb5f53":"eli5.show_weights(clf)","99a1a112":"eli5.show_weights(clf2)","f2a8f6ad":"eli5.show_weights(clfFiveFold)","59ecabf7":"# To fix ELI5 incompatibility issue with latest XGB\n# https:\/\/stackoverflow.com\/questions\/53783731\/eli5-show-prediction-not-showing-probability\n\nfrom xgboost import XGBClassifier, XGBRegressor\ndef _check_booster_args(xgb, is_regression=None):\n    # type: (Any, bool) -> Tuple[Booster, bool]\n    if isinstance(xgb, eli5.xgboost.Booster): # patch (from \"xgb, Booster\")\n        booster = xgb\n    else:\n        booster = xgb.get_booster() # patch (from \"xgb.booster()\" where `booster` is now a string)\n        _is_regression = isinstance(xgb, XGBRegressor)\n        if is_regression is not None and is_regression != _is_regression:\n            raise ValueError(\n                'Inconsistent is_regression={} passed. '\n                'You don\\'t have to pass it when using scikit-learn API'\n                .format(is_regression))\n        is_regression = _is_regression\n    return booster, is_regression\n\neli5.xgboost._check_booster_args = _check_booster_args","694c0399":"colnames = list(X_val.columns)\nelist = []\nfor i in range(10):\n    e = eli5.explain_prediction(clf2, X_val.iloc[i], top=10, feature_names=colnames)\n    elist.append(e)","817796e5":"for i in range(10):\n    etext = eli5.formatters.text.format_as_text(elist[i])\n    print(etext)\n    print('------')\n","eb66a9e5":"df_eli = eli5.formatters.as_dataframe.explain_prediction_df(clf2, X_val.iloc[1], top=10, feature_names=colnames)","3817bce1":"print(df_eli)","e7ba28d6":"x = eli5.explain_prediction_xgboost(clfFiveFold, X_val.iloc[1])\nprint(x)","8a2b3978":"# Imports and Functions","ee14cdae":"### Label Encoder","80cf44f0":"## Not sure what is wrong with ELI5 prediction explainer?","29e6e8a0":"### Device metrics","dcb0645f":"# Reduce train test memory size before training","9d73d1bd":"# Compare Results","829a5e70":"### Feature Interaction (arbitrary)","afec1687":"# XGB Training 3: Five-fold Validation (directly use)","ad562a89":"# Loading Data","48da74f2":"# Mega Feature Engineering\n\nhttps:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm","31fd50a0":"# ELI5 to check prediction on specific instances","eb0f88b8":"### Email Domain","278c4384":"# XGB Training 2: Train Test Split","64b3db15":"### Useful features","601c424e":"# Create X and y tables","eae64508":"# XGB Training","dee9d6f9":"We can see that there is a lot of positive (Fraud) cases which was failed to be detected, which really showing up in the low Recall metrics","9e8bc395":"# XGB Training 1: Simple Fit (No validation, No CV)","9c0a25bb":"### Value_to_Mean and Value_to_STD","389b100b":"# Overview\n\nI would like to focus on diagnostics on model performance in this kernel.\n\nI'm standing on top of giants - thanks to the following kernels for the baseline:\n- https:\/\/www.kaggle.com\/davidcairuz\/feature-engineering-lightgbm Thanks for the super rich feature engineering, and the 5-fold CV \n- https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s Thanks for the super speedy xgb\n\n","7438a352":"### Various: Log, decimal, Datetime","dfab8116":"### Count Encoding"}}