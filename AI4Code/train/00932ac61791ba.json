{"cell_type":{"27bbe90f":"code","6e9a8522":"code","f3142b85":"code","e03505b0":"code","42a993b2":"code","9fb837f0":"code","e540bcb5":"code","b52380c8":"code","a800441f":"code","79583a39":"code","e384e3b6":"code","88541c32":"code","5c2aaadd":"code","6c58d428":"code","b4475abe":"code","c6c8eed6":"code","4d45c1df":"code","e7991cdd":"code","b0d815e5":"code","cb9058fd":"code","ae73d006":"code","04fc72e3":"code","739763ec":"code","0980f7f5":"code","3f965826":"code","1b8defa8":"markdown","6f7c659d":"markdown","581e7f66":"markdown","96cb27ff":"markdown","e4a6ab0c":"markdown","7fe73bd4":"markdown","3f2113b7":"markdown","fa0963b3":"markdown","1b1b8d79":"markdown","718277ef":"markdown","7cd4305d":"markdown","1cf33edd":"markdown","2df01aa9":"markdown","ceb1279c":"markdown","734cae68":"markdown","171fbe10":"markdown","f9e23249":"markdown"},"source":{"27bbe90f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e9a8522":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","f3142b85":"data = pd.read_csv(\"..\/input\/horoscopes\/horoscopes.csv\", error_bad_lines=False, sep = \"|\", header = None, names = [\"text\", \"date\", \"sign\"], index_col = 0)\ndata.head()","e03505b0":"data","42a993b2":"len(data)","9fb837f0":"words = \"\"\nstopwords = set(STOPWORDS)\nfor review in data.text.values:\n    text = str(review)\n    text = text.split()\n    words += \" \".join([(i.lower() + \" \") for i in text])\n    \ncloud = WordCloud(width = 500, height = 500, background_color = 'white', stopwords = stopwords, min_font_size = 10)\ncloud.generate(words)\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(cloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title(\"Horoscopes Word Cloud\", fontsize = 16)\n    \nplt.show() ","e540bcb5":"import string\n\ndef clean_text(text):\n    words = str(text).split()\n    words = [i.lower() + \" \" for i in words]\n    words = \" \".join(words)\n    words = words.translate(words.maketrans('', '', string.punctuation))\n    return words\n\ndata['text'] = data['text'].apply(clean_text)","b52380c8":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 15000\nmax_length = 50\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\ntokenizer.fit_on_texts(data.text.values)\nword_index = tokenizer.word_index","a800441f":"get_word = {v: k for k, v in word_index.items()}","79583a39":"sequences = tokenizer.texts_to_sequences(data.text.values[::100])\n\nn_gram_sequences = []\nfor sequence in sequences:\n    for i,j in enumerate(sequence):\n        if i < (len(sequence) - 10):\n            s = sequence[i:i + 10]\n            for k, l in enumerate(s):\n                n_gram_sequences.append(s[:k + 1])\n        \nnp.array(n_gram_sequences).shape\n","e384e3b6":"n_gram_sequences = np.array(n_gram_sequences)\nmax_len = max([len(i) for i in n_gram_sequences])","88541c32":"padded = pad_sequences(n_gram_sequences, maxlen = max_len, padding = 'pre')\ninput_seq, labels = padded[:,:-1], padded[:,-1]\nlabels = tf.keras.utils.to_categorical(labels, num_classes = vocab_size)","5c2aaadd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n\ndef create_model():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 64, input_length=max_len-1))\n    model.add(Bidirectional(LSTM(20)))\n    model.add(Dense(vocab_size, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n    ","6c58d428":"use_tpu = False\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()","b4475abe":"history = model.fit(input_seq, labels, epochs=100, verbose=1)","c6c8eed6":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","4d45c1df":"avg_length = int(len(words.split())\/len(data))  ## average length of horoscope","e7991cdd":"def write_horoscope(seed_text):\n    for _ in range(avg_length):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n        pred_probs = model.predict(token_list)\n        predicted = np.random.choice(np.linspace(0, vocab_size - 1, vocab_size), p = pred_probs[0])\n        if predicted == 1: ## if it's OOV, pick the next most likely one.\n            pred_probs[0][1] = 0\n            predicted = np.argmax(pred_probs)\n        output_word = get_word[predicted]\n        seed_text += \" \" + output_word\n    print(seed_text)","b0d815e5":"write_horoscope(\"today will\")","cb9058fd":"write_horoscope(\"your life\")","ae73d006":"write_horoscope(\"This week\")","04fc72e3":"write_horoscope(\"you\")","739763ec":"write_horoscope(\"Love\")","0980f7f5":"write_horoscope(\"The next\")","3f965826":"write_horoscope(\"You will\")","1b8defa8":"First, I'm going to make a wordcloud to see what words are commonly used.","6f7c659d":"I've incorporated some random sampling (weighted by the probabilites predicted by the model), just to provide some variation in the output.","581e7f66":"If you don't look at them too closely, these might be able to pass for real horoscopes! Thanks for reading! :)","96cb27ff":"Now, we create and train the model. I'm using Embedding, Bidirectional LSTM, and Dense layers, but you can play around with this and see what works best!","e4a6ab0c":"Now, we're going to clean the text, removing puntuation and making everything lowercase.","7fe73bd4":"Now, let's write some horoscopes!","3f2113b7":"I'll be using data from this GitHub repo, which has collected around 3 years of horoscopes from the NY Post: https:\/\/github.com\/dsnam\/markovscope","fa0963b3":"Now, we're going to put together some n-gram sequences. This means that we're going to keep track of different sequences of words in the text. There's a lot of input data, so I'm going to use a stride to reduce it and take every 100th line, but you can play around with this. ","1b1b8d79":"We're going to pad the sequences to all be the same length, and then set up our labels. For any sequence, the \"label\" is going to be the last word. So, if the n-gram is \"I love machine learning\", the model input would be \"I love machine\" and the label would be \"learning\". In this way, the model learns to read phrases and predict what words should follow it.","718277ef":"## Preparing input","7cd4305d":"Now, we're going to use Keras Preprocessing layers to tokenize the text and turn it into arrays of numbers to be input into the model. For more information about tokenizers, see: https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/what-is-tokenization-nlp\/\n\nShoutout to the Tensorflow Udacity course (https:\/\/www.udacity.com\/course\/intro-to-tensorflow-for-deep-learning--ud187), which gave me some starter code for this. It's a great resource if you're looking learn some new deep learning skills.","1cf33edd":"## Visualizing data","2df01aa9":"This horoscope is completely computer-generated using Tensorflow (I only added punctuation). Now, let's see how we got there.","ceb1279c":"The model predicts probabilities for each word in the vocabulary. To generate text, we start with a \"seed\" or input text, find the most probably next word, add it to the text, and then continue to do that until we've achieved the desired number of words.","734cae68":"## Generating Horoscopes","171fbe10":"## Loading data","f9e23249":"### This week, even if they have under your skin (on the work front especially), you need to relax and let life happen. The more you try to force things, the more others will give you. What happens next will prove it..."}}