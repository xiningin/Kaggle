{"cell_type":{"116ac779":"code","e466b26d":"code","750de958":"code","dde049d3":"code","55d42035":"code","5df2f354":"code","a003e463":"code","f6b043b5":"code","77888403":"code","4fec993e":"code","8fa37bef":"code","9aa2dde6":"code","82d9cbb0":"code","f10b3311":"code","166577f1":"code","ba3a147a":"code","6f64cc8e":"code","05f0b1c2":"code","dbf3122b":"code","9c273340":"code","bf4ef2e7":"code","4af00452":"code","2a2d9929":"code","4c135ce4":"code","6451584d":"code","99b44177":"code","cf4e5432":"code","d17fc90a":"code","0a2b7f79":"code","e4ea2fd1":"code","a21d6932":"code","07afdf24":"code","984d681d":"code","b346c8a7":"code","6745f17d":"code","443e33b8":"code","21a7c620":"code","f538e4d4":"code","98c0c047":"code","50e23b09":"code","1a2cf212":"code","69cd321e":"code","ca2e5165":"code","e64430d4":"code","d6bdaba7":"code","c71205df":"code","bf0e501e":"code","9613ab4c":"code","88e8d1bc":"code","e207bedb":"code","194de42a":"code","efcf3667":"code","9d35cf96":"code","819e7be7":"code","7e5afd8c":"code","74bd8ba3":"code","5fe5b5f8":"code","03360d33":"code","37374dc7":"code","8d1be55e":"markdown","edbc7d23":"markdown","5b4c1f52":"markdown","f478d4dd":"markdown","88060203":"markdown","669ce8ec":"markdown","af1678b5":"markdown","16be6b43":"markdown","48edd8b2":"markdown","9a2b660e":"markdown","650a80ee":"markdown","593d601e":"markdown","0ea82976":"markdown","e5fbfba3":"markdown","7ee471ce":"markdown","4494ace3":"markdown","26a6a9b1":"markdown","4ded387e":"markdown","8f8a31f0":"markdown","27c8ca74":"markdown","0bbc57d2":"markdown","55236881":"markdown","7babbd52":"markdown","b66ac744":"markdown","ec044911":"markdown","f4103ba8":"markdown","6f03bb50":"markdown","c9bf68b6":"markdown","82ef2531":"markdown","34673bca":"markdown","4fa6f07b":"markdown","4388df52":"markdown","fe6ed320":"markdown","ef39ce3c":"markdown","8c5ae4f8":"markdown","f9b84c90":"markdown"},"source":{"116ac779":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","e466b26d":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n# Preview the data\ntrain.head()","750de958":"train.isnull().sum().values","dde049d3":"test.isnull().sum().values","55d42035":"features1=train.drop(['id','claim'], axis=1).copy()\n# Checking 'NaN' values.\ndef missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing = missing_percentage(features1)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","5df2f354":"train[train.isnull().sum(axis=1) >=5].shape","a003e463":"train[train.isnull().sum(axis=1) >=8].shape","f6b043b5":"# summarize the number of rows with missing values for each column\nfor i in range(train.shape[1]):\n    # count number of rows with missing values\n    n_miss = train.iloc[:,i].isnull().sum()\n    perc = n_miss \/ train.shape[0] * 100\n    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))","77888403":"df = pd.DataFrame()\ndf[\"n_missing\"] = train.drop([\"id\", \"claim\"], axis=1).isna().sum(axis=1)\ndf[\"claim\"] = train[\"claim\"].copy()\n\nfig, ax = plt.subplots(figsize=(12,5))\nax.hist(df[df[\"claim\"]==0][\"n_missing\"],\n        bins=40, edgecolor=\"black\",\n        color=\"darkseagreen\", alpha=0.7, label=\"claim is 0\")\nax.hist(df[df[\"claim\"]==1][\"n_missing\"],\n        bins=40, edgecolor=\"black\",\n        color=\"darkorange\", alpha=0.7, label=\"claim is 1\")\nax.set_title(\"Missing values distributionin in each target class\", fontsize=20, pad=15)\nax.set_xlabel(\"Missing values per row\", fontsize=14, labelpad=10)\nax.set_ylabel(\"Amount of rows\", fontsize=14, labelpad=10)\nax.legend()\nplt.show();","4fec993e":"# summarize the number of rows with missing values for each column\nfor i in range(test.shape[1]):\n    # count number of rows with missing values\n    n_miss = test.iloc[:,i].isnull().sum()\n    perc = n_miss \/ test.shape[0] * 100\n    print('> %d, Missing test data: %d (%.1f%%)' % (i, n_miss, perc))","8fa37bef":"# Count the number of null values that occur in each row\ntrain[\"null_count\"] = train.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = train.groupby(\"null_count\")[\"claim\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\nnull_data[\"6 or More Null Values\"] = sum([v for k, v in enumerate(counts.values()) if k > 5])\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[20, 10])\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5)\n_ = plt.title(\"Percentage of Null Values Per Row (Train Data)\", fontsize=14)","9aa2dde6":"train.duplicated(subset='id', keep='first').sum()","82d9cbb0":"len(train)-len(train.drop_duplicates())","f10b3311":"skew =train.skew().sort_values(ascending =False )\nskew_df= pd.DataFrame({'skew':skew})\nskew_df.head()","166577f1":"skew_df[(skew_df['skew']>=1) |(skew_df['skew']<=-1) ].index","ba3a147a":"ax = sns.distplot(train['f32'])","6f64cc8e":"sns.boxplot(data=train['f32'], saturation=.3)","05f0b1c2":"trainessai= train.copy()\ntrainessai['f32_log'] = np.log(train['f32' ])\n\nprint(train['f32'].skew())\n\nprint(trainessai['f32_log'].skew())","dbf3122b":"ax = sns.distplot(trainessai['f32_log'])","9c273340":"kurtosis= pd.DataFrame(train.kurtosis(),columns=['Kurtosis'])\nkurtosis.head()","bf4ef2e7":"kurtosis[(kurtosis['Kurtosis']>=3) |(kurtosis['Kurtosis']<=-3) ].index","4af00452":"sns.boxplot(data=train['f3'], saturation=.5)","2a2d9929":"ax = sns.distplot(trainessai['f3'])","4c135ce4":"var= train.var().sort_values(ascending =True )\nvar_df= pd.DataFrame({'var':var})\nvar_df.head(5)","6451584d":"ax = sns.distplot(train['f107'])","99b44177":"train['f107'].describe()","cf4e5432":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(IQR)","d17fc90a":"df_out = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\n\nprint(df_out.shape)","0a2b7f79":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","e4ea2fd1":"train.describe().T","a21d6932":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show","07afdf24":"cat_columns = train.drop(['id','claim'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','claim'], axis=1).select_dtypes(include=['int64','float64']).columns","984d681d":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(20, 24))\nfor feature in num_columns[0:10]:\n    plt.subplot(5, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","b346c8a7":"train.corr()['claim'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","6745f17d":"train.corr().style.background_gradient(cmap='viridis')","443e33b8":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);","21a7c620":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =1\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data =  train.iloc[0:5000,:], y = 'f'+str(n), x = 'claim',ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title('f'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","f538e4d4":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(3,2,figure=fig,hspace=.2,wspace=.2)\nn =1\nfor i in range(3):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data = train.iloc[0:2000,:], y = 'f'+str(n),  hue = 'claim',ax=ax, alpha =.7, fill=False)\n        ax.set_title('f'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('KDE plot of train target with  features', fontsize=16,y=.93);","98c0c047":"train.claim.value_counts()","50e23b09":"# Categorical features distribution \nplt.figure()\nsns.countplot(train['claim'], label='claim')\nplt.xlabel(feature, fontsize=9); plt.legend()\nplt.xticks(rotation=45)\nplt.show()","1a2cf212":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","69cd321e":"m = TSNE(learning_rate=50)\ndf_numeric =train.drop(['id','claim'], axis=1).iloc[:50000,:]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(df_numeric)\n%timeit \nprint(tsne_features.shape)","ca2e5165":"dataa=train.drop(['id'], axis=1).iloc[:50000,:].dropna()\ndataa['x']=tsne_features[:, 0]\ndataa['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='claim', data=dataa)\n# Show the plot\nplt.show()","e64430d4":"# Import MiniBatchKmeans \nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import  RobustScaler \nfrom yellowbrick.cluster import KElbowVisualizer\n# Split the data into training and test set\npipe = Pipeline(steps=[('imputer', SimpleImputer(strategy='median',add_indicator=True)),\n                    ('QuantileTransformer', QuantileTransformer(output_distribution='uniform'))])\nX_train =pipe.fit_transform(train.drop(['id','claim'], axis=1))","d6bdaba7":"# Define K-means model \nkmeans = MiniBatchKMeans( random_state=42)\n\nvisualizer = KElbowVisualizer(kmeans, k=(1,40))\n\nvisualizer.fit(X_train)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","c71205df":"kmeans1 = MiniBatchKMeans(n_clusters=6 ,random_state=42)\nkmean_label1= kmeans1.fit_predict(X_train)\nprint(kmean_label1)","bf0e501e":"clus = KMeans(n_clusters=2, max_iter=2000)\nkmean_label= clus.fit_predict(X_train)\nprint(kmean_label)","9613ab4c":"#Getting the Centroids\ncentroids = kmeans1.cluster_centers_ \n#Getting unique labels\nu_labels = np.unique(kmean_label1)\n \n#plotting the results:\n \nfor i in u_labels:\n    plt.scatter(train.iloc[kmean_label1 == i ,1] , train.iloc[kmean_label1 == i , 2] , label = i)\n\n#plt.scatter(centroids[:,1] , centroids[:,2] , s = 80, color = 'k')\nplt.legend()\nplt.show()","88e8d1bc":"#Getting the Centroids\ncentroids = clus.cluster_centers_ \n#Getting unique labels\nu_labels = np.unique(kmean_label)\n#plotting the results:\n \nfor i in u_labels:\n    plt.scatter(train.iloc[kmean_label == i , 1] , train.iloc[kmean_label == i , 2] , label = i)\n#plt.scatter(centroids[:,1] , centroids[:,2] , s = 80, color = 'k')\nplt.legend()\nplt.show()","e207bedb":"train1=train.copy()\ntrain1['cluster1'] = kmean_label1 \ntrain1['cluster1'] = train1['cluster1'].astype('category')\ntrain1['cluster'] = kmean_label \ntrain1['cluster'] = train1['cluster'].astype('category')","194de42a":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab=pd.crosstab(train1['cluster'], train1['claim'], margins = True)\nH=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","efcf3667":"cross_tab","9d35cf96":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab1=pd.crosstab(train1['cluster1'], train1['claim'], margins = True)\nH=cross_tab1\/cross_tab1.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","819e7be7":"cross_tab1","7e5afd8c":"fig = plt.figure(figsize=(18,26))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(3, 2, figure= fig, hspace= .2, wspace= .05)\nn =1\nfor i in range(3):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train1, y='f'+str(n+1), x='f'+str(n), hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title('f{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Claim\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","74bd8ba3":"# select non-numeric columns\ncat_columns = train.drop(['id','claim'], axis=1).select_dtypes(exclude=['int64','float64']).columns","5fe5b5f8":"# select the float columns\nnum_columns = train.drop(['id','claim'], axis=1).select_dtypes(include=['int64','float64']).columns","03360d33":"all_columns = (num_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","37374dc7":"if set(all_columns) == set(train.drop(['id','claim'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","8d1be55e":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preparation<\/center><\/h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling:\n\nOutlier Handling\n\nScaling\n\nFeature Engineering\n\nFeature Selection \n\n\n","edbc7d23":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Modeling<\/center><\/h3>\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Evaluation  <\/center><\/h3>\n\n\n\n\n**ROC Curve**\n\nAUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.\n\n![image.png](attachment:1247afe0-22ba-46bf-a415-a302398dac8d.png)\n\n\nwe have done all EDA needed to chose the best preprocessing steps and begin modeling .\nWork is in progress .. \n\nUpvote if you find it useful .","5b4c1f52":"we have a lot of null data : \n\n**Filling null values**\n\nSometimes rather than dropping NA values, you'd rather replace them with a valid value. *This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values.*\n\n**Number of rows with at least 5 missing value:**\n\n\n","f478d4dd":"### Num Features ","88060203":"df= pd.concat([train.drop([\"id\", \"claim\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,130), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i\/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","669ce8ec":"###  KDE plot of target with  features ","af1678b5":"### Kurtosis \n\n**Describe:**\n\n\u00b7 Kurtosis is one of the two measures that quantify shape of a distribution. **kutosis determine the volume of the outlier**\n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. \n\n\u00b7 Kurtosis describes the peakedness of the distribution.\n\n\u00b7 If the distribution is tall and thin it is called a leptokurtic distribution(Kurtosis > 3). Values in a leptokurtic distribution are near the mean or at the extremes.\n\n\u00b7 A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic(Kurtosis <3) distribution.\n\n\u00b7 A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic(Kurtosis=3) distribution. A mesokurtic distribution looks more close to a normal distribution.\n\n\u00b7 Kurtosis is sometimes reported as \u201cexcess kurtosis.\u201d Excess kurtosis is determined by subtracting 3 from the kurtosis. This makes the normal distribution kurtosis equal 0.\n\n**Important Notes:**\n\n\u00b7 Along with skewness, kurtosis is an important descriptive statistic of data distribution. However, the two concepts must not be confused with each other. Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.\n\n\u00b7 It is the sharpness of the peak of a frequency-distribution curve .It is actually the measure of outliers present in the distribution.\n\n\u00b7 **High kurtosis** in a data set is an indicator that **data has heavy outliers**.\n\n\u00b7 **Low kurtosis** in a data set is an indicator that **data has lack of outliers.**\n\n    The kurtosis of a normal distribution is 3.\n    If a given distribution has a kurtosis less than 3, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\n    If a given distribution has a kurtosis greater than 3, it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.","16be6b43":"## Target ","48edd8b2":"* * * * ### Duplicates ","9a2b660e":"This plot kinda agrees with previous one but it looks like the KDE of some categorical values are pretty much flat compared to other value.","650a80ee":"**Histograms : numerical data seems to be similar to train numerical data.**\n### Zooming on the correlation between numerical variables and target.","593d601e":"###  exploring target data main statistics","0ea82976":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","e5fbfba3":"### Visual Exploratory ","7ee471ce":"It's clear tat there isn't any clear relation between numerical variables and target.\n\nNow Exploring correlation between all numerical variables. First we get a correlation grid of all numercial variables and target\n","4494ace3":"\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n    \nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n**Eval Metric**: Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target\n    \n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n    \n## Step 1: Import helpful libraries","26a6a9b1":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","4ded387e":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","8f8a31f0":"# Convert Dtypes : ","27c8ca74":"# Kmeans : \n","0bbc57d2":"## Dtypes ","55236881":"### Test data ","7babbd52":"### Box plot of numerical columns","b66ac744":"**IQR Score**\n\nThis technique uses the IQR scores calculated earlier to remove outliers. \n\nThe rule of thumb is that anything not in the range of (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) is an outlier, and can be removed. \n\n","ec044911":"## check that we have all column","f4103ba8":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","6f03bb50":"\nIn order to see the impact of null values for each row, we'll create a count of the null values for that row and store it in a null_count feature. Then, we can group by the null_count values, and see how many rows contain 0, 1, 2, 3, or more null values.size=14)\n","c9bf68b6":"Numerical Data seems to be with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.\n","82ef2531":"## EDA \n\n### Explore the data\n\n    Null Data\n    Categorical data\n    Itrain.isnull().sum().valuess there Text data\n    wich columns will we use\n    IS there outliers that can destory our algo\n    IS there diffrent range of data\n    Curse of dimm...\n    \n\n####  Null Data \n**How sparse is my data?**\nMost data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column.","34673bca":"## t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. \n","4fa6f07b":"### Correlation ","4388df52":"## Outlier Identification\n### Skewness : \n\nSkewness is computed for each row or each column of the data present in the DataFrame object.\n\n\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. \n\n**Important Notes:**\n\n\u00b7 If the skewness is between **-0.5 and 0.5**, the data are **fairly symmetrical**\n\n\u00b7 If the skewness is between **-1 and \u2014 0.5** or between **0.5 and 1**, the **data are moderately skewed**\n\n\u00b7 If the skewness is **less than -1 or greater than 1**, the data are **highly skewed**","fe6ed320":"### Numerical features distribution\n#### Histograms of numerical features","ef39ce3c":"### Variance : \nFeatures with low variance should be eliminated","8c5ae4f8":"### Quantile data :","f9b84c90":"### Num\/Cat Features "}}