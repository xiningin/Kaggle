{"cell_type":{"f8445c79":"code","20300a22":"code","71f44710":"code","92e85ae2":"code","410672f8":"code","ea007dee":"code","72c3d03e":"code","a243e696":"code","38bb2e9c":"code","e7e92e5f":"code","07847459":"code","ed851130":"code","f05f4302":"code","68089905":"code","739ed360":"code","5fab47f5":"code","2a2d86fd":"code","459f40e3":"code","322fa639":"code","e3230bc4":"code","edc5ca29":"code","4f2d6007":"code","d8e517a6":"code","7c4c0b6a":"code","5b72058a":"code","2f1c1fe0":"code","bbcbf138":"code","a3da2e91":"code","c03bda83":"code","6874e689":"code","cc31cf4b":"markdown","5bd34c6f":"markdown","b4bb6ffd":"markdown","e0ea9247":"markdown","6f00437b":"markdown","25272fd3":"markdown","ac4a3eea":"markdown"},"source":{"f8445c79":"!pip install sentence-transformers","20300a22":"import os\nimport re  # For preprocessing\nimport en_core_web_sm\nfrom difflib import SequenceMatcher\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom time import time  # To time our operations\nimport glob\nimport json\nimport zipfile\nfrom tqdm import tqdm\nimport multiprocessing\nimport scipy\n\nimport torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForQuestionAnswering\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Dict\nfrom sklearn.metrics.pairwise import cosine_similarity #for cosine similarity\nfrom gensim.models.phrases import Phrases, Phraser #For create relevant phrases\nfrom gensim.models import Word2Vec #Our model type\n","71f44710":"import logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","92e85ae2":"# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","410672f8":"root_path = '\/kaggle\/input\/CORD-19-research-challenge'\n\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'doi': str\n})\n","ea007dee":"meta_df.head()","72c3d03e":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n","a243e696":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if 'abstract' in content:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            else:\n                self.abstract.append('Not provided.')\n            # Body text\n            if 'body_text' in content:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            else:\n                self.body_text.append('Not provided.')\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n\n\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n","38bb2e9c":"def get_date_dt(all_json, meta_df):\n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [],\n             'abstract_summary': []}\n    for idx, entry in tqdm(enumerate(all_json), desc=\"Parsing the articles Json's content\", total=len(all_json)):\n        content = FileReader(entry)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n        # no metadata, skip this paper\n        if len(meta_data) == 0:\n            continue\n\n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n        \n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n        try:\n            # if more than one author\n            authors = meta_data['authors'].values[0].split(';')\n            if len(authors) > 2:\n                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n                dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n            else:\n                # authors will fit in plot\n                dict_['authors'].append(\". \".join(authors))\n        except Exception as e:\n            # if only one author - or Null valie\n            dict_['authors'].append(meta_data['authors'].values[0])\n\n        # add the title information, add breaks when needed\n        dict_['title'].append(meta_data['title'].values[0])\n\n        # add the journal information\n        dict_['journal'].append(meta_data['journal'].values[0])\n    return pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal'])","e7e92e5f":"def Initialization_Word2Vec_model():\n    w2v_model = Word2Vec(min_count=50,\n                     window=10,\n                     size=200,\n                     sample=6e-5,\n                     alpha=0.02,\n                     min_alpha=0.0003,\n                     negative=20,\n                     workers=multiprocessing.cpu_count() -1)\n    return w2v_model\n","07847459":"def create_sentences(df, col_name):\n    \"\"\"Build sentences to Word2Vec model \"\"\"\n    t = time()\n    sent = [row.split() for row in df[col_name]]\n    phrases = Phrases(sent, min_count=50, progress_per=100, max_vocab_size=1000000)\n    bigram = Phraser(phrases)\n\n    print('Time to create sentences: {} mins'.format(round((time() - t) \/ 60, 2)))\n    return bigram, bigram[sent]\n\ndef Build_Word2Vec_vocab(w2v_model, sentences,update=True):\n    t = time()\n    w2v_model.build_vocab(sentences, progress_per=10000, update=update)\n    print('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))\n    return w2v_model\n\ndef Train_Word2Vec_model(w2v_model, sentences):\n    t = time()\n    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n    print('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))\n    return w2v_model\n\ndef cleaning(spacy_doc):\n    \"\"\"Lemmatizes and removes stopwords\n    doc needs to be a spacy Doc object \"\"\"\n\n    txt = [t.lemma_ for t in spacy_doc if\n            t.dep_ not in ['prep', 'punct', 'det'] and\n            len(t.text.strip()) > 2 and\n            t.lemma_ != \"-PRON-\" and\n            not t.is_stop]\n    if len(txt) > 2:\n        return ' '.join(txt)\n    else:\n        return \"no text\"\n\ndef word2vec_preprocessing(df, column, offline=True) -> pd.DataFrame:\n    \"\"\" Prepare column from pd.DataFrame with text to Word2Vec model \"\"\"\n    url_pattern = r\"(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n    data = df.drop_duplicates(column)\n    data = data[\n        data[column].apply(lambda x: len(x) > 0) &\n        data[column].apply(lambda x: len(x.split()) > 3)\n        ]\n    data.reset_index(inplace=True, drop=True)\n    data[column] = data[column].map(lambda x: re.sub(url_pattern, ' ', str(x)))\n    brief_cleaning = (re.sub(r\"[^a-zA-Z']+\", ' ', str(row)).lower() for row in data[column])\n    nlp = en_core_web_sm.load(disable=['ner', 'parser', 'tagger'])  # disabling Named Entity Recognition for speed\n    t = time()\n    txt = []\n    if offline:\n        paper_ids = data['paper_id']\n        index = []\n        for idx, doc in enumerate(nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1, n_process=5)):\n            txt.append(cleaning(doc))\n            index.append(paper_ids[idx]) #TODO verify its ok\n        print('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))\n    else:\n        for doc in nlp.pipe(brief_cleaning):\n            txt.append(cleaning(doc))\n\n    df_clean = data\n    print([t for t in txt if t is None])\n    df_clean[f'clean_{column}'] = txt\n    return df_clean\n","ed851130":"def preprocessing(corpus: List, is_offline: bool):\n    dt_corpus = pd.DataFrame({'clean': corpus})\n    procesed = word2vec_preprocessing(dt_corpus, 'clean', offline=is_offline)\n    return procesed['clean_clean'].to_list()\n\ndef get_relevant_articles(query,  articles_abstract):\n    queries: List[str] = preprocessing([query], is_offline=False)\n    articles_abstract.score = articles_abstract.score.astype(float) # to make the scores as float in order to not lose precision\n    for q in queries:\n        query_key_words: List[str] = get_unk_kw(phraser, q)\n        embedded_query = embed_sentence(w2v_model, query_key_words)\n        for idx, abstract in tqdm(articles_abstract.iterrows(), desc=\"Iterate over all articles abstract\", total=len(articles_abstract)):\n            abs_score = get_abstract_score(w2v_model, embedded_query, abstract['clean_abstract'])\n            articles_abstract.at[idx, 'score'] = abs_score\n    scored_articles_abstract = articles_abstract.sort_values('score', ascending=False)\n    return scored_articles_abstract","f05f4302":"data_dt = get_date_dt(all_json, meta_df)","68089905":"to_train = False\nif to_train:\n    w2v_model = Initialization_Word2Vec_model()\n    df_clean = word2vec_preprocessing(data_dt, 'body_text')\n    phraser, sentences = create_sentences(df_clean, 'clean_body_text')\n    update_w2v_vocab = len(w2v_model.wv.vocab) != 0\n    w2v_model = Build_Word2Vec_vocab(w2v_model, sentences, update_w2v_vocab)\n    w2v_model = Train_Word2Vec_model(w2v_model, sentences)\n    # w2v_model.save('saved_model\/w2v_model_on_all_abstract_full_text.w2v')\n    w2v_model.init_sims(replace=True)\nelse:\n    with open(\"\/kaggle\/input\/w2v-model\/saved_model\/cleaned_to_w2v_all_document.pkl\", 'rb') as f:\n        df_clean = pickle.load(f)\n    with open(\"\/kaggle\/input\/w2v-model\/phraser.pkl\", 'rb') as f:\n        phraser = pickle.load(f)\n    w2v_model = Word2Vec.load('\/kaggle\/input\/w2v-model\/saved_model\/w2v_model_on_all_abstract_full_text.w2v')","739ed360":"print(\"most similar words to kaletra:\")\nfor s_w in w2v_model.wv.most_similar(positive=[ 'kaletra']):\n    print(s_w)\nprint('#'*100)\nprint(\"most similar words to bat:\")\nfor s_w in w2v_model.wv.most_similar(positive=['bat']):\n    print(s_w)\nprint('#'*100)\nprint(\"most similar words to dead and people:\")\nfor s_w in w2v_model.wv.most_similar(positive=['dead', 'people']):\n    print(s_w)","5fab47f5":"ALPHA = 0.5\n\ndef embed_sentence(model, tokens: List[str]) -> List:\n    res = []\n    for t in tokens:\n        try:\n            vec = model.wv.word_vec(t, use_norm=False)\n            res.append(vec)\n        except KeyError:\n            # logging.debug(f'Unidentified word while embedding:{t}')\n            continue\n    return res\n\n\ndef get_all_abstracts(phraser, cleaned_df):\n    column = 'abstract'\n    df = word2vec_preprocessing(cleaned_df, column, offline=True)\n    df[f'clean_{column}'] = df[f'clean_{column}'].map(lambda x: get_unk_kw(phraser, x))\n    df['score'] = [0] * len(df)\n    return df\n\n\ndef get_unk_kw(phraser, query):\n    if type(query) == str:\n        return list(set(phraser[query.split()]))\n    else:\n        return []\n\n\ndef preprocessing(corpus: List, is_offline: bool):\n    dt_corpus = pd.DataFrame({'clean': corpus})\n    procesed = word2vec_preprocessing(dt_corpus, 'clean', offline=is_offline)\n    return procesed['clean_clean'].to_list()\n\n\ndef get_abstract_score(model: Word2Vec, embedded_query:List, abstract: List[str]) -> float:\n    f_score = 0\n    valid_scores_sum = 1  # so we wouldn't divide by zero\n    if type(abstract) != list or len(abstract) == 0:\n        return f_score\n    embedded_abstract = embed_sentence(model, abstract)\n    for q_t in embedded_query:\n        scores = model.wv.cosine_similarities(q_t, embedded_abstract)\n        valid_scores = [s for s in scores if s > ALPHA]\n        valid_scores_sum += len(valid_scores)\n        f_score += np.sum(valid_scores)\n    norm_score = f_score \/ valid_scores_sum # to normalize by the number of tokens that was counted\n    return norm_score\n","2a2d86fd":"# articles_abstract: pd.DataFrame = get_all_abstracts(phraser, all_data)","459f40e3":"print(w2v_model.wv.most_similar(positive=['medicine', 'covid'], negative=['sars']))\nprint(w2v_model.wv.most_similar(positive=['dead', 'covid','israel']))\nprint(w2v_model.wv.similarity('kaletra', 'covid'))\nprint(w2v_model.wv.similarity('kaletra', 'sars'))\n\n","322fa639":"with open('\/kaggle\/input\/w2v-model\/parsed_abstract_ran_offline.pk', 'rb') as f: \n    articles_abstract = pickle.load(f)\n","e3230bc4":"articles_abstract['body_text'].head()","edc5ca29":"encoder = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")","4f2d6007":"encoded_articles_abstract = encoder.encode(articles_abstract['abstract'].tolist())\narticles_abstract['encoded_articles_abstract'] = encoded_articles_abstract    ","d8e517a6":"def query_questions(query, articles_abstract):    \n    encoded_query = encoder.encode([query])\n    articles_abstract['distances'] = scipy.spatial.distance.cdist(encoded_query, articles_abstract['encoded_articles_abstract'].tolist(), \"cosine\")[0]\n    articles_abstract = articles_abstract.sort_values('distances').reset_index()[:70]\n    \n    articles_abstract['sentence_list'] = [body.split(\". \") for body in articles_abstract['body_text'].to_list()] \n    paragraphs = []\n    for index, ra in articles_abstract.iterrows():\n        para_to_add = [\". \".join(ra['sentence_list'][n:n+7]) for n in range(0, len(ra['sentence_list']), 7)]        \n        para_to_add.append(ra['abstract'])\n        paragraphs.append(para_to_add)\n    articles_abstract['paragraphs'] = paragraphs\n    answers = answer_question(query, articles_abstract)\n    return answers","7c4c0b6a":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForQuestionAnswering\nfrom transformers import pipeline\n\ndef get_QA_bert_model():\n    torch_device = 0 if torch.cuda.is_available() else -1\n    tokenizer = AutoTokenizer.from_pretrained(\"ahotrod\/albert_xxlargev1_squad2_512\")\n    model = AutoModelForQuestionAnswering.from_pretrained(\"ahotrod\/albert_xxlargev1_squad2_512\")\n    return pipeline('question-answering', model=model, tokenizer=tokenizer,device=torch_device)","5b72058a":"nlp_qa = get_QA_bert_model()","2f1c1fe0":"def answer_question(question: str, context_list):\n    # anser question given question and context\n    answers =[]\n    all_para = [item for sublist in context_list['paragraphs'].to_list() for item in sublist] \n    print(f\"paragraph to scan: {len(all_para)}\")\n    for _, article in context_list.iterrows():\n        for context in article['paragraphs']:\n            if len(context) < 10:\n                continue\n            with torch.no_grad():\n                answer = nlp_qa(question=question, context=context)\n            answer['paragraph'] = context\n            answer['paper_id'] = article['paper_id']\n            answers.append(answer)            \n    df = pd.DataFrame(answers)\n    df = df.sort_values(by='score', ascending=False)\n    return df\n\n# def answer_question(question: str, context_list):\n#     # anser question given question and context\n#     torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n#     answers =[]\n#     all_para = [item for sublist in context_list['paragraphs'].to_list() for item in sublist] \n#     print(f\"paragraph to scan: {len(all_para)}\")\n#     i=0\n#     for _, article in context_list.iterrows():\n#         for context in article['paragraphs']:\n#             if len(context) < 10:\n#                 continue\n#             i = i+1\n#             if i % 100 == 0:\n#                 print(i)\n                \n#             encoded_dict = tokenizer.encode_plus(\n#                                 question, context,\n#                                 add_special_tokens = True,\n#                                 max_length = 500,\n#                                 pad_to_max_length = True,\n#                                 return_tensors = 'pt'\n#                            )\n\n#             input_ids = encoded_dict['input_ids'].to(torch_device)\n#             token_type_ids = encoded_dict['token_type_ids'].to(torch_device)\n#             with torch.no_grad():  \n#                 start_scores, end_scores = model(input_ids, token_type_ids=token_type_ids)\n#             all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n#             start_index = torch.argmax(start_scores)\n#             end_index = torch.argmax(end_scores)\n# #             print(\"total score: \", start_scores[0][start_index].float() + end_scores[0][end_index].float())\n#             answer = tokenizer.convert_tokens_to_string(all_tokens[start_index:end_index+1])\n#             answer = answer.replace('[CLS]', '').replace('<pad>', '').replace('[SEP]', '')\n#             if len(answer.strip()) > 6 and similar(question.lower().strip(), answer.lower().strip()) < 0.8 and question.lower().strip() not in answer.lower().strip():\n#                 answers.append({'answer': answer, 'paragraph': context, 'paper_id': article['paper_id'], 'total_score':start_scores[0][start_index].item()+end_scores[0][end_index].item() })\n#     return pd.DataFrame(answers)","bbcbf138":"answer1 = query_questions(\"Are there geographic variations in the rate of COVID-19 spread?\", articles_abstract)\nanswer1.to_csv(\"\/kaggle\/working\/1_Are there geographic variations in the rate of COVID-19 spread.csv\")\nanswer1.head()","a3da2e91":"answer2 = query_questions(\"What works have been done on infection spreading?\", articles_abstract)\nanswer2.to_csv(\"\/kaggle\/working\/2_What works have been done on infection spreading.csv\")\nanswer2.head()","c03bda83":"answer3 =  query_questions(\"Are there geographic variations in the mortality rate of COVID-19?\", articles_abstract)\nanswer3.to_csv(\"\/kaggle\/working\/3_Are there geographic variations in the mortality rate of COVID.csv\")\nanswer3.head()","6874e689":"answer4 =  query_questions(\"Is there any evidence to suggest geographic based virus mutations?\", articles_abstract)\nanswer4.to_csv(\"\/kaggle\/working\/4_Is there any evidence to suggest geographic based virus mutations.csv\")\nanswer4.head()","cc31cf4b":"And than we load it again","5bd34c6f":"write something that show its a great results","b4bb6ffd":"Loading the data","e0ea9247":"Now lets check that the w2v model make sense:\n1. First we will see what it gices as the most similar words to a drig named Kaltera\n2. We word see what it gives as the most similar word to bat\n3. We will chec some sematic word calculation such as dead and country\n","6f00437b":"We will load our pretrained w2v model which was trained in this notebook earlier on.","25272fd3":"Suppose to run in offline:","ac4a3eea":"> Doing some sanity check: \nLooking for 20 Q&A manually tagged in the article corpus:\n"}}