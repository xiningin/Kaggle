{"cell_type":{"402cba3b":"code","25c284ff":"code","877d14a6":"code","5b068740":"code","61cb5cd1":"code","58a281b2":"code","ba32da34":"code","f18cb86b":"code","f3205bce":"code","785dd58a":"code","683ab75e":"code","c1465efe":"code","a68de2a0":"code","ed0ddd27":"code","d51ff39b":"code","e675e2ec":"code","f287e8ee":"code","8cd42764":"code","b918bf56":"code","f73ba8e7":"code","75df1058":"markdown","324f0db5":"markdown","e8466bff":"markdown","ff8ae6f5":"markdown","12777760":"markdown","e1e4b910":"markdown","a91e27f0":"markdown","109e7713":"markdown","6699407c":"markdown","70d1ade0":"markdown","cfd7ac5a":"markdown","b1816805":"markdown","679b20c2":"markdown","f7a58e8d":"markdown","3e17a945":"markdown","11ca54d1":"markdown","8c2817f8":"markdown","8c428ee9":"markdown","d5ec73cb":"markdown","613519ec":"markdown","47450cbf":"markdown","892e7002":"markdown","5f53df60":"markdown","c74cbf0d":"markdown"},"source":{"402cba3b":"import warnings\nfrom sklearn.exceptions import DataConversionWarning\n\n# Suppress warnings\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)","25c284ff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree, svm\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Load train and test data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/test.csv\")\n\n# All data\ndata  = train.append(test, sort=False)","877d14a6":"# Columns\nprint(len(data.columns))\ndata.columns","5b068740":"# Example\ndata.sample(n=10)","61cb5cd1":"types = pd.DataFrame(data.dtypes).rename(columns={0: 'type'}).sort_values(by=['type'],ascending=False)\ntypes","58a281b2":"# Check missing values\ndef check_missing(df):\n    null_val = df.isnull().sum()\n    percent = 100 * df.isnull().sum()\/len(df)\n    missing_table = pd.concat([null_val, percent], axis=1)\n    col = missing_table.rename(columns = {0 : 'Num', 1 : 'Rate'})\n    return col\n\n# Display columns missing values are under 1%.\nprint(\"Data #\"+str(len(data)))\ncols = check_missing(data)\n\ntypes.join(cols).sort_values(by=\"Rate\", ascending=False)","ba32da34":"# Drop Cabin\ndata.drop(['Cabin'], axis=1, inplace = True)\n\n# \"Embarked\": Fill NA and map into integer\ndata[\"Embarked\"] = data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0])\ndata[\"Embarked\"] = data[\"Embarked\"].map({\"S\": 0, \"C\" : 1, \"Q\" : 2})\n\n# \"Name\": Map passenger's name with their title\ntitle_mapping    = {\n    '(.+)Mr\\.(.+)': 1, '(.+)Master\\.(.+)': 1,\n    '(.+)Dr\\.(.+)': 2, '(.+)Don\\.(.+)': 2, '(.+)Major\\.(.+)': 2,\n    '(.+)Sir\\.(.+)':2, '(.+)Col\\.(.+)': 2, '(.+)Jonkheer\\.(.+)': 2,\n    '(.+)Capt\\.(.+)': 2,'(.+)Countess\\.(.+)': 2, '(.+)Dona\\.(.+)': 2,\n    '(.+)Rev\\.(.+)': 3,\n    '(.+)Ms\\.(.+)': 4, '(.*)Miss\\.(.+)': 4, '(.+)Mrs\\.(.+)': 4,\n    '(.+)Mme\\.(.+)': 4,'(.+)Lady\\.(.+)': 4, '(.+)Mlle\\.(.+)': 4 \n}\ndata[\"Title\"] = data[\"Name\"].replace(title_mapping, regex=True).astype('int')\n\n# \"Sex\": Map male and female\ndata[\"Sex\"] = data[\"Sex\"].map({\"male\": 0, \"female\": 1})","f18cb86b":"# Do nothing.","f3205bce":"# Estimate missing age from title\nfor i in range(1, 5):\n    age_to_estimate = data.groupby('Title')['Age'].median()[i]\n    data.loc[(data['Age'].isnull()) & (data['Title'] == i), 'Age'] = age_to_estimate\n\n# Estimate missing fare from pclass\nfor i in range(1, 4):\n    fare_to_estimate = data.groupby('Pclass')['Fare'].median()[i]\n    data.loc[(data['Fare'].isnull()) & (data['Pclass'] == i), 'Fare'] = fare_to_estimate\n\n# Standardize numerical values\nss = StandardScaler()\nss.fit_transform(data[['Age', 'Fare']])\n\n# Cut Age into 10 categolies\ndata[\"AgeBin\"]  = pd.qcut(data[\"Age\"], 10, duplicates=\"drop\", labels=False)\n\n# Cut Fare into 10 categolies\ndata[\"FareBin\"] = pd.qcut(data[\"Fare\"], 10, duplicates=\"drop\", labels=False)","785dd58a":"# Add FamilySize\ndata['FamilySize'] = data[\"Parch\"] + data[\"SibSp\"]\n\n# Add IsFamily\ndata['IsFamily'] = data[\"Parch\"] + data[\"SibSp\"]\ndata.loc[data['IsFamily'] > 1, 'IsFamily']  = 2\ndata.loc[data['IsFamily'] == 1, 'IsFamily'] = 1\ndata.loc[data['IsFamily'] == 0, 'IsFamily'] = 0\n\n# Add FamilySurvival\nDEFAULT_SURVIVAL_VALUE = 0.5\ndata['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\n\n# Get last name\ndata['LastName'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Family has same Lastname and Fare\nfor grp, grp_df in data.groupby(['LastName', 'Fare']):\n    if(len(grp_df) != 1):\n        # A Family group is found\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n            elif (smin==0.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0\n\n# Family(or group) has same Ticket No\nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n                elif (smin==0.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0","683ab75e":"# Drop Useless data\ntrain_target = data[:891][\"Survived\"].values\ndata.drop(['Name', 'PassengerId', 'Age', 'Fare', 'Ticket', 'LastName'], axis = 1, inplace = True)\n\n# One-Hot Encoding Categorical variables\ndata = pd.get_dummies(data, columns=[\"Embarked\", \"Title\", \"Sex\", \"IsFamily\", \"FamilySurvival\"], drop_first=True)\n\n# Set data\ntrain = data[:891]\ntest  = data[891:]\n\n# Data types\ndata.dtypes","c1465efe":"# Plot correlations between variables\nplt.figure(figsize=(20, 10))\nsns.heatmap(train.corr(), annot=True, fmt='.2f')","a68de2a0":"possible_features = train.columns.copy().drop('Survived')\n\n# Check feature importances\nselector = SelectKBest(f_classif, len(possible_features))\nselector.fit(train[possible_features], train_target)\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Feature importances:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], possible_features[indices[i]]))","ed0ddd27":"# Display all possible features with Survival\nfig, axs = plt.subplots(8, 2, figsize=(20, 30))\nfor i in range(0, 16):\n    sns.countplot(possible_features[i], data=train, hue=train_target, ax=axs[i%8, i\/\/8])","d51ff39b":"# Feature params\nfparams = \\\n    ['Sex_1', 'Title_4', 'FamilySurvival_1.0', 'Pclass', 'FareBin', \n     'FamilySurvival_0.5', 'Embarked_1', 'IsFamily_1', 'IsFamily_2', \n     'Parch', 'Title_3', 'AgeBin', 'SibSp']\n\n# Get params\ntrain_features = train[fparams].values\ntest_features  = test[fparams].values\n\n# Number of Cross Validation Split\nCV_SPLIT_NUM = 6\nN_ESTIMATORS = 300\n\n# Debug mode\nDEBUG_MODE = False","e675e2ec":"# Params for RandomForestClassifier\nrfgs_parameters = {\n    'n_estimators': [N_ESTIMATORS],\n    'max_depth'   : [2,3,4],\n    'max_features': [2,3,4],\n    \"min_samples_split\": [2,3,4],\n    \"min_samples_leaf\": [2,3,4]\n}\n\nif not DEBUG_MODE: # Do Gridsearch\n    rfc_cv = GridSearchCV(RandomForestClassifier(), rfgs_parameters, cv=CV_SPLIT_NUM)\n    rfc_cv.fit(train_features, train_target)\n    print(\"RFC GridSearch score: \"+str(rfc_cv.best_score_))\n    print(\"RFC GridSearch params: \")\n    print(rfc_cv.best_params_)","f287e8ee":"# Params for GradientBoostingClassifier\ngbcgs_parameters = {\n    'loss' : [\"deviance\",\"exponential\"],\n    'n_estimators' : [N_ESTIMATORS],\n    'learning_rate': [0.02,0.03,0.04,0.05,0.06],\n    'max_depth':  [2,3,4],\n    'max_features': [2,3,4],\n    \"min_samples_split\": [2,3,4],\n    'min_samples_leaf': [2,3,4]\n}\n\nif not DEBUG_MODE: # Do Gridsearch\n    gbc_cv = GridSearchCV(GradientBoostingClassifier(), gbcgs_parameters, cv=CV_SPLIT_NUM)\n    gbc_cv.fit(train_features, train_target)\n    print(\"GBC GridSearch score: \"+str(gbc_cv.best_score_))\n    print(\"GBC GridSearch params: \")\n    print(gbc_cv.best_params_)","8cd42764":"# Params for SVM\nsvcgs_parameters = {\n    'kernel': ['rbf'],\n    'C':     [10,20,30,40,50,60,70],\n    'gamma': [0.005,0.006,0.007,0.008,0.009,0.01,0.011],\n    'probability': [True]\n}\n\nif not DEBUG_MODE: # Do Gridsearch\n    svc_cv = GridSearchCV(svm.SVC(), svcgs_parameters, cv=CV_SPLIT_NUM)\n    svc_cv.fit(train_features, train_target)\n    print(\"SVC GridSearch score: \"+str(svc_cv.best_score_))\n    print(\"SVC GridSearch params: \")\n    print(svc_cv.best_params_)","b918bf56":"if not DEBUG_MODE: # Do Gridsearch\n    # Voting Classifier\n    vc = VotingClassifier(estimators=[('rfc', rfc_cv.best_estimator_), ('gbc', gbc_cv.best_estimator_), ('svm', svc_cv.best_estimator_)], voting='soft', n_jobs=4, weights=[3,1,2])\n    vc = vc.fit(train_features, train_target)","f73ba8e7":"arr = [\n    {'model': vc, 'filename': 'vc_submission.csv'},\n    {'model': rfc_cv.best_estimator_, 'filename': 'rfc_submission.csv'},\n    {'model': gbc_cv.best_estimator_, 'filename': 'gbc_submission.csv'},\n    {'model': svc_cv.best_estimator_, 'filename': 'svc_submission.csv'}\n]\nif not DEBUG_MODE: # Output\n    for v in arr:\n        # Predict and output to csv\n        survived = v['model'].predict(test_features)\n        pred = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")['PassengerId'])\n        pred['Survived'] = survived.astype(int)\n        pred.to_csv(\"..\/working\/\" + v['filename'], index = False)","75df1058":"Display feature importances","324f0db5":"For data pre-processing, categorize variables\n```\n# Categorical String variables\nEmbarked        object\nCabin           object\nTicket          object\nSex             object\nName            object\n\n# Categorical Integer variables (have order)\nParch            int64\nSibSp            int64\nPclass           int64\n\n# Numerical variables\nFare           float64\nAge            float64\n```","e8466bff":"```\n- Numerical variables\nCol     Type    Num   Rate\nAge     float64 263   20.091673\nFare    float64 1     0.076394\n```\n* **Age**\n  * Fill missing values from median of same name title (Mr,Mrs,etc)\n  * Standardize values\n  * Divide into 10 categories\n* **Fare**\n  * Fill missing values from median of same Pclass\n  * Standardize values\n  * Divide into 10 categories","ff8ae6f5":"# Data Visualizations <a id=\"visualization\"><\/a>","12777760":"Check missing rate","e1e4b910":"Drop useless data and keep as Training and Test set.","a91e27f0":"# Data Pre-Processing <a id=\"data-process\"><\/a>","109e7713":"Display all possible features with Survival","6699407c":"# Load Data and Libraries <a id=\"load\"><\/a>","70d1ade0":"SVM Grid Search","cfd7ac5a":"GradientBoostingClassifier Grid Search","b1816805":"```\n- Categorical String variables\nCol         Type      Num     Rate\nCabin       object    1014    77.463713\nEmbarked    object    2       0.152788\nName\t\tobject    0       0.000000\nSex         object    0       0.000000\nTicket      object    0       0.000000\n```\n* **Cabin**\n  * Just drop this variable, because 77.4% of values aremissing.\n* **Embarked**\n  * Fill missing values as mode value.(S)\n  * Map strings into integers: S->0, C->1, Q->2\n  * One-Hot Encoding\n* **Name**\n  * Map strings into integers: Mr,Master->1, Dr,Don,Major,etc->2, Rev->3, Ms,Miss,Mrs,Mme->4\n  * One-Hot Encoding\n* **Sex**\n  * Map strings into integers: male->0, female->1\n  * One-Hot Encoding\n* **Ticket**\n  * Leave as it is.","679b20c2":"# Estimate Values and Output <a id=\"estimate\"><\/a>","f7a58e8d":"* [Load Data and Libraries](#load)\n* [Check Data](#check)\n* [Data Pre-Processing](#data-process)\n* [Data Visualizations](#visualization)\n* [Training and Estimation](#training)","3e17a945":"There're 12 variables in dataset","11ca54d1":"```\n- Categorical Integer variables (have order)\nCol     Type    Num   Rate\nPclass  int64   0     0.000000\nSibSp   int64   0     0.000000\n```\n* **Ticket**\n  * Leave as it is.\n* **Ticket**\n  * Leave as it is.","8c2817f8":"Pre-process data by their variable types, Categorical, Numerical, Integer, String.  \nAnd we now process training and test set together. (data = training + test)  \n(because we can process values in same scale)","8c428ee9":"# Training and Estimation <a id=\"training\"><\/a>","d5ec73cb":"# Check Data <a id=\"check\"><\/a>","613519ec":"Check data example and types","47450cbf":"This time, pick variables by their importances\n```\nFeature importances:\n68.85 Sex_1\n67.32 Title_4\n32.23 FamilySurvival_1.0\n24.60 Pclass\n22.45 FareBin\n7.05 FamilySurvival_0.5\n6.36 Embarked_1\n6.01 IsFamily_1\n2.11 IsFamily_2\n1.83 Parch\n1.28 Title_3\n0.92 AgeBin\n0.53 SibSp\n0.21 FamilySize\n0.18 Title_2\n0.04 Embarked_2\n```","892e7002":"RandomForestClassifier Grid Search","5f53df60":"VotingClassifier RF+ETC+GBC+SVM","c74cbf0d":"```\n- Generate new variable information from other variables\nFamirySize\nIsFamily\nFamilySurvival\n```\n* **FamilySize**\n  * Size of Family: Parch + SibSp\n* **IsFamily**\n  * 0 families->0\n  * 1 families->1\n  * 2 or more families->2\n  * One-Hot Encoding\n* **FamilySurvival**\n  * Regard ones have same lastname or same ticketname as family\n  * One of their family survived -> 1\n  * All of their family dead -> 0\n  * No data -> 0.5\n  * One-Hot Encoding"}}