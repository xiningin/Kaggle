{"cell_type":{"059d1a38":"code","68f5d97a":"code","5d083e0b":"code","4220cc81":"code","7ef65353":"code","05b2f7f7":"code","29bb9502":"code","de0f18e0":"code","986eba3a":"code","d523bf6f":"code","179e07f0":"code","bbf1feb2":"code","50a295d4":"code","92a0c753":"code","c39fd16d":"code","030762fa":"markdown","4fc0c8a7":"markdown"},"source":{"059d1a38":"import numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","68f5d97a":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')","5d083e0b":"train.head()","4220cc81":"# Check if there is any NaN in both datasets\nprint(\"Number os missing data trainset:\", train.isna().any().sum())\nprint(\"Number os missing data testset:\", test.isna().any().sum())","7ef65353":"# Main statistics from train dataset\ntrain.describe()","05b2f7f7":"# Splitting data into X and y values and one hot encoding y value in order to train a model and get predictions in output file desireble format\ny = train.target\nX = train.drop(['id','target'], axis=1)\n\nencoder = OneHotEncoder(categories = 'auto')\ny_enc = encoder.fit_transform(y.values.reshape(X.shape[0],1)).toarray()\n\n# Split validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y_enc, train_size=0.8, test_size=0.2, random_state=0)","29bb9502":"# Those lines below I used once to find best parameters to train a model using sklean's randomized search\n\n#from sklearn.model_selection import RandomizedSearchCV\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 50)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n","de0f18e0":"# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestRegressor()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# # Fit the random search model\n# rf_random.fit(X_train, y_train)","986eba3a":"# rf_random.best_params_","d523bf6f":"# I then created a model with best parameters and used a Pipeline to train the model.\nmodel = RandomForestRegressor(n_estimators=150, max_depth=70, min_samples_split=2, min_samples_leaf=4, max_features='sqrt', bootstrap=True,random_state=0)\n\nmy_pipeline = Pipeline(steps=[  ('scale', StandardScaler()),\n                                ('model', model)])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n","179e07f0":"# Removing item ID from test data.\ntest_ = test.iloc[:,1:]","bbf1feb2":"# Get predictions\noutput = my_pipeline.predict(test_) # Your code here","50a295d4":"pred_df = pd.DataFrame(output, columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\npred_df['id'] =  test['id'].values","92a0c753":"# pred_df","c39fd16d":"# Creating an output file to submit and verify competition score.\noutput = pred_df[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]\noutput.to_csv('submission.csv', index=False)","030762fa":"Loading and taking a look at train data","4fc0c8a7":"In order to practice machine learning, I started to work with this playground tabular data. This notebook is one of my first work in a long time without create ML models and I tried to use some strategies I read.\n\nI did the following tests here:\n1. Trained a random forest regressor with default hiperparameters to set a baseline model and score.\n2. Used min max scaler to try to reduce the dimensions on data. Here I made a mistake, i think, because min max is influenced by outliers, which I was trying to \"fix\".\n3. Tried StandardScaler with \"with_std=False\".\n4. Tried a model without scaling the data. This was the best model so far => RandomForestRegressor(n_estimators=150, max_depth=70, min_samples_split=2, min_samples_leaf=4, max_features='sqrt', bootstrap=True,random_state=0)\n\nAlso, I used an encoder with target variables and used sklearn pipelines to train and predict with the model.\n\nThere is a lot to improve here because the best placement I got was around 900 (by Jun 30). I used a random grid CV to get the best parameters for the model.\n\nI will try a random forest classifier as well to see what changes it will return."}}