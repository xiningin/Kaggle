{"cell_type":{"56f8f26c":"code","91eb0252":"code","2cca726f":"code","08168e1f":"code","cb9000a8":"code","2ab165c6":"code","95e68293":"code","33d7f2d4":"code","210fb70e":"code","e15e6da9":"code","e76c3a70":"code","63d71c0e":"code","dc0916d6":"code","08840b7d":"code","66b80b8e":"code","c4c5ed9c":"code","4f03822e":"code","952656d1":"code","df4f0a03":"code","5e4ed410":"markdown","e01b01fb":"markdown","03fb2bd6":"markdown","e5c8b773":"markdown","cf3cb750":"markdown","3ea93289":"markdown","f8d25375":"markdown"},"source":{"56f8f26c":"import pandas as pd\nimport numpy as np\nimport gc\nfrom datetime import datetime\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset","91eb0252":"class SingleDataset(Dataset):\n    def __init__(self, x, is_sparse=False):\n        self.x = x.astype('float32')\n        self.is_sparse = is_sparse\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        if self.is_sparse: x = x.toarray().squeeze()\n        return x    ","2cca726f":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","08168e1f":"def get_data():\n    train_data = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\n    test_data = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n\n    X_nums = np.vstack([\n        train_data.iloc[:, 11:-1].to_numpy(),\n        test_data.iloc[:, 11:].to_numpy()\n    ])\n    X_nums = (X_nums - X_nums.mean(0)) \/ X_nums.std(0)\n\n    X_cat = np.vstack([\n        train_data.iloc[:, 1:11].to_numpy(),\n        test_data.iloc[:, 1:11].to_numpy()\n    ])\n    encoder = preprocessing.OneHotEncoder(sparse=False)\n    X_cat = encoder.fit_transform(X_cat)\n\n    X = np.hstack([X_cat, X_nums])\n    y = train_data['target'].to_numpy().reshape(-1, 1)\n    \n    del train_data, test_data\n    gc.collect()\n    \n    return X, y, X_cat.shape[1], X_nums.shape[1]","cb9000a8":"X, y, n_cats, n_nums = get_data()","2ab165c6":"class SwapNoiseMasker(object):\n    def __init__(self, probas):\n        self.probas = torch.from_numpy(np.array(probas))\n\n    def apply(self, X):\n        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask","95e68293":"class TransformerEncoder(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attention = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n        \n    def forward(self, x_in):\n        attention_out, _ = self.attention(x_in, x_in, x_in)\n        x = self.layernorm_1(x_in + attention_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x","33d7f2d4":"class TransformerAutoEncoder(torch.nn.Module):\n    def __init__(\n            self, \n            num_inputs, \n            n_cats, \n            n_nums, \n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512, \n            emphasis=.75, \n            task_weights=[10, 14],\n            mask_loss_weight=2,\n        ):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.n_cats = n_cats\n        self.n_nums = n_nums\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.emphasis = emphasis\n        self.task_weights = np.array(task_weights) \/ sum(task_weights)\n        self.mask_loss_weight = mask_loss_weight\n\n        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        \n        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n\n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n\n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n        return x\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        \n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n        return (x1, x2, x3), (reconstruction, predicted_mask)\n\n    def split(self, t):\n        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n\n    def feature(self, x):\n        attn_outs, _ = self.forward(x)\n        return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n\n    def loss(self, x, y, mask, reduction='mean'):\n        _, (reconstruction, predicted_mask) = self.forward(x)\n        x_cats, x_nums = self.split(reconstruction)\n        y_cats, y_nums = self.split(y)\n        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n\n        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n\n        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n\n        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]","210fb70e":"model_params = dict(hidden_size = 1024,num_subspaces = 8,embed_dim = 128,num_heads = 8,\n                    dropout = 0, feedforward_dim = 512,emphasis = .75,mask_loss_weight = 2)\nbatch_size = 384\ninit_lr = 3e-4\nlr_decay = 0.998\nmax_epochs = 1501\n\nrepeats = [  2,  2,  2,  4,  4,  4,  8,  8,  7, 15,  14]\nprobas =  [.95, .4, .7, .9, .9, .9, .9, .9, .9, .9, .25]\nswap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n\nbce_logits = torch.nn.functional.binary_cross_entropy_with_logits\nmse = torch.nn.functional.mse_loss","e15e6da9":"dl = DataLoader(\n            dataset=SingleDataset(X),\n            batch_size = batch_size,\n            shuffle = True,\n            pin_memory = True,\n            drop_last=True,\n            )","e76c3a70":"model = TransformerAutoEncoder(\n    num_inputs=X.shape[1],\n    n_cats=n_cats,\n    n_nums=n_nums,\n    **model_params\n).cuda()\n\nmodel_checkpoint = 'model_checkpoint.pth'\n\nprint(model)","63d71c0e":"noise_maker = SwapNoiseMasker(swap_probas)\noptimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)","dc0916d6":"for epoch in range(max_epochs):\n    t0 = datetime.now()\n    model.train()\n    meter = AverageMeter()\n    for i, x in enumerate(dl):\n        x = x.cuda()\n        x_corrupted, mask = noise_maker.apply(x)\n        optimizer.zero_grad()\n        loss = model.loss(x_corrupted, x, mask)\n        loss.backward()\n        optimizer.step()\n        \n        meter.update(loss.detach().cpu().numpy())\n    \n    delta = (datetime.now() - t0).seconds\n    scheduler.step()\n    print('\\r epoch {:5d} - loss {:.6f} - {:4.6f} sec per epoch'.format(epoch, meter.avg, delta), end='')","08840b7d":"torch.save({\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"model\": model.state_dict()\n    }, model_checkpoint\n)\n\nmodel_state = torch.load(model_checkpoint)\nmodel.load_state_dict(model_state['model'])","66b80b8e":"X_train = X[:300_000, :]\nX_test = X[300_000:, :]","c4c5ed9c":"del X\ngc.collect()","4f03822e":"dl = DataLoader(dataset=SingleDataset(X_train), batch_size=1024, shuffle=False, pin_memory=True, drop_last=False)\ntrain_features = []\nmodel.eval()\nwith torch.no_grad():\n    for x in dl:\n        train_features.append(model.feature(x.cuda()).detach().cpu().numpy())\ntrain_features = np.vstack(train_features)","952656d1":"dl = DataLoader(dataset=SingleDataset(X_test), batch_size=1024, shuffle=False, pin_memory=True, drop_last=False)\ntest_features = []\nmodel.eval()\nwith torch.no_grad():\n    for x in dl:\n        test_features.append(model.feature(x.cuda()).detach().cpu().numpy())\ntest_features = np.vstack(test_features)","df4f0a03":"np.save('dae_features_train.npy', train_features)\nnp.save('dae_features_test.npy', test_features)","5e4ed410":"# Step-3: Transformer Encoder","e01b01fb":"# Denoise-Transformer-AutoEncoder\u2693\n\nReference: https:\/\/github.com\/ryancheunggit\/Denoise-Transformer-AutoEncoder\/","03fb2bd6":"# Step-1: Encoding columns","e5c8b773":"## Utils","cf3cb750":"# Step-5: Training","3ea93289":"# Step-2: Applying noise","f8d25375":"# Step-4: Model"}}