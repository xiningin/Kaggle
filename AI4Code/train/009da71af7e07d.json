{"cell_type":{"845ab683":"code","de55b6bd":"code","b24fe352":"code","f1964e5b":"code","be5b1a8e":"code","437215c7":"code","0a5be031":"code","0f091fc4":"code","a134b592":"code","f8eca39f":"code","f931545b":"code","5c9fa46f":"code","ef9e361f":"code","23a9800b":"code","484e9e5c":"code","134e9c64":"code","3139d2e9":"code","bf4c6d83":"code","6ac1b2a9":"code","9260df55":"code","ed46aafa":"code","d69c245b":"code","6723d5a1":"code","67cf29fd":"code","00d6caa3":"code","b0ce9a96":"code","7bf1452f":"code","b989a1a8":"code","84ee3904":"code","47898a38":"code","14d11131":"code","d80e2e5b":"markdown","bbcf4227":"markdown","8a4f1d7a":"markdown","c6705f94":"markdown","3146c334":"markdown","4fc850d4":"markdown","c049034f":"markdown","cb1df208":"markdown","7bcc469e":"markdown","94a67900":"markdown","1a35bcc6":"markdown","347fe75e":"markdown","8601e536":"markdown","7775a83b":"markdown","c82226de":"markdown","e45f7597":"markdown","90f457aa":"markdown","a08da621":"markdown","e776f98d":"markdown"},"source":{"845ab683":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de55b6bd":"!pwd","b24fe352":"# Remember : You are declaring the dataframes here, rerun if dataframes get messed up\ntestX = pd.read_csv(\"\/kaggle\/input\/gene-expression\/data_set_ALL_AML_independent.csv\")\ntrainX = pd.read_csv(\"\/kaggle\/input\/gene-expression\/data_set_ALL_AML_train.csv\")\nlabels = pd.read_csv(\"\/kaggle\/input\/gene-expression\/actual.csv\") # contains Y (labels) for both train and test","f1964e5b":"trainX.head(n=3)\n","be5b1a8e":"testX.head(n=3)\n","437215c7":"print(len(testX), \"rows, and\", len(testX.columns),\"columns\")\nprint(testX.columns)","0a5be031":"print(trainX.shape)\nprint(trainX.columns)","0f091fc4":"print(len(labels), \"rows, and\", len(labels.columns),\"columns\")\nprint(labels.columns)","a134b592":"#@title Data Imputation\n# are there any NA values\n\nany([trainX[col].isna().any() for col in trainX.columns]) # We're asking \"Yo! Any NAs?\"\n\n# Luckily, nah!\n# There's functions like dropna() that you can use to deal with NAs","f8eca39f":"genes = trainX.iloc[:,:2]\n\ntrain_X = trainX[ sorted([valid_name for valid_name in trainX.columns[2:] if valid_name[:4] != 'call'], key = lambda colname: int(colname) ) ].T\ntest_X = testX[ sorted([valid_name for valid_name in testX.columns[2:] if valid_name[:4] != 'call'], key = lambda colname: int(colname) ) ].T","f931545b":"train_Y, test_Y =  1*(labels[:38][\"cancer\"] == \"AML\"), 1*(labels[38:][\"cancer\"] == \"AML\")","5c9fa46f":"from sklearn.decomposition import PCA\nreducer3 = PCA(n_components = 3)\nreducer2 = PCA(n_components = 2)\ntrain_X_PCA3 = reducer3.fit_transform(train_X)\ntrain_X_PCA2 = reducer2.fit_transform(train_X)","ef9e361f":"train_X_PCA3, train_X_PCA2","23a9800b":"from mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize =(10,6))\nax = plt.axes(projection='3d')\n\nax.scatter3D(train_X_PCA3[:,0], train_X_PCA3[:,1], train_X_PCA3[:,2], c=np.array([train_Y]).T);","484e9e5c":"from sklearn.neighbors import KNeighborsClassifier as knn","134e9c64":"# Initiate KNeighborsClassifier object\nthreeNN = knn(n_neighbors=3)","3139d2e9":"threeNN.fit(train_X, train_Y)","bf4c6d83":"test_Y_hat = threeNN.predict(test_X)","6ac1b2a9":"# Get the accuracy\nprint(\"Accuracy: \",np.sum((test_Y_hat - test_Y) == 0)\/len(test_Y))\n# Get the 'confusion matrix' using sklearn.metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nconfusion_matrix(test_Y, test_Y_hat)","9260df55":"def kNN(k, train_X, train_Y,test_X,test_Y):\n    return  accuracy_score(knn(n_neighbors=k).fit(train_X, train_Y).predict(test_X),test_Y)","ed46aafa":"kNN(3, train_X, train_Y,test_X,test_Y)\n","d69c245b":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_X_scaled = scaler.fit_transform(train_X)\ntest_X_scaled = scaler.transform(test_X)\n\nfrom sklearn.preprocessing import RobustScaler\nrscaler = RobustScaler()\ntrain_X_rscaled= rscaler.fit_transform(train_X)\ntest_X_rscaled = rscaler.transform(test_X)","6723d5a1":"train_X_PCA2 = reducer2.fit_transform(train_X_rscaled)\ntest_X_PCA2 = reducer2.transform(test_X_rscaled)","67cf29fd":"# k = 1,3,5,15, 25\n# standard scaler\n# pca\nresults = pd.DataFrame(np.zeros([5,4]),columns = [\"basic\",\"standard-scaled\",\"robust-scaled\",\"PCA reduced\"], index=[1,3,5,15,25])\nfor k in [1,3,5,15,25]:\n    results.loc[k] = [kNN(k, train_X, train_Y,test_X,test_Y),kNN(k, train_X_scaled, train_Y,test_X_scaled,test_Y),kNN(k, train_X_rscaled, train_Y,test_X_rscaled,test_Y),kNN(k, train_X_PCA2, train_Y,test_X_PCA2,test_Y)]","00d6caa3":"results","b0ce9a96":"for col in results.columns:\n    plt.plot(results.index,results[col], label = col)\nplt.legend()","7bf1452f":"from sklearn.linear_model import LogisticRegression\nLRclf = LogisticRegression()\nLRclf.fit(train_X, train_Y)\n# LRclf.fit(train_X_scaled, train_Y)","b989a1a8":"# to keep track of indices of genes, we use enumerate()\nprint(accuracy_score( LRclf.predict(test_X),test_Y))\nsorted(enumerate(np.squeeze(LRclf.coef_)), key = lambda x: x[1])[:7129 -16:-1]","84ee3904":"top15AMLindices = np.array(sorted(enumerate(np.squeeze(LRclf.coef_)), key = lambda x: x[1])[:7129 -16:-1])[:,0]\ngenes.iloc[top15AMLindices]","47898a38":"top15ALLindices = np.array(sorted(enumerate(np.squeeze(LRclf.coef_)), key = lambda x: x[1])[:15])[:,0]\ngenes.iloc[top15ALLindices]","14d11131":"import seaborn as sns\nimport scipy.stats as ss\n\nindices = np.hstack([top15ALLindices,top15AMLindices]) # so we first take 15 ALL-contributing genes, then 15 AML-contributing genes\n\ntrain_X_std = pd.DataFrame(ss.zscore(train_X.iloc[:,indices]), columns = [gene_desc[:30] for gene_desc in genes.iloc[indices,0]]) # Remember our training data has top 27 rows ALL, then rest AML\n\nplt.figure(figsize = (10,6))\nsns.heatmap(train_X_std.T,linewidths=.05,cmap=\"coolwarm\")\nplt.xlabel(\"Traning sampeles\")","d80e2e5b":"Now let's make a 3-D plot, using the three principal comonents of train data as features, and show corresponding labels as colors","bbcf4227":"What if we wanted to know which genes are more contributing in differentiating AML from ALL?\n\nThen another classification algorithm called Logistic Regression might be useful. (*Note that this regression is actually used for classification*)\n\nLogistic Regression tries to fit a S-shaped ([Sigmoid curve](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function) ) over the data. The sigmoid curve has asymptotes at y = 0 and y = 1, and the best-fitting simoid curve is obtained by optimizing the weights associated with each feature (each gene here). ![](https:\/\/sebastianraschka.com\/images\/faq\/logistic-why-sigmoid\/5.png) ![](https:\/\/sebastianraschka.com\/images\/faq\/logistic-why-sigmoid\/7.png)\n#### Fig 3: Sigmoid curve equation and graph\nThen we can interpret the weights (the coefficients) as the predicitive power a feature.\n### Task: Get the top 10 determining genes in our example","8a4f1d7a":"### Awesome! Now let's see how well did our model really do?\nAccuracy is the most straight-forward metric of model performance, a confusion matrix gives us:\n\n<table>\n<tr>\n    <td style=\"color:green\"> True Positive <\/td>\n    <td style=\"color:red\"> False Positive <\/td>\n<\/tr>\n\n<tr>\n    <td style=\"color:red\">False Negative<\/td>\n    <td style=\"color:green\">True Negative<\/td>\n<\/tr>\n<\/table>","c6705f94":"### Train (Fit) the classifier with X as train_x (all but first two rows) and train_y","3146c334":"So now we built a basic classifier. There is significant scope to improve, or **_preprocess_** our model.\nSome approaches you should try to use:\n### 1) Standardize the data: \nIs it fair for genes with higher average expression to weigh as much as genes with lower average expression for the same change in expression? If we have some genes that show high fluctuations in expression irrespective of AML or ALL, could their changes 'confuse' our model, or make it difficult to training our model. When your features might not be in the same range, it might be helpful to center the data by the mean of training data and scale it by standard deviation of training  data (which is just the z-scores for training data, and hopefully close to z-scores for test data). This can be done by sklearn.preprocessing.StandardScaler However, if your data has many outliers, you should scale \n\n","4fc850d4":"### 2) Reduce Dimensions\nWith 7129 features for each of the 38 training data points, how many numbers do you think the k-NN algorithm is using right now? This is called the curse of dimensionality. Now that we know how to project our data to lower dimensions using PCA, do you think we could use the two principal components as input features for each training data point? While this is not always the case, we know this should work for our data because when we visualized the data (2D plot), the AML- and ALL-labelled points were prety well separated. However, remember that PCA is unsupervised learning it doesn't take into account the labels and could have thus done rather more harm than good by making the classes overlap instead of separate. [Click here](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/03\/06064252\/Image_cont_26.jpg) to see an example of that circumstance.","c049034f":"### 5) Make the labels 0 for ALL 1 for AML and get train_Y (first 38) test_Y (38 onwards)","cb1df208":"### Which preprocessing stepa and which ```k``` parameter gives you the best classifier?\n### How does the accuracy vary with k?\n","7bcc469e":"### Get prdicted labels for test_X ( This are predicted y values or $\\hat{y}$)","94a67900":"For each of the three datasets, get the\n-  Number of rows\n- Numbeer of Columns\n- Title of columns","1a35bcc6":"# Data visualization\nWell, let's plot the train_X and show train_Y labels.\nBut wait how many features do you need to show? Now how do we show these **dimensions**?\nTurns out we can only visualize 2 or 3 dimensions.\nSolution: perform **Dimensionality Reduction**\n\n### Dimensionality Reduction\nWe find a lower dimensional representation data that best explains the variation in the data.\n\n<div align=\"center\">\n    <figure class=\"image\"><img src=\"https:\/\/miro.medium.com\/max\/1024\/1*vfLvJF8wHaQjDaWv6Mab2w.png\" alt=\"kNN example\" width=\"600\">\n        <figcaption> <h4> Figure 1: Dimensionality Reduction example: Principal Component Analysis transforming 3D to 2D (adapted from a medium article by SaiGayatri Vadali) <\/h4>\n        <\/figcaption>\n    <\/figure>\n<\/div>\n\n\nSome algorithms for Dimensionality Reduction include PCA, U-MAP, t-SNE","347fe75e":"# Classification\nClassification is a supervised learning where you predict particular labels, here \"AML\" versus \"ALL\" (technically 0 or 1). While we are performing binary classification, i.e. comparing probability of getting 0 or 1, in multi-class classfication, you would compare probability of getting more than 2 labels, and predict out of different classes.\n\n## k-Nearest Neighbors\nOne simple classification approach is to look at k nearest training data points to a test data point, and choose the label most common among the k-nearest neighbors. Of course, your results can eaaily vary with the choice of k parameter.\n\n\n<div align=\"center\">\n    <figure class=\"image\"><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/03\/knn3.png\" alt=\"kNN example\" width=\"600\">\n        <figcaption> <h4> Figure 2: k-Nearest Neighbors example. Green dot is the test data point. If k = 3, prediction is triangle, but if k = 5 prediction is square. (adapted from analyticsvidhya.com)<\/h4>\n        <\/figcaption>\n    <\/figure>\n<\/div>\n\n\n### Get the knn implememntation from sklearn","8601e536":"Now that you understand how dimensionality reduction basically works, you can even use UMAP and t-SNE without even reading about the underlying algorithms, almost like a black box. UMAP projections are very widely used nowadays to plot scRNA seq data for different cell populations and compare different cell types. Feel free to use scikit APIs for UMAP and t-SNE. \nUMAP \nRefer to [sckit-learn website](https:\/\/scikit-learn.org\/stable\/) to explore","7775a83b":"- Check the shape of both both reduced vectors\n- Do you find any similarity in the columns of the two vectors\n","c82226de":"# Explore the Data \n- Get the first 3 lines of training data using```trainX.head(n=3)``` (and test data similarly)\n- Get the full list of columns in train and test\n- Get a feel of the datasets","e45f7597":"# Lesson 8: Machine Learning - Classification, Dimesionality Reduction\n\nYou have already started Machine Learning when you performed the linear regression analysis, but let's talk about Machine Learning in general first, then, as promised earlier, we'll move to our larger dataset. \n\n<table style=\"width:100%;border: 1px solid black;padding: 8px;\">\n\t<tr>\n        <th rowspan=4><p style=\"text-align:center;font-size:200%;\"> Machine Learning<\/p><p style=\"text-align:center;\">(Learn from data and make decisions)<\/p><\/th>\n        <th rowspan=2><p style=\"text-align:center;font-size:200%;\"> Supervised Learning <\/p><p style=\"text-align:center;\">(Predictive Model)<\/p><\/th>\n        <td style=\"text-align:center;\"> Classification <\/td>\n\t<\/tr>\n    <tr>\n    \t<td style=\"text-align:center;\"> Regression <\/td>\n    <\/tr>\n    <tr>\n        <th rowspan=2><p style=\"text-align:center;font-size:200%;\"> Unsupervised Learning<\/p><p style=\"text-align:center;\">(Non-predicitve Model)<\/p><\/th>\n        <td style=\"text-align:center;\"> Clustering <\/td>\n\t<\/tr>\n    <tr>\n        <td style=\"text-align:center;\"> Dimensionality Reduction <\/td>\n\t<\/tr>\n<\/table>\n\n\n### Supervised Learning: \nUse training set with correct inputs and outputs to predict outputs for test data inputs. \n#### Classification: \n - Inputs(X): Features \n - Outputs(y): binary or multiple classes\n \n#### Regression: \n - Inputs(X): Independent Variable \n - Outputs(y): Dependent Variable (Continuous)\n \n### Unupervised Learning:\nFind patterns among inputs (features), no labels in data\n#### Clustering:\n- Find groups within data (Example: Phylogeny tree)\n\n#### Dimensionality Reduction:\n- Find a lower dimension representation of higher dimensional data\n\n","90f457aa":"# Data Cleaning and manipulation\n### 1) We don't need those 'call.\\*' columns\nHow do you remove all those particular columns with column name of 'call.xx' type?\n\n### 2) What are the rows and what are the columns?\nBetter representation is probably with genes as columns and samples as rows (Transpose the data)\n\n### 3) Can we separate the gene descriptions and gene accession numbers?\nBoth test and train share these two row (after transpose) and we could keep them separate from numerical contents of test and train.\n### 4) Are the column names in order?\nSort the columns by column names","a08da621":"Similarly, can you make a 2-D plot of our training data and test data combined? Name the principal components vector (which will be of shape (72,2)) ```X_PCA2```\n\n*Make sure you get the two labels forming largely visibly distinct clusters. Ask for help if you dont't*\n\n*Hint: Should you PCA after or before combining train and test data*\n\n\n<details><summary>Try yourself and then find the solution code here<\/summary>\n\nX = np.vstack([train_X,test_X]) <br>\nX_PCA2 = reducer2.fit_transform(X) <br>\nY = np.hstack([[train_Y],[test_Y]]).T <br>\nplt.scatter(X_PCA2[:,0],X_PCA2[:,1],c=Y) <br>\n    or<br>\ntrain_X_PCA2 = reducer2.fit_transform(train_X) <br>\ntest_X_PCA2 = reducer2.transform(test_X) <br>\nX_PCA2 = np.vstack([train_X_PCA2,test_X_PCA2]) <br>\nY = np.hstack([[train_Y],[test_Y]]).T <br>\nplt.scatter(X_PCA2[:,0],X_PCA2[:,1],c=Y) <br>\n<\/details>\n","e776f98d":"To confirm this visually, let us make a heatmap of expression level of these top 15 ALL-contributing and top 15 AML-contributing genes for 10 AML patients, and 10 ALL patients.\n### Interpeting the heatmap of selected genes\nWhich regions do you find warmer for the first 27 columns (ALL samples)? The top 15 genes or the bottom 15?\nWhich regions do you find warmer for the last 11 columns (AML samples)? The top 15 genes or the bottom 15?"}}