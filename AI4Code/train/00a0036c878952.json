{"cell_type":{"2266d04f":"code","181d7ca6":"code","e99dd54e":"code","3fb7b507":"code","a6d49372":"code","fe6071d7":"code","c11bc8da":"code","4b634639":"code","6921a1fb":"code","3393f394":"code","fe6f5997":"code","9a960fbe":"code","9fc13086":"code","f0bcb29a":"code","5aeaa799":"code","665f4a0e":"code","8759bcfc":"code","e0f56d54":"code","0720b0d7":"code","09a43541":"code","1bf4e28b":"code","7169fbb5":"code","e855dc35":"code","2fdafe76":"code","8d830d87":"code","bc5df0e3":"code","6b0d1f90":"markdown","57969f22":"markdown","39878f40":"markdown","e9583dd1":"markdown","a23b5910":"markdown","cc2d1df3":"markdown","d4151e92":"markdown","006da815":"markdown","2151d72e":"markdown","d7f3ccd9":"markdown","c60031f1":"markdown","4d7c2c5f":"markdown","f07c3cad":"markdown","dda18307":"markdown","6421aa91":"markdown","38bd78b6":"markdown","d7700953":"markdown","7c9b932a":"markdown","2bb28601":"markdown","d1c0c954":"markdown","f5374626":"markdown","065e01d2":"markdown"},"source":{"2266d04f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#split\nfrom sklearn.model_selection import train_test_split\n\n#score and Accuracy\nfrom sklearn.metrics import precision_score, accuracy_score, confusion_matrix\n\n#models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.neighbors import NearestCentroid,KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier","181d7ca6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Color Palettes\ncolors = [\"#1F2642\",\"#DF7979\", \"#B6B6B6\"]\nsns.palplot(sns.color_palette(colors))","e99dd54e":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ntrain_data.head().style.set_caption(\"Train Dataframe\").set_properties(**{'background-color': colors[2],\n                                                                         'color':colors[0],'border': '1.5px  solid black'})","3fb7b507":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ntest_data.head().style.set_caption(\"Test Dataframe\").set_properties(**{'background-color': colors[2],\n                                                                         'color':colors[0],'border': '1.5px  solid black'})","a6d49372":"train_data.isnull().sum()\/len(train_data)* 100","fe6071d7":"test_data.isnull().sum()\/len(train_data)* 100","c11bc8da":"#imputing the Embarked with mode\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace = True)\n\n#removing column Cabin with missing data more than 50%\ntrain_data.drop(['Cabin'], axis=1, inplace=True)\ntest_data.drop(['Cabin'], axis=1, inplace=True)\n\n#imputing the Age with meadian \ntrain_data['Age'].fillna(train_data['Age'].median(),inplace = True)\ntest_data['Age'].fillna(test_data['Age'].median(),inplace = True)\n\n#impiting fare in test data with median\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace = True)","4b634639":"sns.countplot(x=train_data['Survived'], palette = colors);","6921a1fb":"sns.countplot(x=train_data['Pclass'], palette = colors);","3393f394":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = round(sum(women)\/len(women)* 100)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = round(sum(men)\/len(men) * 100)\n\nprint(f\"Women who survived: {rate_women}% \")\nprint(f\"Men who survived: {rate_men}%\")","fe6f5997":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\"] # Feature Selection\n\nX = pd.get_dummies(train_data[features])\ny = train_data[\"Survived\"] # Target Value\n\nX_testing = pd.get_dummies(test_data[features].fillna(-1))","9a960fbe":"#spliting the train test\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state = 42)\nprint(f\" Xtrain:{X_train.shape}, ytrain:{y_train.shape}\")","9fc13086":"#finding the best models with base parameters\nmodels = [\n    ('Logistic Regression', LogisticRegression(max_iter=1000)),\n    ('Ridge', RidgeClassifier()),\n    ('SGD Classifier', SGDClassifier(max_iter=1000, tol=1e-3)),\n    ('Support Vector Classifier', SVC()),\n    ('NuSVC', NuSVC()),\n    ('Decision Tree', DecisionTreeClassifier()),\n    ('Gaussian NB', GaussianNB()),\n    ('Bernoulli NB', BernoulliNB()),\n    ('Perc', Perceptron()),\n    ('Nearest Centroid', NearestCentroid()),\n    ('Random Forest Classifier', RandomForestClassifier()),\n    ('Ada Boost Classifier', AdaBoostClassifier()),\n    ('XGB Classifier', XGBClassifier(verbosity = 0)),\n    ('Passive Aggressive', PassiveAggressiveClassifier())\n]\n\nresults = dict()\nfor name, model in models:\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    \n    \n    score = accuracy_score(y_test, pred)\n    cm = confusion_matrix(y_test,pred)\n    precision = precision_score(y_test, pred) \n    results[name] = score ","f0bcb29a":"#storing the results into dataframe\ndf_results = pd.DataFrame([results])\ndf_results = df_results.transpose()\ndf_results = df_results.rename(columns={0:'Score'}).sort_values(by='Score',ascending=False)","5aeaa799":"df_results.style.set_properties(**{'background-color': colors[1],\n                                    'color': colors[0],\n                                    'border': '0.5px  solid black'})","665f4a0e":"fig = plt.figure(figsize=(15,9))\n\nax = sns.barplot(data=df_results, \n                 y=df_results.index, \n                 x='Score',\n                 saturation=0.8,\n                 linewidth=0.1,\n                 color = colors[0])\n","8759bcfc":"model = RandomForestClassifier()\nmodel.fit(X_train,y_train)\npredict= model.predict(X_test)","e0f56d54":"from sklearn.metrics import plot_roc_curve\nA = plot_roc_curve(model, X_test, y_test);","0720b0d7":"fig,ax = plt.subplots(figsize=(4,4))\nax= sns.heatmap(confusion_matrix(y_test, predict),\n                annot = True,\n                cbar= True,\n                fmt='g',\n                cmap = colors)\nplt.xlabel('True Label')\nplt.ylabel('Predicted Label')\nplt.title('Confusion-Matrix')\nplt.tight_layout()  \nplt.show()","09a43541":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predict))","1bf4e28b":"import optuna\n\nimport sklearn.datasets\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\ndef objective(trial):\n    x, y = X_train,y_train\n\n    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n    max_depth = trial.suggest_int(\"max_depth\", 2, 100, log=True)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1,1000)    \n    min_samples_split = trial.suggest_int(\"min_samples_split\",1,5)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\",1,5)\n    \n\n    rf = sklearn.ensemble.RandomForestClassifier(criterion =criterion,\n            max_depth=max_depth, \n            n_estimators=n_estimators\n        )\n\n    score = cross_val_score(rf, x, y, n_jobs=-1, cv=3)\n    accuracy = score.mean()\n    return accuracy\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=101)","7169fbb5":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","e855dc35":"rfc_params = {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 196, 'min_samples_split': 4, 'min_samples_leaf': 1}\n\nmodel = RandomForestClassifier(**rfc_params)\nmodel.fit(X_train,y_train)\nprediction= model.predict(X_test)","2fdafe76":"sub_pre = model.predict(X_testing)","8d830d87":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': sub_pre})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","bc5df0e3":"output.head().style.set_caption(\"Submission Dataframe\").set_properties(**{'background-color': colors[2],\n                                                                         'color':colors[0],'border': '1.5px  solid black'})","6b0d1f90":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'> <span style='background:#1A2540'>THE END<\/span>**","57969f22":"Using the best parameters in the model","39878f40":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'>  \ud83e\uddeeModel Selection and Predicting <\/span>**","e9583dd1":"### <span style=\"color:#F65557;font-weight:bold\"> Filling missing Values <span>","a23b5910":"The Dataset contains some nan values we will take a look.","cc2d1df3":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'> \ud83d\udcd5Importing Libraries <\/span>**","d4151e92":"##### After looking checking the multiple models the best performing models are XGB and Random Forest Classifier","006da815":"**This is my first notebook on kaggle. As this is my first notebook I keep on updating the notebook after learning new and better presentation of codes and plots.**\n**This is most basic dataset among the begineers of machine learning entusiast. There are better and robust approaches out there but I love to learn anything a bit simpler and understandable so here is my notebook lets get started.**\n\n**The task is to predict survival of passangers with 0 = \"Not Survived\" and 1 = \"Survived\"**","2151d72e":"### <span style=\"color:#F65557;font-weight:bold\">Tuning Random Forest Classifer with Optuna <span>\n\n    \n##### Using the Bayesian Optimization hyper parameter tuner (Optuna)","d7f3ccd9":"Balanced Data \"Hurray\"","c60031f1":"As we can see there are 3 columns with some missing values and imputing them is important for the model to predict Accurately.\n\n* Cabin has more than 50% missing data so we will remove it from both data.","4d7c2c5f":"##### I selected multiple model to see the baseline of the models and which baseline gives the good score before tuning the parameters","f07c3cad":"**Theme:**\n\nI love to pre select my color pallet for the visualization so it will look clean and consitent through out the vizualization.","dda18307":"<h2><center> <span style=\"font-family:Georgia\"> <span style=\"color:white;font-weight:bold\"> <span style=\"background:#1A2540\">\u270c\ufe0f If you like my notebook and found it usefull please do upvote<\/span><\/span><\/span><\/center><\/h2>","6421aa91":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'> \ud83d\udea2 Titanic Survival Prediction <\/span>**","38bd78b6":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'>  \ud83d\udd0dExploratory Data Analysis <\/span>**","d7700953":"<img src=\"https:\/\/imgur.com\/oLUbTMG.gif\">","7c9b932a":"### <span style=\"color:#F65557;font-weight:bold\"> Accuracy <span>","2bb28601":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'> \ud83d\udcbdImporting DataSet <\/span>**\n","d1c0c954":"### <span style=\"color:#F65557;font-weight:bold\"> Balance of Target column <span>","f5374626":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'> <span style='background:#1A2540'> \ud83d\udce6 Submission<\/span>**","065e01d2":"# **<center><span style='font-family:Georgia'><span style=\"color:white;font-weight:bold\"> <span style='background:#1A2540'>  \ud83d\udee0\ufe0f Hyper Parameter Tuning <\/span>**"}}