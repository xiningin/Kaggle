{"cell_type":{"2d41a1de":"code","c20dd8ed":"code","99344fb6":"code","5ff5d663":"code","b50e8e55":"code","568f4c81":"code","b9fae15d":"code","17b2db84":"code","6c7ce527":"code","f80708c7":"code","50ac86d3":"code","b1e239ea":"code","ceaf0ab9":"code","5b4dbb2a":"code","92cb6748":"code","dfae3d53":"code","160bd4c6":"code","063ddba8":"code","768e8308":"code","295117cf":"code","d9384498":"code","d5a412c8":"code","fcf833fd":"code","b6d3e9c1":"code","eddaf917":"code","3cca14d3":"code","f0dd61ac":"code","cc2f76f9":"markdown","2e30e1a3":"markdown","721ab5e3":"markdown","3d43de2c":"markdown","bfebf6fa":"markdown","e7662932":"markdown","a04e3d12":"markdown","cde15756":"markdown","8619094a":"markdown","e226aef8":"markdown","c047179f":"markdown","f6381df7":"markdown"},"source":{"2d41a1de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","c20dd8ed":"train = pd.read_csv(r'..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv(r'..\/input\/30-days-of-ml\/test.csv')\nsub = pd.read_csv(r'..\/input\/30-days-of-ml\/sample_submission.csv')","99344fb6":"train.shape, test.shape, sub.shape","5ff5d663":"train.head()","b50e8e55":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","568f4c81":"plt.figure(figsize=(15,5))\nsns.kdeplot(train['target'], shade = True, alpha = 0.9, linewidth = 1.5, facecolor=(1, 1, 1, 0), edgecolor=\".2\")\nplt.title(\"Target\", fontdict={'fontsize':20})\nplt.show()","b9fae15d":"fig, axes =plt.subplots(5,2, figsize=(15,15), sharex=True)\naxes = axes.flatten()\nobject_bol = train.dtypes == 'object'\nfor ax, catplot in zip(axes, train.dtypes[object_bol].index):\n    sns.countplot(y=catplot, data=train, ax=ax)\nplt.tight_layout()     \nplt.show()","17b2db84":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[10:24])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[10:24][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[10:24][i]], shade = True, alpha = 0.9, linewidth = 1.5, facecolor=(1, 1, 1, 0), edgecolor=\".2\")\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","6c7ce527":"print('train continuous features: ')\ntrain.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","f80708c7":"print('test continuous features: ')\ntest.describe().T.style.bar(subset=['mean'], color='#606ff2')\\\n                            .background_gradient(subset=['std'], cmap='PuBu')\\\n                            .background_gradient(subset=['50%'], cmap='PuBu')","50ac86d3":"from sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\ncat_cols = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\nfor i in cat_cols:\n    train[i] = oe.fit_transform(np.array(train[i]).reshape(-1,1))\n    test[i] = oe.transform(np.array(test[i]).reshape(-1,1))","b1e239ea":"cont_cols = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']\ntrain[cont_cols] = np.log(train[cont_cols])\ntest[cont_cols] = np.log(test[cont_cols])","ceaf0ab9":"X = train.drop('target',axis=1)\ny = train['target']","5b4dbb2a":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n         'max_depth': trial.suggest_int('max_depth', 1, 10), # Extremely prone to overfitting!\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n        'n_estimators': trial.suggest_int('n_estimators', 400, 20000, 100), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter.\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.99, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n        'n_jobs':4,\n    } \n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","92cb6748":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","dfae3d53":"# study = optuna.create_study(direction = 'minimize')\n# study.optimize(objective,n_trials=15)\n# study.best_params","160bd4c6":"xgb_params = {'max_depth': 1,\n 'learning_rate': 0.8616220572782383,\n 'n_estimators': 12900,\n 'eta': 0.011828729736491916,\n 'subsample': 0.9,\n 'colsample_bytree': 0.7,\n 'colsample_bylevel': 0.30000000000000004,\n 'min_child_weight': 1.5207698213399512,\n 'reg_lambda': 0.0005791179588496165,\n 'reg_alpha': 0.17560065566511068,\n 'gamma': 0.1553307899331432,'tree_method':'gpu_hist','n_jobs':4}","063ddba8":"def cross_val(X, y, model, params, folds=10):\n\n    kf = KFold(n_splits=folds, shuffle=True, random_state=2021)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.values[train_idx], y.values[train_idx]\n        x_test, y_test = X.values[test_idx], y.values[test_idx]\n\n        alg = model(**params,random_state = 2021)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=400,\n                verbose=False)\n        pred = alg.predict(x_test)\n        error = mean_squared_error(y_test, pred,squared = False)\n        print(f\" mean_squared_error: {error}\")\n        print(\"-\"*50)\n    \n    return alg","768e8308":"xgb_model = cross_val(X, y, XGBRegressor, xgb_params)","295117cf":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-4, 1e4),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-4, 1e4),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.02 , 1),\n        'max_depth' : trial.suggest_int('max_depth', 1 , 10),\n        'n_estimators' : trial.suggest_int('n_estimators', 400, 20000, 100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n        'subsample' : trial.suggest_discrete_uniform('subsample' , 0.2, 0.99, 0.1), \n        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n        'n_jobs':4\n    }\n    \n    \n    model = LGBMRegressor(**params, random_state=2021)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","d9384498":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","d5a412c8":"# study = optuna.create_study(direction = 'minimize')\n# study.optimize(objective,n_trials=15)\n# study.best_params","fcf833fd":"lgb_params = {'reg_alpha': 0.00016893787477692912,\n 'reg_lambda': 7380.524142310997,\n 'learning_rate': 0.05382799531839089,\n 'max_depth': 4,\n 'n_estimators': 9900,\n 'min_child_weight': 10.617199131630898,\n 'subsample': 0.2,\n 'colsample_bytree': 0.2,'device_type' : 'gpu','n_jobs':4}","b6d3e9c1":"lgb_model = cross_val(X, y, LGBMRegressor, lgb_params)","eddaf917":"xgb = XGBRegressor(**xgb_params)\nlgb = LGBMRegressor(**lgb_params)","3cca14d3":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', lgb),\n                ('xgb', xgb)\n            ],\n            weights = [0.15,0.25]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits ","f0dd61ac":"sub['target'] = xgb_model.predict(test)\nsub.to_csv(f'xgb.csv',index = False)\n\nsub['target'] = lgb_model.predict(test)\nsub.to_csv(f'lgb.csv',index = False)\n\nsub['target'] = predictions\nsub.to_csv(f'vote.csv',index = False)","cc2f76f9":"### Continuous features","2e30e1a3":"## Final voting (xgb+lgb):","721ab5e3":"## Model Building:","3d43de2c":"### Categorical features","bfebf6fa":"### Data Preprocessing","e7662932":"* **lightgbm:**","a04e3d12":"* **xgboost:**","cde15756":"* these are the best params recovered from Optuna.","8619094a":"### Prediction and Submission:","e226aef8":"### Data visualization","c047179f":"### Optuna:\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.","f6381df7":"* these are the best params recovered from Optuna."}}