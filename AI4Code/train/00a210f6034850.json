{"cell_type":{"3f0dd221":"code","34fe3cb5":"code","b312de69":"code","054bd985":"code","de53a67c":"code","549cc2dd":"code","59701243":"code","47759b22":"code","6085bbb7":"code","335aaa9d":"code","64bf0fca":"code","4ad39b33":"code","ce236d40":"code","3222d25f":"code","ea957e91":"code","1c1b31bc":"code","73a99a5e":"code","a557b67a":"code","7a85a386":"code","27f66628":"code","77efb867":"code","ed0b147a":"code","096920be":"code","ffb388ca":"code","c4f8160a":"code","4abd89c8":"code","2341a411":"code","96046040":"code","a3f3e400":"code","ca522358":"code","b514edf3":"code","31e900a4":"code","bc14b9c2":"code","d093ab8d":"code","0de8a952":"code","3e17a0a4":"code","e89245e7":"code","a0f0180f":"code","0b818d3f":"code","252f7e6f":"code","caee5209":"code","43f16c1f":"code","7ecd2b16":"code","6bb9c2d5":"code","e45c0d72":"code","07ab2726":"markdown","c7235e1f":"markdown","85f05500":"markdown"},"source":{"3f0dd221":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","34fe3cb5":"import pandas as pd\nimport tensorflow as tf\nimport re\nimport numpy as np\ntf.__version__","b312de69":"southpark=pd.read_csv('..\/input\/southparklines\/All-seasons.csv')","054bd985":"southpark.head()","de53a67c":"def clean_text(text):\n    text = text.lower()\n    \n    text = re.sub(r\"\\n\", \"\",  text)\n    text = re.sub(r\"[-()]\", \"\", text)\n    text = re.sub(r\"\\.\", \" .\", text)\n    text = re.sub(r\"\\!\", \" !\", text)\n    text = re.sub(r\"\\?\", \" ?\", text)\n    text = re.sub(r\"\\,\", \" ,\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"ohh\", \"oh\", text)\n    text = re.sub(r\"ohhh\", \"oh\", text)\n    text = re.sub(r\"ohhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhh\", \"oh\", text)\n    text = re.sub(r\"ohhhhhh\", \"oh\", text)\n    text = re.sub(r\"ahh\", \"ah\", text)\n    \n    return text","549cc2dd":"text=[]\nfor line in southpark.Line:\n    text.append(clean_text(line))\n","59701243":"#counting length of each sentence by splitting a sentence into words\nlength=[]\n\nfor line in text:\n    #print(line.split())\n    length.append(len(line.split()))\nlengths = pd.DataFrame(length, columns=['counts'])","47759b22":"lengths.describe()","6085bbb7":"print(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))\n\nprint(np.percentile(lengths, 99))\n","335aaa9d":"max_line_len=30\nshort_text=[]\nfor line in text:\n    if(len(line.split() )<= max_line_len):\n        short_text.append(line)\nshort_text[:5]\n","64bf0fca":"vocab={}\nfor line in short_text:\n    for word in line.split():\n        if word not in vocab:\n            vocab[word]=1\n        else :\n            vocab[word]+=1","4ad39b33":"print(\"Size of vocab is\",len(vocab))","ce236d40":"#limit occurance of word, that is used more than 3 times\nthreshold = 3\ncount=0\nfor k,v in vocab.items():\n    \n    if v>=threshold:\n        count+=1\n    else: \n        #print(v)\n        pass\n#count\nprint(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","3222d25f":"source_vocab_to_int = {}\nword_num=0\nfor k,v in vocab.items():\n    if v >= threshold:\n        source_vocab_to_int[k]=word_num\n        word_num+=1\nlen(source_vocab_to_int)\n","ea957e91":"target_vocab_to_int={}\nword_num=0\nfor k,v in vocab.items():\n    if v>= threshold:\n        target_vocab_to_int[k]=word_num\n        word_num+=1\nlen(target_vocab_to_int)","1c1b31bc":"# adding essential token to the vocab (dictionary)\ntokens = ['<PAD>','<EOS>','<UNK>','<GO>']\nfor token in tokens:\n    source_vocab_to_int[token]=len(source_vocab_to_int)+1\nfor token in tokens:\n    target_vocab_to_int[token]=len(target_vocab_to_int)+1","73a99a5e":"# int to vocab mapping\nsource_int_to_vocab={v_i:v for v,v_i in source_vocab_to_int.items()}\ntarget_int_to_vocab={v_i:v for v,v_i in target_vocab_to_int.items()}\n","a557b67a":"# Check the length of the dictionaries.\nprint(len(source_vocab_to_int))\nprint(len(source_int_to_vocab))\nprint(len(target_vocab_to_int))\nprint(len(target_int_to_vocab))","7a85a386":"# creating source and  target text\nsource_text = short_text[:-1]\ntarget_text = short_text[1:]\n\nfor i in range(len(target_text)):\n    target_text[i] += ' <EOS>'","27f66628":"print(len(source_text))\nprint(len(target_text))","77efb867":"source_text[:3]","ed0b147a":"\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional,GRU,CuDNNLSTM,CuDNNGRU\nfrom keras.models import Model, load_model\n\n","096920be":"num_samples=3000\nshort_source=source_text[:3000]\nshort_target=target_text[:3000]\nshort_source_token = [nltk.word_tokenize(sent) for sent in short_source]\nshort_target_token = [nltk.word_tokenize(sent) for sent in short_target]","ffb388ca":"print(short_source_token[:5])\nprint(short_target_token[:5])","c4f8160a":"data_size = len(short_source_token)\n\n# We will use the first 0-80th %-tile (80%) of data for the training\ntraining_input  = short_source_token[:round(data_size*(80\/100))]\ntraining_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\ntraining_output = short_target_token[:round(data_size*(80\/100))]\n\n# We will use the remaining for validation\nvalidation_input = short_source_token[round(data_size*(80\/100)):]\nvalidation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\nvalidation_output = short_target_token[round(data_size*(80\/100)):]\n\nprint('training size', len(training_input))\nprint('validation size', len(validation_input))","4abd89c8":"# word encoding for dictionary\nvocab = {}\nfor question in short_source_token:\n    for word in question:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n\nfor answer in short_target_token:\n    for word in answer:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1  ","2341a411":"#remove rare words\nthreshold = 3\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1\nprint(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","96046040":"# creating dictionaries\nword_code_start=1\nword_code_padding=0\n\n\nword_num=2 # position 1 is left for word_code_start\nencoding = {}\ndecoding = {1 : 'START'}\nfor word, count in vocab.items():\n    if count>=threshold:\n        encoding[word] = word_num\n        decoding[word_num]= word\n        word_num+=1\nprint(\"No. of vocab used : \", word_num)","a3f3e400":"#word not in dictionary makes as <UNK>\ndecoding[len(encoding)+2] = '<UNK>'\nencoding['<UNK>'] = len(encoding)+2","ca522358":"dict_size=word_num+1","b514edf3":"#converting into vector\ndef transform_into_vector(encoding, data, vector_size=20):\n    transformed_data=np.zeros(shape=(len(data),vector_size))\n    for i in range(len(data)):\n        for j in range(min(len(data[i]),vector_size)):\n            try:\n                transformed_data[i][j] = encoding[data[i][j]]\n            except:\n                transformed_data[i][j] = encoding['<UNK>']\n    return transformed_data","31e900a4":"INPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\nencoded_training_input = transform_into_vector(\n    encoding, training_input, vector_size=INPUT_LENGTH)\nencoded_training_output = transform_into_vector(\n    encoding, training_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_training_input', encoded_training_input.shape)\nprint('encoded_training_output', encoded_training_output.shape)","bc14b9c2":"encoded_training_input[:5]","d093ab8d":"#encoding validation dataset\nencoded_validation_input = transform_into_vector(encoding, validation_input,vector_size=INPUT_LENGTH)\nencoded_validation_output = transform_into_vector(encoding, validation_output,vector_size=OUTPUT_LENGTH)\nprint('Encoded_validation_input',encoded_validation_input.shape)\nprint('ENcoded_validation_output',encoded_validation_output.shape)","0de8a952":"import tensorflow as tf\ntf.keras.backend.clear_session()","3e17a0a4":"INPUT_LENGTH=20\nOUTPUT_LENGTH=20\nencoder_input = Input(shape=(INPUT_LENGTH,))\ndecoder_input= Input(shape=(OUTPUT_LENGTH,))\nprint(encoder_input.shape)\nprint(decoder_input.shape)\nfrom keras.layers import SimpleRNN\nencoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH,mask_zero=True)(encoder_input)\nencoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\nencoder_last=encoder[:,-1,:]\n\nprint('Encoder',encoder)\nprint('Encoder_last', encoder_last)\n\ndecoder = Embedding(dict_size,128, input_length=OUTPUT_LENGTH,mask_zero=True)(decoder_input)\ndecoder = LSTM(512, return_sequences=True, unroll=True)(decoder,initial_state=[encoder_last, encoder_last])\n","e89245e7":"from keras.layers import Activation, dot, concatenate\n\n# Equation (7) with 'dot' score from Section 3.1 in the paper.\n# Note that we reuse Softmax-activation layer instead of writing tensor calculation\nattention = dot([decoder, encoder], axes=[2, 2])\nattention = Activation('softmax', name='attention')(attention)\nprint('attention', attention)\n\ncontext = dot([attention, encoder], axes=[2,1])\nprint('context', context)\n\ndecoder_combined_context = concatenate([context, decoder])\nprint('decoder_combined_context', decoder_combined_context)\n\n# Has another weight + tanh layer as described in equation (5) of the paper\noutput = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\noutput = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\nprint('output', output)","a0f0180f":"model = Model(inputs=[encoder_input,decoder_input],outputs=[output])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary","0b818d3f":"training_encoder_input = encoded_training_input\ntraining_decoder_input = np.zeros_like(encoded_training_output)\ntraining_decoder_input[:, 1:] = encoded_training_output[:,:-1]\ntraining_decoder_input[:, 0] = word_code_start\ntraining_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n\nvalidation_encoder_input = encoded_validation_input\nvalidation_decoder_input = np.zeros_like(encoded_validation_output)\nvalidation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\nvalidation_decoder_input[:, 0] = word_code_start\nvalidation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]","252f7e6f":"print(training_encoder_input.shape)\nprint(training_decoder_output.shape)\nprint(training_decoder_input.shape)\nprint(validation_encoder_input.shape)\nprint(validation_decoder_input.shape)\nprint(validation_decoder_output.shape)","caee5209":"model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n          #validation_split=0.1,\n          batch_size=128, epochs=100)\n\nfrom keras.models import load_model\n\nmodel.save('model_attention_enc_dec.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model","43f16c1f":"from keras.models import load_model\nmodel = load_model('model_attention_enc_dec.h5')","7ecd2b16":"#model testing\ndef prediction(raw_text):\n    clean_input = clean_text(raw_text)\n    input_token = [nltk.word_tokenize(clean_input)]\n    input_token = [input_token[0][::-1]] #reversing input token\n    encoder_input = transform_into_vector(encoding, input_token, 20)\n    decoder_input = np.zeros(shape=(len(encoder_input),OUTPUT_LENGTH))\n    decoder_input[:,0] = word_code_start\n    for i in range(1, OUTPUT_LENGTH):\n        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n        decoder_input[:,i] = output[:,i]\n    return output\ndef decode(decoding, vector):\n    \n    text = ''\n    for i in vector:\n        if i == 0:\n            break\n        text += ' '\n        text += decoding[i]\n    return text","6bb9c2d5":"for i in range(100):\n    seq_index = np.random.randint(1, len(short_source))\n    output = prediction(short_source[seq_index])\n    print ('Query:', short_source[seq_index])\n    print ('Bot:', decode(decoding, output[0]))","e45c0d72":"'''raw_input = input()\noutput = prediction(raw_input)\nprint (decode(decoding, output[0]))\n'''","07ab2726":"**Model Building******","c7235e1f":"**END******","85f05500":"I would like to thank this [notebook](http:\/\/www.kaggle.com\/currie32\/a-south-park-chatbot) for helping in input pipeline preparation"}}