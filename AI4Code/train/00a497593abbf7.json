{"cell_type":{"b025335a":"code","386adfd5":"code","6ba0fd1d":"code","791a1cc7":"code","c7013165":"code","f0f601c9":"code","38d145cb":"code","a73ef91b":"code","758c6a62":"code","2e6f54c7":"code","8be2fd0b":"code","a83305f4":"code","9bdd264a":"code","94e4c6eb":"code","672a2039":"code","51853e00":"code","771eed9e":"code","53e32ccb":"code","029d5d25":"code","98a616a9":"code","f990280f":"code","d6f6308e":"code","325679fd":"code","80bc21d0":"code","98c2d832":"code","28b9480e":"code","2fc2b13f":"code","3fe5cd2f":"code","195a4251":"code","407e0244":"code","5e66a627":"code","2024078e":"code","1276ee0f":"code","4ac48991":"code","5711c2b0":"code","651aa277":"code","79df3fb0":"markdown","afd71c80":"markdown","56fb9bd8":"markdown","4a2e05b2":"markdown","ee0c50ff":"markdown","a0a854c0":"markdown","446156be":"markdown","7c4953c1":"markdown"},"source":{"b025335a":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img1.png\")","386adfd5":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img2.png\")","6ba0fd1d":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img3.png\")","791a1cc7":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img4.png\")","c7013165":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img5.png\")","f0f601c9":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs\/reg_img6.png\")","38d145cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a73ef91b":"#S\u0130mple Linear Regression\n\n#Y \u2248 \u03b2 0 + \u03b2 1 X.\n\n#this is our equation of simple linear regression model, here b0 is called \n#bias or intercept and b1 is coefficient\n\n#First we'll import the required libraries matplotlib for data visualization\n#pandas for dataframe,list,series data structures of python\n\n#Second we'll import the csv files we need,to do that:\n#  1.Choose file explorer and make sure that your csv file is in the correct directory\n#  2.Use the read_csv command to import the required csv file\n\n#  or choose variable explorer and click on the import data icon,find the file\n#  that you want to import and click next,choose the data type dataframe and done.\n#  if you import it as array then you should make some arrangement after that \n#  if we import the dataset as array,skiprows=1.\n#  then convert it by using df=pd.DataFrame( nameofvariable(array),columns=['col1','col2'])\n\n#if we import it as dataframe we can directly use it,and in spyder datasets have names\n#you can directly use it like following\n#plt.scatter(jobcsv.experience,jobcsv.income)\n#if it is separated by ; then use sep=';' before go on in read_csv()\n\ndataFrame=pd.read_csv('..\/input\/regression\/job2.csv')\ndataFrame.head()\n\ncar_dataFrame=pd.read_csv('..\/input\/regression\/car.csv')\n\ndecision_dataFrame=pd.read_csv('..\/input\/regression\/car.csv',sep=',')\n\nrandForest_dataFrame=pd.read_csv('..\/input\/regression\/car.csv',sep=',')","758c6a62":"#after getting this csv file visualize this data by using scatter plot\n#let us choose one of the features of age and experience in simple linear regression\nplt.scatter( dataFrame.experience,dataFrame.income )\nplt.xlabel('Experince')\nplt.ylabel('Income')\nplt.show()\n\n#here we saw a normal scatter plot here x is a feature that affects the values of y\n#experience is a feature that affects the income","2e6f54c7":"#now we'll go on with learning how to fit  line into this graph\n#we'll be working with sklearn library,so import it\n\nfrom sklearn.linear_model import LinearRegression\n\n#initialize the linear regression model\nlinear_reg_model=LinearRegression()\n\n#here why don't we use just x=dataFrame.experince ,cause this is a series\n#or why don't we use x=dataFrame.experince.values,let's show the size of it\n\nx=dataFrame.experience.values.shape\nprint(x)\n#see it gives (12,) that means 12 rows and 1 column but it is\n#not preferable by sklearn library so reshape it to (14,1) by using\n#reshape(-1,1) method as below\nx=dataFrame.experience.values.reshape(-1,1)\ny=dataFrame.income.values.reshape(-1,1 )\n#print( type(x)) x is now an array has the shape (14,1)\n\n#fitting the line into our scatter plot\n\nlinear_reg_model.fit( x,y )","8be2fd0b":"#after creating the equation now we find b0 and b1\n#there are two ways of finding the bo,bo is the point where graph intersects\n#the yaxis.So,give 0 to x.First way of finding b0 is,\n\nb0=linear_reg_model.predict([[0]])\nprint('bo: ',b0)\n#we must use 0 in 2D type array,cause we can predict more than one values\n#Second way of finding b0 is using intercept_ method\nb0=linear_reg_model.intercept_ \nprint('b0: ',b0)\n#these two result must be same\n\n#and lets find b1 by using coefficient_ method\nb1=linear_reg_model.coef_\nprint('b1: ',b1)","a83305f4":"#testing manually\n\n#and now we can use our simple AI,we'll show you two ways\n#first manually predict 'How much money does a worker that has 30 years of\n#experience gain ?'\n\nexperience_=30\n\nnew_income=b0+b1*experience_\nprint('new income: ', new_income )\n\n#another way to use it -automatically- is,\nprint( 'new income with predict method:',linear_reg_model.predict([[experience_]]))\n#so they are same as it can be seen","9bdd264a":"y_head=linear_reg_model.predict(x)\nplt.scatter( x , y )\n#if we plot this line in red we see the following output\nplt.plot( x, y_head , color='red')\nplt.show()\n\nprint('r_square score: ',r2_score(y,y_head))\n\n#evaluation of regression models\n\n#residual=y-y_head\n#here y_head is the result of real prediction results\n#square_residual=residual^2\n#so Squared Sum Residual( SSR ) is: sum( (y-y_head)^2 )\n#Assume that we've found y_head_avg\n#so Squared Sum Total is sum( (y-y_head_avg)^2 )\n#r_squared=1-( SSR-SST ) ,the cloesest value of R2 is the better prediction results","94e4c6eb":"#it is time to see that already fitted line,let's plot it\n#but first we must predict all the x values in our graph\n#we'll call that prediction results as y_head\nimport numpy as np\n\n#to see better how the fitted line changes we are scaling our array into a broader array\narray=np.arange(min(x),max(x),0.01) #fix the (1500,)\narray=array.reshape(-1,1)\n\n#y_head includes the prediction results according to our predicted model\ny_head=linear_reg_model.predict(array)\n\n\nplt.scatter( x , y )\n#if we plot this line in red we see the following output\nplt.plot( array , y_head , color='red')\nplt.show()\n\n\n","672a2039":"#Multiple Linear Regression\n#we'll use same data frame to show multiple linear regression\n#and the previously imported libraries are enough to create a multiple linear regression model\n\ndataFrame.head()\n\n#our equation will be like\n#y=b0+b1*x1+b2*x2+...+bn*xn","51853e00":"#all columns and 0. and 2.column as a feature,independent variables\nx=dataFrame.iloc[:,[0,1]].values\n#we'll be separating dataFrame as we're taking all rows and just the 0. and 1. columns\n#why we are taking just 0. and 1. columns? Cause They are the predictors...\nprint(type(x))\nprint(x.shape) #Check the shape,it is usable\n#y is the dependent variable\ny=dataFrame.income.values.reshape(-1,1)","771eed9e":"#initializing our multiple linear regression model\nmultiple_linear_regression_model=LinearRegression()\nmultiple_linear_regression_model.fit(x,y)\n\n#Let's find the intercept and coefficients...\n\nprint( 'b0: ',multiple_linear_regression_model.intercept_ )\nprint( 'b1: ',multiple_linear_regression_model.coef_ )#we'll see two values at there\n","53e32ccb":"#now let's predict that how much many does a man that has\n# 5 years of experience and is 35 years old and another man that\n#has 10 years of experience and 35 years old gain money ?\nmultiple_linear_regression_model.predict([[35,5],[35,10]])\n\n\ny_head=multiple_linear_regression_model.predict(x)\n\n#both age and experience affects the income,plot both of them in scatter mode\nplt.scatter( dataFrame.experience,dataFrame.income,color='violet',label='experience' )\nplt.scatter( dataFrame.age,dataFrame.income,color='green',label='age' )\n\nplt.xlabel('Experince-Age')\nplt.ylabel('Income')\n\n#draw the fitting line\nplt.plot( x , y_head , color='red')\nplt.show()","029d5d25":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs2\/ou.png\")","98a616a9":"#now we'll change the dataset that we've been working on\n\n#our equation will be like\n#y=b0+b1*x+b2*x^2+...+bn*x^n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ncar_dataFrame.head()","f990280f":"#x is independent max_speed and y is dependent price variables don't forget the reshape them\nx=car_dataFrame.max_speed.values.reshape(-1,1)\ny=car_dataFrame.price.values.reshape(-1,1)\nx[:5] # see what's inside x again","d6f6308e":"#First I'll show you the simple linear regression with a different dataset\nlinear_model=LinearRegression()\nlinear_model.fit(x,y)\n\ny_head=linear_model.predict(x)\n#it is enough to draw a simple linear regression line,and again our reai prediction result in y_head","325679fd":"#go on with polynomial regression model\n\n#as the degree increases we get better prediction results in PolynomialFeatures(degree=10)\npolynomial_regression_model=PolynomialFeatures(degree=10)\nx_polynomial=polynomial_regression_model.fit_transform(x)\n\nlinear_model_poly=LinearRegression()\nlinear_model_poly.fit(x_polynomial,y)\n\ny_head_poly=linear_model_poly.predict(x_polynomial)\n#result of the prediction results of x_polynomial is in y_head_poly\ny_head_poly[:5]\n","80bc21d0":"plt.scatter(x,y)\nplt.plot(x,y_head,color='red',label='linear')\nplt.plot(x,y_head_poly,color='green',label='polynomial')\nplt.legend()\nplt.show()\n#so we see polynomial regression algorithm gives us better results","98c2d832":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs3\/dec.png\")","28b9480e":"decision_dataFrame.loc[:5,::]\n#first five rows and all columns in the dataset","2fc2b13f":"x=decision_dataFrame.iloc[:,0].values.reshape(-1,1)\ny=decision_dataFrame.iloc[:,1].values.reshape(-1,1)\n\nprint('x shape: ',x.shape,'\\ny shape: ',y.shape)","3fe5cd2f":"from sklearn.tree import DecisionTreeRegressor\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nprint('The prediction result of 123 km\/h: ',tree_reg.predict([[900]]))\nprint('The prediction result of 128 km\/h: ',tree_reg.predict([[900]]))\n      ","195a4251":"x_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=tree_reg.predict(x_)\n\nprint('x shape: ',x_.shape,'\\ny_head shape: ',y_head.shape)","407e0244":"plt.scatter(x,y,color='red')\nplt.plot(x_,y_head,color='green')\nplt.xlabel('max_speed km\/h')\nplt.ylabel('price')\nplt.show()\n","5e66a627":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/reg-imgs3\/rand.png\")","2024078e":"randForest_dataFrame.head()","1276ee0f":"x=randForest_dataFrame.iloc[:,0].values.reshape(-1,1)\ny=randForest_dataFrame.iloc[:,1].values\n\nprint('x shape: ',x.shape,'\\ny_head shape:',y.shape)","4ac48991":"\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf=RandomForestRegressor(n_estimators=100, random_state=42 )\nrf.fit(x,y)\n\ny_head=rf.predict(x)\n\nprint('385 km\/h price: ',rf.predict([[385]]))\n\nplt.scatter(x,y,color='red' )\nplt.plot(x,y_head,color='green')\nplt.xlabel('max_speed')\nplt.ylabel('price')\nplt.show()\n\n","5711c2b0":"print('r2_score: ',r2_score(y,y_head))","651aa277":"x_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=rf.predict(x_)\n\nplt.scatter(x,y,color='red' )\nplt.plot(x_,y_head,color='green')\nplt.xlabel('max_speed')\nplt.ylabel('price')\nplt.show()\n","79df3fb0":"<h1 id=\"multiple\"> 2.Multiple Linear Regression<\/h1><br>\n<p>A simple linear regression is a function that allows an analyst or statistician to make predictions about one variable based on the information that is known about another variable. Linear regression can only be used when one has two continuous variables\u2014an independent variable and a dependent variable. The independent variable is the parameter that is used to calculate the dependent variable or outcome. A multiple regression model extends to several explanatory variables. <br>\n\nThe multiple regression model is based on the following assumptions: <br>\n\n1.There is a linear relationship between the dependent variables and the independent variables. <br>\n2.The independent variables are not too highly correlated with each other.<br>\n\n**Application** <br>\nFor example, an analyst may want to know how the movement of the market affects the price of Exxon Mobil (XOM). In this case, his linear equation will have the value of the S&P 500 index as the independent variable, or predictor, and the price of XOM as the dependent variable.<br>\n\nIn reality, there are multiple factors that predict the outcome of an event. The price movement of Exxon Mobil, for example, depends on more than just the performance of the overall market. Other predictors such as the price of oil, interest rates, and the price movement of oil futures can affect the price of XOM and stock prices of other oil companies. To understand a relationship in which more than two variables are present, a multiple linear regression is used.<\/b>","afd71c80":"<ul>\n    <li><a href=\"#simplelinear\">1.Simple Linear Regression<\/a><\/li>\n    <li><a href=\"#multiple\">2.Multiple Linear Regression<\/a><\/li>\n    <li><a href=\"#poly\">3.Polynomial Regression<\/a><\/li>\n    <li><a href=\"#dec\">4.Decision Tree Regression<\/a><\/li>\n    <li><a href=\"#rand\">5.Random Forest Regression<\/a><\/li>\n    \n<\/ul>","56fb9bd8":"<h1 id=\"dec\"> 4.**Decision Tree Regression** <\/h1>\n<p>\nA decision tree is a supervised machine learning model used to predict a target by learning decision rules from features. As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions.<\/p>\n\n","4a2e05b2":"**Why fit_transform ?**\n\nTo center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation.\n\nx\u2032=(x\u2212\u03bc)\/\u03c3\n\nYou do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters \u03bc and \u03c3 (values) that you used for centering the training set.\n\nHence, every sklearn's transform's fit() just calculates the parameters (e.g. \u03bc and \u03c3 in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n\nfit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x\u2032. Internally, it just calls first fit() and then transform() on the same data.\n\n**Briefly,**\n\nFirstly, all estimators are trained (or \"fit\") on some training data. That part is fairly straightforward.\n\nSecondly, all of the scikit-learn estimators can be used in a pipeline and the idea with a pipeline is that data flows through the pipeline. Once fit at a particular level in the pipeline, data is passed on to the next stage in the pipeline but obviously the data needs to be changed (transformed) in some way; otherwise, you wouldn't need that stage in the pipeline at all. So, transform is a way of transforming the data to meet the needs of the next stage in the pipeline.\n\nIf you're not using a pipeline, I still think it's helpful to think about these machine learning tools in this way because, even the simplest classifier is still performing a classification function. It takes as input some data and produces an output. This is a pipeline too; just a very simple one.\n\n**In summary, fit performs the training, transform changes the data in the pipeline in order to pass it on to the next stage in the pipeline, and fit_transform does both the fitting and the transforming in one possibly optimized step.**","ee0c50ff":"<h1 id=\"poly\"> 3.Polynomial Regression<\/h1><br>\n<p>\n    In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x       and the dependent variable y is modelled as an nth degree polynomial in x.<br><br>\n    \n    **Why Polynomial Regression?** <br><br>\n    \n    To overcome under-fitting, we need to increase the complexity of the model.<br>\n    \n   **Overfitting** <br> Happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.<br>\n   **Underfitting** <br> Refers to a model that can neither model the training data nor generalize to new data.\n\nAn underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.\n\nUnderfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.<br>\n    \n  We can see that RMSE has decreased and R\u00b2-score has increased as compared to the linear line.<br>\n  \n **The Bias vs Variance trade-off** <br><br>\n \nBias refers to the error due to the model\u2019s simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in under-fitting.\nVariance refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in over-fitting the data.<br>\nThe below picture summarizes our learning.<br>\n    \n    \n<\/p>","a0a854c0":"<h1 id=\"rand\"> 5.Random Forest Regression<\/h1>\nThe Random Forest is one of the most effective machine learning models for predictive analytics, making it an industrial workhorse for machine learning.\n\nBackground\nThe random forest model is a type of additive model that makes predictions by combining decisions from a sequence of base models. More formally we can write this class of models as:\n\ng(x)=f0(x)+f1(x)+f2(x)+...\n \nwhere the final model  g  is the sum of simple base models  fi . Here, each base classifier is a simple decision tree. This broad technique of using multiple models to obtain better predictive performance is called model ensembling. In random forests, all the base models are constructed independently using a different subsample of the data.","446156be":"**REGRESSION ALGORITHMS** <br>\n\nRegression models are used to predict a continuous value. Predicting prices of a house given the features of house like size, price etc. is one of the common examples of Regression. It is a supervised technique. <br>\n\n**SUPERVISED ML ALGORITHMS** <br>\n\nBasically, in this Supervised MLAlgorithms, input data is called training data and has a known label or result such as spam\/not-spam or a stock price at a time.<br>\nIn this, a model is prepared through a training process. Also, this required to make predictions. And is corrected when those predictions are wrong. The training process continues until the model achieves the desired level.<br>\n* Example problems are classification and regression.<br>\n* Example algorithms include logistic regression and back propagation Neural Network.<br>\n\n**UNSUPERVISED ML ALGORITHMS** <br>\n\nIn this Unsupervised Machine Learning, input data is not labeled and does not have a known result.<br>\nWe have to prepare a model by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to reduce redundancy.<br>\n* Example problems are clustering, dimensionality reduction, and association rule learning.<br>\n* Example algorithms include the Apriori algorithm and k-Means.<br>\n\n**SEMI-SUPERVISED LEARNING** <br>\n\nInput data is a mixture of labeled and unlabeled examples.<br>\nThere is a desired prediction problem. But the model must learn the structures to organize the data as well as make predictions.<br>\n* Example problems are classification and regression.<br>\n* Example algorithms are extensions to other flexible methods. That make assumptions about how to model the unlabeled data.<br>\n\n**Machine Learning Algorithms Grouped By Similarity** <br>\nML Algorithms are often grouped by a similarity in terms of their function.\nFor example, tree-based methods, and the neural network inspired methods.\nI think this is the most useful way to group machine learning algorithms and it is the approach we will use here.\nThis is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories. Such as Learning Vector Quantization. That is both a neural network method and an instance-based method. There are also categories that have the same name. That describes the problem and the class of algorithms. Such as Regression and Clustering.\nWe could handle these cases by listing ML algorithms twice. Either by selecting the group that subjectively is the \u201cbest\u201d fit. I like this latter approach of not duplicating algorithms to keep things simple. <br>\n**i. Regression Algorithms**<br>\n\nRegression Algorithms is concerned with modeling the relationship between variables. That we use to refine using a measure of error in the predictions made by the model.\nThese methods are a workhorse of statistics. Also, have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm.","7c4953c1":"<h1 id=\"simplelinear\"> 1.Simple Linear Regression model <\/h1> \n<p>\nSimple linear regression is a statistical method that enables users to summarise and study relationships between two continuous (quantitative) variables. Linear regression is a linear model wherein a model that assumes a linear relationship between the input variables (x) and the single output variable (y). Here the y can be calculated from a linear combination of the input variables (x). When there is a single input variable (x), the method is called a simple linear regression. When there are multiple input variables, the procedure is referred as multiple linear regression.\n\n**Application:** <br>\nsome of the most popular applications of Linear regression algorithm are in financial portfolio prediction, salary forecasting, real estate predictions and in traffic in arriving at ETAs.<\/p>"}}