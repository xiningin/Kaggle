{"cell_type":{"578927b9":"code","bfa9f216":"code","85e7b650":"code","66864c0a":"code","5ff60e51":"code","7bace369":"code","2b146df1":"code","ee55dfa8":"code","f6449b7e":"code","36821702":"code","d87226c7":"code","6fd1f93b":"code","9cc376ed":"code","0b6180c5":"code","e32ad24d":"code","0d8a8616":"code","4b4507c9":"code","f5c84aec":"code","7f3adb72":"code","3168677b":"code","9a3359b1":"code","1342d7d6":"code","57b32466":"code","c7897bde":"code","0e68a4eb":"code","fdaef6ef":"code","9fadbc2b":"code","66416fce":"code","a2f53db7":"code","0e272445":"code","14bf51e1":"code","0be6f50a":"code","72eac987":"code","812f135a":"code","f589a16d":"code","3a8f4d08":"code","5efcf285":"code","2feb15f8":"code","4a230eff":"code","8ed04f04":"code","3fc8d8d9":"code","0291db66":"markdown","2f421565":"markdown","1857f0fc":"markdown","25e2890a":"markdown","17df91c1":"markdown","850cc45a":"markdown","90d868a1":"markdown","5a7d4cf8":"markdown","17e79c68":"markdown","7cd3adf1":"markdown","282f656e":"markdown","1337f042":"markdown","c78bf557":"markdown","76d8e44a":"markdown","c7e461dc":"markdown","687bec89":"markdown","aec42b3b":"markdown","ea8ef536":"markdown","7747b1fd":"markdown","a75ff451":"markdown","a3878a71":"markdown","f7b9d90a":"markdown","35a40c18":"markdown","1225e06d":"markdown","dbef41f0":"markdown","36b4e088":"markdown","4cc197ff":"markdown","f43116a2":"markdown","961afb75":"markdown","8c97e768":"markdown","e8b8411b":"markdown","9a9785f2":"markdown","f5e070a8":"markdown"},"source":{"578927b9":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x); pd.options.display.max_rows = 15\nglobal directory; directory = '..\/input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()\/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()\/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()\/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()\/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}\/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False\/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True \/ False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] \/= counts['tally'].sum()\/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())\/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 5): return data.head(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n        \ndef time_number(date): return hours(date)+minutes(date)\/60+seconds(date)\/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)\/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)\/12+day(date)\/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)\/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']\/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']\/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    if type(column) is str:\n        cond = f'data[\"{column}\"]{condition}'\n    else:\n        cond = f'column{condition}'\n    return data.loc[eval(cond)]\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()","bfa9f216":"files()","85e7b650":"data = read('ks-projects-201801.csv')","66864c0a":"head(data)","5ff60e51":"head(data, 5)","7bace369":"head(  data['id']  )","2b146df1":"head(  data['currency']  )","ee55dfa8":"describe(data)","f6449b7e":"head(data)","36821702":"help(tally)","d87226c7":"head(data, 1)","6fd1f93b":"tally(  data['state']  )","9cc376ed":"tabulate(data['state'], data['goal'], method = 'mean')","0b6180c5":"help(tabulate)","e32ad24d":"goal = tabulate(data['state'], data['goal'], method = 'mean')","0d8a8616":"plot(x = 'state', y = 'mean', data = goal)","4b4507c9":"plot(x = 'state', y = 'mean', data = goal, style = 'barplot')","f5c84aec":"head(data, 1)","7f3adb72":"tally(data['country'])","3168677b":"tabulate(data['country'], data['state'], method = 'count_percent')","9a3359b1":"country = tabulate(data['country'], data['state'], method = 'count_percent')","1342d7d6":"query(data = country, column = 'state', condition = '==\"successful\"')","57b32466":"country_success = query(data = country, column = 'state', condition = '==\"successful\"')","c7897bde":"plot(data = country_success, x = 'country', y = 'Percent%', top = 30, style = 'barplot')","0e68a4eb":"head(data, 1)","fdaef6ef":"tally(data['currency'])","9fadbc2b":"tabulate(data['currency'], data['state'], method = 'count_percent')","66416fce":"currency = tabulate(data['currency'], data['state'], method = 'count_percent')\n\nquery(data = currency, column = 'state', condition = '==\"successful\"')","a2f53db7":"head(   year(data['launched'])  , 10)","0e272445":"launched = tabulate(year(data['launched']), data['main_category'], method = 'count_percent')\n\nhead(launched)","14bf51e1":"year(data['launched'])","0be6f50a":"data = query(data, column = year(data['launched']), condition = '>1970')","72eac987":"plot(data = launched, x = 'main_category', y = 'Percent%', colour = 'launched', style = 'barplot')","812f135a":"plot(data = launched, x = 'main_category', y = 'Percent%', column = 'launched', style = 'lineplot')","f589a16d":"head(    year_month(data['launched'])  )","3a8f4d08":"months = tabulate(year_month(data['launched']), data['state'], method = 'count_percent')\n\nmonths_percent = query(data = months, column = 'state', condition = '==\"successful\"')\n\nhead(months_percent)","5efcf285":"plot(data = months_percent, x = 'launched', y = 'Percent%', style = 'scatterplot')","2feb15f8":"plot(data = months_percent, x = 'launched', y = 'Percent%', style = 'regplot')","4a230eff":"plot(data = months_percent, x = 'launched', y = 'Percent%', style = 'regplot', power = 3)","8ed04f04":"#Your code goes here","3fc8d8d9":"# Your code goes here","0291db66":"You can see we only got the years.\n\nNow, do the same, check how much MAIN_CATEGORY has changed.\n\nWhich was most popular in 2014? 2015?","2f421565":"[<h1>CLICK to SKIP BELOW CODE TO CONTENT<\/h1>](#Content)","1857f0fc":"<a id='Quick'><\/a>\n<h1> 3. Analysis of Time Data <\/h1>\n\nNow, let's analyse **launched** using **YEAR**. This'll convert the dates to keep only YEARS.","25e2890a":"Also it's a bit strange --> why is there 1970?\n\nLet's remove it using **QUERY** and **YEAR**","17df91c1":"**Reading CSV data in**\n\nFirst, we want to know what files are in the database.\n\nRemember to execute commands in a cell, press CTRL + ENTER","850cc45a":"Once you are done, and satisfied with your work, let the tutor mark you.\n\nNote, if you can't get it, it's fine. Marks are awarded for trying the questions out. We don't mind if the output is wrong.\n\n<h2>REMINDER ABOUT ASSIGNMENT 1 AGAIN<\/h2>\n\nNext week night (Friday Week 3), you will receive a Kaggle link to **Assignment 1**. It's worth 7 marks, and super easy :).\n\nThe stuff you learnt in week 1 and today & next week will be enough to complete Assignment 1 entirely.\n\nIt's due in 2 weeks time on Saturday week 5 10pm. We need you to submit a URL \/ LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.","90d868a1":"In Python, getting 1 column is used by indexing [] and [\"...\"]","5a7d4cf8":"(2) Now, we want to know per COUNTRY and per MAIN_CATEGORY, what is the SUCCESS PERCENTAGE.\n\n1. Use TABULATE for country, main_category and state using COUNT_PERCENT method.\n2. QUERY only successful.\n3. BAR PLOT using COLUMN for each country, and show X = main_category, Y = Percent\n\nThe Plot should look like:\n<img src=\"https:\/\/drive.google.com\/uc?id=1BXyt9196iCClmJ-Eh8Dm-Vdnunnly_br\" style=\"width: 500px;\"\/>","17e79c68":"But, we only want the SUCCESSFUL %.\n\nSo, use QUERY, and filter == \"successful\" out.","7cd3adf1":"<h2> Welcome to MARK5826 Week 2!<\/h2>\n\n<h3> Thanks for taking this first time ever offered course! I hope you'll love it! <\/h3>\n\nLecturer In Charge: Junbum Kwon;Teaching Assistant: Daniel Han-Chen & James Lin\n\nIn week 1, we want you to be exposed to the basics of programming, and the basics of Python.\n\n<h2>AIM<\/h2>\n\nThis week (week 2), we are focusing on data analysis.\n\nYou will be using library functions.\n\n<h2>ASSIGNMENT 1 (week 3 to 5) 7 marks<\/h2>\n\nNext week night (Friday Week 3), you will receive a Kaggle link to **Assignment 1**. It's worth 7 marks, and super easy :).\n\nThe stuff you learnt in week 1 and today & next week will be enough to complete Assignment 1 entirely.\n\nIt's due in 2 weeks time on Saturday week 5 10pm. We need you to submit a URL \/ LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.\n\n<h2> Suggestions from Week 1 <\/h2>\n\n1. **|Slides|** We will upload the slides through email \/ moodle before the lectures!\n2. **|Applications|** Week 1 was intro. Week 2+ will be all industry applications.\n3. **|Lab Delivery|**  Speaking will be slower now (and content has been cut back a lot. We will explain more --> more quality over quantity).\n4. **|Too fast|** We will go slowly on the hard parts of the class. In fact, we will go through how to do Lab Questions as a class.\n5. **|Heavy Content|** Sorry about the overwhelming content! I'm sure from week 2 onwards, the code you'll see should look familiar.\n6. **|Slow Computers|** Some people's computers are slow. We have decided to optimise the code below (removing superfluous code)\n7. **|Heavy Content|** Lab Organisation has been improved, with one streamlined marking system.\n8. **|Python Documentation|** We will now have a Google Doc style documentation system. It is colloborative, however, we will endeavour to fill it in ourselves.\n9. **|Lab Questions Weavement During Class|** At the start of the lab, we will directly read the Lab Q first. Then, you can see how the Lab Q can be answered.\n\n<h2>Week Topics<\/h2>\n\n(You can click the links below)\n\n[SKIP BELOW CODE TO CONTENT](#Content)\n<hr>\n1.[Reading Data](#Content)\n\n2.[Kickstarter Data](#Kickstarter)\n\n3.[Analysis of Time Data](#Quick)\n\n4.[Processing and Using Dates](#Year)\n\n5.[Lab Questions](#Lab)\n","282f656e":"<a id='Lab'><\/a>\n<h1> 5. Lab Questions <\/h1>\n\n<img src=\"https:\/\/previews.123rf.com\/images\/christianchan\/christianchan1503\/christianchan150300425\/37144675--now-it-s-your-turn-note-pinned-on-cork-.jpg\" style=\"width: 300px;\"\/>","1337f042":"I don't see any errors, so directly lets use TABULATE.","c78bf557":"Choose to plot ALL countries, so I placed top = 30. You can do top = 10000 ur choice, but the X Axis will be longer IF the data has 10000 rows","76d8e44a":"Now, let's check the actual currency which might affect success rates. I expect it to have a similar trend with COUNTRY vs STATE\n\nFirst, inspect CURRENCY using TALLY again","c7e461dc":"First, let us see how the GOAL relates to PERCENTAGE SUCCESS.\n\nUsing TABULATE, we can summarise a dataset's information into a table. The goal is to find the AVERAGE GOAL per STATE.\n\nLet us check what TABULATE does after the code below usin HELP","687bec89":"We can see a clearer trend if we draw a CUBIC graph (degree 3)","aec42b3b":"Clearly, % Success have been decreasing over the years!\n\nLet's draw a REGRESSION LINE to forecast what might happen! Use regplot in STYLE","ea8ef536":"Let's check the country vs success rate.\n\nCountry is \"country\". Let us first inspect the column for data issues using the tally command","7747b1fd":"<a id='Content'><\/a>\n<h1> 1. Reading Data <\/h1>","a75ff451":"Now, we use TABULATE again to summarise the country and success rate.\n\nWe want to know the success rate per country, so we use the PERCENT mode in TABULATE","a3878a71":"In Python, comments can be placed inside code (text that is not executed). You need to place the # symbol.","f7b9d90a":"Clearly, US wins, with over 35% success rate. Next is Great Britain 35% ish. Worst is IT, JP (Japan and Italy?) at around 15-17%","35a40c18":"You can also instead of plotting multiple lines, split it into multiple plots using COLUMN instead of COLOUR","1225e06d":"Use the COLOUR option to plot multiple lines instead of 1","dbef41f0":"Let us see a description or summary of the data\n\n(It's a bit slow since theres a lot of calculations to be done - so give it some time)\n\nDescribe gives you information about the:\n\n1. Data Type (float? 1.002, - decimals), (int? integer 1,2,3), (object? string)\n2. Missing% (how much of the data is NAN values - missing)\n3. Nunique (how many unique values are in the data - [1,1,1,2,2,3] has 3 unique values == [1,2,3]\n4. Top3 (the top 3 values which are the most occuring)\n5. Min (minimum value in the data)\n6. Mean (average value in the data)\n7. Median (the middle value in the data)\n8. Max (the maximum value in the data)\n9. Sample (an example of what the data looks like)","36b4e088":"Finally, let's check what happens to the SUCCESS RATE over time. This time, let's not just use YEARS, but MONTH + YEARS.\n\nUse the year_month command.\n\nSo, 2012 July (7th month) will be == 2012+7\/12.\n\n<a id='Year'><\/a>\n<h1> 4. Processing and Using Dates <\/h1>","4cc197ff":"Likewise, choose only STATE == \"successful\"","f43116a2":"We will be using the second file.\n\nRead it in using read(...)","961afb75":"**Showing what's inside the data**\n\nUse the head(..., N) command to show the first N rows","8c97e768":"(1) In this exercise, you need to:\n\n1. Figure out WHY success rate has been decreasing. Our hypothesis is that theres MORE projects, hence the decrease.\n2. Your job is to verfiy this.\n4. Use TABULATE to get the COUNT of the YEAR_MONTH to see how many projects are launched in that month\n5. PLOT a REGPLOT and compare this to the SUCCESS per year_month.\n\n6. Explain to the tutor what's your insight.\n\nIt should look like this:\n<img src=\"https:\/\/drive.google.com\/uc?id=1eDVCGTcGJ8JN3aUqixDSrk8WP7CilSse\" style=\"width: 300px;\"\/>","e8b8411b":"<a id='Kickstarter'><\/a>\n<h1> 2. Kickstarter Data <\/h1>\n\n**Aim for Kickstarter data**\n\nOur aim for the Kickstarter data is to see how the STATE (failed, successful) is correlated with the other variables.\n\nThis means, first we need to check the STATE column.\n\nNote - all datasets in real life are very corrupt. In fact, theres a statistic which says that over 80% of a data scientists' job is just data cleaning and data manipulation.\n\nFirst, let just inpsect the STATE column.\n\nSince it's over 300,000 rows, it's infeasible to check all rows.\n\nLet's use the TALLY command. This counts how many occurences of each unique value is in the data.\n\nIf you want more information on a function, type HELP","9a9785f2":"But WHY??? Is the number of projects correlated?\n\nSo over the years, would more projects = overall low success rate?","f5e070a8":"Clearly, a SUCCESSFUL campaign has a very very low GOAL.\n\nThis essentially means more expensive projects are more likely to fail.\n\nYou can see SUSPENDED has a very high GOAL. This is also not good, and is indicative of failure."}}