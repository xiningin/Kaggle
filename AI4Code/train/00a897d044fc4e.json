{"cell_type":{"04e94fbd":"code","95856f35":"code","b0fc8488":"code","2a3fd447":"code","1a396dba":"code","7ebd4d1d":"code","b20dc386":"code","1e232797":"code","78fdf0c1":"code","99d2301f":"code","8ec983d8":"code","d4bf2d40":"code","f58879b4":"code","73e81025":"code","aa2740a4":"code","f89e3b7f":"code","33d86222":"code","026c9b2d":"code","042c513c":"code","c95a783d":"code","6841173e":"code","cd4de7af":"code","139346ce":"code","21b341a0":"code","8723ace6":"code","c183f3ee":"code","9a5da910":"code","3c670aa8":"code","a6ef2cec":"markdown","26ec91fe":"markdown","a0ed2f91":"markdown","7e8e8a85":"markdown","eec44a03":"markdown","4c30a3f8":"markdown","a47b6acc":"markdown","61cf07eb":"markdown","a327fefe":"markdown","c599425d":"markdown","61d1c0bc":"markdown","4c42644c":"markdown","181d9e65":"markdown","7782e3ba":"markdown"},"source":{"04e94fbd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import BayesianRidge,LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, OrthogonalMatchingPursuit\nfrom sklearn.svm import SVR, NuSVR, LinearSVR\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.neighbors import KNeighborsRegressor, KernelDensity, KDTree\nfrom sklearn.metrics import *\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython import display, utils\n\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 300)\npd.set_option('max_colwidth', 400)\n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","95856f35":"!pip install quandl","b0fc8488":"import quandl\nimport warnings\nimport itertools\nimport numpy as np\nimport pandas as pd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import PowerTransformer\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels import tsa\nfrom scipy import stats\n\n\n#from arima_utils import ad_fuller_test, plot_rolling_stats\n#from arima_utils import plot_acf_pacf, arima_gridsearch_cv","2a3fd447":"import quandl\ngold_df = quandl.get(\"WGC\/GOLD_DAILY_USD\", authtoken=\"ao5ZsdzsxHBykZGZ6tZ5\")","1a396dba":"data = gold_df.reset_index()\ndata.head(10)","7ebd4d1d":"plt.style.use('seaborn')\ngold_df.Value.plot(figsize=(15, 6), color= 'darkcyan')\nplt.show()","b20dc386":"from pylab import rcParams\nrcParams['figure.figsize'] = 17,15\nrcParams['lines.color'] = 'teal'\n\nseries = gold_df.Value.values\nresult = seasonal_decompose(series, model='additive', period=120)\nsns.set()\n\nplt.style.use('bmh')\nresult.plot()\n\nplt.show()","1e232797":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(9, 6))\nsns.distplot(data.Value , bins=50, kde=True, hist=True, fit=norm, color = 'darkcyan');\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data.Value)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('passengers distribution')\n\n#Get also the QQ-plot\nfig = plt.figure(figsize=(9, 6))\nres = stats.probplot(data.Value, plot=plt)\nplt.show()","78fdf0c1":"plt.style.use('fivethirtyeight')\n\n\npt = PowerTransformer(method='box-cox', standardize=False)\nptTargetbc = pt.fit_transform(data.Value.values.reshape(-1, 1))\nptTargetbc = pd.DataFrame(ptTargetbc)\n\npt2 = PowerTransformer(method='yeo-johnson', standardize=True)\nptTargetyc = pt2.fit_transform(data.Value.values.reshape(-1, 1))\nptTargetyc= pd.DataFrame(ptTargetyc)\n\nplt.figure(1, figsize=(8, 4)); plt.title('Box-Cox')\n#ptTargetbc.hist(bins=100, color='cyan')\nsns.distplot(ptTargetbc, kde=False, bins=30, color = 'darkcyan')\nplt.figure(2, figsize=(8, 4))\nres = stats.probplot(ptTargetbc.values.ravel(), plot=plt)\nplt.show()\nplt.figure(3, figsize=(8, 4)); plt.title('yeo-johnson')\n#ptTargetyc.hist(bins=100)\nsns.distplot(ptTargetyc, kde=False, bins=30, color='darkgreen')\nplt.figure(4, figsize=(8, 4))\nres = stats.probplot(ptTargetyc.values.ravel(), plot=plt)\nplt.show()","99d2301f":"sns.set()\nplt.style.use('seaborn')\n#plt.figure(figsize=(12, 8))\n\npd.plotting.lag_plot(data['Value'])","8ec983d8":"sns.set()\nplt.style.use('seaborn')\n#plt.figure(figsize=(12, 8))\n\npd.plotting.lag_plot(data['Value'], lag=2)","d4bf2d40":"plt.style.use('seaborn-poster')\npd.plotting.autocorrelation_plot(data.Value) ","f58879b4":"plt.figure(figsize=(20, 12))\ndata.Value.plot(color='darkorange', lw = 3)\ndata.Value.rolling(120).mean().plot(color='k', lw=2)","73e81025":"import quandl\ndata = quandl.get(\"WGC\/GOLD_DAILY_USD\", authtoken=\"ao5ZsdzsxHBykZGZ6tZ5\")\ndata","aa2740a4":"data.head(20)","f89e3b7f":"data.tail(20)","33d86222":"upsampled = data.resample('D').mean()\nupsampled.head(10)","026c9b2d":"data.shape","042c513c":"upsampled.shape","c95a783d":"lin_interpolated = upsampled.interpolate(method='linear')\nprint(lin_interpolated.head(32))\nplt.style.use('fivethirtyeight')\n\nlin_interpolated.plot(color='teal')\nplt.show()","6841173e":"pol_interpolated = upsampled.interpolate(method='polynomial', order=5)\nprint(pol_interpolated.head(32))\nplt.style.use('seaborn-poster')\n\npol_interpolated.plot(color='darkred')\nplt.show()","cd4de7af":"pol_interpolated = upsampled.interpolate(method='spline', order=5)\nprint(pol_interpolated.head(32))\nplt.style.use('seaborn-poster')\n\npol_interpolated.plot()\nplt.show()","139346ce":"series","21b341a0":"import itertools\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","8723ace6":"from matplotlib.pyplot import figure\n\ndef plot_rolling_stats(ts):\n        figure(num=None, figsize=(18, 7), dpi=80, linewidth=5)\n        rolling_mean = ts.rolling(window=24,center=False).mean()\n        rolling_std = ts.rolling(window=24,center=False).std()\n\n        #Plot rolling statistics:\n        orig = plt.plot(ts, color='c',label='Original')\n        mean = plt.plot(rolling_mean, color='red', label='Rolling Mean')\n        std = plt.plot(rolling_std, color='black', label = 'Rolling Std')\n        \n        plt.legend(loc='best')\n        plt.title('Rolling Mean & Standard Deviation')\n        plt.show(block=False)","c183f3ee":"def ad_fuller_test(ts):\n    dftest = adfuller(ts, autolag='AIC')\n      \n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n        print(dfoutput)\n","9a5da910":"def auto_arima(param_max=1,series=pd.Series(),verbose=True):\n    # Define the p, d and q parameters to take any value \n    # between 0 and param_max\n    p = d = q = range(0, param_max+1)\n    print('p=', p)\n    print('d=', d)\n    print('q=', q)\n    # Generate all different combinations of seasonal p, d and q triplets\n    pdq = [(x[0], x[1], x[2]) for x in list(itertools.product(p, d, q))]\n    \n    model_resuls = []\n    best_model = {}\n    min_aic = 10000000\n    for param in pdq:\n        try:\n            mod = sm.tsa.ARIMA(series, order=param)\n\n            results = mod.fit()\n            \n            if verbose:\n                print('ARIMA{}- AIC:{}'.format(param, results.aic))\n            model_resuls.append({'aic':results.aic,\n                                 'params':param,\n                                 'model_obj':results})\n            if min_aic>results.aic:\n                best_model={'aic':results.aic,\n                            'params':param,\n                            'model_obj':results}\n                min_aic = results.aic\n        except Exception as ex:\n            print(ex)\n    if verbose:\n        print(\"Best Model params:{} AIC:{}\".format(best_model['params'],\n              best_model['aic']))  \n        \n    return best_model, model_resuls\n\n\ndef arima_gridsearch_cv(series, cv_splits=2,verbose=True,show_plots=True):\n    # prepare train-test split object\n    tscv = TimeSeriesSplit(n_splits=cv_splits)\n    \n    # initialize variables\n    splits = []\n    best_models = []\n    all_models = []\n    i = 1\n    \n    # loop through each CV split\n    for train_index, test_index in tscv.split(series):\n        print(\"*\"*20)\n        print(\"Iteration {} of {}\".format(i,cv_splits))\n        i = i + 1\n        \n        # print train and test indices\n        if verbose:\n            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        splits.append({'train':train_index,'test':test_index})\n        \n        # split train and test sets\n        train_series = series.iloc[train_index]\n        test_series = series.iloc[test_index]\n        \n        print(\"Train shape:{}, Test shape:{}\".format(train_series.shape,\n              test_series.shape))\n        \n        # perform auto arima\n        _best_model, _all_models = auto_arima(series=train_series)\n        best_models.append(_best_model)\n        all_models.append(_all_models)\n        \n        # display summary for best fitting model\n        if verbose:\n            print(_best_model['model_obj'].summary())\n        results = _best_model['model_obj']\n       # plt.figure(figsize=(15, 9))\n        if show_plots:\n            # show residual plots\n            residuals = pd.DataFrame(results.resid)\n            #plt.figure(figsize=(15, 9))\n            residuals.plot(figsize=(14, 6))\n            plt.title('Residual Plot')\n            plt.show()\n            #plt.figure(figsize=(15, 9))\n            residuals.plot(kind='kde', figsize=(14, 6))\n            plt.title('KDE Plot')\n            plt.show()\n            print(residuals.describe())\n        \n            # show forecast plot\n            fig, ax = plt.subplots(figsize=(18, 4))\n            fig.autofmt_xdate()\n            ax = train_series.plot(ax=ax)\n            test_series.plot(ax=ax)\n            fig = results.plot_predict(test_series.index.min(), \n                                       test_series.index.max(), \n                                       dynamic=True,ax=ax,\n                                       plot_insample=False)\n            plt.title('Forecast Plot ')\n            plt.legend()\n            plt.show()\n            \n           # train_series = train_series.reindex(pd.date_range(train_series.index.min(), \n            #                      train_series.index.max(), \n            #                      freq='D')).fillna(method='ffill')\n            # show error plot\n           # insample_fit = list(results.predict(train_series.index.min()+1, \n                                         #       train_series.index.max(),freq='D')) \n            \n           # plt.plot((np.exp(train_series.iloc[1:].tolist())-\\\n           #                  np.exp(insample_fit)))\n            #plt.title('Error Plot')\n            plt.show()\n    return {'cv_split_index':splits,\n            'all_models':all_models,\n            'best_models':best_models}\n    ","3c670aa8":"if __name__ == '__main__':\n    \n    import quandl\n    gold_df = quandl.get(\"WGC\/GOLD_DAILY_USD\", authtoken=\"ao5ZsdzsxHBykZGZ6tZ5\")\n    \n    new_df = gold_df.reindex(pd.date_range(gold_df.index.min(), \n                                  gold_df.index.max(), \n                                  freq='D')).fillna(method='ffill')\n    print(new_df.shape)\n    gold_df.plot(figsize=(15, 6))\n    plt.show()\n    \n    # log series\n    log_series = np.log(new_df.Value)\n    \n    ad_fuller_test(log_series)\n    plot_rolling_stats(log_series)\n    \n    # Using log series with a shift to make it stationary\n    log_series_shift = log_series - log_series.shift()\n    log_series_shift = log_series_shift[~np.isnan(log_series_shift)]\n    \n    ad_fuller_test(log_series_shift)\n    plot_rolling_stats(log_series_shift)\n    \n    # determining p and q\n   # plot_acf_pacf(log_series_shift)\n    \n    \n    new_df['log_series'] = log_series\n    new_df['log_series_shift'] = log_series_shift\n    print(new_df.head())\n    # cross validate \n    results_dict = arima_gridsearch_cv(new_df.log_series,cv_splits=5)","a6ef2cec":">One the key assumptions behind the ARIMA models we will be\ndiscussing next. Stationarity refers to the property where for a time series its mean,\nvariance, and autocorrelation are time invariant. In other words, mean, variance,\nand autocorrelation do not change with time.\n\n>Statistical tests that help us understand if a given series is stationary\nor not. The **Augmented Dickey Fuller test** begins with a null hypothesis of series being\nnon-stationary, ","26ec91fe":"### upsampling","a0ed2f91":"The ARIMA model is a logical progression and combination of the two models. Yet if we combine AR\nand MA with a differenced series, what we get is called as ARIMA(p,d,q) model.\nwhere,\n\n    \u2022 p is the order of Autoregression\n\n    \u2022 q is the order of Moving average\n\n    \u2022 d is the order of differencing\n\nThus, for a stationary time series ARIMA models combine autoregressive and moving average concepts\nto model the behavior of a long running time series and helps in forecasting. Let\u2019s now apply these concepts\nto model gold price forecasting.","7e8e8a85":"# **Box-Jenkins method with ARIMA**","eec44a03":"## **Time Series Analysis**","4c30a3f8":"### Rolling Plot","a47b6acc":">- Auto Regressive or AR Modeling: A simple linear regression model where current\nobservation is regressed upon one or more prior observations. the dependency on prior values is denoted by p or the order of AR model.\n>- Moving Average or MA Modeling: Is again essentially a linear regression model that\nmodels the impact of noise\/error from prior observations to current one. \n","61cf07eb":">## Interpolation\n>### You can use interpolate function to fill those NaN rows created above after resampling using different methods like :\n>\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.\n\n>\u2018time\u2019: Works on daily and higher resolution data to interpolate given length of interval.\n\n>\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n>\u2018pad\u2019: Fill in NaNs using existing values.\n\n>\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019, \u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the numerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5).\n\n>\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019: Wrappers around the SciPy interpolation methods of similar names. See Notes.\n\n>\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which replaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n>#### https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.interpolate.html","a327fefe":"#### *\u2018polynomial\u2019 and \u2018spline\u2019 require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5)*","c599425d":"### Effects of power transform on time series","61d1c0bc":">If the test statistic of AD Fuller test is less than the critical value(s), we reject the null hypothesis of nonstationarity. The AD Fuller test is available as part of the statsmodel library. Since it is quite evident that our original series of gold prices is non-stationary, we will perform a log transformation and see if we are able to obtain stationarity. ","4c42644c":">### *The AR and MA models were known long before Box-Jenkin\u2019s methodology was presented. Yet this methodology presented a classic approach to identify and apply these models for forecasting.*","181d9e65":">The Box-Jenkin\u2019s methodology consists of a wide range of statistical models which are widely used to model\ntime series for forecasting. For this section, we will be concentrating on one such model called as ARIMA.\nARIMA stands for Auto Regressive Integrated Moving Average model. Let\u2019s look at the basics and constituents \nof this model and then build on our understanding to forecast gold prices.","7782e3ba":">In this case, we generated forecast for time periods for which we already had data. This helps us in\nvisualizing and understanding how the model is performing. This is also called as **back testing**. Out of sample\nforecasting is also supported by statsmodels through its forecast() method. Also, the plots\nshowcases values in the transformed scale, i.e. log scale. Inverse transformation can be easily applied to get\ndata back in original form.\nYou should also note that commodity prices are impacted by a whole lot of other factors like global\ndemand, economic conditions like recession and so on. Hence, what we showcased here was in certain ways\na na\u00efve modeling of a complex process. We would need more features and attributes to have sophisticated\nforecasts."}}