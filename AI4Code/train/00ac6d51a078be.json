{"cell_type":{"81ee443c":"code","da2f99cc":"code","bcaabcae":"code","08cd0df7":"code","b1fbdf2c":"code","68232837":"code","1a7422d5":"code","4b4fa020":"code","163af956":"code","48d45699":"code","3f8785bb":"code","27e17d5c":"code","2a927f82":"code","2327c5a8":"code","9cfb2ee1":"code","92e3cb5f":"markdown","76a6dd3e":"markdown","4aa4a243":"markdown","a37f5875":"markdown","96782ad4":"markdown","da1914d8":"markdown","abc9f59f":"markdown","720f53b9":"markdown","91fc0db0":"markdown","8bb4e9fa":"markdown","07b85997":"markdown","aa726fd2":"markdown","cec11ff0":"markdown","6306b5bc":"markdown","03a77ae8":"markdown","369acf66":"markdown","0ea53de9":"markdown","6cbbf4e5":"markdown","17a47346":"markdown","a11bc1a7":"markdown","6bd6526d":"markdown","868a451a":"markdown"},"source":{"81ee443c":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#nltk\nimport nltk\n\n#stop-words\nfrom nltk.corpus import stopwords\nstop_words=set(nltk.corpus.stopwords.words('english'))\n\n# tokenizing\nfrom nltk import word_tokenize,sent_tokenize\n\n#keras\nimport keras\nfrom keras.preprocessing.text import one_hot,Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Flatten ,Embedding,Input\nfrom keras.models import Model","da2f99cc":"sample_text_1=\"bitty bought a bit of butter\"\nsample_text_2=\"but the bit of butter was a bit bitter\"\nsample_text_3=\"so she bought some better butter to make the bitter butter better\"\n\ncorp=[sample_text_1,sample_text_2,sample_text_3]\nno_docs=len(corp)\n","bcaabcae":"vocab_size=50 \nencod_corp=[]\nfor i,doc in enumerate(corp):\n    encod_corp.append(one_hot(doc,50))\n    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))","08cd0df7":"# length of maximum document. will be nedded whenever create embeddings for the words\nmaxlen=-1\nfor doc in corp:\n    tokens=nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen=len(tokens)\nprint(\"The maximum number of words in any document is : \",maxlen)","b1fbdf2c":"# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\npad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\nprint(\"No of padded documents: \",len(pad_corp))","68232837":"for i,doc in enumerate(pad_corp):\n     print(\"The padded encoding for document\",i+1,\" is : \",doc)","1a7422d5":"# specifying the input shape\ninput=Input(shape=(no_docs,maxlen),dtype='float64')","4b4fa020":"'''\nshape of input. \neach document has 12 element or words which is the value of our maxlen variable.\n\n'''\nword_input=Input(shape=(maxlen,),dtype='float64')  \n\n# creating the embedding\nword_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n\nword_vec=Flatten()(word_embedding) # flatten\nembed_model =Model([word_input],word_vec) # combining all into a Keras model","163af956":"embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) \n# compiling the model. parameters can be tuned as always.","48d45699":"print(type(word_embedding))\nprint(word_embedding)","3f8785bb":"print(embed_model.summary()) # summary of the model","27e17d5c":"embeddings=embed_model.predict(pad_corp) # finally getting the embeddings.","2a927f82":"print(\"Shape of embeddings : \",embeddings.shape)\nprint(embeddings)","2327c5a8":"embeddings=embeddings.reshape(-1,maxlen,8)\nprint(\"Shape of embeddings : \",embeddings.shape) \nprint(embeddings)","9cfb2ee1":"for i,doc in enumerate(embeddings):\n    for j,word in enumerate(doc):\n        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)","92e3cb5f":"After this all the unique words will be reprsented by an integer. For this we are using **one_hot** function from the Keras. Note that the **vocab_size**  is specified large enough so as to ensure **unique integer encoding**  for each and every word.\n\n**Note one important thing that the integer encoding for the word remains same in different docs. eg 'butter' is  denoted by 31 in each and every document.**","76a6dd3e":"**PARAMETERS OF THE EMBEDDING LAYER --- **\n\n**'input_dim' = the vocab size that we will choose**. \nIn other words it is the number of unique words in the vocab.\n\n**'output_dim'  = the number of dimensions we wish to embed into**. Each word will be represented by a vector of this much dimensions.\n\n**'input_length' = lenght of the maximum document**. which is stored in maxlen variable in our case.","4aa4a243":"#### ACTUALLY CREATING THE EMBEDDINGS using KERAS EMBEDDING LAYER","a37f5875":"## [Don't forget to upvote ;) ]","96782ad4":"## THE END !!!","da1914d8":"In this kernel I have explained the keras embedding layer. To do so I have created a sample corpus of just 3 documents and that should be sufficient to explain the working of the keras embedding layer.\n","abc9f59f":"Now all the documents are of same length (after padding). And so now we are ready to create and use the embeddings.\n\n**I will embed the words into vectors of 8 dimensions.**","720f53b9":"#### PADDING THE DOCS (to make very doc of same length)","91fc0db0":"## [ Please Do upvote the kernel;) ]","8bb4e9fa":"#### Now this makes it easier to visualize that we have 3(size of corp) documents with each consisting of 12(maxlen) words and each word mapped to a 8-dimensional vector.","07b85997":"## A Detailed Guide to understand the Word Embeddings and Embedding Layer in Keras.","aa726fd2":"**If you want to see the application of Keras embedding layer on a real task eg text classification then please check out my [this](https:\/\/github.com\/mrc03\/IMDB-Movie-Review-Sentiment-Analysis) repo on Github in which I have used the embeddings to perform sentiment analysis on IMdb movie review dataset.**","cec11ff0":"Embeddings are useful in a variety of machine learning applications. Because of the fact I have attached many data sources to the kernel where I fell that embeddings and Keras embedding layer may prove to be useful.","6306b5bc":"#### INTEGER ENCODING ALL THE DOCUMENTS","03a77ae8":"#### CREATING SAMPLE CORPUS OF DOCUMENTS ie TEXTS","369acf66":"**The Keras Embedding layer requires all individual documents to be of same length.**  Hence we wil pad the shorter documents with 0 for now. Therefore now in Keras Embedding layer the **'input_length'**  will be equal to the length  (ie no of words) of the document with maximum length or maximum number of words.\n\nTo pad the shorter documents I am using **pad_sequences** functon from the Keras library.","0ea53de9":"#### GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC DOCUMENT","6cbbf4e5":"Just like above we can now use any other document. We can sent_tokenize the doc into sentences.\n\nEach sentence has a list of words which we will integer encode using the 'one_hot' function as below. \n\nNow each sentence will be having different number of words. So we will need to pad the sequences to the sentence with maximum words.\n\n**At this point we are ready to feed the input to Keras Embedding layer as shown above.**\n\n**'input_dim' = the vocab size that we will choose**\n\n**'output_dim'  = the number of dimensions we wish to embed into**\n\n**'input_length' = lenght of the maximum document**","17a47346":"#### IMPORTING MODULES","a11bc1a7":"Before diving in let us skim through some of the applilcations of the embeddings : \n\n**1 ) The first application that strikes me is in the Collaborative Filtering based Recommender Systems where we have to create the user embeddings and the movie embeddings by decomposing the utility matrix which contains the user-item ratings.**\n\nTo see a complete tutorial on CF based recommender systems using embeddings in Keras you can follow **[this](https:\/\/www.kaggle.com\/rajmehra03\/cf-based-recsys-by-low-rank-matrix-factorization)** kernel of mine.\n\n\n**2 ) The second use is in the Natural Language Processing and its related applications whre we have to create the word embeddings for all the words present in the documents of our corpus.**\n\nThis is the terminology that I shall use in this kernel.\n\n\n**Thus the embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space.**","6bd6526d":"#### HOW TO WORK WITH A REAL PIECE OF TEXT","868a451a":"The resulting shape is (3,12,8).\n\n**3---> no of documents**\n\n**12---> each document is made of 12 words which was our maximum length of any document.**\n\n**& 8---> each word is 8 dimensional.**\n\n "}}