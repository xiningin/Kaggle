{"cell_type":{"1790f186":"code","7f8a10cb":"code","8c6708e0":"code","65985b36":"code","ed41ef5f":"code","eaa1deb6":"code","0452992a":"code","c9b07f08":"code","a8513b6c":"code","2ae8c72b":"code","8b223d91":"code","a96aedd5":"code","fb02f35d":"code","f48008c3":"code","08ad62bd":"code","ddf4950f":"code","f54f08bb":"code","6959c576":"code","d6f06be3":"code","721674c1":"code","fc8e75af":"code","e9d611a4":"code","9bfec03f":"code","4dfd695a":"code","acfc67ec":"code","5790a44e":"code","83707654":"code","d720e19e":"code","25c6ba9b":"code","c9cf193c":"code","b7d30daa":"code","d62cd9a2":"code","aa78ffe3":"code","3fe3a050":"code","a5554b46":"code","c44cc90a":"code","2457788a":"code","94da96e9":"code","5c3f5ba0":"code","aa33979a":"markdown","3002919b":"markdown","ec2e553a":"markdown","0aded4fe":"markdown","224f0bf9":"markdown","8a350414":"markdown","421ed091":"markdown","72ba3a74":"markdown","6bb24b2d":"markdown","110f5648":"markdown","2d6b983e":"markdown","d13e831a":"markdown","ca561269":"markdown","b3dbb0d7":"markdown","980583b8":"markdown","f32c12ff":"markdown","efcef4e2":"markdown","c989a602":"markdown"},"source":{"1790f186":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f8a10cb":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.utils as utils\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Lambda\nfrom keras import backend as K\nfrom keras import regularizers\nfrom tensorflow.keras import regularizers, layers\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nimport seaborn as sns\n","8c6708e0":"train = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")","65985b36":"train.head()","ed41ef5f":"train.columns","eaa1deb6":"train.describe()","0452992a":"train.dtypes","c9b07f08":"test.head()","a8513b6c":"test.describe()","2ae8c72b":"test.columns","8b223d91":"test.dtypes","a96aedd5":"train.isnull().sum()","fb02f35d":"test.isnull().sum()\n","f48008c3":"train = train.drop(['id','pubchem_id'],axis=1)\nprint(train.shape)\ntest = test.drop(['id','pubchem_id'],axis=1)\nprint(test.shape)","08ad62bd":"eat = train['Eat']\nsns.displot(eat, kde=True, color=\"g\")\nplt.xlabel('Energy')\nplt.ylabel('% of Distributions')\nplt.title('Distribution of Energy');","ddf4950f":"from sklearn.model_selection import train_test_split\nX = train.drop(['Eat'], axis = 1)\nY = train['Eat']\ny=Y.values\nx=X.values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)","f54f08bb":"model = Sequential()\nmodel.add(Dense(1000, input_dim=x.shape[1],kernel_initializer='normal', activation='sigmoid'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(500,kernel_initializer='normal', activation='sigmoid'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(50,kernel_initializer='normal'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(50,kernel_initializer='normal'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nparam_grid = dict(gamma=[1,3,5], C=[1,3,5])\ncv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\ngrid = GridSearchCV(SVC(cache_size=1024), param_grid=param_grid, cv=cv, n_jobs=14, verbose=10)\n\n# # defining parameter range \n# param_grid = {'C': [0.1, 1, 10, 100],  \n#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n#               'gamma':['scale', 'auto'],\n#               'kernel': ['linear']}  \n   \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1) \n   \n# # fitting the model for grid search \n# grid.fit(x_train, y_train, verbose=1,epochs=1)\n\nhist1 = model.fit(x_train, y_train,validation_data=(x_test,y_test), verbose=1,epochs=10)","6959c576":"model.summary()","d6f06be3":"m1 = model.evaluate(x_test,y_test)\nprint(m1)","721674c1":"plt.plot(hist1.history['loss'])\nplt.plot(hist1.history['val_loss'])\nplt.title('Model 1 Performance')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\nplt.show()","fc8e75af":"model2 = Sequential()\nkeras.layers.Dense(256, input_dim=x.shape[1], activation='relu')\nkeras.layers.Dense(128, activation='relu')\nkeras.layers.Dense(64, activation='relu')\nkeras.layers.Dense(1)\nmodel2.add(Dense(1000, input_dim=x.shape[1],kernel_initializer='normal', activation='sigmoid'))\nmodel2.add(Dropout(0.15))\nmodel2.add(Dense(500,kernel_initializer='normal', activation='sigmoid'))\nmodel2.add(Dropout(0.15))\nmodel2.add(Dense(50,kernel_initializer='normal'))\nmodel2.add(Dropout(0.15))\nmodel2.add(Dense(50,kernel_initializer='normal'))\nmodel2.add(Dropout(0.15))\nmodel2.add(Dense(1))\nmodel2.compile(loss='mean_squared_error', optimizer='adam')\n\nparam_grid2 = dict(gamma=[1,3,5], C=[1,3,5])\ncv2 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\ngrid2 = GridSearchCV(SVC(cache_size=1024), param_grid=param_grid, cv=cv, n_jobs=14, verbose=10)\n\nhist2 = model2.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)","e9d611a4":"model2.summary()","9bfec03f":"m2 = model2.evaluate(x_test,y_test)\nprint(m2)","4dfd695a":"plt.plot(hist2.history['loss'])\nplt.plot(hist2.history['val_loss'])\nplt.title('Model 2 Performance')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\nplt.show()","acfc67ec":"model3 = tf.keras.Sequential()\nmodel3.add(Dense(1000, input_dim=x.shape[1],kernel_initializer='normal', activation='sigmoid'))\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dropout(0.15))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dropout(0.15))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dropout(0.15))\nkeras.layers.Dense(1)\n\nparam_grid3 = dict(gamma=[1,3,5], C=[1,3,5])\ncv3 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\ngrid3 = GridSearchCV(SVC(cache_size=1024), param_grid=param_grid, cv=cv, n_jobs=14, verbose=10)\n\nmodel3.compile(loss='mean_squared_error', optimizer='adam')\nhist3 = model3.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)","5790a44e":"model3.summary()","83707654":"m3 = model3.evaluate(x_test,y_test)\nprint(m3)","d720e19e":"plt.plot(hist3.history['loss'])\nplt.plot(hist3.history['val_loss'])\nplt.title('Model 3 Performance')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\nplt.show()","25c6ba9b":"model4 = Sequential()\nmodel4.add(Dense(1000, input_dim=x.shape[1],kernel_initializer='normal', activation='sigmoid'))\nmodel4.add(Dropout(0.25))\nmodel4.add(Dense(500,kernel_initializer='normal', activation='sigmoid'))\nmodel4.add(Dropout(0.25))\nmodel4.add(Dense(50,kernel_initializer='normal'))\nmodel4.add(Dropout(0.25))\nmodel4.add(Dense(1))\nmodel4.compile(loss='mean_squared_error', optimizer='adam')\n\nparam_grid4 = dict(gamma=[1,3,5], C=[1,3,5])\ncv4 = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\ngrid4 = GridSearchCV(SVC(cache_size=1024), param_grid=param_grid, cv=cv, n_jobs=14, verbose=10)\n\nhist4 = model4.fit(x_train, y_train,validation_data=(x_test,y_test), verbose=1,epochs=10)","c9cf193c":"model4.summary()","b7d30daa":"m4 = model4.evaluate(x_test,y_test)\nprint(m4)","d62cd9a2":"plt.plot(hist4.history['loss'])\nplt.plot(hist4.history['val_loss'])\nplt.title('Model 4 Performance')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'])\nplt.show()","aa78ffe3":"print(f'Model 1 Score: {m1}')\nprint(f'Model 2 Score: {m2}')\nprint(f'Model 3 Score: {m3}')\nprint(f'Model 4 Score: {m4}')","3fe3a050":"fig, axs = plt.subplots(2, 2)\n\naxs[0, 0].plot(hist1.history['loss'])\naxs[0, 0].plot(hist1.history['val_loss'])\naxs[0, 0].set_title('Model 1 Performance')\n\naxs[0, 1].plot(hist2.history['loss'])\naxs[0, 1].plot(hist2.history['val_loss'])\naxs[0, 1].set_title('Model 2 Performance')\n\naxs[1, 0].plot(hist3.history['loss'])\naxs[1, 0].plot(hist3.history['val_loss'])\naxs[1, 0].set_title('Model 3 Performance')\n\naxs[1, 1].plot(hist4.history['loss'])\naxs[1, 1].plot(hist4.history['val_loss'])\naxs[1, 1].set_title('Model 4 Performance')\n\nfor ax in axs.flat:\n    ax.set(xlabel='x-label', ylabel='y-label')\n\nfor ax in axs.flat:\n    ax.label_outer()","a5554b46":"bestIndex = min(m1,m2,m3,m4)\nbestModel = model\n\nif(bestIndex == m4):\n    print('Best Model was Model 4')\n    bestModel = model4\nelif (bestIndex == m3):\n    print('Best Model was Model 3')\n    bestModel = model3\nelif (bestIndex == m2):\n    print('Best Model was Model 2')\n    bestModel = model2\nelse:\n    print('Best Model was Model 1')\n    bestModel = model","c44cc90a":"submission = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/sample_submission.csv')\nprint(len(submission))","2457788a":"predict = model.predict(test)\nprint(len(predict))\nsubmission['Eat'] = predict","94da96e9":"submission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","5c3f5ba0":"print(submission.to_string(index = False))","aa33979a":"Imports:","3002919b":"I am going off of which produced the best and smallest score. \n\nIt seemed to be either the first or fourth model I created.\n\nThe reason for these two doing better is because they had an ideal amount of\nlayers for both Dense and Dropout.\n\nI think my model 2 and 3 were not performing well because I had the Neural Network condense down and then re-expand which was causing problems.\n\nI made this if-else statement above to help me just automate which model was the best. \n\n","ec2e553a":"I always want to make sure there are no missing values, this time there seems to be all the data needed.","0aded4fe":"Here is where I split the data","224f0bf9":"# Here is where I save to 'submission.csv'.","8a350414":"# Model 3","421ed091":"# Model 1","72ba3a74":"# Ryan Kendrick","6bb24b2d":"# Model 4","110f5648":"Here is a visualization of the distribution of the target value for the energy.","2d6b983e":"# Here is where I select the best model and use it on target vector","d13e831a":"There is just one more column in the train dataframe and that is for the 'Eat' variable.","ca561269":"# Here is where I print the content of the CSV file.","b3dbb0d7":"# Model 2 ","980583b8":"# Here is where I conduct an exploratory data analysis","f32c12ff":"I will need to see what type of data is in the dataframe. It looks like they are all numbers, mostly float values.","efcef4e2":"# Here is where I load the data","c989a602":"First I will just observe what are in these CSV files."}}