{"cell_type":{"e8a19e42":"code","e798c4f1":"code","8e066458":"code","1ffaf900":"code","73b910cc":"code","4de485f1":"code","85807c6c":"code","712f6141":"code","1f644d97":"code","64f1b479":"code","e432335a":"code","a28bdd18":"code","3dd12a01":"code","c4ecc237":"code","be9011d9":"code","b398f361":"code","624e3893":"code","470f9ce4":"code","f72b85a6":"code","f8c3b171":"code","12cfb7b1":"code","a00c136f":"code","2d7240de":"code","91dfb454":"code","444f81b7":"code","71bd2ae6":"code","a567463c":"code","d931da17":"code","d0f610a2":"code","bd5300c0":"code","fa0335c6":"code","541c5bfe":"code","3133a0b3":"code","78da786f":"code","8f8090af":"code","d0c51b1b":"code","5c4d4026":"code","e76327a7":"code","92bb5c2b":"code","1ab86cb1":"code","0a2db0fd":"code","c80adc29":"code","3d786fc5":"code","64ef7ad0":"code","c307dac6":"code","9fb04568":"code","d1e4416c":"code","67d1077c":"code","a0e48936":"code","cc41088f":"code","7b1b5c9f":"code","8009d840":"code","efa19ce8":"code","3c39718e":"code","cb831150":"code","85d03f42":"code","21e87f73":"code","af37656b":"code","a8a63c3f":"code","59035caa":"code","69b6858b":"code","605e6aef":"code","ecee1e04":"code","1c566e01":"code","56704844":"code","d96619eb":"code","57b7da11":"code","9141c2d7":"code","d4d9a465":"code","1650200d":"code","51ac9cd3":"code","670a6f14":"code","9b3d01b9":"code","057b179e":"code","91b33257":"code","4b9cae2c":"code","517462b6":"markdown","b5f1ea84":"markdown","7c5f5783":"markdown","7528115b":"markdown","85b7eb08":"markdown","8ceb4ebe":"markdown","a36ceba1":"markdown","35ca1e2e":"markdown","59244a48":"markdown","0b0c9dc7":"markdown","b36d3396":"markdown","e4bc9e42":"markdown","1356b0a2":"markdown","e8ebbc11":"markdown","fc4c53a9":"markdown","d93cb6f1":"markdown","380462de":"markdown","12ef3823":"markdown","155834da":"markdown","151a538b":"markdown","ff9e964d":"markdown","a50557d1":"markdown","071c3f1c":"markdown","4e589eb8":"markdown","4a38f7f9":"markdown","ad3914b5":"markdown","a7997fe5":"markdown","1f361047":"markdown","6b3d62a9":"markdown","e9099e7e":"markdown","a6789224":"markdown","96c2951f":"markdown","b0491922":"markdown","3be2e8db":"markdown","965973a4":"markdown","c53cd3bd":"markdown","d9abb55a":"markdown","ad5e53e3":"markdown","e06ded4c":"markdown","f1615628":"markdown","441e2fea":"markdown","6b44127d":"markdown"},"source":{"e8a19e42":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport statsmodels.api as sm \nimport warnings\nwarnings.filterwarnings('ignore')","e798c4f1":"df = pd.read_csv('..\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv')\ndf.head()","8e066458":"df.shape","1ffaf900":"df.info()","73b910cc":"df.describe().T","4de485f1":"df.floor.unique()","85807c6c":"df[\"floor\"] = df[\"floor\"].apply(str.strip).replace(\"-\", np.nan)\ndf[\"floor\"] = pd.to_numeric(df[\"floor\"], downcast=\"float\")","712f6141":"df.info()","1f644d97":"df.isnull().sum()*100\/df.shape[0]","64f1b479":"df[\"floor\"].fillna(np.mean(df.floor), inplace=True)","e432335a":"df.isnull().sum()*100\/df.shape[0]","a28bdd18":"plt.figure(figsize=(8,5))\nplt.boxplot(df['total (R$)'])\nplt.show()","3dd12a01":"from scipy.stats.mstats import winsorize\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import normalize\n\ntitle_font = {'family': 'arial', 'color': 'darkred','weight': 'bold','size': 13 }\ncurve_font  = {'family': 'arial', 'color': 'darkblue','weight': 'bold','size': 10 }\n\nplt.figure(figsize=(38,28))\n\ncolumns=[ 'area', 'rooms', 'bathroom', 'parking spaces', 'floor',\n         'hoa (R$)', 'rent amount (R$)',\n       'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfor i in range(10):\n    plt.subplot(5, 10, i+1)\n    plt.hist(df[columns[i]])\n    plt.title(columns[i]+str(\"\/Orjinal\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+11)\n    plt.hist(winsorize(df[columns[i]], (0, 0.10)))\n    plt.title(columns[i]+str(\"\/Winsorize\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+21)\n    plt.hist(np.log(df[columns[i]]+1))\n    plt.title(columns[i]+str(\"\/Log\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+31)\n    plt.hist(scale(df[columns[i]]))\n    plt.title(columns[i]+str(\"\/Scale\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+41)\n    plt.hist(normalize(np.array(df[columns[i]]).reshape(1,-1).reshape(-1,1)))\n    plt.title(columns[i]+str(\"\/Normalize\")  , fontdict=title_font)","c4ecc237":"plt.figure(figsize=(38,28))\n\ncolumns=[ 'area', 'rooms', 'bathroom', 'parking spaces', 'floor',\n         'hoa (R$)', 'rent amount (R$)',\n       'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfor i in range(10):\n    plt.subplot(5, 10, i+1)\n    plt.boxplot(df[columns[i]])\n    plt.title(columns[i]+str(\"\/Orjinal\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+11)\n    plt.boxplot(winsorize(df[columns[i]], (0, 0.03))) # %95\n    plt.title(columns[i]+str(\"\/Winsorize\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+21)\n    plt.boxplot(np.log(df[columns[i]]+1))\n    plt.title(columns[i]+str(\"\/Log\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+31)\n    plt.boxplot(scale(df[columns[i]]))\n    plt.title(columns[i]+str(\"\/Scale\")  , fontdict=title_font)\nfor i in range(10):\n    plt.subplot(5, 10, i+41)\n    plt.boxplot(normalize(np.array(df[columns[i]]).reshape(1,-1).reshape(-1,1)))\n    plt.title(columns[i]+str(\"\/Normalize\")  , fontdict=title_font)","be9011d9":"log_threshold_variables= pd.DataFrame()\nvariables = [ 'area', 'rooms', 'bathroom', 'parking spaces', 'floor',\n         'hoa (R$)', 'rent amount (R$)',\n       'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\nfor j in variables:\n    for threshold_worth in np.arange(1,5,1):\n        \n        #logarithm Transformed\n        q75_log, q25_log = np.percentile(np.log(df[j]), [75 ,25])\n        caa_log = q75_log - q25_log\n        \n        #Orjinal Data\n        q75, q25 = np.percentile(df[j], [75 ,25])\n        caa= q75 - q25\n        \n        # Winsorize Data\n        q75_win, q25_win = np.percentile(winsorize(df[j],(0, 0.03)), [75 ,25])\n        caa_win= q75 - q25\n        \n        #logarithm Transformed\n        min_worth_log = q25_log - (caa_log*threshold_worth)\n        max_worth_log = q75_log + (caa_log*threshold_worth)\n        \n        #Orjinal Data\n        min_worth= q25 - (caa*threshold_worth) \n        max_worth = q75 + (caa*threshold_worth) \n        \n        # Winsorize Data\n        min_worth_win= q25_win - (caa_win*threshold_worth) \n        max_worth_win = q75_win + (caa_win*threshold_worth)\n        \n        number_of_outliers_log = len((np.where((np.log(df[j]) > max_worth_log)| \n                                               (np.log(df[j]) < min_worth_log))[0]))\n        \n        number_of_outliers = len((np.where((df[j] > max_worth)| \n                                               (df[j] < min_worth))[0]))\n        \n        number_of_outliers_win = len((np.where((winsorize(df[j],(0, 0.03)) > max_worth_win)| \n                                               (winsorize(df[j],(0, 0.03)) < min_worth_win))[0]))\n        \n        log_threshold_variables = log_threshold_variables.append({'threshold_worth': threshold_worth,\n                                                            'number_of_outliers' : number_of_outliers, \n                                                            'number_of_outliers_log': number_of_outliers_log,\n                                                            \"number_of_outliers_win\":number_of_outliers_win\n                                                            }, ignore_index=True)\n    print(\"-\"*10,\"\",j,\"-\"*10)\n    display(log_threshold_variables)\n    log_threshold_variables = pd.DataFrame()","b398f361":"plt.boxplot(df['total (R$)'], whis=4) # Not log transformed whis = 4 \nplt.show()","624e3893":"plt.boxplot(np.log(df['total (R$)']), whis=4) # log Transformed whis = 4 \nplt.show()","470f9ce4":"df[\"city\"]= df[\"city\"].replace({\"S\u00e3o Paulo\":0, 'Porto Alegre':1, 'Rio de Janeiro':2, 'Campinas':3,'Belo Horizonte':4})\ndf['animal']= pd.get_dummies(df['animal'],drop_first=True)\ndf['furniture']= pd.get_dummies(df['furniture'],drop_first=True)","f72b85a6":"df_log= df[['area','rooms',\"bathroom\",\"parking spaces\",\"floor\",\"hoa (R$)\",\"rent amount (R$)\",\"property tax (R$)\",\"fire insurance (R$)\",\"total (R$)\"]]\n\ndf_add=df[['city',\"animal\",\"furniture\"]] # Not Transform Log because we apply get_dummies() these columns\n\ndf_log = np.log(df_log+1) # We don't want taking -inf values in dataframe. \n\n\ndf_log=pd.concat([df_log,df_add],axis=1)\ndf_log.head()","f8c3b171":"# IQR* 4 Log Transform\n\nq1 = df_log['total (R$)'].quantile(0.25)\nq3 = df_log['total (R$)'].quantile(0.75)\niqr = q3-q1 #Interquartile range\nlow  = q1-1.5*iqr #acceptable range\nhigh = q3+4*iqr #acceptable range\nlow,high","12cfb7b1":"# IQR* 4 Log Transform Boxplot\n\ndf_log['total (R$)']=np.where(df_log['total (R$)'] > high,high,df_log['total (R$)']) # upper limit\n\nplt.boxplot(df_log['total (R$)'])\nplt.show()","a00c136f":"df_corr=df_log.corr()\ndf_corr","2d7240de":"plt.figure(figsize=(18,10))\nax=sns.heatmap(df_corr, square=True, annot=True, linewidths=.5, vmin=0, vmax=1, cmap='viridis')\nax.set_ylim(13,0)\nplt.title(\"Correlation Matrix\", fontdict=title_font)\n\nplt.show()","91dfb454":"df_log=df_log.drop([\"rent amount (R$)\",\"property tax (R$)\",\"fire insurance (R$)\",\"hoa (R$)\"], axis=1)\ndf_log.head()","444f81b7":"df_win = df.copy()","71bd2ae6":"df_win['total (R$)'] = winsorize(df['total (R$)'], (0, 0.03))","a567463c":"df_win=df_win.drop([\"rent amount (R$)\",\"property tax (R$)\",\"fire insurance (R$)\",\"hoa (R$)\"], axis=1)\ndf_win.head()","d931da17":"df_win_1 = df.copy()","d0f610a2":"df_win_1['total (R$)'] = winsorize(df['total (R$)'], (0, 0.03))\ndf_win_1=df_win_1.drop([\"rent amount (R$)\",\"property tax (R$)\",\"fire insurance (R$)\",\"hoa (R$)\"], axis=1)","bd5300c0":"df_win_1.head()","fa0335c6":"# IQR* 1 Log Transform\nq1 = df_win_1['total (R$)'].quantile(0.25)\nq3 = df_win_1['total (R$)'].quantile(0.75)\niqr = q3-q1 #Interquartile range\nlow  = q1-1.5*iqr #acceptable range\nhigh = q3+1*iqr #acceptable range\nlow,high","541c5bfe":"df_win_1['total (R$)']=np.where(df_win_1['total (R$)'] > high,high,df_win_1['total (R$)']) # upper limit","3133a0b3":"df_win_2 = df.copy()","78da786f":"df_win_2['total (R$)'] = winsorize(df['total (R$)'], (0, 0.03))\ndf_win_2=df_win_2.drop([\"rent amount (R$)\",\"property tax (R$)\",\"fire insurance (R$)\",\"hoa (R$)\"], axis=1)","8f8090af":"df_win_2.head()","d0c51b1b":"# IQR* 1 Log Transform\nq1 = df_win_2['total (R$)'].quantile(0.25)\nq3 = df_win_2['total (R$)'].quantile(0.75)\niqr = q3-q1 #Interquartile range\nlow  = q1-1.5*iqr #acceptable range\nhigh = q3+2*iqr #acceptable range\nlow,high","5c4d4026":"df_win_2['total (R$)']=np.where(df_win_2['total (R$)'] > high,high,df_win_2['total (R$)']) # upper limit","e76327a7":"df_log.to_csv('Log_Brazil')\ndf_win.to_csv('Winsorize_Brazil')","92bb5c2b":"from sklearn.metrics import mean_squared_error ,r2_score,explained_variance_score,max_error\nfrom sklearn.model_selection import train_test_split, cross_val_score ,cross_val_predict,GridSearchCV, cross_validate\nfrom statsmodels.tools.eval_measures import mse, rmse\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm ","1ab86cb1":"def create_model(X,y,model,tip):\n    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.20, random_state=111)\n    model.fit(X_train, y_train)\n    \n    prediction_train=model.predict(X_train)\n    prediction_test=model.predict(X_test)\n    \n    cv = cross_validate(estimator=model,X=X,y=y,cv=10,return_train_score=True)\n    \n    d = pd.Series({'mean_squared_error_train':mean_squared_error(y_train,prediction_train),\n                   'mean_squared_error_test':mean_squared_error(y_test,prediction_test),\n                   'RMSE Train':np.sqrt(mean_squared_error(y_train,prediction_train)),\n                   'RMSE Test':np.sqrt(mean_squared_error(y_test,prediction_test)),\n                   'r2_score_train':r2_score(y_train,prediction_train),\n                   'r2_score_test':r2_score(y_test,prediction_test),\n                   'explained_variance_score_train':explained_variance_score(y_train,prediction_train),\n                   'explained_variance_score_test':explained_variance_score(y_test,prediction_test),\n                   'max_error_train':max_error(y_train,prediction_train),\n                   'max_error_test':max_error(y_test,prediction_test),\n                   \"Cross_val_train\":cv['train_score'].mean(),\n                   \"Cross_val_test\":cv['test_score'].mean()\n                  },name=tip)\n    return d","0a2db0fd":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test =  train_test_split(X_scl, y, test_size=0.20, random_state=111) \n      \nlm = LinearRegression()\nlm.fit(X_train, y_train)","c80adc29":"metrics=pd.DataFrame()\nmetrics=metrics.append(create_model(X_scl,y,lm,tip='Log_IQR*4'))\nmetrics","3d786fc5":"X = df_win.drop([\"total (R$)\"], axis=1)\ny = df_win['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nlm = LinearRegression()\n\nmetrics=metrics.append(create_model(X_scl,y,lm,tip='Winsorize_IQR*0'))\nmetrics","64ef7ad0":"X = df_win_1.drop([\"total (R$)\"], axis=1)\ny = df_win_1['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nlm = LinearRegression()\n\nmetrics=metrics.append(create_model(X_scl,y,lm,tip='Winsorize_IQR*1'))\nmetrics","c307dac6":"X = df_win_2.drop([\"total (R$)\"], axis=1)\ny = df_win_2['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nlm = LinearRegression()\n\nmetrics=metrics.append(create_model(X_scl,y,lm,tip='Winsorize_IQR*2'))\nmetrics","9fb04568":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nlm = LinearRegression()\n\nmodels= pd.DataFrame()\nmodels=models.append(create_model(X_scl,y,lm,tip='Linear_Model'))\nmodels","d1e4416c":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nKnn=KNeighborsRegressor(n_neighbors=5)\n\nmodels=models.append(create_model(X_scl,y,Knn,tip='Knn_model'))\nmodels","67d1077c":"Knn=KNeighborsRegressor()\nk_range = list(range(1,25))\nparameter = dict(n_neighbors=k_range)\ngrid = GridSearchCV(Knn, parameter, cv=10, scoring='r2')\nGrds = grid.fit(X,y)\nprint('The best parameters:', Grds.best_estimator_)\nprint('The best score:', Grds.best_score_)","a0e48936":"Knn = KNeighborsRegressor(n_neighbors=23)\n\nmodels=models.append(create_model(X_scl,y,Knn,tip='Knn_model_Tuning'))\nmodels","cc41088f":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\ncart_model = DecisionTreeRegressor()\n\nmodels=models.append(create_model(X_scl,y,cart_model,tip='cart_model'))\nmodels","7b1b5c9f":"cart_params= {'min_samples_split':range(2,20), \n             \"max_leaf_nodes\":range(2,10),\n             \"max_features\":range(0,5)}\n\ncart_cv_model = GridSearchCV(cart_model, cart_params, cv=10)\n\ncart_cv_model.fit(X_train,y_train)\nprint(\"The best Parameters\"+str(cart_cv_model.best_params_))","8009d840":"cart_model = DecisionTreeRegressor(max_features=4 , max_leaf_nodes=9 , min_samples_split=6 )\n\nmodels=models.append(create_model(X_scl,y,cart_model,tip='cart_model_tuning'))\nmodels","efa19ce8":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nrandom_model = RandomForestRegressor(n_estimators=25, random_state=2)\n\nmodels=models.append(create_model(X_scl,y,random_model,tip='random_model'))\nmodels","3c39718e":"rf_params = {'max_depth': [2,3,5,8,10],\n            \"max_features\":[1,2,3,4],\n            \"min_samples_split\":[2,5,40]}\n\nrf_cv_model= GridSearchCV(random_model, rf_params , cv=10)\n\nrf_cv_model.fit(X_train,y_train)\nprint(\"The best paramters :\"+str(rf_cv_model.best_params_))","cb831150":"random_model = RandomForestRegressor(n_estimators=25, random_state=2,max_depth=10, max_features=4 ,min_samples_split=2)\n\nmodels=models.append(create_model(X_scl,y,random_model,tip='random_model_tuning'))\nmodels","85d03f42":"plt.figure(figsize=(20,10))\nimportance_level = pd.Series(data=random_model.feature_importances_,\n                        index= X.columns)\n\nimportance_level_sorted = importance_level.sort_values()\n\nimportance_level_sorted.plot(kind='barh', color='darkblue')\nplt.title('Importance Level of the Features')\nplt.show()","21e87f73":"from sklearn.svm import SVR\n\nX = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nsvm_model = SVR()\n\nmodels=models.append(create_model(X_scl,y,svm_model,tip='svm_model'))\nmodels","af37656b":"svr_params = {\"C\": np.arange(0.1, 2, 0.1)}\n\nsvr_cv_model = GridSearchCV(svm_model,svr_params, cv=10 ).fit(X_train,y_train)\n\nprint(\"The Best Parameters :\"+str(svr_cv_model.best_params_))","a8a63c3f":"svm_model = SVR(C=1.9000000000000001)\n\nmodels=models.append(create_model(X_scl,y,svm_model,tip='svm_model_Tuning'))\nmodels","59035caa":"from sklearn.ensemble import BaggingRegressor\n\nX = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler=StandardScaler()\nX_scl=scaler.fit_transform(X)\n\nbag_model = BaggingRegressor(bootstrap_features=True)\n\nmodels=models.append(create_model(X_scl,y,bag_model,tip='bag_model'))\nmodels","69b6858b":"bag_params = {\"n_estimators\": range(2,20)}\n\nbag_cv_model = GridSearchCV(bag_model,bag_params, cv=10 ).fit(X_train,y_train)\n\nbag_cv_model.best_params_","605e6aef":"bag_model = BaggingRegressor(n_estimators=19, random_state=45)\n\nmodels=models.append(create_model(X_scl,y,bag_model,tip='bag_model_tuning'))\nmodels","ecee1e04":"from xgboost import XGBRegressor\n\nX = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler= StandardScaler()\nX_scl= scaler.fit_transform(X)\n\nxgb = XGBRegressor(base_score=0.5, verbose=False)\n\nmodels=models.append(create_model(X_scl,y,xgb,tip='xgb_model'))\nmodels","1c566e01":"xgb_params = {'colsample_bytree':[0.4,0.5,0.6, 0.9, 1],\n             \"n_estimators\":[100,200,500,1000],\n             \"max_depth\":[2,3,4,5,6],\n             \"learning_rate\":[0.1,0.01,0.5]}\n\nxgb_cv_model = GridSearchCV(xgb,xgb_params, cv=10, n_jobs= -1 , verbose=False )\n\nxgb_cv_model.fit(X_train,y_train)","56704844":"print(\"The Best Parameters :\"+str(xgb_cv_model.best_params_))","d96619eb":"xgb = XGBRegressor(base_score=0.5,colsample_bytree=0.5,learning_rate=0.01\n                   ,max_depth=6,n_estimators=1000,verbose=False )\n\nmodels=models.append(create_model(X_scl,y,xgb,tip='xgb_model_tuning'))\nmodels","57b7da11":"from lightgbm import LGBMRegressor\n\nX = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler= StandardScaler()\nX_scl= scaler.fit_transform(X)\n\nlgbm = LGBMRegressor()\n\nmodels=models.append(create_model(X_scl,y,lgbm,tip='lgbm_model'))\nmodels","9141c2d7":"lgbm_params = {'colsample_bytree':[0.4,0.5,0.6, 0.9, 1],\n             \"n_estimators\":[100,200,500,1000],\n             \"max_depth\":[2,3,4,5,6],\n             \"learning_rate\":[0.1,0.01,0.5]}\n\nlgbm_cv_model = GridSearchCV(lgbm,lgbm_params, cv=10, n_jobs= -1 , verbose=False )\n\nlgbm_cv_model.fit(X_train,y_train)\nprint(\"The Best Parameters :\"+str(lgbm_cv_model.best_params_))","d4d9a465":"lgbm = LGBMRegressor(colsample_bytree=0.6,learning_rate=0.01\n                   ,max_depth=6,n_estimators=1000 )\n\nmodels=models.append(create_model(X_scl,y,lgbm,tip='lgbm_model_tuning'))\nmodels","1650200d":"from catboost import CatBoostRegressor\n\nX = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler= StandardScaler()\nX_scl= scaler.fit_transform(X)\n\ncatboost_model = CatBoostRegressor(verbose=False)\n\nmodels=models.append(create_model(X_scl,y,catboost_model,tip='catboost_model'))\nmodels","51ac9cd3":"catb_params = {\n    \"iterations\":[200,500,1000],\n    \"learning_rate\":[0.01,0.03,0.05,0.1],\n    \"depth\":[3,4,5,6,7,8]}\n\ncat_cv_model = GridSearchCV(catboost_model,catb_params, cv=10, n_jobs= -1 ,verbose=False )\n\ncat_cv_model.fit(X_train,y_train)\nprint(\"The Best Parameters :\"+str(cat_cv_model.best_params_))","670a6f14":"catboost_model = CatBoostRegressor(depth=7 , iterations=500, learning_rate= 0.05,verbose=False)\n\nmodels=models.append(create_model(X_scl,y,catboost_model,tip='catboost_model_tuning'))\nmodels","9b3d01b9":"X = df_log.drop([\"total (R$)\"], axis=1)\ny = df_log['total (R$)']\n\nscaler= StandardScaler()\nX_scl= scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test =  train_test_split(X_scl, y, test_size=0.20, random_state=111)\n\ncatboost_model = CatBoostRegressor(depth=7 , iterations=500, learning_rate= 0.05,verbose=False).fit(X,y)\n\nprint(catboost_model.get_scale_and_bias())","057b179e":"catboost_model.get_feature_importance(data=None,prettified=False,thread_count=-1,verbose=False,)","91b33257":"print(\"Total = '8.243122100830078 + area * 26.03681985 + rooms * 5.60436687 + bathroom * 13.64171516 + parking spaces * 7.6913577 + floor * 18.73900675 +\" \n      ,\"city *17.16975768 + animal * 1.34374804 + furniture * 9.77322794  \")","4b9cae2c":"plt.figure(figsize=(20,10))\nimportance_level = pd.Series(data=catboost_model.feature_importances_,\n                        index= X.columns)\n\nimportance_level_sorted = importance_level.sort_values()\n\nimportance_level_sorted.plot(kind='barh', color='darkblue')\nplt.title('Importance Level of the Features')\nplt.show()","517462b6":"# Log Transform Data","b5f1ea84":"# Correlation","7c5f5783":"# Model Tuning","7528115b":"# Bagging Trees","85b7eb08":"# IQR Method","8ceb4ebe":"## Goal \n\nWe will examine the effects of the variables 'furniture', 'animal', 'floor', 'parking spaces', 'bathroom', 'rooms', 'area' and 'city' on our target variable 'total'. Our aim is to predict our target variable correctly by guessing if our dependent variables are not provided. In summary; To be able to estimate house prices in Brazil according to the criteria of dependent variables.","a36ceba1":"# Histogram","35ca1e2e":"# Winsorize Data and threshold = 1","59244a48":"Yes. It seems that our best model is our logarithm transform and IQR * 4 dataset. Cross Validate and RMSE Test values are the best ones It seems,. Then we will do the modeling process with this dataframe.","0b0c9dc7":"# BoxPlot","b36d3396":"# Missing Value","e4bc9e42":"# Model Tuning","1356b0a2":"# Model Tuning","e8ebbc11":"# CART MODEL","fc4c53a9":"### As you can see, our model that best describes and predicts our data was the Cat Boost Algorithm.","d93cb6f1":"## Columns\n- id\n- city: City where the property is located\n- area:Area do imovel \/ Property area\n- rooms:Numero de quartos\/ Quantity of rooms\n- bathroom:Numero de banheiros \/ Quantity of bathroom\n- parking spaces:Numero de vagas \/ Quantity of parking spaces\n- floor:Andar \/ Floor\n- animal:Aceita animais? \/ Acept animals?\n- furniture: Mobilhada? \/ Furniture?\n- hoa: Valor do condominio \/ Homeowners association tax\n- rent amount: Valor do Aluguel \/ Rent amount\n- property tax: IPTU \/ Property tax\n- fire insurance: Seguro Incendio \/ Fire Insurance\n- total: Valor total \/ Total","380462de":"# Brazilian Houses To Rent","12ef3823":"# Model Tuning","155834da":"# KNN Model","151a538b":"# Machine Learning","ff9e964d":"Here, we can focus on our target variable 'total'. the target variable has less contradictory value when converted to logarithm however, it should be noted that winsorize is clear of all outliers for us in the 3 * IQR.In these situations, we can make a wrong decision these between dataframe. Therefore, we have to study both.","a50557d1":"# Outlier Values","071c3f1c":"# Random Forest","4e589eb8":"# Model Tuning","4a38f7f9":"# CatBoost","ad3914b5":"# Result CatBoost","a7997fe5":"As we understand from Boxplot and Histogram graphics, the best ways to get rid of outliers are winsorized and transform logarithms. Now, if you transform or winsorize our data into a logarithm, how much will we be outliers? let's find it","1f361047":"# Be Carefull\n\n- Before moving on to modeling, we need to leave hoa (R) + \ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61 (\ud835\udc45) + real estate tax (R) + \ud835\udc53\ud835\udc56\ud835\udc5f\ud835\udc52\ud835\udc56\ud835\udc5b\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 (\ud835\udc45) columns in our data. Because when we add these variables, we reach our target variable. The variables I mentioned statistically are the variables that determine our target variable. If these variables were given to us, we would collect and reach our target variable directly. Therefore, if we are not guessing one of these variables, we must delete them and focus on other variables that affect our Target variable.\n\n- We drop these variables from our dataframe before the model because it can negatively affect the metrics of our model.","6b3d62a9":"# Winsorize And threshold = 2","e9099e7e":"# Encoder Transform","a6789224":"# Winsorize Data and threshold = 0","96c2951f":"# XGBoost","b0491922":"# Light GBM","3be2e8db":"# SVM Model","965973a4":"# Log Transform IQR * 4","c53cd3bd":"# Winsorize And threshold = 0","d9abb55a":"# Linear Model","ad5e53e3":"# Winsorize Data and threshold = 2","e06ded4c":"# Model Tuning","f1615628":"# Model Tuning","441e2fea":"# Model Tuning","6b44127d":"# Winsorize And threshold = 1"}}