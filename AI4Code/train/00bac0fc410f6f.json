{"cell_type":{"43ff2f97":"code","288b888d":"code","6ea4c04c":"code","904a0123":"code","af67dfb6":"code","88bcc733":"code","b40ff2ef":"code","b1d45bcb":"code","80edad5d":"code","a1014fe1":"code","71fa7bb1":"code","0c90e8ed":"code","997405e2":"code","37d5ccd9":"code","7600b44a":"markdown","c8b7b82d":"markdown","eef28219":"markdown","a68f73d0":"markdown","ead32f87":"markdown","75c59692":"markdown","1e4cbbc1":"markdown","12013d3a":"markdown","b9eca527":"markdown"},"source":{"43ff2f97":"%%time\n!pip install ..\/input\/kerasapplications\/keras-team-keras-applications-3b180cb -f .\/ --no-index\n!pip install ..\/input\/efficientnet\/efficientnet-1.1.0\/ -f .\/ --no-index\n!conda install -c conda-forge -y gdcm","288b888d":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport random\nimport json\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom skimage.segmentation import clear_border\nfrom skimage.measure import label, regionprops\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\nimport pydicom\nprint('tensorflow version:', tf.__version__)","6ea4c04c":"def seed_all(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_all(2020)\nDATA_PATH = '..\/input\/osic-pulmonary-fibrosis-progression'\nBATCH_SIZE = 10\nFEATURES = True\nN_FOLDS = 4\nADV_FEATURES = False\nC_SIGMA, C_DELTA = tf.constant(70, dtype='float32'), tf.constant(1000, dtype='float32')\nQS = [.05, .50, .95]\nIMG_SIZE = 224\nRESIZE = 224\nSEQ_LEN = 12\nCUTOFF = 2\nLAMBDA = .8\nMDL_VERSION = 'v5'\nMODELS_PATH = '.'\nMASKED = False\nDECAY = False","904a0123":"train = pd.read_csv(f'{DATA_PATH}\/train.csv')\ntrain.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\ntest = pd.read_csv(f'{DATA_PATH}\/test.csv')\nsubm = pd.read_csv(f'{DATA_PATH}\/sample_submission.csv')\nsubm['Patient'] = subm['Patient_Week'].apply(lambda x: x.split('_')[0])\nsubm['Weeks'] = subm['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsubm =  subm[['Patient','Weeks','Confidence','Patient_Week']]\nsubm = subm.merge(test.drop('Weeks', axis=1), on='Patient')\ntrain['SPLIT'] = 'train'\ntest['SPLIT'] = 'val'\nsubm['SPLIT'] = 'test'\ndata = train.append([test, subm])\nprint('train:',  train.shape, 'unique Pats:', train.Patient.nunique(),\n      '\\ntest:', test.shape,  'unique Pats:', test.Patient.nunique(),\n      '\\nsubm:', subm.shape,  'unique Pats:', subm.Patient.nunique(),\n      '\\ndata',  data.shape,  'unique Pats:', data.Patient.nunique())\ndata['min_week'] = data['Weeks']\ndata.loc[data.SPLIT == 'test', 'min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","af67dfb6":"data = pd.concat([data, pd.get_dummies(data.Sex), pd.get_dummies(data.SmokingStatus)], axis=1)\nif FEATURES:\n    base = data.loc[data.Weeks == data.min_week]\n    base = base[['Patient', 'FVC']].copy()\n    base.columns = ['Patient', 'min_week_FVC']\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    base = base[base.nb == 1]\n    base.drop('nb', axis=1, inplace=True)\n    data = data.merge(base, on='Patient', how='left')\n    data['relative_week'] = data['Weeks'] - data['min_week']\n    del base\nif ADV_FEATURES:\n    target_cols = ['FVC']\n    enc_cols =  [ \n        'Female',\n        'Male',\n        'Currently smokes',\n        'Ex-smoker',\n        'Never smoked'\n    ]\n    for t_col in target_cols:\n        for col in enc_cols:\n            col_name = f'_{col}_{t_col}_'\n            data[f'enc{col_name}mean'] = data.groupby(col)[t_col].transform('mean')\n            data[f'enc{col_name}std'] = data.groupby(col)[t_col].transform('std')\n    data['TC'] = 0\n    data.loc[data['Weeks'] == 0, 'TC'] = 1\nprint(data.shape)\nprint(data.columns)","88bcc733":"feat_cols = [\n    'Female', 'Male',\n    'Currently smokes', \n    'Ex-smoker', 'Never smoked'\n]\nscale_cols = [\n    'Percent', \n    'Age', \n    'relative_week', \n    'min_week_FVC'\n]\nscale_cols.extend([x for x in data.columns if 'FVC_mean' in x])\nscale_cols.extend([x for x in data.columns if 'FVC_std' in x])\nscaler = MinMaxScaler()\ndata[scale_cols] = scaler.fit_transform(data[scale_cols])\nfeat_cols.extend(scale_cols)\ntrain = data.loc[data.SPLIT == 'train']\ntest = data.loc[data.SPLIT == 'val']\nsubm = data.loc[data.SPLIT == 'test']\ndel data\nprint(feat_cols)\ntrain.head()","b40ff2ef":"class DataGenOsic(Sequence):\n    def __init__(self, df, tab_cols,\n                 batch_size=8, mode='fit', shuffle=False, \n                 aug=None, resize=None, masked=True, cutoff=2,\n                 seq_len=12, img_size=224):\n        self.df = df\n        self.shuffle = shuffle\n        self.mode = mode\n        self.aug = aug\n        self.resize = resize\n        self.masked = masked\n        self.cutoff = cutoff\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.seq_len = seq_len\n        self.tab_cols = tab_cols\n        self.on_epoch_end()\n    def __len__(self):\n        return int(np.floor(len(self.df) \/ self.batch_size))\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.df))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    def __getitem__(self, index):\n        batch_size = min(self.batch_size, len(self.df) - index * self.batch_size)\n        X_img = np.zeros((batch_size, self.seq_len, self.img_size, self.img_size, 3), dtype=np.float32)\n        X_tab = self.df[index * self.batch_size : (index + 1) * self.batch_size][self.tab_cols].values\n        pats_batch = self.df[index * self.batch_size : (index + 1) * self.batch_size]['Patient'].values\n        for i, pat_id in enumerate(pats_batch):\n            imgs_seq = self.get_imgs_seq(pat_id)\n            X_img[i, ] = imgs_seq\n        if self.mode == 'fit':\n            y = np.array(\n                self.df[index * self.batch_size : (index + 1) * self.batch_size]['FVC'].values, \n                dtype=np.float32\n            )\n            return (X_img, X_tab), y\n        elif self.mode == 'predict':\n            y = np.zeros(batch_size, dtype=np.float32)\n            return (X_img, X_tab), y\n        else:\n            raise AttributeError('mode parameter error')\n    def load_scan(self, pat_id):\n        if self.mode == 'fit':\n            path = f'{DATA_PATH}\/train\/{pat_id}'\n        elif self.mode == 'predict':\n            path = f'{DATA_PATH}\/test\/{pat_id}'\n        else:\n            raise AttributeError('mode parameter error')\n        file_names = sorted(os.listdir(path), key=lambda x: int(os.path.splitext(x)[0]))\n        idxs = [\n            int(i * len(file_names) \/ (self.seq_len + 2 * self.cutoff)) \n            for i in range(self.seq_len + 2 * self.cutoff)\n        ]\n        slices = [\n            pydicom.read_file(path + '\/' + file_names[idx])\n            for idx in idxs[self.cutoff:-self.cutoff]\n        ]\n        if len(slices) < self.seq_len:\n            for i in range(self.seq_len - len(slices)):\n                slices.append(\n                    pydicom.read_file(path + '\/' + os.listdir(path)[-1])\n                )\n        return slices\n    def get_pixels_hu(self, scans):\n        image = np.stack([s.pixel_array.astype(float) for s in scans])\n        image = image.astype(np.int16)\n        image[image == -2000] = 0\n        intercept = scans[0].RescaleIntercept\n        slope = scans[0].RescaleSlope\n        if slope != 1:\n            image = slope * image.astype(np.float64)\n            image = image.astype(np.int16)\n        image += np.int16(intercept)\n        return np.array(image, dtype=np.int16)\n    def get_imgs_seq(self, pat_id):\n        seq_imgs = []\n        slices = self.load_scan(pat_id)\n        scans = self.get_pixels_hu(slices)\n        for img_idx in range(self.seq_len):\n            img = scans[img_idx]\n            if self.masked:\n                mask = self.get_lungs_mask(img)\n                img[mask == False] = img.min()\n            if self.resize:\n                img = cv2.resize(img, (self.resize, self.resize))\n            img = (img - np.min(img)) \/ (np.max(img) - np.min(img))\n            img = np.repeat(img[..., np.newaxis], 3, -1)\n            seq_imgs.append(img)                 \n        return np.array(seq_imgs).astype(np.float32)\n    def get_lungs_mask(self, img):\n        mask = img < -200\n        mask = clear_border(mask)\n        mask = label(mask)\n        areas = [r.area for r in regionprops(mask)]\n        areas.sort()\n        if len(areas) > 2:\n            for region in regionprops(mask):\n                if region.area < areas[-2]:\n                    for coordinates in region.coords:\n                        mask[coordinates[0], coordinates[1]] = 0\n        return mask > 0","b1d45bcb":"gkf = GroupKFold(n_splits=N_FOLDS)\ntrain['fold'] = -1\nfor i, (train_idx, val_idx) in enumerate(gkf.split(train, groups=train['Patient'])):\n    train.loc[val_idx, 'fold'] = i","80edad5d":"train_datagen = DataGenOsic(\n    df=train.loc[train['fold'] != 0], \n    tab_cols=feat_cols,\n    batch_size=BATCH_SIZE,\n    mode='fit', \n    shuffle=True, \n    aug=None, \n    resize=RESIZE,\n    masked=MASKED,\n    cutoff=CUTOFF,\n    seq_len=SEQ_LEN, \n    img_size=IMG_SIZE\n)\nval_datagen = DataGenOsic(\n    df=train.loc[train['fold'] == 0],\n    tab_cols=feat_cols,\n    batch_size=BATCH_SIZE,\n    mode='fit', \n    shuffle=False, \n    aug=None,\n    resize=RESIZE,\n    masked=MASKED,\n    cutoff=CUTOFF,\n    seq_len=SEQ_LEN, \n    img_size=IMG_SIZE\n)","a1014fe1":"(Xt_img, Xt_tab), yt = val_datagen.__getitem__(0)\nprint('test X img: ', Xt_img.shape)\nprint('test X tab: ', Xt_tab.shape)\nprint('test y: ', yt.shape)\nfig, axes = plt.subplots(figsize=(10, 8), nrows=BATCH_SIZE, ncols=SEQ_LEN)\nfor j in range(BATCH_SIZE):\n    for i in range(SEQ_LEN):\n        axes[j, i].imshow(Xt_img[j][i])\n        axes[j, i].axis('off')\n        axes[j, i].set_title(yt[j])\nplt.show()\nprint(yt)","71fa7bb1":"def metric(y_true, y_pred, pred_std):\n    clip_std = np.clip(pred_std, 70, 9e9)  \n    delta = np.clip(np.abs(y_true - y_pred), 0 , 1000)  \n    return np.mean(-1 * (np.sqrt(2) * delta \/ clip_std) - np.log(np.sqrt(2) * clip_std))\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = (y_pred[:, 2] - y_pred[:, 0]) \/ 2\n    fvc_pred = y_pred[:, 1]\n    sigma_clip = tf.maximum(sigma, C_SIGMA)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C_DELTA)\n    sq2 = tf.sqrt(tf.dtypes.cast(2, dtype=tf.float32))\n    metric = sq2 * (delta \/ sigma_clip) + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\ndef qloss(y_true, y_pred):\n    q = tf.constant(np.array([QS]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q - 1) * e)\n    return K.mean(v)\ndef mloss(lmbd):\n    def loss(y_true, y_pred):\n        return lmbd * qloss(y_true, y_pred) + (1 - lmbd) * score(y_true, y_pred)\n    return loss\ndef get_lr_callback(batch_size=10, epochs=100, warmup=.3, plot=False):\n    lr_start = 0.0001\n    lr_max = 0.001 * batch_size\n    lr_min = 0.00001\n    lr_ramp_ep = epochs * warmup\n    lr_sus_ep = 0\n    lr_decay = 0.97\n    def lr_scheduler(epoch):\n            if epoch < lr_ramp_ep:\n                lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            elif epoch < lr_ramp_ep + lr_sus_ep:\n                lr = lr_max\n            else:\n                lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            return lr\n    if plot == False:\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=False)\n        return lr_callback \n    else: \n        return lr_scheduler\nclass LogPrintingCallback(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        self.val_score = []        \n    def on_epoch_end(self, epoch, logs=None):\n        self.val_score.append(logs['val_score'])\n        if epoch % 5 == 0 or epoch == (EPOCHS - 1):\n            print(\n                f\"\\tEPOCH {epoch + 1} | loss: {logs['loss']:.2f} | score: {logs['score']}\",\n                f\"| val loss: {logs['val_loss']:.2f} | val score: {logs['val_score']}\"\n            )\n    def on_train_end(self, lowest_val_loss, logs=None):\n        best_epoch = np.argmin(self.val_score)\n        best_score = self.val_score[best_epoch]\n        print(f'best model at epoch {best_epoch + 1} | score: {best_score}')\ndef get_model(inputs_seq_shape, inputs_tab_shape, units=128, mult=2, prob=.4, lmbd=.8):\n    bottleneck = efn.EfficientNetB0(\n        weights='..\/input\/effnetweights\/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5', \n        include_top=False, \n        pooling='avg'\n    )\n    bottleneck = M.Model(inputs=bottleneck.inputs, outputs=bottleneck.layers[-2].output)\n    inputs_seq = L.Input(shape=(*inputs_seq_shape, ))\n    inputs_tab = L.Input(shape=(inputs_tab_shape, ))\n    x = L.TimeDistributed(bottleneck)(inputs_seq) \n    x = L.TimeDistributed(L.BatchNormalization())(x)\n    x = L.GlobalMaxPooling3D()(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(prob)(x)\n    x = L.Dense(int(mult * units), activation='relu')(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(prob)(x)\n    comb_inputs = L.concatenate([inputs_tab, x]) # concatenate features from EfficientNet network and tabular data\n    x = L.Dense(units, activation='relu')(comb_inputs)\n    x = L.Dropout(prob)(x)\n    x = L.Dense(units, activation='relu')(x)\n    x = L.Dropout(prob)(x)\n    preds = L.Dense(3, activation='linear')(x)\n    model = M.Model([inputs_seq, inputs_tab], preds)\n    model.compile(\n        loss=mloss(lmbd),\n        optimizer=Adam(lr=0.01), \n        metrics=[score]\n    )\n    return model","0c90e8ed":"EPOCHS = 400\nmodel_file = f'{MODELS_PATH}\/model_{MDL_VERSION}.h5'\nearlystopper = EarlyStopping(\n        monitor='val_score', \n        patience=20, \n        verbose=0,\n        mode='min'\n        )\nmodelsaver = ModelCheckpoint(\n    model_file, \n    monitor='val_score', \n    verbose=1, \n    save_best_only=True,\n    mode='min'\n)\nlrreducer = ReduceLROnPlateau(\n    monitor='val_score',\n    factor=.1,\n    patience=10,\n    verbose=0,\n    min_lr=1e-5,\n    mode='min'\n)\ncallbacks = [earlystopper, modelsaver, LogPrintingCallback()]\nif DECAY:\n    lr_scheduler_plot = get_lr_callback(batch_size=10, plot=True)\n    xs = [i for i in range(EPOCHS)]\n    y = [lr_scheduler_plot(x) for x in xs]\n    plt.plot(xs, y)\n    plt.title(f\"lr schedule from {y[0]:.3f} to {max(y):.3f} to {y[-1]:.5f}\")\n    plt.show()\n    callbacks.append(get_lr_callback(BATCH_SIZE))\nelse:\n    callbacks.append(lrreducer)","997405e2":"%%time\nmodel = get_model(\n    inputs_seq_shape=(SEQ_LEN, IMG_SIZE, IMG_SIZE, 3), \n    inputs_tab_shape=len(feat_cols), \n    units=128, mult=4, prob=0, lmbd=LAMBDA\n)\nhistory = model.fit(\n    train_datagen,\n    validation_data=val_datagen,\n    batch_size=BATCH_SIZE, \n    epochs=EPOCHS, \n    callbacks=callbacks,\n    verbose=1\n)","37d5ccd9":"history_file = f'{MODELS_PATH}\/history_{MDL_VERSION}.txt'\ndict_to_save = {}\nfor k, v in history.history.items():\n    dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\nwith open(history_file, 'w') as file:\n    json.dump(dict_to_save, file)\nep_max = EPOCHS\nplt.plot(history.history['loss'][:ep_max], label='loss')\nplt.plot(history.history['val_loss'][:ep_max], label='val_loss')\nplt.legend()\nplt.show()\nplt.plot(history.history['score'][:ep_max], label='score')\nplt.plot(history.history['val_score'][:ep_max], label='val score')\nplt.legend()\nplt.show()","7600b44a":"## Train model","c8b7b82d":"**TODO:**\n1. Inference part => Done [here](https:\/\/www.kaggle.com\/vgarshin\/osic-keras-images-and-tabular-data-inference)\n2. Understand if there is a profit from this stuff","eef28219":"Data generator provides a batch with images sequences (sequence of CT images) with tabular data for patients in batch. We need ordered CT scans in order to simulate 3D analysis of images, outputs of neural network's bottleneck are concateneted with tabular data and feeded to head with dense layers to obtain quantile regression model:","a68f73d0":"## Import libraries","ead32f87":"Here is an awkward attempt to develop a model which utilizes both images and tabular data. The main idea came from [this article](https:\/\/www.pyimagesearch.com\/2019\/02\/04\/keras-multiple-inputs-and-mixed-data\/):\n\n![image.png](attachment:image.png)\n\nThis notebook is partialy based on [Osic-Multiple-Quantile-Regression-Starter](https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter) to deal with tabular data.\n\n[EfficientNet](https:\/\/github.com\/qubvel\/efficientnet) is used as backbone to create features from images.\n\nHere is [a model inference notebook](https:\/\/www.kaggle.com\/vgarshin\/osic-keras-images-and-tabular-data-inference).","75c59692":"## Load tabular data and create new features","1e4cbbc1":"## Data Generator to load CT scans","12013d3a":"# OSIC: keras model training on images and tabular data","b9eca527":"Visualization of a single batch:"}}