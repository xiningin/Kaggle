{"cell_type":{"6b91590d":"code","950dfd5e":"code","7a9aaf80":"code","07edbe07":"code","b9ceaddb":"code","7d44a54f":"code","1f9d2944":"markdown"},"source":{"6b91590d":"# General imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import make_pipeline\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\nsns.set_style('darkgrid')\n\n# Regression model imports\nfrom sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Ignore warnings\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn","950dfd5e":"# Data loading\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head(5)","7a9aaf80":"# Data Preprocessing\n\n# Remove ID Column\ntrain_id = train[\"Id\"]\ntest_id = test[\"Id\"]\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)\n\n# Delete outliers (2 very large & cheap houses) => we should avoid deleting too much outliers for robustness, however\ntrain.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index, axis=0, inplace=True)\n\n# Fix right-skewedness of target distribution by logarithm\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Concatenate\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train['SalePrice'].values\nall_data = pd.concat([train, test]).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\n# Process NULL values\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median())) # Fill with neighborhood median area\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data.drop(['Utilities'], axis=1, inplace=True) # Not needed for extreme skewedness\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n# Transform numerical features that are really in fact categorical\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n\n# Transform categorical features into indices (0, ..., n_classes - 1)\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n    'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n    'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n    'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n    'YrSold', 'MoSold')\nfor col in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[col].values))\n    all_data[col] = lbl.transform(list(all_data[col].values))\n\n# Add new feature for total area\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n# Apply Box-Cox to skewed features\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lam)\n    \n# Transform categorical features (dtype('object')) into one-hot indicator variables\nall_data = pd.get_dummies(all_data)\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","07edbe07":"# Modelling\n\n# Define error function (5-Fold Cross-Validation)\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=1).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return(rmse)\n\n# Define models\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, \n                             verbosity=0, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n# Get model scores\nmodels = [('Lasso', lasso), ('ElasticNet', ENet), ('Kernel Ridge', KRR),\n          ('Gradient Boosting', GBoost), ('Xgboost', model_xgb), ('LGBM', model_lgb)]\nfor model in models:\n    score = rmsle_cv(model[1])\n    print('{} score: {} ({})'.format(model[0], score.mean(), score.std()))","b9ceaddb":"# Stacking Models\n\n# 1. Averaging Model\nclass AveragingModel(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    \n    def fit(self, x, y):\n        self.models_ = [clone(model) for model in self.models]\n        for model in self.models_:\n            model.fit(x, y)\n        return self\n\n    def predict(self, x):\n        predictions = np.column_stack([\n            model.predict(x) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)\n    \naveraging_model = AveragingModel(models=[ENet, GBoost, KRR, lasso])\nscore = rmsle_cv(averaging_model)\nprint(\"Averaging model score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n# 2. Stacking Model\nclass StackingModel(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    \n    def fit(self, x, y):\n        self.base_models_ = [list() for model in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=1)\n        \n        out_of_fold_predictions = np.zeros((x.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(x, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(x[train_index], y[train_index])\n                y_pred = instance.predict(x[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n            \n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    \n    def predict(self, x):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(x) for model in trained_base_models]).mean(axis=1) \n            for trained_base_models in self.base_models_])\n        return self.meta_model_.predict(meta_features)\n    \nstacking_model = StackingModel(base_models=[ENet, GBoost, KRR], meta_model=lasso)\nscore = rmsle_cv(stacking_model)\nprint(\"Stacking model score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","7d44a54f":"# Submit\nfor model in [stacking_model, model_xgb, model_lgb]:\n    model.fit(train.values, y_train)\n\nstacking_pred = np.expm1(stacking_model.predict(test.values))\nxgb_pred = np.expm1(model_xgb.predict(test.values))\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nensemble = stacking_pred * 0.7 + xgb_pred * 0.15 + lgb_pred * 0.15\n\nsubmission = pd.DataFrame({'Id': test_id, 'SalePrice': ensemble})\nsubmission.to_csv('submission.csv', index=False)","1f9d2944":"*Disclaimer: This notebook is based on a wonderful work by Serigne (https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook). I followed along his notebook and wrote down the essential code in order to understand the intuition behind all the analysis.*"}}