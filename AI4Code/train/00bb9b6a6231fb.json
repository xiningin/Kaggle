{"cell_type":{"bd39a7d1":"code","4162bc68":"code","74583391":"code","a9bd2854":"code","ebe3f8e4":"code","4a4a38c8":"code","fe9f89c8":"code","3daafe58":"code","f2f59eab":"code","6ab09d60":"code","209850ea":"code","8788e984":"code","7a7475b6":"code","1e2a984c":"code","5221bf0b":"code","6d1188cc":"code","6d90e8a3":"code","c207f514":"code","804cdfb2":"code","d51ab35c":"code","b26d0aa2":"code","547d4e51":"code","10d8bbcd":"code","b501fa8d":"code","56d92f94":"code","4c658d18":"code","0fe9a4c1":"code","83f69f84":"code","310f80aa":"code","7507c6f5":"code","f8feef5b":"code","3dcdd909":"code","8fd9c8f0":"code","ce749020":"code","0558bee8":"code","c701316d":"code","62cb9eae":"code","17c7a238":"code","b0f1f425":"code","eebeb81b":"code","627826d5":"code","4155b9a8":"code","64d9906e":"code","a51dbb46":"code","1292b614":"code","e447f0cd":"code","855d146e":"code","b04612ec":"code","c4e5d5fb":"code","2a1bd705":"code","d0e06f57":"code","728dda26":"code","95da0d70":"code","b7e0b43d":"code","01926e69":"code","8736de7a":"code","3dc2dcff":"code","1d9d6f89":"code","35247801":"code","bbb7c479":"markdown","5ffcafda":"markdown","533958b2":"markdown","05743156":"markdown","1c30f92a":"markdown","4b24a915":"markdown","4614223e":"markdown","166d5a36":"markdown","b2da2d2b":"markdown","5e24072d":"markdown","8ff6d856":"markdown","f2ebf0b6":"markdown","e42bce02":"markdown","5298795e":"markdown","35a44fb1":"markdown","3205bc83":"markdown","5ea9fb8f":"markdown","f213be68":"markdown","677097c8":"markdown","321b7db6":"markdown","fa5ccf5c":"markdown","1214f4e9":"markdown","f350d5f2":"markdown","39d5ebe4":"markdown","6b5169b7":"markdown","869632f9":"markdown","83d4e8ac":"markdown","40a499a1":"markdown","a2860239":"markdown","0f0c621b":"markdown","c6d142a5":"markdown","5c2b877e":"markdown","50e15ee5":"markdown","c8232308":"markdown","8d44f278":"markdown","1fde98e7":"markdown","a1101fcf":"markdown","cbedcf91":"markdown","ac4229f6":"markdown","d4dd231f":"markdown","527804fb":"markdown","7820ff5f":"markdown","56dc6af7":"markdown","c35fbd1e":"markdown","7c97690d":"markdown","bd2eab13":"markdown","8b7b34d6":"markdown"},"source":{"bd39a7d1":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4162bc68":"train= pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ntest = pd.read_csv('..\/input\/titanic\/test.csv', index_col = \"PassengerId\")","74583391":"train","a9bd2854":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\n","ebe3f8e4":"test.head()","4a4a38c8":"train.head()","fe9f89c8":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features = %i \" % train.shape[1])","3daafe58":"train.isnull().sum()","f2f59eab":"train.dtypes","6ab09d60":"train.columns[train.dtypes==object]\n\n","209850ea":"train[\"Sex\"].value_counts()","8788e984":"train[\"Embarked\"].value_counts()","7a7475b6":"cleanup_nums = { \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2 },\"Sex\": {\"male\": 0, \"female\": 1}}","1e2a984c":"train.replace(cleanup_nums, inplace=True)\ntest.replace(cleanup_nums, inplace=True)\ntrain.head()","5221bf0b":"train['Cabin'].unique()","6d1188cc":"train['Ticket'].unique()","6d90e8a3":"# let's make boxplots to visualise outliers in the continuous variables \n# Age and Fare\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","c207f514":"# first we plot the distributions to find out if they are Gaussian or skewed.\n# Depending on the distribution, we will use the normal assumption or the interquantile\n# range to find outliers\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')","804cdfb2":"# find outliers\n# Age\nUpper_boundary = train.Age.mean() + 3* train.Age.std()\nLower_boundary = train.Age.mean() - 3* train.Age.std()\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_boundary, upperboundary=Upper_boundary))\n \n# Fare\nIQR = train.Fare.quantile(0.75) - train.Fare.quantile(0.25)\nLower_fence = train.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Fare.quantile(0.75) + (IQR * 3)\nprint('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))","d51ab35c":"train['Age'] = np.where(train['Age']>73, 73, train['Age'])\ntest['Age'] = np.where(test['Age']>73, 73, test['Age'])\ntrain['Age'].max()","b26d0aa2":"train['Fare'] = np.where(train['Fare']>100, 100, train['Fare'])\ntest['Fare'] = np.where(test['Fare']>100, 100, test['Fare'])\ntrain['Fare'].max()","547d4e51":"train.corr()","10d8bbcd":"\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(20, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,linewidths=.5, annot= True)","b501fa8d":"train['Family_name']=train['Name'].str.split(', ').str[0]\ntest['Family_name']=test['Name'].str.split(', ').str[0]\ntrain","56d92f94":"train['Title']=train['Name'].str.split(', ').str[1].str.split('.').str[0]\ntest['Title']=test['Name'].str.split(', ').str[1].str.split('.').str[0]\ntrain['Title'].unique()","4c658d18":"train['Title'] =train['Title'].replace(['Ms','Mlle'], 'Miss')\ntrain['Title'] = train['Title'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntrain['Title'] =train['Title'].replace(['Rev','Mlle','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')\n\ntest['Title'] =test['Title'].replace(['Ms','Mlle'], 'Miss')\ntest['Title'] = test['Title'].replace(['Mme','Dona','the Countess','Lady'], 'Mrs')\ntest['Title'] =test['Title'].replace(['Rev','Mlle','Jonkheer','Dr','Capt','Don','Col','Major','Sir'], 'Mr')","0fe9a4c1":"train['Title']","83f69f84":"\n\ncleanup_nums = { \"Title\": {\"Mr\": 0, \"Mrs\": 1, \"Miss\": 2, \"Master\": 3 } }\ntrain.replace(cleanup_nums, inplace=True)\ntest.replace(cleanup_nums, inplace=True)","310f80aa":"train['Age'].fillna((train['Age'].mean()), inplace=True) # I will fill the columns that do with nan values with mean age number \ntest['Age'].fillna((test['Age'].mean()), inplace=True)\n\nbins = [0, 2, 18, 35, 65, np.inf]\nnames = ['<2', '2-18', '18-35', '35-65', '65+']\n\ntrain['AgeRange'] = pd.cut(train['Age'], bins, labels=names)\ntest['AgeRange'] = pd.cut(test['Age'], bins, labels=names)\n\nNumberedAgeCategories = {'<2':0 , '2-18':1, '18-35':2, '35-65':3, '65+':4}\ntrain['AgeRange']=train['AgeRange'].map(NumberedAgeCategories)  \ntrain['AgeRange']=pd.to_numeric(train['AgeRange'])\ntest['AgeRange']=test['AgeRange'].map(NumberedAgeCategories)  \ntest['AgeRange']=pd.to_numeric(test['AgeRange'])\ntrain","7507c6f5":"train['FamilySize']= train['SibSp']+train['Parch']+1\ntest['FamilySize']= test['SibSp']+test['Parch']+1\ntrain","f8feef5b":"cabin_only = train[[\"Cabin\"]].copy()\ncabin_only[\"Cabin_Data\"] = cabin_only[\"Cabin\"].isnull().apply(lambda x: not x) # extract rows that do not contain null Cabin data.","3dcdd909":"cabin_only[\"Deck\"] = cabin_only[\"Cabin\"].str.slice(0,1)\ncabin_only[\"Room\"] = cabin_only[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ncabin_only[cabin_only[\"Cabin_Data\"]]\ncabin_only","8fd9c8f0":"cabin_only.drop([\"Cabin\", \"Cabin_Data\"], axis=1, inplace=True, errors=\"ignore\")\ncabin_only[\"Deck\"] = cabin_only[\"Deck\"].fillna(\"N\") # assign 'N' for the deck name of the null Cabin value. \ncabin_only[\"Room\"] = cabin_only[\"Room\"].fillna(cabin_only[\"Room\"].mean()) # use mean to fill null Room values.","ce749020":"cabin_only","0558bee8":"cabin_only=cabin_only.join(pd.get_dummies(cabin_only['Deck'], prefix='Deck'))\ncabin_only=cabin_only.drop(['Deck'], axis=1)\ncabin_only","c701316d":"train=pd.concat([train,cabin_only],axis=1)\ntrain.shape\n","62cb9eae":"cabin_only_test = test[[\"Cabin\"]].copy()\ncabin_only_test[\"Cabin_Data\"] = cabin_only_test[\"Cabin\"].isnull().apply(lambda x: not x) # extract rows that do not contain null Cabin data.\ncabin_only_test[\"Deck\"] = cabin_only_test[\"Cabin\"].str.slice(0,1)\ncabin_only_test[\"Room\"] = cabin_only_test[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\ncabin_only_test[cabin_only_test[\"Cabin_Data\"]]\ncabin_only_test.drop([\"Cabin\", \"Cabin_Data\"], axis=1, inplace=True, errors=\"ignore\")\ncabin_only_test[\"Deck\"] = cabin_only_test[\"Deck\"].fillna(\"N\") # assign 'N' for the deck name of the null Cabin value. \ncabin_only_test[\"Room\"] = cabin_only_test[\"Room\"].fillna(cabin_only_test[\"Room\"].mean()) # use mean to fill null Room values.\ncabin_only_test=cabin_only_test.join(pd.get_dummies(cabin_only_test['Deck'], prefix='Deck'))\ncabin_only_test=cabin_only_test.drop(['Deck'], axis=1)\ntest=pd.concat([test,cabin_only_test],axis=1)","17c7a238":"\n# extract numbers from the ticket\ntrain['Ticket_numerical'] = train.Ticket.apply(lambda s: s.split()[-1])\ntrain['Ticket_numerical'] = np.where(train.Ticket_numerical.str.isdigit(), train.Ticket_numerical, np.nan)\ntrain['Ticket_numerical'] = train['Ticket_numerical'].astype('float')\ntrain[\"Ticket_numerical\"] = train[\"Ticket_numerical\"].fillna(0) # some tickets have string values only, so we will assign a 0 for their ticket_numerical.\n\n\ntest['Ticket_numerical'] = test.Ticket.apply(lambda s: s.split()[-1])\ntest['Ticket_numerical'] = np.where(test.Ticket_numerical.str.isdigit(), test.Ticket_numerical, np.nan)\ntest['Ticket_numerical'] = test['Ticket_numerical'].astype('float')\ntest[\"Ticket_numerical\"] = test[\"Ticket_numerical\"].fillna(0) \n\n# extract the first part of ticket as category\ntrain['Ticket_categorical'] = train.Ticket.apply(lambda s: s.split()[0])\ntrain['Ticket_categorical'] = np.where(train.Ticket_categorical.str.isdigit(), np.nan, train.Ticket_categorical)\ntrain[\"Ticket_categorical\"] = train[\"Ticket_categorical\"].fillna(\"NONE\") # some tickets have digit values only, so we will assign 'NONE' for their ticket_categorical.\ntrain['Ticket_numerical'].tolist()\n \ntest['Ticket_categorical'] = test.Ticket.apply(lambda s: s.split()[0])\ntest['Ticket_categorical'] = np.where(test.Ticket_categorical.str.isdigit(), np.nan, test.Ticket_categorical)\ntest[\"Ticket_categorical\"] = test[\"Ticket_categorical\"].fillna(\"NONE\") # some tickets have digit values only, so we will assign 'NONE' for their ticket_categorical.\ntest['Ticket_numerical'].tolist()\n\ntrain[['Ticket', 'Ticket_numerical', 'Ticket_categorical']].head()","b0f1f425":"train.isna().sum()","eebeb81b":"# Fare in the test dataset contains one null value, I will replace it by the median \ntrain.Fare.fillna(train.Fare.median(), inplace=True)\ntest.Fare.fillna(train.Fare.median(), inplace=True)","627826d5":"train= train.drop(['Cabin'], axis=1)\ntest= test.drop(['Cabin'], axis=1)\n","4155b9a8":"#label encoder can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\nlabel_encoder = LabelEncoder()\nfor col in train.columns[train.dtypes == \"object\"]:\n    train[col] = label_encoder.fit_transform(train[col].astype('str'))\n\nfor col in test.columns[test.dtypes == \"object\"]:\n    test[col] = label_encoder.fit_transform(test[col].astype('str'))\n\n# drop rows with null values    \ntrain.dropna(inplace=True)\n\nX = train.drop('Survived', axis=1)\n\n# create our response variable\ny = train['Survived']\n\n\n#train_test_ split is used to split the dataset into two pieces, so that the model can be trained and tested on different data.\n#This is a better method for evaluating the model performance rather than testing it on the training data only. \nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n","64d9906e":"def featureSelection(label):    \n    clf = DecisionTreeClassifier(random_state=0)\n    if(label=='Decision Tree'):\n        clf = DecisionTreeClassifier(random_state=0)\n    if(label=='Random Forest'):\n        clf = RandomForestClassifier(random_state=0)\n    if(label=='XGBoost'):\n        clf = XGBClassifier(random_state=0)  \n    if(label=='Extra Trees'):\n        clf = ExtraTreesClassifier(random_state=0)  \n        \n    clf= clf.fit(X_train, y_train)\n    \n    arr= dict(zip(X_train.columns, clf.feature_importances_)) ## this is used to write the feature name next to the probability\n    data= pd.DataFrame.from_dict(arr,orient='index', columns=['importance'])\n    return data.sort_values(['importance'], ascending=False)","a51dbb46":"fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,10)) # one row, three columns\nr=featureSelection(\"Decision Tree\")\nv=featureSelection(\"Random Forest\")\ns=featureSelection(\"XGBoost\")\nt=featureSelection(\"Extra Trees\")\nr.plot.bar(y=\"importance\", rot=70, title=\"Decision Tree Features with their corresponding importance values\",ax=ax1)\nv.plot.bar(y=\"importance\", rot=70, title=\"Random Forest Features with their corresponding importance values\", ax=ax2)\ns.plot.bar(y=\"importance\", rot=70, title=\"XGBoost Features with their corresponding importance values\", ax=ax3)\nt.plot.bar(y=\"importance\", rot=70, title=\"Extra Trees Features with their corresponding importance values\", ax=ax4)\nplt.tight_layout() \n","1292b614":"\nlogit_model = LogisticRegression(max_iter=10000)\nlogit_model.fit(X_train, y_train)\n \nimportance = pd.Series(np.abs(logit_model.coef_.ravel()))\nimportance.index = X_train.columns\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","e447f0cd":"def get_best_model_and_accuracy(model, params, X, y):\n    grid_clf_auc = GridSearchCV(model,param_grid=params,error_score=0.,scoring = 'roc_auc')\n    grid_clf_auc.fit(X, y) # fit the model and parameters\n    print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\n    print('Grid best score (AUC): ', grid_clf_auc.best_score_)","855d146e":"dt=DecisionTreeClassifier(random_state=0)\nparam_grid = {\"max_depth\": [3,7,10,50,100],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}  \nprint(\"Decision Tree train:\")\nget_best_model_and_accuracy(dt, param_grid, X_train, y_train)\nprint(\"Decision Tree test:\")\nget_best_model_and_accuracy(dt, param_grid, X_test, y_test)","b04612ec":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nLR=LogisticRegression(max_iter=100000,random_state=0)\nprint(\"Logistic Regression train:\")\nget_best_model_and_accuracy(LR, param_grid, X_train, y_train)\nprint(\"Logistic Regression test:\")\nget_best_model_and_accuracy(LR, param_grid, X_test, y_test)","c4e5d5fb":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n}\nrf=RandomForestClassifier(random_state=0)\nprint(\"Random Forest train:\")\nget_best_model_and_accuracy(rf, param_grid, X_train, y_train)\nprint(\"Random Forest test:\")\nget_best_model_and_accuracy(rf, param_grid, X_test, y_test)","2a1bd705":"train=train[train.columns.drop(list(train.filter(regex='Deck')))]\ntrain= train.drop(['AgeRange'], axis=1)\ntest=test[test.columns.drop(list(test.filter(regex='Deck')))]\ntest= test.drop(['AgeRange'], axis=1)\nX = train.drop('Survived', axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\ntrain","d0e06f57":"dt=DecisionTreeClassifier(random_state=0)\nparam_grid = {\"max_depth\": [3,7,10,50,100],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}  \nprint(\"Decision Tree train:\")\nget_best_model_and_accuracy(dt, param_grid, X_train, y_train)\nprint(\"Decision Tree test:\")\nget_best_model_and_accuracy(dt, param_grid, X_test, y_test)","728dda26":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nLR=LogisticRegression(max_iter=100000,random_state=0)\nprint(\"Logistic Regression train:\")\nget_best_model_and_accuracy(LR, param_grid, X_train, y_train)\nprint(\"Logistic Regression test:\")\nget_best_model_and_accuracy(LR, param_grid, X_test, y_test)","95da0d70":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n}\nrf=RandomForestClassifier(random_state=0)\nprint(\"Random Forest train:\")\nget_best_model_and_accuracy(rf, param_grid, X_train, y_train)\nprint(\"Random Forest test:\")\nget_best_model_and_accuracy(rf, param_grid, X_test, y_test)","b7e0b43d":"param_grid = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [3, 4, 5]\n        }\nXGB= XGBClassifier(random_state=0)\nprint(\"XGBoost train:\")\nget_best_model_and_accuracy(XGB, param_grid, X_train, y_train)\nprint(\"XGBoost test:\")\nget_best_model_and_accuracy(XGB, param_grid, X_test, y_test)","01926e69":" param_grid={\n    \n     'n_estimators':[100,500,1000], 'max_depth':[5,6,9], 'min_samples_split':[5,6,9], 'min_samples_leaf':[4,5,6,9]\n       \n    }\nET= ExtraTreesClassifier(random_state=0) \nprint(\"ExtraTrees train:\")\nget_best_model_and_accuracy(ET, param_grid, X_train, y_train)\nprint(\"ExtraTrees test:\")\nget_best_model_and_accuracy(ET, param_grid, X_test, y_test)","8736de7a":"rf= RandomForestClassifier(random_state=0,max_depth= 7, max_features= 'sqrt', n_estimators= 500)\nrf.fit(X_train,y_train)\npredictions = rf.predict(test)","3dc2dcff":"sns.barplot(x='Sex', y='Survived', data=train, estimator=lambda x: sum(x==0)*100.0\/len(x))","1d9d6f89":"sns.barplot(x='Title', y='Survived', data=train, estimator=lambda x: sum(x==0)*100.0\/len(x))","35247801":"results_df = pd.DataFrame()\nresults_df[\"PassengerId\"] = test.index\nresults_df[\"Survived\"] = predictions\nresults_df.to_csv(\"my_submissions\", index=False)\nresults_df.head(5)","bbb7c479":"Table of Contents: \n1. Data Preparation\n     - [Data Exploration & Cleaning ](#0)\n2. Data Anaysis\n      - [Feature Construction](#1)\n      - [Feature selection](#2)\n      - [Model Selection ](#3)\n      \n  ","5ffcafda":"The difference between train and test scores are small which is a good indication that there is no over-fitting. Finally, it would be reasonable to choose the model with the hight score for X_Test dataset, which in this case would be Random Forest. ","533958b2":"# Data Exploration and Cleaning<a id=\"0\"><\/a>","05743156":"We can divide 'Age' into categories and see whether this will be better than the original 'Age' feature. ","1c30f92a":"\nData cleaning is the process of ensuring that your data is correct, consistent and usable. This improves the quality of the training data for analytics and enables accurate decision-making.<br\/>\n\nFor data cleaning, we will focus on three points: \n* Non-numerical data \n* Missing values\n* Outliers \n","4b24a915":"It is reasonable to remove the Cabin column now that we have extracted two new features from it : 'Deck' and 'Room'. <br\/>\nFor the rest of the null values, we can drop the rows with null values since they are few and this will not affect our machine learning model performance.","4614223e":"Now let's take a look at the Cabin and Ticket features.","166d5a36":"First we plot the distributions to find out if they are Gaussian or skewed.\nDepending on the distribution, we will use the normal assumption or the interquantile range to find outliers","b2da2d2b":"Now we will the scoring of each model. ","5e24072d":"Now we will take a look at the correlation matrix to get a quick insight about the relationships between features. ","8ff6d856":"# Feature Construction <a id=\"1\"><\/a>\nFeature construction is a process which builds intermediate features from the original descriptors in a dataset. The aim is to build more efficient features for a machine learning task.","f2ebf0b6":"Model selection is the process of selecting one final machine learning model from among a collection of machine learning models for a training dataset. ","e42bce02":"In addition, we can also derive the marital status of each member from the Name feature. ","5298795e":"After adding all these new features, we need to check whether we have null values and deal with them. ","35a44fb1":"First, let's check the size of null values we have.","3205bc83":"Next, we can work out some meaningful interpretation from the Cabin column.<br\/>\nDespite the fact that this feature contains too many unique values and null values, we can still see that this feature carries useful information like the deck group and the room number ( One letter followed by numbers). <br\/>\nWe can use this to check whether there is any relation between the deck and the fare amount or the Pclass of a passenger. ","5ea9fb8f":"For this dataset, we will examine the effect of features on 5 different models: <\/br>\n\n1. Decision Trees:A Decision Tree is a Flow Chart, and can help you make decisions based on previous experience. \n2. Random Forests:Random Forest is essentially a collection of Decision Trees.\n3. Logistic Regression: Logistic Regression is a type of Generalized Linear Models. Logistic regression models the probabilities for classification problems with two possible outcomes.\n4. XGBoost: XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n5. Extra Trees: Extra Trees is an ensemble machine learning algorithm that combines the predictions from many decision trees.","f213be68":"Age has a normal distribution while the fare feature has skewed distribution. For the age feature, we will use the Gaussian assumption , and the interquantile range for Fare.","677097c8":"According to the Feature Selection graphs for random forest ( the selected model ), features of Sex and Title had the greatest influence. In order to determine the most important category within each feature, I will use visuals: ","321b7db6":"Similarily, for the ticket feature, there is a pattern in the format which is letters followed by words; so we will extract this data to be able to see if it has an effect on the y_target('Survived' column). ","fa5ccf5c":"First, we will explore the size of data that we have. ","1214f4e9":"Now we will to examine the data types of our features","f350d5f2":"We can see that the are too many unique values for Cabin and Ticket features; it might be reasonable to remove them, but we will try to extract to some information from them. ","39d5ebe4":"# Feature Selection <a id=\"2\"><\/a>","6b5169b7":"Now I will repeat the same process but with some columns(those with the predix 'Deck' and AgeRange column) being removed. ","869632f9":"Here we will deal the the null values in the Cabin column.","83d4e8ac":"From the 'Name' feature, we can astract other important features such as the family name to identify members of the same family. It is likely that members of the same family withh have the same family name.","40a499a1":"Since SibSp include information about the number of siblings and spouses altogether and Parch includes information about the number of nannies, we can extract the family size from this info. Another option would have been to use the family name to identify member of the same family but since the resemblance of family name might be a coincidence we should avoid using this feature for more accurate results. ","a2860239":"Although the difference is very small, removing them still results in better results. ","0f0c621b":"In the tutorial we are required to make predictions for the 'Survived' Column in the titanic dataset.\n","c6d142a5":"As noted in the data analysis part, 0 refers to male. ","5c2b877e":"It seems that all the columns with prefix 'Deck' and the AgeRange seem to have little importance in all 4 classifiers. We will check under the 'Model Selection' part whether removing them will improve the accuarcy of the classifier. ( This is an A\/B testing method)","50e15ee5":"0 in this plot refers to men with the title \"Mr\". ","c8232308":"We can see here that Sex and Embarked are nominal features, and they contain a few number of unique values. Some ML libraries do not take categorical variables as input. Thus, we will convert them into numerical variables.","8d44f278":"I referred to this great kernel for outlier detection https:\/\/www.kaggle.com\/anuragnegi\/feature-engineering-outliers-handling-ensembling.","1fde98e7":"Before deleting the columns with prefix 'Deck' and the AgeGroup column, I will check the accuracy score with and without these columns in 3 classifiers as a test to make sure removing them is beneficial.","a1101fcf":"We will use the best paramaters for random forest classifier that were identified by GridSearchCV in the code above. ","cbedcf91":"Grid search is used to tune hyperparameters to improve model performance. You can read more about it here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html.","ac4229f6":"Now we will replace outliers with reasonable values based on the above calculations.","d4dd231f":"Again, we need to make sure our values are numerical so we will represent the column 'Deck' in a different way using pandas dummies. ","527804fb":"# Model Selection<a id=\"3\"><\/a>","7820ff5f":"From the results, we can see that we have 5 columns of type 'Object'. We need to decide on how we will use these features. The main purpose here is to prepare data that can be used in machine learning models and for that, we need our data to be numerical. ","56dc6af7":"Now I will find the accuracy score for the rest of the classifiers after removing these features. ","c35fbd1e":"Now I will just repeat the same process for test data. You can overlook this part. ","7c97690d":"Feature Selection is the process of selecting a subset of relevant features for use in model construction.","bd2eab13":"Hence, we can conlude the men of age 18 and above had the largest chance for survival. ","8b7b34d6":"Here we will display the features with their corresponding importance values based on each model."}}