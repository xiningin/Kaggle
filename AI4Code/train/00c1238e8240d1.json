{"cell_type":{"4beed389":"code","08afe6f4":"code","8733dc30":"code","834eb5a6":"code","aad9e7ce":"code","ce4aba2b":"code","f2e7646b":"code","d95773b2":"code","af914c37":"code","b230faa5":"code","65fc03e7":"code","5c1340aa":"code","42843704":"code","804f7bb1":"code","06a7f423":"code","89933b09":"code","82db5fa6":"code","5b4f1f50":"markdown","d96209a2":"markdown","0d3e95b0":"markdown","f0bde21e":"markdown","8258eaa7":"markdown","147677b7":"markdown","a792518f":"markdown","0da63507":"markdown","5d30db68":"markdown","d8014aff":"markdown","74707fd1":"markdown","1abca64c":"markdown","6c31b47a":"markdown","b56b4cfd":"markdown","22c39314":"markdown"},"source":{"4beed389":"import pandas as pd\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset,DataLoader\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom fastprogress.fastprogress import master_bar, progress_bar\nfrom torchvision.transforms.functional import to_grayscale\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom torchvision.utils import make_grid\n!pip install efficientnet-pytorch\nfrom efficientnet_pytorch import EfficientNet","08afe6f4":"normal = Path('..\/input\/surface-crack-detection\/Negative')\ncracks = Path('..\/input\/surface-crack-detection\/Positive')\n\ntrain_data = [(o,0) for o in normal.iterdir()]\ntrain_data_cracks = [(o,1) for o in cracks.iterdir()]\ntrain_data.extend(train_data_cracks)\n\ntrain_data = pd.DataFrame(train_data, columns=[\"filepath\",\"cracks\"])\ntrain_data.head()","8733dc30":"bs = 64\nlr=1e-5\nwd=0.01","834eb5a6":"np.random.seed(42)\nmsk = np.random.rand(len(train_data)) < 0.9\n\ntrain_df = train_data[msk].reset_index()\nval_df = train_data[~msk].reset_index()","aad9e7ce":"fig,a =  plt.subplots(1,5)\nfig.set_figheight(30)\nfig.set_figwidth(30)\nfor i in range(5):\n    img = cv.imread(str(train_data.iloc[i]['filepath']))\n    a[i].imshow(img)\n    title = \"Good Concrete \" + str(i)\n    a[i].set_title(title)","ce4aba2b":"fig,a =  plt.subplots(1,5)\nfig.set_figheight(30)\nfig.set_figwidth(30)\nfor i in range(-5, 0):\n    img = cv.imread(str(train_data.iloc[i]['filepath']))\n    a[i].imshow(img)\n    title = \"Concrete with cracks \" + str(5 + i)\n    a[i].set_title(title)","f2e7646b":"class ConcreteDataset(Dataset):\n    def __init__(self, df, transforms=None, is_test=False):\n        self.df = df\n        self.transforms = transforms\n        self.is_test = is_test\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n        img = Image.open(self.df.iloc[idx]['filepath'])\n        \n        if self.transforms:\n            #when using albumation we have to pass data as dictionary to transforms.\n            #The output after transformation is also a dictionary\n            #we need to take the value from dictionary. That is why we are giving an image at the end.\n            img = self.transforms(**{\"image\": np.array(img)})[\"image\"]\n        \n        if self.is_test:\n            return img\n        else:\n            cracks_tensor = torch.tensor([self.df.iloc[idx]['cracks']], dtype=torch.float32)\n            return img, cracks_tensor\n        ","d95773b2":"imagenet_stats = {'mean':[0.485, 0.456, 0.406], 'std':[0.229, 0.224, 0.225]}\ntrain_tfms = A.Compose([\n    A.Cutout(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Flip(p=0.5),\n    ToTensor(normalize=imagenet_stats)\n        ])\n    \ntest_tfms = A.Compose([\n        ToTensor(normalize=imagenet_stats)\n        ])","af914c37":"train_ds = ConcreteDataset(train_df, transforms=train_tfms)\nval_ds = ConcreteDataset(val_df, transforms=test_tfms, is_test=True)\nlen(train_ds), len(val_ds)","b230faa5":"train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size=64, \n                    num_workers=2, pin_memory=True)","65fc03e7":"xb, yb = next(iter(train_dl))\nxb.shape, yb.shape","5c1340aa":"class ConcreteModel(nn.Module):\n    def __init__(self, eff_name = 'efficientnet-b0', n_outs = 1):\n        super().__init__()\n        #downloading the pretrained efficientnet model\n        self.backbone = EfficientNet.from_pretrained(eff_name)\n        #getting the number of input layers in the classifiers\n        in_features = getattr(self.backbone,'_fc').in_features\n        #replacing the existing classifier with new one\n        self.classifier = nn.Sequential(nn.Linear(in_features, in_features\/\/2),\n                                       nn.Dropout(p=0.2),\n                                       nn.Linear(in_features\/\/2, in_features\/\/4),\n                                       nn.Dropout(p=0.2),\n                                       nn.Linear(in_features\/\/4, n_outs))\n    \n    def forward(self, input_of_model):\n        \n        \"\"\"\n        here the input shape is torch.Size([64, 3, 227, 227])\n        we need to extract the features. In my understanding it means taking the output before passing to classifier\n        https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch#example-feature-extraction\n        \"\"\"\n        out_before_classifier = self.backbone.extract_features(input_of_model) #the output size is torch.Size([64, 1280, 7, 7])\n        \n        #to convert the 7x7 to 1x1 we use a adaptive average pool 2d\n        pool_output = F.adaptive_avg_pool2d(out_before_classifier, 1) #the output is torch.Size([64, 1280, 1, 1])\n        \n        \"\"\"\n        now before passing to the classifier, we need to flatten it. Using view operation for the same\n        the size parameter is the length on a particular axis. size(0) = 64 size(1) = 1280 size(2) and size(3) is 1\n        \"\"\"\n        classifier_in = pool_output.view(pool_output.size(0),-1) #this operation will convert torch.Size([64, 1280, 1, 1]) to torch.Size([64, 1280])\n        \n        #this is then fed into a custom classifier which outputs the predicition\n        classifier_out = self.classifier(classifier_in) #the classifier output will be of size torch.Size([64, 1])\n        return classifier_out","42843704":"def get_device():\n    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ndef get_model(model_name='efficientnet-b0',lr=1e-5,wd=0.01,freeze_backbone=False,opt_fn=torch.optim.AdamW,device=None):\n    device = device if device else get_device()\n    model = ConcreteModel(eff_name=model_name)\n    if freeze_backbone:\n        for parameter in model.backbone.parameters():\n            parameter.requires_grad = False\n    opt = opt_fn(model.parameters(),lr=lr,weight_decay=wd)\n    model = model.to(device)\n    return model, opt\n\ndef training_step(xb,yb,model,loss_fn,opt,device,scheduler):\n    xb,yb = xb.to(device), yb.to(device)\n    out = model(xb)\n    opt.zero_grad()\n    loss = loss_fn(out,yb)\n    loss.backward()\n    opt.step()\n    scheduler.step()\n    return loss.item()\n    \ndef validation_step(xb,yb,model,loss_fn,device):\n    xb,yb = xb.to(device), yb.to(device)\n    out = model(xb)\n    loss = loss_fn(out,yb)\n    out = torch.sigmoid(out)\n    return loss.item(),out\n\ndef get_data(train_df,valid_df,train_tfms,test_tfms,bs):\n    train_ds = ConcreteDataset(df=train_df,transforms=train_tfms)\n    valid_ds = ConcreteDataset(df=valid_df,transforms=test_tfms)\n    train_dl = DataLoader(dataset=train_ds,batch_size=bs,shuffle=True,num_workers=4)\n    valid_dl = DataLoader(dataset=valid_ds,batch_size=bs*2,shuffle=False,num_workers=4)\n    return train_dl,valid_dl","804f7bb1":"def fit(epochs,model,train_dl,valid_dl,opt,device=None,loss_fn=F.binary_cross_entropy_with_logits):\n    \n    device = device if device else get_device()\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_dl)*epochs)\n    val_rocs = []\n    tloss = []\n    vloss = []\n    \n    #Creating progress bar\n    mb = master_bar(range(epochs))\n    mb.write(['epoch','train_loss','valid_loss','val_roc'],table=True)\n\n    for epoch in mb:    \n        trn_loss,val_loss = 0.0,0.0\n        val_preds = np.zeros((len(valid_dl.dataset),1))\n        val_targs = np.zeros((len(valid_dl.dataset),1))\n        \n        #Training\n        model.train()\n        \n        #For every batch \n        for xb,yb in progress_bar(train_dl,parent=mb):\n            trn_loss += training_step(xb,yb,model,loss_fn,opt,device,scheduler) \n        trn_loss \/= mb.child.total\n        tloss.append(trn_loss)\n\n        #Validation\n        model.eval()\n        with torch.no_grad():\n            for i,(xb,yb) in enumerate(progress_bar(valid_dl,parent=mb)):\n                loss,out = validation_step(xb,yb,model,loss_fn,device)\n                val_loss += loss\n                bs = xb.shape[0]\n                val_preds[i*bs:i*bs+bs] = out.cpu().numpy()\n                val_targs[i*bs:i*bs+bs] = yb.cpu().numpy()\n\n        val_loss \/= mb.child.total\n        vloss.append(val_loss)\n        val_roc = roc_auc_score(val_targs.reshape(-1),val_preds.reshape(-1))\n        val_rocs.append(val_roc)\n\n        mb.write([epoch,f'{trn_loss:.6f}',f'{val_loss:.6f}',f'{val_roc:.6f}'],table=True)\n    return model,val_rocs, tloss, vloss","06a7f423":"train_dl,valid_dl = get_data(train_df,val_df,train_tfms,test_tfms,bs)\nmodel, opt = get_model(model_name='efficientnet-b0',lr=1e-4,wd=1e-4)","89933b09":"model,val_rocs, train_loss, valid_loss = fit(10,model,train_dl,valid_dl,opt)","82db5fa6":"plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(train_loss, '-bx')\nplt.plot(valid_loss, '-rx')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Loss vs. No. of epochs');\nplt.grid()","5b4f1f50":"<a id=\"dataloader\"><\/a>\n## 7. Dataloader","d96209a2":"<a id=\"training\"><\/a>\n## 9. Training\n\nI have created some utility functions to make the training process easy. These functions are self explanatory. `get_device` returns where the model is training. We can use the `get_model` to get the sepecific type of efficientnet with desired learning rate, weight decay, backbone freeze etc. The `get_data` returns the dataloaders.","0d3e95b0":"<a id=\"transformation\"><\/a>\n## 6. Transformation\n\nHere we are making a model that predicts if an image contains a crack or not we can use many transformations like rotate, flips, random erasing etc","f0bde21e":"Here data is given as two folders. One containing positive cracks and other containing no cracks. We can use the [ImageFolder](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#imagefolder) as dataset here. But I am going with the conventional way and creating a datset.","8258eaa7":"## This is a work in progress\n\n<h2 style=\"background-color:powderblue;\">If you liked the content, please consider upvoting it will motivate me to make similar contents.<\/h2>","147677b7":"Lets define the initial value of batchsize, learning rate and weightdecay","a792518f":"<a id=\"model\"><\/a>\n## 8. Pytorch Model\n\nHere we are planning to use the efficientnet family of models. The constructor is written in such a way that it accepts the name of the model and creates it. We use the [extract_features](https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch#example-feature-extraction) to get the features from efficientnet before passing to classifier. The in_features are taken and is used to make a custom classifier with dropouts. As we are using a model that can work on the family of efficientnets, we are not usere about the size returned by the `extract_features` so we are using an `adaptive_avg_pool2d` layer to solve this issue. The output from the pooling layer is flattened and passed to classifier.","0da63507":"### Images with cracks","5d30db68":"This is where the actual training happens. The `CosineAnnealingLR` scheduler is used here. This function returns the trained model, validation roc, train and validation loss. This loss is later used for plotting the graph.","d8014aff":"<a id=\"dataset\"><\/a>\n## 5. Dataset\n\nThis dataset is used by the dataloader to load the images and its label. I have used Albumation for transformation purpose. In the `__getitem__()`, the image is read and converted to a numpy array before transformation.","74707fd1":"Table of Contents\n1. [Introduction](#intro)\n2. [Import libraries and data](#ild)\n3. [Create train and validation dataframe](#ctvd)\n4. [Sample images](#sample)\n5. [Dataset](#dataset)\n6. [Transformation](#transformation)\n7. [Dataloader](#dataloader)\n8. [Pytorch model](#model)\n9. [Training](#training)\n10. Testing\n11. Conclusion\n12. Whats next\n\n<a id=\"intro\"><\/a>\n## 1. Introduction\nCracks in buildings are one important indicator of the safety of a building. Timely identification of these cracks and proper countermeasures are required if we want to ensure the safety of the building and people in it. In this kernel, I am describing how we can use efficientnet models to identify\u00a0cracks from images. This notebook makes use of transfer learning from efficientnet models. Also, the model used here is\u00a0efficientnet agnostic, you can use any of the available efficientnets from b0 till b7.\u00a0\n\n<a id=\"ild\"><\/a>\n## 2. Import libraries and data","1abca64c":"<a id=\"sample\"><\/a>\n## 4. Sample images\n\n### Images with no cracks","6c31b47a":"<a id=\"ctvd\"><\/a>\n## 3. Create train and validation dataframe\n\nFor simplicity I am taking 90% of data for training and 10% data for testing","b56b4cfd":"![image.png](attachment:image.png)","22c39314":"As the Dataset and transformations are ready, we can use the Dataloader and check if we are able to properly load the image and labels."}}