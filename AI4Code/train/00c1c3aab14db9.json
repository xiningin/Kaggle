{"cell_type":{"203b7d70":"code","de81858f":"code","86679739":"code","b0ed09e6":"code","45e28a59":"code","1c4cf195":"code","c63dffe9":"code","151c8e9d":"code","f44d7e1a":"code","798b31c9":"code","2f6bc44a":"code","3059a9fb":"code","95495b8f":"markdown","94082311":"markdown","1ac261af":"markdown","bb0466e1":"markdown","7a5feafe":"markdown","0cfe9b7a":"markdown","f2bdb892":"markdown","ba90445e":"markdown"},"source":{"203b7d70":"import numpy as np\nfrom numpy.linalg import norm\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","de81858f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(\"Data Path: \", os.path.join(dirname, filename))\n        iris = pd.read_csv(os.path.join(dirname, filename), sep = \",\")\n        #data cleaning\n        iris.drop(columns=\"Id\",inplace=True)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86679739":"#features and labels\nX=iris.iloc[:,0:4].values\ny=iris.iloc[:,4].values\n\n#Train and Test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","b0ed09e6":"# define knn.\nclass knn:\n    \"\"\"\n    Set attributes for KNN Function.\n    \"\"\"\n    def __init__(self, n_neighbors = 5, p=2):\n        self.n_neighbors = n_neighbors\n        self.p = p\n        \n    # function that returns the suitable label from a set of tied labels.\n    def choose_label(self):\n        \"\"\"\n        **Input should be of numpy array format**\n        tied_labels: labels which are tied.\n        nearest_labels: labels of records which are nearest to test sample\n        distance_array: distance of test sample from nearest records.\n        Output: \n            Relevant label\n        \"\"\"\n        # Return a dictionary of labels and their indices.\n        labels_idx = {label: np.argwhere(self.nearest_labels == label).ravel() for label in self.tied_labels}\n        # Return an array of tied labels and their avg distance.\n        labels_dist = np.array([[label, (self.distance_array[label_val]).sum()] for label, label_val in labels_idx.items()])\n        # now check if there are any duplicacy in labels avg distances.\n        unique_dist, cts = np.unique(labels_dist[:, 1], return_counts = True)\n        if np.any(cts - 1):\n            # get indexes of minimum distances and their corresponding labels\n            min_dist_tied_labels = (labels_dist[:, 0])[np.argwhere(labels_dist[:, 1] == np.argmin(unique_dist)).ravel()]\n            return min_dist_tied_labels[np.random.randint(0, len(min_dist_tied_labels))]\n        return (labels_dist[:, 0])[np.argmin(labels_dist[:, 1])]\n    \n    def fit(self, X, y):\n        \"\"\"\n        store X and y into self and no fitting as the dataset is the model here...\n        \"\"\"\n        self.X = X\n        self.y = y\n        return self\n    \n    # define Methods = predict, as there is nothing to fit here.\n    def predict(self, x_test):\n        \"\"\"\n        x_test: Expected array of 2D, if not expand it dimension.\n        \"\"\"\n        if x_test.ndim == 1:\n            x_test = np.expand_dims(x_test, 0)\n        # empty list.\n        test_label = []\n        for test_sample in x_test:\n            # distance of test sample from all train data points\n            dist_diff = norm((self.X - test_sample), self.p, axis = 1)\n            # chose n_neighbors minimum distances and return their index\n            min_idx = np.argpartition(dist_diff, self.n_neighbors)[:self.n_neighbors]\n            # choose the corresponding labels.\n            self.nearest_labels = self.y[min_idx]\n            # count the occurrences of labels\n            unique_labels, counts = np.unique(self.nearest_labels, return_counts = True)\n            # sort the counts in decreasing order and obtain the indices...\n            counts_idx = np.flip(np.argsort(counts))\n            # Arrange the class labels accordingly...\n            unique_labels = unique_labels[counts_idx]\n            # now check if the entries in counts_idx are unique... if not then there is some tie going on.\n            _, counts_of_count = np.unique(counts[counts_idx], return_counts = True)\n            if np.any(counts_of_count - 1): #check if any entries in counts have occurences > 1.\n                # tied labels\n                self.tied_labels = unique_labels[:counts_of_count[0]]# since counts is sorted descending.\n                self.dist_arr = dist_diff[min_idx]\n                # return the suitable one from these tied labels.\n                test_label.append(self.choose_label())\n            else:\n                test_label.append(unique_labels[0])\n        return np.array(test_label)","45e28a59":"# create an instance of KNN\nknn_classif = knn(n_neighbors = 5)\n# fit the model...\nknn_classif = knn_classif.fit(X_train, y_train)","1c4cf195":"# Let's predict from the test data.\npredictions = knn_classif.predict(X_test).astype(np.object)","c63dffe9":"plt.figure(figsize = (25, 30))\ncm = confusion_matrix(y_test, predictions, labels=['Iris-virginica', 'Iris-versicolor', 'Iris-setosa'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Iris-virginica', 'Iris-versicolor', 'Iris-setosa'])\ndisp.plot()\nplt.title(\"Confusion Matrix with our version of Code.\")\nplt.show()","151c8e9d":"print(\"Accuracy on test data: %.3f\"%((6+12+11)\/(6+12+11+1)))","f44d7e1a":"from sklearn.neighbors import KNeighborsClassifier","798b31c9":"# Construct a KNN object\nneigh = KNeighborsClassifier(n_neighbors=5)\n# fit\nneigh.fit(X_train, y_train)\n# predict\npredictions_sk = neigh.predict(X_test)","2f6bc44a":"# Lets plot the confusion matrix.\nplt.figure(figsize = (25, 30))\ncm_sk = confusion_matrix(y_test, predictions_sk, labels=['Iris-virginica', 'Iris-versicolor', 'Iris-setosa'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_sk, display_labels=['Iris-virginica', 'Iris-versicolor', 'Iris-setosa'])\ndisp.plot()\nplt.title(\"Confusion Matrix with Scikit Version.\")\nplt.show()","3059a9fb":"# Again the accuracy...\nprint(\"Accuracy on test data with sklearn model: %.3f\"%((6+12+11)\/(6+12+11+1)))","95495b8f":"### Train the model","94082311":"### Display confusion matrix","1ac261af":"### KNN Classifier Function Class","bb0466e1":"# KNN Classifier","7a5feafe":"### With Sklearn KNN class, let's see what results we're seeing.","0cfe9b7a":"### Dataset","f2bdb892":"### Accuracy of constructed KNN on test-set.\n$$accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$$","ba90445e":"# Import Essential Libraries"}}