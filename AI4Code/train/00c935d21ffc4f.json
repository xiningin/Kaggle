{"cell_type":{"6f7b5da7":"code","4fbbc09b":"code","fe713d84":"code","9518a816":"code","f3927fd5":"code","14f32776":"code","156f1cea":"code","efdee9c8":"code","f764f651":"code","3f8ccd5e":"code","cf0532e6":"code","6581a167":"code","fc40aabc":"code","0824e07c":"code","e82ce985":"code","f8589607":"code","e65f70fb":"code","5884479b":"code","a458a654":"code","0c949d95":"code","b27eae73":"code","6f8bc0c6":"code","384a083f":"code","6e0a8251":"code","da703b3b":"code","13021674":"code","0051eaac":"code","e11468ce":"code","204d03af":"code","86c6d63d":"code","8aa03f0a":"code","fb4831d6":"code","e1955e8a":"code","9d6057ce":"code","6e5794f0":"code","226c00aa":"code","4be5f132":"code","9b47ea10":"code","7c035b48":"code","f359c277":"code","2df5c998":"code","0b2ea3f3":"code","5c4b9d64":"markdown","2151a9eb":"markdown","c82fda9f":"markdown","53028e77":"markdown","0e3ac44b":"markdown","a7436d5c":"markdown","b8823b85":"markdown","d0290236":"markdown","7778497f":"markdown","e0c19043":"markdown","174e4ae3":"markdown","5e7f2940":"markdown","c439ce0f":"markdown","6c85dbb0":"markdown","f68bf0b5":"markdown","da313b55":"markdown","793404d7":"markdown","f2601ade":"markdown","5ca53c5f":"markdown","1d4dec71":"markdown","e2c9b481":"markdown","07c52015":"markdown","1db59d6b":"markdown","a1143aab":"markdown","6e746433":"markdown","9794d18e":"markdown","2059a8bb":"markdown","cb59b768":"markdown","7dc2b297":"markdown","87f0cbab":"markdown","662c82dc":"markdown","baae04fe":"markdown"},"source":{"6f7b5da7":"!pip install textstat","4fbbc09b":"#Loading libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n\nimport re\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict,Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nimport string\nnltk.download('stopwords')\n\n\nstop=set(stopwords.words('english'))\nplt.style.use('seaborn')\n\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport textstat\nfrom textblob import TextBlob \nfrom tqdm import tqdm\nfrom statistics import *\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go","fe713d84":"train=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')\n\n\n\ndf=pd.concat([train,test])\ndf['OriginalTweet']=df['OriginalTweet'].astype(str)\ndf['Sentiment']=df['Sentiment'].astype(str)\n\ntrain['OriginalTweet']=train['OriginalTweet'].astype(str)\ntrain['Sentiment']=train['Sentiment'].astype(str)\n\ntest['OriginalTweet']=test['OriginalTweet'].astype(str)\ntest['Sentiment']=test['Sentiment'].astype(str)\ndf.head()","9518a816":"print('Training Set Shape = {}'.format(train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test.memory_usage().sum() \/ 1024**2))\nprint(\"\\n\")\nprint(train.head())\nprint(\"\\n\")\nprint(train.info())","f3927fd5":"# Drop duplicates\ntrain.drop_duplicates()\nprint(\" Shape of dataframe after dropping duplicates: \", df.shape)","14f32776":"#Null values\n\nnull= df.isnull().sum().sort_values(ascending=False)\ntotal =df.shape[0]\npercent_missing= (df.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data)\n","156f1cea":"#Missing data as white lines \nimport missingno as msno\nmsno.matrix(df,color=(0.3,0.36,0.44))","efdee9c8":"print('Total tweets in this data: {}'.format(df.shape[0]))\nprint('Total Unique Users in this data: {}'.format(df['UserName'].nunique()))","f764f651":"print(df.Sentiment.unique())\nprint(df.Sentiment.value_counts())","3f8ccd5e":"# We will copy the text in another column so that the original text is also there for comparison\n\ndf['text'] = df.OriginalTweet\ndf[\"text\"] = df[\"text\"].astype(str)\n\ntrain['text'] = train.OriginalTweet\ntrain[\"text\"] = train[\"text\"].astype(str)\n\ntest['text'] = test.OriginalTweet\ntest[\"text\"] = test[\"text\"].astype(str)\n\n# Data has 5 classes, let's convert them to 3\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"positive\"\n    elif x == \"Extremely Negative\":\n        return \"negative\"\n    elif x == \"Negative\":\n        return \"negative\"\n    elif x ==  \"Positive\":\n        return \"positive\"\n    else:\n        return \"neutral\"\n    \ndf['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\ntrain['sentiment']=train['Sentiment'].apply(lambda x:classes_def(x))\ntest['sentiment']=test['Sentiment'].apply(lambda x:classes_def(x))\ntarget=df['sentiment']\n\ndf.sentiment.value_counts(normalize= True)\n    ","cf0532e6":"class_df = df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\nclass_df.style.background_gradient(cmap='winter')","6581a167":"percent_class=class_df.text\nlabels= class_df.sentiment\n\ncolors = ['#17C37B','#F92969','#FACA0C']\n\nmy_pie,_,_ = plt.pie(percent_class,radius = 1.2,labels=labels,colors=colors,autopct=\"%.1f%%\")\n\nplt.setp(my_pie, width=0.6, edgecolor='white') \n\nplt.show()\n\n","fc40aabc":"fig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\nx=train.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='train'),row=1,col=1)\nx=test.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='test'),row=1,col=2)","0824e07c":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train[train['sentiment']==\"positive\"]['text'].str.len()\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\n\ntweet_len=train[train['sentiment']==\"negative\"]['text'].str.len()\nax2.hist(tweet_len,color='#F92969')\nax2.set_title('Negative Sentiments')\n\ntweet_len=train[train['sentiment']==\"neutral\"]['text'].str.len()\nax3.hist(tweet_len,color='#FACA0C')\nax3.set_title('Neutral Sentiments')\n\nfig.suptitle('Characters in tweets')\nplt.show()","e82ce985":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ndf['length'] = df['text'].apply(length)\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(df[df['sentiment'] == \"neutral\"]['length'], alpha = 0.5, bins=bins, label='neutral')\nplt.hist(df[df['sentiment'] == \"positive\"]['length'], alpha = 0.7, bins=bins, label='positive')\nplt.hist(df[df['sentiment'] == \"negative\"]['length'], alpha = 0.8, bins=bins, label='negative')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","f8589607":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train[train['sentiment']==\"positive\"]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\n\n\ntweet_len=train[train['sentiment']==\"negative\"]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='#F92969')\nax2.set_title('Negative Sentiments')\n\ntweet_len=train[train['sentiment']==\"neutral\"]['text'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len,color='#FACA0C')\nax3.set_title('Neutral Sentiments')\n\nfig.suptitle('Words in a tweet')\nplt.show()","e65f70fb":"fig,(ax1,ax2, ax3)=plt.subplots(1,3,figsize=(15,5))\n\nword=train[train['sentiment']==\"positive\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='#17C37B')\nax1.set_title('Positive')\n\n\nword=train[train['sentiment']==\"negative\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='#F92969')\nax2.set_title('Negative')\n\nword=train[train['sentiment']==\"neutral\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='#FACA0C')\nax3.set_title('Neutral')\n\n\nfig.suptitle('Average word length in each tweet')","5884479b":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['sentiment']==target ]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","a458a654":"np.array(stop)","0c949d95":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n\nfor val in stop: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = \"white\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","b27eae73":"corpus=create_corpus(\"positive\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1     \n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#17C37B')","6f8bc0c6":"corpus=create_corpus(\"negative\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n          \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#F92969')","384a083f":"corpus=create_corpus(\"neutral\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n               \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#FACA0C')","6e0a8251":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"positive\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='#17C37B')","da703b3b":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"negative\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n                \nx,y=zip(*dic.items())\nplt.bar(x,y, color='#F92969')","13021674":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"neutral\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='#FACA0C')","0051eaac":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","e11468ce":"sns.barplot(x=y,y=x)","204d03af":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\ndf['hash']=df['text'].apply(lambda x:find_hash(x))\ntemp=df['hash'].value_counts()[:][1:11]\ntemp= temp.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\nsns.barplot(x=\"Hashtag\",y=\"count\", data = temp)","86c6d63d":"from matplotlib import cm\nfrom math import log10\n\nlabels = df['hash'].value_counts()[:][2:11].index.tolist()\ndata = df['hash'].value_counts()[:][2:11]\n\ndf['hash'].value_counts()[:][1:11].index.tolist()\n#number of data points\nn = len(data)\n#find max value for full ring\nk = 10 ** int(log10(max(data)))\nm = k * (1 + max(data) \/\/ k)\n\n#radius of donut chart\nr = 1.5\n#calculate width of each ring\nw = r \/ n \n\n#create colors along a chosen colormap\ncolors = [cm.terrain(i \/ n) for i in range(n)]\n\n#create figure, axis\nfig, ax = plt.subplots()\nax.axis(\"equal\")\n\n#create rings of donut chart\nfor i in range(n):\n    #hide labels in segments with textprops: alpha = 0 - transparent, alpha = 1 - visible\n    innerring, _ = ax.pie([m - data[i], data[i]], radius = r - i * w, startangle = 90, labels = [\"\", labels[i]], labeldistance = 1 - 1 \/ (1.5 * (n - i)), textprops = {\"alpha\": 0}, colors = [\"white\", colors[i]])\n    plt.setp(innerring, width = w, edgecolor = \"white\")\n\nplt.legend()\nplt.show()","8aa03f0a":"\ndef mentions(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ndf['mentions']=df['text'].apply(lambda x:mentions(x))\n\ntemp=df['mentions'].value_counts()[:][1:11]\ntemp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n\nsns.barplot(x=\"Mentions\",y=\"count\", data = temp)","fb4831d6":"b = df['mentions'].value_counts()[:][1:11].index.tolist()\na = df['mentions'].value_counts()[:][1:11].tolist()\nrow = pd.DataFrame({'scenario' : []})\nrow[\"scenario\"] = b\nrow[\"Percentage\"] = a\nfig = px.treemap(row, path= [\"scenario\"], values=\"Percentage\",title='Tree of Mentions')\nfig.show()","e1955e8a":"#Remove Urls and HTML links\ndef remove_urls(text):\n    url_remove = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\ndf['text_new']=df['text'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text_new'].apply(lambda x:remove_html(x))\n\n","9d6057ce":"\n# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['text_new']=df['text'].apply(lambda x:lower(x))\n\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['text']=df['text_new'].apply(lambda x:remove_num(x))\n\n","6e5794f0":"\n#Remove stopwords & Punctuations\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['text_new']=df['text'].apply(lambda x:punct_remove(x))\n\n\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['text']=df['text_new'].apply(lambda x:remove_stopwords(x))\n\n","226c00aa":"#Remove mentions and hashtags\ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['text_new']=df['text'].apply(lambda x:remove_mention(x))\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['text']=df['text_new'].apply(lambda x:remove_hash(x))\n","4be5f132":"\n#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['text_new']=df['text'].apply(lambda x:remove_space(x))\n\ndf = df.drop(columns=['text_new'])","9b47ea10":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\n\ndf_pos = df[df[\"sentiment\"]==\"positive\"]\ndf_neg = df[df[\"sentiment\"]==\"negative\"]\ndf_neu = df[df[\"sentiment\"]==\"neutral\"]\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df_pos.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n   \n\nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greens\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Sentiment',fontsize=35);\n\ncomment_words = ''\n\nfor val in df_neg.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\n\n\n\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Reds\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)  \nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\n\n\ncomment_words = ''\nfor val in df_neu.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud3 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greys\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutal Sentiment',fontsize=35);","7c035b48":"del df_pos\ndel df_neg\ndel df_neu","f359c277":"# Define functions\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 30\n\n\n\npositive= train[\"sentiment\"]== \"positive\"\nnegative= train[\"sentiment\"]== \"negative\"\nneutral= train[\"sentiment\"]== \"neutral\"\n\npositive_unigrams = defaultdict(int)\nneutral_unigrams = defaultdict(int)\nnegative_unigrams = defaultdict(int)\n\n# Unigrams\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet):\n        positive_unigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet):\n        negative_unigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet):\n        neutral_unigrams[word] += 1        \n        \ndf_positive_unigrams = pd.DataFrame(sorted(positive_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_unigrams = pd.DataFrame(sorted(negative_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_unigrams = pd.DataFrame(sorted(neutral_unigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=150)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_unigrams[0].values[:N], x=df_positive_unigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_unigrams[0].values[:N], x=df_negative_unigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_unigrams[0].values[:N], x=df_neutral_unigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common unigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","2df5c998":"# Bigrams\npositive_bigrams = defaultdict(int)\nneutral_bigrams = defaultdict(int)\nnegative_bigrams = defaultdict(int)\n\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        positive_bigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        negative_bigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        neutral_bigrams[word] += 1        \n        \ndf_positive_bigrams = pd.DataFrame(sorted(positive_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_bigrams = pd.DataFrame(sorted(negative_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_bigrams = pd.DataFrame(sorted(neutral_bigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_bigrams[0].values[:N], x=df_positive_bigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_bigrams[0].values[:N], x=df_negative_bigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_bigrams[0].values[:N], x=df_neutral_bigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common bigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","0b2ea3f3":"# Trigrams\npositive_trigrams = defaultdict(int)\nneutral_trigrams = defaultdict(int)\nnegative_trigrams = defaultdict(int)\n\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        positive_trigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        negative_trigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        neutral_trigrams[word] += 1        \n        \ndf_positive_trigrams = pd.DataFrame(sorted(positive_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_trigrams = pd.DataFrame(sorted(negative_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_trigrams = pd.DataFrame(sorted(neutral_trigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_trigrams[0].values[:N], x=df_positive_trigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_trigrams[0].values[:N], x=df_negative_trigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_trigrams[0].values[:N], x=df_neutral_trigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common trigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common trigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","5c4b9d64":"<font size=\"+2\" color=\"Red\"><b>Please Upvote if you like the work<\/b><\/font>\n\n### It gives motivation to a working professional (like me) to contribute more.","2151a9eb":"\n\n<a id=\"10\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>10. Mentions<\/b><\/font><br>\n\n","c82fda9f":"## Unigrams","53028e77":"\n\n<a id=\"12\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>12. Wordclouds<\/b><\/font><br>\n","0e3ac44b":"\n\n<a id=\"3\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. Number of characters<\/b><\/font><br>\n","a7436d5c":"Contents:\n\n* [1. Data](#1)\n* [2. Class Distribution](#2)\n* [3. Number of characters](#3)\n* [4. Number of words in a tweet](#4)\n* [5. Avg. word length in a tweet](#5)\n* [6. Common Stop-words](#6)\n* [7. Punctuations](#7)\n* [8. Common words](#8)\n* [9. Hashtagss](#9)\n* [10. Mentions](#10)\n* [11. Basic pre-processing](#11)\n* [12. Wordclouds](#12)\n* [13. N-grams](#13)\n\n\n","b8823b85":"# Tri-grams","d0290236":"\n\n<a id=\"9\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>9. Hashtags<\/b><\/font><br>\n","7778497f":"<a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Data<\/b><\/font><br>","e0c19043":"# EDA, Metafeatures & Viz","174e4ae3":"### There is uneven distribution of classes with positive taking the largest of pie followed by negative.\n## The colors used above will represent the classes ahead.","5e7f2940":"<a id=\"2\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Class Distribution<\/b><\/font><br>\n","c439ce0f":"#### Common words feature punctuations, we need extensive data cleaning","6c85dbb0":"\n\n<a id=\"11\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>11. Pre-processing<\/b><\/font><br>\n\n","f68bf0b5":"\n\n<a id=\"8\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>8. Common Words<\/b><\/font><br>\n\n","da313b55":"#### Same pattern of uneven distribution in both train and test data","793404d7":"\n\n<a id=\"4\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Number of words in a tweet<\/b><\/font><br>\n\n\n\n","f2601ade":"\n\n<a id=\"6\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>6. Common Stopwords in the tweets<\/b><\/font><br>\n\n","5ca53c5f":"# Let's get started..........\n![](https:\/\/habrastorage.org\/webt\/t6\/sr\/jr\/t6srjrmjjmm6qn8gpld9emy4txu.gif)","1d4dec71":"#### Null values in location","e2c9b481":"# **About this notebook:**\n\n\n\n![](https:\/\/media1.tenor.com\/images\/ed7cffc243c6a6ffe63058e79d1ea0ac\/tenor.gif?itemid=16735375)\n\n\nThis notebook aims at presenting EDA & Viz on Covid-19 tweets. The emphasis has been on the text (tweets) and its in-depth analysis for pre-processing. The modeling will be done in a separate notebook.\nThis notebook is prepared on the Covid-19 tweets which are tagged manually from Highly Negative to Highly Positive - i.e. five classes. In this EDA we will change them to 3 classes (Positive, Negative & Neutal).","07c52015":"<font size=\"+3\" color=\"Green\"><b>Please Upvote if you liked the work<\/b><\/font>","1db59d6b":"#### More cleaning to be done","a1143aab":"#### While lower casing is required, we can see that hashtags contain keywords related to coronavirus ","6e746433":"\n\n<a id=\"5\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Average word length in a tweet<\/b><\/font><br>\n\n\n\n\n","9794d18e":"\n\n<a id=\"13\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>13. N-grams<\/b><\/font><br>\n","2059a8bb":"## Bi grams","cb59b768":"<font size=\"+3\" color=\"Green\"><b>Future Work:<\/b><\/font>\n\n<font size=\"+1\" color=\"Black\"><b>The modeling kernel notebook will soon be published here (link).\nIf you like my work then please leave an upvote, I am working professional (in data science and analytics) and take my time out to prepare these. Soon I will be preparing tutorials and notebooks on Market Mix Modeling which is seeing a rapid rise in the industry. \nYou can also leave the suggestions in the comment box.\nHappy learning. :)<\/b><\/font>\n","7dc2b297":"\n![#Precious](https:\/\/i.imgur.com\/5YSC6pg.gif)","87f0cbab":"<font size=+4 color=\"Black\"><center><b>EDA & Viz for Covid-19 tweets<\/b><\/center><\/font>\n<font size=-1 color=\"Black\"><center><b>* preprocessing & modeling to follow<\/b><\/right><\/font>","662c82dc":"#### A lot stopwords are present. Require preprocessing","baae04fe":"\n\n<a id=\"7\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>7. Punctuations<\/b><\/font><br>\n\n \n\n\n"}}