{"cell_type":{"91a2dd27":"code","214e679c":"code","b1f8ba0b":"code","1861e03f":"code","306e030e":"code","bccd7184":"code","2af678f0":"code","4080f2df":"code","85cbfce4":"code","48d0c99c":"code","62297134":"code","46348a92":"code","218dbc22":"code","00371d9a":"code","f0167f62":"code","78757aa3":"code","8fec6994":"code","18648f45":"code","ba424ede":"code","7bf5fdcf":"code","7ef640d3":"code","4a446fd6":"code","21d5715a":"code","0e683e64":"code","73948fd9":"code","cfb463ef":"code","b4a39ab0":"code","b7cf24c6":"code","991465d7":"code","34a9e6c8":"code","63184443":"code","c4bec2ba":"code","015f35c7":"code","8cc0b477":"code","27b75ff6":"code","30196d3f":"markdown","56a94daf":"markdown","fd60a96f":"markdown","a27e2e26":"markdown","66df10c7":"markdown","722bf055":"markdown","769f67fd":"markdown","41a8f171":"markdown","5bd1d60b":"markdown","d8e563b5":"markdown","227391db":"markdown","6e71c810":"markdown","e0229a41":"markdown","1da66144":"markdown","63cd536f":"markdown","97089013":"markdown","ffcb6f48":"markdown","2c38278a":"markdown","ca81a2e2":"markdown","92024a4c":"markdown","e39521df":"markdown","2a00eab4":"markdown","48dc86a2":"markdown","e51794b4":"markdown","669f2620":"markdown","cc809081":"markdown","7141e577":"markdown"},"source":{"91a2dd27":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport time\nimport datetime\nimport gc\nimport random\nimport re\nimport operator\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,TensorDataset,Dataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.optim.optimizer import Optimizer\n\nfrom keras.preprocessing.text import Tokenizer,text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef seed_everything(SEED=42):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    os.environ['PYTHONHASHSEED']=str(SEED)\n    # torch.backends.cudnn.benchmark = False\n\ndef init_func(worker_id):\n    np.random.seed(SEED+worker_id)\n\n    \ntqdm.pandas()\nSEED=42\nseed_everything(SEED=SEED)","214e679c":"%%time\ndef load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/quora-insincere-questions-classification\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index\n\nglove = '..\/input\/quora-insincere-questions-classification\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n\nprint(\"Extracting GloVe embedding\")\nembeddings_dict_glove= load_embed(glove)\nprint(\"Number of embeddings loaded:\",len(embeddings_dict_glove))","b1f8ba0b":"path=\"..\/input\/\"\ntrain=pd.read_csv(path+\"hackereathmlinterntest\/756269323c1011e9\/dataset\/hm_train.csv\")\ntest=pd.read_csv(path+\"hackereathmlinterntest\/756269323c1011e9\/dataset\/hm_test.csv\")\nsample=pd.read_csv(path+\"hackereathmlinterntest\/756269323c1011e9\/dataset\/sample_submission.csv\")\n\nprint(train.shape,test.shape,sample.shape)\ntrain.head()","1861e03f":"test.head()","306e030e":"print(\"For ID\",train['hmid'].nunique()==train.shape[0])\nprint(\"For text\",train['cleaned_hm'].nunique()==train.shape[0])","bccd7184":"print(\"For ID\",train['hmid'].nunique()==train.shape[0])\nprint(\"For text\",train['cleaned_hm'].nunique()==train.shape[0])","2af678f0":"train_id=train['hmid']\ntrain.drop(columns=['hmid'],inplace=True)\nprint(train.shape)\ntrain.head()","4080f2df":"# dropping the duplicates \ntrain.drop_duplicates(inplace=True)\nprint(train.shape)\ntrain.head()","85cbfce4":"sns.countplot(train['predicted_category'])\nplt.xticks(rotation='90')\nplt.show()","48d0c99c":"target_info=train['predicted_category'].value_counts().reset_index()\ntarget_info['percentage']=(target_info['predicted_category']\/train.shape[0]*100).astype(str)+\" %\"\ntarget_info","62297134":"plt.figure(figsize=(15,5))\nplt.subplot(131)\nsns.distplot(train['num_sentence'],kde=False)\nplt.title(\"Train Distribution\")\nplt.yscale(\"log\")\nplt.subplot(132)\nsns.boxplot(train['predicted_category'],y=train['num_sentence'])\nplt.xticks(rotation=\"90\")\nplt.subplot(133)\nsns.distplot(test['num_sentence'],kde=False)\nplt.title(\"Test Distribution\")\nplt.yscale(\"log\")\nplt.show()","46348a92":"plt.figure(figsize=(15,5))\nplt.subplot(131)\nsns.countplot(train['reflection_period'])\nplt.title(\"Train Distribution\")\nplt.subplot(132)\nsns.countplot(train['reflection_period'],hue=train['predicted_category'])\nplt.subplot(133)\nsns.countplot(test['reflection_period'])\nplt.title(\"Test Distribution\")\nplt.tight_layout()\nplt.show()","218dbc22":"train['num_words']=train['cleaned_hm'].apply(lambda x:len(x.split()))\ntest['num_words']=test['cleaned_hm'].apply(lambda x:len(x.split()))\nprint(train.shape,test.shape)\ntrain.head()","00371d9a":"print(\"The average word length in train is\",train['num_words'].mean(),\n                      \"and in test is\",test['num_words'].mean())\n\nplt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.distplot(train['num_words'],kde=False)\nplt.yscale(\"log\")\nplt.title(\"Train Distribution\")\nplt.subplot(122)\nsns.distplot(test['num_words'],kde=False)\nplt.yscale(\"log\")\nplt.title(\"Test Distribution\")\nplt.show()","f0167f62":"print(\"The median word length in train is\",train['num_words'].median(),\n                      \"and in test is\",test['num_words'].median())","78757aa3":"def build_vocab(sentences,verbose=True):\n    vocab={}\n    for sentence in tqdm(sentences,disable=(not verbose)):\n        for word in sentence:\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n                \n    print(\"Number of words found in vocab are\",len(vocab.keys()))\n    return dict(sorted(vocab.items(), key=operator.itemgetter(1))[::-1])\n\ndef sen(x):\n    return x.split()\n\ndef check_coverage(vocab,embeddings_dict):\n    # words that dont have embeddings\n    oov={}\n    # stores words that have embeddings\n    a=[]\n    i=0\n    k=0\n    for word in tqdm(vocab.keys()):\n        if embeddings_dict.get(word) is not None:                    # implies that word has embedding\n            a.append(word)\n            k=k+vocab[word]\n        else:\n            oov[word]=vocab[word]\n            i=i+vocab[word]\n    \n    print(\"Total embeddings found in vocab are\",len(a)\/len(vocab)*100,\"%\")\n    print(\"Total embeddings found in text are\",k\/(k+i)*100,\"%\")\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return dict(sorted_x)","8fec6994":"puncts=[',','.','!','$','(',')','%','[',']','?',':',\";\",\"#\",'\/','\"',\"'\",\"-\",\"|\",'*']\n\ncontraction_mapping={\"haven't\":\"have not\",\"hadn't\":\"had not\",\"wasn't\":\"was not\",\"he's\":\"he is\",\n                     \"couldn't\":\"could not\",\"she's\":\"she is\",\"i'm\":\"i am\",\"we've\":\"we have\",\n                     \"wouldn't\":\"would not\",\"That's\":\"That is\",\"we're\":\"we are\",\"isn't\":\"is not\",\n                     \"hasn't\":\"has not\",\"they're\":\"they are\",\"She's\":\"She is\",\"He's\":\"He is\",\"weren't\":\"were not\",\n                    \"there's\":\"there is\",\"i've\":\"i have\",\"you've\":\"you have\",\"We've\":\"We have\",\"we'd\":\"we would\",\n                    \"We're\":\"We are\",\"who's\":\"who is\",\"they'll\":\"they will\",\"what's\":\"what is\",\"she'd\":\"she would\",\n                    \"They're\":\"They are\",\"aren't\":\"are not\",\"shouldn't\":\"should not\",\"There's\":\"There is\",\n                     \"we'll\":\"we will\",\"I`m\":\"I am\",\"You're\":\"You are\",\"i'd\":\"i would\",\"he'll\":\"he will\",\n                    \"they'd\":\"they would\",\"Didn't\":\"Did not\",\"CAN'T\":\"CANNOT\",\"THAT'S\":\"That is\",\"you;ll\":\"you will\",\n                    \"You'll\":\"You will\",\"Can't\":\"cannot\",\"would've\":\"would have\",\"you\\'re\":\"you are\",\"i'll\":\"i will\",\n                    \"DIDN'T\":\"did not\",\"film\/theater\":\"film or theater\",\"that\\'s\":\"that is\",\"Let's\":\"Lets\",\"We'd\":\"We would\",\n                    \"They've\":\"They have\",\"she'll\":\"she will\",\"Haven't\":\"Have not\",\"it'll\":\"it will\",\"you'd\":\"you would\",\n                    \"I'VE\":\"I have\",\"I`ve\":\"I have\",\"She'll\":\"She will\",\"It'll\":\"It will\",\"Hadn't\":\"Had not\",\"I\\'ve\":\"I have\",\n                     \"You\\'re\":\"You are\",\"b'day\":\"birthday\",\"DON'T\":\"Do not\",\"it'd\":\"it would\",\"You've\":\"You have\",\n                     \"I'LL\":\"I will\",\"don\\'t\":\"do not\",\"what\\'s\":\"what is\",\"won't\":\"will not\",\n                    \"he'd\":\"he would\",\"I'M\":\"I AM\"}\n\nmisspelled_words={\"fiancA\":\"fiance\",\"couldnat\":\"could not\",\"giftz\":\"gifts\",\n                \"othersa\":\"others\",\"nerous\":\"nervous\",\"wasnat\":\"was not\",\n                \"aIam\":\"I am\",\"arace\":\"a race\",\"10class\":\"10 class\",\n                 \"aHow\":\"How\",\"aWait\":\"Wait\",\"aMumma\":\"Mumma\",\"aWhy\":\"Why\",\n                \"B+\":\"B +\",\"INAGURATION\":\"INAUGURATION\",\"wonat\":\"will not\",\n                \"3+\":\"3 +\",\"Thereas\":\"There is\",\"Letas\":\"Let's\",\n                \"Valentineas\":\"Valentines\",\"genervous\":\"generous\",\n                 \"brotheras\":\"brother's\",\"Dhubai\":\"Dubai\",\n                 \"shiridi\":\"shirdi\",\"PAPPER\":\"PAPER\",\"booka|\":\"book\",\n                 \"aHey\":\"Hey\",\"seek&hide\":\"seek and hide\"}\n\ndef replace_misspelled(x):\n    for word in misspelled_words.keys():\n        x=x.replace(word,misspelled_words[word])\n        \n    return x\n\ndef replace_contraction_mapping(x):\n    for contract in contraction_mapping.keys():\n        x=x.replace(contract,contraction_mapping[contract])\n    \n    return x\n    \ndef replace_puncts(x):\n    for p in puncts:\n        x=x.replace(p,f' {p} ')\n        \n    return x\n\ncleaned_sen=train['cleaned_hm'].progress_apply(replace_misspelled)\ncleaned_sen=cleaned_sen.progress_apply(replace_contraction_mapping)\ncleaned_sen=cleaned_sen.progress_apply(replace_puncts)\nsentences=cleaned_sen.progress_apply(sen)\nvocab=build_vocab(sentences)\noov=check_coverage(vocab,embeddings_dict_glove)\n\n# for index,sen in enumerate(cleaned_sen):\n#     if \"Valentineas\" in sen.split():\n#         print(sen,index)\n#         print(\"\\n\")","18648f45":"# cleaning the train data and test data\n\n# replace the misspelled word\ntrain_clean_hm=train['cleaned_hm'].progress_apply(replace_misspelled)\ntest_clean_hm=test['cleaned_hm'].progress_apply(replace_misspelled)\n\n# replace contraction mapping\ntrain_clean_hm=train_clean_hm.progress_apply(replace_contraction_mapping)\ntest_clean_hm=test_clean_hm.progress_apply(replace_contraction_mapping)\n\n# replace punctuations\ntrain_clean_hm=train_clean_hm.progress_apply(replace_puncts)\ntest_clean_hm=test_clean_hm.progress_apply(replace_puncts)","ba424ede":"max_words=20000\nmax_len=70\nembed_dim=300\n\n\ntokenizer=Tokenizer(num_words=max_words,filters=None,lower=False)\ntokenizer.fit_on_texts(list(train_clean_hm.apply(sen).values))\n\nprint(\"The length of vocabulary is\",len(tokenizer.word_index))\n\nX=tokenizer.texts_to_sequences(list(train_clean_hm.apply(sen).values))\nX_test=tokenizer.texts_to_sequences(list(test_clean_hm.apply(sen).values))\n\n# padding and truncating\nX=np.array(pad_sequences(X,maxlen=max_len,padding='pre',truncating='pre'))\nX_test=np.array(pad_sequences(X_test,maxlen=max_len,padding='pre',truncating='pre'))\n\n\n# target\ntarget_encoder={\"affection\":0,\"achievement\":1,\n                \"bonding\":2,\"enjoy_the_moment\":3,\n               \"leisure\":4,\"nature\":5,\n               \"exercise\":6}\n\ntarget_decoder=dict(zip(target_encoder.values(),target_encoder.keys()))\n\ny=pd.get_dummies(train['predicted_category'].map(target_encoder)).values\n\nprint(\"Training Shape\",X.shape,y.shape)\nprint(\"Test Shape\",X_test.shape)","7bf5fdcf":"def give_embed_glove(word_index):\n    nb_words=min(max_words,len(word_index)+1)\n    embeddings_matrix_glove = np.zeros((nb_words, embed_dim))\n    for word,index in word_index.items():\n        if index>=max_words:\n            continue\n        # implies that word has embedding\n        if embeddings_dict_glove.get(word) is not None:\n            embeddings_matrix_glove[index]=embeddings_dict_glove.get(word)\n            \n    \n    return embeddings_matrix_glove\n\n\nembeddings_matrix_glove=give_embed_glove(tokenizer.word_index)\nprint(\"The Shape of the glove matrix is\",embeddings_matrix_glove.shape)","7ef640d3":"def cross_entropy(y_true,y_pred,eps=1e-8):\n    \"\"\"\n    y_true : (m,classes) contaning true values.\n    y_pred : (m,classes) contaning predictions.\n\n    Returns : Cross entropy loss between y_true and y_pred.\n    \"\"\"\n    predictions=np.clip(y_pred,eps,1-eps)\n    m=predictions.shape[0]\n    loss=-np.sum(y_true*np.log(predictions))\/m\n    return loss\n\ndef f1(y_true,y_pred,threshold=0.5,eps=1e-8):\n    \"\"\"    \n    y_true : (m,classes) contaning true values.\n    y_pred : (m,classes) contaning predictions.\n    \n    Returns : Average Weighted F1 Score.\n    \"\"\"\n    predictions=np.argmax(y_pred,axis=1)\n    targets=np.argmax(y_true,axis=1)\n    return f1_score(targets,predictions,average=\"weighted\")","4a446fd6":"class CrossEntropyLoss(nn.Module):\n    \"\"\"\n    y_true = (N,C)\n    y_pred = (N,C)\n    Cross Entropy Loss\n    \"\"\"\n    def __init__(self,eps=1e-8):\n        super(CrossEntropyLoss,self).__init__()\n        self.eps=eps\n    \n    def forward(self,y_true,y_pred):\n        y_pred=torch.clamp(y_pred,self.eps,1-self.eps)\n        m=y_pred.shape[0]\n        loss=-torch.sum(y_true*torch.log(y_pred))\/m\n        \n        return loss","21d5715a":"# code inspired from: https:\/\/github.com\/anandsaha\/pytorch.cyclic.learning.rate\/blob\/master\/cls.py\nclass CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","0e683e64":"class Attention(nn.Module):\n    def __init__(self,hidden_dim,max_len):\n        super(Attention,self).__init__()\n        \n        self.hidden_dim=hidden_dim\n        self.max_len=max_len\n        \n        self.tanh=nn.Tanh()\n        self.linear=nn.Linear(in_features=self.hidden_dim,out_features=1,bias=False)\n        self.softmax=nn.Softmax(dim=1)        \n        \n    def forward(self,h):\n        \n        m=self.tanh(h)\n        \n        alpha=self.linear(m)\n        \n        alpha=torch.squeeze(alpha)           # shape of alpha will be batch_size*max_len\n        \n#         print(\"Alpha shape:\",alpha.shape)\n        \n        # softmax(note that softmax is along dimension 1)\n        alpha=self.softmax(alpha)\n        \n        # unsequezzing alpha to get shape as batch_size*max_len*1\n        alpha=torch.unsqueeze(alpha,-1)\n        \n        # we have to define r\n        r=h*alpha\n        \n        # now we have to take sum and shape of r is batch_size*hidden_size\n        r=torch.sum(r,dim=1)\n        \n        return r","73948fd9":"# model params\nhidden_units=64\n\nclass LstmGru(nn.Module):\n    def __init__(self,embeddings_matrix):\n        super(LstmGru,self).__init__()\n\n        self.embedding=nn.Embedding.from_pretrained(torch.Tensor(embeddings_matrix),freeze=True)\n        \n        self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=hidden_units,\n                              bidirectional=True,batch_first=True)\n        \n        # gru output goes to lstm\n        self.gru=nn.GRU(input_size=2*hidden_units,hidden_size=hidden_units,\n                           bidirectional=True,batch_first=True)\n        \n        # lstm attention\n        self.lstm_attention=Attention(2*hidden_units,max_len)\n        \n        # gru attention\n        self.gru_attention=Attention(2*hidden_units,max_len)\n        \n        \n        self.linear1=nn.Linear(in_features=8*hidden_units,out_features=32)\n        self.batch1=nn.BatchNorm1d(32)\n        self.relu1=nn.ReLU()\n        self.drop1=nn.Dropout(0.25)\n        \n        self.linear2=nn.Linear(in_features=32,out_features=7)\n        self.softmax=nn.Softmax(dim=1)\n        \n        \n    def forward(self,X):\n        batch_size=X.shape[0]\n        \n        embeds=self.embedding(X.long())\n        \n        h_lstm,_=self.lstm(embeds)\n        h_gru,_=self.gru(h_lstm)\n        \n        # max pooling over time\n        h_lstm_max,_=torch.max(h_lstm,1)\n        h_gru_max,_=torch.max(h_gru,1)\n        \n        # mean average pooling over time\n        h_lstm_mean=torch.mean(h_lstm,1)\n        h_gru_mean=torch.mean(h_gru,1)\n        \n        # attention\n        h_lstm_attend=self.lstm_attention(h_lstm)\n        h_gru_attend=self.gru_attention(h_gru)\n\n        h=torch.cat((h_lstm_attend,h_gru_attend,h_lstm_max,h_gru_max),dim=1)\n        \n        output=self.relu1(self.batch1(self.linear1(h)))\n        output=self.drop1(output)\n        \n        output=(self.linear2(output))\n        output=self.softmax(output)\n        \n        return output\n    \ndef initialize_model(embeddings_matrix):\n    model=LstmGru(embeddings_matrix)\n    \n    # setting all the dtypes to float\n    model.float()\n    \n    # pushing the code to gpu\n    model.cuda()\n    \n    # params\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(\"Total trainiable Param's are\",trainable_params)\n    \n    return model","cfb463ef":"initialize_model(embeddings_matrix_glove)","b4a39ab0":"def fit_data(model,optimizer,loss_fn,scheduler=None,\n             train_iterator=None,val_iterator=None,\n            m_train=None,m_val=None,num_classes=None,epochs=None,fname=None):\n    \"\"\"\n        model : pytroch model\n        optimizer : any optimizer from torch.optim\n        m_train : number of training examples\n        m_val : number of validation examples\n        epochs : number of epochs.\n        fname : file name to save the model(it always saves the best)\n        \n        returns\n        best train preds and best val preds(selected based on validation score)\n        and evaluations(which cotains losses,accuracy and f1 score of every epoch)\n    \"\"\"\n    \n    train_loss=[]\n    val_loss=[]\n    train_f1=[]\n    val_f1=[]\n    evals={}\n    \n    best_train_preds=np.zeros((m_train,num_classes))\n    best_val_preds=np.zeros((m_val,num_classes))\n    best_val_f1=0\n    \n    for ep_num in range(epochs):\n        print(\"Epoch\",\"{0}\/{1}:\".format(ep_num+1,epochs))\n        \n        # measuring the current time\n        start=datetime.datetime.now()\n        \n        # iterating through batch\n        train_targets=np.zeros((m_train,num_classes))\n        train_preds=np.zeros((m_train,num_classes))\n    \n        # start train_index\n        train_index=0\n        \n        # setting up the model in train\n        model.train()\n    \n        for batch,(X_train,y_train) in enumerate(train_iterator):\n            optimizer.zero_grad()\n            \n            X_train=Variable(X_train.cuda())\n            y_train=Variable(y_train.type('torch.FloatTensor').cuda())\n            \n            y_pred=model.forward(X_train)\n            \n            if scheduler:\n                scheduler.batch_step()\n                \n            loss=loss_fn(y_train,y_pred)\n            \n            # appending the preds\n            train_preds[train_index:train_index+X_train.shape[0]]=y_pred.cpu().detach().numpy().reshape((-1,num_classes))\n            \n            # appending the targets\n            train_targets[train_index:train_index+X_train.shape[0]]=y_train.cpu().detach().numpy().reshape((-1,num_classes))\n            \n            # backprop\n            loss.backward()\n            \n            # update the weights\n            optimizer.step()\n            \n            train_index=train_index+X_train.shape[0]\n            \n            logger=str(train_index)+\"\/\"+str(m_train)\n\n            print(logger,end='\\r')\n            \n        # setting up in evaluation mode\n        model.eval()\n        \n        val_targets=np.zeros((m_val,num_classes))\n        val_preds=np.zeros((m_val,num_classes))\n        val_index=0\n        \n        for batch,(X_val,y_val) in enumerate(val_iterator):\n            \n            X_val=Variable(X_val.cuda())\n            y_val=Variable(y_val.type('torch.FloatTensor').cuda())\n            \n            y_pred=model.forward(X_val)\n            \n            # appending the preds\n            val_preds[val_index:val_index+X_val.shape[0]]=y_pred.cpu().detach().numpy().reshape((-1,num_classes))\n            \n            # appending the targets\n            val_targets[val_index:val_index+X_val.shape[0]]=y_val.cpu().detach().numpy().reshape((-1,num_classes))\n            \n            val_index=val_index+X_val.shape[0]\n            \n        # finding the losses and f1 score \n        trainloss=cross_entropy(train_targets,train_preds)\n        valloss=cross_entropy(val_targets,val_preds)\n        \n        trainf1=f1(train_targets,train_preds)\n        valf1=f1(val_targets,val_preds)\n        \n        train_loss.append(trainloss),val_loss.append(valloss)\n        train_f1.append(trainf1),val_f1.append(valf1)\n        \n        # end measuring time \n        end=datetime.datetime.now()\n        \n        print(\"Seconds = \",round((end-start).total_seconds()),end=\" \")\n        \n        print(\"train loss = \",round(trainloss,5),end=\" \")\n        print(\"train f1 = \",round(trainf1,5),end=\" \")\n\n        print(\"val loss = \",round(valloss,5),end=\" \")\n        print(\"val f1 = \",round(valf1,5))\n        \n        if valf1>best_val_f1:\n            print(\"Validation F1 score increased from\",round(best_val_f1,5),\"to\",round(valf1,5),\\\n                                      \"Saving the model at\",fname)\n            \n            torch.save(model.state_dict(),fname)\n            \n            best_val_f1=valf1\n            best_train_preds=train_preds\n            best_val_preds=val_preds\n        print(\"\\n\")\n    \n    # outside of epoch loop\n    evals['train_loss']=train_loss\n    evals['val_loss']=val_loss\n    evals['train_f1']=train_f1\n    evals['val_f1']=val_f1\n    evals['best_val_f1']=best_val_f1\n    \n    return best_train_preds,best_val_preds,evals","b7cf24c6":"def predict_on_test(model,test_iterator,m_test):\n    # model at evaluation mode\n    model.eval()\n    \n    test_preds=np.zeros((m_test,num_classes))\n    test_index=0\n    \n    start=datetime.datetime.now()\n    \n    for batch,X_test in enumerate(test_iterator):\n        X_test=Variable(X_test[0].cuda())\n        \n        y_pred=model.forward(X_test)\n        # appending the preds\n        test_preds[test_index:test_index+X_test.shape[0]]=y_pred.cpu().detach().numpy().reshape((-1,num_classes))\n        \n        test_index=test_index+X_test.shape[0]\n        \n        logger=str(test_index)+\"\/\"+str(m_test)\n        \n        if batch<len(test_iterator)-1:\n            print(logger,end='\\r')\n        else:\n            print(logger,end=\" \")\n            \n    end=datetime.datetime.now()\n    print(\"Predictions done on test data in\",round((end-start).total_seconds()),\"seconds\")\n    \n    return test_preds","991465d7":"print(\"Training Shape\",X.shape,y.shape)\nprint(\"Testing Shape\",X_test.shape)","34a9e6c8":"def make_dataset(X_train,y_train,X_val,y_val,batch_size):\n    X_train,y_train=torch.Tensor(X_train),torch.Tensor(y_train)\n    X_val,y_val=torch.Tensor(X_val),torch.Tensor(y_val)\n\n    # train dataset and val dataset contains pair of X and y for each example \n    train_dataset=TensorDataset(X_train,y_train)\n    val_dataset=TensorDataset(X_val,y_val)\n    test_dataset=TensorDataset(X_test)\n\n    # now I will pass this to data loader\n    # shuffle set to true imples for every epoch data is shuffled\n    train_iterator=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n    val_iterator=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n    \n    return train_iterator,val_iterator","63184443":"n_folds=5\nnum_classes=7\nkfold=StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=SEED)\n\nscores=np.zeros((n_folds,))\n\n# oof preds \noof_preds=np.zeros((X.shape[0],num_classes))\n# test preds\ntest_preds=np.zeros((X_test.shape[0],num_classes))\n\n\n# batch size and epochs per fold\nbatch_size=64\nep=[8]*n_folds\n\n# making the test iterator  and for predictions the batch size can be more , so that we can predict fast\nX_test=torch.Tensor(X_test)\ntest_dataset=TensorDataset(X_test)\ntest_iterator=DataLoader(test_dataset,batch_size=256,shuffle=False)\nm_test=X_test.shape[0]\n\n# one point to note is that kfold.split works for labels of form [0,1,1,2]\nfor fold,(train_index,val_index) in enumerate(kfold.split(X,np.argmax(y,axis=1))):\n    X_train,X_val=X[train_index],X[val_index]\n    y_train,y_val=y[train_index],y[val_index]\n    \n    m_train,m_val=X_train.shape[0],X_val.shape[0]\n    \n    print(\"================================= FOLD\",fold+1,\"=============================================\")\n\n    print(\"Training Shape:\",X_train.shape,y_train.shape)\n    print(\"Validation Shape:\",X_val.shape,y_val.shape)\n    \n    train_iterator,val_iterator=make_dataset(X_train,y_train,X_val,y_val,batch_size=batch_size)\n    \n    gc.enable()\n    del X_train,y_train,X_val,y_val\n    gc.collect()\n    \n    model=initialize_model(embeddings_matrix_glove)\n    \n    base_lr=1e-3\n    max_lr=1e-2\n    is_scheduler=True\n    if is_scheduler:\n        step_size=1468                   # 2 times the iteration in an epoch\n        optimizer=torch.optim.Adam(model.parameters(),lr=max_lr)\n        scheduler=CyclicLR(optimizer,base_lr=base_lr,max_lr=max_lr,\n                              step_size=step_size,mode='triangular')\n    else:\n        optimizer=torch.optim.Adam(model.parameters(),lr=base_lr)\n        scheduler=None\n        \n    \n    loss_fn=CrossEntropyLoss()\n    epochs=ep[fold]\n    fname=\"LstmGru\"+str(fold+1)+\".pt\"\n    best_train_preds,best_val_preds,evals=fit_data(model,optimizer,loss_fn,scheduler,train_iterator,val_iterator,\n                                              m_train,m_val,num_classes,epochs,fname)\n    print(\"Loading the model\")\n    model=LstmGru(embeddings_matrix_glove)\n    model.float()\n    model.cuda()\n    model.load_state_dict(torch.load(fname))\n    preds=predict_on_test(model,test_iterator,m_test)\n    \n    gc.enable()\n    del model\n    gc.collect()\n    \n    # saving the oof preds\n    oof_preds[val_index]=best_val_preds\n    \n    # test predictions\n    test_preds=test_preds+preds\/n_folds\n    \n    # storing the scores\n    scores[fold]=evals['best_val_f1']","c4bec2ba":"print(\"The F1 Score on the total data is\",f1(y,oof_preds))\nprint(\"\\n\")\nprint(\"The fold scores are\",scores,\"and the mean is\",np.mean(scores),\"and std is\",np.std(scores))","015f35c7":"sub=pd.DataFrame()\nsub['hmid']=test['hmid']\nsub['predicted_category']=pd.Series(np.argmax(test_preds,axis=1)).map(target_decoder)\nsub.head()","8cc0b477":"plt.figure(figsize=(12,5))\nsns.countplot(sub['predicted_category'])\nplt.show()","27b75ff6":"sub.to_csv(\"first_sub.csv\",index=False)","30196d3f":"## NUMBER OF SENTENCES","56a94daf":"## METRICS","fd60a96f":"The code for the analysis written below is removed.\n\nAs you can see the words that dont have embedding are in compact form an not in loose form. For example if haven't was written as have not then there is no problem finding a embedding. As haven't and have not doesn't change the meaning of the sentence we can replace them easily. The mappings are defined below.","a27e2e26":"Here we will check the text.","66df10c7":"## MODEL","722bf055":"So median word length is 14 for train and 13 for test.","769f67fd":"So there are statments with number of sentences nearly 60 (which is very high). And you can see from the other graph that the sentences which have 60 sentences are mostly belong to \"affection\". in test data the num of sentences are even more with approx 70 sentences.","41a8f171":"## EMBEDDINGS MATRIX","5bd1d60b":"## TARGET VARIABLE\n\nDistribution of target variable.","d8e563b5":"## TRAINING","227391db":"So even in the test data we have duplicated rows , but we have to just the predict the category, so it's fine here. Now I will remove duplicated rows in the train dataset. As ID is not at all used even in the future, so I am dropping the ID from the train dataset.","6e71c810":"## SOME USEFUL FUNCTIONS","e0229a41":"## CYCLIC LEARNING RATE","1da66144":"## LOSS","63cd536f":"## TOKENIZING AND PADDING","97089013":"## MAKING THE SUBMISSION FILE","ffcb6f48":"## DATA","2c38278a":"So there are some rows in the dataset which have same text and different ID. So the rows are duplicated only with ID changed. So we need to remove those duplicated rows in the train dataset. Let us see how is the case in test dataset.","ca81a2e2":"## WORD LENGTH\n\n   In this we will examin the word length of the sentences.","92024a4c":"The word length looks same. But there are some sentences for which the length is 1200 which is very high for RNN,LSTM. But in train and test , the average word length is 19 and 20. As there are outliers , it's better we see the median length also. ","e39521df":"## REFLECTION PEROID\n\nIt represents the time of happiness. This variable only takes two values 24h and 3m. Let us see the distribution in train and test and also the relation with target.","2a00eab4":"So we can see easily that from 60321 to the examples dropped to 58682. That's a decrease of 1639 examples.","48dc86a2":"## TEXT","e51794b4":"## DROPPING THE DUPLICATES","669f2620":"## EMBEDDINGS","cc809081":"Number of 3m are more in test data than in train data.","7141e577":"The dataset looks highly imbalanced. There are 7 categories. "}}