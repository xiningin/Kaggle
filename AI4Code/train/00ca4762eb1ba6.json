{"cell_type":{"daa15582":"code","75012f77":"code","67fc3f22":"code","57b10bd4":"code","8baef2d0":"code","1afa27ec":"code","ac21b092":"code","bbfe7598":"code","e867d6e4":"code","a83839db":"code","a43b3e78":"code","4402e0bb":"code","e996d097":"code","d647756c":"code","5ea0fdd3":"code","325911dc":"code","6944bbb1":"code","ea8a5063":"code","6b69cf4b":"code","f27c1e4b":"code","2182c834":"code","32ec888e":"code","0962f743":"code","16668be5":"code","0dd1ca26":"code","94e165ef":"code","9059abcb":"code","8c142dc3":"code","e99b22a5":"code","49dc8c0f":"code","2c3bc408":"code","429803fc":"code","23a9a208":"code","8b130556":"code","1e07edbf":"code","5c081b2e":"code","d297da41":"code","17a7989e":"code","ef5cf59d":"code","b3e15e68":"code","6de2ab1a":"code","2194e3ff":"code","915cf342":"code","cb1964be":"code","94e40385":"code","f711a7eb":"code","5876a84b":"code","e980497b":"code","d4efd8ea":"code","a8dc0246":"code","ae34b859":"code","e2d23255":"code","18f88035":"code","366bcefe":"markdown","098c882d":"markdown","241c8257":"markdown","2b6167ab":"markdown","c88aa4b8":"markdown","ceae3632":"markdown","63008153":"markdown","8d65f26e":"markdown","818830d7":"markdown","b6390dc7":"markdown","ec97730a":"markdown","f323611f":"markdown","127b3fed":"markdown","3c63abb2":"markdown","1a93ee46":"markdown","512760bc":"markdown","9a45628d":"markdown","0bd4750c":"markdown","43493342":"markdown","ddc1fa85":"markdown","3eb07071":"markdown","f8944871":"markdown","f7c3be45":"markdown","b30964a2":"markdown","65a5a693":"markdown","108577ef":"markdown","041723ed":"markdown","4dfba284":"markdown","0950dc94":"markdown"},"source":{"daa15582":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.nlp import *\nfrom sklearn.linear_model import LogisticRegression","75012f77":"??texts_from_folders()","67fc3f22":"PATH='..\/input\/aclimdb\/aclImdb\/'\nnames = ['neg','pos']","57b10bd4":"!pwd","8baef2d0":"!ls -h","1afa27ec":"%ls {PATH}","ac21b092":"%ls {PATH}train","bbfe7598":"%ls {PATH}train\/pos | head","e867d6e4":"trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\nval,val_y = texts_labels_from_folders(f'{PATH}test',names)","a83839db":"print(type(trn),len(trn),len(trn_y),len(val), len(val_y))","a43b3e78":"trn[0]","4402e0bb":"trn_y[0]","e996d097":"val[0]","d647756c":"val_y[0]","5ea0fdd3":"veczr = CountVectorizer(tokenizer=tokenize)","325911dc":"trn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)","6944bbb1":"trn_term_doc","ea8a5063":"trn_term_doc[0]","6b69cf4b":"val_term_doc[0]","f27c1e4b":"print(trn_term_doc.shape[0],'\\n',trn_term_doc.shape[1],'\\n', trn_term_doc[:1].shape[0],'\\n', trn_term_doc[:1].shape[1],'\\n', trn_term_doc[:2])","2182c834":"print(trn_term_doc.shape[1],'\\n', trn_term_doc[24999:25000])","32ec888e":"vocab = veczr.get_feature_names(); vocab[5000:5005]","0962f743":"vocab = veczr.get_feature_names(); vocab[44956:44959] # 44957 10\uac1c?","16668be5":"vocab = veczr.get_feature_names(); vocab[20961:20965] # 20961 3\uac1c","0dd1ca26":"vocab = veczr.get_feature_names(); vocab[50880:50979]","94e165ef":"w0 = set([o.lower() for o in trn[0].split(' ')]); w0","9059abcb":"len(w0)","8c142dc3":"veczr.vocabulary_['absurd']","e99b22a5":"trn_term_doc[0,1297]","49dc8c0f":"trn_term_doc[0,5000]","2c3bc408":"def pr(y_i):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","429803fc":"x=trn_term_doc\ny=trn_y\n\nr = np.log(pr(1)\/pr(0))\nb = np.log((y==1).mean() \/ (y==0).mean())","23a9a208":"pre_preds = val_term_doc @ r.T + b\npreds = pre_preds.T>0\n(preds==val_y).mean()","8b130556":"x=trn_term_doc.sign()\nr = np.log(pr(1)\/pr(0))\n\npre_preds = val_term_doc.sign() @ r.T + b  # \ud55c \ub2e8\uc5b4\uac00 \uc5ec\ub7ec\ubc88 \ub098\uc654\ub290\ub0d0 \ubcf4\ub2e4\ub294 \ud55c\ubc88\uc774\ub77c\ub3c4 \ub098\uc654\ub294\uc9c0\uac00 \uc911\uc694 sign\ucc98\ub9ac\npreds = pre_preds.T>0\n(preds==val_y).mean()","1e07edbf":"m = LogisticRegression(C=1e8, dual=True)\n%time m.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()","5c081b2e":"m = LogisticRegression(C=1e8, dual=True)\n%time m.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()","d297da41":"m = LogisticRegression(C=0.1, dual=True) # dual\uc758 \uc758\ubbf8\ub294 \uc5f4\ubcf4\ub2e4 \ud589\uc774 \ub9ce\uc740 \uacbd\uc6b0\ub294 \uc218\ud559\uc801 \uc7ac\uacf5\uc2dd\ud654 \ucc98\ub9ac\ub97c \uc801\uc6a9\ud558\uc5ec \ube68\ub77c\uc9d0(\uba87\ubd84\uc774 \uba87\ucd08\ub85c..)  \n%time m.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()","17a7989e":"m = LogisticRegression(C=0.1, dual=True)\n%time m.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()","ef5cf59d":"veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n# \uc704 \uad6c\ubb38\uc5d0\uc11c ngram_range\ub97c \uc548 \uc900 \uc0c1\ud0dc\uc5d0\uc11c max_features=800000\ub97c \uc0ac\uc6a9\ud558\uba74,\n# \uc720\ub2c8\uadf8\ub7a8, \ud2b8\ub9ac\uadf8\ub7a8 \uc0c1\uad00\uc5c6\uc774 \ube48\ub3c4\uc218\ub85c \uc815\ub82c\ud558\uc5ec 80\ub9cc\uac1c\uc5d0\uc11c \ucd5c\ube48\uc21c\uc73c\ub85c \uc790\ub984\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)","b3e15e68":"trn_term_doc.shape","6de2ab1a":"vocab = veczr.get_feature_names()","2194e3ff":"vocab[200000:200005]","915cf342":"y=trn_y\nx=trn_term_doc.sign()\nval_x = val_term_doc.sign()","cb1964be":"r = np.log(pr(1) \/ pr(0))\nb = np.log((y==1).mean() \/ (y==0).mean())","94e40385":"m = LogisticRegression(C=0.1, dual=True)\n%time m.fit(x, y);\n\npreds = m.predict(val_x)\n(preds.T==val_y).mean()","f711a7eb":"r.shape, r","5876a84b":"np.exp(r)","e980497b":"x_nb = x.multiply(r)\nm = LogisticRegression(dual=True, C=0.1)\n%time m.fit(x_nb, y);\n\nval_x_nb = val_x.multiply(r)\npreds = m.predict(val_x_nb)\n(preds.T==val_y).mean()","d4efd8ea":"sl=2000","a8dc0246":"# Here is how we get a model from a bag of words\nmd = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)","ae34b859":"learner = md.dotprod_nb_learner()\n%time learner.fit(0.02, 1, wds=1e-6, cycle_len=1)","e2d23255":"%time learner.fit(0.02, 2, wds=1e-6, cycle_len=1)","18f88035":"%time learner.fit(0.02, 2, wds=1e-6, cycle_len=1)","366bcefe":"* \ud76c\uc18c\ud589\ub82c\uc5d0\uc11c (0, 20961)    5  \ub294 0\ubc88 \ubb38\uc11c(0\ubc88 txt\ud30c\uc77c\uc744 \uc62c\ub824\uc11c \ub9cc\ub4e0 0\ubc88\uc9f8 \ubc30\uc5f4)\uc5d0\ub294 20961\ubc88\uc9f8 \ub2e8\uc5b4\uac00 5\ubc88 \ub098\uc654\ub2e4\ub294 \uc758\ubbf8 ","098c882d":"Here we fit regularized logistic regression where the features are the trigrams.","241c8257":"* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https:\/\/www.aclweb.org\/anthology\/P12-2018)","2b6167ab":"`fit_transform (trn)`\uc740 \ud6c8\ub828 \uc138\ud2b8\uc5d0\uc11c \uc5b4\ud718\ub97c \ucc3e\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ud6c8\ub828 \uc138\ud2b8\ub97c \ubb38\uc11c\ud589\ub82c\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4. \uac80\uc99d \uc138\ud2b8\uc5d0 * \ub3d9\uc77c\ud55c \ubcc0\ud658 *\uc744 \uc801\uc6a9\ud574\uc57c\ud558\ubbc0\ub85c \ub450 \ubc88\uc9f8 \uc904\uc5d0\uc11c\ub294`transform (val)`\uba54\uc11c\ub4dc \ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. `trn_term_doc` \ubc0f`val_term_doc`\ub294 \ud76c\uc18c \ud589\ub82c\uc785\ub2c8\ub2e4. `trn_term_doc [i]`\ub294 \ud6c8\ub828 \ubb38\uc11c i\ub97c \ub098\ud0c0\ub0b4\uba70 \uc5b4\ud718\uc758 \uac01 \ub2e8\uc5b4\uc5d0 \ub300\ud55c \uac01 \ubb38\uc11c\uc758 \ub2e8\uc5b4 \uc218\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.\n\n* train\uc138\ud2b8\uc640 val\uc138\ud2b8\uac00 \ub9cc\uc57d \uc0ac\uc6a9\ud55c \ub2e8\uc5b4\uac00 \ub2e4\ub974\ub2e4\uba74 \uc77c\ubc18\uc801\uc73c\ub85c unknown \ub2e8\uc5b4 \uc0ac\uc6a9, \ub2e8\uc5b4\uac00 \uc138\ubc88\ubcf4\ub2e4 \uc801\uac8c \ub4f1\uc7a5\ud558\uba74 \uc911\uc694\ud558\uc9c0 \uc54a\uc740 unknown\uc73c\ub85c \ubd80\ub974\uae30\ub3c4 \ud568","c88aa4b8":"* TDM(Term Document Matrix: \ubb38\uc11c\ud589\ub82c)\uc740 \uac01 \ub9ac\ubdf0\uc5d0 \uc5b4\ub5a4 \ub2e8\uc5b4\uac00 \ub4e4\uc5b4 \uc788\ub294 \uc9c0\uc5d0 \ub300\ud55c \ub9ac\uc2a4\ud2b8\ub97c \uc21c\uc11c\uc640 \uc0c1\uad00\uc5c6\uc774 \ub9cc\ub4e0 \uac83\n\n   -. \ub2e8\uc5b4\uc9d1\ud569(\ub098\ud0c0\ub098\ub294 \uc720\uc77c\ud55c \ub2e8\uc5b4\uc758 \ubaa8\ub4e0 \ub9ac\uc2a4\ud2b8) : This, movie, is, good, the, bad \n\n   -. \ub2e8\uc5b4\uc9d1\ud569\uc5d0 \ud574\ub2f9 \ub9ac\ubdf0\uc5d0 \ub2e8\uc5b4\ub4e4\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0b8\uac8c TDM\n   \n   \uc5d1\uc140\uc5d0\uc11c \ud14c\uc774\ube14\uc774 TDM, \ub2e8\uc5b4\ub85c \uc774\ub8e8\uc5b4\uc9c4 \uceec\ub7fc\uba85\uc774 \ub2e8\uc5b4\uc9d1\ud569, row\ub85c \ud45c\uc2dc\ub41c\ub370 BOW(Bag Of Words): \uc544\uc8fc \uc608\uc05c \uc9c1\uc0ac\uac01\ud615 \ud589\ub825\uc774\ub77c\uc11c \uc218\ud559\uc801\uc778 \uacc4\uc0b0(\ud2b9\ud788 \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0)\uc744 \ud560\uc218 \uc788\uc74c","ceae3632":"...and the regularized version","63008153":"## References","8d65f26e":"### Logistic regression","818830d7":"* L1 \uc815\uaddc\ud654\ub294 \uac00\uc911\uce58\uce3c \uc808\ub300\uac12\uc744 \uc0ac\uc6a9\ud558\uace0 L2 \uc815\uaddc\ud654\ub294 \uac00\uc911\uce58\uc758 \uc81c\uacf1\uc744 \uc0ac\uc6a9.\n* L1 \uc815\uaddc\ud654\ub294 \ucd5c\ub300\ud55c \ub9ce\uc740 \ubcc0\uc218\ub4e4\uc744 0\uc73c\ub85c \ub9cc\ub4e4\uace0 L2 \uc815\uaddc\ud654\ub294 \ubaa8\ub4e0\uac83\uc744 \uc791\uac8c \ub9cc\ub4dc\ub294 \ud6a8\uacfc\uac00 \uc788\uc74c\n* scikit-learn\uc774 \uc81c\uacf5\ud558\ub294 logistc regression\uc5d0\uc11c L2\uac00 \ube60\ub984.(L2\uac00 \uc544\ub2c8\uba74 dual=True\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc74c, L2\uac00 \uae30\ubcf8\uac12)","b6390dc7":"* \ubb38\uc11c\uac00 \uc8fc\uc5b4\uc84c\uc744\ub54c \uae0d\uc815\uc801(class=1)\uc77c \ud655\ub960 = (\uae0d\uc815\uc758 \ud074\ub798\uc2a4\uc77c\ub54c, \uc774 \ud2b9\uc815 \ub9ac\ubdf0\uc77c \ud655\uc728)x(\uc784\uc758\uc758 \ub9ac\ubdf0\uac00 \uae0d\uc815\uc77c \ud655\ub960)\/ (\uc774 \ud2b9\uc815\ub9ac\ubdf0\uc77c \ud655\ub960) \n\n*  p(c=1|d) = p(d|c=1) x p(c=1) \/ p(d)\n\n* \uc2e4\uc81c\ub85c \ub354 \uc911\uc694\ud55c \uc815\ubcf4\ub294 \ud074\ub798\uc2a4\uac00 0\uc774\ub0d0 1\uc774\ub0d0 : \uc989 \ud074\ub798\uc2a4\uac00 1\uc77c \ud655\ub960\uc744 \ud074\ub798\uc2a4\uac00 0 \uc778 \ud655\ub960\ub85c \ub098\ub204\uc5c8\uc744\ub54c \uadf8 \uc22b\uc790\uac00 1\ubcf4\ub2e4 \ud06c\uba74 \ud074\ub798\uc2a4 1(\uae0d\uc815)\uc774\ub77c\uace0 \ud560\uc218 \uc788\uace0 1\ubcf4\ub2e4 \uc791\uc73c\uba74 \ud074\ub798\uc2a4 0(\ubd80\uc815)\uc774\ub77c \ud560\uc218 \uc788\uc74c.\n\n* \uc6b0\ub9ac\uac00 \ud574\uc57c\ud560 \uac83\uc740 \uc774 \ub958\ubc14 \ud3ec\ud568\ud558\ub294 \ub2e8\uc5b4\ub4e4\uc744 \ubcf4\uace0 \uadf8 \ub2e8\uc5b4\ub4e4\uc774 \ud074\ub798\uc2a4\uac00 1\uc77c\ub54c \ud655\ub960\uc744 \ubaa8\ub450 \uacf1\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Really? \ub2e8\uc5b4\uc758 \uc120\ud0dd\uc740 \ub3c5\ub9bd\uc801\uc774\uc9c0 \uc54a\ub294\ub370 \ub3c5\ub9bd\uc801\uc778 \uac83\ucc98\ub7fc \uc774\ub7f0 \uc21c\uc9c4\ud55c \uac00\uc815\uc744 \uc801\uc6a9\ud55c\uac8c \uc21c\uc9c4\ud55c \ubca0\uc774\uc9c0(Naive Bayes) ","ec97730a":"...and binarized Naive Bayes.","f323611f":"## IMDB dataset and the sentiment classification task","127b3fed":"The [large movie review dataset](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n\nThe **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n\nTo get the dataset, in your terminal run the following commands:\n\n`wget http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz`\n\n`gunzip aclImdb_v1.tar.gz`\n\n`tar -xvf aclImdb_v1.tar`","3c63abb2":"### Trigram with NB features","1a93ee46":"We define the **log-count ratio** $r$ for each word $f$:\n\n$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n\nwhere ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents.","512760bc":"* \ub2e8\uc5b4\uc758 \uc21c\uc11c\ub97c \ub0a0\ub824\ubc84\ub9bc. not\uc758 \uc5b4\ub290 \ub2e8\uc5b4 \uc55e\uc5d0 \uc788\ub294\uc9c0 \uc21c\uc11c\ub294 \uc911\uc694\ud558\uc9c0\ub9cc \uc601\ud654\ud3c9\uc774 \uae0d\uc801\uc801\uc778\uc9c0 \ubd80\uc815\uc801\uc778\uc9c0 \ud310\ub2e8\ud558\ub294\ub370\ub294 \ub9cc\uc57d \ud130\ubb34\ub2c8\uc5c6\ub294('absurd')\uac00 \ub9ce\uc774 \ubcf4\uc774\uba74 \ubd80\uc815\uc801\uc778 \ub258\uc559\uc2a4\uc784\n\n* sklearn\uc740 TDM\uc744 \ub9cc\ub4e4\uae30 \uc704\ud55c \ud568\uc218(CountVectorizer)\ub97c \uac00\uc9c0\uace0 \uc788\uc74c\n\n* \ud1a0\ud070\ud654(Tokenization):\ud14d\uc2a4\ud2b8\ub97c \ub2e8\uc5b4\uc758 \ubaa9\ub85d\uc73c\ub85c \ubc14\uafb8\ub294 \uac83 , \ud14d\uc2a4\ud2b8 \uc870\uac01\uc744 \ud1a0\uadfc \ub9ac\uc2a4\ud2b8\ub864 \ubc14\uafb8\ub294 \uac83(\uc88b\uc740 Tokenizer\ub294 \"this movie isn't good\" => 'this' 'movie' 'is' 'n't' 'good' 7\uac1c\ub85c \ubc14\uafc8","9a45628d":"[`CountVectorizer`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`).","0bd4750c":"## Naive Bayes","43493342":"Here is the text of the first review","ddc1fa85":"`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. `trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary.","3eb07071":"Our next model is a version of logistic regression with Naive Bayes features described [here](https:\/\/www.aclweb.org\/anthology\/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment.","f8944871":"Here is how we can fit logistic regression where the features are the unigrams.","f7c3be45":"Here is the formula for Naive Bayes.","b30964a2":"**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**","65a5a693":"## fastai NBSVM++","108577ef":"Here we fit regularized logistic regression where the features are the trigrams' log-count ratios.","041723ed":"\ub2e4\uc74c \ubaa8\ub378\uc740 [\uc5ec\uae30] (https:\/\/www.aclweb.org\/anthology\/P12-2018)\uc5d0 \uc124\uba85 \ub41c Naive Bayes \uae30\ub2a5\uc774 \ud3ec\ud568 \ub41c \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubc84\uc804\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \ubb38\uc11c\uc5d0 \ub300\ud574 \uc704\uc5d0\uc11c \uc124\uba85\ud55c\ub300\ub85c \uc774\uc9c4\ud654 \ub41c \uae30\ub2a5\uc744 \uacc4\uc0b0\ud558\uc9c0\ub9cc \uc774\ubc88\uc5d0\ub294 bigram\uacfc trigram\ub3c4 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uac01 \uae30\ub2a5\uc740 \ub85c\uadf8 \uc218 \ube44\uc728\uc785\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uc5ec \uac10\uc815\uc744 \uc608\uce21\ud569\ub2c8\ub2e4.  ngram_range=(1,3) \uc635\uc158\uc744 \uc0ac\uc6a9\ud558\uba74 Bi-gram, trigram \uc0ac\uc6a9\uac00\ub2a5","4dfba284":"### Tokenizing and term document matrix creation","0950dc94":"Here is the $\\text{log-count ratio}$ `r`.  "}}