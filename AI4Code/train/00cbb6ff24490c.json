{"cell_type":{"48e91a40":"code","38d4cfcb":"code","5a29209d":"code","3e0b00b3":"code","810c138c":"code","c7ea897d":"code","844e4892":"code","a86380f4":"markdown","09f33076":"markdown","6fe13eca":"markdown","443538d7":"markdown","f508a4a3":"markdown","6d5ab34f":"markdown","b4ed9a59":"markdown"},"source":{"48e91a40":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# Importing Deep Learning Libraries\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\nfrom keras.models import Model,Sequential\nfrom keras.optimizers import Adam,SGD,RMSprop","38d4cfcb":"picture_size = 48\nfolder_path = \"..\/input\/face-expression-recognition-dataset\/images\/\"","5a29209d":"batch_size  = 64\n\ndatagen_train  = ImageDataGenerator()\ndatagen_val = ImageDataGenerator()\n\ntrain_set = datagen_train.flow_from_directory(folder_path+\"train\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=True)\n\n\ntest_set = datagen_val.flow_from_directory(folder_path+\"validation\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=False)","3e0b00b3":"from keras.optimizers import Adam,SGD,RMSprop\n# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(64,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 4th Convolution layer\nmodel.add(Conv2D(256,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flattening\nmodel.add(Flatten())\nmodel.add(Dense(7, activation='softmax'))\n\nopt = Adam(lr=0.001)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","810c138c":"epochs = 15\n\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n\n\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=2, min_lr=0.00001, mode='auto')\ncallbacks_list = [checkpoint,reduce_lr]\n\nhistory = model.fit_generator(generator=train_set,\n                                steps_per_epoch=train_set.n\/\/train_set.batch_size,\n                                epochs=epochs,\n                                validation_data = test_set,\n                                validation_steps = test_set.n\/\/test_set.batch_size,\n                                callbacks=callbacks_list\n                                )","c7ea897d":"model.save_weights(\".\/model.h5\")","844e4892":"plt.figure(figsize=(20,10))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()","a86380f4":"# Saving the Model","09f33076":"## Data loading and preprocessing\n\nHere there isn't much of data preprocessing going on, The horizontal_flip flips images randomly so our model is more robust\n\nThe flow_from_directory part in tensorflow takes care of target labels(y, which in our case is the name of the folder) and features(x)","6fe13eca":"## If you like this notebook please do upvote\nFor Pytorch implementation : [link](https:\/\/www.kaggle.com\/deepakvelmurugan\/facial-expression-recognition-in-pytorch)","443538d7":"## Training the model\n\n* This takes care of training our model, in tensorflow we have this fit method which does all the things for us like predicting on validation and training images.\n* The ModelCheckpoint saves our best weights so that we can use it in our test time.","f508a4a3":"# Plotting Accuracy & Loss","6d5ab34f":"## Import the necessary header files required\n\nCheck whether you have GPU availablity since training in CPU is very slow\n\nPrerequisites: \n* Knows intermediate python\n* Classification problems in machine learning\n* How **Forward prop** and **Back prop** works in neural nets\n* Basics of gradient descent (also nice to know kinds of parameter updates)","b4ed9a59":"## Building our neural network\n\nThis is completely upto your choice you can prefer any kind of structure that you want.\n\n*In torch make sure your conv and pool dimensions are proper* or else the matrices\/tensors can't undergo proper matrix multipilcation.\n\n***Remember to flatten for shifting from convolutional layers to fully connected layers***\n\nThe ReduceLROnPlateau reduces the learning rate whenever our network is stuck at a local minima.\n\n### Important formulas for calculating dimensions\n\n* Output of conv layer would have dimension -> ((H - F + 2P)\/\/s) + 1 *we can't have decimals here*\n\n  *where H - height of input(in our case image) , F - kernel_size(filter size) , P - padding & s - stride (vice versa for width)* \n  \n  \n* If you want the input dimensions (height & width) to remain same then you can use this -> (F - 1)\/2\n\n  *where F - kernel_size(filter size)*"}}