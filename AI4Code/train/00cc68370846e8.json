{"cell_type":{"f2f0d6d0":"code","949b6ee7":"code","b40e0f3a":"code","e5b43244":"code","a48012ab":"code","6fe4b98a":"code","5abf642f":"code","619df59f":"code","6f21f62c":"code","c7cb4861":"code","abae71cb":"code","98e9c483":"code","033633f6":"code","2d545ca6":"code","35aa0070":"code","f3a76535":"code","665797ec":"markdown","26be0a05":"markdown","3494a033":"markdown","e43dfcd9":"markdown","ae6c0ed6":"markdown","e2a2cef0":"markdown","705b99a1":"markdown"},"source":{"f2f0d6d0":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing feature selection and processing packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import RobustScaler\n\n# importing modelling packages\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier","949b6ee7":"# using datatable for faster loading\n\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\nsub = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()","b40e0f3a":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64','float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e5b43244":"train_data = train.copy()\ntest_data = test.copy()","a48012ab":"train_data.info()","6fe4b98a":"# splitting data into float and boolean\n\ntrain_data_boolean = train_data.select_dtypes(include = 'bool')\ntest_data_boolean = test_data.select_dtypes(include = 'bool')\n\ntrain_data_float = train_data.select_dtypes(include = 'float16')\ntest_data_float = test_data.select_dtypes(include = 'float16')","5abf642f":"# adding feature 'ones'\n\ntrain_data['ones'] = train_data_boolean.drop(['f22','target'],axis=1).sum(axis=1)\ntest_data['ones'] = test_data_boolean.drop('f22',axis=1).sum(axis=1)","619df59f":"# reducing memory\n\ntrain_1 = reduce_mem_usage(train)\ntest_1 = reduce_mem_usage(test)","6f21f62c":"# memory reduced - float64 downcast to float16\n\ntrain_1.info()","c7cb4861":"train_data = train_1.copy()\ntest_data = test_1.copy()\n\n# dropping 'id' variable\n\ntrain_data = train_data.drop('id',axis=1)\ntest_data = test_data.drop('id',axis=1)","abae71cb":"# splitting data \n\nX = train_data.drop('target',axis=1)\ny = train_data['target'] # the target variable","98e9c483":"# dropping all boolean columns other than f22 and ones\n\ncolumns_to_use = train_data_float.columns.tolist()+['f22']\n#columns_to_use = ['f179','f22'] # keeping only decently correlated variables\n\nX = X[columns_to_use]\ntest_data = test_data[columns_to_use]","033633f6":"X.head()","2d545ca6":"# scaling data for faster run = some memory problem for now\n\n#rs = RobustScaler()\n#X = rs.fit_transform(X)\n#test_data = rs.transform(test_data)","35aa0070":"folds = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n\npredictions_cb = np.zeros(len(test_data))\ncat_oof = np.zeros(X.shape[0])\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    print(f\"Fold: {fold+1}\")\n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model_cb =  CatBoostClassifier(task_type='GPU',verbose=0) \n    \n    model_cb.fit(X_train, y_train)\n    pred_cb = model_cb.predict_proba(X_val)[:,1]\n    cat_oof[val_idx] = pred_cb\n    print('ROC: ',roc_auc_score(y_val,pred_cb))\n    \n    print(\"-\"*50)\n    \n    predictions_cb += model_cb.predict_proba(test_data)[:,1] \/ folds.n_splits","f3a76535":"# submission\n\nsub['target'] = predictions_cb\nsub.to_csv('baseline_cb.csv',index = False) ","665797ec":"**Please have a look at the key take-aways from this run, at the very end of the code.**\n\n**May save you some time when you try simple feature engineering at your end :)**","26be0a05":"<div style=\"background-color:rgba(205, 29, 31, 0.5);\">\n    <h1><center>Data Split and Feature Creation<\/center><\/h1>\n<\/div>","3494a033":"<div style=\"background-color:rgba(205, 29, 31, 0.5);\">\n    <h1><center>Importing Libraries and Data<\/center><\/h1>\n<\/div>","e43dfcd9":"<div style=\"background-color:rgba(205, 29, 31, 0.5);\">\n    <h1><center>Reducing Memory Usage<\/center><\/h1>\n<\/div>","ae6c0ed6":"**Results Summary**\n\n1. gave a score of 0.85294 when no feature added\n2. gave a score of 0.85303 when 'ones' added with all others - **BEST**\n3. gave a score of 0.85177 when only f22 and 'ones' kept among binary variables\n4. gave a score of 0.85111 when only f22 kept among binary variables - **WORST**\n\n**What next?**\n\n1. Will try LGBM and XGBoost\n2. May do ensembling (weighted or power averaging)\n3. Will try stacking with Log Regression or a Ridge Classifier as my meta-model\n4. Will tune the best 2-3 models I get, using Optuna\n5. I may have a look at GAMs - Please share any references you have :)","e2a2cef0":"<div style=\"background-color:rgba(205, 29, 31, 0.5);\">\n    <h1><center>Baseline CatBoost<\/center><\/h1>\n<\/div>","705b99a1":"<div style=\"background-color:rgba(205, 29, 31, 0.5);\">\n    <h1><center>Take-aways<\/center><\/h1>\n<\/div>"}}