{"cell_type":{"b774152c":"code","108fc4c6":"code","aa7373df":"code","80b8567f":"code","50291061":"code","f21bbf1a":"code","6c8ef5fa":"code","8dad5b9d":"code","86ae9c51":"code","e975a3f1":"code","1c5eb5bf":"code","e83cd77a":"code","aa95984f":"code","84d5be18":"markdown","7eef4de5":"markdown","b7df78c4":"markdown","edfd4ad4":"markdown","52d5f7ff":"markdown","f14977cb":"markdown","3ff9e5f6":"markdown","02d06754":"markdown","0a399841":"markdown","b4288c23":"markdown","ce47dbbf":"markdown","d6d494b9":"markdown","4c0b9ddf":"markdown","3b47f8bc":"markdown","81486754":"markdown","7a3fdb19":"markdown"},"source":{"b774152c":"import torch\nimport torchvision\nimport torchvision.transforms as transforms","108fc4c6":"transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])","aa7373df":"# \u4e0b\u8f7d\u6570\u636e\u96c6\n# \u8bad\u7ec3\u96c6\ntrainset=torchvision.datasets.CIFAR10(root=\".\/data\",train=True,download=True,\n                                      transform=transform)\ntrainloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True,\n                                        num_workers=2)\n# \u6d4b\u8bd5\u96c6\ntestset=torchvision.datasets.CIFAR10(root=\".\/data\",train=True,download=False,\n                                      transform=transform)\ntestloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True,\n                                        num_workers=2)\nclasses=(\"plane\",\"car\",\"bird\",\"cat\",\"deer\",\n         \"dog\",\"frog\",\"horse\",\"ship\",\"truck\")","80b8567f":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img=img\/2+0.5\n    npimg=img.numpy()\n    plt.imshow(np.transpose(npimg,(1,2,0)))\n    \n# \u83b7\u5f97\u968f\u673a\u6570\ndataiter=iter(trainloader)\nimages,labels=dataiter.next()\n\n# \u5c55\u793a\u56fe\u50cf\nimshow(torchvision.utils.make_grid(images))\n# \u5c55\u793a\u56fe\u50cf\u6807\u7b7e\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))","50291061":"import torch.nn as nn\nimport torch.nn.functional as F  # \u8f7d\u5165\u6fc0\u6d3b\u51fd\u6570","f21bbf1a":"# \u5efa\u7acb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.conv1=nn.Conv2d(3,6,5)\n        self.pool=nn.MaxPool2d(2,2)\n        self.conv2=nn.Conv2d(6,16,5)\n        self.fc1=nn.Linear(16*5*5,120)\n        self.fc2=nn.Linear(120,84)\n        self.fc3=nn.Linear(84,10)\n        \n    def forward(self,x):\n        x=self.pool(F.relu(self.conv1(x)))\n        x=self.pool(F.relu(self.conv2(x)))\n        x=x.view(-1,16*5*5)\n        x=F.relu(self.fc1(x))\n        x=F.relu(self.fc2(x))\n        x=self.fc3(x)\n        return x\n    \nnet=Net()\nnet","6c8ef5fa":"import torch.optim as optim\n\ncriterion=nn.CrossEntropyLoss()\noptimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)","8dad5b9d":"for epoch in range(2):  # \u591a\u6279\u91cf\u5faa\u73af\n    \n    running_loss=0.0\n    for i,data in enumerate(trainloader,0):\n        # \u6bcf\u6b21\u83b7\u5f97\u4e00\u4e2abatch_size\u7684\u6570\u636e\n        inputs,labels=data\n        \n        # \u68af\u5ea6\u8bbe\u7f6e\u4e3a0(\u8fd9\u91cc\u662f\u68af\u5ea6\uff0c\u5e76\u6ca1\u6709\u628a\u6743\u91cd\u8bbe\u7f6e\u4e3a0)\n        optimizer.zero_grad()\n        \n        # \u6b63\u5411\u4f20\u64ad\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316\n        outputs=net(inputs)\n        loss=criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        # \u6253\u5370\u72b6\u6001\u4fe1\u606f\n        running_loss+=loss.item()\n        if i%2000==0:    # \u6bcf2000\u6b21\u6253\u5370\u4e00\u6b21\n            print(\"[%d,%5d] loss: %.3f\" % (epoch+1,i+1,running_loss\/2000))\n            running_loss=0.0\n\nprint(\"Finished Training\")","86ae9c51":"dataiter=iter(testloader)\nimages,labels=dataiter.next()\n\n# \u663e\u793a\u56fe\u7247\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))","e975a3f1":"outputs=net(images)\noutputs","1c5eb5bf":"_,predicted=torch.max(outputs,1)\n\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                              for j in range(4)))","e83cd77a":"correct=0\ntotal=0\nwith torch.no_grad():\n    for data in testloader:\n        images,labels=data\n        outputs=net(images)\n        _,predicted=torch.max(outputs,1)\n        total+=labels.size(0)\n        correct+=(predicted==labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct \/ total))","aa95984f":"class_correct=[0. for i in range(10)]\nclass_total=[0. for i in range(10)]\n\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] \/ class_total[i]))","84d5be18":"\u5c55\u73b0\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\u3002","7eef4de5":"torch\u521b\u5efa\u4e86\u4e00\u4e2atorchvision\uff0c\u5b83\u5305\u542b\u4e86\u4e00\u4e9b\u5904\u7406\u57fa\u672c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\n\u5305\u62ecImagenet\u3001CIFAR10\u3001MNIST\u7b49\u3002\u9664\u4e86\u6570\u636e\u52a0\u8f7d\u4ee5\u5916\uff0ctorchvision\u8fd8\u5305\u542b\u4e86\u56fe\u50cf\u8f6c\u6362\u5668\uff0c\ntorchvision.datasets\u548ctorchvision.utils\uff0ctorch.DataLoader\u3002","b7df78c4":"## \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc","edfd4ad4":"## \u8bfb\u53d6\u548c\u5f52\u4e00\u5316CIFAR10","52d5f7ff":"\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6765\u770b\u770b\u6574\u4f53\u7684\u6b63\u786e\u7387\u3002","f14977cb":"torchvision\u7684\u8f93\u51fa\u662f[0,1]\u7684PILImage\u56fe\u50cf\uff0c\u6211\u4eec\u628a\u5b83\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u8303\u56f4\u4e3a[-1,1]\u7684\u5f20\u91cf\u3002","3ff9e5f6":"CIFAR10\u6570\u636e\u96c6\u672b\uff0c\u4e3a2*32*32\uff0c\u53733\u4e2a\u989c\u8272\u901a\u9053\uff0c32*32\u50cf\u7d20\u3002","02d06754":"## \u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","0a399841":"# \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668","b4288c23":"## \u8bad\u7ec3\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668","ce47dbbf":"\u4e0b\u9762\u8ba9\u6211\u4eec\u6765\u770b\u770b\u6a21\u578b\u5bf9\u4e8e\u54ea\u4e00\u7c7b\u7684\u8bc6\u522b\u6548\u679c\u6bd4\u8f83\u597d\u3002","d6d494b9":"## \u5173\u4e8e\u6570\u636e","4c0b9ddf":"## \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668","3b47f8bc":"\u6b65\u9aa4\uff1a<p>\n1.\u4f7f\u7528torchvision\u52a0\u8f7d\u548c\u5f52\u4e00\u5316CIFAR10\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6<p>\n2.\u5b9a\u4e49\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc<p>\n3.\u5b9a\u4e49\u635f\u5931\u51fd\u6570<p>\n4.\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u7f51\u7edc<p>\n5.\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u7f51\u7edc<p>","81486754":"\u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u5e26\u52a8\u91cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002","7a3fdb19":"## \u8bad\u7ec3\u7f51\u7edc"}}