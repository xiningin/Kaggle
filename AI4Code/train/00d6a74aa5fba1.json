{"cell_type":{"b15633b0":"code","def213b3":"code","595c0f32":"code","71d351a8":"code","29f30ff0":"code","188710bc":"code","39e31f95":"code","4b68187e":"code","89ce425e":"code","4ff12064":"code","2081604a":"code","040738c0":"code","a15a8a67":"code","ab897cb7":"code","4531b3af":"code","ab85232c":"code","b352e7e0":"code","aafb7677":"code","fcdbe49f":"code","faf9d86c":"code","1ccbe05f":"code","32b79da1":"code","eb4ac8bf":"code","9a16c6a6":"code","f4db29b0":"code","b311e7a1":"code","5795842b":"code","823e7c55":"code","a88ff0d4":"code","061d6509":"markdown","564c35e4":"markdown","b67e29c9":"markdown","2135ebbb":"markdown","6ca5d00b":"markdown","12cac791":"markdown","8c38aa71":"markdown","d002e212":"markdown"},"source":{"b15633b0":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","def213b3":"data = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndata.head()","595c0f32":"data.isna().sum()","71d351a8":"data=data.drop(\"article_link\",axis=1)","29f30ff0":"data.head()","188710bc":"# checking the class distribution\nsns.countplot(data['is_sarcastic'])","39e31f95":"# importing the NLP libraries\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re,string,unicodedata\nfrom bs4 import BeautifulSoup","4b68187e":"stop = set(stopwords.words('english'))\npunctuation=list(string.punctuation)\nstop.update(punctuation)","89ce425e":"def html_strip(text):\n    soup = BeautifulSoup(text,\"html.parser\")\n    return soup.get_text()\n\ndef remove_square_bracs(text):\n    return re.sub('\\[[^]]*\\]','',text)\n\ndef remove_urls(text):\n    return re.sub(r'http\\S+','',text)\n\ndef remove_stopwords(text):\n    final_txt=[]\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_txt.append(i.strip())\n    return \" \".join(final_txt)\n\ndef remove_noisy_data(text):\n    txt = html_strip(text)\n    txt = remove_square_bracs(text)\n    txt = remove_urls(text)\n    txt = remove_stopwords(text)\n    return text","4ff12064":"data['headline']=data['headline'].apply(remove_noisy_data)","2081604a":"plt.figure(figsize=(25,25))\nwordcloud = WordCloud(max_words = 1000, width = 1400, height=500).generate(\" \".join(data[data.is_sarcastic==0].headline))\nplt.imshow(wordcloud,interpolation='bilinear')","040738c0":"plt.figure(figsize=(25,25))\nwordcloud = WordCloud(max_words = 1000, width = 1400, height=500).generate(\" \".join(data[data.is_sarcastic==1].headline))\nplt.imshow(wordcloud,interpolation='bilinear')","a15a8a67":"words = []\nfor i in data.headline.values:\n    words.append(i.split())\n\nwords[:3]","ab897cb7":"import gensim\n\nembed_dim = 200\n\nword2vec_model = gensim.models.Word2Vec(sentences = words,size = embed_dim,window=5, min_count=1)","4531b3af":"len(word2vec_model.wv.vocab)","ab85232c":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer= Tokenizer(num_words = 35000)\ntokenizer.fit_on_texts(words)\ntokenized_sequences = tokenizer.texts_to_sequences(words)\nx = pad_sequences(tokenized_sequences,maxlen=20)\n","b352e7e0":"vocab_size = len(tokenizer.word_index)+ 1","aafb7677":"def weight_matrix(model,vocab):\n    vocab_size= len(vocab)+1\n    wgt_matrix = np.zeros((vocab_size,embed_dim))\n    for word, i in vocab.items():\n        wgt_matrix[i]=model[word]\n    return wgt_matrix\n\n","fcdbe49f":"embedding_vec = weight_matrix(word2vec_model,tokenizer.word_index)","faf9d86c":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional,LSTM,GRU,Dense","1ccbe05f":"model =Sequential([\n    Embedding(vocab_size, output_dim = embed_dim,weights=[embedding_vec],input_length =20,trainable=True),\n    Bidirectional(LSTM(128, recurrent_dropout=0.1, dropout=0.3, return_sequences=True)),\n    Bidirectional(GRU(32, recurrent_dropout=0.1,dropout=0.1)),\n    Dense(1,activation='sigmoid')\n])","32b79da1":"from tensorflow.keras.optimizers import Adam\nmodel.compile(optimizer = Adam(lr=0.01),loss='binary_crossentropy', metrics =['acc'])\n\ndel embedding_vec","eb4ac8bf":"model.summary()","9a16c6a6":"from sklearn.model_selection import train_test_split\n\ntrainX,testX,trainY,testY = train_test_split(x,data.is_sarcastic,test_size=0.3,random_state=0)","f4db29b0":"hist = model.fit(trainX,trainY,batch_size=128, validation_data =(testX,testY))","b311e7a1":"pred = model.predict_classes(testX)\npred[:5]","5795842b":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(testY,pred)\ncm","823e7c55":"sns.heatmap(cm,annot=True,fmt='g')","a88ff0d4":"from sklearn.metrics import classification_report\n\nprint(classification_report(testY,pred,target_names = ['Not Sarcastic','Sarcastic']))","061d6509":"# Converting text into accetable format","564c35e4":"# Preprocessing","b67e29c9":"# Data Cleaning","2135ebbb":"# Funtion to create weight matrix","6ca5d00b":"# Model Analysis","12cac791":"# Model Training","8c38aa71":"# Tokenizing","d002e212":"# wordclouds for sarcastic and non sarcastic data"}}