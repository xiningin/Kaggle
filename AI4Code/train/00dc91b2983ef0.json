{"cell_type":{"460df5f7":"code","ca4cd2d0":"code","d416ce6e":"code","7b78796e":"code","5d3d89e1":"code","b4c1e48e":"code","4afe7f61":"code","ae860db6":"code","826b624a":"code","19a6b2db":"code","41f19673":"code","6583d7cf":"code","efde5525":"code","8bf7dfb5":"code","302daec4":"code","c878e995":"code","8f4c4034":"markdown","0fa88287":"markdown","ed9b90a1":"markdown","63466cff":"markdown","8f191e71":"markdown","d0b90d9f":"markdown","fb163203":"markdown","f889af48":"markdown","5481f265":"markdown","2c1fd81e":"markdown","7a879e36":"markdown","7f6344c1":"markdown","e25b2b2a":"markdown"},"source":{"460df5f7":"import numpy as np\nimport cv2\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\nfrom keras.models  import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\nimport random,os,glob\nimport matplotlib.pyplot as plt","ca4cd2d0":"dir_path = '..\/input\/garbage classification\/Garbage classification'","d416ce6e":"img_list = glob.glob(os.path.join(dir_path, '*\/*.jpg'))","7b78796e":"len(img_list)","5d3d89e1":"train=ImageDataGenerator(horizontal_flip=True,\n                         vertical_flip=True,\n                         validation_split=0.1,\n                         rescale=1.\/255,\n                         shear_range = 0.1,\n                         zoom_range = 0.1,\n                         width_shift_range = 0.1,\n                         height_shift_range = 0.1,)\n\ntest=ImageDataGenerator(rescale=1\/255,\n                        validation_split=0.1)\n\ntrain_generator=train.flow_from_directory(dir_path,\n                                          target_size=(300,300),\n                                          batch_size=32,\n                                          class_mode='categorical',\n                                          subset='training')\n\ntest_generator=test.flow_from_directory(dir_path,\n                                        target_size=(300,300),\n                                        batch_size=32,\n                                        class_mode='categorical',\n                                        subset='validation')\n\nlabels = (train_generator.class_indices)\nprint(labels)\n\nlabels = dict((v,k) for k,v in labels.items())\nprint(labels)","b4c1e48e":"for image_batch, label_batch in train_generator:\n  break\nimage_batch.shape, label_batch.shape","4afe7f61":"print (train_generator.class_indices)\n\nLabels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n\nwith open('labels.txt', 'w') as f:\n  f.write(Labels)\n","ae860db6":"model=Sequential()\n#Convolution blocks\n\nmodel.add(Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n#model.add(SpatialDropout2D(0.5)) # No accuracy\n\nmodel.add(Conv2D(64,(3,3), padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n#model.add(SpatialDropout2D(0.5))\n\nmodel.add(Conv2D(32,(3,3), padding='same',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2)) \n\n#Classification layers\nmodel.add(Flatten())\n\nmodel.add(Dense(64,activation='relu'))\n#model.add(SpatialDropout2D(0.5))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,activation='relu'))\n\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6,activation='softmax'))\n\nfilepath=\"trained_model.h5\"\ncheckpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint1]\n\n","826b624a":"model.summary()","19a6b2db":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc']) # RMS PROP - No accuracy\n\n#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n","41f19673":"history = model.fit_generator(train_generator,\n                              epochs=100,\n                              steps_per_epoch=2276\/\/32,\n                              validation_data=test_generator,\n                              validation_steps=251\/\/32,\n                              workers = 4,\n                              callbacks=callbacks_list) \n#41 epoch - 75% #73- 76.9%\n#78 epoch - 80%","6583d7cf":"from keras.preprocessing import image\n\nimg_path = '..\/input\/garbage classification\/Garbage classification\/plastic\/plastic75.jpg'\n\nimg = image.load_img(img_path, target_size=(300, 300))\nimg = image.img_to_array(img, dtype=np.uint8)\nimg=np.array(img)\/255.0\n\nplt.title(\"Loaded Image\")\nplt.axis('off')\nplt.imshow(img.squeeze())\n\np=model.predict(img[np.newaxis, ...])\n\n#print(\"Predicted shape\",p.shape)\nprint(\"Maximum Probability: \",np.max(p[0], axis=-1))\npredicted_class = labels[np.argmax(p[0], axis=-1)]\nprint(\"Classified:\",predicted_class)\n\n","efde5525":"classes=[]\nprob=[]\nprint(\"\\n-------------------Individual Probability--------------------------------\\n\")\n\nfor i,j in enumerate (p[0],0):\n    print(labels[i].upper(),':',round(j*100,2),'%')\n    classes.append(labels[i])\n    prob.append(round(j*100,2))\n    \ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(classes))\n    plt.bar(index, prob)\n    plt.xlabel('Labels', fontsize=12)\n    plt.ylabel('Probability', fontsize=12)\n    plt.xticks(index, classes, fontsize=12, rotation=20)\n    plt.title('Probability for loaded image')\n    plt.show()\nplot_bar_x()","8bf7dfb5":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# ________________ Graph 1 -------------------------\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\n# ________________ Graph 2 -------------------------\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,max(plt.ylim())])\nplt.title('Training and Validation Loss')\nplt.show()","302daec4":"import tensorflow as tf\nimport keras\nfile = \"Garbage.h5\"\nkeras.models.save_model(model,file)\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(file)\ntflite_model=converter.convert()\nopen(\"garbage.tflite\",'wb').write(tflite_model)","c878e995":"from IPython.display import FileLinks\nFileLinks('.')","8f4c4034":"# Train","0fa88287":"### Writing the labels file","ed9b90a1":"# Getting files from kernel","63466cff":"# Testing PREDICTION \n##### Note: Path is of training dataset (pl. don't mind)","8f191e71":"# Accuracy Graph","d0b90d9f":"## Dataset Input","fb163203":"# Image Augmentation","f889af48":"# Building CNN & Saving keras model","5481f265":"### Compiling Model using categorical cross entropy loss function & Adam Optimizer","2c1fd81e":"## Converting to TFLite\n#### Note: Image Size is 300","7a879e36":"# Summarizing our model","7f6344c1":"[Callback model Checkpoint Reference](https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/)","e25b2b2a":"# Import Libraries"}}