{"cell_type":{"020d3b42":"code","597d5129":"code","6a02a342":"code","5fad41c1":"code","a830f0f7":"code","919058bf":"code","b900752d":"code","e9cc1f1e":"code","145157a2":"code","e4bb9d59":"code","3ae2afef":"code","c2fd3a61":"code","8a7422b2":"code","a8c934f6":"code","2bb2d0df":"code","d9d224dc":"code","15273c75":"code","3569dc9a":"code","f94b2869":"code","822a80f3":"code","a85905a8":"code","83813b69":"code","e60db852":"code","782112c0":"code","69bd96e4":"code","5f450469":"code","d5f96a99":"code","7b17bbfa":"code","91750011":"code","e0347308":"code","fa27cf76":"code","90b5342f":"code","b443ad5a":"code","34d3e5d5":"code","263c407c":"code","05e3236d":"code","0386c7f8":"code","5705e785":"code","1a2e11ac":"code","6b22d686":"code","c996f654":"code","c038ad98":"markdown","64db4aaa":"markdown","50128da2":"markdown","f0e0ecc8":"markdown","36abfbcb":"markdown","a0c7177b":"markdown","30d943d0":"markdown","866f1846":"markdown","34f2790d":"markdown","77a13725":"markdown","90b3fbe4":"markdown","5fa1cc92":"markdown","3dd793a8":"markdown","37b69ce8":"markdown","4f94be91":"markdown","fdb6b758":"markdown","6faded9b":"markdown","52a09a3a":"markdown","4289d491":"markdown","9c5f6def":"markdown","e5ab97fa":"markdown","aa9776e8":"markdown","52273c04":"markdown","569b28a2":"markdown"},"source":{"020d3b42":"import numpy as np\nimport pandas as pd\nimport plotly\nimport plotly.graph_objs as go\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport nltk\nfrom wordcloud import WordCloud\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob,Word\nfrom collections import Counter\nimport string\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n","597d5129":"train_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain_df.head()","6a02a342":"train_df.shape","5fad41c1":"train_df.isna().sum()","a830f0f7":"train_df.nunique()","919058bf":"test_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df.head()","b900752d":"test_df.shape","e9cc1f1e":"sample_submission_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample_submission_df.head()","145157a2":"to_replace_by_space = re.compile('[\/(){}\\[\\]|@,;]')\npunctuation = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nstopwords = set(stopwords.words('english'))\n\ndef data_preprocessing(text):\n    text = text.lower() \n    text = re.sub(punctuation, '',text)\n    text = re.sub(to_replace_by_space, \" \", text) \n    text = re.sub(bad_symbols, \"\", text)\n    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) \n    text = re.sub(' +', ' ', text)\n    return text","e4bb9d59":"train_df['excerpt'] = train_df['excerpt'].apply(data_preprocessing)","3ae2afef":"# Bar Plot Function\ndef bar_plot_func(data_frame,x,x_title,y,title,colors=None,text=None,color_discrete_sequence=['mediumaquamarine']):\n    fig=px.bar(x=x,y=y,text=text,labels={x:x_title.title()},data_frame=data_frame,\n               color=colors,barmode='group',template='simple_white',\n               color_discrete_sequence=color_discrete_sequence)\n    texts=[data_frame[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'    \n    fig['layout'].title=title\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').title()\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n    fig.show()\n\n\n# Histogram Function\ndef hist_plot_func(data_frame,x,title,color_discrete_sequence=['indianred']):\n    fig=px.histogram(x=data_frame[x],color_discrete_sequence=color_discrete_sequence,opacity=0.8)\n    fig['layout'].title=title\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n    fig.show()","c2fd3a61":"title='Target Column distribution'\nhist_plot_func(train_df, 'target' ,title)","8a7422b2":"temp = train_df['license'].dropna()\ntemp = temp.value_counts().to_frame().reset_index()\ntitle = 'Unique licenses Count'\nbar_plot_func(temp,'index','License',['license'],title=title)","a8c934f6":"title='Standard Error Distribution'\nhist_plot_func(train_df, 'standard_error',title,color_discrete_sequence=['skyblue'])","2bb2d0df":"# Unique URLs\nurl_unique_list = train_df['url_legal'].dropna().apply(lambda x : re.findall('https?:\/\/([A-Za-z_0-9.-]+).*',x)[0]).unique()\nprint(url_unique_list)","d9d224dc":"# Unique Site Counts\nurl_count = {}\nfor sent in train_df['url_legal'].dropna().values:\n    url = re.findall('https?:\/\/([A-Za-z_0-9.-]+).*',sent)[0]\n    if url in url_count:\n        url_count[url] += 1\n    else:\n        url_count[url] = 1\n    \nurl_count_df = pd.DataFrame(data=url_count.items())\nurl_count_df = url_count_df.sort_values(by=1,ascending=False).rename(columns={0:'Site',1:'Count'})\nurl_count_df","15273c75":"title = 'Unique Sites count'\nbar_plot_func(url_count_df, 'Site','Site Name',['Count'],title=title,color_discrete_sequence=['coral'])","3569dc9a":"# 1. Create list of words\ntext= ' '.join(t for t in train_df['excerpt'])\nwords_list= text.split()\n# 2. Create word to count dictionary\nword_freq= {}\nfor word in set(words_list):\n    word_freq[word]= words_list.count(word)  \n# 3.Sorting the dictionary \nword_freq = dict(sorted(word_freq.items(), reverse=True, key=lambda item: item[1]))\n# 4. Sort the data and put it in a data frame for the visualization\nword_freq_temp = dict(itertools.islice(word_freq.items(), 25))\nword_freq_df = pd.DataFrame(word_freq_temp.items(),columns=['word','count']).sort_values('count',ascending=False)","f94b2869":"bar_plot_func(word_freq_df.reset_index(),'word','Words',['count'],title='Top 20 frequent words',color_discrete_sequence=['dimgray'])","822a80f3":"def wordcloud(text,stopwords,ngram=1):\n    # text: if ngram>1, text should be a dictionary\n    wordcloud = WordCloud(width=1400, \n                          height=800,\n                          random_state=2021,\n                          background_color='black',\n                          stopwords=stop)\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    \nwordcloud(train_df['excerpt'],stop)","a85905a8":"from nltk.util import ngrams    \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    \n    #compute frequency distribution for all the bigrams in the text\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output","83813b69":"two_grams = get_n_grans_count(text, n_grams=2, min_freq=10)\ntwo_grams_df = pd.DataFrame(data=two_grams.items())\ntwo_grams_df = two_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Two grams',1:'Count'})\ntwo_grams_df","e60db852":"bar_plot_func(two_grams_df.iloc[:20],'Two grams','Two grams',['Count'],title='Top 20 frequent bigram',color_discrete_sequence=['dimgray'])","782112c0":"two_grams_temp = {j.replace(' ','_'):k for j,k in two_grams.items()}\nwordcloud(two_grams_temp,stop,ngram=2)","69bd96e4":"three_grams = get_n_grans_count(text, n_grams=3, min_freq=0)\nthree_grams_df = pd.DataFrame(data=three_grams.items())\nthree_grams_df = three_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Three grams',1:'Count'})\nthree_grams_df","5f450469":"bar_plot_func(three_grams_df.iloc[:20],'Three grams','Three grams',['Count'],title='Top 20 frequent trigram',color_discrete_sequence=['dimgray'])","d5f96a99":"three_grams_temp = {j.replace(' ','_') : k for j, k in three_grams.items()}\nwordcloud(three_grams_temp,stop,ngram=3)","7b17bbfa":"# Libraries\nimport torch \nfrom torch import nn \nimport torch.nn.functional as F\nimport numpy as np \nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl \nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup, AdamW","91750011":"# As we have seen above in EDA that url_legal and license have around 90% missing value. \n# So we will drop those columns and use the remaing ones.\n\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\",usecols=[\"id\",\"excerpt\",\"target\"])\nprint(\"train shape\",df.shape)\ndf.head()","e0347308":"# Similarly with test data.\n\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\ntest_df.head()","fa27cf76":"#remove \\n and replace \\'s with 'sfrom the text\ndef prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","90b5342f":"# Max Word Count\nmax_words = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\nprint(\"maximum words in instance:\",max_words)","b443ad5a":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n# read training data\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\n# create folds\ndf = create_folds(df, num_splits=5)","34d3e5d5":"# BERT Config\nBATCH_SIZE = 16\nEPOCHS = 15\nNUM_TRAIN_STEPS = int((df.shape[0]\/BATCH_SIZE)*EPOCHS)\nNUM_WARMUP_STEPS = 0\nFOLDS = df.kfold.unique()\nNUM_FOLDS = df.kfold.nunique() ","263c407c":"# RMSE\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1e-8\n        \n    def forward(self,output,target):\n        return torch.sqrt(F.mse_loss(output,target)+self.eps)","05e3236d":"class BertModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = transformers.AutoModel.from_pretrained(\"..\/input\/bert-base-uncased\")\n        self.drop = nn.Dropout(0.5)\n        self.fc = nn.Linear(768,1)\n    def forward(self,inputs):\n        out = self.model(**inputs)\n        last_hiddens = out[0]\n        out = self.drop(last_hiddens[:,0,:].squeeze(1))\n        return self.fc(out)\n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)\n        return [optimizer],[scheduler]\n    def loss_fn(self,output,target):\n        return RMSELoss()(output.view(-1),target.view(-1))\n    def training_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        return loss\n    def validation_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        self.log(\"val_loss\",loss,prog_bar=True)","0386c7f8":"# Tokenize Dataset and Dataloader\nclass BertDataset(Dataset):\n    def __init__(self,texts,labels,max_len):\n        super().__init__()\n        self.texts = texts\n        self.max_len = max_len\n        self.labels = labels\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self,idx):\n        text = \" \".join(self.texts[idx].split())\n        label = self.labels[idx]\n        inputs = self.tokenizer(text,return_tensors=\"pt\",max_length = self.max_len, padding=\"max_length\",truncation=True)\n        return {\n            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n                      \"attention_mask\":inputs[\"attention_mask\"][0],},\n            \"label\":torch.tensor(label,dtype=torch.float)\n        }","5705e785":"# Training\nfor fold in FOLDS:\n    print(\"Fold :\",fold)\n    train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n    train_dataset = BertDataset(train_df.excerpt.values,train_df.target.values,max_len=max_words)\n    valid_dataset = BertDataset(valid_df.excerpt.values,valid_df.target.values,max_len=max_words)\n    train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n    valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    bert_model = BertModel() \n    trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\")],checkpoint_callback=False)\n    trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n    trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\")","1a2e11ac":"# Load Weights and Inference\nprediction = np.zeros(test_df.shape[0]) \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor fold in FOLDS:\n    print(\"Fold:\",fold)\n    loaded_model = BertModel.load_from_checkpoint(f\".\/checkpoint_{fold}fold.ckpt\",map_location=device)\n    loaded_model.to(device)\n    loaded_model.eval() \n    #using the same BertDataset module of train, here dummy labels are provided\n    test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones(test_df.shape[0]),max_len=max_words)\n    test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    output = []\n    for batch in test_dataloader:\n        x  = batch[\"inputs\"]\n        for key in x.keys():\n            x[key] = x[key].to(device)\n        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n        out = loaded_model(x)\n        output.extend(out.cpu().detach().numpy())\n    prediction += np.hstack(output)","6b22d686":"test_df[\"target\"] = prediction\/NUM_FOLDS\nsub = test_df.drop(\"excerpt\",axis=1) \nsub.to_csv(\"submission.csv\",index=False)","c996f654":"sub.head()","c038ad98":"##### Here we can see that 'url_legal' and 'license' have repeating values while other columns have all values unique. ","64db4aaa":"##### In two columns - 'url_legal' and 'license' have missing values. Rest other columns have no missing values.","50128da2":"##### BERT Model Configurations","f0e0ecc8":"### Trigrams","36abfbcb":"### Bigrams","a0c7177b":"<h2><center>Model : BERT & PyTorch<\/center><\/h2>","30d943d0":"<h4>Evaluation Metrics<\/h4>\nSubmissions are scored on the root mean squared error <b>RMSE<\/b>\n\n![128958_2016-06-23 13_45_36-Root Mean Squared Error _ Kaggle.png](attachment:b3251df0-e5cc-40e7-a00b-e3f72b15eaac.png)\n\nwhere  is the predicted value,  is the original value, and  is the number of rows in the test data.","866f1846":"## Refer my new notebook for better score model using Pytorch Lightining [RoBERTa](https:\/\/www.kaggle.com\/harshsharma511\/pytorch-lightining-roberta)","34f2790d":"<h2><center>Exploratory Data Analysis (EDA) <\/center><\/h2>","77a13725":"##### Train Data","90b3fbe4":"##### Create Folds\nInspired By [Abhishek Thankur's Notebook](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds)","5fa1cc92":"### Unigrams","3dd793a8":"<h3>Data Preprocessing<\/h3>\n\nText processing is an import step in NLP. It transforms text into more digestable form so that ML algorithms can perform better. We will apply 'data_preprocessing' function on 'excerpt' column.\n<h4>Preprocessing Steps<\/h4>\n<ol>\n  <li>Lowercasing<\/li>\n  <li>Removing Stop Words<\/li>\n  <li>Removing Digits<\/li>\n  <li>Removing URLs, HTML Tags<\/li>\n<\/ol>\n","37b69ce8":"##### In this notebook I will drive you through all the important steps required to understand and complete this competition.\n\n##### If you find this notebook helpful. Please Upvote.","4f94be91":"<h4>Descriptive Features<\/h4>\n<ol>\n  <li>id : unique ID for excerpt<\/li>\n  <li>url_legal : URL of source<\/li>\n  <li>license : license of source material<\/li>\n  <li>excerpt : text to predict reading ease of<\/li>\n  <li>target : reading ease<\/li>\n  <li>standard_error : measure of spread of scores among multiple raters for each excerpt<\/li>\n<\/ol>","fdb6b758":"#### GOAL\nThe objective of this Competition is to build a Machine Learning model to rate the complexity of reading passages for grade 3-12 classroom use based on pattern extracted from analysing <b>6 descriptive features.<\/b>\n\nIn simple words, we are given training.csv file in which we have (among other) 2 columns: excerpt and target, we will have to train Machine Learning model(s) that can approximate the relationship between excerpt and the target.","6faded9b":"<h1><center>CommonLit Readability Prize<\/center><\/h1>\n<h1><center>ONE STOP: Understanding + EDA + BERT<\/center><\/h1>","52a09a3a":"##### Test Data","4289d491":"<h2><center>Understand the Problem<\/center> <\/h2>\n","9c5f6def":"##### Sample Submission Data","e5ab97fa":"##### Test Data","aa9776e8":"#### Now lets do some Data Visualization to get more insights out of data. For this, first we will define two function (bar plot function and histogram function) to make the plotting easier.\n","52273c04":"![header.png](attachment:30c6c636-9e92-40a0-befa-a6f627e2e74b.png)","569b28a2":"#### Read Data\n\n##### Train Data"}}