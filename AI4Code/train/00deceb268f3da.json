{"cell_type":{"83202c33":"code","9fb897a4":"code","99abff4e":"code","be3ecca5":"code","becadb31":"code","977f0a1b":"code","5c60ad36":"code","408b887a":"code","bb5f682a":"code","2100ef3f":"code","41d8db06":"code","bb912c17":"code","aeb88cd2":"code","019a6e1d":"code","aa94ea52":"markdown","6ffd9c46":"markdown","f3bbec28":"markdown","21c09126":"markdown","75a4a5a4":"markdown","850665d7":"markdown","43dd9c61":"markdown","9c0d9856":"markdown","170d069a":"markdown","d2e35c7e":"markdown","ee783d6a":"markdown","74a1c012":"markdown"},"source":{"83202c33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fb897a4":"data = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv',\n                  parse_dates=['date'])\ndata.head(3)","99abff4e":"%%time\n\n#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\n\ndata['CPM'] = data.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\ndata.CPM.values[-10:]","be3ecca5":"data.drop(columns=['total_revenue', 'revenue_share_percent'], inplace=True)","becadb31":"data.drop(data[data['CPM'] < 0].index, inplace=True)","977f0a1b":"data['day_of_week'] = data['date'].dt.dayofweek\ndata.sample(10)","5c60ad36":"TERMINAL_DATE = '2019-06-22'    # Specified by hidden goal\nVALIDATION_DATE = '2019-06-19'  # To create a validation dataset\n\n\nX, y = data.drop(columns=['CPM', 'date']), data['CPM'] \n\nX_train = X[data['date'] < VALIDATION_DATE]\ny_train = y[data['date'] < VALIDATION_DATE]\n\nX_val = X[(data['date'] >= VALIDATION_DATE) & (data['date'] < TERMINAL_DATE)]\ny_val = y[(data['date'] >= VALIDATION_DATE) & (data['date'] < TERMINAL_DATE)]\n\nX_test = X[data['date'] >= TERMINAL_DATE]\ny_test = y[data['date'] >= TERMINAL_DATE]\n\ntrain_q95 = y_train.quantile(0.95)\nval_q95 = y_val.quantile(0.95)\ntest_q95 = y_test.quantile(0.95)\n\nX_train, y_train = X_train[y_train <= train_q95], y_train[y_train <= train_q95]\nX_val, y_val = X_val[y_val <= val_q95], y_val[y_val <= val_q95]\nX_test, y_test = X_test[y_test <= test_q95], y_test[y_test <= test_q95]\n\nprint('TOTAL:', X.shape, y.shape)\nprint('train:', X_train.shape, y_train.shape)\nprint('validation:', X_val.shape, y_val.shape)\nprint('test:', X_test.shape, y_test.shape)","408b887a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(data=data.corr(), cmap='coolwarm', square=True)\nplt.show","bb5f682a":"X_train.describe()","2100ef3f":"X_train.info()","41d8db06":"cat_features = list(range(0, 11)) + [14]\nnum_features = list(range(11, 14))\n\ncat_features, num_features","bb912c17":"from catboost import Pool, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error","aeb88cd2":"%%time\n\n# initialize Pool\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features=cat_features)\nvalidation_pool = Pool(X_val, \n                  y_val,\n                  cat_features=cat_features)\ntest_pool = Pool(X_test,\n                 cat_features=cat_features) \n\n# specify the training parameters \nmodel = CatBoostRegressor(cat_features=cat_features,\n                          verbose=100,\n                          random_seed=2020)\n#train the model\nmodel.fit(train_pool,\n          eval_set=validation_pool)\n\n# make the prediction using the resulting model\npreds = model.predict(test_pool)\nprint(preds)","019a6e1d":"test_mse = mean_squared_error(y_test, preds)\nprint(f'TEST MSE: {test_mse:.2f}')\nif test_mse < 4850:\n    print(f'{test_mse} < 4850\\nHidden goal is achieved! HOCHU ZACHET!')","aa94ea52":"We also should drop any rows with CPM < 0.","6ffd9c46":"Only EDA wee need is to create corrplot of some ids to stare at them and posess some chakra magick.","f3bbec28":"# Feature Selection","21c09126":"# EDA","75a4a5a4":"Whoa, that was strong sorcery. Now let's just look at the features and divide num bois from cat toys.","850665d7":"# Data Loading and Preprocessing","43dd9c61":"# Model & Predictions","9c0d9856":"# Train\/Test Splitting","170d069a":"\n# Goal\u00b6\n\nThe main idea of this notebook is inspired by [this work](https:\/\/www.kaggle.com\/akshaypaliwal709\/predicting-the-reserve-price-base-cpm) and we will calculate the CPM (cost per thousand impressions) on the [data](https:\/\/www.kaggle.com\/saurav9786\/real-time-advertisers-auction) provided by Sauran Anand before 21.06.2019 and try to predict this CPM using machine learning model on the targets after 21.06.2019.\nWe also have hidden goal of minimising MSE which will be revealed later on.","d2e35c7e":"Columns used to create target variable can't be used as features and can now be dropped.","ee783d6a":"Assuming strong seasonality we will operate data as categorical feature in form of days of week.","74a1c012":"First, let's calculate target variable using original formula provided by Akshai Paliwal."}}