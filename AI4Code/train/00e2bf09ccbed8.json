{"cell_type":{"5f3e9bac":"code","66240803":"code","b1383c12":"code","10f0ba41":"code","8cbbc9a3":"code","a312be8e":"code","18a1f284":"code","e08abac3":"code","c087840b":"code","42bf2872":"code","64517ad5":"code","676391ee":"code","f021e30b":"code","41909ce5":"code","708fd6e6":"code","855aa30f":"code","736a505a":"code","625163e3":"code","9d16ce2f":"code","fc5b00f3":"code","12c13f85":"code","07d30f91":"code","520fb507":"code","014ff1a2":"code","0b2e22bc":"code","063f18a4":"code","1df99fbc":"code","3840f703":"code","e50714c9":"code","e6da407b":"code","090c4789":"code","e43d4daa":"code","de96226f":"code","0114aea7":"code","2560d44f":"code","d5dfd209":"code","527e708f":"code","c8e925af":"code","a7c61df6":"code","266bd460":"code","7d0ab0ee":"code","1fc7f130":"code","b3ffe9f2":"code","6448fcd2":"code","8f3ce6d6":"code","482bd123":"code","d9bf0191":"code","902c9453":"code","7f8addab":"code","d42e7ba3":"code","399071c2":"code","04a593dd":"code","4001d2ab":"code","dbe6532b":"code","1b27d875":"code","2840af4d":"code","6f8e4cde":"code","c6c0e52b":"code","d5b9abea":"code","26fd9b03":"code","6e3b8d3f":"code","06bcaf79":"code","8f2de5d6":"code","db08fcac":"code","fcbd7a36":"code","d7819998":"code","e9987303":"code","eb07a022":"code","ef0ff534":"code","34ec3d8e":"code","263567c3":"markdown","7a9c8da7":"markdown","9b5ee1cb":"markdown","d9511d2c":"markdown","5463658d":"markdown","101b50af":"markdown","439b7729":"markdown","4047017d":"markdown","a3bd131f":"markdown","e5d2419b":"markdown","ebf7d57c":"markdown","3932e606":"markdown","8ae2809d":"markdown","5914d85a":"markdown","100df0b0":"markdown","5e514821":"markdown","3e0c04b1":"markdown","62be8de7":"markdown","9801c55e":"markdown","d54f402e":"markdown","fce1870a":"markdown","b57d1b71":"markdown"},"source":{"5f3e9bac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","66240803":"#Getting the required data sets\ntrain = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\n\n","b1383c12":"train.head()","10f0ba41":"#Getting the shape of the data\nprint(train.shape)\nprint(test.shape)","8cbbc9a3":"# A simple function to generate various different random colors that can be used to give different colors to your visualizations\n\ndef random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors\n","a312be8e":"# Checking the Null Values\nprint(train.isnull().sum())\ntest.isnull().sum()","18a1f284":"train.dropna(inplace=True)","e08abac3":"train.head()","c087840b":"train.describe()","42bf2872":"#Checking the distribuition of tweets\ndist = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ndist.style.background_gradient(cmap='Purples')","64517ad5":"#Making a count plot\nplt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","676391ee":"#Making a funnle chart for better visualization\nfig = go.Figure(go.Funnelarea(\n    text =dist.sentiment,\n    values = dist.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","f021e30b":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","41909ce5":"results_jaccard=[]\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","708fd6e6":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\n\ntrain = train.merge(jaccard,how='outer')","855aa30f":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","736a505a":"train.head()","625163e3":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"b\")","9d16ce2f":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","fc5b00f3":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","12c13f85":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","07d30f91":"#Jaccard Score\nplt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","520fb507":"new_k = train[train['Num_word_text']<=2]","014ff1a2":"new_k.groupby('sentiment').mean()['jaccard_score']","0b2e22bc":"new_k[new_k['sentiment']=='positive']","063f18a4":"new_k[new_k['sentiment']=='neutral']","1df99fbc":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","3840f703":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","e50714c9":"train.head()","e6da407b":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","090c4789":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","e43d4daa":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","de96226f":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","0114aea7":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","2560d44f":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","d5dfd209":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","527e708f":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","c8e925af":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","a7c61df6":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","266bd460":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","7d0ab0ee":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","1fc7f130":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","b3ffe9f2":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","6448fcd2":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","8f3ce6d6":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","482bd123":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","d9bf0191":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","902c9453":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","7f8addab":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","d42e7ba3":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","399071c2":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","04a593dd":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","4001d2ab":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","dbe6532b":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","1b27d875":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","2840af4d":"plot_wordcloud(Neutral_sent.text,max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","6f8e4cde":"plot_wordcloud(Positive_sent.text,title=\"Word Cloud Of Positive tweets\",title_size=30)","c6c0e52b":"plot_wordcloud(Negative_sent.text,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","d5b9abea":"df_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","26fd9b03":"df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set","6e3b8d3f":"df_train = df_train[df_train['Num_words_text']>=3]","06bcaf79":"def save_model(output_dir, nlp, new_model_name):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","8f2de5d6":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","db08fcac":"def get_model_out_path(sentiment):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    return model_out_path","fcbd7a36":"def get_training_data(sentiment):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","d7819998":"sentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n# For DEmo Purposes I have taken 3 iterations you can train the model as you want\ntrain(train_data, model_path, n_iter=3, model=None)","e9987303":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=3, model=None)","eb07a022":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","ef0ff534":"selected_texts = []\nMODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","34ec3d8e":"df_submission['selected_text'] = df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","263567c3":"# Since in this competition we have to predict selected text, I will generate meta fetaures according to jaccard scores as those will give a better efect on the prediction. I will calculate jaccard similarities","7a9c8da7":"## We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n    \n## It will be interesting to see the word unique to different sentiments\n## Let's Look at Unique Words in each Segment\n\n## We will look at unique words in each segment in the Following Order:\n### Positive\n### Negative\n### Neutral\n\n","9b5ee1cb":"# Modelling\n\n## Modelling the Problem as NER\n\n### Named Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories. ","d9511d2c":"# Most common words Sentiments Wise\n\n## Let's look at the most common words in different sentiments\n","5463658d":"# So we have text Id , text, selected_text and then sentiment for the text.","101b50af":"### Training positive sentiment","439b7729":"# Conclusion Of EDA\n* We can see from the jaccard score plot that there is peak for negative and positive plot around score of 1 .That means there is a cluster of tweets where there is a high similarity between text and selected texts ,if we can find those clusters then we can predict text for selected texts for those tweets irrespective of segment\n\n* Let's see if we can find those clusters,one interesting idea would be to check tweets which have number of words lesss than 3 in text, because there the text might be completely used as text","4047017d":"## Training the model","a3bd131f":"# This clearly shows that the data is skewed towards neutral sentiment","e5d2419b":"# EDA Analysis","ebf7d57c":"# Since we have only one Null value we will drop it ","3932e606":"# It looks like selected text is a subset of of text. ","8ae2809d":"# This competition is a little different than the normal ones where we have to just determine the polarity of the tweet. Since we already have the polarity here we have to determine the words\/sentences that can give us that polarity","5914d85a":"Cleaning the Corpus\n\nNow Before We Dive into extracting information out of words in text and selected text,let's first clean the data\n","100df0b0":"# Most Common words in Text\n## Let's also look at the most common words in Text\n","5e514821":"# Evaluation Metric:\n\n## In this competition the evaluation metric is Jacard Score. This is majorly used in majority CNN tasks.\n## If you like this kernal please do give an upvote","3e0c04b1":"* Thus its clear that most of the times , text is used as selected text.We can improve this by preprocessing the text which have word length less than 3.We will remember this information and use it in model building","62be8de7":"# Meta-Features","9801c55e":"# Calculating the most common words in the target selection","d54f402e":"# Removing stop words","fce1870a":"## We can see a lot of similarities in this especially in the neutral category. Lets look more closely","b57d1b71":"# WordClouds\n\n## I will be building wordclouds in the following order:\n### WordCloud of Neutral Tweets\n### WordCloud of Positive Tweets\n### WordCloud of Negative Tweets\n"}}