{"cell_type":{"278de252":"code","93b8261f":"code","e5204860":"code","3d740c52":"code","718de045":"code","b322a8cf":"code","3cc10527":"code","cd0e700c":"code","99570464":"code","14347afc":"code","9f98deef":"code","9a50f479":"code","e296a833":"code","70f8eda0":"code","e241871e":"code","eb2b6e49":"code","254607d8":"code","962faf6f":"code","01ee0b0c":"code","5a4726b6":"code","1a7a4622":"code","ba8b3c2e":"code","59c99bcf":"code","62436cd6":"code","57600d88":"code","52ed03a2":"code","b87b0da3":"code","f5386b9b":"code","e5eb69db":"code","13416cc5":"code","0e0b0628":"code","73f6bbe4":"code","e9e11590":"code","6c399190":"code","50800ff5":"code","7fac92ab":"code","9f9a01d3":"code","66dd3d39":"code","c0d3b493":"code","1c8a4d49":"markdown","6254ee15":"markdown","528f4186":"markdown","9f2e9d76":"markdown"},"source":{"278de252":"\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nimport string\n\n#!pip install readability\nimport sys\nsys.path = [\n    '..\/input\/readability-package',\n] + sys.path\nimport readability\nimport spacy\n# import textstat\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","93b8261f":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.head(2)","e5204860":"ex_excerpt = train_df.iloc[0].excerpt\nex_excerpt","3d740c52":"sns.distplot(train_df[\"target\"])","718de045":"sns.distplot(train_df[\"standard_error\"])","b322a8cf":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.head(2)","3cc10527":"def readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    words_count = results['sentence info']['words']\n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    sentences_per_paragraph = results['sentence info']['sentences_per_paragraph']\n    type_token_ratio = results['sentence info']['type_token_ratio']\n    syllables = results['sentence info']['syllables']\n    wordtypes = results['sentence info']['wordtypes']\n    word_diversity = wordtypes\/words_count\n    sentences = results['sentence info']['sentences']\n    paragraphs = results['sentence info']['paragraphs']\n    long_words = results['sentence info']['long_words']\n    complex_words = results['sentence info']['complex_words'] \n    complex_words_dc = results['sentence info']['complex_words_dc'] \n    #14\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    #9\n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    #6\n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n    #6\n    \n    return [chars_per_word, syll_per_word, words_per_sent,sentences_per_paragraph,word_diversity,\n            type_token_ratio,\n            syllables,\n            words_count,\n            wordtypes,\n            sentences,\n            paragraphs,\n            long_words,\n            complex_words,\n            complex_words_dc,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","cd0e700c":"def spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\n    https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","99570464":"def pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","14347afc":"def generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    qut = passage.count(\"\u2018\")\n    \n    # Some other stats\n#     num_char = len(passage)\n#     num_words = len(passage.split(\" \"))\n#     unique_words = len(set(passage.split(\" \") ))\n#     word_diversity = unique_words\/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            longest_word, avg_len_word]","9f98deef":"def create_folds(data: pd.DataFrame, num_splits: int):\n    \"\"\" \n    This function creates a kfold cross validation system based on this reference: \n    https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n    \"\"\"\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","9a50f479":"class CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\"sentences_per_paragraph\",\n            \"word_diversity\",\"type_token_ratio\",\n            \"syllables\",\n            \"words_count\",\n            \"wordtypes\",\n            \"sentences\",\n            \"paragraphs\",\n            \"long_words\",\n            \"complex_words\",\n            \"complex_words_dc\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","e296a833":"dataset = CLRDataset(train_df, train=True)\ndf = dataset.get_df()\ndf.head() # train dataframe","70f8eda0":"# This is just here to investigate different features\nplt.scatter((df[\"smog\"]), df[\"target\"])\nplt.show()","e241871e":"test_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\ntest_df.head(2) # test dataframe","eb2b6e49":"def set_seed(seed=42):\n    \"\"\" Sets the Seed \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(42)","254607d8":"features = [\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\"sentences_per_paragraph\",\n            \"type_token_ratio\",\"syllables\",\"words_count\",\"wordtypes\",\"sentences\",\"paragraphs\",\n            \"long_words\",\"complex_words\",\"complex_words_dc\",\n            \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n            \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n            \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"]\nfeatures+=get_spacy_col_names()\nfeatures+=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n            \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n            \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n\nfeatures+= [ \"int_count\",\"fer_score\",\"non_fer_count\",\"non_fer_pres\",\"std_sentences_len\",\"std_words_len\",\"mean_of_top10_words_len\",\n            \"mean_of_top2_sentences_len\",\"mean_of_tail2_sentences_len\",#\"periods\",\"commas\", \"semis\", \"exclaims\", \"questions\",\n                                        \"word_diversity\",\n                                         \"longest_word\", #\"avg_len_word\",\n\n#\"Kincaid_sen_mean\",\"ARI_sen_mean\",\"Coleman-Liau_sen_mean\",\"FleschReadingEase_sen_mean\",\"GunningFogIndex_sen_mean\",\"LIX_sen_mean\",\"SMOGIndex_sen_mean\",\"RIX_sen_mean\",\"DaleChallIndex_sen_mean\"\n# \"Kincaid_sen_std\",\"ARI_sen_std\",\"Coleman-Liau_sen_std\",\"FleschReadingEase_sen_std\",\"GunningFogIndex_sen_std\",\"LIX_sen_std\",\"SMOGIndex_sen_std\",\"RIX_sen_std\",\"DaleChallIndex_sen_std\",\n\"Kincaid_sen_max\",\"ARI_sen_max\",\"Coleman-Liau_sen_max\",\"FleschReadingEase_sen_max\",\"GunningFogIndex_sen_max\",\"LIX_sen_max\",\"SMOGIndex_sen_max\",\"RIX_sen_max\",\"DaleChallIndex_sen_max\"\n           ]\n# currently better results without the other_df features","962faf6f":"en_fer_words = pd.read_csv(r\"..\/input\/english-word-frequency\/unigram_freq.csv\")\nen_fer_words.index = en_fer_words.word\nen_fer_words.drop(\"word\",axis=1,inplace=True)\n\nen_fer_words[\"count\"] = en_fer_words[\"count\"]\/23135851162 #max\nen_fer_words = en_fer_words.iloc[:60000,:]\nf_w = dict(zip(list(en_fer_words.index) , list(en_fer_words[\"count\"])))","01ee0b0c":"delimiters = \" \" , \",\" , \".\" , \";\" , \"!\", \"\\n\", \"?\"\nregexPattern = \"|\".join(map(re.escape, delimiters))\n\n\n\ndf[\"fer_score\"] = 0.0\ndf[\"non_fer_count\"] = 0.0\ndf[\"non_fer_pres\"] = 0.0\ndf[\"int_count\"] = 0.0\nen_fer_words = en_fer_words[\"count\"]\nfor j in tqdm(df.index):\n    all_words = re.split(regexPattern, re.sub(r'[^\\w\\s]', '', df.excerpt[j]).replace(\"\\n\", \" \"))\n    l = 0\n    r = 0\n    o = 0\n#     print(all_words)\n    for i in all_words:\n        fer_read = f_w.get(i.lower(), 0)\n        l +=fer_read\n\n        if fer_read == 0:\n            try:\n                int(i)\n                o+=1\n            except:\n                if i == \" \" or i == \"\":\n                    continue\n                else:\n                    r +=1\n    df.loc[j,[\"fer_score\",\"non_fer_count\",\"non_fer_pres\",\"int_count\"]] = l , r , r\/len(all_words) , o\n    \n\n    \n    \n\ntest_df[\"fer_score\"] = 0.0\ntest_df[\"non_fer_count\"] = 0.0\ntest_df[\"non_fer_pres\"] = 0.0\ntest_df[\"int_count\"] = 0.0\nen_fer_words = en_fer_words[\"count\"]\nfor j in tqdm(test_df.index):\n    all_words = re.split(regexPattern, re.sub(r'[^\\w\\s]', '', test_df.excerpt[j]).replace(\"\\n\", \" \"))\n    l = 0\n    r = 0\n    o = 0\n    \n    for i in all_words:\n        fer_read = f_w.get(i.lower(), 0)\n        l +=fer_read\n        if fer_read == 0:\n            try:\n                int(i)\n                o+=1\n            except:\n                if i == \" \" or i == \"\":\n                    continue\n                else:\n                    r +=1\n              \n    test_df.loc[j,[\"fer_score\",\"non_fer_count\",\"non_fer_pres\",\"int_count\"]] = l , r , r\/len(all_words) , o","5a4726b6":"df[[\"fer_score\",\"non_fer_count\",\"non_fer_pres\",\"int_count\"]]","1a7a4622":"df[[\"Kincaid_sen_mean\",\"ARI_sen_mean\",\"Coleman-Liau_sen_mean\",\"FleschReadingEase_sen_mean\",\"GunningFogIndex_sen_mean\",\"LIX_sen_mean\",\"SMOGIndex_sen_mean\",\"RIX_sen_mean\",\"DaleChallIndex_sen_mean\"]] = 0.0\ndf[[\"Kincaid_sen_std\",\"ARI_sen_std\",\"Coleman-Liau_sen_std\",\"FleschReadingEase_sen_std\",\"GunningFogIndex_sen_std\",\"LIX_sen_std\",\"SMOGIndex_sen_std\",\"RIX_sen_std\",\"DaleChallIndex_sen_std\"]] = 0.0\ndf[[\"Kincaid_sen_max\",\"ARI_sen_max\",\"Coleman-Liau_sen_max\",\"FleschReadingEase_sen_max\",\"GunningFogIndex_sen_max\",\"LIX_sen_max\",\"SMOGIndex_sen_max\",\"RIX_sen_max\",\"DaleChallIndex_sen_max\"]] = 0.0\ndelimiters = \" \" , \",\" , \".\" , \";\" , \"!\", \"\\n\", \"?\"\nregexPattern = \"|\".join(map(re.escape, delimiters))\n\ndf[\"words\"] = df.excerpt.apply(lambda x:re.split(regexPattern, x))\ndf[\"std_words_len\"] = 0.0\ndf[\"mean_of_top10_words_len\"] = 0.0\nfor i in tqdm(df.index):\n    words_list = df.words[i]\n    b = []\n    for j in words_list:\n        b.append(len(j))\n    \n    \n    result = filter(lambda x: x >1, b)\n    b = list(result)\n    \n    df.std_words_len[i] = np.std(b)\n    b.sort()\n    df[\"mean_of_top10_words_len\"][i] = np.mean(b[-10:])\n    \n    \n\n    \ndelimiters = \".\" , \"!\" , \"\\n\" , \";\"  ,\"?\"\nregexPattern = \"|\".join(map(re.escape, delimiters))\n\n\ndf[\"sentences_list\"] = df.excerpt.apply(lambda x:re.split(regexPattern, x))\ndf[\"std_sentences_len\"] = 0.0\ndf[\"mean_of_top2_sentences_len\"] = 0.0\ndf[\"mean_of_tail2_sentences_len\"] = 0.0\nfor i in tqdm(df.index):\n    sent_list = df.sentences_list[i]\n    b = []\n    sent_score = []\n    \n    result = filter(lambda x: len(x) >2, df[\"sentences_list\"][i])\n    sent_list = list(result)\n    for j in sent_list:\n        sent_score.append(list(readability.getmeasures(j, lang='en')['readability grades'].values()))\n\n        b.append(len(j))\n        \n    sent_score_std = np.std(sent_score,axis=0)\n    sent_score_mean = np.mean(sent_score,axis=0) \n    sent_score_max = np.max(sent_score,axis=0) \n    \n    \n    df.std_sentences_len[i] = np.std(b)\n    b.sort()\n    df[\"mean_of_top2_sentences_len\"][i] = np.mean(b[-2:])\n    df[\"mean_of_tail2_sentences_len\"][i] = np.mean(b[:2])\n    df.loc[i,[\"Kincaid_sen_mean\",\"ARI_sen_mean\",\"Coleman-Liau_sen_mean\",\"FleschReadingEase_sen_mean\",\"GunningFogIndex_sen_mean\",\"LIX_sen_mean\",\"SMOGIndex_sen_mean\",\"RIX_sen_mean\",\"DaleChallIndex_sen_mean\"]] = sent_score_mean\n    df.loc[i,[\"Kincaid_sen_std\",\"ARI_sen_std\",\"Coleman-Liau_sen_std\",\"FleschReadingEase_sen_std\",\"GunningFogIndex_sen_std\",\"LIX_sen_std\",\"SMOGIndex_sen_std\",\"RIX_sen_std\",\"DaleChallIndex_sen_std\"]] = sent_score_std\n    df.loc[i,[\"Kincaid_sen_max\",\"ARI_sen_max\",\"Coleman-Liau_sen_max\",\"FleschReadingEase_sen_max\",\"GunningFogIndex_sen_max\",\"LIX_sen_max\",\"SMOGIndex_sen_max\",\"RIX_sen_max\",\"DaleChallIndex_sen_max\"]] = sent_score_max\n","ba8b3c2e":"df","59c99bcf":"test_df[[\"Kincaid_sen_mean\",\"ARI_sen_mean\",\"Coleman-Liau_sen_mean\",\"FleschReadingEase_sen_mean\",\"GunningFogIndex_sen_mean\",\"LIX_sen_mean\",\"SMOGIndex_sen_mean\",\"RIX_sen_mean\",\"DaleChallIndex_sen_mean\"]] = 0.0\ntest_df[[\"Kincaid_sen_std\",\"ARI_sen_std\",\"Coleman-Liau_sen_std\",\"FleschReadingEase_sen_std\",\"GunningFogIndex_sen_std\",\"LIX_sen_std\",\"SMOGIndex_sen_std\",\"RIX_sen_std\",\"DaleChallIndex_sen_std\"]] = 0.0\ntest_df[[\"Kincaid_sen_max\",\"ARI_sen_max\",\"Coleman-Liau_sen_max\",\"FleschReadingEase_sen_max\",\"GunningFogIndex_sen_max\",\"LIX_sen_max\",\"SMOGIndex_sen_max\",\"RIX_sen_max\",\"DaleChallIndex_sen_max\"]] = 0.0\n\ndelimiters = \" \" , \",\" , \".\" , \";\", \"!\", \"\\n\", \"?\" \nregexPattern = \"|\".join(map(re.escape, delimiters))\n\ntest_df[\"words\"] = test_df.excerpt.apply(lambda x:re.split(regexPattern, x))\n\ntest_df[\"words\"] = test_df.excerpt.apply(lambda x:x.split())\ntest_df[\"std_words_len\"] = 0.0\ntest_df[\"mean_of_top10_words_len\"] = 0.0\nfor i in tqdm(test_df.index):\n    words_list = test_df.words[i]\n    b = []\n    for j in words_list:\n        b.append(len(j))\n    \n    \n    result = filter(lambda x: x >1, b)\n    b = list(result)\n    \n    test_df.std_words_len[i] = np.std(b)\n    b.sort()\n    test_df[\"mean_of_top10_words_len\"][i] = np.mean(b[-10:])\n    \n    \n\n    \ndelimiters = \".\" , \"!\" , \"\\n\" , \";\", \"?\" \nregexPattern = \"|\".join(map(re.escape, delimiters))\n\n\ntest_df[\"sentences_list\"] = test_df.excerpt.apply(lambda x:re.split(regexPattern, x))\ntest_df[\"std_sentences_len\"] = 0.0\ntest_df[\"mean_of_top2_sentences_len\"] = 0.0\ntest_df[\"mean_of_tail2_sentences_len\"] = 0.0\nfor i in tqdm(test_df.index):\n    sent_list = test_df.sentences_list[i]\n    b = []\n    sent_score = []\n    result = filter(lambda x: len(x) >2, test_df[\"sentences_list\"][i])\n    sent_list = list(result)\n    for j in sent_list:\n        sent_score.append(list(readability.getmeasures(j, lang='en')['readability grades'].values()))\n        b.append(len(j))\n        \n    sent_score_std = np.std(sent_score,axis=0)\n    sent_score_mean = np.mean(sent_score,axis=0) \n    sent_score_max = np.max(sent_score,axis=0) \n    \n    result = filter(lambda x: x >2, b)\n    b = list(result)\n    \n    test_df.std_sentences_len[i] = np.std(b)\n    b.sort()\n    test_df[\"mean_of_top2_sentences_len\"][i] = np.mean(b[-2:])\n    test_df[\"mean_of_tail2_sentences_len\"][i] = np.mean(b[:2])\n    test_df.loc[i , [\"Kincaid_sen_mean\",\"ARI_sen_mean\",\"Coleman-Liau_sen_mean\",\"FleschReadingEase_sen_mean\",\"GunningFogIndex_sen_mean\",\"LIX_sen_mean\",\"SMOGIndex_sen_mean\",\"RIX_sen_mean\",\"DaleChallIndex_sen_mean\"]] = sent_score_mean\n    test_df.loc[i ,[\"Kincaid_sen_std\",\"ARI_sen_std\",\"Coleman-Liau_sen_std\",\"FleschReadingEase_sen_std\",\"GunningFogIndex_sen_std\",\"LIX_sen_std\",\"SMOGIndex_sen_std\",\"RIX_sen_std\",\"DaleChallIndex_sen_std\"]] = sent_score_std\n    test_df.loc[i ,[\"Kincaid_sen_max\",\"ARI_sen_max\",\"Coleman-Liau_sen_max\",\"FleschReadingEase_sen_max\",\"GunningFogIndex_sen_max\",\"LIX_sen_max\",\"SMOGIndex_sen_max\",\"RIX_sen_max\",\"DaleChallIndex_sen_max\"]] = sent_score_max\n    ","62436cd6":"'DaleChallIndex_sen_meanKincaid_sen_std' in features","57600d88":"\"\"\" I normalize the data here, could be useful depending on your model\"\"\"\nscaler = MinMaxScaler()\ndf[features] = scaler.fit_transform(df[features])\ntest_df[features] = scaler.transform(test_df[features])","52ed03a2":"def train_pred_one_fold(model_name: str, fold: int, df: pd.DataFrame, test_df: pd.DataFrame, features: list, rmse: list):\n    \"\"\"\n    This function trains and predicts on one fold of your selected model\n    df is the train df, test_df is the test_df\n    X features are defined in features\n    y output is target\n    oof score is printed and stored in the rmse list\n    \"\"\"\n    train = df[df.kfold == fold]\n    X_train = train[features]\n    y_train = train[\"target\"]\n \n    valid = df[df.kfold != fold]\n    X_valid = valid[features]\n    y_valid = valid[\"target\"]\n    \n    X_test = test_df[features]\n\n    if model_name == 'ridge':\n        model = Ridge(alpha=5)    \n        model.fit(X_train, y_train)\n        oof = model.predict(X_valid)\n        print(np.sqrt(mean_squared_error(y_valid, oof)))\n        rmse.append(np.sqrt(mean_squared_error(y_valid, oof)))\n        test_preds = model.predict(X_test)\n        \n    else:\n        test_preds = 0\n        raise Exception(\"Not Implemented\")\n        \n    return test_preds","b87b0da3":"def train_pred(model_name: str, df: pd.DataFrame, test_df: pd.DataFrame, features: list):\n    \"\"\"\n    This function trains and predicts multiple fold using train_pred_one_fold\n    The average rmse is printed the the test data predictions are returned\n    The last column is the average result from all folds to be submitted\n    \"\"\"\n    print(f\"model_name: {model_name}\")\n    all_preds = pd.DataFrame()\n    rmse = list()\n    for f in range(2):\n        all_preds[f\"{model_name}_{f}\"] = train_pred_one_fold(model_name, f, df, test_df, features, rmse)\n\n    all_preds[f\"{model_name}\"] = all_preds.mean(axis=1)\n    print(\"---------\")\n    print(f\"avg rmse: {np.mean(rmse)}\")\n    return all_preds","f5386b9b":"def prep_sub(preds: pd.DataFrame, col_name: str):\n    \"\"\"\n    This function takes an output prediction df from train_pred\n    and sets it to a format that can be submitted to the competition\n    \"\"\"\n    sub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n    sub[\"target\"] = preds[col_name]\n    sub.to_csv(\"submission.csv\", index=False)\n    return sub","e5eb69db":"ridge_preds = train_pred('ridge', df, test_df, features)\nridge_preds","13416cc5":"# sub = prep_sub(ridge_preds, 'ridge')\n# sub","0e0b0628":"#word level sen analysisaa\n#hard & easy & fer* words\n#formal - informal\n#libscore - word - sent\n","73f6bbe4":"# en_fer_words_dict = dict()\n# for i in tqdm(en_fer_words.index):\n#     en_fer_words_dict[str(i)] = en_fer_words[\"count\"][str(i)]","e9e11590":"# df.to_csv(r\".\/data.csv\")","6c399190":"ridge_preds[[\"xgb1\" , \"xgb2\",\"xgb3\",\"xgb4\",\"xgb5\"]] = 0","50800ff5":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=5)\n\na = []\nz = 1\nfor X,Y in kfold.split(df.index):\n    model = xgb.XGBRegressor(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n    \n    model.fit(df.loc[X,features] , df.loc[X,\"target\"])\n    a.append(model.feature_importances_)\n    predsxg = model.predict(df.loc[Y,features])\n    print(np.sqrt(mean_squared_error(predsxg , df.loc[Y,\"target\"])))\n    ridge_preds[f\"xgb{z}\"] = model.predict(test_df.loc[:,features])\n    z+=1","7fac92ab":"features_imp = pd.DataFrame(np.mean(a, axis=0) , features).sort_values(by=0 , ascending=False).head(80)","9f9a01d3":"plt.figure(figsize=(9,22))\nplt.barh(features_imp.index, features_imp.iloc[:,0])\n","66dd3d39":"ridge_preds[\"ridge\"] =np.mean(ridge_preds.loc[:,[\"ridge_0\" , \"ridge_1\" , \"xgb3\"  ,\"xgb5\"]].values, axis=1)","c0d3b493":"sub = prep_sub(ridge_preds, 'ridge')\nsub","1c8a4d49":"# Overview\n\nThis notebook was my attempt to achieve a score comparable to baseline neural networks such as pretrained transformers with only a simple ridge regression algorithm. Even more important than the score, is the feature engineering in this notebook. \n\nThis notebook uses the readability library ([link to offline version dataset](https:\/\/www.kaggle.com\/ravishah1\/readability-package) - see how I use it below as submissions must be without internet) to generate 24 powerful traditional features. These features range from common statistics such as words per sentence to readability scoring measures such as kincaid. I also use the spacy libraries en_core_web_lg to generate 300 features. Lastly I incorporate 31 part of speech tag features using nltk. \n\nHopefully combining these features with more advanced models will help you improve your score.\n\nI hope you find these features useful. Upvote if you use these features. Comment questions and suggestions. I'll probably add more features to this notebook in the future.\n\nVersion Summary:\n\nV1-6: Incomplete versions may have errors and bugs\n\nV7: The original version\n\nV8-9: Some experimental ideas with pearson's correlation and feature transformations - currently no improvement\n\nV10\/11: Same as V7 but fixed duplicate column name bug and typos","6254ee15":"# Feature Engineering","528f4186":"# Peeking at the Data","9f2e9d76":"# Modeling"}}