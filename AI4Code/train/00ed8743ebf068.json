{"cell_type":{"7c5f0e2d":"code","ee862ca6":"code","f20d904d":"code","14d0ce27":"code","162769fb":"code","ff78caf5":"code","7a26aea3":"code","fac7300c":"code","59c4d05c":"code","9bdedb97":"code","5734f4e1":"code","c24455df":"code","9627887e":"code","d6e581ed":"code","76b350bd":"code","41a9effd":"code","6b626bab":"code","85965d5b":"code","6ee090e8":"code","4e248792":"code","c74c0e87":"code","4903069a":"code","f3386622":"code","0ace05eb":"code","195dda18":"code","a55891ef":"code","d9101ffa":"code","5cbac437":"code","ca9bf8a6":"code","4df94d32":"code","3e815c13":"markdown","3b4f60ca":"markdown","bd3f639b":"markdown","e78aa889":"markdown","4ecd1d67":"markdown","908f4b8c":"markdown","b35c1095":"markdown","fd0a9c03":"markdown","d4eec23f":"markdown","9933f91b":"markdown","7c541521":"markdown","dd4a9670":"markdown","ef8e3841":"markdown","678e883b":"markdown","4a23e740":"markdown","91d7ddf5":"markdown","900dab8e":"markdown","b1b0e50d":"markdown","036fc162":"markdown","80b09476":"markdown","b6c5edc6":"markdown","7347a7f6":"markdown","a051acac":"markdown","174300fb":"markdown","c3b38806":"markdown","0df1bc24":"markdown","bde342db":"markdown","936453f3":"markdown","e199b6e1":"markdown","71d357c5":"markdown","812617a6":"markdown","33722845":"markdown","f508bfff":"markdown","3b4b7e11":"markdown","b43fed69":"markdown","1790eba0":"markdown","4fd6358c":"markdown","7bec94a3":"markdown","be7c1caf":"markdown","4bc91b4e":"markdown","0aa090b8":"markdown","b815dbe3":"markdown","96fa6113":"markdown","b1dd2a31":"markdown","708bab23":"markdown","b5d4a7da":"markdown","a990fb1c":"markdown","fdd14b06":"markdown","af404db0":"markdown","fa1bc019":"markdown","3a815e46":"markdown","5a353851":"markdown","eca8571d":"markdown","0f7663df":"markdown","91d1a2bc":"markdown","50983cba":"markdown","bb1925ba":"markdown","14bc7785":"markdown","db954383":"markdown","abe8a509":"markdown","a44ffe4f":"markdown","2c5ca26b":"markdown","69e88ce9":"markdown"},"source":{"7c5f0e2d":"import numpy as np\nimport random\nimport pandas as pd\nfrom pandas.tools import plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)  \nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import  accuracy_score\n\n\nimport xgboost as xgb\nimport lightgbm as  lgb\nfrom xgboost.sklearn import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\n# auxiliary function\nfrom sklearn.preprocessing import LabelEncoder\ndef random_colors(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","ee862ca6":"df = pd.read_csv('..\/input\/Iris.csv')\ntable = ff.create_table(df.head())\n","f20d904d":"py.iplot(table,filename='jupyter-table1')\n","14d0ce27":"py.iplot(ff.create_table(df.describe()),filename='jupyter-table1')\n","162769fb":"df.info()\n","ff78caf5":"Species = df['Species'].unique()\nSpecies\n","7a26aea3":"species_count = df['Species'].value_counts()\ndata = [go.Bar(\n    x = species_count.index,\n    y = species_count.values,\n    marker = dict(color = random_colors(3))\n)]\npy.iplot(data)\n","fac7300c":"corelation = df.corr()\ndata = [go.Heatmap(z = np.array(corelation.values),\n                   x = np.array(corelation.columns),\n                   y = np.array(corelation.columns),\n                     colorscale='Blackbody',)\n       ]\npy.iplot(data)\n","59c4d05c":"setosa = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-setosa'], y = df['SepalWidthCm'][df.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-versicolor'], y = df['SepalWidthCm'][df.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = df['SepalLengthCm'][df.Species =='Iris-virginica'], y = df['SepalWidthCm'][df.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nfig = dict(data=data)\npy.iplot(fig, filename='styled-scatter')","9bdedb97":"setosa = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-setosa'], y = df['PetalWidthCm'][df.Species =='Iris-setosa']\n                   , mode = 'markers', name = 'setosa')\nversicolor = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-versicolor'], y = df['PetalWidthCm'][df.Species =='Iris-versicolor']\n                   , mode = 'markers', name = 'versicolor')\nvirginica = go.Scatter(x = df['PetalLengthCm'][df.Species =='Iris-virginica'], y = df['PetalWidthCm'][df.Species =='Iris-virginica']\n                   , mode = 'markers', name = 'virginica')\ndata = [setosa, versicolor, virginica]\n\nfig = dict(data=data)\npy.iplot(fig, filename='styled-scatter')","5734f4e1":"trace0 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-setosa'],\n                boxmean=True, name = 'setosa')\n\ntrace1 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-versicolor'],\n                boxmean=True, name = 'versicolor')\n\ntrace2 = go.Box(y=df['PetalWidthCm'][df['Species'] == 'Iris-virginica'],\n                boxmean=True, name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","c24455df":"trace0 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-setosa'],name = 'setosa')\n\ntrace1 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-versicolor'], name = 'versicolor')\n\ntrace2 = go.Box(y=df['PetalLengthCm'][df['Species'] == 'Iris-virginica'], name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","9627887e":"trace0 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-setosa'], name = 'setosa')\n\ntrace1 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-versicolor'], name = 'versicolor')\n\ntrace2 = go.Box(y=df['SepalLengthCm'][df['Species'] == 'Iris-virginica'], name = 'virginica')\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","d6e581ed":"setosa = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-setosa'])\n\nversicolor = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-versicolor'])\n\nvirginica = go.Box(y=df['SepalWidthCm'][df['Species'] == 'Iris-virginica'])\n\ndata = [trace0, trace1, trace2]\npy.iplot(data)","76b350bd":"plt.subplots(figsize = (10,8))\nplotting.andrews_curves(df.drop(\"Id\", axis=1), \"Species\")","41a9effd":"g = sns.lmplot(x=\"SepalWidthCm\", y=\"SepalLengthCm\", hue=\"Species\", data=df)","6b626bab":"g = sns.lmplot(x=\"PetalWidthCm\", y=\"PetalLengthCm\", hue=\"Species\", data=df)","85965d5b":"x = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny = df['Species']","6ee090e8":"encoder = LabelEncoder()\ny = encoder.fit_transform(y)","4e248792":"y","c74c0e87":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 101)","4903069a":"lr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)\nlr_predict = lr_model.predict(x_test)\n\nLogReg=accuracy_score(lr_predict,y_test)","f3386622":"svm_model = SVC(kernel='linear')\nsvm_model.fit(x_train,y_train)\nsvc_predict = svm_model.predict(x_test)\n\nSVM_sc=accuracy_score(svc_predict,y_test)","0ace05eb":"nb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\nnb_predict = nb_model.predict(x_test)\n\nNaive_bayes_score=accuracy_score(nb_predict,y_test)","195dda18":"dt_model = DecisionTreeClassifier(max_leaf_nodes=3)\ndt_model.fit(x_train,y_train)\ndt_predict = dt_model.predict(x_test)\n\nDecision_Tree_score=accuracy_score(dt_predict,y_test)","a55891ef":"rfc_model = RandomForestClassifier(max_depth=3)\nrfc_model.fit(x_train,y_train)\nrfc_predict = rfc_model.predict(x_test)\n\nRandom_Forest_score=accuracy_score(rfc_predict,y_test)","d9101ffa":"etc_model = ExtraTreesClassifier()\netc_model.fit(x_train,y_train)\netc_predict = etc_model.predict(x_test)\n\nExtra_Tree_Classifier_score=accuracy_score(etc_predict,y_test)\n","5cbac437":"knn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train,y_train)\nknn_predict = knn_model.predict(x_test)\n\nknn_score=accuracy_score(knn_predict,y_test)","ca9bf8a6":"xg_model = xgb.XGBClassifier()\nxg_model = xg_model.fit(x_train,y_train)\nxg_score=xg_model.score(x_test, y_test)","4df94d32":"print(\"The accuracy of Logistic Regression Model is: \",LogReg)\nprint(\"The accuracy of Decision Tree Model is: \",Decision_Tree_score )\nprint(\"The accuracy of KNN Model is: \",knn_score )\nprint(\"The accuracy of SVM Model is: \",SVM_sc )\nprint(\"The accuracy of Naive Bayes Classifier is: \",Naive_bayes_score )\nprint(\"The accuracy of XG Boost is: \",xg_score)\nprint(\"The accuracy of Random Forest is: \", Random_Forest_score)\nprint(\"The accuracy of Extra Tree Classifier is: \",Extra_Tree_Classifier_score)","3e815c13":"given the coloums are<br>\nSepalLengthCm<br>\nSepalWidthCm<br>\nPetalLengthCm<br>\nPetalWidthCm<br>\nSpecies<br>","3b4f60ca":"**Naive Bayes** is a simple, yet effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.\n(https:\/\/towardsdatascience.com\/introduction-to-naive-bayes-classification-4cffabb1ae54)","bd3f639b":"# <a id='types_species'> Types of Species<\/a>","e78aa889":"Andrews curves are a method for visualizing multidimensional data by mapping each observation onto a function.\n\nSource - https:\/\/dzone.com\/articles\/andrews-curves","4ecd1d67":"# <a id='ml'>What is machine learning ?<\/a><br> ","908f4b8c":"# <a id='list'> List of algorithms<\/a>","b35c1095":"# <a id='lin_sepal'>Linear regression based on sepal<\/a><br>","fd0a9c03":"So we have equally distributed species all are of 50","d4eec23f":"So there are three types of species \n\nIris-setosa<br\/>\nIris-versicolor<br\/>\nIris-virginica<br\/>","9933f91b":"# <a id='value_sepal_length'>Values distribution based on sepal length<\/a><br>","7c541521":"# <a id='load_data'>Load data set<\/a>","dd4a9670":"Lets create a regression plot for both petal and sepal","ef8e3841":"# <a id='lin_petal'>Linear regression based on petal<\/a><br>","678e883b":"# <a id='logistic'>Logistic regression<\/a><br>","4a23e740":"As you can see Iris-setosa Iris-versicolor Iris-virginica are converted to 0, 1, 2 respectively","91d7ddf5":"# <a id='andrew'>Andrew curves<\/a><br>","900dab8e":"# <a id='desc'> Description<\/a>\n","b1b0e50d":"# <a id='value_petal_length'>Values distribution based on petal length<\/a><br>","036fc162":"First we are splitting the data set into training data and testing data which is 7:3 ratio ","80b09476":"# <a id='random'>Random forest<\/a><br>","b6c5edc6":"**Logistic regression** is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\n(https:\/\/www.medcalc.org\/manual\/logistic_regression.php)","7347a7f6":"<a href='#'>Preface<\/a><br>\n<a href='#desc'>description<\/a><br>\n<a href='#about'>About notebook<\/a><br>\n<a href='#load_lib'>Load libraries<\/a><br>\n<a href='#load_data'>Load Dataset<\/a><br>\n<a href='#visual'>Let's Visualize the dataset<\/a><br>\n- <a href='#types_species'> Types of species<\/a>\n- <a href='#corelation'>Corelation between features<\/a><br>\n- <a href='#visual_sepal'>Visualizing species based on sepal length and width<\/a><br>\n- <a href='#visual_petal'>Visualizing species based on petal length and width<\/a><br>\n- <a href='#value_petal_width'>Values distribution based on petal width<\/a><br>\n- <a href='#value_petal_length'>Values distribution based on petal length<\/a><br>\n- <a href='#value_sepal_length'>Values distribution based on sepal length<\/a><br>\n- <a href='#value_sepal_width'>Values distribution based on sepal width<\/a><br>\n- <a href='#andrew'>Andrew curves<\/a><br>\n- <a href='#lin_sepal'>Linear regression based on sepal<\/a><br>\n- <a href='#lin_petal'>Linear regression based on petal<\/a><br>\n\n<a href='#ml'>Machine Learning<\/a><br>\n- <a href='#list'> List of algorithms<\/a>\n- <a href='#logistic'>Logistic regression<\/a><br>\n- <a href='#decision'>Decision tree<\/a><br>\n- <a href='#knn'>KNN<\/a><br>\n- <a href='#svm'>SVM<\/a><br>\n- <a href='#nbc'>Naive Bayes Classification<\/a><br>\n- <a href='#random'>Random forest<\/a><br>\n- <a href='#etc'>Extra Tree Classifier<\/a><br>\n- <a href='#xgboost'>XGBoost<\/a><br>\n- <a href='#lbgm'>LigthGBM<\/a><br>\n\n\n","a051acac":"<img src = \"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*TudH6YvvH7-h5ZyF2dJV2w.jpeg\">","174300fb":"So by the defination we see that we need data and we do have the data (Iris dataset).\nBut how will we test the dataset ?","c3b38806":"<img src=\"http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/Knn_k1_z96jba.png\">","0df1bc24":"**\u201cSupport Vector Machine\u201d (SVM)** is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well.\n\nSupport Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane\/ line).\n(https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/)\n","bde342db":"**Decision tree** is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter \/ differentiator in input variables.\n(https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/)","936453f3":"#  <a id='visual_petal'>Visualizing species based on petal length and width<\/a><br>","e199b6e1":"<img src = \"https:\/\/image.slidesharecdn.com\/logitregression-161121215510\/95\/intro-to-logistic-regression-4-638.jpg?cb=1479765630\">","71d357c5":"The beauty of this powerful algorithm lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage.\n\nIt\u2019s no wonder then that CERN recognized it as the best approach to classify signals from the Large Hadron Collider. This particular challenge posed by CERN required a solution that would be scalable to process data being generated at the rate of 3 petabytes per year and effectively distinguish an extremely rare signal from background noises in a complex physical process. XGBoost emerged as the most useful, straightforward and robust solution.\n\n(https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/an-end-to-end-guide-to-understand-the-math-behind-xgboost\/)","812617a6":"<img src = \"https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-trees-titanic-tutorial.png\">","33722845":"<img src=\"https:\/\/i1.wp.com\/dataaspirant.com\/wp-content\/uploads\/2017\/04\/Random-Forest-Introduction.jpg?resize=690%2C345\">","f508bfff":"For that we will split out data set into three parts train, test, validation sets.<br>\nwe are going to use the scikit-learn library which has all the required functions and machine learning algorithms required for this notebook","3b4b7e11":"**K nearest neighbors** is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.\n(https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/)","b43fed69":"so there is no null values available in the data set\n","1790eba0":"<img src=\"https:\/\/static.vecteezy.com\/system\/resources\/previews\/000\/145\/921\/non_2x\/vector-iris-flower-banner-line-art.jpg\">\n","4fd6358c":"Again based on petal we can easily classify setosa and for versicolor and virginica also we can classify but there is a thin line which should be taken care of","7bec94a3":"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n- Id\n- SepalLengthCm\n- SepalWidthCm\n- PetalLengthCm\n- PetalWidthCm\n- Species","be7c1caf":"We can easily differentiate setosa based on Sepal but for versicolor and virginica its difficult because the data is scattred.","4bc91b4e":"The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.\n\nPlease go through the blog for in-depth description of machine learning\nhttps:\/\/www.expertsystem.com\/machine-learning-definition\/","0aa090b8":"<img src=\"https:\/\/i.imgur.com\/e7MIgXk.png\">","b815dbe3":"**Random Forest** is considered to be a panacea of all data science problems. On a funny note, when you can\u2019t think of any algorithm (irrespective of situation), use random forest!\n\nRandom Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n\n(https:\/\/www.analyticsvidhya.com\/blog\/2016\/04\/complete-tutorial-tree-based-modeling-scratch-in-python\/#nine)","96fa6113":"In this notebook we will look into famous dataset which is iris, we will analyse the dataset with plotly library which is very interactive library in python then later we will apply different macine learning algorithms and see the best accuracy.\n\n","b1dd2a31":"\n(https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/)","708bab23":"We have seen the visualization part\n<br>\nNow lets see the how to apply machine learning to the dataset","b5d4a7da":"# <a id='svm'>SVM<\/a><br>","a990fb1c":"# <a id='nbc'>Naive Bayes Classification<\/a><br>","fdd14b06":"# <a id='decision'>Decision tree<\/a><br>","af404db0":"Since it is a classification problem we will be using<br>\nLogistic regression<br>\nDecision tree<br>\nKNN<br>\nSVM<br>\nNaive Bayes Classification<br>\nRandom forest<br>\nXGBoost<br>","fa1bc019":"# <a id='value_sepal_width'>Values distribution based on sepal width<\/a><br>","3a815e46":"# <a id='knn'>KNN<\/a><br>","5a353851":"- From the above four graph you can see that the distribution of setosa < vericolor < virginica\n- There are few outliers which can be explained by the scatter plot graph.","eca8571d":"# <a id='value_petal_width'>Values distribution based on petal width<\/a><br>","0f7663df":"# <a id='corelation'>Corelation between features<\/a><br>","91d1a2bc":"# <a href='etc'>Extra Tree Classifier<\/a><br>","50983cba":"# <a id='about'>About the notebook<\/a>\n","bb1925ba":"Before we split our data lets look at the output we want to predict.<br> \nWe want to predict the given sepal and petal dimensions follows to which type of species.<br>\nwe have 3 type of species  Iris-setosa Iris-versicolor Iris-virginica.<br>\nWe will convert those species names to a categorical values using label encoding.<br>","14bc7785":"<img src = \"https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/Bayes_rule.png\">","db954383":"# <a id='visual'>Now lets start visualizing the data set<\/a>","abe8a509":"# <a id='load_lib'>Let's load the required libraries<\/a>\n","a44ffe4f":"Now, we can check the accuracy score of different models which we applied on the dataset.","2c5ca26b":"# <a id='xgboost'>XGBoost<\/a><br>","69e88ce9":"# <a id='visual_sepal'>Visualizing species based on Sepal length and width<\/a><br>"}}