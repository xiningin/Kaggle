{"cell_type":{"5c66d469":"code","88a05012":"code","b32a3b91":"code","50a2e7e4":"code","fcebeacb":"code","79dc19a2":"code","96beb2b3":"code","3918d993":"code","6f9d0307":"code","8765fa81":"code","6b7faa51":"code","984a2c04":"code","b027462f":"code","818e97a7":"code","9db3bb35":"code","8a73aca7":"code","ca6c6acc":"code","1c423a05":"code","0069ad56":"code","bdba3f1e":"code","c172b11d":"code","223f9e3a":"markdown","0d1c6cb1":"markdown","036c352e":"markdown","4e9dc626":"markdown","e5039786":"markdown","d14c35b1":"markdown","a06180c5":"markdown","f0159c60":"markdown","6a5a2d5c":"markdown","ad5952c3":"markdown","833ac208":"markdown","7f0c3dc2":"markdown","3252a74e":"markdown","b62a1835":"markdown","c9ef59d0":"markdown","e7a4946e":"markdown","b9af3031":"markdown","c6691d29":"markdown","12820c0f":"markdown","814e2226":"markdown","f565039a":"markdown"},"source":{"5c66d469":"# All imports\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openslide\nimport pandas as pd\nimport skimage.io\nfrom skimage import morphology\nimport pprint\nimport cv2\nimport json\n\n\ndef otsu_filter(channel, gaussian_blur=True):\n    \n    \"\"\"Otsu filter.\"\"\"\n    \n    if gaussian_blur:\n        channel = cv2.GaussianBlur(channel, (5, 5), 0)\n    channel = channel.reshape((channel.shape[0], channel.shape[1]))\n\n    return cv2.threshold(channel, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\ndef detect_tissue(input_slide, sensitivity=3000):\n    \n    \"\"\"\n    Description\n    ----------\n    Find RoIs containing tissue in WSI.\n    Generate mask locating tissue in an WSI. Inspired by method used by\n    Wang et al. [1]_.\n    .. [1] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew\n    H. Beck, \"Deep Learning for Identifying Metastatic Breast Cancer\",\n    arXiv:1606.05718\n    Credit: Github-wsipre\n    \n    Parameters\n    ----------\n    input_slide: numpy array\n        Slide to detect tissue on.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 3000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n        \n    Returns (3)\n    -------\n    -Tissue binary mask as numpy 2D array, \n    -Tiers investigated,\n    -Time Stamps from running tissue detection pipeline\n    \"\"\"\n    \n    # For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n\n    # Convert from RGB to HSV color space\n    slide_hsv = cv2.cvtColor(input_slide, cv2.COLOR_BGR2HSV)\n    time_stamps[\"re-color\"] = time.time()\n    # Compute optimal threshold values in each channel using Otsu algorithm\n    _, saturation, _ = np.split(slide_hsv, 3, axis=2)\n\n    mask = otsu_filter(saturation, gaussian_blur=True)\n    time_stamps[\"filter\"] = time.time()\n    # Make mask boolean\n    mask = mask != 0\n\n    mask = morphology.remove_small_holes(mask, area_threshold=sensitivity)\n    mask = morphology.remove_small_objects(mask, min_size=sensitivity)\n    time_stamps[\"morph\"] = time.time()\n    mask = mask.astype(np.uint8)\n    mask_contours, tier = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    time_stamps[\"contour\"] = time.time()\n    time_stamps = {\n        key: (value - time_stamps[\"start\"]) * 1000 for key, value in time_stamps.items()\n    }\n    return mask_contours, tier, time_stamps\n\n\ndef draw_tissue_polygons(input_slide, tissue_contours, plot_type, line_thickness=None):\n    \n    \"\"\"\n    Description\n    ----------\n    Plot Tissue Contours as numpy array on.\n    Credit: Github-wsipre\n\n    Parameters\n    ----------\n    input_slide: numpy array\n        Slide to draw contours onto\n    tissue_contours: numpy array \n        These are the identified tissue regions as cv2 contours\n    plot_type: str (\"line\" | \"area\")\n        The desired display type for the tissue regions\n    line_thickness: int\n        If the polygon_type==\"line\" then this parameter sets thickness\n\n    Returns (1)\n    -------\n    - Numpy array of tissue mask plotted\n    \"\"\"\n\n    tissue_color = 1\n\n    for cnt in tissue_contours:\n        if plot_type == \"line\":\n            output_slide = cv2.polylines(input_slide, [cnt], True, tissue_color, line_thickness)\n        elif plot_type == \"area\":\n            if line_thickness is not None:\n                warnings.warn(\n                    '\"line_thickness\" is only used if ' + '\"polygon_type\" is \"line\".'\n                )\n\n            output_slide = cv2.fillPoly(input_slide, [cnt], tissue_color)\n        else:\n            raise ValueError('Accepted \"polygon_type\" values are \"line\" or \"area\".')\n\n    return output_slide\n\n\ndef tissue_cutout(input_slide, tissue_contours):\n    \n    \"\"\"\n    Description\n    ----------\n    Set all parts of the in_slide to black except for those\n    within the provided tissue contours\n    Credit: https:\/\/stackoverflow.com\/a\/28759496\n    \n    Parameters\n    ----------\n    input_slide: numpy array\n            Slide to cut non-tissue backgound out\n    tissue_contours: numpy array \n            These are the identified tissue regions as cv2 contours\n            \n    Returns (1)\n    -------\n    - Numpy array of slide with non-tissue set to black\n    \"\"\"\n    \n    # Get intermediate slide\n    base_slide_mask = np.zeros(input_slide.shape[:2])\n    \n    # Create mask where white is what we want, black otherwise\n    crop_mask = np.zeros_like(base_slide_mask) \n    \n    # Draw filled contour in mask\n    cv2.drawContours(crop_mask, tissue_contours, -1, 255, -1) \n    \n    # Extract out the object and place into output image\n    tissue_only_slide = np.zeros_like(input_slide)  \n    tissue_only_slide[crop_mask == 255] = input_slide[crop_mask == 255]\n    \n    return tissue_only_slide\n\n\ndef getSubImage(input_slide, rect):\n    \n    \"\"\"\n    Description\n    ----------\n    Take a cv2 rectagle object and remove its contents from\n    a source image.\n    Credit: https:\/\/stackoverflow.com\/a\/48553593\n    \n    Parameters\n    ----------\n    input_slide: numpy array \n            Slide to pull subimage off \n    rect: cv2 rect\n        cv2 rectagle object with a shape of-\n            ((center_x,center_y), (hight,width), angle)\n    \n    Returns (1)\n    -------\n    - Numpy array of rectalge data cut from input slide\n    \"\"\"\n    \n    width = int(rect[1][0])\n    height = int(rect[1][1])\n    box = cv2.boxPoints(rect)\n\n    src_pts = box.astype(\"float32\")\n    dst_pts = np.array(\n        [[0, height - 1], [0, 0], [width - 1, 0], [width - 1, height - 1]],\n        dtype=\"float32\",\n    )\n    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n    output_slide = cv2.warpPerspective(input_slide, M, (width, height))\n    return output_slide\n\n\ndef color_cut(in_slide, color = [255,255,255]):\n    \n    \"\"\"\n    Description\n    ----------\n    Take a input image and remove all rows or columns that\n    are only made of the input color [R,G,B]. The default color\n    to cut from image is white.\n    \n    Parameters\n    ----------\n    input_slide: numpy array \n        Slide to cut white cols\/rows \n    color: list\n        List of [R,G,B] pixels to cut from the input slide\n    \n    Returns (1)\n    -------\n    - Numpy array of input_slide with white removed\n    \"\"\"\n    #Remove by row\n    row_not_blank = [row.all() for row in ~np.all(in_slide == color, axis=1)]\n    output_slide = in_slide[row_not_blank, :]\n    \n    #Remove by col\n    col_not_blank = [col.all() for col in ~np.all(output_slide == color, axis=0)]\n    output_slide = output_slide[:, col_not_blank]\n    return output_slide\n\ndef detect_and_crop(image_location,sensitivity= 3000,downsample_lvl= -1,show_plots= \"simple\"):\n    \n    \"\"\"\n    Description\n    ----------\n    This method performs the pipeline as described in the notebook:\n    https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70\n    \n    Parameters\n    ----------\n    image_location:str\n        Location of the slide image to process\n    sensitivity:int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 3000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n    downsample_lvl: int\n        The level at which to downsample the slide. This can be referenced in\n        reverse order to access the lowest resoltuion items first.\n        [-1] = lowest resolution\n        [0] = highest resolution\n    show_plots: str (verbose|simple|none)\n        The types of plots to display:\n            - verbose - show all steps of process\n            - simple - show only last step\n            - none - show none of the plots\n        \n    Returns (4)\n    -------\n    - Numpy array of final produciton(prod) slide\n    - Percent memory reduciton from original slide\n    - Time stamps from stages of the pipeline\n    - Time stamps from the Tissue Detect pipeline\n    \"\"\"\n\n    # For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n\n    # Open Slide\n    wsi = skimage.io.MultiImage(image_location)[downsample_lvl]\n    time_stamps[\"open\"] = time.time()\n\n    # Get returns from detect_tissue()\n    (\n        tissue_contours,\n        tier,\n        time_stamps_detect,\n    ) = detect_tissue(wsi, sensitivity)\n    time_stamps[\"tissue_detect\"] = time.time()\n    # Get Tissue Only Slide\n    base_slide_mask = np.zeros(wsi.shape[:2])\n    tissue_slide = draw_tissue_polygons(base_slide_mask, tissue_contours, \"line\", 5)\n    tissue_only_slide = tissue_cutout(wsi, tissue_contours)\n    time_stamps[\"tissue_trim\"] = time.time()\n    # Get minimal bounding rectangle for all tissue contours\n    if len(tissue_contours) == 0:\n        img_id = image_location.split(\"\/\")[-1]\n        print(f\"No Tissue Contours - ID: {img_id}\")\n        return None, 1.0\n\n    all_bounding_rect = cv2.minAreaRect(np.concatenate(tissue_contours))\n    # Crop with getSubImage()\n    smart_bounding_crop = getSubImage(tissue_only_slide, all_bounding_rect)\n    time_stamps[\"crop\"] = time.time()\n    \n    #cut empty space\n    prod_slide = color_cut(smart_bounding_crop, [0,0,0])\n    time_stamps[\"trim_white\"] = time.time()\n\n    # Get size change\n    base_size = get_disk_size(wsi)\n    final_size = get_disk_size(prod_slide)\n    pct_change = final_size \/ base_size\n    if show_plots == \"simple\":\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt.imshow(smart_bounding_crop)\n        plt.show()\n    elif show_plots == \"verbose\":\n        # Set-up dictionary for plotting\n        verbose_plots = {}\n        # Add Base Slide to verbose print\n        verbose_plots[f\"Base Slide\\n{get_disk_size(wsi):.2f}MB\"] = wsi\n        # Add Tissue Only to verbose print\n        verbose_plots[f\"Tissue Detect\\nNo Change\"] = tissue_slide\n        # Add Bounding Boxes to verbose print\n        verbose_plots[\n            f\"Bounding Boxes\\n{get_disk_size(smart_bounding_crop):.2f}MB\"\n        ] = smart_bounding_crop\n        # Add Cut Slide to verbose print\n        verbose_plots[\n            f\"Cut Slide\\n{get_disk_size(prod_slide):.2f}MB\"\n        ] = prod_slide\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plot_figures(verbose_plots, 2, 2)\n    elif show_plots == \"none\":\n        pass\n    else:\n        pass\n    time_stamps = {\n        key: (value - time_stamps[\"start\"]) * 1000 for key, value in time_stamps.items()\n    }\n    return prod_slide, (1 - pct_change), time_stamps, time_stamps_detect\n\n\ndef get_disk_size(numpy_image):\n    \"\"\"Return disk size of a numpy array\"\"\"\n    return (numpy_image.size * numpy_image.itemsize) \/ 1000000\n\n\ndef plot_figures(figures, nrows=1, ncols=1):\n    \n    \"\"\"\n    Description\n    ----------\n    Plot a dictionary of figures.\n    Credit: https:\/\/stackoverflow.com\/a\/11172032\n\n    Parameters\n    ----------\n    figures: dict \n        <title, figure> for those to plot\n    ncols: int \n        number of columns of subplots wanted in the display\n    nrows: int \n        number of rows of subplots wanted in the figure\n    \n    Returns(0)\n    ----------\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n    for ind, title in enumerate(figures):\n        axeslist.ravel()[ind].imshow(figures[title], aspect=\"auto\")\n        axeslist.ravel()[ind].set_title(title)\n    plt.tight_layout()\n    plt.show()\n    return \n\n\ndef get_timings(time_stamp_dict, verbose = False):\n    \"\"\"\n    Description\n    ----------\n    Get timing defferentials and percentages from a dictionary of timestamps \n    \n    Parameters\n    ----------\n    time_stamp_dict: dict\n        <description:time> for each of the measured time points\n        \n    Returns (2)\n    -------\n    - Dictionary with the time differentials at each time point\n    - Dictionary with the time percentages for each point compared to total\n    \"\"\"\n    \n    time_diffs = {}\n    dict_list = list(time_stamp_dict.items())\n    for i in range(len(dict_list) - 1):\n        time_diffs[dict_list[i + 1][0]] = dict_list[i + 1][1] - dict_list[i][1]\n    total_time = list(time_stamp_dict.values())[-1]\n    time_pcts = {k: v \/ (total_time + 0.0001) for k, v in time_diffs.items()}\n    if verbose:\n        print(f\"Total Between Funcitons:\")\n        print(json.dumps(time_diffs, indent=4))\n        print(f\"Pct Between Funcitons:\")\n        print(json.dumps(time_pcts, indent=4))\n        print(f\"Timing Totals:\")\n        print(json.dumps(time_stamp_dict, indent=4))\n    return time_diffs, time_pcts\n\ndef comp_timings(time_stamp_low, time_stamp_high, verbose=False):\n    \"\"\"\n    Description\n    ----------\n    Take two timestamp dictionaries and compare them.\n    \n    Parameters\n    ----------\n    time_stamp_low: dict\n        <description:time> for each of the measured time points (shorter process)\n    time_stamp_high: dict\n        <description:time> for each of the measured time points (longer process)\n    \n    Returns (1)\n    -------\n    - Dictionary with the raw differences between the two dicts\n    - Dictionary with the percentage differences between the two dicts\n    \"\"\"\n    \n    raw_comp = {k:time_stamp_high[k]-time_stamp_low[k] for k,v in time_stamp_low.items()}\n    pct_comp = {k:time_stamp_low[k]\/time_stamp_high[k] for k,v in time_stamp_low.items()}\n    if verbose:\n        print(f\"Timing Diffs Raw (High - Low):\")\n        print(json.dumps(raw_comp, indent=4))\n        print(f\"Timing: Diffs Pct (Low \/ High):\")\n        print(json.dumps(pct_comp, indent=4))\n    return raw_comp, pct_comp\n    ","88a05012":"# Set up example slide and run pipeline on low resolution\nslide_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_images\/\"\nannotation_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_label_masks\/\"\nexample_id = \"0032bfa835ce0f43a92ae0bbab6871cb\"\nexample_slide = f\"{slide_dir}{example_id}.tiff\"\n(processed_slide_low, pct_change_low, \ntime_stamps_pipeline_low, detect_time_low) = detect_and_crop(\n                                image_location=example_slide, \n                                downsample_lvl=-1, \n                                show_plots=\"verbose\"\n                                )\nprint(\"Pipeline Timings: Low\")\npipe_times_low, pipe_time_pct_low = get_timings(time_stamps_pipeline_low, verbose=True)\nprint(\"-\"*30)\nprint(\"Tissue Detect Timings: Low\")\ntiss_times_low, tiss_time_pct_low = get_timings(detect_time_low, verbose=True)","b32a3b91":"(processed_slide_med, pct_change_med, \n time_stamps_pipeline_med, detect_time_med) = detect_and_crop(\n                                        image_location=example_slide, \n                                        downsample_lvl=-2, show_plots=\"verbose\"\n                                        )\n\nprint(\"Pipeline Timings: Med\")\npipe_times_med, pipe_time_pct_med = get_timings(time_stamps_pipeline_med, verbose=True)\nprint(\"-\"*30)\nprint(\"Tissue Detect Timings: Med\")\ntiss_times_med, tiss_time_pct_med = get_timings(detect_time_med, verbose=True)","50a2e7e4":"print(\"Pipeline Timings: Med\")\npipe_times_comp, pipe_time_pct_comp = comp_timings(pipe_times_low, pipe_times_med,verbose=True)\nprint(\"-\"*30)\nprint(\"Tissue Detect Timings: Med\")\ntiss_times_comp, tiss_time_pct_comp = comp_timings(tiss_times_low,tiss_times_med,verbose=True)\nprint(\"-\"*30)\nprint(f\"Processed Size Ratio: {processed_slide_low.size \/ processed_slide_med.size}\")\ntime_full_low, time_full_med = time_stamps_pipeline_low[\"crop\"], time_stamps_pipeline_med[\"crop\"]\nprint(f\"Processed Time Ratio: {time_full_low \/ time_full_med}\")\n","fcebeacb":"# Open slide on lowest resolution\nlow_res_lvl = -1\nimg_low = skimage.io.MultiImage(example_slide)[low_res_lvl]\nplt.imshow(img_low)\nplt.show()\n","79dc19a2":"# Detect Tissue and \n(\n    tissue_contours,\n    tier,\n    time_stamps_detect,\n) = detect_tissue(img_low)\n\n# Copy for compare\nsmart_bounding_boxes = img_low.copy()\n\n# Get small level bounding boxes\nall_bounding_rect = cv2.minAreaRect(np.concatenate(tissue_contours))\nall_bounding_box = cv2.boxPoints(all_bounding_rect)\nall_bounding_box = np.int0(all_bounding_box)\ncv2.drawContours(smart_bounding_boxes, [all_bounding_box], 0, (255, 0, 0), 4)\nplt.imshow(smart_bounding_boxes)\nplt.show()","96beb2b3":"# Open slide on lowest resolution\nhigh_res_lvl = -2\nimg_high = skimage.io.MultiImage(example_slide)[high_res_lvl]\nplt.imshow(img_high)\nplt.show()","3918d993":"smart_scale = img_high.copy()\nscale = 4\n# Get center, size, and angle from rect\nscaled_rect = (\n    (all_bounding_rect[0][0] * scale, all_bounding_rect[0][1] * scale),\n    (all_bounding_rect[1][0] * scale, all_bounding_rect[1][1] * scale),\n    all_bounding_rect[2],\n)\nscaled_bounding_box = cv2.boxPoints(scaled_rect)\nscaled_bounding_box = np.int0(scaled_bounding_box)\ncv2.drawContours(smart_scale, [scaled_bounding_box], 0, (255, 0, 0), 10)\nplt.imshow(smart_scale)\nplt.show()","6f9d0307":"scaled_smart_crop = getSubImage(smart_scale, scaled_rect)\nplt.imshow(scaled_smart_crop)\nplt.show()","8765fa81":"big_slide_cut = color_cut(scaled_smart_crop, color=[255,255,255])\nplt.imshow(big_slide_cut)\nplt.show()","6b7faa51":"resized_prod_slide = cv2.resize(big_slide_cut, (512, 512))\nplt.imshow(resized_prod_slide)\nplt.show()","984a2c04":"(tissue_contours_high,\n    tier_high,\n    time_stamps_detect_high,\n) = detect_tissue(resized_prod_slide, sensitivity=3000)\n\n# Get Tissue Only Slide\nbase_slide_mask = np.zeros(resized_prod_slide.shape[:2])\ntissue_slide_high = draw_tissue_polygons(base_slide_mask, tissue_contours_high, \"line\", 5)\ntissue_only_slide = tissue_cutout(resized_prod_slide, tissue_contours_high)\nplt.imshow(tissue_only_slide)\nplt.show()","b027462f":"def detect_tissue_external(input_slide, sensitivity=3000):\n    \n    \"\"\"\n    Description\n    ----------\n    Find RoIs containing tissue in WSI and only return the external most.\n    Generate mask locating tissue in an WSI. Inspired by method used by\n    Wang et al. [1]_.\n    .. [1] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew\n    H. Beck, \"Deep Learning for Identifying Metastatic Breast Cancer\",\n    arXiv:1606.05718\n    Credit: Github-wsipre\n    \n    Parameters\n    ----------\n    input_slide: numpy array\n        Slide to detect tissue on.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 3000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n        \n    Returns (3)\n    -------\n    -Tissue binary mask as numpy 2D array, \n    -Tiers investigated,\n    -Time Stamps from running tissue detection pipeline\n    \"\"\"\n    \n    # For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n\n    # Convert from RGB to HSV color space\n    slide_hsv = cv2.cvtColor(input_slide, cv2.COLOR_BGR2HSV)\n    time_stamps[\"re-color\"] = time.time()\n    # Compute optimal threshold values in each channel using Otsu algorithm\n    _, saturation, _ = np.split(slide_hsv, 3, axis=2)\n\n    mask = otsu_filter(saturation, gaussian_blur=True)\n    time_stamps[\"filter\"] = time.time()\n    # Make mask boolean\n    mask = mask != 0\n\n    mask = morphology.remove_small_holes(mask, area_threshold=sensitivity)\n    mask = morphology.remove_small_objects(mask, min_size=sensitivity)\n    time_stamps[\"morph\"] = time.time()\n    mask = mask.astype(np.uint8)\n    mask_contours, tier = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    time_stamps[\"contour\"] = time.time()\n    time_stamps = {\n        key: (value - time_stamps[\"start\"]) * 1000 for key, value in time_stamps.items()\n    }\n    return mask_contours, tier, time_stamps\n\n\n(tissue_contours_high,\n    tier_high,\n    time_stamps_detect_high,\n) = detect_tissue_external(resized_prod_slide)\n\n# Get Tissue Only Slide\nbase_slide_mask = np.zeros(resized_prod_slide.shape[:2])\ntissue_slide_high = draw_tissue_polygons(base_slide_mask, tissue_contours_high, \"line\", 5)\ntissue_only_slide = tissue_cutout(resized_prod_slide, tissue_contours_high)\nplt.imshow(tissue_only_slide)\nplt.show()","818e97a7":"def new_detect_and_crop(image_location=\"\",sensitivity: int = 3000, downsample_lvl = -1,\n                        show_plots= \"simple\", out_lvl=-2, shape=(512,512)):\n    \"\"\"\n    Description\n    ----------\n    This method performs the pipeline as described in the notebook:\n    https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detect-scaling-bounding-boxes-fast\n    \n    Parameters\n    ----------\n    image_location:str\n        Location of the slide image to process\n    sensitivity:int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 3000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n    downsample_lvl: int\n        The level at which to downsample the slide. This can be referenced in\n        reverse order to access the lowest resoltuion items first.\n        [-1] = lowest resolution\n        [0] = highest resolution\n    show_plots: str (verbose|simple|none)\n        The types of plots to display:\n            - verbose - show all steps of process\n            - simple - show only last step\n            - none - show none of the plots\n    out_lvl: int\n        The level at which the final slide should sample at. This can be referenced in\n        reverse order to access the lowest resoltuion items first.\n        [-1] = lowest resolution\n        [0] = highest resolution\n    shape: touple\n        (height, width) of the desired produciton(prod) image\n        \n    Returns (4)\n    -------\n    - Numpy array of final produciton(prod) slide\n    - Percent memory reduciton from original slide\n    - Time stamps from stages of the pipeline\n    - Time stamps from the Tissue Detect pipeline\n    \"\"\"\n    # For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n\n    # Open Small Slide\n    wsi_small = skimage.io.MultiImage(image_location)[downsample_lvl]\n    time_stamps[\"open_small\"] = time.time()\n\n    # Get returns from detect_tissue() ons mall image\n    (   tissue_contours,\n        tier,\n        time_stamps_detect,\n    ) = detect_tissue_external(wsi_small, sensitivity)\n    \n    base_slide_mask = np.zeros(wsi_small.shape[:2])\n\n    # Get minimal bounding rectangle for all tissue contours\n    if len(tissue_contours) == 0:\n        img_id = image_location.split(\"\/\")[-1]\n        print(f\"No Tissue Contours - ID: {img_id}\")\n        return None, 0, None, None\n    \n    # Open Big Slide\n    wsi_big = skimage.io.MultiImage(image_location)[out_lvl]\n    time_stamps[\"open_big\"] = time.time()\n    \n    #Get small boudning rect and scale\n    bounding_rect_small = cv2.minAreaRect(np.concatenate(tissue_contours))\n\n    # Scale Rectagle to larger image\n    scale = int(wsi_big.shape[0] \/ wsi_small.shape[0])\n    scaled_rect = (\n        (bounding_rect_small[0][0] * scale, bounding_rect_small[0][1] * scale),\n        (bounding_rect_small[1][0] * scale, bounding_rect_small[1][1] * scale),\n        bounding_rect_small[2],\n    )\n    # Crop bigger image with getSubImage()\n    scaled_crop = getSubImage(wsi_big, scaled_rect)\n    time_stamps[\"scale_bounding\"] = time.time()\n    \n    #Cut out white\n    white_cut = color_cut(scaled_crop)\n    time_stamps[\"white_cut_big\"] = time.time()\n    \n    #Scale\n    scaled_slide = cv2.resize(white_cut, shape)\n    time_stamps[\"resize_big\"] = time.time()\n    \n    # Get returns from detect_tissue() on small image\n    (   tissue_contours_big,\n        tier_big,\n        time_stamps_detect,\n    ) = detect_tissue_external(scaled_slide, sensitivity)\n    prod_slide = tissue_cutout(scaled_slide, tissue_contours_big)\n    time_stamps[\"remove_tissue\"] = time.time()\n\n    # Get size change\n    base_size_high = get_disk_size(wsi_big)\n    final_size = get_disk_size(prod_slide)\n    pct_change = final_size \/ base_size_high\n    \n    if show_plots == \"simple\":\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt.imshow(smart_bounding_crop)\n        plt.show()\n    elif show_plots == \"verbose\":\n        # Set-up dictionary for plotting\n        verbose_plots = {}\n        # Add Base Slide to verbose print\n        verbose_plots[f\"Smaller Slide\\n{get_disk_size(wsi_small):.2f}MB\"] = wsi_small\n        # Add Tissue Only to verbose print\n        verbose_plots[f\"Tissue Detect Low\\nNo Change\"] = wsi_big\n        # Add Larger Plot cut with bounding boxes\n        verbose_plots[f\"Larger scaled\\n{get_disk_size(scaled_crop):.2f}MB\"] = scaled_crop\n        # Add Bounding Boxes to verbose print\n        verbose_plots[\n            f\"Final Produciton\\n{get_disk_size(prod_slide):.2f}MB\"\n        ] = prod_slide\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt = plot_figures(verbose_plots, 2, 2)\n    elif show_plots == \"none\":\n        pass\n    else:\n        pass\n    time_stamps = {\n        key: (value - time_stamps[\"start\"]) * 1000 for key, value in time_stamps.items()\n    }\n    return prod_slide, (1 - pct_change), time_stamps, time_stamps_detect\n\n(processed_slide_med, pct_change_med, \n time_stamps_pipeline_med, detect_time_med) = new_detect_and_crop(\n                                        image_location=example_slide, \n                                        show_plots=\"verbose\"\n                                        )","9db3bb35":"tiss_times_comp, tiss_time_pct_comp = get_timings(time_stamps_pipeline_med, verbose=True)","8a73aca7":"base_pipe_low = %timeit -o detect_and_crop(image_location=example_slide, downsample_lvl=-1, show_plots=\"none\")","ca6c6acc":"base_pipe_high = %timeit -o detect_and_crop(image_location=example_slide, downsample_lvl=-2, show_plots=\"none\")","1c423a05":"new_pipe_high = %timeit -o new_detect_and_crop(image_location=example_slide,show_plots=\"none\")","0069ad56":"base = base_pipe_low.best\nbase_high = base_pipe_high.best\nnew = new_pipe_high.best\nbase_new = new \/  base\nbase_old = base_high \/  base\nnew_old = base_old \/ base_new\nprint(\"Ratio Time Increase from low res image to high (best time):\")\nprint(f\"Base->Old: {base_old:.2f}\")\nprint(f\"Base->New: {base_new:.2f}\")\nprint(f\"Old->New: {new_old:.2f}\")","bdba3f1e":"from tqdm.notebook import tqdm\nimport os\nfrom multiprocessing import Pool\nsave_dir = \"\/kaggle\/train_images\/\"\nos.makedirs(save_dir, exist_ok=True)\ntrain_data_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/train.csv\")\n\n## 5 Image sample for example *******************\nshow_image_ids = list(train_data_df.image_id.sample(5))\n## All Images for full produciton\nall_image_ids = list(train_data_df.image_id)\n\ndef make_images(image_id):\n    load_path = slide_dir + image_id + '.tiff'\n    save_path = save_dir + image_id + '.png'\n    \n    (prod_slide, pct_change, \n     timestamps_pipeline, timestamps_detect) = new_detect_and_crop(image_location=load_path,\n                                                                      show_plots=\"none\")\n    if prod_slide is None: return 0\n    cv2.imwrite(save_path, prod_slide)\n    return [image_id, pct_change, timestamps_pipeline]\n        \n\nwith Pool(processes=4) as pool:\n    show_run_results = list(\n        tqdm(pool.imap(make_images, show_image_ids), total = len(show_image_ids))\n    )\n","c172b11d":"!tar -czf train_images.tar.gz ..\/train_images\/*.png","223f9e3a":"## Findings:\n* Tissue Detect timings increase proportionally linear with the size of the image.\n* Opening the image takes a slightly higher proportionate time.\n* Since Tissue Detect is the majority, the overall time increases proportionally","0d1c6cb1":"### \u2191 Pipeline Code Hidden Above \u2191\n\nA full explaination of the pipeline is documented in the above mentioned [notebook](https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70\/). ","036c352e":"## Crop High Res Image with Bounding Box","4e9dc626":"![tissue_logo.001.jpeg](attachment:tissue_logo.001.jpeg)\n\nTissue Detection is a key aspect to research in the domain of computer vision applied to cancer classification. My main focus in this competition so far has been exploring previous work done in this domain and furthering its application towards this dataset. Notebooks in this collection include the following.\n* [Base Notebook](https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70) : Tissue Detection Intro and First Application\n* [Base Dataset Generation](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-td-conv-png-512x512): Notebook to export images to zip file\n* [Scaling Bounding Boxes **(Currently Here)**](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-scaling-bounding-boxes-4xfaster): 4x speed increase to base notebook\n* [Tissue Dection Metadata Analysis](https:\/\/www.kaggle.com\/dannellyz\/tissue-detection-bounding-box-metadata-eda-viz\/): Exploring features from bounding boxes discovery on the slides","e5039786":"# Appling New Pipeline to Slide Corpus\nI went ahead and added a section that can be used to run the new pipeline on the slides. Following my other notebook this also pulls from the lead of [@xhlulu](https:\/\/www.kaggle.com\/xhlulu) and thier notebook: [PANDA: Resize and Save Train Data](https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data). For now I went ahead and put the show on `show_plots=\"verbose\"` and onel set the images to 5, but can be uncommented to run all images.\n\nI also have run this script and made a new dataset with the images.","d14c35b1":"# Next Steps\n* It is clear that a square final size causes serious distortions as well as adds a bit of unnecessary wasted space. I would imagine this is due to the fact that most of the contours are more rectangular than square. Measuring these ratios, rectangularity, could lead to a more optimal base shape to cast all slides. \n* Since opening and cutting the slide take the most time as of now I am also going to look into those functions for optimization.\n* As the code is getting quite bulky I am going to make a Kaggle Utility so that this code can simply be imported. \n\n### If anyone has any leads on the above or other feedback\/recommendations please let me know!","a06180c5":"## External Contour Fix\n\n\n```\ndef detect_tissue(...):\n...\nmask_contours, tier = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n...\n```\n#### \u2191 Changed to \u2193\n```\ndef detect_tissue_external(...):\n...\nmask_contours, tier = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n...\n```","f0159c60":"# New Pipeline Analysis\n\nGiven that so much of the time it takes to process the images comes from the Tissue Detection the new pipeline seeks to handle that portion on the lowest resolution then scale up the resulting bounding boxes to the high resolution images. Then to cut the large image down as much as possible before re-runing the tissue detect on the higher resolution. Below is a step through of this process.","6a5a2d5c":"## Open High Resolution Image","ad5952c3":"## Resize to desired injest\nUnlike the other pipeline I included resizes as a key feature.","833ac208":"# PANDA Tissue Detect: Scaling Bounding Boxes\n\nI started my research in this competition by taking academic papers and applying their technique to discover tissue within the provided Whole Slide Images. The notebook I created, [Tissue Detection and Size Optimization ~70% Shrink](https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70\/), walked through the steps of my process, but focused only on the highest level downsampled images (least resolution). I made another [notebook that exported these images](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-png-512x512-pre-process) as (512x512), however, after completing this I realized that the resolutions on the images were not quite up to what I wanted.\n\nThis notebook started with running my pipeline on larger images and realizing that the time it took was unmanageable. Therefore, I edited the pipeline to speed up processing. I did this by detecting min-area-rect bounding boxes around tissue samples on the low resolution images and scaling that box up the larger images. After this croping the image by the bounding box and then cutting out the empty space. This is all then followed by re-sizing the image to the export shape and finally re-detecting the tissue on the larger slide to remove the background.\n\n## This method increased my image pre-processing speed by 4 times\n\n![pipeline.001.jpeg](attachment:pipeline.001.jpeg)","7f0c3dc2":"## Detect Tissue and Find Bounding Boxes","3252a74e":"# Timings: Running Pipeline on Medium Resolution \nIndex of [-2] in the level array. We also know that the provided images only have three layers of downsampling (1.0,4.0,16.0). This is to say that the image processed here is 4x bigger in both dimensions than the previous image. Therefore making the ratio 1\/16.","b62a1835":"## Scale Smaller Bounding Boxes to High Res Image","c9ef59d0":"## Detect Tissue and Remove Remaning White Space\nOn these high resolution images some of the white even from within the images is cut as it is seen as part of the background. This offers a good time to show the effect of [contour hierarchies](https:\/\/docs.opencv.org\/3.1.0\/d9\/d8b\/tutorial_py_contours_hierarchy.html). Knowing that these white spaces are very valuable to the analysis we can switch the flag on the contour processing to only include the external contours and drop those encomapssed by others. This flag is: `RETR_EXTERNAL`.","e7a4946e":"## Open low resolution image","b9af3031":"# Analysing New Pipeline Speed\nAs the timings below show the pipeline has been greatly speed up to only 633ms or roughly a 4x speed increase. This is compared to the old pipeline, 2.730 seconds, which is about 16x longer than the base processing time which makes since as we observed the time scaling linearlly with the size of the array.\n\n### More than a 4x time decrease with the new pipeline","c6691d29":"## Crop White Space From Image","12820c0f":"# Revised\/Faster Pipeline\n\n### \u2193 Code Hidden Below \u2193","814e2226":"## Findings:\n* Tissue Detection encompasses almost 60% of the pre-processing time\n* Opening and Cutting the files are next most costly at about 12% each","f565039a":"# Timings: Running Pipeline on Lowest Resolution \nThis is the highest indexed downsample level therefore referenced with [-1]. All times are in milliseconds."}}