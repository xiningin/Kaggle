{"cell_type":{"e9c6fbd8":"code","c9e0d0d3":"code","1eaa4000":"code","1310ab62":"code","9f9e5d4f":"code","944abca7":"code","991395aa":"code","34de5390":"code","7567e199":"code","06508837":"code","ccf77d61":"code","0f73bdba":"code","ce6be530":"code","fbd03255":"code","bd4b2e63":"code","9a628bca":"code","30d975ef":"code","7988a438":"markdown","c1dbe9cc":"markdown","97a589b1":"markdown","4cb60faf":"markdown","e5038462":"markdown","3435bd08":"markdown","9e5abb62":"markdown","1b19092a":"markdown","dec578eb":"markdown","a99b5271":"markdown","0af87993":"markdown","4285bebb":"markdown","a0d050bd":"markdown","046b0857":"markdown","537d460a":"markdown","867c7c91":"markdown","91b3fe5c":"markdown","637b3aa8":"markdown","f14e0028":"markdown","f6385d83":"markdown"},"source":{"e9c6fbd8":"# example of preparing the apple and orange dataset\nfrom os import listdir\nfrom numpy import asarray\nfrom numpy import vstack\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nfrom numpy import savez_compressed\n \n# load all images in a directory into memory\ndef load_images(path, size=(256,256)):\n    data_list = list()\n    # enumerate filenames in directory, assume all are images\n    for filename in listdir(path):\n        # load and resize the image\n        pixels = load_img(path + filename, target_size=size)\n    # convert to numpy array\n        pixels = img_to_array(pixels)\n        # store\n        data_list.append(pixels)\n    return asarray(data_list)\n \n# dataset path\npath = '\/kaggle\/input\/cyclegan\/apple2orange\/apple2orange\/'\n# load dataset A\ndataA1 = load_images(path + 'trainA\/')\ndataAB = load_images(path + 'testA\/')\ndataA = vstack((dataA1, dataAB))\nprint('Loaded dataA: ', dataA.shape)\n# load dataset B\ndataB1 = load_images(path + 'trainB\/')\ndataB2 = load_images(path + 'testB\/')\ndataB = vstack((dataB1, dataB2))\nprint('Loaded dataB: ', dataB.shape)\n# save as compressed numpy array\nfilename = 'apple2orange_256.npz'\nsavez_compressed(filename, dataA, dataB)\nprint('Saved dataset: ', filename)","c9e0d0d3":"# load and plot the prepared dataset\nfrom numpy import load\nfrom matplotlib import pyplot\n# load the dataset\ndata = load('apple2orange_256.npz')\ndataA, dataB = data['arr_0'], data['arr_1']\nprint('Loaded: ', dataA.shape, dataB.shape)\n# plot source images\nn_samples = 3\nfor i in range(n_samples):\n    pyplot.subplot(2, n_samples, 1 + i)\n    pyplot.axis('off')\n    pyplot.imshow(dataA[i].astype('uint8'))\n# plot target image\nfor i in range(n_samples):\n    pyplot.subplot(2, n_samples, 1 + n_samples + i)\n    pyplot.axis('off')\n    pyplot.imshow(dataB[i].astype('uint8'))\npyplot.show()","1eaa4000":"! pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","1310ab62":"from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n# define layer\nlayer = InstanceNormalization(axis=-1)","9f9e5d4f":"from random import random\nfrom numpy import load\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom keras.models import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom matplotlib import pyplot","944abca7":"# define the discriminator model\ndef define_discriminator(image_shape):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # source image input\n    in_image = Input(shape=image_shape)\n    # C64\n    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C128\n    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C256\n    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C512\n    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # second last output layer\n    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n    d = InstanceNormalization(axis=-1)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    # patch output\n    patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n    # define model\n    model = Model(in_image, patch_out)\n    # compile model\n    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n    return model","991395aa":"# generator a resnet block\ndef resnet_block(n_filters, input_layer):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # first layer convolutional layer\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # second convolutional layer\n    g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    # concatenate merge channel-wise with input layer\n    g = Concatenate()([g, input_layer])\n    return g\n","34de5390":"def define_generator(image_shape, n_resnet=9):\n    # weight initialization\n    init = RandomNormal(stddev=0.02)\n    # image input\n    in_image = Input(shape=image_shape)\n    # c7s1-64\n    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # d128\n    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # d256\n    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n     # R256\n    for _ in range(n_resnet):\n         g = resnet_block(256, g)\n     # u128\n    g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # u64\n    g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    g = Activation('relu')(g)\n    # c7s1-3\n    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n    g = InstanceNormalization(axis=-1)(g)\n    out_image = Activation('tanh')(g)\n    # define model\n    model = Model(in_image, out_image)\n    return model","7567e199":"# define a composite model for updating generators by adversarial and cycle loss\ndef define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n    # ensure the model we're updating is trainable\n    g_model_1.trainable = True\n    # mark discriminator as not trainable\n    d_model.trainable = False\n    # mark other generator model as not trainable\n    g_model_2.trainable = False\n    # discriminator element\n    input_gen = Input(shape=image_shape)\n    gen1_out = g_model_1(input_gen)\n    output_d = d_model(gen1_out)\n    # identity element\n    input_id = Input(shape=image_shape)\n    output_id = g_model_1(input_id)\n    # forward cycle\n    output_f = g_model_2(gen1_out)\n    # backward cycle\n    gen2_out = g_model_2(input_id)\n    output_b = g_model_1(gen2_out)\n    # define model graph\n    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n    # define optimization algorithm configuration\n    opt = Adam(lr=0.0002, beta_1=0.5)\n    # compile model with weighting of least squares loss and L1 loss\n    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n    return model","06508837":"# load and prepare training images\ndef load_real_samples(filename):\n    # load the dataset\n    data = load(filename)\n    # unpack arrays\n    X1, X2 = data['arr_0'], data['arr_1']\n    # scale from [0,255] to [-1,1]\n    X1 = (X1 - 127.5) \/ 127.5\n    X2 = (X2 - 127.5) \/ 127.5\n    return [X1, X2]\n","ccf77d61":"# select a batch of random samples, returns images and target\ndef generate_real_samples(dataset, n_samples, patch_shape):\n    # choose random instances\n    ix = randint(0, dataset.shape[0], n_samples)\n    # retrieve selected images\n    X = dataset[ix]\n    # generate 'real' class labels (1)\n    y = ones((n_samples, patch_shape, patch_shape, 1))\n    return X, y\n","0f73bdba":"# generate a batch of images, returns images and targets\ndef generate_fake_samples(g_model, dataset, patch_shape):\n    # generate fake instance\n    X = g_model.predict(dataset)\n    # create 'fake' class labels (0)\n    y = zeros((len(X), patch_shape, patch_shape, 1))\n    return X, y","ce6be530":"# save the generator models to file\ndef save_models(step, g_model_AtoB, g_model_BtoA):\n    # save the first generator model\n    filename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n    g_model_AtoB.save(filename1)\n    # save the second generator model\n    filename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n    g_model_BtoA.save(filename2)\n    print('>Saved: %s and %s' % (filename1, filename2))","fbd03255":"# generate samples and save as a plot and save the model\ndef summarize_performance(step, g_model, trainX, name, n_samples=5):\n    # select a sample of input images\n    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n    # generate translated images\n    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n    # scale all pixels from [-1,1] to [0,1]\n    X_in = (X_in + 1) \/ 2.0\n    X_out = (X_out + 1) \/ 2.0\n    # plot real images\n    for i in range(n_samples):\n        pyplot.subplot(2, n_samples, 1 + i)\n        pyplot.axis('off')\n        pyplot.imshow(X_in[i])\n     # plot translated image\n    for i in range(n_samples):\n        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n        pyplot.axis('off')\n        pyplot.imshow(X_out[i])\n    # save plot to file\n    filename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n    pyplot.savefig(filename1)\n    pyplot.close()","bd4b2e63":"# update image pool for fake images\ndef update_image_pool(pool, images, max_size=50):\n    selected = list()\n    for image in images:\n        if len(pool) < max_size:\n            # stock the pool\n            pool.append(image)\n            selected.append(image)\n        elif random() < 0.5:\n            # use image, but don't add it to the pool\n            selected.append(image)\n        else:\n            # replace an existing image and use replaced image\n            ix = randint(0, len(pool))\n            selected.append(pool[ix])\n            pool[ix] = image\n    return asarray(selected)","9a628bca":"# train cyclegan models\ndef train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n    # define properties of the training run\n    n_epochs, n_batch, = 10, 1\n    # determine the output square shape of the discriminator\n    n_patch = d_model_A.output_shape[1]\n    # unpack dataset\n    trainA, trainB = dataset\n    # prepare image pool for fakes\n    poolA, poolB = list(), list()\n    # calculate the number of batches per training epoch\n    bat_per_epo = int(len(trainA) \/ n_batch)\n    # calculate the number of training iterations\n    n_steps = bat_per_epo * n_epochs\n    # manually enumerate epochs\n    for i in range(n_steps):\n        # select a batch of real samples\n        X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n        X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n        # generate a batch of fake samples\n        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n        # update fakes from pool\n        X_fakeA = update_image_pool(poolA, X_fakeA)\n        X_fakeB = update_image_pool(poolB, X_fakeB)\n        # update generator B->A via adversarial and cycle loss\n        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n        # update discriminator for A -> [real\/fake]\n        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n        # update generator A->B via adversarial and cycle loss\n        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n        # update discriminator for B -> [real\/fake]\n        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n        # summarize performance\n        print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n        # evaluate the model performance every so often\n        if (i+1) % (bat_per_epo * 1) == 0:\n        # plot A->B translation\n            summarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n            # plot B->A translation\n            summarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n        if (i+1) % (bat_per_epo * 5) == 0:\n            # save the models\n            save_models(i, g_model_AtoB, g_model_BtoA)","30d975ef":"from keras.models import load_model\n# load image data\ndataset = load_real_samples('apple2orange_256.npz')\nprint('Loaded', dataset[0].shape, dataset[1].shape)\n# define input shape based on the loaded dataset\nimage_shape = dataset[0].shape[1:]\n# generator: A -> B\ng_model_AtoB = define_generator(image_shape)\n# generator: B -> A\ng_model_BtoA = define_generator(image_shape)\n# discriminator: A -> [real\/fake]\nd_model_A = define_discriminator(image_shape)\n# discriminator: B -> [real\/fake]\nd_model_B = define_discriminator(image_shape)\n# composite: A -> B -> [real\/fake, A]\nc_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n# composite: B -> A -> [real\/fake, B]\nc_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n# train models\ntrain(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)","7988a438":"Random batch of samples are selected as input to the discriminator and composite generator models.","c1dbe9cc":"A sample of generated images is required to update each discriminator model in each training iteration.","97a589b1":"## Composite Model","4cb60faf":"Save each generator model to the current directory in H5 format, including the training iteration number in the filename","e5038462":"## Preparing The Dataset","3435bd08":"Uses a given generator model to generate translated versions of a few randomly selected source photographs and saves the plot to file","9e5abb62":"A function creates the 9-resnet block version for 256\u00d7256 input images.","1b19092a":"## Discriminator Model","dec578eb":"Loading our paired images dataset in compressed NumPy array format","a99b5271":"## Training the Model","0af87993":"A resnet block creates two Convolution-InstanceNorm blocks with 3\u00d73 filters and 1\u00d71 stride and without a ReLU activation after the second block","4285bebb":"Implementation of a 70\u00d770 PatchGAN discriminator model as per the design of the model in the paper. The model takes a 256\u00d7256 sized image as input and outputs a patch of predictions","a0d050bd":"The composite model has two inputs for the real photos from Domain-A and Domain-B, and four outputs for the discriminator output, identity generated image, forward cycle generated image, and backward cycle generated image.\n\nOnly the weights of the first or main generator model are updated for the composite model and this is done via the weighted sum of all loss functions.","046b0857":"Loading all photographs from the train and test folders and creating**** an array of images for category A and another for category B.Both arrays are then saved to a new file in compressed NumPy array format.","537d460a":"Takes all six models (two discriminator, two generator, and two composite models) as arguments along with the dataset and trains the models.","867c7c91":"The generator is an encoder-decoder model architecture. The model takes a source image and generates a target image.It is done by first downsampling or encoding the input image down to a bottleneck layer, then interpreting the encoding with a number of ResNet layers that use skip connections, followed by a series of layers that upsample or decode the representation to the size of the output image","91b3fe5c":"Implementation of instance normalization ","637b3aa8":"An image pool of 50 generated images for each discriminator model that is first populated and probabilistically either adds new images to the pool by replacing an existing image or uses a generated image directly.","f14e0028":"loading the dataset and plotting some of the photos to confirm that the image data is handled correctly.","f6385d83":"## Generator Model"}}