{"cell_type":{"9d7676c6":"code","3ba816a4":"code","6aa04f30":"code","fc805b92":"code","faac0c88":"code","bb379eda":"code","cfe399a4":"code","a42b96bf":"code","371d30e5":"code","96ec74df":"code","4c691257":"code","180c3487":"code","55be534d":"code","30bdd994":"code","a3398e81":"code","133af6f4":"code","e9625520":"code","691aa538":"code","6316d92e":"code","a5c13105":"markdown","8cbd0351":"markdown","59437dd8":"markdown","9b597ea6":"markdown","a783e5ce":"markdown","ba6bbe08":"markdown","37813772":"markdown","6352d626":"markdown","48e3dcbf":"markdown","e6c5122d":"markdown","b5a20259":"markdown"},"source":{"9d7676c6":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nos.chdir(\"..\/input\/Data Science\/ML\")\n\n# let's read both the files train and test\namestraindf = pd.read_csv('train.csv', index_col=0)\namestestdf = pd.read_csv('test.csv', index_col=0)\n\n# check the rows and columns in each data frame\nprint(amestraindf.shape, amestestdf.shape) \n\n## Note: \n##(1460, 80) (1459, 79), please note ID column we converted as index while reading the dataframe from both the files, \n##so total of 80 fields now.\n\n# concat both the dataframes now\n# before concatnenating them, lets; create the SalePrice columns in the test dataframe with 0 values.\namestestdf['SalePrice'] = 0\namesdf = pd.concat([amestraindf, amestestdf], axis=0)\nprint(amesdf.shape)\n##Note:\n##(2919, 80)  (1460 rows from train and 1459 rows from test, so total of 2919 rows in the concatenated datafrmae )\n\n\n# now let's check th dtypes of the dataframe. It has object , int64 as well as float64 dtypes.\namesdf.dtypes.unique()  \n\n\n## let's check the qualitative and quantitaive fields and their respective count.\nqualitative = amesdf.select_dtypes(include='object').columns\n##len(qualitative)  # out of 80 columns in the dataframe, 43 are of Qualitative types(categorical)\nqualitative.sort_values()\nqualitative =[ 'Alley', 'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n               'BsmtFinType2', 'BsmtQual', 'CentralAir', 'Condition1', 'Condition2',\n               'Electrical', 'ExterCond', 'ExterQual', 'Exterior1st', 'Exterior2nd',\n               'Fence', 'FireplaceQu', 'Foundation', 'Functional', 'GarageCond',\n               'GarageFinish', 'GarageQual', 'GarageType', 'Heating', 'HeatingQC',\n               'HouseStyle', 'KitchenQual', 'LandContour', 'LandSlope', 'LotConfig',\n               'LotShape', 'MSZoning', 'MasVnrType', 'MiscFeature', 'Neighborhood',\n               'PavedDrive', 'PoolQC', 'RoofMatl', 'RoofStyle', 'SaleCondition',\n               'SaleType', 'Street', 'Utilities' ]\n## Note:\n## so 43 qualitative fields, total of 80 fields.\n## Now based on the study of the data thoroughly, futher out of 43 qualitative,28 categorical and 15 oridinal(sort of rank)\n## so finally, 43 quantitative, 19 categorical, 18 ordinal\n\n\nordinal =    ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n               'BsmtFinType2', 'HeatingQC',  'KitchenQual','FireplaceQu', \n               'GarageQual', 'GarageCond','GarageFinish', 'PoolQC', 'Fence' ]\n\ncategorical = [ 'Alley', 'BldgType', 'CentralAir', 'Condition1', 'Condition2',\n                'Electrical', 'Exterior1st', 'Exterior2nd',\n                'Foundation', 'Functional', 'GarageType', 'Heating', 'HouseStyle', \n                'LandContour', 'LandSlope', 'LotConfig', 'LotShape', 'MSZoning', 'MasVnrType', \n                'MiscFeature', 'Neighborhood', 'PavedDrive', 'RoofMatl', 'RoofStyle', 'SaleCondition',\n               'SaleType', 'Street', 'Utilities' ]\n\n\nquantitative = amesdf.select_dtypes(exclude='object').columns\n## len(quantitative)  # out of 80 columns in the dataframe, 37 are of Quanitative types(numeric)\nquantitative  = [ '1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n       'EnclosedPorch', 'Fireplaces', 'FullBath', 'GarageArea', 'GarageCars',\n       'GarageYrBlt', 'GrLivArea', 'HalfBath', 'KitchenAbvGr', 'LotArea',\n       'LotFrontage', 'LowQualFinSF', 'MSSubClass', 'MasVnrArea', 'MiscVal',\n       'MoSold', 'OpenPorchSF', 'OverallCond', 'OverallQual', 'PoolArea',\n       'SalePrice', 'ScreenPorch', 'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF',\n       'YearBuilt', 'YearRemodAdd', 'YrSold' ]\n\n","3ba816a4":"## Now let's check the missing and NA values\nimport matplotlib.pyplot\n\nmissing = amesdf.isna().sum().sort_values(ascending=False)\nmissing[missing>0].plot(kind=\"bar\", figsize=(10,6))\nlen(missing[missing>0]) # total 34 fields have NA or missing values in the dataframe.\n# missing[missing>0].index\nmissingcols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage',\n               'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt', 'GarageType',\n               'BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1',\n               'MasVnrType', 'MasVnrArea', 'MSZoning', 'Functional', 'Utilities',\n               'BsmtHalfBath', 'BsmtFullBath', 'Exterior2nd', 'Exterior1st',\n               'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea', 'KitchenQual', 'BsmtFinSF2',\n               'GarageCars', 'BsmtFinSF1', 'SaleType', 'Electrical']\n\n##Note:\n## so out of 34 fields, few are quantitaive( numerical) and few are qualitative(catergorical or ordinal)\n## so we will handle the missing values for each case differently.\n# For most columns in the data set provided NA does not mean NA but indicates that feature does not exis.\n\nmissing_categorical = ['MiscFeature', 'Alley' , 'GarageType', 'MasVnrType', 'MSZoning', \n                       'Functional','Utilities', 'Exterior2nd', 'Exterior1st', 'Electrical' , 'SaleType']\namesdf[missing_categorical].isna().sum()\n\n#             # MiscFeature    2814\n#             # Alley          2721\n#             # GarageType      157\n#             # MasVnrType       24\n#             # MSZoning          4\n#             # Functional        2\n#             # Utilities         2\n#             # Exterior2nd       1\n#             # Exterior1st       1\n#             # Electrical        1\n## Note:\n## from the above output, on studying the dataset, we extrapolate that 'MSZoning', 'Functional', 'Utilities', 'Exterior2nd', 'Exterior1st', 'Electrical'\n## having na values does not mean \"No\" rather they should be substituted with some true values.\n## SO \"missing _categorical\" can be further divided into 2 sets:\n\nmissing_categorical_NAtoNO =    [ 'MiscFeature', 'Alley' , 'GarageType', 'MasVnrType' ]\n\nmissing_categorical_NAtoValue = ['MSZoning', 'Functional','Utilities', 'Exterior2nd', 'Exterior1st', 'Electrical','SaleType']\n\nmissing_ordinal =               ['PoolQC', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n                                'BsmtCond','BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'KitchenQual' ]\n\nmissing_quantitative =          ['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'BsmtHalfBath', 'BsmtFullBath',\n                                  'BsmtUnfSF' , 'TotalBsmtSF', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2','GarageCars' ]\n\n","6aa04f30":"# Now let's handle ordinal, categorical and missing or NA values\n\n# for missing_categorical_NAtoNO  fields, will replace NA with 'No'\nfor col in missing_categorical_NAtoNO:\n    amesdf[col] = amesdf[col].fillna('No')\n# now check if categoricals has yet any missing value.\namesdf[missing_categorical_NAtoNO].isna().sum()\n\n# for missing_categorical_NAtoValue  fields, will replace NA with different values.\n# so lets' replace NA for these fileds with maxvalue in the dataset i.e.\n#                 'Electrical' - 'SBrkr'\n#                 'MSZoning' - 'RL'\n#                 'Utilities' - 'AllPub'\n#                 'Functional' - 'Typ'\n#                 'Exterior1st' - 'VinylSd'\n#                 'Exterior2nd' - 'VinylSd'\n#                 'SaleType'    - \"WD\"\n\n\nfor cols in missing_categorical_NAtoValue:\n    maxvalue = amesdf[cols].value_counts().index[0]\n#     print(maxvalue)\n    amesdf[cols] = amesdf[cols].fillna(maxvalue)\n# now check if they has yet any missing value.\namesdf[missing_categorical_NAtoValue].isna().sum()\n\n\n# MasVnrType & GarageType has some values as \"None\" , so let's convert it to \"No\"\namesdf[\"MasVnrType\"] = amesdf[\"MasVnrType\"].replace('None', 'No')\n\n# now lets confirm how many fields are yet to be handled for missing or na values.\namesdf.isna().sum()[amesdf.isna().sum()>0]  # This returns nothing but \"missing_ordinal\"\namesdf.isna().sum()[amesdf.isna().sum()>0].sort_values(ascending=False)\n\n# so now time to handle missing values for ordinal\n# also to convert the all ordinal columns  from string to numeric values.\n# from above, we know ordinal are total 15 columns out of which 13 have missing values as pasted below.\n\n\nordinal =        ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                  'BsmtFinType2', 'HeatingQC',  'KitchenQual','FireplaceQu', \n                  'GarageQual', 'GarageCond','GarageFinish', 'PoolQC', 'Fence' ]\n\nmissing_ordinal= ['PoolQC', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n                  'BsmtCond','BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'KitchenQual', 'SaleType' ]\n\namesdf[\"ExterQual\"] = amesdf['ExterQual'].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['1','2','3','4','5'])\namesdf[\"ExterQual\"] = amesdf[\"ExterQual\"].astype('int64')\nprint(amesdf[\"ExterQual\"])\n\namesdf[\"ExterCond\"] = amesdf['ExterCond'].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['1','2','3','4','5'])\namesdf[\"ExterCond\"] = amesdf[\"ExterCond\"].astype('int64')\n\n\namesdf[\"HeatingQC\"] = amesdf['HeatingQC'].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['1','2','3','4','5'])\namesdf[\"HeatingQC\"] = amesdf[\"HeatingQC\"].astype('int64')\n\n\namesdf[\"PoolQC\"] = amesdf['PoolQC'].replace([np.nan, 'Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4'])\namesdf[\"PoolQC\"] = amesdf[\"PoolQC\"].astype('int64')\n\namesdf[\"Fence\"] = amesdf['Fence'].replace([np.nan, 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'], ['0','1','2','3','4'])\namesdf[\"Fence\"] = amesdf[\"Fence\"].astype('int64')\n\namesdf[\"FireplaceQu\"] = amesdf['FireplaceQu'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"FireplaceQu\"] = amesdf[\"FireplaceQu\"].astype('int64')\n\n\namesdf[\"GarageFinish\"] =  amesdf[\"GarageFinish\"].replace([np.nan, \"Unf\", \"RFn\", \"Fin\"], ['0','1','2','3'])\namesdf[\"GarageFinish\"] = amesdf[\"GarageFinish\"].astype(\"int64\")\n\namesdf[\"GarageQual\"] =  amesdf['GarageQual'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"GarageQual\"] =  amesdf['GarageQual'].astype(\"int64\")\n\n\namesdf[\"GarageCond\"] =  amesdf['GarageCond'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"GarageCond\"] =  amesdf['GarageCond'].astype(\"int64\")\n\namesdf[\"BsmtCond\"] =  amesdf['BsmtCond'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"BsmtCond\"] =  amesdf['BsmtCond'].astype(\"int64\")\n\namesdf[\"BsmtExposure\"] =  amesdf['BsmtExposure'].replace([np.nan, 'No','Mn', 'Av', 'Gd'], ['0','1','2','3','4'])\namesdf[\"BsmtExposure\"] =  amesdf['BsmtExposure'].astype(\"int64\")\n\namesdf[\"BsmtQual\"] =  amesdf['BsmtQual'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"BsmtQual\"] =  amesdf['BsmtQual'].astype(\"int64\")\n\namesdf[\"BsmtFinType1\"] = amesdf['BsmtFinType1'].replace(['GLQ','ALQ','BLQ','Rec','LwQ','Unf',np.nan],['6','5','4','3','2','1','0'])\namesdf[\"BsmtFinType1\"] =  amesdf['BsmtFinType1'].astype(\"int64\")\n\namesdf[\"BsmtFinType2\"] = amesdf['BsmtFinType2'].replace(['GLQ','ALQ','BLQ','Rec','LwQ','Unf',np.nan],['6','5','4','3','2','1','0'])\namesdf[\"BsmtFinType2\"] =  amesdf['BsmtFinType2'].astype(\"int64\")\n\namesdf[\"KitchenQual\"] = amesdf['KitchenQual'].replace([np.nan, 'Po','Fa', 'TA', 'Gd', 'Ex'], ['0','1','2','3','4', '5'])\namesdf[\"KitchenQual\"] =  amesdf['KitchenQual'].astype(\"int64\")\n\n","fc805b92":"# Now let's check if any of the oridinal columns have nan or missing values.Also see if they are all integerstype now.\n# So form below code, we can say all ordinals are now\n\nprint(amesdf[missing_ordinal].isna().sum())\nprint(amesdf[missing_categorical].isna().sum())\nprint(amesdf[ordinal].dtypes)\nprint(amesdf[categorical].dtypes)\nprint(amesdf[quantitative].dtypes)\n","faac0c88":"# let's handle missing or na values for  numerical or quantitative fields.\namesdf[missing_quantitative].isna().sum()\n            # LotFrontage     486\n            # GarageYrBlt     159\n            # MasVnrArea       23\n            # BsmtHalfBath      2\n            # BsmtFullBath      2\n            # BsmtUnfSF         1\n            # TotalBsmtSF       1\n            # GarageArea        1\n            # BsmtFinSF1        1\n            # BsmtFinSF2        1\n            # GarageCars        1\n# garagecars missing value will be replaced by the most frequent number used i.e. 2\namesdf[\"GarageCars\"] = amesdf[\"GarageCars\"].fillna(2)\n\n# garagearea missing value will be replaced by the most frequent number used i.e. its mean \namesdf[\"GarageArea\"] = amesdf[\"GarageArea\"].fillna(amesdf[\"GarageArea\"].mean())\n\n# GarageYrBlt has too much of odd data year starting from 1895, we will replace with median.\namesdf[\"GarageYrBlt\"] = amesdf[\"GarageYrBlt\"].fillna( amesdf[\"GarageYrBlt\"].median()).astype('int64')\n\n# BsmtFinSF1 and BsmtFinS21, BsmtUnfSF , TotalBsmtSF ,BsmtHalfBath, BsmtFullBath  none means no basement, so can be repalced with 0\namesdf[\"BsmtFinSF1\"] = amesdf[\"BsmtFinSF1\"].fillna(0)\namesdf[\"BsmtFinSF2\"] = amesdf[\"BsmtFinSF1\"].fillna(0)\namesdf[\"BsmtUnfSF\"] = amesdf[\"BsmtUnfSF\"].fillna(0)\namesdf[\"TotalBsmtSF\"] = amesdf[\"TotalBsmtSF\"].fillna(0)\namesdf[\"BsmtHalfBath\"] = amesdf[\"BsmtHalfBath\"].fillna(0)\namesdf[\"BsmtFullBath\"] = amesdf[\"BsmtFullBath\"].fillna(0)\namesdf[\"MasVnrArea\"] = amesdf[\"MasVnrArea\"].fillna(0)\namesdf[\"LotFrontage\"] = amesdf[\"LotFrontage\"].fillna(0)\n\n# lets checl if there is still any na or missing\namesdf[missing_quantitative].isna().sum()\n\n","bb379eda":"## lets print the dataframe shape\nprint(amesdf.shape)\n# GarageCond, GarageQual and GarageFin can be combined to get a garage score by taking theri mean.\n# will drop the orignal columns after that\namesdf['GarageScore'] = (amesdf['GarageQual'] + amesdf['GarageCond'] + amesdf['GarageFinish'])\/3\namesdf.drop(['GarageQual', 'GarageCond', 'GarageFinish'], axis=1, inplace =True)\n\n# let's take the mean of all Porch to create the totalporch and drop the original columns\namesdf['PorchSF'] = ( (amesdf['OpenPorchSF'] + amesdf['EnclosedPorch'] + amesdf['3SsnPorch']+ amesdf['ScreenPorch']) \/4 )\namesdf[[\"PorchSF\",\"SalePrice\"]].corr()\n\n\n# now lets create a new field BsmntScore using bsmnt related fields.\namesdf['BsmntScore'] = (amesdf['BsmtQual'] + amesdf['BsmtCond'] + amesdf['BsmtExposure'])\/3\namesdf.drop( ['BsmtQual' ,'BsmtCond', 'BsmtExposure'], axis=1, inplace=True )\n\n\n# will create a score by using BsmtFinType1 - BsmtFinSF1 and BsmtFinType2 - BsmtFinSF2 and \n# also TotalBsmntSF area is sum of BsmtFinSF1, BsmtFinSF2 and BsmtUnfSF, will drop BsmtUnfSF and TotalBsmntSF\n# to handle divide by zero issue, I replaced 0 with 1 while using the below denominator \namesdf['BsmntSFScore'] = (amesdf['BsmtFinType1']*amesdf['BsmtFinSF1'].values + amesdf['BsmtFinType2'].values*amesdf['BsmtFinSF2'].values) \/ (amesdf['BsmtFinSF1'].replace(0,1).values + amesdf['BsmtFinSF2'].replace(0,1).values)\namesdf.drop([\"BsmtFinType1\",\"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\"], axis=1, inplace=True)\n\n\n\n## As GrLivArea is sum of 1stFlrSF , 2ndFlrSF  and LowQualFinSF, so we can drop 1stFlrSF , 2ndFlrSF  and LowQualFinSF\namesdf.drop([\"1stFlrSF\",\"2ndFlrSF\", \"LowQualFinSF\"], axis=1, inplace=True)\n\n\n## lets create another columns TotalBath using all other bath fields.\namesdf['TotalBath'] = amesdf[\"BsmtFullBath\"]+ amesdf[\"BsmtHalfBath\"] + amesdf[\"FullBath\"] + amesdf[\"HalfBath\"]\namesdf.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace=True)\n\n\n# Convert year numbers to \"Years since...\" numbers\n#'YearBuilt', 'YearRemodAdd','YrSold' by themselves do not mean much so let's conver them to how old it is.\n    \namesdf[\"YearBuiltAgo\"] = 2010-amesdf[\"YearBuilt\"]\namesdf[\"YearRemodAddAgo\"] = 2010-amesdf[\"YearRemodAdd\"]\namesdf[\"YrSoldAgo\"] = 2010-amesdf[\"YrSold\"]\namesdf[\"GarageYrBltAgo\"] = 2010-amesdf[\"GarageYrBlt\"]\namesdf.drop ([\"YearBuilt\",\"YearRemodAdd\",\"YrSold\",\"GarageYrBlt\"], axis=1, inplace=True)\n\n## let's convert SaleMonth (1-12) into Winter (12,1,2), Sprng(3,4,5), Summer(6,7,8), and Autumn (9,10,11)\n## so \"MoSold\" will be converted from categorical to numerical\namesdf['MoSold'] = amesdf['MoSold'].replace([12,1,2,3,4,5,6,7,8,9,10,11], ['Winter', 'Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer', 'Summer',\n                                                  'Summer', 'Autumn', 'Autumn', 'Autumn'])\n\n\n","cfe399a4":"pd.set_option('display.max.columns', 1000)\n\n# we can check if there is any nan or missing value in the cleansed dataframe \nlen(amesdf.isna().sum()[amesdf.isna().sum()>0])\n\n# let's save it to csv to have a glance on the cleased data.\namesdf.to_csv(\"amesdf.csv\", index=False)\n\n# now total of 65 columns 2919 rows (1460 from train and 1459 from test data)\n# now let's check how many qualitative and quantitative\namesdf.shape  \nlen(amesdf.select_dtypes(include='object').columns.sort_values()) # 29 qualitative\nlen(amesdf.select_dtypes(exclude='object').columns.sort_values()) # 36 quanitative\n\n\nqualitative_cleaned =  amesdf.select_dtypes(include='object').columns.sort_values()\n# lets convers the index to array\nqualitative_cleaned = list(qualitative_cleaned.values)\n\nquantitative_cleaned =   amesdf.select_dtypes(exclude='object').columns.sort_values()\n# lets convers the index to array\nquantitative_cleaned = list(quantitative_cleaned.values)\n\n\n# now lets get the dummies to convert all categorical to numerical(dummies columns)\nfor cols in qualitative_cleaned:\n    dummies =(pd.get_dummies(amesdf[cols], prefix=cols))\n    amesdf = pd.concat([amesdf, dummies], 1)   \n    amesdf= amesdf.drop(cols, axis=1)\n\n\n# now lets check the total columns after converting all categorical into dummies(mumeirc)\n##2919 rows , 228 columns now\namesdf.shape  \n","a42b96bf":"# lets check if it has all numeric or yet any object data type.It has no object type,so we are good with all numeric fields. reqd for machine learning.\namesdf.select_dtypes(include='object').columns\n\n# let's check the correlations now.\namesdf.iloc[0:1460, :].corr()[\"SalePrice\"].sort_values()\n\nimport seaborn as sns\n\n# lets plot GrLivArea for the train data against Sale Price.\namesdf[[\"SalePrice\",\"GrLivArea\"]].iloc[0:1460, :].corr()\nsns.lmplot(data= amesdf[[\"SalePrice\",\"GrLivArea\"]].iloc[0:1460, :], x=\"GrLivArea\", y = 'SalePrice',  sharex=True)\n\n# from the scatterplot, it is clear that two sales are outliers as it has very less price but the biggest square area.\namesdf[amesdf.GrLivArea >= 4500][\"SalePrice\"]\namesdf = amesdf.drop([524,1299], axis=0)\n\n# so now check if the outlier are gone.\nsns.lmplot(data= amesdf[[\"SalePrice\",\"GrLivArea\"]].iloc[0:1458, :], x=\"GrLivArea\", y = 'SalePrice',  sharex=True)\n\n","371d30e5":"amesdf.to_csv(\"..\/final.csv\")","96ec74df":"# lets split the file separate the train and test data provided by kaggle.\n# remmeber ,the train.csv had intitally 1460 so now, it is 1458 rows after deleting the 2 rows above.\n# and test would contain 1459 as usual. so lets' split now and also drop Index label.\namesdftrain = amesdf.iloc[0:1458, :].reset_index(drop=True)\namesdftest = amesdf.iloc[1458:, :]\n\nX_train = amesdftrain.drop(\"SalePrice\", axis=1) \n\nfrom sklearn.preprocessing import RobustScaler\nX_train_scaled = RobustScaler().fit_transform(X_train)\n\n\nY_train =  amesdftrain[\"SalePrice\"]\nY_train.skew()   ### 1.881\n# lets conver it to natural log.\n\nY_train = np.log1p(amesdftrain[\"SalePrice\"])\n\n\nY_train.skew() \n# 0.121 reduced alot. so we will take this as depenedent variable and after preciting will convert them back to\n# its original value by using exponential np.exp()\n \nX_test = amesdftest.drop(\"SalePrice\", axis=1) \nX_test_scaled = RobustScaler().fit_transform(X_test)\n\nX_train_scaled.shape, X_test_scaled.shape\n\n","4c691257":"amesdftest.shape","180c3487":"from sklearn.decomposition import PCA\npca =PCA(228)\n# pca =PCA(0.95)\npca.fit(X_train_scaled)\n\nX_train_scaled_pca= pca.fit_transform(X_train_scaled)\nX_test_scaled_pca= pca.fit_transform(X_test_scaled)\n\n\nprint(np.round(pca.explained_variance_ratio_*100, decimals=1) )\nprint(np.round(pca.components_, decimals=1) )\nprint(X_train_scaled_pca.shape, X_test_scaled_pca.shape)\n","55be534d":"from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor , AdaBoostRegressor, GradientBoostingRegressor \nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nlinear = make_pipeline( RobustScaler(), LinearRegression() )\nlasso = make_pipeline( RobustScaler(), Lasso() )\nridge  = make_pipeline( RobustScaler(), Ridge() )\nelastic = make_pipeline( RobustScaler(), ElasticNet() )\ndecisiontree = make_pipeline( RobustScaler(), DecisionTreeRegressor() )\nneighbors = make_pipeline( RobustScaler(), KNeighborsRegressor() ) \nsvr  = make_pipeline( RobustScaler(), SVR() )\nExtratree = make_pipeline( RobustScaler(), ExtraTreesRegressor() )\nRandomForest = make_pipeline(RobustScaler(), RandomForestRegressor() )\nAdaBoost = make_pipeline( RobustScaler(), AdaBoostRegressor() )\nGradientBoost = make_pipeline( RobustScaler(), GradientBoostingRegressor() )\nXGBoost =  make_pipeline( RobustScaler(), XGBRegressor() )      \nLightGBM =  make_pipeline( RobustScaler(),  LGBMRegressor() )    \nStacking=StackingCVRegressor(regressors=\n(linear, lasso, ridge, elastic, decisiontree, neighbors, svr, Extratree,RandomForest,AdaBoost, GradientBoost, XGBoost,\n LightGBM ), meta_regressor=XGBoost, use_features_in_secondary=True) \n\nmodel = {\n    \n         \"linear\":         linear  ,\n         \"lasso\":          lasso  , \n         \"ridge\":          ridge   ,\n         \"elastic\":        elastic ,\n         \"decisiontree\":   decisiontree ,\n         \"neighbors\":      neighbors     ,\n         \"SVR\":             svr  ,        \n         \"ExtraTree\":       Extratree  ,  \n         \"RandomForest\":    RandomForest  ,\n         \"AdaBoost\":        AdaBoost    ,  \n         \"GradientBoost\":   GradientBoost,  \n         \"XGBBoost\":        XGBoost    ,\n         \"LightGBM\":        LightGBM ,\n         \"Stacking\":       Stacking     \n        \n        }\n\n\nscore=[] \nalgorithm = []                            \n                             \n                             \nfor index, model in model.items():\n    rmse = np.mean(np.sqrt(-(cross_val_score(model, X_train.values, Y_train.values, cv=KFold(n_splits=10,shuffle=True, random_state=0), scoring=\"neg_mean_squared_error\"))))\n    score.append(rmse)\n    algorithm.append(model)                                         \n    print(index,\"RMSE score {} \".format(rmse), datetime.now() )\n\nprint(\"minimum score is \", np.min(score))\n                             \n\n","30bdd994":"## let's use the GridSearch hyperparamter optimzation library to find the best prameter values of each algorithm.\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport warnings\nwarnings.simplefilter('ignore')\n\nX_train_scaled  = RobustScaler().fit_transform(X_train)\n\n## lets create a dictionary with all models and their respective parameter to be tuned.\ntuning = {\n          Lasso() :      {'alpha':        [1,0.1,0.01,0.001,0.0001,1e-04,1e-05] , \n                         'max_iter' :    [100,500, 1000, 1500, 2000, 2500, 3000]\n                         },\n          Ridge() :      {'alpha':        [0.1,1, 10,  50, 100, 200, 500, 1000] ,\n                         'max_iter' :     [100,500, 1000, 1500, 2000, 2500, 3000]\n                         },\n          ElasticNet() :     {'alpha':       [1,0.1,0.01,0.001,0.0001,1e-04,1e-05] ,\n                            'max_iter' :     [100,500, 1000, 1500, 2000, 2500, 3000]\n            \n                             },\n          DecisionTreeRegressor():   \n                              {'max_depth' : [5, 7, 9, 11, 13, 15],\n                              'min_samples_split' :  [2,3 ,4,5,6, 8, 10],\n                              'min_samples_leaf'  :  [1,2,3 ,4,5,6, 8, 10],\n                              'max_features'      :  ['auto', 'sqrt', 'log2']\n                             },\n          KNeighborsRegressor() : \n                            {'n_neighbors' : [1,2,3, 4 ,5 ,6 ,7 ,8, 9, 10],\n                            'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                            'p'           : [1, 2]  ,\n                             'n_jobs'     : [1,2, 3,4,5]\n                            },\n    \n         SVR()     :     {\n                        'kernel' :      ['linear', 'poly', 'rbf', 'sigmoid'],\n                        'C'      :      [1, 2, 3,4 ,5],\n                        'epsilon':      [0.001, 0.01,0.5 ,0.1,1.0, 1.5, 2.0],\n                        'max_iter' :    [100,300,500, 1000, 1500, 2000, 2500, 3000]  \n                        },\n\n        ExtraTreesRegressor() :   { 'n_estimators' :     [100 ,500 ,1000 ,1500 ,2000, 2500, 3000 ,3500],\n                           'max_depth' :        [4 ,5 ,6 ,7 ,8, 9, 10,11, 12],\n                          'min_samples_split' : [2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'min_samples_leaf'  : [1,2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'max_features'      : ['auto', 'sqrt', 'log2']\n                         },\n                          \n       RandomForestRegressor() :  \n                          {'n_estimators' :      [100 ,500 ,1000 ,1500 ,2000, 2500, 3000, 3500],\n                          'max_depth' :         [4 ,5 ,6 ,7 ,8, 9, 10,11, 12],\n                          'min_samples_split' : [2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'min_samples_leaf'  : [1,2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'max_features'      : ['auto', 'sqrt', 'log2']\n                           } ,   \n       AdaBoostRegressor() : \n                            {\n                            'learning_rate':      [0.001, 0.005, 0.01, 0.05, 0.1],\n                            'n_estimators' :      [100 ,500 ,1000 ,1500 ,2000, 2500, 3000, 3500],\n                            'loss'         :      ['linear', 'square','exponential']\n                            },\n                \n        GradientBoostingRegressor() :   \n                            {\n                          'learning_rate':      [0.001, 0.005, 0.01, 0.05, 0.1],\n                          'n_estimators' :      [100 ,500 ,1000 ,1500 ,2000, 2500, 3000, 3500],\n                          'max_depth' :         [4 ,5 ,6 ,7 ,8, 9, 10,11, 12],\n                          'min_samples_split' : [2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'min_samples_leaf'  : [1,2,3 ,4,5 ,6 ,7 ,8, 9, 10],\n                          'max_features'      : ['auto', 'sqrt', 'log2']\n                            }   ,\n          \n         XGBRegressor():    { 'learning_rate' :[0.001, 0.005, 0.01, 0.05, 0.1],\n                              'n_estimators' : [100 ,500 ,1000 ,1500 ,2000, 2500, 3000, 3500]\n                  \n                            },\n          LGBMRegressor():  { 'learning_rate' :[0.001, 0.005, 0.01, 0.05, 0.1],\n                               'n_estimators' : [100 ,500 ,1000 ,1500 ,2000, 2500, 3000, 3500]\n            \n                             }                \n          \n          \n         }\n\n   \n     \nfor key ,val in tuning.items():\n#     grid = GridSearchCV(estimator=key, param_grid=val)  \n    grid = RandomizedSearchCV(estimator=key, param_distributions=val, n_iter=10, random_state=7)\n    grid.fit(X_train_scaled, Y_train)\n    print(grid.best_score_, grid.best_params_,)\n   \n","a3398e81":"\nlinear = make_pipeline( RobustScaler(), LinearRegression() )\n\nlasso = make_pipeline(  RobustScaler(), Lasso(alpha = 0.001, max_iter = 500) )\n\nridge  = make_pipeline(  RobustScaler(), Ridge(alpha = 10, max_iter = 300) )\n\nelastic = make_pipeline(  RobustScaler(), ElasticNet(alpha = 0.001, max_iter = 500) )\n\ndecisiontree = make_pipeline(  RobustScaler(), DecisionTreeRegressor(max_depth=7, max_features= 'auto', min_samples_leaf =8, min_samples_split= 3) )\n\nneighbors = make_pipeline(  RobustScaler(), KNeighborsRegressor(algorithm= 'auto', leaf_size= 20, n_neighbors= 4, p= 1, n_jobs=2) ) \n\nsvr  = make_pipeline(  RobustScaler(), SVR(kernel='rbf', C= 3, epsilon= 0.01, max_iter = 2500) )\n\nExtratree = make_pipeline(  RobustScaler(), ExtraTreesRegressor(n_estimators= 100, min_samples_split= 4, min_samples_leaf= 4, max_features= 'auto', max_depth = 10) )\n\nRandomForest = make_pipeline( RobustScaler(), RandomForestRegressor(n_estimators=100, min_samples_split = 4, min_samples_leaf= 4, max_features = 'auto', max_depth= 10) )\n\nAdaBoost = make_pipeline(  RobustScaler(), AdaBoostRegressor(n_estimators= 1000, loss= 'linear', learning_rate= 0.05) )\n\nGradientBoost = make_pipeline(  RobustScaler(), GradientBoostingRegressor(n_estimators= 3500, min_samples_split= 8, min_samples_leaf = 6, max_features= 'sqrt', max_depth = 5, learning_rate= 0.05) )\n\nXGBoost =  make_pipeline(  RobustScaler(), XGBRegressor(n_estimators=3000 , learning_rate=.01) )     \n\nLightGBM =  make_pipeline( RobustScaler(), LGBMRegressor(n_estimators=1000 ,learning_rate = 0.01,random_state=0) )    \n\nStacking=StackingCVRegressor(regressors=\n                      (lasso,  elastic, decisiontree, neighbors, svr, GradientBoost, XGBoost, LightGBM ),\n                       meta_regressor=XGBoost,\n                       use_features_in_secondary=True) \n\nmodel = {\n    \n         \"linear\":          linear  ,\n         \"lasso\":           lasso  , \n         \"ridge\":           ridge   ,\n         \"elastic\":         elastic ,\n         \"decisiontree\":    decisiontree ,\n         \"neighbors\":       neighbors     ,\n         \"SVR\":             svr  ,        \n         \"ExtraTree\":       Extratree  ,  \n         \"RandomForest\":    RandomForest  ,\n         \"AdaBoost\":        AdaBoost    ,  \n         \"GradientBoost\":   GradientBoost,  \n         \"XGBBoost\":        XGBoost  ,  \n         \"LightGBM\":        LightGBM ,\n         \"Stacking\":        Stacking     \n        \n        }\n\n\nscore=[] \nalgorithm = []                            \n                             \n                             \nfor index, model in model.items():\n    rmse = np.mean(np.sqrt(-(cross_val_score(model, X_train.values, Y_train.values, cv=KFold(n_splits=10,shuffle=True, random_state=0), scoring=\"neg_mean_squared_error\"))))\n    score.append(rmse)\n    algorithm.append(model)                                         \n    print(index,\"RMSE score {:.4f} \".format(rmse), datetime.now() )\n\nprint(\"minimum score is {:.4f} \".format( np.min(score)) )\n                             \n\n# model\tscore\n# ridge \t0.1168\n# elastic \t0.1170\n# lasso \t0.1185\n# stacking\t0.1195\n# GradientBoost \t0.1199\n# XGBBoost \t0.1219\n# linear\t0.1229\n# LightGBM \t0.1259\n# RandomForest \t0.1423\n# ExtraTree \t0.1446\n# AdaBoost \t0.1721\n# SVR \t0.1768\n# decisiontree \t0.1889\n# neighbors \t0.2067\n\n\n","133af6f4":"# let's fit the top 7 performing models as we saw RMSE above, they are : \n# lasso,elastic, ridge, GradientBoost, XGBoost, LightGBM, Stacking\n\nlasso_model = lasso.fit(X_train_scaled, Y_train) \nelastic_model = elastic.fit(X_train_scaled, Y_train) \nridge_model = ridge.fit(X_train_scaled,Y_train) \nGradientBoost_model = GradientBoost.fit(X_train_scaled, Y_train) \nXGBoost_model = XGBoost.fit(X_train_scaled, Y_train) \nLightGBM_model = LightGBM.fit(X_train_scaled, Y_train) \nStacking_model = Stacking.fit(X_train_scaled, Y_train) \n\n\n# let's use blending : blend the predicted values from each of the models above and take theeir  average.\n\nY_train_pred =  (.10 * lasso.predict(X_train_scaled)         + \n                 .10 * elastic_model.predict(X_train_scaled)       +  \n                 .10*  ridge_model.predict(X_train_scaled)          +\n                 .10 * GradientBoost_model.predict(X_train_scaled) + \n                 .10 * XGBoost_model.predict(X_train_scaled)       + \n                 .10 * LightGBM_model.predict(X_train_scaled)      + \n                 .40 * Stacking_model.predict(X_train_scaled) \n                 ) \n \n\nfrom sklearn.metrics import mean_squared_error, r2_score\nRMSE_train_blended = np.sqrt(mean_squared_error(Y_train, Y_train_pred ))\nR2_train_blended = np.sqrt(r2_score(Y_train, Y_train_pred ))\nprint(\"RMSE for the blended model is {:.4f}\".format(RMSE_train_blended) )\nprint(\"R2 for the blended model is {:.4f}\".format(R2_train_blended) )","e9625520":"# We can check the heterosedascticity now which is prettey fine .\nimport matplotlib.pyplot as plt\nplt.scatter(Y_train-Y_train_pred, Y_train_pred )","691aa538":"# finally let's predict the test data now \n\nY_test_pred =  (.10 * lasso.predict(X_test_scaled)                + \n                 .10 * elastic_model.predict(X_test_scaled)       +  \n                 .10*  ridge_model.predict(X_test_scaled)         +\n                 .10 * GradientBoost_model.predict(X_test_scaled) + \n                 .10 * XGBoost_model.predict(X_test_scaled)       + \n                 .10 * LightGBM_model.predict(X_test_scaled)      + \n                 .40 * Stacking_model.predict(X_test_scaled) \n                 ) \n \nY_test_pred_actual = np.expm1(Y_test_pred)\n\n","6316d92e":"final_result = pd.DataFrame( { 'Id':amestestdf.index, 'SalePrice':Y_test_pred_actual })\nfinal_result.to_csv(\"..\/final_result_submission.csv\",index= False)\n\n\n","a5c13105":"Data Set from the House Prices: Advance Regression Techniques competition on Kaggle: \n[link](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)\n\n\t\nOverview:\n\t\nThe data was provided as train.csv (with house prices), and test.csv (new observations with all the same features as train.csv, but missing the Sale Price. \nTrain.CSV has 81 columns including ID and SalePrice whereas Test.CSV has only 80 columns without SalePrice field which We need to predict.\n\t\n **Approach:**\n\nCleaned\/processed train.csv and test.csv together. Performed manual feature removal\/alteration based on detailed reading of the data description file. Obtained feature list using AIC minimization method and applied this list in model development with no changes.\nProcessed the train.csv dataset, performed feature engineering by a number of methods and optimized model hyperparameters  performed a stacking algorithm of different models based on train\/test optimization, and created the final csv file for Kaggle submission.\n\n**Repository Contents:**\nPlease find my GIT repository link where all my work for this project is checkedin.\n[kaggle link](https:\/\/github.com\/mofidanjum\/DataScienceGit\/tree\/master\/My%20Kaggle%20-%20Projects\/Boston%20House%20Predcition%20-%20Kaggle)\n\n\t\nData Set Study & Findings:\nThis project has been completed by thoroughly studying the data a sort of gaining Domain Knowledge about majority of the fields possible.\nThe data set has these many fields:\n\t\t          \n`Id       MSSubClass\tMSZoning\tLotFrontage\tLotArea\tStreet\tAlley\tLotShape\tLandContour\tUtilities\tLotConfig\tLandSlope\tNeighborhood\tCondition1\tCondition2\tBldgType\tHouseStyle\tOverallQual\tOverallCond\tYearBuilt\tYearRemodAdd\tRoofStyle\tRoofMatl\tExterior1st\tExterior2nd\tMasVnrType\tMasVnrArea\tExterQual\tExterCond\tFoundation\tBsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1\tBsmtFinSF1\tBsmtFinType2\tBsmtFinSF2\tBsmtUnfSF\tTotalBsmtSF\tHeating\tHeatingQC\tCentralAir\tElectrical\t1stFlrSF\t2ndFlrSF\tLowQualFinSF\tGrLivArea\tBsmtFullBath\tBsmtHalfBath\tFullBath\tHalfBath\tBedroomAbvGr\tKitchenAbvGr\tKitchenQual\tTotRmsAbvGrd\tFunctional\tFireplaces\tFireplaceQu\tGarageType\tGarageYrBlt\tGarageFinish\tGarageCars\tGarageArea\tGarageQual\tGarageCond\tPavedDrive\tWoodDeckSF\tOpenPorchSF\tEnclosedPorch\t3SsnPorch\tScreenPorch\tPoolArea\tPoolQC\tFence\tMiscFeature\tMiscVal\tMoSold\tYrSold\tSaleType\tSaleCondition\tSalePrice`\n\t\n\t\n","8cbd0351":"###  Finding the  right value of  model parameters - Hyperparameter Optimization","59437dd8":"### Get Dummies to convert all qualitative columns to numeric columns","9b597ea6":"### Now fit the models.\n","a783e5ce":"## Predicting House Prices in Ames Iowa using Regression","ba6bbe08":"###  EDA","37813772":"### Evalutaing models again with tuned hyperparameters","6352d626":"### Evaluating Models","48e3dcbf":"### Handling missing or NA values","e6c5122d":"## Feature Engineering","b5a20259":"### Dimensional Reduction - PCA\n\nNotice the code below shows .95 for the number of component parameters.It means that as per scikit learn choose the number of principle components such that 95% of the variance is retained and the 1st component itself is good enough to explain more than 95 %  i.e. 97.8 % of the variance as can be seen below.So we will not use this PCA rather will go ahead with its original data set."}}