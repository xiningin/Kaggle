{"cell_type":{"4778dafd":"code","ba7f0aae":"code","354f7595":"code","7699eec8":"code","bfa5ac42":"code","e720b226":"code","917b7034":"code","1ce2b1c6":"code","e738b2fd":"code","2c807a2b":"code","8d7163a7":"code","041adeb0":"code","fc8da5fe":"code","b10c4c0c":"code","fe470e12":"code","5699a687":"code","e481a524":"code","1ce05834":"code","490da3f6":"code","370f5a6b":"code","e17d9fe0":"code","5ab5fe21":"code","8c8c3172":"code","08b5aa14":"code","00ee5897":"code","e7b4f1be":"code","009c8636":"code","8d3187b9":"code","b2354b11":"code","76d4f931":"code","3cbd5b8c":"code","db639512":"code","87d984f9":"code","371fc09a":"code","149406d9":"code","827b3941":"code","8beb0fec":"code","76024c6a":"code","01ba73f0":"code","85f30b81":"code","5e42af32":"code","3d1892da":"code","10fbea4a":"code","58da43bf":"code","ac02cd02":"code","0a0cdaec":"code","305e2629":"code","e1ffce1a":"code","03e0dacb":"code","bde591b0":"code","703fc75d":"code","02318a4a":"code","5ca46bec":"code","79bc423e":"code","1a0934ca":"code","5956e80c":"code","7cbd1956":"code","dcb333c7":"code","062ead43":"code","e747b929":"markdown","78ad3561":"markdown","b6c77688":"markdown","c057271b":"markdown","ec263873":"markdown","2bef8559":"markdown","a7765394":"markdown","4e9c06c5":"markdown","5908de8f":"markdown","2bc5f1cd":"markdown","7f848c1e":"markdown","8d9dcf39":"markdown","7c273042":"markdown","0e99ad7f":"markdown","465f6d4f":"markdown","5d965e47":"markdown","7a90f313":"markdown","29f36eda":"markdown","356b67db":"markdown","27545082":"markdown","6d2d1e0a":"markdown","68518f7a":"markdown","ec7a4d79":"markdown","e8f8c0f3":"markdown","d29e48eb":"markdown","020b96ab":"markdown","5cd50130":"markdown","ae9bee1a":"markdown","d90efd6f":"markdown","2165ef3e":"markdown","ad568fce":"markdown","f4d3551a":"markdown","b314b14e":"markdown","59ce93b7":"markdown","ba1080ef":"markdown","425d471a":"markdown","8bfd0f1a":"markdown","15bf3bc9":"markdown","64603f49":"markdown","ec077048":"markdown","a49ae365":"markdown","332951bb":"markdown","0c8b88f5":"markdown","5b344874":"markdown","2123e5cd":"markdown","c2a9f652":"markdown","163b9a6f":"markdown","589a6e2e":"markdown","c2c551bb":"markdown","5256e9ee":"markdown","2b07b5be":"markdown","99b595af":"markdown","c4f54c46":"markdown","36584d7d":"markdown","fd50b74a":"markdown","3544e947":"markdown","aca38b63":"markdown","61d6bed2":"markdown","6f9244c3":"markdown","ed2e7994":"markdown"},"source":{"4778dafd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba7f0aae":" housing = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')","354f7595":"housing.head()","7699eec8":"housing.columns","bfa5ac42":"housing.info()","e720b226":"housing.ocean_proximity.value_counts()","917b7034":"housing.describe()","1ce2b1c6":"# We can call hist() method on the entire dataframe\n# it uses pandas' hist() method to plot the histogram of each numerical attribute in the df\nhousing.hist(figsize=(20,15), bins=50)","e738b2fd":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) #specifying random_state makes sure the same split is performed each time this function is called","2c807a2b":"housing.median_income.hist(bins=50)","8d7163a7":"housing['income_cat'] = pd.cut(housing['median_income'],\n                              bins=[0.,1.5,3.0,4.5,6.,np.inf],\n                              labels=[1,2,3,4,5])\nhousing['income_cat'].hist()\nhousing.head()","041adeb0":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","fc8da5fe":"strat_test_set['income_cat'].value_counts()\/len(strat_test_set)","b10c4c0c":"housing['income_cat'].value_counts()\/len(housing)","fe470e12":"# random train-test split of housing data containing income_cat attribute\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","5699a687":"def income_cat_proportions(data):\n    return data['income_cat'].value_counts()\/len(data)\ncompare_props = pd.DataFrame({\n    'Overall': income_cat_proportions(housing),\n    'Stratified': income_cat_proportions(strat_test_set),\n    'Random': income_cat_proportions(test_set)\n}).sort_index()\n\ncompare_props['Random %error'] = 100 * compare_props['Random']\/compare_props['Overall']-100\ncompare_props['Stratified %error'] = 100 * compare_props['Stratified']\/compare_props['Overall']-100\ncompare_props","e481a524":"# the income_cat attribute is dropped from the train and test datasets\n# We are still keeping the stratified samples\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_cat', axis=1, inplace=True)","1ce05834":"housing = strat_train_set.copy()","490da3f6":"housing.plot(kind='scatter',x='longitude',y='latitude',figsize=(10,8))","370f5a6b":"housing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.1,figsize=(10,8))","e17d9fe0":"import matplotlib.pyplot as plt\nhousing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.4,\n            s=housing['population']\/100, label='population',figsize=(10,8),\n            c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True)\nplt.legend()","5ab5fe21":"corr_matrix = housing.corr()\ncorr_matrix # corr() returns a dataframe","8c8c3172":"corr_matrix['median_house_value'].sort_values(ascending=False)","08b5aa14":"from pandas.plotting import scatter_matrix\nattributes = ['median_house_value','median_income','total_rooms','housing_median_age'] # choosing some important attributes to reduce number of plots in figure\nscatter_matrix(housing[attributes],figsize=(15,12))","00ee5897":"housing.plot(kind='scatter',x='median_income',y='median_house_value',alpha=0.1, figsize=(10,8))","e7b4f1be":"# let us create some new attributes that will make much more sense\nhousing['rooms_per_household'] = housing['total_rooms']\/housing['households']\nhousing['bedrooms_per_room'] = housing['total_bedrooms']\/housing['total_rooms']\nhousing['population_per_household'] = housing['population']\/housing['households']","009c8636":"corr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","8d3187b9":"housing = strat_train_set.drop('median_house_value', axis=1)\nhousing_labels = strat_train_set['median_house_value'].copy()","b2354b11":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')","76d4f931":"housing_num = housing.drop('ocean_proximity', axis=1)\nhousing_num.head()","3cbd5b8c":"imputer.fit(housing_num)","db639512":"imputer.statistics_","87d984f9":"housing_num.median().values","371fc09a":"X = imputer.transform(housing_num) # returns a plain numpy array containing the transformed features\nhousing_tr = pd.DataFrame(X, columns = housing_num.columns) # put it back in a dataframe\nhousing_tr.head()","149406d9":"housing_cat = housing[['ocean_proximity']]\nhousing_cat.head(10)\n# housing.head()","827b3941":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","8beb0fec":"ordinal_encoder.categories_","76024c6a":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot #scipy matrix and not numpy array","01ba73f0":"# we can call toarray() method if we want in a dense array\nhousing_cat_1hot.toarray()","85f30b81":"# we can get the list of categories using the encoder's categories_ instance variable\ncat_encoder.categories_","5e42af32":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 # these are just the indices of required columns\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self \n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix]\/X[:,households_ix]\n        population_per_household = X[:, population_ix]\/X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix]\/X[:,rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","3d1892da":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# the names of transformers 'imputer', 'attribs_adder', 'std_scaler' are user-defined\n# Pipeline constructor takes a list of name\/estimator pairs defining a sequence of steps\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder',CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler())\n])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","10fbea4a":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = ['ocean_proximity']\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs),\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\nprint(housing_prepared.shape)\nprint(housing_prepared)","58da43bf":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","ac02cd02":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","0a0cdaec":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","305e2629":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","e1ffce1a":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                        scoring='neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-scores)","03e0dacb":"def display_scores(scores):\n    print('Scores:',scores)\n    print('\\nMean:', scores.mean())\n    print(\"\\nStandard Deviation\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","bde591b0":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                            scoring='neg_mean_squared_error',cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","703fc75d":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)","02318a4a":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                               scoring='neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","5ca46bec":"from sklearn.model_selection import GridSearchCV\n\n# two dictionaries are passed in the param_grid\n# dict1 = 3*4 = 12 combinations tried out\n# dict2 = 1*2*3 = 6 combinations tried out\n# In total 12+6 = 18 combinations tried out\nparam_grid = [\n    {'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},\n    {'bootstrap':[False], 'n_estimators':[3,10], 'max_features':[2,3,4]}\n]\n\n\nforest_reg = RandomForestRegressor()\n\n# cv = 5 means 5-fold cross validation\n# 18 hyperparameter combinations for each fold => 18*5 = 90 total rounds of training!\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                          scoring = 'neg_mean_squared_error',\n                          return_train_score = True)\n\ngrid_search.fit(housing_prepared, housing_labels)","79bc423e":"grid_search.best_params_ # returns the best hyperparameter values","1a0934ca":"grid_search.best_estimator_ # returns the best estimator directly","5956e80c":"# we can also print the evaluation scores\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","7cbd1956":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\n# returns an array representing the importance of each attribute in the training set ","dcb333c7":"# we are trying to get a list of all the attribute names - orginal + extra added by us + dummy attributes after one hot encoding\nextra_attribs = ['rooms_per_hhold','pop_per_hhold','bedrooms_per_room'] # a list of names for additional attributes that we added previously\ncat_encoder = full_pipeline.named_transformers_['cat']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nprint((attributes))\nsorted(zip(feature_importances, attributes), reverse = True) # lists the attributes in decreasing order of feature_importances","062ead43":"final_model = grid_search.best_estimator_\n\n# separate out the predictors and labels\nX_test = strat_test_set.drop('median_house_value',axis = 1)\ny_test = strat_test_set['median_house_value'].copy()\n\n# apply the transformations\nX_test_prepared = full_pipeline.transform(X_test)\n\n# make predictions\nfinal_predictions = final_model.predict(X_test_prepared)\n\n# evaluate\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","e747b929":"Since the median can only be computed on numerical attributes, we need to create a copy of the data without the ocean_proximity attribute.","78ad3561":"The info() method is useful to get a quick description of the data, such as total number of rows, dtypes of attributes, number of non-null values etc.","b6c77688":"The most promising attribute to predict the median house value is the median income as they have the highest correlation, so we can focus on their correlation.","c057271b":"The transformer has one hyperparameter, add_bedrooms_per_room, set to True by default. This hyperparameter will allow us to easily find out whether adding this attribute helps the ML algorithm or not. We can add as many hyperparameters as we want when we aren't sure about a good combination of their values.","ec263873":"- The new bedrooms_per_room attribute is much more correlated with the median house value than the toal number of rooms or bedrooms. \n- The number of rooms per household is also more informative than the total number of rooms in a district - obviously larger houses are more expensive.","2bef8559":"To get the corresponding feature name next to its importance value, we can do the following.","a7765394":"### Data Analysis\n","4e9c06c5":"We can get the list of categories using the categories_ instance variable which is a list of categories for each categorical attribute (in our case just for ocean_proximity).","5908de8f":"### Select and train a model\nWe can start building our model on the training set and evaluating it on the training set itself. Let's first train a Linear Regression model.","2bc5f1cd":"A prediction error of \\\\$68,628 is not very great. It is an example of model underfitting.\n<br>\nIn case of underfitting we can try the following:\n- Try a more powerful model.\n- Feed the model with better features.\n- reduce the constraints on the model (if the model is regularized).\n\nWe can try a more complex model such as DecisionTree and let's see how it performs.","7f848c1e":"### Evaluation on the test set\nNow we can evaluate how our model performs on the test set. What we need to do is:\n- get the predictors and labels from our test set\n- apply the full_pipeline to transform the test data (call transform() and not fit_transform()!)\n- predict the values of target attribute\n- measure the performance\n","8d9dcf39":"Random forest has performed better than linear regression and decision tree. However, the score on the training set is still lower than that on the validation sets, meaning that the model is still overfitting the training set.\n\nSeveral approaches can be tried to handle overfitting, however before doing that, it is often a good idea to try some other models. The goal should be to shortlist a few promising models.","7c273042":"- Here each row represents a district.\n- There are 10 attributes.\n- Target attribute is median_house_value.\n<br>\n","0e99ad7f":"If a categorical attribute has a large number of cateogires, then one-hot encoding will result in a large number of input features that may slow down training and badly affect the performance. In such cases, it is often useful to replace the categorical features with some numeric attributes related to the categories.","465f6d4f":"#### Feature scaling\nThere are two ways to scale the attributes: min-max scaling (normalization) and standardization.\n- Min-max scaling: values are shifted and rescaled such that they happen to be in the range of 0 and 1. It is done by subtracting min value and dividing by max minus min value.\n- Standardization: first it subtracts the mean, and then divides by the standard deviation, resulting distribution has zero mean and unit variance. It does not bound the values to a specific range, that might be a problem for some algorithms like neural networks that expect the input ranging from 0 to 1. However, standardization is much less affected by outliers.\n<br>\nScikit-learn provides us with MinMaxScaler and StandardScaler.\n\n##### Note: It is important to fit the scalers to training data only, not to the full dataset or test dataset. Only then you can use them to transform the training set and test set or new data.\n\n#### Transformation Pipelines\nThe various transformations need to be executed in right order. Scikit-learn provides the Pipeline class to help with such sequences of transformations.","5d965e47":"Now we can remove the income_cat attribute from housing so that the data is back in its original form.","7a90f313":"### More data exploration to gain insights\nWe can create a copy of the training set so that we can explore it without worrying about harming the original data.\n","29f36eda":"We kept the test_size = 0.2, n_splits = 1 and random_state = 42. The split() method returns two numpy arrays, with the indices of train and test samples respectively.\nWe used a for loop to assign strat_train_set and strat_test_set as training and test dataset respectively by .loc[index] using the indices returned by split()","356b67db":"Some important points about pearson correlation coefficient:\n- The correlation coefficient ranges from -1 (strong negative correlation) to 1 (strong positive correlation).\n- When correlation = 0, it means there is no linear correlation between the attributes.\n- It only captures linear correlation and may completely miss out on non-linear relationships.","27545082":"We can look at the correlation of our target variable median_house_value with every other attribute as follows.","6d2d1e0a":"- There are 20640 instances in the dataset.\n- total_bedrooms attribute has only 20433 non-null values, which means that 207 instances have a missing value for this attribute. We need to take care of these missing\/null values before feeding the data to the machine learning algorithm.\n- ocean_proximity is a categorical variable while all othere are numeric type. We can use value_counts() to get unique values\/categories and their count for ocean_proximity attribute.","68518f7a":"#### Sampling bias and Stratified sampling\nIn the above case, we have considered purely random sampling methods which is generally fine if our dataset is large enough (relative to the number of attributes). But if it is not, we run the risk of introducing <u> Sampling bias <\/u>.\n<br>\n*Problem with purely random sampling* - skewed test set would result in biased predictions.\n<br><br>\nWe can use *stratified sampling*: the population is divided into homogeneous subgroups called *strata*, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population.\n<br><br>\nAssume that median_income is a very important attribute to predict the median housing prices. We then want to make sure that the test set is representative of various categories of incomes in the whole dataset, and is not just chosen randomly.\n<br>\nSince median_income is numerical, we will convert it into categorical attribute.","ec7a4d79":"##### All but the last estimator must be transformers. Meaning that the last one may or may not be a transformer, but all others must be transformers.\nWhen fit() method is called on the pipeline, it calls fit_transform sequentially on all transformers, passing output of each call as the parameter to the next call, until it reaches the final estimator for which it only calls fit() method.\n\n#### Handling numeric and categorical attributes together\nIt is more convenient if we can have a single transformer that can handle all columns, applying appropriate transformations to each column. Scikit-Learn has introduced the columnTransformer for this purpose.","e8f8c0f3":"#### Where have we come so far:\n- We identified a few data quirks that need to be cleaned before model training.\n- We found some interesting correlations between the attributes.\n- We also noticed that some attributes have tail-heavy distributions, so we will need to transform them.\n\n#### What can still be done before training?\n- We can try out different attribute combinations. \n- For example, total_rooms or total_bedrooms in a district is not very useful if we don't know how many households are there. What we really need is number of rooms per household.\n- We can also make population per household as another interesting attribute.","d29e48eb":"### Create test set\n#### Data snooping bias\nOur brain is amazing at detecting patterns, highly prone to overfitting. If we look at the test set, we may find an interesting pattern that can influence our decision on the choice of learning algorithm. It usually doesn't lead to a generalized model and is prone to what we call data snooping bias.\n<br>\n#### Random train-test split\nTo prevent this bias, it is always a good idea to randomly split the data into training and test set and keep the test set aside, while doing observations, analyses and training on the train set.\n<br>\nTo create the test set, pick some instances randomly, which is typically 20% of the dataset (or less if the dataset is too large which is not the case here).\n<br><br>\nScikit-learn provides a handy way to split the data into train-test sets using train_test_split.","020b96ab":"This trained imputer can be used to transform the training set by replacing the missing values by the learned medians.","5cd50130":"No error at all! It is much likely that the decision tree has badly overfit the data. How can we be so sure without trying the model on new data?\nWe don't want to touch our test set until we are confident of our model's performance on the training set. So we need to use a part of the training set for training, and part for model validation.\n\n#### Cross-Validation for better evaluation\nWe have two options:\n- split the training set using train_test_split into training and validation sets.\n- use Scikit-Learn's K-fold cross-validation feature. It randomly splits the training set into K distinct subsets called folds, then it trains and evaluates the model K times, picking a different fold for evaluation each time and using other K-1 folds for training. The result is an array containing K evaluation scores.","ae9bee1a":"#### One-hot encoding\n*Important*:\nOrdinal encoding assumes sequential ranking among the different categories and it makes more sense if our attribute has values like 'bad', 'average', 'good', 'excellent'. But it is not the case for ocean_proximity.\n<br><br>\nIn such cases, a common way is to create a binary attribute per category. This is called one-hot encoding because only one attribute will be equal to 1 (hot) and the others will be 0 (cold).\n<br>\nThe new variables are called dummy attributes.","d90efd6f":"Few things to notice from these histograms:\n- The median income attribute does not look like it's expressed in USD. The data has been scaled and capped at 15 (actualy 15.0001 when you check max value in housing.describe()) for higher median incomes, and at 0.4999 for lower median incomes. The numbers represent roughly tens of thousands of dollars e.g. 3 actually means about $30,000.\n- The housing median age and the median house value were also capped. We need to pay special attention to median_house_value as it is our target variable and we don't want our machine learning models to produce outputs beyond the specific range.\n- All the attributes have very different scales. We will need to do feature scaling.\n- Many histograms are *tail heavy*, they extend much farther to the right of the median than to the left. We will try to transform these attributes later on to have more bell-shaped distributions.","2165ef3e":"So we were right! Decision tree seems to be overfitting so badly that it performs worse than linear regression.\n<br>\nWe can try RandomForestRegressor and see how it performs.","ad568fce":"#### Randomized Search\nGrid search is preferred when the number of combinations of hyperparameters to be tried out is generally small. But if the hyperparameter search space is large, it is often a good idea to use RandomizedSearchCV instead.\n- It tries a random value for each hyperparameter at every iteration. For example, if you set the number of iterations to 1000, this approach explores 1000 different values for each hyperparameter.\n- We have more control over the computing budget as we can adjust the number of iterations.","f4d3551a":"### Analysing the best models and their errors\nAfter shortlisting the best models, we can inspect them further to get more information, such as the importance of features in making predictions.\n<br>\nAs RandomForestRegressor has given the best performance, we can use its best estimator returned by the grid search to look for feature importance as shown below.","b314b14e":"*Note*: The null values are ignored (for example, count of total_bedrooms is 20433 and not 20640).\n<br><br>\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances that have a given value range.","59ce93b7":"#### Visualizing geographical data\nAs latitude and longitude represent geographical information, we can create a scatterplot of all districts to visualize the data.","ba1080ef":"While looking at the mean error, we notice that decision tree performs even worse than linear regression model. The decision tree has a score of approx 71084, generally with std of 2621.\nWe can compute the same scores for linear regression just to be sure.","425d471a":"From this plot, we see that:\n- The correlation is very strong, we can clearly see an upward trend.\n- The price cap is clearly visible as a horizontal line at \\\\$500,000. Also reveals some other less obvious straight lines around \\\\$450,000, another around \\\\$350,000, perhaps one around \\\\$280,000. We may need to remove the corresponding districts to prevent our algorithms from learning to reproduce these data quirks.","8bfd0f1a":"##### We can clearly see that all the categories of income_cat attribute have the SAME DISTRIBUTION in the test set as that in the original dataset.\nFor more information on stratified sampling, refer to https:\/\/medium.com\/@arshaikh5775\/stratified-sampling-62fcab68a052\n\n##### Comparing stratified sampling with random sampling in terms of sampling bias:\n","15bf3bc9":"Look at the correlation matrix again:","64603f49":"##### Note: Sklearn's cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so scoring function is actually the opposite of the MSE (i.e. negative value), which is why our code computes -scores before calculating the rmse.\n\nDisplay the results:","ec077048":"The describe() method shows a summary for numerical attributes.","a49ae365":"Most of the median incomes are clustered around 1.5 to 6. We can use pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5, category 2 from 1.5 to 3 and so on.\n<br>\n<br>\npandas.cut() - Use *cut* when you need to segment and sort data values into bins. It is useful for going from a continuous variable to a categorical variable. For example, ages to group of age ranges.","332951bb":"The above plot tells us that the housing prices are very much related to the location (e.g. close to the ocean) and to the population density.","0c8b88f5":"We can further tweak the above plot to display more information as follows.\n- The radius of each circle represents the district's population (option s).\n- The color represents the price (option c).\n- We are using a predefined colormap 'jet' which ranges from blue (small values) to red (large values)","5b344874":"### Data Preparation\nCopy the strat_train_set and separate the predictors and labels. We don't want to apply the same transformations to the predictors and target variable.","2123e5cd":"We have compared the income_cat proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated with purely random sampling. As we can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.","c2a9f652":"Evaluate the performance of the LinearRegression model on the training set. We are using Mean squared error from Scikit-Learn's metrics module and then use sqrt() to calculate root mean squared error.","163b9a6f":"Most machine learning algorithms prefer to work with numbers, so we can convert these categories from text to numbers. For this we can use Scikit-learn's OrdinalEncoder class.","589a6e2e":"This looks like California, but other than that it is difficult to see any pattern. Setting the alpha=0.1 makes it much easier to visualize the places where there is a high density of data points.","c2c551bb":"Now as the income has been converted into categories, we are ready to do stratified sampling based on the income category.\n<br>\nFor this, we can use Scikit-learn's StratifiedShuffleSplit class.","5256e9ee":"### Fine-tuning the hyperparameters\nAfter shortlisting the promising models, we need to fine-tune them, that involves making small adjustments such as changing values of hyperparameters to improve the performance of the models.\n<br>\nOne conventional approach is to manually keep changing the hyperparameters until we find a great combination of hyperparameter values. This is a very tedious task though.\n\n#### Grid Search\nWe can use Sklearn's GridSearchCV to do the search for us. We can pass the hyperparameters we want to experiment with and the values we want to try, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation.","2b07b5be":"The imputer computes the median of each attribute and store the results in its stastics_ instance variable.","99b595af":"#### Ensemble methods\nAnother way to fine-tune our system is to try to combine the models that perform the best. The group of models is called 'ensemble' that usually performs better than the best individual models.","c4f54c46":"#### Data cleaning\nWe noticed earlier that total_bedrooms column has missing values, so let's fix this. We can either drop the rows with missing values or fill them with some values (such as mean, median, zero).\n<br>\nWe can set the missing values to median value. For that we need to compute the median value on the training set, use it to fill the missing values in the training set, and save the median value to use it later to replace missing values in the test set or new data.\n<br><br>\nWe can use Scikit-Learn's SimpleImputer class to handle missing values.","36584d7d":"### Introduction\nI have been learning about machine learning from quite a sometime now but has never been confident enough about my skills and knowledge. So, I decided to learn it from a practical perspective along-with having a reasonable intuition behind what's really going on. I came across this book by O'Reilly called [Hands-on machine learning with Scikit-Learn, Keras and Tensorflow](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=asc_df_1492032646\/?tag=hyprod-20&linkCode=df0&hvadid=385599638286&hvpos=&hvnetw=g&hvrand=2497627214649937380&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9005550&hvtargid=pla-523968811896&psc=1&tag=&ref=&adgrpid=79288120515&hvpone=&hvptwo=&hvadid=385599638286&hvpos=&hvnetw=g&hvrand=2497627214649937380&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9005550&hvtargid=pla-523968811896). After going through its content, I really felt this is going to help me gain a hands-on experience of machine learning. The book really provides you the required theory behind the algorithms along with codes and explanations working on real datasets! You can literally follow along coding while reading the book. \n<br>\n<br>\nThe book is divided into two parts basically - first part deals with supervised and unsupervised learning while the second part focuses on deep learning.\n\nThis notebook walks you through the content of Chapter-2 that deals with building an end-to-end machine learning project. I think it is a good idea to code along reading, grasping the concepts, googling anything that is not familiar, and taking notes in this notebook itself. I really find it helpful to learn machine learning this way when you are just starting out and don't know where to start! ","fd50b74a":"Now we can fit the imputer instance to the training data using the fit() method:","3544e947":"Here is what we have done:\n- imported the ColumnTransformer class\n- get the list of numerical column names and the list of categorical column name\n- then we construct the ColumnTransformer whose constructor takes a list of tuples: where each tuple contains a name, a transformer (that can be another pipeline), and a list of names (or indices) of columns that the transformer should be applied to.\n- finally apply the ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis ( the transformers should return the same number of rows).","aca38b63":"#### Pandas' scatter_matrix method\nIt is another way to check for correlation between each pair of numeric attributes by plotting them against each other.","61d6bed2":"#### Custom Tranformers\nThough Scikit-learn provides us many useful transformers, sometimes we may need to create our own to do some specific task. We want our transformers to work seamlessly with Scikit-learn functionalities such as pipelines (which will be discussed shortly - just a way to apply transformations in given sequence). \n<br><br>\nSince Scikit-learn relies on duck typing (don't know what it is? it is just a concept used in dynamically typed language such as python), all we need is to create a class for transformation that we want to do, and implement three methods: fit() - returning self, transform(), and fit_transform(). We can get the last one using TransformerMixin as a base class.\n<br>\nIf we add BaseEstimator as a base class, we will get two extra methods get_params() and set_params() that can be used for automatic hyperparameter tuning.\n<br><br>\nAs an example, we have created a transformer class below to add combined attributes as discussed earlier.","6f9244c3":"#### Ordinal encoding \nNot for our categorical variable though! We discuss it in just a minute.","ed2e7994":"#### Looking for correlations\nWe can compute the correlation coefficient (pearson's r) between every pair of attributes using corr() function.\n<br><br>\n#### Pandas.df.corr() method\nDataFrame.corr(method='pearson', min_periods=1) <br>\nCompute pairwise correlation of columns, excluding NA\/null values."}}