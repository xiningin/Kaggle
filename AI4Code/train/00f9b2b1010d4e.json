{"cell_type":{"7b7102fc":"code","53ede1f3":"code","7745149c":"code","c422be37":"code","d3ef5320":"code","5e96b6a6":"code","4b02146e":"code","9bd4faa6":"code","2fe39612":"code","425f1471":"code","4529f091":"code","93f1caaf":"code","b860a0b1":"code","937c60d4":"code","0b350fc2":"code","45d72929":"code","3e5da291":"code","504bd8ee":"code","c0a9189e":"code","aa474b2d":"code","45006799":"code","a928f970":"code","5ca1514e":"code","6accdd64":"code","7f127240":"code","6594a0a8":"code","a32c6846":"code","fc53d4ea":"code","7e9ba534":"code","e4ee2256":"code","6b94aa69":"code","26f80ba4":"code","5d229969":"code","9064c255":"code","b8493877":"code","e4898cc9":"code","908558f3":"code","cec98f28":"code","466b73fe":"code","a52c927b":"code","2c5141ca":"code","d8d2c986":"code","4fec9ea0":"code","5ab3a25a":"code","fb68c37f":"code","56e62816":"code","e0bac679":"code","056cad53":"code","002c8ec2":"code","3d555557":"code","7646d9a3":"code","9881509a":"code","49e9d2dc":"code","b0dd6e15":"code","074aefbe":"code","dff4454e":"code","ba61e298":"code","40305922":"code","ad5684eb":"code","1aadd444":"code","5f81773a":"code","8c072ca4":"code","cc237d0c":"code","a4590020":"code","019dd5af":"code","ed8397df":"code","66a8f6e0":"code","319af680":"code","73051a8e":"code","561230c5":"code","ad558c6e":"code","5638010c":"code","a9fb5a61":"code","1a7765f6":"code","77c9e5d1":"code","aa9fea98":"code","3140b862":"code","aa387058":"code","36aa9e73":"code","4f59867c":"code","2ead8773":"code","4fac63e3":"code","15968b9f":"code","66b86e47":"code","949fa2cf":"code","e5c023d8":"code","0e97a28e":"code","096e9422":"code","6627a161":"code","f7f5e159":"code","7020cf7c":"code","97135566":"code","53bfb40d":"code","74d2008f":"code","ef51522d":"code","6aaceff0":"code","d34393b8":"code","e320a916":"code","ad410f5f":"code","52c971b5":"code","297e807e":"code","5932e48f":"code","ffb198f9":"code","8b48ac71":"code","e92df136":"code","3427453c":"code","3f37d43f":"code","344e3140":"code","9bd9414f":"code","da38af28":"code","edd50bf8":"code","48727fe6":"code","3a04c2c3":"code","badbd9b0":"code","c2d64701":"code","d228e933":"code","32caffc7":"code","4071f43a":"code","4af7cdbe":"code","b2725fc1":"code","24e611b8":"code","4ae3eedd":"code","61319418":"code","2d114569":"code","0991e985":"code","02271aa5":"code","34f8cf49":"code","3f04c19c":"code","0619c35f":"code","678a8708":"code","b92d7300":"code","a3c5e89c":"code","26e3d0d3":"code","5a4ec3a4":"markdown","6c065973":"markdown","f9ab5067":"markdown","4ffde4a0":"markdown","525c8827":"markdown","37045a3f":"markdown","a6b544d9":"markdown","7a5249b8":"markdown","7c74bdc8":"markdown","a2d4353e":"markdown","e8c8f19b":"markdown","adf1a44f":"markdown","1541d0ec":"markdown","a990786c":"markdown","20205f2d":"markdown","af0f12a2":"markdown","5919ac7d":"markdown","c9b89b4c":"markdown","71861a76":"markdown","fbfeb1eb":"markdown","9b6fdc47":"markdown","ace295d4":"markdown","485bba06":"markdown","6d5c2f48":"markdown","8d23d20e":"markdown","965a20f9":"markdown","58e5352e":"markdown","727d3be3":"markdown","da966b0b":"markdown","68dd4d4d":"markdown","be22b67b":"markdown","2b357d01":"markdown","4ed5ef06":"markdown","2d45a2fb":"markdown","b4353045":"markdown","18958555":"markdown","f1b133db":"markdown","c40480bf":"markdown","0c9d0742":"markdown","c120f4d2":"markdown","93f0cb17":"markdown","f999954b":"markdown","febfe4cd":"markdown","32b127bd":"markdown","2ed231fd":"markdown","421eac06":"markdown","15b8e28b":"markdown","8c041f31":"markdown","51ec96ad":"markdown","1f79bb22":"markdown","6c71a913":"markdown","df4b77f9":"markdown","463a6f9b":"markdown","1d85e428":"markdown","e589eb25":"markdown","4a0acb23":"markdown","d462a3a2":"markdown"},"source":{"7b7102fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53ede1f3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom glob import glob\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","7745149c":"# Lets load all the datasets:\n\ncustomers_df = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv\")\ngeolocation_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\norders_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\norder_items_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\norder_payments_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv')\norder_reviews_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\nproducts_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\nsellers_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv')\ncategory_transalations_df = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv')","c422be37":"# Lets check the size of each df:\ndf_names = ['customers_df','geolocation_df', 'orders_df', 'order_items_df','order_payments_df',\n            'order_reviews_df','products_df','sellers_df','category_transalations_df']\nfor df in df_names:\n    print(\"Dataset {} has shape {}\".format(df, eval(df).shape))","d3ef5320":"customers_df.head()","5e96b6a6":"geolocation_df.head()","4b02146e":"orders_df.head()","9bd4faa6":"order_items_df.head()","2fe39612":"order_payments_df.head()","425f1471":"order_reviews_df.head()","4529f091":"products_df.head()","93f1caaf":"sellers_df.head()","b860a0b1":"category_transalations_df.head()","937c60d4":"# Lets check the null values:\n\nfor df in df_names:\n    print(\"In {} there are approximately {} null values\".format(df, eval(df).isnull().sum().sum()))","0b350fc2":"orders_df.head()","45d72929":"orders_df.isnull().sum()","3e5da291":"orders_df[\"order_approved_at\"] = orders_df[\"order_approved_at\"].fillna(orders_df[\"order_purchase_timestamp\"])\norders_df[\"order_delivered_carrier_date\"] = orders_df[\"order_delivered_carrier_date\"].fillna(orders_df[\"order_approved_at\"])\norders_df[\"order_delivered_customer_date\"] = orders_df[\"order_delivered_customer_date\"].fillna(orders_df[\"order_estimated_delivery_date\"])","504bd8ee":"orders_df.isnull().sum()","c0a9189e":"products_df.isnull().sum()","aa474b2d":"products_df=products_df.rename(columns={'product_name_lenght':'product_name_len','product_description_lenght':'product_description_len'})","45006799":"products_df.isnull().sum()","a928f970":"products_df[products_df['product_weight_g'].isnull()]","5ca1514e":"products_df = products_df[~(products_df['product_weight_g'].isnull())]","6accdd64":"products_df[products_df['product_name_len'].isnull()].head()","7f127240":"products_df = products_df[~(products_df['product_name_len'].isnull())]","6594a0a8":"products_df.isnull().sum()","a32c6846":"order_reviews_df.isnull().sum()","fc53d4ea":"# let drop the review_comment_title column\norder_reviews_df = order_reviews_df.drop(['review_comment_title'],axis =1 )","7e9ba534":"#Lets replace missing review messages with string 'NONE'\norder_reviews_df['review_comment_message'] = order_reviews_df['review_comment_message'].fillna('NONE')","e4ee2256":"order_reviews_df.isnull().sum()","6b94aa69":"# Lets merge all the datasets and then analyse null values and clean \n\ndf = pd.merge(orders_df,order_payments_df, on=\"order_id\")\ndf = pd.merge(df,customers_df, on=\"customer_id\")\ndf = pd.merge(df,order_items_df, on=\"order_id\")\ndf = pd.merge(df,sellers_df, on=\"seller_id\")\ndf = pd.merge(df,order_reviews_df, on=\"order_id\")\ndf = pd.merge(df,products_df, on=\"product_id\")\n#df = pd.merge(df,geolocation_df, left_on=\"\" right_on=\"geolocation_zip_code_prefix\")\ndf = pd.merge(df,category_transalations_df, on=\"product_category_name\")\n\ndf.shape","26f80ba4":"df.head()","5d229969":"df.isnull().sum()","9064c255":"df.info()","b8493877":"print(\"Number of unique categories: \", len(products_df.product_category_name.unique()))","e4898cc9":"plt.figure(figsize=(10,6))\ntop_25_prod_categories = products_df.groupby('product_category_name')['product_id'].count().sort_values(ascending=False).head(25)\nsns.barplot(x=top_25_prod_categories.index, y=top_25_prod_categories.values)\nplt.xticks(rotation=80)\nplt.xlabel('Product Category')\nplt.title('Top 25 Most Common Categories');\nplt.show()","908558f3":"order_items_df.head()","cec98f28":"# Lets check the top 5 most selling products\norder_items_df.product_id.value_counts().head()","466b73fe":"order_items_df.product_id.value_counts().head(1)","a52c927b":"bestseller_product = order_items_df.product_id.value_counts().head(1)\nbestseller_product_id = bestseller_product.index[0]\nbestseller_product_freq = bestseller_product[0]\nprint(\"Frequency of Top Seller Product: {}\".format(bestseller_product_freq))\nprod_cat_name_eng = df[['product_category_name_english']][df.product_id == bestseller_product_id].head(1).iloc[0][0]\nprint('Top Seller Product Category: ',prod_cat_name_eng)","2c5141ca":"plt.figure(figsize=(10,6))\ntop_10_cust_states = customers_df.groupby('customer_state')['customer_id'].count() \\\n.sort_values(ascending = False).head(10)\nsns.barplot(x=top_10_cust_states.index, y = top_10_cust_states.values)\nplt.title('Customer Frequency by State')\nplt.show()","d8d2c986":"plt.figure(figsize=(10,6))\ntop_10_seller_states = sellers_df.groupby('seller_state')['seller_id'].count() \\\n.sort_values(ascending = False).head(10)\nsns.barplot(x=top_10_seller_states.index, y = top_10_seller_states.values)\nplt.title('Seller Frequency by State')\nplt.show()","4fec9ea0":"order_items_df.head()","5ab3a25a":"order_items_df.seller_id.value_counts().head(10)","fb68c37f":"sellers_df.head()","56e62816":"plt.figure(figsize=(10,6))\ntop_25_sellers = order_items_df.groupby('seller_id')['order_item_id'].count().sort_values(ascending=False).head(25)\nsns.barplot(x=top_25_sellers.index, y=top_25_sellers.values)\nplt.xticks(rotation=80)\nplt.xlabel('Sellers')\nplt.title('Top 25 Most Common Sellers');\nplt.show()","e0bac679":"df['seller_state'][df['seller_id'].isin(top_25_sellers.index)].value_counts().head()","056cad53":"from datetime import datetime\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","002c8ec2":"df.head()","3d555557":"# Remove duplicate entries\ndf= df.drop_duplicates(subset={'order_id','customer_id','order_purchase_timestamp','order_delivered_customer_date'}, keep='first')\ndf=df.reindex()\ndf.head()","7646d9a3":"df['total_payment'] = df['payment_value'] * df['payment_installments']","9881509a":"# monetary\ngrouped_df = df.groupby('customer_unique_id')['total_payment'].sum()\ngrouped_df = grouped_df.reset_index()\ngrouped_df.columns = ['customer_unique_id', 'monetary']\ngrouped_df.head()","49e9d2dc":"# frequency\nfrequency = df.groupby('customer_unique_id')['order_id'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['customer_unique_id', 'frequency']\nfrequency.head()","b0dd6e15":"# merge the two dfs\ngrouped_df = pd.merge(grouped_df, frequency, on='customer_unique_id', how='inner')\ngrouped_df.head()","074aefbe":"\ndf['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'], infer_datetime_format=True, errors='ignore')\n\nmax_date = max(df['order_purchase_timestamp'])\ndf['diff_days'] =  (max_date-df['order_purchase_timestamp']).dt.days","dff4454e":"# Recency\nrecency = df.groupby('customer_unique_id')['diff_days'].min()\nrecency = recency.reset_index()\nrecency.columns = ['customer_unique_id', 'recency']\nrecency.head()","ba61e298":"# merge the grouped_df to recency df\nrfm_df = pd.merge(grouped_df, recency, on='customer_unique_id', how='inner')\nrfm_df.head()","40305922":"# Plot RFM distributions\nplt.figure(figsize=(12,10))\n# Plot distribution of R\nplt.subplot(3, 1, 1); sns.distplot(rfm_df['recency'])\n# Plot distribution of F\nplt.subplot(3, 1, 2); sns.distplot(rfm_df['frequency'])\n# Plot distribution of M\nplt.subplot(3, 1, 3); sns.distplot(rfm_df['monetary'])\n# Show the plot\nplt.show()","ad5684eb":"sns.boxplot(rfm_df['recency'])","1aadd444":"sns.boxplot(rfm_df['frequency'])","5f81773a":"sns.boxplot(rfm_df['monetary'])","8c072ca4":"# removing (statistical) outliers for monetary\nQ1 = rfm_df.monetary.quantile(0.05)\nQ3 = rfm_df.monetary.quantile(0.95)\nIQR = Q3 - Q1\nrfm_df = rfm_df[(rfm_df.monetary >= Q1 - 1.5*IQR) & (rfm_df.monetary <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = rfm_df.frequency.quantile(0.05)\nQ3 = rfm_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm_df = rfm_df[(rfm_df.frequency >= Q1 - 1.5*IQR) & (rfm_df.frequency <= Q3 + 1.5*IQR)]","cc237d0c":"sns.boxplot(rfm_df['monetary'])","a4590020":"sns.boxplot(rfm_df['frequency'])","019dd5af":"rfm_df_scaled = rfm_df[['monetary', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df_scaled)\nrfm_df_scaled.shape","ed8397df":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['monetary', 'frequency', 'recency']\nrfm_df_scaled.head()","66a8f6e0":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","319af680":"kmeans.labels_","73051a8e":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd)","561230c5":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","ad558c6e":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)\nkmeans.labels_","5638010c":"# assign the label\nrfm_df['cluster_id'] = kmeans.labels_\nrfm_df.head()","a9fb5a61":"# plot\nsns.boxplot(x='cluster_id', y='monetary', data=rfm_df)","1a7765f6":"# plot\nsns.boxplot(x='cluster_id', y='frequency', data=rfm_df)","77c9e5d1":"# plot\nsns.boxplot(x='cluster_id', y='recency', data=rfm_df)","aa9fea98":"df.head()","3140b862":"# frequency\nfrequency = df.groupby('seller_id')['order_item_id'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['seller_id', 'frequency']\nfrequency.head()","aa387058":"# monetary\nmonetary = df.groupby('seller_id')['total_payment'].sum()\nmonetary = monetary.reset_index()\nmonetary.columns = ['seller_id', 'monetary']\nmonetary.head()","36aa9e73":"# monetary\nrecency = df.groupby('seller_id')['diff_days'].min()\nrecency = recency.reset_index()\nrecency.columns = ['seller_id', 'recency']\nrecency.head()","4f59867c":"rfm_seller_df = pd.merge(frequency, monetary, on='seller_id', how='inner')\nrfm_seller_df = pd.merge(rfm_seller_df, recency, on='seller_id', how='inner')\nrfm_seller_df.head()","2ead8773":"# Plot RFM distributions\nplt.figure(figsize=(12,10))\n# Plot distribution of R\nplt.subplot(3, 1, 1); sns.distplot(rfm_seller_df['recency'])\n# Plot distribution of F\nplt.subplot(3, 1, 2); sns.distplot(rfm_seller_df['frequency'])\n# Plot distribution of M\nplt.subplot(3, 1, 3); sns.distplot(rfm_seller_df['monetary'])\n# Show the plot\nplt.show()","4fac63e3":"sns.boxplot(rfm_seller_df['recency'])","15968b9f":"sns.boxplot(rfm_seller_df['frequency'])","66b86e47":"sns.boxplot(rfm_seller_df['monetary'])","949fa2cf":"# removing (statistical) outliers for monetary\nQ1 = rfm_seller_df.monetary.quantile(0.05)\nQ3 = rfm_seller_df.monetary.quantile(0.95)\nIQR = Q3 - Q1\nrfm_seller_df = rfm_seller_df[(rfm_seller_df.monetary >= Q1 - 1.5*IQR) & (rfm_seller_df.monetary <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = rfm_seller_df.frequency.quantile(0.05)\nQ3 = rfm_seller_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm_seller_df = rfm_seller_df[(rfm_seller_df.frequency >= Q1 - 1.5*IQR) & (rfm_seller_df.frequency <= Q3 + 1.5*IQR)]\n\n# outlier treatment for recency\nQ1 = rfm_seller_df.recency.quantile(0.05)\nQ3 = rfm_seller_df.recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm_seller_df = rfm_seller_df[(rfm_seller_df.recency >= Q1 - 1.5*IQR) & (rfm_seller_df.recency <= Q3 + 1.5*IQR)]","e5c023d8":"sns.boxplot(rfm_seller_df['recency'])","0e97a28e":"sns.boxplot(rfm_seller_df['frequency'])","096e9422":"sns.boxplot(rfm_seller_df['monetary'])","6627a161":"rfm_seller_df_scaled = rfm_seller_df[['monetary', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_seller_df_scaled = scaler.fit_transform(rfm_seller_df_scaled)\nrfm_seller_df_scaled.shape","f7f5e159":"rfm_seller_df_scaled = pd.DataFrame(rfm_seller_df_scaled)\nrfm_seller_df_scaled.columns = ['monetary', 'frequency', 'recency']\nrfm_seller_df_scaled.head()","7020cf7c":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_seller_df_scaled)\nkmeans.labels_","97135566":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_seller_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd)","53bfb40d":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=100)\n    kmeans.fit(rfm_seller_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_seller_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","74d2008f":"# final model with k=2\nkmeans = KMeans(n_clusters=2, max_iter=50)\nkmeans.fit(rfm_seller_df_scaled)\nkmeans.labels_","ef51522d":"# assign the label\nrfm_seller_df['cluster_id'] = kmeans.labels_\nrfm_seller_df.head()","6aaceff0":"# plot\nsns.boxplot(x='cluster_id', y='monetary', data=rfm_seller_df)","d34393b8":"# plot\nsns.boxplot(x='cluster_id', y='recency', data=rfm_seller_df)","e320a916":"# plot\nsns.boxplot(x='cluster_id', y='frequency', data=rfm_seller_df)","ad410f5f":"order_reviews_df.head()","52c971b5":"len(order_reviews_df)","297e807e":"reviews_df = order_reviews_df[~(order_reviews_df['review_comment_message'] == 'NONE')]","5932e48f":"reviews_df.head()","ffb198f9":"reviews_df = reviews_df[['review_score','review_comment_message']]","8b48ac71":"reviews_df.head()","e92df136":"!python -m spacy download pt_core_news_sm","3427453c":"import re, nltk, spacy, string\nimport pt_core_news_sm\nnlp = pt_core_news_sm.load()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","3f37d43f":"def clean_text(text):\n    text = text.lower()  # Make the text lowercase\n    text = re.sub('\\[.*\\]','', text).strip() # Remove text in square brackets if any\n    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # Remove words containing numbers\n    return text.strip()","344e3140":"reviews_df.review_comment_message = reviews_df.review_comment_message.apply(lambda x: clean_text(x))","9bd9414f":"# portugese stopwords\nstopwords = nlp.Defaults.stop_words\n\n# lemmatizer function\ndef lemmatizer(text):\n    doc = nlp(text)\n    sent = [token.lemma_ for token in doc if not token.text in set(stopwords)]\n    return ' '.join(sent)","da38af28":"reviews_df['lemma'] = reviews_df.review_comment_message.apply(lambda x: lemmatizer(x))","edd50bf8":"reviews_df.head(10)","48727fe6":"#Using a word cloud find the top 50 words by frequency among all the reviews\n\n#!pip install wordcloud\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(stopwords=stopwords,max_words=50).generate(str(reviews_df.lemma))\n\nprint(wordcloud)\nplt.figure(figsize=(10,6))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","3a04c2c3":"#top 30 bigram frequency among the reviews\n\ndef get_top_n_bigram(text, ngram=1, top=None):\n    vec = CountVectorizer(ngram_range=(ngram, ngram), stop_words=stopwords).fit(text)\n    bag_of_words = vec.transform(text)\n\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:top]","badbd9b0":"top_30_unigrams = get_top_n_bigram(reviews_df.lemma,ngram=1, top=30)\ntop_30_bigrams = get_top_n_bigram(reviews_df.lemma,ngram=2, top=30)\ntop_30_trigrams = get_top_n_bigram(reviews_df.lemma,ngram=3, top=30)","c2d64701":"df1 = pd.DataFrame(top_30_unigrams, columns = ['unigram' , 'count'])\nplt.figure(figsize=(12,6))\nfig = sns.barplot(x=df1['unigram'], y=df1['count'])\nplt.xticks(rotation = 80)\nplt.show()","d228e933":"df2 = pd.DataFrame(top_30_bigrams, columns = ['bigram' , 'count'])\nplt.figure(figsize=(12,6))\nfig = sns.barplot(x=df2['bigram'], y=df2['count'])\nplt.xticks(rotation = 80)\nplt.show()","32caffc7":"df3 = pd.DataFrame(top_30_trigrams, columns = ['trigram' , 'count'])\nplt.figure(figsize=(12,6))\nfig = sns.barplot(x=df3['trigram'], y=df3['count'])\nplt.xticks(rotation = 80)\nplt.show()","4071f43a":"tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words=stopwords)\ndtm = tfidf.fit_transform(reviews_df.lemma)","4af7cdbe":"tfidf.get_feature_names()[:10]","b2725fc1":"len(tfidf.get_feature_names())","24e611b8":"reviews_df.review_score.value_counts()","4ae3eedd":"from sklearn.decomposition import NMF\n\n#Load nmf_model with the n_components \nnum_topics =  5 \n\n#keep the random_state =40\nnmf_model = NMF(n_components=num_topics, random_state=40)\n\nW1 = nmf_model.fit_transform(dtm)\nH1 = nmf_model.components_","61319418":"colnames = [\"Topic\" + str(i) for i in range(nmf_model.n_components)]\ndocnames = [\"Doc\" + str(i) for i in range(len(reviews_df.lemma))]\ndf_doc_topic = pd.DataFrame(np.round(W1, 2), columns=colnames, index=docnames)\nsignificant_topic = np.argmax(df_doc_topic.values, axis=1)\ndf_doc_topic['dominant_topic'] = significant_topic\nreviews_df['topic'] = significant_topic","2d114569":"pd.set_option('display.max_colwidth', -1)\nreviews_df[['review_comment_message','lemma','review_score','topic']][reviews_df.topic==0].head(20)","0991e985":"temp = reviews_df[['review_comment_message','lemma','review_score','topic']].groupby('topic').head(20)\ntemp.sort_values('topic')","02271aa5":"!pip install google_trans_new","34f8cf49":"#  google translate from portuguese to english\nfrom google_trans_new import google_translator\ntranslator = google_translator()\n\ndef translate_pt_to_eng(sent):\n    translated_sent = translator.translate(sent,lang_tgt='en',lang_src='pt')\n    return translated_sent","3f04c19c":"!pip install --user googletrans","0619c35f":"from googletrans import Translator\ntranslator = Translator()\n","678a8708":"translator.translate('veritas lux mea', src='la', dest='en')","b92d7300":"reviews_df['lemma'].head()","a3c5e89c":"print(translate_pt_to_eng('receber prazo estipular'))","26e3d0d3":"reviews_df['lemma'] = reviews_df['lemma'].apply(lambda x : translate_pt_to_eng(x))","5a4ec3a4":"Lets only keep the review messages and the review score to see what the customers are liking and disliking ","6c065973":"No more missing values in products dataset.\nLets check the reviews dataset","f9ab5067":"### Sellers in cluster 2 are have a higher frequency\n\n#### Looking at the RFM analysis done for sellers, can be used to find characteristics of these better performing sellers in cluster 2 and this report can be published as a guiding principles for the new merchants onboarding on the Olist Store so that they can learn and grow faster.","4ffde4a0":"No useful information can be gathered with just dimension and product id. We need to know what that product is. Lets drop these 610 rows","525c8827":"## K Means Clustering with random K","37045a3f":"Lets check the rows where the product name is missing","a6b544d9":"### Description:\nOlist, a Brazilian e-commerce marketplace integrator, is an online e-commerce site aggregation platform designed to facilitate direct sales on e-commerce sites of Brazil. The company's platform connects entrepreneurs with major online retailers and allows shopkeepers to advertise and sell in the marketplaces without complication, enabling retail companies to reach out to the international marketplaces, improve the shopping experience and modify their purchasing behavior.\nOlist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners.\n\nAfter a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.\n","7a5249b8":"### Lets now do an RFM(Recency, Frequncy, Monetary) Analysis for Behavioural Segmentaion of Customers","7c74bdc8":"### Outlier treatment","a2d4353e":"There are 9 csv files","e8c8f19b":"### Silhouette analysis","adf1a44f":"### Cleaning the messages","1541d0ec":"### Using wordcloud to visualize the top 50 words by frequency","a990786c":"## Monetary","20205f2d":"### Lets do some Customer Sentiment Analysis using the review comments","af0f12a2":"### Unigrams, Bigram, Trigrams Frequency Analysis","5919ac7d":"Customers in cluster_id 2 are spending more than other customers","c9b89b4c":"### Finding the Optimal Number of Clusters","71861a76":"## Finding optimal Clusters\n### Elbow Curve method","fbfeb1eb":"Lets do some cleaning of the review messages","9b6fdc47":"## Top 25 Sellers by frequency of sales","ace295d4":"### Sellers in cluster 2 are earning more than other sellers","485bba06":"All missing values are handled.","6d5c2f48":"Customers in cluster_id 1 have made purchases more recently than others","8d23d20e":"## Lets do the RFM Analysis for the Sellers","965a20f9":"## Translating Portugese to English using Google Translator\n### Attempting to provide better names to the topic indexes thus obtained above","58e5352e":"## Using NMF for Topic modelling for the review messages","727d3be3":"### Dataset description:\n\nThe dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers.The geolocation dataset that relates Brazilian zip codes to lat\/lng coordinates.\n","da966b0b":"### Lets check the Customer Frequency by State","68dd4d4d":"### Lemmatizing the words","be22b67b":"We need real review messages so lets drop the NONE values","2b357d01":"### Modelling K-Means Clustering","4ed5ef06":"### Almost all the Sellers in cluster 2 have done business more recently","2d45a2fb":"Nothing much can be inferred about the frequency","b4353045":"### Data Schema\n\n![image.png](attachment:14c53b18-18b1-415c-ba88-34fb9df66497.png)","18958555":"Maximum number of products fall under these top 25 categories","f1b133db":"### Lets check for Outliers","c40480bf":"## Scaling","0c9d0742":"### Few Problems that can be solved using the above dataset:\n\n1. Sentiment Analysis using the customer reviews\n2. Customer segmentation\n3. Delivery date prediction using seller geolocation and customer geolocation\n4. Sales prediction during festivals\n5. Most ordered and least ordered products in a category\n6. Regionwise best and worst performing seller\n7. Best performing product category\n\n","c120f4d2":"Product name, description and category is missing for more than 600 products,\nthe size details of 2 products is missing we can impute dimension details with the median value or we can drop those 2 rows","93f0cb17":"Lets pick 5 has the n_components as the review_score is also of 5 point grades","f999954b":"### Lets check the Seller's Frequency by State","febfe4cd":"#### Inspiration\n\nHere are some inspiration for possible outcomes from this dataset.\n\n**NLP**:\n\nCustomer sentiment analysis using reviews\n\n**Clustering**:\n\nSome customers didn't write a review. Use Customer Segmentation to find out whether they happy or not.\n\n**Sales Prediction**:\n\nWith purchase date information we can predict future sales.\n\n**Delivery Performance**:\n\nDelivery performance Analysis and find ways to optimize delivery times.\n\n**Product Quality**:\n\nProducts categories that are more prone to customer insatisfaction.\n\n**Feature Engineering**:\n\nCreate features from this rich dataset.","32b127bd":"### Location Analysis\n\nLets check which from which state do most customers come from","2ed231fd":"Best Selling Product","421eac06":"#### Outlier detection","15b8e28b":"All the missing values are timestamp values, these values are \n\n1. approve timestamp\n2. carrier delivery timestamp\n3. customer delivery timestamp\n\nOrder approval follows the order purchase and usually happens within a short interval, hence missing approval timestamp can be substituted with the purchase timestamp\n\nAs per observation the carrier delivery timestamp is often same as the order approval date, so we can substitute the carrier delivery date with the approval date\n\nAlso it is seen that estimated delivery date is often later than the actual customer delivery date so we can easily substitute the missing customer delivery date with the estimated delivery date\n","8c041f31":"### Silhouette Analysis","51ec96ad":"## Products Analysis\n#### Top 25 Product catgeories","1f79bb22":"No missing values in orders dataset.\nNow lets check products datasets","6c71a913":"#### Description of various columns in different csv files\n\n`olist_customers_dataset.csv`\n\n![image.png](attachment:9b29b69a-5182-4608-b3ff-6b308ac28ca2.png)\n\n`olist_sellers_dataset.csv`\n\n![image.png](attachment:f244db69-f246-4f2d-86b5-76068839e04f.png)\n\n`olist_order_items_dataset.csv`\n\n![image.png](attachment:69074a44-7156-4c84-8c26-6a368ad0ae56.png)\n\n`olist_order_payments_dataset.csv`\n\n![image.png](attachment:6228eb26-5df8-4c9d-830a-745ec0e2b558.png)\n\n`olist_orders_dataset.csv`\n\n![image.png](attachment:f76b7bfe-77bf-48b0-a641-b133d3b5ad4f.png)\n\n`olist_order_reviews_dataset.csv`\n\n![image.png](attachment:0fdac81b-3ebe-4281-82c8-6c5513062ae6.png)\n\n`olist_products_dataset.csv`\n\n![image.png](attachment:c9b061d0-343f-4457-9819-05bd807fda1e.png)\n\n`olist_geolocation_dataset.csv`\n\n![image.png](attachment:22b8b109-f0df-44a8-a28f-d4ca97154a54.png)\n\n`product_category_name_translation.csv`\n\n![image.png](attachment:a91fdbf9-1b8a-4ec4-98d9-5d016a8edc79.png)","df4b77f9":"## TfidfVectorizer","463a6f9b":"### Monetary and Frequency have outliers","1d85e428":"So we see that orders, order_reviews and products datasets have missing values.\nLets take a closer look at them and see if we can treat the missing values","e589eb25":"#### Most of these top 25 sellers are from the state of SP","4a0acb23":"#### Scaling","d462a3a2":"We can use the same code above just replacing the groupby column"}}