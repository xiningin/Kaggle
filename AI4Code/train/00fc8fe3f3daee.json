{"cell_type":{"c6778546":"code","62e8d610":"code","1fde2bfb":"code","eb2c7a88":"code","1c0ee524":"code","801e80cf":"code","012db1fd":"code","667c2ffd":"code","d6e7eb43":"code","8e827e03":"code","04d26d7e":"code","3251044c":"code","69b3ad60":"code","34a0396c":"code","83405b69":"code","4b95a847":"code","60be9c91":"code","5a6cc986":"code","9c7fb838":"code","b65c9911":"code","a8264540":"code","1b8ee84d":"code","1206e8df":"code","61631efe":"code","f933f545":"code","8125e9b4":"code","e84df93e":"code","20fc4c52":"code","06b92ac3":"code","caa0c040":"code","367c41df":"code","e65cbacf":"code","fdcb576d":"code","fed79f26":"code","617a8f9e":"code","935f5bc4":"code","77aa3576":"code","b83601eb":"code","ced382ec":"code","afdad943":"code","021e85fa":"code","6983fc7b":"code","42691e0e":"code","85de4d43":"code","f91bf398":"code","71b23d50":"code","8b7cc63f":"code","a2f44d2f":"code","eb302ed5":"code","45248c6a":"code","0e76b4b3":"code","9afa6e59":"code","67568b9f":"code","8768f905":"code","c4adbaad":"code","d616ec10":"code","26ede1e1":"markdown","4f0bb020":"markdown","cc8f0aaf":"markdown","ee68e9ec":"markdown","fb0a83d6":"markdown","190f1ae0":"markdown","250d6d13":"markdown","ddd65a56":"markdown","19d78f52":"markdown","9f233c75":"markdown","ca3a8c7b":"markdown","6082c117":"markdown","1b90226f":"markdown","8476c794":"markdown","024c8a25":"markdown","243624a0":"markdown","0b20993b":"markdown","6ed58892":"markdown","02a3253c":"markdown","8198342f":"markdown","eee1d11a":"markdown","65bccaf1":"markdown","78ce51c3":"markdown","f0e1eacc":"markdown","224536ab":"markdown","655aa141":"markdown"},"source":{"c6778546":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport pyLDAvis\nimport pyLDAvis.sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom pprint import pprint\nfrom time import time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score,roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport keras\nprint('Done')","62e8d610":"os.listdir('\/kaggle\/input')","1fde2bfb":"dataset = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndataset.head()","eb2c7a88":"dataset.shape","1c0ee524":"dataset.isnull().sum()","801e80cf":"#Removing URLs with a regular expression\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\nfor i in range(len(dataset)):\n  dataset.at[i,'Comments Text'] = remove_urls(dataset.iloc[i]['Comments Text'])\ndataset.head()","012db1fd":"# Convert to list\ndata = dataset['Comments Text'].values.tolist()\n\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\nprint(data[:1])","667c2ffd":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","d6e7eb43":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","8e827e03":"# Define functions for stopwords, bigrams, trigrams and lemmatization\n\nstop_words = set(stopwords.words(\"english\"))\n\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","04d26d7e":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","3251044c":"dataset = []\nfor i in range(len(data_lemmatized)):\n    dataset.append(\" \".join(data_lemmatized[i]))\ndataset = pd.Series(dataset)","69b3ad60":"no_features = 15000\n\n# NMF is able to use tf-idf\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=no_features)\ntfidf = tfidf_vectorizer.fit_transform(dataset)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n\n# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\ntf_vectorizer = CountVectorizer(min_df=0.05,max_features=no_features)\ntf = tf_vectorizer.fit_transform(dataset)\ntf_feature_names = tf_vectorizer.get_feature_names()","34a0396c":"no_topics = 2\n\n# Run NMF\nnmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5,max_iter=10000).fit(tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=no_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(tf)","83405b69":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 25\nprint('NMF')\ndisplay_topics(nmf, tfidf_feature_names, no_top_words)\nprint('LDA')\ndisplay_topics(lda, tf_feature_names, no_top_words)","4b95a847":"# Create Document \u2014 Topic Matrix\nlda_output = lda.transform(tf)\n# column names\ntopicnames = ['Topic' + str(i) for i in range(lda.n_components)]\n# index names\ndocnames = ['Doc' + str(i) for i in range(len(dataset))]\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\ndf_document_topics = df_document_topic\ndataset2 = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndf_document_topics.reset_index(inplace=True,drop=True)\ndataset2['label'] = df_document_topics['dominant_topic']","60be9c91":"dataset2.head()","5a6cc986":"# Create Document \u2014 Topic Matrix\nnmf_output = nmf.transform(tfidf)\n# column names\ntopicnames = ['Topic' + str(i) for i in range(nmf.n_components)]\n# index names\ndocnames = ['Doc' + str(i) for i in range(len(dataset))]\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(nmf_output, 2), columns=topicnames, index=docnames)\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\ndf_document_topics = df_document_topic\ndataset1 = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndf_document_topics.reset_index(inplace=True,drop=True)\ndataset1['label'] = df_document_topics['dominant_topic']","9c7fb838":"dataset1.head()","b65c9911":"dataset1[dataset1['label']==1]","a8264540":"for i in range(20):\n    print(dataset1[dataset1['label']==1].iloc[i][0])\n    print('\\n')","1b8ee84d":"dataset2[dataset2['label']==1]","1206e8df":"for i in range(20):\n    print(dataset2[dataset2['label']==1].iloc[i][0])\n    print('\\n')","61631efe":"dataset1.head(15)","f933f545":"for i in range(len(dataset2)):\n  dataset1.at[i,'Comments Text'] = remove_urls(dataset1.iloc[i]['Comments Text'])\ndataset1.head()","8125e9b4":"# Convert to list\ndata = dataset1['Comments Text'].values.tolist()\n\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\n# Remove distracting commas\ndata = [re.sub(\",\", \"\", sent) for sent in data]\n\n# Remove distracting commas\ndata = [sent.lower() for sent in data]\n\n# Remove distracting dots\ndata = [sent.replace('.', '') for sent in data]\n\nprint(data[:1])","e84df93e":"tweets = np.array(data)\nlabels = np.array(dataset2['label'])","20fc4c52":"print(len(tweets),len(labels))","06b92ac3":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nmax_words = 20000\nmax_len = 400\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(tweets)\nsequences = tokenizer.texts_to_sequences(tweets)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","caa0c040":"X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","367c41df":"model1 = Sequential()\nmodel1.add(layers.Embedding(max_words, 40))\nmodel1.add(layers.LSTM(40,dropout=0.5))\nmodel1.add(layers.Dense(1,activation='sigmoid'))\n\nmodel1.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model1.fit(X_train, y_train, epochs=7,validation_data=(X_test, y_test))","e65cbacf":"test_loss, test_acc = model1.evaluate(X_test,  y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","fdcb576d":"y_pred = model1.predict(X_test)","fed79f26":"from sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(y_test, np.around(y_pred, decimals=0))\nimport seaborn as sns\nconf_matrix = pd.DataFrame(matrix, index = ['Not Depression\/Anxiety','Anxiety\/Depression'],columns = ['Not Depression\/Anxiety','Anxiety\/Depression'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","617a8f9e":"model2 = Sequential()\nmodel2.add(layers.Embedding(max_words, 40))\nmodel2.add(layers.LSTM(40,dropout=0.5,return_sequences=True))\nmodel2.add(layers.LSTM(40,dropout=0.5))\nmodel2.add(layers.Dense(1,activation='sigmoid'))\n\nmodel2.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model2.fit(X_train, y_train, epochs=5,validation_data=(X_test, y_test))","935f5bc4":"test_loss, test_acc = model2.evaluate(X_test,  y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","77aa3576":"y_pred = model2.predict(X_test)","b83601eb":"matrix = confusion_matrix(y_test, np.around(y_pred, decimals=0))\nconf_matrix = pd.DataFrame(matrix, index = ['Not Depression\/Anxiety','Anxiety\/Depression'],columns = ['Not Depression\/Anxiety','Anxiety\/Depression'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","ced382ec":"model3 = Sequential()\nmodel3.add(layers.Embedding(max_words, 40))\nmodel3.add(layers.Bidirectional(layers.LSTM(40,dropout=0.5)))\nmodel3.add(layers.Dense(1,activation='sigmoid'))\n\nmodel3.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model3.fit(X_train, y_train, epochs=8,validation_data=(X_test, y_test))","afdad943":"y_pred = model3.predict(X_test)","021e85fa":"matrix = confusion_matrix(y_test, np.around(y_pred, decimals=0))\nconf_matrix = pd.DataFrame(matrix, index = ['Not Depression\/Anxiety','Anxiety\/Depression'],columns = ['Not Depression\/Anxiety','Anxiety\/Depression'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","6983fc7b":"test = np.array(['I feel stress, sadness and anxiety - just want to sleep until the lockdown ends'])\ntest_sequence = tokenizer.texts_to_sequences(test)\ntest_sequence = pad_sequences(test_sequence, maxlen=max_len)\ntest_prediction = model3.predict(test_sequence)\nif np.around(test_prediction, decimals=0)[0][0] == 1.0:\n    print('The model predicted depressive\/anxious language')\nelse:\n    print(\"The model predicted other type of language\")","42691e0e":"os.listdir('\/kaggle\/input\/depression-anxiety-tweets\/Tweets data')[:5]","85de4d43":"tweets = pd.read_csv('\/kaggle\/input\/depression-anxiety-tweets\/Tweets data\/0314_1.csv')\ntweets.head()","f91bf398":"for dirname, _, filenames in os.walk('\/kaggle\/input\/depression-anxiety-tweets\/Tweets data'):\n    for filename in filenames:\n        if filename!='0314_1.csv':\n            temp = pd.read_csv(os.path.join(dirname, filename))\n            tweets = pd.concat([tweets, temp], ignore_index=True)","71b23d50":"tweets.shape","8b7cc63f":"tweets.sort_values(by=['date'],inplace=True)\ntweets.reset_index(drop=True,inplace=True)\ntweets = tweets[['date','text']]","a2f44d2f":"tweets_dataset = tweets.copy()\ntweets.head()","eb302ed5":"#Removing non-ascii characters (for example, arabian chars)\ntweets.text.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n#Making all fields string type\nfor i in range(len(tweets)):\n  tweets.at[i,'text'] = str(tweets.iloc[i]['text'])\n#Removing URLs\nfor i in range(len(tweets)):\n  tweets.at[i,'text'] = remove_urls(tweets.iloc[i]['text'])\n# Convert to list\ndata = tweets.text.values.tolist()\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]","45248c6a":"data = np.array(data)\ndata[:10]","0e76b4b3":"sequences = tokenizer.texts_to_sequences(data)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","9afa6e59":"predictions = model3.predict(tweets)","67568b9f":"np.around(predictions, decimals=0)","8768f905":"tweets_dataset['label'] = np.around(predictions, decimals=0)","c4adbaad":"tweets_dataset[tweets_dataset['label']==1.0].head(10)","d616ec10":"for i in range(10):\n    print(tweets_dataset.iloc[i*2]['text'])\n    print('\\n')","26ede1e1":"# Data preprocessing for Topic Classification\n\nAs you could see before, our text processing pipeline did very well its task, so we'll use it again to feed and fit our classifier.","4f0bb020":"Let's look for depressive\/anxious tweets","cc8f0aaf":"Fantastic! Let's move on a predict over a much bigger dataset.","ee68e9ec":"# Data exploration for Topic Modeling\n\nEssentially what we'll do is to explore how the unlabeled data is to then preprocess it.","fb0a83d6":"As you can see, NMF delivers better results because it's more specific and really determines depressive\/anxious comments meanwhile LDA labels all entries that contain depression\/anxiety related words and it makes the labeling more general. **Let's keep NMF's results.**\n\nAs you can see above, now the dataset is properly labeled; the label 1 - Depression & Anxiety-  is pretty coherent. Even when label 0 still contains text with relevant words, if you deep dive into it you'll notice the entries don't seem to be comments related to Depression & Anxiety. Said that, let's move on: It's time now to classify tweets.","190f1ae0":"# Topics' Comparison\n\nLet's see how coherent the topics are for each model. Remember we're looking for specifity, otherwise the classifier would get wrong results.","250d6d13":"It achieved very good results - at least to be a so simple model, but seems like it cannot interpret very good the underlying text structure. Let's try a more complex network\n\n## Model 2\nWe'll stack to LSTM layers and increase a little bit the embedding dimension. Let's see how it performs.","ddd65a56":"# Testing the model with depressive\/anxious language","19d78f52":"# Data preprocessing for Topic Modeling\n\nWe need to apply some NLP tasks to prepare the data to be labeled. These are the main tasks:\n\n* Remove URLs\n* Remove emails\n* Lowercase all text\n* Remove punctuation signs\n* Remove stop words\n* Lemmatize text","9f233c75":"# Model training\n\nWe'll explore how LDA and NMF can create the topics and depending on the outcomes we'll select the proper one for this project. Essentially we're looking for focused topics, otherwise the purpose of this project won't be reached.\n\nThe crucial difference between both models is that LDA adds a Dirichlet prior on top of the data generating process, meaning NMF qualitatively leads to worse mixtures, which could affect our dataset's topic quality. \n\nRegarding the library we'll be using: Scikit-Learn - the reasons are more than obvious, even when Gensim has more capabilities, it's also more complex and much more slower - we're looking to keep the things as simpler as possible and get results as quick as possible.\n\nThe outcome of this stage will be the original dataframe with its labels: 1 for depression\/anxiety comments and 0 for other type of comments.","ca3a8c7b":"# Depressive\/Anxious tweets import and exploration\n\nAlright, we're about to explore a dataset that I've acquired implementing OMGOT script - It's essentilly a Python code to get tweets without the need of using the Twitter API. Why did I do it this way? Because in order to use the Twitter API you need a developer account and you all know how complex is to get it approved. In addition, the Twitter API only allows you to retrieve a small quantity of tweets. In the other hand, the mentioned script allows you to get whatever the tweets you want, in the location you want and with the keywords that you want. \n\nThe dataset is made up of several CSV files, each one containing all available tweets related to depression and anxiety topics. We've limited them because would get complex to measure our model in a dataset that contains spread content. Also, the dataset contains tweets from 03\/14 to 03\/27 - one week before and after the lockdown started along US. Let's see what patterns we can find.","6082c117":"# Conclusions\n\nAlright, we've reached the end of this notebook. We went through several steps, including NLP for clustering and classification. What I would like to highlight at this point is that Deep Learning is an approach for when you have tons of data available, otherwise most of the times regular\/classic ML models would perform better. As you could see, DL this time delivered an acceptable score, but for any other task such as in medical field, definitely you will need to get more data to increase the recall score. For now, I train the same model with the same data on SageMaker and should get similar results.\n\nThanks for reading.","1b90226f":"In order to feed our models, we need to create sequences of data using the tweets dataset. For this task we'll use the Tokenizer object with its useful methods.","8476c794":"Splitting the data, this to be able to measure how well our models perform.","024c8a25":"# Data Vectorization for Topic Modeling\n\nThis step is crucial, otherwise our models that can only interpret numerical data won't be able to process our text, and we'll do it in a very particular way: we'll implement TFIDF vectorizer and CountVectorizer. The reason why we do this is to then compare NMF and LDA models' topics.","243624a0":"Let's print some random depressive\/anxious tweets to see how well the results are.","0b20993b":"Our first dataset contains 7145 entries without null values","6ed58892":"# Dominant Topics' Extraction\n\nEssentially what we'll do is to attach every document to its respective label.","02a3253c":"It's time to explain how the dataset is composed. I queried several tweets from the beginning of the lockdown at US. I focused on downloading tweets that contain the keywords 'Anxiety', 'Depression', 'Anxious' and 'Depressive' in order to limit the topics, otherwise these would be very general. Let's put them all on a same dataframe to start the exploration.","8198342f":"# Model building and training\n\nWe'll go through a few Deep Learning architectures trying to find the one that fits better the data. In order to select the best model, we'll be looking at the confusion matrix and identifying the model that better predicts the TP cases.\n\nEssentially we'll create several Recurrent Neural Networks stacking Keras layers. **Keep in mind that the data available could not be enough to train a DL model and the best approach could be to use a ML classic model**. Let's see what model performs the best.","eee1d11a":"This model performed really well for the data available and delivered better results than the previous ones and its' abvious, this model trains on the data in chronologial and anti-chronological way, just to try to find out deeper patters. **For this oportunity we'll keep the 3rd model, which is the one that delivers better results.** (or at least before the final commit)  - Let's test the model with a sentence including depressive\/anxious language.","65bccaf1":"As you can see, NMF delivers better term mixtures for the topic 1 which is the one that cares us the most. Actually, if you read carefully the topic 1 you'll find out that it's what we were looking for since the beginning. Just because NMF is able to work with ngrams is the reason why we get better results and delivers depressive\/anxious actual mixtures and not just terms about it.\n\nLet's go more in depth and get all #1 labels for both models and see which one is more coherent.","78ce51c3":"It decreased a little bit the TPs. For this task this wouldn't work for us. Let's try with a very powerful model, usually used for NLP tasks.\n\n# Model 3\nWe'll implement a Bidirectional LSTM layer. Let's see how it performs.","f0e1eacc":"# Imports","224536ab":"## Model 1\nA very basic RNN with single LSTM layer.","655aa141":"# The Project\n\nIn this opportunity we'll go through a very particular topic. We all know the lockdown during the COVID-19 is affecting all of us in different ways, but the most frequents are depression and anxiety which is an expected outcome - the natural responses to confinement are precisely these, and most of the people don't even know it. It's been a hard time, people are afraid of uncertainty, of losing their jobs as many people have already done, The conditions are met for a major emotional imbalance.\n\nExperts recommend to stay away from social media because it accelerates the depression process, and who is depressed already will be even more, however people expressions on it are a key instrument to determine how a population is feeling. Most of the social media active people express how they feel in tweets, facebook posts, comments and even Instagram captions. So, starting from there, **can we implement Deep Learning to discover depression and anxiety on tweets out there?**\n\nThe next project will cover several steps, but a 10000 feet overview would be:\n\n* **Topic Modeling** - where we'll be looking for two labels: 1 - Depression & anxiety comments, 0 - Other, in Facebook comments prior known to contain depression expressions\n* **Topic Classification** - We'll implement a Keras Recurrent Neural Network to find out depression in a tweets dataset.\n\nTo achieve both tasks, we'll go through:\n\n* **Data collection** - Getting data from different sources to accomplish the main objective.\n* **Data cleaning** - We'll have to take all the data which is already in different formats and clean it up to then be able to use it.\n* **Natural Language Processing for Topic Modeling** - We'll need to transform the text data into a type that can be interpreted by ML models.\n* **Unsupervised Learning tasks for Topic Modeling** - This is crucial, because most of the data we can find out there is unlabeled, so we first need to identify patterns in it.\n* **Supervised Learning tasks for Topic Classification**- Once the data is labeled, we'll go through a Neural Network creation & training to then classify tweets.\n* Predict depression and anxiety in unseen tweets before and after lockdown\n* Results' charting and conclusions\n\n\n**The Depression & Anxiety Facebook comments dataset was obtained at https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6111060\/ - for future references.**\n\n**The COVID-19 tweets were acquired by myself implementing tweets scrapping. The dataset is composed by several tweets distribuited in the first week of the US lockdown**"}}