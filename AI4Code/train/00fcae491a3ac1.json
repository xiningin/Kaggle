{"cell_type":{"83c93c03":"code","702a2e82":"code","b218c057":"code","53560bbe":"code","f1950cd1":"code","775b73d5":"code","7c52c230":"code","aabbb15e":"code","cd9508a5":"code","fd132809":"code","9a203e71":"code","048d72a2":"code","2e369bc6":"code","b3c84894":"code","18a8203f":"code","864fa601":"code","c898677c":"code","2fe0aeb1":"code","e36d739b":"code","73dee592":"code","13ebc69e":"code","2ff7caf5":"code","1fef7f50":"code","06bdac4d":"code","3088e4e4":"code","39255406":"code","7c300fd1":"code","385a0cfe":"code","753900d8":"code","3afd48bb":"code","315faae5":"code","d2a9e5e1":"code","2e9a9549":"code","d4431ae3":"code","ae261f64":"code","95abb53f":"code","95592db1":"code","3ad5d260":"code","1a0ab9ad":"code","cd4d834f":"code","4486f8a6":"code","1b6a7ead":"code","08c3cf35":"code","4d44b53b":"code","6c4de33c":"code","aeaf9703":"code","e4917d70":"code","9e6ed4db":"code","24b0c845":"code","8562bc9b":"markdown","9d9016a9":"markdown","26dce29f":"markdown","afd6f370":"markdown","9f36b310":"markdown","521be0dc":"markdown","2010e1de":"markdown","a4a5dea4":"markdown","27c28cbd":"markdown","73e23d7a":"markdown","5dbe58ae":"markdown","86ee5206":"markdown","26103d13":"markdown","488f3df9":"markdown","134ca807":"markdown","38235ad6":"markdown","b24e65a5":"markdown","c5cf4d3f":"markdown","6df60acd":"markdown","25cabc4c":"markdown","c2500b48":"markdown","1ba53c55":"markdown","e1309524":"markdown","bd37e1e6":"markdown"},"source":{"83c93c03":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport os\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport scipy.stats\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","702a2e82":"# Read CSV files \n\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","b218c057":"# combine the train data and test data\nall_data = pd.concat([train_data,test_data],axis=0)\nall_data.shape","53560bbe":"# show the first 5 rows \nall_data.head(5)","f1950cd1":"#show statsitcal summary \nall_data.describe()","775b73d5":"#showing the data object type and numbers of  non null values for each variable\nall_data.info()","7c52c230":"#show the number of columns and rows\nall_data.shape\n","aabbb15e":"#show columns names \nall_data.columns","cd9508a5":"# show columns with object data type\nall_data.select_dtypes(include=['object']).columns","fd132809":"# show unique values for object variable data type\ndef print_unique_col_values(data):\n       for column in data:\n            if data[column].dtypes=='object':\n                print(f'{column}: {data[column].unique()}') \nprint(print_unique_col_values(all_data))                ","9a203e71":"# It is clear that all object type is categorical , so i will change all of it to categorical\nall_data[list(all_data.select_dtypes(include=['object']))]= all_data.select_dtypes(include=['object']).apply(lambda x: x.astype('category'))","048d72a2":"# Display all rows to see the variable types  \nwith pd.option_context('display.max_rows', None):\n  print(all_data.dtypes)","2e369bc6":"# counting null percentage of null values to drop columns with high rates of null values\nTotal = all_data.isnull().sum().sort_values(ascending=False)\nPercent = Total\/all_data.isnull().count().sort_values(ascending=False)\ndata_percent = pd.concat([Total,Percent],axis=1,keys=['Total', 'Percent'])\ndata_percent.head(30)","b3c84894":"# drop all columns with more than 15% of missing values to avoid wrong filling of missing data \n\nall_data2=all_data.drop((data_percent[data_percent['Percent'] > 0.15]).index,axis=1)\nall_data2","18a8203f":"# drop missing value rows in (GarageQual, GarageFinish, GarageType, GarageCond,GarageYrBlt\t) which are related to garage and same thing to rows in (BsmtFinType1,BsmtCond,BsmtQual,BsmtExposure,BsmtFinType2,BsmtFinType2)whcih are related to basment because of all of these vaules correlated to each other and have same number of missing vaules means at the same rows all of these values are missed \n\nall_data2.dropna(subset=['GarageQual', 'GarageFinish', 'GarageCond', 'GarageYrBlt','BsmtFinType1','BsmtCond','BsmtQual','BsmtExposure','BsmtFinType2','BsmtFinType2'],inplace=True)\nall_data2.isnull().sum().sort_values(ascending=False)","864fa601":"# Now we only have 3 variable contain null values. I will replace MasVnrType null values with \"None\" and MasVnrArea with \"0\" because based on the recorded values it means it doesnt have values. and drop the only electrical values.\n\nall_data2['MasVnrType'].fillna('None',inplace=True)\nall_data2['MasVnrArea'].fillna(0,inplace=True)\nall_data2.dropna(inplace=True,axis=0)\nall_data2[['MasVnrType','MasVnrArea','Electrical']].isnull().sum()\n","c898677c":"# Show  all columns \npd.set_option(\"display.max_columns\", None)\nall_data2","2fe0aeb1":"#plot SalePrice distripution graph\nfig, ax = plt.subplots(2,1,figsize=(20,20))\nax = ax.flatten()\nscipy.stats.probplot(all_data2['SalePrice'],plot=ax[1])\nsns.distplot(all_data2['SalePrice'],fit=norm, ax =ax[0])\nax[0].set_xlabel('saleprice')\nax[0].set_ylabel('Freq')\nax[0].set_title(\"SALE PRICE NORMAL\")\nplt.show()","e36d739b":"#normalize data using box cox transformation because it perform better in fixing skewness \nall_data2['SalePrice'], e =scipy.stats.boxcox(all_data2['SalePrice'])","73dee592":"#plot SalePrice distripution graph\nfig, ax = plt.subplots(2,1,figsize=(20,20))\nax = ax.flatten()\nscipy.stats.probplot(all_data2['SalePrice'],plot=ax[1])\nsns.distplot(all_data2['SalePrice'],fit=norm, ax =ax[0])\nax[0].set_xlabel('saleprice')\nax[0].set_ylabel('Freq')\nax[0].set_title(\"SALE PRICE NORMAL\")\nplt.show()","13ebc69e":"# well now it is better.\n#check Skewness and Kurtosis\nprint(\"Skewness: \",  all_data2['SalePrice'].skew())\nprint(\"Kurtosis: \",  all_data2['SalePrice'].kurt())","2ff7caf5":"#remove outlayers using Standard Deviations by removing all data out 3 times standard deviation larger or smaller from the mean \nSalePrice_mean, SalePrice_std = all_data2['SalePrice'].mean(), all_data2['SalePrice'].std()\ncut_off = SalePrice_std * 3\nlower, upper = SalePrice_mean - cut_off, SalePrice_mean + cut_off\n# identify outliers\nall_data2.drop(all_data2[(all_data2.SalePrice < lower) ].index,inplace=True)\nall_data2.drop(all_data2[(all_data2.SalePrice > upper) ].index,inplace=True)\n","1fef7f50":"#plot the Sale Price  distribution \nfig, ax = plt.subplots(2,1,figsize=(20,20))\nax = ax.flatten()\nscipy.stats.probplot(all_data2['SalePrice'],plot=ax[1])\nsns.distplot(all_data2['SalePrice'],fit=norm, ax =ax[0])\nax[0].set_xlabel('saleprice')\nax[0].set_ylabel('Freq')\nax[0].set_title(\"SALE PRICE NORMAL\")\nplt.show()","06bdac4d":"# creat categorical variable to encode it since  machine learning algorthim on sklearn deals only with numbers \ncategories = list(all_data2.select_dtypes(include= 'category').columns)\ncategorical = all_data2[categories]\nsales = all_data2['SalePrice']\ncategorical1 =pd.concat([categorical,sales],axis=1) \ncategorical1\n\n","3088e4e4":"# plot scatter diagram between independent categorical variable and the dependet variable 'Sale Price'\nfig, axs = plt.subplots(13, 3, figsize=(18, 70))\naxs = axs.flatten()\nplt.subplots_adjust(right=1.3,hspace=0.3)\nfor i, col in enumerate(list(categorical1)):\n    sns.scatterplot(y='SalePrice', x=col, ax=axs[i], data=categorical1)\n    axs[i].set_xlabel('SalePrice')\n    axs[i].set_ylabel(col)\nplt.show()","39255406":"#encoding categorical variables\ncategorical1 = categorical1.drop('SalePrice',axis=1)\nencoder = LabelEncoder()\nencoded_cat = categorical1.apply(lambda x: encoder.fit_transform(x))\nencoded_cat = pd.concat([encoded_cat,sales],axis=1)\ncor = encoded_cat.corr()","7c300fd1":"# categorical variable with pearson coef larger than 0.5\nlcor = abs(cor.iloc[0:(len(cor.index)-1),len(cor.columns)-1]).sort_values(ascending=False )\nlcor[(lcor>0.5)]","385a0cfe":"# drop categorical variables with correlation less than 0.5\nlcor= list(lcor[(lcor>0.5)].reset_index()['index'])\nencoded_cat = encoded_cat[lcor]\nencoded_cat","753900d8":"numeric_data = all_data2.select_dtypes(exclude= 'category').drop(['Id'],axis=1)\nnumeric_data\n","3afd48bb":"# correlation matrix between numerical variable\ncorrnum = numeric_data.corr()\n# sort numeric varaible based on correlation to the dependent variable \nlnum = abs(corrnum.iloc[0:(len(corrnum.index)-1),len(corrnum.columns)-1]).sort_values(ascending=False )\nlnum","315faae5":"# plot scatter diagram between independent numerical variable and the dependet variable 'Sale Price'\nfig, axs = plt.subplots(9, 4, figsize=(18, 50))\naxs = axs.flatten()\nplt.subplots_adjust(right=1.3,hspace=0.3)\nfor i, col in enumerate(list(lnum.index)):\n    sns.scatterplot(y='SalePrice', x=col, ax=axs[i], data=numeric_data)\n    axs[i].set_xlabel('SalePrice')\n    axs[i].set_ylabel(col)\nplt.show()","d2a9e5e1":"# Combine  data where possible \n # Age of building   \nnumeric_data['Age']= numeric_data['YrSold'] - numeric_data['YearBuilt']\n # Home  quality \nnumeric_data['home_quality'] = numeric_data['OverallQual'] + numeric_data['OverallCond']\n  # Total area \nnumeric_data['Area'] = (numeric_data['BsmtFinSF1'] + numeric_data['BsmtFinSF2'] +\n                         numeric_data['1stFlrSF'] + numeric_data['2ndFlrSF'])\nnumeric_data['NumBath'] = (numeric_data['FullBath'] + (0.5 * numeric_data['HalfBath']) +\n                            numeric_data['BsmtFullBath'] + (0.5 * numeric_data['BsmtHalfBath']))\nnumeric_data['porch_area'] = (numeric_data['OpenPorchSF'] + numeric_data['3SsnPorch'] +\n                              numeric_data['EnclosedPorch'] + numeric_data['ScreenPorch'] +\n                              numeric_data['WoodDeckSF'])\n # Create variables from weak correlated variable to avoid droping it and losing data \nnumeric_data['haspool'] = numeric_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nnumeric_data['hasfireplace'] = numeric_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#credit goes to 'https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition'","2e9a9549":"# Drop variable that: 1- feature created from them to avoid presence of multicollinearity \n#                     2-  Has small value of  pearson coef \n\nnumeric_data.drop(['GarageArea','TotalBsmtSF','YearBuilt','OpenPorchSF','LotArea','MasVnrArea','MSSubClass','LowQualFinSF','WoodDeckSF','EnclosedPorch','3SsnPorch','PoolArea','Fireplaces','FullBath','HalfBath','BsmtFullBath','BsmtHalfBath','OverallQual','OverallCond','YrSold','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','ScreenPorch','MiscVal'],axis=1, inplace=True)\nnumeric_data","d4431ae3":"# Scale data using MinMax scaler \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumeric_data[['porch_area','Area','GrLivArea','2ndFlrSF','1stFlrSF','YearRemodAdd','GarageYrBlt']] = scaler.fit_transform(numeric_data[['porch_area','Area','GrLivArea','2ndFlrSF','1stFlrSF','YearRemodAdd','GarageYrBlt']])\nnumeric_data","ae261f64":"# Comine the data and split the dependent variable from the independet one\nFinal_data = pd.concat([numeric_data,encoded_cat],axis=1)\nx = Final_data.drop('SalePrice',axis=1)\ny = Final_data['SalePrice']","95abb53f":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)","95592db1":"# Model Metrics \n # Root mean square error\ndef rmse(y, y_predict):\n    return print('RMSE:',mean_squared_error(y, y_predict,squared= False))\n\n# coefficient of determination R2\ndef R2(y_test, y_predict):\n                 return print('R2:',r2_score(y_test, y_predict))\n    \n","3ad5d260":"# Fit a linear regression model on the training set\nlinreg = LinearRegression()\nmodel = linreg.fit(x_train,y_train)","1a0ab9ad":"# use the fitted model to create predction\ny_predict = model.predict(x_test)","cd4d834f":"rmse(y_test, y_predict)\nR2(y_test, y_predict)","4486f8a6":"# Create summary using statemodels OLS \nx_train1 = sm.add_constant(x_train)\nmodel1 = sm.OLS(y_train,x_train1)\nresult = model1.fit()\nprint(result.summary())","1b6a7ead":"# drop all insiginficant variables where value in 'P>|t|' columns > 0.05 'level of significant'\nx_train.drop(['YearRemodAdd','1stFlrSF','2ndFlrSF','GrLivArea','BedroomAbvGr','MoSold','ExterQual'],axis=1,inplace=True)\nx_test.drop(['YearRemodAdd','1stFlrSF','2ndFlrSF','GrLivArea','BedroomAbvGr','MoSold','ExterQual'],axis=1,inplace=True)\n","08c3cf35":"# Rerun the model \nx_train1 = sm.add_constant(x_train)\nmodel1 = sm.OLS(y_train,x_train1)\nresult = model1.fit()\nprint(result.summary())","4d44b53b":"result.params","6c4de33c":"#check multicloniarty using Varince Inflation Factor \"VIF\"\ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\n\ncalc_vif(x_train1)","aeaf9703":"# Residual \n# scale the residual to get good plot \nres = MinMaxScaler().fit_transform(result.resid.values.reshape(-1,1))\n#plot scatter plot between Residuals and predicted values \nplt.figure(figsize=(15,10))\nplt.scatter(result.fittedvalues,res)\nplt.xlabel('Predicted values ')\nplt.ylabel('Residuals ')\nplt.title('Residuals vs Predicted Values plot ')\nplt.show()","e4917d70":"# Residual for each variable \nfig,ax = plt.subplots(4,4,figsize=(30,25))\nax = ax.flatten()\nplt.subplots_adjust(hspace=0.3)\nfor i,j in enumerate(list(x_train1)):\n    ax[i].scatter(x_train1[j],res)\n    ax[i].set_xlabel(j)\n    ax[i].set_ylabel('Residuals')\n    ax[i].set_title('Residuals vs Predicted plot ')\nplt.autoscale()\nplt.show()\n","9e6ed4db":"## QQ plot \nfig = plt.figure(figsize=(40,20))\nax = fig.add_subplot(2, 2, 1)\nsm.qqplot(res,line='s',ax=ax)\nplt.autoscale()\nplt.show()","24b0c845":"# sharpio wilk test \nscipy.stats.shapiro(res)\n","8562bc9b":"## 1.5 Data Visualization and outlayers removing ","9d9016a9":"## 2.2 Select evaluation metrics ","26dce29f":"## 2.1 Split data to training and testing data","afd6f370":"since there is no VIF value larger than 5 ; there is no presence of multicloniarity","9f36b310":"## 1.2 Import the Datasets","521be0dc":"## 2.4 Apply the model","2010e1de":"### 3. independent numerical variable ","a4a5dea4":"## 2.5.1 multicolniarty ","27c28cbd":"### 1. Dependet variable ","73e23d7a":"# 2. Build the model","5dbe58ae":"## 2.4 evaluate the model","86ee5206":"# <h1  align='center'>Houses Price Prediction Using Linear Regression<\/h1>","26103d13":"## 1.1 Import Liberies","488f3df9":"It is clear that there is Skewness and it could be fixed by normalizing the data ","134ca807":"## 1.3 Explore the data ","38235ad6":"### 2. independent categorical variable ","b24e65a5":"## 1.4 Cleaning the data ","c5cf4d3f":"## 2.5 Improve model ","6df60acd":"## 2.3 Select Models","25cabc4c":"## 1.6 Feature Engineering ","c2500b48":"## 2.5.2 Homoscedasticty","1ba53c55":"### I will devide it to three parts:\n### 1. Dependet variable \n### 2. independent categorical variable \n### 3. independent numerical variable ","e1309524":"# 1. DATA EXPLORATION ","bd37e1e6":"I am going to use only linear regression "}}