{"cell_type":{"9a4f33d4":"code","321fa3c0":"code","c0768847":"code","df127cf0":"code","78bb458a":"code","2836b375":"code","817ba555":"code","baffcaf8":"code","e82b4b89":"code","b74db43b":"code","d7a89dd2":"code","6b1ce703":"code","c497cf0e":"code","e48ce274":"code","e59d3743":"code","7beb5782":"code","9d79ff14":"code","3c9a9988":"code","d611d67f":"code","8582328f":"code","f70bd732":"code","ceb7452c":"markdown","9d0e1f15":"markdown","5cf52092":"markdown","14e24ad8":"markdown","5ceda1c9":"markdown","3a3a5858":"markdown","cee084b1":"markdown","28a813ab":"markdown","d5ecc15e":"markdown"},"source":{"9a4f33d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","321fa3c0":"#imports \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","c0768847":"#data-exploration\n\ndf=pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","df127cf0":"df.head()","78bb458a":"sns.set_style('darkgrid')\nsns.countplot(df.price_range)","2836b375":"#create_folds.py\nfrom sklearn import model_selection\n\ndf['kfold']=-1\n\ndf=df.sample(frac=1).reset_index(drop=False)\n\nkf=model_selection.KFold(n_splits=5)\n\nfor fold,(trn_,val_) in enumerate(kf.split(X=df)):\n    df.loc[val_,'kfold']=fold\n","817ba555":"df.drop('index',axis=1,inplace=True\n       )","baffcaf8":"df.head()","e82b4b89":"#rf-classifier-train.py\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\n\n\ndef run(fold):\n    \n    #train-val-split\n    \n    df_train=df[df.kfold!=fold].reset_index(drop=True)\n    df_val=df[df.kfold==fold].reset_index(drop=True)\n\n    x_train=df_train.drop('price_range',axis=1).values\n    y_train=df_train['price_range']\n\n    x_val=df_val.drop('price_range',axis=1).values\n    y_val=df_val['price_range']\n\n\n    clf=ensemble.RandomForestClassifier(criterion='entropy',max_depth=123,n_estimators=225)\n    clf.fit(x_train,y_train)\n\n    preds=clf.predict(x_val)\n\n    accuracy=metrics.accuracy_score(y_val,preds)\n    \n    print(f'fold : {fold} , Accuracy = {accuracy}')\n    \nif __name__=='__main__':\n    for f_ in range(5):\n        run(f_)","b74db43b":"param_grid = {'n_estimators':[100,200,250,300,400,500],'criterion':['gini','entropy'],\n              'max_depth':[1,2,5,7,11,15],\n}","d7a89dd2":"from sklearn import metrics,model_selection\n\n\nclassifier=ensemble.RandomForestClassifier(n_jobs=-1)\n\nparam_grid = {'n_estimators':[100,200,250,300,400,500],'criterion':['gini','entropy'],\n              'max_depth':[1,2,5,7,11,15],\n}\n\n#initalzing the grid search \n#estimator below is the model that we have defined\n#accuracy is defined as the metric\n#cv=5 means we are using 5 fold cv(cross-validation) (not straified)\n\nmodel=model_selection.GridSearchCV(\n    estimator=classifier,\n    param_grid=param_grid,\n    \n    scoring='accuracy',\n    verbose=10,\n    n_jobs=-1,\n    cv=5\n\n)\n\nx_train=df[df.columns].drop('price_range',axis=1)\ny_train=df.price_range.values\nmodel.fit(x_train,y_train)\n\nprint(f'Best Score = {model.best_score_}')\n\nprint('Best parameter set : ')\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n     print(f\"\\t{param_name}: {best_parameters[param_name]}\")","6b1ce703":"df","c497cf0e":"df.corr()","e48ce274":"#checking highly correlated features\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(df, 3))","e59d3743":"#let's try KNN","7beb5782":"from sklearn.neighbors import KNeighborsClassifier\n\ndef run(fold):\n\n    df_train=df[df.kfold!=fold].reset_index(drop=True)\n    df_val=df[df.kfold==fold].reset_index(drop=True)\n\n    x_train=df_train.drop('price_range',axis=1).values\n    y_train=df_train['price_range']\n\n    x_val=df_val.drop('price_range',axis=1).values\n    y_val=df_val['price_range']\n\n\n    \n    clf=KNeighborsClassifier(n_neighbors=15)\n    clf.fit(x_train,y_train)\n\n    preds=clf.predict(x_val)\n\n    accuracy=metrics.accuracy_score(y_val,preds)\n    \n    print(f'fold : {fold} , Accuracy = {accuracy}')\n    \nif __name__=='__main__':\n    for f_ in range(5):\n        run(f_)","9d79ff14":"#Nice without any hyperparameter tuning ...it's already better than RandomForest","3c9a9988":"#trying to find the perfect k-value","d611d67f":"fold=0\ndf_train=df[df.kfold!=fold].reset_index(drop=True)\ndf_val=df[df.kfold==fold].reset_index(drop=True)\n\nx_train=df_train.drop('price_range',axis=1).values\ny_train=df_train['price_range']\n\nx_val=df_val.drop('price_range',axis=1).values\ny_val=df_val['price_range']\n\nacc=[]\nfor k in range(1,30):\n    clf=KNeighborsClassifier(n_neighbors=k)\n    clf.fit(x_train,y_train)\n    preds=clf.predict(x_val)\n\n    accuracy=metrics.accuracy_score(y_val,preds)\n    acc.append(accuracy)","8582328f":"import seaborn as sns","f70bd732":"plt.figure(figsize=(10,5))\nplt.xlabel('K-neighbour')\nplt.ylabel('accuracy')\nsns.set_style('darkgrid')\nsns.lineplot(x=list(range(1,30)),y=acc,color='red')","ceb7452c":"# Back to exploration and engineering","9d0e1f15":"# lol that wasn't of much help\nlet's look at the data again !","5cf52092":"# Inference : It's a multiclass-class classification problem\n### the data is unformly distributed among all classes\n### Cross Validation - Kfold would be fine no need of stratified\n### Evaluation metric - macro-F1 score or accuracy would be fine seeing the uniform distribution","14e24ad8":" {\n    n_estimators=100,\n    *,\n    criterion='gini',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n    ccp_alpha=0.0,\n    max_samples=None,\n)","5ceda1c9":"Was able to improve about 1 % ","3a3a5858":"Inference : No highly releated pairs :(","cee084b1":"    Inference : without any hyperparameter-optimization or feature engineering the model seems pretty accurate\n    \n   <b> Best Accuracy for Random-Forest = 88.75 %","28a813ab":"# hyperparameter optimization","d5ecc15e":"Inference KNN performs way better than,without much tuning"}}