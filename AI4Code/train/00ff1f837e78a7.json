{"cell_type":{"f18cc712":"code","46fe73d0":"code","fc2b130b":"code","175d1810":"code","eccf6942":"code","9e7f3a92":"code","37cbd27b":"code","9dd2ad6b":"code","9242574d":"code","9c748e7d":"code","78220b90":"code","462fecc8":"code","800a7b64":"code","f2e6ded6":"code","70ff6808":"code","cdd08c90":"code","615c156f":"code","a5e8c67f":"code","31eba6f6":"code","d46a2378":"code","2ada4778":"code","11e77922":"code","a011dcfe":"code","5b5c6bac":"code","2c3ab6cd":"code","a2c0bebd":"code","9dfbbfce":"code","a45bf6ac":"code","b194974a":"code","f961995c":"code","ea2b2fcb":"code","ec419556":"code","da5ffb67":"code","c1461c2e":"code","16b1cce6":"code","76899b70":"code","501718a9":"code","4a578fd9":"code","d39d7c4e":"code","116f25a2":"code","b681f443":"code","21e81e29":"code","5d916217":"code","dd8b2444":"code","ed80a705":"code","d7681c25":"code","af5b53e8":"code","a66822e0":"code","15a533f5":"code","7dabad66":"code","78a47d93":"code","16a9cb7c":"code","63e8f51c":"code","36a06c68":"code","b307b32e":"code","5a1fa76d":"code","114dfed7":"code","a7be5715":"code","0c83084a":"code","49c01088":"code","27cf2e24":"markdown","263d02f6":"markdown","bff00f3f":"markdown","c6ee439c":"markdown","f5d92408":"markdown","03eb5d6d":"markdown","850e21ba":"markdown","f6191160":"markdown","85081e1c":"markdown","a4ac067a":"markdown","65271339":"markdown","50dc1baa":"markdown","fad13ab7":"markdown","cdcb5b9c":"markdown","74a6beb2":"markdown","a8faff00":"markdown","11f45666":"markdown","2437d767":"markdown","279743bd":"markdown","1c73de0c":"markdown","d1208a92":"markdown","f4d8461d":"markdown","b4bfd048":"markdown","1e66f4e3":"markdown","f03fcb11":"markdown","546ecdc0":"markdown","15628664":"markdown","3a66e78a":"markdown","74953d7d":"markdown","7dee2c80":"markdown","2b0b6981":"markdown","361731c2":"markdown","56f8c2f4":"markdown","108c0934":"markdown","4e40a1d1":"markdown","d06a625d":"markdown","9b2dab2f":"markdown","c440a11b":"markdown","99f58d8d":"markdown","7f535968":"markdown","030c51dd":"markdown","d8b40eed":"markdown","badf69c7":"markdown","192cc20d":"markdown","6642bfc5":"markdown","2c76c2b3":"markdown","8d41f77f":"markdown","3d8fd8b6":"markdown","f2a4ce42":"markdown","680c6a58":"markdown","da6a856a":"markdown","000b9fc8":"markdown","f391afbd":"markdown","8deb3c9f":"markdown","fc4a80c3":"markdown","56875691":"markdown"},"source":{"f18cc712":"!pip install missingpy","46fe73d0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.graphics.gofplots as sm\n\nimport sklearn.neighbors._base\nimport sys\nsys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n\nfrom missingpy import MissForest\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom yellowbrick.regressor import ResidualsPlot\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()","fc2b130b":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","175d1810":"train.shape, test.shape","eccf6942":"house_prices = pd.concat([train, test], axis=0)\nhouse_prices.head()","9e7f3a92":"cat_features = ['BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n       'BsmtFinType2', 'BsmtQual', 'CentralAir', 'Condition1', 'Condition2',\n       'Electrical', 'ExterCond', 'ExterQual', 'Exterior1st', 'Exterior2nd',\n       'FireplaceQu', 'Foundation', 'Functional', 'GarageCond',\n       'GarageFinish', 'GarageQual', 'GarageType', 'Heating', 'HeatingQC',\n       'HouseStyle', 'KitchenQual', 'LandContour', 'LandSlope', 'LotConfig',\n       'LotShape', 'MSSubClass', 'MSZoning', 'MasVnrType', 'GarageCars',\n       'MoSold', 'Neighborhood', 'PavedDrive', 'RoofMatl',\n       'RoofStyle', 'SaleCondition', 'SaleType', 'Street', 'Utilities',\n       'YrSold']\nnum_features = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n       'EnclosedPorch', 'Fireplaces', 'FullBath', 'GarageArea',\n       'GarageYrBlt', 'GrLivArea', 'HalfBath', 'KitchenAbvGr', 'LotArea',\n       'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal', 'OpenPorchSF',\n       'OverallCond', 'OverallQual', 'PoolArea', 'SalePrice', 'ScreenPorch',\n       'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF', 'YearBuilt',\n       'YearRemodAdd']\nhouse_prices.info()","37cbd27b":"missing_values = pd.DataFrame(dict(\n    round(100 * house_prices.drop('SalePrice', axis=1).isnull().sum() \/\n          len(house_prices), 2)), index=[0])\nmissing_values = missing_values.melt(var_name = 'Features', value_name = 'Percentage')\n\nplt.figure(figsize=(20,10))\nsns.barplot('Features', 'Percentage', data=missing_values, color='skyblue')\nplt.title(\"Percentage of Missing Values\", size=25, weight='bold')\nplt.xlabel(\"Features\", size=14)\nplt.ylabel(\"Percentage\", size=14)\nplt.xticks(rotation=90, size=14)\nplt.show()","9dd2ad6b":"house_prices.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)","9242574d":"missing_values = missing_values[(missing_values.Percentage > 0) & (missing_values.Percentage < 79)]\nplt.figure(figsize=(20,10))\nsns.barplot(missing_values.Features, missing_values.Percentage, color='skyblue')\nplt.title(\"Missing Values < 80%\", size=25, weight='bold')\nplt.xlabel(\"Features\", size=14)\nplt.ylabel(\"Percentage\", size=14)\nplt.xticks(rotation=90, size=14)\nplt.show()","9c748e7d":"house_prices.drop('SalePrice', axis=1).isnull().sum().sum()","78220b90":"missing_values_under_10 = missing_values[(missing_values.Percentage > 0) & (missing_values.Percentage < 11)]\nfor feature in missing_values_under_10.Features:\n    if(feature in cat_features):\n        house_prices[feature] = house_prices[feature].fillna(house_prices[feature].mode()[0])\n    else:\n        house_prices[feature] = house_prices[feature].fillna(np.mean(house_prices[feature]))","462fecc8":"house_prices.columns[house_prices.isnull().sum() > 0]","800a7b64":"df = house_prices[['LotArea', 'Neighborhood', 'GarageCars', 'MSZoning', 'BldgType', 'GarageType', 'LotFrontage']]\ndf.head()","f2e6ded6":"imputer =  MissForest()\nimputed_df = imputer.fit_transform(pd.get_dummies(df))\nhouse_prices.LotFrontage = pd.DataFrame(imputed_df, columns=pd.get_dummies(df).columns)['LotFrontage']","70ff6808":"house_prices.FireplaceQu = house_prices.FireplaceQu.fillna('NA')","cdd08c90":"house_prices.drop('SalePrice', axis=1).isnull().sum().sum()","615c156f":"fig, (ax1, ax2) =  plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsns.distplot(train.SalePrice, bins=100, fit=norm, ax=ax1)\nsm.ProbPlot(train.SalePrice).qqplot(line='s', ax=ax2)\n(mu, sigma) = norm.fit(train.SalePrice)\nplt.xlabel(\"Sale Price\", size=14)\nplt.xticks(size=14)\nplt.show()","a5e8c67f":"plt.figure(figsize=(20,10))\nseaborn_plot = house_prices.corr()['SalePrice'].sort_values(ascending=False)[1:11].plot(kind='bar')\nfor p in seaborn_plot.patches:\n    seaborn_plot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.xticks(rotation=0, size=14)\nplt.title(\"Top 10 correlated features vs SalePrice\", size=25)\nplt.show()","31eba6f6":"plt.figure(figsize=(20,10))\nsns.boxplot(house_prices.OverallQual, house_prices.SalePrice)\nplt.title(\"OverallQual vs SalePrice\", size=25, weight='bold')\nplt.xlabel(\"OverallQual\", size=14)\nplt.xticks(size=14)\nplt.show()","d46a2378":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsns.scatterplot(house_prices.GrLivArea, house_prices.SalePrice, ax = axes[0])\nsns.kdeplot(house_prices.GrLivArea, house_prices.SalePrice, ax = axes[1])\nplt.xticks(size=14)\nplt.show()","2ada4778":"plt.figure(figsize=(20,10))\nsns.boxplot(house_prices.GarageCars, house_prices.SalePrice)\nplt.title(\"GarageCars vs SalePrice\", size=25, weight='bold')\nplt.xlabel(\"GarageCars\", size=14)\nplt.xticks(size=14)\nplt.show()","11e77922":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsns.scatterplot(house_prices.GarageArea, house_prices.SalePrice, ax = axes[0])\nsns.kdeplot(house_prices.GarageArea, house_prices.SalePrice, ax = axes[1])\nplt.xticks(size=14)\nplt.show()","a011dcfe":"house_prices.corr()['GarageArea'].sort_values(ascending=False)[1:4]","5b5c6bac":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsns.scatterplot(house_prices.TotalBsmtSF, house_prices.SalePrice, ax = axes[0])\nsns.kdeplot(house_prices.TotalBsmtSF, house_prices.SalePrice, ax = axes[1])\nplt.xticks(size=14)\nplt.show()","2c3ab6cd":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsns.scatterplot(house_prices['1stFlrSF'], house_prices.SalePrice, ax = axes[0])\nsns.kdeplot(house_prices['1stFlrSF'], house_prices.SalePrice, ax = axes[1])\nplt.xticks(size=14)\nplt.show()","a2c0bebd":"house_prices.corr()['1stFlrSF'].sort_values(ascending=False)[1:4]","9dfbbfce":"plt.figure(figsize=(20,10))\nsns.boxplot(house_prices.FullBath, house_prices.SalePrice)\nplt.title(\"FullBath vs SalePrice\", size=25, weight='bold')\nplt.xlabel(\"FullBath\", size=14)\nplt.xticks(size=14)\nplt.show()","a45bf6ac":"plt.figure(figsize=(20,10))\nsns.boxplot(house_prices.TotRmsAbvGrd, house_prices.SalePrice)\nplt.title(\"TotRmsAbvGrd vs SalePrice\", size=25, weight='bold')\nplt.xlabel(\"TotRmsAbvGrd\", size=14)\nplt.xticks(size=14)\nplt.show()","b194974a":"house_prices.corr()['TotRmsAbvGrd'].sort_values(ascending=False)[1:4]","f961995c":"df = house_prices.copy()\ndf.YearBuilt = pd.cut(df.YearBuilt, range(1850,2030,25))\nplt.figure(figsize=(20,10))\nsns.boxplot(df.YearBuilt, df.SalePrice)\nplt.title(\"YearBuilt vs SalePrice\", size=25, weight='bold')\nplt.xlabel(\"YearBuilt\", size=14)\nplt.show()","ea2b2fcb":"house_prices.corr()['YearBuilt'].sort_values(ascending=False)[1:4]","ec419556":"le = LabelEncoder()\nfor feature in cat_features:\n    house_prices[feature] = le.fit_transform(house_prices[feature])","da5ffb67":"train = house_prices.iloc[:len(train), :]\ntest = house_prices.iloc[len(train):, :]","c1461c2e":"scaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\ntest = pd.DataFrame(scaler.transform(test), columns=test.columns)\ntest.drop('SalePrice', axis=1, inplace=True)","16b1cce6":"train.drop(523, inplace=True)\ntrain.drop(1298, inplace=True)","76899b70":"train.shape, test.shape","501718a9":"fig, (ax1, ax2) =  plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsm.ProbPlot(np.log(house_prices.GrLivArea)).qqplot(line='s', ax=ax1)\nax1.set_title('Probability Plot of Log GrLivArea', size=25)\nsns.distplot(np.log(house_prices.GrLivArea), fit=norm, ax=ax2)\nplt.title('Dist of Log of GrLivArea', size=25)\nplt.show()","4a578fd9":"fig, (ax1, ax2) =  plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nsm.ProbPlot(np.log(train.TotalBsmtSF)).qqplot(line='s', ax=ax1)\nax1.set_title('Probability Plot of Log TotalBsmtSF', size=25)\nsns.distplot(np.log(train.TotalBsmtSF), fit=norm, ax=ax2)\nplt.title('Dist of Log of TotalBsmtSF', size=25)\nplt.show()","d39d7c4e":"drop_features = ['TotRmsAbvGrd', '1stFlrSF', 'GarageArea']\nfor feature in drop_features:    \n    train.drop([feature], axis=1, inplace=True)\n    test.drop([feature], axis=1, inplace=True)","116f25a2":"rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\nrvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\nstats.ttest_ind(rvs1, rvs2)\nstats.ttest_ind(rvs1, rvs2, equal_var = False)","b681f443":"pca = PCA(.95)\ntrain_pca = pca.fit_transform(train.drop('SalePrice', axis=1))\ntest_pca = pca.transform(test)","21e81e29":"print(\"95% Variance amounts to {} Features\".format(pca.n_components_))","5d916217":"train.shape, test.shape","dd8b2444":"train_pca.shape, test_pca.shape","ed80a705":"model_lasso = Lasso(alpha =0.0005, random_state=42)\nmodel_enet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=42)\nmodel_kernal = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nmodel_gboosting = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=42)\nmodel_xgboosintg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=42, nthread=-1)\nmodel_rf = RandomForestRegressor(n_estimators=3000, max_depth=4, \n                                 max_features='sqrt', min_samples_leaf=15, \n                                 min_samples_split=10, random_state=42)\nmodels = {'model_lasso': model_lasso, \n         'model_enet': model_enet, \n         'model_kernal': model_kernal,\n         'model_gboosting': model_gboosting,\n         'model_xgboosintg': model_xgboosintg,\n         'model_rf': model_rf}","d7681c25":"X = train_pca\ny = train['SalePrice'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)","af5b53e8":"for (model, machine) in models.items():\n    machine.fit(X_train, y_train)","a66822e0":"for (model, machine) in models.items():\n    y_pred = machine.predict(X_test)\n    rsquared = machine.score(X_test, y_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n    print(\"Model: {} \\nRsquare: {} \\nRMSE: {}\".format(model, rsquared, RMSE))","15a533f5":"plt.figure(figsize=(20, 10))\nvisualizer = ResidualsPlot(model_kernal, qqplot=True, hist=False)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)\nvisualizer.show()       \nplt.show()","7dabad66":"params_grid = {'alpha': [0.4, 0.5, 0.6, 1.0, 1.2, 1.4, 1.5],\n            'degree': [2, 3, 4],\n            'coef0': [1, 1.5, 2, 2.5, 3, 3.5]}\ngrid = GridSearchCV(model_kernal, params_grid, cv=10, scoring='neg_mean_squared_error',\n                    verbose=1, n_jobs=-1)\ngrid.fit(X, y)\ngrid.best_estimator_","78a47d93":"grid.best_score_","16a9cb7c":"grid = RandomizedSearchCV(model_kernal, params_grid, cv=10, scoring='neg_mean_squared_error',\n                    verbose=1, n_jobs=-1)\ngrid.fit(X, y)\ngrid.best_estimator_","63e8f51c":"grid.best_score_","36a06c68":"y_pred_kernal = grid.best_estimator_.predict(X_test)\nRMSE = np.sqrt(mean_squared_error(y_test, y_pred_kernal))\nprint(\"RMSE : {}\\nScore: {}\".format(RMSE, grid.best_estimator_.score(X_test, y_test)))","b307b32e":"plt.figure(figsize=(20, 10))\nvisualizer = ResidualsPlot(grid.best_estimator_, qqplot=True, hist=False)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)\nvisualizer.show()       \nplt.show()","5a1fa76d":"X = train.drop('SalePrice', axis=1)\ny = train['SalePrice'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)","114dfed7":"import statsmodels.api as sm\nmodel_lr = sm.OLS(y_train, X_train).fit()\nprint(model_lr.summary())","a7be5715":"from sklearn.feature_selection import RFE\n\nrfe = RFE(model_lasso, n_features_to_select=2)\nrfe = rfe.fit(X_train, y_train)","0c83084a":"plt.figure(figsize=(20,10))\nfeature_ranking = pd.DataFrame({'Feature': X_train.columns, 'Ranking': rfe.ranking_}).sort_values(by='Ranking')\nsns.barplot(feature_ranking.Feature, feature_ranking.Ranking)\nplt.title(\"RFE Ranking\", size=25)\nplt.xticks(rotation=90)\nplt.show()","49c01088":"grid.best_estimator_","27cf2e24":"Initializing Machines","263d02f6":"FullBath: Full bathrooms above grade\n\nObservation: In essence, with increase in bathrooms there is an increase in SalePrice. ","bff00f3f":"Sale Price is not normally distribution with a positive skewness and peakedness. Lets see which features are most correlated with SalePrice.  ","c6ee439c":"GarageCars: Size of garage in car capacity\n\nObservation: In essence, with increase in capacity of storing cars there is an increase in SalePrice. ","f5d92408":"PCA is a statistical approach that can be used to analyse interrelationships among a large number of features and to explain these features in terms of their common underlying dimensions (actors). The objective is to find a way of condensing the information contained in a number of original features into a smaller set of features with a minimal loss of information. Hence, in this case, I found out that 95% variance in the dataset is from 57 features. Hence I converted my trainset and test set accordingly. ","03eb5d6d":"Hyper-Parameter Tuning","850e21ba":"OverallQual: Overall material and finish quality.\n\nObservation: With increase in quality, there is an increase in SalePrice.","f6191160":"# Principal Component Analysis","85081e1c":"The feature 'FirePlaceQu' refers to 'Fireplace quality'. According to the dictionary, this feature is None with houses for no FirePlaces i.e feature 'Fireplaces' is set to 0 hence I will impute this value with 'NA' as described in the data dictionary. \n\nSnippet from DD:\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n\t\t","a4ac067a":"# Preprocessing\n\nPreprocessing is the step before modelling. Our data should be representable to the model in order to give good predictions. There are some issues when it comes to regression modelling such as:\n1. Auto-Correlation: occurs when the residual errors are dependent on each other.\n2. Multicollinearity: correlation between independent features\n3. Heteroskedasticity: Homoscedasticity describes a situation in which the error term (that is, the \u201cnoise\u201d) is the same across all values of the independent variables. A scatter plot of residual values vs predicted values is a goodway to check for homoscedasticity. There should be no clear pattern in the distribution and if there is a specific pattern, the data is heteroscedastic.","65271339":"I binned the feature 'YearBuilt' in a temproray dataframe to understand the boxplot in a more intuitive way. \n\nYearBuilt: Original construction date\n\nObservation: Latest properties are sold at a higher rate (of course) but there is a slight correlation between 'OverallQual' and 'YearBuilt' but I will consider to not drop this feature as it can provide extra information to the model. ","50dc1baa":"# Conclusion:\n\nWe can have a lot of debates about the techniques I used in this notebook as this is just what I call scratching the surface of multivariate analysis. I'm afraid I could not take time to explain about data leakage and how to implement pipelines but I will share this amazing notebook on data leakage if you are curious. \n\nLink: https:\/\/www.kaggle.com\/alexisbcook\/data-leakage\n\n# References\n\nI highly recommend going through this amazing book: \nMultivariate Data Analysis: by Joseph F. Hair, William C. Black, Barry J. Babin and Rolph E. Anderson. \n\nLink: shorturl.at\/bclQ5\n\n### Please upvote if you like my work and comment your feedback!","fad13ab7":"#### Optional: Checking for P-Value using OLS","cdcb5b9c":"The ultimate success of any multivariate technique, including multiple regression, starts with the selection of the variables to be used in the analysis. Because multiple regression is a dependence technique, which variables to use if a key aspect of it.\n\nWhat should we consider while choosing a feature? \n\n1. Strong Theory: The selection of features should be based principally on conceptual or theoritcal grounds, even when the objective is solely for prediction. \n2. Measurement Error: Measurement Error refers to the degree to which is an accurate and consistent measure of the concept being studied. It can be appraoches by either employing multiple variables to reduce reliance on any single variable or using structural equation modelling (SEM). \n3. Specification Error: The most problematic issue in independent variable selection is specification error, which concerns the inclusion of irrelevent features or the omission of relevant features. Most troublesome is of course, omission of relavant features. When correlation exists between the features, the greater the correlation the greater is the bias introduced in the model. \n\nThumb Rule: When in doubt, include potentially irrelavant variables (they can only confuse interpretation) rather than possibly omitting a relavant variable (which can bias all regression estimates). ","74a6beb2":"I will compare results of the model performance with the features I decided to drop and see which works best but for now I'll drop them as proposed earlier. I will choose to transform features for now, but will also try to use their original format and check the performance of the model. If I see a slight difference, it should be worth it otherwise I'll use the original feature. ","a8faff00":"rfe.ranking_ returns an array with positive integer values to indicate the attribute ranking with a lower score indicating a higher ranking.","11f45666":"Training dataset has a total of 1558 missing values where as Testing dataset has 1615 out of total 3173 missing values. ","2437d767":"Standard Scaler","279743bd":"Lets look at the features with less than 80% missing values, next. ","1c73de0c":"# Imputing\n\nRule of Thumb:\n\n1. Under 10%: Any of the imputation methods can be applied when missing data are this low.\n2. 10 to 20%: The increased presence of missing data makes regression methods most preffered for MCAR data, whereas model-based methods are necessary with MAR missing data. \n3. Over 20%: If it is deemed neccesary to impute missing data with the level is over 20%, then regression method for MCAR situations and model-based methods for MAR missing data. \n\n","d1208a92":"Evaluating with Tuned KernalRidge","f4d8461d":"The impact of a feature to improve the prediction of the dependent feature is related not only to its correlation with the dependent feature but also to its correlation with additional independent variables. Collinearity is the association measured as the correlation between two independent features. Multicollinearity refers to the correlation among the independent features. \n\nAs the collinearity rises, the unique variance explained by each independent feature decreases and the shared prediction percentage rises. Because this shared prediction can count only once, the overall prediction increases much more slowly as independent features with high multicollinearity are added.\n\nHence, to maximise the prediction from a given number of features, the researcher should look for independent features that have low multicollinearity with other independent features but also high correlations with the dependent variable. ","b4bfd048":"# Understanding the Data","1e66f4e3":"Evaluation","f03fcb11":"Confidence Level:\n\n1. Frame the Null Hypothesis \n2. We would like to reject Null Hypothesis (if the obtained P-Value is <alpha we reject null hypothesis)\n4. We need to obtain P-Value (Probability Value)\n5. To obtain the P-Value we need to perform some statistical test\n6. While choosing the statistical test we need to understand, how many variables are there? and whether they are categorical or continous?","546ecdc0":"For all features with p-value greater than alpha can be considered as statistically insignificant as proposed earlier and essentially use this result for feature engineering. ","15628664":"Assumptions for T-test: \n1. **Independent samples**\n2. **Large enough sample size or observations come from a normally-distributed population**\n3. **Variances are equal**","3a66e78a":"I imputed a categorical feature with its mode and a numerical feature with its mean as proposed and used RandomForestRegressor to impute feature 'LotFrontage' and feature knowledge to impute 'FireplaceQu' with a replacement value. \n\nNow that there are no null values left, I can start with some EDA. \n\nThe road I'll take would be understanding the target variable 'SalePrice' first and try to know more about it and then analyse each feature independently and mostly figuring out its impact on the target variable and segmenting its propeties. \n\nThe criteria would be to check if the feature is significant enough for modelling and what will be our preprocessing steps for the given feature. ","74953d7d":"The feature 'LotFrontage' refers to 'Linear feet of street connected to property' according to the data dictionary. In order to impute this feature using a model appraoch, I tried to figure out which features might be significant. I am no expert in real estate, but I used google a lot to try and figure out the features. \n\n1. LotArea: Lot size in square feet\n2. Neighbourhood: Physical locations\n3. GarageCars: Size of garage in car capacity\n4. MSZoning: The general zoning classification\n5. BldgType: Type of dwelling\n6. GarageType: Garage location\n\nI will use model-based approach which is Random Forest Imputation using MissForest since there are a lot of non-metric features and will omit Regression and KNN imputation. ","7dee2c80":"Data Transformation provide the principal means of correcting nonnormality and heteroscedasticity. Skewed distributions can be transformed by taking the square root, log, squared or cubed or even the inverse. Usually negative skewed distributions are best transformed by taking the squared or cubed transformation, whereas the log or square root typically works best of positive skewness. \n\nThumb Rule:\n1. Transformations should be applied to the independent features except in the case of hetroscedasticity. \n2. Hetroscedasticity can only be remedied by transformation of the dependent feature in a dependence relationship.\n3. Transformations may change the interpretation of the feature, for instance log transform translates the relationship into a measure of proportional change (elasticity); always be sure to explore thoroughly the possible interpretations of the transformed feature.\n4. Use features in their original format when profiling or interpreting results. ","2b0b6981":"Training dataset has 1460 rows and 81 columns and Testing dataset has 1459 rows and 80 columns. The missing column is of course the taget column we wish to predict. \n\nIn multivariate study, increased sample sizes produce graeter power for the statistical tests. As simple size increases, it must be decided if the power is too high. By 'too high' it is meant that as simple size increases smaller and smaller effects such as correlation will be found statistically significant, until at very large sample sizes almost any effect is significant. \n\nPower is defined by the probability of correctly rejecting the null hypothesis when it should be rejected. Thus, Power is the probability that statistical significance will be indicated if it is present. \n\nHence, why not set both alpha (probability of rejecting null hypothesis when it is actually true or false positive) and beta (probability of not rejecting null hypothesis when it is actually false) at acceptable levels? Because they are inversely proportional. \n\nThus, reducing alpha reduces the power of the statistical tests hence there should be a balance between the level of alpha and the resulting power.  \n\nHence, likewise large samples over 1000 observations make the statistical tests overly sensitive, often indicating that almost any relationship is statistically significant. \n\nThumb Rule:\n1. A power level of 0.80 should be achieved at the desired significance level. \n2. More stringent significant levels (eg, 0.01 instead of 0.05) require large samples to achieve the desired power level.\n3. An increase in power is most likely achieved by increasing the sample size. ","361731c2":"I noticed that most missing values around the corner of 5-6% which is fair to believe can be imputed with regular mean\/mode imputation with two exceptions of feature 'LotFrontage' with about 16% missing values and 'FirePlaceQu' with about 50%.","56f8c2f4":"Removing outliers from GrLivArea as proposed","108c0934":"Since we are only exploring the data for now, I will combine the two datasets. ","4e40a1d1":"# Significance Testing","d06a625d":"TotalBsmtSF: Total square feet of basement area\n\nObservation: Highly concentrated and follows a linear relationship","9b2dab2f":"TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n\nObservation: As mentioned above, since TotRmsAbvGrd and GrLivArea is correlated, therefore we just need one of them in the final model. Hence, I will consider only GrLivArea. \n\nPre-processing: Drop feature","c440a11b":"GrLivArea: Above grade (ground) living area square feet.\n\nI saw the scatter-plot was overlapping hence chose to plot a kde-plot as well. \n\nObservation: With increase in area per square feet, there is an increase in SalePrice. \n\nPre-processing: Remove 'extreme' outliers (2 observations in the bottom right)","99f58d8d":"# The Curse of Dimensionality\n\nA big misconception in machine learning and data analytics is the confusion between 2 totally different concepts: number of features and number of dimensions. \n\nFor example, consider 3 pipes connected to the same valve and meter to detect the volume of water flowing. Thus the number of features we have in our dataset is 3 (Volume of fluid 1, Volume of fluid 2, Volume of fluid 3). Although we have 3 features in this dataset, we can\neasily see that there is a proportionality or a correlation between them (namely 1x: 10x:\n100x). \n\nThis correlation exists because we have only one control point, i.e we have 3 features but what really matters for machine learning is the degrees of freedom in the dataset. How many different pipes can we control independently of each other? Can we have the first pipe totally opened while the second totally closed? In this system we can\u2019t, by design. So even if we collect the data of the 3 flows, it's redundant information. One flow of information would be enough. The rest can be deduced by proportionality or correlation. Only when we use an independent valve on each pipe, we can say that the number of dimensions is equal to the number of features. But then, we need to try all the different combinations between all the states of all the valves on all the pipes, before we can say that we \u201ccovered\u201d the whole exploration space. If we consider only 2 states per valve: totally open and totally closed. with each additional valve added to the system, we have 2 times more combinations to explore. Each additional pipe, with an additional valve (Or knob) will add an additional dimension to the space, and double the data needed. We would need exponentially more data to cover all the possible combinations. \n\nThat\u2019s a well-known problem in data analytics called **\"The Curse of Dimensionality\"** problem. One math tool to discover the independent dimensions from the different features is **Principal Component Analysis or PCA**. ","7f535968":"#### Optional: Model Refinement Using RFE\n\nRecursive Feature Elimination (RFE) as its title suggests recursively removes features, builds a model using the remaining attributes and calculates model accuracy. RFE is able to work out the combination of attributes that contribute to the prediction on the target variable (or class)\n\nThe goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the less important features are pruned from the the current set of features. This procedure is recursively repeated on the pruned dataset until the desired number of features to select is reached.","030c51dd":"Plotting Residuals Plot","d8b40eed":"Label Encoding","badf69c7":"Statistical methodology:\n\n1. Look at the data frame and find out what are the variables are useful for my analysis.\n2. If there are categorical variables we need to check their responses (Binomial, Nominal, Ordinal)\n3. If it is continuous, we need to check whether the variabe is normally distributed or not \n4. Choose the hypothesis approach \/ choose the right statistical test\n5. Obtain the P-value (Probability Value)\n6. We will check the hypothesis and take a decision on it.\n\nNote: in statistics three words are banned: \n1. 100% \n2. 0% \n3. Accept","192cc20d":"# Importing Dependencies","6642bfc5":"1stFlrSF: First Floor square feet\n\nObservation: As mentioned above, since 1stFlrSF and TotalBsmtSF is correlated, therefore we just need one of them in the final model. Hence, I will consider only TotalBsmtSF. \n\nPre-processing: Drop feature","2c76c2b3":"Training","8d41f77f":"# Detecting and Handling Outliers\n\nIn the previous section, we already identified some outliers based on the characteristics of the relationship with the target feature 'SalePrice' which is the objective of the analysis. The typical approach is to first convert the data to standard scores, which have a mean of 0 and standard deviation of 1 because the values are expressed in a standardized form, comparisons across variables can be made easily. Although, I already observed a few outliers hence I will not perform any outlier detection on any feature. \n\nOutlier Detection is a complex subject and there are some great notebooks on Kaggle for it. \n\nThumb Rule: \n1. For small datasets (80 or below), outliers are generally with Z-score > 2.5.\n2. For larger datasets, threshold is upto 4.","3d8fd8b6":"I think it is fair to say that features which have more than 80% null values can deteriorate the performance of the model by introducing bias and hence I will consider to drop those features such as 'Alley', 'PoolQC', 'Fence' and 'MiscFeature' and hope for no loss of information. ","f2a4ce42":"KernalRidge got R-square: 0.89 and RMSE: 0.29 which is the best out of all the models.\n\nLets see the residual plot. ","680c6a58":"I will use this to select features in the modelling section of this notebook. ","da6a856a":"GarageArea: Size of garage in square feet\n\nObservation: As mentioned above, since GarageCars and GarageArea is correlated, therefore we just need one of them in the final model. Hence, I will consider only GarageCars since its correlation is higher with the target feature 'SalePrice'. \n\nPre-processing: Drop feature","000b9fc8":"# Modelling\n\nIn order to understand the concept of overfitting, there needs to be an understanding of **Degrees of Freedom**.\n\nWhat happens to generalizibility as sample size increases? We can perfectly predict one observation with a single variable, but what about all other observations? \nThere we need to search for the best regression model, one with the highest predictive accuracy for the largest sample. The degree of generalizibility is represented by degrees of freedom, calculated as:\n\nDegrees of freedom (df) = Sample Size - Number of Estimated Parameters\n\nor \n\nDegrees of freedom (df) = N - (Number of Independent features + 1)\n\nThe larger the degrees of freedom, the more generalizable the model. Degrees of Freedom increase for a given sample by reducing the number of independent variables. Thus, objective is to achieve the highest predictive accuracy with the most degrees of freedom. \n\nNo specific guidelines determine how large the degrees of freedom are, just that they are indicative of the generalizibility of the results and give an idea of the overfitting for any regression model. ","f391afbd":"# Transformation","8deb3c9f":"# Exploratory Data Analysis","fc4a80c3":"### Framework\n\n**1. Setting up two competing hypotheses** - Each hypothesis test includes two hypothesis about the population.  One is the null hypothesis, notated as Ho, which is a statement of a particular parameter value.  This hypothesis is assumed to be true until there is evidence to suggest otherwise.  The second hypothesis is called the alternative, or research, hypothesis, notated as Ha.  The alternative hypothesis is a statement of a range of alternative values in which the parameter may fall.  One must also check that any assumptions (conditions) needed to run the test have been satisfied e.g. normality of data, independence, and number of success and failure outcomes.\n\n**2. Set in advanced some level of significance, called alpha.**  This value is used as a probability cutoff for making decisions about the null hypothesis.  As we will learn later, this alpha value represents the probability we are willing to place on our test for making an incorrect decision in regards to rejecting the null hypothesis.  The most common alpha value is 0.05  or 5%. Other popular choices are 0.01 (1%) and  0.1 (10%).\n\n**3. Calculate a test statistic and the p-value (or find rejection region)** Gather sample data and calculate a test statistic where the sample statistic is compared to the parameter value.  The test statistic is calculated under the assumption the null hypothesis is true, and incorporates a measure of standard error and assumptions (conditions) related to the sampling distribution.  Such assumptions could be normality of data, independence, and number of success and failure outcomes. A p-value is found by using the test statistic to calculate the probability of the sample data producing such a test statistic or one more extreme.  The rejection region is found by using alpha to find a critical value; the rejection region is the area that is more extreme than the critical value.\n\n**4. Make a test decision about the null hypothesis -**  In this step we decide to either reject the null hypothesis or decide to fail to reject the null hypothesis.  Notice we do not make a decision where we will accept the null hypothesis. \n\n**5. State an overall conclusion -** Once we have found the p-value or rejection region, and made a statistical decision about the null hypothesis (i.e. we will reject the null or fail to reject the null).  Following this decision, we want to summarize our results into an overall conclusion for our test.","56875691":"# Hi!\n\nTom Peters said in his book 'Thriving on Chaos', \"We are drowing in information, and starved for knowledge\". \n\nToday businesses must be profitable, react quicker, and offer higher quality products and services, and do it all with fewer people and at lower cost. An essential requirement in this process is knowledge. \n\nThe information available for decision making exploded in the recent years, and will continue to do so in the future. How do we make knowledge out of it, is where my inspiraton comes from. \n\nIn this notebook, I will try to scratch the surface of multivariate analysis using regression techniques and statistical means to predict a sale price of a house. \n\nIf you like my work, please upvote and comment your feedback! It means a lot!"}}