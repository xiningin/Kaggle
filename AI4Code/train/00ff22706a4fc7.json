{"cell_type":{"0da89925":"code","38272ce5":"code","ba73d1e9":"code","3eca9edd":"code","73d38ca8":"code","209273f2":"code","5a896d4d":"code","d654f1be":"code","657927be":"code","d2463548":"code","2d84ecbe":"code","9edb660b":"code","648d8cb0":"code","e799522e":"code","8a37c09a":"code","a077e6ac":"code","47c33be4":"code","62a3479f":"code","411c5991":"code","4dc8f093":"markdown","b79e508b":"markdown","829fb805":"markdown","414b0b0b":"markdown","1abe9bd1":"markdown","74b893f3":"markdown","50e3f65b":"markdown","eeb3f998":"markdown","a15c5fdb":"markdown","20240739":"markdown","0fc9389c":"markdown"},"source":{"0da89925":"import sys \nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","38272ce5":"\nfrom sklearn.metrics.pairwise import cosine_similarity,paired_distances ,paired_cosine_distances, pairwise_distances , pairwise_distances_argmin,pairwise_distances_argmin_min\n\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering ,DBSCAN, KMeans","ba73d1e9":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n","3eca9edd":"\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.merge(train_targets,extra_targets,on ='sig_id')\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets.to_list()\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt ","73d38ca8":"train,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv','..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntrain.cp_type.unique()\n\ntrain =  train.loc[train.cp_type=='trt_cp']\n\ntrain.head()\n","209273f2":"train_clus = train.drop(['cp_time','cp_dose','cp_type'],axis=1).copy()\n\n#here we eight target more, the idea is that  trials from the same experiment should have similar targets\n\nTARGETS_WEIGHT = 1000\n\ntrain_clus[target_cols] = train_clus[target_cols] *TARGETS_WEIGHT","5a896d4d":"kmeans=KMeans(n_clusters=3000,max_iter=100)\n\n\nkmeans.fit(train_clus)","d654f1be":"import pickle as pk\n\npk.dump(kmeans,open('first_kmean_try.pk','wb'))\n\nresults_train_clus = train_clus.copy()\nresults_train_clus['kmeans1-clusters'] = kmeans.predict(train_clus)\n\nresults_train_clus['kmeans1-clusters'].value_counts()","657927be":"good_clusters = results_train_clus['kmeans1-clusters'].value_counts().loc[results_train_clus['kmeans1-clusters'].value_counts() < 7].index.to_list()\n\ntrain['kmeans1-clusters'] = results_train_clus['kmeans1-clusters']\n\ngood_cluster_duplicates= train.loc[train['kmeans1-clusters'].isin(good_clusters),['kmeans1-clusters','cp_time','cp_dose'] ].groupby('kmeans1-clusters').apply(lambda x: x.duplicated().sum())\n\nplt.hist(good_cluster_duplicates)","d2463548":"plt.hist(results_train_clus['kmeans1-clusters'].value_counts(),bins=20)","2d84ecbe":"distance_matrix= pairwise_distances(train_clus,metric='l1')\n\ndistance_matrix.shape\n\ndistance_matrix.tofile('distance_matrix_l1_pure')\n\ndistance_matrix\n","9edb660b":"\ntrain_partitions ={}\n\nfor key,group in train.groupby(['cp_dose','cp_time']):\n    \n    train_partitions[key]=group.drop(['cp_dose','cp_time'],axis=1)\n\n","648d8cb0":"SAME_TIME_DOSE_PENALIZATION = 2000\npenalization_same_group= train.apply(lambda x: SAME_TIME_DOSE_PENALIZATION * train.index.isin(train_partitions[tuple(x[['cp_dose','cp_time']].values.tolist())].index),axis=1)\n\n\n\n","e799522e":"penalization_same_group.values","8a37c09a":"distance_matrix = distance_matrix.reshape((len(train),len(train)))\n\nmat_penalization = np.concatenate(penalization_same_group.values).reshape((len(train),len(train))) - 2000 * np.eye(len(train),len(train))\n\ntotal_matrix = distance_matrix + mat_penalization\n\ntotal_matrix.tofile('distance_matrix_l1_WsamegroupPenalization2')\n","a077e6ac":"\nresults_train_clus = train_clus.copy()\nresults_train_clus['aggclus1-clusters'] = aggclus.fit_predict(total_matrix)\n","47c33be4":"results_train_clus = pd.read_csv('..\/input\/moafoldsmiscelanea\/aggclus3_results.csv')","62a3479f":"\nresults_train_clus['aggclus1-clusters'].value_counts()","411c5991":"plt.hist(results_train_clus['aggclus1-clusters'].value_counts(),bins=20)","4dc8f093":"Unluckily I can not run the fll code on a kernel and each distance matrix file weights 3 GB so please tell me in the comments if you know of a way to create the distance matrix sequentially, or exploit some trick. ","b79e508b":"BTW Upvote if you find this useful  :)\n","829fb805":"Here I show how to obtain 2 possible groupings using different cluster techiniqes\n    - kmeans\n    - hierarchical clustering\n(Every experiment should include at most 6 trials with different time-dose combinations.)\nI then proceed to use the latter because of its nicer separation properties.\nI encourage those who come after me to try to improve this results and share their thoughts.","414b0b0b":"As you can well see hierarchical clustering using the distance matrix with the same time-dose penalization  gives a better distribution, in the sense that the majority of clusters ( experiments) involve 5 or 6 sig-ids (trials)","1abe9bd1":"Since the dawn of MOA countless souls have wondered  and mused upon the te separation between experiments. As dusk creeps on this competition, in a last effort to improve the correlation between CV and leaderboad score, a light shines in the darkness.\n","74b893f3":"Here we create a same time-dose penalization to add to the whole distance matrix,","50e3f65b":"## distance matrix creation","eeb3f998":"# Hierarchical clustering","a15c5fdb":"P.S. : check my kernel on how to use this for Group kfolding","20240739":"Happy Kaggling !","0fc9389c":"unluckily the number of sig_ids per clusters does not peak at 5 or 6 as expected."}}