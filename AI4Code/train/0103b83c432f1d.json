{"cell_type":{"0b058ea5":"code","f2a19212":"code","1249c4c4":"code","3f8a11d6":"code","b2a1b07e":"code","b5b389bc":"code","6673d78f":"code","1e22b50b":"code","0aea44c9":"code","91bcff7c":"code","7f8115ab":"code","ee199edb":"code","3d8750e0":"code","e6fb0f78":"code","51dd8fed":"code","14022914":"code","65b00775":"code","e11e5f47":"code","2ba5bd6e":"markdown","e9e1c156":"markdown","e5ef1759":"markdown"},"source":{"0b058ea5":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Jupyter Specific Packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom IPython.display import display\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv(\"..\/input\/train_V2.csv\")#.sample(20000)\ntest = pd.read_csv(\"..\/input\/test_V2.csv\")#.sample(20000)","f2a19212":"train.head()","1249c4c4":"for col in [\"matchId\",\"Id\",\"groupId\"]:\n    print(\"Does {} Feature Overlap Between Train\/Test Set?         {}\".format(col, any(np.intersect1d(test[col].unique(), train[col].unique()))))","3f8a11d6":"matchcount = train.matchId.nunique()\nprint(\"Number of unique matches: {}\".format(train.matchId.nunique()))\nprint(\"Train Shape Before: {} Rows, {} Cols\".format(*train.shape))\ntrain = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.50):]),:]\nprint(\"Train Shape After: {} Rows, {} Cols\".format(*train.shape))","b2a1b07e":"# Label Encoder\nfrom sklearn import preprocessing\n# Encoder:\nlbl = preprocessing.LabelEncoder()\nfor col in ['matchType']:\n    lbl.fit(train[col])\n    train[col] = lbl.transform(train[col])\n    test[col] = lbl.transform(test[col])","b5b389bc":"id_cols = [\"Id\",\"groupId\",\"matchId\"]\nexclude = [\"Id\",\"groupId\",\"matchId\"]\ntrainlen = train.shape[0]\n# LGBM Dataset\nmatchcount = train.matchId.nunique()\n\ntraining = train.loc[train.matchId.isin(sorted(train.matchId.unique())[:int(matchcount* 0.85)]),\n                    [x for x in train.columns if x not in exclude]]\nprint(\"Training Shape: {} Rows, {} Cols\".format(*training.shape))\nvalidating = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.85):]),\n                       [x for x in train.columns if x not in exclude]]\nprint(\"Validating Shape: {} Rows, {} Cols\".format(*validating.shape))\n\ntrain_y = training.winPlacePerc\ntraining.drop(\"winPlacePerc\", axis =1, inplace=True)\nvalid_y = validating.winPlacePerc\nvalidating.drop(\"winPlacePerc\", axis =1, inplace=True)\n                                                             \nlgb_train = lgb.Dataset(training, train_y,feature_name = \"auto\")\nlgb_valid = lgb.Dataset(validating, valid_y, feature_name = \"auto\")\ndel training, validating","6673d78f":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_boost_round' : 5000\n#     \"learning_rate\": 0.01,\n#     \"num_leaves\": 180,\n#     \"feature_fraction\": 0.50,\n#     \"bagging_fraction\": 0.50,\n#     'bagging_freq': 4,\n#     \"max_depth\": -1,\n#     \"reg_alpha\": 0.3,\n#     \"reg_lambda\": 0.1,\n#     #\"min_split_gain\":0.2,\n#     \"min_child_weight\":10,\n#     'zero_as_missing':True\n                }","1e22b50b":"stage = 'model training'\ngbm = lgb.train(lgbm_params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=[lgb_train, lgb_valid],\n                feature_name='auto',\n                early_stopping_rounds=50,\n                verbose_eval=250\n                )\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(gbm, max_num_features=25, ax=ax)\nplt.title(\"Light GBM Feature Importance\\n\")\nplt.show()","0aea44c9":"pred = gbm.predict(test.loc[:,[x for x in test.columns if x not in id_cols]])\ntest['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\naux = test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ntest_sub = test.merge(aux, how='left', on=['matchId','groupId'])\n    \nsubmission = test_sub[['Id', 'winPlacePerc']]\nsubmission.to_csv('PubGG_LGBM.csv', index=False)\ndisplay(submission.head())","91bcff7c":"notcat = [\"assists\",\"boosts\",\"damageDealt\",\"DBNOs\",\"heals\",\"headshotKills\",\"heals\",\"killPlace\",\"killPoints\",\"kills\",\n         \"killStreaks\",\"longestKill\",\"maxPlace\",\"numGroups\",\"revives\",\"rideDistance\",\"roadKills\",\"swimDistance\",\n         \"teamKills\",\"vehicleDestroys\",\"walkDistance\",\"weaponsAcquired\",\"winPoints\"]","7f8115ab":"y = train.winPlacePerc.copy()\ntrain.drop(\"winPlacePerc\",axis =1, inplace=True)","ee199edb":"def agg_dataset(dataset):\n    for id_col in id_cols:\n        agg_features = dataset.groupby(id_col).agg({k:[\"sum\",\"mean\",\"std\"] for k in\n                                [\"killPlace\",\"walkDistance\",\"numGroups\",\"maxPlace\",\"kills\",\"longestKill\",\"weaponsAcquired\"]})\n        agg_features.columns = pd.Index([\"{}_agg_\".format(id_col) + e[0] +\"_\"+ e[1] for e in agg_features.columns.tolist()])\n        dataset = pd.merge(dataset,agg_features, on = id_col, how= \"left\")\n    return dataset","3d8750e0":"train = agg_dataset(train)\ntest = agg_dataset(test)","e6fb0f78":"# Remove Columns with 95%+ Missing\nmissing = round(train.isnull().sum()\/ train.shape[0]*100).reset_index().rename({\"index\":\"columns\",0:\"missing\"}, axis =1 )\nhigh_missing_columns = missing.loc[missing.missing > 65, \"columns\"]\nprint(\"Columns to remove (65% missing Values and Over)\\n\", list(high_missing_columns))\ntrain.drop(high_missing_columns,axis =1, inplace= True)\ntest.drop(high_missing_columns,axis =1, inplace= True)","51dd8fed":"train = pd.concat([train.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n\ntraining = train.loc[train.matchId.isin(sorted(train.matchId.unique())[:int(matchcount* 0.85)]),\n                    [x for x in train.columns if x not in exclude]]\nprint(\"Training Shape: {} Rows, {} Cols\".format(*training.shape))\nvalidating = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.85):]),\n                       [x for x in train.columns if x not in exclude]]\nprint(\"Validating Shape: {} Rows, {} Cols\".format(*validating.shape))\n\ntrain_y = training.winPlacePerc\ntraining.drop(\"winPlacePerc\", axis =1, inplace=True)\nvalid_y = validating.winPlacePerc\nvalidating.drop(\"winPlacePerc\", axis =1, inplace=True)\n                                                             \nlgb_train = lgb.Dataset(training, train_y,feature_name = \"auto\")\nlgb_valid = lgb.Dataset(validating, valid_y, feature_name = \"auto\")\n# del training, validating","14022914":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_boost_round' : 5000\n#     \"learning_rate\": 0.01,\n#     \"num_leaves\": 180,\n#     \"feature_fraction\": 0.50,\n#     \"bagging_fraction\": 0.50,\n#     'bagging_freq': 4,\n#     \"max_depth\": -1,\n#     \"reg_alpha\": 0.3,\n#     \"reg_lambda\": 0.1,\n#     #\"min_split_gain\":0.2,\n#     \"min_child_weight\":10,\n#     'zero_as_missing':True\n                }\n\nstage = 'model training'\ngbm = lgb.train(lgbm_params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=[lgb_train, lgb_valid],\n                feature_name='auto',\n                early_stopping_rounds=50,\n                verbose_eval=250\n                )\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(gbm, max_num_features=25, ax=ax)\nplt.title(\"Light GBM Feature Importance\\n\")\nplt.show()","65b00775":"pred = gbm.predict(test.loc[:,[x for x in test.columns if x not in id_cols]])\ntest['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\naux = test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ntest_sub = test.merge(aux, how='left', on=['matchId','groupId'])\n    \nsubmission = test_sub[['Id', 'winPlacePerc']]\nsubmission.to_csv('AGG_PubGG_LGBM.csv', index=False)","e11e5f47":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))\nsubmission.head()","2ba5bd6e":"## Add Some Aggregate Features","e9e1c156":"## Pre-Processing..\n\nSince MatchId, Id, and GroupId do not intersect between the train and test set, this model should exclude them as features, and instead create aggregating features to enable the model to generalize these groups.","e5ef1759":"## PlayerUnknown's AI\n_By Nick Brooks, October 2018_"}}