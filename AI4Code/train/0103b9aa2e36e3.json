{"cell_type":{"66658a85":"code","af5d7216":"code","c50952f2":"code","e8b874c0":"code","735f0b0d":"code","e0716e65":"code","ff59ecc1":"code","38315382":"code","ad787b11":"code","ce9e21e3":"code","b91093bc":"code","22079b56":"code","bc25cc79":"code","439365a2":"code","9a272d50":"code","3bde879c":"code","ea83ca63":"code","4d4aee1a":"code","fc9a384f":"code","25bd4424":"code","dddd7cab":"code","bd2aace4":"code","c47da31c":"code","3d5bb1ed":"markdown","0295d6bf":"markdown","01175a87":"markdown","50040a46":"markdown","aa3628e7":"markdown","1b5f4fd8":"markdown","41088e8d":"markdown","c22c51b3":"markdown","0a63fe69":"markdown","079a3ff1":"markdown","320db41e":"markdown","04c59368":"markdown","1fff168a":"markdown","52d96fef":"markdown","240458f3":"markdown","34ce610c":"markdown","efa208b5":"markdown","6e74d3ea":"markdown","8eae7f0e":"markdown","1d56d5ed":"markdown"},"source":{"66658a85":"%%html \n<img src=\"https:\/\/miro.medium.com\/max\/2319\/0*NBVi7M3sGyiUSyd5.png\"\/>","af5d7216":"# we need to import category_encoders\ntry: \n    import category_encoders\nexcept:\n    !pip install category_encoders\n    import category_encoders","c50952f2":"import pandas as pd\n\ndata = {'Temperature': ['Hot', 'Cold', 'Very Hot', 'Warm', 'Hot', 'Warm', 'Warm', 'Hot', 'Hot', 'Cold'],\n        'Color': ['Red', 'Yellow', 'Yellow', 'Blue', 'Red', 'Blue', 'Blue', 'Yellow', 'Red', 'Yellow'],\n        'Target': [1,1,1,0,1,0,1,0,1,1]}\n\ndf = pd.DataFrame(data, columns=['Temperature','Color','Target'])\ndf.head()","e8b874c0":"from sklearn.preprocessing import OneHotEncoder\n\nohc = OneHotEncoder()\nohe = ohc.fit_transform(df['Temperature'].values.reshape(-1,1)).toarray()\n\ndfOneHot = pd.DataFrame(ohe, columns=['Temp_'+str(ohc.categories_[0][i]) for i in range(len(ohc.categories_[0]))])\ndfoh = pd.concat([df, dfOneHot], axis=1)\ndfoh.head()","735f0b0d":"df_ohe = pd.get_dummies(df, prefix=[\"Temp\"], columns=[\"Temperature\"])\ndf_ohe.head()","e0716e65":"from sklearn.preprocessing import LabelEncoder\n\ndf_le = df.copy()\ndf_le[\"Temp_label_encoded\"] = LabelEncoder().fit_transform(df[\"Temperature\"])\ndf_le.head()","ff59ecc1":"df_le['Temp_factorize_enocded'] = pd.factorize(df['Temperature'])[0].reshape(-1,1)\ndf_le.head()","38315382":"Temp_dict = {\n    'Cold' : 1,\n    'Warm' : 2,\n    'Hot' : 3,\n    'Very Hot' : 4\n    }\n\ndf_oe = df.copy()\ndf_oe['Temp_ordinal_encoded'] = df['Temperature'].map(Temp_dict)\ndf_oe.head()","ad787b11":"import category_encoders as ce\n\nencoder = ce.HelmertEncoder(cols=['Temperature'], drop_invariant=True)\ndfh = encoder.fit_transform(df['Temperature'])\ndfhe = pd.concat([df, dfh], axis=1)\ndfhe.head()","ce9e21e3":"encoder = ce.BinaryEncoder(cols=['Temperature'], drop_invariant=True)\ndfb = encoder.fit_transform(df['Temperature'])\ndfbin = pd.concat([df, dfb], axis=1)\ndfbin.head()","b91093bc":"fe = df.groupby('Temperature').size()\/len(df)\n\ndffe = df.copy()\ndffe['Temp_freq_encoded'] = df['Temperature'].map(fe)\ndffe.head()","22079b56":"# Basic Mean Encoding\nme = df.groupby('Temperature')['Target'].mean()\n\ndfme = df.copy()\ndfme['Temp_mean_encoded'] = df['Temperature'].map(me)\ndfme.head()","bc25cc79":"# Mean Encoding with Smoothing\n\n# global mean\nmean = df['Target'].mean()\n\n# count and mean of each value group\nagg = df.groupby('Temperature')['Target'].agg(['count','mean'])\ncounts = agg['count']\nmeans = agg['mean']\nweight = 100\n\n# smoothed mean\nsmooth = (counts * means + weight * mean) \/ (counts + weight)\n\ndfme.loc[:, 'Temp_smooth_mean_encoded'] = df['Temperature'].map(smooth)\ndfme.head()","439365a2":"import numpy as np\n\n# probability for target = 1 i.e Good\ndf_woe = df.groupby('Temperature')['Target'].mean()\ndf_woe = pd.DataFrame(df_woe)\ndf_woe = df_woe.rename(columns = {'Target' : 'Good'})\n\n# probability for target != 1 i.e Bad\ndf_woe['Bad'] = 1 - df_woe['Good']\n# adding small value for avoiding Divide by Zero exception\ndf_woe['Bad'] = np.where(df_woe['Bad'] == 0, 0.000001, df_woe['Bad'])\n\n# compute WoE\ndf_woe['WoE'] = np.log(df_woe['Good']\/df_woe['Bad'])\ndf_woe","9a272d50":"df['Temp_WoE'] = df['Temperature'].map(df_woe['WoE'])\ndf.head()","3bde879c":"# probability for target = 1 i.e Good\ndf_pre = df.groupby('Temperature')['Target'].mean()\ndf_pre = pd.DataFrame(df_woe)\ndf_pre = df_pre.rename(columns = {'Target' : 'Good'})\n\n# probability for target != 1 i.e Bad\ndf_pre['Bad'] = 1 - df_pre['Good']\n# adding small value for avoiding Divide by Zero exception\ndf_pre['Bad'] = np.where(df_pre['Bad'] == 0, 0.000001, df_pre['Bad'])\n\n# compute WoE\ndf_pre['PR'] = df_pre['Good']\/df_pre['Bad']\ndf_pre","ea83ca63":"df['Temp_PRE'] = df['Temperature'].map(df_pre['PR'])\ndf.head()","4d4aee1a":"df.drop(columns=['Temp_WoE', 'Temp_PRE'], inplace=True)","fc9a384f":"encoder = ce.BackwardDifferenceEncoder(cols=['Color'])\ndfb = encoder.fit_transform(df['Color'])\ndfBD = pd.concat([df, dfb], axis=1)\ndfBD.head()","25bd4424":"encoder = ce.LeaveOneOutEncoder(cols=['Color'])\ndfl = encoder.fit_transform(df['Color'], df['Target'])\ndfLOO = pd.concat([df, dfl], axis=1)\ndfLOO.head()","dddd7cab":"encoder = ce.JamesSteinEncoder(cols=['Color'])\ndfj = encoder.fit_transform(df['Color'], df['Target'])\ndfJS = pd.concat([df, dfj], axis=1)\ndfJS.head()","bd2aace4":"encoder = ce.MEstimateEncoder(cols=['Color'])\ndfm = encoder.fit_transform(df['Color'], df['Target'])\ndfmE = pd.concat([df, dfm], axis=1)\ndfmE.head()","c47da31c":"encoder = ce.CatBoostEncoder(cols=['Color'])\ndfc = encoder.fit_transform(df['Color'], df['Target'])\ndfCB = pd.concat([df, dfc], axis=1)\ndfCB.head()","3d5bb1ed":"<a id='14'>14. M-estimator Encoding<\/a>\n\nM-Estimate Encoder is a simplified version of Target Encoder. It has only one hyper-parameter \u2014 m, which represents the power of regularization. The higher the value of m results, into stronger shrinking. Recommended values for m is in the range of 1 to 100.","0295d6bf":"<a id='6'>6. Frequency Encoding<\/a>\n\nFrequency Encoding **is a way to utilize the frequency of the categories as labels**. In the cases where the frequency is related somewhat with the target variable, it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data","01175a87":"<a id='4'>4. Helmert Encoding<\/a>\n\nIn Helmert encoding, **the mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels.**","50040a46":"<a id='5'>5. Binary Encoding<\/a>\n\nBinary encoding **converts a category into binary digits. Each binary digit creates one feature column.** If there are n unique categories, then binary encoding results in the only log(base 2)\u207f features. In this example, we have four features; thus, the total number of the binary encoded features will be three features.\n\n![Binary Encoding](https:\/\/miro.medium.com\/max\/3552\/1*VuNZWUX6b7GUGB0zRu2zrA.png)","aa3628e7":"<a id='1'>1. One Hot Encoding<\/a>\n\n\nOne Hot Encoding is very popular. **In this method, we map each category to a vector that contains 1 and 0 denoting the presence or absence of the feature**. We can represent all categories by N-1 (N= No of Category) as that is sufficient to encode the one that is not included. Usually, for *Regression, we use N-1 (drop first or last column of One Hot Coded new feature )*, but for *classification, the recommendation is to use all N columns* without as most of the tree-based algorithm builds a tree based on all available\n\nOne Hot Encoding can be done by pandas `get_dummies` as well as sklearn's `OneHotEncoder`","1b5f4fd8":"<a id='7'>7. Mean Encoding\/Target Encoding<\/a>\n\nMean Encoding or Target Encoding is one viral encoding approach plays prominent role in Kaggle Competitions. **Mean encoding is similar to label encoding, except here labels are correlated directly with the target**. For example, in mean target encoding for each category in the feature label is decided with the mean value of the target variable on a training data.\n\n*Note:*\n\nThey tend to OverFit, be cautious in using Mean Encoding\n\n![Mean Enoding](https:\/\/miro.medium.com\/max\/9247\/1*iiM9g-qCa-Vff_HAFk-ppQ.png)","41088e8d":"**<u> instead of sklearn's OHE, pandas get_dummies is easy <\/u>**","c22c51b3":"**There is some bug in the HashEncoder**","0a63fe69":"<a id='3'>3. Ordinal Encoding<\/a>\n\nWe do Ordinal encoding **to ensure the encoding of variables retains the ordinal nature of the variable**. This is reasonable only for ordinal variables. *This encoding looks almost similar to Label Encoding* but slightly different as Label coding would not consider whether variable is ordinal or not and it will assign sequence of integers.\n\n*Example:*\nIf we consider in the temperature scale as the order, then the ordinal value should from cold to \u201cVery Hot. \u201c Ordinal encoding will assign values as ( Cold(1) <Warm(2)<Hot(3)<\u201dVery Hot(4)).","079a3ff1":"<a id='10'>10. Hashing Encoding<\/a>\n\nIn Feature Hashing, a vector of categorical variables gets converted to a higher dimensional space of integers, where the distance between two vectors of categorical variables in approximately maintained the transformed numerical dimensional space","320db41e":"> **Big Thanks to the article: https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02**","04c59368":"<a id='15'>15. CatBoost Encoding<\/a>\n\nThis is very similar to leave-one-out encoding, but calculates the values \u201con-the-fly\u201d. ","1fff168a":"<a id='9'>9. Probability Ratio Encoding<\/a>\n\nProbability Ratio Encoding is similar to Weight Of Evidence(WoE), with the only difference is the only ratio of good and bad probability is used instead of logarithmic the ratio of good and bad probabilies","52d96fef":"<a id='13'>13. James-Stein Encoding<\/a>\n\nFor feature value, James-Stein estimator returns a weighted average of:\n1. The mean target value for the observed feature value.\n2. The mean target value (regardless of the feature value).\n\nThe James-Stein encoder shrinks the average toward the overall average. It is a target based encoder. James-Stein estimator has, however, one practical limitation \u2014 it was defined only for normal distributions.","240458f3":"<a id='8'>8. Weight of Evidence Encoding<\/a>\n\nWeight of Evidence (WoE) **is a measure of the \u201cstrength\u201d of a grouping technique to separate good and bad. ** \n\nWoE is well suited for Logistic Regression because the Logit transformation is simply the log of the odds, i.e., ln(P(Goods)\/P(Bads))","34ce610c":"<a id='11'>11. Backward Difference Encoding<\/a>\n\nSimilar to Helmert Encoding. In backward difference coding, **the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level**.","efa208b5":"# Categorical Variable Encoding\n\n1. [One Hot Encoding](#1)\n2. [Label Encoding](#2)\n3. [Ordinal Encoding](#3)\n4. [Helmert Encoding](#4)\n5. [Binary Encoding](#5)\n6. [Frequency Encoding](#6)\n7. [Mean Encoding\/Target Encoding](#7)\n8. [Weight of Evidence Encoding](#8)\n9. [Probability Ratio Encoding](#9)\n10. [Hashing](#10)\n11. [Backward Difference Encoding](#11)\n12. [Leave One Out Encoding](#12)\n13. [James-Stein Encoding](#13)\n14. [M-estimator Encoding](#14)\n15. [CatBoost Encoding](#15)","6e74d3ea":"`\nencoder = ce.HashingEncoder(cols=['Color'])\ndfH = encoder.fit_transform(df['Color'])\ndfHash = pd.concat([df, dfH], axis=1)\ndfHash.head()\n`","8eae7f0e":"<a id='12'>12. Leave One Out Encoding<\/a>\n\nThis is very similar to target encoding but excludes the current row\u2019s target when calculating the mean target for a level to reduce the effect of outliers.","1d56d5ed":"<a id='2'>2. Label Encoding<\/a>\n\nIn Label Encoding, **each category is assigned a value from 1 through N (here N is the number of categories for the feature**. One major issue with this approach is there is no relation or order between these classes, but the algorithm might consider them as some order, or there is some relationship. \n\nLabel Encoding can be done by pandas `factorize` as well as sklearn's `LabelEncoder`"}}