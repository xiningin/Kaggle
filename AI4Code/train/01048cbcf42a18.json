{"cell_type":{"a1df3321":"code","3d9d08d1":"code","b95281ad":"code","f7738d6a":"code","3872b6a4":"code","b38077ea":"code","3c343c7e":"code","c88bdfd2":"code","b9e4e1a6":"code","65389c9f":"code","fab6d657":"code","8fe1a6ab":"code","872b6ddc":"code","6a51f071":"code","05ba81a2":"code","218b0608":"code","9f675185":"code","439634d9":"code","37a8588d":"code","674909fa":"code","ac9f5ade":"code","163cc151":"code","3b65aa6a":"code","cbbded22":"code","ee47456a":"code","c28cdfa7":"code","86080de8":"code","244860ed":"code","f1cf8b03":"code","a238be9c":"code","26dda268":"code","c4636362":"code","8906f479":"code","b052f73d":"code","7fd4d3b2":"code","d2ea5c08":"code","5763f057":"markdown","adb99935":"markdown","3125836f":"markdown","5d7746c7":"markdown","4e4aadd3":"markdown"},"source":{"a1df3321":"#!pip install -q focal_loss\n#!pip install -q tensorflow_addons","3d9d08d1":"import tensorflow as tf\nimport numpy as np \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization,Resizing,RandomFlip,RandomRotation,RandomZoom\nfrom tensorflow.keras.callbacks import Callback,EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\nimport sklearn\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score,classification_report,confusion_matrix\nimport seaborn as sns\nimport os,os.path\nimport glob\nimport tensorflow_datasets as tfds","b95281ad":"directory1 = '\/kaggle\/input\/food41\/images\/'","f7738d6a":"AUTO = tf.data.experimental.AUTOTUNE\nimg_height = 150\nimg_width = 150\ninput_shape = (img_height,img_width,3)\nrand_int = 42\n\ntrain = tf.keras.preprocessing.image_dataset_from_directory(directory1,\n  \n  seed=rand_int,validation_split = 0.25,subset='training',\n  image_size=(img_height, img_width),\n  batch_size=64,shuffle=True)\n\ntest = tf.keras.preprocessing.image_dataset_from_directory(\n  directory1,\n  seed=rand_int,validation_split = 0.25,subset='validation',\n  image_size=(img_height, img_width),\n  batch_size=64, shuffle=True)\n\n\n","3872b6a4":"class_names = train.class_names\nprint(class_names)\nprint('No of classes: ',len(class_names))\nnum_classes = len(class_names)\n","b38077ea":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(25,25 ))\nfor images, labels in train.take(1):\n      for i in range(8):\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","3c343c7e":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(25,25 ))\nfor images, labels in test.take(1):\n      for i in range(8):\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","c88bdfd2":"def image_augmentation(image,label):\n    rand_seed = 42\n    \n    image = tf.cast(image,'float32')\n    image = tf.image.random_brightness(image, max_delta = 0.2,seed=rand_seed)\n    image = tf.image.random_contrast(image, 0.9, 1.1, seed = rand_seed)\n    image = tf.image.random_flip_left_right(image, seed=rand_seed)\n    image = tf.image.random_flip_up_down(image, seed = rand_seed)\n    image = image\/255.0\n    return image,label","b9e4e1a6":"def resize_and_rescale(image, label):\n    image = tf.cast(image,'float32')\n    image = image\/255.0\n    return image,label   ","65389c9f":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train.prefetch(AUTOTUNE).map(resize_and_rescale,num_parallel_calls=AUTOTUNE)\n\ntest_ds = test.prefetch(AUTOTUNE).map(resize_and_rescale,num_parallel_calls=AUTOTUNE)\n\n","fab6d657":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(25,25 ))\nfor images, labels in train_ds.take(1):\n    plt.suptitle('Augmented Images')\n    for i in range(8):\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].numpy())\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")\n    break","8fe1a6ab":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 100\nimage_size = 76  # We'll resize input images to this size\npatch_size = 12  # Size of the patches to be extract from the input images\nnum_patches = (image_size \/\/ patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier","872b6ddc":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","6a51f071":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","05ba81a2":"plt.figure(figsize=(4, 4))\nfor image,label in train_ds.take(1):\n  k = np.random.randint(1,10)\n  plt.imshow(image[k].numpy())\n  plt.axis(\"off\")\n\n  resized_image = tf.image.resize(\n      tf.convert_to_tensor([image[k]]), size=(image_size, image_size)\n  )\n  patches = Patches(patch_size)(resized_image)\n  print(f\"Image size: {image_size} X {image_size}\")\n  print(f\"Patch size: {patch_size} X {patch_size}\")\n  print(f\"Patches per image: {patches.shape[1]}\")\n  print(f\"Elements per patch: {patches.shape[-1]}\")\n\n  n = int(np.sqrt(patches.shape[1]))\n  plt.figure(figsize=(4, 4))\n  for i, patch in enumerate(patches[0]):\n      ax = plt.subplot(n, n, i + 1)\n      patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n      plt.imshow(patch_img.numpy())\n      plt.axis(\"off\")","218b0608":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","9f675185":"pretrained = tf.keras.applications.ResNet152V2( include_top=False, weights='imagenet')\npretrained.trainable = False\n#pretrained.summary()","439634d9":"last_layer = pretrained.layers[-1].output\nx = layers.Conv2D(256,(1,1),activation='relu')(last_layer)\nx = layers.Conv2D(128,(1,1),activation='relu')(x)\nx = layers.Conv2D(3,(1,1),activation='relu')(x)\nx = tf.keras.layers.UpSampling2D(size = (5,5))(x)\nx = tf.keras.layers.UpSampling2D(size = (5,5))(x)\n\n\nx = layers.experimental.preprocessing.Resizing(image_size, image_size)(x)\n\nconv_block = tf.keras.Model(inputs = pretrained.input, outputs = x)","37a8588d":"input_shape = (150,150,3)\ninputs = layers.Input(shape=input_shape)\n\ndef transformer_block():\n    \n    conv = conv_block(inputs)\n    \n    \n    # Create patches.\n    patches = Patches(patch_size)(conv)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n   \n    \n    return features","674909fa":"output = transformer_block()\nlogits = layers.Dense(num_classes,activation='softmax')(output)","ac9f5ade":"model = keras.Model(inputs=inputs, outputs=logits)","163cc151":"#tf.keras.utils.plot_model(model)","3b65aa6a":"model.summary()","cbbded22":"optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)","ee47456a":"model.compile(optimizer=optimizer,loss=keras.losses.SparseCategoricalCrossentropy(),metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"), \n                                                                                                             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),],)","c28cdfa7":"history = model.fit( train_ds ,epochs=100, shuffle=True) ","86080de8":"model.evaluate(test_ds,return_dict = True, verbose=0)","244860ed":"model.save('.\/food101_resVIT')","f1cf8b03":"model = tf.keras.models.load_model('.\/food101_resVIT')","a238be9c":"fig, ax = plt.subplots(1, 2, figsize=(16, 5))\nax = ax.ravel()\n\nfor i, met in enumerate([ \"accuracy\", \"loss\"]):\n    ax[i].plot(history.history[met])\n    ax[i].set_title(\"Model {}\".format(met))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(met)\n    ","26dda268":"def show_batch(image_batch, label_batch,predictions):\n    \n    plt.figure(figsize=(20, 20))\n    for n in range(9):\n        ax = plt.subplot(3, 3, n + 1)\n        plt.imshow(image_batch[n] )\n        \n        pred_label = class_names[np.argmax(predictions[n])]\n        label = class_names[label_batch[n]]\n        if  pred_label == label :\n            plt.title('Correct Prediction.\\nPredicted Class={}\\n Correct Class={}'.format(pred_label,label))\n        else:\n            plt.title('Wrong Prediction.\\nPredicted Class={}\\n Correct Class={}'.format(pred_label,label))\n            \n        plt.axis(\"off\")\n        \n        ","c4636362":"image_batch, label_batch = next(iter(train_ds))\npredictions = model.predict(train_ds)\n\nshow_batch(image_batch.numpy(), label_batch.numpy(),predictions)","8906f479":"image_batch, label_batch = next(iter(test_ds))\npredictions = model.predict(test_ds)\n\nshow_batch(image_batch.numpy(), label_batch.numpy(),predictions)","b052f73d":"plt.figure(figsize=(4, 4))\nfor image,label in train_ds.take(1):\n  k = np.random.randint(1,10)\n  image = conv_block(image)\n  plt.imshow(image[k].numpy())\n  plt.axis(\"off\")","7fd4d3b2":"plt.figure(figsize=(4, 4))\nfor image,label in test_ds.take(1):\n  k = np.random.randint(1,10)\n  image = conv_block(image)\n  plt.imshow(image[k].numpy())\n  plt.axis(\"off\")","d2ea5c08":"#Test Dataset\ntest_y = []\nfor sample in (test_ds):\n    test_y.append(sample[1])\ntest_y = np.asarray(test_y)\na= model.predict(test_ds)\nprediction = np.argmax(a,axis=-1)\ntest_y = test_y.flatten()\nf1_score = sklearn.metrics.f1_score(test_y,prediction,average='macro')\nbal_acc = sklearn.metrics.balanced_accuracy_score(test_y, prediction)\npre = precision_score(test_y,prediction,average='macro')\nrec = recall_score(test_y,prediction,average='macro')\nconfusion = confusion_matrix(test_y, prediction)\nclass_rep = classification_report(test_y,prediction)\nprint(test_y.shape , prediction.shape)\nprint('balanced accuracy =',bal_acc)\nprint('F1 score = ',f1_score)\nprint('Precision =',pre)\nprint('Recall =',rec)\nprint('Classification Report =\\n',class_rep)\nprint('Confusion Matrix =\\n',confusion)\nplt.figure(figsize=(12, 6))\nplt.title('####  THE CONFUSION MATRIX OF THE MODEL WITH TESTING DATA ####')\nsns.heatmap(confusion, annot = True, fmt = 'g' ,vmin = 0, cmap = 'Blues')\nprint('\\n')","5763f057":"## Multi Layer Perceptron Block","adb99935":"## Transformer Block","3125836f":"## Patch Encoding Block","5d7746c7":"## Patch Creation Block","4e4aadd3":"## Hyperparameters"}}