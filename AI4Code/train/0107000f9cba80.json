{"cell_type":{"2b381092":"code","e82f649e":"code","230bc10a":"code","f3f33fd5":"code","397e817d":"code","82a44b16":"markdown","746ba706":"markdown","ea2dfd72":"markdown","5e0e0ea5":"markdown","8d0627a4":"markdown","f3f997cd":"markdown"},"source":{"2b381092":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e82f649e":"#Below are three exploratory functions.\n\ndef col_types(df, target, limit=10): \n    '''\n    This function distinguishes three different types of data:\n        - Numerical data\n        - Categorical data with low cardinality (< lim)\n        - Categorical data with hight cardinality (> lim)\n    Cardinality is the number values the categorical data takes.\n    '''\n    if target in df.columns:\n        df.drop(target, axis=1, inplace=True)\n    #\n    num_cols = [p for p in df.columns \n                if df[p].dtype in ['int64','float64']]\n    cat_cols_low  = [p for p in df.columns if (df[p].dtype == 'object' \n                                           and df[p].nunique() < limit )] \n    cat_cols_high = [p for p in df.columns if (df[p].dtype == 'object' \n                                           and df[p].nunique() > limit )]\n    return num_cols, cat_cols_low, cat_cols_high\n\ndef explorer(df, target, limit=10):\n    '''\n    This function explores and prints all information necessary about a \n    dataset to start develop an effective pipeline.\n    '''\n    print('This is an exploration:  \\n')\n    print('Our dataset has:')\n    print('{} rows and {} columns'.format(df.shape[0], df.shape[1]), '\\n')\n    print('Missing values:')\n    a = df.isnull().sum()\n    print( [[a.index[i], a[i]] for i in range( len(a) ) if a[i] > 0], '\\n')\n    print('Column types:')\n    num_cols, cat_cols_low, cat_cols_high = col_types(df, target)\n    print('Numerical columns: ', num_cols)\n    print('Low cardinality categorical columns: ', cat_cols_low)\n    print('High cardinality categorical columns: ', cat_cols_high)\n    print('\\n')\n    pass\n\n# We import the train_data and test_data\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv',index_col=0)\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv',index_col=0)\n\n\n#And we explore them:\ntarget = 'Survived'\nexplorer(train_data, target) \nexplorer(test_data, target)\n\n  ","230bc10a":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor\n\n#First we need to discern three types of columns\nnum_cols, cat_cols_low, cat_cols_high = col_types(train_data, 'Survived')\n\n#An appropriate strategy for the 'Age' still needs to be found.\nnum_cols.remove('Age') \n\n\nnumerical_transformer = SimpleImputer(strategy='mean') \n\ncategorical_transformer = Pipeline([ \n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder())   ])\n\ndata_preprocessor = ColumnTransformer( transformers=[\n    ('ignore', 'drop', cat_cols_high) ,     \n    ('cat', categorical_transformer, cat_cols_low) , \n    ('num', numerical_transformer, num_cols)  ])\n\n\nmodel = XGBRegressor(n_estimators=2500, learning_rate=0.01)\n\nclf = Pipeline(steps=[ ('prep', data_preprocessor),\n                      ('learn', model)    ])","f3f33fd5":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\n\n\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv',index_col=0 )\n\ny = train_data.Survived\nX = train_data.drop( ['Survived'] , axis=1 ) \n\nX_train, X_val, y_train, y_val = train_test_split(X,y,random_state=0)\n\nclf.fit(X_train, y_train)\npreds = np.around( clf.predict(X_val) , decimals=0).astype(int)\nres = accuracy_score(y_val, preds, normalize=False)\n\nprint('Of the {} entries, {} or {}% were predicted correctly'.format( \n               len(y_val), res, round(  res\/len(y_val) *100, 1)))\n    \n","397e817d":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\",index_col=0)\nclf.fit(X, y)\npreds = np.around( clf.predict(test_data) , decimals=0).astype(int)\n\noutput = pd.DataFrame({ 'PassengerId' : test_data.index, \n                       'Survived': preds \n                         })\noutput.to_csv('Test_predictions.csv', index=False)","82a44b16":"From our exploration, we can draw a few conclusions:\n- *Cabin* has around 80% of its values missing, and should be dropped.\n- *Name*, and *Ticket* are almost unique for every passenger, so their predictive power will be limited.\n- *Sex* and *Embarked* should be One-Hot Encoded. The 2 missing values for *Embarked* can be imputed.\n- The 1 missing value for *Fare* in the test data should be imputed.\n\nThe most interesting challenge in the data is the *Age* category, wich has 20% of its values missing. Experience shows that simply dropping the *Age* column  produces better results than imputing the mean value. But it is an open question how this data could be used to our advantage.","746ba706":"The next kernel trains the model uses all the training data, and gives us predictions to enter into the competition.","ea2dfd72":"It turns out 78.0 % of the test data was predicted accurately.","5e0e0ea5":"Now that our pipeline is set up, we can give our model a test-run using part of the known data to train the model, and part of the data to test its accuracy.","8d0627a4":"The first kernel is all about exploration: getting to know the data we work with.\n","f3f997cd":"In the column below we will create a pipeline to prepare our data and use a gradient boosting to train our data."}}