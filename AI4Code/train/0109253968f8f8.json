{"cell_type":{"be64ec8f":"code","067d44a3":"code","d6259b7e":"code","f392bb85":"code","074fc87d":"code","64fbdbfc":"code","66c32c30":"code","d8b35fcf":"code","c5978bde":"markdown","0f2fc8f7":"markdown","73b466b8":"markdown","cc1fe625":"markdown","da821581":"markdown","2aa91ad0":"markdown","2d45007d":"markdown","49725548":"markdown","8503d69d":"markdown","8bb75369":"markdown"},"source":{"be64ec8f":"#Import necessay libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nimport seaborn as sns\n\n#Preprocessing\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n","067d44a3":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nprint(train.shape,test.shape)\n#add extra one columns\ntrain['kfold']=-1\n#Distributing the data 5 shares\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(X=train)):\n    #print(fold,train_indicies,valid_indicies)\n    train.loc[valid_indicies,'kfold'] = fold\n\n    \nprint(train.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain.to_csv(\"trainfold_10.csv\",index=False)","d6259b7e":"#import the data and shape\ntrain = pd.read_csv(\".\/trainfold_10.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n#preview the train data\nprint(train.shape,test.shape)\ntrain.sample()","f392bb85":"plt.rc('figure',figsize= (10,12))\nsns.set_context('paper',font_scale=1)\n\nplt.title('Missing value status',fontweight = 'bold')\nax = sns.heatmap(train.isnull().sum().to_frame(),annot=True,fmt = 'd',cmap = 'vlag')\nax.set_xlabel('Amount Missing')\nplt.show()\n","074fc87d":"#statistical analysis of train data\nprint(\"Describe\",train.describe())\nprint(\"--------------------------------------------------------------------------\")\n\n#information of train data\nprint(\"Information\",train.info())\nprint(\"---------------------------------------------------------------------------\")\n\nprint(\"correlation\",train.corr())","64fbdbfc":"# Plot dataframe\nheat = train.corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(heat)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(heat, annot=False, mask=mask, cmap=\"RdYlGn\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();\n\n","66c32c30":"#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ntest = test[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {\n        'learning_rate': 0.03628302216953097,\n        'subsample': 0.7875490025178,\n        'colsample_bytree': 0.11807135201147,\n        'max_depth': 3,\n        'booster': 'gbtree', \n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'random_state':40,\n        'n_estimators':10000\n        \n        \n    }\n    \n    model= XGBRegressor(**xgb_params,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor',\n                       gpu_id=0)\n    model.fit(xtrain,ytrain,early_stopping_rounds=300,eval_set=[(xvalid,yvalid)],verbose=2000)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","d8b35fcf":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions),axis=1)\nprint(preds)\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\",index=False)\nprint(\"success\")","c5978bde":"#### **KFold**-->Split=5","0f2fc8f7":"### **HeatMap analysis of Missing data**","73b466b8":"### **Read input data**","cc1fe625":"#### **Predictions | SubmissionFile**","da821581":"#### **Necessary Library**","2aa91ad0":"## XGBRegresor | KFOLD-10 & OptunaTunning\n\nOptuna:\nOptuna is an open-source hyperparameter optimization toolkit designed to deal with machine learning and non-machine learning(as long as we can define the objective function). It provides a very imperative interface to fully support Python language with the highest modularity level in code\n","2d45007d":"**Reference:**\n\n1. [https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/005_visualization.html](http:\/\/)\n\n2. [https:\/\/www.kaggle.com\/venkatkumar001\/30days-optuna-xgb](http:\/\/)\n3. [https:\/\/www.kaggle.com\/miguelquiceno\/30-days-kfold-xgboost](http:\/\/)\n4. [https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna](http:\/\/)","49725548":"#  **ThankYou**","8503d69d":"### **Build XgboostRegressor Model**","8bb75369":"#### Statistical analysis"}}