{"cell_type":{"fc98e69a":"code","1c3c8ef3":"code","697de746":"code","0aa95bab":"code","171afdb2":"code","ac32cb35":"code","2a31226a":"code","c3870fde":"code","5a67a68c":"code","9974d46f":"code","062b8224":"code","f9cf428e":"code","30677b32":"code","871af91b":"code","af4bf645":"code","42fd04b9":"code","35ea1a8c":"code","79c810e4":"markdown","9ba3a27d":"markdown","c212527d":"markdown","c88c9096":"markdown","32af93f3":"markdown","63d76604":"markdown","16c15b97":"markdown","6fe18a9a":"markdown","bbb61213":"markdown","ce8483d4":"markdown","eded3254":"markdown","c8cd869b":"markdown","4b2f86a9":"markdown","9dd6a16b":"markdown","e7544ab8":"markdown","ed718a9c":"markdown","f0b7b5bc":"markdown","93dd0066":"markdown"},"source":{"fc98e69a":"import pandas as pd","1c3c8ef3":"df = pd.read_csv(\"\/kaggle\/input\/churn-modeling-dataset\/Churn_Modelling.csv\")\ndf.head()","697de746":"df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1, inplace=True)\ndf.shape","0aa95bab":"df.isna().sum()","171afdb2":"df['Exited'].value_counts(normalize=True)","ac32cb35":"import seaborn as sns\n\nax = sns.countplot(x=\"Geography\", data=df)","2a31226a":"X = df.drop('Exited', axis=1)\ny = df['Exited']","c3870fde":"from sklearn.model_selection import train_test_split\n\n# Divide data into training and validation subsets\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0, stratify=y)","5a67a68c":"categorical_cols = ['Geography', 'Gender']\n\n# Select numerical columns\nnumerical_cols = ['CreditScore',\n                  'Age',\n                  'Tenure',\n                  'Balance',\n                  'NumOfProducts',\n                  'HasCrCard',\n                  'IsActiveMember',\n                  'EstimatedSalary']","9974d46f":"from sklearn.compose import ColumnTransformer\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(), categorical_cols)\n    ])","062b8224":"from imblearn.over_sampling import SMOTE\n\nsmt = SMOTE(random_state=42)","f9cf428e":"from sklearn.experimental import enable_hist_gradient_boosting  \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nmodel = HistGradientBoostingClassifier()","30677b32":"pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('smote', smt),\n                            ('model', model)\n                          ])\n\n# Preprocessing of training data, fit model \npipeline.fit(x_train, y_train)","871af91b":"y_pred = pipeline.predict(x_valid)","af4bf645":"from sklearn.metrics import classification_report\nprint(classification_report(y_valid, y_pred))","42fd04b9":"new_data = pd.DataFrame({'CreditScore': 500, 'Geography': 'Spain', 'Gender': 'Female', 'Age': 30,\n                  'Tenure': 1, 'Balance': 0., 'NumOfProducts': 2, 'HasCrCard': 0, 'IsActiveMember': 1, \n                  'EstimatedSalary': 10258.2}, index=[0])\nnew_data","35ea1a8c":"pipeline.predict(new_data)","79c810e4":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac We don't need of the <b>RowNumber, CustomerId, Surname <\/b> columns, <code> so just drop \ud83d\uddd1<\/code><\/p><\/blockquote>\n","9ba3a27d":"### SMOTE","c212527d":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcacThe data is balanced?<\/p><\/blockquote>","c88c9096":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; Note that we have an imbalanced dataset, so \"it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset\". We can do this using the <code> stratify <\/code> parameter.\n<\/div>","32af93f3":"[**Reference**](https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/)","63d76604":"### Using ColumnTransformer","16c15b97":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac Select Numerical and Categorical columns<\/p><\/blockquote>","6fe18a9a":"### New data","bbb61213":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; Only 20% of the Exited labels are of the type 1 in the whole dataset, so we need  to apply some data balancing technique. In this notebook we will use the SMOTE technique.\n<\/div>","ce8483d4":"### Missing Values","eded3254":"### Predict","c8cd869b":"### Show data by geografy","4b2f86a9":"[**Reference**](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.pipeline.Pipeline.html)","9dd6a16b":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; The pipeline have the following steps: <b> Preprocessing, SMOTE, Model<\/b> .\n<\/div>","e7544ab8":"### Split the data in train and validation data","ed718a9c":"### Define the HistGradientBoostingClassifier model","f0b7b5bc":"### Create the Pipeline","93dd0066":"### Churn Prediction \ud83c\udfc3\u200d\ud83d\udca8"}}