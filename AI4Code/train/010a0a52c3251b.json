{"cell_type":{"52c9d718":"code","feec3642":"code","454315e5":"code","fbaa0da0":"code","08e19b08":"code","c5031346":"code","694bf877":"code","a20d3145":"code","787585d8":"code","1e090a2f":"code","23ba2b5e":"code","294d18b2":"code","31142348":"code","ad937114":"code","962bc374":"markdown","44a9e9c0":"markdown","fad6e5aa":"markdown","3040efa0":"markdown","bce1b0e8":"markdown","29f0064d":"markdown","ff9a8e72":"markdown","183c6130":"markdown","b88a9645":"markdown"},"source":{"52c9d718":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")  # for testing the final model after model selection","feec3642":"train.info()","454315e5":"test.info()","fbaa0da0":"# Assign predictors and target\nX_train = train.drop(['Survived'], axis=1)\ny_train = train['Survived'].copy()\n\n# For numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), # impute missing values with median\n    ('minmax_scaler', MinMaxScaler()),             # scale features\n])\n\n# For categorical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')), # impute missing values with mode\n    ('cat_encoder', OneHotEncoder())                      # convert text to numbers\n])\n\n# Full pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs_all = X_train.select_dtypes(['float64', 'int64']).columns\nnum_attribs = num_attribs_all.drop('PassengerId') # drop features\n\ncat_attribs_all = X_train.select_dtypes('object').columns\ncat_attribs = cat_attribs_all.drop(['Ticket', 'Cabin', 'Name']) # drop features\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])\n\n# Apply pre-processing to features\nX_train_prepared = full_pipeline.fit_transform(X_train)","08e19b08":"from sklearn.tree import DecisionTreeClassifier\n\n# Fit the model\nclf = DecisionTreeClassifier()\nclf.fit(X_train_prepared, y_train)\nclf.score(X_train_prepared, y_train) # accuracy on training set","c5031346":"# Accuracy for each time the model is trained on a new subset of data\nfrom sklearn.model_selection import cross_val_score\nscore = cross_val_score(clf, X_train_prepared, y_train, cv=10, scoring=\"accuracy\")\nscore","694bf877":"score.mean()","a20d3145":"score.std()","787585d8":"from sklearn.neighbors import KNeighborsClassifier\n\n# Fit the model\nknn = KNeighborsClassifier()\nknn.fit(X_train_prepared, y_train)\nknn.score(X_train_prepared, y_train) # accuracy on training set","1e090a2f":"score = cross_val_score(knn, X_train_prepared, y_train, cv=10, scoring=\"accuracy\")\nscore","23ba2b5e":"score.mean()","294d18b2":"score.std()","31142348":"# Obtain predictions\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred_clf = cross_val_predict(clf, X_train_prepared, y_train, cv=10)\ny_train_pred_knn = cross_val_predict(knn, X_train_prepared, y_train, cv=10)","ad937114":"# Create matrices\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"Decision tree classifier\")\nprint(confusion_matrix(y_train, y_train_pred_clf))\nprint(\"\\n\")\nprint(\"K-nearest neighbors classifier\")\nprint(confusion_matrix(y_train, y_train_pred_knn))","962bc374":"### Cross validation","44a9e9c0":"## Decision tree classifier","fad6e5aa":"From StatQuest's Machine Learning Fundamentals series.\n\n**Cross validation:**\n* Allows us to compare different machine learning algorithms and get a sense for how they will work in practice\n* Involves splitting the data into subsets (also known as folds)\n* For testing, each subset is used one at a time and results are summarised at the end\n* So if the data is split into 10 folds, the model is trained and evaluated 10 times, picking a different fold for evaluation each time and training on the 9 other folds\n\n**The confusion matrix:**\n* Tells you what your machine learning algorithm did right and what it did wrong\n* Summarised as a table containing the accuracy of classifications","3040efa0":"# Introduction: Cross Validation and the Confusion Matrix","bce1b0e8":"### Cross validation","29f0064d":"## Pre-processing","ff9a8e72":"Each row represents an *actual class*. Each column represents a *predicted class*.\nThe first row considers the negative class (non-survival): the decision tree classifier correctly classified 458 passengers as not surviving (*true negatives*), while the remaining 91 were wrongly classified as surviving (*false positives*). The second row considers the positive class (survival): 99 passengers were wrongly classified as not surviving (*false negatives*), while 243 passengers were correctly classified as surviving (*true positives*).","183c6130":"## Confusion matrices","b88a9645":"## K-nearest neighbors classifier"}}