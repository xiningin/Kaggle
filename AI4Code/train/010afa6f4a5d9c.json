{"cell_type":{"a12bb745":"code","3677d5d8":"code","0cb74733":"code","9d94ebc0":"code","821212cc":"code","3f881fc8":"code","3c7ab111":"code","365950ef":"code","f02beb20":"code","46862b5d":"code","b80126c6":"code","d553da48":"code","b7683e48":"code","f46ee82e":"code","649a9244":"code","7d0976ed":"code","f6219a5f":"code","021670df":"code","b7f3a19d":"markdown","598274e0":"markdown","18ebd2b9":"markdown","c3ba78df":"markdown","00197e89":"markdown","6c231bc6":"markdown","ca4e0d73":"markdown","3232c3aa":"markdown","958089d4":"markdown","14faa888":"markdown","24a39882":"markdown","26b749b3":"markdown","dd1abb27":"markdown","5c523832":"markdown","82ca1628":"markdown","7593bed0":"markdown","ee261fe1":"markdown","2272cf07":"markdown","41d72fd3":"markdown","047ced63":"markdown","3a8f8b90":"markdown"},"source":{"a12bb745":"!mkdir ..\/input\/segmentation\n!mkdir ..\/input\/segmentation\/test\n!mkdir ..\/input\/segmentation\/train\n!mkdir ..\/input\/segmentation\/train\/augmentation\n!mkdir ..\/input\/segmentation\/train\/image\n!mkdir ..\/input\/segmentation\/train\/mask\n!mkdir ..\/input\/segmentation\/train\/dilate","3677d5d8":"import os\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\nfrom glob import glob\nfrom tqdm import tqdm","0cb74733":"INPUT_DIR = os.path.join(\"..\", \"input\")\n\nSEGMENTATION_DIR = os.path.join(INPUT_DIR, \"segmentation\")\nSEGMENTATION_TEST_DIR = os.path.join(SEGMENTATION_DIR, \"test\")\nSEGMENTATION_TRAIN_DIR = os.path.join(SEGMENTATION_DIR, \"train\")\nSEGMENTATION_AUG_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"augmentation\")\nSEGMENTATION_IMAGE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"image\")\nSEGMENTATION_MASK_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"mask\")\nSEGMENTATION_DILATE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"dilate\")\nSEGMENTATION_SOURCE_DIR = os.path.join(INPUT_DIR, \\\n                                       \"pulmonary-chest-xray-abnormalities\")\n\nSHENZHEN_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \"ChinaSet_AllFiles\", \\\n                                  \"ChinaSet_AllFiles\")\nSHENZHEN_IMAGE_DIR = os.path.join(SHENZHEN_TRAIN_DIR, \"CXR_png\")\nSHENZHEN_MASK_DIR = os.path.join(INPUT_DIR, \"shcxr-lung-mask\", \"mask\", \"mask\")\n\nMONTGOMERY_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \\\n                                    \"Montgomery\", \"MontgomerySet\")\nMONTGOMERY_IMAGE_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \"CXR_png\")\nMONTGOMERY_LEFT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                        \"ManualMask\", \"leftMask\")\nMONTGOMERY_RIGHT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                         \"ManualMask\", \"rightMask\")\n\nDILATE_KERNEL = np.ones((15, 15), np.uint8)\n\nBATCH_SIZE=2\n\n#Prod\nEPOCHS=56\n\n#Desv\n#EPOCHS=16","9d94ebc0":"montgomery_left_mask_dir = glob(os.path.join(MONTGOMERY_LEFT_MASK_DIR, '*.png'))\nmontgomery_test = montgomery_left_mask_dir[0:50]\nmontgomery_train= montgomery_left_mask_dir[50:]\n\nfor left_image_file in tqdm(montgomery_left_mask_dir):\n    base_file = os.path.basename(left_image_file)\n    image_file = os.path.join(MONTGOMERY_IMAGE_DIR, base_file)\n    right_image_file = os.path.join(MONTGOMERY_RIGHT_MASK_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    left_mask = cv2.imread(left_image_file, cv2.IMREAD_GRAYSCALE)\n    right_mask = cv2.imread(right_image_file, cv2.IMREAD_GRAYSCALE)\n    \n    image = cv2.resize(image, (512, 512))\n    left_mask = cv2.resize(left_mask, (512, 512))\n    right_mask = cv2.resize(right_mask, (512, 512))\n    \n    mask = np.maximum(left_mask, right_mask)\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (left_image_file in montgomery_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)\n    else:\n        filename, fileext = os.path.splitext(base_file)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_mask%s\" % (filename, fileext)), mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext)), mask_dilate)","821212cc":"def add_colored_dilate(image, mask_image, dilate_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    dilate_image_gray = cv2.cvtColor(dilate_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    dilate = cv2.bitwise_and(dilate_image, dilate_image, mask=dilate_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n    dilate_coord = np.where(dilate!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n    dilate[dilate_coord[0],dilate_coord[1],:] = [0,0,255]\n\n    ret = cv2.addWeighted(image, 0.7, dilate, 0.3, 0)\n    ret = cv2.addWeighted(ret, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef add_colored_mask(image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(image, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef diff_mask(ref_image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(ref_image, 0.7, mask, 0.3, 0)\n    return ret","3f881fc8":"base_file = os.path.basename(montgomery_train[0])\n\nimage_file = os.path.join(SEGMENTATION_IMAGE_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_MASK_DIR, base_file)\ndilate_image_file = os.path.join(SEGMENTATION_DILATE_DIR, base_file)\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n                          \nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\n\naxs[0, 0].set_title(\"X-Ray\")\naxs[0, 0].imshow(image)\n\naxs[0, 1].set_title(\"Mask\")\naxs[0, 1].imshow(mask_image)\n\naxs[0, 2].set_title(\"Dilate\")\naxs[0, 2].imshow(dilate_image)\n\naxs[0, 3].set_title(\"Merged\")\naxs[0, 3].imshow(merged_image)\n\nbase_file = os.path.basename(montgomery_test[0])\nfilename, fileext = os.path.splitext(base_file)\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\ndilate_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext))\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n\naxs[1, 0].set_title(\"X-Ray\")\naxs[1, 0].imshow(image)\n\naxs[1, 1].set_title(\"Mask\")\naxs[1, 1].imshow(mask_image)\n\naxs[1, 2].set_title(\"Dilate\")\naxs[1, 2].imshow(dilate_image)\n\naxs[1, 3].set_title(\"Merged\")\naxs[1, 3].imshow(merged_image)","3c7ab111":"shenzhen_mask_dir = glob(os.path.join(SHENZHEN_MASK_DIR, '*.png'))\nshenzhen_test = shenzhen_mask_dir[0:50]\nshenzhen_train= shenzhen_mask_dir[50:]\n\nfor mask_file in tqdm(shenzhen_mask_dir):\n    base_file = os.path.basename(mask_file).replace(\"_mask\", \"\")\n    image_file = os.path.join(SHENZHEN_IMAGE_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n        \n    image = cv2.resize(image, (512, 512))\n    mask = cv2.resize(mask, (512, 512))\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (mask_file in shenzhen_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)\n    else:\n        filename, fileext = os.path.splitext(base_file)\n\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_mask%s\" % (filename, fileext)), mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext)), mask_dilate)","365950ef":"base_file = os.path.basename(shenzhen_train[0].replace(\"_mask\", \"\"))\n\nimage_file = os.path.join(SEGMENTATION_IMAGE_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_MASK_DIR, base_file)\ndilate_image_file = os.path.join(SEGMENTATION_DILATE_DIR, base_file)\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n                          \nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\n\naxs[0, 0].set_title(\"X-Ray\")\naxs[0, 0].imshow(image)\n\naxs[0, 1].set_title(\"Mask\")\naxs[0, 1].imshow(mask_image)\n\naxs[0, 2].set_title(\"Dilate\")\naxs[0, 2].imshow(dilate_image)\n\naxs[0, 3].set_title(\"Merged\")\naxs[0, 3].imshow(merged_image)\n\nbase_file = os.path.basename(shenzhen_test[0].replace(\"_mask\", \"\"))\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nfilename, fileext = os.path.splitext(base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\n\nfilename, fileext = os.path.splitext(base_file)\nimage_file = os.path.join(SEGMENTATION_TEST_DIR, base_file)\nmask_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                               \"%s_mask%s\" % (filename, fileext))\ndilate_image_file = os.path.join(SEGMENTATION_TEST_DIR, \\\n                                 \"%s_dilate%s\" % (filename, fileext))\n\nimage = cv2.imread(image_file)\nmask_image = cv2.imread(mask_image_file)\ndilate_image = cv2.imread(dilate_image_file)\nmerged_image = add_colored_dilate(image, mask_image, dilate_image)\n\naxs[1, 0].set_title(\"X-Ray\")\naxs[1, 0].imshow(image)\n\naxs[1, 1].set_title(\"Mask\")\naxs[1, 1].imshow(mask_image)\n\naxs[1, 2].set_title(\"Dilate\")\naxs[1, 2].imshow(dilate_image)\n\naxs[1, 3].set_title(\"Merged\")\naxs[1, 3].imshow(merged_image)","f02beb20":"train_files = glob(os.path.join(SEGMENTATION_IMAGE_DIR, \"*.png\"))\ntest_files = glob(os.path.join(SEGMENTATION_TEST_DIR, \"*.png\"))\nmask_files = glob(os.path.join(SEGMENTATION_MASK_DIR, \"*.png\"))\ndilate_files = glob(os.path.join(SEGMENTATION_DILATE_DIR, \"*.png\"))\n\n(len(train_files), \\\n len(test_files), \\\n len(mask_files), \\\n len(dilate_files))","46862b5d":"# From: https:\/\/github.com\/zhixuhao\/unet\/blob\/master\/data.py\ndef train_generator(batch_size, train_path, image_folder, mask_folder, aug_dict,\n        image_color_mode=\"grayscale\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_directory(\n        train_path,\n        classes = [image_folder],\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_directory(\n        train_path,\n        classes = [mask_folder],\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = img \/ 255\n    mask = mask \/ 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","b80126c6":"# From: https:\/\/github.com\/jocicmarko\/ultrasound-nerve-segmentation\/blob\/master\/train.py\ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) \/ (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef unet(input_size=(256,256,1)):\n    inputs = Input(input_size)\n    \n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","d553da48":"# From: https:\/\/github.com\/zhixuhao\/unet\/blob\/master\/data.py\ndef test_load_image(test_file, target_size=(256,256)):\n    img = cv2.imread(test_file, cv2.IMREAD_GRAYSCALE)\n    img = img \/ 255\n    img = cv2.resize(img, target_size)\n    img = np.reshape(img, img.shape + (1,))\n    img = np.reshape(img,(1,) + img.shape)\n    return img\n\ndef test_generator(test_files, target_size=(256,256)):\n    for test_file in test_files:\n        yield test_load_image(test_file, target_size)\n        \ndef save_result(save_path, npyfile, test_files):\n    for i, item in enumerate(npyfile):\n        result_file = test_files[i]\n        img = (item[:, :, 0] * 255.).astype(np.uint8)\n\n        filename, fileext = os.path.splitext(os.path.basename(result_file))\n\n        result_file = os.path.join(save_path, \"%s_predict%s\" % (filename, fileext))\n\n        cv2.imwrite(result_file, img)","b7683e48":"def add_suffix(base_file, suffix):\n    filename, fileext = os.path.splitext(base_file)\n    return \"%s_%s%s\" % (filename, suffix, fileext)\n\ntest_files = [test_file for test_file in glob(os.path.join(SEGMENTATION_TEST_DIR, \"*.png\")) \\\n              if (\"_mask\" not in test_file \\\n                  and \"_dilate\" not in test_file \\\n                  and \"_predict\" not in test_file)]\n\nvalidation_data = (test_load_image(test_files[0], target_size=(512, 512)),\n                    test_load_image(add_suffix(test_files[0], \"dilate\"), target_size=(512, 512)))\n\nlen(test_files), len(validation_data)","f46ee82e":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\ntrain_gen = train_generator(BATCH_SIZE,\n                            SEGMENTATION_TRAIN_DIR,\n                            'image',\n                            'dilate', \n                            train_generator_args,\n                            target_size=(512,512),\n                            save_to_dir=os.path.abspath(SEGMENTATION_AUG_DIR))\n\nmodel = unet(input_size=(512,512,1))\nmodel.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, \\\n                  metrics=[dice_coef, 'binary_accuracy'])\nmodel.summary()\n\nmodel_checkpoint = ModelCheckpoint('unet_lung_seg.hdf5', \n                                   monitor='loss', \n                                   verbose=1, \n                                   save_best_only=True)\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=len(train_files) \/ BATCH_SIZE, \n                              epochs=EPOCHS, \n                              callbacks=[model_checkpoint],\n                              validation_data = validation_data)","649a9244":"fig, axs = plt.subplots(1, 2, figsize = (15, 4))\n\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\ntraining_accuracy = history.history['binary_accuracy']\nvalidation_accuracy = history.history['val_binary_accuracy']\n\nepoch_count = range(1, len(training_loss) + 1)\n\naxs[0].plot(epoch_count, training_loss, 'r--')\naxs[0].plot(epoch_count, validation_loss, 'b-')\naxs[0].legend(['Training Loss', 'Validation Loss'])\n\naxs[1].plot(epoch_count, training_accuracy, 'r--')\naxs[1].plot(epoch_count, validation_accuracy, 'b-')\naxs[1].legend(['Training Accuracy', 'Validation Accuracy'])","7d0976ed":"test_gen = test_generator(test_files, target_size=(512,512))\nresults = model.predict_generator(test_gen, len(test_files), verbose=1)\nsave_result(SEGMENTATION_TEST_DIR, results, test_files)","f6219a5f":"image = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0003_0.png\")\npredict_image = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0003_0_predict.png\")\nmask_image = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0003_0_dilate.png\")\n\nfig, axs = plt.subplots(4, 3, figsize=(16, 16))\n\naxs[0, 0].set_title(\"Predicted\")\naxs[0, 0].imshow(add_colored_mask(image, predict_image))\naxs[0, 1].set_title(\"Gold Std.\")\naxs[0, 1].imshow(add_colored_mask(image, mask_image))\naxs[0, 2].set_title(\"Diff.\")\naxs[0, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0003_0.png\")\npredict_image = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0003_0_predict.png\")\nmask_image = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0003_0_dilate.png\")\n\naxs[1, 0].set_title(\"Predicted\")\naxs[1, 0].imshow(add_colored_mask(image, predict_image))\naxs[1, 1].set_title(\"Gold Std.\")\naxs[1, 1].imshow(add_colored_mask(image, mask_image))\naxs[1, 2].set_title(\"Diff.\")\naxs[1, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0020_0.png\")\npredict_image = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0020_0_predict.png\")\nmask_image = cv2.imread(\"..\/input\/segmentation\/test\/CHNCXR_0020_0_dilate.png\")\n\naxs[2, 0].set_title(\"Predicted\")\naxs[2, 0].imshow(add_colored_mask(image, predict_image))\naxs[2, 1].set_title(\"Gold Std.\")\naxs[2, 1].imshow(add_colored_mask(image, mask_image))\naxs[2, 2].set_title(\"Diff.\")\naxs[2, 2].imshow(diff_mask(mask_image, predict_image))\n\nimage = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0016_0.png\")\npredict_image = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0016_0_predict.png\")\nmask_image = cv2.imread(\"..\/input\/segmentation\/test\/MCUCXR_0016_0_dilate.png\")\n\naxs[3, 0].set_title(\"Predicted\")\naxs[3, 0].imshow(add_colored_mask(image, predict_image))\naxs[3, 1].set_title(\"Gold Std.\")\naxs[3, 1].imshow(add_colored_mask(image, mask_image))\naxs[3, 2].set_title(\"Diff.\")\naxs[3, 2].imshow(diff_mask(mask_image, predict_image))","021670df":"!tar zcf results.tgz --directory=..\/input\/segmentation\/test .","b7f3a19d":"# 2. Data preparation\nPrepare the input segmentation directory structure.","598274e0":"Define appropriate constants for directory paths and training parameters","18ebd2b9":"Data augmentation helper function for training the net","c3ba78df":"# **Contents**\n1. [Overview](#1.-Overview)\n1. [Data preparation](#2.-Data-preparation)\n1. [Segmentation training](#3.-Segmentation-training)\n1. [Results](#4.-Results)","00197e89":"Define some useful functions to display images with segmentation as overlays","6c231bc6":"1. Resize Shenzhen Hospital chest x-ray images to 512x512 pixels\n1. Dilate masks to gain more information on the edge of lungs\n1. Split images into training and test datasets\n1. Write images to \/segmentation directory","ca4e0d73":"# 1. Overview\nThis notebook follows the work of [Kevin Mader](https:\/\/www.kaggle.com\/kmader\/training-u-net-on-tb-images-to-segment-lungs\/notebook) for lung segmentation. Our motivation is to automatically identify lung opacities in chest x-rays for the [RSNA Pneumonia Detection Challenge](https:\/\/www.kaggle.com\/c\/rsna-pneumonia-detection-challenge\/leaderboard). \n\nMedical Image Segmentation is the process of automatic detection of boundaries within images. In this exercise, we train a convolutional neural network with [U-Net](https:\/\/arxiv.org\/abs\/1505.04597) architecture, which training strategy relies on the strong use of data augmentation to improve the efficiency of available annotated samples.\n\nThe training is done with two chest x-rays datasets: [Montgomery County and Shenzhen Hospital](https:\/\/ceb.nlm.nih.gov\/repositories\/tuberculosis-chest-x-ray-image-data-sets\/). The Montgomery County dataset includes manually segmented lung masks, whereas Shenzhen Hospital dataset was manually segmented by [Stirenko et al](https:\/\/arxiv.org\/abs\/1803.01199). The lung segmentation masks were dilated to load lung boundary information within the training net and the images were resized to 512x512 pixels.","3232c3aa":"Show some Shenzhen Hospital chest x-rays and its lung segmentation masks from training and test dataset to verify the procedure above. In merged image it is possible to see the difference between the dilated mask in blue and the original mask in red.","958089d4":"Print the count of images and segmentation lung masks available to test and train the model","14faa888":"Show some Montgomery chest x-rays and its lung segmentation masks from training and test dataset to verify the procedure above. In merged image it is possible to see the difference between the dilated mask in blue and the original mask in red.","24a39882":"1. Combine left and right lung segmentation masks of Montgomery chest x-rays\n1. Resize images to 512x512 pixels\n1. Dilate masks to gain more information on the edge of lungs\n1. Split images into training and test datasets\n1. Write images to \/segmentation directory","26b749b3":"Show some results from model fitting history","dd1abb27":"**<h1>RSNA Pneumonia Detection Challenge<h1>**\nSociedade Beneficente de Senhoras - Hospital S\u00edrio-Liban\u00eas - Brazil","5c523832":"Select test and validation files","82ca1628":"Make lung segmentation predictions","7593bed0":"U-net architecture","ee261fe1":"Import required Python libraries","2272cf07":"# 3. Segmentation training\n\nReferences: https:\/\/github.com\/zhixuhao\/unet\/, https:\/\/github.com\/jocicmarko\/ultrasound-nerve-segmentation","41d72fd3":"# 4. Results\n\nBelow, we see some results from our work, presented as Predicted, Gold Standard (manually segmented) and the difference between segmentations.\n\nThe next step will be the selection of lungs area on RSNA images dataset and the generation of a lungs-only image dataset.","047ced63":"Helper functions to load test chest x-ray images","3a8f8b90":"Prepare the U-Net model and train the model. It will take a while..."}}