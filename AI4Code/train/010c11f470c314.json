{"cell_type":{"52852223":"code","3996f4d2":"code","7aaa9101":"code","d2513d21":"code","1f044766":"code","85a1995e":"code","708adc2a":"code","44d53281":"code","4488edd9":"code","a9bd3ffd":"code","ded1b5e7":"code","04c68867":"code","a5782dbf":"code","716b0dcd":"code","8a1b2f50":"code","6e361426":"code","0b97bed0":"code","ac2b6d01":"code","c200545e":"code","cef51024":"code","c2cd1ef3":"code","0feb5c84":"code","1b539802":"code","5645ec15":"code","634e6400":"code","df76e9d1":"code","07a23446":"code","5590335a":"code","68e3961a":"code","38c29758":"code","c288495d":"code","ed746208":"code","c78bcc6a":"code","a1f0ec07":"code","d0c47aa0":"code","b07c5430":"code","5dbcc39c":"code","b7bab400":"code","5c524086":"code","2e4bed13":"code","ae250f6b":"code","f4dc0e56":"code","a195cd37":"code","51e93763":"code","1c7707d7":"code","9da1ca55":"markdown","272e72bd":"markdown","542bafd5":"markdown","d3827ba0":"markdown","1213166d":"markdown","2c74c690":"markdown","f5cb10a3":"markdown","67d6735b":"markdown","37d711a2":"markdown","53b3408c":"markdown","8eda1d39":"markdown","ae4fb9ab":"markdown","1942bc8c":"markdown","d678f883":"markdown","03f20254":"markdown","403ac3b9":"markdown","164027ab":"markdown","85711e0d":"markdown","5231e991":"markdown","d3d63870":"markdown","52808e58":"markdown","afcf5f52":"markdown","81cea3c0":"markdown","98f3e47c":"markdown","fd1f7df5":"markdown","4037565f":"markdown","d9db7483":"markdown","8f0772f0":"markdown"},"source":{"52852223":"import os\nimport sys\nimport warnings\n\nimport scipy\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","3996f4d2":"print('-'*30,'VERSIONS','-'*30)\nprint('Python version: {}'.format(sys.version))\nfor name, module in [('Pandas', pd),\n                     ('Matplotlib', matplotlib),\n                     ('Numpy', np),\n                     ('Seaborn', sns),\n                     ('Scikit-Learn', sklearn),\n                     ('Scipy', scipy)]:\n    print('{} version: {}'.format(name, module.__version__))","7aaa9101":"# Setting matplotlib constants\nmatplotlib.style.use('ggplot')\nsns.set_style('whitegrid')","d2513d21":"data_dir = '\/kaggle\/input\/titanic'\ntrain_fn = 'train.csv'\ntest_fn = 'test.csv'\n\ntrain = pd.read_csv(os.path.join(data_dir, train_fn))\ntest = pd.read_csv(os.path.join(data_dir, test_fn))\n\nfor dataset, fn in [(train, train_fn), (test, test_fn)]:\n    dataset.name = fn.split('.')[0]","1f044766":"train.head(3)","85a1995e":"train.describe()","708adc2a":"train.info()","44d53281":"for dataset in [train, test]:\n    print('{} dataset'.format(dataset.name).upper())\n    print('-'*20)\n    print(dataset.isnull().sum())\n    print('='*20)","4488edd9":"dtypes = ['int64','float64','object']\nfor d in dtypes:\n    print('\\n','*'*20,'{} type Attributes'.format(d.upper()), '*'*20, end='\\n\\n')\n    print(list(train.select_dtypes(d).columns), end='\\n\\n')\n    for field in list(train.select_dtypes(d).columns):\n        print('{} ==> {}'.format(field, 'CONTINUOUS' if train[field].value_counts().count() > 10 else 'CATEGORICAL'))","a9bd3ffd":"# One very easy way to visualize the missing fields is by plotting the heatmap\nplt.figure(figsize=(10,8))\nsns.heatmap(train.isnull())","ded1b5e7":"# Age distributions\n_, a = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\na[0][0].set_title('Age distribution')\na[0][1].set_title('Pclass 1 age dist')\na[1][0].set_title('Pclass 2 age dist')\na[1][1].set_title('Pclass 3 age dist')\nsns.distplot(train['Age'], ax=a[0][0])\nsns.distplot(train[train['Pclass']==1]['Age'], ax=a[0][1])\nsns.distplot(train[train['Pclass']==2]['Age'], ax=a[1][0])\nsns.distplot(train[train['Pclass']==3]['Age'], ax=a[1][1])\nplt.tight_layout()","04c68867":"# Appplying changes to both datasets\nfor dataset in [train, test]:\n    dataset.Age = dataset[['Age', 'Pclass']].apply(\n        lambda x: dataset[dataset.Pclass == x[1]].Age.median() if pd.isnull(x[0]) else x[0],\n        axis=1\n    )","a5782dbf":"\n# Lets take a look at these values first\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nsns.countplot(x='Embarked', data=train, ax=ax[0])\nsns.countplot(x='Embarked', data=train, hue='Sex', ax=ax[1])","716b0dcd":"# Imputing embarked with mode\n\nfor dataset in [train, test]:\n    dataset.Embarked.fillna(dataset.Embarked.mode()[0],\n                            inplace=True)\n    \n# Imputing fare with median\nfor dataset in [train, test]:\n    dataset.Fare.fillna(dataset.Fare.median(),\n                        inplace=True)","8a1b2f50":"# Now lets see what effect the column cabin has on the passenger's survivavility\n\nnum_passenger_with_cabin_data = train[~pd.isna(train.Cabin)].shape[0]\nnum_passenger_without_cabin_data = train.shape[0] - num_passenger_with_cabin_data\n\nprint('Number of passengers with cabin data: {}\\nNumber of passengers without cabin data: {}'.format(\n    num_passenger_with_cabin_data, num_passenger_without_cabin_data\n))\n\n# Passengers with cabin data who survived vs. perished\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nsns.countplot(train[~pd.isna(train.Cabin)].Survived, ax=ax[0])\nsns.countplot(train[pd.isna(train.Cabin)].Survived, ax=ax[1])\n\nax[0].set_title('Passenger with cabin information')\nax[1].set_title('Passenger without cabin information')\n\nplt.tight_layout()","6e361426":"for dataset in [train, test]:\n    dataset['IsCabin'] = dataset['Cabin'].apply(\n        lambda x: 0 if pd.isnull(x) else 1\n    )","0b97bed0":"# Lets take a look at the null values now\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n\nsns.heatmap(train.isnull(), ax=ax[0])\nax[0].set_title('Train dataset null values')\n\nsns.heatmap(test.isnull(), ax=ax[1])\nax[1].set_title('Test dataset null values')","ac2b6d01":"# In this stage we are creating new features such as FamilySize, IsAlone etc\n\nfor dataset in [train, test]:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    # Passengers who have a familysize greater then 1 is definitely not alone\n    dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0\n    \n    # Creating title attribute\n    dataset['Title'] = dataset['Name'].apply(\n        lambda x: x.split(',')[1].split('.')[0].strip()\n    )\n    \n    # Binning fare attribute values\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    \n    # Binning age values into 4 segments\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n# Taking a look at the newly created title attribute\ntrain.Title.value_counts()","c200545e":"# There are many uncommon titles, so we can bin the less frequent titles\n\ntrain_title_names = (train.Title.value_counts() < 10)\ntest_title_names = (test.Title.value_counts() < 10)\n\n# All titles which occurs less than 10 times will now be replaced by 'Others'\ntrain.Title = train.Title.apply(\n    lambda x: 'Others' if train_title_names.loc[x] else x\n)\ntest.Title = test.Title.apply(\n    lambda x: 'Others' if test_title_names.loc[x] else x\n)","cef51024":"# Now lets apply label encoding to the non numeric fields, we are using label encoder and not one hot encoding\n# to preserve the ordinality among the variables\n\nle = LabelEncoder()\n\nfor dataset in [train, test]:\n    dataset.Sex = le.fit_transform(dataset.Sex)\n    dataset.Embarked = le.fit_transform(dataset.Embarked)\n    dataset.Title = le.fit_transform(dataset.Title)\n    dataset.AgeBin = le.fit_transform(dataset.AgeBin)\n    dataset.FareBin = le.fit_transform(dataset.FareBin)","c2cd1ef3":"_, ax = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(10,5))\n\nsns.countplot(x='Survived', data=train, ax=ax[0])\nsns.countplot(x='Sex', data=train, ax=ax[1])\n\nax[0].set_title('Survived vs. Casualties')\nax[1].set_title('Male (1) vs. Female (0) Count')","0feb5c84":"sns.countplot(x='Survived', data=train, hue='Sex')","1b539802":"# Lets check the percentage of people who survived\n\npercentage_of_survived = 100 * (train['Survived'].value_counts() \/ train.shape[0])\n\n# Percentage of Males who survivied\nsurvived_male = train[(train['Survived']==1) & (train['Sex']==1)].shape[0]\ntotal_male = train[train['Sex']==1].shape[0]\n\n# Percentage of Females who survivied\nsurvived_female = train[(train['Survived']==1) & (train['Sex']==0)].shape[0]\ntotal_female = train[train['Sex']==0].shape[0]\n\nprint('Percentage of passngers in both target classes.')\nprint(percentage_of_survived)\nprint('-'*50)\nprint('Percentage of male passengers who survived: {:0.2f}'.format(100*(survived_male\/total_male)))\nprint('Percentage of female passengers who survived: {:0.2f}'.format(100*(survived_female\/total_female)))","5645ec15":"# Different passenger classes\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n\nsns.countplot(x='Pclass', data=train, ax=ax[0])\nsns.countplot(x='Pclass', data=train, hue='Survived', ax=ax[1])\n\nax[0].set_title('Passenger Class counts')\nax[1].set_title('Passenger Class with respect to Survival')","634e6400":"# Distributions of numerical columns and identification of ourliers\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n\nsns.distplot(train['Age'], bins=30, ax=ax[0])\nax[0].set_title('Distribution of ages')\n\nsns.distplot(test['Fare'], bins=30, ax=ax[1])\nax[1].set_title('Distribution of Fare')","df76e9d1":"_, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\nsns.scatterplot(x='Age', y='Fare', data=train, ax=ax[0])\nsns.scatterplot(x='Age', y='Fare', data=train, hue='Survived', ax=ax[1])\n\nax[0].set_title('Age vs. Fare')\nax[1].set_title('Age vs. Fare w.r.t Survived')","07a23446":"# Boxplot for Pclass vs age\n\nplt.figure(figsize=(10, 4))\nsns.boxplot(x='Pclass', y='Age', data=train)","5590335a":"# Correlation\n\nplt.figure(figsize=(12,10))\nsns.heatmap(train.corr(), square=True, annot=True)","68e3961a":"# How the independent variables are correlated with dependent\n\nplt.figure(figsize=(10, 6))\ntrain.corr().Survived.sort_values(ascending=False)[1:].plot(kind='bar',\n                                                            fontsize=15)","38c29758":"# Final look at our dataset\ntrain.head(3)","c288495d":"from scipy.stats import mode\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV","ed746208":"# Creating X and y with only the training dataset for validation\n\nremove_cols = ['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin']\nX = train.drop(\n    labels=remove_cols,\n    axis=1\n)\ny = train['Survived']\n\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n\n# Shapes\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\n\nprint('X_test shape: {}'.format(X_test.shape))\nprint('y_test shape: {}'.format(y_test.shape))","c78bcc6a":"# Method for handling trained model\n\ndef plt_conf_mat(y_true, y_pred, title):\n    # Plotting confusion matrix\n    \n    plt.figure(figsize=(8,6))\n    sns.heatmap(confusion_matrix(y_true, y_pred),\n                annot=True,\n                square=True)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"{} Confusion Matrix\".format(title))\n    plt.show()\n\ndef model_data(trained_model, model_name, test_data=(X_test, y_test)):\n    X_test, y_test = test_data\n    prediction = trained_model.predict(X_test)\n    accuracy = accuracy_score(y_test, prediction)\n    \n    # Output scores\n    print('{} Accuracy: {}'.format(\n        model_name, accuracy\n    ))\n    print('Classification Report')\n    print('='*50)\n    print(classification_report(y_test, prediction))\n    \n    plt_conf_mat(y_test, prediction, model_name)\n    return accuracy, prediction","a1f0ec07":"# Starting simple with Logistic Regression\n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\n\nlr_acc, lr_pred = model_data(logistic_regression, 'Logistic Regression',\n                             (X_test, y_test))","d0c47aa0":"svc = SVC(C=10, gamma=0.01, kernel='rbf')\nsvc.fit(X_train, y_train)\n\nsvc_acc, svc_pred = model_data(svc, 'Support Vector Classifier',\n                             (X_test, y_test))","b07c5430":"decision_tree = DecisionTreeClassifier(criterion='entropy',\n                                       max_depth=10,\n                                       min_samples_leaf=1,\n                                       min_samples_split=20)\ndecision_tree.fit(X_train, y_train)\n\ndtree_acc, dtree_pred = model_data(decision_tree, \n                                  'Decision Tree Classifier',\n                                  (X_test, y_test))","5dbcc39c":"random_forest = RandomForestClassifier(criterion='entropy',\n                                       max_depth=20,\n                                       min_samples_leaf=1,\n                                       min_samples_split=10,\n                                       n_estimators=15)\nrandom_forest.fit(X_train, y_train)\n\nrf_acc, rf_pred = model_data(random_forest,\n                             'Random Forest Classifier',\n                             (X_test, y_test))","b7bab400":"gradient_boosting = GradientBoostingClassifier(criterion='mse',\n                                               learning_rate=0.025,\n                                               min_samples_leaf=9,\n                                               n_estimators=1000,\n                                               max_depth=3,\n                                               subsample=0.25)\n\ngradient_boosting.fit(X_train, y_train)\n\ngb_acc, gb_pred = model_data(gradient_boosting,\n                             'Gradient Boosting Classifier',\n                             (X_test, y_test))","5c524086":"# Lets first create a dataframe \n\npredictions = pd.DataFrame(columns=['DecisionTree',\n                                    'RandomForest',\n                                    'GradientBoosting'])\npredictions['DecisionTree'] = dtree_pred\npredictions['RandomForest'] = rf_pred\npredictions['GradientBoosting'] = gb_pred\n\n# Lets take a look at the end of the dataframe\npredictions.head()","2e4bed13":"final_predictions = []\nfor preds in predictions.iterrows():\n    final_predictions.append(mode(preds[1].values)[0][0])\n    \nplt_conf_mat(y_test, final_predictions, 'Max Voting')\nprint('Accuracy: {}'.format(accuracy_score(y_test, final_predictions)))\nprint('Classification Report')\nprint('='*50)\nprint(classification_report(y_test, final_predictions))","ae250f6b":"remove_cols = ['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin']\nX_train = train.drop(\n          labels=remove_cols,\n          axis=1\n        )\ny_train = train['Survived']\n\nremove_cols.remove('Survived')\nX_test = test.drop(\n        labels=remove_cols,\n        axis=1\n)","f4dc0e56":"# Shapes\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\nprint('y_train shape: {}'.format(y_train.shape))","a195cd37":"# Training the models\n\ndecision_tree = DecisionTreeClassifier(criterion='entropy',\n                                       max_depth=10,\n                                       min_samples_leaf=1,\n                                       min_samples_split=20)\nrandom_forest = RandomForestClassifier(criterion='entropy',\n                                       max_depth=200,\n                                       min_samples_leaf=1,\n                                       min_samples_split=10,\n                                       n_estimators=15)\ngradient_boosting = GradientBoostingClassifier(criterion='mse',\n                                               learning_rate=0.025,\n                                               min_samples_leaf=9,\n                                               n_estimators=1000,\n                                               max_depth=3,\n                                               subsample=0.25)\n\nmodel_preds = []\n\nfor model in [decision_tree, random_forest, gradient_boosting]:\n    model.fit(X_train, y_train)\n    model_preds.append(model.predict(X_test))\n    \n# Max voting\nmax_voting_pred = []\nfor row in np.array(model_preds).T:\n    max_voting_pred.append(mode(row)[0][0])","51e93763":"submission = pd.DataFrame(data={\n    'PassengerId': test['PassengerId'],\n    'Survived': model_preds[2]\n})\nsubmission.tail(5)","1c7707d7":"# submission.to_csv('submission.csv', index=False)","9da1ca55":"# Gradient Boosting\n\n**Looking for the best parameters using GridSearchCV**\nCode for grid search:<br\/>\n\n```python\nparams = {\n    'n_estimators': [10,50,100,250,500,750,1000],\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.01, 0.025, 0.05, 0.1, 0.5, 1],\n    'subsample': [0.25, 1.0, 1.5],\n    'criterion': ['friedman_mse', 'mse'],\n    'min_samples_leaf': [2, 5, 9, 10]\n}\n\ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid=params, verbose=.5, n_jobs=-1, cv=5)\ngrid.fit(X_train, y_train)\n```\n\nBest parameters turned out to be:<br\/>\n\n```\ngrid.best_params_\n>>> {'criterion': 'mse',\n     'learning_rate': 0.025,\n     'loss': 'exponential',\n     'min_samples_leaf': 9,\n     'n_estimators': 1000,\n     'subsample': 0.25}\n```","272e72bd":"# Training with the entire dataset and creating result","542bafd5":"# Variable Identification\n\nIn this step our goal is to identify different kinds of variables present in our dataset and proceed accordingly. For example, **Dependent** and **Independent** variables:\n\n### Dependent Variable\nThe variable that depends on other factors that are measured. These variables are expected to change as a result of an experimental manipulation of the independent variable(s).\n### Independent Variable\nThe variable that is stable and unaffected by the other variables you are trying to measure. If refers to the condition of an experiment that is systematically manipulated by the investigator.\n\nSimply, The **independent variable** is the **cause**. Its value is independent of other variables in your study. Whereas, the **dependent variable** is the **effect**. Its value depends on changes in the independent variable.\n\n### Categorical Variable\nA categorical variable (sometimes called a nominal variable) is one that has two or more categories, but there is no intrinsic ordering to the categories. For example, gender is a categorical variable having two categories (male and female) and there is no intrinsic ordering to the categories.\n\n### Ordinal Variable\nAn ordinal variable is similar to a categorical variable. The difference between the two is that there is a clear ordering of the categories. For example, suppose you have a variable, economic status, with three categories (low, medium and high). In addition to being able to classify people into these three categories, you can order the categories as low, medium and high.\n\n\n\n\nRefer to this [article](https:\/\/www.scribbr.com\/methodology\/independent-and-dependent-variables\/) if you want to read about this topic. For distinction between **categorical**, **ordinal** and **numerical** variables refer to this [page](https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/whatstat\/what-is-the-difference-between-categorical-ordinal-and-numerical-variables\/)","d3827ba0":"# Bivariate analysis\n\nBivariate analysis means the analysis of bivariate data. It is one of the simplest form of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variable X and Y.<br\/>\nRead more from the [source](https:\/\/www.statisticshowto.com\/bivariate-analysis\/)","1213166d":"### Binning\n\n_[Defn](https:\/\/en.wikipedia.org\/wiki\/Data_binning):_ **Binning** or **Bucketing** is a data pre-processing technique used to reduce the effects of minor observations errors. The original data values which fall into a given small interval, a bin, are replaced by a value representative of that interval, often the central value. It is a form of quantization.\n\nIn the following part, we are going to apply binning. Binning or discretization is the process of transforming numerical variables into categorical counterparts. An example is to bin values for Age into categories such as 20-39, 40-59, 60-79 etc. In our case we will be binning less frequent titles to \"Others\".<br\/>\nBinning may improve accuracy of the predictive models by replacing the noise or non-linearity. Finally, binning allows easy identification of outliers, invalid and missing values of numerical variables. ([source](https:\/\/www.saedsayad.com\/binning.htm))","2c74c690":"# Submission","f5cb10a3":"# Hypothesis Generation:\n\nIn this step we would have to be a little creative and try think outside the box. We would have to come up with different hypothesis for the cases for survival of this tragedy. We may have some intuition such as, _females and children is more likely to be prioritized for rescue, Survival rate should increate as the fare increases (Since the higher paying passengers may be more important)_ and so on. For more, we would have to dive deep into the dataset itself. Whatever hypothesis we come up with in this phase, our task would be to validate them during the **Data Exploration** phase.<br\/>\n\n","67d6735b":"# Decision Tree\n\n**Looking for the best parameters using GridSearchCV**\nCode for grid search:<br\/>\n\n```python\nparams = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5,10,20,50,100],\n    'min_samples_split': [2,5,10,15,20,30,40],\n    'min_samples_leaf': [1,2,5,7,10,15,20,35,50]\n}\n\ngrid = GridSearchCV(DecisionTreeClassifier(),\n                    param_grid=params,\n                    n_jobs=-1,\n                    verbose=.5)\n\ngrid.fit(X_train, y_train)\n```\n\nBest parameters turned out to be:<br\/>\n\n```\ngrid.best_params_\n>>> {'criterion': 'entropy',\n     'max_depth': 10,\n     'min_samples_leaf': 1,\n     'min_samples_split': 10}\n```","37d711a2":"# Handling Missing Values\n\nDepending on the situation there are mainly three types of missing values:\n* **Missing Completely at Random (MCAR)**\n* **Missing at Random (MAR)**\n* **Missing Not at Random (MNAR)**\n\nBut there are many ways of handling missing values, For example\n#### Imputation:\nImputation is a procedure for entering a value for specific data item where the response is missing or unusable. ([source](https:\/\/stats.oecd.org\/glossary\/detail.asp?ID=3462))\n#### Dropping:\nIn this method we simply drop the rows containing missing data. Although this method is very straaight forward, but now very practical since if we're missing some attribute's entry for majority of the rows then dropping the rows would just result in a loss of information.\n\nFor imputation this following simplication works out (most of the time)\n* Imputation\n    * Continuous Variable\n        * Mean\n        * Median\n        * Regression\n    * Categorical Variable\n        * Mode\n        * Classification","53b3408c":"# Data Extraction\n\nWell, we wont have to do much in this stage, since the data is provided by Kaggle in a CSV format. So in this case, data extraction is as simple as reading a csv file. In real life scenario however, this is one of the most tedious stages of them all, we may have to work on scrappers and alot of automation pipelines.","8eda1d39":"# Feature Engineering\n\nThe most dynamic step in any data science project, Feature Engineering is when we generate attributes based on the acquired dataset. This requires alot of knowledge in the domain and well understanding of the dataset we're working with.\n\n_**Defn**: Feature engineering is the process of transformaing raw data into features that better represent the underlying problem to predictive models, resulting in improved model accuracy on unseen data._\n\nRefer to the [this](https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/) article to learn more about the importance of feature engineering. There are some great kernels for this dataset which goes into Feature Engineering in much more details. I will add a link to some of them below.<br\/>\n\nFor now, we are just creating the following attributes<br\/>\n`FamilySize`, `IsAlone`, `Title`, `FareBin`, `AgeBin`","ae4fb9ab":"# Imports\n\n**For Exploratory Data Analysis**","1942bc8c":"## Inputing Embarked Column\nThe Embarked column had the least number of missing values, so we're simply going to impute this with the mode (Since this is a categorical attribute)","d678f883":"# Basic Lookup\n\nNow that we have read the dataset, lets take a look at it with the following pandas methods ```DataFrame.head()```, ```DataFrame.describe()``` and ```DataFrame.info()```. These don't do much in terms of finding insights, but they sure help finding inconsistencies of the top. Like number of missing values, data types etc.","03f20254":"Now, this is not a must, but it's a good practise to check what versions of the different modules we will be working with.","403ac3b9":"# Max Voting\n\nMax-voting, which is generally used for classification problems, is one of the simplest ways of combining predictions from multiple machine learning algorithms. In max-voting, each base model makse a prediction and votes for each sample. One the sample class with the highest votes is included in the final predictive class.<br\/>\n\nScikit-Learn already has the `sklearn.ensemble.VotingClassifier`, where in the `voting` argument we can give one of two values `hard`, `soft` (defaults to `hard`). If 'hard', the classifier uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. But here, we are doing things manually ('hard' voting)<br\/>\n\nRead more about the class from scikit-learns [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html).\n\n### Using DecisionTree, RandomForest, GradientBoosting Classifiers","164027ab":"# References\n\n* https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/whatstat\/what-is-the-difference-between-categorical-ordinal-and-numerical-variables\/\n* https:\/\/en.wikipedia.org\/wiki\/Data_exploration\n* https:\/\/stats.oecd.org\/glossary\/detail.asp?ID=3462ome ama\n* https:\/\/www.statisticshowto.com\/bivariate-analysis\/\n\n## Some amazing notebooks for this dataset\n\n* https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n* https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n* https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n\n### Thank you so much for the view! ","85711e0d":"# Problem Definition:\n\n![Titanic](https:\/\/media.snl.no\/media\/26195\/article_topimage_Titanic-Cobh-Harbour-1912.JPG)\n\nOut of all the aforementioned stages, this is probably the simplest one, since most of the times the problem definition is given and what you are pursuing. For example, for this challenge, the problem definition is _\"Given a set of passenger attributes, can we come up with a system which makes prediction whether a passenger (who's attributes will not be known to the system) has survived or not.\"_","5231e991":"So, it seems like the presence of cabin attribute plays a very significant role to the final outcome. Lets create a column indicating wheather cabin data is present or not.","d3d63870":"# Support Vector Classifier\n\n**Looking for the best parameters using GridSearchCV**\nCode for grid search:<br\/>\n\n```python\nparams = {\n    'C':[0.001,0.01,0.1,1,10,100],\n    'gamma': [0.01,0.1,1,10,100]\n}\n\ngrid = GridSearchCV(SVC(), param_grid=params, verbose=.5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n```\n\n**Ouput:**<br\/>\n```\nGridSearchCV(cv=None, error_score=nan,\n             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n                           class_weight=None, coef0=0.0,\n                           decision_function_shape='ovr', degree=3,\n                           gamma='scale', kernel='rbf', max_iter=-1,\n                           probability=False, random_state=None, shrinking=True,\n                           tol=0.001, verbose=False),\n             iid='deprecated', n_jobs=-1,\n             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n                         'gamma': [0.01, 0.1, 1, 10, 100]},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0.5)\n```\n\nBest parameters turned out to be:<br\/>\n\n```\ngrid.best_params_\n>>> {'C': 10, 'gamma': 0.01}\n```","52808e58":"## Reading the datasets","afcf5f52":"# Predictive Modelling\n\n## Necessary Imports","81cea3c0":"# Looking for missing values\n\nFor now lets just check if we have some missing values in our dataset, remember to check in both train as well as test dataset. Don't worry it's not cheating, we're just making sure that we are not missing any field, and if so we are taking proper measure to deal with them. If we were to alter our model based on the test dataset that would have been cheating. :)","98f3e47c":"# Introduction\nIn this notebook we will see the necessary stages required to get a descent score on the **Titanic: Machine Learning from Disaster** dataset. We will be going through all the steps and not try not to skip out on any important stage. While getting into any problem which involves AI, we generally need to go through the following stages.\n- Problem Definition\n- Hypothesis Generation\n- Data Extraction\n- Data Exploration and Transformation\n- Model Building","fd1f7df5":"# Univariate Analysis\n\nUnivariate analysis is the simplest form of analyzing data. \"Uni\" means \"one\", so in other words your data has only one variable. It doesn't deal with causes or relationships (unlike regression) and it's major purpose is to describe; it takes data, summarizes that data and finds patterns in the data.<br\/>\nA variable in univariate analysis is just a condition or subset that your data falls into. You can think of it as a \"category\". For example, the analysis might look at a variable of \"age\" or it might look at \"height\" or \"weight\". However, it doesn't look at more than one variable at a time otherwise it becomes bivariate analysis (or in the case of 3 or more variables it would be called multivariate analysis).<br\/>\nRead more from the [source](https:\/\/www.statisticshowto.com\/univariate\/)","4037565f":"Simplest of the bunch\n## Logistic Regression","d9db7483":"# Random Forest\n\n**Looking for the best parameters using GridSearchCV**\nCode for grid search:<br\/>\n\n```python\nparams = {\n    'n_estimators': [2,5,10,15,20,25,30],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [10,20,50,100,150,200,300,450,500],\n    'min_samples_leaf': [1,3,5,10,15],\n    'min_samples_split': [2,5,10,15,20,35,50]\n}\n\ngrid = GridSearchCV(RandomForestClassifier(), param_grid=params, verbose=.5, n_jobs=-1, cv=5)\ngrid.fit(X_train, y_train)\n```\n\nBest parameters turned out to be:<br\/>\n\n```\ngrid.best_params_\n>>> {'criterion': 'entropy',\n     'max_depth': 20,\n     'min_samples_leaf': 1,\n     'min_samples_split': 10,\n     'n_estimators': 15}\n```","8f0772f0":"# Data Exploration\n\n[Data Exploration](https:\/\/en.wikipedia.org\/wiki\/Data_exploration) is probably the most important step in any data science project. In this stage the dataset will go through a series of transformation (depending on the problem of course). This step can be divided into following substages:\n- **Variable Identification** (Dependent, Independent, Categorical, Continuous etc)\n- **Handling Missing values** (Looking for missing values and then taking appropriate measures)\n- **Univariate Analysis** (Continuous Variable Analysis, Categorical Variable Analysis)\n- **Bivariate Analysis** (Study of the empirical relationships among variables)\n- **Outlier Treatment**\n\nThere are some great kernels for the Titanic Dataset, which has way more detailed approach to Data Exploration. I will link some of those notebooks at the end."}}