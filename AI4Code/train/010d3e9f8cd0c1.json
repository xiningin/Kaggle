{"cell_type":{"ac0f2231":"code","18438700":"code","21abeb4e":"code","06c9dfa4":"code","6e8ba1b4":"code","a36e1ef4":"code","f2af1ff8":"code","793b3d2b":"code","27f69a52":"code","b3bc773c":"code","fc31fa50":"code","eec89d86":"code","e91888b0":"code","0d57d90c":"code","54dffba9":"code","1a9c1557":"code","fb7ac39b":"code","e975ea0c":"code","d81de44c":"code","8f7c346d":"code","1e8df6fd":"code","3461e276":"code","1fa51bb1":"code","ee0052ef":"code","904d4c45":"code","ffd75253":"code","3edd0719":"code","07109155":"code","832a837d":"code","c47ca839":"code","2cdcb9bd":"code","60746f5f":"code","cf29cf3e":"code","39a822b1":"code","b856bac4":"code","428565f4":"code","842569f0":"code","7c1380d0":"code","61b80013":"code","c0141093":"code","e14d6b88":"code","2c92cc9d":"code","8a158ac0":"code","a920042e":"code","41fec7b4":"code","719053b4":"code","2d7310a3":"code","94feb121":"code","71b7649e":"code","d0138b1d":"code","49a002fd":"code","2d34a82a":"code","b0755f05":"code","6127228c":"code","54841569":"code","74ecf84a":"code","09bca85f":"code","d51d85a1":"code","4a2c6547":"code","fac7e8ca":"code","ca5afa2d":"code","b6c9eb77":"code","fd0cd3a6":"code","68953cb9":"code","9f8353fa":"code","bca81a6c":"code","5c68f0c2":"code","e1c20f95":"code","695e3817":"code","0c6c7a6c":"code","1883cf6a":"code","7dede84c":"code","d9653ac1":"code","3598a411":"code","f1741041":"code","afddbead":"code","5800e407":"code","0111b2cc":"code","6ff09f10":"code","c6ba0fa9":"code","37eec8ef":"code","d81ae057":"code","0b2f8aa7":"code","b89f596e":"code","1ab25ccd":"code","76b50360":"code","25057715":"code","e8b01023":"code","b9b50ff2":"code","7b581a9a":"code","fa9bff6f":"code","60c730b4":"code","085e1012":"code","51912cae":"markdown","c9ee434b":"markdown","2f25ccb9":"markdown","fb70aecb":"markdown","8c021e21":"markdown","e2016007":"markdown","74a135bf":"markdown","04705f9f":"markdown","35ddfab4":"markdown","982ed1c9":"markdown","08603187":"markdown"},"source":{"ac0f2231":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18438700":"import datatable as dt\nfrom tqdm import tqdm","21abeb4e":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl","06c9dfa4":"!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","6e8ba1b4":"# treelite\nimport treelite\nimport treelite_runtime ","a36e1ef4":"train_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ntrain_data = train_data_datatable.to_pandas()","f2af1ff8":"df=train_data.copy()","793b3d2b":"# train_data['date']\n# df[['weight','resp']].describe()","27f69a52":"# df['weight'].value_counts()\/len(df)\n# 0.17 20%\u5de6\u53f3\n#  \u4e3a\u4ec0\u4e48weight=0\u4e5f\u4f1a\u653e\u4e0a\u6765\n# \u867d\u7136weight=0 \u4f46\u662fresp\u4ecd\u662f\u51c6\u786e\u7684\u5427 \u5ba4\u53cb\u6709\u4fe1\u606f\u53ef\u7528\n# df[df['weight']==0]['resp'].plot(kind='hist')","b3bc773c":"# len(df[(df['weight']==0) & (df['resp']>0)])\/len(df[df['weight']==0])","fc31fa50":"# len(df[df['resp']>0])\/len(df)","eec89d86":"# len(df[(df['weight']!=0) & (df['resp']>0)])\/len(df[df['weight']!=0])","e91888b0":"# # \u5dee\u5f02\u663e\u8457\u6027\u68c0\u9a8c\n# deltap=0.5044130406145096-0.5024071123449428\n# p=0.504069666022587\n# z=(deltap)\/np.sqrt(p*(1-p)*(1\/len(df[df['weight']==0])+1\/len(df[df['weight']!=0])))\n# z","0d57d90c":"# \u53ef\u89c1weight\u4e0b\u786e\u5b9e\u6709\u4e00\u4e9b\u7504\u522b\u4f5c\u7528\n# import scipy.stats as st\n# st.norm.cdf(z)","54dffba9":"# df['resp'].plot(kind='hist')\n# \u9650\u5b9aweight==0\u524d\u540e\u7684resp\u5206\u5e03\u57fa\u672c\u4e00\u81f4 \u6240\u4ee5\u4fe1\u606f\u57fa\u672c\u662f\u6709\u7528\u7684\n# weight\u662f\u4ec0\u4e48\uff1f\n# weight\u91cc\u9762\u4e5f\u662f\u542b\u6709\u9884\u5224\u548c\u4fe1\u606f\u7684 \u6240\u4ee5\u53ef\u4ee5\u7406\u8bba\u4e0a\u53ef\u4ee5\u52a0\u5165\u7279\u5f81\u7684\u884c\u5217","1a9c1557":"# \u5185\u5728\u903b\u8f91\u662f \u5bf9\u4e8eweight>0\u7684\u80a1\u7968 \u6211\u8fdb\u884c\u4e86\u4ea4\u6613\uff0c\u7136\u540e\u4f53\u73b0\u51fa\u6765\u7684resp\u662f\u5e02\u9762\u4e0a\u7684\u6536\u76ca\u60c5\u51b5\uff1f-- \u56e0\u4e3aweight=0\u65f6\u540c\u6837\u6709resp","fb7ac39b":"# len(df[(df['weight']>=0.5) & (df['resp']>0)])\/len(df[df['weight']>=0.5])","e975ea0c":"# len(df[(df['weight']>=1) & (df['resp']>0)])\/len(df[df['weight']>=1])","d81de44c":"# \u7ee7\u7eed\u770b\u4e0bweight\u4e0eresp\u7684\u5173\u8054","8f7c346d":"# \u56de\u5f52\u95ee\u9898","1e8df6fd":"# \u591a\u76ee\u6807\u600e\u6837\u5229\u7528","3461e276":"# xgb\u4e0eNN\u7684\u96c6\u6210","1fa51bb1":"## \u7f3a\u5931\u503c\u6682\u4e0d\u5904\u7406","ee0052ef":"# np.percentile(df['weight'],[0,5,25,50,75,95,100])","904d4c45":"## \u6682\u65f6\u53ea\u53d6weight>0\u7684\u90e8\u5206\n# df=df[df['weight']>0]","ffd75253":"## \u76ee\u524d\u5148\u76f4\u63a5\u6839\u7eddresp\u6b63\u8d1f\u6027\u8f6c\u5316\u4e3a0-1\u95ee\u9898\ndf['action']=df['resp'].apply(lambda x: 1 if x>0 else 0)","3edd0719":"# df['action'].value_counts()","07109155":"import copy\ndef add_features(df, features):\n    new_features = copy.deepcopy(features)\n    \n    # todo\n    df[\"cross_1_2\"] = df[\"feature_1\"] \/ (df[\"feature_2\"] + 1e-5)\n    df[\"cross_41_42_43\"] = df[\"feature_41\"] + df[\"feature_42\"] + df[\"feature_43\"]\n    new_features.extend([\"cross_1_2\", \"cross_41_42_43\"])\n\n    return df, new_features","832a837d":"## \u4f7f\u7528\u539f\u59cb\u5168\u90e8feature\nfeatures = [c for c in df.columns if 'feature' in c]\ntarget = ['action']","c47ca839":"df,new_features=add_features(df,features)","2cdcb9bd":"del train_data","60746f5f":"# original dataframe;feature list;target list;and time point used to split dataset\n# def splitbydate(oridf,feature,target,test_end,test_start,train_end,train_start=0): \n#     if 'date' not in oridf.columns:\n#         print('Error!')\n#         return\n#     else:\n#         feature_=feature.copy()\n#         target_=target.copy()\n#         feature_.append('date')\n#         target_.append('date')\n#         #print(target_)\n#         x_df=oridf[feature_]\n#         y_df=oridf[target_]\n#         X_train,X_test=x_df[(x_df['date']>=train_start) & (x_df['date']<train_end)],x_df[(x_df['date']>=test_start) & (x_df['date']<test_end)]\n#         y_train,y_test=y_df[(y_df['date']>=train_start) & (y_df['date']<train_end)],y_df[(y_df['date']>=test_start) & (y_df['date']<test_end)]\n#         vali=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=test_start) & (oridf['date']<test_end)]\n#         return X_train[feature],X_test[feature],y_train[target],y_test[target],vali","cf29cf3e":"# original dataframe;feature list;target list;and time point used to split dataset\ndef splitbydate_more(oridf,feature,target,test_end,test_start,train_end,train_start=0): \n    if 'date' not in oridf.columns:\n        print('Error!')\n        return\n    else:\n        feature_=feature.copy()\n        target_=target.copy()\n        feature_.append('date')\n        target_.append('date')\n        #print(target_)\n        x_df=oridf[feature_]\n        y_df=oridf[target_]\n        X_train,X_test=x_df[(x_df['date']>=train_start) & (x_df['date']<train_end)],x_df[(x_df['date']>=test_start) & (x_df['date']<test_end)]\n        y_train,y_test=y_df[(y_df['date']>=train_start) & (y_df['date']<train_end)],y_df[(y_df['date']>=test_start) & (y_df['date']<test_end)]\n        vali=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=test_start) & (oridf['date']<test_end)]\n        trai=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=train_start) & (oridf['date']<train_end)]\n        return X_train[feature],X_test[feature],y_train[target],y_test[target],trai,vali","39a822b1":"# X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,features,target,500,450,400,85)","b856bac4":"def utility_scoring(df):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    from math import sqrt\n    u = 0\n    Pi = []\n    Pis = []\n    count_i = len(df['date'].unique())\n    for i in list(df['date'].unique()):\n        #print(\"date value= \", i)\n        #print(10*\"=\")\n        tmp = df[df['date'] == i][[\"date\",\"weight\",\"resp\",\"action\"]]\n        tmp[\"mult\"] = tmp['weight'] * tmp['resp'] * tmp['action']\n        Pi.append(tmp[\"mult\"].sum())\n        Pis.append((tmp[\"mult\"].sum())**2)\n    \n    t =  sum(Pi)\/sqrt(sum(Pis)) * sqrt(250\/count_i)\n    u = min(max(t,0),6)*sum(Pi)\n    return u","428565f4":"thres=0.5","842569f0":"import xgboost as xgb","7c1380d0":"# we need\/have\n# \u5e76\u4e0d\u6d89\u53ca\u5230\u4e0e\u539f\u59cb\u771f\u5b9e\u503c\u7684\u6bd4\u8f83 \u53ea\u662f\u81ea\u987e\u81ea\u5f97\u8f93\u51fa\n# \u5148\u5f04\u4e00\u4e2a\u7edd\u5bf9\u8bc4\u5206\u7684\u51fd\u6570- \u5bf9\u6bd4\u5f97\u5206\u4e0eauc\u8868\u73b0\u7684\u4e00\u81f4\u6027\n# \u53ef\u4ee5\u5f04\u4e00\u4e2a\u6307\u6807 \u4f8b\u5982\u9884\u6d4b\u5f97\u5206\/\u771f\u5b9e\u5f97\u5206\n\n# trai vai\ndef utility_score(predt,dtrain):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    y = dtrain.get_label()\n    tarlen=len(y)\n    if tarlen==len(trai):\n        u=utility_scoring(trai)\n    elif tarlen==len(vali):\n        u=utility_scoring(vali)\n#     vali['actionv'] = (predt> 0.5)*1\n    \n    return 'utilityscore',float(u)\n\n# trai vai\ndef relative_utility_score(predt,dtrain):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    \n    y = dtrain.get_label()\n    tarlen=len(y)\n    if tarlen==len(trai):\n        trai['action']=(predt> thres)*1\n        u2=utility_scoring(trai)\n        u=u2\/trai_ori\n    elif tarlen==len(vali):\n        vali['action']=(predt> thres)*1\n        u2=utility_scoring(vali)\n        u=u2\/vali_ori\n#     vali['actionv'] = (predt> 0.5)*1\n    \n    return 'r_utilityscore',float(u)","61b80013":"# params['learning_rate']=0.05\n# params","c0141093":"# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n# bst = xgb.train(params,\n#                 dtrain,\n#                 feval=utility_score,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20)","e14d6b88":"# trai_ori=utility_scoring(trai)\n# vali_ori=utility_scoring(vali)","2c92cc9d":"import gc\ngc.enable()","8a158ac0":"# early_stop = xgb.callback.EarlyStopping(rounds=10,\n#                                     metric_name='r_utilityscore',\n#                                     data_name='eval')","a920042e":"SEED = 21\nparams = {\n    #general parameter\n    \n    'booster':'gbtree',\n    \n    #tree parameter\n    \n    # shrink the weight to make the process more conservative\n    'learning_rate': 0.05,\n    # minimum loss reduce needed to make a new partition\n    'gamma':0,\n    # max depth of the tree\n    'max_depth': 8,\n    'min_child_weight':10,\n    'tree_method': 'gpu_hist', # Let's use GPU for a faster experiment\n    # \u968f\u673a\u91c7\u6837\n    'colsample_bytree': 0.72,                 \n    'subsample': 0.8,\n    \n    'seed': SEED,\n    \n    #print every 10 times\n#     'verbose_eval':100,\n   \n    #learning task parameter\n    # \u53ef\u8003\u8651\u66f4\u6362\u76ee\u6807\u51fd\u6570\u4e3aauc\n    'objective':'binary:logistic',\n    'disable_default_eval_metric': 1\n#     'eval_metric':['auc']\n#     'n_boost_round': 600,\n#     'early_stopping_rounds':20\n    \n\n}","41fec7b4":"# results={}","719053b4":"# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n\n# bst = xgb.train(params,\n#                 dtrain,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 feval=relative_utility_score,\n#                 num_boost_round=50,\n#                 callbacks=[early_stop]\n# #                 evals_result=results\n# #                 early_stopping_rounds=10\n#                )","2d7310a3":"# import matplotlib.pyplot as plt","94feb121":"# plt.figure(figsize=(15,10))\n# plt.plot(np.arange(1000),results['train']['r_utilityscore'])\n# plt.plot(np.arange(1000),results['eval']['r_utilityscore'])","71b7649e":"# import matplotlib.pyplot as plt\n# plt.figure(figsize=(20,15)) \n# xgb.plot_importance(bst,ax=plt.gca())","d0138b1d":"# fscore=pd.DataFrame.from_dict(bst.get_fscore(),orient='index')\n# fscore.columns=['fs']\n# fscore=fscore.sort_values(by=['fs'],ascending=False)\n# # fscore","49a002fd":"# plt.figure(figsize=(15,10))\n# plt.bar(fscore.index,fscore['fs'].values)","2d34a82a":"# fscore[fscore['fs']>1000]","b0755f05":"# print(\"# original features: {}\".format(len(features)))\n# print(\"# features included in xgb model: {}\".format(len(fscore)))","6127228c":"# import re\n# def cutnum(x):\n#     res=re.match(r'([a-z]*)([0-9]*)',x).group(2)\n#     return int(res)","54841569":"# one feature has none improvement on prediction\n# select top 60% feature to service for final training\n# fscore['num_f']=fscore.index.map(cutnum)\n# fscore_se=fscore[:90]\n# fscore_se","74ecf84a":"# f_feature=[]\n# for i in fscore_se['num_f']:\n# #     print(i)\n#     f_feature.append(features[i])\n# f_feature","09bca85f":"# new_features","d51d85a1":"# del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid","4a2c6547":"X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,new_features,target,500,500,85+415,85)\ndtrain = xgb.DMatrix(X_train.values, label=y_train.values)\ntrai_ori=utility_scoring(trai)\n# vali_ori=utility_scoring(vali)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\nbst = xgb.train(params,\n        dtrain,\n        num_boost_round=600)","fac7e8ca":"# subtree_depth_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(8,13)\n#     for min_child_weight in range(7,10)\n# ]","ca5afa2d":"# subtree_depth_params","b6c9eb77":"# cv_times=5\n# start_list=[90,110,130,150,170]","fd0cd3a6":"# import gc\n# gc.enable()\n\n# del train_data","68953cb9":"# bestscore=0\n# bestparams=params\n# cv_score=[]","9f8353fa":"# def tune_parameters(params,params_valuelist):\n#     for max_depth, min_child_weight in params_valuelist:\n#         print(\"CV with max_depth={}, min_child_weight={}\".format(\n#                                  max_depth,\n#                                  min_child_weight))\n#         # Update our parameters\n#         params['max_depth'] = max_depth\n#         params['min_child_weight'] = min_child_weight\n#         score=[]\n#         for i in range(cv_times):\n#             # train length 250\n#             # test legth 50\n#             # 500 450 420 170\n#             # 480 430 400 150\n#             # 460 410 380 130\n#             # 110\n#             # 90\n#             X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#             dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#             dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#             bst = xgb.train(params,\n#                     dtrain,\n#                     num_boost_round=500,\n#                     evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                     early_stopping_rounds=20,\n#                     verbose_eval=100)\n#             score.append(bst.best_score)\n#             del X_train,X_test,y_train,y_test,vali\n\n#         cv_score.append(np.mean(score))\n#     #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n#     #                              max_depth,\n#     #                              min_child_weight))\n#         if np.mean(score)>bestscore:\n#     #         bestparams=params\n#             bestscore=np.mean(score)\n#             best_max_depth,best_min_child_weight=max_depth,min_child_weight\n\n#     bestparams['max_depth'] = max_depth\n#     bestparams['min_child_weight'] = min_child_weight\n#     bestparams","bca81a6c":"# for max_depth, min_child_weight in subtree_depth_params:\n#     print(\"CV with max_depth={}, min_child_weight={}\".format(\n#                              max_depth,\n#                              min_child_weight))\n#     # Update our parameters\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     score=[]\n#     for i in range(cv_times):\n#         # train length 250\n#         # test legth 50\n#         # 500 450 420 170\n#         # 480 430 400 150\n#         # 460 410 380 130\n#         # 110\n#         # 90\n#         X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#         dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#         dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#         bst = xgb.train(params,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20,\n#                 verbose_eval=100)\n#         score.append(bst.best_score)\n#         del X_train,X_test,y_train,y_test,vali\n    \n#     cv_score.append(np.mean(score))\n# #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n# #                              max_depth,\n# #                              min_child_weight))\n#     if np.mean(score)>bestscore:\n# #         bestparams=params\n#         bestscore=np.mean(score)\n#         best_max_depth,best_min_child_weight=max_depth,min_child_weight\n        \n# bestparams['max_depth'] = max_depth\n# bestparams['min_child_weight'] = min_child_weight\n# bestparams","5c68f0c2":"# score","e1c20f95":"# import matplotlib.pyplot as plt","695e3817":"# plt.plot([str(x) for x in subtree_depth_params],cv_score)","0c6c7a6c":"# bestparams","1883cf6a":"# bestscore","7dede84c":"# subtree_depth_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(6,10)\n#     for min_child_weight in range(8,12)\n# ]","d9653ac1":"# del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid","3598a411":"import gc\ngc.enable()","f1741041":"# bestscore=0\n# bestparams=params\n# cv_score=[]\n# for lr in [.3, .2, .1, .05, .01, .005]:\n#     print(\"learning rate={}\".format(\n#                              lr))\n#     # Update our parameters\n#     params['learning_rate'] = lr\n#     score=[]\n#     for i in range(cv_times):\n#         X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,f_feature,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#         trai_ori=utility_scoring(trai)\n#         vali_ori=utility_scoring(vali)\n#         dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#         dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#         bst = xgb.train(params,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 feval=relative_utility_score,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20,\n#                 verbose_eval=100)\n#         score.append(bst.best_score)\n#         del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid\n    \n#     cv_score.append(np.mean(score))\n# #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n# #                              max_depth,\n# #                              min_child_weight))\n#     if np.mean(score)>bestscore:\n#         bestparams=params\n#         bestscore=np.mean(score)","afddbead":"# cv_score","5800e407":"# plt.plot([str(x) for x in subtree_depth_params],cv_score)\n# plt.xticks(rotation=60)","0111b2cc":"# bestparams['max_depth'] = 6\n# bestparams['min_child_weight'] = 10\n# bestparams","6ff09f10":"# del X_train,X_test,y_train,y_test,vali","c6ba0fa9":"# X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,500,470,440,85)\n# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n# bst = xgb.train(bestparams,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20)","37eec8ef":"# \u57fa\u4e8etreelite\u8fdb\u884c\u7f16\u8bd1\nmodel = treelite.Model.from_xgboost(bst)","d81ae057":"# generate shared library\ntoolchain = 'gcc'\nmodel.export_lib(toolchain=toolchain, libpath='.\/mymodel_score_600rounds_fullfeature_1.so',\n                 params={'parallel_comp': 32}, verbose=True)","0b2f8aa7":"# predictor from treelite\n# predictor = treelite_runtime.Predictor('.\/mymodel_score.so', verbose=True)","b89f596e":"# dtest=xgb.DMatrix(X_test.values, label=y_test.values)","1ab25ccd":"# def utility_scoring(df):\n#     \"\"\"\n#     To get the utility score used in the challenge.\n#     \"\"\"\n#     from math import sqrt\n#     u = 0\n#     Pi = []\n#     Pis = []\n#     count_i = len(df['date'].unique())\n#     for i in list(df['date'].unique()):\n#         #print(\"date value= \", i)\n#         #print(10*\"=\")\n#         tmp = df[df['date'] == i][[\"date\",\"weight\",\"resp\",\"actionv\"]]\n#         tmp[\"mult\"] = tmp['weight'] * tmp['resp'] * tmp['actionv']\n#         Pi.append(tmp[\"mult\"].sum())\n#         Pis.append((tmp[\"mult\"].sum())**2)\n    \n#     t =  sum(Pi)\/sqrt(sum(Pis)) * sqrt(250\/count_i)\n#     u = min(max(t,0),6)*sum(Pi)\n#     return u","76b50360":"# treelite\n# batch = treelite_runtime.Batch.from_npy2d(X_test.values)\n# predicted_treelite = predictor.predict(batch)","25057715":"# predicted_treelite","e8b01023":"# from sklearn.metrics import log_loss\n# score = log_loss(y_test.values, predicted_treelite)\n# print(score)","b9b50ff2":"# best_i = 0\n# best_u = 0\n# for i in [0.41, 0.45,0.49,0.5, 0.51,0.52, 0.55, 0.6,0.63, 0.65]:\n    \n#     vali['action'] = (predictor.predict(batch) > i)*1\n#     u = utility_scoring(vali)\n#     print(u)\n#     if u > best_u:\n#         best_u = u\n#         best_i = i","7b581a9a":"# import janestreet\n# env = janestreet.make_env() # initialize the environment\n# iter_test = env.iter_test() # an iterator which loops over the test set","fa9bff6f":"# threshold=best_i","60c730b4":"# best_i","085e1012":"# for (test_df, pred_df) in tqdm(iter_test):\n#     if test_df['weight'].item() > 0:\n#         # inference with treelite\n#         batch = treelite_runtime.Batch.from_npy2d(test_df.loc[:, features].values)\n#         pred_df.action = (predictor.predict(batch) > threshold).astype('int')\n#     else:\n#         pred_df.action = 0\n#     env.predict(pred_df)","51912cae":"## \u5f53\u524d\u60c5\u51b5\u66f4\u65b0\uff0820210216\uff09  \n1. \u5173\u4e8eauc \u4e0e utility score\u7684\u5173\u7cfb  auc\u57fa\u672c\u7ef4\u6301\u4e0d\u53d8\uff0cus\u4f1a\u968f\u7740\u8fed\u4ee3\u6b21\u6570\u7684\u589e\u52a0\u589e\u957f\uff0c\u4f46\u5728LB\u4e0a\u7684\u8868\u73b0\u4e0d\u89c1\u597d\u8f6c  \n2. \u5173\u4e8e\u591a\u9636\u6bb5\u6a21\u578b","c9ee434b":"## \u672c\u7248\u672c\u91cd\u8981\u66f4\u6539 \n\u63cf\u8ff0\uff1a\u63d0\u524d\u6620\u5c04\u62100-1 \u8f6c\u5316\u4e3a0-1classification\n1. missing=none\n2. \u5b8c\u6210cv\n3. \u5b8c\u6210\u4e00\u7ec4\u53c2\u6570\u7684\u5bfb\u4f18 metric  \n## \u672c\u7248\u672c\u76ee\u6807 8000","2f25ccb9":"## prepare the librarias used below","fb70aecb":"## weight=0 but resp's info is ready to use","8c021e21":"## generate ground truth action","e2016007":"# xgboost\u53c2\u6570\nhttps:\/\/www.kaggle.com\/code1110\/janestreet-faster-inference-by-xgb-with-treelite","74a135bf":"# load data","04705f9f":"# Simple pre_process for xgb","35ddfab4":"1,0 \u8fd8\u662f\u6bd4\u8f83\u5747\u8861\u7684\uff0c\u6240\u4ee5\u4e0d\u5b58\u5728\u6837\u672c\u4e0d\u5747\u8861\u7684\u95ee\u9898","982ed1c9":"## Generally weights larger than 1 do correspond to leverage","08603187":"# splitbydate:\u8f93\u5165\u56db\u4e2adate\u8282\u70b9\u62c6\u5206\u539f\u59cb\u6570\u636e\u96c6\u4e3atrain and test"}}