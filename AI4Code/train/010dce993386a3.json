{"cell_type":{"f72e1ca1":"code","8dc46321":"code","1bfc5073":"code","b4091616":"code","8f2662b7":"code","76b75d8c":"code","842494b5":"code","8e5d4d5c":"code","e91360a5":"code","817da9ea":"code","62e4cea9":"code","3c6e569b":"code","8c8ff576":"code","96159b51":"code","5accb75e":"code","69546766":"code","d3a558e2":"code","a5afbd27":"code","98eb9a12":"code","13865d52":"code","e7724098":"code","3ccbd633":"code","33759055":"code","d0fc2d6c":"code","45f39b4c":"code","0c27bc85":"code","b1e147f1":"code","8765f4ec":"code","c6672823":"code","a55d90b6":"code","4cf53d54":"code","55d3a51f":"code","ab254db0":"code","a83d7e46":"code","7bfb4ff4":"code","47aabe0c":"code","b85f9882":"code","405febb8":"code","8a2e2ffe":"code","5b381caf":"code","047f65b9":"code","64bb078b":"code","64612a16":"code","b4fc7ce6":"code","dd174157":"code","00b4d222":"code","fa86462c":"code","8dd1d630":"code","766babee":"code","6e3014f2":"code","fb92bcb6":"code","f8c3d9e4":"code","621550fc":"code","e4ffc239":"code","1f7969ad":"code","373493ac":"code","e0cfa0cd":"code","e7d7bd18":"code","f67b8555":"code","eadea754":"code","034e98b4":"code","43af80ad":"code","ac109814":"code","b73fb488":"code","b7ec0ee0":"code","a128d82f":"code","7996cc74":"markdown","f967d85b":"markdown","142d3451":"markdown","cbfc944b":"markdown","7e3612ae":"markdown","ad59de80":"markdown","dc8ba6df":"markdown","21031e13":"markdown","fe7e7482":"markdown","cf06fb9a":"markdown","95562641":"markdown","95432b06":"markdown","a1e943d3":"markdown","7c0903e0":"markdown","596eefaf":"markdown","404a9762":"markdown","80d960b3":"markdown","64fd56d6":"markdown","196803bc":"markdown","aad8e208":"markdown","df645b04":"markdown","3148705d":"markdown","ba138e3f":"markdown","ffb80d13":"markdown","76b37b6e":"markdown","b5976e2c":"markdown","7c732414":"markdown","928ae0a0":"markdown","526abb39":"markdown","17cbbea6":"markdown","d02d7fdf":"markdown","4ea9a9d7":"markdown","7af5406f":"markdown","ec4221bc":"markdown","641749bb":"markdown","fe65caf2":"markdown","20bd6730":"markdown","2d9e687e":"markdown","44fe2e8b":"markdown","13ef218f":"markdown","63b412a3":"markdown","4a9d4628":"markdown","fc67bd29":"markdown","96bc8a21":"markdown","834771b9":"markdown","00a63568":"markdown","96fdd270":"markdown","1846a527":"markdown","99a0250b":"markdown","a4da37e9":"markdown","6682f124":"markdown","68ca0053":"markdown"},"source":{"f72e1ca1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plotting stuff\nimport plotly.express as px \nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 20})\n\n#Machine learning stuff\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8dc46321":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Greens):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","1bfc5073":"train_df=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_df.head()\n","b4091616":"px.scatter(train_df, x='PassengerId', y='Age', color='Survived')","8f2662b7":"train_df.iloc[630]","76b75d8c":"train_df.drop([630], inplace=True)","842494b5":"bins = np.linspace(-2, 3, 10)\ng = sns.FacetGrid(train_df, col=\"Sex\", hue=\"Survived\", palette=\"Set1\", height=5, aspect=1.5, col_wrap=2)\ng.map(plt.hist, 'Survived', bins=bins, ec='k')\n\ng.axes[-1].legend()\ng.axes[0].set_ylabel('Value Count')\n\ng.axes[0].set_xlabel('')\ng.axes[1].set_xlabel('')\n\ng.axes[0].set_xticks([0, 1])\ng.axes[0].set_xticklabels(['Dead ', 'Survived'])\n\nplt.show()","8e5d4d5c":"bins = np.linspace(-2, 2, 10)\ng = sns.FacetGrid(train_df, col=\"Pclass\", hue=\"Survived\", height=5, aspect=1.5, palette=\"Set1\", col_wrap=3)\ng.map(plt.hist, 'Survived', bins=bins, ec='k')\ng.axes[0].set_ylabel('Value Count')\n\nfor i in range(3):\n    g.axes[i].set_xlabel('')\n\ng.axes[0].set_xticks([0, 1])\ng.axes[0].set_xticklabels(['Dead ', 'Survived'])\ng.axes[-1].legend()\nplt.show()","e91360a5":"# Subtracting one to match passenger id with index\ntrain_df.drop([258], inplace=True)\ntrain_df.drop([679], inplace=True)\ntrain_df.drop([737], inplace=True)","817da9ea":"px.scatter_3d(train_df, x='PassengerId', y='Fare', color='Survived', z='Pclass')","62e4cea9":"class1_df=train_df.loc[train_df['Pclass'] == 1]\n# class1_df\npx.scatter(class1_df, x='Fare', y='PassengerId', color='Survived')","3c6e569b":"px.scatter(train_df, x='PassengerId', y='Ticket', color='Survived')","8c8ff576":"ticket_df=train_df.groupby('Ticket').sum()[['Survived']]","96159b51":"px.bar(ticket_df)","5accb75e":"bins = np.linspace(-2, 2, 10)\ng = sns.FacetGrid(train_df, col=\"Embarked\", hue=\"Survived\", height=4.0, aspect=1.3, palette=\"Set1\", col_wrap=3)\ng.map(plt.hist, 'Survived', bins=bins, ec='k')\nfor i in range(3):\n    g.axes[i].set_xlabel('')\n\ng.axes[0].set_xticks([0, 1])\ng.axes[0].set_xticklabels(['Dead ', 'Survived'])\ng.axes[-1].legend()\nplt.show()","69546766":"temp_df=train_df.groupby(['SibSp']).mean()\ntemp_df","d3a558e2":"fig=px.bar(temp_df, x=temp_df.index, y='Survived')\n\nfig.show()","a5afbd27":"temp2_df=train_df.groupby(['Parch']).mean()","98eb9a12":"px.bar(temp2_df, x=temp2_df.index, y='Survived')","13865d52":"train_df['FamSize']=train_df['SibSp']+train_df['Parch']\ntrain_df.head()","e7724098":"temp3_df=train_df.groupby(['FamSize']).mean()\npx.bar(temp3_df, x=temp3_df.index, y='Survived')","3ccbd633":"bins = np.linspace(-2, 2, 10)\ng = sns.FacetGrid(train_df, col='FamSize', hue=\"Survived\", palette=\"Set1\", col_wrap=3)\ng.map(plt.hist, 'Survived', bins=bins, ec='k')\nfor i in range(9):\n    g.axes[i].set_xlabel('')\n\ng.axes[0].set_xticks([])\n# g.axes[0].set_xticklabels(['Dead ', 'Survived'])\nplt.show()","33759055":"train_df['Cabin'].isna().sum()","d0fc2d6c":"train_df.set_index('PassengerId')\n# Dropping Cabin\ntrain_df=train_df[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'FamSize', 'Parch', 'SibSp', 'Ticket', 'Fare', 'Embarked']]\n","45f39b4c":"train_df.isna().sum()","0c27bc85":"fig, axes = plt.subplots(figsize=(20, 10))\ntrain_df.hist('Age', ax=axes)\n\nstd=train_df['Age'].std()\n\naxes.plot([train_df['Age'].mean(),train_df['Age'].mean()], [0, 400], label='Mean')\naxes.fill_betweenx([0, 400], [train_df['Age'].mean()-std,train_df['Age'].mean()-std], [train_df['Age'].mean()+std,train_df['Age'].mean()+std], color='g', alpha=0.2, label='SD')\n\naxes.plot([train_df['Age'].median(),train_df['Age'].median()], [0, 400], label='Median')\naxes.plot([train_df['Age'].mode(),train_df['Age'].mode()], [0, 400], label='Mode')\n# \naxes.set_xlabel('Age')\naxes.set_ylabel('Value Counts')\n\naxes.legend()","b1e147f1":"sns.boxplot(x='Age', data=train_df)","8765f4ec":"# Q1 = train_df['Age'].quantile(0.25)\n# Q3 = train_df['Age'].quantile(0.75)\n# #Interquartile range\n# IQR = Q3 - Q1\n# whisker_width = 1.5\n# #upper and lower whiskers\n# lower_whisker = Q1 -(whisker_width*IQR)\n# upper_whisker = Q3 +(whisker_width*IQR)\n\n# #Drop outliers\n# train_df.drop(train_df.loc[train_df['Age'] > upper_whisker].index, inplace=True)\n# # train_df.drop(train_df.loc[train_df['Age'] < lower_whisker].index, inplace=True)\n","c6672823":"train_df['Age'].fillna(float(train_df['Age'].median()), inplace=True)\n\ntrain_df['Embarked'].value_counts()\ntrain_df['Embarked'].fillna('S', inplace=True)\n\ntrain_df.isna().sum()","a55d90b6":"# Dropping a few columns as names and cabin\n# Feature=train_df[['Pclass', 'Sex', 'Age', 'FamSize', 'SibSp', 'Parch', 'Fare']]\nFeature=train_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n# Feature=train_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n\n#test: removing Fare and Age\n# Feature=train_df[['Pclass', 'Sex', 'SibSp', 'Parch']]\n\n# Replace male and female with 1 and 0, respectively\nFeature['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\n# One hot encoding for Embarked\nFeature = pd.concat([Feature, pd.get_dummies(train_df['Embarked'])], axis=1)\n\nFeature.head()","4cf53d54":"x=Feature\n# x[0:5]","55d3a51f":"y=train_df['Survived'].values\ny[0:5]\n\n","ab254db0":"x= preprocessing.StandardScaler().fit(x).transform(x)\nx[0:5]","a83d7e46":"#X and y have been defined in the data preparation step above\n#Splitting the set in train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","7bfb4ff4":"Kmax=40\n#accuracy indicators\nmean_acc = np.zeros((Kmax))\nstd_acc = np.zeros((Kmax))\n\nfor k in range(1,Kmax+1):\n    #Define the model and fit to train test\n    knn=KNeighborsClassifier(n_neighbors=k).fit(x_train, y_train)\n    #Predict on the test set\n    y_hat=knn.predict(x_test)\n    #Verify accuracy against test set labels\n    mean_acc[k-1] = metrics.accuracy_score(y_test, y_hat)\n    std_acc[k-1] = np.std(y_hat==y_test)\/np.sqrt(y_hat.shape[0])\n\nmean_acc","47aabe0c":"plt.figure(figsize=(20,10))\nplt.plot(mean_acc)\nplt.xlabel('K')\nplt.ylabel('Mean Accuracy')","b85f9882":"parameters = {'n_neighbors': range(1,15),\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'p': [1,2]}\n\nKNN = KNeighborsClassifier()","405febb8":"knn_cv=GridSearchCV(KNN, parameters, cv=10).fit(x_train, y_train)\n","8a2e2ffe":"print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\nprint(\"accuracy :\",knn_cv.best_score_)","5b381caf":"knnScore=knn_cv.score(x_test, y_test)\nknnScore","047f65b9":"parameters = {'criterion': ['gini', 'entropy'],\n     'splitter': ['best', 'random'],\n     'max_depth': [2*n for n in range(1,10)],\n     'max_features': ['auto', 'sqrt'],\n     'min_samples_leaf': [1, 2, 4],\n     'min_samples_split': [2, 5, 10]}\n\ntree = DecisionTreeClassifier()","64bb078b":"tree_cv=GridSearchCV(tree, parameters, cv=10).fit(x_train, y_train)\n","64612a16":"print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\nprint(\"accuracy :\",tree_cv.best_score_)","b4fc7ce6":"treeScore=tree_cv.score(x_test, y_test)\ntreeScore","dd174157":"parameters ={'C':[0.001, 0.01,0.1,1],\n             'penalty':['l2'],\n             'solver':['lbfgs']}","00b4d222":"lr=LogisticRegression()\nlogreg_cv=GridSearchCV(lr, parameters, cv=10).fit(x_train, y_train)\nlogreg_cv","fa86462c":"print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","8dd1d630":"logregScore=logreg_cv.score(x_test, y_test)\nlogregScore","766babee":"parameters = {'kernel':['rbf'],\n              'C': np.logspace(-3, 3, 5),\n              'gamma':np.logspace(-3, 3, 5)}\nsvm_new=svm.SVC()","6e3014f2":"svm_cv=GridSearchCV(svm_new,parameters, cv=10).fit(x_train, y_train)\n","fb92bcb6":"print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\nprint(\"accuracy :\",svm_cv.best_score_)","f8c3d9e4":"svmScore=svm_cv.score(x_test, y_test)\nsvmScore","621550fc":"scores=[knnScore, treeScore, logregScore, svmScore]\nxax=[0, 1, 2, 3]\n\nfig, axes=plt.subplots(figsize=(20,10))\n\naxes.bar(xax, scores, width=0.5, color=['r', 'g', 'b', 'k'], alpha=0.7)\n\naxes.set_xticks(xax)\naxes.set_xticklabels(['KNN', 'Decision Tree', 'LogRegression', 'SVM'])\naxes.set_ylabel('SCORE')","e4ffc239":"y_hat=svm_cv.predict(x_test)\ncnf_matrix = confusion_matrix(y_test, y_hat)\nnp.set_printoptions(precision=2)\n\n# print(classification_report(y_test,y_hat))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Dead','Survived'],normalize= False,  title='Confusion matrix')\n","1f7969ad":"svm_cv.fit(x,y)","373493ac":"test_df=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_df=test_df[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']]\n\ntest_df.head()","e0cfa0cd":"sns.boxplot(data=test_df[['Age','Fare']])","e7d7bd18":"Q1 = test_df['Fare'].quantile(0.25)\nQ3 = test_df['Fare'].quantile(0.75)\n#Interquartile range\nIQR = Q3 - Q1\nwhisker_width = 1.5\n#upper and lower whiskers\nlower_whisker = Q1 -(whisker_width*IQR)\nupper_whisker = Q3 +(whisker_width*IQR)\n\n#Drop outliers\ntest_df.loc[test_df['Fare'] > upper_whisker, 'Fare']=float(test_df['Fare'].mean())\n\n# train_df.drop(train_df.loc[train_df['Age'] < lower_whisker].index, inplace=True)\n","f67b8555":"sns.boxplot(data=test_df[['Age','Fare']])","eadea754":"#substitute age outliers\ntest_df['Age'].fillna(test_df['Age'].mean(), inplace=True)\n#substitute Fare and embarked outliers\ntest_df['Fare'].fillna(float(test_df['Fare'].mean()), inplace=True)\ntest_df['Embarked'].value_counts()\ntest_df['Embarked'].fillna('S', inplace=True)\n\ntest_df.isna().sum()","034e98b4":"test_df['FamSize']=test_df['SibSp']+test_df['Parch']\n","43af80ad":"test_df.set_index('PassengerId')\n\n# test_df.dropna(inplace=True)\n# Dropping a few columns as names and cabin\n# Feature=test_df[['Pclass', 'Sex', 'Age', 'FamSize', 'SibSp', 'Parch', 'Fare']]\nFeature=test_df[['Pclass', 'Sex', 'Age',  'SibSp', 'Parch', 'Fare']]\n# Feature=test_df[['Pclass', 'Sex', 'Age',  'SibSp', 'Fare']]\n\n# test, removing age and fare\n# Feature=test_df[['Pclass', 'Sex', 'SibSp', 'Parch']]\n\n# Replace male and female with 1 and 0, respectively\nFeature['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\n# One hot encoding for Embarked\nFeature = pd.concat([Feature, pd.get_dummies(test_df['Embarked'])], axis=1)\n\n# Feature.head()\n\nfinal_x=Feature\n# x[0:5]\n\nfinal_x= preprocessing.StandardScaler().fit(final_x).transform(final_x)\n# x[0:5]","ac109814":"pred_from_svm_cv=svm_cv.predict(final_x)","b73fb488":"#KNN\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': pred_from_knn_cv})\n#Decision Tree\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': pred_from_tree_cv})\n#Logistic Regression\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': pred_from_lr_cv})\n#SVM\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': pred_from_svm_cv})\n","b7ec0ee0":"output.head()","a128d82f":"output.to_csv('submission.csv', index=False)","7996cc74":"It looks like SVM is the best performing model","f967d85b":"## Family Size","142d3451":"## Support Vector Machine","cbfc944b":"Let's standardize the data","7e3612ae":"As the boxplot shows, the distribution is right-skewed, with a few outliers that may affect the mean. Let's try and use the Median instead, to replace NaN values (see below).","ad59de80":"## Logistic Regression","dc8ba6df":"A quick look at the age and fare distributions. 'Fare' has quite a few outliers, here we can't just get rid of them, as we will need to submit a prediction for each entry. But since the model is trained on a less skewed distribution, if we use them as they are this will result in a low out of sample accuracy. I'll try to substitute (some of) them with the mean.","21031e13":"> 'Pclass', 'Sex', 'Age',  'SibSp', 'Parch', 'Fare' \n\nIs the best subset I found to predict the survival through a Support Vector Machine model. The correlation between data and survival is quite simple and explains the ratio they used to choose who to save. \"Women and childern first!\" but also \"Rich and wealthy first!\"\n\n- The higher Fare and Pclass, the higher the chance of survival\n\n- Women had a higher survival rate than men\n\n- The lower the age, the higher the chance of survival\n\n- The family size has an optimum value around 3\n\nThe analysis presented above scores 0.78708. I am sure that there is much to be done to improve it and I am very much open to any comments or suggestions. This one was just my first attempt to apply my moderate knowledge of data science to make predictions. \n\nCheers!\n\n","fe7e7482":"Unsurprisingly, going down from upper to lower class the ratio dead\/survived increases. The wealthy first  passengers were saved, while the peasants were left to drown... way to go, Titanic crew!","cf06fb9a":"Against my expectations, it looks like the port where the passengers were embarked is correlated with their survival.","95562641":"# Predictions on the Test Set\n\nTime to compute the predictions on the actual test set, to prepare a submission file. \nFirst, the test set needs to be imported and undergo the same data preparations steps carried out for the Train test","95432b06":"Here's how the distributions look like after treating the outliers","a1e943d3":"It looks like the prediction accuracies don't change with or without those data points. (comment or delete the previous cell to see those points in the graph below)","7c0903e0":"It looks like Parch and SibSp have a similar trend, so let's add them up in a new column, that I will call FamSize, as it represents the family size. Again, it looks like there is an optimum number, then the chance of survival decreases again. However, for some reason adding this data or substituting it to Parch and SibSp does not seem to improve the predictions. ","596eefaf":"## Cabin","404a9762":"Here we need to select the features we will use to train the ML algorithms. I did a bit of testing with different combinations and subsets of columns. So far, I get the best results using the following:\n\n- Pclass\n- Sex\n- Age\n- SibSp\n- Parch\n- Fare\n- Embarked\n\nMale and Female are substituted with 0 and 1. Similarly, One Hot Encoding is used for 'Embarked'","80d960b3":"## Decision Tree","64fd56d6":"Let's try to see how the size of the groups (families) affected the survival of the passengers.","196803bc":"## Survival by Passenger Class","aad8e208":"# ML model testing\n\nIn the following, four different ML models are trained and tested. First, the optimum paramenters are searched through a grid search on a parameter grid, then the models are fitted and tested on the test set. ","df645b04":"We'll refit svn_cv to the whole train test","3148705d":"Let's compute predictions using the best ML method: SVM. The prediction is than converted in a dataframe and written to a 'submission.csv' for submission.","ba138e3f":"# A Beginner's Attempt to the Titanic Challenge","ffb80d13":"The outliers may be trimmed out (see commented code below) but I have tried that and the accuracy of the predictions gets worse. I wish I knew why... \n\nAnyway, since our goal is a more accurate predicion, I shall keep them. ","76b37b6e":"Check for NaN","b5976e2c":"First, let's calculate the optimum K value","7c732414":"## K Nearest Neighbours","928ae0a0":"# Ticket\n\nHonestly, I am not sure how to treat this data. It looks like it does have some correlation with survival. It is definitely connected with the size of the groups\/families that embarked together. It may be used to recover the information about people embarked together that do not count as family members according to Parch and SibSp. But I wouldn't know how to do this. So I'll leave it out for now...","526abb39":"## Port","17cbbea6":"## Survival by Sex","d02d7fdf":"And split the data set in a train and test set. 80% - 20% seems like a reasonable choice","4ea9a9d7":"However, for Emabarked we can't compute the mode of a categorical value. We compute the frequency of each values and substitute the most frequent one, as it's more likely that those passengers embarked from that port. Conceptually is the same thing as using the mode.","7af5406f":"This is my first attempt at a Kaggle competion. Also, this is the first time I have a go at trying to make prediction on a dataset without any predefined instructions. I only have a couple of online courses in data science and a few years of python programming as a background. Let's see what comes out. \n\nHere's what this notebook is **NOT**:\n- A top scoring one. My best score is somewhere around 80% (a little below that to be totally honest)\n\n- A tutorial for beginners. This is just how I approach the problem, it's not meant as a guide. Any external feedback on how to improve is much appreaciated\n\n- A scam. I've seen many notebooks scoring one, just feeding the example results back as a submission. You won't see this, or any other kind of trickery in this notebook. ","ec4221bc":"# **Data Preparation**\n\nRedefining index and preparing data for machine learning algorithm. \n","641749bb":"Recreate the Family Size columns, just to be consistent. We're not actually using it in the final version","fe65caf2":"Create the feature, as done for the training set.","20bd6730":"Let's now optimize the parameters for a decision Tree","2d9e687e":"Let's substitute the NaN occurences. The only columns containing NaN are 'Age' and 'Embarked'.\n\nFor \"Age\" we can just compute the mean value and use it in place of NaN. Let's have a quick look at the distribution, just to be sure. It looks a bit skewed, it's not the nice Gaussian distribution I would have expected","44fe2e8b":"# Conclusions\n\n","13ef218f":"If we have a look at the confusion matrix, we notice that the main problem are the false negatives. ","63b412a3":"# Exploratory Data Analysis\n\nReading the training dataset. On this, we're gonna do a little EDA to try and see which entries have a predictive value and which ones are useless","4a9d4628":"Here's just a function to plot confusion matrices that I had in some old notebook from a course. I'll just keep this here, in case we need it later on. ","fc67bd29":"Let's define X and Y","96bc8a21":"# Compare the scores of the different optimized models","834771b9":"## Survival By Age\n\nIt looks like the Age of the passengers is not strongly correlated with their survival to the disaster. Except for the infants (age 0-1) which were almost all saved and those older than 65 that were all left to drown. Zoom in on the areas to see. ","00a63568":"Of course our labels will be the Survived column","96fdd270":"\"Women and children first!\" \n\n1921 was probably not a great time to be a woman. On the bright side, you would have had a bigger chance of surviving the Titanic disaster!","1846a527":"The Cabin may or may not play a role, but more than half of the values are NaN. I will exclude this column from the data ","99a0250b":"Let's try optimize the other parameters, scanning a smaller k range","a4da37e9":"It also looks like there's a correlation between Fare and Survival rate among upper class passengers. There are only three passengers that paid a Fare around 500. I am not sure whether these should be treated as outliers. It does make sense that those who were the whealthiest among the whealthy were saved. It's appalling, but it does make sense. Let's try with and without them.","6682f124":"Well, then let's start, shall we? \n\nFirst things first, let's get the imports out of the way","68ca0053":"Mr. Algernon Henry Wilson looks like an outlier to me. He's the only person his Age that survived. I'll remove it from the test set. "}}