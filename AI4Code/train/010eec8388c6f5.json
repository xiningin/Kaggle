{"cell_type":{"f729b1fc":"code","a42955c8":"code","5487cc18":"code","3d2b1c1b":"code","6d9e1c30":"code","33fb4ecc":"code","ff1d5073":"code","cc066988":"code","7c91eb76":"code","a5579792":"code","b4206e54":"code","cacc1bcf":"code","53f3fc98":"code","18b85627":"code","80b3deb6":"code","07639d83":"code","ee6f6efd":"code","96ff67b7":"code","4f56216f":"code","77d98781":"code","2498e2e2":"code","774f50cd":"code","30707cf0":"code","cd2a4ca0":"code","ad844790":"code","3e3dc7a7":"code","f7bd8fc4":"code","78af2ab5":"code","c6e879c8":"code","6e5ba835":"code","c24634ab":"code","423a491a":"code","3787ff37":"code","60b2a12b":"code","9c46de2a":"code","69f14c28":"code","9a7fdc2d":"code","cc5f2e06":"code","9ab838c7":"code","e99e9dd3":"code","284cbeb6":"code","692e58a4":"code","60714ff2":"code","210555bc":"code","170a1587":"code","1ec7d9cd":"code","f12d0c81":"code","3de56be7":"code","83df2a9d":"code","9bc9f3cc":"code","2ef38244":"code","101bce43":"code","b6eac4e4":"code","be1e53d3":"code","e1cd8cf9":"code","f5be804b":"code","a56377d9":"code","fa081c62":"code","dcc1a28e":"code","807b0fd6":"code","48cd8c3a":"code","fad77d6a":"code","dbee15dd":"code","8144ccd9":"code","1779426b":"code","ee194801":"code","126a5e3f":"code","4fec681b":"code","067ae422":"code","5ab03d25":"code","916b5c68":"code","4813909f":"code","6ff24a74":"code","b9197634":"code","af956add":"code","428baa82":"code","6a2a1d6a":"code","a42e8ad7":"code","2aa8387a":"code","ac14bf98":"code","a8904e82":"code","3bf0fa82":"code","7c3a636a":"code","44d74ced":"code","05af05f9":"code","17a9be8c":"markdown","6688ddd0":"markdown","7e49b9cb":"markdown","b41cff41":"markdown","d87790c4":"markdown","d37a4481":"markdown","416dc155":"markdown","c5b03e8c":"markdown","025d4c4e":"markdown","dbba9261":"markdown","0be5a7bc":"markdown","b5b31be8":"markdown","1ae306a3":"markdown","386d758a":"markdown","661ed258":"markdown","26907a9d":"markdown","e9cfa33b":"markdown","2fe7c78a":"markdown","bb0ada73":"markdown","5b8d496b":"markdown","2d092783":"markdown","52a5551e":"markdown","1b622e9f":"markdown","27622911":"markdown","52c30a5c":"markdown","c9686f6d":"markdown","e910d94e":"markdown","88848639":"markdown","18beb9d8":"markdown","68585bc2":"markdown","c2ebd9c8":"markdown","7a10f729":"markdown","4e5b39b8":"markdown","ec3e16ca":"markdown","42263dfd":"markdown","e8932599":"markdown"},"source":{"f729b1fc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a42955c8":"cars = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')","5487cc18":"cars.head()","3d2b1c1b":"cars.shape","6d9e1c30":"cars.describe()","33fb4ecc":"cars.info()","ff1d5073":"#Car_Name\n\ncars.Car_Name.value_counts()","cc066988":"cars.drop(['Car_Name'],axis=1,inplace = True)","7c91eb76":"cars.head()","a5579792":"# Year\n\nplt.figure(figsize = (15,5))\nsns.boxplot(data=cars)\nplt.show()","b4206e54":"# from the boxplot we can see that kms_Driven has outliers","cacc1bcf":"q1 = cars['Kms_Driven'].quantile(0.25)\nq3 = cars['Kms_Driven'].quantile(0.75)\niqr = q3-q1\n\nUL = q3 + (1.5 * iqr)\nLL = q1 - (1.5 * iqr)\nprint(iqr,UL,LL)","53f3fc98":"cars[cars['Kms_Driven']>UL]","18b85627":"cars[cars['Kms_Driven']>UL].count()['Kms_Driven']","80b3deb6":"#outlier removal from Kms_Driven\n\ndf = cars[cars['Kms_Driven']<UL]\ncars=df\ncars","07639d83":"sns.distplot(df['Year'])","ee6f6efd":"# The Years variable is left skewed","96ff67b7":"sns.distplot(df['Selling_Price'])\nplt.show()","4f56216f":"# the selling price is right skewed","77d98781":"sns.distplot(df['Present_Price'])\nplt.show()","2498e2e2":"#the present_price is right skewed","774f50cd":"sns.distplot(df['Kms_Driven'])\nplt.show()","30707cf0":"#The Kms_Driven are almost normally distributed after removing the outliers, the max values lie between 20000 to 50000\n#kms","cd2a4ca0":"sns.countplot(cars['Fuel_Type'])\nplt.show()","ad844790":"# From this bar plot we can see that there are three categories of Fuel_Type\n#Petrol Fuel_type is the maximum in number and CNG cars are the least","3e3dc7a7":"sns.countplot(cars['Seller_Type'])\nplt.show()","f7bd8fc4":"# There are two types of sellers : Individual and Dealer\n# The seller_type dealer is greater than the individual seller_type","78af2ab5":"sns.countplot(cars['Transmission'])\nplt.show()","c6e879c8":"# The Transmission feature has 2 categories\n#Manual and Automatic","6e5ba835":"sns.countplot(cars['Owner'])\nplt.show()","c24634ab":"# The cars having 0 previous owners is more than the cars having one previous owner.","423a491a":"fig, (ax1, ax2,ax3) = plt.subplots(1,3,figsize = (15,5))\n\n#scatter plot 1\nax1.scatter(x=cars['Year'],y= cars['Selling_Price'])\nax1.set_title('Years v\/s Selling_Price')\n\n#scatter plot 2\nax2.scatter(x=cars['Present_Price'], y=cars['Selling_Price']) \nax2.set_title('Present_Price v\/s Selling_Price')\n\n#scatter plot 3\nax3.scatter(x=cars['Kms_Driven'],y=cars['Selling_Price'])\nax3.set_title('Kms_Driven v\/s Selling_Price')\n\nplt.draw()  ","3787ff37":"fig,axes = plt.subplots(2,2,figsize=(20,12))\n\nsns.boxplot(x=cars.Fuel_Type,y=cars.Selling_Price,ax=axes[0][0])\naxes[0][0].set_title('Fuel_Type v\/s Selling_Price')\n\nsns.boxplot(x=cars.Transmission,y=cars.Selling_Price,ax=axes[0][1])\naxes[0][1].set_title('Transmission v\/s Selling_Price')\n\nsns.boxplot(x=cars.Owner,y=cars.Selling_Price,ax=axes[1][0])\naxes[1][0].set_title('Owner v\/s Selling_Price')\n\nsns.boxplot(x=cars.Seller_Type,y=cars.Selling_Price,ax=axes[1][1])\naxes[1][1].set_title('Seller_Type v\/s Selling_Price')","60b2a12b":"sns.lmplot(x='Kms_Driven',y='Selling_Price',data=cars,fit_reg=False,col='Transmission',row='Seller_Type')   \nplt.show()","9c46de2a":"sns.lmplot(x='Present_Price',y='Selling_Price',data=cars,fit_reg=False,col='Transmission',row='Seller_Type',hue='Fuel_Type')   \nplt.show()","69f14c28":"#Fuel_Type\n\ncars.Fuel_Type.value_counts()","9a7fdc2d":"cars.Seller_Type.value_counts()","cc5f2e06":"cars.Transmission.value_counts()","9ab838c7":"cars = pd.get_dummies(cars,columns=['Fuel_Type','Seller_Type','Transmission'],drop_first=True)","e99e9dd3":"cars.info()","284cbeb6":"cars.shape","692e58a4":"cars.head()","60714ff2":"#Heatmap to show the correlation between various variables of the dataset\n\nplt.figure(figsize=(10, 8))\ncor = cars.corr()\nax = sns.heatmap(cor,annot=True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","210555bc":"y = cars['Selling_Price']\nX = cars.drop(['Selling_Price'],axis=1)","170a1587":"#Splitting the data into train and test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.30 , random_state = 1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","1ec7d9cd":"#standardization of the data\nfrom sklearn.preprocessing import StandardScaler\n\nsc=StandardScaler() \nX_train=sc.fit_transform(X_train)\nX_train=pd.DataFrame(X_train,columns=X.columns)\n\nX_test=sc.fit_transform(X_test)\nX_test=pd.DataFrame(X_test,columns=X.columns)","f12d0c81":"#Building model using sklearn(Gradient Descent)\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # training the algorithm\n\n# Getting the coefficients and intercept\n\nprint('coefficients:\\n', lin_reg.coef_)\nprint('\\n intercept:', lin_reg.intercept_)\n#coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])  \n#print(coeff_df)\n\n#Now predicting on the test data\n\ny_pred = lin_reg.predict(X_test)","3de56be7":"# compare the actual output values for X_test with the predicted values\n\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.reset_index(inplace=True,drop=True)\ndf","83df2a9d":"#Showing the difference between the actual and predicted value\n\ndf1 = df.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","9bc9f3cc":"#Calculating the accuracy \n\nfrom sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred))\n\n#or\n#print('rsquare_Train', lin_reg.score(X_train, y_train))\n#print('rsquare_Test', lin_reg.score(X_test, y_test)) ","2ef38244":"# Building a linear Regression model using statsmodels (OLS)","101bce43":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\ny = cars['Selling_Price']\nX = cars.drop(['Selling_Price'],axis=1)\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nprint(model.summary())","b6eac4e4":"# 1. Durbin Watson Test\n\n#Ho: Linear Regression Residuals are not correlated\n#H1: Errors are correlated.\n\nfrom statsmodels.stats.api import durbin_watson\ndurbin_watson(model.resid)","be1e53d3":"#2. time series analysis graph \n\nimport statsmodels.tsa.api as smt #tsa time series anlaysis\n\nacf = smt.graphics.plot_acf(model.resid, lags=40 , alpha=0.05) #model.resid comes from statsmodel \nacf.show()\n\n# from this graph we dont see any pattern in the residuals so this shows no autocorrelation","e1cd8cf9":"#1. Jarque berua test\n\nfrom scipy import stats\nprint(stats.jarque_bera(model.resid))\n\n#ho : the data is normally distributed\n#h1: the errors are not normally distributed","f5be804b":"#2. Histogram\n\nimport seaborn as sns\n\nsns.distplot(model.resid)","a56377d9":"#3. QQ plot\n\nimport pylab\n\nstats.probplot(model.resid, dist = 'norm', plot = pylab)\nplt.show()","fa081c62":"#4. shapiro wilk test\n\n# Ho: The Data \/ Errors are Normal in Nature\n# H1: The Data is not Normal\n\nfrom scipy.stats import shapiro\n\nteststats, pvalue = shapiro(model.resid)\nprint(pvalue)\nprint(\"reject the null ho\")","dcc1a28e":"#1. Visual representation\n\n%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport statsmodels.stats.api as sms\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\ndef linearity_test(model, y):\n    '''\n    Function for visually inspecting the assumption of linearity in a linear regression model.\n    It plots observed vs. predicted values and residuals vs. predicted values.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    * y - observed values\n    '''\n    fitted_vals = model.predict()\n    resids = model.resid\n\n    fig, ax = plt.subplots(1,2)\n    \n    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n    ax[0].set(xlabel='Predicted', ylabel='Observed')\n    \n    #LOWESS (Locally Weighted Scatterplot Smoothing) is a popular tool used in regression analysis that creates a smooth line \n    #through a timeplot or scatter plot to help you to see relationship between variables and foresee trends.\n\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n    \nlinearity_test(model, y)","807b0fd6":"#2. Rainbow test\n\nimport statsmodels.api as sm\nsm.stats.linear_rainbow(res=model, frac=0.5)\n# frac : we are not checking the whole data we are just checking the fraction of it","48cd8c3a":"from statsmodels.stats.api import het_goldfeldquandt\nfrom statsmodels.compat import lzip\n","fad77d6a":"#1. Goldfeld Quandt Test:\n\n# Ho: The residuals are not heteroscedastic \/ same variance \/ homoscedastic\n# H1: The residuals are Heteroscedastic \/ unequal variance\n\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlzip(name, test)\n\n#exog - x varibles and endog - y variables","dbee15dd":"#2. Visual representation\n\nfitted_vals = model.predict()\nresids = model.resid\nresids_standardized = model.get_influence().resid_studentized_internal\nfig, ax = plt.subplots(1,2,figsize=(20,12))\n\nsns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Residuals vs Fitted', fontsize=16)\nax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n\nsns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Scale-Location', fontsize=16)\nax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n\nplt.show()","8144ccd9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\ndf = pd.DataFrame({'vif': vif[1:]}, index=X.columns)\ndf","1779426b":"df[df.vif > 5].index","ee194801":"## After removing multicollinear feature 'Fuel_Type_Diesel'....cars1\ncars1 = cars\ncars1.drop(['Fuel_Type_Diesel'],axis=1,inplace=True)\n\nX_vif = cars1.drop(['Selling_Price'],axis=1)\ny_vif = cars1['Selling_Price']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg_vif = LinearRegression()\nlin_reg_vif.fit(X, y)\n\nprint(f'Coefficients: {lin_reg_vif.coef_}')\nprint(f'Intercept: {lin_reg_vif.intercept_}')\nprint(f'R^2 score: {lin_reg_vif.score(X, y)}')","126a5e3f":"## After removing multicollinear feature 'Fuel_Type_Diesel'\n\nimport warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nX = cars1.drop(['Selling_Price'],axis=1)\ny = cars1['Selling_Price']\n\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y,X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","4fec681b":"vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","067ae422":"#After checking the assumptions found that Normality criteria not met\n\n# we will apply transformation on the data to make the data meet the assumption","5ab03d25":"# Residual plot\n\nsns.set(style = 'whitegrid')\n\ncars1['predictions'] = model.predict(X_constant)\nresiduals = model.resid\n\nax = sns.residplot(cars1.predictions, residuals, lowess = True, color = 'g')\nax.set(xlabel = 'Fitted value', ylabel = 'Residuals', title = 'Residual vs Fitted Plot \\n')\nplt.show()","916b5c68":"## for sqrt(X)\n\nfinal_df = cars1.transform(lambda x: x**0.5)\nfinal_df.head()","4813909f":"X_final = final_df.drop(['Selling_Price','predictions'],axis=1)\ny_final = final_df.Selling_Price\nX_constant_final = sm.add_constant(X_final)\nmodel_final = sm.OLS(y_final, X_constant_final).fit()\npredictions_final = model_final.predict(X_constant_final)\nmodel_final.summary()","6ff24a74":"#After transformating the data the accuracy\/R2 score for the model improved.\n\n#We can look further into the different regularization techniques with different values of alpha and build models\n\n#The best R2 score that this model is giving is using these parameters","b9197634":"from sklearn.linear_model import RidgeCV,Ridge\n\nalphas = 10**np.linspace(10,-2,100)*0.5\n\nridgecv = RidgeCV(alphas = alphas,normalize = True)\nridgecv.fit(X_train, y_train)\nridgecv.alpha_","af956add":"rr = Ridge(alpha = ridgecv.alpha_, normalize = True)\nrr.fit(X_train, y_train)","428baa82":"print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rr.predict(X_test))))\n\nprint('r2_score:', metrics.r2_score(y_test, rr.predict(X_test)))","6a2a1d6a":"from sklearn.linear_model import LassoCV,Lasso\n\nlasso = Lasso(max_iter = 10000, normalize = True)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train, y_train)\n    coefs.append(lasso.coef_)","a42e8ad7":"lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\nlassocv.fit(X_train, y_train)\n\nlasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(X_train, y_train)","2aa8387a":"print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lasso.predict(X_test))))\n\nprint('r2_score:', metrics.r2_score(y_test, lasso.predict(X_test)))","ac14bf98":"# Plot the coefficients\nplt.figure(figsize=(8, 5))\n\ncolnames = X_train.columns\n\nplt.plot(range(len(colnames)), lasso.coef_, linestyle='none',marker='*',markersize=5,color='red')\nplt.xticks(range(len(colnames)), colnames.values, rotation=60) \nplt.margins(0.02)\nplt.show()","a8904e82":"# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\n\n# how much importance should be given to l1 reguralization\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')","3bf0fa82":"cv_model.fit(X_train, y_train)","7c3a636a":"print('Optimal alpha: %.8f'%cv_model.alpha_)\n#The amount of penalization chosen by cross validation\n\nprint('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\n#The compromise between l1 and l2 penalization chosen by cross validation\n\nprint('Number of iterations %d'%cv_model.n_iter_)\n#number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.","44d74ced":"# train model with best parameters from CV\nelastic = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nelastic.fit(X_train, y_train)","05af05f9":"print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, elastic.predict(X_test))))\n\nprint('r2_score:', metrics.r2_score(y_test, elastic.predict(X_test)))","17a9be8c":"#### UNIVARIATE ANALYSIS","6688ddd0":"#### Asssumption 3 - Linearity of residuals","7e49b9cb":"We can see that the Ridge model is performing better than the Lasso model.\n\n#### 3. ElasticNet Regression\n\nElasticNet combines the properties of both Ridge and Lasso regression. It works by penalizing the model using both the l2-norm and the l1-norm.","b41cff41":"# LINEAR REGRESSION MODELS","d87790c4":"These 8 values are greater than the upper limit value 99417.5\n\nWe would remove these values","d37a4481":"### Assumptions","416dc155":"1. The older the car the lesser the selling price\n2. The selling price of those cars is greater whose current ex-showroom price(present_price) is greater i.e the present_price and sellin_price is directly proportional.\n3. As the Km_Driven increases the Selling_price of the car decreases ","c5b03e8c":"### Importing Libraries","025d4c4e":"Out of the 3 regularization models the Elastic Net Model is performing the best on this dataset.","dbba9261":"### EDA","0be5a7bc":"Car_Name contains 98 different values so it is better to drop this column","b5b31be8":"### Importing the dataset","1ae306a3":"#### Assumption 5- NO MULTI COLLINEARITY","386d758a":"\n#### Assumption 2- Normality of Residuals","661ed258":"The value of alpha that results in the smallest cross-validation error is 0.000332.","26907a9d":"For Linear Regression, we need to check if the 5 major assumptions hold.\n\n1. No Auto correlation\n2. Linearity of variables\n3. Normality of error terms\n4. No Heteroscedacity\n5. No strong MultiCollinearity","e9cfa33b":"We now perform 10-fold cross-validation to choose the best alpha, refit the model, and compute the associated score:","2fe7c78a":"#### Assumption 4 - Homoscedasticity_test","bb0ada73":"#### 2. Lasso Regression\n\nLasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n\nThe loss function for Lasso Regression can be expressed as below:\n\nLoss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)\n\nWe now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we'll use the Lasso() function; however, this time we'll need to include the argument max_iter = 10000. Other than that change, we proceed just as we did in fitting a ridge model:","5b8d496b":"From summary also we can see the durbin watson value ,this is v close to 2 which indicates no autocorrelation","2d092783":"OR","52a5551e":"The target variable Selling Price is highly correlated with:\n1. Present Price\n2. Fuel Type\n3. Seller Type","1b622e9f":"1. All the individual seller_type are having only petrol cars.\n2. Dealers selling manual transmission cars are selling all the 3 types of fuel cars, most expensive being the diesel cars","27622911":"This is Vehicle dataset from cardekho Dataset . This dataset contains information about used cars listed on website cardekho.com. We are going to use for finding predictions of price with the use of regression models.\n\nThe datasets consist of several independent variables include:\n\nCar_Name : This column should be filled with the name of the car.\n\nYear : This column should be filled with the year in which the car was bought.\n\nSelling_Price : This column should be filled with the price the owner wants to sell the car at.\n\nPresent_Price : This is the current ex-showroom price of the car.\n\nKms_Driven : This is the distance completed by the car in km.\n \nFuel_Type : Fuel type of the car i.e Diesel,Petrol,CNG\n\nSeller_Type : Defines whether the seller is a dealer or an individual.\n\nTransmission : Defines whether the car is manual or automatic.\n\nOwner : Defines the number of owners the car has previously had.","52c30a5c":"#### Bivarate  analysis","c9686f6d":"#### Multivariate Analysis","e910d94e":"fig,axes = plt.subplots(1,3,figsize=(15,5))\n\ncars.plot(kind='scatter',x='Year',y=\"Selling_Price\",ax=axes[0])\naxes[0].set_title('Years v\/s Selling_Price')\n\ncars.plot(kind='scatter',x='Present_Price',y=\"Selling_Price\",ax=axes[1])\naxes[1].set_title('Present_Price v\/s Selling_Price')\n\ncars.plot(kind='scatter',x='Kms_Driven',y=\"Selling_Price\",ax=axes[2])\naxes[2].set_title('Kms_Driven v\/s Selling_Price')\n\nplt.show()","88848639":"#### Converting categorical variables to dummy variables","18beb9d8":"#### 1. Ridge Regression\n\nRidge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n\nLoss function = OLS + alpha * summation (squared coefficient values)\n\nIn the above loss function, alpha is the parameter we need to select. A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.\n\nInstead of arbitrarily choosing alpha value ,it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated ridge regression function, RidgeCV()","68585bc2":"The value of alpha that results in the smallest cross-validation error is 0.0814.","c2ebd9c8":"1. The Diesel cars are having the highest selling_price with most number of outliers being present.\n   Diesel > CNG > Petrol in terms of seeling price\n2. Automatic cars are expesnive than manual cars\n3. The cars with no previous owner are expensive than with a previous owner.\n4. Individuals are selling there cars at lesser price than the cars being sold by the dealers","7a10f729":"#### Assumption 1- No autocorrelation\n","4e5b39b8":"There are all non null values present in the columns\n\nIn this regression model the dependent variable will be 'Selling_price' rest all the variables will be considered as independent variables\n\n\nFor buliding linear regression model we need all numerical variables,so the features containing object datatype are either converted or dropped","ec3e16ca":"pvalue (0) < alpha (0.05)\nso we reject the null hypothesis \nthe errors are not normally distributed","42263dfd":"### Linear Regression Model\n\nThe simplest form of regression is the linear regression, which assumes that the predictors have a linear relationship with the target variable.\n\nThe linear regression equation can be expressed in the following form:\n\ny = a1x1 + a2x2 + a3x3 + ..... + anxn + b\n\n* y is the target variable.\n* x1, x2, x3,...xn are the features.\n* a1, a2, a3,..., an are the coefficients.\n* b is the parameter of the model.","e8932599":"### Regularized Regression "}}