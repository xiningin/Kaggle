{"cell_type":{"e1ea2e0f":"code","0e96d26d":"code","887d4334":"code","93660812":"code","7709527b":"code","0966d3a8":"code","301002f0":"code","ce706dbc":"code","d614b883":"code","723fbdf4":"code","2653ebbb":"code","23ed041b":"code","ec7ac711":"code","bb64200d":"code","87a2d0dd":"code","efb5d288":"code","80f76202":"code","7d2535e8":"code","39c29727":"code","f3f68756":"code","7530328c":"code","9af5ea4a":"code","e356b393":"code","ba82b86c":"code","33057119":"code","595b87cd":"code","853d096d":"code","6b715673":"code","713e3c15":"code","b9f2d467":"code","9c0667d4":"code","531226a5":"code","3fcd804a":"code","d3ddb0af":"code","2d16cc9c":"code","8ddd56d0":"code","07ae6635":"code","c4c465ca":"code","64f603e2":"code","690c75db":"code","98e9e3b5":"code","9947b8e4":"code","d45116e2":"code","1790c84e":"code","5148473e":"code","903ee4f6":"code","eaa19527":"code","5f88493b":"code","03892d23":"code","af9f56bc":"code","9b01a0bf":"code","66813f63":"code","d7bad7ea":"code","c5ae15a0":"code","b6414713":"code","da87f6f7":"code","41a5c7f1":"code","1878cbd4":"code","b1a843ae":"code","79b615c4":"code","f100bd77":"code","84803edf":"code","b0cce100":"markdown","4585e91b":"markdown","4624a4c8":"markdown","0628c9fd":"markdown","740b8897":"markdown","2dcffaf7":"markdown","a9e2e518":"markdown","b990c77a":"markdown"},"source":{"e1ea2e0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0e96d26d":"import matplotlib.pyplot as plt","887d4334":"dipole_moments_csv=pd.read_csv(\"..\/input\/dipole_moments.csv\")\nmagnetic_shielding_tensors_csv=pd.read_csv(\"..\/input\/magnetic_shielding_tensors.csv\")\nmulliken_charges_csv=pd.read_csv(\"..\/input\/mulliken_charges.csv\")\npotential_energy_csv=pd.read_csv(\"..\/input\/potential_energy.csv\")\nsample_submission_csv=pd.read_csv(\"..\/input\/sample_submission.csv\")\nscalar_coupling_contributions_csv=pd.read_csv(\"..\/input\/scalar_coupling_contributions.csv\")\nstructures_csv=pd.read_csv(\"..\/input\/structures.csv\")\ntest_csv=pd.read_csv(\"..\/input\/test.csv\")\ntrain_csv=pd.read_csv(\"..\/input\/train.csv\")\n\nprint(\"dipole_moments.shape:\",dipole_moments_csv.shape)\nprint(\"magnetic_shielding_tensors.shape:\",magnetic_shielding_tensors_csv.shape)\nprint(\"mulliken_charges.shape:\",mulliken_charges_csv.shape)\nprint(\"potential_energy:\",potential_energy_csv.shape)\nprint(\"sample_submission:\",sample_submission_csv.shape)\nprint(\"scalar_coupling_contributions.shape:\",scalar_coupling_contributions_csv.shape)\nprint(\"structures.shape:\",structures_csv.shape)\nprint(\"test_csv.shape:\",test_csv.shape)\nprint(\"train_csv.shape:\",train_csv.shape)","93660812":"################## Auxiliar functions\n\n# Memory reduct\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n# Merge train_csv with Mulliken charges\n\ndef map_mulliken_charge(database,atom_idx) :\n    database = pd.merge(database,mulliken_charges_csv,how = 'left',\n                 left_on = ['molecule_name',f'atom_index_{atom_idx}'],\n                 right_on = ['molecule_name','atom_index']\n                 )\n    database = database.rename(columns={'mulliken_charge': f'mulliken_charge_{atom_idx}'}\n                  )\n    database = database.drop('atom_index',axis = 1)\n    database = reduce_mem_usage(database)\n    return database\n\n# Calculate number each type of atoms in a molecule\ndef number_each_atoms_molecule(df,df1,index):\n    df['Num_each_atom_in_mol'] = df.groupby(['molecule_name','atom'])['molecule_name'].transform('count')\n    df1 = pd.merge(df1, df, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{index}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    df1= df1.rename(columns={'x':f'x{index}','y':f'y{index}','z':f'z{index}','atom':f'atom{index}'})\n    df1 = df1.drop('atom_index',axis=1)\n    df1 = reduce_mem_usage(df1)\n    \n    return df1\n\n    # GENERATION OF TYPES OF COUPLINGS\ndef database_type(db_type):\n    #db_type = db[db['type'] == index]\n    \n    coupling= db_type[['scalar_coupling_constant']].copy()\n    dipole = db_type[['Dipole']].copy()\n    potential = db_type[['potential_energy']].copy()\n    fermi = db_type[['fc']].copy()\n    spin_dipolar = db_type[['sd']].copy()\n    paramagnetic_spin = db_type[['pso']].copy()\n    diamagnetic_spin = db_type[['dso']].copy()\n    mulliken_0 = db_type[['mulliken_charge_0']].copy()\n    mulliken_1 = db_type[['mulliken_charge_1']].copy()\n    \n    db_type = db_type.drop(['type', 'scalar_coupling_constant','Dipole','potential_energy','fc','sd','pso','dso','mulliken_charge_0','mulliken_charge_1'], axis=1)\n    db_type = db_type.drop(['id','molecule_name'], axis=1)\n    db_type = reduce_mem_usage(db_type)\n    \n    return [db_type, coupling, dipole, potential, fermi, spin_dipolar,paramagnetic_spin,diamagnetic_spin,mulliken_0,mulliken_1]\n\n# Metrics MAE\ndef metric_mae(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    y_true = df.scalar_coupling_constant.values\n    y_pred = df.prediction.values\n    mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n    maes.append(mae)\n    \n    return np.mean(maes)\n# Modification of structures_csv\n\ndef mod_structures(structures_csv):\n    structures_csv.round(5)\n    atomic_num = {'H':1, 'C':6, 'N':7, 'O':8, 'F':9}\n    atoms = structures_csv['atom'].values\n    atomic_number = [atomic_num[x] for x in atoms]\n    \n    structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n    structures_csv['Atomic_mass'] = atomic_number\n    structures_csv['Tot_atoms_molecule']= structures_csv.groupby(['molecule_name'])['molecule_name'].transform('count')\n\n    # Center of gravity. \n\n    structures_csv['mass_x'] = structures_csv['Atomic_mass'] * structures_csv['x']\n    structures_csv['mass_y'] = structures_csv['Atomic_mass'] * structures_csv['y']\n    structures_csv['mass_z'] = structures_csv['Atomic_mass'] * structures_csv['z']\n    structures_csv['Mol_mass']= structures_csv.groupby(['molecule_name'])['Atomic_mass'].transform('sum')\n\n    structures_csv['sum_mass_x']= structures_csv.groupby(['molecule_name'])['mass_x'].transform('sum')\n    structures_csv['sum_mass_y']= structures_csv.groupby(['molecule_name'])['mass_y'].transform('sum')\n    structures_csv['sum_mass_z']= structures_csv.groupby(['molecule_name'])['mass_z'].transform('sum')\n\n    structures_csv['XG']= structures_csv['sum_mass_x'] \/ structures_csv['Mol_mass']\n    structures_csv['YG']= structures_csv['sum_mass_y'] \/ structures_csv['Mol_mass']\n    structures_csv['ZG']= structures_csv['sum_mass_z'] \/ structures_csv['Mol_mass']\n\n    # Total size of molecule\n\n    structures_csv['min_x_mol'] = structures_csv.groupby(['molecule_name'])['x'].transform('min')\n    structures_csv['max_x_mol'] = structures_csv.groupby(['molecule_name'])['x'].transform('max')\n    structures_csv['Size_mol_x'] = structures_csv['max_x_mol'] - structures_csv['min_x_mol']\n\n    structures_csv['min_y_mol'] = structures_csv.groupby(['molecule_name'])['y'].transform('min')\n    structures_csv['max_y_mol'] = structures_csv.groupby(['molecule_name'])['y'].transform('max')\n    structures_csv['Size_mol_y'] = structures_csv['max_y_mol'] - structures_csv['min_y_mol']\n\n    structures_csv['min_z_mol'] = structures_csv.groupby(['molecule_name'])['z'].transform('min')\n    structures_csv['max_z_mol'] = structures_csv.groupby(['molecule_name'])['z'].transform('max')\n    structures_csv['Size_mol_z'] = structures_csv['max_z_mol'] - structures_csv['min_z_mol']\n\n    structures_csv['Size_mol'] = np.sqrt(np.square(structures_csv['Size_mol_x'])+np.square(structures_csv['Size_mol_y'])+np.square(structures_csv['Size_mol_z']))\n    structures_csv['Cos_x_size_mol'] = structures_csv['Size_mol_x'] \/ structures_csv['Size_mol']\n    structures_csv['Cos_y_size_mol'] = structures_csv['Size_mol_y'] \/ structures_csv['Size_mol']\n    structures_csv['Cos_z_size_mol'] = structures_csv['Size_mol_z'] \/ structures_csv['Size_mol']\n\n    structures_csv = structures_csv.drop(['min_x_mol','max_x_mol','min_y_mol','max_y_mol','min_z_mol','max_z_mol'], axis=1)\n    structures_csv = structures_csv.drop({'mass_x','mass_y','mass_z','sum_mass_x','sum_mass_y','sum_mass_z','Atomic_mass'}, axis=1)\n    structures_csv = reduce_mem_usage(structures_csv)\n    return structures_csv\n\n\n# Creation of superfeatures\ndef superfeatures(train_csv):\n    # distances \n    train_csv['Dist_XG_to_x0'] = train_csv['XG_x']-train_csv['x0']\n    train_csv['Dist_YG_to_y0'] = train_csv['YG_x']-train_csv['y0']\n    train_csv['Dist_ZG_to_z0'] = train_csv['ZG_x']-train_csv['z0']\n    train_csv['Dist_XG_to_x1'] = train_csv['XG_x']-train_csv['x1']\n    train_csv['Dist_YG_to_y1'] = train_csv['YG_x']-train_csv['y1']\n    train_csv['Dist_ZG_to_z1'] = train_csv['ZG_x']-train_csv['z1']\n    \n    dx =train_csv['x0']-train_csv['x1']\n    dy =train_csv['y0']-train_csv['y1']\n    dz =train_csv['z0']-train_csv['z1']\n    distances_train = np.sqrt(dx**2 + dy**2 + dz**2)\n\n    train_csv['At_dist']=distances_train\n    train_csv['At_dist_x']=dx\n    train_csv['At_dist_y']=dy\n    train_csv['At_dist_z']=dz\n    \n    train_csv['Mol_dist_mean']=train_csv.groupby('molecule_name')['At_dist'].transform('mean')\n    train_csv['Mol_dist_max']=train_csv.groupby('molecule_name')['At_dist'].transform('max')\n    train_csv['Mol_dist_min']=train_csv.groupby('molecule_name')['At_dist'].transform('min')\n\n    train_csv['Num_coupl_mol'] = train_csv.groupby('molecule_name')['molecule_name'].transform('count')\n    train_csv['Num_coupl_mol_and_type'] = train_csv.groupby(['molecule_name','type'])['molecule_name'].transform('count')\n    \n    train_csv['Atom_0_coupl_mol'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n    train_csv['Atom_0_coupl_mol_and_type'] = train_csv.groupby(['molecule_name', 'atom_index_0','type'])['id'].transform('count')\n    \n    train_csv['Atom_1_coupl_mol'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n    train_csv['Atom_1_coupl_mol_and_type'] = train_csv.groupby(['molecule_name', 'atom_index_1','type'])['id'].transform('count')\n\n    train_csv['Rel_dist_x'] = train_csv['At_dist_x'] \/ (train_csv['At_dist'])\n    train_csv['Rel_dist_y'] = train_csv['At_dist_y'] \/ (train_csv['At_dist'])\n    train_csv['Rel_dist_z'] = train_csv['At_dist_z'] \/ (train_csv['At_dist'])\n    \n    distances_0_CG_train = np.sqrt(np.square(train_csv['Dist_XG_to_x0'])+np.square(train_csv['Dist_YG_to_y0'])+np.square(train_csv['Dist_ZG_to_z0']))\n    distances_1_CG_train = np.sqrt(np.square(train_csv['Dist_XG_to_x1'])+np.square(train_csv['Dist_YG_to_y1'])+np.square(train_csv['Dist_ZG_to_z1']))\n    train_csv['Dist_CG_x0'] = distances_0_CG_train\n    train_csv['Dist_CG_x1'] = distances_1_CG_train\n\n    train_csv['Rel_XG_dist_x0'] = train_csv['Dist_XG_to_x0'] \/ (train_csv['Dist_CG_x0'] + 1e-5) \n    train_csv['Rel_YG_dist_x0'] = train_csv['Dist_YG_to_y0'] \/ (train_csv['Dist_CG_x0'] + 1e-5)\n    train_csv['Rel_ZG_dist_x0'] = train_csv['Dist_ZG_to_z0'] \/ (train_csv['Dist_CG_x0'] + 1e-5)\n\n    train_csv['Rel_XG_dist_x1'] = train_csv['Dist_XG_to_x1'] \/ (train_csv['Dist_CG_x1'] + 1e-5) \n    train_csv['Rel_YG_dist_x1'] = train_csv['Dist_YG_to_y1'] \/ (train_csv['Dist_CG_x1'] + 1e-5)\n    train_csv['Rel_ZG_dist_x1'] = train_csv['Dist_ZG_to_z1'] \/ (train_csv['Dist_CG_x1'] + 1e-5)\n\n    train_csv['At_0_dist_mean'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('mean')\n    train_csv['At_0_dist_max'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('max')\n    train_csv['At_0_dist_min'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('min')\n\n    train_csv['At_0_dist_x_max'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_x'].transform('max')\n    train_csv['At_0_dist_x_min'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_x'].transform('min')\n    train_csv['At_0_dist_y_max'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_y'].transform('max')\n    train_csv['At_0_dist_y_min'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_y'].transform('min')\n    train_csv['At_0_dist_z_max'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_z'].transform('max')\n    train_csv['At_0_dist_z_min'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist_z'].transform('min')\n    \n    train_csv['At_1_dist_mean'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('mean')\n    train_csv['At_1_dist_max'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('max')\n    train_csv['At_1_dist_min'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('min')\n    \n    train_csv['At_1_dist_x_max'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_x'].transform('max')\n    train_csv['At_1_dist_x_min'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_x'].transform('min')\n    train_csv['At_1_dist_y_max'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_y'].transform('max')\n    train_csv['At_1_dist_y_min'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_y'].transform('min')\n    train_csv['At_1_dist_z_max'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_z'].transform('max')\n    train_csv['At_1_dist_z_min'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist_z'].transform('min')\n    \n    #cosine\n    uve_0_x = train_csv['XG_x']-train_csv['x0']\n    uve_0_y = train_csv['YG_x']-train_csv['y0']\n    uve_0_z = train_csv['ZG_x']-train_csv['z0']\n\n    uve_1_x = train_csv['XG_x']-train_csv['x1']\n    uve_1_y = train_csv['YG_x']-train_csv['y1']\n    uve_1_z = train_csv['ZG_x']-train_csv['z1']\n\n    train_csv['cos_G_x0'] = ((uve_0_x * dx)+(uve_0_y * dy)+(uve_0_z * dz))\/((abs(distances_train)*abs(distances_0_CG_train))+ 1e-2)\n    train_csv['cos_G_x1'] = ((uve_1_x * dx)+(uve_1_y * dy)+(uve_1_z * dz))\/((abs(distances_train)*abs(distances_1_CG_train))+ 1e-2)\n\n    dist_xy = np.sqrt(np.square(dx)+np.square(dy))\n    dist_xz = np.sqrt(np.square(dx)+np.square(dz))\n    dist_yz = np.sqrt(np.square(dy)+np.square(dz))\n\n    uve_0_xy = np.sqrt(np.square(uve_0_x)+np.square(uve_0_y))\n    uve_0_xz = np.sqrt(np.square(uve_0_x)+np.square(uve_0_z))\n    uve_0_yz = np.sqrt(np.square(uve_0_y)+np.square(uve_0_z))\n    uve_1_xy = np.sqrt(np.square(uve_1_x)+np.square(uve_1_y))\n    uve_1_xz = np.sqrt(np.square(uve_1_x)+np.square(uve_1_z))\n    uve_1_yz = np.sqrt(np.square(uve_1_y)+np.square(uve_1_z))\n\n    train_csv['cos_G_x0_pl_xy'] = ((dx*uve_0_x)+(dy*uve_0_y))\/((abs(dist_xy)*abs(uve_0_xy))+1e-2)\n    train_csv['cos_G_x0_pl_xz'] = ((dx*uve_0_x)+(dz*uve_0_z))\/((abs(dist_xz)*abs(uve_0_xz))+1e-2)\n    train_csv['cos_G_x0_pl_yz'] = ((dy*uve_0_y)+(dz*uve_0_z))\/((abs(dist_yz)*abs(uve_0_yz))+1e-2)\n    train_csv['cos_G_x1_pl_xy'] = ((dx*uve_1_x)+(dy*uve_1_y))\/((abs(dist_xy)*abs(uve_1_xy))+1e-2)\n    train_csv['cos_G_x1_pl_xz'] = ((dx*uve_1_x)+(dz*uve_1_z))\/((abs(dist_xz)*abs(uve_1_xz))+1e-2)\n    train_csv['cos_G_x1_pl_yz'] = ((dy*uve_1_y)+(dz*uve_1_z))\/((abs(dist_yz)*abs(uve_1_yz))+1e-2)\n\n    #---------------------------------------------------------\n    train_csv['Mol_x_mean']=train_csv.groupby('molecule_name')['x0'].transform('mean')\n    train_csv['Mol_x_max']=train_csv.groupby('molecule_name')['x0'].transform('max')\n    train_csv['Mol_x_min']=train_csv.groupby('molecule_name')['x0'].transform('min')\n    train_csv['Mol_y_mean']=train_csv.groupby('molecule_name')['y0'].transform('mean')\n    train_csv['Mol_y_max']=train_csv.groupby('molecule_name')['y0'].transform('max')\n    train_csv['Mol_y_min']=train_csv.groupby('molecule_name')['y0'].transform('min')\n    train_csv['Mol_z_mean']=train_csv.groupby('molecule_name')['z0'].transform('mean')\n    train_csv['Mol_z_max']=train_csv.groupby('molecule_name')['z0'].transform('max')\n    train_csv['Mol_z_min']=train_csv.groupby('molecule_name')['z0'].transform('min')\n\n    dx0_mean= train_csv['x0']-train_csv['Mol_x_mean']\n    dy0_mean= train_csv['y0']-train_csv['Mol_y_mean']\n    dz0_mean= train_csv['z0']-train_csv['Mol_z_mean']\n\n    dx0_max= train_csv['x0']-train_csv['Mol_x_max']\n    dy0_max= train_csv['y0']-train_csv['Mol_y_max']\n    dz0_max= train_csv['z0']-train_csv['Mol_z_max']\n\n    dx0_min= train_csv['x0']-train_csv['Mol_x_min']\n    dy0_min= train_csv['y0']-train_csv['Mol_y_min']\n    dz0_min= train_csv['z0']-train_csv['Mol_z_min']\n\n    dx1_mean= train_csv['x1']-train_csv['Mol_x_mean']\n    dy1_mean= train_csv['y1']-train_csv['Mol_y_mean']\n    dz1_mean= train_csv['z1']-train_csv['Mol_z_mean']\n\n    dx1_max= train_csv['x1']-train_csv['Mol_x_max']\n    dy1_max= train_csv['y1']-train_csv['Mol_y_max']\n    dz1_max= train_csv['z1']-train_csv['Mol_z_max']\n\n    dx1_min= train_csv['x1']-train_csv['Mol_x_min']\n    dy1_min= train_csv['y1']-train_csv['Mol_y_min']\n    dz1_min= train_csv['z1']-train_csv['Mol_z_min']\n\n    dist_x0_mean_xy = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean))\n    dist_x0_mean_xz = np.sqrt(np.square(dx0_mean)+np.square(dz0_mean))\n    dist_x0_mean_yz = np.sqrt(np.square(dy0_mean)+np.square(dz0_mean))\n    \n    train_csv['cos_Mean_x0_pl_xy'] = ((dx*dx0_mean)+(dy*dy0_mean))\/((abs(dist_xy)*abs(dist_x0_mean_xy))+1e-2)\n    train_csv['cos_Mean_x0_pl_xz'] = ((dx*dx0_mean)+(dz*dz0_mean))\/((abs(dist_xz)*abs(dist_x0_mean_xz))+1e-2)\n    train_csv['cos_Mean_x0_pl_yz'] = ((dy*dy0_mean)+(dz*dz0_mean))\/((abs(dist_yz)*abs(dist_x0_mean_yz))+1e-2)\n\n    dist_x0_max_xy = np.sqrt(np.square(dx0_max)+np.square(dy0_max))\n    dist_x0_max_xz = np.sqrt(np.square(dx0_max)+np.square(dz0_max))\n    dist_x0_max_yz = np.sqrt(np.square(dy0_max)+np.square(dz0_max))\n    \n    train_csv['cos_Max_x0_pl_xy'] = ((dx*dx0_max)+(dy*dy0_max))\/((abs(dist_xy)*abs(dist_x0_max_xy))+1e-2)\n    train_csv['cos_Max_x0_pl_xz'] = ((dx*dx0_max)+(dz*dz0_max))\/((abs(dist_xz)*abs(dist_x0_max_xz))+1e-2)\n    train_csv['cos_Max_x0_pl_yz'] = ((dy*dy0_max)+(dz*dz0_max))\/((abs(dist_yz)*abs(dist_x0_max_yz))+1e-2)\n\n    dist_x0_min_xy = np.sqrt(np.square(dx0_min)+np.square(dy0_min))\n    dist_x0_min_xz = np.sqrt(np.square(dx0_min)+np.square(dz0_min))\n    dist_x0_min_yz = np.sqrt(np.square(dy0_min)+np.square(dz0_min))\n    \n    train_csv['cos_Min_x0_pl_xy'] = ((dx*dx0_min)+(dy*dy0_min))\/((abs(dist_xy)*abs(dist_x0_min_xy))+1e-2)\n    train_csv['cos_Min_x0_pl_xz'] = ((dx*dx0_min)+(dz*dz0_min))\/((abs(dist_xz)*abs(dist_x0_min_xz))+1e-2)\n    train_csv['cos_Min_x0_pl_yz'] = ((dy*dy0_min)+(dz*dz0_min))\/((abs(dist_yz)*abs(dist_x0_min_yz))+1e-2)\n\n    dist_x0_mean = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean)+np.square(dz0_mean))\n    dist_x0_max = np.sqrt(np.square(dx0_max)+np.square(dy0_max)+np.square(dz0_max))\n    dist_x0_min = np.sqrt(np.square(dx0_min)+np.square(dy0_min)+np.square(dz0_min))\n    \n    train_csv['cos_Mean_x0'] = ((dx*dx0_mean)+(dy*dy0_mean)+(dz*dz0_mean))\/((abs(distances_train)*abs(dist_x0_mean))+1e-2)\n    train_csv['cos_Max_x0'] = ((dx*dx0_max)+(dy*dy0_max)+(dz*dz0_max))\/((abs(distances_train)*abs(dist_x0_max))+1e-2)\n    train_csv['cos_Min_x0'] = ((dx*dx0_min)+(dy*dy0_min)+(dz*dz0_min))\/((abs(distances_train)*abs(dist_x0_min))+1e-2)\n\n    dist_x1_mean_xy = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean))\n    dist_x1_mean_xz = np.sqrt(np.square(dx1_mean)+np.square(dz1_mean))\n    dist_x1_mean_yz = np.sqrt(np.square(dy1_mean)+np.square(dz1_mean))\n    \n    train_csv['cos_Mean_x1_pl_xy'] = ((dx*dx1_mean)+(dy*dy1_mean))\/((abs(dist_xy)*abs(dist_x1_mean_xy))+1e-2)\n    train_csv['cos_Mean_x1_pl_xz'] = ((dx*dx1_mean)+(dz*dz1_mean))\/((abs(dist_xz)*abs(dist_x1_mean_xz))+1e-2)\n    train_csv['cos_Mean_x1_pl_yz'] = ((dy*dy1_mean)+(dz*dz1_mean))\/((abs(dist_yz)*abs(dist_x1_mean_yz))+1e-2)\n\n    dist_x1_max_xy = np.sqrt(np.square(dx1_max)+np.square(dy1_max))\n    dist_x1_max_xz = np.sqrt(np.square(dx1_max)+np.square(dz1_max))\n    dist_x1_max_yz = np.sqrt(np.square(dy1_max)+np.square(dz1_max))\n    \n    train_csv['cos_Max_x1_pl_xy'] = ((dx*dx1_max)+(dy*dy1_max))\/((abs(dist_xy)*abs(dist_x1_max_xy))+1e-2)\n    train_csv['cos_Max_x1_pl_xz'] = ((dx*dx1_max)+(dz*dz1_max))\/((abs(dist_xz)*abs(dist_x1_max_xz))+1e-2)\n    train_csv['cos_Max_x1_pl_yz'] = ((dy*dy1_max)+(dz*dz1_max))\/((abs(dist_yz)*abs(dist_x1_max_yz))+1e-2)\n\n    dist_x1_min_xy = np.sqrt(np.square(dx1_min)+np.square(dy1_min))\n    dist_x1_min_xz = np.sqrt(np.square(dx1_min)+np.square(dz1_min))\n    dist_x1_min_yz = np.sqrt(np.square(dy1_min)+np.square(dz1_min))\n    \n    train_csv['cos_Min_x1_pl_xy'] = ((dx*dx1_min)+(dy*dy1_min))\/((abs(dist_xy)*abs(dist_x1_min_xy))+1e-2)\n    train_csv['cos_Min_x1_pl_xz'] = ((dx*dx1_min)+(dz*dz1_min))\/((abs(dist_xz)*abs(dist_x1_min_xz))+1e-2)\n    train_csv['cos_Min_x1_pl_yz'] = ((dy*dy1_min)+(dz*dz1_min))\/((abs(dist_yz)*abs(dist_x1_min_yz))+1e-2)\n\n    dist_x1_mean = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean)+np.square(dz1_mean))\n    dist_x1_max = np.sqrt(np.square(dx1_max)+np.square(dy1_max)+np.square(dz1_max))\n    dist_x1_min = np.sqrt(np.square(dx1_min)+np.square(dy1_min)+np.square(dz1_min))\n    \n    train_csv['cos_Mean_x1'] = ((dx*dx1_mean)+(dy*dy1_mean)+(dz*dz1_mean))\/((abs(distances_train)*abs(dist_x1_mean))+1e-2)\n    train_csv['cos_Max_x1'] = ((dx*dx1_max)+(dy*dy1_max)+(dz*dz1_max))\/((abs(distances_train)*abs(dist_x1_max))+1e-2)\n    train_csv['cos_Min_x1'] = ((dx*dx1_min)+(dy*dy1_min)+(dz*dz1_min))\/((abs(distances_train)*abs(dist_x1_min))+1e-2)\n\n    # Edit columns\n\n    train_csv = train_csv.drop({'x0','x1','y1','z0','z1'}, axis=1)\n    train_csv = train_csv.drop({'y0'}, axis=1)\n    train_csv = train_csv.drop({'XG_x','YG_x','ZG_x'}, axis=1) \n    #train_csv = train_csv.drop({'atom_index_0','atom_index_1'}, axis=1)\n    train_csv= train_csv.drop({'atom0','atom1','XG_y','YG_y','ZG_y','Tot_atoms_molecule_y','Mol_mass_y'}, axis=1)\n    train_csv = train_csv.rename(columns={'Tot_atoms_molecule_x':'Tot_atoms_mol','Mol_mass_x':'Mol_mass'})\n    train_csv= train_csv.drop({'Size_mol_x_y','Size_mol_y_y','Size_mol_z_y','Size_mol_y'}, axis=1)\n    train_csv = train_csv.rename(columns={'Size_mol_x_x':'Size_mol_x','Size_mol_y_x':'Size_mol_y', 'Size_mol_z_x':'Size_mol_z','Size_mol_x':'Size_mol'})\n    train_csv= train_csv.drop({'Cos_x_size_mol_y','Cos_y_size_mol_y','Cos_z_size_mol_y'}, axis=1)\n    train_csv = train_csv.rename(columns={'Cos_x_size_mol_x':'Cos_x_size_mol','Cos_y_size_mol_x':'Cos_y_size_mol','Cos_z_size_mol_x':'Cos_z_size_mol'})\n    train_csv= train_csv.drop({'molecule_index_y'}, axis=1)\n    train_csv = train_csv.rename(columns={'molecule_index_x':'molecule_index'})\n\n    train_csv = reduce_mem_usage(train_csv)\n    \n    return train_csv\n\n# Split train_csv in several dataframes, each type of coupling\n\ndef split_train_dev(db,index, split_coef = 0.1, threshold = 0.90):\n    \n    db_type = db[db['type'] == index]\n    train_csv, val_csv = train_test_split(db_type, test_size = split_coef, random_state=42)\n    # Threshold for removing correlated variables\n    # https:\/\/www.kaggle.com\/adrianoavelar\/gridsearch-for-eachtype-lb-1-0\n\n    # Absolute value correlation matrix\n    corr_matrix = train_csv.corr().abs()\n    # Getting the upper triangle of correlations\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    # Select columns with correlations above threshold\n    \n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print(to_drop)\n    for col in to_drop:\n        if col == 'fc':\n            to_drop.remove('fc')\n        if col == 'dso':\n            to_drop.remove('dso')\n        if col == 'sd':\n            to_drop.remove('sd')          \n        \n    #to_drop.remove('fc') \n\n    print('There are %d columns to remove.' % (len(to_drop)))\n    print('The columns are: ', to_drop )\n    \n    train_csv = train_csv.drop(to_drop,axis=1)\n    val_csv = val_csv.drop(to_drop,axis=1)\n    train_csv = reduce_mem_usage(train_csv)\n    val_csv = reduce_mem_usage(val_csv)\n    \n    print('Training shape: ', train_csv.shape)\n    print('Validation shape: ', val_csv.shape)\n\n    return train_csv, val_csv , to_drop","7709527b":"# Edit structures_csv\nstructures_mod_csv = mod_structures(structures_csv)","0966d3a8":"# Merge train_csv and test_csv with structures_csv\n\ntrain_csv = number_each_atoms_molecule(structures_mod_csv, train_csv,0)\ntrain_csv = number_each_atoms_molecule(structures_mod_csv, train_csv,1)\n\ntest_csv = number_each_atoms_molecule(structures_mod_csv, test_csv,0)\ntest_csv = number_each_atoms_molecule(structures_mod_csv, test_csv,1)","301002f0":"# Merge train_csv with scalar_coupling contributions\ntrain_csv = pd.merge(train_csv, scalar_coupling_contributions_csv, how = 'left',\n                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])\n\n# Merge train_csv with mulliken\ntrain_csv = map_mulliken_charge(train_csv,0)\ntrain_csv = map_mulliken_charge(train_csv,1)","ce706dbc":"# Creation a lot of features\ntrain_csv = superfeatures(train_csv)\ntest_csv = superfeatures(test_csv)","d614b883":"# Merge train_csv with dipole moments and potential energy\ntrain_csv = pd.merge(train_csv,dipole_moments_csv,left_on='molecule_name', right_on='molecule_name')\ntrain_csv = pd.merge(train_csv,potential_energy_csv,left_on='molecule_name', right_on='molecule_name')\ndipole=np.sqrt(np.square(train_csv['X'])+np.square(train_csv['Y'])+np.square(train_csv['Z']))\ntrain_csv = train_csv.drop({'X','Y','Z'}, axis=1)\ntrain_csv['Dipole']=dipole\ntrain_csv = reduce_mem_usage(train_csv)","723fbdf4":"train_csv.head()","2653ebbb":"test_csv.head()","23ed041b":"print(train_csv.shape)\nprint(test_csv.shape)","ec7ac711":"## DEEP NEURAL NETWORK \n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization \nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense, Dropout\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","bb64200d":" # TRAINING. HYPERPARAMETERS\n\nepochs_1JHN= 80\nbatch_1JHN = 32\n\nepochs_2JHH= 120\nbatch_2JHH = 128\n\nepochs_2JHN= 100\nbatch_2JHN = 128\n\nepochs_3JHN= 200\nbatch_3JHN = 128\n\nepochs_1JHC= 60 \nbatch_1JHC = 128\n\nepochs_2JHC= 120\nbatch_2JHC = 1024\n\nepochs_3JHH= 80\nbatch_3JHH = 512\n\nepochs_3JHC= 70\nbatch_3JHC = 1024\n","87a2d0dd":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Split train and validation set, and split them in several dataframes, each type of coupling \n\ntrain_1JHN, val_1JHN, to_drop = split_train_dev(train_csv,'1JHN', split_coef = 0.05, threshold = 1)\ntrain_1JHN = database_type(train_1JHN)\nval_1JHN = database_type(val_1JHN)\ntest_1JHN = test_csv[test_csv['type'] == '1JHN']\ntest_1JHN = test_1JHN.drop(to_drop, axis=1)\n\nplt.plot(train_1JHN[4], train_1JHN[1], 'b.')\nplt.show()","efb5d288":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights1.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ntrain = {'scalar_coupling': train_1JHN[1].values,         \n        'dipolar_moment': train_1JHN[2].values,\n        'potential_energy': train_1JHN[3].values,\n         'fermi_coupling': train_1JHN[4].values,\n        'spin_dipolar': train_1JHN[5].values,\n        'paramagnetic_spin': train_1JHN[6].values,\n        'diamagnetic_spin': train_1JHN[7].values,\n        'mulliken_0': train_1JHN[8].values,\n        'mulliken_1': train_1JHN[9].values}\nvalidation = {'scalar_coupling': val_1JHN[1].values,\n        'dipolar_moment': val_1JHN[2].values,\n        'potential_energy': val_1JHN[3].values,\n         'fermi_coupling': val_1JHN[4].values,\n        'spin_dipolar': val_1JHN[5].values,\n        'paramagnetic_spin': val_1JHN[6].values,\n        'diamagnetic_spin': val_1JHN[7].values,\n        'mulliken_0': val_1JHN[8].values,\n        'mulliken_1': val_1JHN[9].values}\n\ndef model_coupling_constant_1JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n \n    xsc = BatchNormalization()(X_input)\n    xsc = Dense(256, activation='elu')(xsc) \n    xsc = Dense(512, activation='elu')(xsc)\n    xsc = Dense(64, activation='elu')(xsc)\n    xsc = Dense(64, activation='elu')(xsc)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(16, activation='elu')(xsc)\n    xsc = Dense(4, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n    \nModel_Coupling_1JHN = model_coupling_constant_1JHN(train_1JHN[0])\nModel_Coupling_1JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_1JHN = Model_Coupling_1JHN.fit(train_1JHN[0],train, validation_data=(val_1JHN[0],validation),epochs=epochs_1JHN,verbose=1,batch_size = batch_1JHN, callbacks=[best_param])#, early_stopping])","80f76202":"plt.plot(history_1JHN.history['loss'])\nplt.plot(history_1JHN.history['val_loss'])\nplt.title('loss 1JHN')\nplt.ylabel('Loss 1JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","7d2535e8":"Model_Coupling_1JHN.load_weights(\"weights1.best.hdf5\")\nModel_Coupling_1JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\n\nmodel_check_1JHN = Model_Coupling_1JHN.predict(val_1JHN[0])\nlog_MAE_1JHN = metric_mae(val_1JHN[1], model_check_1JHN)\nval_1JHN[1] = val_1JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_1JHN=\", log_MAE_1JHN)","39c29727":"inter_1JHN = pd.DataFrame(test_1JHN['id']) \ninter_1JHN['type'] = test_1JHN['type']\ntest_1JHN_bis = test_1JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_1JHN = Model_Coupling_1JHN.predict(test_1JHN_bis)\ninter_1JHN['pred_values'] = model_predict_1JHN","f3f68756":"# mejorado, \n#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights2.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\n# Split train_csv in several dataframes, each type of coupling\n\ntrain_2JHH, val_2JHH, to_drop = split_train_dev(train_csv,'2JHH', split_coef = 0.1, threshold = 1)\ntrain_2JHH = database_type(train_2JHH)\nval_2JHH = database_type(val_2JHH)\ntest_2JHH = test_csv[test_csv['type'] == '2JHH']\ntest_2JHH = test_2JHH.drop(to_drop, axis=1)\n\nplt.plot(train_2JHH[4], train_2JHH[1], 'b.')\nplt.show()","7530328c":"def model_coupling_constant_2JHH(X):  \n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHH = model_coupling_constant_2JHH(train_2JHH[0]) # R1\nModel_Coupling_2JHH.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHH = Model_Coupling_2JHH.fit(train_2JHH[0],{'scalar_coupling': train_2JHH[1].values,\n                            'fermi_coupling': train_2JHH[4].values},validation_data=(val_2JHH[0],{'scalar_coupling': val_2JHH[1].values,\n                            'fermi_coupling': val_2JHH[4].values}),epochs=epochs_2JHH,verbose=1,batch_size = batch_2JHH, callbacks=[best_param])#, early_stopping])","9af5ea4a":"plt.plot(history_2JHH.history['loss'])\nplt.plot(history_2JHH.history['val_loss'])\nplt.title('loss 2JHH')\nplt.ylabel('Loss 2JHH')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","e356b393":"Model_Coupling_2JHH.load_weights(\"weights2.best.hdf5\")\nModel_Coupling_2JHH.compile(loss='mae', optimizer='Adam')\n\nmodel_check_2JHH = Model_Coupling_2JHH.predict(val_2JHH[0])\nlog_MAE_2JHH = metric_mae(val_2JHH[1], model_check_2JHH)\nval_2JHH[1] = val_2JHH[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHH=\", log_MAE_2JHH)","ba82b86c":"inter_2JHH = pd.DataFrame(test_2JHH['id']) \ninter_2JHH['type'] = test_2JHH['type']\ntest_2JHH_bis = test_2JHH.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHH = Model_Coupling_2JHH.predict(test_2JHH_bis)\ninter_2JHH['pred_values'] = model_predict_2JHH","33057119":"# mejorado \n\n#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights3.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\n# Split train and validation set\n\ntrain_2JHN, val_2JHN, to_drop = split_train_dev(train_csv,'2JHN', split_coef = 0.1, threshold = 1)\ntrain_2JHN = database_type(train_2JHN) # ESTE YA NO ES TAN LINEAL ENTRE 'FC' Y 'COUPLING CONSTANT' ALGUN PUNTO ESTA MUY FUEra\nval_2JHN = database_type(val_2JHN)\ntest_2JHN = test_csv[test_csv['type'] == '2JHN']\ntest_2JHN = test_2JHN.drop(to_drop, axis=1)\n\nplt.plot(train_2JHN[4], train_2JHN[1], 'b.')\nplt.show()","595b87cd":"def model_coupling_constant_2JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHN = model_coupling_constant_2JHN(train_2JHN[0]) # R1\nModel_Coupling_2JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHN = Model_Coupling_2JHN.fit(train_2JHN[0],{'scalar_coupling': train_2JHN[1].values,\n                            'fermi_coupling': train_2JHN[4].values},validation_data=(val_2JHN[0],{'scalar_coupling': val_2JHN[1].values,\n                            'fermi_coupling': val_2JHN[4].values}),epochs=epochs_2JHN,verbose=1,batch_size = batch_2JHN, callbacks=[best_param])","853d096d":"plt.plot(history_2JHN.history['loss'])\nplt.plot(history_2JHN.history['val_loss'])\nplt.title('loss 2JHN')\nplt.ylabel('Loss 2JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","6b715673":"Model_Coupling_2JHN.load_weights(\"weights3.best.hdf5\")\nModel_Coupling_2JHN.compile(loss='mae', optimizer='Adam')\n\nmodel_check_2JHN = Model_Coupling_2JHN.predict(val_2JHN[0])\nlog_MAE_2JHN = metric_mae(val_2JHN[1], model_check_2JHN)\nval_2JHN[1] = val_2JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHN=\", log_MAE_2JHN)","713e3c15":"inter_2JHN = pd.DataFrame(test_2JHN['id']) \ninter_2JHN['type'] = test_2JHN['type']\ntest_2JHN_bis = test_2JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHN = Model_Coupling_2JHN.predict(test_2JHN_bis)\ninter_2JHN['pred_values'] = model_predict_2JHN","b9f2d467":"# Split train and validation set\ntrain_3JHN, val_3JHN, to_drop = split_train_dev(train_csv,'3JHN', split_coef = 0.1, threshold = 1)\ntrain_3JHN = database_type(train_3JHN) # ESTE YA NO ES TAN LINEAL ENTRE 'FC' Y 'COUPLING CONSTANT'\nval_3JHN = database_type(val_3JHN)\ntest_3JHN = test_csv[test_csv['type'] == '3JHN']\ntest_3JHN = test_3JHN.drop(to_drop, axis=1)\n\nplt.plot(train_3JHN[4], train_3JHN[1], 'b.')\nplt.show()","9c0667d4":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights4.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ndef model_coupling_constant_3JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHN = model_coupling_constant_3JHN(train_3JHN[0]) # R1\nModel_Coupling_3JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHN = Model_Coupling_3JHN.fit(train_3JHN[0],{'scalar_coupling': train_3JHN[1].values,\n                            'fermi_coupling': train_3JHN[4].values},validation_data=(val_3JHN[0],{'scalar_coupling': val_3JHN[1].values,\n                            'fermi_coupling': val_3JHN[4].values}),epochs=epochs_3JHN,verbose=1,batch_size = batch_3JHN, callbacks=[best_param])","531226a5":"plt.plot(history_3JHN.history['loss'])\nplt.plot(history_3JHN.history['val_loss'])\nplt.title('loss 3JHN')\nplt.ylabel('Loss 3JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","3fcd804a":"Model_Coupling_3JHN.load_weights(\"weights4.best.hdf5\")\nModel_Coupling_3JHN.compile(loss='mae', optimizer='Adam')\n\nmodel_check_3JHN = Model_Coupling_3JHN.predict(val_3JHN[0])\nlog_MAE_3JHN = metric_mae(val_3JHN[1], model_check_3JHN)\nval_3JHN[1] = val_3JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHN=\", log_MAE_3JHN)","d3ddb0af":"inter_3JHN = pd.DataFrame(test_3JHN['id']) \ninter_3JHN['type'] = test_3JHN['type']\ntest_3JHN_bis = test_3JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHN = Model_Coupling_3JHN.predict(test_3JHN_bis)\ninter_3JHN['pred_values'] = model_predict_3JHN","2d16cc9c":"# MEJORADA \n\n# Split train_csv in several dataframes, each type of coupling\n\ntrain_1JHC, val_1JHC, to_drop = split_train_dev(train_csv,'1JHC', split_coef = 0.1, threshold = 1.0)\n\ntrain_1JHC = database_type(train_1JHC)\nval_1JHC = database_type(val_1JHC)\ntest_1JHC = test_csv[test_csv['type'] == '1JHC']\ntest_1JHC = test_1JHC.drop(to_drop, axis=1)\n\nplt.plot(train_1JHC[4], train_1JHC[1], 'b.')\nplt.show()","8ddd56d0":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights5.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ntrain = {'scalar_coupling': train_1JHC[1].values,         \n        'dipolar_moment': train_1JHC[2].values,\n        'potential_energy': train_1JHC[3].values,\n         'fermi_coupling': train_1JHC[4].values,\n        'spin_dipolar': train_1JHC[5].values,\n        'paramagnetic_spin': train_1JHC[6].values,\n        'diamagnetic_spin': train_1JHC[7].values,\n        'mulliken_0': train_1JHC[8].values,\n        'mulliken_1': train_1JHC[9].values}\nvalidation = {'scalar_coupling': val_1JHC[1].values,\n        'dipolar_moment': val_1JHC[2].values,\n        'potential_energy': val_1JHC[3].values,\n         'fermi_coupling': val_1JHC[4].values,\n        'spin_dipolar': val_1JHC[5].values,\n        'paramagnetic_spin': val_1JHC[6].values,\n        'diamagnetic_spin': val_1JHC[7].values,\n        'mulliken_0': val_1JHC[8].values,\n        'mulliken_1': val_1JHC[9].values}\n\ndef model_coupling_constant_1JHC(X):\n    X_input = Input(shape = (X.shape[1],))\n \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(32, activation='elu')(xsc) #esta no\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_1JHC = model_coupling_constant_1JHC(train_1JHC[0])\nModel_Coupling_1JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_1JHC = Model_Coupling_1JHC.fit(train_1JHC[0],train, validation_data=(val_1JHC[0],validation),epochs=epochs_1JHC,verbose=1,batch_size = batch_1JHC, callbacks=[best_param])","07ae6635":"plt.plot(history_1JHC.history['loss'])\nplt.plot(history_1JHC.history['val_loss'])\nplt.title('loss 1JHC')\nplt.ylabel('Loss 1JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')\n","c4c465ca":"Model_Coupling_1JHC.load_weights(\"weights5.best.hdf5\")\nModel_Coupling_1JHC.compile(loss='mae', optimizer='Adam')\n\nmodel_check_1JHC = Model_Coupling_1JHC.predict(val_1JHC[0])\nlog_MAE_1JHC = metric_mae(val_1JHC[1], model_check_1JHC)\nval_1JHC[1] = val_1JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_1JHC=\", log_MAE_1JHC)","64f603e2":"inter_1JHC = pd.DataFrame(test_1JHC['id']) \ninter_1JHC['type'] = test_1JHC['type']\ntest_1JHC_bis = test_1JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_1JHC = Model_Coupling_1JHC.predict(test_1JHC_bis)\ninter_1JHC['pred_values'] = model_predict_1JHC","690c75db":"# Split train and validation set\n\ntrain_2JHC, val_2JHC, to_drop = split_train_dev(train_csv,'2JHC', split_coef = 0.1, threshold = 1.0)\ntrain_2JHC = database_type(train_2JHC) #  ESTE YA NO ES TAN LINEAL ENTRE 'FC' Y 'COUPLING CONSTANT'\nval_2JHC = database_type(val_2JHC)\ntest_2JHC = test_csv[test_csv['type'] == '2JHC']\ntest_2JHC = test_2JHC.drop(to_drop, axis=1)\n\nplt.plot(train_2JHC[4], train_2JHC[1], 'b.')\nplt.show()","98e9e3b5":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights6.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ndef model_coupling_constant_2JHC(X):\n\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(32, activation='elu')(xsc) #esta no\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHC = model_coupling_constant_2JHC(train_2JHC[0]) # R1\nModel_Coupling_2JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHC = Model_Coupling_2JHC.fit(train_2JHC[0],{'scalar_coupling': train_2JHC[1].values,\n                            'fermi_coupling': train_2JHC[4].values},validation_data=(val_2JHC[0],{'scalar_coupling': val_2JHC[1].values,\n                            'fermi_coupling': val_2JHC[4].values}),epochs=epochs_2JHC,verbose=1,batch_size = batch_2JHC, callbacks=[best_param])","9947b8e4":"plt.plot(history_2JHC.history['loss'])\nplt.plot(history_2JHC.history['val_loss'])\nplt.title('loss 2JHC')\nplt.ylabel('Loss 2JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","d45116e2":"Model_Coupling_2JHC.load_weights(\"weights6.best.hdf5\")\nModel_Coupling_2JHC.compile(loss='mae', optimizer='Adam')\n\nmodel_check_2JHC = Model_Coupling_2JHC.predict(val_2JHC[0])\nlog_MAE_2JHC = metric_mae(val_2JHC[1], model_check_2JHC)\nval_2JHC[1] = val_2JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHC=\", log_MAE_2JHC)","1790c84e":"inter_2JHC = pd.DataFrame(test_2JHC['id']) \ninter_2JHC['type'] = test_2JHC['type']\ntest_2JHC_bis = test_2JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHC = Model_Coupling_2JHC.predict(test_2JHC_bis)\ninter_2JHC['pred_values'] = model_predict_2JHC","5148473e":"# Mejorada,\n\n# Split train_csv in several dataframes, each type of coupling\n\ntrain_3JHH, val_3JHH, to_drop = split_train_dev(train_csv,'3JHH', split_coef = 0.1, threshold = 1.0)\ntrain_3JHH = database_type(train_3JHH) # ESTE YA NO ES TAN LINEAL ENTRE 'FC' Y 'COUPLING CONSTANT'\nval_3JHH = database_type(val_3JHH)\ntest_3JHH = test_csv[test_csv['type'] == '3JHH']\ntest_3JHH = test_3JHH.drop(to_drop, axis=1)\n\nplt.plot(train_3JHH[4], train_3JHH[1], 'b.')\nplt.show()","903ee4f6":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights7.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ndef model_coupling_constant_3JHH(X):\n\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    #xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHH = model_coupling_constant_3JHH(train_3JHH[0]) # R1\nModel_Coupling_3JHH.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHH = Model_Coupling_3JHH.fit(train_3JHH[0],{'scalar_coupling': train_3JHH[1].values,\n                            'fermi_coupling': train_3JHH[4].values},validation_data=(val_3JHH[0],{'scalar_coupling': val_3JHH[1].values,\n                            'fermi_coupling': val_3JHH[4].values}),epochs=epochs_3JHH,verbose=1,batch_size = batch_3JHH, callbacks=[best_param])","eaa19527":"plt.plot(history_3JHH.history['loss'])\nplt.plot(history_3JHH.history['val_loss'])\nplt.title('loss 3JHH')\nplt.ylabel('Loss 3JHH')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","5f88493b":"Model_Coupling_3JHH.load_weights(\"weights7.best.hdf5\")\nModel_Coupling_3JHH.compile(loss='mae', optimizer='Adam')\n\nmodel_check_3JHH = Model_Coupling_3JHH.predict(val_3JHH[0])\nlog_MAE_3JHH = metric_mae(val_3JHH[1], model_check_3JHH)\nval_3JHH[1] = val_3JHH[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHH=\", log_MAE_3JHH)","03892d23":"inter_3JHH = pd.DataFrame(test_3JHH['id']) \ninter_3JHH['type'] = test_3JHH['type']\ntest_3JHH_bis = test_3JHH.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHH = Model_Coupling_3JHH.predict(test_3JHH_bis)\ninter_3JHH['pred_values'] = model_predict_3JHH","af9f56bc":"# Mejorada,\n\n# Split train_csv in several dataframes, each type of coupling\n\ntrain_3JHC, val_3JHC, to_drop = split_train_dev(train_csv,'3JHC', split_coef = 0.1, threshold = 1.0)\ntrain_3JHC = database_type(train_3JHC) # ESTE YA NO ES TAN LINEAL ENTRE 'FC' Y 'COUPLING CONSTANT'\nval_3JHC = database_type(val_3JHC)\ntest_3JHC = test_csv[test_csv['type'] == '3JHC']\ntest_3JHC = test_3JHC.drop(to_drop, axis=1)\n\nplt.plot(train_3JHC[4], train_3JHC[1], 'b.')\nplt.show()","9b01a0bf":"#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nfilepath = \"weights8.best.hdf5\"\nbest_param =  ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', period=1)\n\ndef model_coupling_constant_3JHC(X):\n\n    X_input = Input(shape = (X.shape[1],))\n                    \n    #concat1 = concatenate([concat, x2_output])    \n    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) #0,1\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHC = model_coupling_constant_3JHC(train_3JHC[0]) # R1\nModel_Coupling_3JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHC = Model_Coupling_3JHC.fit(train_3JHC[0],{'scalar_coupling': train_3JHC[1].values,\n                            'fermi_coupling': train_3JHC[4].values},validation_data=(val_3JHC[0],{'scalar_coupling': val_3JHC[1].values,\n                            'fermi_coupling': val_3JHC[4].values}),epochs=epochs_3JHC,verbose=1,batch_size = batch_3JHC, callbacks=[best_param])","66813f63":"plt.plot(history_3JHC.history['loss'])\nplt.plot(history_3JHC.history['val_loss'])\nplt.title('loss 3JHC')\nplt.ylabel('Loss 3JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","d7bad7ea":"Model_Coupling_3JHC.load_weights(\"weights8.best.hdf5\")\nModel_Coupling_3JHC.compile(loss='mae', optimizer='Adam')\n\nmodel_check_3JHC = Model_Coupling_3JHC.predict(val_3JHC[0])\nlog_MAE_3JHC = metric_mae(val_3JHC[1], model_check_3JHC)\nval_3JHC[1] = val_3JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHC=\", log_MAE_3JHC)","c5ae15a0":"inter_3JHC = pd.DataFrame(test_3JHC['id']) \ninter_3JHC['type'] = test_3JHC['type']\ntest_3JHC_bis = test_3JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHC = Model_Coupling_3JHC.predict(test_3JHC_bis)\ninter_3JHC['pred_values'] = model_predict_3JHC","b6414713":"# Concatenate all predictions' model\npred = pd.concat([inter_1JHN, inter_1JHC, inter_2JHN, inter_3JHN, inter_2JHC, inter_2JHH, inter_3JHH, inter_3JHC])","da87f6f7":"pred = pred.sort_values(by=['id'], ascending=[True])\npred = pred['pred_values']","41a5c7f1":" # SUBMISSION:\npredictions = sample_submission_csv.copy()\npredictions['scalar_coupling_constant'] = pred\npredictions.to_csv('submission_MOLECULAR.csv', index=False)","1878cbd4":"# CHECKING THE MODEL WITH MAE METRICS\n\nprint(\"log MAE_1JHN=\", log_MAE_1JHN)\nprint(\"log MAE_2JHH=\", log_MAE_2JHH)\nprint(\"log MAE_2JHN=\", log_MAE_2JHN)\nprint(\"log MAE_3JHN=\", log_MAE_3JHN)\nprint(\"log MAE_1JHC=\", log_MAE_1JHC)\nprint(\"log MAE_2JHC=\", log_MAE_2JHC)\nprint(\"log MAE_3JHH=\", log_MAE_3JHH)\nprint(\"log MAE_3JHC=\", log_MAE_3JHC)","b1a843ae":"score = (log_MAE_1JHN+log_MAE_2JHH+log_MAE_2JHN+log_MAE_3JHN+log_MAE_1JHC+log_MAE_2JHC+log_MAE_3JHH+log_MAE_3JHC)\/8\nprint('SCORE:', score)","79b615c4":"#############################################################","f100bd77":"#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#train_csv_1JHC = scaler.fit_transform(train_1JHC[0])\n#train_csv_1JHC = pd.DataFrame(train_csv_1JHC)\n\n#val_csv_1JHC = scaler.fit_transform(val_1JHC[0])\n#val_csv_1JHC = pd.DataFrame(val_csv_1JHC)\n#train_csv_1JHC.columns = train_1JHC[0].columns\n#val_csv_1JHC.columns = val_1JHC[0].columns\n","84803edf":"def add_center(df):\n    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n\ndef add_distance_to_center(df):\n    df['d_c'] = (((df['x_c'] - df['x'])**np.float32(2) +\n                  (df['y_c'] - df['y'])**np.float32(2) +\n                  (df['z_c'] - df['z'])**np.float32(2))**np.float32(0.5))\n\ndef add_distance_between(df, suffix1, suffix2):\n    df[f'd_{suffix1}_{suffix2}'] = ((\n        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n    )**np.float32(0.5))","b0cce100":"2) 2JHH","4585e91b":" 7) 3JHH","4624a4c8":"5) 1JHC","0628c9fd":" 6) 2JHC","740b8897":"1) 1JHN","2dcffaf7":"4) 3JHN","a9e2e518":"3) 2JHN","b990c77a":"8) 3JHC"}}