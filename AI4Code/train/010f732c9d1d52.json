{"cell_type":{"b65d8330":"code","d6a755eb":"code","967f3e3d":"code","a26e770c":"code","22903674":"code","79059009":"code","e6420478":"code","015aaefa":"code","90da2b13":"code","d4fe5203":"code","bf5688bc":"code","00613082":"code","6669b26c":"code","faa55436":"markdown","0faeea89":"markdown","dde50015":"markdown","ba6ccc13":"markdown","dfd245ec":"markdown"},"source":{"b65d8330":"#Show some images\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nplt.figure(figsize=(18,7)) #Size for the plot\n\nphotos_daisy = '..\/input\/flowers-recognition\/flowers\/daisy' #Img to plot\nimg = os.listdir(photos_daisy) #Img list\n\nfor i, nameimg in enumerate(img[:18]):  #Loop for a total of 16 images\n    plt.subplot(3,6,i+1) #Rows 2, cols  8 and position for each img\n    img = mpimg.imread(photos_daisy + '\/' + nameimg) #Read the img\n    plt.imshow(img)  #Show img","d6a755eb":"#Argumentation with ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\n#Create the dataset generator\ndatagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    rotation_range = 30,\n    width_shift_range = 0.25,\n    height_shift_range = 0.25,\n    shear_range = 15,\n    zoom_range = [0.5, 1.5],\n    validation_split=0.2 #Take 20% of img for validation\n)\n\n#Generators for train and test\ndata_gen_train = datagen.flow_from_directory('..\/input\/flowers-recognition\/flowers', target_size=(224,224), #At the same time resize images to 224x224\n                                                     batch_size=32, shuffle=True, subset='training')                   #To training\ndata_gen_test = datagen.flow_from_directory('..\/input\/flowers-recognition\/flowers', target_size=(224,224),  #At the same time resize images to 224x224\n                                                     batch_size=32, shuffle=True, subset='validation')                 #To validation\n\n\nplt.figure(figsize=(15,5)) #Size for the plot\n#Print 10 img of the argumentation\nfor img, label in data_gen_train:\n    for i in range(10):\n        plt.subplot(2,5,i+1)  #Rows, cols and position\n        #plt.xticks([])  #Delete size of x\n        #plt.yticks([])  #Delete size of y\n        plt.imshow(img[i])  #Show img\n    break\nplt.show()","967f3e3d":"import tensorflow as tf\nimport tensorflow_hub as hub\n\n#Pre-trained model mobilenet_v2 for TensorFlow 2\n#For this module, the size of the input images is fixed to height x width = 224 x 224 pixels.\n#For more information visit thr url\nurl = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/4\"\nmobilenetv2 = hub.KerasLayer(url, input_shape=(224,224,3)) # Batch input shape.","a26e770c":"#Freez the model\nmobilenetv2.trainable = False","22903674":"#Change the last layer\nmodel = tf.keras.Sequential([\n    mobilenetv2,\n    tf.keras.layers.Dense(5, activation='softmax') #To 5 because we have 5 types of flowers\n])","79059009":"#Show the summary of the pre-trained model\nmodel.summary()","e6420478":"#Compile the model \nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","015aaefa":"#Train the model, with 15 epochs is enough \nepochs = 15\n\nrecord = model.fit(\n    data_gen_train, epochs=epochs, batch_size=32, #Data_gen_train for the train dataset\n    validation_data=data_gen_test                 #Validation_data for the validation dataset\n)","90da2b13":"#Train graph\nacc = record.history['accuracy']  #Get the values of accuracy\nval_acc = record.history['val_accuracy']  #Get the values of validation accuracy\n\nloss = record.history['loss']  #Get the values of loss\nval_loss = record.history['val_loss']  #Get the values of validation loss\n\n#Range of epoch for the graph\nrango_epochs = range(15) #Total of epoch is 15\n\n#Accuracy graph\nplt.figure(figsize=(8,8)) #Size for the plot of graphs\nplt.subplot(1,2,1)  #Rows, cols and position\nplt.plot(rango_epochs, acc, label='Training Accuracy')\nplt.plot(rango_epochs, val_acc, label='Accuracy Testing')\nplt.legend(loc='lower right')\nplt.title('Training and testing accuracy')\n\n#Loss graph\nplt.subplot(1,2,2) #Rows, cols and position\nplt.plot(rango_epochs, loss, label='Loss of training')\nplt.plot(rango_epochs, val_loss, label='Loss of testing')\nplt.legend(loc='upper right')\nplt.title('Loss of training and testing')\nplt.show()","d4fe5203":"#Classify an Image\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport cv2\n\nfolders = '..\/input\/flowers-recognition\/flowers'  #Folder of all flowers\nfloders_nm = sorted((os.listdir(folders)))  #Get the folders name and sort alphabetically\n\n#From the dataset\ndef classify_ds(img):\n    img = Image.open(img)  #Open the image\n    img = np.array(img).astype(float)\/255  #Make an array with the image\n\n    img = cv2.resize(img, (224,224))  #Resize the image to 224x224\n    prediction = model.predict(img.reshape(-1, 224, 224, 3))  #Make the prediction\n    return np.argmax(prediction[0], axis=-1)  #Return the result\n\n#Form internet\ndef classify_web(url):\n    response = requests.get(url) #Get an image from a url\n    img = Image.open(BytesIO(response.content)) #Open the image\n    img = np.array(img).astype(float)\/255  #Make an array with the image\n\n    img = cv2.resize(img, (224,224))  #Resize the image to 224x224\n    prediction = model.predict(img.reshape(-1, 224, 224, 3))  #Make the prediction\n    return np.argmax(prediction[0], axis=-1)  #Return the result","bf5688bc":"#Predicting an image from the dataset in a easy way\nimg = '..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg' #Daisy dataset Image \nimage = mpimg.imread('..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg') #Read an img to show\npred = classify_ds(img)  #Call the model\nprint(pred)  #Print the prediction with an image of deasy flower and it should be 0\nplt.figure(figsize=(15,5)) #Size for the plot of img\nplt.subplot(1,1,1) #Rows, cols and position\nplt.imshow(image) #Show img","00613082":"#Predicting an image from the dataset with the name\nimg = '..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg' #Dataset Image\nimage = mpimg.imread('..\/input\/flowers-recognition\/flowers\/daisy\/100080576_f52e8ee070_n.jpg') #Read an img to show\npred = classify_ds(img)  #Call the model\nprint(\"The flower is: \" + floders_nm[pred])  #Print the name of the folder regarding prediction\nplt.figure(figsize=(15,5)) #Size for the plot of img\nplt.subplot(1,1,1) #Rows, cols and position\nplt.imshow(image) #Show img","6669b26c":"#Test model with a non-dataset image\nurl = 'https:\/\/www.lovingly.com\/wp-content\/uploads\/2019\/09\/red-rose-on-black-background.jpg' #Should be 1\npred = classify_web (url)\nprint(\"The flower is: \" + floders_nm[pred])\nresponse = requests.get(url) #Get an image from a url\nimage = Image.open(BytesIO(response.content)) #Open the image\nplt.figure(figsize=(15,5)) #Size for the plot of img\nplt.subplot(1,1,1) #Rows, cols and position\nplt.imshow(image) #Show img","faa55436":"# **Now we have a train model to use**\n\nFor the first experiment we only made a simple prediction, but at once we made the functions to made other predictions","0faeea89":"# **Argumentation**\n\nHow do I get more data, if I don\u2019t have \u201cmore data\u201d?\n\nYou don\u2019t need to hunt for novel new images that can be added to your dataset. Why? Because, neural networks aren\u2019t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.\n\n![Tenis](https:\/\/nanonets.com\/blog\/content\/images\/2018\/11\/1_L07HTRw7zuHGT4oYEMlDig.jpeg)\n\nSo, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.\n\n![Numbers](https:\/\/nanonets.com\/blog\/content\/images\/2018\/11\/1_dJNlEc7yf93K4pjRJL55PA--1-.png)\n\nA convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).\n\nIn order words, to avoid overfitting problem, we need to artificially expand our dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations. Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n\nPlease read through the following article in case you need more information: [Click Here](https:\/\/nanonets.com\/blog\/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2\/)","dde50015":"# **Steal a model**\n\nWell we don't steal a model, we only usea a pre-trained model from tensorflow hub\n\nIn this code [mobilenet v2](https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/4) without feature vector is used and this is perfect because in a simple way don't have the last layer","ba6ccc13":"In the last metod we have a simple prediction and was a \"0\" because only return a numer by the sequence of the folders\n\n* 0 Daisy\n* 1 Dandelion\n* 2 Rose\n* 3 Sunflower\n* 4 Tulip\n\nNow in the next code we obtain the name of the champion instead of a numer\n","dfd245ec":"# **Test a image from the internet**\n\n**Now to test our model in a slightly more realistic environment we let's try with an image of internet**"}}