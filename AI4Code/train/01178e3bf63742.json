{"cell_type":{"5ee3c60a":"code","5531d3eb":"code","2e3b9d02":"code","34e97d69":"code","de098eeb":"code","06ca1831":"code","202a5d86":"code","cda18c3a":"code","68d8b532":"code","196ec445":"code","dc89990d":"code","54234235":"code","ddf561a3":"code","bfaafe19":"code","ebbfdaca":"code","a184977b":"code","9af5e7a5":"code","e8c61277":"code","312fcfe0":"code","70f51a32":"code","e79e4145":"code","8e720ad3":"code","d080d450":"code","d2c6c19c":"code","d30481ad":"code","93a7fddf":"code","c80c5310":"code","f0376c19":"code","efdcd862":"code","9c10c9cf":"code","20db0604":"code","e5c070dd":"code","9aef4309":"code","70ce30a9":"code","0be6e5cb":"code","6f3aecdd":"code","5412b962":"code","e8cb9989":"code","9a9b4dd3":"code","53691a97":"code","ded9b711":"code","30be1097":"code","05314b01":"code","470b5183":"code","38daa056":"code","e9788d04":"code","62356c8a":"code","03bee98e":"code","078267e2":"code","34b4dc5b":"code","eafc41b6":"code","f8626f57":"code","ed38e649":"code","5e785c9a":"code","d5f1eca1":"code","5bf14cf8":"code","2c1a67c1":"code","d15643d5":"code","93641489":"code","4d28cbe0":"code","7c8b4081":"markdown","02490747":"markdown","72a26413":"markdown"},"source":{"5ee3c60a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5531d3eb":"# Loading Essential libraries \nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\n%config Completer.use_jedi = False # this to force autocompletion ","2e3b9d02":"data_path  = '..\/input\/eng-hindi-translator\/hin.txt'\nwith open(data_path, 'r', encoding = 'utf-8') as f:\n    lines = f.read(1000)\nlines    \n    \n   ","34e97d69":"lines.strip().split('\\n')[0].split('\\t')[:2]","de098eeb":"data_path  = '..\/input\/eng-hindi-translator\/hin.txt'\nwith open(data_path, 'r', encoding = 'utf-8') as f:\n    lines = f.read()\nlines    \n    \n   ","06ca1831":"# now lets split the lines \ndef to_lines(text):\n    sents = text.strip().split('\\n')\n    sents = [text[0:2] for text in [i.split('\\t') for i in sents]]\n    return sents","202a5d86":"df = to_lines(lines)","cda18c3a":"df","68d8b532":"df = pd.DataFrame(df, columns = [ 'english', 'hindi'])","196ec445":"df = df.sample(frac = 1).reset_index(drop = True)\ndf","dc89990d":"# now lets do some preprocessing \n# lowercasing all the words\ndf['english'] = df.english.apply(lambda x:x.lower())\ndf['hindi'] = df.hindi.apply(lambda x:x.lower())\n","54234235":"!pip install text_hammer\nimport text_hammer as th","ddf561a3":"df['english'] = df.english.progress_apply(lambda x:th.cont_exp(x))","bfaafe19":"df.head()","ebbfdaca":"## Remove quotes \nimport string\nexclude = set(string.punctuation)\nimport re\ndf['english'] = df.english.apply(lambda x:re.sub(\"'\",'',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub(\"'\",'',x))\n# removing punctuation\ndf['english'] = df.english.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n                                 \ndf['hindi'] = df.hindi.apply(lambda x: ''.join(ch for ch in x if ch not in exclude ))\n# remove numericals number \ndf['english'] = df.english.apply(lambda x:re.sub('[0-9]+','',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub('[0-9]+','',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub('[\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]','',x))\n# Remove extra spaces\ndf['english']=df['english'].apply(lambda x: x.strip())\ndf['hindi']=df['hindi'].apply(lambda x: x.strip())\ndf['english']=df['english'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['hindi']=df['hindi'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['hindi'] = df.hindi.apply(lambda x: re.sub('[A-Za-z]+','',x))","a184977b":"df.head()","9af5e7a5":"df['hindi'] = 'START_ '+ df['hindi'] + ' _END'","e8c61277":"df['hindi_len'] = df.hindi.apply(lambda x:len(x.split()))\ndf['english_len'] = df.english.apply(lambda x:len(x.split()))","312fcfe0":"df.head()","70f51a32":"cleaned_df = df[(df.hindi_len <= 20)*(df.english_len <= 20)]","e79e4145":"cleaned_df.to_csv('cleaned.csv')","8e720ad3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\ny = [len(x.split()) for x in cleaned_df.english]\nplt.plot(y)\nplt.title('English words length')\nprint(np.max(y))","d080d450":"y = [len(x.split()) for x in cleaned_df.hindi]\nplt.plot(y)\nplt.title('Hindi words length')\nprint(np.max(y))","d2c6c19c":"hindi_max_len = 20 \nenglish_max_len = 18","d30481ad":"all_eng_words=set()\nfor eng in cleaned_df.english:\n    for word in eng.split():\n        if word not in all_eng_words:\n            all_eng_words.add(word)\n\nall_hindi_words=set()\nfor hin in cleaned_df.hindi:\n    for word in hin.split():\n        if word not in all_hindi_words:\n            all_hindi_words.add(word)\ninput_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_hindi_words))\ninput_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\nreverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\nreverse_target_char_index = dict((i, word) for word, i in target_token_index.items())","93a7fddf":"eng_vocab_size = len(all_eng_words) + 1\nhin_vocab_size = len(all_hindi_words) + 1\nprint(eng_vocab_size, hin_vocab_size)","c80c5310":"from sklearn.model_selection import train_test_split\nX, y = cleaned_df['english'], cleaned_df['hindi']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)\nX_train.shape, X_test.shape","f0376c19":"def generate_batch(X = X_train, y = y_train, batch_size = 128):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = np.zeros((batch_size, english_max_len),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, hindi_max_len),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, hindi_max_len,hin_vocab_size ),dtype='float32')\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text.split()):\n                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n                for t, word in enumerate(target_text.split()):\n                    if t<len(target_text.split())-1:\n                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n                    if t>0:\n                        # decoder target sequence (one hot encoded)\n                        # does not include the START_ token\n                        # Offset by one timestep\n                        decoder_target_data[i, t - 1,target_token_index[word]] = 1.\n            yield([encoder_input_data, decoder_input_data], decoder_target_data)","efdcd862":"latent_dim = 300 # this is the embedding dimension\nunits = 300","9c10c9cf":"from keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\n\n# Encoder using functional api of keras\nencoder_inputs = Input(shape = (None,))\nenc_emb = Embedding(eng_vocab_size, latent_dim ,mask_zero = True )(encoder_inputs)\nencoder_lstm = LSTM(units, return_state = True)\nencoder_outputs , state_h, state_c = encoder_lstm(enc_emb)\nencoder_states = [state_h, state_c]\n","20db0604":"### Setting up the decoder using encoder_states as initial states\ndecoder_inputs = Input(shape = (None,))\ndec_emb_layer = Embedding(hin_vocab_size, latent_dim, mask_zero  = True)\ndec_emb = dec_emb_layer(decoder_inputs)\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(units, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(hin_vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","e5c070dd":"model.summary()","9aef4309":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy')","70ce30a9":"train_samples = len(X_train)\nval_samples = len(X_test)\nbatch_size = 128\nepochs = 100","0be6e5cb":"model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n                    steps_per_epoch = train_samples\/\/batch_size,\n                    epochs=epochs,\n                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n                    validation_steps = val_samples\/\/batch_size)\n\n","6f3aecdd":"model.save_weights('nmt_weights.h5')","5412b962":"# Encode the input sequence to get the \"thought vectors\"\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)\n","e8cb9989":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index['START_']\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += ' '+sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '_END' or\n           len(decoded_sentence) > 50):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","9a9b4dd3":"train_gen = generate_batch(X_train, y_train, batch_size = 1)\nk=-1\n","53691a97":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","ded9b711":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","30be1097":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","05314b01":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","470b5183":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","38daa056":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","e9788d04":"from keras.preprocessing.sequence import pad_sequences\ndef encode_sequence(tokenizer, length, data):\n    seq = tokenizer.texts_to_sequences(data)\n    seq = pad_sequences(seq, maxlen = length, padding = 'post')\n    return seq","62356c8a":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.2, random_state = 42)","03bee98e":"print(train.shape, test.shape)","078267e2":"# preparing the train and test data\nX_train = encode_sequence(eng_tokenizer, eng_len, train.english)\nX_test = encode_sequence(eng_tokenizer, eng_len, test.english)\ny_train = encode_sequence(hin_tokenizer, hin_len, train.hindi)\ny_test = encode_sequence(hin_tokenizer, hin_len, test.hindi)\n\n","34b4dc5b":"from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\nimport tensorflow\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\nfrom tensorflow.keras.layers import Dropout,RepeatVector\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, GRU, Dense, Embedding\nfrom keras import optimizers","eafc41b6":"# Building model architecture\ndef define_model(in_vocab_size, out_vocab_size, in_timesteps_len, out_timesteps_len, units):\n    model = Sequential()\n    model.add(Embedding(in_vocab_size, units, input_length = in_timesteps_len, mask_zero = True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps_len))\n    model.add(LSTM(units, return_sequences = True))\n    model.add(Dense(out_vocab_size, activation = 'softmax'))\n    return(model)","f8626f57":"model = define_model(eng_vocab_size, hin_vocab_size, eng_len, hin_len, 100)","ed38e649":"rms = optimizers.RMSprop(lr = 0.001)\nmodel.compile(optimizer = rms, loss = 'sparse_categorical_crossentropy')","5e785c9a":"model.summary()","d5f1eca1":"history = model.fit(X_train, y_train, epochs = 50, batch_size = 512, validation_split = 0.1)","5bf14cf8":"preds = model.predict_classes(X_test)\npreds","2c1a67c1":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None    ","d15643d5":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t  = get_word(i[j], hin_tokenizer)\n        if j> 0:\n            if (t == get_word(i[j-1], hin_tokenizer)) or (t == None):\n                temp.append('')\n            else:\n                temp.append(t)\n        else:\n            if (t == None):\n                temp.append('')\n            else:\n                temp.append(t)\n    preds_text.append(' '.join(temp))","93641489":"pred_df = pd.DataFrame({'actual': test[:,0], 'predicted':preds_text})\npred_df","4d28cbe0":"preds_text","7c8b4081":"****so we need now a data consist of encoder input, decoder input, and decoder output since our dataset can be big enough thats why we will take use of generator ****","02490747":"**From here the max_length for english and hindi will be decided **","72a26413":"## Encoder -Decoder architecture"}}