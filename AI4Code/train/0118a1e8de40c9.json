{"cell_type":{"cd9a89ba":"code","12cc125f":"code","281a8e7f":"code","2b3e366b":"code","e7d78fdf":"code","66a9febe":"code","cf272e67":"code","5ec882f6":"code","baef7faf":"code","2dfc06e8":"code","5dbd1504":"code","d208ebb3":"code","998cde41":"code","51bab886":"code","8a224eaa":"code","00c37683":"code","4f31f820":"code","101d1e54":"code","e43efa87":"code","ecb969c3":"code","8d8d730b":"code","55984b5a":"code","7106e433":"code","f307c34f":"markdown","d02d1ee8":"markdown","eefd16ab":"markdown","4cbc8203":"markdown","8a57da1f":"markdown","1da608b9":"markdown","925e772a":"markdown","19b44a06":"markdown"},"source":{"cd9a89ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# loading important modules\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\nfrom imblearn.over_sampling import SMOTE\nimport itertools\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","12cc125f":"# Reading the dataset\nsales_data = pd.read_csv(\"..\/input\/SalesKaggle3.csv\")","281a8e7f":"# Gist of the dataset\nsales_data.head()","2b3e366b":"#Statistical description of the dataset\nsales_data.describe()","e7d78fdf":"# Includes categorical variable \nsales_data.describe(include='all')","66a9febe":"# Basic questions about the dataset\n\n# 1. Number of enteries \nprint(sales_data.shape)\n# We have 198917 rows and 14 columns \n\n# 2. Total number of products & unique values of the columns \nprint(\"*****************\")\nprint(sales_data.nunique())\n\n# 3. Count of the historical and active state \nprint(\"*****************\")\nprint(sales_data[sales_data['File_Type'] == 'Historical']['SKU_number'].count())\nprint(sales_data[sales_data['File_Type'] == 'Active']['SKU_number'].count())\n\n# 3.1 Split the dataset into two parts based on the file_type \nsales_data_hist = sales_data[sales_data['File_Type'] == 'Historical']\nsales_data_act = sales_data[sales_data['File_Type'] == 'Active']","cf272e67":"sales_data['MarketingType'].value_counts().plot.bar(title=\"Freq dist of Marketing Type\")","5ec882f6":"sales_data['File_Type'].value_counts().plot.bar(title=\"Freq dist of File Type\")","baef7faf":"sales_data['New_Release_Flag'].value_counts().plot.bar(title=\"Freq dist of New_Release_Flag\")","2dfc06e8":"sales_data_act['MarketingType'].value_counts().plot.bar(title=\"Freq dist of MarketingFile Type - active states\")","5dbd1504":"sales_data_hist['MarketingType'].value_counts().plot.bar(title=\"Freq dist of MarketingFile Type - hist states\")","d208ebb3":"col_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(16,12))\n\nfor i, col_val in enumerate(col_names):\n        \n    sns.distplot(sales_data_hist[col_val], hist=True, ax=ax[i])\n    ax[i].set_title('Freq dist '+col_val, fontsize=10)\n    ax[i].set_xlabel(col_val, fontsize=8)\n    ax[i].set_ylabel('Count', fontsize=8)\n    \nplt.show()","998cde41":"col_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(8,40))\n\nfor i, col_val in enumerate(col_names):\n        \n    sns.boxplot(y=sales_data_hist[col_val], ax=ax[i])\n    ax[i].set_title('Box plot - '+col_val, fontsize=10)\n    ax[i].set_xlabel(col_val, fontsize=8)\n    \nplt.show()","51bab886":"# Percentile based outlier removal \ndef percentile_based_outlier(data, threshold=95):\n    diff = (100 - threshold) \/ 2.0\n    minval, maxval = np.percentile(data, [diff, 100 - diff])\n    return (data < minval) | (data > maxval)\n\ncol_names = ['StrengthFactor','PriceReg', 'ReleaseYear', 'ItemCount', 'LowUserPrice', 'LowNetPrice']\n\nfig, ax = plt.subplots(len(col_names), figsize=(8,40))\n\nfor i, col_val in enumerate(col_names):\n    x = sales_data_hist[col_val][:1000]\n    sns.distplot(x, ax=ax[i], rug=True, hist=False)\n    outliers = x[percentile_based_outlier(x)]\n    ax[i].plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)\n\n    ax[i].set_title('Outlier detection - '+col_val, fontsize=10)\n    ax[i].set_xlabel(col_val, fontsize=8)\n    \nplt.show()","8a224eaa":"# Converting maarketing type to categorical variable \nsales_data['MarketingType'] = sales_data['MarketingType'].astype('category')\nsales_data['MarketingType'] = sales_data['MarketingType'].cat.codes\n\n# Splitting the historical and active state\nsales_data_hist = sales_data[sales_data['File_Type'] == 'Historical']\nsales_data_act = sales_data[sales_data['File_Type'] == 'Active']","00c37683":"# Columns to remove \nremove_col_val = ['Order', 'File_Type', 'SKU_number', 'SoldCount', 'ReleaseNumber', 'SoldFlag']\n\ny = sales_data_hist['SoldFlag']\n\nsales_data_hist = sales_data_hist.drop(remove_col_val, axis=1)\nsales_data_act = sales_data_act.drop(remove_col_val, axis=1)\n\n# create training and testing vars\ntraining_features, testing_features, training_target, testing_target = train_test_split(sales_data_hist, y, test_size=0.2)\nprint(training_features.shape, training_target.shape)\nprint(testing_features.shape, testing_target.shape)","4f31f820":"print(\"Class 0 numbers: \" , len(training_target[training_target==0.0]))\nprint(\"Class 1 numbers: \" , len(training_target[training_target==1.0]))","101d1e54":"x_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n                                                  test_size = .1,\n                                                  random_state=12)","e43efa87":"# Balancing the classes using SMOTE\nsm = SMOTE(random_state=12, ratio = 1.0)\nx_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n\nprint(\"Class 0 numbers: \" , len(y_train_res[y_train_res==0.0]))\nprint(\"Class 1 numbers: \" , len(y_train_res[y_train_res==1.0]))","ecb969c3":"clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)\nclf_rf.fit(x_train_res, y_train_res)","8d8d730b":"print('Validation Results')\nprint(clf_rf.score(x_val, y_val))\nprint(recall_score(y_val, clf_rf.predict(x_val)))\n\npred = clf_rf.predict(testing_features)\n\nprint('\\nTest Results')\nprint(clf_rf.score(testing_features, testing_target))\nprint(recall_score(testing_target, pred))\n\nprint('\\nROC AUC score')\nprint(roc_auc_score(testing_target, pred))","55984b5a":"class_names = ['Not Sold', 'Sold']\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testing_target, pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","7106e433":"# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nn_classes = 2\ny_score = clf_rf.predict_proba(testing_features)\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testing_target.ravel(), y_score[:,1].ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\nplt.figure()\nlw = 2\n\nplt.plot(fpr['micro'], tpr['micro'], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc['micro'])\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","f307c34f":"Inventory Management (Evaluation & Planning)\n\nAuthor: Govindji Mishra                                                                                                       Created: Feb 04, 2019 \n                                                                                                                                            \nProblem Statement:\nContext: A retail firm has many products in their inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year\n\nObjective: The sales and growth team of the retail firm wants to determine which products from their inventory should they retain to sell and the ones to discard\n\nData: The data given contains both historical sales data AND active inventory\n\nGoal: We have a to building a binary classifier which gives us a list of product ID which need to retained in the inventory or list of products that need to be removed","d02d1ee8":"Confusion Matrix","eefd16ab":"Numeric Variable\nPlots with a kernel density estimate and histogram with bin size determined automatically","4cbc8203":"Note: We will be using the historical dataset for the analysis and training the model\nUnivariate distribution plots\nThis section shows a frequency histogram for the selected variable along with the density and normal curves for the data\n\nThe box plot shows the basic statistics of the data like median, 25th and 75th quantiles and the outliers.\n\nCategorical Variable\nShows the frequency distribution of the difference factors","8a57da1f":"ROC","1da608b9":"Univariate outlier detection\nTo analysis the outliers in the numeric features of the dataset","925e772a":"Univariate Outlier treatment\nMany algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the results and make results less reliable, that's why we have to recognize all the outliers and treat them.","19b44a06":"Predictive Modelling\nThe classification module predicts the SKU which needs to kept in the inventory (Active state)"}}