{"cell_type":{"59c061b2":"code","ff7dc3a8":"code","3b6aa8e7":"code","13106e1e":"code","50723c96":"code","328a197d":"code","1d12c2e5":"code","63c7268b":"code","37ee7198":"code","b70ac32c":"code","c7ed495e":"code","ed91ff84":"code","9e180d32":"code","be4ae612":"code","0bde874f":"code","36b3b194":"code","737a9075":"code","30108540":"code","6c8fd743":"code","2cf4d97f":"code","824f742d":"code","9f34a02b":"code","d51c0654":"code","6a669cca":"code","bbb8723e":"code","9fe224b2":"code","40042019":"code","ebb4f232":"code","c6c9779b":"code","882307b6":"code","09bddb3e":"code","84cd1be6":"markdown","b7ce9ec6":"markdown","d35b591b":"markdown","79895a71":"markdown","dd507547":"markdown","bb417e56":"markdown","b2d91c34":"markdown","943aff08":"markdown"},"source":{"59c061b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ff7dc3a8":"df_submission=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ndf_sales_train=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')","3b6aa8e7":"print('submission ' +str(len(df_submission)))\nprint('sales train ' +str(len(df_sales_train)))\nprint('tests ' +str(len(df_test)))","13106e1e":"def get_basic_df_info(df):\n    print(\"------ top 3 records ----- \")\n    print (df.head(3));\n    print(\"------- data information ----\")\n    print(df.info())\n    print(\"--------- Describe ----------\")\n    print(df.describe())\n    print (\"-------- Columns values ---------\")\n    print(df.columns)\n    print (\"------- data types values -------\")\n    print(df.dtypes)\n    print(\"-------- Missing values --------\")\n    print(df.isnull().sum())\n    print(\"-------- Nan values ---------\")\n    print(df.isna().sum())\n    print(\"-------- Data shape values -------\")\n    print(df.shape)","50723c96":"get_basic_df_info(df_sales_train) ","328a197d":"\ndf_sales_train['date'] = pd.to_datetime(df_sales_train['date'])\ndf_sales_train.head(3)\n#print(df_sales_train.date.value_counts().head(3))\n#get train dates sorted\n#train_dates = df_sales_train.date.value_counts()\n#train_dates = train_dates.sort_index()\n#print(train_dates.head(10))","1d12c2e5":"import matplotlib as plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n### get saisonality per month\nmonth = df_sales_train.date.dt.month\nyear = df_sales_train.date.dt.year\ndates = df_sales_train.date\nitem  = df_sales_train.item_id\nshop = df_sales_train.shop_id\nsales = df_sales_train.item_cnt_day *  df_sales_train.item_price\n\ndel df_sales_train\nmonthly_data = pd.concat([year,month,dates,item,shop,sales], axis=1)\nmonthly_data.columns = ['year','month','dates','item','shop','sales']\nmonthly_data.head(3)","63c7268b":"#Plot data\ntrend = monthly_data.groupby(monthly_data.dates)['sales'].mean()\ntrend.plot(kind='line')\nplt.show()","37ee7198":"#plotting months\n#understand saisonality - are they trends for sales during months ?\nm = monthly_data.groupby(monthly_data.month)['sales'].mean()\nm.plot(kind='line')\nplt.show()","b70ac32c":"#reduce the data\ndef reduce_mem_usage(train_data):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage. \n    \"\"\"\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","c7ed495e":"from statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller","ed91ff84":"data = monthly_data[[\"dates\", \"item\",\"sales\"]]\ndf_train = data.set_index(['dates'])\ndf_train.sort_index(inplace=True)\ndf_train.index.name = 'datetimeindex'\ndf_train.head(3)","9e180d32":"#plot df_train \ndf_train.plot(figsize=(19, 4))\nplt.show()","be4ae612":"#reduce the train data\ndf_train = reduce_mem_usage(df_train)","0bde874f":"#test if data is stationary or not\n#H0 It is not stationary\n#H1 it is stationary\n\ndef test_fuhler(train_sales):\n    ad_fuller_result = adfuller(train_sales)\n    print(f'ADF Statistic: {ad_fuller_result[0]}')\n    print(f'p-value: {ad_fuller_result[1]}')\n    print('ADF Statistic: %f' % ad_fuller_result[0])\n    if ad_fuller_result[1] <= 0.05:\n        print('This a strong evidence against the null hypothesis(H0), reject H0 - the data is stationary')\n    else:\n        print('Wee evidence against the null hypothesis(H0) - the data is not stationary')","36b3b194":"#issue here - should investigate!\n#test_fuhler(df_train['sales'])","737a9075":"df_train['sales'].shift(1)","30108540":"### differiantion\n#assume data is not stationary - data is not seasonal \ndf_train['sales diff'] = df_train['sales'] - df_train['sales'].shift(1)\n#if seasonal then you should shift by 12 (you seen pattern during months)\ndf_train['sales saison'] = df_train['sales'] - df_train['sales'].shift(12)","6c8fd743":"df_train.head(15)","2cf4d97f":"#issue here - should investigate!\n#test_fuhler(df_train['sales'])","824f742d":"df_train['sales diff'].plot()","9f34a02b":"df_train['sales saison'].plot()","d51c0654":"#data merge\n#run only if necessary\n''''_item_cat=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')  \ndf_item=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ndf_shop=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ndf_item_cat_all = df_item.merge(df_item_cat, left_on='item_category_id', right_on='item_category_id')\ndf_train_item = df_sales_train.merge(df_item_cat_all, left_on='item_id', right_on='item_id')\ndf_sales_train = df_train_item.merge(df_shop, left_on='shop_id', right_on='shop_id')\n\n#del df_sales_train, df_item_cat_all, df_train_item\ndef del_not_used_df(*args):\n    for arg in args:\n        del arg\n        \ndef set_test_df (df):\n    get_shop =  df['shop_id']==1\n    return df[get_shop]\n\n#print(len(df))\n#df.head(10)\n\ndel df_item_cat_all\ndel df_train_item\n''''''","6a669cca":"ds = df_sales_train.pivot_table(index = ['shop_id', 'item_id'], values = ['item_cnt_day'], columns =['date_block_num'], fill_value = 0, aggfunc='sum')\nds.reset_index(inplace=True)\n#merge data set with shop and item\nds = pd.merge(df_sales_train, ds, on = ['item_id', 'shop_id'], how = 'left')\nds.head(3)","bbb8723e":"#plot head plot - head graph\nimport seaborn as sns\nmonthly_data_h = monthly_data.groupby(['year','month','dates']).size()\nmonthly_data_h = monthly_data_h.reset_index()\nmonthly_data_h = monthly_data_h.rename(columns={0 :'size'})\nmonthly_data_h = monthly_data_h.groupby(['year','month']).mean()['size']\nmonthly_data_h = monthly_data_h.unstack(level=1)\nprint(monthly_data_h)\n\nsns.heatmap(monthly_data_h)\nplt.show()","9fe224b2":"#get information by shops \/ year \/ months\nmonthly_shop_sales = monthly_data.copy()\nmonthly_shop_sales = monthly_shop_sales.reset_index()\nmonthly_shop_sales = monthly_shop_sales.groupby(['shop','month']).mean()['sales']\nprint(monthly_shop_sales.head(10))\nmonthly_shop_sales = monthly_shop_sales.unstack(level=0)\nprint(monthly_shop_sales.head(3))\nsns.heatmap(monthly_shop_sales)\nplt.show()","40042019":"#check the trend on quarter (jan-feb-mar), (apr-mai-jun) - (jul-aug-sep) - (oct-nov-dec)\n# data re-sampled based on each month\nmonth_sales = monthly_data.resample('MS', on='dates').sales.sum()\nquarter_sales = monthly_data.resample('Q', on='dates').sales.sum()\n\n\n# aggregating multiple fields for each motnh\nmonthly_data.resample('Q', on='dates').agg({'sales':'sum', 'item':'count','shop':'nunique'})","ebb4f232":"# Grouping data based on month and store type\nmonthly_data.groupby([pd.Grouper(key='dates', freq='M'), 'shop']).sales.sum().head(15)\n\n# grouping data and named aggregation on item, item_count, and sales\nmonthly_data.groupby([pd.Grouper(key='dates', freq='M'), 'item']).agg(unique_items=('item', 'nunique'),\n         total_quantity=('item','count'),\n         total_amount=('sales','sum'))","c6c9779b":"#let's normalize by total amount per month for each item and shop\nmonth_sale_pct = monthly_data.groupby(monthly_data.dates).apply(lambda x: x \/x.sum())\nprint(month_sale_pct)","882307b6":"#found min max of each shop - get month count from min date to max date\nmonthly_data.groupby(['shop']).agg({'dates': [np.min,np.max]})","09bddb3e":"#run only if necessary\n'''\ndf_train_array = np.column_stack((df_train['item'].values,df_train['sales'].values))\ndf_train_array\ndel df_train_array\n'''","84cd1be6":"# Stationary and differencing\n\nA **stationary** time series is one whose properties do not depend on the time at which the series is observed.Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary \u2014 it does not matter when you observe it, it should look much the same at any point in time.\n\nSome cases can be confusing \u2014 a time series with cyclic behaviour (but with no trend or seasonality) is stationary. This is because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.\n\nIn general, **a stationary time series will have no predictable patterns in the long-term**. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance. So, stationary time series is one whose mean and variance is constant over time.\n\nWhen we have determined that we have stationarity, you can model it using the ARIMA (AutoRegressive Moving Average). For stationary data you can approximated with stationary ARIMA mode (SARIMA).","b7ce9ec6":"# Import all requires libraries for data analysis\nIn this step, we will import libraries and data to be analysed","d35b591b":"Merge data to get a complete dataset with all data","79895a71":"### ","dd507547":"Get the length of all dataframes","bb417e56":"# Data investigation - other","b2d91c34":"Explaining the model **SARIMA** is used for non-stationary series, that is, where the data do not fluctuate around the same mean, variance and co-variance. This model can identify trend and seasonality, which makes it so important. \n\nThe SARIMA consists of other forecasting models: \n* **AR**: Auto regressive model (can be a simple, multiple or non-linear regression) \n* **MA**: Moving averages model. The moving average models can use weighting factors, where the observations are weighted by a trim factor (for the oldest data in the series) and with a higher weight for the most recent observations.","943aff08":"get data into dataframe"}}