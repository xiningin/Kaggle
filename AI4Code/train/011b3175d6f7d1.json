{"cell_type":{"872bb277":"code","b284eda7":"code","94b9aab9":"code","9f80a711":"code","ad64cf04":"code","46e5efba":"code","d2697eb2":"code","66e9a814":"code","7277ac71":"code","81b95e85":"code","030bb260":"code","f22580fd":"code","53b66aba":"code","e6cf93da":"code","550cb425":"code","dbc820f0":"code","281b6d69":"code","6a134b94":"code","cb49e25e":"code","006a3a21":"code","dea094ca":"code","546c2da4":"code","04d47416":"code","e8e1b47f":"code","92cc383b":"code","7f7fc326":"code","c5b55825":"code","a99448a1":"code","feb38f2a":"code","d46458df":"code","07bf35d9":"code","4d0eac52":"code","b869356a":"code","023f484a":"code","4fd0df3d":"code","b8c99031":"code","844bef26":"code","ee81f988":"code","52d6f2b1":"code","c96f8c87":"code","68c40d27":"code","916dcb41":"code","1943eb75":"code","6f3f907b":"code","84774806":"code","76ffee29":"code","d10f9837":"code","267b7ea8":"code","582eb400":"code","0a9e9802":"code","615eb86f":"code","9ec8e047":"code","492d1f95":"code","22f05a7d":"code","8c0149c0":"code","126bc5a5":"code","3dbfdc43":"code","8030c70c":"code","550364bd":"code","a6cc89c8":"code","87dedf73":"code","6284653a":"code","b243b9e6":"code","53c6deb2":"code","e5e52972":"code","24353fe6":"code","5e42e990":"code","77ca0053":"code","bed70da4":"code","009b9b34":"code","26e922c0":"code","7f943c3f":"code","d038aa04":"code","12682002":"code","9e75e5f7":"code","c056ce36":"code","5b246a78":"code","a0e3fd14":"code","259bab70":"code","91a49fc0":"code","9a50d1c5":"code","3ea12b29":"code","fece65e5":"code","4483f5ae":"code","37333b73":"code","843e8a2d":"code","fad253b6":"code","81def12f":"code","33aab27a":"code","fd868dbc":"code","5df16cc7":"code","39f59796":"code","e77d653c":"code","ab447693":"code","7440d37d":"code","afec4ac0":"code","5ae6a2a5":"markdown","0d9a74b1":"markdown","9f8484fb":"markdown","e4b8e87c":"markdown","90d97ea1":"markdown","c5639143":"markdown"},"source":{"872bb277":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b284eda7":"import matplotlib.pyplot as plt\nimport seaborn as sln\n%matplotlib inline","94b9aab9":"data=pd.read_csv('\/kaggle\/input\/pump-sensor-data\/sensor.csv')","9f80a711":"data.head()","ad64cf04":"pd.set_option('display.max.columns',999)","46e5efba":"data.head()","d2697eb2":"data['machine_status'].value_counts()","66e9a814":"## confused about what does recovering mean? is broken means completely broken?","7277ac71":"## lets check what possible states can be reached\n### Normal,recovering,broken\n# check the idx before the machine broke\ndata[data['machine_status']=='BROKEN']","81b95e85":"# each observation is taken a 1 min of gap\n# first failure occured at 17155 index after approx 13 days\n#lets check status of 17154\ndata.iloc[17154]['machine_status']","030bb260":"data.iloc[17156]['machine_status']","f22580fd":"data.iloc[17154]","53b66aba":"## this is weird everytime time machine breaks from normal within one min????\n## first recovering state \ndata[data['machine_status']=='RECOVERING']","e6cf93da":"# okay and then machine starts recovering\n## so what they do to to change it state?\n","550cb425":"## it took 17155\/60 hours to break\n17155\/60","dbc820f0":"# days\n286\/24\n","281b6d69":"# second broken\n24510\/60","6a134b94":"408.5\/24","cb49e25e":"### lets average it for days \n## lets pick up three most important sensor from artgor kernel\n## sensor_00,sensor_04,sensor_01\n## avg about 1440 rows","006a3a21":"idx=0\nidx2=1440\nmean_sensor0=[]\nmean_sensor4=[]\nmean_sensor1=[]\nmean_sensor47=[]\nfor i in range(0,20):\n    mean_sensor0.append(data['sensor_00'].iloc[idx:idx2].mean(axis=0))\n    mean_sensor4.append(data['sensor_04'].iloc[idx:idx2].mean(axis=0))\n    mean_sensor1.append(data['sensor_01'].iloc[idx:idx2].mean(axis=0))\n    mean_sensor47.append(data['sensor_47'].iloc[idx:idx2].mean(axis=0))\n    idx+=1440\n    idx2+=1440\n    ","dea094ca":"import plotly.express as px","546c2da4":"len(range(1,21))","04d47416":"avg_days=pd.DataFrame({'day':range(1,21),'sensor_00':mean_sensor0,'sensor__01':mean_sensor1,'sensor__04':mean_sensor4,'sensor__47':mean_sensor47})","e8e1b47f":"fig = px.line(avg_days, x=\"day\", y=\"sensor_00\", \n        line_shape=\"spline\", render_mode=\"svg\")\n\nfig.show()","92cc383b":"fig=px.line(data[data['machine_status']=='BROKEN'],x='timestamp',y='sensor_00',line_shape='spline',render_mode='svg')\nfig.show()","7f7fc326":"fig = px.line(avg_days, x=\"day\", y=\"sensor__01\", \n        line_shape=\"spline\", render_mode=\"svg\")\n\nfig.show()","c5b55825":"fig = px.line(avg_days, x=\"day\", y=\"sensor__04\", \n        line_shape=\"spline\", render_mode=\"svg\")\n\nfig.show()","a99448a1":"data.head()","feb38f2a":"## high correlation between sensor__00 to sensor__12\n## sensor_12 to sensor_36","d46458df":"## sesnor 15 to sensor__15 to sensor 36 seems uncorrelated lets check?","07bf35d9":"data_copy=data.copy(True)\ndata_copy=data_copy.drop(['Unnamed: 0','timestamp','sensor_15'],axis=1)","4d0eac52":"def correlation(dataset, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] <= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n    return col_corr","b869356a":"no_corr_cols=correlation(data_copy,0.05)","023f484a":"no_corr_cols","4fd0df3d":"data_copy=data_copy.drop('machine_status',axis=1)","b8c99031":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(data_copy, 20))","844bef26":"corr = data_copy.corr()\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps\ncorr.style.background_gradient(cmap='coolwarm')","ee81f988":"## sensor_47,sensor_48","52d6f2b1":"fig = px.line(avg_days, x=\"day\", y=\"sensor__47\", \n        line_shape=\"spline\", render_mode=\"svg\")\n\nfig.show()","c96f8c87":"for col in data_copy.columns:\n    data_copy[col] = data_copy[col].fillna(data_copy[col].mean())","68c40d27":"data[\"machine_status\"]=data['machine_status'].astype('category')","916dcb41":"data['status']=data['machine_status'].cat.codes","1943eb75":"# 0 IS BROKEN,1 IS NORMAL 2 IS RECOVERING","6f3f907b":"data[\"machine_status\"].cat.categories","84774806":"data[\"machine_status\"].cat.codes.unique()","76ffee29":"plt.figure(figsize=(10,10))\nplt.plot(data['status'],label='state')","d10f9837":"data_copy['status']=data['status']","267b7ea8":"data_copy.plot(figsize=(15,120), subplots=True)","582eb400":"def dist(col):\n    try:\n        \n        sln.distplot(data_copy[data_copy['status']==1][col],label=col+'_normal')\n        sln.distplot(data_copy[data_copy['status']==0][col],label=col+'_broken')\n        sln.distplot(data_copy[data_copy['status']==2][col],label=col+'_recovering')\n        plt.legend()\n        plt.show()\n    except:\n        pass","0a9e9802":"for col in data_copy.columns:dist(col)","615eb86f":"! pip install factor-analyzer==0.3.2","9ec8e047":"from factor_analyzer import FactorAnalyzer","492d1f95":"# need to perform Bartlett's test to know if factors actually are present\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nchi_square_value,p_value=calculate_bartlett_sphericity(data_copy)\nprint(chi_square_value, p_value)","22f05a7d":"## KMO test\nfrom factor_analyzer.factor_analyzer import calculate_kmo\nkmo_all,kmo_model=calculate_kmo(data_copy)\nprint(kmo_model)","8c0149c0":"# choosing no of factors\n# Create factor analysis object and perform factor analysis\nfa = FactorAnalyzer()\nfa.fit(data_copy, 25)\n# Check Eigenvalues\nev, v = fa.get_eigenvalues()","126bc5a5":"# Create scree plot using matplotlib\nplt.scatter(range(1,data_copy.shape[1]+1),ev)\nplt.plot(range(1,data_copy.shape[1]+1),ev)\nplt.title('Scree Plot')\nplt.xlabel('Factors')\nplt.ylabel('Eigenvalue')\nplt.grid()\nplt.show()","3dbfdc43":"ev>1","8030c70c":"# 9 factors ev>1 are considered as factors","550364bd":"fa = FactorAnalyzer(rotation=\"varimax\",n_factors=9)\nfa.fit(data_copy)","a6cc89c8":"fa.loadings_.shape","87dedf73":"loadings=pd.DataFrame(fa.loadings_)","6284653a":"### Now we may pick top most similar vales\nfor col in loadings.columns:\n    print(loadings.nlargest(4, col).index)","b243b9e6":"## so 20,18,19,23 are similar\n## 4,10,11,2\n#41,42,38,40\n#34,35,13,33\n#7,9,8,6\n#44,45,43,48\n#5,0,50,6\n#40,37,46,43\n#26,48,47,29","53c6deb2":"## lets take one sensor out of nine categories and check for spearman correlation\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr(method='spearman').abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(data_copy, 20))","e5e52972":"\nsln.scatterplot(data_copy['sensor_17'],data_copy['sensor_18'])","24353fe6":"data_copy['sensor_18'].plot()\n","5e42e990":"data_copy['sensor_17'].plot()","77ca0053":"df=data_copy[['sensor_20','sensor_18','sensor_19','sensor_23']]","bed70da4":"fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(10,6))\nfor i, ax in enumerate(axes.flatten()):\n    print(i)\n    data = df[df.columns[i]]\n    ax.plot(data, color='red', linewidth=1)\n    # Decorations\n    ax.set_title(df.columns[i])\n    ax.xaxis.set_ticks_position('none')\n    ax.yaxis.set_ticks_position('none')\n    ax.spines[\"top\"].set_alpha(0)\n    ax.tick_params(labelsize=6)","009b9b34":"## SCALING for plotting purpose\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()","26e922c0":"scaled=pd.DataFrame(sc.fit_transform(data_copy),columns=data_copy.columns)","7f943c3f":"scaled.head()","d038aa04":"plt.plot(scaled['sensor_00'],label='00')\nplt.plot(scaled['sensor_01'],label='01')\nplt.plot(scaled['sensor_04'],label='04')\nplt.legend()","12682002":"plt.figure(figsize=(10,7))\n#plt.plot(scaled['sensor_00'],label='00')\nplt.plot(scaled['sensor_00'].rolling(100).mean(),label='mean')\nplt.legend()","9e75e5f7":"## check stationarity\nscaled['sensor_00'].hist()\nplt.show()","c056ce36":"from statsmodels.tsa.stattools import adfuller","5b246a78":"for col in scaled.columns:\n    result = adfuller(X)\n","a0e3fd14":"from sklearn import preprocessing\nfrom sklearn.feature_selection import SelectKBest , chi2\nfrom sklearn.impute import SimpleImputer","259bab70":"data.drop(['Unnamed: 0','sensor_15'],axis=1,inplace=True)","91a49fc0":"train = scaled[:int(0.8*(len(scaled)))]\nvalid = scaled[int(0.8*(len(scaled))):]","9a50d1c5":"from statsmodels.tsa.vector_ar.var_model import VAR","3ea12b29":"data_copy.head()","fece65e5":"### granger causality test\nfrom statsmodels.tsa.stattools import grangercausalitytests\nmaxlag=12\ndef grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n    \n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df","4483f5ae":"grangers_causation_matrix(scaled, variables = scaled.columns)       ","37333b73":"model = VAR(endog=train)\n","843e8a2d":"for i in [1,2,3,4,5,6,7,8,9,10,11,12,13]:\n    result = model.fit(i)\n    print('Lag Order =', i)\n    print('AIC : ', result.aic)\n    print('BIC : ', result.bic)\n    print('FPE : ', result.fpe)\n    print('HQIC: ', result.hqic, '\\n')","fad253b6":"x = model.select_order(maxlags=15)\nx.summary()","81def12f":"#lag=7\nmodel_fit = model.fit(7)\n\ncols=train.columns","33aab27a":"prediction = model_fit.forecast(model_fit.y, steps=len(valid))\n#converting predictions to dataframe\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\npred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])","fd868dbc":"for j in range(0,51):\n    for i in range(0, len(prediction)):\n        pred.iloc[i][j] = prediction[i][j]","5df16cc7":"pred.columns=cols","39f59796":"valid=valid.reset_index()","e77d653c":"valid=valid.drop('index',axis=1)","ab447693":"for col in cols.tolist():\n    print(f'rmse value for', {col}, 'is : ', mean_squared_error(valid[col].values,pred[col].values))","7440d37d":"def diff(col):\n    plt.plot(pred[col],label='predicted')\n    plt.plot(valid[col],label='valid')\n    plt.legend()\n    plt.show()\n    ","afec4ac0":"for col in pred.columns:diff(col)","5ae6a2a5":"## DICKEY-FULLER-TEST","0d9a74b1":"### Lets see a correlation between the sensors","9f8484fb":"## ADF statistic -ve means stationary","e4b8e87c":"### Lets visualize the cycle of some sensors till the first break","90d97ea1":"## FACTOR ANALYSIS","c5639143":"## The Breakpoint is at 12 and 18 day"}}