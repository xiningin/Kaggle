{"cell_type":{"24d69b2d":"code","4436808d":"code","d125a80d":"code","b698dcd4":"code","a84cfc9a":"code","1778237b":"code","2ac5164a":"code","ffc4175d":"code","5cc68473":"code","1d7ba181":"code","f6486e50":"code","c014a883":"code","f2aee30e":"code","f5683e06":"code","a6f71282":"code","f806a0aa":"code","1052089d":"code","3d45c0da":"code","be038ec0":"code","bc8fab2c":"code","c76cd5c1":"markdown","4857b900":"markdown","21fea2bb":"markdown","3b807083":"markdown","e0f9e201":"markdown","777d89db":"markdown","ef6d2a68":"markdown","df03f686":"markdown","9792addf":"markdown"},"source":{"24d69b2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4436808d":"# importing important LIberaries\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom imblearn.over_sampling import SMOTE\nfrom lightgbm import LGBMClassifier\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nfrom sklearn.svm import SVC\nimport plotly.express as px\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=Warning)","d125a80d":"# Loading data\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf","b698dcd4":"# shape of data\ndf.shape","a84cfc9a":"# Data type\ndf.info()","1778237b":"# Checking null values\ndf.isnull().sum()","2ac5164a":"# Droping Unnamed: 32 Column\ndf.drop('Unnamed: 32', axis = 1, inplace = True)","ffc4175d":"# Describing the data \/\/ Statistical Data analysis\npd.set_option('precision',5)\ndf.describe()","5cc68473":"# Diagnosis Pie chart\nprint(df.diagnosis.value_counts())\ndf.diagnosis.value_counts().plot.pie();","1d7ba181":"# Heatmap\nplt.figure(figsize=(30,30))\nsns.heatmap(df.corr(),annot = True, cmap = 'ocean');","f6486e50":"# Getting Mean Columns\nm_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n# Getting Se Columns\ns_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se']\n# Getting Worst column\nw_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","c014a883":"# Heatmap\nplt.figure(figsize=(15,15))\nsns.heatmap(df[m_col].corr(),annot = True, cmap = 'Blues');","f2aee30e":"# pairplot for mean columns\nsns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues');","f5683e06":"# Heatmap for se columns\nplt.figure(figsize=(15,15))\nsns.heatmap(df[s_col].corr(),annot = True, cmap = 'Greens');","a6f71282":"# pairplot for se columns\nsns.pairplot(df[m_col],hue = 'diagnosis', palette='Greens');","f806a0aa":"# Heatmap for Worst columns\nplt.figure(figsize=(15,15))\nsns.heatmap(df[w_col].corr(),annot = True, cmap = 'Oranges');","1052089d":"# pairplot for worst columns\nsns.pairplot(df[w_col],hue = 'diagnosis', palette='Oranges');","3d45c0da":"# Getting Features\nfeatures = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\nx = df[features]\n\n# Getting Predicting Value\ny = df['diagnosis']","be038ec0":"def preprocessing(data,features,labels,test_size = 0.2,random_state =42, tune = 'n',cv_folds = 5):\n    \n    print('Checking if labels or features are categorical! [*]\\n')\n    cat_features=[i for i in features.columns if features.dtypes[i]=='object']\n    if len(cat_features) >= 1 :\n        index = []\n        for i in range(0,len(cat_features)):\n            index.append(features.columns.get_loc(cat_features[i]))\n        print('Features are Categorical\\n')\n        ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), index)], remainder='passthrough')\n        print('Encoding Features [*]\\n')\n        features = np.array(ct.fit_transform(features))\n        print('Encoding Features Done [',u'\\u2713',']\\n')\n    if labels.dtype == 'O':\n        le = LabelEncoder()\n        print('Labels are Categorical [*] \\n')\n        print('Encoding Labels \\n')\n        labels = le.fit_transform(labels)\n        print('Encoding Labels Done [',u'\\u2713',']\\n')\n    else:\n        print('Features and labels are not categorical [',u'\\u2713',']\\n')\n        \n    ## SMOTE ---------------------------------------------------------------------\n    print('Applying SMOTE [*]\\n')\n    \n    sm=SMOTE(k_neighbors=4)\n    features,labels=sm.fit_resample(features,labels)\n    print('SMOTE Done [',u'\\u2713',']\\n')\n    \n    ## Splitting ---------------------------------------------------------------------\n    print('Splitting Data into Train and Validation Sets [*]\\n')\n    \n    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size= test_size, random_state= random_state)\n    print('Splitting Done [',u'\\u2713',']\\n')\n    \n    ## Scaling ---------------------------------------------------------------------\n    print('Scaling Training and Test Sets [*]\\n')\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(x_train)\n    X_val = sc.transform(x_test)\n    print('Scaling Done [',u'\\u2713',']\\n')\n    \n    print('Training All Basic Classifiers on Training Set [*] \\n')\n    \n    parameters_svm= [\n    {'kernel': ['rbf'], 'gamma': [0.1, 0.5, 0.9, 1],\n        'C': np.logspace(-4, 4, 5)},\n    ]\n    parameters_lin = [{\n    'penalty': ['l1', 'l2', ],\n    'solver': ['newton-cg', 'liblinear', ],\n    'C': np.logspace(-4, 4, 5),\n    }]\n    parameters_knn = [{\n    'n_neighbors': list(range(0, 11)),\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'kd_tree', 'brute'],\n    }]\n    parameters_dt = [{\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': [4,  6,  8,  10,  12,  20,  40, 70],\n\n    }]\n    parameters_rfc = [{\n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500, 750, 1000],\n    'max_features': [2, 3],\n    }]\n    parameters_xgb = [{\n    'max_depth': [4,  6,  8,  10],\n    'learning_rate': [0.3, 0.1],\n    }]\n    parameters_lgbm =  {\n    'learning_rate': [0.005, 0.01],\n    'n_estimators': [8,16,24],\n    'boosting_type' : ['gbdt', 'dart'],\n    'objective' : ['binary'],\n    }\n    paramters_pac = {\n        'C': np.logspace(-4, 4, 20)},\n    \n    \n    param_nb={}\n    parameters_ada={\n            'learning_rate': [0.005, 0.01],\n            'n_estimators': [8,16,24],\n    }\n    paramters_sgdc = [{\n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'loss': ['hinge', 'log'],\n    'alpha':np.logspace(-4, 4, 20),\n    }]\n    models =[(\"LR\", LogisticRegression(), parameters_lin),(\"SVC\", SVC(),parameters_svm),('KNN',KNeighborsClassifier(),parameters_knn),\n    (\"DTC\", DecisionTreeClassifier(),parameters_dt),(\"GNB\", GaussianNB(), param_nb),(\"SGDC\", SGDClassifier(), paramters_sgdc),('RF',RandomForestClassifier(),parameters_rfc),\n    ('ADA',AdaBoostClassifier(),parameters_ada),('XGB',GradientBoostingClassifier(),parameters_xgb),('LGBN', LGBMClassifier(),parameters_lgbm),\n    ('PAC',PassiveAggressiveClassifier(),paramters_pac)]\n\n    results = []\n    names = []\n    finalResults = []\n    accres = []\n\n    for name,model, param in models:\n        \n        model.fit(x_train, y_train)\n        model_results = model.predict(x_test)\n        accuracy = accuracy_score(y_test, model_results)\n        print('Validation Accuracy is :',accuracy)\n        print('Applying K-Fold Cross validation on Model {}[*]'.format(name))\n        accuracies = cross_val_score(estimator=model, X=x_train, y=y_train, cv=cv_folds, scoring='accuracy')\n        print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n        acc = accuracies.mean()*100\n        print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100)) \n        results.append(acc)\n        names.append(name)\n        accres.append((name,acc))\n        if tune == 'y' and not name == 'GNB':\n            print('Applying Grid Search Cross validation for model {} []\\n'.format(name))\n            cv_params = param\n            grid_search = GridSearchCV(\n            estimator=model,\n            param_grid=cv_params,\n            scoring='accuracy',\n            cv=cv_folds,\n            n_jobs=-1,\n            verbose=4,\n                )\n            grid_search.fit(X_train, y_train)\n            best_accuracy = grid_search.best_score_\n            best_parameters = grid_search.best_params_\n            print(\"Best Accuracy for model {}: {:.2f} %\".format(name,best_accuracy*100))\n            print(\"Best Parameters: for model {}\".format(name), best_parameters)\n            print('Applying Grid Search Cross validation Done[',u'\\u2713',']\\n')\n            \n        print('Training Compeleted Showing Predictions [',u'\\u2713','] \\n')\n    accres.sort(key=lambda k:k[1],reverse=True)\n    print(\"\\n The Accuracy of the Models Are:\\n \")\n    tab = pd.DataFrame(accres)\n    print(tab)\n    sns.barplot(x=tab[1], y=tab[0], palette='mako');\n    print(\"\\n\\nModel With Highest Accuracy is: \\n\",accres[0],'\\n\\n')","bc8fab2c":"preprocessing(df,x,y)","c76cd5c1":"> As we can see that the last column Unnamed: 32 has all NaN value so we will drop this column","4857b900":"# Data Visualization","21fea2bb":"# Data Modeling","3b807083":"***For SE columns***","e0f9e201":"***Worst columns***","777d89db":"# Summary\n* ***LightGBN was the best here with the accuracy of 97%***","ef6d2a68":"# Please do leave your valuable feedbacks in the comments and any improvements or suggestions are welcomed!!!!\n***Dont forgot to upvote :)***","df03f686":"***For All Data***","9792addf":"***For Mean Columns***"}}