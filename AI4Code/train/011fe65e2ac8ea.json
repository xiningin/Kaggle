{"cell_type":{"a1ec1ab6":"code","9a5687f1":"code","92e5267b":"code","6ac4267a":"code","422d7ae3":"code","325b96f0":"code","6d57f469":"code","a3b99f96":"code","be3a773e":"code","e3496473":"code","3b4a5e59":"code","2adfc040":"code","c9dc07f3":"markdown","d0a81d39":"markdown","2b70605c":"markdown","536df54b":"markdown"},"source":{"a1ec1ab6":"!pip install ..\/input\/textstat\/Pyphen-0.10.0-py3-none-any.whl\n!pip install ..\/input\/textstat\/textstat-0.7.0-py3-none-any.whl","9a5687f1":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport spacy\nimport gensim\n\n\nnltk.data.path.append(\"\/kaggle\/input\/stopwords\/stopwords\")\n\nfrom pandas import DataFrame\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport optuna\n#nltk.download('stopwords')","92e5267b":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","6ac4267a":"excerpt1 = train_df['excerpt'].min()\n\ne = re.sub(\"[^a-zA-Z]\", \" \", excerpt1)\ne = e.lower()\n        \ne = nltk.word_tokenize(e)\n        \ne = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \nlemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e]\ne=\" \".join(e)","422d7ae3":"#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed ","325b96f0":"train_df[\"excerpt_preprocessed\"] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","6d57f469":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")#\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","a3b99f96":"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin', binary=True)","be3a773e":"word2vec_train = np.zeros((len(train_df.index),300),dtype=\"float32\")\nword2vec_test = np.zeros((len(test_df.index),300),dtype=\"float32\")\n\nfor i in range(len(train_df.index)):\n    word2vec_train[i] = avg_feature_vector(train_df[\"excerpt_preprocessed\"][i],word2vec_model, 300)\n    \nfor i in range(len(test_df.index)):\n    word2vec_test[i] = avg_feature_vector(test_df[\"excerpt_preprocessed\"][i],word2vec_model, 300)","e3496473":"target = train_df['target'].to_numpy()\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'n_jobs': -1\n}\n\n\ndef objective(trial):\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'seed': 42,\n        'n_jobs': -1,\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    train_x, test_x, train_y, test_y = train_test_split(word2vec_train, target, test_size=0.25, random_state=42)\n    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.3, random_state=42)\n    \n    train_set = lgb.Dataset(train_x, train_y)\n    valid_set = lgb.Dataset(valid_x, valid_y, reference=train_set)\n    \n    model = lgb.train(params,\n                      train_set,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      valid_sets=[train_set, valid_set])\n    \n    for_param_pred = model.predict(test_x)\n    rmse = np.sqrt(mse(test_y, for_param_pred))\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n\npred = np.zeros(test_df.shape[0])\n\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, target))\nrmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = target[tr_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n        \n    # Training\n    for_inference_param = {\n                            'boosting_type': 'gbdt',\n                            'metric': 'rmse',\n                            'objective': 'regression',\n                            'seed': 42,\n                            'n_jobs': -1}\n    \n    for_inference_param.update(study.best_params)\n    model = lgb.train(for_inference_param,\n                      train_set,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      valid_sets=[train_set, val_set])\n        \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mse(y_va, y_pred))\n    rmses.append(rmse)\n        \n    #Inference\n    test_pred = model.predict(word2vec_test)\n    pred += test_pred \/ 5  ","3b4a5e59":"sample_submission.target = pred\nsample_submission.to_csv('submission.csv',index=False)","2adfc040":"sample_submission","c9dc07f3":"## Optuna + LightGBM\n1. optimize params\n1. cross validation","d0a81d39":"# submission file","2b70605c":"# **Part-of-Speech tagging**","536df54b":"**I wrote this while looking at https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline., https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline**\n\n**datasets: https:\/\/www.kaggle.com\/alvaromunoz\/textstat**\n\n**datasets: https:\/\/www.kaggle.com\/nltkdata\/stopwords**\n\n**How to https:\/\/stackoverflow.com\/questions\/3522372\/how-to-config-nltk-data-directory-from-code**"}}