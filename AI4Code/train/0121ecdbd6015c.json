{"cell_type":{"b644aeea":"code","c7993fb2":"code","3604872e":"code","9e48cd59":"code","c46b248e":"code","ad064f15":"code","19189c0f":"code","066f0d54":"code","20f26e37":"code","a1a62fd6":"code","c7294bf8":"code","a397b9a0":"code","8bf4f436":"code","ace83271":"code","90e06d94":"code","21d739f7":"code","5c5a2dd7":"code","215c1bda":"code","3602759f":"code","66b10a07":"code","d978cbba":"code","a34ae50b":"code","43bb4edd":"code","67a7029c":"code","f49e96de":"code","05f26fd6":"code","a6bf8426":"code","117f1800":"markdown","f550b2ca":"markdown","22b7648a":"markdown","9592a7d1":"markdown","7a4bd5e5":"markdown","bb1c8a8a":"markdown","0c90cde2":"markdown","005fa60a":"markdown","22f3af63":"markdown","3372554d":"markdown","40b2af71":"markdown","246cc057":"markdown","705a4ba9":"markdown","6e92842a":"markdown","58000297":"markdown","778d26e6":"markdown","0cd8b423":"markdown","3a6f738e":"markdown","bb2e89cc":"markdown","261a3b32":"markdown","b1152fef":"markdown","296f67f9":"markdown"},"source":{"b644aeea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# imports dans l'ordre d'utilisation \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow import keras\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c7993fb2":"nRowsRead = None # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/granddebat\/LA_TRANSITION_ECOLOGIQUE.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'LA_TRANSITION_ECOLOGIQUE.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","3604872e":"df.head(1)","9e48cd59":"df.columns.values.tolist()","c46b248e":"q1q2_tmp = df.loc[:,['id', \"QUXVlc3Rpb246MTYw - Quel est aujourd'hui pour vous le probl\u00e8me concret le plus important dans le domaine de l'environnement ?\", 'QUXVlc3Rpb246MTYx - Que faudrait-il faire selon vous pour apporter des r\u00e9ponses \u00e0 ce probl\u00e8me ?']]\nq1q2_tmp.head(1)","ad064f15":"q1q2 = q1q2_tmp.rename(columns={\"QUXVlc3Rpb246MTYw - Quel est aujourd'hui pour vous le probl\u00e8me concret le plus important dans le domaine de l'environnement ?\": \"Q1\", 'QUXVlc3Rpb246MTYx - Que faudrait-il faire selon vous pour apporter des r\u00e9ponses \u00e0 ce probl\u00e8me ?': \"Q2\"})\nq1q2.head(1)","19189c0f":"q1q2['Q1'].value_counts().head(4)","066f0d54":"q1q2[q1q2['Q1']=='Les d\u00e9r\u00e8glements climatiques (crue, s\u00e9cheresse)']","20f26e37":"q1q2_lignes_1 = q1q2[q1q2['Q1'].isin(['Les d\u00e9r\u00e8glements climatiques (crue, s\u00e9cheresse)', 'La biodiversit\u00e9 et la disparition de certaines esp\u00e8ces', \"La pollution de l'air\", \"L'\u00e9rosion du littoral\"])]\nq1q2_lignes_1.head(5)","a1a62fd6":"q1q2_lignes_2 = q1q2_lignes_1[pd.notna(q1q2_lignes_1['Q1']) & pd.notna(q1q2_lignes_1['Q2'])]\nq1q2_lignes_2.head(5)","c7294bf8":"le = preprocessing.LabelEncoder()\nle.fit(q1q2_lignes_2['Q1'])\nle.classes_","a397b9a0":"q1q2_lignes_2.loc[:,'Q1E'] = le.transform(q1q2_lignes_2['Q1']).tolist()\nq1q2_lignes_2.head(3)","8bf4f436":"# Note: reshape(-1, 1) permet ca, -1 indiquant de conserver le nombre de ligne, cf doc de reshape\n# y = q1q2_lignes_2['Q1E'].values.reshape(-1,1)\n# ici on fait un one hot encoding\ny = to_categorical(q1q2_lignes_2['Q1E'].values)\ny, y.shape","ace83271":"X = q1q2_lignes_2['Q2']\nX, X.shape","90e06d94":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 333)\nX_train, X_test, y_train, y_test","21d739f7":"max_words = 1001\nmax_len = 1000\n# http:\/\/faroit.com\/keras-docs\/1.2.2\/preprocessing\/text\/\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\n# https:\/\/keras.io\/preprocessing\/sequence\/\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","5c5a2dd7":"sequences_matrix","215c1bda":"def modelgraph_transition_eco():\n    \"\"\"\n    Function creating the Emojify-v2 (nope the transition eco) model's graph.\n    \n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n\n    Returns:\n    model -- a model instance in Keras\n    \"\"\"\n    \n    ### START CODE HERE ###\n    # Define sequences_matrix as the input of the graph, it should be of shape [max_len]\n    inputs = Input(name='inputs',shape=[max_len])\n    \n    # Embedding layer to be learned\n    embedding_layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(units = 128, return_sequences = True)(embedding_layer)\n    # Add dropout with a probability of 0.5\n    X = Dropout(rate=0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(units = 128)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(rate=0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(4)(X)\n    # Add a softmax activation\n    X = Activation(activation=\"softmax\")(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=inputs, outputs=X)\n    \n    ### END CODE HERE ###\n    \n    return model","3602759f":"model = modelgraph_transition_eco()\nmodel.summary()","66b10a07":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","d978cbba":"model.fit(sequences_matrix, y_train, epochs = 1, batch_size = 1024, shuffle=True)","a34ae50b":"model.save('\/kaggle\/working\/modele_lstm_classif_gdte.h5')","43bb4edd":"modele_lstm_classif_gdte = keras.models.load_model('\/kaggle\/working\/modele_lstm_classif_gdte.h5')\nmodele_lstm_classif_gdte","67a7029c":"sequences_test = tok.texts_to_sequences(X_test)\nsequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)\nsequences_matrix_test","f49e96de":"loss, acc = modele_lstm_classif_gdte.evaluate(sequences_matrix_test, y_test)\nprint()\nprint(\"Test accuracy = \", acc)","05f26fd6":"pred_test = modele_lstm_classif_gdte.predict(sequences_matrix_test)\npred_test","a6bf8426":"pd.crosstab(np.argmax(y_test, axis=1), np.argmax(pred_test, axis=1), rownames=['Actual'], colnames=['Predicted'], margins=True)","117f1800":"Transformation de pred_test et y_test afin d'avoir des array de shape (nb_cas_test,), de valeurs parmi [0,1,2,3] qui correspondent aux reponses de Q1","f550b2ca":"La variable predite doit aussi etre onehot encoded\/reshapee en (m,c) avec m le nombre de lignes dans q1q2_lignes_2 et c le nombre de categories.","22b7648a":"# Preparation des donnees La transition Ecologique\n\n## chargement des donnees","9592a7d1":"## Selection des lignes\n* Conservation des lignes avec une des 4 reponses cochables sur Q1\n* Conservation des lignes avec des reponses sur Q2","7a4bd5e5":"A quoi ressemble les sequences_matrix ?","bb1c8a8a":"# Application aux donnees de test","0c90cde2":"# Visualisation des resultats\n\nFaisons par exemple une matrice des resultats obtenus pour le test\navec en ligne les vrais r\u00e9ponses et en colonne les r\u00e9ponses pr\u00e9dites...  ","005fa60a":"# Objectif\n\nSimplement de mettre en oeuvre les connaissances acquises avec la specialisation Deep Learning sur Coursera.  \n\nOn va tenter de determiner a partir de la reponse a la question 2, quelle etait la reponse a la question 1 choisie. \n\n## imports","22f3af63":"test consistant a recreer le modele a partir du fichier modele_lstm_classif_gdte","3372554d":"## Sauvegarde du modele\n\nRessource : https:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize","40b2af71":"## Scission en jeux de donnees de test et train","246cc057":"# Modele\n\nModification du modele emojify_V2 du notebook semaine 2 du cours 5 Sequence Models\nmelangee avec le notebook Simple LSTM for text classification\n\n## Construction du graphe du modele","705a4ba9":"# Word embedding\n\nComment representer les mots utilises dans le formulaire de La Transition Ecologique.\nLa methode utilisee ici sera d'entrainer une couche d'Embedding de Keras.  \n\nLe texte des reponses doit etre prepare en amont cf https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n\nDans un premier temps, pas de pretraitement du genre suppression de mots ou utilisation d'un dictionnaire. ","6e92842a":"## Encodage de la variable predite\n\nAssociation d'un chiffre a chaque reponse","58000297":"## Pretraitement pour le word embedding\n* tokenization\n* conversion en sequence\n* padding pour que toutes les sequences aient la meme taille","778d26e6":"Appel de la fonction pour creation du modele","0cd8b423":"application a la colonne Q1 => creation d'une colonne Q1E pour colonne Q1 Encodee ","3a6f738e":"# Choix de la m\u00e9trique\n\ncf https:\/\/machinelearningmastery.com\/how-to-choose-loss-functions-when-training-deep-learning-neural-networks\/  \nemojify notebook of week 2 of course 5 Sequence Models  \nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics  \n\nL'entropie crois\u00e9e est la metrique de base.","bb2e89cc":"## Definition de la metrique et compilation","261a3b32":"## Selection des colonnes \ncorrespondant aux questions 1 & 2","b1152fef":"## Entrainement du modele\n\nModifier les valeurs d'epochs et batch_size permet d'obtenir de meilleurs resultats\nPar exemple, epoch=1 et batch_size=1024 donne de moins bons resultats que epoch=5 et batch_size=256","296f67f9":"Bizarrement, il n'y a aucune prediction 0 (l'erosion du littoral...), qui est tres certainement la reponse minoritaire a Q1 (parmi les cases cochees), voir d'autres notebooks pour s'en assurer."}}