{"cell_type":{"c49fb0a7":"code","3a43c323":"code","16162a6a":"code","507a8095":"code","85099b7b":"code","63eb2208":"code","8346a345":"code","2e59efd8":"code","d6706f2e":"code","22da3b8b":"code","afac5b3d":"code","b71e9081":"code","f0a93d83":"code","f2de841a":"code","3a9f626f":"code","f56b673d":"code","f9f7639f":"code","1bd1d9a7":"code","b2ec683f":"code","2049a621":"code","0f18faea":"code","ff21e3f0":"code","60cc3ac0":"code","dd34ca58":"code","76bcde6e":"code","3985c84e":"code","2c0ba77d":"code","3b76c8c6":"code","16d35601":"code","4c3d9147":"code","ce748b98":"code","0a6801a3":"markdown","b0298cec":"markdown","28477a31":"markdown","d256ee53":"markdown","bf6f6f40":"markdown","792f935f":"markdown","b9af0dab":"markdown","e704f7bb":"markdown","da6344bf":"markdown","efd41051":"markdown","2a53f566":"markdown","4e578054":"markdown","3bf391be":"markdown","36bb9ac6":"markdown","a64b2c57":"markdown","bfaf7abb":"markdown","212549ac":"markdown","f7fbcb57":"markdown","8a0ecbc2":"markdown","77e80bf3":"markdown","654e41b8":"markdown","d0d6dec0":"markdown","1bf162fe":"markdown","c1ccc1e3":"markdown","d50bb17c":"markdown","1466a2de":"markdown"},"source":{"c49fb0a7":"'''\nfrom apiclient.discovery import build\nimport pandas as pd\n\n# Data to be stored\ncategory = []\nno_of_samples = 1700\n\n# Gathering Data using the Youtube API\napi_key = \"AIzaSyAS9eTgOEnOJ2GlJbbqm_0bR1onuRQjTHE\"\nyoutube_api = build('youtube','v3', developerKey = api_key)\n\n# Travel Data\ntvl_titles = []\ntvl_descriptions = []\ntvl_ids = []\n\nreq = youtube_api.search().list(q='travel vlogs', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(tvl_titles)<no_of_samples):\n    for i in range(len(res['items'])):\n        tvl_titles.append(res['items'][i]['snippet']['title'])\n        tvl_descriptions.append(res['items'][i]['snippet']['description'])\n        tvl_ids.append(res['items'][i]['id']['videoId'])\n        category.append('travel')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n        req = youtube_api.search().list(q='travelling', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    else:\n        break\n\n\n# Science Data\nscience_titles = []\nscience_descriptions = []\nscience_ids = []\n\nnext_page_token = None\nreq = youtube_api.search().list(q='robotics', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(science_titles)<no_of_samples):\n    if(next_page_token is not None):\n        req = youtube_api.search().list(q='robotics', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    for i in range(len(res['items'])):\n        science_titles.append(res['items'][i]['snippet']['title'])\n        science_descriptions.append(res['items'][i]['snippet']['description'])\n        science_ids.append(res['items'][i]['id']['videoId'])\n        category.append('science and technology')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n    else:\n        break\n    \n# Food Data\nfood_titles = []\nfood_descriptions = []\nfood_ids = []\n\nnext_page_token = None\nreq = youtube_api.search().list(q='delicious food', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(food_titles)<no_of_samples):\n    if(next_page_token is not None):\n        req = youtube_api.search().list(q='delicious food', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    for i in range(len(res['items'])):\n        food_titles.append(res['items'][i]['snippet']['title'])\n        food_descriptions.append(res['items'][i]['snippet']['description'])\n        food_ids.append(res['items'][i]['id']['videoId'])\n        category.append('food')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n    else:\n        break\n\n# Food Data\nmanufacturing_titles = []\nmanufacturing_descriptions = []\nmanufacturing_ids = []\n\nnext_page_token = None\nreq = youtube_api.search().list(q='3d printing', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(manufacturing_titles)<no_of_samples):\n    if(next_page_token is not None):\n        req = youtube_api.search().list(q='3d printing', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    for i in range(len(res['items'])):\n        manufacturing_titles.append(res['items'][i]['snippet']['title'])\n        manufacturing_descriptions.append(res['items'][i]['snippet']['description'])\n        manufacturing_ids.append(res['items'][i]['id']['videoId'])\n        category.append('manufacturing')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n    else:\n        break\n    \n# History Data\nhistory_titles = []\nhistory_descriptions = []\nhistory_ids = []\n\nnext_page_token = None\nreq = youtube_api.search().list(q='archaeology', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(history_titles)<no_of_samples):\n    if(next_page_token is not None):\n        req = youtube_api.search().list(q='archaeology', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    for i in range(len(res['items'])):\n        history_titles.append(res['items'][i]['snippet']['title'])\n        history_descriptions.append(res['items'][i]['snippet']['description'])\n        history_ids.append(res['items'][i]['id']['videoId'])\n        category.append('history')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n    else:\n        break\n    \n# Art and Music Data\nart_titles = []\nart_descriptions = []\nart_ids = []\n\nnext_page_token = None\nreq = youtube_api.search().list(q='painting', part='snippet', type='video', maxResults = 50)\nres = req.execute()\nwhile(len(art_titles)<no_of_samples):\n    if(next_page_token is not None):\n        req = youtube_api.search().list(q='painting', part='snippet', type='video', maxResults = 50, pageToken=next_page_token)\n        res = req.execute()\n    for i in range(len(res['items'])):\n        art_titles.append(res['items'][i]['snippet']['title'])\n        art_descriptions.append(res['items'][i]['snippet']['description'])\n        art_ids.append(res['items'][i]['id']['videoId'])\n        category.append('art and music')\n            \n    if('nextPageToken' in res):\n        next_page_token = res['nextPageToken']\n    else:\n        break\n \n    \n# Construct Dataset\nfinal_titles = tvl_titles + science_titles + food_titles + manufacturing_titles + history_titles + art_titles\nfinal_descriptions = tvl_descriptions + science_descriptions + food_descriptions + manufacturing_descriptions + history_descriptions + art_descriptions\nfinal_ids = tvl_ids + science_ids + food_ids + manufacturing_ids + history_ids + art_ids\ndata = pd.DataFrame({'Video Id': final_ids, 'Title': final_titles, 'Description': final_descriptions, 'Category': category}) \ndata.to_csv('Videos_data.csv')\n'''","3a43c323":"import pandas as pd\nimport nltk\n#nltk.download()\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","16162a6a":"# Import Data\nvdata = pd.read_csv('Videos_data.csv')\nvdata = data.iloc[:, 1:]     # Remove extra un-named column\nvdata.head(10)","507a8095":"# Missing Values\nnum_missing_desc = data.isnull().sum()[2]    # No. of values with msising descriptions\nprint('Number of missing values: ' + str(num_missing_desc))\nvdata = data.dropna()","85099b7b":"# Change to lowercase\nvdata['Title'] = vdata['Title'].map(lambda x: x.lower())\nvdata['Description'] = vdata['Description'].map(lambda x: x.lower())\n\n# Remove numbers\nvdata['Title'] = vdata['Title'].map(lambda x: re.sub(r'\\d+', '', x))\nvdata['Description'] = vdata['Description'].map(lambda x: re.sub(r'\\d+', '', x))\n\n# Remove Punctuation\nvdata['Title']  = vdata['Title'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\nvdata['Description']  = vdata['Description'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n\n# Remove white spaces\nvdata['Title'] = vdata['Title'].map(lambda x: x.strip())\nvdata['Description'] = vdata['Description'].map(lambda x: x.strip())\n\n# Tokenize into words\nvdata['Title'] = vdata['Title'].map(lambda x: word_tokenize(x))\nvdata['Description'] = vdata['Description'].map(lambda x: word_tokenize(x))\n \n# Remove non alphabetic tokens\nvdata['Title'] = vdata['Title'].map(lambda x: [word for word in x if word.isalpha()])\nvdata['Description'] = vdata['Description'].map(lambda x: [word for word in x if word.isalpha()])\n\n# filter out stop words\nstop_words = set(stopwords.words('english'))\nvdata['Title'] = vdata['Title'].map(lambda x: [w for w in x if not w in stop_words])\nvdata['Description'] = vdata['Description'].map(lambda x: [w for w in x if not w in stop_words])\n\n# Word Lemmatization\nlem = WordNetLemmatizer()\nvdata['Title'] = vdata['Title'].map(lambda x: [lem.lemmatize(word,\"v\") for word in x])\nvdata['Description'] = vdata['Description'].map(lambda x: [lem.lemmatize(word,\"v\") for word in x])\n\n# Turn lists back to string\nvdata['Title'] = vdata['Title'].map(lambda x: ' '.join(x))\nvdata['Description'] = vdata['Description'].map(lambda x: ' '.join(x))","63eb2208":"vdata.head(10)","8346a345":"# Encode classes\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(vdata.Category)\nvdata.Category = le.transform(vdata.Category)\nvdata.head(10)","2e59efd8":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_title = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\ntfidf_desc = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nlabels = vdata.Category\nfeatures_title = tfidf_title.fit_transform(vdata.Title).toarray()\nfeatures_description = tfidf_desc.fit_transform(vdata.Description).toarray()\nprint('Title Features Shape: ' + str(features_title.shape))\nprint('Description Features Shape: ' + str(features_description.shape))","d6706f2e":"# Plotting class distribution\nvdata['Category'].value_counts().sort_values(ascending=False).plot(kind='bar', y='Number of Samples', \n                                                                title='Number of samples for each class')","22da3b8b":"# Best 5 keywords for each class using Title Feaures\nfrom sklearn.feature_selection import chi2\nimport numpy as np\nN = 10\nfor current_class in list(le.classes_):\n    current_class_id = le.transform([current_class])[0]\n    features_chi2 = chi2(features_title, labels == current_class_id)\n    indices = np.argsort(features_chi2[0])\n    feature_names = np.array(tfidf_title.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n    print(\"# '{}':\".format(current_class))\n    print(\"Most correlated unigrams:\")\n    print('-' *30)\n    print('. {}'.format('\\n. '.join(unigrams[-N:])))\n    print(\"Most correlated bigrams:\")\n    print('-' *30)\n    print('. {}'.format('\\n. '.join(bigrams[-N:])))\n    print(\"\\n\")","afac5b3d":"# Best 5 keywords for each class using Description Features\nfrom sklearn.feature_selection import chi2\nimport numpy as np\nN = 10\nfor current_class in list(le.classes_):\n    current_class_id = le.transform([current_class])[0]\n    features_chi2 = chi2(features_description, labels == current_class_id)\n    indices = np.argsort(features_chi2[0])\n    feature_names = np.array(tfidf_desc.get_feature_names())[indices]\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n    print(\"# '{}':\".format(current_class))\n    print(\"Most correlated unigrams:\")\n    print('-' *30)\n    print('. {}'.format('\\n. '.join(unigrams[-N:])))\n    print(\"Most correlated bigrams:\")\n    print('-' *30)\n    print('. {}'.format('\\n. '.join(bigrams[-N:])))\n    print(\"\\n\")","b71e9081":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import AdaBoostClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(vdata.iloc[:, 1:3], vdata['Category'], random_state = 0)\nX_train_title_features = tfidf_title.transform(X_train['Title']).toarray()\nX_train_desc_features = tfidf_desc.transform(X_train['Description']).toarray()\nfeatures = np.concatenate([X_train_title_features, X_train_desc_features], axis=1)","f0a93d83":"X_train.head()","f2de841a":"y_train.head()","3a9f626f":"# Naive Bayes\nnb = MultinomialNB().fit(features, y_train)\n# SVM\nsvm = linear_model.SGDClassifier(loss='modified_huber',max_iter=1000, tol=1e-3).fit(features,y_train)\n# AdaBoost\nadaboost = AdaBoostClassifier(n_estimators=40,algorithm=\"SAMME\").fit(features,y_train)","f56b673d":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.utils.np_utils import to_categorical\n\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 20000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n# This is fixed.\nEMBEDDING_DIM = 100\n\n# Combining titles and descriptions into a single sentence\ntitles = vdata['Title'].values\ndescriptions = vdata['Description'].values\ndata_for_lstms = []\nfor i in range(len(titles)):\n    temp_list = [titles[i], descriptions[i]]\n    data_for_lstms.append(' '.join(temp_list))\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(data_for_lstms)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# Convert the data to padded sequences\nX = tokenizer.texts_to_sequences(data_for_lstms)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\n# One-hot Encode labels\nY = pd.get_dummies(vdata['Category']).values\nprint('Shape of label tensor:', Y.shape)\n\n# Splitting into training and test set\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state = 42)","f9f7639f":"# Define LSTM Model\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(6, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","1bd1d9a7":"# Training LSTM Model\nepochs = 5\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)","b2ec683f":"import matplotlib.pyplot as plt\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='test')\nplt.legend()\nplt.show();","2049a621":"X_train, X_test, y_train, y_test = train_test_split(vdata.iloc[:, 1:3], vdata['Category'], random_state = 0)\nX_test_title_features = tfidf_title.transform(X_test['Title']).toarray()\nX_test_desc_features = tfidf_desc.transform(X_test['Description']).toarray()\ntest_features = np.concatenate([X_test_title_features, X_test_desc_features], axis=1)","0f18faea":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nX_test_title_features = tfidf_title.transform(X_test['Title']).toarray()\nX_test_desc_features = tfidf_desc.transform(X_test['Description']).toarray()\ntest_features = np.concatenate([X_test_title_features, X_test_desc_features], axis=1)\n\n# Naive Bayes\ny_pred = nb.predict(test_features)\ny_probas = nb.predict_proba(test_features)\n\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names=list(le.classes_)))\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(le.classes_), yticklabels=list(le.classes_))\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix - Naive Bayes')\nplt.show()\n\nskplt.metrics.plot_precision_recall_curve(y_test, y_probas)\nplt.title('Precision-Recall Curve - Naive Bayes')\nplt.show()","ff21e3f0":"# SVM\ny_pred = svm.predict(test_features)\ny_probas = svm.predict_proba(test_features)\n\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names=list(le.classes_)))\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(le.classes_), yticklabels=list(le.classes_))\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix - SVM')\nplt.show()\n\nskplt.metrics.plot_precision_recall_curve(y_test, y_probas)\nplt.title('Precision-Recall Curve - SVM')\nplt.show()","60cc3ac0":"# Adaboost Classifier\ny_pred = adaboost.predict(test_features)\ny_probas = adaboost.predict_proba(test_features)\n\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names=list(le.classes_)))\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(le.classes_), yticklabels=list(le.classes_))\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix - Adaboost Classifier')\nplt.show()\n\nskplt.metrics.plot_precision_recall_curve(y_test, y_probas)\nplt.title('Precision-Recall Curve - Adaboost Classifier')\nplt.show()","dd34ca58":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state = 42)\ny_probas = model.predict(X_test)\ny_pred = np.argmax(y_probas, axis=1)\ny_test = np.argmax(Y_test, axis=1)\n\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names=list(le.classes_)))\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=list(le.classes_), yticklabels=list(le.classes_))\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix - LSTM')\nplt.show()\n\nskplt.metrics.plot_precision_recall_curve(y_test, y_probas)\nplt.title('Precision-Recall Curve - LSTM')\nplt.show()","76bcde6e":"#import data\nadata = pd.read_csv('collected_sports_data.csv' )","3985c84e":"adata.head(10)","2c0ba77d":"#delete columns which are not required\nadata.drop(adata.iloc[:, 4:42], inplace = True, axis = 1) ","3b76c8c6":"adata.head(10)","16d35601":"# Change to lowercase\nadata['Title 1'] = adata['Title 1'].map(lambda x: x.lower())\nadata['Meta Description 1'] = adata['Meta Description 1'].map(lambda x: x.lower())\nadata['H1-1'] = adata['H1-1'].map(lambda x: x.lower())\n\n# Remove Punctuation\nadata['Title 1'] = adata['Title 1'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\nadata['Meta Description 1'] = adata['Meta Description 1'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\nadata['H1-1'] = adata['H1-1'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n\n# Remove white spaces\nadata['Title 1'] = adata['Title 1'].map(lambda x: x.strip())\nadata['Meta Description 1'] = adata['Meta Description 1'].map(lambda x: x.strip())\nadata['H1-1'] = adata['H1-1'].map(lambda x: x.strip())\n","4c3d9147":"adata.head(10)","ce748b98":"import pandas as pd\ndef find(dec,k):\n    r=[]\n    for i in dec.index:\n        if k in dec['Meta Description 1'][i]:\n            r.append(dec['Original Url'][i])\n    return r\n\n\n# Import Data\n#adata = pd.read_csv('collected_sports_data.csv' )\nadata=adata[['Original Url', 'Meta Description 1']]\n\n#Search unigram keyword which is extracted from videos data.\n\nresult=find(adata, \"travel\") \nfor i in result:\n    print(\" Url Link \",i)","0a6801a3":"The above links in the output re-directs to the advertisement video.","b0298cec":"### Missing Values","28477a31":"### Label Encoding classes","d256ee53":"# Data Collection","bf6f6f40":"### Importing Libraries","792f935f":"The cleaning of the text is performed in the following manner:\n\n- Converting to Lowercase\n- Removing numerical values, because they do not contribute towards predicting the category\n- Removing Punctuation because special characters like $, !, etc. do not hold any useful information \n- Removing extra white spaces\n- Tokenizing into words - This means to convert a text string into a list of 'tokens', where each token is a word. Eg. The sentence 'My Name is Rishi' becomes ['My', 'Name', 'is', 'Rishi']\n- Removing all non-alphabetic words\n- Filtering out stop words such as and, the, is, etc. because they do not contain useful information for text classification\n- Lemmatizing words - Lemmatizing reduces words to their base meaning, such as words 'fly' and 'flying' are both convert to just 'fly'","b9af0dab":"# Importing Advertisement Dataset","e704f7bb":"# Data Preprocessing and Cleaning","da6344bf":"# END","efd41051":"# Text Classification","2a53f566":"### LSTM ","4e578054":"# Data Prepocessing and cleaning","3bf391be":"# Modeling and Training","36bb9ac6":"The idea here is to gather my own data for classification. I am targeting data of videos available on Youtube. The data is collected for **6 categories**:\n\n- Travel Blogs \n- Science and Technology \n- Food \n- Manufacturing \n- History \n- Art and Music \n\nTo perform the required data collection, I used the **Youtube API v3**. I decided to use the Youtube API since I needed to collected >1700 samples, since it has an option to get data from subsequent pages of the search results.","a64b2c57":"### Data Analysis and Feature Exploration","bfaf7abb":"### Vectorizing text features using TF-IDF ","212549ac":"### Adaboost Classifier","f7fbcb57":"This function below mateches the unigram with advertisement data. And gives the output the url link of the advertisement related to the given keyword.","8a0ecbc2":"Now let us see if the features are correctly extracted from the text data by checking the most important features for each class","77e80bf3":"Features for both **Title** and **Description** are extracted and then concantenated in order to construct a final feature matrix","654e41b8":"Advertisement data is collected via web scrapping.\nThe Data iscollected from www.adforum.com\nUsing brower extention and and the extractor tool Screaming Frog SEO spyder","d0d6dec0":"### SVM","1bf162fe":"### Text Cleaning","c1ccc1e3":"### Naive Bayes","d50bb17c":"# Performance Evaluation","1466a2de":"### Data Preprocessing"}}