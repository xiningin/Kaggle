{"cell_type":{"5e89d440":"code","dfcffa5f":"code","e503b2cd":"code","d7b78838":"code","6e505f98":"code","a7d30abf":"code","b23574dd":"code","ff73eee8":"code","f356b1c3":"code","4e6a86d3":"code","48012416":"code","34679768":"code","9cf6bb20":"code","454e8417":"code","fbc580c2":"code","d6ffe8cc":"code","92d2735e":"code","4e0c1707":"code","46aa3e0f":"code","02dc4755":"code","e7f7f5a4":"code","551026e4":"code","fbc02b16":"code","51073e41":"code","1e000616":"code","dc0421a5":"code","dc781d77":"code","6e128483":"code","1b86717e":"code","ffbeceb2":"code","12f90961":"code","00dc7dd6":"code","95aeab4c":"markdown","7057352b":"markdown","a634c941":"markdown","0079026e":"markdown","e476156e":"markdown","19144f02":"markdown","68743013":"markdown","c8ed994b":"markdown","4b61eeff":"markdown","dc8de173":"markdown","3b257182":"markdown","78dd850c":"markdown","bcdc9f23":"markdown"},"source":{"5e89d440":"internet_on = False","dfcffa5f":"if internet_on==True:\n    !pip install fairseq fastBPE ","e503b2cd":"if internet_on==True:\n    !ls \/root\/.cache\/pip\/wheels\/df\/60\/ff\/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684","d7b78838":"if internet_on==True:\n    !ls \/root\/.cache\/pip\/wheels\/fb\/85\/9b\/286072121774d5b8b0253ab66271b558069189cbe795bc6084","6e505f98":"if internet_on==True:\n    !mv \/root\/.cache\/pip\/wheels\/df\/60\/ff\/1764bce64cccd9d2c06ba19e5f6f4108ad29e2d48e1068c684\/* \/kaggle\/working\n    !mv \/root\/.cache\/pip\/wheels\/fb\/85\/9b\/286072121774d5b8b0253ab66271b558069189cbe795bc6084\/* \/kaggle\/working","a7d30abf":"if internet_on==False:\n    !pip install ..\/input\/fairseq-and-fastbpe\/sacrebleu-1.4.9-py3-none-any.whl ","b23574dd":"# These files were saved in version 1 of this notebook when internet_on was True\nif internet_on==False:\n    !pip install ..\/input\/v1-fairseq-fastbpe\/fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl\n    !pip install ..\/input\/v1-fairseq-fastbpe\/fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl","ff73eee8":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","f356b1c3":"#From: https:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer\nfrom types import SimpleNamespace\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nclass BERTweetTokenizer():\n    \n    def __init__(self,pretrained_path = 'pretrained_models\/BERTweet_base_transformers\/'):\n        \n        self.bpe = fastBPE(SimpleNamespace(bpe_codes= pretrained_path + \"bpe.codes\"))\n        self.vocab = Dictionary()\n        self.vocab.add_from_file(pretrained_path + \"dict.txt\")\n        self.cls_token_id = 0\n        self.pad_token_id = 1\n        self.sep_token_id = 2\n        self.pad_token = '<pad>'\n        self.cls_token = '<s>'\n        self.sep_token = '<\/s>'\n        \n    def bpe_encode(self,text):\n        return self.bpe.encode(text)\n    \n    def encode(self,text,add_special_tokens=False):\n        subwords = self.bpe.encode(text)\n        input_ids = self.vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    def tokenize(self,text):\n        return self.bpe_encode(text).split()\n    \n    def convert_tokens_to_ids(self,tokens):\n        input_ids = self.vocab.encode_line(' '.join(tokens), append_eos=False, add_if_not_exist=False).long().tolist()\n        return input_ids\n    \n    #from: https:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look\n    def decode_id(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@')\n    \n    def decode_id_nospace(self,id):\n        return self.vocab.string(id, bpe_symbol = '@@ ')","4e6a86d3":"tokenizer = BERTweetTokenizer('\/kaggle\/input\/bertweet-base-transformers\/')","48012416":"tokenizer.encode('positive')","34679768":"tokenizer.encode('negative')","9cf6bb20":"tokenizer.encode('neutral')","454e8417":"tokenizer.decode_id([14058])","fbc580c2":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","d6ffe8cc":"train_df.sentiment.value_counts(dropna=False)","92d2735e":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","4e0c1707":"MAX_LEN = 96\nPATH = '..\/input\/bertweet-base-transformers\/'\nsentiment_id = {'positive': 1809, 'negative': 3392, 'neutral': 14058}","46aa3e0f":"ct = train_df.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n    \n    # ID_OFFSETS\n    # From: https:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look (comments)\n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode_id([t])\n        if text1[text1.find(w,idx)-1] == \" \":\n            idx+=1\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n        else:\n            offsets.append((idx,idx+len(w)))\n            idx += len(w)\n\n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    if len(enc)<92:\n        input_ids[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc)+5] = 1\n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,toks[-1]+1] = 1        \n    if len(enc)>91:\n        input_ids[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask[k,:96] = 1        \n        if len(toks)>0:\n            start_tokens[k,toks[0]+1] = 1\n            end_tokens[k,96-1] = 1","02dc4755":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]    \n    if len(enc)<92:\n        input_ids_t[k,:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:len(enc)+5] = 1\n    if len(enc)>91:\n        input_ids_t[k,:96] = [0] + enc[:91] + [2,2] + [s_tok] + [2]\n        attention_mask_t[k,:96] = 1  ","e7f7f5a4":"all=[]\ncount=0\nfor k in range(train_df.shape[0]):    \n    a = np.argmax(start_tokens[k,])\n    b = np.argmax(end_tokens[k,])\n    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)\n    st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    all.append(jaccard(st,train_df.loc[k,'selected_text']))\nprint('>>>> Jaccard =',np.mean(all))","551026e4":"improve_jacc_review = False","fbc02b16":"if improve_jacc_review == True:\n    all=[]\n    count=0\n    for k in range(train_df.shape[0]):    \n        a = np.argmax(start_tokens[k,])\n        b = np.argmax(end_tokens[k,])\n        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n        st = st.replace('<unk>','')\n        if jaccard(st,train_df.loc[k,'selected_text'])<.3:\n            print(k)\n            print(st)\n            print(train_df.loc[k,'selected_text'])\n            print()\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    print('>>>> Jaccard =',np.mean(all))","51073e41":"def scheduler(epoch):\n    return 5e-5 * 0.2**epoch","1e000616":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'model.bin',config=config,from_pt=True)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x[0])\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x[0])\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","dc0421a5":"#5-fold CV\nn_splits = 5","dc781d77":"#This will loop over more than one seed and average all folds from all seeds together\nn_seeds = 1","6e128483":"#Set equal to False if you already have model trained and just want to generate predictions. You'll need to save the model weights to input>pre-trained-model.\ntrainModel=False","1b86717e":"if trainModel==True:\n\n    for x in range(n_seeds): \n        \n        jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\n        skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n            \n            print('#'*25)\n            print('### FOLD %i'%(fold+1))\n            print('#'*25)\n\n            K.clear_session()\n            model = build_model()\n\n            reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n            \n            sv = tf.keras.callbacks.ModelCheckpoint(\n                '%s-roberta-%i-%x.h5'%(VER,fold,x), monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=True, mode='auto', save_freq='epoch')\n\n            hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n                epochs=3, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n                validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n                [start_tokens[idxV,], end_tokens[idxV,]]))\n\n            print('Loading model...')\n            model.load_weights('%s-roberta-%i-%x.h5'%(VER,fold,x))\n\n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]\/(n_splits*n_seeds)\n            preds_end += preds[1]\/(n_splits*n_seeds)\n\n            # DISPLAY FOLD JACCARD\n            all = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)                   \n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))\n            jac.append(np.mean(all))\n            print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n            print()\n        \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","ffbeceb2":"if trainModel==False:\n        \n    DISPLAY=1\n    \n    for x in range(n_seeds): \n        \n        jac = []\n    \n        oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n        oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n    \n        preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n        \n        print('#'*70)\n        print('### SEED %x'%(x+1))\n        print('#'*70)\n\n        skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777+x)\n        for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):  \n            \n            print('#'*25)\n            print('### MODEL %i'%(fold+1))\n            print('#'*25)\n\n            K.clear_session()\n            model = build_model()\n            model.load_weights('..\/input\/bertweet-files\/v0-roberta-%i-%x.h5'%(fold,x))\n\n            print('Predicting Test...')\n            preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n            preds_start += preds[0]\/(n_splits*n_seeds)\n            preds_end += preds[1]\/(n_splits*n_seeds)\n            \n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n            \n            # DISPLAY FOLD JACCARD\n            all = []; pos = []; neg = []; nue = []\n            for k in idxV:\n                a = np.argmax(oof_start[k,])\n                b = np.argmax(oof_end[k,])\n                if a>b: \n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n                else:\n                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n                    enc = tokenizer.encode(text1)\n                    st = tokenizer.decode_id_nospace(enc[a-1:b])\n                st = st.replace('<unk>','')                                              \n                if train_df.loc[k,'sentiment']=='positive':\n                    pos.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='negative':\n                    neg.append(jaccard(st,train_df.loc[k,'selected_text']))\n                if train_df.loc[k,'sentiment']=='neutral':\n                    st = text1\n                    nue.append(jaccard(st,train_df.loc[k,'selected_text']))\n                all.append(jaccard(st,train_df.loc[k,'selected_text']))  \n            jac.append(np.mean(all))\n            print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n            print('>>>> FOLD %i Neutral Jaccard ='%(fold+1),np.mean(nue))\n            print('>>>> FOLD %i Positive Jaccard ='%(fold+1),np.mean(pos))\n            print('>>>> FOLD %i Negative Jaccard ='%(fold+1),np.mean(neg))                 \n            print()\n            \n        print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","12f90961":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:a+3])\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode_id_nospace(enc[a-1:b])\n    st = st.replace('<unk>','')\n    if test_df.loc[k,'sentiment']=='neutral':\n        st = text1\n    all.append(st)","00dc7dd6":"test_df['selected_text'] = all\ntest_df[['textID','selected_text']].to_csv('submission.csv',index=False)","95aeab4c":"# Inference","7057352b":"How good does our process work?","a634c941":"This notebook uses BERTweet from:\n\nhttps:\/\/github.com\/VinAIResearch\/BERTweet\n\nhttps:\/\/arxiv.org\/abs\/2005.10200\n\n(see discussion here: https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/152861)","0079026e":"This is mostly adjusting and combining code from the following:\n\nhttps:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705 (for majority of script)\n\nhttps:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712 (for inference and CNN head, switched from dropout to batch normalization)\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/setup-tokenizer (for tokenizer)\n\nhttps:\/\/www.kaggle.com\/nandhuelan\/bertweet-first-look (for offsets and decoding)","e476156e":"# Test predictions","19144f02":"# Load  data and libraries","68743013":"# Data preproccesing","c8ed994b":"Note: I could not figure out how to get sacrebleu, but found it loaded to Kaggle already","4b61eeff":"# Model","dc8de173":"See tips on how to install offline: https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/113195","3b257182":"# Train","78dd850c":"# TensorFlow BERTweet","bcdc9f23":"Changes from V2: Changed CNN head and added LR schedule\n\nChanges from V3: Adjusted epochs and batch size to get fold 4 training to learn (last iteration was stuck and produced a 0.65 jaccard)\n\nChanges from V6: Adjusted post-processing for unk tokens. Added jaccard scores for sentiments. Set neutral equal to text instead of relying on model (jaccard is higher in CV when we do this)."}}