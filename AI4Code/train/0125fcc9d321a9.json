{"cell_type":{"ba0541d5":"code","aaa134b1":"code","50d63536":"code","600c50c9":"code","34e08efc":"code","f7ffe7fc":"code","897ab3ab":"code","fbf375f9":"code","4580aec6":"code","d3fc8eab":"code","56fffe48":"code","c459f9d0":"code","c2c2e40f":"code","0f63d1eb":"code","89ebc575":"code","5c5fa44c":"code","68c70572":"code","050ad661":"code","baf057b6":"code","829b683c":"code","d92b6213":"code","325c89f3":"code","c66031b4":"code","4dfad06b":"code","b3b25f05":"code","2a159add":"code","daf534dc":"code","6b41e65a":"code","9be523dd":"code","f1590bbe":"code","a581d027":"code","5e6cb806":"code","f7bfbd64":"code","cd23cfb3":"code","4108d347":"code","25552ddb":"markdown","ff109eb0":"markdown","a68e7319":"markdown","d059f158":"markdown","0bc1a1b0":"markdown","4cb5cfb0":"markdown","0fea59b4":"markdown","5a64c9cc":"markdown","657c5301":"markdown","034d79cc":"markdown","fe4e0162":"markdown","c79b2225":"markdown","3dc83fd1":"markdown","8beb26b0":"markdown","494558b2":"markdown","5184b7c3":"markdown","3a87ae21":"markdown","b6cd5b94":"markdown","ff6d1ee4":"markdown","1677425b":"markdown","2ce8d5f9":"markdown","e83a40a0":"markdown","79627fda":"markdown","d82c23b5":"markdown","c7c6253c":"markdown","5077e1fe":"markdown","8718acf4":"markdown","b022abda":"markdown","8ddec885":"markdown","1fc71893":"markdown","75edfc8a":"markdown"},"source":{"ba0541d5":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","aaa134b1":"train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')","50d63536":"train = train.drop('Unnamed: 0', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","600c50c9":"train.head()","34e08efc":"train.shape","f7ffe7fc":"all_cols = train.columns\nfeature_cols = [col for col in train.columns if col not in ['diabetes_mellitus']]\ntarget_cols = ['diabetes_mellitus']","897ab3ab":"cat_cols = [col for col in feature_cols if train[col].dtype == 'O']\ncont_cols = [col for col in feature_cols if col not in cat_cols]","fbf375f9":"trainprofile = ProfileReport(train, minimal=True) # set minimal=True for large datasets","4580aec6":"trainprofile.to_widgets()","d3fc8eab":"sum(train.isna().sum(axis=1) > 0)","56fffe48":"sum(train.isna().sum(axis=0) > 0)","c459f9d0":"from sklearn.impute import SimpleImputer","c2c2e40f":"imputer = SimpleImputer(strategy='mean')\n\nstart_time = time.time()\ntrain[cont_cols] = imputer.fit_transform(train[cont_cols])\nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","0f63d1eb":"imputer = SimpleImputer(strategy='most_frequent')\n\nstart_time = time.time()\ntrain[cat_cols] = imputer.fit_transform(train[cat_cols])\nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","89ebc575":"train.head()","5c5fa44c":"from IPython.display import Image\nImage(filename='..\/input\/imputation-outputs\/SimpleImputer.PNG') ","68c70572":"train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ntrain = train.drop('Unnamed: 0', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","050ad661":"# Code taken from https:\/\/stackoverflow.com\/a\/43899681\/12890869\n\nfrom sklearn.preprocessing import LabelEncoder\nstart_time = time.time()\nfor col in cat_cols:\n    le = LabelEncoder()\n    fit_by = pd.Series([i for i in train[col].unique() if type(i) == str]) # gets unique values w\/o NaN\n    le.fit(fit_by) # Fit on unique values\n    # Set transformed col leaving np.NaN as they are\n    train[col] = train[col].apply(lambda x: le.transform([x])[0] if type(x) == str else x)\n    \nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","baf057b6":"train.head()","829b683c":"from sklearn.impute import KNNImputer","d92b6213":"knn_imputer = KNNImputer(n_neighbors=2)\n\nstart_time = time.time()\ntrain = knn_imputer.fit_transform(train)\nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","325c89f3":"train = pd.DataFrame(train, columns=all_cols)\ntrain.head()","c66031b4":"train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ntrain = train.drop('Unnamed: 0', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","4dfad06b":"# Code taken from https:\/\/stackoverflow.com\/a\/43899681\/12890869\n\nfrom sklearn.preprocessing import LabelEncoder\n\nstart_time = time.time()\nfor col in cat_cols:\n    le = LabelEncoder()\n    fit_by = pd.Series([i for i in train[col].unique() if type(i) == str]) # gets unique values w\/o NaN\n    le.fit(fit_by) # Fit on unique values\n    # Set transformed col leaving np.NaN as they are\n    train[col] = train[col].apply(lambda x: le.transform([x])[0] if type(x) == str else x)\n    \nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","b3b25f05":"# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\n# to be used as the estimator for iterative imputing\nfrom sklearn.ensemble import RandomForestClassifier","2a159add":"estimator = RandomForestClassifier(n_estimators=20)\nitr_imputer = IterativeImputer()\nstart_time = time.time()\ntrain = itr_imputer.fit_transform(train)\nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","daf534dc":"train = pd.DataFrame(train, columns=all_cols)\ntrain.head()","6b41e65a":"train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ntrain = train.drop('Unnamed: 0', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","9be523dd":"from sklearn.impute import MissingIndicator","f1590bbe":"miss_indicator = MissingIndicator()\nstart_time = time.time()\nX_miss = miss_indicator.fit_transform(train)\nend_time = time.time()\ntime_taken = end_time - start_time\nprint(f\"Time Taken: {time_taken}\")","a581d027":"train.shape","5e6cb806":"train = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ntrain = train.drop('Unnamed: 0', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","f7bfbd64":"# Code taken from https:\/\/stackoverflow.com\/a\/43899681\/12890869\n\nfrom sklearn.preprocessing import LabelEncoder\n\nstart_time = time.time()\nfor col in cat_cols:\n    le = LabelEncoder()\n    fit_by = pd.Series([i for i in train[col].unique() if type(i) == str]) # gets unique values w\/o NaN\n    le.fit(fit_by) # Fit on unique values\n    # Set transformed col leaving np.NaN as they are\n    train[col] = train[col].apply(lambda x: le.transform([x])[0] if type(x) == str else x)\n    \nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","cd23cfb3":"estimator = RandomForestClassifier(n_estimators=20)\nitr_imputer = IterativeImputer(add_indicator=True) # setting add_indicator=True returns missing indicators alongwith the imputed dataframe\nstart_time = time.time()\ntrain = itr_imputer.fit_transform(train)\nend_time = time.time()\ntime_taken = end_time - start_time\nprint('Time Taken: {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_taken \/\/ 3600, (time_taken % 3600) \/\/ 60, (time_taken % 3600) % 60))","4108d347":"train.shape","25552ddb":"### KNNImputer doesn't work on strings so we need to encode the strings into float or int keeping the NaN values\n#### We will be using LabelEncoder to keep things simple","ff109eb0":"# Alternatively you might use ML algorithms that handle missing values inherently\n* Catboost: [@tensorgirl's](https:\/\/www.kaggle.com\/usharengaraju) Starter Notebook\n [https:\/\/www.kaggle.com\/usharengaraju\/widsdatathon2021-catboost-starter](https:\/\/www.kaggle.com\/usharengaraju\/widsdatathon2021-catboost-starter)\n* LightGBM: [@hamzafarooq50's](https:\/\/www.kaggle.com\/hamzafarooq50) Notebook\n[https:\/\/www.kaggle.com\/hamzafarooq50\/lightgbm-encoding-0-85883-score](https:\/\/www.kaggle.com\/hamzafarooq50\/lightgbm-encoding-0-85883-score)\n* XgBoost","a68e7319":"### Imputing Categorical Variables","d059f158":"<div id='uni'> <\/div>\n\n# Univariate Imputation","0bc1a1b0":"#### Notice that `ethnicity`, `gender`, `hospital_admit_source`, `icu_admit_source`, `icu_stay_type` and `icu_type` have all been encoded","4cb5cfb0":"### 180 columns + 160 missing indicator columns","0fea59b4":"<div id='iterative'> <\/div>\n\n# Iterative Imputer","5a64c9cc":"* [KNN Imputer](#knn) <br>\n* [Iterative Imputer](#iterative) <br>","657c5301":"### Imputing Continuous Variables","034d79cc":"![](https:\/\/stefvanbuuren.name\/fimd\/fig\/ch01-miflow-1.png)","fe4e0162":"### KNNImputer shouldn't be used with large datasets as it takes a lot of time<\/h3>","c79b2225":"## Dropping Columns","3dc83fd1":"### One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. `SimpleImputer`). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. `IterativeImputer`).","8beb26b0":"### Most of the times the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. Therefore, missingness itself might be a good indicator to classify the labels  ","494558b2":"<link rel=\"preconnect\" href=\"https:\/\/fonts.gstatic.com\">\n<link href=\"https:\/\/fonts.googleapis.com\/css2?family=Itim&family=Roboto&display=swap\" rel=\"stylesheet\">\n<h1 style=\"text-align:center; color:blue; font-family: 'Itim', cursive;\">Handling Missing Values with Different Imputation Techniques <\/h1>","5184b7c3":"<div id='multi'> <\/div>\n\n# Multivariate Imputation","3a87ae21":"### If we drop all the rows with any missing value we are barely left with any row","b6cd5b94":"<h2>If you learnt something new, consider <span style='color:green'>UPVOTING<\/span> my notebook<\/h2>","ff6d1ee4":"<div id=\"univsmulti\"> <\/div>\n\n# Univariate vs. Multivariate Imputation","1677425b":"### A strategy for imputing missing values by modeling each feature with missing values as a function of other features","2ce8d5f9":"<div id='knn'> <\/div>\n\n# KNN Imputer","e83a40a0":"## Dropping Rows","79627fda":"### Like KNNImputer, Iterative Imputer also doesn't work on strings","d82c23b5":"<div id=\"drop\"> <\/div>\n\n# Dropping Rows and Columns with Missing Values","c7c6253c":"### If we drop all columns with missing values we lose 160 columns","5077e1fe":"### We can impute missing values with a provided constant value, or using the statistics (mean, median or most frequent) of each column.","8718acf4":"<div id='miss'> <\/div>\n\n# Missing Indicator","b022abda":"### This isn't an imputation technique, but this might be the first thing that comes in mind to deal with missing values","8ddec885":"<div id=\"basiceda\"> <\/div>\n\n# Basic EDA","1fc71893":"## Table of contents\n\n* [Basic EDA](#basiceda) <br>\n* [Dropping Rows and Columns](#drop) <br>\n* [Univariate Vs. Multivariate Imputation](#univsmulti) <br>\n    * [Univariate Imputation](#uni)<br>\n        * [Simple Imputer](#sim) <br>\n    * [Multivariate Imputation](#multi)<br>\n        * [KNN Imputer](#knn) <br>\n        * [Iterative Imputer](#iterative) <br>\n* [Missing Indicator](#miss) <br>\n* [Missing Indicator + Iterative Imputer](#miss+itr) <br>","75edfc8a":"<div id='miss+itr'> <\/div>\n\n# Missing Indicator + Iterative Imputer"}}