{"cell_type":{"93e2f865":"code","e849e845":"code","baf14b01":"code","d1c78f03":"code","dddfb705":"code","5eb234e2":"code","5f25412e":"code","e6abd5a5":"code","b11dd03a":"code","311d3651":"code","488dd1a2":"code","16e7180c":"code","f910db65":"code","72a1dc76":"code","b6842304":"code","7303596c":"code","79aac0d8":"code","ef8f5143":"markdown","93b21651":"markdown","17ffaf6a":"markdown","fefc2e54":"markdown","970ed847":"markdown","64135562":"markdown","e3193d85":"markdown","fba63395":"markdown","7cf16425":"markdown","1183fcd0":"markdown","1c92338f":"markdown","4b5df7eb":"markdown","5fa1240f":"markdown","b7ae18aa":"markdown","1846091b":"markdown","f834081f":"markdown","18dd4ecc":"markdown","ba057c10":"markdown","e1ceab7f":"markdown","ce9b21f5":"markdown","0a6873ce":"markdown","e21b42bf":"markdown","303c381d":"markdown","0fa753ae":"markdown","6178a1c2":"markdown","51e817b9":"markdown","7fb19cdf":"markdown","4796c8af":"markdown"},"source":{"93e2f865":"import numpy as np\nimport pandas as pd\nfrom statsmodels.stats.proportion import proportions_ztest\nimport os\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec\n\nfrom matplotlib.offsetbox import AnchoredText\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom IPython.display import Image","e849e845":"data = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv', low_memory=False)\n\nquestions = data.iloc[0, :].T\ndata = data.iloc[1:, :].dropna(how=\"all\")\n\n# removing outliers and keeping only classes of interest\ndata = data[(data['Time from Start to Finish (seconds)'].astype(float)  >= 123.75) & (data['Time from Start to Finish (seconds)'].astype(float) <= 13895.0)]\ndata = data[(data['Q5'] == 'Data Scientist') | (data['Q5'] == 'Data Engineer') | (data['Q5'] == 'Machine Learning Engineer')]","baf14b01":"#thanks to @subinium\nfig, ax = plt.subplots(1,1, figsize=(15, 6))\ndata_q5 = data['Q5'].value_counts()\nax.bar(data_q5.index, data_q5, width=0.55, \n       edgecolor='gray', color='lightblue',\n       linewidth=0.7)\n\nfor i in data_q5.index:\n    ax.annotate(f\"{data_q5[i]}\", \n                   xy=(i, data_q5[i] + 100),\n                   va = 'center', ha='center',fontweight='light', fontfamily='serif',\n                   color='black')\n\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.set_ylim(0, 4200)    \nax.set_xticklabels(data_q5.index, fontfamily='serif')\nax.set_yticklabels(np.arange(0, 4001, 500),fontfamily='serif')\nfig.text(0.1, 0.95, 'Current Role', fontsize=16, fontweight='bold', fontfamily='serif') \nplt.show()","d1c78f03":"Image(\"..\/input\/assets\/table1_v4.png\")","dddfb705":"Image(\"..\/input\/assets\/table2_v4.png\")","5eb234e2":"Image(\"..\/input\/assets\/table3_v4.png\")","5f25412e":"Image(\"..\/input\/assets\/table4_v4.png\")","e6abd5a5":"Image(\"..\/input\/assets\/Kaggle Tree - Midpoint.png\", width = 3000, height = 1500)","b11dd03a":"Image(\"..\/input\/assets\/table5_v4.png\")","311d3651":"Image(\"..\/input\/assets\/table6_v4.png\")","488dd1a2":"Image(\"..\/input\/assets\/Kaggle Tree - Final - v2.png\")","16e7180c":"Image(\"..\/input\/assets\/table7_v4.png\")","f910db65":"data = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv', low_memory=False)\n\nquestions = data.iloc[0, :].T\ndata = data.iloc[1:, :].dropna(how=\"all\")","72a1dc76":"# it is general best practice in survey analysis to remove outliers in survey duration. As a rule of thumb, we select responses between the 5th and 95th percentile.\n# let's compute these first\ndata['Time from Start to Finish (seconds)'].astype(float).quantile([.05, .95])","b6842304":"# removing outliers and keeping only classes of interest\ndata = data[(data['Time from Start to Finish (seconds)'].astype(float)  >= 123.75) & (data['Time from Start to Finish (seconds)'].astype(float) <= 13895.0)]\ndata = data[(data['Q5'] == 'Data Scientist') | (data['Q5'] == 'Data Engineer') | (data['Q5'] == 'Machine Learning Engineer')]","7303596c":"# here we compute two-proportions z-test for multi-select question types\nsubsets = [['Data Engineer', 'Data Scientist'],\n           ['Data Scientist', 'Machine Learning Engineer'],\n           ['Machine Learning Engineer', 'Data Engineer']]\nquestions = ['Q7_Part_1', 'Q7_Part_2', 'Q7_Part_3',\n             'Q7_Part_4', 'Q7_Part_5', 'Q7_Part_6',\n             'Q7_Part_7', 'Q7_Part_8' ,'Q7_Part_9',\n             'Q7_Part_10', 'Q7_Part_11', 'Q7_Part_12',\n             'Q7_OTHER', 'Q9_Part_1', 'Q9_Part_2', \n             'Q9_Part_3', 'Q9_Part_4', 'Q9_Part_5', \n             'Q9_Part_6', 'Q9_Part_7', 'Q9_Part_8',\n             'Q9_Part_9', 'Q9_Part_10', 'Q9_Part_11', \n             'Q9_OTHER', 'Q12_Part_1', 'Q12_Part_2', \n             'Q12_Part_3', 'Q12_OTHER', 'Q14_Part_1',\n             'Q14_Part_2', 'Q14_Part_3', 'Q14_Part_4', \n             'Q14_Part_5', 'Q14_Part_6', 'Q14_Part_7', \n             'Q14_Part_8', 'Q14_Part_9', 'Q14_Part_10', \n             'Q14_Part_11', 'Q14_OTHER', 'Q17_Part_1', \n             'Q17_Part_2', 'Q17_Part_3', 'Q17_Part_4', \n             'Q17_Part_5', 'Q17_Part_6', 'Q17_Part_7', \n             'Q17_Part_8', 'Q17_Part_9', 'Q17_Part_10', \n             'Q17_Part_11', 'Q17_OTHER', 'Q27_A_Part_1', \n             'Q27_A_Part_2', 'Q27_A_Part_3', 'Q27_A_Part_4', \n             'Q27_A_Part_5', 'Q27_A_Part_6', 'Q27_A_Part_7', \n             'Q27_A_Part_8' ,'Q27_A_Part_9', 'Q27_A_Part_10', \n             'Q27_A_Part_11', 'Q27_A_OTHER', 'Q28_A_Part_1', \n             'Q28_A_Part_2', 'Q28_A_Part_3', 'Q28_A_Part_4', \n             'Q28_A_Part_5', 'Q28_A_Part_6', 'Q28_A_Part_7', \n             'Q28_A_Part_8' ,'Q28_A_Part_9', 'Q28_A_Part_10', \n             'Q28_A_OTHER', 'Q28_B_Part_1', \n             'Q28_B_Part_2', 'Q28_B_Part_3', 'Q28_B_Part_4', \n             'Q28_B_Part_5', 'Q28_B_Part_6', 'Q28_B_Part_7', \n             'Q28_B_Part_8' ,'Q28_B_Part_9', 'Q28_B_Part_10', \n             'Q28_B_OTHER']\nquestionSets = [['Q7_Part_1', 'Q7_Part_2', 'Q7_Part_3',\n                 'Q7_Part_4', 'Q7_Part_5', 'Q7_Part_6',\n                 'Q7_Part_7', 'Q7_Part_8' ,'Q7_Part_9',\n                 'Q7_Part_10', 'Q7_Part_11', 'Q7_Part_12',\n                 'Q7_OTHER'],\n               ['Q9_Part_1', 'Q9_Part_2', 'Q9_Part_3',\n                'Q9_Part_4', 'Q9_Part_5', 'Q9_Part_6',\n                'Q9_Part_7', 'Q9_Part_8' ,'Q9_Part_9',\n                'Q9_Part_10', 'Q9_Part_11', 'Q9_OTHER'],\n                ['Q12_Part_1', 'Q12_Part_2', 'Q12_Part_3', \n                 'Q12_OTHER'],\n               ['Q14_Part_1', 'Q14_Part_2', 'Q14_Part_3',\n                'Q14_Part_4', 'Q14_Part_5', 'Q14_Part_6',\n                'Q14_Part_7', 'Q14_Part_8' ,'Q14_Part_9',\n                'Q14_Part_10', 'Q14_Part_11', 'Q14_OTHER'],\n               ['Q17_Part_1', 'Q17_Part_2', 'Q17_Part_3',\n                'Q17_Part_4', 'Q17_Part_5', 'Q17_Part_6',\n                'Q17_Part_7', 'Q17_Part_8' ,'Q17_Part_9',\n                'Q17_Part_10', 'Q17_Part_11', 'Q17_OTHER'],\n                ['Q27_A_Part_1', 'Q27_A_Part_2', 'Q27_A_Part_3', \n                 'Q27_A_Part_4', 'Q27_A_Part_5', 'Q27_A_Part_6', \n                 'Q27_A_Part_7', 'Q27_A_Part_8' ,'Q27_A_Part_9', \n                 'Q27_A_Part_10', 'Q27_A_Part_11', 'Q27_A_OTHER'],\n               ['Q28_A_Part_1', 'Q28_A_Part_2', 'Q28_A_Part_3',\n                'Q28_A_Part_4', 'Q28_A_Part_5', 'Q28_A_Part_6',\n                'Q28_A_Part_7', 'Q28_A_Part_8' ,'Q28_A_Part_9',\n                'Q28_A_Part_10', 'Q28_A_OTHER'],\n               ['Q28_B_Part_1', 'Q28_B_Part_2', 'Q28_B_Part_3', \n                'Q28_B_Part_4', 'Q28_B_Part_5', 'Q28_B_Part_6', \n                'Q28_B_Part_7', 'Q28_B_Part_8' ,'Q28_B_Part_9', \n                'Q28_B_Part_10', 'Q28_B_OTHER']]\nresults = []\n\n# this should be fp of course...but let's indulge in nesting here, because we can ;)\nfor i in range(0, len(questions)):\n    for j in range(0, len(subsets)):\n        for k in range(0, len(questionSets)):\n            if questions[i] in questionSets[k]:\n                subsetA = subsets[j][0]\n                subsetB = subsets[j][1]\n                question = questions[i]\n                questionSet = questionSets[k]\n\n                countA = data[data['Q5'] == subsetA][question].count()\n                countB = data[data['Q5'] == subsetB][question].count()\n                count = np.array([countA, countB])\n                \n                # denominator should be all respondents that have at least one selection\n                nobsA = data[data['Q5'] == subsetA][questionSet].dropna(how='all').shape[0]\n                nobsB = data[data['Q5'] == subsetB][questionSet].dropna(how='all').shape[0]\n                nobs = np.array([nobsA, nobsB])\n\n                value = .05\n                stat, pval = proportions_ztest(count, nobs, value)\n\n                print('.', end = '')\n\n                results.append([question, subsetA, subsetB, '{0:0.3f}'.format(countA\/nobsA), '{0:0.3f}'.format(countB\/nobsB), \n                                '{0:0.3f}'.format(pval), countA, nobsA,  countB, nobsB])\n            else:\n                'do nothing'\nresults","79aac0d8":"# here we compute two-proportions z-test for single select question types\nsubsets = [['Data Engineer', 'Data Scientist'],\n           ['Data Scientist', 'Machine Learning Engineer'],\n           ['Machine Learning Engineer', 'Data Engineer']]\n\nquestions = ['Q2', 'Q4']\n\nresults = []\n\n# this should also be fp of course...but let's indulge in nesting here, because we can ;)\nfor i in range(0, len(questions)):\n    for j in range(0, len(subsets)):\n        for k in range(0, data[questions[i]].nunique()-1):\n            subsetA = subsets[j][0]\n            subsetB = subsets[j][1]\n            question = questions[i]\n            answer = data[question].value_counts().index[k]\n            \n            if data[data['Q5'] == subsetA][question].nunique() > k:\n                countA = data[data['Q5'] == subsetA][question].value_counts()[k]\n            else:\n                countA = 0.0\n            \n            if data[data['Q5'] == subsetB][question].nunique() > k:\n                countB = data[data['Q5'] == subsetB][question].value_counts()[k]\n            else:\n                countB = 0.0\n                \n            count = np.array([countA, countB])\n\n            # denominator should be all respondents that have at least one selection\n            nobsA = data[data['Q5'] == subsetA][question].dropna(how='all').shape[0]\n            nobsB = data[data['Q5'] == subsetB][question].dropna(how='all').shape[0]\n            nobs = np.array([nobsA, nobsB])\n\n            value = .05\n            stat, pval = proportions_ztest(count, nobs, value)\n\n            print('.', end = '')\n\n            results.append([question, answer, subsetA, subsetB, '{0:0.3f}'.format(countA\/nobsA), '{0:0.3f}'.format(countB\/nobsB), \n                            '{0:0.3f}'.format(pval), countA, nobsA,  countB, nobsB])\nresults","ef8f5143":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nWell, besides the sample being one very educated group with the largest proportion being masters degrees for all groups, and there are some differences between groups, there are no differences with meaningful implication.  We can see slightly more bachelor degrees in Group C compared with B, and more Doctorates in Group B compared with A, but we are looking for a distinction where one group differentiates from both other groups so we have a unique identifier that may help us paint the picture of who the group is.\n<\/p>\n","93b21651":"<p style=\"font-family:Times New Roman; font-size:16px;\">\n    <i>Do you like cheese?<\/i>\n<\/p>\n<table style=\"font-family:Times New Roman; font-size:16px;\">\n  <tr style=\"background-color:white; border-top: 1px solid black; border-bottom: 1px solid black;\">\n    <th>Answer<\/th>\n    <th>a<\/th>\n    <th>b<\/th>\n    <th>c<\/th>\n  <\/tr>\n  <tr>\n    <td style=\"text-align:left\">Yes<\/td>\n    <td style=\"width:100px\">44.8%<\/td>\n    <td style=\"width:100px\">41.9%<\/td>\n    <td style=\"width:100px\">66.2%<\/td>\n  <\/tr>\n<\/table>","17ffaf6a":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nObviously the survey did not include questions about food preferences, this is just a guide to prepare you for the format.  What we need to add is the two-proportions z-test results, which will tell us when a difference in proportion is actually statistically significant.\n<\/p>","fefc2e54":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo the data does paint a picture. Did you solve it?\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\n <em>Group A is Data Engineer, Group B is Data Scientist, and Group C is Machine Learning Engineer<\/em>. Did you guess right? Much of the distinction between data scientist and data engineer can be confirmed, and the overindexing of deep learning algorithms and associated tools identifies the machine learning engineer.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nBut we note lots of commonalities, and as we\u2019ve seen in the tables, there are practitioners of everything in every class looked at along our investigative trail. There were more classes without significant differentiation than classes with. Some of that might be sample issues, but overall it demonstrates the overlap between roles and that these roles are specializations within a sport where everyone knows a lot about each position.\n<\/p>\n","970ed847":"![image.png](attachment:image.png)","64135562":"# References\n[Link to Google Drawing for our custom tree](https:\/\/docs.google.com\/drawings\/d\/1JZcTw50fTpd-MqdreSzFlwKIy-MPO5tWQ-6G_K-hB44\/edit?usp=sharing \"Link to Google Drawing for our custom tree\")\n\n[Link to Google Spreadsheet for our custom data tables](https:\/\/docs.google.com\/spreadsheets\/d\/1t47VvKadiJb76THT7Hu0ugFJVkkeC-J0i4QzR6A6Q5s\/edit?usp=sharing)","e3193d85":"<div style=\"background:white; border-radius: 5px\">\n<h1 style=\"color: red; font-family:Impact; text-align:center;\">The data scientist, the data engineer, and the machine learning engineer<\/h1>\n<p><\/p>\n<\/div>","fba63395":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nWell now we have some interesting results.  Many more practitioners in Group B use RStudio when compared to both other groups, almost two fold Group A and three times Group C.  And Group A has a greater proportion who use Notepad++.  We also note that Group B is also more likely to use Jupyter, but the actual difference is not as interesting, all groups use Jupyter, it is the most used integrated environment for all.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nWhile it would be tempting to look at other differences within the table, none of those differences are statistically significant, in that we can\u2019t reject the hypothesis that the difference between them is 0.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nFormulating your own hypotheses as to who is who already?  Let\u2019s continue.\n<\/p>\n","7cf16425":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nThis question is multi-select and we can quickly see most practitioners use a few programming languages regularly.  The average number of languages used regularly for Group A is 2.9, 2.6 for Group B, Group C averages 2.5 languages, so the most diverse is Group A.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nWithin the languages used we do see a host of significant differences.  Let\u2019s start with the biggest significant differences. Over two thirds of Group A use SQL regularly, while only half of Group B do and a little more than a third of practitioners in Group C.  Group A also has more Java users than either group.  Group C on the other hand has many more practitioners who use C++ compared with their peers in Group A and B, slightly over 1 in 4 that do so.  Likely you are starting to formulate some guesses at this point, we\u2019ve learned a lot.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nLots of clues are now accumulating, and like any good detective we need to start putting this together.  You\u2019ve seen this in tons of movies, the detective wall where we begin to narrow in on the right suspect.\n<\/p>\n\n","1183fcd0":"![image.png](attachment:image.png)","1c92338f":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nIn terms of cloud computing Amazon Elastic Compute Cloud leads the way for all groups with no significant distinction for any group from the others.  Lambda within Amazon Web Services is common as well but Group A has significantly more practitioners than Group B or Group C.  The only other clue is that more practitioners in Group B use something else, but it is only 3.4% of those practitioners overall. Now let's take a look at the algorithms our practitioners use.\n<\/p>","4b5df7eb":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nOh boy, have we found our smoking gun?! With this new clue table things start to get very dynamic.  Looking at algorithm activity, Group C displays some notable differences.  First, we must acknowledge that there are practitioners for every algorithm in each column, generating the question as to whether there are more similarities than differences between the groups.  However given our pursuit is to identify the role of each group, we will focus on differences.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nIn terms of algorithms we note Group A averages 2.6 algorithms per practitioner, Group B 3.2 and Group C using the most variety averaging 3.5 algorithms per user.  This is the reverse of the diversity trend we saw with programming languages, with Group A using the most and Group C using the fewest.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nWe also note that Group C has significantly more practitioners using neural networks and deep learning models, which is what is increasing the number of algorithms this group uses.  And while regression, decision tree, and gradient boost are commonly practiced in all groups we see statistically significant higher proportions for these within Group B.  The only unique difference for Group A is the number of practitioners that don\u2019t use any algorithms.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo let\u2019s look at our detective board and see how the clues summarize based on our 6 questions and 61 classes we reviewed.\n<\/p>\n","5fa1240f":"![image.png](attachment:image.png)","b7ae18aa":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nBut do you really know them that well? With a focus in this survey on skill and tool usage, let\u2019s see if the differences we find in the survey data align with the expected differences the \u201cinterweb\u201d tells us.\nTo make it more interesting, let's pretend we\u2019ve been given a somewhat random set of clues, consisting in the least sexy and yet most efficient of the visualizations, the table! First let\u2019s agree on something, if an analyst is stranded on a desert island and can only bring one visualization with them, the table is probably it. Now let\u2019s consider an example before starting our investigation. We will present the data in a table, with columns labelled A, B, and C.  Here\u2019s our example:\n<\/p>","1846091b":"![image.png](attachment:image.png)","f834081f":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nWe\u2019ll start with gender.  We note right away that each segment is skewed very male, a standard issue in the STEM professions that society is making some headway to rectify.  The survey data lines up with generally available data on female representation. <a href=\"https:\/\/towardsdatascience.com\/are-women-crushing-the-gender-gap-in-data-science-and-related-fields-3ab7ae299b4b\" target=\"_blank\">https:\/\/towardsdatascience.com\/are-women-crushing-the-gender-gap-in-data-science-and-related-fields-3ab7ae299b4b<\/a>\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nLooking at the differences between the groups, we note that the gender distribution is fairly similar across all three groups. There is a statistically significant difference in that Group C has more males than Group B, but while this gap is statistically significant the gap itself is rather small and does not drive a significant implication.  If we filled three rooms with 1000 members of each group, it is unlikely anyone would observe any gender difference between them.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo let\u2019s move on to see if we can find data more differentiating and solve our riddle.\n<\/p>\n","18dd4ecc":"## Appendix\nWe saved the code for the end in order not to spoil our investigation.","ba057c10":"![image.png](attachment:image.png)","e1ceab7f":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nSurvey data is a small but rich data set in this big data world, often filled with insight that comes directly from participants, insights that are much harder to infer from larger, busier, fast moving big data sets. It is a historical approach that comes from a time of data scarcity, when enterprise and organizations relied on anecdotes and instincts, with only the occasional data-driven insight to guide them.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nSurvey data is a sample, meant to represent some overall population and we use its results to extrapolate a general model of that population that can be useful to guide a variety of decisions.  We slice and dice the data to extract down to smaller and smaller segments within our sample, as we look for differences, test our hypotheses and find our aha moment.  From the differences we extract, better decisions are made about policy, business strategy, etc.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nAnd as we slice and dice, our data set becomes smaller, to the point where we are unsure if the differences between groups we find are indeed statistically relevant. The two-proportions z-test for statistical significance becomes a fundamental tool to separate meaningful differences from the mere hazard of our small segment samples. A good summary of the approach can be found <a href=\"https:\/\/stattrek.com\/hypothesis-test\/difference-in-proportions.aspx\" target=\"_blank\">here<\/a>.\n<\/p>\n\n<h1 style=\"font-family:Times New Roman; text-align:center;\">And Now For Today's Story<\/h1>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nWithin the data science community there is a lot of literature describing the differences between data scientists, data engineers, and machine learning engineers.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px; text-align:center; \">\n<a href=\"https:\/\/sennalabs.com\/en\/blogs\/data-engineer-vs-machine-learning-engineer-vs-data-science\" target=\"_blank\">https:\/\/sennalabs.com\/en\/blogs\/data-engineer-vs-machine-learning-engineer-vs-data-science<\/a>\n<\/p>","ce9b21f5":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nIf we look ahead to the future we see next to no differentiation in terms of the tools each practitioner is looking to add to their tool kit.  So the overlap is likely to continue in terms of knowledge, while differentiation in role will likely also continue as companies scale their data factories. We can conclude from this data that to keep practitioners satisfied an environment that encourages cross-pollination and collaboration will create the best teams. In that regard, solution providers in the space should make sure each personae is represented in the overall value any solution offers, as team adoption will drive the success of these solutions.\n<\/p>","0a6873ce":"![image.png](attachment:image.png)","e21b42bf":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nBut since we are data scientists, we\u2019ll use a comparable visual to try and map out our clues and figure out the identity of each group.\n<\/p>","303c381d":"<p style=\"font-family:Times New Roman; font-size:16px;\">\n    <i>Do you like cheese?<\/i>\n<\/p>\n<table style=\"font-family:Times New Roman; font-size:16px;\">\n  <tr style=\"background-color:white; border-top: 1px solid black; border-bottom: 1px solid black;\">\n    <th>Answer<\/th>\n    <th>a<\/th>\n    <th>b<\/th>\n    <th>c<\/th>\n  <\/tr>\n  <tr>\n    <td style=\"text-align:left\">Yes<\/td>\n    <td style=\"width:100px\">44.8%<\/td>\n    <td style=\"width:100px\">41.9%<\/td>\n    <td style=\"width:100px\">66.2% a,b<\/td>\n  <\/tr>\n<\/table>","0fa753ae":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo we can see here that while there are some statistically significant differences in proportion of users between Jupyter and Python usage, they are less differentiating than the differences on the other side of our chart.  You are more likely to find practitioners using these tools than other groups if you randomly selected some practitioners for interrogation.  Do you have enough clues already?\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nLet\u2019s go on and see how our next table of clues lines up and if we can paint some clues that solve our mystery.\n<\/p>\n","6178a1c2":"<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo how do we interpret this?  Group A and B like cheese equally, statistically there is no difference.  Group C however is statistically different from both A and B, and loves cheese more than both.  That is noted by a and b subscript notations after the proportion.  Since it is a two way test, it means that both a and b are statistically different from C, but I\u2019ve put the notation on the largest proportion for that class to keep the table as clean as possible.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nSo to interpret the above table in terms of our detective work, if there was a theory floating around out there that Data Engineers loved cheese more than other members of the Data Science community, we would start to edge towards the conclusion that Group C might be Data Engineers.\n<\/p>\n\n<p style=\"font-family:Times New Roman; font-size:18px;\">\nNow let\u2019s look at the real data and see if you can guess which is which?\n<\/p>\n","51e817b9":"<h1 style=\"font-family:Times New Roman; text-align:center;\">Looking Ahead<\/h1>","7fb19cdf":"![image.png](attachment:image.png)","4796c8af":"![image.png](attachment:image.png)"}}