{"cell_type":{"78d8ef61":"code","129e2106":"code","ef03fd3c":"code","4109ee24":"code","bf1c6721":"code","ae510ea2":"code","8b217c66":"code","4e411810":"code","6e010685":"code","ade0d96e":"code","74e2dabe":"code","045f2e9b":"code","a539aa66":"code","893b2a00":"code","17fd0899":"code","9300dbb8":"code","65a816bd":"code","2799444c":"code","ca65037d":"code","7abcae93":"code","86ca5e15":"code","c2a64aed":"code","fbf37c36":"code","546a78e7":"code","364a49b3":"code","f0db2732":"code","e3e91ca9":"code","0d47dea8":"code","b35c5b51":"code","c4873c60":"code","08bd9ac9":"code","6dd016b7":"code","3fedc719":"code","3061fccb":"code","a052a706":"code","e0dabf27":"code","0da8ac4c":"code","5cb9e918":"code","8508741e":"code","da082d4a":"code","a41ad115":"code","980b8335":"code","fc5b0070":"code","8946704f":"code","ea5b9a02":"code","45ad59c4":"code","b06e76f7":"code","2a652fbd":"code","541285cb":"code","fe4c59f6":"code","46ffcda3":"code","83e08e2e":"code","861111e3":"code","2cc12b5e":"code","25c8dbf4":"code","fa0995e0":"code","914bea82":"code","2f3f47ef":"code","36a7fdd9":"code","e3576ea3":"code","e4e4e2b2":"code","23beb27c":"code","9f55beb2":"code","fe7407c0":"code","e52c8f2e":"code","c9b613a9":"code","4df14b03":"code","51d6cdac":"code","83048c10":"code","b394939c":"code","5b0a8f67":"code","b9863798":"code","465591cb":"code","d8c65d44":"code","e879e657":"code","89c31bdb":"code","abbb3f88":"code","f6cb5a60":"code","a3ed49e9":"code","1bc4a71f":"code","02bc536d":"code","5068731a":"code","dea54031":"code","89ce5222":"code","899fcd34":"code","c1f8de4d":"code","ac69cf8e":"code","abaa3e18":"code","423ad7d6":"code","b1e03123":"code","dc99223a":"code","d2597598":"code","0ac4bd81":"code","a7d2215e":"code","d5a092ea":"code","8283676f":"code","d7a95484":"code","73e4d28d":"code","055c14f5":"code","58cc0943":"code","3a16f868":"code","3062b2d6":"code","a81a9c07":"code","a261d9ab":"code","75a44223":"code","26fe9c50":"code","3bab183c":"code","03043811":"code","bcef9a30":"code","30d0a3ca":"code","d8f6a526":"code","06fb833a":"code","2b6a1f28":"code","7c2d8a9f":"code","9e561146":"code","18d72a25":"code","adef39b6":"code","a77f72f8":"code","cb4f6104":"code","c3f0e751":"code","9cd62394":"code","d6626e78":"code","606ca8c5":"code","79a26007":"code","454104ec":"code","363262b3":"code","36e9bf7e":"code","7fd4acdd":"code","016211e3":"code","680c801b":"code","47f6ec1f":"code","277867fc":"code","af861e02":"code","a3a0bb20":"code","0f8f51c3":"code","fe52fece":"code","f3a8bc10":"code","7d20e23d":"code","9d68c00c":"code","71fd0f13":"code","92bbb5c1":"code","4add9bb5":"code","a6887f8e":"code","be206646":"code","5d7f5ee2":"code","d69f9b32":"code","aa726851":"code","5aed8dc7":"code","c2d239ca":"code","38eb3890":"code","037bc6c6":"markdown","fb335093":"markdown","fab2fa8c":"markdown","64a6df54":"markdown","65d1eb15":"markdown","4a46d827":"markdown","3d9f43ac":"markdown","f196491b":"markdown","744fe5f4":"markdown","4ee26751":"markdown","c82e3843":"markdown","4ff366b6":"markdown","125ede90":"markdown","a467286d":"markdown","cbf2d3fb":"markdown","29db2778":"markdown","d8c224a2":"markdown","efd7da7d":"markdown","d90e6b62":"markdown","e8a01b02":"markdown","87bfd776":"markdown","02fe663b":"markdown","d10bb430":"markdown","ff9c4261":"markdown","8735b3eb":"markdown","6897b4e8":"markdown","03822c1e":"markdown","c83738f5":"markdown","de36885c":"markdown","42891155":"markdown","518ff459":"markdown","8c082b51":"markdown","5b2c033b":"markdown","40357607":"markdown","901a3a06":"markdown","42f811bd":"markdown","a86aa9f4":"markdown","5d82a0ca":"markdown","0889773a":"markdown","325a67ee":"markdown","e46db230":"markdown","af484c08":"markdown","e281e76a":"markdown","21e6452a":"markdown","6700228b":"markdown","f9d632c7":"markdown","2b1a0348":"markdown","abb818e8":"markdown","3eedc7e8":"markdown","12a942bc":"markdown","96830a88":"markdown","4c451789":"markdown","3970de13":"markdown","98dca851":"markdown","101deca6":"markdown","8dbfd2fc":"markdown","f153d734":"markdown","c7ccb673":"markdown","b04f5f74":"markdown","0ba7f577":"markdown","c7960735":"markdown","746cf80d":"markdown","32919390":"markdown","326ebcd3":"markdown","ea9e0379":"markdown","e9e5f7b5":"markdown","7e687b5a":"markdown","09c8ec78":"markdown","68cb9195":"markdown","03310bc5":"markdown","d9052355":"markdown","03f3bcfb":"markdown","a82205c9":"markdown","598244f3":"markdown","cb6560a6":"markdown","0e1886d2":"markdown","5f735356":"markdown","f3fd6871":"markdown"},"source":{"78d8ef61":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.express as px\nfrom IPython.display import Image\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom imblearn import over_sampling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, RepeatedStratifiedKFold\nfrom scipy.stats import uniform\nimport pickle\nimport xgboost as xgb\n\nsns.set(rc={'figure.figsize':(16,8)})\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"dark\")\nplt.style.use(\"fivethirtyeight\")\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","129e2106":"data = pd.read_csv('\/kaggle\/input\/hr-analytics-classification\/train_LZdllcl.csv')\n\n#see the amount of data in the dataset\nprint('there are ' + str(len(data)) + ' rows of data in the dataset')\nprint(\"Total Feature\", data.shape[1])\ndata.head()","ef03fd3c":"#see the amount of data on each target\ndata['is_promoted'].value_counts()","4109ee24":"print('1. The percentage of employees who have not received a promotion are ' + str(round((50140\/54808)*100,2)) + '%')\nprint('2. The percentage of employees who get promoted are ' + str(round((4668\/54808)*100,2)) + '%')","bf1c6721":"data.info()","ae510ea2":"# assign the categorical data into cats object\ncats = ['department','region','education','gender','recruitment_channel']\n\n# assign the numerical data into nums object\nnums = ['employee_id','no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?',\n        'avg_training_score','is_promoted']","8b217c66":"data[nums].describe()","4e411810":"data[cats].describe()","6e010685":"# assign features with missing value into data_missing object\ndata_missing = data.isnull().sum().reset_index()\ndata_missing.columns = ['feature','missing_value']\ndata_missing = data_missing[data_missing['missing_value'] > 0].reset_index(drop=True)\ndata_missing['percentage'] = (data_missing['missing_value']\/len(data))*100\ndata_missing","ade0d96e":"fig, ax = plt.subplots(figsize = (15,7))\n\nmvalue_plot = sns.barplot(x='feature', y='percentage', data=data_missing,\n           palette = sns.color_palette('Blues_d', n_colors = 13, desat = 1))\n\nax.set_ylim(0,12)\n\nfor p in mvalue_plot.patches:\n    height = p.get_height()\n    mvalue_plot.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.1,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center')\n\ntitle = '''Distribution of Missing Value'''\nfig.text(x=0.125, y=0.8, s = title, fontsize = 24, weight = 'bold')\n\ntext = '''\nThere are 2 features with missing value, 1 for numerical & 1 for non-numerical (categorical data)\n- previous_year_rating as numerical data has 7.52% (4124 rows) missing value\n- education as non-numerical data has 4.40% (2409 rows) missing value\n\nCompared to the dataset, this missing value is not too big\n'''\nfig.text(x=0.125, y=0.55, s = text, fontsize=16)\n\n#plt.savefig('fig\/Distribution of Missing Value.png')\n","74e2dabe":"# check the cardinality or unique value from categorical data \n# and assign thgem into data_cat_unique object\ndisplay(data.select_dtypes(include=['object']).columns)\nprint(data.select_dtypes(include=object).shape)\ndata_cat = data.select_dtypes(include=['object'])\ndata_cat.head(3)","045f2e9b":"data_cat_unique = data_cat.nunique().reset_index()\ndata_cat_unique.columns = ['feature', 'unique value']\ndata_cat_unique = data_cat_unique.sort_values('unique value', ascending=False).reset_index(drop=True)\ndata_cat_unique","a539aa66":"#melihat distribusi unique value dari data categorical\nfig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x = 'feature',y='unique value',data=data_cat_unique,ax=ax, \n               palette=sns.color_palette(\"Blues_d\", n_colors=5, desat=1))\n\nx = np.arange(len(data_cat_unique['feature']))\ny = data_cat_unique['unique value']\n\nfor i, v in enumerate(y):\n    ax.text(x[i]- 0.1, v+0.75, str(v), fontsize = 20, color='gray', fontweight='bold')\n    \ntitle = '''\nDistribution of Unique Value in Categorical Data\n'''\nax.text(1,28,title,horizontalalignment='left',color='black',fontsize=25,fontweight='bold')\n    \n\ntext = '''\nThere are 5 features with categorical data\n- region has 34 unique value\n- department has 9 unique value\n- education has 3 unique value\n- recruitment_channel has 3 unique value\n- gender has 2 unique value\n\n'''\nplt.text(1,23,text,horizontalalignment='left',color='black',fontsize=20,fontweight='normal',va='center')\n    \nax.set_ylim(0,35)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=0)\nplt.tight_layout\n\n#plt.savefig('fig\/Insight Awal ke-9.png')","893b2a00":"data.duplicated().sum()","17fd0899":"#assign numerical data into dat_num object\nnumerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndisplay(data.select_dtypes(include=numerics).columns)\nprint(data.select_dtypes(include=numerics).shape)\ndata_num = data.select_dtypes(include=numerics)\ndata_num.head(3)","9300dbb8":"# look at the distribution of data with boxplot\nfeatures = ['no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?','avg_training_score','is_promoted']\nplt.figure(figsize=(15, 7))\nfor i in range(0, len(features)):\n    plt.subplot(1, 8, i+1)\n    sns.boxplot(y=data[features[i]],color='green',orient='v')\n    plt.tight_layout()","65a816bd":"title = '''\nDistribution of Promoted Employees based on age\n'''\nfig = px.histogram(data, x=\"age\", title = title,\n                   color = \"is_promoted\", \n                   labels= {\"age\":\"Age\",\"is_promoted\": \"Promoted\"},\n                   nbins=35,\n                   color_discrete_sequence=px.colors.qualitative.Pastel1\n                  )\n\nfig.update_layout(yaxis_title=\" \")\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=650)\nfig.show()\n\nprint(\"The average age of the employees is \" + str(round(np.mean(data.age),0)))\nprint(\"The median age of the employees is \" + str(np.median(data.age)))","2799444c":"prob_kpi = data.groupby(['KPIs_met >80%','is_promoted'])['employee_id'].count().reset_index()\nkpi = prob_kpi.pivot_table(index='KPIs_met >80%', columns='is_promoted', \n                       values='employee_id').reset_index()\nkpi.columns = ['kpi', 'not_promoted', 'promoted']\nkpi['total'] = kpi['not_promoted']+kpi['promoted']\nkpi['probability'] = round((kpi['promoted']\/kpi['total'])*100,2)\nkpi = kpi.sort_values(['probability'], ascending=False)\nkpi","ca65037d":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='kpi', y='probability', \n                data=kpi,ax=ax1,color='coral',\n               order=kpi.sort_values('probability',ascending = False).kpi)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Key Performance Indicator'''\nfig.text(x=0.55, y=0.85, s=title, fontsize=24, weight='bold')\n\n#add description\ntext = '''\nEmployees who achieve KPIs of \nmore than 80% have a 16.91% \nchance of getting a promotion\n'''\nfig.text(x=0.55, y=0.65, s=text, fontsize=20)\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Key Performance Indicator', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['More than 80%', 'Less than 80%'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/KPI Promoted Probability.png');","7abcae93":"rating_probs = data.groupby(['previous_year_rating','is_promoted']).agg({\n    'department': ['count']\n}).reset_index()\n\nrating_probs.columns = ['previous_year_rating', 'is_promoted', \"employees\"]\n\nrating_probs = pd.pivot_table(rating_probs,\n                              index = 'previous_year_rating',\n                              columns = 'is_promoted',\n                              values = \"employees\"\n                             ).reset_index()\n\nrating_probs.columns = ['previous_year_rating', 'not_promoted', 'promoted']\nrating_probs['total_employees'] = rating_probs['not_promoted'] + rating_probs['promoted']\nrating_probs['promotion_probs'] = (rating_probs['promoted']\/(rating_probs['not_promoted']+rating_probs['promoted']))*100\nrating_probs = rating_probs.sort_values('promotion_probs', ascending=False).reset_index(drop=True)\nrating_probs","86ca5e15":"#melihat peluang yang dipromosikan berdasarkan previous_year_rating\nfig, ax = plt.subplots(figsize=(15,7))\n#g = plt.bar(x='previous_year_rating', height='promotion_probs',data=rating_probs)\n\ng = sns.barplot(x='previous_year_rating', y='promotion_probs', data=rating_probs,\n                order=rating_probs.sort_values('promotion_probs')['previous_year_rating'],\n                color = 'coral')\n\nfor p in g.patches:\n    height = p.get_height()\n    plt.text(x = p.get_x() + (p.get_width()\/2),\n           y = height + 0.5,\n           s = str('{:.2f}'.format(height)) + '%',\n           ha = 'center'\n          )\n\ntitle = '''\nPromotion by Previous Year Rating\n'''\n\ntext = '''\nEmployees who have a rating of 5 have the greatest \nchance of getting a promotion, which is 16.36% of getting a promotion\nfrom a total of 11,741 employees who get a rating of 5\n'''\n\n#plt.yticks([]) # Hide the y-axis with empty list\nplt.xlabel('Previous Year Rating (1 to 5)', fontsize = 15) # Add the x-label\nplt.ylabel('Promotion Probabilities', fontsize = 15) # Add the y-label\n\nplt.text(x = 0, y = 14, s=title,\n        fontsize = 20, weight = 'bold', alpha = .75)\nplt.text(x = 0, y = 11, s=text,\n        fontsize = 16, alpha = .85)\nplt.tight_layout();\n\n#plt.savefig('fig\/Promotion by Rating.png')","c2a64aed":"data_awards = pd.pivot_table(data,\n                    index = ['awards_won?'],\n                    columns = ['is_promoted'],\n                    aggfunc = {'is_promoted' : ['count']}).reset_index()\ndata_awards.columns = ['awards_won', 'not_promoted', 'promoted']\ndata_awards['total'] = data_awards['not_promoted']+data_awards['promoted']\ndata_awards['probability'] = round((data_awards['promoted']\/data_awards['total'])*100,2)\ndata_awards = data_awards.sort_values(['probability'], ascending=False)\ndata_awards","fbf37c36":"#melihat jumlah karyawan yang dipromosikan berdasarkan yang mendapatkan awards\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='awards_won', y='probability', \n                data=data_awards,ax=ax1,color='coral',\n               order=data_awards.sort_values('probability',ascending = False).awards_won)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Awards Won'''\nfig.text(x=0.55, y=0.8, s=title, fontsize=24, weight='bold')\n\n#add description\ntext = '''\nEmployees who have won an award \nhave a 44.02% chance of being promoted\nwhile those who have never won an award\nonly have a probability of 7.67%.\n'''\nfig.text(x=0.55, y=0.575, s=text, fontsize=20)\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('awards won', fontsize = 20, weight='bold')\nplt.ylabel('probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Awarded', 'Not Awarded'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Promotion by Awards.png');","546a78e7":"prob_los = data.groupby(['length_of_service','is_promoted'])['employee_id'].count().reset_index()\nlos = prob_los.pivot_table(index='length_of_service', columns='is_promoted', \n                       values='employee_id').reset_index()\nlos.columns = ['length_of_service', 'not_promoted', 'promoted']\nlos['promoted'] = los['promoted'].fillna(0)\nlos['total'] = los['not_promoted']+los['promoted']\nlos['probability'] = round((los['promoted']\/los['total'])*100,2)\nlos = los.sort_values(['probability'], ascending=False)\nlos.head()","364a49b3":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='length_of_service', y='probability', \n                data=los,ax=ax1,color='coral',\n               order=los.sort_values('probability',ascending = False).length_of_service)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=9,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Length of Service'''\nfig.text(x=0.38, y=0.95, s=title, fontsize=20, weight='bold')\n\n#add description\ntext = '''\nEmployees with a service length\nof 34 years have a 25% chance of being promoted\n'''\nfig.text(x=0.38, y=0.83, s=text, fontsize=16)\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Length of Service', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\n\nplt.tight_layout\n#plt.savefig('fig\/Promotion by Length of Service.png');","f0db2732":"nof_probs = data.groupby(['no_of_trainings', 'is_promoted']).agg({\n    'department': ['count']\n}).reset_index()\n\nnof_probs.columns = ['no_of_trainings', 'is_promoted','employees']\n\nnof_probs = pd.pivot_table(nof_probs,\n                             index = 'no_of_trainings',\n                             columns = 'is_promoted',\n                             values = 'employees').reset_index()\n\nnof_probs.columns = ['no_of_trainings','not_promoted','promoted']\nnof_probs['total_employees'] = nof_probs['not_promoted'] + nof_probs['promoted']\nnof_probs['promotion_probs'] = (nof_probs['promoted']\/(nof_probs['not_promoted']+nof_probs['promoted']))*100\nnof_probs = nof_probs.sort_values('promotion_probs', ascending=False).reset_index(drop=True)\nnof_probs","e3e91ca9":"# due to there are no value in some rows in promoted column, just drop it\nnof_probs = nof_probs.drop(nof_probs[nof_probs['no_of_trainings']>= 7].index)\nnof_probs","0d47dea8":"#melihat peluang yang dipromosikan berdasarkan no_of_trainings\nfig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x='no_of_trainings', y='promotion_probs', data=nof_probs,\n                order=nof_probs.sort_values('promotion_probs')['no_of_trainings'],\n                color='coral')\n\nfor p in g.patches:\n    height = p.get_height()\n    plt.text(x = p.get_x() + (p.get_width()\/2),\n           y = height + 0.1,\n           s = str('{:.2f}'.format(height)) + '%',\n           ha = 'center'\n          )\n\ntitle = '''\nPromotion by Number of Trainings'\n'''\n\ntext = '''\nMost employees still attend only 1 training, \nso that the greatest probability is in that category (8.81%)\n'''\n\n#plt.yticks([]) # Hide the y-axis with empty list\nplt.xlabel('Number of Trainings', fontsize = 15) # Add the x-label\nplt.ylabel('Promotion Probabilities', fontsize = 15) # Add the y-label\n\nplt.text(x = -0.4, y = 8, s=title,\n        fontsize = 20, weight = 'bold', alpha = .75)\nplt.text(x = -0.4, y = 7, s=text,\n        fontsize = 14, alpha = .85)\nplt.tight_layout();\n\n#plt.savefig('fig\/Promotion by no of trainings.png')","b35c5b51":"g = sns.displot(data=data, x=\"avg_training_score\", \n            hue='is_promoted', kde=True, height=8.27, \n            aspect=11.7\/8.27)\n\ntitle='''\nDistribution of Average Training Scores\n'''\n\nplt.text(x=5, y=2000, s=title, fontsize=16, weight='bold', ha='left')\n\ntext='''\nWhen compared with employees who are not\npromoted, it can be seen that employees with\nan average training score above 90 have\na greater chance of being promoted.\n'''\n\nplt.text(x=5, y=1200, s=text, fontsize=14, ha='left')\n\nplt.xlim(0, 100)\n\n#make red line\nplt.axvline(x=91, ls = '--', color = 'red')\nplt.axvline(x=99, ls = '--', color = 'red')\nplt.annotate(xy=(91,2800), xytext=(86.5+5,3000+5), s='''Score=90''',\n             fontsize=12, weight='bold',\n            arrowprops=dict(ec='black', # color arrow\n                            arrowstyle='simple', #style of arrow\n                           ))\nplt.xlabel('Average Training Score', fontsize = 20, weight='bold')\nplt.ylabel('Number of Promoted Employees', fontsize = 20, weight='bold')\nplt.tight_layout();\n\n#plt.savefig('fig\/Insight Avg Training 2.png');","c4873c60":"prob_avg = data.groupby(['avg_training_score','is_promoted'])['employee_id'].count().reset_index()\navg = prob_avg.pivot_table(index='avg_training_score', columns='is_promoted', \n                       values='employee_id').reset_index()\navg.columns = ['avg_training_score', 'not_promoted', 'promoted']\navg['not_promoted'] = avg['not_promoted'].fillna(0)\navg['promoted'] = avg['promoted'].fillna(0)\navg['total'] = avg['not_promoted']+avg['promoted']\navg['probability'] = round((avg['promoted']\/avg['total'])*100,2)\navg['score90'] = np.where(avg['avg_training_score']>=90, 'high_score', 'low_score')\navg.head()","08bd9ac9":"score = avg.groupby(['score90'])['promoted','total'].sum().reset_index()\nscore['probability'] = round((score['promoted']\/score['total'])*100,2)\nscore","6dd016b7":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='score90', y='probability', \n                data=score,ax=ax1,color='coral',\n               order=score.sort_values('probability',ascending = False).score90)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 3,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Average Training Score'''\nfig.text(x=0.50, y=0.68, s=title, fontsize=24, weight='bold')\n\n#add description\ntext = '''\nemployees with an average training\nscore of more than equal to 90 \nhave a 76.83% chance of being promoted\n'''\nfig.text(x=0.50, y=0.48, s=text, fontsize=20)\n\n#set y axis limit \n#plt.ylim(0, 100)\n\n# add label\nplt.xlabel('Average Training Score', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Average Score >= 90', 'Average Score < 90'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Avg Score Promoted Probability.png');","3fedc719":"# assign categorical data into data_cat object\ndisplay(data.select_dtypes(include=['object']).columns)\nprint(data.select_dtypes(include=object).shape)\ndata_cat = data.select_dtypes(include=['object'])\ndata_cat.head(3)","3061fccb":"#membuat matriks korelasi dari setiap data numerik\nfeatures = ['no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?','avg_training_score','is_promoted']\ncorr_= data[features].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\");","a052a706":"region_probs = data.groupby(['region', 'is_promoted']).agg({\n    'department': ['count']\n}).reset_index()\n\nregion_probs.columns = ['region', 'is_promoted','employees']\n\nregion_probs = pd.pivot_table(region_probs,\n                             index = 'region',\n                             columns = 'is_promoted',\n                             values = 'employees').reset_index()\n\nregion_probs.columns = ['region','not_promoted','promoted']\nregion_probs['promotion_probs'] = (region_probs['promoted']\/(region_probs['not_promoted']+region_probs['promoted']))*100\nregion_probs = region_probs.sort_values('promotion_probs', ascending=False).reset_index(drop=True)\nregion_probs.head()","e0dabf27":"#melihat peluang yang dipromosikan berdasarkan region\nfig, ax = plt.subplots(figsize=(25,10))\n\ng = sns.barplot(x='region', y='promotion_probs', data=region_probs,\n                order=region_probs.sort_values('promotion_probs')['region'],\n                color = 'coral')\n\nplt.xticks(rotation=45)\nfor p in g.patches:\n    height = p.get_height()\n    plt.text(x = p.get_x() + (p.get_width()\/2),\n           y = height + 0.5,\n           s = str('{:.2f}'.format(height)) + '%',\n           ha = 'center'\n          )\n\ntitle = '''\nPromotion by Region\n'''\n\ntext = '''\nBased on region, employees who have a large enough\nopportunity compared to employees in regions 4, 17, 25, 28, 23, 22, 3, 7 \n(probability > 10%)\n'''\n\nplt.yticks([]) # Hide the y-axis with empty list\nplt.xlabel('Region', fontsize = 15) # Add the x-label\nplt.ylabel('Promotion Probabilities', fontsize = 15) # Add the y-label\n\nplt.text(x = 0.4, y = 12, s=title,\n        fontsize = 40, weight = 'bold', alpha = .75)\nplt.text(x = 0.4, y = 9.5, s=text,\n        fontsize = 28, alpha = .85)\nplt.tight_layout();\n\n#plt.savefig('fig\/Promotion by Region.png')","0da8ac4c":"data_department = pd.pivot_table(data,\n                    index = ['department'],\n                    columns = ['is_promoted'],\n                    aggfunc = {'is_promoted' : ['count']}).reset_index()\ndata_department.columns = ['department', 'not_promoted', 'promoted']\ndata_department['total'] = data_department['not_promoted']+data_department['promoted']\ndata_department['probability'] = round((data_department['promoted']\/data_department['total'])*100,2)\ndata_department = data_department.sort_values(['probability'], ascending=False)\ndata_department","5cb9e918":"#melihat jumlah karyawan yang dipromosikan berdasarkan Department\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='department', y='probability', \n                data=data_department,ax=ax1,color='coral',\n               order=data_department.sort_values('probability',ascending = False).department)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Department'''\nfig.text(x=0.6, y=0.825, s=title, fontsize=20, weight='bold')\n\n#add description\ntext = '''\nemployees from the technology department have \nthe greatest chance with a probability value \nof 10.76% to be promoted, while employees from\nthe legal department have the least chance \nwith a probability value of 5.10%\n'''\nfig.text(x=0.6, y=0.65, s=text, fontsize=15)\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('department', fontsize = 20, weight='bold')\nplt.ylabel('probability', fontsize = 20, weight='bold')\n\nplt.tight_layout\n#plt.savefig('fig\/Promotion by Department.png');","8508741e":"prob_edu = data.groupby(['education','is_promoted'])['employee_id'].count().reset_index()\nedu = prob_edu.pivot_table(index='education', columns='is_promoted', \n                       values='employee_id').reset_index()\nedu.columns = ['education', 'not_promoted', 'promoted']\nedu['total'] = edu['not_promoted']+edu['promoted']\nedu['probability'] = round((edu['promoted']\/edu['total'])*100,2)\nedu = edu.sort_values(['probability'], ascending=False)\nedu","da082d4a":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='education', y='probability', \n                data=edu,ax=ax1,color='coral',\n               order=edu.sort_values('probability',ascending = False).education)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Education Level'''\nfig.text(x=0.38, y=0.95, s=title, fontsize=20, weight='bold')\n\n#add description\ntext = '''\nEmployees with a master's education level\nand above have a chance to be promoted by 9.86%\n'''\nfig.text(x=0.38, y=0.83, s=text, fontsize=16)\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Education Level', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels([\"Master's & Above\", 'Below Secondary', \"Bachelor's\"], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Promotion by Education.png');","a41ad115":"prob_gender = data.groupby(['gender','is_promoted'])['employee_id'].count().reset_index()\ngender = prob_gender.pivot_table(index='gender', columns='is_promoted', \n                       values='employee_id').reset_index()\ngender.columns = ['gender', 'not_promoted', 'promoted']\ngender['total'] = gender['not_promoted']+gender['promoted']\ngender['probability'] = round((gender['promoted']\/gender['total'])*100,2)\ngender = gender.sort_values(['probability'], ascending=False)\ngender","980b8335":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='gender', y='probability', \n                data=gender,ax=ax1,color='coral',\n               order=gender.sort_values('probability',ascending = False).gender)\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Gender'''\nfig.text(x=0.48, y=1, s=title, fontsize=20, weight='bold', ha='center')\n\n#add description\ntext = '''\n8.99% of female employees are likely to be promoted\n'''\nfig.text(x=0.49, y=0.9, s=text, fontsize=16, ha='center')\n\n#set y axis limit \nplt.ylim(0, 10)\n\n# add label\nplt.xlabel('Gender', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels([\"Female\", 'Male'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Promotion by Gender.png');","fc5b0070":"prob_gk = data.groupby(['gender','KPIs_met >80%','is_promoted'])['employee_id'].count().reset_index()\ngk = prob_gk.pivot_table(index=['gender','KPIs_met >80%'], columns='is_promoted', \n                       values='employee_id').reset_index()\ngk.columns = ['gender', 'kpi', 'not_promoted','promoted']\ngk['total'] = gk['not_promoted']+gk['promoted']\ngk['probability'] = round((gk['promoted']\/gk['total'])*100,2)\ngk['kpi'] = np.where(gk['kpi']==0, '<80%', '>80%')\ngk","8946704f":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='gender', y='probability', hue='kpi',\n                data=gk,ax=ax1,palette='muted')\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Gender and KPI'''\nfig.text(x=0.525, y=1.025, s=title, fontsize=24, weight='bold',ha='center')\n\n#add description\ntext = '''\nMale and female employees have almost the same chance of being\npromoted if they succeed in achieving KPIs of more than 80%\n'''\nfig.text(x=0.525, y=0.875, s=text, fontsize=20,ha='center')\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Gender', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['female', 'male'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Gender-KPI probability.png');","ea5b9a02":"data['hs'] = np.where(data['avg_training_score']>=90, 1, 0)\nprob_tk = data.groupby(['hs','KPIs_met >80%','is_promoted'])['employee_id'].count().reset_index()\ntk = prob_tk.pivot_table(index=['hs','KPIs_met >80%'], columns='is_promoted', \n                       values='employee_id').reset_index()\ntk.columns = ['high_score', 'kpi', 'not_promoted','promoted']\ntk['total'] = tk['not_promoted']+tk['promoted']\ntk['probability'] = round((tk['promoted']\/tk['total'])*100,2)\ntk['kpi'] = np.where(tk['kpi']==0, '<80%', '>80%')\ntk","45ad59c4":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='high_score', y='probability', hue='kpi',\n                data=tk,ax=ax1,palette='muted')\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Average Training Score and KPI'''\nfig.text(x=0.525, y=1.075, s=title, fontsize=24, weight='bold',ha='center')\n\n#add description\ntext = '''\nEmployees who have an average training score of more than equal to 90 have a greater chance\nof being promoted than employees whose average training score\nis less than 90 even though they have achieved a KPI of more than 80%\n'''\nfig.text(x=0.525, y=0.875, s=text, fontsize=20,ha='center')\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Average Training Score', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Training Score <90', 'Training Score >=90'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/Training-KPI Probability.png');","b06e76f7":"prob_dk = data.groupby(['department','KPIs_met >80%','is_promoted'])['employee_id'].count().reset_index()\ndk = prob_dk.pivot_table(index=['department','KPIs_met >80%'], columns='is_promoted', \n                       values='employee_id').reset_index()\ndk.columns = ['department', 'kpi', 'not_promoted','promoted']\ndk['total'] = dk['not_promoted']+dk['promoted']\ndk['probability'] = round((dk['promoted']\/dk['total'])*100,2)\ndk['kpi'] = np.where(dk['kpi']==0, '<80%', '>80%')\ndk = dk.sort_values(['probability'], ascending=False)\ndk","2a652fbd":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='department', y='probability', hue='kpi',\n                data=dk,ax=ax1,palette=['coral','#4878D0'])\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=15,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Department and Gender'''\nfig.text(x=0.525, y=1.025, s=title, fontsize=24, weight='bold',ha='center')\n\n#add description\ntext = '''\nAll employees in all departments will have a higher probability of \nbeing promoted if they have reached KPI> 80% than those who did not reach KPI> 80%\n'''\nfig.text(x=0.525, y=0.875, s=text, fontsize=20,ha='center')\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Department', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Technology','Sales & Marketing','Procurement',\n                   'Analytics','Operations','Finance','R&D','HR','Legal'], fontsize = 15)\n\nplt.tight_layout\n#plt.savefig('fig\/department-gender Probability.png');","541285cb":"prob_dg = data.groupby(['department','gender','is_promoted'])['employee_id'].count().reset_index()\ndg = prob_dg.pivot_table(index=['department','gender'], columns='is_promoted', \n                       values='employee_id').reset_index()\ndg.columns = ['department', 'gender', 'not_promoted','promoted']\ndg['total'] = dg['not_promoted']+dg['promoted']\ndg['probability'] = round((dg['promoted']\/dg['total'])*100,2)\ndg['gender'] = np.where(dg['gender']=='m', 'male', 'female')\ndg = dg.sort_values(['probability'], ascending=False)\ndg","fe4c59f6":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='department', y='probability', hue='gender',\n                data=dg,ax=ax1,palette=['coral','#4878D0'])\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=12,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Department and Gender'''\nfig.text(x=0.525, y=1.025, s=title, fontsize=24, weight='bold',ha='center')\n\n#add description\ntext = '''\nFemale employees in the analytics and procurement department are more likely\nto be promoted than male employees with the probability values, respectively, of 12.09% and 11.71%\n'''\nfig.text(x=0.525, y=0.875, s=text, fontsize=20,ha='center')\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Department', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Analytics','Procurement','Technology',\n                   'R&D','Operations','Finance','Sales & Marketing','HR','Legal'], fontsize = 18)\n\nplt.tight_layout\n#plt.savefig('fig\/department-gender Probability.png');","46ffcda3":"prob_ak = data.groupby(['awards_won?','KPIs_met >80%','is_promoted'])['employee_id'].count().reset_index()\nak = prob_ak.pivot_table(index=['awards_won?','KPIs_met >80%'], columns='is_promoted', \n                       values='employee_id').reset_index()\nak.columns = ['won_awards', 'kpi', 'not_promoted','promoted']\nak['total'] = ak['not_promoted']+ak['promoted']\nak['probability'] = round((ak['promoted']\/ak['total'])*100,2)\nak['won_awards'] = np.where(ak['won_awards']==0, 'No', 'Yes')\nak['kpi'] = np.where(ak['kpi']==0, '<80%', '>80%')\nak = ak.sort_values(['probability'], ascending=False)\nak","83e08e2e":"#melihat jumlah karyawan yang dipromosikan berdasarkan Kpi\nfig, (ax1) = plt.subplots(nrows=1, ncols=1)\ng = sns.barplot(x='won_awards', y='probability', hue='kpi',\n                data=ak,ax=ax1,palette=['#4878D0','coral'])\n\n#add percentage labels\nfor p in g.patches:\n    height = p.get_height()\n    g.text(x = p.get_x() + (p.get_width()\/2),\n                y = height + 0.4,\n                s = str('{:.2f}'.format(height)) + '%',\n                ha = 'center',\n                fontsize=20,\n                weight='bold')\n    \nplt.xticks(rotation=0)\n\n#add title\ntitle = '''Promotion by Awards Won and KPI'''\nfig.text(x=0.525, y=1.025, s=title, fontsize=24, weight='bold',ha='center')\n\n#add description\ntext = '''\nEmployees who had won awards have a greater chance of \nbeing promoted even if they don't achieve KPI> 80\n'''\nfig.text(x=0.525, y=0.875, s=text, fontsize=20,ha='center')\n\n#set y axis limit \n#plt.ylim(0, 20)\n\n# add label\nplt.xlabel('Won Awards', fontsize = 20, weight='bold')\nplt.ylabel('Probability', fontsize = 20, weight='bold')\ng.set_xticklabels(['Yes','No'], fontsize = 15)\n\nplt.tight_layout\n#plt.savefig('fig\/awards-kpi Probability.png');","861111e3":"data_clean = data.copy()\ndata_clean.info()","2cc12b5e":"data_clean.duplicated().sum()","25c8dbf4":"data_clean['education'].value_counts()","fa0995e0":"data_clean['education'] = data_clean['education'].fillna(data_clean['education'].mode()[0])","914bea82":"data_clean['education'].value_counts()","2f3f47ef":"data_clean['previous_year_rating'].fillna(data_clean['previous_year_rating'].median(), inplace=True)","36a7fdd9":"data_clean.isnull().sum()","e3576ea3":"data_cat = data_clean.select_dtypes(include=['object'])\ndata_cat_unique = data_cat.nunique().reset_index()\ndata_cat_unique.columns = ['fitur', 'unik nilai']\ndata_cat_unique = data_cat_unique.sort_values('unik nilai', ascending=False)\ndata_cat_unique","e4e4e2b2":"data_clean['potential_region'] = np.where(data_clean['region'].isin(['region_4','region_17','region_25','region_28','region_23',\n                                                   'region_3','region_7']),\n                              1, 0)\ndata_clean.head()","23beb27c":"data_cat = data_clean.select_dtypes(include=['object'])\ndata_cat_unique = data_cat.nunique().reset_index()\ndata_cat_unique.columns = ['fitur', 'unik nilai']\ndata_cat_unique = data_cat_unique.sort_values('unik nilai', ascending=False)\ndata_cat_unique","9f55beb2":"\npotential_reg_probs = data_clean.groupby(['potential_region', 'is_promoted']).agg({\n    'department': ['count']\n}).reset_index()\n\npotential_reg_probs.columns = ['potential_region', 'is_promoted','employees']\n\npotential_reg_probs = pd.pivot_table(potential_reg_probs,\n                             index = 'potential_region',\n                             columns = 'is_promoted',\n                             values = 'employees').reset_index()\n\npotential_reg_probs.columns = ['potential_region','not_promoted','promoted']\npotential_reg_probs['total_employees'] = potential_reg_probs['not_promoted'] + potential_reg_probs['promoted']\npotential_reg_probs['promotion_probs'] = (potential_reg_probs['promoted']\/(potential_reg_probs['not_promoted']+potential_reg_probs['promoted']))*100\npotential_reg_probs = potential_reg_probs.sort_values('promotion_probs', ascending=False).reset_index(drop=True)\npotential_reg_probs['potential_region'] = np.where(potential_reg_probs['potential_region'] == 1, 'YES', 'NO')\npotential_reg_probs","fe7407c0":"fig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x='potential_region', y='promotion_probs', data=potential_reg_probs,\n                order=potential_reg_probs.sort_values('promotion_probs')['potential_region'],\n                palette=sns.color_palette(\"Blues_d\", n_colors=40, desat=1)\n               )\n\nfor p in g.patches:\n    height = p.get_height()\n    plt.text(x = p.get_x() + (p.get_width()\/2),\n           y = height + 0.5,\n           s = str('{:.2f}'.format(height)) + '%',\n           ha = 'center'\n          )\n\ntitle = '''\nPromotion Probabilities based on 'potential_region'\n'''\n\ntext = '''\nHypotetically, an employee who were placed in the potential region\nwill have higher chance to get promotion than the employees \nout of potential region\n'''\n\nplt.yticks([]) # Hide the y-axis with empty list\nplt.xlabel('Is in Potential Region?', fontsize = 15) # Add the x-label\nplt.ylabel('Promotion Probabilities', fontsize = 15) # Add the y-label\n\nplt.text(x = -0.5, y = 11, s=title,\n        fontsize = 18, weight = 'bold', alpha = .75)\nplt.text(x = -0.5, y = 9.25, s=text,\n        fontsize = 15, alpha = .85);\n\n#plt.savefig('fig\/Insight Awal ke-3.png')","e52c8f2e":"# membuat fitur performance_level\ndata_clean['performance_level'] = np.where(\n    (\n        (data_clean['previous_year_rating'] == 5) &\n        (data_clean['KPIs_met >80%'] == 1) &\n        (data_clean['awards_won?'] == 1)\n    ),\n\"Best\",\n    np.where((\n        ((data_clean['previous_year_rating'] == 4) & (data_clean['KPIs_met >80%'] == 0) & (data_clean['awards_won?'] == 1)) | \n        ((data_clean['previous_year_rating'] == 4) & (data_clean['KPIs_met >80%'] == 1) & (data_clean['awards_won?'] == 0)) |\n        ((data_clean['previous_year_rating'] == 4) & (data_clean['KPIs_met >80%'] == 1) & (data_clean['awards_won?'] == 1)) |\n        ((data_clean['previous_year_rating'] == 5) & (data_clean['KPIs_met >80%'] == 0) & (data_clean['awards_won?'] == 1)) |\n        ((data_clean['previous_year_rating'] == 5) & (data_clean['KPIs_met >80%'] == 1) & (data_clean['awards_won?'] == 0))\n    ),\n    \"Better\",\n    np.where(\n        ((data_clean['previous_year_rating'] == 3) & (data_clean['KPIs_met >80%'] == 0) & (data_clean['awards_won?'] == 1)) | \n        ((data_clean['previous_year_rating'] == 3) & (data_clean['KPIs_met >80%'] == 1) & (data_clean['awards_won?'] == 0)) |\n        ((data_clean['previous_year_rating'] == 3) & (data_clean['KPIs_met >80%'] == 1) & (data_clean['awards_won?'] == 1)),\n        \"Good\",\"Low\"\n    )\n    )\n)","c9b613a9":"\nperform_probs = data_clean.groupby(['performance_level', 'is_promoted']).agg({\n    'department': ['count']\n}).reset_index()\n\nperform_probs.columns = ['performance_level', 'is_promoted','employees']\n\nperform_probs = pd.pivot_table(perform_probs,\n                             index = 'performance_level',\n                             columns = 'is_promoted',\n                             values = 'employees').reset_index()\n\nperform_probs.columns = ['performance_level','not_promoted','promoted']\nperform_probs['total_employees'] = perform_probs['not_promoted'] + perform_probs['promoted']\nperform_probs['promotion_probs'] = (perform_probs['promoted']\/(perform_probs['not_promoted']+perform_probs['promoted']))*100\nperform_probs = perform_probs.sort_values('promotion_probs', ascending=False).reset_index(drop=True)\nperform_probs","4df14b03":"#melihat peluang yang dipromosikan berdasarkan performance_level\nfig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x='performance_level', y='promotion_probs', data=perform_probs,\n                order=perform_probs.sort_values('promotion_probs',ascending = False).performance_level,\n                color='coral'\n               )\n\nfor p in g.patches:\n    height = p.get_height()\n    plt.text(x = p.get_x() + (p.get_width()\/2),\n           y = height + 0.5,\n           s = str('{:.2f}'.format(height)) + '%',\n           ha = 'center'\n          )\n\ntitle = '''\nPromotion Probabilities based on 'performance_level'\n'''\n\ntext = '''\nBased on the level of performance, employees who are classified\nat the Best level have the greatest opportunity compared to \nemployees who are not at that level\n'''\n\n#plt.yticks([]) # Hide the y-axis with empty list\nplt.xlabel('Is in Potential Region?', fontsize = 15) # Add the x-label\nplt.ylabel('Promotion Probabilities', fontsize = 15) # Add the y-label\n\nplt.text(x = 0.5, y = 45, s=title,\n        fontsize = 20, weight = 'bold', alpha = .75)\nplt.text(x = 0.5, y = 38, s=text,\n        fontsize = 15, alpha = .85);\n\n#plt.savefig('fig\/Promotion by Performance Level.png')","51d6cdac":"data_clean['High_Avg_Tscore'] = np.where(data_clean['avg_training_score']>=90, 1, 0)\ndata_clean.head()","83048c10":"data_clean['male'] = np.where(data_clean['gender']=='m', 1, 0)\ndata_clean.head()","b394939c":"dummies_dept = pd.get_dummies(data_clean['department'], prefix='Dept')\ndata_clean = pd.concat([data_clean, dummies_dept], axis=1)","5b0a8f67":"dummies_edu = pd.get_dummies(data_clean['education'])\ndata_clean = pd.concat([data_clean, dummies_edu], axis=1)","b9863798":"dummies_rc = pd.get_dummies(data_clean['recruitment_channel'])\ndata_clean = pd.concat([data_clean, dummies_rc], axis=1)","465591cb":"#merubah fitur performance_level dari data categorical menjadi numerik menggunakan metode one hot encoding\ndummies_pl = pd.get_dummies(data_clean['performance_level'])\ndata_clean = pd.concat([data_clean, dummies_pl], axis=1)","d8c65d44":"data_clean.info()","e879e657":"features = ['no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?',\n            'avg_training_score','is_promoted','potential_region','Dept_Analytics','Dept_Finance','Dept_HR','Dept_Legal',\n            'Dept_Operations','Dept_Procurement','Dept_R&D','Dept_Sales & Marketing','Dept_Technology',\"Bachelor's\",\n            'Below Secondary',\"Master's & above\", 'other','referred','sourcing','male','High_Avg_Tscore','Best','Better','Good','Low']\n\ndf_pre = data_clean.copy()\nfor var in features:\n    df_pre['log_'+var]= (data_clean[var]+1).apply(np.log)","89c31bdb":"df_pre.info()","abbb3f88":"df_pre.describe()","f6cb5a60":"for var in features:\n    df_pre['nor_'+var] = MinMaxScaler().fit_transform(df_pre[var].values.reshape(len(df_pre),1))","a3ed49e9":"df_pre","1bc4a71f":"sns.set(rc={'figure.figsize':(16,16)})\nsns.heatmap(df_pre[['nor_no_of_trainings','nor_age','nor_previous_year_rating',\n                    'nor_length_of_service','nor_KPIs_met >80%','nor_awards_won?',\n                    'nor_avg_training_score','nor_is_promoted','nor_potential_region',\n                    'nor_Dept_Analytics','nor_Dept_Finance','nor_Dept_HR','nor_Dept_Legal',\n                    'nor_Dept_Operations','nor_Dept_Procurement','nor_Dept_R&D',\n                    'nor_Dept_Sales & Marketing','nor_Dept_Technology',\"nor_Bachelor's\",\n                    'nor_Below Secondary',\"nor_Master's & above\", 'nor_other','nor_referred',\n                    'nor_sourcing','nor_male','nor_High_Avg_Tscore','nor_Best','nor_Better',\n                    'nor_Good','nor_Low']].corr(), annot=True)\n#plt.savefig('fig\/split train test.png')","02bc536d":"x_norm = df_pre[['nor_no_of_trainings','nor_age','nor_previous_year_rating',\n            'nor_length_of_service','nor_KPIs_met >80%','nor_awards_won?',\n            'nor_avg_training_score','nor_potential_region',\n            'nor_Dept_Analytics','nor_Dept_Finance','nor_Dept_HR','nor_Dept_Legal',\n            'nor_Dept_Operations','nor_Dept_Procurement','nor_Dept_R&D',\n            'nor_Dept_Sales & Marketing','nor_Dept_Technology',\"nor_Bachelor's\",\n            'nor_Below Secondary',\"nor_Master's & above\", 'nor_other','nor_referred',\n            'nor_sourcing','nor_male','nor_High_Avg_Tscore',\"nor_Best\",'nor_Better','nor_Good','nor_Low']]\ny_norm = df_pre['nor_is_promoted']","5068731a":"#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x_norm,\n                                                y_norm,\n                                                test_size=0.2, random_state=42) ","dea54031":"df_pre.shape","89ce5222":"xtrain.shape","899fcd34":"xtest.shape","c1f8de4d":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier (random_state=42)\nrf_model = rf.fit(xtrain, ytrain)","ac69cf8e":"y_predicted = rf.predict(xtest)\ny_predicted_train = rf.predict(xtrain)\n#eval\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nprint('\\nclassification report')\nprint(classification_report(ytest, y_predicted)) # generate the precision, recall, f-1 score, num\nprint('Random Forest: ROC AUC = ',str(round(roc_auc_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Precision = ',str(round(precision_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Recall = ',str(round(recall_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Accuracy = ',str(round(accuracy_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: F1-Score = ',str(round(f1_score(ytest, y_predicted)*100,1)), '%')\nconfusion_matrix(ytest, y_predicted)","abaa3e18":"#Save Machine Learning Model\nimport pickle\npickle.dump(rf_model, open('RandomForest.pkl', 'wb'))","423ad7d6":"#Load and print model score\nloaded_model = pickle.load(open('RandomForest.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint(result)","b1e03123":"#print model score to check if the model is overfitting or not\nprint('train score : ', str(round(loaded_model.score(xtrain, ytrain)*100,2)), '%')\nprint('test score : ', str(round(loaded_model.score(xtest, ytest)*100,2)),'%')","dc99223a":"#define function for roc auc curve\ndef plot_roc_curve(fpr, tpr, label=None):\n    \"\"\"\n    The ROC curve, modified from \n    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n    plt.legend(loc='best')","d2597598":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_predicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/Random Forest ROC Curve.png')","0ac4bd81":"#Define function to see precision and recall threshold\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \"\"\"\n    Modified from:\n    Hands-On Machine learning with Scikit-Learn\n    and TensorFlow; p.89\n    \"\"\"\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')","a7d2215e":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_predicted)","d5a092ea":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold RF.png')","8283676f":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_model = knn.fit(xtrain, ytrain)","d7a95484":"# Predict Test\nypredicted = knn.predict(xtest)\n\nprint('\\nclassification report')\nprint(classification_report(ytest, ypredicted)) # generate the precision, recall, f-1 score, num\nprint('KNN: ROC AUC = ',str(round(roc_auc_score(ytest, ypredicted)*100,1)), '%')\nprint('KNN: Precision = ',str(round(precision_score(ytest, ypredicted)*100,1)), '%')\nprint('KNN: Recall = ',str(round(recall_score(ytest, ypredicted)*100,1)), '%')\nprint('KNN: Accuracy = ',str(round(accuracy_score(ytest, ypredicted)*100,1)), '%')\nprint('KNN: F1-Score = ',str(round(f1_score(ytest, ypredicted)*100,1)), '%')\nconfusion_matrix(ytest, ypredicted)","73e4d28d":"#Save Machine Learning Model\nimport pickle\npickle.dump(knn_model, open('KNN.pkl', 'wb'))","055c14f5":"#Load and print model score\nloaded_model = pickle.load(open('KNN.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint(result)","58cc0943":"#print model score to check if the model is overfitting or not\nprint('train score : ', str(round(loaded_model.score(xtrain, ytrain)*100,2)), '%')\nprint('test score : ', str(round(loaded_model.score(xtest, ytest)*100,2)),'%')","3a16f868":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, ypredicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/KNN ROC Curve.png')","3062b2d6":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, ypredicted)","a81a9c07":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold KNN.png')","a261d9ab":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=42, solver = 'liblinear')\nlogreg_model = logreg.fit(xtrain, ytrain)","75a44223":"# Predict the test data\ny_predicted = logreg.predict(xtest)\ny_predicted","26fe9c50":"'''\nThe output of a Logistic regression model is a probability. \nWe can select a threshold value. \nIf the probability is greater than this threshold value, \nthe event is predicted to happen otherwise it is predicted not to happen.\n\nTo get the probability of the label, repectively for class 0 and 1\n'''\n\ny_predicted_proba = logreg.predict_proba(xtest)\ny_predicted_proba","3bab183c":"print('\\nclassification report')\nprint(classification_report(ytest, y_predicted)) # generate the precision, recall, f-1 score, num\nprint('LogReg: ROC AUC = ',str(round(roc_auc_score(ytest, y_predicted)*100,1)), '%')\nprint('LogReg: Precision = ',str(round(precision_score(ytest, y_predicted)*100,1)), '%')\nprint('LogReg: Recall = ',str(round(recall_score(ytest, y_predicted)*100,1)), '%')\nprint('LogReg: Accuracy = ',str(round(accuracy_score(ytest, y_predicted)*100,1)), '%')\nprint('LogReg: F1-Score = ',str(round(f1_score(ytest, y_predicted)*100,1)), '%')\nconfusion_matrix(ytest, y_predicted)","03043811":"#Save Machine Learning Model\nimport pickle\npickle.dump(logreg_model, open('Logreg.pkl', 'wb'))","bcef9a30":"#Load and print model score\nloaded_model = pickle.load(open('Logreg.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint(result)","30d0a3ca":"#print model score to check if the model is overfitting or not\nprint('train score : ', str(round(loaded_model.score(xtrain, ytrain)*100,2)), '%')\nprint('test score : ', str(round(loaded_model.score(xtest, ytest)*100,2)),'%')","d8f6a526":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_predicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/Logreg ROC Curve.png')","06fb833a":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_predicted)","2b6a1f28":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold Logreg.png')","7c2d8a9f":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt_model = dt.fit(xtrain,ytrain)\ny_predicted = dt.predict(xtest)","9e561146":"print('\\nclassification report')\nprint(classification_report(ytest, y_predicted)) # generate the precision, recall, f-1 score, num\nprint('Decision Tree: ROC AUC = ',str(round(roc_auc_score(ytest, y_predicted)*100,1)), '%')\nprint('Decision Tree: Precision = ',str(round(precision_score(ytest, y_predicted)*100,1)), '%')\nprint('Decision Tree: Recall = ',str(round(recall_score(ytest, y_predicted)*100,1)), '%')\nprint('Decision Tree: Accuracy = ',str(round(accuracy_score(ytest, y_predicted)*100,1)), '%')\nprint('Decision Tree: F1-Score = ',str(round(f1_score(ytest, y_predicted)*100,1)), '%')\nconfusion_matrix(ytest, y_predicted)","18d72a25":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# list of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree\nmin_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node\nmax_features = ['auto', 'sqrt'] # Number of features to consider at every split\ncriterion =['entropy']\nsplitter = ['random']\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'max_features': max_features,\n               'criterion':criterion,\n               'splitter': splitter\n                }\n\n# Init Logres dengan Gridsearch, cross validation = 5\ndt2 = DecisionTreeClassifier(random_state=42)\nclf = RandomizedSearchCV(dt2, hyperparameters, cv=5, random_state=42)\n\n#Fitting Model\ndt_model = clf.fit(xtrain, ytrain)\n\n#Prediksi menggunakan model baru\ny_pred = dt_model.predict(xtest)#Check performa dari model\nprint('\\nclassification report')\nprint(classification_report(ytest, y_pred)) # generate the precision, recall, f-1 score, num\nprint('Decision Tree: ROC AUC = ',str(round(roc_auc_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Precision = ',str(round(precision_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Recall = ',str(round(recall_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Accuracy = ',str(round(accuracy_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: F1-Score = ',str(round(f1_score(ytest, y_pred)*100,1)), '%')\nconfusion_matrix(ytest, y_pred)","adef39b6":"#Save Machine Learning Model\nimport pickle\npickle.dump(dt_model, open('Decision Tree.pkl', 'wb'))","a77f72f8":"#Load and print model score\nloaded_model = pickle.load(open('Decision Tree.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint(result)","cb4f6104":"#print model score to check if the model is overfitting or not\nprint('train score : ', str(round(loaded_model.score(xtrain, ytrain)*100,2)), '%')\nprint('test score : ', str(round(loaded_model.score(xtest, ytest)*100,2)),'%')","c3f0e751":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_pred)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/Decision Tree ROC Curve.png')","9cd62394":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_pred)","d6626e78":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold DT.png')","606ca8c5":"from sklearn import tree\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(20, 20))\ntree.plot_tree(dt,\n               feature_names = x_norm.columns.tolist(), \n               class_names=['0','1'],\n               filled = True, max_depth=2, fontsize=12)\nplt.show()","79a26007":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndata_dmatrix = xgb.DMatrix(data=x_norm, label=y_norm)\n\nxtrain, xtest, ytrain, ytest = train_test_split(x_norm,\n                                                y_norm,\n                                                test_size=0.2,\n                                                shuffle = True,\n                                                stratify = y_norm,\n                                                random_state=42) #Splitting the data into Train and Test\n\nxgb_classifier = xgb.XGBClassifier()\nnorm_model = xgb_classifier.fit(x_norm, y_norm)\ny_predicted = norm_model.predict(xtest)\ny_predicted_train = norm_model.predict(xtrain)\n\nprint(\"\\nClassification Report\")\nprint(classification_report(ytest, y_predicted))\nprint('XGBoost: ROC AUC = ', str(round(roc_auc_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Precision = ', str(round(precision_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Recall = ', str(round(recall_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Accuracy = ', str(round(accuracy_score(ytest, y_predicted)*100,1)),\"%\")\nprint(\"XGBoost: F1 Score =  \", str(round(f1_score(ytest, y_predicted)*100,1)),\"%\")\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(ytest, y_predicted)\n","454104ec":"print(\"\\nTrain vs Test Accuration\")\nimport pickle\npickle.dump(xgb_classifier, open('XGBoost_model.pkl', 'wb'))\n\nloaded_model = pickle.load(open('XGBoost_model.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\n#pred = loaded_model.predict(xtest)\n#print(result)\n#print(pred)\n\nprint('train accuracy : ', str(round(loaded_model.score(xtrain, ytrain)*100,2)), '%')\nprint('test accuracy : ', str(round(loaded_model.score(xtest, ytest)*100,2)), '%')","363262b3":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {\n               'min_samples_split': [7,8,9,10],\n               'min_samples_leaf': [1,2,3,4,5],\n               'bootstrap': [True],\n               'n_jobs':[-1]}\n\n# Init random forest dengan randomsearch, cross validation = 5\nrf = RandomForestClassifier(random_state=42)\nclf = RandomizedSearchCV(rf, hyperparameters, cv=20, random_state=42)\n\n#Fitting Model\nbest_model = clf.fit(xtrain,ytrain)\n\n#Nilai hyperparameters terbaik\nprint('Best n_estimators:', best_model.best_estimator_.get_params()['n_estimators'])\nprint('Best max_features:', best_model.best_estimator_.get_params()['max_features'])\nprint('Best max_depth:', best_model.best_estimator_.get_params()['max_depth'])\nprint('Best min_samples_split:', best_model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', best_model.best_estimator_.get_params()['min_samples_leaf'])\n\n\n#Prediksi menggunakan model baru\ny_predicted = best_model.predict(xtest)#Check performa dari model\nprint('\\nclassification report')\nprint(classification_report(ytest, y_predicted)) # generate the precision, recall, f-1 score, num\nprint('Random Forest: ROC AUC = ',str(round(roc_auc_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Precision = ',str(round(precision_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Recall = ',str(round(recall_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: Accuracy = ',str(round(accuracy_score(ytest, y_predicted)*100,1)), '%')\nprint('Random Forest: F1-Score = ',str(round(f1_score(ytest, y_predicted)*100,1)), '%')\nconfusion_matrix(ytest, y_predicted)\n","36e9bf7e":"import pickle\npickle.dump(best_model, open('RandomForest_model.pkl', 'wb'))","7fd4acdd":"loaded_model = pickle.load(open('RandomForest_model.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint(result)","016211e3":"print('train accuracy : ', str(round(loaded_model.score(xtrain, ytrain)*100, 2)), '%')\nprint('test accuracy : ', str(round(loaded_model.score(xtest, ytest)*100, 2)), '%')","680c801b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_predicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/RF ROC Curve.png')","47f6ec1f":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_predicted)","277867fc":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold RF.png')","af861e02":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# list of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree\nmin_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node\nmax_features = ['auto', 'sqrt'] # Number of features to consider at every split\ncriterion =['entropy']\nsplitter = ['random']\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'max_features': max_features,\n               'criterion':criterion,\n               'splitter': splitter\n                }\n\n# Init Logres dengan Gridsearch, cross validation = 5\ndt2 = DecisionTreeClassifier(random_state=42)\nclf = RandomizedSearchCV(dt2, hyperparameters, cv=5, random_state=42)\n\n#Fitting Model\ndt_model = clf.fit(xtrain, ytrain)\n\n#Prediksi menggunakan model baru\ny_pred = dt_model.predict(xtest)#Check performa dari model\nprint('\\nclassification report')\nprint(classification_report(ytest, y_pred)) # generate the precision, recall, f-1 score, num\nprint('Decision Tree: ROC AUC = ',str(round(roc_auc_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Precision = ',str(round(precision_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Recall = ',str(round(recall_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: Accuracy = ',str(round(accuracy_score(ytest, y_pred)*100,1)), '%')\nprint('Decision Tree: F1-Score = ',str(round(f1_score(ytest, y_pred)*100,1)), '%')\nconfusion_matrix(ytest, y_pred)","a3a0bb20":"print(\"\\nTrain vs Test Accuration\")\nimport pickle\npickle.dump(dt_model, open('DT_model.pkl', 'wb'))\n\nloaded_model = pickle.load(open('DT_model.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\n#pred = loaded_model.predict(xtest)\n#print(result)\n#print(pred)\n\nprint('train accuracy : ', str(round(loaded_model.score(xtrain, ytrain)*100, 2)), '%')\nprint('test accuracy : ', str(round(loaded_model.score(xtest, ytest)*100, 2)), '%')","0f8f51c3":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_predicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/DT ROC Curve.png')","fe52fece":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_predicted)","f3a8bc10":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold DT.png')","7d20e23d":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndata_dmatrix = xgb.DMatrix(data=x_norm, label=y_norm)\n\nxtrain, xtest, ytrain, ytest = train_test_split(x_norm,\n                                                y_norm,\n                                                test_size=0.2,\n                                                shuffle = True,\n                                                stratify = y_norm,\n                                                random_state=42) #Splitting the data into Train and Test\n\nxgb_classifier = xgb.XGBClassifier(scale_pos_weight = 3.5)\nxgb_best_model = xgb_classifier.fit(x_norm, y_norm)\ny_predicted = xgb_best_model.predict(xtest)\ny_predicted_train = xgb_best_model.predict(xtrain)\n\nprint(\"\\nClassification Report\")\nprint(classification_report(ytest, y_predicted))\nprint('XGBoost: ROC AUC = ', str(round(roc_auc_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Precision = ', str(round(precision_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Recall = ', str(round(recall_score(ytest, y_predicted)*100,1)),\"%\")\nprint('XGBoost: Accuracy = ', str(round(accuracy_score(ytest, y_predicted)*100,1)),\"%\")\nprint(\"XGBoost: F1 Score =  \", str(round(f1_score(ytest, y_predicted)*100,1)),\"%\")\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(ytest, y_predicted)","9d68c00c":"print(\"\\nTrain vs Test Accuration\")\nimport pickle\npickle.dump(xgb_best_model, open('XGBoost_model.pkl', 'wb'))\n\nloaded_model = pickle.load(open('XGBoost_model.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\n#pred = loaded_model.predict(xtest)\n#print(result)\n#print(pred)\n\nprint('train accuracy : ', str(round(loaded_model.score(xtrain, ytrain)*100, 2)), '%')\nprint('test accuracy : ', str(round(loaded_model.score(xtest, ytest)*100, 2)), '%')","71fd0f13":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n#make roc auc curve\nfpr, tpr, auc_thresholds = roc_curve(ytest, y_predicted)\nprint('ROC AUC Score: ',str(round(auc(fpr, tpr)*100,1)), '%') # AUC of ROC\nplot_roc_curve(fpr, tpr, 'recall_optimized')\n#plt.savefig('fig\/XGB ROC Curve.png')","92bbb5c1":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(ytest, y_predicted)","4add9bb5":"# use the same p, r, thresholds that were previously calculated\nplot_precision_recall_vs_threshold(p, r, thresholds)\n#plt.savefig('fig\/Precision-Recall Threshold XGB.png')","a6887f8e":"data_test_ori = pd.read_csv(\"\/kaggle\/input\/hr-analytics-classification\/test_2umaH9m.csv\")\ndata_test_ori.head()","be206646":"data_test = data_test_ori.copy()\ndata_test.head()","5d7f5ee2":"#see the amount of data in the dataset\nprint('there are ' + str(len(data_test)) + ' rows of data in the dataset')","d69f9b32":"data_test.duplicated().sum()","aa726851":"data_test.isnull().sum()","5aed8dc7":"data_test.drop(['employee_id'], axis=1, inplace=True)\n\ndata_test['education'] = data_test['education'].fillna(data_test['education'].mode()[0])\n\ndata_test['previous_year_rating'].fillna(data_test['previous_year_rating'].median(), inplace=True)\n\n#data_test = data_test.drop_duplicates()\n\ndata_test['potential_region'] = np.where(data_test['region'].isin(['region_4','region_17','region_25','region_28','region_23',\n                                                   'region_3','region_7']),\n                              1, 0)\n\ndata_test['performance_level'] = np.where(\n    (\n        (data_test['previous_year_rating'] == 5) &\n        (data_test['KPIs_met >80%'] == 1) &\n        (data_test['awards_won?'] == 1)\n    ),\n\"Best\",\n    np.where((\n        ((data_test['previous_year_rating'] == 4) & (data_test['KPIs_met >80%'] == 0) & (data_test['awards_won?'] == 1)) | \n        ((data_test['previous_year_rating'] == 4) & (data_test['KPIs_met >80%'] == 1) & (data_test['awards_won?'] == 0)) |\n        ((data_test['previous_year_rating'] == 4) & (data_test['KPIs_met >80%'] == 1) & (data_test['awards_won?'] == 1)) |\n        ((data_test['previous_year_rating'] == 5) & (data_test['KPIs_met >80%'] == 0) & (data_test['awards_won?'] == 1)) |\n        ((data_test['previous_year_rating'] == 5) & (data_test['KPIs_met >80%'] == 1) & (data_test['awards_won?'] == 0))\n    ),\n    \"Better\",\n    np.where(\n        ((data_test['previous_year_rating'] == 3) & (data_test['KPIs_met >80%'] == 0) & (data_test['awards_won?'] == 1)) | \n        ((data_test['previous_year_rating'] == 3) & (data_test['KPIs_met >80%'] == 1) & (data_test['awards_won?'] == 0)) |\n        ((data_test['previous_year_rating'] == 3) & (data_test['KPIs_met >80%'] == 1) & (data_test['awards_won?'] == 1)),\n        \"Good\",\"Low\"\n    )\n    )\n)\n\ndata_test['High_Avg_Tscore'] = np.where(data_test['avg_training_score']>=90, 1, 0)\n\ndata_test['male'] = np.where(data_test['gender']=='m', 1, 0)\n\n#merubah fitur department dari data categorical menjadi numerik menggunakan metode one hot encoding\ndummies_dept = pd.get_dummies(data_test['department'], prefix='Dept')\ndata_test = pd.concat([data_test, dummies_dept], axis=1)\n\n#merubah fitur education dari data categorical menjadi numerik menggunakan metode one hot encoding\ndummies_edu = pd.get_dummies(data_test['education'])\ndata_test = pd.concat([data_test, dummies_edu], axis=1)\n\n#merubah fitur recruitment_channel dari data categorical menjadi numerik menggunakan metode one hot encoding\ndummies_rc = pd.get_dummies(data_test['recruitment_channel'])\ndata_test = pd.concat([data_test, dummies_rc], axis=1)\n\n#merubah fitur performance_level dari data categorical menjadi numerik menggunakan metode one hot encoding\ndummies_pl = pd.get_dummies(data_test['performance_level'])\ndata_test = pd.concat([data_test, dummies_pl], axis=1)\n\nfeatures = ['no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?',\n        'avg_training_score','potential_region','Dept_Analytics','Dept_Finance','Dept_HR','Dept_Legal',\n           'Dept_Operations','Dept_Procurement','Dept_R&D','Dept_Sales & Marketing','Dept_Technology',\"Bachelor's\",\n           'Below Secondary',\"Master's & above\", 'other','referred','sourcing','male','High_Avg_Tscore','Best','Better','Good','Low']\n\nfor var in features:\n    data_test['log_'+var]= (data_test[var]+1).apply(np.log)\n    \nfor var in features:\n    data_test['nor_'+var] = MinMaxScaler().fit_transform(data_test[var].values.reshape(len(data_test),1))\n\ndata_test.drop(['department', 'region', 'education', 'gender', 'recruitment_channel', 'performance_level',\n               'referred', 'log_Better', 'log_Best', \"Bachelor's\", 'High_Avg_Tscore', 'sourcing', \"KPIs_met >80%\",\n                'log_referred', 'Best', 'Dept_Operations', 'Dept_HR', 'log_length_of_service', 'Dept_Technology', 'log_male',\n                'Dept_R&D', 'log_potential_region', 'potential_region', \"Master's & above\", \"awards_won?\", \"log_previous_year_rating\",\n                \"male\", \"log_Dept_Analytics\", 'log_Dept_Finance', \"log_Dept_Operations\", 'other', 'log_Below Secondary', \"age\", \n                \"log_KPIs_met >80%\", 'log_Dept_Sales & Marketing', \"Dept_Sales & Marketing\", 'log_sourcing', 'log_no_of_trainings', \n                \"log_Master's & above\", \"no_of_trainings\", \"log_Dept_R&D\", \"log_Dept_HR\", 'Low', \"Dept_Legal\", 'log_age', \n                \"Dept_Finance\", \"previous_year_rating\", \"log_other\", \"log_Bachelor's\", \"avg_training_score\", \"log_Low\", \"Dept_Analytics\", \n                \"log_Dept_Technology\", \"log_awards_won?\", \"log_avg_training_score\", \"Dept_Procurement\", \"log_Good\", \"log_Dept_Procurement\", \n                \"length_of_service\", \"log_Dept_Legal\", \"Better\", \"log_High_Avg_Tscore\", \"Good\", \"Below Secondary\"\n               ],\n              axis = 1, inplace=True)\n    \ndata_test.head()","c2d239ca":"y_test = xgb_best_model.predict(data_test)\ny_test_prob = xgb_best_model.predict_proba(data_test)\nresults = pd.DataFrame(data = {\n    \"id\": data_test_ori['employee_id'],\n    \"Promoted\": y_test,\n    \"Probability\": y_test_prob[:,1]\n})\nprint(results.head())\nprint(\"The number of Promoted Employee is: \" + str(len(results[results[\"Promoted\"] == 1])))","38eb3890":"print('The percentage of employees who get promoted is', str(round((2040\/23490)*100,2)), '%')\nprint('The number of employees who get promoted using XGBoost model are', str(len(results[results[\"Promoted\"] == 1])), 'out of', str(len(data_test)), 'employees')","037bc6c6":"##### Probability to Get Promotion based on `Average_Training_Score` and `KPI`","fb335093":"**Conclusion**<\/br>\n* Dataframe have 14 columns\n* Dataframe have 54808 rows\n* Total categorical columns are 5 columns\n* Total numerical columns are 9 columns\n* There are 2 columns that have a missing value (`education` and `previous_year_rating`)\n* `is_promoted` column is the target for this dataset","fab2fa8c":"### `High_Avg_Tscore`\n\nThis feature will have information on whether an employee has an average training score of more than or equal to 90 which has a greater chance of getting a promotion\n1. based on the probability information on [avg_training_score](#avg_training_score), the opportunity for employees with a value greater than 90 is 76.83%\n2. If the employee has this value then the value is '1' otherwise it will be worth '0'","64a6df54":"# Hyperparameter Tuning","65d1eb15":"# Data Understanding: Exploratory Data Analysis (EDA)","4a46d827":"**Objective**<br>\n* Build model to predict eligibility for an employee to be promoted or not\n\n**About The Dataset**\n* This dataset is obtained from [HR_Analytics_Classification | Kaggle](https:\/\/www.kaggle.com\/bhrt97\/hr-analytics-classification)","3d9f43ac":"**In the raw data, there are no duplicate values**","f196491b":"### `department`","744fe5f4":"## Load Data","4ee26751":"### Statistical Categorical Data","c82e3843":"# Machine Learning Modelling and Evaluation","4ff366b6":"##### Probability to Get Promotion based on `Department` and `Gender`","125ede90":"__*Fix the missing value in `education` feature with `mode()` function to fill them with the most frequently occuring values*__","a467286d":"## Logistic Regression","cbf2d3fb":"### `male`\n\n*Contains the information to show an employee gender from `gender` feature*\n1. If the `gender` value is `m`, `male` feature will have value `1`, if not the value will be set to `0`\n2. This is to reduce the feature encoding results that may be affect the number of feature\n","29db2778":"## Feature encoding\n*To convert the categorical data into numerical format. The goal is to get the optimum learning process of our model. The technique we choose is `one-hot-encoding`. The reason is the cardinality of the categorical data is not too large*","d8c224a2":"##### Probability to Get Promotion based on `Award` and `KPI`","efd7da7d":"## Describe Data","d90e6b62":"### Statistical Numerical Data","e8a01b02":"## K-Nearest Neighbors","87bfd776":"##### Distribution & Probability of Promoted Employees based on `length_of_service`\n<a id = 'length_of_service' ><\/a>","02fe663b":"#### Numerical Data","d10bb430":"**Conclusion from Statistical Numerical Data**<br>\n\n* The distribution of data for feature `no_of_trainings`, `age`, `length_of_service`, `avg_training_score` looks normal *(mean & median are close enough)*","ff9c4261":"### `performance_level`\n\nThis feature comes from combination of `previous_year_rating`, `KPIs_met >80%`, and `awards_won?`\n\nBased on the probability obtained in [promoted_by_kpi](#promoted_by_kpi), [previous_year_rating](#previous_year_rating), and [awards_won?](#awards_won?) have been shown that the probability will be higher if an employee met this 3 conditions\n\n1. An employee who met their KPI > 80 % has higher chance to get promotion than others who didn't met their KPI\n2. An employee who got 5 rating in the previous year has 9% higher chance to get promoted than others\n3. An employee who won any award in the previous year has 36% higher chance to get promotion thanothers\n\nThis feature will have performance level as its value, from 1 `(low)` to 4 `(best)`, with the provisions below:\n- if KPIs_met = 1 & rating = 5 & awards_won = 1, then `4 (Best)`\n- if KPIs_met = 1 & rating = 4\/5 & awards_won = 1\/0, then `3 (Better)`\n- if KPIs_met = 1 & rating = 3 & awards_won = 1\/0, then `2 (Good)`\n- else `1 (Low)`\n","8735b3eb":"## Fix The Duplicate Values","6897b4e8":"# Data Preparation","03822c1e":"## Random Forest hyperparameter Tuning with Randomized Search","c83738f5":"## Normalization","de36885c":"## Decision Tree","42891155":"##### Probability to Get Promotion based on `region`\n<a id = 'promoted_by_region' ><\/a>","518ff459":"### `education`","8c082b51":"# Introduction","5b2c033b":"**Remove `employee_id` since every rows have each unique values***<\/br>\n\nFrom this step, the dataset is changed into new object called `data_clean`\n* `data_clean` was created to save the clean data (fix missing value and duplicated data)\n* The *feature engineering* results will be saved into `data_clean`","40357607":"#### Check Dulicated Data","901a3a06":"##### Probability to Get Promotion based on `education`\n<a id = 'promoted_by_education' ><\/a>","42f811bd":"# Predictions","a86aa9f4":"**Team Behind This Project**<br>\n* As the final project to finish learning path for Data Science in [Rakamin_Academy](https:\/\/rakamin.com\/), this model was built from a gwoth team called **Hi5**<br>\n\n**The Team Member**<br>\n**1. Herdin Surya Dwi Putra**<br>\nemail: herdinsurya@gmail.com<br>\nphone: +6281272243710<br>\nLinkedin: www.linkedin.com\/in\/herdinsurya<br>\n\n**2. Jomen Pardede**<br>\nemail: jomenpardede@gmail.com<br>\nphone: +6282272055285<br>\nLinkedin: www.linkedin.com\/in\/jomen-pardede<br>\n\n**3. Mia Maryasha**<br>\nemail: maryashamia@gmail.com<br>\nphone: +6281285566246<br>\nLinkedin: www.linkedin.com\/in\/mia-maryasha-738723173\/<br>\n\n**4. M. Jayus Abror**<br>\nemail: mhmdabror1994@gmail.com<br>\nphone: +6282211101091<br>\nLinkedin: www.linkedin.com\/in\/muhammad-abror-b42809119<br>\n\n**5. Moh. Ardiansyah Gonti**<br>\nemail: @gmail.com<br>\nphone: +6285717368356<br>\nLinkedin: www.linkedin.com\/in\/mohammad-ardiansyah-gonti-266706110\/<br>\n\n**Our Friendly Mentor: Ade Irawan**<br>\nemail: @gmail.com<br>\nphone: +6281906486000<br>\nLinkedin: <br>\n","5d82a0ca":"### `performance_level`","0889773a":"##### Probability to Get Promotion based on `KPIs_met >80%`\n<a id = 'promoted_by_kpi'><\/a>","325a67ee":"### Univariate Analysis\n*Do data analysis for each feature separately, look the distrbution of data in details*","e46db230":"## Predict with `xgb_best_model` fom XGBoost","af484c08":"## Statistical Summary","e281e76a":"#### Check Cardinality or Unique Value from Categocial Data\n<a id = 'unique_value' ><\/a>","21e6452a":"### Separation of Categorical & Numerical Data","6700228b":"**Main Instructions**\n* Exploratory analysis from the data, create some visualization to describe the data\n* Describe the pre-processing step, how to extract and create new features, also the reason behind them\n* Split the data into training and testing with optional portion\n* Build the models with matching hyperparameter tune, choose the best model, also the reason behind that\n* Test the model with data test","f9d632c7":"## Random Forest","2b1a0348":"#### Numerical Data\n* See the correlation between features using correlation matrix","abb818e8":"##### Probability to Get Promotion based on `awards_won?`\n<a id = 'awards_won?' ><\/a>","3eedc7e8":"**There are 2 features with missing value, 1 numeric & 1 non-numeric. the `education` feature with non-numeric data types has 4.40% missing value and the `previous_year_rating` feature with numeric data types has 7.52% missing value. When compared with the total row data, the number of missing values is relatively small.**","12a942bc":"##### Probability to Get Promotion based on `department`\n<a id = 'promoted_by_department' ><\/a>","96830a88":"__*From this step, we use `df_pre` as the dataset. At the beginning, this dataset was copied from `data_clean`. So this `df_pre` will be used to save the log transformation result, norm\/std results*__\n","4c451789":"#### Non Numerical Data (Categorical)","3970de13":"### Multivariate Analysis","98dca851":"## Graphical Approach","101deca6":"## Split Train & Test","8dbfd2fc":"## XGBoost","f153d734":"#### Non - Numerical (Categorical)","c7ccb673":"__*Fix the missing value in `previous_year_rating` feature with `median()` function to fill them with the median values. The reason is the distribution of this feature looks normal (the `mean` value and `median` value is close enough)*__","b04f5f74":"**Early Brief**<br>\n* Dataset has 14 features and 54808 rows\n* 5 features are in categorical data **(inlude 1 feature as target, `is_promoted`)**\n* 9 features are in numerical data\n* There are 2 features with missing value: \n    - 2409 rows in `education`\n    - 4124 rows in `previous_year_rating`\n* There are no duplicated data, at least before data preparation ","0ba7f577":"## Feature Engineering","c7960735":"## Fix The Missing Value","746cf80d":"## XGBoost Tuning Hyperparameter","32919390":"##### Probability to Get Promotion based on `previous_year_rating`\n<a id = 'previous_year_rating' ><\/a>","326ebcd3":"### `potential_region`\n\n*Contains the information of whether an employee was placed in potential region to be promoted or not*<\/br>\n**1. based on the probability information on [promoted_by_region](#promoted_by_region), there are 8 regions that have a higher potential than other regions, namely with a probability value> 10%**<\/br>\n**2. The regions are 4, 17, 25, 28, 23, 22, 3, 7 (in the order starting from the one with the highest probability value)**<\/br>\n**3. if the employee is in the region, it will have a value of '1' otherwise it will be worth '0'**\n","ea9e0379":"##### Probability to Get Promotion based on `no_of_trainings`\n<a id = 'no_of_trainings' ><\/a>","e9e5f7b5":"##### Probability to Get Promotion based on `Gender` and `KPI`","7e687b5a":"#### Check Missing Value\n<a id = 'distribution_of_missing_value'><\/a>\n`education`\n`previous_year_rating`\n`employee_id`","09c8ec78":"# Load and Describe Data\n\nIn this project, we create our dataset into 3 different datasets:\n1. `data` as the original data, used for exploratory data analysis\n2. `data_clean` as the cleaned data form missing value and duplicated, used for data cleaning until feature engineering & encoding\n3. `df_pre` as the final dataset used for modeling. This dataset already processed with log\/norm\n","68cb9195":"##### Distribution of Promoted Employees based on `age`\n<a id = 'promoted_by_age' ><\/a>","03310bc5":"##### Probability to Get Promotion based on `gender`\n<a id = 'promoted_by_gender' ><\/a>","d9052355":"##### Distribution & Probability of Promoted Employees based on `avg_training_score`\n<a id = 'avg_training_score' ><\/a>","03f3bcfb":"### `recruitment_channel`","a82205c9":"## Logaritmik","598244f3":"**there are no duplicate from this dataset**","cb6560a6":"##### Probability to Get Promotion based on `Department` and `KPI`","0e1886d2":"## Decision Tree Tuning Hyperparameter","5f735356":"**Categorical Data Conclusion**<\/br>\n* Data majority in `gender` is male with frequency 38496\n* Data majority in `department` is Sales & Marketing with frequency 16840\n* Data majority in `education` is Bachelor's with frequency 36669\n* Data majority in `region` is region_2 with frequency 12343\n* Data majority in `recruitment_channel` is other with frequency 30446","f3fd6871":"Based on the dataset, we decided to create 4 new features using the data from existing features\n\n1. `potential_region`: contains the information of whether an employee was placed in potential region to be promoted or not\n2. `performance_level`: contains the information that show the performance level of an employee. This feature comes from combination of `previous_year_rating`, `KPIs_met >80%`, and `awards_won?`\n3. `High_Avg_Tscore`: contains the information of whether the average training score of an employee is high enough for promotion chance based on the previous data\n4. `male`: contains the information to show an employee gender"}}