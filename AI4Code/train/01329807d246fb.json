{"cell_type":{"3fe27e9b":"code","37b25cfe":"code","09b965b0":"code","64ee6b5a":"code","9acaf9b8":"code","89fbd914":"code","dba765db":"code","214a3d6b":"code","6bbbc707":"code","aecc00f0":"code","a608ad73":"code","892efa13":"code","b3bbf616":"code","87f720f1":"code","b49bbe53":"code","c28f8401":"code","6113dd74":"code","e72b2c13":"code","e8b35bd1":"code","eea3d20a":"code","166c3251":"code","d10d3f6f":"code","2784868a":"code","7e2565ce":"code","8670a257":"code","6e1141ff":"code","b589323f":"code","82a37d50":"code","d63ab45e":"code","841f28aa":"code","e49d61f5":"code","b681fcee":"code","ee8f5e27":"code","9583e4d8":"code","212efbb5":"code","48d8c0c5":"code","e22a5824":"code","6cb46aff":"code","497da88a":"code","0eb9cb65":"code","7e8d8be0":"markdown","cdc6c694":"markdown","4fe3e215":"markdown","f9b72e0f":"markdown","488a3dc2":"markdown","f5952ae4":"markdown","484026de":"markdown","00916c01":"markdown","96e63ef8":"markdown","132e5040":"markdown","442cb0dd":"markdown","fef62bf0":"markdown","a5687803":"markdown","5c6aed6f":"markdown","310e8b94":"markdown","4fffc900":"markdown","238b25bf":"markdown","feefffcb":"markdown","12921ff0":"markdown","dd546320":"markdown","3742d649":"markdown","0e18d1fb":"markdown","7e63c6ae":"markdown"},"source":{"3fe27e9b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\n#for sentiment analysis\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\n\nfrom imblearn.over_sampling import SMOTE,SVMSMOTE,ADASYN\n\nimport joblib\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\n\nfrom gensim.models import KeyedVectors\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37b25cfe":"df = pd.read_csv('..\/input\/robinhood-app-reviews-on-google-play-store\/Robinhood_GooglePlay_Reviews_enUS.csv')","09b965b0":"df.dropna(subset=['content'], inplace = True)\ndf.head()","64ee6b5a":"df.describe(include='all')","9acaf9b8":"sns.set_context('paper')\nax = sns.displot(df['score'], kde = False, color = 'seagreen', bins = 5)\nax.fig.set_figwidth(15)\nax.fig.set_figheight(10)","89fbd914":"df['datetime64'] = df['at'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')) #turn the string representation of datetime into datetime type","dba765db":"score_1 = df[df['score'] == 1]","214a3d6b":"score_1.groupby([score_1[\"datetime64\"].dt.year]).count().plot(kind = \"pie\",y = 'datetime64', figsize=(10,10), colormap='Spectral')\nplt.title(\"'When did 1\u2606 happen?'\", fontsize = 18)","6bbbc707":"ax = score_1[(score_1['datetime64'] > pd.to_datetime('2021-01-01')) & (score_1['datetime64'] < pd.to_datetime('2021-04-30'))].groupby([score_1[\"datetime64\"].dt.month,score_1[\"datetime64\"].dt.day]).count().plot(kind = \"bar\", y = \"datetime64\",  figsize=(20,10), colormap = 'Spectral')\nplt.xlabel('Date', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.title(\"'When did 1\u2606 happen?'\", fontsize = 18)","aecc00f0":"score_1.describe(include = \"all\", datetime_is_numeric=True)['score']['count']","a608ad73":"df[df['score'] == 5].groupby([df[\"datetime64\"].dt.year]).count().plot(kind = \"pie\",y = 'datetime64', figsize=(10,10), colormap='Spectral')\nplt.title(\"'When did 5\u2606 happen?'\", fontsize = 18)","892efa13":"def assign_label(x):\n    if x >= 4:\n        return 1\n    elif x <= 2:\n        return 0\n    else:\n        return 2\n    \ndf['label'] = df['score'].apply(assign_label)","b3bbf616":"Y = [label for label in df['label']]","87f720f1":"contractions = pd.read_json('..\/input\/english-contractions\/contractions.json', typ='series') #getting the list of English contractions\ncontractions = contractions.to_dict()","b49bbe53":"c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return contractions[match.group(0)]\n    return c_re.sub(replace, text)","c28f8401":"BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n\ndef clean_reviews(reviews):\n    cleaned_reviews = []\n    for review in reviews:\n        review = str(review)\n        review = review.lower()\n        review = BAD_SYMBOLS_RE.sub(' ', review)\n        \n        #expand contraction\n        review = expandContractions(review)\n\n        #remove punctuation\n        review = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", review).split())\n\n        #stop words\n        stop_words = set(stopwords.words('english'))\n        word_tokens = nltk.word_tokenize(review) \n        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n        review = ' '.join(filtered_sentence)\n        \n        cleaned_reviews.append(review)\n        \n    return cleaned_reviews","6113dd74":"X = clean_reviews([review for review in df['content']]) ","e72b2c13":"df['text'] = [x for x in X]","e8b35bd1":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 1) #only take into account positive reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Greens\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","eea3d20a":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 0) #only take into account negative reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Reds\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","166c3251":"textt = \" \".join(X[i] for i in range(len(X)) if Y[i] == 2) #only take into account neutral reviews\nwordcloud = WordCloud(width = 512,height = 512, collocations=True, colormap=\"Oranges\").generate(textt)\nplt.figure(figsize = (10, 10), facecolor = 'k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d10d3f6f":"MAX_SEQ_LENGTH = max(len(text.split()) for text in df['text']) #maximum length of a review\nMAX_FEATURES = 10000 #only consider the top 10000 most frequent words in the corpus","2784868a":"df_1 = df[df['score'] != 3] #disregard all neutral reviews","7e2565ce":"#tokenize and vectorize reviews\ncorpus = df_1['text'].values.astype('U') \ntfidf = TfidfVectorizer(max_features = MAX_FEATURES, ngram_range = (1, 2))   \ntdidf_tensor = tfidf.fit_transform(corpus)","8670a257":"X_train, X_test, Y_train, Y_test = train_test_split(tdidf_tensor, df_1['label'].values, test_size = 0.3)","6e1141ff":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, Y_train)","b589323f":"predictions = nb.predict(X_test)","82a37d50":"print(classification_report(Y_test, predictions, digits=3))","d63ab45e":"#tokenize and vectorize reviews\ncorpus = df['text'].values.astype('U') \ntfidf = TfidfVectorizer(max_features = MAX_FEATURES, ngram_range = (1, 2))  \ntdidf_tensor = tfidf.fit_transform(corpus)","841f28aa":"X_train, X_test, Y_train, Y_test = train_test_split(tdidf_tensor, df['label'].values, test_size = 0.3)","e49d61f5":"baseline_model = SVC(kernel = 'linear', decision_function_shape = 'ovo') #for multi-class classification\nbaseline_model.fit(X_train, Y_train)","b681fcee":"predictions = baseline_model.predict(X_test)","ee8f5e27":"print(classification_report(Y_test, predictions, digits=3))","9583e4d8":"baseline_model_2 = SVC(kernel = 'linear', class_weight = 'balanced', decision_function_shape = 'ovo') #setting the class weight to be balanced\nbaseline_model_2.fit(X_train, Y_train)\npredictions = baseline_model_2.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","212efbb5":"smote = SVMSMOTE()\nX_train_smote, Y_train_smote = smote.fit_resample(X_train, Y_train)","48d8c0c5":"svm_smote = SVC(kernel = 'linear', decision_function_shape = 'ovo')\nsvm_smote.fit(X_train_smote, Y_train_smote)","e22a5824":"predictions = svm_smote.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","6cb46aff":"adasyn = ADASYN()\nX_train_adasyn, Y_train_adasyn = adasyn.fit_resample(X_train, Y_train)","497da88a":"svm_adasyn = SVC(kernel = 'linear', decision_function_shape = 'ovo')\nsvm_adasyn.fit(X_train_adasyn, Y_train_adasyn)","0eb9cb65":"predictions = svm_adasyn.predict(X_test)\nprint(classification_report(Y_test, predictions, digits=3))","7e8d8be0":"Bad reviews are filled with words such as: trash, criminal, lose money, poor, corrupt. But most significant are: **market manipulation** and **hedge fund**. These are very consistent with why users boycotted the app: because they believed it was controlled by hedge funds and manipulated the supposedly \"free\" market for their own good.","cdc6c694":"For positive reviews we see a lot of \"good\" words: great, user friendly, nice, easy, fun, etc. The most notable is \"easy use\", which imho is a true statement of how streamlined and simple the app is to trade. ","4fe3e215":"It is immediately evident from this pie chart that most 1star reviews are from the year of 2021. We can continue to restrict the time range to specific months and days of 2021.","f9b72e0f":"On the first look, 89% accuracy sounds awesome for this baseline model. However, because it does not predict anything to be \"neutral\", we have reasons to suspect that this only reflects the underlying class distribution, i.e. because there is such a small number of neutral reviews for training, the model will just predict everything to be either \"positive\" or \"negative\", hence the high accuracy.\n\nHere we have a case of a significant unbalance in the training set leading to a classification model that heavily biases towards the more commons classes. ","488a3dc2":"# Sentiment Analysis: Building the dataset and quick look with Wordcloud","f5952ae4":"# Oversampling with ADASYN","484026de":"Using a very simple model we reach an accuracy of near 95% and acceptable F1 scores for both classes. Results for 0 (negative) is understandably better because we have twice as many samples compared to 1 (postive). The next challenge is to incorporate neutral reviews into the classification. We will have two challenges to overcome:  \n1. neutral reviews can be ambiguous and hard to recognize\n2. there are very few neutral reviews to train","00916c01":"On another hand, if we explore the 5\u2606 reviews we see a very balanced distribution across the years (except for 2015 when the app first came out).","96e63ef8":"A few quick things to note here:\n* **userName**: a lot of reviewers do not show their names and only appear as \"A Google User\"\n* **content** : not all review contents are unique, and from here we can speculate that many of them are probably one-(or few-) word reviews, thus the duplicates\n* **score**: there are a lot of 1s. In fact 50% of all scores are <= 1 (or just 1 in this case because there's no 0 score)\n* **replyContent**: replies from the company are large canned, which results in a rather low number of unique replies as we can see from the table","132e5040":"And finally neutral reviews have mostly very generic, non-sentimental words. ","442cb0dd":"# Baseline model using balanced class_weight parameter","fef62bf0":"Now we can focus on the NLP part of the sentiment analysis. Below we will do multiple modifications to make the reviews more \"readable\" by machine learning: \n1. make everything lowercase\n2. remove bad symbols (unwanted characters)\n3. expand contractions (for example: don't -> do not)\n4. remove punctuations\n5. remove stopwords","a5687803":"# Distributions of Scores","5c6aed6f":"# Oversampling with SMOTE","310e8b94":"Note that when we vectorize the corpus we consider both unigrams and bigrams (two consecutive words), and only consider the top 10000 most frequent words in the corpus. ","4fffc900":"# Sentiment Analysis: Positive vs. Negative with Naive Bayes","238b25bf":"First let's assign labels for reviews based on the score:\n\n* Positive (label 1): a score of 4 or 5\n* Negative (label 0): a score of 1 or 2\n* Neutral (label 2): a score of 3","feefffcb":"From the histogram, we can confirm that indeed most scores are 1. A score of 5 is the second most popular, and 3 (a neutral score) is the least likely. This means this app gets very polarizing reviews: it is either \"loved\" (score of 5) or \"hated\" (score of 1).","12921ff0":"This agrees perfectly with the fact that Robinhood started [restricting GME transactions](https:\/\/www.bloomberg.com\/news\/articles\/2021-01-28\/robinhood-clients-report-trading-restrictions-on-gamestop-amc) on Jan 28th, which caused a major upset and prompted users to leave bad reviews and boycott the app. Most 1\u2606 reviews of 2021 come from the week of Jan 28th as we see almost no reviews on other dates. It is also noteworthy that the total counts of 1\u2606 reviews on Jan 28th and Jan 29th (\\~70k) make up more than half of the all-time total of 1\u2606 reviews (\\~135k, which can be confirmed using describe() on the **score_1** dataframe). ","dd546320":"# Datetime Distribution of when 1 \u2606 happened","3742d649":"This classifier does recognize some neutral reviews, but at the cost of lowering F1 scores for the other two classes and the overall accuracy. It's easy to see that it misclassifies a lot of positive and negative reviews as neutral, resulting in a rather abysmal precision score of ~10% for the neutral class. This calls for a better way to treat the unbalance. ","0e18d1fb":"# Sentiment Analysis: 3 classes - Baseline model with tf-idf and linear SVM","7e63c6ae":"With WordCloud we can see what words\/phrases come up the most for different sentiments:"}}