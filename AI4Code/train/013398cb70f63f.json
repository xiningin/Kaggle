{"cell_type":{"2b4414de":"code","160b9a50":"code","19155cd5":"code","6250b7f1":"code","9042c1bb":"code","142dafe3":"code","f62692f0":"code","25d37809":"code","f0a13e2f":"code","bdb34e99":"code","85d54a8d":"code","47104b3d":"code","5f20d254":"code","1a5f16df":"code","20024ec9":"code","2a9e1468":"code","474c7595":"code","7bf58cbb":"code","468262e2":"code","b6591d77":"code","d959d160":"code","b40e3398":"code","162841e5":"code","eee743ee":"code","3bc444af":"code","3cee61ad":"markdown","de431148":"markdown","bf1870df":"markdown","268944e2":"markdown","c4129911":"markdown","fa29c316":"markdown","deb5ea95":"markdown","a0816e8a":"markdown","ca995dd1":"markdown","7b79b11f":"markdown","0a74cd5c":"markdown","3cd0d789":"markdown","f0ef20fc":"markdown","03aeff0f":"markdown","762f075d":"markdown","0fa490f5":"markdown","6155fdc6":"markdown","829dfd57":"markdown","750538e8":"markdown","c0b9ab57":"markdown","9c0866af":"markdown","28d53d26":"markdown","c6076d7f":"markdown","6e44183e":"markdown","ac87019b":"markdown","cb74a080":"markdown"},"source":{"2b4414de":"#importing libraries needed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nimport re\nfrom re import finditer\nimport nltk\nimport spacy\nfrom nltk.stem import PorterStemmer\nfrom collections import defaultdict\nimport seaborn as sb \nimport matplotlib.pyplot as plt","160b9a50":"# retrieving all files which contain articles we will process\nfiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith(\".json\"):\n            files.append(os.path.join(dirname, filename))\n\nfiles.sort()","19155cd5":"df=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','abstract','full_text_file','publish_time','url'])\ndf","6250b7f1":"word_stemmer = PorterStemmer()\n\n# defining virus and risk factors keywords\ntargets = [set({\"smoke\", \"pulmonary\", \"diabetes\" , \"hypertension\", \"kidney\", \"pre-existing\", \"kidney\", \"renal\", \"immune\", \"deficiency\", \"chemotherapy\",\"oncology\", \"malignant\", \"cancer\", \"neonates\", \"pregnancy\", \"pregnant\", \"neo-natal\" , \"socio-economic\", \"behavior\", \"social\",\"economic\"}),\n         set({\"covid-19\", \"covid19\", \"sars-cov-2\", \"2019-ncov\"})]\n\nreverse_map = {}\n# convert target terms into their stemmed versions for successfull matching\nfor i in range(0, len(targets)):\n    newterms = set()\n    for term in targets[i]:\n        stemmedTerm = word_stemmer.stem(term)\n        newterms.add(stemmedTerm)\n        reverse_map[stemmedTerm] = term\n    targets[i] = newterms\n\n# displaying stemmed targets\nfor term in targets:\n    print(term)","9042c1bb":"# main method which will categorize each paragraph to a specific segment\ndef segment_paragraph(segment, paper_id=None, cnt=None):\n    global targets\n    \n    sentence_tokenized = defaultdict(lambda: defaultdict(lambda: \"\"))\n    sentence_full = defaultdict(lambda: defaultdict(lambda: \"\"))\n    \n    json_txt_full = nltk.sent_tokenize(segment)\n    json_txt = [[word_stemmer.stem(y.lower()) for y in nltk.word_tokenize(x)] for x in json_txt_full]\n    if not paper_id is None:\n        sentence_tokenized[paper_id][cnt] = json_txt\n        sentence_full[paper_id][cnt] = json_txt_full\n    \n    #the list of found words.\n    keywords_found = []\n    \n    for i in range(0, len(json_txt)):\n        \n        # find target keywords in each sentence.\n        matched = set()\n        for j in range(0, len(json_txt[i])):\n            word = json_txt[i][j]                    \n            # check for match\n            for k in range(0, len(targets)):\n                if word in targets[k]:\n                    matched.add(k)\n                    # when k == 0 means it maches one of the risk factors keywords\n                    if(k == 0):\n                        keywords_count[word] += 1\n                                   \n        keywords_found.append(matched)\n        \n    #For each sentence, we check if all terms were located. \n    #If not, then we check if the missing terms were in either the preceding of following sentence.\n    \n    valid_sentence = None\n    valid_tags = None\n    tag_set = set()\n    for i in range(0, len(keywords_found)):\n        if len(keywords_found[i])==len(targets):\n            valid_sentence = json_txt_full[i]\n            valid_tags = json_txt[i]\n            break\n        \n        # at least one target is missing. check the neighbors\n        temp_set = keywords_found[i].copy()\n        if i > 0:\n            temp_set.update(keywords_found[i-1])\n            if len(temp_set)==len(targets):\n                valid_sentence = json_txt_full[i-1] + \" \" + json_txt_full[i]\n                valid_tags = json_txt[i] + (json_txt[i-1])\n                break\n                \n        temp_set = keywords_found[i].copy()\n        if i < (len(keywords_found) - 1):\n            temp_set.update(keywords_found[i+1])\n            if len(temp_set)==len(targets):\n                valid_sentence = json_txt_full[i] + \" \" + json_txt_full[i+1]\n                valid_tags = json_txt[i] + (json_txt[i+1])\n                break          \n    \n    if not valid_sentence:\n        return False, \"\", set()\n    \n    # find the set of tags that were matched\n    matched_set = set()\n    vbase = valid_tags\n    valid_tags = set(valid_tags)\n    for i in range(0, len(targets)-1):\n        matched_set = matched_set.union(targets[i])\n    valid_tags = valid_tags.intersection(matched_set)\n    \n    return True, valid_sentence, valid_tags","142dafe3":"def process_file(json_file):\n        # check if there's a match.\n        json_body = json_file[\"body_text\"]\n        temp = defaultdict(lambda x: \"\")\n        \n         # loop through each paragraph\n        for cnt, x in enumerate(json_body):\n            is_valid, valid_segment, valid_tags = segment_paragraph(x[\"text\"], json_file[\"paper_id\"], cnt)\n            if is_valid:\n                json_dict[json_file[\"paper_id\"]].append({\"text\":x[\"text\"], \"tags\":valid_tags, \"segment\":valid_segment, \"paper_id\":json_file[\"paper_id\"], \"title\":json_file[\"metadata\"][\"title\"]})\n                print (\"found match: {} : {} => {}\".format(json_file[\"paper_id\"], valid_segment, valid_tags))","f62692f0":"# json_dict will hold the tokenized sentences for each paragraph in each document\njson_dict = defaultdict(lambda:[])\n\n#tracks count of each risk factor keyword found from targets\nkeywords_count = defaultdict(int)\n\n#limit number of files processed\nstop_counter = 0\n\nfor file in files:\n    #remove this if condition to process all corpus papers\n    if(stop_counter == 500):\n        break\n    process_file(json.load(open(file, \"r\")))\n    stop_counter+=1","25d37809":"#converting keywords dictionary to a dataframe to easily analyze it\nkeywords_df = pd.DataFrame(keywords_count.items(), columns=['Keyword', 'Total_Keyword_Count'])\nkeywords_df = keywords_df.set_index('Keyword')\n\n#adding same context keywords together\nkeywords_df.loc['pregnant'] += keywords_df.loc['pregnanc']\nkeywords_df = keywords_df.drop(['pregnanc'])\n\n#sorting dataframe to show most frequently mentioned keyword per corpus\nkeywords_df = keywords_df.sort_values(by=['Total_Keyword_Count'])\n\n# Plotting keywords count across corpus\ngraph = keywords_df['Total_Keyword_Count'].plot(kind = 'barh', figsize = (10,6))\nplt.xlabel('Total_Keyword_Count')\nplt.ylabel('Keyword')\nprint(graph)","f0a13e2f":"#creating a new column holding risk factor category for each row that we mentioned above\ncategories = []\nfor keyword in keywords_df.index:\n    if(keyword == 'pulmonari' or keyword == 'smoke'):\n        categories.append('Smoking and pre-existing pulmonary diseases')\n    elif(keyword == 'pregnant' or keyword == 'neonat'):\n        categories.append('Neonates and pregnant women')\n    elif(keyword == 'behavior' or keyword == 'socio-econom' or keyword == 'social' or keyword == 'econom'):\n        categories.append('Socio-economic and behavioral factors')\n    else:\n        categories.append('Co-infections and co-morbidities')\nkeywords_df.insert(1,\"Risk_Factor_Category\", categories, True) \nkeywords_df = keywords_df.sort_values(by=['Total_Keyword_Count'] , ascending = False)\n\n#plotting frequency of mentioning each category\ngroup_df = keywords_df.groupby(\"Risk_Factor_Category\",as_index=False)['Total_Keyword_Count'].sum()\nprint(group_df.index.values)\ngraph = group_df.plot(kind = 'bar')\nplt.xlabel('Risk_Factor_Category')\nplt.xticks(group_df.index,group_df['Risk_Factor_Category'].values)\nplt.title('Frequency of mentioned topics',size = 20)\nprint(graph)","bdb34e99":"#method which gets URL of each paper using its title\ndef get_URL(title):\n    if( not df.loc[df['title'] == title]['url'].empty):\n        return \" \"+df.loc[df['title'] == title]['url'].values[0]\n    else:\n        return \"\"","85d54a8d":"topics = defaultdict(lambda: {\"text\":[], \"title\":[], \"rawtag\":\"\"})\n\nfor paper_id, found_objs in json_dict.items():\n    \n    for ele in found_objs:\n        \n        # for each tag (usually only one) see which topic this falls under\n        for tag in ele[\"tags\"]:\n            topics[reverse_map[tag]][\"text\"].append(ele[\"segment\"])\n            topics[reverse_map[tag]][\"title\"].append(ele[\"title\"])\n            topics[reverse_map[tag]][\"rawtag\"] = tag\n            \nhtmls = defaultdict(lambda: \"\")\nfor topic_name in topics:\n    htmlstr = \"<div class='test_output'>\"\n    htmlstr += \"<br \/><div style='font-weight:bold;'>{}<\/div><br \/>\".format(topic_name)\n    htmlstr += \"<div style='display:table;'>\"\n    \n    for q, entry in enumerate(topics[topic_name][\"text\"]):\n        splinter = nltk.word_tokenize(entry)\n        \n        for i in range(0, len(splinter)):\n            if word_stemmer.stem(splinter[i])==topics[topic_name][\"rawtag\"]:\n                splinter[i] = \"<span style='background-color:#FFCC33;'>\" + splinter[i] + \"<\/span>\"\n            elif word_stemmer.stem(splinter[i]) in targets[-1]:\n                splinter[i] = \"<span style='background-color:#ADD8E6;'>\" + splinter[i] + \"<\/span>\"\n        \n        if(get_URL(topics[topic_name][\"title\"][q]) != \"\"):\n            formatted = \" \".join(splinter) + \"<u> <a href=\" + get_URL(topics[topic_name][\"title\"][q]) +\">\" + \"<span style='color:#0099cc;'> [\" + topics[topic_name][\"title\"][q] + \"] <\/span>\" + \"<\/a> <\/u>\"\n        else:\n            formatted = \" \".join(splinter) + \"<span style='color:#0099cc;'> [\" + topics[topic_name][\"title\"][q] + \"] <\/span>\"\n            \n        \n        htmlstr += \"<div style='display:table-row;'>\"\n        htmlstr += \"<div style='display:table-cell;padding-right:15px;font-size:20px;'>\u2022<\/div><div style='display:table-cell;'>\" + formatted + \"<\/div>\"\n        htmlstr += \"<\/div>\"\n        \n    htmlstr += \"<\/div>\"\n    htmlstr += \"<\/div>\"\n    htmls[topic_name] = htmlstr","47104b3d":"display(HTML(htmls[\"smoke\"]))","5f20d254":"display(HTML(htmls[\"pulmonary\"]))","1a5f16df":"display(HTML(htmls[\"kidney\"]))","20024ec9":"display(HTML(htmls[\"pre-existing\"]))","2a9e1468":"display(HTML(htmls[\"renal\"]))","474c7595":"display(HTML(htmls[\"immune\"]))","7bf58cbb":"display(HTML(htmls[\"deficiency\"]))","468262e2":"display(HTML(htmls[\"chemotherapy\"]))","b6591d77":"display(HTML(htmls[\"cancer\"]))","d959d160":"display(HTML(htmls[\"malignant\"]))","b40e3398":"display(HTML(htmls[\"neonates\"]))","162841e5":"display(HTML(htmls[\"pregnant\"]))","eee743ee":"display(HTML(htmls[\"pregnancy\"]))","3bc444af":"display(HTML(htmls[\"social\"]))","3cee61ad":"# Highlights regarding potiential risk factors of smoking and pre-existing pulmonary disease\n** Note: These are not the complete results as not all json files are sent to process_article() due to time constraints, but you can do it!**","de431148":"## We want to use this analysis for two assumptions:\n* To visualize most frequent mentioned topics(from their keywords) and that rises a possibility of its importance regarding COVID-19\n* To visualize least frequent mentioned topics and that rises a possibility of the need to explore these topics more.","bf1870df":"# Send all json files from data acquired to process_article()","268944e2":"**In this notebook, we are trying to find *data on potential risks factors***\n","c4129911":"# Identify sets of unigrams(1-gram) we are searching for.\n\n1. Virus Unigrams\n    * We need to find keywords that link to the virus (example: \"COVID-19\", \"SARS-CoV-2\", etc..)\n    \n    \n2. Risk Factors Unigrams\n    * To explore risk factor of lung diseases we can search for \"smoke\" and \"pulmonary\". \n    * To explore risk factor of co-infections and co-morbidities we can search for \"diabetes\" , \"hypertension\", \"kidney\", \"pre-existing\", \"kidney\", \"renal\", \"immune\", \"deficiency\", \"chemotherapy\", \"cancer\", \"oncology\", \"malignant\" and \"cancer\"\n    * To explore risk factor of neonates and pregnant women we can search for \"neonates\", \"pregnancy\", \"pregnant\" and \"neo-natal\"\n    * To explore risk socio-economic and behavioral factors we can search for \"socio-economic\", \"behavior\", \"social\" and \"economic\"\n    ","fa29c316":"# Now better view on relative articles found \n* By highlighting virus and risk factors keywords\n* Adding their URL beside each relative paragraph found","deb5ea95":"# Highlights regarding potiential socio-economic and behavioral risk factors\n** Note: These are not the complete results as not all json files are sent to process_article() due to time constraints, but you can do it!**","a0816e8a":"# Let's Start!","ca995dd1":"**Since COVID-19 is the new puzzle all the world is trying to solve. We aim to make it easier for scientists to investigate and learn about the spreading virus to help fight it. **\n![CoronaVirusHeader.jpg](attachment:CoronaVirusHeader.jpg)","7b79b11f":"# Highlights regarding potiential risk factors of neonates and pregnant women\n** Note: These are not the complete results as not all json files are sent to process_article() due to time constraints, but you can do it!**","0a74cd5c":"# Highlights regarding potiential risk factors of co-infections and co-morbidities\n** Note: These are not the complete results as not all json files are sent to process_article() due to time constraints, but you can do it!**","3cd0d789":"### Plotting frequency of each risk factor category which are divided into 4 categories\n* Smoking and pre-existing pulmonary diseases\n* Co-infections and co-morbidities\n* Neonates and pregnant women\n* Socio-economic and behavioral factors","f0ef20fc":"# Data Used","03aeff0f":"* Getting results only related to new generation of coronavirus which is COVID-19.\n* Easily manipulating\/adding keywords that we are searching for.\n* Visualizing what are the most topics mentioned in relation with COVID-19.\n* Adding URL next to every highlight (if URL is available in dataset for the paper) for easy completion of reading when interested.","762f075d":"# Imports","0fa490f5":"# Segmenting each paragraph","6155fdc6":"### Plotting keywords count across corpus","829dfd57":"# Analysis from keywords count per corpus","750538e8":"# Introduction","c0b9ab57":"# Pros","9c0866af":"**We are using NLP (natural language processing) approaches to:**\n* Find keywords on potential risk factors in the papers and analyze these findings.\n* Get the highlights of each paper about potential risk factors and view them in an easy to make it easier to investigate those factors.","28d53d26":"* Getting faulty results pointing to COVID-19 symptoms\/effect instead of risk factor.","c6076d7f":"# Cons","6e44183e":"\"diabetes\" , \"hypertension\", \"kidney\", \"pre-existing\", \"kidney\", \"renal\", \"immune\", \"deficiency\", \"chemotherapy\", \"cancer\", \"oncology\", \"malignant\", \"cancer\", \"neonates\", \"pregnancy\", \"pregnant\", \"neo-natal\" , \"socio-economic\", \"behavior\", \"social\",\"economic\"}),\n         set({\"covid-19\", \"covid19\", \"sars-cov-2\", \"2019-ncov\"})]","ac87019b":"# Methodology","cb74a080":"# Now for each json file,we will send body text paragraph to segment_paragraph()"}}