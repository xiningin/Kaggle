{"cell_type":{"fda89496":"code","fd906367":"code","3637311d":"code","e7fd12c5":"code","ec8d3010":"code","ce4a757a":"code","d705f851":"code","02fb69a4":"code","f9b689c8":"markdown","ca3acf04":"markdown","e42389fd":"markdown","aab84bbc":"markdown"},"source":{"fda89496":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd906367":"os.listdir('..\/input\/efficientnets\/')\nPATH = 'input\/efficientnets'","3637311d":"dfs = []\ni = 0\nfor df_loc in os.listdir('..\/input\/efficientnets\/'):\n    print('..\/input\/efficientnets\/{}'.format(df_loc))\n    df = pd.read_csv('..\/input\/efficientnets\/{}'.format(df_loc))\n#     df.head()\n    dfs.append(df)\n# dfs    ","e7fd12c5":"from scipy.stats import rankdata","ec8d3010":"for i in range(4) :\n    dfs[i]['target'] = rankdata(dfs[i]['target'], method='min')\n# dfs[0]","ce4a757a":"dfs[0]['target'] = (dfs[0]['target'] + dfs[1]['target'] + dfs[2]['target'] + dfs[3]['target'])\/4","d705f851":"dfs[0]","02fb69a4":"\ndfs[0]['target'] = rankdata(dfs[0]['target'], method='min')\ndfs[0].to_csv('sol.csv' , index = False)","f9b689c8":"## In this notebook, we make an attempt to improve our blending(Or ensembling) by using ranks instead of absolute numbers.\n![image.png](attachment:image.png)\n#### Reasoning behind this logic is that the ROC-AUC metric used here only cares about the rank of the prediction(Explained Below)","ca3acf04":"## Taking advantage of this information, let's try blending using rank\n* (Not absolute values)\n* Previous score was 0.888","e42389fd":"> **Dmytro Danevskyi wrote:**\n> \n> The target metric in this competition is based on ranks rather than on actual values. That means that as long as the order of your values is fixed, the metric will stay the same.\n> \n> To illustrate:\n> \n> ```\n> target = [1, 0, 1, 1, 0]\n> preds = [0.5, 0.25, 0.2, 0.3, 0.1]\n> \n> metric = roc_auc_score(target, preds)  # 0.833\n> \n> target = [1, 0, 1, 1, 0]\n> preds = [0.7, 0.15, 0.1, 0.2, 0.05]\n> \n> metric = roc_auc_score(target, preds)  # 0.833\n> ```\n> \n> As you can see, only the rank of the predictions matters. Not the actual value.\n\n> That means that two different models that give the **same** score could actually output completely **different** values. They are not even required to be in (0, 1) range!\n> \n> ```\n> target = [1, 0, 1, 1, 0]\n> preds = [100, 25, 20, 30, 10]\n> \n> metric = roc_auc_score(target, preds)  # 0.833\n> ```\n> \n> Then, if you will try to average the predictions of two non-calibrated models, you might observe that the score is not necessarily getting better and in some cases, it could become even worse! This happens because the prediction scales of these two models are not directly comparable because of the aforementioned issue.\n> \n> How this can be fixed?\n> \n> One simple solution is to bring the predictions to the same scale, e.g. with `scipy.stats.rankdata` function. This will turn scores into ranks, i.e. `[0.7, 0.15, 0.1, 0.2, 0.05]` will be turned into `[5, 3, 2, 4, 1]`. After this, the predictions could be blended.\n> \n> Note that this not always lead to better results and is highly dependent on which exactly models are blended, how strong is the bias, and so on.\n> \n> To illustrate, I decided to naively blend my best scoring model (ResNet18) that gives 0.914 on the public leaderboard with the best-scoring public kernel available (0.927) and I got 0.925. After I preprocessed both predictions with `rankdata`, my score improved to 0.933.\n\n","aab84bbc":"* Ranking again"}}