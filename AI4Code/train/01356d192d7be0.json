{"cell_type":{"9176a6b0":"code","bbf40cff":"code","693a5c0d":"code","c7c8d7b7":"code","cda2a27a":"code","17ddad82":"code","9d6816c9":"code","4cbc88f4":"code","06e5249b":"code","4a05781e":"code","6f0ead0f":"code","b29d427d":"code","6c024c37":"code","a46912dc":"code","68fc3b07":"code","1f8ecb01":"code","6c709059":"code","cd8174e7":"code","6bcf2350":"code","5344590c":"code","0b03bb76":"code","729b9fe3":"code","1db4ee3c":"code","b6e94359":"code","fe4ab509":"code","65ae709b":"code","1ed1eca6":"code","ab6e84da":"code","93d4ff4b":"code","8104ef4d":"code","027fc340":"code","80f6f982":"code","5d403980":"code","908d22b7":"code","d21a3f7f":"code","1d067bda":"code","bf14c1d3":"code","d9371c4a":"code","cc222654":"code","4c0a5a9b":"code","654fe067":"code","0114c704":"code","89f2aa60":"code","f2ac7b0c":"code","5bc1954f":"code","1f073466":"code","78a95abd":"code","39d9ea70":"code","9432a0ad":"code","1fdf60ba":"code","eccd3275":"code","02188b59":"code","9029f3c9":"code","e38e6a3e":"code","0ab64e36":"code","1aac6c1e":"code","8df2b4b6":"code","61839f21":"code","effdcd14":"code","d033d1f2":"code","9ef1f2dc":"code","a6863887":"code","c44fd849":"code","32bd11f0":"code","7c84db45":"code","f2c84c22":"code","f8ce459f":"code","7650cbef":"code","11c38db3":"code","f78bd5f8":"code","ac599602":"code","a66090a3":"code","6836d431":"code","9e804e9f":"code","9e77591b":"code","b7030cd5":"code","ece532f2":"code","9cc2d7ac":"code","22f54bec":"code","01cc322c":"code","adff18be":"code","084f2721":"code","02f6e76b":"code","b04a71a0":"code","564684fb":"code","7d66a0e1":"code","0ce7d6c5":"code","7491ffe4":"code","90dfbc7c":"code","46ce2c6c":"code","3f8f61cb":"markdown","8b1af291":"markdown","2913d8f0":"markdown","09cf3d3d":"markdown","403fc11a":"markdown","02480d32":"markdown","4a97702b":"markdown","b8ca98a7":"markdown","f3b26dff":"markdown","17d1211d":"markdown","b18861f9":"markdown","f0a26ead":"markdown","43860866":"markdown","2a1b8182":"markdown","60018814":"markdown","53834a08":"markdown","9ab61c94":"markdown","8b68716b":"markdown","8dcf7327":"markdown","10e803c2":"markdown","b956a29a":"markdown","a231f7ab":"markdown","5e6117a0":"markdown","11c6812d":"markdown","98de3038":"markdown","9dd0cae6":"markdown","7dcd7606":"markdown","ac884336":"markdown","2d41e752":"markdown","c18b2e7f":"markdown","6d8187f5":"markdown","a6e8996c":"markdown","c78c57d7":"markdown","7e55adef":"markdown","32c9a5fe":"markdown","823b0c03":"markdown","1d113bfb":"markdown","57f4455e":"markdown","1803a216":"markdown","8ed57026":"markdown","06ef0c5d":"markdown","1733c51b":"markdown","d4b6e12d":"markdown","ab0b5546":"markdown","412199d2":"markdown","72eb4c6e":"markdown","6f798df9":"markdown","b727d159":"markdown","e83d5c25":"markdown","4abd524a":"markdown","24c4b5ae":"markdown","496bd834":"markdown","c08f7052":"markdown","c1b94b7d":"markdown","7af99c12":"markdown","a35561de":"markdown","8f577268":"markdown","01dd3f15":"markdown","1e6d8111":"markdown","70c61fba":"markdown","15d1fd7e":"markdown","12ceace4":"markdown","28841a9d":"markdown","2c79cfa7":"markdown","3a656473":"markdown","2e615608":"markdown","e355f2c3":"markdown","d576b94c":"markdown","6138d7d9":"markdown","aff3f7c0":"markdown"},"source":{"9176a6b0":"!pip install seaborn==0.11.0 # upgrading seaborn library to use newer plots and features!!","bbf40cff":"# libraries used\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n# Encoders\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Strategic imports\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# Machine learning Models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n\nfrom tensorflow import keras\nimport tensorflow as tf\n\n# imports to mute warnings\npd.options.display.max_rows=200\npd.set_option('mode.chained_assignment', None)\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nsimplefilter(\"ignore\", category=RuntimeWarning)\n\nsns.__version__","693a5c0d":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","c7c8d7b7":"train.isna().sum()","cda2a27a":"test.isna().sum()","17ddad82":"plt.figure(figsize=(12, 8))\nsns.set(font_scale=1.5)\nsns.heatmap(train.corr(), cmap='coolwarm', annot=True, annot_kws={'size':15})\nplt.show()","9d6816c9":"plt.figure(figsize=(15, 8))\nsns.violinplot(data=train, x='Pclass', hue='Sex', y='Age', inner='box')\nplt.show()","4cbc88f4":"temp = train.copy()\ntemp['Cabin'] = temp.Cabin.str.extract(pat='([A-Z])')\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\n\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0])\nax[0].set_title('Pclass-Cabin Proportions', x=0.28, y=1.04, size=25)\n\ntemp.Cabin.fillna('missing', inplace=True)\ntemp_missing = temp.loc[temp.Cabin == 'missing']\n\nsns.countplot(data=temp_missing, x='Cabin', hue='Pclass')\nax[1].set_title('Missing Cabin proportions', x=0.27, y=1.04, size=25)\n\nplt.show()","06e5249b":"temp = train.copy()\ntemp['Cabin'] = temp.Cabin.str.extract(pat='([A-Z])')\ntemp.Cabin.fillna('missing', inplace=True)\n\nsns.set(font_scale=1.4)\nfig, ax = plt.subplots(2, 2, figsize=(16, 17))\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0, 0])\nax[0, 0].set_title('Cabin-Pclass proportions', x=0.28, y=1.04, size=25)\n\n\n\ntemp.Cabin.replace({'A':'ABC', 'B':'ABC', 'C':'ABC', \n                    'D':'DE', 'E':'DE', 'F':'FG', \n                    'G':'FG', 'T':'ABC', 'missing':'M'}, inplace=True)\n\n\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0, 1])\nax[0, 1].set_title('Deck-Pclass Proportions', x=0.27, y=1.04, size=25)\nax[0, 1].set_xlabel('Deck', size=18)\n\n\n\nsns.barplot(data=temp, x='Cabin', y='Survived', hue='Pclass', ax=ax[1, 0])\nax[1, 0].set_title('Deck Survival rate', x=0.18, y=1.02, size=25)\nax[1, 0].set_xlabel('Deck', size=18)\n\n\nsns.barplot(data=temp, x='Cabin', y='Fare', hue='Pclass', ax=ax[1, 1])\nax[1, 1].set_title('Deck Fare', x=0.1, y=1.02, size=25)\nax[1, 1].set_xlabel('Deck', size=18)\n\nplt.show()","4a05781e":"def imputer(df):\n    \n    # imputing missing age values\n    \n    age_impute_series = df.groupby(['Pclass', 'Sex']).Age.transform('mean')\n    df.Age.fillna(age_impute_series, inplace=True)\n    \n    # imputing Cabin missing value.\n    df.Cabin = df.Cabin.str.extract(pat='([A-Z])')\n    \n    df.Cabin.fillna('M', inplace=True)\n    \n    df['Deck'] = df.Cabin.replace({'A':'ABC', 'B':'ABC', 'C':'ABC', 'D':'DE', 'E':'DE', 'F':'FG', \n                                   'G':'FG', 'T':'ABC'}) # we will drop the Cabin here and only take Deck from here on with us!!\n    \n    df.drop('Cabin', axis=1, inplace=True)\n    \n    # lets just finally fill all the left over missig value with the mode of the feature.\n    for feature in df.columns:\n        df[feature].fillna(df[feature].mode()[0], inplace=True)\n        \n    return df","6f0ead0f":"train_imputed = imputer(train.copy())\ntest_imputed = imputer(test.copy())","b29d427d":"cat_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Deck', 'Embarked']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nfor i, feature in enumerate(cat_features):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(data=train_imputed, x=feature, hue='Survived')   ","6c024c37":"num_features = ['Fare', 'Age']\n\nplt.figure(figsize=(16, 14))\nfor i, feature in enumerate(num_features):\n    plt.subplot(2, 2, i+1)\n    plt.hist(x=[train_imputed[feature][train_imputed['Survived'] == 1], train_imputed[feature][train_imputed['Survived']==0]],\n            stacked=True, label=['Survived', 'Not Survived'], bins=20, color=['orange', 'b'])\n    plt.legend()\n    plt.xlabel(f'{feature}', fontsize=15)\n    plt.ylabel('Count', fontsize=15)\n\nfor i, feature in enumerate(num_features):\n    plt.subplot(2, 2, i+3)\n    sns.kdeplot(data=train_imputed, x=feature, clip=[0, 200], hue='Survived', fill=True)","a46912dc":"temp = train_imputed.copy()\ntemp['Fare'] = pd.qcut(temp.Fare, 7)\n\nplt.figure(figsize=(15, 8))\nsns.countplot(data=temp, x='Fare', hue='Survived')\n\nplt.ylabel('Count', fontsize=18, labelpad=5)\nplt.xlabel('Fare', fontsize=18, labelpad=10)\nplt.tick_params(axis='x', labelsize=13)\n\nplt.title('Fare Categories Survival Count plot', y=1.03, fontsize=25, loc='Left')\nplt.show()","68fc3b07":"temp = train_imputed.copy()\ntemp['Age_cat'] = pd.cut(temp.Age, bins=[0, 5, 24, 30, 36, np.inf])\ntemp['Age_qcat'] = pd.qcut(temp.Age, 15, precision=2)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 14))\n\nsns.countplot(data=temp, x='Age_qcat', hue='Survived', ax=ax[0])\nax[0].tick_params(axis='x', labelsize=9)\n\nsns.countplot(data=temp, x='Age_cat', hue='Survived', ax=ax[1])\n\nplt.show()","1f8ecb01":"temp = train_imputed.copy()\ntemp['Ticket_type'] = pd.Series(np.where(temp.Ticket.str.contains(pat='[A-Z]'), 'alphabet', 'numeric'), index=temp.index)\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\n\ny=1.01\nsize=17\nfig, ax = plt.subplots(3, 2, figsize=(16, 19))\nsns.set(font_scale=1)\n\nsns.countplot(data=temp, x='Embarked', hue='Ticket_type', ax=ax[0, 0])\nax[0, 0].set_title('Ticket Type V\/s Embarked', size=size, loc='Left', y=y)\n\n\nsns.countplot(data=temp, x='Pclass', hue='Ticket_type', ax=ax[0, 1])\nax[0, 1].set_title('Pclass V\/s Ticket Type', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Embarked', hue='Pclass', ax=ax[1, 0])\nax[1, 0].set_title('Pclass V\/s Embarked', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Fare_cat', hue='Ticket_type', ax=ax[1, 1])\nax[1, 1].set_title('Fare Category V\/s Ticket type', size=size, loc='Left', y=y)\nax[1, 1].tick_params(axis='x', labelsize=9.5)\n\nsns.countplot(data=temp, x='Ticket_type', hue='Survived', ax=ax[2, 0])\nax[2, 0].set_title('Ticket_type - Survived Plot', size=size, loc='Left', y=y)\n\nsns.countplot(data=temp, x='Deck', hue='Ticket_type', ax=ax[2, 1])\nax[2, 1].set_title('Ticket_type - Deck Plot', size=size, loc='Left', y=y)\n\nplt.show()","6c709059":"def ticket_extractor(ticket):\n    alpha = re.sub('\\d', '', ticket)\n    if alpha:\n        return alpha\n    else:\n        num = re.search('\\d{1,9}', ticket)\n        return ticket\n    \ntemp = train_imputed.copy()\ntemp['Ticket_extracted'] = temp.Ticket.apply(ticket_extractor)\nfor i in range(len(temp.Ticket)):\n    try:\n        int(temp.Ticket_extracted.iloc[i])\n        temp.Ticket_extracted.iloc[i] = f'Num_{len(temp.Ticket_extracted.iloc[i])}'\n    except:\n        continue\n        \nfor label, pattern in [('_ca', 'C[.]?A[.]?'), ('_PC', 'PC'), ('_SOTON', 'SOTON'), ('_STON', 'STON'), \n                       ('_WC', 'W[.]?[\/]?C'), ('_SC', 'S[.]?C[.]?'), ('_A', 'A[.]?'), ('_SOC', 'S[.]?O[.]?[\/]?C'), \n                       ('_PP', 'PP'), ('_FC', '(F.C.|F.C.C.)'), ('_LS_number', 'Num_(6|7)'), ('_SS_number', 'Num_(3|4|5)'), \n                       ('rare', '^[^_]')]:\n    temp.Ticket_extracted[temp.Ticket_extracted.str.contains(pattern)] = label\n    \ntemp['Ticket_extracted'].value_counts(dropna=False)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 15))\nsns.countplot(data=temp, x='Ticket_extracted', hue='Survived', ax=ax[0])\nax[0].set_title('Extracted Ticket - Survival \"count\" plot', size=20, loc='Left', y=1.03)\n\nsns.barplot(data=temp, x='Ticket_extracted', y='Survived', ax=ax[1])\nax[1].set_title('Extracted Ticket - Survival \"chance\" plot', size=20, loc='Left', y=1.03)\n\nplt.show()","cd8174e7":"temp['Ticket_frequency'] =temp.Ticket.map(temp.Ticket.value_counts(dropna=False))\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\n\ny=1.03\nsize=25\nfig, ax = plt.subplots(2, 1, figsize=(15, 15))\nsns.set(font_scale=1.4)\n\nsns.countplot(data=temp, x='Ticket_frequency', hue='Survived', ax=ax[0])\nax[0].set_title('Ticket frequency - Survived Plot', loc='Left', y=y, size=size)\nax[0].set_xlabel('')\nax[0].set_ylabel('')\n\nsns.countplot(data=temp, x='Fare_cat', hue='Ticket_frequency', ax=ax[1])\nax[1].set_title('Ticket frequency - Fare category Plot', loc='Left', y=y, size=size)\nax[1].set_xlabel('')\nax[1].set_ylabel('')\n\nplt.show()","6bcf2350":"temp = train_imputed.copy()\ntemp['Ticket_frequency'] =temp.Ticket.map(temp.Ticket.value_counts(dropna=False))\ntemp['Fare_cat'] = pd.qcut(temp.Fare, 7, precision=2)\ntemp['Fare_per_individual'] = temp['Fare'] \/ temp['Ticket_frequency']\ntemp['Fare_per_individual_cat'] = pd.qcut(temp.Fare_per_individual, 7, precision=2)\n\ny = 1.03\nsize = 18\nsns.set(font_scale=1.2)\nfig, ax = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n\nsns.kdeplot(data=temp, x='Fare_per_individual', hue='Survived', clip=[0, 200], fill=True, ax=ax[0])\n\nsns.kdeplot(data=temp, x='Fare', hue='Survived', clip=[0, 200],  fill=True, ax=ax[1])\nax[1].set_ylabel('')\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n\nsns.countplot(data=temp, x='Fare_per_individual_cat', hue='Survived', ax=ax[0])\nax[0].tick_params(axis='x', labelsize=9.5)\n#ax[0].set_ylabel('')\n#ax[0].set_xlabel('')\n\nsns.countplot(data=temp, x='Fare_cat', hue='Survived', ax=ax[1])\nax[1].tick_params(axis='x', labelsize=9.5)\nax[1].set_ylabel('')\n#ax[1].set_xlabel('')\n\nplt.show()","5344590c":"temp = train_imputed.copy()\ntemp['Family_size'] = temp['SibSp'] + temp['Parch'] + 1\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.countplot(data=temp, x='Family_size', hue='Survived', ax=ax)\nax.set_title('Family Size - Survived Plot', size=25, loc='Left', y=1.04)\n\nplt.show()","0b03bb76":"temp['Family_size_cat'] = temp['Family_size'].replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                      ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                      ,8:'large_family', 9:'large_family', 10:'large_family', \n                                                       11:'large_family'})\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\n\nsns.countplot(data=temp, x='Family_size_cat', hue='Survived', ax=ax)\nax.set_title('Family Category - Survived Plot', size=25, loc='Left', y=1.04)\n\nplt.show()","729b9fe3":"temp = train_imputed.copy()\ntemp['Name_length'] = temp.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.histplot(data=temp, x='Name_length', hue='Survived', kde=True, fill=True, ax=ax)\nax.set_title('Name Length - Survived Plot', size=20, loc='Left', y=1.03)\n\nplt.show()","1db4ee3c":"temp = train_imputed.copy()\n\ntemp['Title'] = temp.Name.str.extract(pat='([a-zA-Z]+\\.)')\n\ntemp.Title[~temp.Title.isin(['Mr.', 'Miss.', 'Mrs.', 'Master.'])] = 'rare'","b6e94359":"fig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.countplot(data=temp, x='Title', hue='Survived', ax=ax)\nax.set_title('Title - Survived Plot', loc='Left', size=25, y=1.03)\nplt.show()","fe4ab509":"temp = train_imputed.copy()\ntemp['Family_name'] = temp.Name.str.split(',', n=1, expand=True).iloc[:, 0]\ntemp['Family_size'] = temp['SibSp'] + temp['Parch'] + 1\ntemp['Ticket_frequency'] = temp.Ticket.map(temp.Ticket.value_counts()) \n\ntemp['Family_survival_rate'] = temp.Family_name.map(temp.groupby(['Family_name']).Survived.median())\ntemp['Ticket_group_survival_rate'] = temp.Ticket.map(temp.groupby(['Ticket']).Survived.median())\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8), sharey=True)\n\nsns.countplot(data=temp[temp['Family_size'] > 1], x='Family_survival_rate', hue='Survived', ax=ax[0])\n# we will only take into account those instance with Family size > 1 for visualization to explore the effectiveness of the \n# Survival rate feature\n\nsns.countplot(data=temp[temp['Ticket_frequency'] > 1], x='Ticket_group_survival_rate', hue='Survived', ax=ax[1])\n# we will only take into account those instance with Ticket_frequency > 1 for visualization to explore the effectiveness of the \n# Survival rate feature\n\nplt.show()","65ae709b":"def feature_creator(df_train, df_test):\n    \n    # creating binned fare feature\n    df_train['Fare_cat'] = pd.qcut(df_train['Fare'], 7)\n    df_test['Fare_cat'] = pd.qcut(df_test['Fare'], 7)\n    \n    df_train['Fare_cat'] = LabelEncoder().fit_transform(df_train['Fare_cat'])\n    df_test['Fare_cat'] = LabelEncoder().fit_transform(df_test['Fare_cat'])\n    \n    \n    \n    # creating binned age feature\n    df_train['Age_cat'] = pd.cut(df_train.Age, bins=[0, 5, 24, 30, 36, np.inf])\n    df_test['Age_cat'] = pd.cut(df_test.Age, bins=[0, 5, 24, 30, 36, np.inf])\n    \n    df_train['Age_cat'] = LabelEncoder().fit_transform(df_train['Age_cat'])\n    df_test['Age_cat'] = LabelEncoder().fit_transform(df_test['Age_cat'])\n    \n    \n    \n    # Title feature\n    for df in [df_train, df_test]:\n        df['Title'] = df.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        df.Title[~df.Title.isin(['Mr.', 'Miss.', 'Mrs.', 'Master.'])] = 'rare'\n        \n        \n\n    # creating Ticket frequency feature\n    df_full = pd.concat([df_train, df_test], axis=0)\n    df_full['Ticket_frequency'] = df_full.Ticket.map(df_full.Ticket.value_counts())\n    df_train = df_full.loc[df_train.index]\n    df_test = df_full.loc[df_test.index].loc[df_test.index].drop('Survived', axis=1)\n    \n    \n    \n    # creating extracted ticket feature\n    for df in [df_train, df_test]:\n        \n        df['Ticket_extracted'] = df.Ticket.apply(ticket_extractor)\n        for i in range(len(df.Ticket)):\n            try:\n                int(df.Ticket_extracted.iloc[i])\n                df.Ticket_extracted.iloc[i] = f'Num_{len(df.Ticket_extracted.iloc[i])}'\n            except:\n                continue\n\n        for label, pattern in [('_ca', 'C[.]?A[.]?'), ('_PC', 'PC'), ('_SOTON', 'SOTON'), ('_STON', 'STON'), \n                               ('_WC', 'W[.]?[\/]?C'), ('_SC', 'S[.]?C[.]?'), ('_A', 'A[.]?'), ('_SOC', 'S[.]?O[.]?[\/]?C'), \n                               ('_PP', 'PP'), ('_FC', '(F.C.|F.C.C.)'), ('_LS_number', 'Num_(6|7)'), \n                               ('_SS_number', 'Num_(3|4|5)'), ('rare', '^[^_]')]:\n            df.Ticket_extracted[df.Ticket_extracted.str.contains(pattern)] = label\n            \n            \n    \n    # Fare per individual feature\n    #df_train['Fare_per_individual'] = df_train['Fare'] \/ df_train['Ticket_frequency']\n    #df_test['Fare_per_individual'] = df_test['Fare'] \/ df_test['Ticket_frequency']\n    \n    #df_train['Fare_per_individual_cat'] = pd.qcut(df_train['Fare_per_individual'], 7)\n    #df_test['Fare_per_individual_cat'] = pd.qcut(df_test['Fare_per_individual'], 7)\n    \n    #df_train['Fare_per_individual_cat'] = LabelEncoder().fit_transform(df_train['Fare_per_individual_cat'])\n    #df_test['Fare_per_individual_cat'] = LabelEncoder().fit_transform(df_test['Fare_per_individual_cat'])\n    \n    \n    \n    # Family size feature\n    df_train['Family_size'] = df_train['SibSp'] + df_train['Parch'] + 1\n    df_test['Family_size'] = df_test['SibSp'] + df_test['Parch'] + 1\n    \n    \n    \n    # creating binned family sized feature\n    df_train['Family_size_cat'] = df_train.Family_size.replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                               ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                               ,8:'large_family', 9:'large_family', 10:'large_family'\n                                                               ,11:'large_family'})\n    \n    df_test['Family_size_cat'] = df_test.Family_size.replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                               ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                               ,8:'large_family', 9:'large_family', 10:'large_family' \n                                                               ,11:'large_family'})\n    \n    \n    \n    # Name length feature\n    df_train['Name_length'] = df_train.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\n    df_test['Name_length'] = df_test.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\n    \n    \n    \n    # survival rate feature\n    df_full = pd.concat([df_train, df_test], axis=0).reset_index()\n\n    df_full['Family_name'] = df_full.Name.str.split(',', n=1, expand=True).iloc[:, 0]\n    \n    df_full['Family_Survival'] = 0.5\n    \n\n    for grp, grp_df in df_full.groupby(['Family_name', 'Fare']):\n        if (len(grp_df) != 1):\n            # Fare and Family_name are used as category to group families together, as these features should be same for\n            # families and once we get the groups we filter for grp_df which have ore than 1 length, hence a family.\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n\n    for _, grp_df in df_full.groupby('Ticket'):\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                    smax = grp_df.drop(ind)['Survived'].max()\n                    smin = grp_df.drop(ind)['Survived'].min()\n                    passID = row['PassengerId']\n                    if (smax == 1.0):\n                        df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n                    elif (smin==0.0):\n                        df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n     \n    df_full.set_index('PassengerId', inplace=True)\n    df_train = df_full[:891]\n    df_test = df_full[891:]\n    \n    \n    # finally we will drop the family name feature we created to help us create the Survival rate feature.\n    df_train.drop(['Family_name'], axis=1, inplace=True)\n    df_test.drop(['Family_name', 'Survived'], axis=1, inplace=True)\n    \n    # lets finally drop the survived from our training set and create the target variable\n    y_train =df_train.Survived\n    df_train.drop('Survived', axis=1, inplace=True)\n    \n    return df_train, y_train, df_test","1ed1eca6":"X_train, y_train, X_test = feature_creator(train_imputed.copy(), test_imputed.copy())\ny_train = y_train.astype(int)","ab6e84da":"X_train.head(5)","93d4ff4b":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n\nsize = 20\ny = 1.03\nX_train['Family_Survival'] = np.round(X_train.Family_Survival, 2)\n\nsns.countplot(data=pd.concat([X_train, y_train], axis=1), x='Family_Survival', hue='Survived', ax=ax[0])\nax[0].set_title('Survival rate in training set', loc='Left', size=size, y=y)\n\n\nX_test.Family_Survival = np.round(X_test.Family_Survival, 2)\nsns.countplot(data=X_test, x='Family_Survival', ax=ax[1])\nax[1].set_title('Survival rate - Count plot test set', loc='Left', size=size, y=y)\n\nplt.show()","8104ef4d":"# if we wish to create a transformer which we can grid search in a pipeline, that transformer has to inherit from the \n# BaseEstimator class, we will also make our FeatureEngineering transformer inherit from the TransformerMixin which just \n# provides the functionality of fit_transform() method to our transformer.\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self, bin_fare=False, bin_age=True, family_size=True, bin_family_size=True,\n                 drop_Name_length=False, drop_Ticket_frequency=False, drop_all=True, drop_Family_Survival=True, \n                 drop_Ticket_extracted=False, scaling='StandardScaler', target_encode_title=True, test=False):\n        self.bin_fare = bin_fare\n        self.bin_age = bin_age\n        #self.bin_fare_individual_cat = bin_fare_individual_cat\n        self.family_size = family_size\n        self.bin_family_size = bin_family_size\n        self.drop_Name_length = drop_Name_length\n        self.drop_Ticket_frequency = drop_Ticket_frequency\n        self.drop_all = drop_all\n        self.drop_Ticket_extracted = drop_Ticket_extracted\n        self.scaling = scaling\n        self.drop_Family_Survival = drop_Family_Survival\n        self.target_encode_title = target_encode_title\n        self.transformer = None\n        self.drop_list = []\n        self.test = test\n        \n    def fit(self, X, y=None):\n        return self # the fit method usually return self only, if not sure, try fitting a scikit learn estimator\/transformer.. \n                    # only the fitted object will be returned, fit() will include code in case of these transformers and\n                    # estimators but we do not need to have some fitted variables on our dataset before we go for transform,\n                    # or atleast the way i am doing it i don't need to. :) \n    \n    def transform(self, X, y=None):\n        \n        X['Pclass'] = X['Pclass'].astype(str) # import convert numeric values to str if we wish to use get_dummies on them.\n        dummies = pd.get_dummies(X.loc[:, ['Pclass', 'Sex']], drop_first=True)\n        X = pd.concat([X, dummies], axis=1)\n        X.drop(['Pclass', 'Sex'], axis=1, inplace=True)\n        \n        # lets also drop Name and Ticket feature as we have extracted what we needed!!\n        X.drop(['Name', 'Ticket'], axis=1, inplace=True)\n        \n        \n        \n        \n        if self.bin_fare:\n            # so if we self.bin_fare = True we want the binned fare, so drop the continuous fare feature.\n            self.drop_list.append('Fare')\n            \n        else:\n            self.drop_list.append('Fare_cat')\n            \n        \n        if self.bin_age:\n            self.drop_list.append('Age')\n        \n        else:\n            self.drop_list.append('Age_cat')\n            \n            \n        #if self.bin_fare_individual_cat:\n            #self.drop_list.append('Fare_per_individual')\n            \n        #else:\n            #self.drop_list.append('Fare_per_individual_cat')\n                \n                \n                \n                \n        if self.family_size:\n            self.drop_list.extend(['SibSp', 'Parch'])\n            \n            if self.bin_family_size:\n                self.drop_list.append('Family_size')\n                \n            else:\n                self.drop_list.append('Family_size_cat')\n                \n        else:\n            self.drop_list.extend(['Family_size', 'Family_size_cat'])\n            \n        if self.drop_Name_length:\n            X.drop('Name_length', axis=1, inplace=True)\n            \n        if self.drop_Ticket_frequency:\n            X.drop('Ticket_frequency', axis=1, inplace=True)\n            \n        if self.drop_Family_Survival:\n            X.drop('Family_Survival', axis=1, inplace=True)\n            \n        if self.drop_all:\n            X.drop(self.drop_list, axis=1, inplace=True)\n            \n       \n        self.cols = ['Title', 'Ticket_extracted'] if self.target_encode_title else ['Ticket_extracted']\n        \n        if not self.test and len(X) > 400:\n            # we want the encoder object to be fitted with traing set only so we will check 'not self.test' and we also do not\n            # want to program to pass through the if statement in the case of cross validation test is being considered, \n            # hence we also check len(X) > 400 assuming the cross val test size will always stay less than 400.\n            self.target_encoder = TargetEncoder(cols=self.cols, smoothing=5)\n            X.loc[:, self.cols] = self.target_encoder.fit_transform(X.loc[:, self.cols], y_train.loc[X.index])\n\n        else:\n            X.loc[:, self.cols] = self.target_encoder.transform(X.loc[:, self.cols])\n                \n                \n        if self.drop_Ticket_extracted:\n            X.drop('Ticket_extracted', axis=1, inplace=True)\n        \n        # finally lets convert all remaining categorical features into on hot features\n        X = pd.get_dummies(X, drop_first=True)  \n     \n        # only features with no. of unique values as 2, will one hot encoded features and we do not want to apply Normalization\n        # on them(reason explained in a bit)\n        features_to_scale = [feature for feature in X.columns if X[feature].nunique() != 2]  \n        if not self.test and len(X) > 400:    \n            if self.scaling == 'StandardScaler':\n                self.transformer = StandardScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n                \n            elif self.scaling == 'RobustScaler':\n                self.transformer = RobustScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n                \n            elif self.scaling == 'MinMaxScaler':\n                self.transformer = MinMaxScaler()\n                X.loc[:, features_to_scale] = self.transformer.fit_transform(X.loc[:, features_to_scale])\n        \n        else:\n            X.loc[:, features_to_scale] = self.transformer.transform(X.loc[:, features_to_scale])\n            \n        return X\n            ","027fc340":"def results(grid, n=10):\n    for index, row in pd.DataFrame(grid.cv_results_)[['params', \n                                                      'mean_test_score']].nlargest(n=n, columns='mean_test_score').iterrows():\n        print(f'{row[0]} : {row[1]}')","80f6f982":"param_grid_pipeline = {'feature_engineering__bin_fare':[True, False],\n                       'feature_engineering__bin_age':[True, False],\n                       #'feature_engineering__bin_fare_individual_cat':[True, False],\n                       'feature_engineering__family_size':[True, False],\n                       'feature_engineering__bin_family_size':[True, False],\n                       'feature_engineering__drop_Name_length':[False],\n                       'feature_engineering__drop_Ticket_frequency':[False],\n                       'feature_engineering__drop_all':[True],\n                       'feature_engineering__drop_Ticket_extracted':[True, False],\n                       'feature_engineering__target_encode_title':[True, False],\n                       'feature_engineering__scaling':['StandardScaler'],\n                       'feature_engineering__drop_Family_Survival':[True]} \n\n# we will always drop Survival rate during grid search because we wish to find the best dataset without including it.\n\npipeline_ = Pipeline([('feature_engineering', FeatureEngineering()),\n                 ('model', LogisticRegression())])\n\ncv = StratifiedShuffleSplit(n_splits=20, test_size=0.15, random_state=101)\n\ngrid = GridSearchCV(pipeline_, param_grid_pipeline, \n                    cv=cv.split(X_train, X_train.Deck),\n                    scoring='accuracy', verbose=2, n_jobs=-1)\n\ngrid.fit(X_train.copy(), y_train)","5d403980":"pd.DataFrame(grid.cv_results_)['mean_test_score'].isna().sum()","908d22b7":"results(grid, n=20)","d21a3f7f":"fe = FeatureEngineering(bin_fare=False, bin_age=True, family_size=True, bin_family_size=True,\n                 drop_Name_length=False, drop_Ticket_frequency=False, drop_all=True, drop_Family_Survival=False, \n                 drop_Ticket_extracted=False, scaling='StandardScaler', target_encode_title=False, test=False)","1d067bda":"X_train_fe = fe.fit_transform(X_train.copy())","bf14c1d3":"X_train_fe","d9371c4a":"fe.test = True","cc222654":"X_test_fe = fe.transform(X_test.copy())","4c0a5a9b":"X_test_fe","654fe067":"def parameter_plot(model, X, y, n_estimators=[100, 200, 300, 400, 600, 900, 1300, 1700, 2000, 2500], \n                   hyper_param=None, **kwargs):\n    param_name, param_vals = hyper_param\n    param_grid = {'n_estimators':n_estimators,\n                  f'{param_name}':param_vals}\n    \n    grid = GridSearchCV(model(**kwargs), param_grid, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42), \n                        scoring='accuracy', n_jobs=-1, verbose=2)\n    grid.fit(X, y)\n    results = pd.DataFrame(grid.cv_results_)['mean_test_score'].values\n    results = results.reshape(len(param_vals), len(n_estimators))\n    \n    plt.figure(figsize=(15, 9))\n    for i in range(1, len(param_vals) + 1):\n        plt.plot(n_estimators, results[i-1], label=f'{param_name} - {param_vals[i-1]}')\n      \n    plt.legend()\n    plt.show()","0114c704":"def learning_curve_plotter(Model, X, y, params_1, params_2, step=50):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n    \n    plt.figure(figsize=(16, 7))\n    for i, (name, params) in enumerate([params_1, params_2]):\n        train_score = []\n        val_score = []\n        plt.subplot(1, 2, i+1)\n        for j in range(100, len(X_train), step):\n            model = Model(**params).fit(X_train[:j], y_train[:j])\n            train_score.append(model.score(X_train[:j], y_train[:j]))\n            val_score.append(model.score(X_test, y_test))\n            \n        plt.plot(train_score, 'r-', label='Training accuracy')\n        plt.plot(val_score, 'b-', label='Validation accuracy')\n        plt.title(f'{name}')\n        plt.xlabel('Training set size')\n        plt.ylabel('Accuracy')\n        plt.legend()\n            \n    plt.show()","89f2aa60":"param_grid_logreg = {'penalty':['elasticnet'],\n                     'C':0.01 * np.arange(100),\n                     'l1_ratio':0.1 * np.arange(10),\n                     'solver':['saga']} # these params are the best params","f2ac7b0c":"grid_logreg = GridSearchCV(LogisticRegression(), param_grid_logreg, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","5bc1954f":"grid_logreg.fit(X_train_fe, y_train)","1f073466":"results(grid_logreg, n=60)","78a95abd":"params_logreg = {'C': 0.28, 'l1_ratio': 0.9, 'penalty': 'elasticnet', 'solver': 'saga'}","39d9ea70":"param_grid_knn = {'n_neighbors':np.arange(50),\n                  'weights':['uniform'],\n                  'algorithm':['ball_tree'],\n                  'leaf_size':np.arange(1, 40, 2)}","9432a0ad":"grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","1fdf60ba":"grid_knn.fit(X_train_fe, y_train)","eccd3275":"results(grid_knn, n=60)","02188b59":"params_knn = {'algorithm': 'ball_tree', 'leaf_size': 1, 'n_neighbors': 7, 'weights': 'uniform'}","9029f3c9":"param_grid_svc = {'C':[0.001, 0.01, 0.1, 1, 5],\n                  'kernel':['rbf'],\n                  'gamma':0.01 * np.arange(100),\n                  'probability':[True]}","e38e6a3e":"grid_svc = GridSearchCV(SVC(), param_grid_svc, cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","0ab64e36":"grid_svc.fit(X_train_fe, y_train)","1aac6c1e":"results(grid_svc, n=60)","8df2b4b6":"params_svc = {'C': 1, 'gamma': 0.09, 'kernel': 'rbf', 'probability': True} # first set of params with lowest C and gamma","61839f21":"parameter_plot(RandomForestClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 8, 9, None]))","effdcd14":"param_grid_random = {'n_estimators':[300, 500, 1000],\n                     'max_depth':[5, 9],\n                     'max_samples':[0.5, 0.7, 0.9],\n                     'max_features':[0.5, 0.7, 0.9], \n                     'min_samples_split':[2, 5, 8]\n                    } ","d033d1f2":"grid_random = GridSearchCV(RandomForestClassifier(), param_grid_random, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","9ef1f2dc":"grid_random.fit(X_train_fe, y_train)","a6863887":"results(grid_random, n=162)","c44fd849":"params_1 = ('Regularized model', {'max_depth': 5, 'max_features': 0.5, 'max_samples': 0.9,\n                                  'min_samples_split': 8, 'n_estimators': 300} )\n\nparams_2 = ('Best model', {'max_depth': 9, 'max_features': 0.7, 'max_samples': 0.9, \n                           'min_samples_split': 8, 'n_estimators': 300})\n\nlearning_curve_plotter(RandomForestClassifier, X_train_fe, y_train, params_1, params_2, step=30)","32bd11f0":"params_random = {'max_depth': 5, 'max_features': 0.5, 'max_samples': 0.9,\n                                  'min_samples_split': 8, 'n_estimators': 300}","7c84db45":"parameter_plot(GradientBoostingClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6, 9]))","f2c84c22":"parameter_plot(GradientBoostingClassifier, X_train_fe, y_train, hyper_param=('learning_rate', [0.01, 0.03, 0.05, 0.07, 0.1]),\n              max_depth=3)","f8ce459f":"param_grid_gradient = {'max_depth':[3, 4],\n                       'n_estimators':[300, 400, 500],\n                       'learning_rate':[0.01, 0.03, 0.05],\n                       'subsample':[0.5, 0.7],\n                       'max_features':[0.5, 0.7],\n                       #'min_samples_split':[2, 5, 8, 12]\n                      }","7650cbef":"grid_gradient = GridSearchCV(GradientBoostingClassifier(), param_grid_gradient, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","11c38db3":"grid_gradient.fit(X_train_fe, y_train)","f78bd5f8":"results(grid_gradient, n=60)","ac599602":"params_1 = ('Regularized model', {'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.5, \n                                  'n_estimators': 500, 'subsample': 0.5})\nparams_2 = ('Best model', {'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.7, \n                           'n_estimators': 400, 'subsample': 0.7})\n\nlearning_curve_plotter(GradientBoostingClassifier, X_train_fe, y_train, params_1, params_2, step=30)","a66090a3":"params_gradient = {'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.5, 'n_estimators': 500, 'subsample': 0.5}","6836d431":"parameter_plot(XGBClassifier, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6]))","9e804e9f":"parameter_plot(XGBClassifier, X_train_fe, y_train, \n               hyper_param=('learning_rate', [0.01, 0.03, 0.05, 0.07, 0.1]), max_depth=4) ","9e77591b":"param_grid_xgb = {'n_estimators':[400, 600],\n                  'learning_rate':[0.01, 0.03, 0.05],\n                  'max_depth':[3, 4],\n                  'subsample':[0.5, 0.7],\n                  'colsample_bylevel':[0.5, 0.7],\n                  'reg_lambda':[15, None],\n                 }","b7030cd5":"grid_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, \n                        cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='accuracy', verbose=2, n_jobs=-1)","ece532f2":"grid_xgb.fit(X_train_fe, y_train)","9cc2d7ac":"results(grid_xgb, n=60)","22f54bec":"params_1 = ('Regularized model',{'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 400,\n                                 'reg_lambda': 15, 'subsample': 0.5})\nparams_2 = ('Best model', {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 600, \n                           'reg_lambda': None, 'subsample': 0.7} )\n\nlearning_curve_plotter(XGBClassifier, X_train_fe, y_train, params_1, params_2, step=50)","01cc322c":"params_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}","adff18be":"logreg = LogisticRegression(**params_logreg)\nsvc = SVC(**params_svc)\nknn = KNeighborsClassifier(**params_knn)\nrfc = RandomForestClassifier(**params_random)\ngradient = GradientBoostingClassifier(**params_gradient)\nxgb = XGBClassifier(**params_xgb)\n\nestimators = [('logreg', logreg), ('knn', knn), ('svc', svc), ('rfc', rfc), ('gradient', gradient), \n              ('xgb', xgb)]\n\n\nstack = StackingClassifier(estimators=estimators,\n                           cv=10, n_jobs=-1)","084f2721":"y_preds = logreg.fit(X_train_fe, y_train).predict(X_test_fe)","02f6e76b":"y_preds = svc.fit(X_train_fe, y_train).predict(X_test_fe)","b04a71a0":"y_preds = knn.fit(X_train_fe, y_train).predict(X_test_fe)","564684fb":"y_preds = rfc.fit(X_train_fe, y_train).predict(X_test_fe)","7d66a0e1":"y_preds = gradient.fit(X_train_fe, y_train).predict(X_test_fe)","0ce7d6c5":"y_preds = xgb.fit(X_train_fe, y_train).predict(X_test_fe)","7491ffe4":"submission = pd.DataFrame({'PassengerId':test.index, \n              'Survived':y_preds})","90dfbc7c":"submission.to_csv('submission.csv', index=False)","46ce2c6c":"pd.read_csv('submission.csv')","3f8f61cb":"# Complete Beginner's guide to top 3%\n\nIn the following notebook i have tried to give information about:\n* intuition behind EDA practices undertaken in notebook.\n* Various traps that beginners should be aware of.\n* Comprehensive model hyper parameter tuning guideline.\n* Stacking","8b1af291":"If found the kernel useful, upvote it !!  :)\n\nIf you have any suggestions please provide them in comment section.","2913d8f0":"Everything looks great, there are trends which all the categories in categorical feature follow with target. We can observe that it will be beneficial to combine the SibSp and Parch into a common feature named Family_size and further simplification into category such as 'Alone', 'Small_family', 'large_family' can help us capture the trend better, which is that for small family the survival rate is better, a person is more likely to survive if they had 2 to 4 Family size(Parch + SibSp)!!","09cf3d3d":"**Findings:**\n* Looking at the above results, model loved the family size feature and specially when binned!!\n* bin fare feature was also liked, though it gets a little confused between binning and not.\n* it likes the Name length too.\n* Ticket_frequency was liked too.\n* I also let the model decide whether it wishes to work with all the features or we should drop the feature we appended in the self.drop_list, it did not like not dropping them.. which was what we expected.\n* it is a little confused between RobustScaler and StandardScaler, lets test out both..\n\nso finally, we will create dataset using following features...\n\n* bin_fare = [True, False] -- finally we pick **False**\n* family_size = True \n* bin_family_size = True\n* drop_Name_length = False \n* drop_Ticket_frequency = True\n* drop_all = True  \n* scaling = ['StandardScaler', 'RobustScaler'] -- finally we pick **StandardScaler**\n\n\nSo we have our final few datasets candidates!!!","403fc11a":"***Now how does this feature help us get better results??***\n\nFirst we have to understand that getting an accuracy score of 80% and getting to top 4% of the leaderboard is possible just by tunning model hyper parameters well and performing good feature encoding practices, but moving up from there is very difficult unless you have feature engineered very good features such as the one we are creating right now... The difference of 80% to 84% accuracy is just of about 2-4 more correct survival predictions. i got up to 79 % with just stacking and good encoding practice, check it out ->https:\/\/www.kaggle.com\/awwalmalhi\/comprehensive-beginners-guide-to-top-6\n\nLets say we have few family members or ticket group members which were not in training set but in test set, we can easily, with confidence assign a survival rate to them in the test set depending upon the above results and turns out that ususally people are able to push from 75% to 81-83% with this feature!! just by predicting few more stubborn instances as 1!!","02480d32":"### RandomForestClassifier","4a97702b":"### GradientBoostingClassifier","b8ca98a7":"As seen from above, no general trend is visible by categorizing ticket with or without alphabets. There are few points to note though: \n* Mostly numeric tickets were sold at the 'Q' which is Queenstown.\n* Proportion of ticket with alphabets bought is less for Pclass 3 as seen from 'Pclass V\/s Ticket type plot'.\n\n\nTicket type does not show any patterns in the Fare V\/s Ticket type plot..... it was kind of disappointing :(\n\nOverall there is no pattern in Ticket type V\/s Fare, Deck, Pclass.... This feature does not look that good.\n\n\nFinally, it may become useful... again.. you already know by now :D...we will have to test it out in Dataset creation section using grid search.\n\n\nNow lets extract even finer ticket types, maybe model finds some use out of it... we should check it out!!","f3b26dff":"### KNeighborsClassifier","17d1211d":"Great!! we can see that with increase in the name length the survival rate increases!","b18861f9":"### XGBClassifier","f0a26ead":"### Logistic Regression","43860866":"### Name Title","2a1b8182":"One of the features that can be derived from the Name feature is the Name length, the idea behind this is that people with higher status usually had longer names, we can use this fact to create a feature with name lengths.\nI got to know about this relation in this excellent notebook -> https:\/\/www.kaggle.com\/sreevishnudamodaran\/ultimate-eda-fe-neural-network-model-top-2","60018814":"In the data provided to us, each individual has a SibSp and Parch, so if we add these features together for an individual and add 1 to it, 1 being the individual themself, we can create a new feature Family_size.\n\nIt is an important feature as it reveals that passengers with family size 2 - 4 had a better survival rate than passengers travelling alone or who had larger families.","53834a08":"The feature looks more balanced now and we can see that now we can say that deck 'M' is the deck where most of the pclass 2 and 3 are present whereas deck ABC is where most pclass 1 are present. We can see from the plots that deck DE had the highest survival rate!!! and deck 'M' has the lowest survival rate. \n\nThe great thing about this simplification from 'Cabin' to 'Deck' is that we have cornered the Pclass 2 and 3 passengers which had the lowest survival rates in new deck M we created!! in all the other decks it seems that they had better chance of survival!! Compare the **Deck-Pclass Proportions** and the **Deck Survival rate**, you can clearly see that those few Pclass 2 and 3 passenger in deck DE and FG have good survival chance, most of the Pclass 2 and 3 passengers are in the Deck 'M' and their survival rate is not good!!\n\nThis feature helps to harness this pattern!!!","9ab61c94":"**Findings:**\n* We can clearly see from the above plot that the survival chance for people with ticket frequency from 2-4 was more.\n* We were also able to uncover new information, it seems that higher frequency mean higher ticket fare which is actually the cummulative ticket price for all passenger, which means that Fare column does not contain the fare for an individual person but the total fare for a group with same ticket!! \n* we might be able to derive a new feature which is fare_per_individual, lets check it out if it is good or not!!","8b68716b":"from the second plot we can see that (0.0, 5.0] is a great age bracket for survival, whereas (24.0, 34.0] is the worst, for the (30.0, 36.0] category chances seems to 50-50, i have also plotted a qcut plot with 15 quantiles, just to show that how i visualized the age distributions in the second plot, for the second plot i wanted to have categories with some significant info gain, the above plot shows some good survival rate distribution for the categories we have selected after we went through the first plot.... Just a small trick to bin features to get better info gain for beginners to keep in mind.. \n\n**After testing out**:\n\nThe Age_cat feature improved the results for XGBClassifier and the GradientBoostingCLassifier significantly, observed 1% improvement from continuous age feature to Age_cat!!!","8dcf7327":"In this section we will be finding best hyper parameters for a number of models with different strengths and weaknesses so that we can finally blend the results and achieve higher accuracy. The idea is that every model has different strengths and weaknesses, there will be instances that,  for eg. SVC classifies correctly but LogisticRegression cannot, but LogisticRegression might classify some other instances correctly which SVC cannot, these difference will be learnt by the final **meta model** in stacking and we will be able to achieve higher accuracy.\n\nWe will train the following models as base models for stacking:\n* LogisticRegression\n* SVC\n* KNeighborsClassifier\n* RandomForestClassifier\n* GradientBoostingClassifier\n* XGBCLassifier\n\n\n#### Tips:\n\n* When grid searching hyperparameters you should look at top few best results to get a better idea, if you find a parameter mix which gives you a decent result(not the best) but it is more regulrized than the top result, choose the regularized option and accept the loss in accuracy(if its not a lot), This will ensure that the variance is low and you will actually have more chance of achieving similar result with first model than the best model as per grid search. \n\n* The above practice becomes a cumbersome for tree ensemble methods if you just have to go through the grid search results, simple reason being that there are too many parameters which affect the bias and variance, unlike LogisticRegression or SVC. In this case we will plot the results!!! check out the code below.","10e803c2":"The above plot shows that Mrs., Miss. and Master. titles had better chances if survival, chances of survival is less than that for survival in case of Mr. and rare title, but it is worse for the Mr. title.","b956a29a":"### Binning age feature","a231f7ab":"We can see that the meidan age(white dot in the middle) for each gender is slightly different and there is a general trend of decrease in median age for both the genders from Pclass 1 to 3.","5e6117a0":"Its quite clear from the above plot that which set of params we are going to use, So, you should not just pick the best grid search result.... check for a regularized options that give you decent accuracy result too.","11c6812d":"***Missing value imputation***\n* imputing Age feature\n* imputing Embarked feature\n\nWe were able to find better ways to impute the Age and Embarked feature with reasonable support from the present data explored via Visualizations, you should too explore the data through EDA and find out best ways to impute data rather straight away using sklearn.impute or .fillna()\n\n\n***Feature creation through EDA***\n\nwe were able to explore feature simplification, combination possibilities and possibilities to derive new features with proper support for these decisions from the present data...\n * Bin Fare feature to simplify and express patterns\n * Bin Age feature to simplify and express patterns\n * Better way to extract information from Ticket feature and creation of Ticket_frequency feature and Ticket_extracted feature\n * through the Ticket frequency feature we were able to gain information regarding Fare_per_individual feature and possibility of binning it.\n * Family size feature and possibility of simplification\n * Name length feature\n * Survival rate feature\n \n \nuntil now we have created few features and explored possibilities for simplification, we have many possible ways to create the final dataset. There are a lot of decision we will have to make now, some of them are as follows:\n* Is binned Fare feature better or the continuous one?\n* is Binned Age feature better of the continuous one?\n* Should we keep the SibSp and Parch rather than the Combined Family size feature?\n* If we should keep the Family_size feature, should we simplify it?\n* if we simplify the Family_size feature, should we one hot encode it or Target encode it?\n* What features should we drop?\n* Should we use StandardScaler, MinMaxScaler or RobustScaler?\n\n\nThese are some of good questions that come to my mind... There can be many more, but lets keep things simple or the class will become difficult to debug XD","98de3038":"***This is the most important feature that almost all the top leaderboard submissions will have.***\n\n\nIntuition behind the Survival rate feature:\n\nIt was observed that usually people belonging to a family and Ticket group survived together or died together, this can help us assign survival rate for instances in test set which belonged to a family or ticket group in training set, this can help us make it easier for the model predict few more instances and boost the score... All this made more clear by the visualiations below!!\n","9dd0cae6":"### EDA conclusion","7dcd7606":"lower learning rates are good!! lets test out small learning rates with a slightly higher one, we can see that as n_estiators increase the model starts to overfit and the accuracy goes down.","ac884336":"The reg_lambda : None, max_depth:4 and n_estimators:600 which are more than the regularized model are the features which are making the best model overfit the data.","2d41e752":"From the Fare plot we can see that as the fare increases the ratio of Survived to Not Survived also increases, we might be able to simplify the Fare feature into bins and capture this trend in a better way, it might or might not be helpful to do so, so we will have to cross check this decision by **grid searching this through a pipeline**(*detailed explanation on this technique later*)!! \n","c18b2e7f":"Ticket feature contains tickets with few of them with alphabets and some of them with only numbers, we could extract this relation but it might not be useful as it may be the case that this relation is related to the Embarked feature and depending upon where you Embarked you might have ticket code in only numbers or with some alphabets... rather we can also extract the frequency of tickets, there are people with same ticket numbers which shows that group of friends and family travelled together and with same ticket number.\n\nfirst lets check out if just dividing ticket into ticket with alphabets and only numeric serial code helps or not... Then we will perform even finer ticket categorization.","6d8187f5":"# Hyper parameter Tuning","a6e8996c":"Pclass correlates best with the age feature, so we should use the pclass to impute the missing age values!! now lets check if there are any relationship of age with respect to pclass and sex combined","c78c57d7":"This is an important EDA step, to explore the distribution of values of feature with the Target, this EDA step provides us with good understanding of the features and helps us uncover the possibility of feature simplification, combinination etc.","7e55adef":"Yup!! looks great, this is the trend we observed in the plot in last section. Binning helped us express this relation better or we can say that we are able it visualize this feature better as humans, but maybe the continuous form is better as per the algorithm... But atleast we know what we wish to check out by asking this question to our model that what it likes and what it does not(*More on this in Data set creation section*)!! \n\nIf we wish to use the feature as above, we will have to perform Ordinal encoding, where we encode the higher fare category with higher numeric value, for eg:- (-0.001, 7.55] -> 1, (7.55, 7.854] -> 2, (7.854, 8.05] -> 3 and so on. \n\n**Warning**:-\n\nWe can use LabelEncoder from scikit learn api to perform ordinal encoding only because there is order already present in our category which can be correctly understood by the LabelEncoder. But if the order present cannot be understood by the LabelEncoder for eg: Freezing, Cold, Warm, Hot, Lava Hot will be encoded as 2, 1, 5, 3, 4 on the basis of alphabetical order, which is not the correct order for this ordinal feature, it should be 1, 2, 3, 4, 5!! \n\n**Always beware of this trap!!!**","32c9a5fe":"#### Lets summarize the way we just grid searched good parameters\n\n* we first checked out our tree ensemble model's performance on the given dataset and then shortlisted range of max depths and n_estimators which are the mone of the most influential parameters.\n\n* then we grid searched the shorlisted parameter values and with some other parameters such as max_samples and max_features, these parameters are must to search as they diversify our trees and make them less similar to each other.\n\n* after we get the results, we then start searching the small parameter space near the optimal parameters obtained from gridsearch, we will usually see increase in accuracy for next few small gridsearchs this way.\n\n* Check for overfitting and pick that model that has good accuracy with least overfitting...","823b0c03":"Well, things are crystal clear now!! those passengers which were assigned survival rate 0 because the median survival of the group was 0 actually only very few of them survived!! same the case when we see the passengers with median family survival rate 1, majority survived!!","1d113bfb":"**Findings :**\n* From the above plot we can see that the probability distribution function did not show much difference, for the fare_per_individual it just got squeezed to the left and the corresponding values in the y axis were scaled equally...\n* But when we tried out making fare_per_individual category, we can see that fare_per_individual category gave us a better steady increase in survival rate from left to righ!!! \n\nUntil now we have Pointed out a lot of possibilities for feature engineering and there so many tough decisions to make as per what to keep and what to drop, we only want the best, informative features to achieve best results, But there is no shortcut to this, we cannot just simply know what works best with the given data and our models.... We will have to test it all out!! Though the Feature Engineering class will help us to quickly narrow down our search for the best data sets by quickly dropping out dataset made by selecting features which are not good. ","57f4455e":"Looks great!! the survival pattern is quite clear... but can we simplify and combine the family size 2-4 as 'small family' and family size of 1 as 'alone' to simplify the feature?","1803a216":"So we only have missing values in Age, Cabin for both train and test, Embarked and Fare have missing values for train and test respectively, we can impute them simply using median or mean of the whole feature.","8ed57026":"* With survival rate : 0.799\n* Without survival rate : 0.787","06ef0c5d":"### Survival Rate","1733c51b":"### Ticket based features","d4b6e12d":"### Family Size Feature","ab0b5546":"We can see that max depth of 3 is not good in terms of accuracy, but it will be great to prevent overitting... as we increase the depth accuracy increases, Lets pick max depth of 9 and 5 to test out, later we will see that though max depth 9 gives better accuracy in grid search, it tends to create a overfitted model whereas max depth 5 will create a model with slightly lower accuracy but significantly balanced","412199d2":"Looks decent, what attracts my attention is the ticket A, PC, CA, Large_Serial_number and Small_Serial_number as they are quite a few of these tickets to be confident in the survival chances shown by these tickets. This feature might help us improve the probability of survival of few passengers and may push there probability of survival over 50% if they actually survived, to get classified as survived, which is actually the problem with many instances as they get classified as not survived, i.e probability of survival is lower than 50%.","72eb4c6e":"* With survival rate : 0.803\n* Without survival rate : 0.7751","6f798df9":"### Dataset creation","b727d159":"**Small Note before we start:**\n\nThere are a lot of excellent EDA kernels, but few of them combine training and test set to perform EDA, for the given dataset i do not think it is as big of a deal(why i think so, i will explain at the end) as it should be, you should never combine train and test set!! titanic dataset is the starting point of many individuals in the field of data science and looking at this practice can make it very easy for beginners to develop the habit of peeking into the test set, i would go as far as saying that once you have separated test and training sets you should consider test set as something that you will get in future once you have trained the best model as per training data, consider it as something which is only given to you when you are to make final predictions.\n\nNow why do i think it is not too bad to do so, reason is that... if we look at it from competition point of view, using the test data to extract information which can be helpful to make it easier for making predictions on test data and boost your raking is plausible, Given that titanic dataset is something which will always stay of the same size, that is no new out of sample data will be generated , such practice can help boost score. But it is deterimental for beginners to see such practice in action, developing the habit to combine test and train datasets is easy, the pros who do it in there kernels already now the implications, but a beginner does not. So i wanted to just point this out before you begin going through the kernel, We have avoided combining test and training dataset in EDA, but we will be combining them to derive the Ticket_frequency feature, i will try to give a plausible explanation for the same when we create that feature in Feature engineering section, But lets say if data set was not of same size for eternity, we will not create Ticket frequency feature if peeking at the test data was the only option!!\n","e83d5c25":"### Binning the Fare feature","4abd524a":"max depth of 3 looks good","24c4b5ae":"# Stacking","496bd834":"* With survival rate : 0.811\n* Without survival rate : 0.7966","c08f7052":"### Name Length Feature","c1b94b7d":"* With survival rate : 0.8133\n* Without survival rate : 0.7918","7af99c12":"What is of importance to us is the left plot where with certain confidence we have assigned the survival rates to the instances and we have about 175 instances for which we have some certainity of final outcome.","a35561de":"**Now we are ready to ask questions!!!** Lets create FeatureEngineering class and use it in a pipeline with Logistic Regression(we can use other estimators too, but i prefer to use a simple one to keep computational expense low and search fast), we will pass the questions we wish to ask as the parameter to this class in the pipeline and grid search the whole feature space to narrow down our search for the best dataset!!\n\n* Is binned Fare feature better or the continuous one?\n* is Binned Age feature better of the continuous one?\n* Should we keep the SibSp and Parch rather than the Combined Family size feature?\n* If we should keep the Family_size feature, should we simplify it?\n* if we simplify the Family_size feature, should we one hot encode it or Target encode it?\n* What features should we drop?\n* Should we use StandardScaler, MinMaxScaler or RobustScaler?\n\nBefore we get to the above Q\/A setion, lets make few transformations i have noted below.\n* Pclass and Sex to one hot encodings\n* Drop Name and Ticket","8f577268":"* With survival rate : 0.806\n* Without survival rate : 0.782","01dd3f15":"### SVC","1e6d8111":"Stacking as above yielded 0.799 with survival rate and 0.784 without survival rate, an improvement of 0.145 from without survival rate feature... But i expected more and when i explored the individual model accuracy, i knew why i did not find many notebooks with survival rate feature and stacking...\n\n* With survival rate : 0.799\n* Without survival rate : 0.784","70c61fba":"* With survival rate : 0.806\n* Without survival rate : 0.7942","15d1fd7e":"For the titanc dataset many have observed that binning the continuous features helped improve the results slightly, as you can see that binning can let you express the survival chances a little better and in a simpler way, So always test this out with other datasets you work with and test out whether it helps or not!!","12ceace4":"# Exploratory Data Analysis\n\n\n\n## Missing value imputation\n\nFor beginners EDA becomes quite an overwhelming task, sometimes it may even be difficult to take the first step, you just don't know where to start and how to ask questions to your data!! My advice is to just start with plotting the relationship of continuous and categorical features with the target. If you have a continuous variable, look at its ditribution to find if it is skewed or not.\n\n\nWhile imputing missing values it is always beneficial to perform a correlation check and see which features correlate well with the feature we wish to impute, then you may make some visualizations to better understand the way you should impute the data.","28841a9d":"## Feature creation through EDA\n\n\n### Target Distribution","2c79cfa7":"**Note**:\n\nThe way we created the survival rate feature has been copied from the following notebook -->https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83, i tried creating my own, but it did not work!! :P.... to be honset in my personal opinion features created by leaking target into the dataset is not a good practice and there is not much learning one could gain by observing such practices... such techniques do not exist in real work data science, Datasets are huge and there will be lot of out of sample data from multiple sources, you can't rely on leaking target into the dataset and overfitting your model on the training set, so your model will perform horribly with such leaked features on out of sample data... One could argue that the way we created this feature is somewhat similar to target encoding, where we use target to encode the categorical feature.. But that is why target encoded features are notorious for making the model overfit the data and require many techniques to appease this problem such as adding smoothing etc. **I am yet to find results as good as 83-85% and without using survival rate type feature** if someone can reach it, thats remarkable!!!\n\nwhy does this work in case titanic dataset?\n\nwell simple reason is, as we discussed above, there are features which have deeper logical relation with target which humans can find out but not machines and the dataset is small, with same kind of noise in training and test and bound to stay of same size for eternity... So you can get away with using such feature and boosting accuracy...","3a656473":"One way to impute values would be to just fill in the Cabins using the proportions as shown above. The missing values for Cabins in Pclass 1 as we see in the right plot, are less than all the other classes and these missing values will be imputed into the Cabins A,B,C,D,E only as they are the only Cabins where the Pclass 1 passengers are present, depending upon the already available Pclass 1 passenger propotions in these Cabins as shown left plot, But imputing values like this will create noise in already available information!! We might get a great result if the missing values got imputed correctly, But that will depend upon luck. So we should just impute the missing value as 'missing', atleast we retain the already available information free of noise.\n\nWe should also just simplify this feature and Combine Cabin A, B, C as deck ABC and D, E as DE and F, G as FG.","2e615608":"Now the next features with missing value is the Cabin feature, but the Cabin feature has a lot of missing value for us to arrive at a acceptable conclusion to impute missing values using correlation, and the feature has to many missing values for us to simply impute the missing values with the mean or median for the values of the feature.","e355f2c3":"You can see that we are only scaling the features which are not like the one hot encoded columns or in general which do not have zeros as most of the column entries, this is because if we were to pass the one hot encoded columns through the a StandardScaler or RobustScaler, all the zeros will get converted to some other value when scaled and the one hot encoding will be wrecked.. MinMaxScaler will retain it only if the min value is 0, We can understand this property of scaling transformers when we look at there functions, StandardScaler subtracts mean of all values by the value being scaled in numerator, but MinMaxScaler subtracts the min value by the value being scaled, hence if we are scaling 0 and 0 is the min value we retain 0, But that wont be the case for the StandardScaler.... So look out for this detail.","d576b94c":"Lets impute the missing values as per the above understandings...","6138d7d9":"# Feature Engineering","aff3f7c0":"Yup, things look simpler and neat!! But again, it looks good to us but model might not find it as helpful as we think, maybe it could have found patterns with those family size numbers and some other feature, which it would not be able to harness as easily now, you never know... what we can be sure about is that we have made the feature simpler, if we one hot encode it we will have fewer dimensions, which is a good thing. Lets say we had many features which had to be one hot encoded but there cardinality(number of different categories in categorical feature) was high, label encoding was not an option and if we one hot encoded the feature we would be struck by curse of dimensionality, in such case if all the categorical features could be simplified in such a easy, beautiful way, nothing like that!!\n\nBut we do not have that problem at present, we have a small dataset and not aot of features with high cardinality, so we will have to figure out whether this feature helps or not. "}}