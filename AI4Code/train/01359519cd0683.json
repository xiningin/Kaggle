{"cell_type":{"b2e56936":"code","2f36edbf":"code","2fe4a8b1":"code","d32f6bee":"code","9e4ca640":"code","6882fd78":"code","c5622976":"code","3125ede2":"code","b756814a":"code","67dc7f24":"code","7ca10391":"code","420721a2":"code","9bb578b8":"code","9630e3e3":"code","6d5cfe8c":"code","874008b2":"code","4f1400fa":"code","807b120a":"code","c13374d7":"code","11b35273":"code","d3cbaf66":"code","ce430d0d":"code","a218653d":"code","eed58e8e":"code","6078e54c":"code","b91b457b":"code","5bbbc1ce":"code","16e2ac9a":"code","543cce3a":"code","634a0858":"code","bfeec8cc":"code","37ba6571":"code","d939c193":"code","a6dc6e73":"code","5e18fd58":"code","f34ba34c":"code","ed3669b4":"code","9f160998":"code","97ef2a0b":"code","0cad0c8b":"code","2dd2f1d6":"code","3400f849":"code","0a4ea449":"code","fc4f1feb":"code","06a959b2":"markdown","7c60362f":"markdown","b1bbf7cd":"markdown","df3b9f21":"markdown","92f782b8":"markdown","c74241ac":"markdown","1120e0c6":"markdown","e98272c4":"markdown","d88c2ef8":"markdown","6c9b82c0":"markdown","e1aedaab":"markdown","4c9eaf2b":"markdown","26bad8d9":"markdown","dd1d4ac4":"markdown","f6aa6452":"markdown","85242d8b":"markdown","3c4895cf":"markdown","9c2242b8":"markdown","8960fe29":"markdown","23227cd7":"markdown"},"source":{"b2e56936":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport gc\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Any results you write to the current directory are saved as output.","2f36edbf":"#application_train\/test\napp_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Testing data shape:',app_train.shape)\napp_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape:',app_test.shape)\napp_train.head()\napp_test.head()","2fe4a8b1":"#bureau\nbureau = pd.read_csv('..\/input\/bureau.csv')\nprint('Testing data shape:',bureau.shape)\nbureau.head()","d32f6bee":"#bureau\nbure_bal = pd.read_csv('..\/input\/bureau_balance.csv')\nprint('Testing data shape:',bure_bal.shape)\nbure_bal.head()","9e4ca640":"#previous_application\nprev = pd.read_csv('..\/input\/previous_application.csv')\nprint('Testing data shape:',prev.shape)\nprev.head()","6882fd78":"# POS_CASH_BALANCE\npos_cash = pd.read_csv('..\/input\/POS_CASH_balance.csv')\nprint('Testing data shape:',pos_cash.shape)\npos_cash.head()","c5622976":"# credit_card_balance\ncc = pd.read_csv('..\/input\/credit_card_balance.csv')\nprint('Testing data shape:',cc.shape)\ncc.head()","3125ede2":"# installments_payment:\ninstal = pd.read_csv('..\/input\/installments_payments.csv')\nprint('Testing data shape:',instal.shape)\ninstal.head()","b756814a":"#checking missing data app_train\ntotal = app_train.isnull().sum().sort_values(ascending = False)\npercent = (app_train.isnull().sum()\/app_train.isnull().count()*100).sort_values(ascending = False)\nmissing_app_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_app_train_data.head(20)","67dc7f24":"#checking missing data app_test\ntotal = app_test.isnull().sum().sort_values(ascending = False)\npercent = (app_test.isnull().sum()\/app_test.isnull().count()*100).sort_values(ascending = False)\nmissing_app_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_app_test_data.head(20)","7ca10391":"#missing data bureau\ntotal = bureau.isnull().sum().sort_values(ascending = False)\npercent = (bureau.isnull().sum()\/bureau.isnull().count()*100).sort_values(ascending = False)\nmissing_bureau_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_bureau_data.head(20)","420721a2":"#missing data bureau_balance\ntotal = bure_bal.isnull().sum().sort_values(ascending = False)\npercent = (bure_bal.isnull().sum()\/bure_bal.isnull().count()*100).sort_values(ascending = False)\nmissing_bure_bal_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_bure_bal_data.head(20)","9bb578b8":"#missing data previous_application\ntotal = prev.isnull().sum().sort_values(ascending = False)\npercent = (prev.isnull().sum()\/prev.isnull().count()*100).sort_values(ascending = False)\nmissing_prev_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_prev_data.head(20)","9630e3e3":"#missing data POS_CASH_balance\ntotal = pos_cash.isnull().sum().sort_values(ascending = False)\npercent = (pos_cash.isnull().sum()\/pos_cash.isnull().count()*100).sort_values(ascending = False)\nmissing_pos_cash_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_pos_cash_data.head(20)","6d5cfe8c":"#missing data POS_CASH_balance\ntotal = cc.isnull().sum().sort_values(ascending = False)\npercent = (cc.isnull().sum()\/cc.isnull().count()*100).sort_values(ascending = False)\nmissing_cc_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_cc_data.head(20)","874008b2":"print('There are {} features in the app_train'.format(len(app_train.columns)))\nprint('There are {} features in the app_test'.format(len(app_test.columns)))\nprint('There are {} features in the bureau'.format(len(bureau.columns)))\nprint('There are {} features in the bure_bal'.format(len(bure_bal.columns)))\nprint('There are {} features in the bure_bal'.format(len(prev.columns)))\nprint('There are {} features in the pos_cash'.format(len(pos_cash.columns)))\nprint('There are {} features in the credit_card_balance'.format(len(cc.columns)))","4f1400fa":"# 3.3 Categorical features excoding\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns,dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\napp_train1,new_columns = one_hot_encoder(app_train)","807b120a":"#Collection of target and feature\nfor f in new_columns:\n    print('Target Correlation by:', f)\n    print(app_train1[[f, 'TARGET']].groupby(f, as_index=False).sum())\n    print('-'*10, '\\n')","c13374d7":"app_train['TARGET'].value_counts()","11b35273":"app_train1 = app_train\napp_train['CODE_GENDER'].value_counts()\nprint(app_train['TARGET'].sum())","d3cbaf66":"#Collection of target and feature\ncolumns = [f for f in app_train.columns]\nCategoric_feats = app_train.select_dtypes('object').apply(pd.Series.nunique,axis = 0)\nk = app_train['TARGET'].sum()\n\nfor f in columns:\n    if f in Categoric_feats:\n        print('Target Correlation by:', f)\n        print(app_train[[f, 'TARGET']].groupby(f, as_index=False).sum())\n        print('-'*10, '\\n')\n\n#using crosstabs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.crosstab.html\n#print(pd.crosstab(app_train['Title'],app_train['TARGET']))","ce430d0d":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#\nplt.figure(figsize=[16,6])\n# plt.subplot(231)\n# plt.boxplot(app_train['EXT_SOURCE_1'], showmeans = True, meanline = True)\n# plt.title('EXT_SOURCE_1 Boxplot')\n# plt.ylabel('EXT_SOURCE_1')\n\n# plt.subplot(232)\n# plt.boxplot(app_train['EXT_SOURCE_2'], showmeans = True, meanline = True)\n# plt.title('EXT_SOURCE_2 Boxplot')\n# plt.ylabel('EXT_SOURCE_2 ')\n\n# plt.subplot(233)\n# plt.boxplot(app_train['EXT_SOURCE_3'], showmeans = True, meanline = True)\n# plt.title('EXT_SOURCE_3 Boxplot')\n# plt.ylabel('EXT_SOURCE_3 ')\n\nplt.subplot(131)\nplt.hist(x = [app_train[app_train['TARGET']==1]['EXT_SOURCE_1'], app_train[app_train['TARGET']==0]['EXT_SOURCE_1']], \n         stacked=True, color = ['g','r'],label = ['1','0'])\nplt.title('EXT_SOURCE_1 have difficulties or not')\nplt.xlabel('EXT_SOURCE_1 ($)')\nplt.ylabel('payment difficulties')\nplt.legend()\n\nplt.subplot(132)\nplt.hist(x = [app_train[app_train['TARGET']==1]['EXT_SOURCE_2'], app_train[app_train['TARGET']==0]['EXT_SOURCE_2']], \n         stacked=True, color = ['g','r'],label = ['1','0'])\nplt.title('EXT_SOURCE_2 have difficulties or not')\nplt.xlabel('EXT_SOURCE_2 ($)')\nplt.ylabel('payment difficulties')\nplt.legend()\n\nplt.subplot(133)\nplt.hist(x = [app_train[app_train['TARGET']==1]['EXT_SOURCE_3'], app_train[app_train['TARGET']==0]['EXT_SOURCE_3']], \n         stacked=True, color = ['g','r'],label = ['1','0'])\nplt.title('EXT_SOURCE_3 have difficulties or not')\nplt.xlabel('EXT_SOURCE_3($)')\nplt.ylabel('payment difficulties')\nplt.legend()","a218653d":"#copy data\napp_train1 = app_train","eed58e8e":"def LabelPrep(df):\n    x = []\n    for i in df.unique():\n        x.append(i)\n    return x    \nx_NAME_TYPE_SUITE = LabelPrep(app_train['NAME_TYPE_SUITE'])\nprint(x_NAME_TYPE_SUITE)\nx_NAME_TYPE_SUITE1 = {}","6078e54c":"x_NAME_TYPE_SUITE = LabelPrep(app_train['NAME_TYPE_SUITE'])\nfig, saxis = plt.subplots(2, 3,figsize=(20,16))\n\nsns.barplot(x = 'CODE_GENDER', y = 'TARGET', data=app_train, ax = saxis[0,0])\nsns.barplot(x = 'NAME_TYPE_SUITE', y = 'TARGET', data=app_train, ax = saxis[0,1])\nsaxis[0,1].set_xticklabels(saxis[0,1].get_xticklabels(), rotation=30)\nsns.barplot(x = 'NAME_HOUSING_TYPE', y = 'TARGET',data=app_train, ax = saxis[0,2])\nsaxis[0,2].set_xticklabels(saxis[0,2].get_xticklabels(), rotation=-30)\n\nsns.pointplot(x = 'CODE_GENDER', y = 'TARGET', data=app_train, ax = saxis[1,0])\nsns.pointplot(x = 'NAME_TYPE_SUITE', y = 'TARGET', data=app_train, ax = saxis[1,1])\nsaxis[1,1].set_xticklabels(saxis[1,1].get_xticklabels(), rotation=30)\nsns.pointplot(x = 'NAME_HOUSING_TYPE', y = 'TARGET', data=app_train, ax = saxis[1,2])\nsaxis[1,2].set_xticklabels(saxis[1,2].get_xticklabels(), rotation=30)","b91b457b":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,12))\n\n#AMT_GOODS_PRICE\uff1aLoan annuity\nsns.boxplot(x = 'CODE_GENDER', y = 'AMT_GOODS_PRICE', hue = 'TARGET', data = app_train, ax = axis1)\naxis1.set_title('CODE_GENDER vs AMT_GOODS_PRICE Comparison')\n\n#DAYS_EMPLOYED How many days before the application the person started current employment\nsns.violinplot(x = 'CODE_GENDER', y = 'DAYS_EMPLOYED', hue = 'TARGET', data = app_train, split = True, ax = axis2)\naxis2.set_title('CODE_GENDER vs DAYS_EMPLOYED Comparison')\n\n#DAYS_BIRTH How many days before the application did client change his registration\nsns.boxplot(x = 'CODE_GENDER', y ='DAYS_BIRTH', hue = 'TARGET', data = app_train, ax = axis3)\naxis3.set_title('CODE_GENDER vs DAYS_BIRTH Comparison')\n","5bbbc1ce":"#graph distribution of qualitative data: between two categorical feature\nfig, qaxis = plt.subplots(1,3,figsize=(18,12))\n\nsns.barplot(x = 'FLAG_OWN_CAR', y = 'TARGET', hue = 'CODE_GENDER', data=app_train, ax = qaxis[0])\naxis1.set_title('FLAG_OWN_CAR vs CODE_GENDER')\n\nsns.barplot(x = 'FLAG_OWN_CAR', y = 'TARGET', hue = 'FLAG_OWN_REALTY', data=app_train, ax  = qaxis[1])\naxis1.set_title('FLAG_OWN_CAR vs FLAG_OWN_REALTY')\n\nsns.barplot(x = 'HOUSETYPE_MODE', y = 'TARGET', hue = 'CODE_GENDER', data=app_train, ax  = qaxis[2])\naxis1.set_title('HOUSETYPE_MODE vs CODE_GENDER Survival Comparison')","16e2ac9a":"#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(18,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"WEEKDAY_APPR_PROCESS_START\", y=\"TARGET\", hue=\"CODE_GENDER\", data=app_train,\n              palette={\"F\": \"blue\", \"M\": \"pink\",\"XNA\":\"yellow\"},\n              markers=[\"*\", \"o\",\"x\"],ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"NAME_FAMILY_STATUS\", y=\"TARGET\", hue=\"CODE_GENDER\", data=app_train,\n              palette={\"F\": \"blue\", \"M\": \"pink\",\"XNA\":\"yellow\"},\n              markers=[\"*\", \"o\",\"x\"], ax = maxis2)","543cce3a":"#copy\/Delete part of the modification.\napp_train = pd.read_csv('..\/input\/application_train.csv')\napp_train1 = app_train","634a0858":"replc = app_train['NAME_FAMILY_STATUS'].unique()\n# print(len(replc))\n# app_train['NAME_FAMILY_STATUS'].replace('Single \/ not married','a')\n# app_train['NAME_FAMILY_STATUS'].replace('Married','b')\n# app_train['NAME_FAMILY_STATUS'].replace('Married','b')\n\napp_train['NFS1'] = app_train['NAME_FAMILY_STATUS']\na = 0\nb = ['a','b','c','d','e','f']\nfor i in replc:\n    app_train['NFS1'] = app_train['NFS1'].replace(i,b[a])\n    a =a+1\n# print(app_train['NAME_FAMILY_STATUS'][1])\napp_train['NFS1'].unique()","bfeec8cc":"#facetgrid: https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html\ne = sns.FacetGrid(app_train, col = 'NFS1')\ne.map(sns.pointplot, 'FLAG_OWN_CAR', 'TARGET', 'FLAG_OWN_REALTY', ci=95.0, palette = 'deep')\n# e.set(xlim = ['a','b','c','d','e','f'])\ne.add_legend()","37ba6571":"del app_train\ngc.collect()\napp_train = app_train1","d939c193":"#plot distributions of number of enquiries to Credit Bureau about the client one hour before application\na = sns.FacetGrid(app_train, hue = 'TARGET', aspect=4 )\na.map(sns.kdeplot, 'AMT_REQ_CREDIT_BUREAU_HOUR', shade= True )\na.set(xlim=(0 , app_train['AMT_REQ_CREDIT_BUREAU_HOUR'].max()))\na.add_legend()","a6dc6e73":"#plot distributions of \na = sns.FacetGrid(app_train, hue = 'TARGET', aspect=4 )\na.map(sns.kdeplot, 'AMT_INCOME_TOTAL', shade= True )\na.set(xlim=(0 , app_train['AMT_INCOME_TOTAL'].max()))\na.add_legend()","5e18fd58":"#histogram comparison of 'TARGET', WEEKDAY_APPR_PROCESS_START','CODE_GENDER'\napp_train['NCT1'] = app_train['NAME_CONTRACT_TYPE']\napp_train['CG1'] = app_train['CODE_GENDER']\nh = sns.FacetGrid(app_train, col = 'CG1', row = 'NCT1', hue = 'TARGET')\nh.map(plt.hist, 'EXT_SOURCE_1',alpha = 0.5)\nh.add_legend()","f34ba34c":"del app_train\ngc.collect()\napp_train = app_train1","ed3669b4":"import random\ncolumns1 = app_train.columns\nprint(columns)\ncolumns = [i for i in columns1]","9f160998":"#Explore relationships between selecting 8 features \nimport random\ncolumns1 = app_train.columns\ncolumns = [i for i in columns1]\ncol_to_index = {columns[i]:'var'+str(i) for i in range(len(columns))}\ncorrCols = random.sample(columns,8)\ncorrCols.append('TARGET')\nsampleDf = app_train[corrCols]\nfor col in corrCols:\n    if col != 'TARGET':\n        sampleDf.rename(columns = {col:col_to_index[col]},inplace = True)\npp = sns.pairplot(sampleDf, hue = 'TARGET', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","97ef2a0b":"# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('..\/input\/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('..\/input\/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    #find if documents in the application_train\/test\n    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n\n    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n    \n    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_ANNUITY']\n    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] \/ (1 + df['CNT_CHILDREN'])\n    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] \/ (1 + df['AMT_INCOME_TOTAL'])\n    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_BIRTH']\n    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] \/ df['DAYS_EMPLOYED']\n    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_BIRTH']\n    df['NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_EMPLOYED']\n    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_INCOME_TOTAL']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n    dropcolum=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n    'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7',\n    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n    df= df.drop(dropcolum,axis=1)\n    del test_df\n    gc.collect()\n    return df","0cad0c8b":"# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows=None, nan_as_category=True):\n    bureau = pd.read_csv('E:\/PythonWork\/HomeCredit\/\/bureau.csv', nrows=num_rows)\n    bb = pd.read_csv('E:\/PythonWork\/HomeCredit\/\/bureau_balance.csv', nrows=num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n\n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n    del bb, bb_agg\n    gc.collect()\n\n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n\n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg","2dd2f1d6":"# Preprocess previous_applications.csv\ndef previous_applications(num_rows=None, nan_as_category=True):\n    prev = pd.read_csv('E:\/PythonWork\/HomeCredit\/previous_application.csv', nrows=num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n    # Add feature: value ask \/ value received percentage\n    prev['AMT_CREDIT'].fillna(1)\n    prev['AMT_CREDIT'].replace(0,1)\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] \/ prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['max', 'mean'],\n        'AMT_APPLICATION': ['max', 'mean'],\n        'AMT_CREDIT': ['max', 'mean'],\n        'APP_CREDIT_PERC': ['max', 'mean'],\n        'AMT_DOWN_PAYMENT': ['max', 'mean'],\n        'AMT_GOODS_PRICE': ['max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['max', 'mean'],\n        'DAYS_DECISION': ['max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n\n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg","3400f849":"# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows=None, nan_as_category=True):\n    pos = pd.read_csv('E:\/PythonWork\/HomeCredit\/POS_CASH_balance.csv', nrows=num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    #\u6839\u636e\u5b57\u6bb5\u8fde\u63a5\n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg","0a4ea449":"def installments_payments(num_rows=None, nan_as_category=True):\n    ins = pd.read_csv('E:\/PythonWork\/HomeCredit\/installments_payments.csv', nrows=num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['AMT_INSTALMENT'].fillna(1)\n    ins['AMT_INSTALMENT'].replace(0,1)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum', 'min', 'std'],\n        'DBD': ['max', 'mean', 'sum', 'min', 'std'],\n        'PAYMENT_PERC': ['max', 'mean', 'var', 'min', 'std'],\n        'PAYMENT_DIFF': ['max', 'mean', 'var', 'min', 'std'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min', 'std'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum', 'std'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'std']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg","fc4f1feb":"def credit_card_balance(num_rows=None, nan_as_category=True):\n    cc = pd.read_csv('E:\/PythonWork\/HomeCredit\/credit_card_balance.csv', nrows=num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg","06a959b2":"Aggregate tables through the key \u2018SK_ID_CURR\u2019, with the mean or min\/max method.\n\nJoin the tables ( bureau with installments_payments) using the key 'SK_ID_CURR'","7c60362f":"Aggregate tables through the key \u2018SK_ID_CURR\u2019, with the mean or min\/max method.\n\nJoin the tables ( bureau with previous_application) using the key 'SK_ID_CURR'","b1bbf7cd":"# 3.1.2 Exploration the relationship of target and categorical features","df3b9f21":"# 2.Missing Data Checking\n\n","92f782b8":"# 3.1.4 Function of Preprocessing application_train.csv\nStructure features use the original features.  \n\n'AMT' means amount\n\n'AMT_CREDIT'+'AMT_ANNUITY'-->'NEW_CREDIT_TO_ANNUITY_RATIO'\n\nMeans the annuity ratio of per credit\n\n'AMT_CREDIT'+'AMT_GOODS_PRICE'-->'NEW_CREDIT_TO_GOODS_RATIO'\n\n'CNT_CHILDREN'+'AMT_INCOME_TOTAL'-->'NEW_INC_PER_CHLD'\n\n'DAYS_BIRTH'+'DAYS_EMPLOYED'-->'NEW_EMPLOY_TO_BIRTH_RATIO'\n\n'AMT_INCOME_TOTAL'+'AMT_ANNUITY'-->'NEW_ANNUITY_TO_INCOME_RATIO'\n","c74241ac":"# 3.4 Preprocessing the pos_cash datasets","1120e0c6":"   AMT_INCOME_TOTAL distribution is the long tail distribution. ","e98272c4":"The connection fields of the bureau.csv and bureau_balance.csv are 'SK_ID_BUREAU'. For each customer, calculate the  mean value of different Bureau's credit,and aggregating data based on the 'SK_ID_BUREAU' dimension.\n\nJoin the tables ( bureau_balance and bureau ) using the key 'SK_ID_BUREAU'.\n\nJoin the tables ( bureau with app_train and test) using the key 'SK_ID_CURR'","d88c2ef8":"# 3.2 Preprocessing the bureau_and_balance datasets","6c9b82c0":"Aggregate tables through the key \u2018SK_ID_CURR\u2019, with the mean or min\/max method.\n\nJoin the tables ( bureau with POS_CASH_balance) using the key 'SK_ID_CURR'","e1aedaab":"# 3.5 Preprocessing the installments_payments datasets","4c9eaf2b":"# 3.1.3 Exploration of the feature with statistics","26bad8d9":"# 3.3 Preprocessing the previous_applications datasets","dd1d4ac4":"    We can see the male has more difficulties to repay the loan,maybe the ratio of the male application is much more than female. The house is larger ,the difficulty is larger.","f6aa6452":"# 3.6 Preprocessing the credit_card_balance datasets","85242d8b":"# 3.1 Preprocessing the application_datasets","3c4895cf":"# 3.1.1 Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","9c2242b8":"# 3.Data Exploring\n\nApplication_train.csv and Application_test.csv has the most nan values\n\n# 3.1 How many features in this competitions?","8960fe29":"# 1.Data\n\nThe data is provided by [Home Credit](http:\/\/www.homecredit.net\/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task. \n\nThere are 7 different sources of data:\n\n* application_train\/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature `SK_ID_CURR`. The training application data comes with the `TARGET` indicating 0: the loan was repaid or 1: the loan was not repaid. \n* bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n* bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. \n* previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`. \n* POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. \n\nThe picture of the relationship of the datasets is from the kernel [Start Here: A Gentle Introduction](https:\/\/www.kaggle.com\/sugarleila\/start-here-a-gentle-introduction\/edit)\n","23227cd7":"Aggregate tables through the key \u2018SK_ID_CURR\u2019, with the mean or min\/max method.\n\nJoin the tables ( bureau with credit_card_balance) using the key 'SK_ID_CURR'"}}