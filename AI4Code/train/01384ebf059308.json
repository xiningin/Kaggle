{"cell_type":{"162105ba":"code","facd9ecd":"code","c752cf2e":"code","e2513441":"code","2fb2cdaa":"code","9556e55f":"code","2ba7b5e1":"code","bfd44624":"code","5e17189e":"code","ad05e58a":"code","8a1a9b67":"code","a968a1bd":"code","fcdd949c":"code","3b2d9e7d":"code","b89e61a3":"code","8d77dfff":"code","c9c5d72f":"code","65d8f2b8":"code","a1d2ad59":"code","f2e8a3de":"code","768f5430":"code","ebe7213c":"code","7741267e":"markdown","126b0a58":"markdown","b075a6f7":"markdown","536fcc24":"markdown","162d181c":"markdown","6deba683":"markdown","fb993afa":"markdown","d2ead90d":"markdown","534f4287":"markdown"},"source":{"162105ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory+96D\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","facd9ecd":"from scipy import stats\nimport statsmodels.api as sm\nfrom sklearn import preprocessing\nfrom datetime import date\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.set_option('display.Max_columns', 100)\npd.set_option('display.Max_rows', 100)\nfrom sklearn.impute import SimpleImputer\n\ndf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nlabel = 'SalePrice'\ndf.drop(columns=['Id'], inplace=True)\n\nprint('Data shape:', df.shape)\nprint('------------------------\\nData types:\\n', df.dtypes.value_counts())\nprint('------------------------')\n\ndf.head(10)","c752cf2e":"# Rename feature names starting with numeric \nfor col in df:\n    if col[0].isdigit():\n        df.rename(columns={col:'n'+ col}, inplace=True)\n        \n# Convert YearBuilt, YearRemodAdd, GarageYrBlt features to Age during sell (YrSold - YrFeature)\ndf['AgeBuilt'] = df['YrSold'] - df['YearBuilt']\ndf['AgeRemod'] = df['YrSold'] - df['YearRemodAdd']\ndf['GarageAge'] = df['YrSold'] - df['GarageYrBlt']\ndf.drop(columns=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], inplace=True)","e2513441":"def map_ordinals(df):\n    \n    LotShape = {'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0}\n    df.LotShape = df.LotShape.map(LotShape)\n    \n    LandContour = {'Low':0,'Lvl':1,'Bnk':2, 'HLS':3}\n    df.LandContour = df.LandContour.map(LandContour)\n    \n    LandSlope = {'Gtl':1,'Mod':2,'Sev':3}\n    df.LandSlope = df.LandSlope.map(LandSlope)\n    \n    ExterQual = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1}\n    df.ExterQual = df.ExterQual.map(ExterQual)\n    \n    ExterCond = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1}\n    df.ExterCond = df.ExterCond.map(ExterCond)\n    \n    BsmtQual = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n    df.BsmtQual = df.BsmtQual.map(BsmtQual)\n    \n    BsmtCond = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n    df.BsmtCond = df.BsmtCond.map(BsmtCond)\n    \n    BsmtExposure = {'Gd':4,'Av':3, 'Mn':2, 'No':1, 'NA':0}\n    df.BsmtExposure = df.BsmtExposure.map(BsmtExposure)\n    \n    BsmtFinType1 = {'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0}\n    df.BsmtFinType1 = df.BsmtFinType1.map(BsmtFinType1)\n    \n    BsmtFinType2 = {'GLQ':6,'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0}\n    df.BsmtFinType2 = df.BsmtFinType2.map(BsmtFinType2)\n    \n    HeatingQC = {'Ex':5,'Gd':4, 'TA':3, 'Fa':2, 'Po':1}\n    df.HeatingQC = df.HeatingQC.map(HeatingQC)\n\n    KitchenQual = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1}\n    df.KitchenQual = df.KitchenQual.map(KitchenQual)\n    \n    Functional = {'Typ':7,'Min1':6,'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0}\n    df.Functional = df.Functional.map(Functional)\n    \n    FireplaceQu = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n    df.FireplaceQu = df.FireplaceQu.map(FireplaceQu)\n    \n    GarageFinish = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0}\n    df.GarageFinish = df.GarageFinish.map(GarageFinish)\n    \n    GarageQual = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n    df.GarageQual = df.GarageQual.map(GarageQual)\n    \n    GarageCond = {'Ex':5,'Gd':4,'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n    df.GarageCond = df.GarageCond.map(GarageCond)\n    \n    PavedDrive = {'Y':2, 'P':1, 'N':0}\n    df.PavedDrive = df.PavedDrive.map(PavedDrive)\n    \n    PoolQC = {'Ex':4,'Gd':3,'TA':2, 'Fa':1, 'NA':0}\n    df.PoolQC = df.PoolQC.map(PoolQC)\n    \n    Fence = {'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, 'NA':0}\n    df.Fence = df.Fence.map(Fence)\n    \n    return df\n\ndf = map_ordinals(df)","2fb2cdaa":"print('Data shape:', df.shape)\nprint('------------------------\\nData types:\\n', df.dtypes.value_counts())\nprint('------------------------')","9556e55f":"n_obs = df.shape[0]\nmissing_df = pd.DataFrame(columns=['Dtype', 'Missing', 'Missing ratio'])\nfor col in df:\n    n_missing = df[col].isnull().sum()\n    if n_missing:\n        missing_df.loc[col] = [df[col].dtype, n_missing, (n_missing\/ n_obs) * 100]\n    \nmissing_df.sort_values(by = ['Dtype', 'Missing'], ascending=True)","2ba7b5e1":"# Handling missing data\nimputer0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value= 0)\ndf[['BsmtQual','BsmtCond','BsmtFinType1','BsmtExposure','BsmtFinType2','BsmtFullBath','BsmtHalfBath','GarageAge','GarageFinish','GarageQual','GarageCond','FireplaceQu','Fence','PoolQC']] = imputer0.fit_transform(df[['BsmtQual','BsmtCond','BsmtFinType1','BsmtExposure','BsmtFinType2','BsmtFullBath','BsmtHalfBath','GarageAge','GarageFinish','GarageQual','GarageCond','FireplaceQu','Fence','PoolQC']].values)\n\nimputer1 = SimpleImputer(missing_values=np.nan, strategy='median')\ndf[['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF','GarageCars', 'GarageArea','KitchenQual', 'Functional']] = imputer1.fit_transform(df[['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars', 'GarageArea', 'KitchenQual', 'Functional']].values)\n\nimputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf[['MSZoning', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Electrical', 'SaleType']] = imputer2.fit_transform(df[['MSZoning', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Electrical', 'SaleType']].values)\n\n\ndf[['GarageType']] = df[['GarageType']].replace(np.nan, 'No garage')\n\ndf[['Alley']] = df[['Alley']].replace(np.nan, 'No alley')\ndf[['MiscFeature']] = df[['MiscFeature']].replace(np.nan, 'None')\n\ndf.dropna(axis=1, inplace=True)\nprint('Data shape after missing value imputation: ', df.shape)\ndf","bfd44624":"def num_univarstats_r(df, numeric=True):\n    corr_col = 'Corr ' + label\n    univar_df = pd.DataFrame(columns=['Dtype', 'Count', 'Unique', 'Mode', 'Mean', 'Min', 'Q1', 'Median', 'Q3', 'Max', 'Std', 'Skew', 'Kurt', corr_col])\n\n    for col in df:\n        if pd.api.types.is_numeric_dtype(df[col]):\n            univar_df.loc[col] = [df[col].dtype, df[col].count(), df[col].nunique(), df[col].mode().values[0], df[col].mean(), df[col].min(), df[col].quantile(0.25), df[col].median(), df[col].quantile(0.75), df[col].max(), df[col].std(), df[col].skew(), df[col].kurt(), df[col].corr(df[label])]\n            \n#         else:\n#             univar_df.loc[col] = [df[col].dtype, df[col].count(), df[col].nunique(), df[col].mode().values[0], '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']   \n    return univar_df\n    \ndf_stats = num_univarstats_r(df).sort_values(by=['Dtype', 'Corr SalePrice', 'Skew'], ascending=False)\ndf_stats","5e17189e":"# Transform target variable to remove positive skewness\ndf[label] = np.log(df[label])\nprint('SalePrice Skewness:', df[label].skew(), ' Kurtosis:', df[label].kurt())\n\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(121)\nsns.histplot(data=df, x=label, kde=True, ax=ax)\n\nax = fig.add_subplot(122)\nstats.probplot(df[label], plot=ax);\n\n  ","ad05e58a":"skwdlist = df_stats.index[(round(abs(df_stats['Corr SalePrice']),1) > 0.1) & (abs(df_stats['Corr SalePrice']) != 1) & (round(abs(df_stats['Skew'])) > 1)].tolist()\nprint(skwdlist)","8a1a9b67":"\n# Removal of Outliers identified numerically\ndef remove_num_outlier(df, col):\n    \n    Q1 = np.percentile(df[col], 25, interpolation = 'midpoint')\n    Q3 = np.percentile(df[col], 75, interpolation = 'midpoint')\n    IQR = Q3 - Q1\n    print(\"Old Shape: \", df.shape)\n\n    upper = np.where(df[col] >= (Q3+1.5*IQR))\n    lower = np.where(df[col] <= (Q1-1.5*IQR))\n\n    df.drop(upper[0], inplace = True)\n    df.drop(lower[0], inplace = True)\n    print(\"New Shape: \", df.shape)\n    \n# Removal of Outliers identified graphically\ndef remove_grph_outlier(df, col, xlim, ylim=None):\n    print(col, ' Skew before:', df[col].skew())\n    if ylim:\n        df = df[~((df[col] > xlim) & (df[label] < ylim))]\n    else:\n        df = df[(df[col] < xlim)]\n    print(col, ' Skew after:', df[col].skew())\n    print(df.shape)\n    return df\n\ndf = remove_grph_outlier(df, 'LotFrontage', 300)\ndf = remove_grph_outlier(df, 'LotArea', 100000)\n# df = remove_grph_outlier(df, 'BsmtFinSF1', 5000)\n# df = remove_grph_outlier(df, 'n1stFlrSF', 4000)\n# df = remove_grph_outlier(df, 'TotalBsmtSF', 6000)\ndf = remove_grph_outlier(df, 'GrLivArea', 4000, 12.5)\ndf = remove_grph_outlier(df, 'OpenPorchSF', 500, 11.5)\ndf = remove_grph_outlier(df, 'EnclosedPorch', 500)\nfor col in skwdlist:\n    print(col)\n    if df[col].skew() < 1:\n        skwdlist.remove(col)\n       \nprint(skwdlist)","a968a1bd":"# boxcox\/yeojohnson transformation to remove skewness\nfor col in skwdlist:\n    if df[col].min() > 0:\n        df[col], lam = stats.boxcox(df[col])\n        print(col, ' skew after boxcox:', df[col].skew())\n    else:\n        df[col], lam = stats.yeojohnson(df[col])\n        print(col, ' skew after yeojohnson:', df[col].skew())\n","fcdd949c":"excllist = df_stats.index[(abs(df_stats['Corr SalePrice']) < 0.05) & (abs(df_stats['Skew']) > 1)].tolist()\nprint(excllist)","3b2d9e7d":"def bar_chart(df, feature, label):\n    \n    groups = df[feature].unique()\n    df_grouped = df.groupby(feature)\n    group_labels = []\n    for g in groups:\n        g_list = df_grouped.get_group(g)\n        group_labels.append(g_list[label])\n        \n    # Calculate the ANOVA results\n    oneway = stats.f_oneway(*group_labels)\n    \n    # Next, calculate t-tests with Bonferroni correction for p-value threshold\n    unique_groups = df[feature].unique()\n    ttests = []\n    \n    for i, group in enumerate(unique_groups):\n        for i2, group_2 in enumerate(unique_groups):\n            if i2 > i:\n                type_1 = df[df[feature]==group]\n                type_2 = df[df[feature]==group_2]\n                if len(type_1[label]) < 2 or len(type_2[label]) < 2:\n                    print(\"'\" + group + \"' n = \" + str(len(type_1)) + \"; '\" + group_2 + \"' n = \" + str(len(type_2)) + \"; no t-test performed\")\n                else:\n                    t, p = stats.ttest_ind(type_1[label], type_2[label])\n                    ttests.append([group, group_2, t.round(4), p.round(4)])\n    \n    if len(ttests) > 0:                # Avoid divide by 0\n        p_threshold = 0.05\/len(ttests) # Bonferroni corrected p-value determined\n    else:\n        p_threshold = 0.05\n        \n    # Add all descriptive statistics in the diagram\n    textstr = '            ANOVA' + '\\n'\n    textstr += 'F:                ' + str(oneway[0].round(2)) + '\\n'\n    textstr += 'p-value:          ' + str(oneway[1].round(2)) + '\\n\\n'\n    textstr += 'Sig. comparisons (Bonferroni-corrected)'  + '\\n'\n    \n    for ttest in ttests:\n        if ttest[3] <= p_threshold:\n            textstr += ttest[0] + '-' + ttest[1] + ': t=' + str(ttest[2]) + ', p=' + str(ttest[3]) + '\\n'\n    \n    ax = sns.barplot(x=df[feature], y=df[label])\n    ax.text(1, 0.1, textstr, fontsize=12, transform=plt.gcf().transFigure)  \n    plt.show()\n    \ndef heteroscedasticity(df, feature, label):\n    from statsmodels.stats.diagnostic import het_breuschpagan\n    from statsmodels.stats.diagnostic import het_white\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n    \n    # Fit the OLS model\n    model = ols(formula=(label + '~' + feature), data=df).fit()\n    \n    output_df = pd.DataFrame(columns=['LM stat', 'LM p-value', 'F-stat', 'F p-value'])\n    \n    try:\n        white_test = het_white(model.resid, model.model.exog)\n        output_df.loc['white'] = white_test\n    except:\n        print(\"Unable to run White test of heteroscedasticity\")\n        \n    bp_test = het_breuschpagan(model.resid, model.model.exog)\n    output_df.loc['Br-Pa'] = bp_test\n    \n    return output_df.round(3)\n    \n    \ndef scatter(feature, label):\n    \n    #Calculate the regression line\n    m, b, r, p, err = stats.linregress(feature, label)\n    \n    textstr = 'y = ' + str(round(m,2)) + 'x + ' + str(round(b, 2)) + '\\n'\n    textstr += 'r2 = ' + str(round(r**2, 2)) + '\\n'\n    textstr += 'p = ' + str(round(p, 2)) + '\\n'\n    textstr += str(feature.name) + ' Skew = ' + str(round(feature.skew(), 2)) + '\\n'\n    textstr += str(label.name) + ' Skew = ' + str(round(label.skew(), 2)) + '\\n'\n    textstr += str(heteroscedasticity(pd.DataFrame(label).join(pd.DataFrame(feature)), feature.name, label.name))\n   \n    sns.set(color_codes=True)\n    ax = sns.jointplot(x=feature, y=label, kind='reg')\n    ax.fig.text(1, 0.14, textstr, fontsize=12, transform=plt.gcf().transFigure) \n    plt.show()\n    \ndef anova(df, feature, label): # For categorical feature\n    \n    groups = df[feature].unique()\n    df_grouped = df.groupby(feature)\n    group_labels = []\n    for g in groups:\n        g_list = df_grouped.get_group(g)\n        group_labels.append(g_list[label])\n    \n    return stats.f_oneway(*group_labels)\n\ndef bivarstats(df, label):\n    \n    output_df = pd.DataFrame(columns=['stat','+\/-', 'Effect size', 'p-value'])\n    for col in df:\n        if not col == label:\n            if df[col].isnull().sum() == 0:\n                if pd.api.types.is_numeric_dtype(df[col]): # Only calculate r, p-value for the numeric columns\n                    r, p = None, None\n                    if abs(df[col].skew()) > 1:\n                        r, p = stats.spearmanr(df[label], df[col])\n                        output_df.loc[col] = ['r', np.sign(r), abs(round(r, 3)), round(p, 3)]\n                    else:\n                        r, p = stats.pearsonr(df[label], df[col])\n                        output_df.loc[col] = ['r', np.sign(r), abs(round(r, 3)), round(p, 3)]\n                    scatter(df[col], df[label])\n                else:\n                    F, p = anova(df[[col, label]], col, label)\n                    output_df.loc[col] = ['F', '', round(F, 3), round(p, 3)]\n                    bar_chart(df, col, label)\n            else:\n                output_df.loc[col] = [np.nan, np.nan, np.nan, np.nan]\n            \n    return output_df.sort_values(by=['Effect size', 'stat'], ascending=False)\n","b89e61a3":"df_bvar = bivarstats(df, label)\ndf_bvar","8d77dfff":"# Feature selection: \n# Select the feature \n#   if r: p < 0.05 and r > 0.1\n#   else p < 0.05\nselected_features = df_bvar.index[((df_bvar['stat'] == 'F') & (df_bvar['p-value'] < 0.05)) | ((df_bvar['stat'] == 'r') & (df_bvar['p-value'] < 0.05) & (df_bvar['Effect size'] > 0.1))].tolist()\n\nprint(len(selected_features))\nprint(selected_features)","c9c5d72f":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\nselected_features.append(label)\ndf_selected = df[df.columns[df.columns.isin(selected_features)]]\n\n# Save the selected features to a csv\ndf_selected.to_csv('data_selected.csv')\n\ncorrplot(df_selected, annot=None)","65d8f2b8":"# Boxplot for categorical selected_features\ncategorical = [f for f in selected_features if df.dtypes[f] == 'object']\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=45)\nf = pd.melt(df, id_vars=['SalePrice'], value_vars=categorical)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","a1d2ad59":"def mlr_prepare(df): # Generate dummy features, perform scaling\n    \n    for col in df:\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=True))\n\n    df = df.select_dtypes(np.number)\n    df_minmax = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n    return df_minmax\n\ndef mlr(df, label): # Run MLR\n\n    y = df[label]\n    X = df.drop(columns=[label]).assign(const=1)\n    \n    results = sm.OLS(y, X).fit()\n    return results\n\ndef mlr_fit(results, actual, roundto=10): # Calculate fit statistics, create a record entry for the modelling results table\n    df_features = mlr_feature_df(results)\n    residuals = np.array(actual) - np.array(results.fittedvalues)\n    rmse = np.sqrt(sum((residuals**2))\/len(actual))\n    mae = np.mean(abs(np.array(actual) - np.array(results.fittedvalues)))\n    fit_stats =[round(results.rsquared, roundto), round(results.rsquared_adj, roundto),\n               round(results.rsquared - results.rsquared_adj, roundto), round(rmse, roundto),\n               round(mae, roundto), [df_features.index.values]]\n    return fit_stats\n\ndef mlr_feature_df(results): # Generate the DataFrame that allows us to sort features by t and p values\n    df_features = pd.DataFrame({'coef': results.params, 't': abs(results.tvalues), 'p': results.pvalues})\n    df_features.drop(labels=['const'], inplace=True)\n    df_features.sort_values(by=['t', 'p'])\n    return df_features\n\ndef mlr_step(df, label, min=2): # Control mlr and mlr_fit by removing a cetain criterion of features at a time\n    # Create the empty model results table\n    df_models = pd.DataFrame(columns=['R2', 'R2a', 'diff', 'RMSE', 'MAE', 'features'])\n    \n    #Prepare the data by generating dummies and scaling\n    df =  mlr_prepare(df)\n    \n    # Run the first model with all features\n    results = mlr(df, label)\n    \n    # Generate the fit statistics for the model\n    df_models.loc[str(len(results.params))] = mlr_fit(results, df[label], 10)\n    \n    # Generate feature table that allows sorting coef labels based on t and p\n    df_features = mlr_feature_df(results)\n    \n    # Step through a series of reduced models until\n    while len(results.params) >= min:\n        df = df.drop(columns=[df_features.index[0]]) # Drop the least effective feature\n        results = mlr(df, label)                     # Re-run \n        df_features = mlr_feature_df(results)        # Re-generate the features summary table\n        df_models.loc[str(len(results.params))] = mlr_fit(results, df[label], 10)\n        \n    # Save the full models table to a csv\n    df_models.to_csv(label + '.csv')\n    \n    # Return a shortened version without feature list\n    df_models.drop(columns=['features'], inplace = True)\n    \n    return df_models\n","f2e8a3de":"# Run MLR starting with selected_features and removing the least significant feature at each step\ndf_models = mlr_step(df_selected.copy(), label)\ndf_models\n","768f5430":"# VIF = Variance Inflation Factor = 1\/(1-R2)\ndef vif(df): \n    from sklearn.linear_model import LinearRegression\n    \n    vif_dict, tolerance_dict = {}, {}\n    for col in df:\n        y = df[col]\n        X = df.drop(columns=[col])\n        \n        r_squared = LinearRegression().fit(X, y).score(X, y)\n        \n        if r_squared < 1: # Prevent division by zero error\n            vif = 1\/(1 - r_squared)\n        else:\n            vif = 100\n        vif_dict[col] = vif\n        \n        tolerance = 1 - r_squared\n        tolerance_dict[col] = tolerance\n        \n        df_vif = pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict})\n        \n    return df_vif.sort_values(by=['VIF'], ascending=False)\n# < 10 : adequate\n# < 5 : good\n# < 3 : ideal\ndf_vif_ip = mlr_prepare(df_selected)\ndf_vif_ip.drop(columns=[label], inplace=True)\n\ndf_vif = vif(df_vif_ip)\ndf_vif","ebe7213c":"hvif_list = df_vif.index[(df_vif['VIF'] > 10)].tolist()\nprint(len(hvif_list))\nprint(hvif_list)","7741267e":"Features **highly\/moderately skewed** and **heavy tailed** with **high\/good correlation** to **SalePrice** are candidates for Outlier drop, boxcox\/yeojohnson transformation.  ","126b0a58":"#### Missing Data","b075a6f7":"#### Outlier Mining","536fcc24":"The target variable, **SalePrice** distribution is positively skewed and heavy tailed. \n","162d181c":"#### For Numeric Features : Measures of Centre, Spread and Correlation with Target ","6deba683":"### Preparing environment and get data","fb993afa":"### Data Preprocessing","d2ead90d":"Features **highly\/moderately skewed** and **heavy tailed** with **low correlation** to **SalePrice** are candidates for exclusion","534f4287":"### Exploratory Data Analysis"}}