{"cell_type":{"00ae8c6b":"code","0249eae8":"code","42827e32":"code","def59fa5":"code","adb2362f":"code","1d0352ce":"code","c7f8b926":"code","5acc81e8":"code","825e376e":"code","c6a46f0d":"code","a5a12305":"code","f775765a":"code","79d99c6f":"code","65cd473a":"code","ec7f7222":"code","a4472503":"code","e1d07053":"code","a678993a":"code","cc104da8":"code","f15153ef":"code","d8e7e86d":"code","3efc9b5e":"code","5274f28b":"code","fc55c538":"code","c632147d":"code","45f9b85d":"code","a6482461":"code","1b78f045":"code","ce96ab1c":"code","4a88fc4d":"code","34ea8ccd":"code","5f9e9447":"code","ee51c049":"code","5a02604e":"code","f95dec9e":"code","8cacb81c":"markdown","23e861c0":"markdown","3e53d64d":"markdown","7a53f597":"markdown","8d69b381":"markdown","96c214c9":"markdown","03263a08":"markdown","909a9aa4":"markdown","91c7c8db":"markdown","2ba59ec2":"markdown","e013b53e":"markdown","d6c32bdd":"markdown","6e10f982":"markdown"},"source":{"00ae8c6b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf, month_plot\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom pandas.plotting import autocorrelation_plot\nfrom sklearn.metrics import mean_squared_error","0249eae8":"series = pd.read_csv('\/kaggle\/input\/watercsv\/water.csv', header=0, index_col=0,parse_dates=True ,squeeze=True)\nseries.head()","42827e32":"series.describe()","def59fa5":"#smoothing average good way to check trend, popular check in stock data\n\nfig = plt.figure(figsize=(15,8))\nplt.plot(series, color='black')\nplt.plot(series.rolling(window=3, min_periods=1).mean(), color='yellow')\nplt.plot(series.ewm(alpha=0.7).mean(), color='red')\nplt.show()","adb2362f":"autocorrelation_plot(series) # it is not whote noise!!! Data are correlated","1d0352ce":"fig = plt.figure(figsize=(15,8))\nplt.subplot(1,3,1)\nplt.boxplot(series)\nplt.subplot(1,3,2)\nplt.hist(series)\nplt.subplot(1,3,3)\nseries.plot(kind='kde')\nplt.show()","c7f8b926":"X = series.values\nsize = int(len(X)*0.6)\ntrain, test = X[0:size], X[size:]\n\n\nhistory = [x for x in train]\npredictions = list()\nresiduals = list()\n\nfor i in range(len(test)):\n    yhat = history[-1]\n    predictions.append(yhat)\n    obs = test[i]\n    history.append(obs)\n    residual = yhat - obs\n    residuals.append(residual)\n    print('Predicted=%.3f, Expected=%.3f' % (yhat,obs))\n\nrmse = np.sqrt(mean_squared_error(test, predictions))\nprint('RMSE: %.3f' % rmse)    \n    \n    ","5acc81e8":"fig = plt.figure(figsize=(15,8))\nplt.plot(history, color='blue')\nplt.plot([None for i in train] + [x for x in test], color='red')\nplt.plot([None for i in train] + [x for x in predictions], color='pink')\nplt.show()","825e376e":"residuals = pd.Series(residuals)\nresiduals.describe()","c6a46f0d":"residuals.plot(kind='kde')","a5a12305":"autocorrelation_plot(residuals) #no correclation  - good!!!","f775765a":"#test Dickey-Fuller\n\ndef adful_test(X):\n    results = adfuller(X)\n    print(\"statistic:%.3f\" % results[0])\n    print(\"p_value: %.7f\" % results[1])\n    print(\"num: %3.f\" % results[2])\n    print(\"samples: %1.f\" % results[3])\n    if results[1]<0.05:\n        print(\"stationary\")\n    else:\n        print(\"non-stationary\")","79d99c6f":"adful_test(series)","65cd473a":"def differ(series,d):\n    X = series.values\n    diff = list()\n    for i in range(1, len(X)):\n        dif = X[i] - X[i-1]\n        diff.append(dif)\n    return pd.Series(diff)","ec7f7222":"series = pd.read_csv('\/kaggle\/input\/watercsv\/water.csv', header=0, index_col=0,parse_dates=True ,squeeze=True)\ndifferences = differ(series,1)\nadful_test(differences)","a4472503":"differences.plot()","e1d07053":"plt.figure() \nplt.subplot(211) \nplot_acf(series, lags=20, ax=plt.gca()) \nplt.subplot(212) \nplot_pacf(series, lags=20, ax=plt.gca()) \nplt.show()","a678993a":"X = series.values\nsize = int(len(X)*0.5)\ntrain, test = X[0:size], X[size:]\n\nhistory = [x for x in train]\nprediction = list()\n\nfor i in range(len(test)):\n    model = ARIMA(history, order=(4,1,1))\n    model_fit = model.fit(disp=0)\n    yhat = model_fit.forecast()[0]\n    prediction.append(yhat)\n    \n    obs = test[i]\n    history.append(obs)\n    print('Predicted:%.3f, Expected:%.3f' % (yhat, obs))\nrmse = np.sqrt(mean_squared_error(test, prediction))\nrmse","cc104da8":"def evaluating_arima_model(X, arima_order):\n    X = X.astype('float32')\n    size = int(len(X)*0.5)\n    train, test = X[0:size], X[size:]\n    history = [x for x in train]\n    predictions = list()\n    \n    for t in range(len(test)):\n        model = ARIMA(history, order = arima_order)\n        model_fit = model.fit(trend='nc', disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        \n        obs = test[t]\n        history.append(obs)\n    rmse = np.sqrt(mean_squared_error(test, predictions))\n    return rmse","f15153ef":"def evaluate_param(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = np.float('inf'), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p, d, q)\n                try:\n                    rmse = evaluating_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s, rmse:%.3f' % (order, rmse))\n                except:\n                    continue\n    print('Best ARIMA%s, rmse:%.3f' % (best_cfg, best_score))\n                ","d8e7e86d":"series = pd.read_csv('\/kaggle\/input\/watercsv\/water.csv', header=0, index_col=0,parse_dates=True ,squeeze=True)\n\np_values = range(0,5)\nd_values = range(0,3)\nq_values = range(0,5)\nimport warnings\nwarnings.filterwarnings('ignore')\nevaluate_param(series.values, p_values, d_values, q_values)","3efc9b5e":"series = pd.read_csv('\/kaggle\/input\/watercsv\/water.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n\nX = series.values\n\nsize = int(len(X)*0.5)\n\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nres = list()\n\nfor i in range(len(test)):\n    model = ARIMA(history, order=(2,1,0))\n    model_fit = model.fit(trend='nc', disp=0)\n    yhat = model_fit.forecast()[0]\n    predictions.append(yhat)\n    obs = test[i]\n    history.append(obs)\n    r = obs - yhat\n    res.append(r)\nrmse = np.sqrt(mean_squared_error(test, predictions))","5274f28b":"res = pd.DataFrame(res, columns=['val'])\nres.describe()","fc55c538":"autocorrelation_plot(res)","c632147d":"plt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.hist(res.val)\nplt.subplot(1,2,2)\nplt.boxplot(res.val)","45f9b85d":"from statsmodels.graphics.gofplots import qqplot\nqqplot(res.val, line='s')","a6482461":"series = pd.read_csv('\/kaggle\/input\/watercsv\/water.csv', header=0, index_col=0, squeeze=True, parse_dates=True)\n\nX = series.values\nsize = int(len(X)*0.5)\n\ntrain, test = X[0:size], X[size:]\nhistory = [x for x in train]\npredictions = list()\nbias = 3.064462 \n\nfor i in range(len(test)):\n    model = ARIMA(history, order=(2,1,0))\n    model_fit = model.fit(tresd='nc',disp=0)\n    yhat = bias + np.float( model_fit.forecast()[0])\n    predictions.append(yhat)\n    history.append(test[i])\n    \nrmse = np.sqrt(mean_squared_error(test, predictions))\nresiduals = [test[i] - predictions[i] for i in range(len(test))]\nresiduals = pd.DataFrame(residuals)\n\n\nplt.figure(figsize=(15,8))\nplt.plot(test, color='r')\nplt.plot(predictions, color='b')\nplt.legend(['test', 'pred'])\nplt.show()","1b78f045":"rmse","ce96ab1c":"residuals.describe()","4a88fc4d":"residuals.plot(kind='kde')","34ea8ccd":"from statsmodels.tsa.api import Holt, ExponentialSmoothing, SimpleExpSmoothing","5f9e9447":"X = series.values\nsize = int(len(X)*0.7)\ntrain, test = X[0:size], X[size:]\n\nmodel_holt = Holt(train, exponential=True)\nmodel_fit = model_holt.fit()\n\nmodel_HW = ExponentialSmoothing(train, trend='add')\nmodel_fit_HW = model_HW.fit()","ee51c049":"model_pred = model_fit.forecast(steps=24)\nmodel_HW_pred = model_fit_HW.forecast(steps=24)","5a02604e":"model_fit_HW.params","f95dec9e":"plt.figure()\nplt.plot(X, color='r')\nplt.plot([None for x in train] + [b for b in model_pred], color='green')\nplt.plot([None for i in train] + [g for g in model_HW_pred], color='black')\nplt.legend(['history', 'Holt', 'HW'])\nplt.show()","8cacb81c":"p = 4 and q = 1","23e861c0":"automatically selected order(2,1,0) get better score than my (4,1,1) however still persistence model is better!!!","3e53d64d":"# Persistence model","7a53f597":"[](http:\/\/)Arima model did not get better results than naive approach","8d69b381":"# Grid search for p, d, q parameters","96c214c9":"d - parameter","03263a08":"Not better with bias correction","909a9aa4":"# Exponential smoothing (Holt, Holt-Winter)","91c7c8db":"# Residuals","2ba59ec2":"# ARIMA MODEL","e013b53e":"Model with my parameters","d6c32bdd":"# Bias - correct prediction","6e10f982":"> p , q - parameters"}}