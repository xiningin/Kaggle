{"cell_type":{"acdecd4e":"code","ad6fbd41":"code","36e58523":"code","2c55bc02":"code","6dd0c9c1":"code","262ab40e":"code","dcc40474":"code","1b547661":"code","392763e2":"code","bd95443e":"code","caeaaf99":"code","6ba5c18f":"markdown","b31c677e":"markdown","9baf325d":"markdown"},"source":{"acdecd4e":"import numpy as num\nfrom sklearn.datasets import load_iris\ndir(load_iris())\nY = load_iris().target\nXs = load_iris().data[:,(2,3)]","ad6fbd41":"x1 = num.arange( min(Xs[:,0]) - 1 , max(Xs[:,0]) + 1 , .01 )\nx2 = num.arange( min(Xs[:,1]) - 1 , max(Xs[:,1]) + 1 , .01 )\nX1 , X2 = num.meshgrid( x1 , x2  )\nmeshed_Xs = num.c_[ X1.ravel() , X2.ravel() ]","36e58523":"from sklearn.preprocessing import PolynomialFeatures\nPF_obj = PolynomialFeatures( degree=6, include_bias=False )\nXspoly = PF_obj . fit_transform( Xs )\nmeshed_Xspoly = PF_obj . fit_transform( meshed_Xs )","2c55bc02":"from sklearn.linear_model import LogisticRegression\nLRre_obj = LogisticRegression( multi_class='multinomial' , solver='newton-cg' )\nLRre_obj . fit( Xs , Y )\nymeshlR = LRre_obj.predict( meshed_Xs )\nymeshlR = ymeshlR . reshape ( X1.shape  )","6dd0c9c1":"import matplotlib.pyplot as pyp\npyp.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\npyp.contourf( X1 , X2 , ymeshlR , alpha=0.3  )\npyp.scatter ( Xs[:,0] , Xs[:,1] , c=Y , s=30 , edgecolor='k' )\npyp.xlabel ( 'Patal length [cm]' )\npyp.ylabel ( 'Patal Width [cm]' )\npyp.xlim( xmin=0 )\npyp.ylim( ymin=0 )\npyp.title( 'Decision Boundary for linear logistic regression' )\npyp.show()","262ab40e":"LRre_obj . fit ( Xspoly , Y )\nymeshNR = LRre_obj . predict ( meshed_Xspoly  )\nymeshNR = ymeshNR . reshape ( X1 . shape)","dcc40474":"pyp.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\npyp.contourf( X1 , X2 , ymeshNR , alpha=0.3  )\npyp.scatter ( Xs[:,0] , Xs[:,1] , c=Y , s=30 , edgecolor='k' )\npyp.xlabel ( 'Patal length [cm]' )\npyp.ylabel ( 'Patal Width [cm]' )\npyp.xlim( xmin=0 )\npyp.ylim( ymin=0 )\npyp.title( 'Decision Boundary for nonlinear logistic regression' )\npyp.show()","1b547661":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nSVML_obj = Pipeline(  [ ( 'scaler' , StandardScaler() ) , ( 'linear_svc' , LinearSVC(  C = 1 , loss = 'hinge' ,multi_class = 'ovr' ) ) ]  )\nSVML_obj . fit ( Xs ,  Y)\nymeshSVM = SVML_obj.predict ( meshed_Xs  )\nymeshSVM = ymeshSVM . reshape ( X1.shape  )","392763e2":"pyp.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\npyp.contourf( X1 , X2 , ymeshSVM , alpha=0.3  )\npyp.scatter ( Xs[:,0] , Xs[:,1] , c=Y , s=30 , edgecolor='k' )\npyp.xlabel ( 'Patal length [cm]' )\npyp.ylabel ( 'Patal Width [cm]' )\npyp.xlim( xmin=0 )\npyp.ylim( ymin=0 )\npyp.title( 'Decision Boundary for linear SVM' )\npyp.show()","bd95443e":"SVML_obj . fit ( Xspoly ,  Y)\nymeshNSVM = SVML_obj . predict ( meshed_Xspoly )\nymeshNSVM= ymeshNSVM.reshape ( X1.shape  )","caeaaf99":"pyp.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\npyp.contourf( X1 , X2 , ymeshNSVM , alpha=0.3  )\npyp.scatter ( Xs[:,0] , Xs[:,1] , c=Y , s=30 , edgecolor='k' )\npyp.xlabel ( 'Patal length [cm]' )\npyp.ylabel ( 'Patal Width [cm]' )\npyp.xlim( xmin=0 )\npyp.ylim( ymin=0 )\npyp.title( 'Decision Boundary for nonlinear SVM' )\npyp.show()","6ba5c18f":"**1. Nonlinear logistic regression**","b31c677e":"This study simply visualizes the differences between a linear and a nonlinear classifier in the case of 2-dimensional features. For this purpose, we use well-known multi-class iris data.","9baf325d":"**2. SVM classifier**"}}