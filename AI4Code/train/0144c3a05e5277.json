{"cell_type":{"c9b4d6c0":"code","6c608bec":"code","60a3dfdd":"code","1405916c":"code","8a78e6ca":"code","71e18c9d":"code","76e76dbd":"code","230dc9b1":"code","2e95ce9b":"code","8dc37f88":"code","0d020b9f":"markdown","e9887078":"markdown","7c04f28d":"markdown","fff0c498":"markdown","d85bf0b1":"markdown","5da97e0d":"markdown","8d75dcef":"markdown","7aee4ccf":"markdown","d0210739":"markdown","fbaccbf0":"markdown","c54ae420":"markdown","7ac7d0a8":"markdown","bcfb3995":"markdown","83b4d965":"markdown","1a9bea0e":"markdown"},"source":{"c9b4d6c0":"batch = 10 # how many adversarial examples to find","6c608bec":"!pip install foolbox","60a3dfdd":"# Imports\nimport time, foolbox, torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.models as models\n\n%matplotlib inline\n\n# Foolbox defaults to a GPU","1405916c":"# sets a bunch of variables - could have an if-else to mess with different networks\nnetwork = models.inception_v3(pretrained=True) # grab the model from torchvision\ndataset = 'imagenet'\nchannels = 3 # RGB\nsize = 224 # image size\nclasses = 1000 \nnetwork.eval();","8a78e6ca":"# Foolbox time \n# Convert the model to a Foolbox model\nfnetwork = foolbox.models.PyTorchModel(network, bounds=(0, 1), num_classes=classes, channel_axis=1) \n\n# get source image and label\n# images are just an array\nimages, labels = foolbox.utils.samples(dataset=dataset, batchsize=batch, data_format='channels_first', bounds=(0, 1))\nimages = images.reshape(batch, channels, size, size)\n\nprint(images.shape)\nprint(\"Labels:      \", labels)\npredictions = fnetwork.forward(images).argmax(axis=-1)\nprint(\"Predictions: \", predictions) # original prediction\nprint(\"Accuracy: \", np.mean(predictions == labels)) # Accuracy of original images\nalready_correct = np.sum(predictions != labels) # keep track of how many were already correct","71e18c9d":"attack = foolbox.attacks.DeepFoolL2Attack(fnetwork, distance=foolbox.distances.Linfinity)\n\nt1 = time.time()\nadversarials = attack(images, labels,  unpack=False) # get the adversarial examples\nt2 = time.time()\n\navg_time = (t2 - t1) \/ batch\nprint(\"Avg Time: \", avg_time, \"seconds\")","76e76dbd":"# this cell & next 2 are mostly from a Foolbox tutorial\nadversarial_classes = np.asarray([a.adversarial_class for a in adversarials])\nprint(\"Labels: \", labels)\nprint(\"Adv. Labels: \", adversarial_classes)\nprint(\"Classification Acc: \", np.mean(adversarial_classes == labels)) # should be 0.0","230dc9b1":"_sum = 0\nfor i in range(batch):\n    _sum += adversarials[i].distance.value\navg_Linf = _sum \/ (batch-already_correct)\nprint(\"Avg L-inf distance: \", avg_Linf)","2e95ce9b":"# The 'Adversarial' objects also provide a 'distance' attribute. Note that the distances\n# can be 0 (misclassified without perturbation) and inf (attack failed).\ndistances = np.asarray([a.distance.value for a in adversarials])\nprint(\"{:.1e}, {:.1e}, {:.1e}\".format(distances.min(), np.median(distances), distances.max()))\nprint(\"{} of {} attacks failed\".format(sum(adv.distance.value == np.inf for adv in adversarials), len(adversarials)))\nprint(\"{} of {} inputs misclassified without perturbation\".format(sum(adv.distance.value == 0 for adv in adversarials), len(adversarials)))","8dc37f88":"# Plot the original, adversarial, & the difference between them\nfor i in range(batch):\n\n    image = images[i]\n    adversarial = adversarials[i].perturbed\n\n    # CHW to HWC (switching the channels)\n    image = image.transpose(1, 2, 0)\n    adversarial = adversarial.transpose(1, 2, 0)\n    if image.shape[2] == 1: # for MNIST (only one color channel)\n        image = image.reshape(28, 28)\n        adversarial = adversarial.reshape(28, 28)\n    \n    plt.figure()\n\n    # Original\n    plt.subplot(1, 3, 1)\n    plt.title('Label: '+ str(labels[i]))\n    plt.imshow(image)\n    plt.axis('off')\n\n    # Adversarial\n    plt.subplot(1, 3, 2)\n    plt.title('Adversarial: ' + str(adversarials[i].adversarial_class))\n    plt.imshow(adversarial)\n    plt.axis('off')\n\n    # Difference\n    plt.subplot(1, 3, 3)\n    plt.title('Difference')\n    difference = adversarial - image\n    plt.imshow(difference \/ abs(difference).max() * 0.2 + 0.5)\n    plt.axis('off')\n\n    plt.show()","0d020b9f":"So, any image where the prediction doesn't match the label is obviously already wrong. (It will come back up at the end.)","e9887078":"This notebook accompanies the following blog post about adversarial machine learning.  \n[Blog Post on Adversarial ML](https:\/\/jvmohr.github.io\/post\/adversarial-machine-learning\/)","7c04f28d":"Note how the classification accuracy has plummeted from 70% to 0%! We'll see below that the images don't even look any different. ","fff0c498":"# Attack ","d85bf0b1":"This notebook uses Foolbox to generate adversarial examples for Inception-V3. 'batch' is how many adversarial examples to find. (Warning: For some attacks on Inception without a GPU, it could take a while. DeepFool is fairly quick, but many others aren't. If you switch the attack, I'd recommend decreasing 'batch' to 1 or 2 to start. )\n\nFoolbox has up to 20 examples for these datasets stored. Not all of these images are originally classified correctly. ","5da97e0d":"# Generating Adversarial Examples With Foolbox","8d75dcef":"Foolbox is not installed by default, so we need to install it like so. ","7aee4ccf":"This cell computes the average L-infinity distance from the original image to the adversarial image. \n(Note: It subtracts 'already_correct' off of 'batch' as nothing needed to change for those images, so their distance was already 0. For those images that were already misclassified, their adversarial class as seen is above is the same as their original prediction.)","d0210739":"# Analysis","fbaccbf0":"The attack being used is DeepFool, a quite fast and powerful gradient-based attack. \n\nThe following link lists more attacks and more information. You should be able to switch most of them in for DeepFool. (There might be a little to change as some attacks are targeted only.) \nhttps:\/\/foolbox.readthedocs.io\/en\/latest\/modules\/attacks.html","c54ae420":"For the images that weren't originally misclassified, the difference between the original image and the adversarial image is very small. I wish you the last of luck in trying to notice any differences between those images! \n\nIf the difference is black, then the original image was misclassified. The adversarial labels for those images matches the original prediction above. ","7ac7d0a8":"Now, we choose our attack. In this case, we're going to use DeepFool. \n(A reminder that more attacks can be found here: https:\/\/foolbox.readthedocs.io\/en\/latest\/modules\/attacks.html)","bcfb3995":"# Setup","83b4d965":"I hope you've enjoyed and learned a bit from this kernel! If you have any questions feel free to leave them down below. ","1a9bea0e":"Most other attacks won't be as fast as DeepFool. Especially attacks in the Decision-Based category. "}}