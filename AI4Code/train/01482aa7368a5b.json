{"cell_type":{"8abfe463":"code","0f653d58":"code","d339c9bd":"code","36d96cf2":"code","47b0f2b9":"code","7229d89b":"code","2f560f05":"code","e62dbc2c":"code","8fea48bf":"code","fb16c71a":"code","80858a59":"code","fd9bf707":"code","ee08b104":"code","3b2b5ef8":"code","c392caa5":"code","81ef3fa7":"markdown","01ebda8a":"markdown","993d072c":"markdown"},"source":{"8abfe463":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\nfrom tensorflow.keras.models import model_from_json, load_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nprint(tf.__version__)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    if filenames:\n        train_csv_path = os.path.join(dirname, filenames[0])\n        test_csv_path = os.path.join(dirname, filenames[1])\n        sample_submission_csv_path = os.path.join(dirname, filenames[2])\n\nprint(train_csv_path)\nprint(test_csv_path)\nprint(sample_submission_csv_path)","0f653d58":"df_train = pd.read_csv(train_csv_path)\ndf_cols = df_train.columns.values.tolist()\nprint(df_cols)","d339c9bd":"categorical_columns = ['POSTED_BY','BHK_OR_RK','ADDRESS']\nfor col in categorical_columns:\n    df_cols.remove(col)","36d96cf2":"Y = df_train[df_cols[-1]].values\nX = df_train[df_cols[:-1]].values\nX, Y = shuffle(X, Y)\nprint(X.shape)\nprint(Y.shape)","47b0f2b9":"num_epoches = 80\nbatch_size = 128\nval_split = 0.15","7229d89b":"Xscalar = StandardScaler()\nXscalar.fit(X)\n\nXtrain = Xscalar.transform(X)\nYtrain = Y","2f560f05":"# Yscalar = StandardScaler()\n# Yscalar.fit(Y.reshape(-1, 1))\n\n# Ytrain = Yscalar.transform(Y.reshape(-1, 1))\n# Ytrain = Ytrain.squeeze()","e62dbc2c":"def classifier1():\n    n_features = Xtrain.shape[1]\n    inputs = Input(shape=(n_features,))\n    x = Dense(512, activation='relu')(inputs)\n    x = Dense(256, activation='relu')(x)\n#     x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(1)(x)\n    model = Model(inputs, outputs)\n    \n    model.compile(\n        loss='mse',\n        optimizer='adam'\n    )\n    history = model.fit(\n                    Xtrain,\n                    Ytrain,\n                    batch_size=batch_size,\n                    epochs=num_epoches,\n                    validation_split=val_split\n                    )\n    return history, model\n    \ndef plot_metrics(history):\n    loss_train = history.history['loss']\n    loss_val = history.history['val_loss']\n    \n    loss_train = np.cumsum(loss_train) \/ np.arange(1,num_epoches+1)\n    loss_val = np.cumsum(loss_val) \/ np.arange(1,num_epoches+1)\n    plt.plot(loss_train, 'r', label='Training loss')\n    plt.plot(loss_val, 'b', label='validation loss')\n    plt.title('Training and Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","8fea48bf":"history1, model1 = classifier1()","fb16c71a":"plot_metrics(history1)","80858a59":"df_test = pd.read_csv(test_csv_path)\ndf_cols_test = df_test.columns.values.tolist()\n\ncategorical_columns = ['POSTED_BY','BHK_OR_RK','ADDRESS']\nfor col in categorical_columns:\n    df_cols_test.remove(col)\ndf_cols_test","fd9bf707":"Xtest = df_test[df_cols_test].values\nXtest.shape","ee08b104":"Xtest = Xscalar.transform(Xtest)\nYpred = model1.predict(Xtest)","3b2b5ef8":"submission_df = pd.read_csv(sample_submission_csv_path)\nsubmission_df['TARGET(PRICE_IN_LACS)'] = Ypred\nsubmission_df.head()","c392caa5":"submission_csv_path = '\/kaggle\/working\/submission.csv'\nsubmission_df.to_csv(submission_csv_path)\n","81ef3fa7":"Let's Load Train data and analyze about Columns in dataframe","01ebda8a":"Model 1 : Only using Numerical Feature columns","993d072c":"Standard Normalize Data"}}