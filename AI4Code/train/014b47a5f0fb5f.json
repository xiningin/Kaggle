{"cell_type":{"85aeb040":"code","4dee1fea":"code","56f3adbf":"code","73c21a58":"code","98e3ec2d":"code","c1a11c05":"code","2e24a941":"code","f46f66a2":"code","bb929f1c":"code","6ba10dd6":"code","804cfb98":"code","09a9687d":"code","27a66de5":"code","a0de6bb4":"code","680a83ec":"code","96869c07":"code","930f5d2c":"code","d6109d61":"code","fcecebd3":"code","592d2d53":"code","8db52001":"code","c08cce8e":"code","f6af29ab":"code","28138d8a":"code","d86d7573":"code","7dd4185f":"code","bda954a1":"code","95402492":"code","92feed14":"code","dd32fb50":"code","bb572dee":"code","eb9e8ca3":"code","8c33b1fd":"code","03f4c3b3":"markdown","b98ddd77":"markdown","6d65fd7b":"markdown","cdd34a6f":"markdown","d3734181":"markdown","748f417d":"markdown","37a1a2ca":"markdown","9b41ff75":"markdown","c6a85539":"markdown","7b678069":"markdown","4eea1746":"markdown","17fcfd66":"markdown","a72e1e58":"markdown","cd15de8b":"markdown"},"source":{"85aeb040":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4dee1fea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot  as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split","56f3adbf":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","73c21a58":"X_train = train_data.drop(['label'],axis = 1)","98e3ec2d":"y_train = train_data.label\ny_train","c1a11c05":"#Converting y_train into a numpy array\ny_train = np.array(y_train)\n","2e24a941":"y_train = to_categorical(y_train, num_classes = 10)","f46f66a2":"y_train.shape","bb929f1c":"X_train = X_train \/ 255\ntest_data = test_data \/ 255","6ba10dd6":"test_data.shape","804cfb98":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test_data.values.reshape(-1,28,28,1)","09a9687d":"# Split the train and the validation set for the fitting\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)","27a66de5":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train) #fitting the imagedatagenerator that we created by training data\n","a0de6bb4":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n","680a83ec":"model.compile(\n             loss = 'categorical_crossentropy',\n             optimizer = RMSprop(lr=0.001),\n             metrics = ['accuracy'])","96869c07":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","930f5d2c":"model.fit(datagen.flow(X_train,y_train, batch_size = 70),\n          epochs = 1,\n          \n          validation_data = (X_val, y_val),\n          callbacks = [learning_rate_reduction] )\n","d6109d61":"#for computiopnal purpuse run  only for 1 epoch.","fcecebd3":"model.summary()","592d2d53":"pred = model.predict(test)","8db52001":"pred = np.argmax(pred,axis = 1)","c08cce8e":"Submission = pd.DataFrame(pred,columns = ['Label'])","f6af29ab":"x= np.arange(1,28001)\nx = pd.DataFrame(x, columns = ['ImageId'])","28138d8a":"Submission = pd.concat([x,Submission],axis = 1)","d86d7573":"Submission.to_csv('Sub.csv',index = False)","7dd4185f":"train_data\ntest_data","bda954a1":"image_size = [224,224]","95402492":"model =keras.applications.VGG16(input_shape = image_size + [3], weights = 'imagenet', include_top = False)\nfor layer in model.layers:\n    layer.trainable = False","92feed14":"model.summary()","dd32fb50":"from keras.applications import ResNet50\nmodel = Sequential()\nmodel.add(ResNet50(include_top = False,\n                  weights = 'imagenet',\n                  pooling = 'Avg'))\nmodel.add(Dense(10,activation = 'softmax'))\nfor layer in model.layers:\n    layer.trainable = False","bb572dee":"model.compile(optimizer = 'adam',\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy']   )","eb9e8ca3":"train = ImageDataGenerator(rescale = 1\/255)\nvalidation = ImageDataGenerator(rescale = 1\/255)\ntesting = ImageDataGenerator(rescale = 1\/255)","8c33b1fd":"train_dataset = train.flow_from_directory( 'train_data',\n                                           target_size = (224,224),\n                                           batch_size = 32,\n                                           class_mode = 'categorical'\n    \n    \n                                           )\n\n\ntesating_dataset = testing.flow_from_directory(\"test_data\",\n                                           target_size = (224,224),\n                                           batch_size = 32,\n                                           class_mode = 'categorical'\n                                                   )","03f4c3b3":"This method is actually used to adjust the learning rate based on the base result.\nIf accuracy is not improving for n numbers( patience=3) of steps then adjust the learning rate.\nHere i put patience = 3 and learning rate will be decreasing to half(factor = .5).","b98ddd77":"Reshaping data into (28,28) pixels grayscale","6d65fd7b":"#Loading the data","cdd34a6f":"Reducing the learning rate","d3734181":"#  Prediction","748f417d":"\n\n\nScaling the data ","37a1a2ca":"# Training the model","9b41ff75":"# Compiling the model","c6a85539":"# Preprocessing of data","7b678069":"Submitting the result","4eea1746":"Label Encoding","17fcfd66":"Spliting the data into train and validation part","a72e1e58":"# Building the model ","cd15de8b":"Augmenting data to prevent from overfitting"}}