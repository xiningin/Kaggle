{"cell_type":{"922c360c":"code","9fc88a62":"code","33e29d6c":"code","0875d1d0":"code","8b477284":"code","895ec3b0":"code","b6384292":"code","28566d22":"code","62cf8480":"code","70a32862":"code","8d152b5e":"code","43d986aa":"code","9795f476":"code","2274a87e":"code","cf57fad7":"code","d66788ad":"code","d81e1702":"code","6ee65579":"code","8745f1d7":"code","5c9d9aaf":"code","7eb6ca10":"code","be77cd32":"markdown","0670551b":"markdown","ad10d25c":"markdown","ef3ac034":"markdown","4fe5b8b7":"markdown","67a76fe1":"markdown","8651b6d7":"markdown","4a1e430f":"markdown","29a234b7":"markdown","4f03ff19":"markdown","d3fea2cf":"markdown","d8e0a9fb":"markdown","c352edc8":"markdown","d786e572":"markdown","732705f1":"markdown","7cfeb013":"markdown","aa73ddcf":"markdown","4f529326":"markdown","a921b34b":"markdown","d26709c0":"markdown","28214b7e":"markdown","1480ae1b":"markdown","346f2951":"markdown","6f0391fe":"markdown","0c166ac3":"markdown","f2751043":"markdown","635f5235":"markdown"},"source":{"922c360c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n","9fc88a62":"df=pd.read_csv('..\/input\/mushrooms.csv')","33e29d6c":"df.head()","0875d1d0":"df.info()","8b477284":"df.describe()","895ec3b0":"sns.countplot(x='class',data=df)","b6384292":"lb_class = LabelEncoder()\ndf[\"class_code\"] = lb_class.fit_transform(df[\"class\"])\ndf[[\"class\", \"class_code\"]].head(5)","28566d22":"sns.set(style=\"darkgrid\")\nfig,axs=plt.subplots(nrows=8,ncols=3,figsize=(30, 75))\n\ni=0\nj=0\nk=0\n\nfor col in df.columns:\n    \n    i=int(k\/3)\n    j=k%3\n    \n    axe=sns.countplot(x=col, hue=\"class\", data=df,ax=axs[i][j]) # for Seaborn version 0.7 and more\n    \n    bars = axe.patches\n    half = int(len(bars)\/2)\n    left_bars = bars[:half]\n    right_bars = bars[half:]\n\n    for left, right in zip(left_bars, right_bars):\n        height_l = np.nan_to_num(left.get_height())\n        height_r = np.nan_to_num(right.get_height())\n        total = height_l + height_r\n\n        axe.text(left.get_x() + left.get_width()\/2., height_l + 40, '{0:.0%}'.format(height_l\/total), ha=\"center\")\n        axe.text(right.get_x() + right.get_width()\/2., height_r + 40, '{0:.0%}'.format(height_r\/total), ha=\"center\")\n    \n    k=k+1","62cf8480":"df.columns","70a32862":"df=pd.get_dummies(data=df,columns=[ 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n       'ring-type', 'spore-print-color', 'population', 'habitat'],drop_first=False)","8d152b5e":"print(len(df.columns))\ndf.head()","43d986aa":"X=df.drop(['class','class_code'],axis=1)\ny=df['class_code']","9795f476":"n_components=[1,10,20,30,40,50,75,100]\n\nfor comp in n_components:\n    pca_comp=PCA(n_components=comp)\n    pca_comp.fit_transform(X)\n    print(comp,sum(pca_comp.explained_variance_ratio_)*100)","2274a87e":"pca = PCA(n_components=40)\npca_x=pd.DataFrame(pca.fit_transform(X))\n\nsum(pca.explained_variance_ratio_)","cf57fad7":"pca_x.head()","d66788ad":"X_train, X_test, y_train, y_test = train_test_split(pca_x,y,test_size=0.3,random_state=15)","d81e1702":"LR_model= LogisticRegression()\n\nLR_model.fit(X_train,y_train)\n\nLR_y_pred = LR_model.predict(X_test)  \n\naccuracy=accuracy_score(y_test, LR_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, LR_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","6ee65579":"GB_model= GaussianNB()\n\nGB_model.fit(X_train,y_train)\n\nGB_y_pred = GB_model.predict(X_test) \n\naccuracy=accuracy_score(y_test, GB_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, GB_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","8745f1d7":"RF_model=RandomForestClassifier(n_estimators=10)\n\nRF_model.fit(X_train,y_train)\nRF_y_pred = RF_model.predict(X_test) \n\naccuracy=accuracy_score(y_test, RF_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, RF_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","5c9d9aaf":"SVM_model=svm.LinearSVC()\n\nSVM_model.fit(X_train,y_train)\nSVM_y_pred = SVM_model.predict(X_test)   \n\naccuracy=accuracy_score(y_test, SVM_y_pred)*100\n\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, SVM_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","7eb6ca10":"knn_model=KNeighborsClassifier()\n\nknn_model.fit(X_train,y_train)\n\nknn_y_pred = knn_model.predict(X_test)  \n\naccuracy=accuracy_score(y_test, knn_y_pred)*100\nprint(\"Accuracy Score: \",\"{0:.2f}\".format(accuracy))\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test, knn_y_pred)),annot=True,fmt=\"g\", cmap='viridis')","be77cd32":"Read the csv file into a Panda Dataframe","0670551b":"<font size=5><b>Exploratory Data Analysis<\/b><\/font>\n\nLet us do some Exploratory Data Analysis on this dataset\n\nThe below graph represents the percentage of  distribution of target class (poisonous or edible) by its individual attributes.\n\nEx: Consider the bruises (bruises=t,no=f) variable,\n\nIf a sample contains bruises (bruises=t), the proabablity that it is edible is 82%","ad10d25c":"Since these are categorical variables without any definite order(nominal variables),dummy variables has to be created for all the  categorical variables.\n\nThis can be accomplished using get_dummies function from pandas","ef3ac034":"**Logistic Regression**\n\n","4fe5b8b7":"Check out the head of the dataframe, this has created 119 columns!","67a76fe1":"**K-Nearest Neighbours**","8651b6d7":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes.This is often the go to \nalgorithm for many classification problems.\n\nThis algorithm has give the perfect accuracy rate of 100% !","4a1e430f":"SVM model gives out an impressive accuracy rate of 99.88 %","29a234b7":"**Naive Bayes**","4f03ff19":"<font size=4><b>Machine Learning<\/b><\/font>\n\nLet us explore the various classification algorithms and check which performs well on this dataset.\nBelow are the various different algorithms we are goig to explore.\n\n* Logistic Regression\n* Naive Bayes\n* Random Forest\n* Support Vector Machine\n* KNN ","d3fea2cf":"Since this has created 119 attributes, it is highly possible that some of the attributes are co-related.One of the way to eliminate this problem and reduce the dimensionality is to use **Principle Component Analysis (PCA)**.\n\n**PCA** is a technique for feature extraction\u200a\u2014\u200aso it combines our input variables in a specific way, then we can drop the \u201cleast important\u201d variables while still retaining the most valuable parts of all of the variables!\n\nAlso please note that PCA does make independent variables less interpretable.\n\nLet's try to figure out how many Principal components we need so that it captures a good variance of the dataset.","d8e0a9fb":"By above Graph we have noticed some intresting observations.\n\n* If a sample contains bruises (bruises=t), the proabablity that it is edible is 82%.\n* If a sample does not have an odor(odor=n), the proabablity that it is edible is 97%.\n* If a sample has a crowded gill-spacing(gill-spacing=w),the proabablity that it is edible is 91%.\n* If a sample has a narrow gill-size(gill-size=n), ,the proabablity that it is poisonous is 89%.\n  \n\nVarious other attributes can be used to determine wheather a sample is edible or poisonous\n","c352edc8":"Check the distribution of the target class.\nHere the data is almost evenly distributed","d786e572":"Logistic Regression achives an impressive accuracy of 99.38% !.This model performs extremely well on this dataset.\nHowever there might be one point of contention, this model has 12 False negatives.\ni.e There are 12 samples which are poisionous but classified as edible.\n\n**Note**: For this dataset, it is OK to have the model to predict an edible sample as poisonous , but it is not acceptable to have a model predict a poisionous sample as edible !.\n\nThis if needed can further be impoved by setting a custom proabablity threshold upon which a sample is regarded as edible.\nEx: If a sample is predicted to be 52% edible, it will be classified as edible under the current model.\nFor a custom threshold of 60% , a sample is classifed as edible only if the probablity is greater than 0.6. If the proabablity that the sample is edible is 0.52, it will be classified as poisonous.\n\nThis can be done using predict_proba function.","732705f1":"**Random Forest**","7cfeb013":"KNN algorithm also provides an accuracy of 100%","aa73ddcf":"<font size=5><b>Mushroom Classification<\/b><\/font>\n\n\n\n**About this Dataset**\n\n\nThis data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one\n\n\n**Attribute Information**\n\n\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y\n4. bruises?: bruises=t,no=f\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n6. gill-attachment: attached=a,descending=d,free=f,notched=n\n7. gill-spacing: close=c,crowded=w,distant=d\n8. gill-size: broad=b,narrow=n\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y\n10. stalk-shape: enlarging=e,tapering=t\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y\n16. veil-type: partial=p,universal=u\n17. veil-color: brown=n,orange=o,white=w,yellow=y\n18. ring-number: none=n,one=o,two=t\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y\n21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d","4f529326":"Do a train-test split on the newly created dataframe with test_size as 0.3","a921b34b":"**Import all the necessary Data processing and Machine Learning libraries**","d26709c0":"**Support Vector Machines**","28214b7e":"**Conclusion:** We can observe that Ramdon Forest and KNN models provide the perfect accuracy on test data and can be concluded as best models for this dataset.","1480ae1b":"Above output shows that the 1st component captures around 16% of variance in the dataset and with only 10 components we capture around 65% of variance.\n\nWith 119 columns in the original dataset, we are able to explain 65% of variance with only 10 pricipal components !.\n\nWe further observe that with 40 components we capture 95% of the variance and with 50 and 75 components we capture aroud 98% and 99% of the variance.\n\n**Note:** One more important aspects to consider while using PCA is that all the variables need to be scaled.However since we are dealing with only categorical variables, i am choosing to skip this particular step as it does not provide a considerable benifit.\n\nLet's go ahead and choose the number of components as 40.","346f2951":"Since sckikit-learn accepts only numeric values.Add a new column Class_code\nand assign a numeric value to both the class.\n\nNote: Edible samples(e) are encoded to 0 and Poisonous samples (p) are encoded to 1.","6f0391fe":"Check the sample data and do some simple analysis on the dataset","0c166ac3":"Check-out the all the columns present in the dataframe","f2751043":"Check out the head of the newly created dataframe after applying PCA.You can observe that a dataframe is created with 40 principal componets.","635f5235":"This model provides an test accuracy of 90%, which is lesser than Logistic regression, let us discard this and proceed with the next algorithm"}}