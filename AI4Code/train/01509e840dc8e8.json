{"cell_type":{"6066fc8e":"code","44b242e8":"code","8334552d":"code","ff2004f6":"code","c4293ad0":"code","317fde12":"code","3256b8b8":"code","7a2b519b":"code","94a1f07b":"code","46603500":"code","0103ce2a":"code","668b19bc":"code","6762c8c9":"code","eaa1aaf7":"code","9cbec00e":"code","5f04b4b1":"code","33ee6410":"code","e105aca6":"markdown","5b2c73f4":"markdown"},"source":{"6066fc8e":"!pip install pytorch-tabnet","44b242e8":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom skimage.filters import threshold_otsu\nfrom tqdm import tqdm\nimport gc\n\nSEED = 0","8334552d":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\", index_col='id')","ff2004f6":"pointy = [0,2,4,9,12,16,19,20,21,23,24,27,28,30,31,32,33,35,39,42,44,46,48,49,51,52,53,56,58,59,60,61,62,63,64,68,69,72,73,75,76,78,79,81,83,84,87,88,89,90,92,93,94,95,98,99]\nbimodal = [x for x in range(0,100) if x not in pointy]\n\npointy = list(map(lambda x: 'f'+str(x), pointy))\nbimodal = list(map(lambda x: 'f'+str(x), bimodal))\n\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]","c4293ad0":"def create_features(df, cols, prefix='new_'):\n    df[prefix+'abs_sum'] = df[cols].sum(axis=1)\n    df[prefix+'std'] = df[cols].std(axis=1)\n    df[prefix+'avg'] = df[cols].mean(axis=1)\n    df[prefix+'max'] = df[cols].max(axis=1)\n    df[prefix+'min'] = df[cols].min(axis=1)\n    \n    return df","317fde12":"train = create_features(train, pointy, 'point_')\ntrain = create_features(train, bimodal, 'bimodal_')\ntest = create_features(test, pointy, 'point_')\ntest = create_features(test, bimodal, 'bimodal_')","3256b8b8":"def check_peak(df, test_df, cols, suffix='_peak'):\n    for col in cols:\n        peak = threshold_otsu(df[col])\n        df[str(col)+suffix] = df[col] > peak\n        test_df[str(col)+suffix] = test_df[col] > peak","7a2b519b":"#check_peak(train, test, bimodal)","94a1f07b":"test.head()","46603500":"X = train.drop([\"target\"], axis=1)\nX_test = test\ny = train[\"target\"]","0103ce2a":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","668b19bc":"import pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score","6762c8c9":"x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","eaa1aaf7":"# TabNetPretrainer\nunsupervised_model_no_preproc = TabNetPretrainer(n_d=32,\n                                                 n_a=32,\n                                                 optimizer_fn=torch.optim.Adam,\n                                                 optimizer_params=dict(lr=2e-2),\n                                                 scheduler_params={\"step_size\":2, # how to use learning rate scheduler\n                                                                   \"gamma\":0.85},\n                                                 mask_type='entmax', # \"sparsemax\",\n                                                )\n\n# fit the model\nunsupervised_model_no_preproc.fit(\n    X,\n    eval_set=[X_test],\n    max_epochs=1000, patience=10,\n    batch_size=1024, virtual_batch_size=1024,\n    num_workers=0,\n    drop_last=False,\n    pretraining_ratio=0.8,\n)\n\n# Make reconstruction from a dataset\nunsupervised_model_no_preproc.save_model('.\/test_pretrain2')\nloaded_pretrain = TabNetPretrainer()\nloaded_pretrain.load_model('.\/test_pretrain2.zip')","9cbec00e":"# define the model\nclf1_nopreproc = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":2, # how to use learning rate scheduler\n                                         \"gamma\":0.85},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='entmax' # \"sparsemax\"\n                      )\n\n# fit the model \nclf1_nopreproc.fit(\n    x_train,y_train,\n    eval_set=[(x_val, y_val)],\n    eval_name=['valid'],\n    eval_metric=['auc'],\n    max_epochs=1000, patience=30,\n    batch_size=1024, virtual_batch_size=1024,\n    num_workers=0,\n    weights=1,\n    drop_last=False,\n    from_unsupervised=loaded_pretrain\n)            ","5f04b4b1":"preds = clf1_nopreproc.predict_proba(X_test)[:,1]","33ee6410":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv', index_col='id')\nsubmission['target'] = preds\nsubmission.to_csv('submission.csv')","e105aca6":"## contribution 2\nThe bimodal distributions clearly influence the target. We can create a boolean comparison as to which peak it sits under\n\nSee: www.kaggle.com\/realtimshady\/eda-feature-exploration","5b2c73f4":"## contribution 1\nI pretty much looked at whether the distribution was a unimodal point, or a bimodal distribution\nThen split the features according to which distribution it was under and then apply feature engineering to each"}}