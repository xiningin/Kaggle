{"cell_type":{"12c2be6e":"code","0d4e89a3":"code","a9fc8681":"code","0f42fed9":"code","8e2f8edb":"code","34c9d185":"code","5c344221":"code","d0a41998":"code","b8115763":"code","d3e8da50":"code","1ce8a06d":"code","7cb26299":"code","c77eb466":"code","d6b3342c":"code","c63378ce":"code","2ffa2f53":"code","a3bfb25b":"code","8f4709fc":"code","857014e5":"code","83b15941":"code","310c492f":"code","1d9a6f2f":"markdown","9894d1a5":"markdown","6fd3c1a0":"markdown","8f31b66e":"markdown","f8989c32":"markdown","5a22d41d":"markdown","329054c9":"markdown","27dd7902":"markdown","713f3c43":"markdown"},"source":{"12c2be6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0d4e89a3":"import os\ndef log(*args):\n    os.system(f'echo \\\"{args}\\\"')\n    print(*args)","a9fc8681":"! pip install --upgrade pip\n! pip install tensorflow_datasets","0f42fed9":"import tensorflow as tf\nfrom tensorflow import keras\nimport pickle\nimport pandas\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport gc\nimport threading\nimport time\nimport tensorflow_datasets.public_api as tfds\nfrom tensorflow.keras import backend as K\nfrom sklearn import metrics","8e2f8edb":"tf.random.set_seed(1234)\nnp.random.seed(1234)\n# The columns give us an idea of what our data looks like\ncolumns = pickle.load(open('\/kaggle\/input\/data-preparation\/columns.pkl','rb'))\nmax_day = 1941\nprint(f'Columns-{len(columns)} : {columns}')\nprint('Target: ',columns[3])\ntarget_col = 3","34c9d185":"# This function is just a wrapper to a function that calls add_time_steps\ndef preprocess(lookback, delay, target_col=target_col ,lookback_step=1,test=False,val_days=0,return_key=False,val=False):\n    # It takes as input a single time series data and applies some transformations to add time step\n    def fn(inputs):\n        # Remember _generate_examples of MyFullDataset we yielded a dictionary containing key and input\n        values = inputs['input'] \n        key = inputs['key'] \n        return add_time_steps_ntarget((key,values),\n                              lookback,delay,\n                              lookback_step=lookback_step,\n                              target_col=target_col,\n                              val_days=val_days,\n                              val=val,\n                              test=test,\n                              return_key=return_key)\n    return fn\n   \n# This function takes an item with all it 1941 days salles, prices, and calendar data adds lookback \n# and generate the inputs and targets. \n# This function output a single target\ndef add_time_steps(inputs, lookback, delay,target_col=target_col,test=False,lookback_step=1,val=False,val_days=0,return_key=False):\n    key,values = inputs\n    max_index = values.shape[0] - 1\n    min_index = 0\n    y=None\n    idx = tf.signal.frame(tf.range(min_index,max_index),lookback,lookback_step)\n    if not test:\n        idx = idx[tf.reduce_all(idx+delay <= max_index,axis=-1)]\n        if val:\n            idx = idx[-val_days:]\n        else:\n            if val_days:\n                idx = idx[:-val_days]\n        y_idx = idx[...,-1]+delay\n        y = tf.gather(values, y_idx)[...,target_col]\n    else:\n        idx = idx[-delay:]\n    X = tf.gather(values, idx)\n    if not test and return_key:\n        return (key,X,y)\n    return (X,y) if not test else (key,X)\n\n# This function takes an item with all it 1941 days salles, prices, and calendar data adds lookback \n# and generate the inputs and targets. \n# this function generates for each input the next 28 forecasts\ndef add_time_steps_ntarget(inputs, lookback, delay,target_col=target_col,test=False,lookback_step=1,val=False,val_days=0,return_key=False):\n    key,values = inputs\n    max_index = values.shape[0] - 1\n    min_index = 0\n    val_steps = val_days\/\/delay\n    y=None\n    idx = tf.signal.frame(tf.range(min_index,max_index),lookback,lookback_step)\n    if not test:\n        y_idx = tf.range(delay) + idx[:,-1,tf.newaxis]\n        \n        select = tf.math.less_equal(y_idx[:,-1],max_index)\n        idx = idx[select]\n        y_idx = y_idx[select]\n        \n        if val:\n            y_idx = y_idx[-val_steps:]\n            idx = idx[-val_steps:]\n        else:\n            if val_days:\n                idx = idx[:-val_steps]\n                y_idx = y_idx[:-val_steps]\n        y = tf.gather(values[...,target_col], y_idx)\n    else:\n        idx = idx[-1:]\n    X = tf.gather(values, idx)\n    if not test and return_key:\n        return (key,X,y)\n    return (X,y) if not test else (key,X)","5c344221":"# We need again this class\nclass MyFullDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        shape = (max_day,len(columns))\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"input\": tfds.features.Tensor(shape=shape,dtype=tf.float32),\n                \"key\": tfds.features.Tensor(shape=(),dtype=tf.int32),\n            }),\n        )\n    \n   \n    def _generate_examples(self,**args):\n        # We no longer need this function because we already build our dataset\n        pass","d0a41998":"log(\"Download dataset\")\nds_path = '\/kaggle\/input\/data-preparation\/'\nbuilder = MyFullDataset(data_dir=ds_path)\nbuilder.download_and_prepare()\n\n# Amazing right look at your dataset info : we only have one split named train.\n# No need to worry we will use our preprocess and build differents pipelines to access the training, validation and test data from this single split\nlog(builder.info)","b8115763":"log(\"Try data access\")\n \ndataset = builder.as_dataset()['train']\n\nfor item in  dataset.take(2):\n    # We access 2 time-series\n    log('key: ', item['key']) \n    log('input: ',item['input'].shape) \n\n# We apply some transformations: add lookback and get the input and target\nlookback = 5\ndelay = 28\npreprocessor = preprocess(lookback, delay)\ndataset = dataset.map(preprocessor)\nfor X,y in  dataset.take(1):\n    log(X.shape,y.shape)\n\ndel dataset\ngc.collect()","d3e8da50":"val_days = 28 # we use last 28 days of each time step for validation\nlookback = 28 # time steps\nlookback_step = 1\ndelay = 28 # We we are forecasting 28 days in the future\nbatch_size = 2**10\nbuffer_size= batch_size*100\nprefetch = 10 # \ntotal_num_examples = 30490\nds_name = 'train' # The split name\nload_weights = True\ntrain = False\nlog(batch_size)","1ce8a06d":"# Since our dataset is not normalized, we need  our first layer to be a normalization layer\ndef build_model():\n    \n    input_ = keras.layers.Input(shape=(lookback,len(columns),))\n    bn = keras.layers.BatchNormalization()(input_)\n    \n    lstm = keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True,recurrent_dropout=0.1))(bn)\n    lstm = keras.layers.Bidirectional(keras.layers.LSTM(256,recurrent_dropout=0.1))(lstm)\n    dense = keras.layers.Dense(delay,activation=keras.activations.relu)(lstm)#delay\n    dense = keras.layers.Activation('relu')(dense)\n    \n    model = keras.models.Model(input_,dense)\n    log(model.summary())\n    model.compile(optimizer=keras.optimizers.Adam(0.01),loss=keras.losses.mean_squared_error, metrics=[])\n    return model","7cb26299":"model = build_model()\ndataset_ = builder.as_dataset()[ds_name].repeat()\n# Let Try to load weight from the previous commit\nif load_weights and os.path.exists('\/kaggle\/input\/modelh5\/model.h5'):\n    log('Loading weights')\n    model.load_weights('\/kaggle\/input\/modelh5\/model.h5')","c77eb466":"# This pipeline provides training dataset\npreprocessor = preprocess(lookback, delay,lookback_step=lookback_step,val_days=val_days)\ntrain_dataset = dataset_.take(total_num_examples).repeat().shuffle(buffer_size=1000).map(preprocessor).unbatch()\ntrain_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(batch_size).prefetch(prefetch)\ntrain_steps = (total_num_examples*max_day-val_days-delay)\/\/batch_size\/\/lookback_step\nlog(train_steps)","d6b3342c":"# This one provides validation dataset\nval_preprocessor = preprocess(lookback, delay,val_days=val_days,val=True,lookback_step=lookback_step)\nval_dataset = dataset_.take(total_num_examples).repeat().map(val_preprocessor).unbatch()\nval_dataset = val_dataset.batch(total_num_examples\/\/10).prefetch(prefetch)\nval_steps = 10\nlog(val_steps)","c63378ce":"if train:\n    log('Start training')\n    history = model.fit(train_dataset,\n                      steps_per_epoch=train_steps,\n                      epochs=10,\n                      validation_data=val_dataset,\n                      validation_steps=val_steps,\n                      callbacks=[\n                            keras.callbacks.EarlyStopping(\n                                monitor='loss',\n                                patience=100,\n                                restore_best_weights=True,\n                            ),\n                            keras.callbacks.ModelCheckpoint(\n                                filepath=f'model.h5',\n                                monitor='val_loss',\n                                save_best_only=True,\n                                save_weights_only=True,\n                            ),\n                        ],\n                )","2ffa2f53":"if train:\n    pd.DataFrame(history.history).plot(figsize=(15,8))","a3bfb25b":"del train_dataset\ndel val_dataset\ngc.collect()","8f4709fc":"#model.load_weights(f'model.h5')","857014e5":"# Evaluate our validation data with the leaderboard\n\ndf_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv',index_col=0)\ndf_val.iloc[total_num_examples:] = 0 # set evaluation to 0\n\nval_preprocessor = preprocess(lookback, delay,val_days=val_days,val=True,lookback_step=lookback_step)\n\nfn_filter_key = lambda input_ : input_['key']\nfor keys in dataset_.take(total_num_examples).map(fn_filter_key).batch(total_num_examples):\n    print('keys : ',keys.shape)\n    keys = keys.numpy()\n    keys = np.argsort(keys) # retrieve items ordering\n\ndataset = dataset_.take(total_num_examples)\ndataset = dataset.map(val_preprocessor).batch(total_num_examples)\n\nfor X,y in dataset:\n    log(f'X: {X.shape}, y: {y.shape}, keys: {keys.shape}')\n    X = tf.reshape(X,(-1,lookback,len(columns)))#\n    y_pred = model.predict(X, batch_size=30490)\n\ndf_val.iloc[:total_num_examples,:] = y_pred[keys]\ndf_val.to_csv('validation_submission.csv')","83b15941":"def generate_submission(model, lookback,lookback_step, delay, out_path='submission.csv'):\n    preprocessor = preprocess(lookback, delay,lookback_step=lookback_step,test=True)\n    df_sub = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv',index_col=0)\n    df_sub.iloc[:total_num_examples,:] = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv',index_col=0).iloc[:,-28:].values\n    \n    dataset = dataset_.take(total_num_examples)\n    dataset = dataset.map(preprocessor).batch(total_num_examples)\n    \n    for keys,X in dataset:\n        keys = np.argsort(keys.numpy()) # retrieve items ordering\n        log('X: ',X.shape)\n        X = tf.reshape(X,(-1,lookback,len(columns)))#\n        y = model.predict(X, batch_size=30490)\n        \n    df_sub.iloc[total_num_examples:,:] = y[keys]\n    df_sub.to_csv(out_path)\n    del dataset","310c492f":"log('Generate Submission')\ngenerate_submission(model,lookback,lookback_step, delay)","1d9a6f2f":"# Training","9894d1a5":"As a beginner, I'll appreciate any suggestions to improve my work. Please don't forget to upvote if you liked this kernel.","6fd3c1a0":"This an example of how to access the split's data by getting creating tf.data.Dataset object","8f31b66e":"## Let define some utils","f8989c32":"### Training & Validation pipelines \n\n","5a22d41d":"## Prepare for training","329054c9":"### We install the TFDS library and import required modules: ","27dd7902":"In the [Data preparation notebook](https:\/\/www.kaggle.com\/tchaye59\/data-preparation), we've prepared the dataset in a suitable format. Now let feed it to a model.","713f3c43":"Now we are ready to build our pipelines \ud83d\ude0b\ufe0f\n\n###  Let create the data source\n\nThe code is the same as what we did to prepare the dataset but since the dataset is already available TFDS will not try to create it"}}