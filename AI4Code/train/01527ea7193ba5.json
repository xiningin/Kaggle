{"cell_type":{"f34ac297":"code","7c797ccb":"code","96052ba1":"code","66ac0e00":"code","92e147fa":"code","76a1d12b":"code","d25e9558":"code","b8b00560":"code","ccea0b66":"code","5b82ea31":"code","144975d2":"markdown","8734a0d2":"markdown","ba02e8ea":"markdown","606d0281":"markdown","6e8bf301":"markdown","2cbe4194":"markdown","fad52909":"markdown","fd9450f6":"markdown"},"source":{"f34ac297":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c797ccb":"data = [[0,0,1,1],[0,1,0,1]]\nX = np.array(data)","96052ba1":"result_and = [[0,0,0,1]]\nresult_or = [[0,1,1,1]]\nresult_xor = [[0,1,1,0]]","66ac0e00":"Y = np.array(result_xor)\n\ndef activate(x):\n    return 1\/(1+np.exp(-x))\ndef intialize_params(inputfeatures , hiddenneurons , outputfeatures):\n    W1=np.random.randn(hiddenneurons, inputfeatures)\n    W2=np.random.randn(outputfeatures, hiddenneurons)\n    b1=np.zeros((hiddenneurons,1))\n    b2=np.zeros((outputfeatures,1))\n    \n    params = {\n        \"W1\":W1,\n        \"W2\":W2,\n        \"b1\":b1,\n        \"b2\":b2,\n    }\n    return params\n\ndef forward(X, Y, params):\n    n = X.shape[1]\n    W1 = params[\"W1\"]\n    W2 = params[\"W2\"]\n    b1 = params[\"b1\"]\n    b2 = params[\"b2\"]\n    \n    Z1= np.dot(W1,X)+b1\n    A1= activate(Z1)\n    Z2= np.dot(W2,A1)+b2\n    A2= activate(Z2)\n    \n    cache = (Z1,A1,W1,b1, Z2,A2,W2,b2)\n    logprobs = np.multiply(np.log(A2), Y)+ np.multiply(np.log(1-A2),(1-Y))\n    cost = -np.sum(logprobs)\/n\n    return cost, cache, A2\n    \ndef backprop(X ,Y, cache):\n    n = X.shape[1]\n    (Z1,A1,W1,b1, Z2,A2,W2,b2)= cache\n    \n    dz2= A2-Y\n    dW2= np.dot(dz2,A1.T)\/n\n    db2= np.sum(dz2,axis=1, keepdims=True)\n    \n    dA1= np.dot(W2.T, dz2)\n    dz1= np.multiply(dA1, A1*(1- A1))\n    \n    dW1= np.dot(dz1,X.T)\/n\n    db1= np.sum(dz1,axis = 1,keepdims = True)\/n\n    \n    gradient= {\"dZ2\": dz2, \"dW2\": dW2, \"db2\": db2,\n                 \"dZ1\": dz1, \"dW1\": dW1, \"db1\": db1}\n    return gradient\n    \ndef update_params(params,gradient,Lr):\n    params[\"W1\"]= params[\"W1\"]-Lr*gradient[\"dW1\"]\n    params[\"W2\"]= params[\"W2\"]-Lr*gradient[\"dW2\"]\n    params[\"b1\"]= params[\"b1\"]-Lr*gradient[\"db1\"]\n    params[\"b2\"]= params[\"b2\"]-Lr*gradient[\"db2\"]\n    return params\n\nhiddenneurons = 2\ninputfeatures= X.shape[0]\noutputfeatures= Y.shape[0]\nparams = intialize_params(inputfeatures, hiddenneurons, outputfeatures)\nepochs= 100000\nLr= 0.01\nlosses = np.zeros((epochs,1))\n\nfor i in range(epochs):\n    losses[i,0], cache, A2 = forward(X,Y,params)\n    gradient = backprop(X, Y,cache)\n    param = update_params(params,gradient,Lr)\n\nplt.grid(True)\nplt.plot(losses)\nplt.xlabel(\"EPOCHS\")\nplt.ylabel(\"Loss value\")\nplt.show()","92e147fa":"X_new = np.array([[1, 1, 0, 0], [0, 1, 0, 1]]) \n# print(X)\n_, _, A2 = forward(X_new, Y, params)\nprint(A2)\nprediction = np.zeros((A2.size))\n\nfor a in A2:\n    for i in range(4):\n        if(a[i]>0.5):\n            prediction[i]=1.0\n\nprint(prediction)","76a1d12b":"X = np.array([[1,1],[1,0],[0,1],[0,0]]) \nX_new = np.array([[1,0],[1,1],[0,0],[0,1]]) \nresult_xor = [0,1,1,0]\nY= np.array(result_xor)","d25e9558":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='lbfgs', alpha=0.00001 ,hidden_layer_sizes=(5, 4), random_state=1)\nclf.fit(X, Y)\nY_pred= clf.predict(X_new)\nprint(Y_pred)","b8b00560":"def Step_activation(x):\n    if x>=0:\n        return 1\n    else:\n        return 0\n\ndef model(x,w,b):\n    o1= np.dot(x,w)+ b\n    y= Step_activation(o1)\n    return y\n\ndef And_model(x):\n    w= np.array([1,1])\n    b= -1.5 \n    # calcuated manually\n    # inital bias = 1 \n    # 0 , 0 \n    # net = 0+0+1 = 1 but output we need is 0\n    # therefore bias = -1\n    # new net = 0+0-1 =-1 but due to step function -1 < 0 so it will be = 0\n    # \n    # 0 , 1\n    # net = 0+ 1 - 1= 0 but due to step function 0 becomes 1 as 0>=0. \n    # therefore bias =-1.5\n    # new net = -.5 which equals 0\n    #\n    #\n    # 1 , 1\n    # net = 1+1-1.5 = 0.5 which equals 1 so no change in bias \n    #\n    #thus bias = -1.5\n    return model(x,w,b)\n\ntest_all = np.array([[0,0],[0,1],[1,0],[1,1]])\n\nfor test in test_all:\n    print(\"AND({}, {}) = {}\".format(test[0], test[1], And_model(test)))","ccea0b66":"def Step_activation(x):\n    if x>=0:\n        return 1\n    else:\n        return 0\n\ndef model(x,w,b):\n    o1= np.dot(x,w)+ b\n    y= Step_activation(o1)\n    return y\n\ndef or_model(x):\n    w= np.array([1,1])\n    b= -1\n    # calcuated manually\n    # inital bias = 1 \n    # 0 , 0 \n    # net = 0+0+1 = 1 but output we need is 0\n    # therefore bias = -1\n    # new net = 0+0-1 =-1 but due to step function -1 < 0 so it will be = 0\n    # \n    # 0 , 1\n    # net = 0+ 1 - 1= 0 but due to step function 0 becomes 1 as 0>=0. \n    # Which we need so no change in bias\n    #\n    # 1 , 1\n    # net = 1+1-1.5 = 0.5 which equals 1 but we need 0 so we change bias\n    # \n    # thus bias = -1\n    return model(x,w,b)\n\ntest_all = np.array([[0,0],[0,1],[1,0],[1,1]])\n\nfor test in test_all:\n    print(\"OR({}, {}) = {}\".format(test[0], test[1], or_model(test)))","5b82ea31":"def Step_activation(x):\n    if x>=0:\n        return 1\n    else:\n        return 0\n\ndef model(x,w,b):\n    o1= np.dot(x,w)+ b\n    y= Step_activation(o1)\n    return y\n\ndef not_model(x):\n    w= -1\n    b= 0.5\n    return model(x,w,b)\n\ndef and_model(x):\n    w= np.array([1,1])\n    b= -1.5\n    return model(x,w,b)\n\ndef or_model(x):\n    w= np.array([1,1])\n    b= -0.5\n    return model(x,w,b)\n\ndef xor_model(x):\n    y1= and_model(x)\n    y2= or_model(x)\n    y3= not_model(y1)\n    x_new= np.array([y2,y3])\n    return and_model(x_new)\n    \ntest_all = np.array([[0,0],[0,1],[1,0],[1,1]])\n\nfor test in test_all:\n    print(\"XOR({}, {}) = {}\".format(test[0], test[1], xor_model(test)))","144975d2":"#### XOR\n\n![Screenshot 2021-10-28 at 10.15.37 AM.png](attachment:52ad138a-c0e9-43e3-bdda-1e4d3df996e3.png)","8734a0d2":"#### AND","ba02e8ea":"### ANN model - Perceptron","606d0281":"### Preparing Data","6e8bf301":"#### Implementation","2cbe4194":"### ANN Model - Backpropagation","fad52909":"#### OR","fd9450f6":"### SKLEARN"}}