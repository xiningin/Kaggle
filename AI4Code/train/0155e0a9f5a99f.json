{"cell_type":{"309c2133":"code","9f1fc370":"code","f9dbb4c7":"code","ccf3ca48":"code","5d880b33":"code","295dfd3d":"code","d886b3f8":"code","448b480d":"code","7ff3a007":"code","406aa49b":"code","d6f08d4f":"code","ebcd13df":"code","8aa0ce19":"code","e3d2d8bf":"code","c3208656":"code","e76cffa0":"code","a820c230":"code","ad62888a":"code","7fac3330":"code","954985f1":"code","9892a23b":"code","5760a428":"code","249021df":"code","75d3ebd5":"code","30bd8278":"code","209fd9f4":"code","bf30a3cb":"code","a5bfcc55":"code","1709bc5e":"code","16f085c4":"code","a0ffbef1":"code","74ac6ed1":"code","de553c78":"code","47868749":"code","f4fe03f4":"code","063abe5a":"code","5db7ff88":"code","b3d5e20e":"code","ac6b3050":"code","8a3b798f":"code","83d8cbc0":"code","74b3e1ed":"code","0856f8a0":"code","ddb5f1a5":"code","15cac688":"code","0f42e06a":"code","c55b0f4f":"code","56c7285d":"code","156a12cb":"code","eb5edf7a":"code","c930a5fb":"code","00826b96":"code","0f60eea0":"code","37651d47":"code","48827dea":"code","93ee2a94":"code","f6efcc8d":"code","67b51f28":"code","5d8d94c3":"code","8c19b86e":"code","eceb643b":"code","b6b3727b":"code","f02f453a":"code","30b15447":"code","a9701dbb":"code","f0ca7ae4":"code","14f0820d":"code","67d9d513":"code","244f615d":"code","0580c376":"code","94ba7170":"code","ff1cf031":"code","28d0b17f":"code","6dde3039":"code","e9aafdbd":"code","19904d4f":"code","96492930":"code","a6f70a32":"code","f3a9b89e":"code","4617699d":"code","f46aaf20":"code","82179c59":"code","e9ebdcc1":"code","186ebe63":"code","e51e80ec":"code","d49e110c":"code","6c70624c":"code","1b179d39":"code","32cec87b":"code","d8576895":"code","6e936c55":"code","0245ac0f":"code","eef909d9":"code","ba584e10":"code","3746dfa3":"code","3ce2ffc0":"code","9cd6da66":"code","d7681cf1":"code","c406092e":"code","79c46f52":"code","b80770bb":"code","320a1ebc":"code","83642724":"code","310ce529":"code","a32cfd3c":"code","3811eb97":"code","97c51ef6":"code","fc5ab28e":"code","047bc689":"markdown","f1592d49":"markdown","e4759d92":"markdown","ea1db4e3":"markdown","5d5d1713":"markdown","3b7b8e63":"markdown","33921c4e":"markdown","bb83b4db":"markdown","0d478922":"markdown","08ae8a94":"markdown","8792425e":"markdown","381a955b":"markdown","831de21b":"markdown","cab8c09c":"markdown","f8506584":"markdown","570a9c28":"markdown","4fdd8710":"markdown","1a79dd46":"markdown","2a9da339":"markdown","1006d3ec":"markdown","b41aa39f":"markdown","e226f348":"markdown","18c8f9cf":"markdown","300944e5":"markdown","8a7eec0b":"markdown","8126c3f3":"markdown","4aee725c":"markdown","9083fc3c":"markdown","2c068a92":"markdown","1330a539":"markdown","faed7afa":"markdown","398fb9a8":"markdown","746c87b7":"markdown","3f2de76c":"markdown","4c3f4b94":"markdown","2540e71e":"markdown","44f575b7":"markdown","b0523391":"markdown","ee233774":"markdown","3dab7960":"markdown","3d4b3d50":"markdown"},"source":{"309c2133":"# Basic Imports\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Visualization \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\nmatplotlib.rcParams['figure.figsize'] = (12,8)  \n\n# Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n\n# Evaluation\nfrom sklearn.metrics import classification_report,confusion_matrix","9f1fc370":"tf.__version__","f9dbb4c7":"# Loading the dataset\ndf_info = pd.read_csv('..\/input\/lendingclub-data-sets\/lending_club_info.csv', index_col='LoanStatNew')\ndf = pd.read_csv('..\/input\/lendingclub-data-sets\/lending_club_loan_two.csv')","ccf3ca48":"# df_info contains description about all the features present in dataset(df)\n# Target is loan_status\ndf_info.head()","5d880b33":"df.head()","295dfd3d":"df.info()","d886b3f8":"# Function to get information about a feature\ndef feature_info(col_name):\n    return '{} : {}'.format(col_name,df_info.loc[col_name]['Description'])\n\n# Info about Target ('loan_status')\nfeature_info('loan_status')","448b480d":"df.describe().T","7ff3a007":"# Get numeric columns\nnum_cols = df.select_dtypes(include=[np.number]).columns.values\nprint('Numeric cols :',num_cols)","406aa49b":"# Get Non-numeric\/categorical columns\ncat_cols = df.select_dtypes(exclude=[np.number]).columns.values\nprint('Categorical cols :',cat_cols)","d6f08d4f":"# Check for missing data\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.tight_layout()","ebcd13df":"# Number of missing values\nprint(df.isnull().sum().sort_values(ascending=False))","8aa0ce19":"# Number of missing %\nprint(round((df.isnull().mean()*100).sort_values(ascending=False),2))","e3d2d8bf":"# Countplot of Target (loan_status)\nsns.countplot(df['loan_status'])","c3208656":"# From above graph we have imbalanced data, where 80% of data points are Fully Paid and 20% charged off\n100*df['loan_status'].value_counts()\/(df['loan_status'].count())","e76cffa0":"# Histogram of loan_amnt\nsns.distplot(df['loan_amnt'], kde=False)","a820c230":"# Correlation plot\nsns.heatmap(df.dropna().corr(), annot=True, cmap='viridis')","ad62888a":"# \"installment\" almost perfect correlation with the \"loan_amount\" feature\nprint(feature_info('installment'),'\\n', feature_info('loan_amnt'))","7fac3330":"# As loan_amnt amount increases installments increases\nsns.scatterplot('installment', 'loan_amnt', data=df)","954985f1":"# open credit lines v\/s total number of credit lines in the borrower's credit file.\nprint(feature_info('total_acc'), '\\n',feature_info('open_acc'),'\\n')\nsns.scatterplot('open_acc', 'total_acc', data=df.dropna())\nplt.tight_layout()","9892a23b":"# Boxplot showing the relationship between the loan_status and the loan_amnt\nsns.boxplot('loan_status','loan_amnt',data=df)","5760a428":"# summary statistics for the loan amount, grouped by the loan_status.\ndf.groupby('loan_status')['loan_amnt'].describe()\n# Charged Off loans \"loan amount\" is a bit higher than Fully Paid loans","249021df":"# Explore the \"grade\" column that LendingClub attributes to the loans.\nprint(np.sort(df['grade'].unique()))","75d3ebd5":"# Explore the \"sub_grade\" column that LendingClub attributes to the loans.\nprint(np.sort(df['sub_grade'].unique()))","30bd8278":"# Countplot of grade per loan_status. \nsns.countplot(df['grade'].sort_values(), hue=df['loan_status'])","209fd9f4":"# Countplot of sub_grade per loan_status. \nplt.figure(figsize=(15,6))\nsns.countplot(df['sub_grade'].sort_values(), palette='rainbow',hue=df['loan_status'])","bf30a3cb":"# Isolating F and G and countplot just for those subgrades\nf_g = df[df['sub_grade'].isin(np.sort(df['sub_grade'].unique())[-10:])]\nsns.countplot(f_g['sub_grade'].sort_values(), palette='rainbow', hue=f_g['loan_status'])\n\n# F and G sub grades often doesn't payback their loans","a5bfcc55":"# Copy of df\ndf1 = df.copy()","1709bc5e":"# Function to check missing data after handling missing data\ndef check_null():\n  return round(df1.isna().mean()*100,2).sort_values(ascending = False)","16f085c4":"# Around 10% of mort_acc is missing\nfeature_info('mort_acc')","a0ffbef1":"df1['mort_acc'].value_counts()","74ac6ed1":"# Features that correlates with 'mort_acc'\ndf1.corr()['mort_acc'].sort_values(ascending=False)","de553c78":"# 'total_acc' have high correlation with 'mort_acc'\n# Lookup table for mort_acc values according to total_acc\nmort_acc_avg = df1.groupby('total_acc')['mort_acc'].mean()\nmort_acc_avg","47868749":"# Function \"fill\" \ndef fill(x):\n  '''\n  Take a row and if mort_acc value is missing fill it with w.r.t total_acc,\n  with help of look up table above\n  if mort_acc not null, no change.\n  '''\n  mort_account = x[0]\n  total_account = x[1]\n  if pd.isnull(mort_account):\n    return mort_acc_avg[total_account]\n  else:\n    return mort_account","f4fe03f4":"# Fill missing values using 'fill' function\ndf1['mort_acc'] = df1[['mort_acc', 'total_acc']].apply(fill, axis=1)","063abe5a":"# Verify changes \ndf1['mort_acc'].isnull().sum()","5db7ff88":"check_null()","b3d5e20e":"print(feature_info('emp_title'),'\\n')\nprint('No. of unique job titles : ', df1['emp_title'].nunique())","ac6b3050":"# Too many unique job titles to convert in to a dummy variable feature. \n# Dropping \"emp_title\"\ndf1.drop('emp_title',axis=1, inplace=True)","8a3b798f":"check_null()","83d8cbc0":"print(feature_info('emp_length'),'\\n')\nprint('Different Employment lengths: \\n', sorted(df1['emp_length'].dropna().unique()))","74b3e1ed":"# Countplot of 'emp_lenth' with given order \"emp_length_order\"\nemp_length_order = [ '< 1 year','1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']\nsns.countplot(df1['emp_length'], order=emp_length_order, hue=df1['loan_status'])\n\n# The fully paid and Charged Off ratio looks same all emp_lengths","0856f8a0":"# Charged Off emp loan_status w.r.t emp_length \nemp_co = df1[df1['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']\nprint(emp_co)","ddb5f1a5":"# Fully Paid emp loan_status w.r.t emp_length \nemp_fp = df1[df1['loan_status']==\"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']\nprint(emp_fp)","15cac688":"# Charged off loan_status Ratio w.r.t emp_length \nemp_len = emp_co\/(emp_fp+emp_co)\nprint(emp_len)","0f42e06a":"# Charge off rates are similar across all employment lengths\nemp_len.plot(kind='bar')","c55b0f4f":"# Dropping the emp_length column as it doen't contain much information\ndf1.drop('emp_length',axis=1, inplace=True)","56c7285d":"check_null()","156a12cb":"print(feature_info('title'),'\\n')\n\nprint('No. of different titles: ', df1['title'].dropna().nunique())\nprint('\\nDifferent titles:\\n',df1['title'].dropna().value_counts())","eb5edf7a":"# 48817 different titles to convert in to dummies\n# Dropping title feature\ndf1.drop('title',axis=1, inplace=True) ","c930a5fb":"check_null()","00826b96":"# pub_rec_bankruptcies and revol_util have only 0.14% and 0.07 % of missing values respectively\n\nprint(feature_info('pub_rec_bankruptcies'),'\\n')\nprint('Different values of pub_rec_bankruptcies: ', df1['pub_rec_bankruptcies'].dropna().unique())\nprint('\\npub_rec_bankruptcies counts:\\n',df1['pub_rec_bankruptcies'].dropna().value_counts())","0f60eea0":"# Mean and Median values of pub_rec_bankruptcies and revol_util\n\nprint('pub_rec_bankruptcies:\\n',df['pub_rec_bankruptcies'].describe())\nprint('\\n\\nrevol_util:\\n',df['revol_util'].describe())","37651d47":"# Fill the missing values with median\ndf1 = df1.fillna(df.median())","48827dea":"check_null()","93ee2a94":"# Get all Categorical cols of df1\ncat_cols1 = df1.select_dtypes(exclude=[np.number]).columns.values\nprint('Categorical cols of df1:',cat_cols1)","f6efcc8d":"print(feature_info('term'),'\\n')\ndf1['term'].unique()","67b51f28":"# Converting ' 36 months' to 36, ' 60 months' to 60\ndf1['term'] = df1['term'].map({' 36 months':36, ' 60 months':60})\ndf1['term'].unique()","5d8d94c3":"print(feature_info('grade'))\nprint(df1['grade'].unique(),'\\n')\nprint(feature_info('sub_grade'))\nprint(df1['sub_grade'].unique())","8c19b86e":"# As subgrade itself cointains grades,\n# dropping 'grade' feature\ndf1.drop('grade', axis=1, inplace=True)","eceb643b":"print(feature_info('home_ownership'),'\\n')\nprint('Unique home_ownership :',df1['home_ownership'].unique(),'\\n')\nprint('Value counts of home_ownership :\\n',df1['home_ownership'].value_counts())","b6b3727b":"# Merging \"NONE\" and \"ANY\" to 'OTHERS'\ndf1['home_ownership'] = df1['home_ownership'].replace(['NONE','ANY'],'OTHER')\nprint('Unique home_ownership :',df1['home_ownership'].unique(),'\\n')\nprint('Value counts of home_ownership :\\n',df1['home_ownership'].value_counts())","f02f453a":"print(feature_info('verification_status'),'\\n')\nprint('Unique verification_status :',df1['verification_status'].unique(),'\\n')\nprint('Value counts of verification_status :\\n',df1['verification_status'].value_counts())","30b15447":"sns.countplot(df1['verification_status'], hue=df['loan_status'])","a9701dbb":"# As we are predicting whether or not a loan would be issued when using our model\n# We wouldn't know beforehand whether or not a loan would be issued. \n# dropping this feature\n\nprint(feature_info('issue_d'),'\\n')\ndf1.drop('issue_d', axis=1, inplace=True)","f0ca7ae4":"# Target Feature\n\nprint(feature_info('loan_status'),'\\n')\nprint('Unique loan_status :',df1['loan_status'].unique(),'\\n')\nprint('Value counts of loan_status :\\n',df1['loan_status'].value_counts())","14f0820d":"# New 'load_repaid' column with 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\".\ndf1['loan_repaid'] = df1['loan_status'].apply(lambda x:1 if x=='Fully Paid' else 0)\n\n#Dropping the original loan_status column\ndf1.drop(['loan_status'], axis=1, inplace=True)\nprint('Unique loan_repaid :',df1['loan_repaid'].unique(),'\\n')\nprint('Value counts of loan_repaid :\\n',df1['loan_repaid'].value_counts())","67d9d513":"# Correlation of the numeric features to the new loan_repaid column\ndf1.corr()['loan_repaid'].sort_values()[:-1].plot.bar()","244f615d":"print(feature_info('purpose'),'\\n')\nprint('Unique purpose :',df1['purpose'].unique(),'\\n')\nprint('Value counts of purpose :\\n',df1['purpose'].value_counts())","0580c376":"print(feature_info('earliest_cr_line'),'\\n')\nprint('Unique earliest_cr_line count :',df1['earliest_cr_line'].nunique(),'\\n')\nprint('Value counts of earliest_cr_line :\\n',df1['earliest_cr_line'].value_counts())","94ba7170":"# Extracting the year from the \"earliest_cr_line\" feature.\n# New numeric feature 'earliest_cr_year'\n# dropping the original earliest_cr_line feature.\n\ndf1['earliest_cr_year'] = df1['earliest_cr_line'].apply(lambda date:int(date[-4:]))\ndf1.drop('earliest_cr_line', axis=1, inplace=True)","ff1cf031":"print(feature_info('initial_list_status'),'\\n')\nprint('Unique initial_list_status count :',df1['initial_list_status'].unique(),'\\n')\nprint('Value counts of initial_list_status :\\n',df1['initial_list_status'].value_counts())","28d0b17f":"print(feature_info('application_type'),'\\n')\nprint('Unique application_type count :',df1['application_type'].unique(),'\\n')\nprint('Value counts of application_type :\\n',df1['application_type'].value_counts())","6dde3039":"df1['address']","e9aafdbd":"# Exctracting only zipcode from address into a new \"zip_code\" column\n\ndf1['zip_code'] = df1['address'].apply(lambda x: x[-5:])\ndf1.drop('address', axis=1, inplace=True)","19904d4f":"print('Value counts of zip_code :\\n',df1['zip_code'].value_counts())","96492930":"df1.corr()['installment'].sort_values(ascending=False)","a6f70a32":"# Dropping \"installment\" as it correlates above 90% with \"loan_amnt\"\ndf1.drop('installment', axis=1, inplace=True)","f3a9b89e":"# df2 as copy of df1\ndf2 = df1.copy()","4617699d":"df2","f46aaf20":"# Take all categorical column of df2 in cat_cols2\ncat_cols2 = df2.select_dtypes(exclude=[np.number]).columns.values\nprint(cat_cols2)","82179c59":"# Get dummies for categorical columns\ndummies = pd.get_dummies(df1[cat_cols2], drop_first=True)\n\n# Drop actual categorical columns\ndf2 = df2.drop(cat_cols2,axis=1)\n\n# Concatenate dummy columns with df2\ndf2 = pd.concat([df2,dummies],axis=1)","e9ebdcc1":"df2.head()","186ebe63":"print('Total rows : ', len(df2)) \nprint('Total columns : ', len(df2.columns)) \nprint('columns : \\n', df2.columns.values) ","e51e80ec":"# last row of df2 as new customer\nnew_cust = df2.iloc[-1]\n\n# All rows excluding last data point\nX = df2.iloc[:-1].drop('loan_repaid', axis=1).values\ny = df2.iloc[:-1]['loan_repaid'].values","d49e110c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=22)","6c70624c":"print('X_train : {} \\ny_train : {}'.format(X_train.shape,y_train.shape))","1b179d39":"print('X_test : {} \\ny_test : {}'.format(X_test.shape,y_test.shape))","32cec87b":"# Using MinMaxScaler to convert values between min:0 and max:1\nscaler = MinMaxScaler()","d8576895":"X_train_sca = scaler.fit_transform(X_train)\nX_test_sca = scaler.transform(X_test)","6e936c55":"# Creating Model\n\nmodel = Sequential()\n# Input layer\nmodel.add(Dense(77, activation='relu', kernel_constraint=max_norm(3)))\nmodel.add(Dropout(0.3)) # Drop 30% of neurons randomly\n# Hidden layers\nmodel.add(Dense(38, activation='relu', kernel_constraint=max_norm(3)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(19, activation='relu', kernel_constraint=max_norm(3)))\nmodel.add(Dropout(0.3))\n# Output layer\nmodel.add(Dense(1, activation='sigmoid'))","0245ac0f":"# Compile the created model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Monitor the \"validation_loss\" and \n# when \"min\" value is reached during training, wait for \"10\" epochs and stop training\nearly_stop = EarlyStopping(monitor='val_loss',\n                    verbose=1,\n                    mode='min',\n                    patience=10)\n\n# Tensor Board\nlogdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntfboard = TensorBoard(logdir,\n                      histogram_freq=1,\n                      update_freq = 'epoch')\n\n# Fit the model\nmodel.fit(X_train_sca,\n        y_train,\n        batch_size=256,\n        epochs=100,\n        validation_data=(X_test_sca, y_test),\n        callbacks=[early_stop, tfboard])","eef909d9":"model.summary()","ba584e10":"# New dataframe \"losses\" with model.history.history data\nlosses = pd.DataFrame(model.history.history)\nlosses","3746dfa3":"# Plot training loss v\/s validation loss\nlosses[['loss','val_loss']].plot()","3ce2ffc0":"# Plot training accuracy v\/s validation accuracy\nlosses[['accuracy','val_accuracy']].plot()","9cd6da66":"# Print the final loss and accuracy on test set.\nprint(f'loss: {model.evaluate(X_test_sca, y_test, verbose=0)[0]}\\naccuracy: {model.evaluate(X_test_sca, y_test, verbose=0)[1]}')\n\n\n# Accuracy of 89% is considering an imbalanced data (80% of data points are Fully Paid and 20% charged off)","d7681cf1":"# Predictions on test set\ny_pred = model.predict_classes(X_test_sca)\nprint(y_pred)","c406092e":"# classification report\nprint(classification_report(y_test,y_pred))","79c46f52":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred))","b80770bb":"# Load tenserboard\n%load_ext tensorboard","320a1ebc":"%tensorboard --logdir logs","83642724":"# As satisfied with obtained accuracy,\n# Before predicting for new customer, training on whole dataset (X and y) without splitting.\n\nX_sca = scaler.fit_transform(X)\n\n# Creating final model\nfinal_model = Sequential()\n# Input layer\nfinal_model.add(Dense(77, activation='relu', kernel_constraint=max_norm(3)))\nfinal_model.add(Dropout(0.3)) # Drop 30% of neurons randomly\n# Hidden layers\nfinal_model.add(Dense(38, activation='relu', kernel_constraint=max_norm(3)))\nfinal_model.add(Dropout(0.3))\nfinal_model.add(Dense(19, activation='relu', kernel_constraint=max_norm(3)))\nfinal_model.add(Dropout(0.3))\n# Output layer\nfinal_model.add(Dense(1, activation='sigmoid'))\n\n\n# Compile the created model\nfinal_model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n\n# Fit the model\nfinal_model.fit(X_sca, y,\n        batch_size=256,\n        epochs=30)","310ce529":"# final_model.save('loan_approval.h5')  ","a32cfd3c":"# New customer \"new_cust\"\nnew_cust","3811eb97":"# Function \"assess_customer\"\ndef assess_customer(cust):\n\n  '''\n  Take new customer and reshape the values to shape that model was trained on. (1,77)\n  Transform the new customer attributes using same MinMaxScaler object\n  return the output based on predicted class.\n  '''\n\n  cust = cust.values.reshape(1,77)\n  cust_sca = scaler.transform(cust)\n\n  if final_model.predict_classes(cust_sca)==1:\n    print (\"Customer is likely to pay back the loan \\nLoan can be approved\")\n  else:\n    print (\"Customer is not likely to pay back the loan \\nLoan can't be approved\")","97c51ef6":"# Model predicted\nassess_customer(new_cust.drop('loan_repaid'))","fc5ab28e":"# Actual\nnew_cust['loan_repaid']","047bc689":"# **Data Preparation**\n---\n- Exploring Data\n- Data Preprocessing\n","f1592d49":"## Predictions and Losses","e4759d92":"# **Reference(s):**\n\nThis notebook is created by learning from a course **Python for Data Science and Machine Learning Bootcamp** created by **Jose Portilla**.","ea1db4e3":"# **Data Understanding**\n---","5d5d1713":"## New Customer Loan Repayment Prediction","3b7b8e63":"**emp_title**","33921c4e":"<h1> <div align=\"center\">Lending Club Loan Approval Prediction <\/div><\/h1>\n\n---\n---\n\n\n**Problem Statement**: \n1. To predict whether or not a borrower will pay back their loan, given historical data on loans given out with information on whether or not the borrower defaulted (charge-off)\n\n2. Assessing a new potential customer whether or not they are likely to pay back the loan in the future.\n\n\n\n&nbsp; \n\n\n\n**Data used**: [Click here to download](https:\/\/www.kaggle.com\/hadiyad\/lendingclub-data-sets\/download)\n\n**Solution by**: Aditya Karanth.\n\n**GitHub Profile**: https:\/\/github.com\/Aditya-Karanth\n\n**Kaggle Profile**: https:\/\/www.kaggle.com\/adityakaranth\n\n\n\n\n","bb83b4db":"**initial_list_status**","0d478922":"Summary","08ae8a94":"## **Data Preprocessing**\n\nIn this section, the focus is on making data suitable for model building.\n- Data Cleaning\n- Feature Engineering\n- Data Transformation\n  - Train Test Split\n  - Normalization","8792425e":"<h2>Project Planning :<\/h2> \n\n---\n---\n&nbsp;\n\n\n### **Data Understanding**\n- **Imports -** \n  - Contains all the imports necessary for reading data, visualizations and model buiding and evaluating.\n- **Getting Data -** \n  - Understanding nature of the data *.info()*, *.describe()*\n  - Function to get information about a each feature.\n  - Checking numerical and categorical features.\n\n&nbsp;\n### **Data Preparation**\n- **Exploring Data**\n  - Checking missing data using heatmaps.\n  - Distribution of dependent variable\n  - Visualization of different features to understand relations and distributions.\n  - Understanding the correlation between features using heatmaps.\n\n- **Data Preprocessing**\n  - **Data Cleaning -**\n    - Handling missing data for each feature using defined *function* and *median*.\n    - Removing features with less information and unwanted features.\n\n  - **Feature Engineering -**\n    - Deriving numerical information from categorical features.\n    - Removing features with same information and high correlation.\n    - Converting all categorical features to numerical using *dummies* and dropping all original categorical features.\n\n  - **Data Tranformation -**\n    - Splitting data in to training and testing.\n\n  - **Normalization -**\n    - Scaling training and testing data using *MinMax Scaler*.\n\n&nbsp;\n### **Modeling**\n- Creating the Sequential *ANN* model.\n- Compiling and Fitting the created model with *Early Stopping* and *Tensor Board* and checking the model summary.\n\n&nbsp;\n### **Evaluation**\n- **Predictions and Losses** -\n  - Predictions on test set\n  - Evaluation of model with classification report and confusion matrix.\n  - Obtaining metrics like loss, accuracy, val_loss, val_accuracy from model\n  - Plotting train v\/s validation metrics of model.\n\n- **Tensor Board**-\n  - visualizing metrics on tensor board in colab.\n\n- **New Customer Loan Repayment Prediction**\n  - Training a model on whole dataset (X and y) without splitting.\n  - Function \"assess_customer\" to assess new customers whether they payback loan or not.\n  - Check model predictions on new customer.\n\n","381a955b":"Saving the Trained Model","831de21b":"# **Evaluation**\n---\n\n- Predictions and Losses\n- Tensor Board\n- New Customer Loan Repayment Prediction","cab8c09c":"**emp_length**","f8506584":"#### Normalization","570a9c28":"### Data Cleaning","4fdd8710":"**grade** and **sub_grade**","1a79dd46":"**home_ownership**","2a9da339":"## **Exploring Data**","1006d3ec":"One hot encoding","b41aa39f":"**term**","e226f348":"**address**","18c8f9cf":"**purpose**","300944e5":"## **Imports**\n","8a7eec0b":"<h3>Handling Missing data<\/h3>","8126c3f3":"## Tensor Board","4aee725c":"**title**","9083fc3c":"**earliest_cr_line**","2c068a92":"#### Train Test Split","1330a539":"**loan_status**","faed7afa":"**application_type**","398fb9a8":"### Data Tranformation\n\n- Train Test Split\n- Normalization\n","746c87b7":"# **Modeling**\n---\n","3f2de76c":"**issue_d**","4c3f4b94":"**verification_status**","2540e71e":"For Google colab","44f575b7":"### Feature Engineering","b0523391":"**Other Features**","ee233774":"**installment**","3dab7960":"## **Getting Data**","3d4b3d50":"**mort_acc**\n\n"}}