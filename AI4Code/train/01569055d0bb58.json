{"cell_type":{"66819579":"code","24262537":"code","b7abe506":"code","a638ccfb":"code","ff99d4a4":"code","8537655c":"code","dcbf6cfa":"code","38ce1e6f":"code","bbcf09ee":"code","34f8dedd":"code","7655864f":"code","4ed7248a":"code","b8cd97fb":"code","fb06b769":"code","31b40433":"code","7de4fa7c":"code","7ff25d0b":"code","2ca48e07":"code","0341a4c5":"code","3872a29d":"code","b6a8005f":"code","559fa6f0":"code","6724e757":"code","9a66cad0":"code","0ef5791f":"code","a9ffab5f":"code","f8fc2341":"code","611dd1f1":"code","b4b50a7f":"markdown","1e6cb779":"markdown","fa0f5075":"markdown","f30882f7":"markdown","ea395a5b":"markdown","baeb8478":"markdown","e09ccda6":"markdown","aca7c730":"markdown","924377a4":"markdown"},"source":{"66819579":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","24262537":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","b7abe506":"def weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)","a638ccfb":"#remove total revenue and measurable impressions since they are used to calculate the target metric\ndf.drop(['total_revenue'], axis = 1, inplace=True)","ff99d4a4":"print(\"The dataset has {} rows and {} columns.\".format(*df.shape))\nprint(\"It contains {} duplicates.\".format(df.duplicated().sum()))","8537655c":"df.CPM.describe()","dcbf6cfa":"df.nunique()","38ce1e6f":"categorial_features = ['site_id', 'ad_type_id', 'geo_id', 'device_category_id', 'advertiser_id', 'os_id',\n                       'integration_type_id', 'monetization_channel_id', 'ad_unit_id']","bbcf09ee":"for col in categorial_features:\n    df[col] = df[col].astype('category')","34f8dedd":"num_feats = df.select_dtypes(include=['float64', 'int64', 'bool', 'object']).copy()\n\n# one-hot encoding of categorical features\ncat_feats = df.select_dtypes(include=['category']).copy()\ncat_feats = pd.get_dummies(cat_feats)\ncat_feats = cat_feats[cat_feats.columns[(cat_feats.sum() > 5000).values]]","7655864f":"features_recoded = pd.concat([num_feats, cat_feats], axis=1)","4ed7248a":"features_recoded.head()","b8cd97fb":"print(\"The dataset has {} rows and {} columns.\".format(*features_recoded.shape))","fb06b769":"features_recoded['date'] = pd.to_datetime(df['date'])","31b40433":"test_df = features_recoded[features_recoded.date >= pd.to_datetime('2019-06-22')]","7de4fa7c":"test_df = test_df[test_df['CPM'] >= 0]\ntest_df = test_df[test_df['CPM'] < test_df['CPM'].quantile(0.95)]","7ff25d0b":"test_X = test_df.drop(['date', 'CPM'], axis = 1)\ntest_y = test_df.CPM","2ca48e07":"train_df = features_recoded[features_recoded.date < pd.to_datetime('2019-06-22')]","0341a4c5":"train_df = train_df[train_df['CPM'] >= 0]\ntrain_df = train_df[train_df['CPM'] < train_df['CPM'].quantile(0.95)]","3872a29d":"train_df_features = train_df.drop(['date', 'CPM'], axis = 1)\ntrain_target = train_df.CPM","b6a8005f":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n# import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# split our data\ntrain_X_train, train_X_test, train_y_train, train_y_test = train_test_split(train_df_features, train_target, test_size = 0.3)","559fa6f0":"# scale data\nsc = StandardScaler()\nX_train = sc.fit_transform(train_X_train)\nX_test  = sc.transform(train_X_test)","6724e757":"# fix random seed for reproducibility\nseed = 15\nnp.random.seed(seed)\n\nprint(tf.__version__)\n\nlinear_model = tf.keras.Sequential([\n    layers.Dense(units=700, activation='relu', kernel_regularizer=regularizers.l2(5)),\n    layers.Dropout(0.5),\n    layers.Dense(units=150, activation='relu', kernel_regularizer=regularizers.l2(1)),\n    layers.Dropout(0.5),\n    layers.Dense(units=1, activation='linear')\n])\n\nkeras.backend.set_epsilon(0.01)\nlinear_model.compile(\n    optimizer=tf.optimizers.Adam(),\n    loss='mean_squared_error')\n\nhistory = linear_model.fit(\n    X_train, train_y_train,\n    epochs=150,\n    # suppress logging\n    verbose=2,\n    # Calculate validation results on 20% of the training data\n    use_multiprocessing=True,\n    validation_split=0.2\n    ,batch_size=2048\n)\n\ny_hat = linear_model.predict(X_test).flatten()\nprint(\"Score on cross-validation is \", mean_squared_error(y_hat, train_y_test))","9a66cad0":"def plot_loss(model_history):\n    train_loss=[value for key, value in model_history.items() if 'loss' in key.lower()][0]\n    valid_loss=[value for key, value in model_history.items() if 'loss' in key.lower()][1]\n    fig, ax1 = plt.subplots()\n    color = 'tab:blue'\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss', color=color)\n    ax1.plot(train_loss, '--', color=color, label='Train Loss')\n    ax1.plot(valid_loss, color=color, label='Valid Loss')\n    ax1.tick_params(axis='y', labelcolor=color)\n    plt.legend(loc='upper left')\n    plt.title('Model Loss')\n    plt.show()\n    \nplot_loss(history.history)","0ef5791f":"print(\"Train set score is \", mean_squared_error(y_hat, train_y_test))","a9ffab5f":"sc = StandardScaler()\nX_train = sc.fit_transform(train_df_features)\nX_test  = sc.transform(test_X)","f8fc2341":"seed = 15\nnp.random.seed(seed)\n\nprint(tf.__version__)\n\nlinear_model = tf.keras.Sequential([\n    layers.Dense(units=700, activation='relu', kernel_regularizer=regularizers.l2(5)),\n    layers.Dropout(0.5),\n    layers.Dense(units=150, activation='relu', kernel_regularizer=regularizers.l2(1)),\n    layers.Dropout(0.5),\n    layers.Dense(units=1, activation='linear')\n])\n\nkeras.backend.set_epsilon(0.01)\nlinear_model.compile(\n    optimizer=tf.optimizers.Adam(),\n    loss='mean_squared_error')\n\n\nhistory = linear_model.fit(\n    X_train, train_target, \n    epochs=150,\n    # suppress logging\n    verbose=2,\n    # Calculate validation results on 20% of the training data\n    use_multiprocessing=True,\n    validation_split=0.2\n    ,batch_size=2048\n)","611dd1f1":"y_hat = linear_model.predict(X_test).flatten()\nprint(\"Test Set MSE is \", mean_squared_error(y_hat, test_y))","b4b50a7f":"Date cut-off:","1e6cb779":"Clean train set","fa0f5075":"Apply a 95%-condition and a non-negative value condition on a test set","f30882f7":"## Process categorial features","ea395a5b":"## Now get train data set","baeb8478":"# CV","e09ccda6":"## Set aside test_X and test_y dataframes for later use (to get final results after CV)","aca7c730":"## Get predictions and compute MSE on deferred test","924377a4":"## Split into train and test datasets (test set will be used in the end)"}}