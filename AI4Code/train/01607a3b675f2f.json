{"cell_type":{"c2a0ef2f":"code","9fe60b23":"code","f554ff0f":"code","20cfacaa":"code","00be4463":"code","9c0a652a":"code","e0adcdc6":"code","b4261a59":"code","f48ae4c8":"code","4c9342fb":"code","a6721bef":"code","577eb5b2":"code","870faad3":"code","ff6e0f26":"code","a4655483":"code","69028cdb":"code","b671afed":"code","8015abc8":"code","945d82ce":"code","a638b38f":"code","e11f7509":"code","211ffadb":"code","e8be6c61":"markdown","1ad70dea":"markdown","ff09ce78":"markdown","2234afb8":"markdown","42371089":"markdown","e44f5025":"markdown","b801571d":"markdown","83134604":"markdown","655c979a":"markdown"},"source":{"c2a0ef2f":"import logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os \nimport pandas as pd\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Layer\n\n%matplotlib inline\n\nlogging.disable(logging.WARNING)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"","9fe60b23":"img_height = 128\nimg_width = 128\nbatch_size = 16","f554ff0f":"img_path = '\/kaggle\/input\/100-bird-species\/train\/ALBATROSS\/001.jpg'\nimg = PIL.Image.open(img_path)\nimg","20cfacaa":"print (\"image size: \", img.size)","00be4463":"data_dir = '\/kaggle\/input\/100-bird-species\/'\n\ndata_dir_train = os.path.join(data_dir, 'train')\ndata_dir_valid = os.path.join(data_dir, 'valid')\ndata_dir_test = os.path.join(data_dir, 'test')","9c0a652a":"train_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir_train,\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)\n\nvalid_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir_valid,\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir_test,\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)","e0adcdc6":"labels = []\nfor x, y in train_ds:\n    labels += y.numpy().tolist() ","b4261a59":"print (\"number of classes: \", len(set(labels)))","f48ae4c8":"plt.hist(labels, bins=310)\nplt.show()","4c9342fb":"def normalize(img, label):\n    return img \/ 255.0, label","a6721bef":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.2),\n    tf.keras.layers.RandomZoom(0.2)\n])","577eb5b2":"train_dataset = (train_ds\n                 .map(normalize)\n                 .map(lambda x, y: (data_augmentation(x), y))\n                 .prefetch(tf.data.AUTOTUNE))\n\nvalid_dataset = valid_ds.map(normalize)\ntest_dataset  = test_ds.map(normalize)","870faad3":"def get_pretrained_mobilenet():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(128, 128, 3)),\n\n        tf.keras.applications.mobilenet.MobileNet(input_shape=(128, 128, 3), weights='imagenet'),\n\n        tf.keras.layers.Dense(768, activation='relu'),\n        tf.keras.layers.Dense(768, activation='relu'),\n        tf.keras.layers.Dense(315)\n    ])\n    model.get_layer(name='mobilenet_1.00_128').trainable = False\n    \n    return model","ff6e0f26":"class DepthwiseSeparableConvolution(Layer):\n\n    def __init__(self, filter, kernel=3):\n        super(DepthwiseSeparableConvolution,self).__init__()\n        self.filter=filter\n        self.kernel=kernel\n    \n    def build(self, input_shape):\n        # Depthwise\n        self.depthwise = tf.keras.layers.DepthwiseConv2D(self.kernel)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.relu1 = tf.keras.layers.ReLU()\n        # Pointwise\n        self.pointwise = tf.keras.layers.Conv2D(self.filter, 1)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.relu2 = tf.keras.layers.ReLU()\n        \n    def call(self, inputs):\n        x = self.depthwise(inputs)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.pointwise(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        \n        return x","a4655483":"def get_simple_mobilenet():\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(128, 128, 3)),\n\n        # [!] Actual MobileNet has more layers and does not use MaxPool\n        DepthwiseSeparableConvolution(128, 3),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        DepthwiseSeparableConvolution(256, 3),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        DepthwiseSeparableConvolution(512, 5),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        DepthwiseSeparableConvolution(1024, 5),\n        \n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Flatten(),\n\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dense(100, activation='relu'),\n        tf.keras.layers.Dense(315)\n    ])\n    \n    return model","69028cdb":"# Just for comparison\ndef get_standard_conv():\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(128, 128, 3)),\n\n        # [!] Actual MobileNet has more layers and does not use MaxPool\n        tf.keras.layers.Conv2D(128, 3),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        tf.keras.layers.Conv2D(256, 3),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        tf.keras.layers.Conv2D(512, 5),\n        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n        tf.keras.layers.Conv2D(1024, 5),\n\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Flatten(),\n\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dense(100, activation='relu'),\n        tf.keras.layers.Dense(315)\n    ])\n    \n    return model","b671afed":"simple_mobilenet = get_simple_mobilenet()\nsimple_mobilenet.summary()","8015abc8":"standard_conv = get_standard_conv()\nstandard_conv.summary()","945d82ce":"model = get_simple_mobilenet()\nmodel.summary()","a638b38f":"model.compile(\n    optimizer='adam', \n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n    metrics=['accuracy'])","e11f7509":"checkpoint_path = \"\/checkpoints\/simple_mobilenet\"\n\nmodel_history = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=5),\n        tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n    ])","211ffadb":"model.load_weights(checkpoint_path)\nmodel.evaluate(test_dataset)","e8be6c61":"We can use the existing MobileNet architecture from Tensorflow. We can use the pretrained ones or train it again from scratch.","1ad70dea":"# Training","ff09ce78":"#### Difference in Parameters\nAs we can see above, the parameters of using standard convolution layer is huge compared to using Depth-wise Separable Convolution.","2234afb8":"### Evaluation\nLet's evaluate our model, which has the least validation loss.","42371089":"# Data Pipeline","e44f5025":"# Modeling","b801571d":"#### Dataset characteristics:\n- The datset contains **310 classes** of different kind of birds.\n- Each class holds around 120 images.\n- We have around 45K images to train the model","83134604":"However, let's try build a simplifed version of MobileNet instead. We will use the **Depth-wise Separable Convolution**, which is the core feature of MobileNet.\n\nEssentially, what Depth-wise separable convolution does is it simplifies the operations of standard convolution layers by having a depth-wise layer to do the multiplication and pointwise layer of size 1x1xN to do the addtion. As a result, it reduces the number of parameters drastically and can reach similar performance to the standard one.\n\n**Note**: Below is not the actual MobileNet architecture.","655c979a":"# Loading Data"}}