{"cell_type":{"41fe57a7":"code","faa29e84":"code","196403db":"code","f9e98b38":"code","33237f15":"code","325c0348":"code","fe5c9279":"code","97966ebf":"code","d4fce7a1":"code","3d1398bf":"code","06a480be":"code","ad6509e8":"code","77d2ff16":"code","6dd17f76":"code","63e9860d":"code","86be3fe5":"code","8987ad16":"code","628f55ce":"code","47e1f0dc":"code","ca98911e":"code","bd2fc30b":"code","cf97a703":"code","74117688":"code","15bf711a":"code","89197ce8":"code","b4adb347":"code","8e3ee972":"code","e4b96cde":"code","282beadf":"code","8c16fbad":"code","0b9637bd":"code","aecd5893":"code","e77f9086":"code","ee1fd2e4":"code","cecfee74":"code","c857fb2e":"code","d04de3f4":"code","5ae54d7a":"code","46568bbc":"code","f4b139ed":"code","4089f3f7":"code","01498515":"code","5e03132e":"code","81971223":"code","a12b5148":"code","16b66a4b":"code","20306450":"code","3e8f08fc":"code","a1e76a21":"code","665f4c19":"code","b95f1c0c":"code","d5a350e9":"code","1af8d459":"code","ac6b0ed7":"code","8730c89c":"code","c68b9aae":"code","1ba00a6d":"code","f0d64576":"code","93b79570":"markdown","21dcf367":"markdown","deff76d8":"markdown","9c69d79d":"markdown","2f37a5a8":"markdown","e44ec8fd":"markdown","6dcc104b":"markdown","797e8725":"markdown","8c690ca0":"markdown","e1670272":"markdown","ccd4fba0":"markdown","42c13669":"markdown","67bd1248":"markdown","2e2ef5a0":"markdown","0db7d92b":"markdown"},"source":{"41fe57a7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","faa29e84":"costa = pd.read_csv('..\/input\/train.csv')","196403db":"costa.head()","f9e98b38":"costa['v2a1'].isnull().sum()","33237f15":"col_list = []\nfor feature in costa.columns: # Loop through all columns in the dataframe\n    if costa[feature].isnull().sum() > 0: # Only apply for columns with categorical strings\n        col_list.append(feature)","325c0348":"col_list","fe5c9279":"X = costa.drop(col_list, axis =1)","97966ebf":"sns.boxplot(x=\"Target\", y=\"escolari\",data=costa, palette=\"coolwarm\")","d4fce7a1":"X.groupby('hogar_mayor')['Target'].value_counts()","3d1398bf":"sns.countplot(x='cielorazo',data=costa, hue ='Target')","06a480be":"columns_list = ['rooms','r4h3', 'r4m3', 'r4t3', 'escolari','paredblolad', 'cielorazo', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2' , 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'hogar_adul', 'hogar_mayor', 'Target']","ad6509e8":"new = X[X['parentesco1']==1]","77d2ff16":"final = new[columns_list]","6dd17f76":"features = final[[i for i in list(final.columns) if i != 'Target']]","63e9860d":"features.info()","86be3fe5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,final['Target'],\n                                                    test_size=0.30)","8987ad16":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","628f55ce":"rfc_pred = rfc.predict(X_test)","47e1f0dc":"from sklearn.metrics import classification_report,confusion_matrix","ca98911e":"print(confusion_matrix(y_test,rfc_pred))","bd2fc30b":"print(classification_report(y_test,rfc_pred))","cf97a703":"costa_test = pd.read_csv('..\/input\/test.csv')","74117688":"col1_list = []\nfor feature in costa_test.columns: # Loop through all columns in the dataframe\n    if costa_test[feature].isnull().sum() > 0: # Only apply for columns with categorical strings\n        col1_list.append(feature)","15bf711a":"col1_list","89197ce8":"X1 = costa_test.drop(col1_list, axis =1)","b4adb347":"newtest = X1[X1['parentesco1']==1]","8e3ee972":"newtest.info()","e4b96cde":"columns_list1 = ['Id', 'rooms','r4h3', 'r4m3', 'r4t3', 'escolari','paredblolad', 'cielorazo', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2' , 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'hogar_adul', 'hogar_mayor', 'idhogar']","282beadf":"final1 = newtest[columns_list1]","8c16fbad":"final2 = final1.drop('Id', axis =1)","0b9637bd":"final3 = final2.drop('idhogar', axis=1)","aecd5893":"final1.reset_index(inplace = True)","e77f9086":"final1.head()","ee1fd2e4":"rfc_pred1 = rfc.predict(final3)","cecfee74":"len(rfc_pred1)","c857fb2e":"my_submission = pd.DataFrame({'Target': rfc_pred1})","d04de3f4":"final_submit = final1.join(my_submission)","5ae54d7a":"final_submit.head()","46568bbc":"my_submit1 = final_submit[['Target', 'idhogar']]","f4b139ed":"submission_base = X1[['Id', 'idhogar']].copy()","4089f3f7":"sample_submission = submission_base.merge(my_submit1, \n                                       on = 'idhogar',\n                                       how = 'left').drop(columns = ['idhogar'])","01498515":"median = sample_submission['Target'].median()","5e03132e":"sample_submission['Target'].fillna(median, inplace= True)\nsample_submission['Target'] = sample_submission['Target'].astype(int)","81971223":"sample_submission.info()","a12b5148":"#sample_submission.to_csv('sample_submission.csv', index = False)","16b66a4b":"from xgboost.sklearn import XGBClassifier ","20306450":"xclas = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=1500,\n       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)","3e8f08fc":"xclas.fit(X_train, y_train)  ","a1e76a21":"xgb_pred1 = xclas.predict(final3)","665f4c19":"len(xgb_pred1)","b95f1c0c":"my_submission_xgb = pd.DataFrame({'Target': xgb_pred1})","d5a350e9":"final_submit_xgb = final1.join(my_submission_xgb)","1af8d459":"my_submit_xgb = final_submit_xgb[['Target', 'idhogar']]","ac6b0ed7":"submission_base_xgb = X1[['Id', 'idhogar']].copy()","8730c89c":"submission_xgb = submission_base_xgb.merge(my_submit_xgb, \n                                       on = 'idhogar',\n                                       how = 'left').drop(columns = ['idhogar'])","c68b9aae":"median = submission_xgb['Target'].median()","1ba00a6d":"submission_xgb['Target'].fillna(median, inplace= True)\nsubmission_xgb['Target'] = submission_xgb['Target'].astype(int)","f0d64576":"submission_xgb.to_csv('submission_xgb.csv', index = False)","93b79570":"# Great - so we improved from .328 to .338 just by using a different algorithm. Now lets play around with the hyper-parameters in XGB","21dcf367":"* In the below groupby table, we can see that 'hogar_mayor' has different distribution with the target variable\nand can be considered as 1st cut variable\n\n* Also - Select variables on common-sense basis for identifying poverty levels by household\n- Aggregate at household level (Since many variables are at a household level) and make prediction for each head of household - variables to aggregate can be number of individuals\/by age in the household, total education etc","deff76d8":"# Another plot (Countplot) to help us visualize the variable's possible link\/impact on the end \"Target\"","9c69d79d":"# Lets run our 1st basic algo and see !","2f37a5a8":"# We got a poor result of 0.318 on RF with the current set of variables.\nLets try out another result with XGBoost algorithm","e44ec8fd":"* First cut analysis, we've dropped columns which have significantly high Null values","6dcc104b":"# 1st cut variables chosen, to check output\n- Using boxplot to see variation of feature with target variable\n- countplot\n- groupby & value counts for distribution across segments\n\nNeed to be aggregated at household level against the head of the household\n- rooms - >5rooms seem to fit target 4 - Already aggregated output\n- r4h3 - Total males in the household - Already aggregated output\n- r4m3 - Total females in the household - Already aggregated output\n- r4t3 - Total persons in the household - Already aggregated output\n- escolari - years of schooling (total for a household, or just use the squared metric later) = check for total household\n    - have 2 variables for a household\n        - total yrs of schooling for everyone in a household\n        - yrs of schooling of head of household\n- paredblolad - if predominant material on the outside wall is block or brick - ALREADY aggregated\n- cielorazo =1 if the house has ceiling - ALREADY aggregated\n- epared1, =1 if walls are bad - ALREADY aggregated\n- epared2, =1 if walls are regular - ALREADY aggregated\n- epared3, =1 if walls are good - ALREADY aggregated\n- etecho1, =1 if roof are bad - ALREADY aggregated\n- etecho2, =1 if roof are regular - ALREADY aggregated\n- etecho3, =1 if roof are good - ALREADY aggregated\n- eviv1, =1 if floor are bad - ALREADY aggregated\n- eviv2, =1 if floor are regular - ALREADY aggregated\n- eviv3, =1 if floor are good - ALREADY aggregated\n- hogar_adul - no. of adults in a household - ALREADY aggregated\n- hogar_mayor - # of individuals 65+ in the household - ALREADY aggregated\n    - SQBescolari - NEED NOT be considered as above we've considered escolari already","797e8725":"# Check model performance on the TEST dataset","8c690ca0":"* Read the \"train\" file after loading the required libraries","e1670272":"# Lets keep tuning","ccd4fba0":"# Visually going through EACH of the variables mentioned","42c13669":"# Making n_trees = 1000, has further improved the result to .357!","67bd1248":"* We can see in the below box plot how the 'escolari' variable varies for different target variables\nand we see it seems to vary with each category. So can be considered in the 1st cut analysis","2e2ef5a0":"# Now that we have out 1st cut variables \n- Make a NEW dataframe with the above variables\n    - addl columns for schooling yrs to be created (In fact this also can be postponed and we work on just the schooling year of the head of the houshold)\n    - Select only rows for the head of household =1 (parentesco1, =1 if household head)","0db7d92b":"* Checking one column for missing values. Now we need to check the extent of the missing values\n- 1st cut, looks like we can drop off the columns which have many missing values\n- Check for outliers\n- check for feature importance using selecKbest & Random forest\n- Feature engg.\/Feature selection is going to be very important as we need to signfificantly reduce the feature space"}}