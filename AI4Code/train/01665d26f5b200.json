{"cell_type":{"0b1095ab":"code","d90b430d":"code","b93ba41e":"code","6f7a18df":"code","58c503ae":"code","a00cc451":"code","ef741c93":"code","7322b5be":"code","e53fd9ab":"code","2a32a5de":"code","60aae73b":"code","c3faffea":"code","e836942d":"code","d40d10aa":"code","dfbd26b3":"code","7fac6b82":"code","30f32b2d":"code","bc45d79b":"code","80ad766a":"code","a305e78f":"code","a818a1d9":"code","5d1dc3f4":"code","1ad566eb":"code","c04c2dac":"code","d27764d4":"code","a7565fbe":"code","d7ec5d78":"code","289bed5d":"code","5c2deed9":"code","9b012d15":"code","f21fc9c6":"code","ebb900c5":"code","765f692d":"code","cd0240e0":"code","31a7dc79":"code","28405749":"code","1aa0edd0":"code","071ba033":"code","82f684c0":"code","a856d73c":"code","e0d973ac":"code","e005c0b8":"code","82d98f59":"code","2fe1852e":"code","7a2c11ff":"code","16998a68":"code","8eba245f":"code","889d4cf9":"code","188efabc":"code","ae36b1cb":"code","ba566dde":"code","f6fa209b":"code","0edc4742":"code","28ab9997":"code","af38f339":"code","1344ea86":"markdown","84ffc87e":"markdown","48161c0c":"markdown","bd6b15ec":"markdown","3d2246cf":"markdown","c891e040":"markdown","8452862c":"markdown","84cfd767":"markdown","e15c98d0":"markdown","2e37b5e9":"markdown","15caa63a":"markdown","6e918b42":"markdown","9eda030d":"markdown","fc0d3951":"markdown","13075081":"markdown","9a3e70b1":"markdown","755b2b48":"markdown","b78269ec":"markdown","69298d12":"markdown","d17faabf":"markdown","bbf4554c":"markdown","b0666e97":"markdown","c94c9575":"markdown","0e19d6ed":"markdown","da2d5e61":"markdown","cc9bc992":"markdown","7f68b5d0":"markdown","31b56e8b":"markdown","519fbf00":"markdown","3300b3fc":"markdown","42487391":"markdown","e2890dd4":"markdown","761ebd46":"markdown","c362f26b":"markdown","4896ca81":"markdown","f73952be":"markdown","c842b1d6":"markdown","4e43a725":"markdown","4ff0f5e4":"markdown","abdd273b":"markdown","076bf71a":"markdown"},"source":{"0b1095ab":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d90b430d":"# Reading the csv file and putting it into 'df' object.\ndf = pd.read_csv('..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndf.head()","b93ba41e":"# Importing test_train_split from sklearn library\nfrom sklearn.model_selection import train_test_split","6f7a18df":"# Putting feature variable to X\nX = df.drop('default.payment.next.month',axis=1)\n\n# Putting response variable to y\ny = df['default.payment.next.month']\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","58c503ae":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","a00cc451":"# fit\nrfc.fit(X_train,y_train)","ef741c93":"# Making predictions\npredictions = rfc.predict(X_test)","7322b5be":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","e53fd9ab":"# Let's check the report of our default model\nprint(classification_report(y_test,predictions))","2a32a5de":"# Printing confusion matrix\nprint(confusion_matrix(y_test,predictions))","60aae73b":"print(accuracy_score(y_test,predictions))","c3faffea":"# Renaming Columns into more understandable\/user-friendly terms\n\n# SEX changed to GENDER\n# PAY_0 changed to PAY_1\n# default.payment.next.month is too long and changed to something simplier, DEFAULT\ndf.rename(columns={'SEX':'GENDER',\n                   'PAY_0':'PAY_1',\n                   'default.payment.next.month':'DEFAULT',} , inplace=True)\n\ndf.drop('ID', axis=1, inplace=True) # Drop column ID\n\ndf.info()  # we see that we have 30,000 observations and no null values","e836942d":"# We inspect the data as a whole|\ndf.describe().T","d40d10aa":"sum_column = df[\"PAY_1\"] + df[\"PAY_2\"]+ df[\"PAY_3\"]+ df[\"PAY_4\"]+ df[\"PAY_5\"]+ df[\"PAY_6\"]\ndf['pay_sum'] = sum_column\nbill_sum = df[\"BILL_AMT1\"]+df[\"BILL_AMT2\"]+df[\"BILL_AMT3\"]+df[\"BILL_AMT4\"]+df[\"BILL_AMT5\"]+df[\"BILL_AMT6\"]\ndf[\"bill_sum\"]=bill_sum\npay_amt = df['PAY_1']+df['PAY_2']+df['PAY_3']+df['PAY_4']+df['PAY_5']+df['PAY_6']\ndf[\"pay_amt_sum\"]=pay_amt","dfbd26b3":"print('Education Column Values: ', df['EDUCATION'].unique())","7fac6b82":" fig, ax = plt.subplots()\nsns.countplot(data=df,x='EDUCATION', order = df['EDUCATION'].value_counts().index, color='salmon')","30f32b2d":" df['EDUCATION'].value_counts()","bc45d79b":"# There exists values 0, 5 and 6 in this column.\n# Since these are unknown (undefined), they can be grouped into the category 4: \"Others\"\n\ndf['EDUCATION'] = df['EDUCATION'].apply(lambda edu_value: edu_value \n                                        if ((edu_value > 0 and edu_value < 4)) \n                                        else 4) # Changes every value of x not within (and inclusive of) 1 ~ 3 to 4  \n\n# Corrected changes\ndf['EDUCATION'].unique()","80ad766a":"# Countplt\nfig, ax = plt.subplots()\nsns.countplot(data=df,x='EDUCATION', order = df['EDUCATION'].value_counts().index, color='salmon');","a305e78f":"print(\"Marriage Column Values: \", df['MARRIAGE'].unique())","a818a1d9":"df['MARRIAGE'] = df['MARRIAGE'].apply(lambda marriage_value: marriage_value\n                                     if (marriage_value > 0 and marriage_value < 3)\n                                     else 3) # changes every value of x not within (and inclusive of) 1 and 2 to 3\n\n# Corrected changes\ndf['MARRIAGE'].unique()","5d1dc3f4":"df['AGE'].unique()","1ad566eb":"## Creating a Function to Distribute the Age\ndef func(x):\n    if(x >=20 and x<30 ):\n        return 1\n    elif(x>=30 and x<40):\n        return 2\n    elif(x>=40 and x<50):\n        return 3\n    elif(x>=50 and x<60):\n        return 4\n    elif(x>=60 and x<=80):\n        return 5","c04c2dac":"## Applying the function\ndf['AGE'] = df['AGE'].apply(func)\n","d27764d4":"\nfig, ax = plt.subplots()\nsns.countplot(data=df,x='AGE', order = df['AGE'].value_counts().index, color='salmon');","a7565fbe":"# Creating a new dataframe with just the categorical explanatory variables\ndf_categorical = df[['GENDER', 'EDUCATION', 'MARRIAGE','AGE','PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'\n                     ,'DEFAULT']]","d7ec5d78":"f, axes = plt.subplots(3, 3, figsize=(19,14), facecolor='white')\nf.suptitle(\"FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)\",size=20)\n\n# Creating plots of each categorical variable to target \nax1 = sns.countplot(x='GENDER', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,0])\nax2 = sns.countplot(x='EDUCATION', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,1])\nax3 = sns.countplot(x='MARRIAGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[0,2])\nax4 = sns.countplot(x='PAY_1', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,0])\nax5 = sns.countplot(x='PAY_2', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,1])\nax6 = sns.countplot(x='PAY_3', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[1,2])\nax7 = sns.countplot(x='PAY_4', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,0])\nax8 = sns.countplot(x='PAY_5', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,1])\nax9 = sns.countplot(x='PAY_6', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\nax10 = sns.countplot(x='AGE', hue = 'DEFAULT', data=df_categorical, palette='Reds', ax=axes[2,2])\n# Setting legends to upper right\nax1.legend(loc=\"upper right\")\nax2.legend(loc=\"upper right\")\nax3.legend(loc=\"upper right\")\nax4.legend(loc=\"upper right\")\nax5.legend(loc=\"upper right\")\nax6.legend(loc=\"upper right\")\nax7.legend(loc=\"upper right\")\nax8.legend(loc=\"upper right\")\nax9.legend(loc=\"upper right\")\nax10.legend(loc=\"upper right\")\n# Changing ylabels to horizontal and changing their positions\nax1.set_ylabel('COUNTS', rotation=0, labelpad=40)  # Labelpad adjusts distance of the title from the graph\nax1.yaxis.set_label_coords(-0.1,1.02)              # (x, y)\nax2.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax2.yaxis.set_label_coords(-0.1,1.02)\nax3.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax3.yaxis.set_label_coords(-0.1,1.02)\nax4.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax4.yaxis.set_label_coords(-0.1,1.02)\nax5.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax5.yaxis.set_label_coords(-0.1,1.02)\nax6.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax6.yaxis.set_label_coords(-0.1,1.02)\nax7.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax7.yaxis.set_label_coords(-0.1,1.02)\nax8.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax8.yaxis.set_label_coords(-0.1,1.02)\nax9.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax9.yaxis.set_label_coords(-0.1,1.02)\nax10.set_ylabel('COUNTS', rotation=0, labelpad=40)\nax10.yaxis.set_label_coords(-0.1,1.02)\n\n# Shifting the Super Title higher\nf.tight_layout()  # Prevents graphs from overlapping with each other\nf.subplots_adjust(top=0.9);","289bed5d":"# generate binary values using get_dummies\nage = pd.get_dummies(df['AGE'], prefix='AGE' )\nmr = pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE' )\ned = pd.get_dummies(df['EDUCATION'],prefix='EDUCATION')\n# merge with main df bridge_df on key values\ndf = df.join(age)\ndf = df.join(mr)\ndf= df.join(ed)","5c2deed9":"df = df.drop(['AGE','MARRIAGE','EDUCATION'],axis=1)","9b012d15":"print(df['DEFAULT'].value_counts(),'\\n')\nprint(len(df['DEFAULT']))","f21fc9c6":"# Frequency of the defaults\ndefault = df['DEFAULT'].sum() # adds up all the default cases in the df\nno_default = len(df['DEFAULT']) - default  # entire dataset - default cases\n\n# Percentage of the defaults\ndefault_perc = round(default\/len(df['DEFAULT']) * 100, 1)\nno_default_perc = round(no_default\/len(df['DEFAULT']) * 100, 1)\n\n# Plotting Target\nfig, ax = plt.subplots(figsize=(10,7))  # Sets size of graph\nsns.set_context('notebook', font_scale=1.2)  # Affects things like size of label, lines and other elements of the plot.\n\nsns.countplot('DEFAULT',data=df, palette=\"Reds\")   \nplt.annotate('Non-default: {}'.format(no_default), \n             xy=(-0.25, 3000), # xy = (x dist from 0, y dist from 0)\n            size=15.5)\n\nplt.annotate('Default: {}'.format(default), \n             xy=(0.8, 3000), # xy = (x dist from 0, y dist from 0)\n            size=15)\nplt.annotate('{}%'.format(no_default_perc), xy=(-0.1, 8000),size=15)\nplt.annotate('{}%'.format(default_perc), xy=(0.9, 8000),size=15)\nplt.title('CREDIT CARD COUNT', size=18)\nplt.xlabel(\"Default\",size=15)\nplt.ylabel('Count', rotation=0, \n           labelpad=40, # Adjusts distance of the title from the graph\n           size=15)\nax.yaxis.set_label_coords(-0.1,.9)\n\nplt.box(False)        # Removes the bounding area\nplt.savefig('target_skew.png', transparent = True)","ebb900c5":"# Freq distribution of all data\nfig, ax = plt.subplots(figsize=(15,15))\npd.DataFrame.hist(df,ax=ax)\nplt.tight_layout();","765f692d":"# Can we infer more? what about the columns for lIMIT_BALANCE?\nx1 = list(df[df['DEFAULT'] == 1]['LIMIT_BAL'])\nx2 = list(df[df['DEFAULT'] == 0]['LIMIT_BAL'])\n\nfig2, ax_lim_bal = plt.subplots(figsize=(12,4))\nsns.set_context('notebook', font_scale=1.2)\n#sns.set_color_codes(\"pastel\")\nplt.hist([x1, x2], bins = 40, density=False, color=['firebrick', 'salmon'])\nplt.xlim([0,600000])\nplt.legend(['Yes', 'No'], title = 'Default', loc='upper right', facecolor='white')\nplt.xlabel('Limit Balance (NT dollar)')\nplt.ylabel('Frequency', rotation=0,labelpad=40)\nplt.title('LIMIT BALANCE HISTOGRAM BY TYPE OF CREDIT CARD', SIZE=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","cd0240e0":"# Now that we have our features, let's plot them on a correlation matrix to remove anything that might \n# cause multi-colinearity within our model\n\nsns.set(style=\"white\")\n# Creating the data\ndata = df.corr()\n\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(data, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(60,50))\n\n\n# Create a custom color palette\ncmap = \\\nsns.diverging_palette(133, 10,\n                      as_cmap=True)  \n# as_cmap returns a matplotlib colormap object rather than a list of colors\n# Green = Good (low correlation), Red = Bad (high correlation) between the independent variables\n\n# Plot the heatmap\ng = sns.heatmap(data=data, annot=True, cmap=cmap, ax=ax, \n                mask=mask, # Splits heatmap into a triangle\n                annot_kws={\"size\":20},  #Annotation size\n               cbar_kws={\"shrink\": 0.8} # Color bar size\n               );\n\n\n# Prevent Heatmap Cut-Off Issue\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Changes size of the values on the label\nax.tick_params(labelsize=25) \n\nax.set_yticklabels(g.get_yticklabels(), rotation=0);\nax.set_xticklabels(g.get_xticklabels(), rotation=80);\n\nplt.savefig('correlation_heatmap.png', transparent = True)","31a7dc79":"df_default_corrs = data.filter(like='DEFAULT')","28405749":"df_default_corrs","1aa0edd0":"df_default_corrs.plot(kind='bar',figsize=(15,10))","071ba033":"# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# Data splitting for 80% Train\/Val and 20% Test \nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state=69) # 20% holdout \nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.25, random_state=69) # Train\/Val\n\n# Initializing the scaler  (Just scale every single time lol)\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nstd.fit(X_train_val.values)\n\n## Scale the Predictors on the train\/val dataset\nX_train_val_scaled = std.transform(X_train_val.values) \n\n## This line instantiates the model. \nrf = RandomForestClassifier() \n\n## Fit the model on your training data.\nrf.fit(X_train_val_scaled, y_train_val) \n\n# Obtain the feature importance\nfeature_importance = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                   columns=['Variable_Importance']).sort_values('Variable_Importance',ascending=True)\n\n# Set seaborn contexts \nsns.set(style=\"whitegrid\")\n\nfeature_importance.plot.barh(figsize=(15,10))","82f684c0":"# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\n\n# Classifier Metrics \nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\nfrom sklearn.metrics import fbeta_score, cohen_kappa_score\n\n# Pre-processing packages\nfrom sklearn.preprocessing import StandardScaler\n\n\n# CV, Gridsearch, train_test_split, model selection packages\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n","a856d73c":"## Baseline model performance evaluation\n\n# to give model baseline report with cross-validation in dataframe \ndef baseline_report_cv_(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Splitting the data into 80% training\/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n#     # Splitting the training data into 60% training data and 20% validation data.\n#     X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n     \n    # Creating a shuffled kfold of 5\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    accuracy     = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='accuracy'))\n    precision    = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='precision'))\n    recall       = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='recall'))\n    f1score      = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='f1'))\n    rocauc       = np.mean(cross_val_score(model, X_train_val, y_train_val,cv=cv, scoring='roc_auc'))\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale = df_models.drop('index', axis=1)\ndf_models_no_scale","e0d973ac":"## Scaled Dataset Model performance evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale = df_models.drop('index', axis=1)\ndf_models_scale","e005c0b8":"## SMOTE Dataset model performance evaluation\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE\n\n# Smote with no scaling with cross-validation in dataframe \ndef baseline_report_cv_smote(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, SMOTE's the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    from imblearn.over_sampling import SMOTE # Allows for smoting if you forget to initialize it before running func\n    \n    # Splitting the data into 80% training\/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the SMOTE data\n        X_smoted, y_smoted = SMOTE(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_smoted, y_smoted) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n\n    \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)\ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_smote(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_smote(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_smote(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_smote(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_smote(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_smote(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_cv_smote = df_models.drop('index', axis=1)\ndf_models_no_scale_cv_smote","82d98f59":"## Oversample Dataset model performance evaluation\n\ndef baseline_report_cv_oversampling(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, oversamples the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Allows for oversampling if you forget to initialize it before running func\n    from imblearn.over_sampling import RandomOverSampler\n    \n    # Splitting the data into 80% training\/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the OverSampled data\n        X_resampled, y_resampled = RandomOverSampler(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_resampled, y_resampled) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n   \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_oversampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_oversampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_oversampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_oversampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_oversampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_oversampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_oversampled = df_models.drop('index', axis=1)\ndf_models_no_scale_oversampled","2fe1852e":"## Undersample Dataset model performance evaluation\n\n\ndef baseline_report_cv_undersampling(model, X, y, n_splits, name):\n    \"\"\"\n    Accepts a model object, X (independent variables), y (target), n_splits and name of the model, undersamples the data\n    and returns a model with various scoring metrics of each classifier model on a cross-validation split\n    ----\n    Input: model object, X, y, n_splits (integer), name (str)\n    Output: Various metric scores of a model.\n    \"\"\"\n    # Allows for undersampling if you forget to initialize it before running func\n    from imblearn.under_sampling import RandomUnderSampler\n    \n    # Splitting the data into 80% training\/validation data and 20% testing data\n    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n    # Splitting the training data into 60% training data and 20% validation data.\n    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=69)\n    \n    \n    #this helps with the way kf will generate indices below\n    X_train_val, y_train_val = np.array(X_train_val), np.array(y_train_val)\n    \n    \n    # Creating a shuffled kfold of 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1000) \n    \n    clf_model_acc_scores_cv = []\n    clf_model_precision_scores_cv = []\n    clf_model_recall_scores_cv = []\n    clf_model_f1_scores_cv = []\n    clf_model_rocauc_scores_cv = []\n    \n    # Manual Cross-Validation\n    for train_ind, val_ind in kf.split(X_train_val, y_train_val):\n\n        # Assigning train and validation values for an individual fold\n        X_train, y_train = X_train_val[train_ind], y_train_val[train_ind]\n        X_val, y_val = X_train_val[val_ind], y_train_val[val_ind] \n\n        # Creating the UnderSampled data\n        X_resampled, y_resampled = RandomUnderSampler(random_state=69).fit_sample(X_train, y_train)\n        \n        # Initializing model\n        clf_model = model.fit(X_resampled, y_resampled) # Train model on SMOTE'd data\n        y_pred = clf_model.predict(X_val)  # Y pred after testing on validation data split\n        \n        # Save scores of model\n        clf_model_acc_score = accuracy_score(y_val, y_pred)\n        clf_model_precision_score = precision_score(y_val, y_pred)\n        clf_model_recall_score = recall_score(y_val, y_pred)\n        clf_model_f1_score = f1_score(y_val, y_pred)   \n        clf_model_rocauc_score = roc_auc_score(y_val, y_pred)\n        \n        # Append scores of model their scoring lists\n        clf_model_acc_scores_cv.append(clf_model_acc_score)\n        clf_model_precision_scores_cv.append(clf_model_precision_score)\n        clf_model_recall_scores_cv.append(clf_model_recall_score)\n        clf_model_f1_scores_cv.append(clf_model_f1_score)\n        clf_model_rocauc_scores_cv.append(clf_model_rocauc_score)\n        \n\n    \n    accuracy     = np.mean(clf_model_acc_scores_cv)\n    precision    = np.mean(clf_model_precision_scores_cv)\n    recall       = np.mean(clf_model_recall_scores_cv)\n    f1score      = np.mean(clf_model_f1_scores_cv)\n    rocauc       = np.mean(clf_model_rocauc_scores_cv)\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'rocauc'       : [rocauc],\n                             'timetaken'    : [0]       })   # timetaken for comparison later\n    return df_model\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_undersampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_undersampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_undersampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_undersampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_undersampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_undersampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_no_scale_undersample = df_models.drop('index', axis=1)\ndf_models_no_scale_undersample","7a2c11ff":"## SMOTE Datset with Scaling for Model Performance Evaluation\n\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_smote(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_smote(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_smote(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_smote(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_smote(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_smote(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_cv_smote = df_models.drop('index', axis=1)\ndf_models_scale_cv_smote","16998a68":"## Oversampling & Scaled Dataset on Model Performance Evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_oversampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_oversampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_oversampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_oversampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_oversampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_oversampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_oversampled = df_models.drop('index', axis=1)\ndf_models_scale_oversampled","8eba245f":"## Undersampling & Scaled Dataset for model performance evaluation\n\n# to evaluate baseline models\ngnb = GaussianNB()\nlogit = LogisticRegression()\nknn = KNeighborsClassifier()\ndecisiontree = DecisionTreeClassifier()\nrandomforest = RandomForestClassifier()\nlinearsvc = LinearSVC()\n\n# Scaling the inputs into model\n# Separate data into X and Y components\nX = df.drop('DEFAULT',axis=1)  \ny = df['DEFAULT']\n\n# Creating n_splits for the function since it already has kfold creation in them\nn_splits = 5\n\n## Scale data (just scale everything lol)\nstd = StandardScaler()\nstd.fit(X.values)\n\n## Scale the Predictors\nX = std.transform(X.values)\n\n# to concat all models\ndf_models = pd.concat([baseline_report_cv_undersampling(gnb, X, y, 5, 'GaussianNB'),\n                       baseline_report_cv_undersampling(logit, X, y, 5, 'LogisticRegression'),\n                       baseline_report_cv_undersampling(knn, X, y, 5, 'KNN'),\n                       baseline_report_cv_undersampling(decisiontree, X, y, 5, 'DecisionTree'),\n                       baseline_report_cv_undersampling(randomforest, X, y, 5, 'RandomForest'),\n                       baseline_report_cv_undersampling(linearsvc, X, y, 5, 'LinearSVC')\n                       ], axis=0).reset_index()\n\ndf_models_scale_undersample = df_models.drop('index', axis=1)\ndf_models_scale_undersample","889d4cf9":"# All the scores of the models\ndf_models_no_scale","188efabc":"df_models_scale","ae36b1cb":"df_models_no_scale_cv_smote","ba566dde":"df_models_no_scale_oversampled","f6fa209b":"df_models_no_scale_undersample","0edc4742":"df_models_scale_cv_smote","28ab9997":"\ndf_models_scale_oversampled","af38f339":"df_models_scale_undersample","1344ea86":"## Making Predictions","84ffc87e":"### Best Model for harmonic mean i.e f1score with recall of 0.74","48161c0c":"### Pretty Same \n","bd6b15ec":"PAY_1,PAY_SUM,PAY_AMT_SUM and PAY_2 highly correlate with the Defaulters\n","3d2246cf":"## Renaming the Columns of the Dataset","c891e040":"## Reading the Dataset","8452862c":"## Creating New Features","84cfd767":"## Building Base Line Model (No Scaling)","e15c98d0":"### Pretty bad model!","2e37b5e9":"## Let's Check all the Models and Select the Best!","15caa63a":"## Building Base Line Model(Scaled)","6e918b42":"## Printing Accuracy","9eda030d":"## Importing Required Libraries","fc0d3951":"As we can infer we get a recall of 0.36 , goal is to maximize the recall score in order to identify maximum defaulters. ","13075081":"#### Recall of 0.934 f1 of 0.381","9a3e70b1":"## Importing Libraries","755b2b48":"## Oversampled(Scaled)","b78269ec":"#### GaussianNB with recall of 0.93 !!!","69298d12":"## SMOTE ","d17faabf":"## Oversampling the Data","bbf4554c":"# Feature Engineering","b0666e97":"## Recall of 0.74 with F1 score of 0.44 !!! Much better model","c94c9575":"### Recall is same 0.93 with further improvement in f1","0e19d6ed":"## Building A Random Forest Model","da2d5e61":"### Good Model with Recall of 0.9323","cc9bc992":"#### Recall of 0.78 with improved f1score of0.43!!\n","7f68b5d0":"## Creating Dependent and Independent Variables","31b56e8b":"#### We can infer it as Highly Imbalanced Dataset","519fbf00":"## Poor Perfomance as compared to scale","3300b3fc":"### Recall is good !!","42487391":"#### Recall of 0.93 with f1 0.38","e2890dd4":"## SMOTE(SCALED)","761ebd46":"### Pretty optimized model","c362f26b":"## Importing Confusion Matrix","4896ca81":"# Predicting Credit Card Default ","f73952be":"#### GuassianNB with Recall of 0.86","c842b1d6":"#### Recall of 0.82 with improved f1 of 0.41","4e43a725":"## Importing Train Test Split","4ff0f5e4":"### It is the Best model with Recall of 0.82 and F1 score of 0.41!!!","abdd273b":"## Undersampling the data","076bf71a":"#### 22 % of Defaulters "}}