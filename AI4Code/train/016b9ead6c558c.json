{"cell_type":{"3f2fae64":"code","997c6ecf":"code","5fdfdfb0":"code","001ad119":"code","7d74a42c":"code","19a37bd0":"code","a2193fbf":"code","e1a823c7":"code","8bbaaaf7":"code","8d53a627":"code","49f29fb2":"code","2b4bbd05":"code","dae7d611":"code","a2099697":"code","c0aeb8a1":"code","b2b3925a":"code","226ad09d":"code","c81261d0":"code","5a30a005":"code","29108be2":"code","aae0e189":"code","b232b0e7":"code","dbfe5e3e":"code","a246dbfc":"code","d90503bf":"code","584129f7":"code","90a0c3ee":"code","a7544b90":"code","7d7e3633":"code","0fec3ac4":"code","1747d43c":"code","82671ba6":"code","0b5137b5":"code","b3332e48":"code","3eeea642":"code","8f488bb7":"code","6651a28f":"code","776bc181":"code","be641a74":"code","eaa13f73":"code","d1e99f91":"code","238a449f":"code","92028156":"code","bf644072":"code","1272d99d":"markdown","88ad6da3":"markdown","f101bcb8":"markdown","0f3d4446":"markdown","fcb37fb0":"markdown","a9e7c551":"markdown","73632ce9":"markdown","17a14b4c":"markdown","46050a98":"markdown","5dc42837":"markdown","0609a049":"markdown","bb4572f0":"markdown","ba040e71":"markdown","a4e05f66":"markdown","dc5b3429":"markdown","0e7b91d0":"markdown","3dbc8c66":"markdown","6e1f8d53":"markdown","e88345b1":"markdown","1185d00f":"markdown","c506df03":"markdown","132636d7":"markdown","e4e48c39":"markdown","38185875":"markdown","918a011c":"markdown","218f92ad":"markdown","9c45b236":"markdown","49baf463":"markdown","5a1fbb93":"markdown","ec0972f3":"markdown","52523ef3":"markdown","3b82cce5":"markdown","14c54713":"markdown","5b23389a":"markdown","08767326":"markdown","44883a4e":"markdown","53302767":"markdown"},"source":{"3f2fae64":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import uniform\nfrom scipy.stats import loguniform as sp_loguniform\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier","997c6ecf":"df_treino = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values = '?')\ndf_treino.set_index('Id',inplace=True)\n\ndf_teste = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", na_values = '?')\ndf_teste.set_index('Id',inplace=True)","5fdfdfb0":"print('Forma do DataFrame:', df_treino.shape)\ndf_treino.info()\n\ndf_treino.rename(columns={'marital.status': 'marital_status', 'native.country': 'native_country'}, inplace=True)\ndf_teste.rename(columns={'marital.status': 'marital_status', 'native.country': 'native_country'}, inplace=True)\n\ndf_treino.head()\n","001ad119":"df_treino.isna().sum()","7d74a42c":"df_treino['income'] = LabelEncoder().fit_transform(df_treino['income'])","19a37bd0":"plt.figure(figsize=(10,8))\nsns.heatmap(df_treino.corr(), vmin=-1, vmax=1, annot=True)\nplt.show()","a2193fbf":"df_treino['age'].hist()\nplt.xlabel(\"Idade\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Distribui\u00e7\u00e3o da idade\")\n","e1a823c7":"df_treino[df_treino['income'] == 0]['age'].hist(label=\"<=50K\")\ndf_treino[df_treino['income'] == 1]['age'].hist(label=\">50K\")\nplt.xlabel(\"Idade\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Discrimina\u00e7\u00e3o da idade por faixa de ganhos\")\n","8bbaaaf7":"df_treino['workclass'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Workclass\")","8d53a627":"df_treino['workclass'] = df_treino['workclass'].fillna(df_treino.mode().iloc[0].loc['workclass'])\ndf_teste['workclass'] = df_teste['workclass'].fillna(df_teste.mode().iloc[0].loc['workclass'])\n\ndf_treino.isna().sum()","49f29fb2":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=df_treino)","2b4bbd05":"df_treino = df_treino.drop('fnlwgt', axis=1)\ndf_teste = df_teste.drop('fnlwgt', axis=1)","dae7d611":"sns.catplot(y=\"education\", x=\"education.num\", kind=\"bar\", data=df_treino)\n","a2099697":"df_treino = df_treino.drop('education', axis=1)\ndf_teste = df_teste.drop('education', axis=1)","c0aeb8a1":"df_treino['education.num'].hist()\nplt.xlabel(\"Anos de Escola\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Distribui\u00e7\u00e3o da Escolaridade\")\n","b2b3925a":"df_treino[df_treino['income'] == 0]['education.num'].hist(label=\"<=50K\")\ndf_treino[df_treino['income'] == 1]['education.num'].hist(label=\">50K\")\nplt.xlabel(\"Escolaridade\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Discrimina\u00e7\u00e3o da escolaridade por faixa de ganhos\")","226ad09d":"df_treino['marital_status'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Marital Status\")","c81261d0":"sns.catplot(y=\"marital_status\", x=\"income\", kind=\"bar\", data=df_treino)","5a30a005":"df_treino['occupation'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Occupation\")","29108be2":"df_treino['occupation'] = df_treino['occupation'].fillna(\"Unknown\")\ndf_treino.isna().sum()\n\ndf_teste['occupation'] = df_teste['occupation'].fillna(\"Unknown\")\n","aae0e189":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", data=df_treino)","b232b0e7":"df_treino['relationship'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Relationship\")","dbfe5e3e":"sns.catplot(y=\"relationship\", x=\"income\", kind=\"bar\", data=df_treino)","a246dbfc":"df_treino['race'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Race\")","d90503bf":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=df_treino)","584129f7":"df_treino['sex'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Sex\")","90a0c3ee":"sns.catplot(y=\"sex\", x=\"income\", kind=\"bar\", data=df_treino)","a7544b90":"df_treino['capital.gain'].hist()\nplt.xlabel(\"Capital Gain\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Distribui\u00e7\u00e3o de Capital Gain\")\n","7d7e3633":"df_treino[df_treino['income'] == 0]['capital.gain'].hist(label=\"<=50K\")\ndf_treino[df_treino['income'] == 1]['capital.gain'].hist(label=\">50K\")\nplt.xlabel(\"Capital Gain\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Discrimina\u00e7\u00e3o de Capital Gain por faixa de ganhos\")","0fec3ac4":"df_treino['capital.loss'].hist()\nplt.xlabel(\"Capital Loss\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Distribui\u00e7\u00e3o de Capital Loss\")","1747d43c":"df_treino[df_treino['income'] == 0]['capital.loss'].hist(label=\"<=50K\")\ndf_treino[df_treino['income'] == 1]['capital.loss'].hist(label=\">50K\")\nplt.xlabel(\"Capital Loss\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Discrimina\u00e7\u00e3o de Capital Loss por faixa de ganhos\")","82671ba6":"df_treino['hours.per.week'].hist()\nplt.xlabel(\"Hours per Week\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Distribui\u00e7\u00e3o de Hours per Week\")","0b5137b5":"df_treino[df_treino['income'] == 0]['hours.per.week'].hist(label=\"<=50K\")\ndf_treino[df_treino['income'] == 1]['hours.per.week'].hist(label=\">50K\")\nplt.xlabel(\"Hours per Week\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.title(\"Discrimina\u00e7\u00e3o de Hours per Week por faixa de ganhos\")","b3332e48":"df_treino['native_country'].value_counts().plot(kind='pie')\nplt.title(\"Distribui\u00e7\u00e3o Native Country\")","3eeea642":"df_treino = df_treino.drop(['native_country'], axis = 1) \ndf_treino.info()\n\ndf_teste = df_teste.drop(['native_country'], axis = 1) \ndf_teste.info()","8f488bb7":"workclass_dummies = pd.get_dummies(df_treino.workclass)\nmarital_status_dummies = pd.get_dummies(df_treino.marital_status)\noccupation_dummies = pd.get_dummies(df_treino.occupation)\nrelationship_dummies = pd.get_dummies(df_treino.relationship)\nrace_dummies = pd.get_dummies(df_treino.race)\nsex_dummies = pd.get_dummies(df_treino.sex)\n\ndf_treino = pd.concat([df_treino, workclass_dummies, marital_status_dummies, occupation_dummies ,relationship_dummies, race_dummies,sex_dummies], axis=1)\n\ndf_treino = df_treino.drop(['workclass', 'marital_status', 'occupation', 'relationship', 'race','sex'], axis = 1)\n\nworkclass_dummies = pd.get_dummies(df_teste.workclass)\nmarital_status_dummies = pd.get_dummies(df_teste.marital_status)\noccupation_dummies = pd.get_dummies(df_teste.occupation)\nrelationship_dummies = pd.get_dummies(df_teste.relationship)\nrace_dummies = pd.get_dummies(df_teste.race)\nsex_dummies = pd.get_dummies(df_teste.sex)\n\ndf_teste = pd.concat([df_teste, workclass_dummies, marital_status_dummies, occupation_dummies ,relationship_dummies, race_dummies,sex_dummies], axis=1)\n\ndf_teste = df_teste.drop(['workclass', 'marital_status', 'occupation', 'relationship', 'race','sex'], axis = 1)\n","6651a28f":"treino, holdout = train_test_split(df_treino, test_size=0.2)\n\nY_holdout = np.array(holdout['income'])\nholdout = holdout.drop(['income'], axis = 1)\n\nY_treino = np.array(treino['income']) \ntreino = treino.drop(['income'], axis = 1)\n\nscaler = sklearn.preprocessing.MinMaxScaler()\n\ntreino = pd.DataFrame((scaler.fit_transform(treino)))\nholdout = pd.DataFrame((scaler.transform(holdout)))\ndf_teste = pd.DataFrame((scaler.transform(df_teste)))\n                       \nX_holdout = np.array(holdout)                        \nX_treino = np.array(treino)\nX_teste = np.array(df_teste)","776bc181":"logreg = LogisticRegression(solver=\"liblinear\")\n\n# Hyperparameters\nparameters_LR = dict(C=np.linspace(0.01, 10, 100),penalty=[\"l1\", \"l2\"],)\n\n# Research\nLog_Reg = RandomizedSearchCV(logreg,parameters_LR,scoring=\"roc_auc\",cv=2,n_iter=50, n_jobs=-1,)\nLog_Reg = Log_Reg.fit(X_treino, Y_treino)\n\n\nprint(\"CLASSIFICADOR 1 (LOGREG) : Melhor AUC ROC = \", Log_Reg.best_score_ )\nprint(\"                         : Melhores Par\u00e2metros= \", Log_Reg.best_params_)","be641a74":"mlp = MLPClassifier(early_stopping=True)\n\nHL_size = [x for x in range(20, 131, 10)]\nalpha = sp_loguniform(0.000001, 0.1)\nparameters_1HL= {'hidden_layer_sizes': HL_size , 'alpha': alpha}\n\nMLP_1HL = RandomizedSearchCV(mlp, parameters_1HL, n_jobs=4, verbose=1, scoring=\"roc_auc\")\nMLP_1HL.fit(X_treino, Y_treino) #CLASSIFICADOR 1\n\nprint(\"CLASSIFICADOR 2 (MLP) : Melhor AUC ROC = \", MLP_1HL.best_score_ )\nprint(\"                      : Melhores Par\u00e2metros= \",MLP_1HL.best_params_)","eaa13f73":"rfc=RandomForestClassifier(random_state=42)\n\nparameters_RF = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\n\nRF = RandomizedSearchCV(rfc, parameters_RF, n_jobs=4, verbose=1, scoring=\"roc_auc\")\nRF.fit(X_treino, Y_treino)\n\nprint(\"CLASSIFICADOR 3 (RANDOM FOREST) : Melhor AUC ROC = \", RF.best_score_ )\nprint(\"                                : Melhores Par\u00e2metros= \",RF.best_params_)","d1e99f91":"from sklearn import svm\nsvm = svm.SVC()\n\nCs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparameters_SVM = {'C': Cs, 'gamma' : gammas}\n\nSVM = RandomizedSearchCV(svm, parameters_SVM, n_jobs=4, verbose=1, scoring=\"accuracy\")\nSVM.fit(X_treino, Y_treino) #CLASSIFICADOR 1\n\nprint(\"CLASSIFICADOR 4 (SVM) : Melhor AUC ROC = \", SVM.best_score_ )\nprint(\"                      : Melhores Par\u00e2metros= \",SVM.best_params_)","238a449f":"\npredicao_LR = Log_Reg.predict(X_holdout)\naccuracy_LR = accuracy_score(Y_holdout, predicao_LR)\nprint(\"acuracia LOGREG : \", accuracy_LR*100)\n\n\npredicao_MLP = MLP_1HL.predict(X_holdout)\naccuracy_MLP = accuracy_score(Y_holdout, predicao_MLP)\nprint(\"acuracia MLP : \", accuracy_MLP*100)\n\n\npredicao_RF = RF.predict(X_holdout)\naccuracy_RF = accuracy_score(Y_holdout, predicao_RF)\nprint(\"acuracia RF : \", accuracy_RF*100)\n\n\n\npredicao_SVM = SVM.predict(X_holdout)\naccuracy_SVM = accuracy_score(Y_holdout, predicao_SVM)\nprint(\"acuracia SVM : \", accuracy_SVM*100)\n","92028156":"Y_teste = RF.predict(X_teste)\n\nOutput_teste = pd.DataFrame()\nOutput_teste[0] = df_teste.index\nOutput_teste[1] = Y_teste\nOutput_teste.columns = ['Id','Income']\nOutput_teste.set_index('Id', inplace=True)\n\nprint(\"Predi\u00e7\u00e3o realizada\")","bf644072":"Output_teste[Output_teste['Income'] == 0] = '<=50K'\nOutput_teste[Output_teste['Income'] == 1] = '>50K'\n\nOutput_teste.to_csv('submission.csv', index = True, index_label = 'Id')\n\nprint(\"Arquivo submission.csv salvo!\")","1272d99d":"Podemos ver pelo gr\u00e1fico acima que a base de dados \u00e9 **desbalanceada** no quesito idade. A maioria dos dados no dataframe s\u00e3o de pessoas com at\u00e9 40 anos, com um decl\u00ednio exponencial a partir dos 50.\n\nCorrela\u00e7\u00e3o com a var\u00edavel de classe","88ad6da3":"### 2.11) CAPITAL GAIN\n\nA vari\u00e1vel Capital Gain \u00e9 bem esparsa, apresentando uma grande concentra\u00e7\u00e3o em valores baixos e nulos. Fica clara a correla\u00e7\u00e3o entre esse atributo e a renda, uma vez que toda a minoria que apresenta capital gain elevado est\u00e1 na faixa de renda mais alta.","f101bcb8":"Uma vez que as features education e education.num est\u00e3o estritamente correlacionadas, considerar ambas na nossa an\u00e1lise introduziria um **vi\u00e9s no classificador**. Assim, devemos excluir uma delas da nossa an\u00e1lise. A feature \"education.num\" ser\u00e1 mantida, por j\u00e1 estar em formato num\u00e9rico e portanto exigir menos preparo, e a **\"education\" exclu\u00edda.**","0f3d4446":"### 3.3) RANDOM FOREST","fcb37fb0":"### 3.4) SUPPORT VECTOR MACHINE (SVM)","a9e7c551":"Como se pode ver, a maioria das features exibe **correla\u00e7\u00e3o positiva** com a renda, o que significa que quanto maior seu valor, maior a chance da pessoa ter uma renda alta. Outro bom resultado \u00e9 que **as features entre si n\u00e3o apresentam alto grau de correla\u00e7\u00e3o**, o que garante que n\u00e3o estamos introduzindo vi\u00e9s no nosso classificador ao considerar informa\u00e7\u00f5es redundantes. A maior correla\u00e7\u00e3o entre features \u00e9 de 15%, entre o n\u00famero de anos de educa\u00e7\u00e3o e horas semanais trabalhadas. A **educa\u00e7\u00e3o tamb\u00e9m \u00e9 o atributo mais influente na renda, com \u00edndice de correla\u00e7\u00e3o de 34%.**\n\nAgora podemos analisar cada uma das features separadamente:\n\n### 2.2) AGE\n\nHistograma com a distribui\u00e7\u00e3o das idades\n\n","73632ce9":"A distribui\u00e7\u00e3o de entradas n\u00e3o \u00e9 equilibrada ao longo dos diferentes anos de escolaridade. H\u00e1 uma grande concentra\u00e7\u00e3o de valores nos 9, 10 e 14 anos de educa\u00e7\u00e3o.","17a14b4c":"A propor\u00e7\u00e3o de pessoas com renda maior aumenta com a quantidade de anos de estudo.\n\n### 2.6) MARITAL STATUS","46050a98":"### 2.5) EDUCATION E EDUCATION.NUM\n\nA seguir vamos analisar as features Education e Education.num. Como j\u00e1 visto no heatmap, o n\u00famero de anos de educa\u00e7\u00e3o da pessoa apresenta uma boa correla\u00e7\u00e3o com sua renda. Por\u00e9m, estes dois atributos s\u00e3o **redundantes**, pois representam a mesma informa\u00e7\u00e3o de formas diversas. Podemos ver no gr\u00e1fico abaixo que cada n\u00edvel de educa\u00e7\u00e3o corresponde exatamente a um certo n\u00famero de anos (portanto n\u00e3o h\u00e1 barra de desvio padr\u00e3o nos valores).","5dc42837":"Podemos agora visualizar a distribui\u00e7\u00e3o de renda nas diferentes ocupa\u00e7\u00f5es. O gr\u00e1fico abaixo mostra que as 4 categorias que apresentam maior m\u00e9dia de renda s\u00e3o exec-managerial, prof-specialty, protective-services e sales. Tamb\u00e9m \u00e9 interessante notar a grande varia\u00e7\u00e3o na renda de pessoas nas for\u00e7as armadas.","0609a049":"A distribui\u00e7\u00e3o da renda mais alta (>50K) aparenta ser normal, com m\u00e9dia perto dos 40 anos. J\u00e1 a renda mais baixa parece sofrer uma queda com o aumento da idade, por\u00e9m \u00e9 necess\u00e1rio considerar a menor quantidade de dados para idades mais avan\u00e7adas.\n\n### 2.3) WORKCLASS \n\nWorkclass \u00e9 uma vari\u00e1vel **categ\u00f3rica**. Para tratar os dados faltantes, precisamos entender a distribui\u00e7\u00e3o dos diferentes tipos de workclass na base de dados.\n","bb4572f0":"### 2.16) NORMALIZA\u00c7\u00c3O DOS DADOS, SEPARA\u00c7\u00c3O DE HOLD-OUT E VETORIZA\u00c7\u00c3O\n\nPara garantir que as diferentes ordens de grandeza das features n\u00e3o ter\u00e3o influ\u00eancia nos seus pesos, precisamos realizar uma **normaliza\u00e7\u00e3o ou padroniza\u00e7\u00e3o dos dados**. Podemos aplicar um Scaler do tipo **minMax** para que as distribui\u00e7\u00f5es mantenham o mesmo formato, por\u00e9m fiquem limitadas a valores entre 0 e 1.\n\nPara finalizar, devemos separar um peda\u00e7o do nosso dataframe (20%) de treino como **hold-out**, para que sejam utilizados na compara\u00e7\u00e3o entre os classificadores depois de treinados. Al\u00e9m disso, tanto na base de treino quanto na de hold-out \u00e9 necess\u00e1rio dividir os X_treino (atributos) e Y_treino (vari\u00e1vel de classe). Por fim, devemos transform\u00e1-las em **vetores numpy** e nossas bases de dados est\u00e3o **pronta para serem usadas no classificadores.**\n","ba040e71":"### 2.1) VARI\u00c1VEIS NUM\u00c9RICAS\n\nUma primeira an\u00e1lise interessante a ser feita entre as vari\u00e1veis num\u00e9ricas \u00e9 o heatmap apresentado abaixo, onde podemos ver a correla\u00e7\u00e3o das features entre si, bem como das features com a vari\u00e1vel de classe : ","a4e05f66":"### 2.15) TRANSFORMA\u00c7\u00c3O DE DADOS CATEG\u00d3RICOS : \n\nPara podermos inserir a base de dados no classificador, ainda \u00e9 necess\u00e1rio transformar os dados categ\u00f3ricos em n\u00fameros. Uma vez que as categorias n\u00e3o apresentam uma ordem, podemos usar o m\u00e9todo de dummies, isso \u00e9, substituir um atributo com N poss\u00edveis denomina\u00e7\u00f5es por N atributos bin\u00e1rios. Ap\u00f3s isso podemos excluir as colunas originais do dataframe","dc5b3429":"Antes de treinar o classificador, devemos **entender melhor a correla\u00e7\u00e3o entre as features e a vari\u00e1vel de classe.**\nPara isso vamos visualizar e analisar a distribui\u00e7\u00e3o dos dados, realizando ajustes e transforma\u00e7\u00f5es onde necess\u00e1rio. Lembrando que todas as modifica\u00e7\u00f5es realizadas na base de treino devem ser tamb\u00e9m realizadas na de teste.\n\nNa sess\u00e3o abaixo est\u00e3o listadas as **caracter\u00edsticas b\u00e1sicas do nosso dataframe**. Al\u00e9m disso, alguns nomes dos atributos foram modificados para facilitar a manipula\u00e7\u00e3o da base de dados no futuro","0e7b91d0":"## 3) Treinamento e Otimiza\u00e7\u00e3o de Diferentes Classificadores\n\nCom os dados pr\u00e9-processados, podemos dar in\u00edcio ao treinamento dos classificadores, com a otimiza\u00e7\u00e3o dos hiperpar\u00e2metros atrav\u00e9s de cross-validation.\n\n### 3.1) REGRESS\u00c3O LOG\u00cdSTICA","3dbc8c66":"## 2) Visualiza\u00e7\u00e3o Geral e Pr\u00e9 Processamento dos Dados Dispon\u00edveis\n\nImportamos tamb\u00e9m as bases de dados, tanto a de treino quanto a de teste, transformando os dados faltantes no tipo na.\nComo a base de dados deste Exerc\u00edcio \u00e9 a mesma que foi utilizada no primeiro trabalho da disciplina, esta sess\u00e3o foi mantida igual. \n","6e1f8d53":"### 2.14) NATIVE COUNTRY\n\nFinalmente, o \u00faltimo atributo a ser analisado \u00e9 o Native Country. Uma vez que a base de dados \u00e9 origin\u00e1ria de um censo estadunidense, a grande maioria das entradas se refere a pessoas nativas de l\u00e1. Todos os outros pa\u00edses possuem quantidades de entrada muito menores, referentes a imigrantes, como pode ser visto no gr\u00e1fico abaixo. Isso faz com que a base de dados seja **extremamente desbalanceada** em rela\u00e7\u00e3o a essa feature, o que comprometeria o desempenho do classificador. Assim, essa coluna foi **exclu\u00edda do dataframe**.","e88345b1":"Como podemos ver acima, o classificador **RANDOM FOREST** \u00e9 o que apresenta maior acur\u00e1cia. Portanto, ele \u00e9 o selecionado para ser usado no pr\u00f3ximo passo\n\n# 5) Aplica\u00e7\u00e3o na Base de Teste e Exporta\u00e7\u00e3o dos Resultados\n\nOs dados de teste j\u00e1 est\u00e3o prontos para serem inseridos no classificador, uma vez que sofreram as mesmas modifica\u00e7\u00f5es que os dados de treino durante o pr\u00e9-processamento. Assim, j\u00e1 podemos realizar a predi\u00e7\u00e3o.","1185d00f":"   # PMR3508 - Aprendizado de M\u00e1quina e Reconhcimento de Padr\u00f5es\n                                    Classification with the Adult Dataset\n\n                                          Paolla Furquim Daud - 9345533\n                                          \nO objetivo dessa atividade \u00e9 treinar quatro classificadores diferentes que prevejam a renda de uma pessoa baseado em dados coletados em um censo. Iremos otimizar os hiperpar\u00e2metros de cada um dos modelos utilizando cross-validation e, por fim, compar\u00e1-los em uma base de teste com diferentes m\u00e9tricas.\n\n1) Instala\u00e7\u00e3o e Importa\u00e7\u00f5es de Pacotes e Bibiotecas Necess\u00e1rias\n\n2) Visualiza\u00e7\u00e3o Geral e Pr\u00e9 Processamento dos Dados Dispon\u00edveis\n\n3) Treinamento e Otimiza\u00e7\u00e3o de Diferentes Classificadores\n\n4) Compara\u00e7\u00e3o e Sele\u00e7\u00e3o do Melhor Modelo\n\n5) Aplica\u00e7\u00e3o na Base de Teste e Exporta\u00e7\u00e3o dos Resultados\n\n## 1) Instala\u00e7\u00e3o e Importa\u00e7\u00f5es de Pacotes e Bibiotecas Necess\u00e1rias\n\nNa sess\u00e3o abaixo realizamos a **instala\u00e7\u00e3o** de todas as bibliotecas que ser\u00e3o necess\u00e1rias ao longo do programa, tanto para a visualiza\u00e7\u00e3o inicial da base de dados e seu pr\u00e9-processamento, quanto para a implementa\u00e7\u00e3o dos diferentes classificadores (Random Forest, Regressor Log\u00edstico, LDA , Rede Neural).\n","c506df03":"Como fica claro, a grande maioria das entradas tem a feature Workclass como **Private, que concentra quase 75% dos dados**. Assim, podemos utilizar o **m\u00e9todo de inser\u00e7\u00e3o da moda**, substituindo os dados faltantes nessa coluna pelo valor modal. \n\nPodemos ver abaixo que a contagem de dados faltantes na feature Workclass agora \u00e9 0.","132636d7":"### 3.2 REDE NEURAL (MLP) ","e4e48c39":"Lembrando que agora a nossa vari\u00e1vel de renda assume o valor 0 quanto menor que 50K e 1 quanto maior, podemos ver que a workclass Self Employed Incorporated tem uma m\u00e9dia de renda consideravelmente maior do que as outras, enquanto Without pay e Never Worked tem, previsivelmente, m\u00e9dia abaixo de 50K.\n\n### 2.4) FNLWGT (Final Weight)\n\nA feature \"fnlwgt\" pode ser **deletada** da base de dados, por **n\u00e3o apresentar correla\u00e7\u00e3o com a v\u00e1riavel de interesse**. Sua inten\u00e7\u00e3o \u00e9 representar a quantidade de pessoas que s\u00e3o identificadas pela entrada, ou seja, quantas pessoas correspondem a descri\u00e7\u00e3o dada pelas features restantes da entrada. Isso n\u00e3o nos traz informa\u00e7\u00f5es relevantes para prever a classe. \n\nO \u00edndice de Pearson calculado anteriormente confirma essa hip\u00f3tese. Seu valor \u00e9 muito pr\u00f3ximo de 0, que indica n\u00e3o correla\u00e7\u00e3o entre as vari\u00e1veis. Portanto vamos excluir essa coluna dos nossos dados.","38185875":"Temos um total de 15 colunas :\n- 1 \u00edndice \n- 13 features (6 num\u00e9ricas e 7 categ\u00f3ricas) \n- 1 vari\u00e1vel de classe (label), com 2 poss\u00edveis categorias\n\nPodemos tamb\u00e9m avaliar se h\u00e1 **dados faltantes** : ","918a011c":"A maior parte das entradas s\u00e3o casadas com um c\u00f4njuge civil, seguido por nunca casados, e divorciados. As outras categorias s\u00e3o bem menos representadas na base de dados.","218f92ad":"### 2.10) SEX\n\nA distribui\u00e7\u00e3o dos dados em rela\u00e7\u00e3o ao sexo dos entrevistados releva que a maior parte das entradas s\u00e3o de homens. Al\u00e9m disso podemos ver uma grande correla\u00e7\u00e3o entre sexo e renda, uma vez que a renda m\u00e9dia dos homens \u00e9 tr\u00eas vezes a renda m\u00e9dia das mulheres. ","9c45b236":"Podemos tamb\u00e9m analisar a distribui\u00e7\u00e3o dos diferentes graus de escolaridade nas nossas entradas, e visualizar em maiores detalhes a correla\u00e7\u00e3o entre essa feature e a renda","49baf463":"Ao mesmo tempo, dado ao bom n\u00famero de atributos \u00e9 poss\u00edvel que o classificador tenha informa\u00e7\u00f5es suficientes para predizer estas entradas mesmo sem a ocupa\u00e7\u00e3o. Portanto a solu\u00e7\u00e3o adotada \u00e9 **substituir os dados faltantes por uma nova categoria \"Unknown\"**. Depois da substitui\u00e7\u00e3o podemos verificar que o n\u00famero de dados faltantes nesse atributo agora \u00e9 0.","5a1fbb93":"As predi\u00e7\u00f5es est\u00e3o no dataframe Output_teste acima. Para pass\u00e1-las para um arquivo .csv, vamos reverter os r\u00f3tulos de 0 e 1 para <=50K e >50K novamente, e exportar! ","ec0972f3":"Como \u00e9 poss\u00edvel ver acima, temos dados faltantes nas features **workclass, occupation e native.country**. Isso \u00e9 um problema que tem que ser resolvido nos pr\u00f3ximos passos.Vamos analisar as features uma a uma, para determinar quais modifica\u00e7\u00f5es s\u00e3o necess\u00e1rias.\n\nPara facilitar a manipula\u00e7\u00e3o dos dados, vamos substituir a vari\u00e1vel categ\u00f3rica de interesse por uma vari\u00e1vel bin\u00e1ria, onde 0 significa renda <= 50K e 1 renda > 50K.\n","52523ef3":"### 2.13) HOURS PER WEEK\n\nEsse atributo possui uma grande concentra\u00e7\u00e3o em 40, que \u00e9 a jornada de trabalho comum. Ele mostra uma correla\u00e7\u00e3o positiva com a renda, sendo que pessoas que trabalham acima da jornada de 8h por dia tendem a ter rendas mais altas.","3b82cce5":"Vejamos tamb\u00e9m a rela\u00e7\u00e3o da feature com a renda.","14c54713":"### 2.12) CAPITAL LOSS\n\nEste atributo tem uma distribui\u00e7\u00e3o muito semelhante ao anterior, com uma grande concentra\u00e7\u00e3o em valores baixos e correla\u00e7\u00e3o positiva com a renda.","5b23389a":"### 2.8) RELATIONSHIP\n\nPodemos ver abaixo os dois gr\u00e1ficos sobre a distribui\u00e7\u00e3o do atributo relacionamentos. A maioria das entradas est\u00e1 concentrada entre as categorias husband e not-in-family. Ao analisar a rela\u00e7\u00e3o entre a categoria e a renda, pode-se notar que a renda m\u00e9dia de maridos e esposas \u00e9 consideravelmente mais alta do que das outras poss\u00edveis categorias. ","08767326":"Podemos ver que os estado civis com maior renda m\u00e9dia s\u00e3o casados, com c\u00f4njuge civil ou nas for\u00e7as armadas, sendo que o segundo apresenta um desvio muito maior. As outras categorias apresentam uma renda m\u00e9dia relativamente semelhante, sendo que nunca casado apresenta a menor delas.\n\n### 2.7) OCCUPATION\n\nA feature occupation apresenta **dados faltantes**, portando devemos tomar uma decis\u00e3o sobre como trat\u00e1-los. Podemos ver que de acordo com o gr\u00e1fico abaixo, h\u00e1 um certo **equil\u00edbrio entre diversas profiss\u00f5es**. Dessa forma, a inser\u00e7\u00e3o da moda n\u00e3o \u00e9 um bom m\u00e9todo de tratar os dados faltantes, uma vez que a **moda n\u00e3o \u00e9 t\u00e3o representativa da nossa distribui\u00e7\u00e3o.**","44883a4e":"### 2.9) RACE\n\nOs gr\u00e1ficos abaixo evidenciam que a grande maioria da popula\u00e7\u00e3o que respondeu ao censo \u00e9 branca, compondo mais de 80% das entradas. Al\u00e9m disso podemos notar uma diferen\u00e7a significativa entre a renda m\u00e9dia de pessoas brancas e asi\u00e1ticas em compara\u00e7\u00e3o \u00e0s outras ra\u00e7as, sendo aproximadamente o dobro.","53302767":"# 4) Compara\u00e7\u00e3o e Sele\u00e7\u00e3o do Melhor Modelo\n \nAgora, com os classificadores de diferentes modelos otimizados para nosso problema, podemos compar\u00e1-los. Para avali\u00e1-los sem poss\u00edvel intefer\u00eancia de overfitting na base usada para treino, vamos utilizar a base de dados de **hold-out** (criada no item 2.16, 20% da base de treino original), realizar a predi\u00e7\u00e3o com cada um dos classificadores e calcular as suas acur\u00e1cias (porcentagem de acerto da classe). O classificador com maior acur\u00e1cia ser\u00e1 ent\u00e3o utilizado para o pr\u00f3ximo passo, predizer as classes da base de dados de teste."}}