{"cell_type":{"b585d78d":"code","289ea36c":"code","c12a87dd":"code","e5c0a080":"code","ac3874f5":"code","e570b027":"code","5f0bf6f7":"code","121f77e8":"code","0abc9d09":"code","feba85be":"code","96e83e77":"code","31fcc120":"code","50eb349d":"code","ccbe7594":"code","b8b8295b":"code","37afbaf7":"code","c8ac0990":"code","2e35912f":"code","133a7e99":"code","8cf42322":"code","f35dee54":"code","fd160a37":"code","1a29854f":"code","7b188a9a":"code","c6841c20":"code","e3bca22e":"code","070fc8b7":"code","c9dfea63":"code","cc613da0":"code","d1a01a4a":"code","7ce51daf":"code","a5366572":"code","292ae04a":"code","d95c64d5":"code","b6030112":"code","6dc3429b":"code","b455bd64":"code","e7db8b9d":"code","073ff9b7":"code","ef167483":"code","f2eabaa9":"code","805f42ce":"code","c2e5a5bd":"code","a7fa2873":"code","ba1874ce":"code","8e60d2ee":"code","69716c80":"code","52dafedb":"code","74ea4eb6":"code","deea1ffc":"code","93981b67":"code","64b0ca13":"code","defac21a":"code","808bafa6":"code","3b1a5bc1":"code","94a24f1a":"code","300bea7d":"code","f24b3ff0":"code","e0954d31":"code","e9066287":"code","e31420f0":"markdown","9a181027":"markdown","17ebfb7c":"markdown","c24f0434":"markdown","fbbefb1c":"markdown","18c9bb73":"markdown","c8cce171":"markdown","74663c68":"markdown","5beadc8b":"markdown","56f0620a":"markdown","a54768b8":"markdown","a034eac0":"markdown","e906ed21":"markdown","e24d5b76":"markdown","f5333273":"markdown","21c80a10":"markdown","f01f8b7b":"markdown","da021d6a":"markdown","547ef0aa":"markdown","794f200c":"markdown","8372fbb7":"markdown","550c917b":"markdown","0a4b5b28":"markdown","e6235f05":"markdown","089f456a":"markdown","55c23db4":"markdown"},"source":{"b585d78d":"import warnings\nwarnings.filterwarnings('ignore')","289ea36c":"import pandas as pd\nimport numpy as np","c12a87dd":"startups = pd.read_csv(\"..\/input\/startup-logistic-regression\/50_Startups.csv\")\nstartups.head()","e5c0a080":"startups.shape","ac3874f5":"startups.info()","e570b027":"# Statistical summary\n'''As you can see that target varaible \"Profit\" that contains outlier.'''\nstartups.describe()","5f0bf6f7":"# Null-value percentage\n(startups.isnull().sum()\/startups.shape[0]) *100","121f77e8":"import matplotlib.pyplot as plt\nimport seaborn as sns","0abc9d09":"sns.boxplot(startups[\"R&D Spend\"])","feba85be":"sns.boxplot(startups[\"Administration\"])","96e83e77":"sns.boxplot(startups[\"Marketing Spend\"])","31fcc120":"'''Target variable contains an outlier'''\nsns.boxplot(startups[\"Profit\"])","50eb349d":"'''Remove outliers are target variable'''\nQ3, Q1 = np.percentile(startups[\"Profit\"], [75 ,25])\nIQR = Q3 - Q1\nIQR","ccbe7594":"# Remove outlier\nstartups = startups[~(startups.Profit< (Q1 - 1.5*IQR))]","b8b8295b":"sns.boxplot(startups[\"Profit\"])","37afbaf7":"# Recheck the dimention of the dataframe\nstartups.shape","c8ac0990":"# Statistical summary for modified dataframe\nstartups.describe()","2e35912f":"# Numerical columns\nsns.pairplot(startups, kind=\"reg\", diag_kind=\"kde\",  hue=\"State\")\nplt.show()","133a7e99":"California = startups[startups.State == \"California\"]\nFlorida    = startups[startups.State == \"Florida\"]\nNew_York   = startups[startups.State == \"New York\"]","8cf42322":"Profit_California = round(California.Profit.mean(), 2)\nProfit_Florida    = round(Florida.Profit.mean(), 2)\nProfit_New_York   = round(New_York.Profit.mean(), 2)\n\nax = sns.barplot(x=\"State\", y=\"Profit\", data=startups)\n\nprint(f\"Average profit in California(in million$) = ${round(Profit_California\/1000000,3)}\")\nprint(f\"Average profit in Florida   (in million$) = ${round(Profit_Florida\/1000000, 3)}\")\nprint(f\"Average profit in New_York  (in million$) = ${round(Profit_New_York\/1000000, 3)}\")","f35dee54":"Administration_California = round(California.Administration.mean(), 2)\nAdministration_Florida    = round(Florida.Administration.mean(), 2)\nAdministration_New_York   = round(New_York.Administration.mean(), 2)\n\nax = sns.barplot(x=\"State\", y=\"Profit\", data=startups)\n\nprint(f\"Average company's administration in California(in million$) = ${round(Administration_California\/1000000, 3)}\")\nprint(f\"Average company's administration in Florida   (in million$) = ${round(Administration_Florida\/1000000, 3)}\")\nprint(f\"Average company's administration in New_York  (in million$) = ${round(Administration_New_York\/1000000, 3)}\")","fd160a37":"Marketing_California = round(California[\"Marketing Spend\"].mean(), 2)\nMarketing_Florida    = round(Florida[\"Marketing Spend\"].mean(), 2)\nMarketing_New_York   = round(New_York[\"Marketing Spend\"].mean(), 2)\n\nax = sns.barplot(x=\"State\", y=\"Profit\", data=startups)\n\nprint(f\"Average marketing in California(in million$) = ${round(Marketing_California\/1000000, 3)}\")\nprint(f\"Average marketing in Florida   (in million$) = ${round(Marketing_Florida\/1000000, 3)}\")\nprint(f\"Average marketing in New_York  (in million$) = ${round(Marketing_New_York\/1000000, 3)}\")","1a29854f":"startups[\"State\"].unique()","7b188a9a":"startups = pd.get_dummies(startups, drop_first=True)\nstartups.rename(columns={\"R&D Spend\":\"R&D\", \"Marketing Spend\":\"Marketing\", \n                         \"State_Florida\":\"Florida\", \"State_New York\":\"New York\"}, inplace=True)","c6841c20":"startups.head()","e3bca22e":"sns.pairplot(startups, kind=\"reg\", diag_kind=\"kde\")\nplt.show()","070fc8b7":"startups.corr()","c9dfea63":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\nstartups_train, startups_test = train_test_split(startups, train_size=0.67, test_size=0.33, random_state=42)","cc613da0":"startups_train.shape","d1a01a4a":"startups_test.shape","7ce51daf":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","a5366572":"'''There is no multi-collinearity between the predictor variables'''\nplt.figure(figsize=[8,5])\nsns.heatmap(startups_train.corr(), annot = True, cmap=\"RdYlGn\", center=0.0)\nplt.show()","292ae04a":"# standard the numerical columns\nnum_col = [\"R&D\",\"Administration\",\"Marketing\",\"Profit\"]\n# num_col = startups_train.iloc[:,-3:]\n\nstartups_train[num_col] = scaler.fit_transform(startups_train[num_col])\nstartups_train.reset_index(drop=True, inplace=True)\nstartups_train.head()","d95c64d5":"y_train = startups_train.pop(\"Profit\")\nX_train = startups_train","b6030112":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","6dc3429b":"# VIF values of the feature variables:\ndef VIF(X):\n    vif = pd.DataFrame()\n    vif[\"Features\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X.shape[1])]\n    vif[\"VIF\"] = round(vif[\"VIF\"], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","b455bd64":"X_train_1 = X_train[[\"R&D\"]]\nX_train_lm = sm.add_constant(X_train_1)\nlr_1 = sm.OLS(y_train, X_train_lm).fit()\nprint(lr_1.summary())\n\nVIF(X_train_1)","e7db8b9d":"X_train_2 = X_train[[\"R&D\",\"Marketing\"]]\nX_train_lm2 = sm.add_constant(X_train_2)\nlr_2 = sm.OLS(y_train, X_train_lm2).fit()\nprint(lr_2.summary())\n\nVIF(X_train_2)","073ff9b7":"Predicted_profit  = lr_2.predict(X_train_lm2)\nresiduals = y_train - Predicted_profit\n\nAct_pred = pd.DataFrame(data={\"Predicted\":Predicted_profit, \"Actual\":y_train, \"Residual Error\":round(residuals,3)})\nAct_pred.reset_index(drop=True, inplace=True)\nAct_pred.head()","ef167483":"# Q-Q plot\n'''The Q-Q plot infers that the residuals meets\u00a0condition of\u00a0homoscedasticity'''\nimport scipy.stats as stats\n\nfig = sm.qqplot(residuals, stats.t, fit=True, line=\"45\")\nfig.suptitle(\"Error Terms\", fontsize = 20)    \nplt.show()","f2eabaa9":"'''The errors should not follow any pattern and equally distributed y=0(i.e; mean=0.000)'''\nplt.scatter(y_train, residuals)\nplt.axhline(y=0.0, color='r', linestyle='-')\nplt.show()","805f42ce":"startups_test[num_col] = scaler.transform(startups_test[num_col])\nstartups_test.head()","c2e5a5bd":"startups_test.describe()","a7fa2873":"y_test = startups_test.pop(\"Profit\")\nX_test = startups_test","ba1874ce":"X_test_lm2 = sm.add_constant(X_test)\nX_test_lm2 = X_test_lm2.drop([\"Administration\",\"Florida\",\"New York\"], axis = 1).reset_index(drop=True)\nX_test_lm2.head()","8e60d2ee":"# Making predictions using the second model\nPredicted_Profit = lr_2.predict(X_test_lm2)","69716c80":"fig = plt.figure()\nplt.scatter(y_test, Predicted_Profit)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)   \nplt.show()","52dafedb":"#Actual vs Predicted - Test dataset.\nc = [i for i in range(1,18,1)]\nfig = plt.figure()\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.plot(c,Predicted_Profit, color=\"red\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle(\"Actual and Predicted\", fontsize=20)              # Plot heading \nplt.xlabel(\"Index\", fontsize=18)                               # X-label\nplt.ylabel(\"Profit\", fontsize=16)                               # Y-label\nplt.show()","74ea4eb6":"# Evaluvate using r-squared metrics\nfrom sklearn.metrics import r2_score\n\nr2 = r2_score(y_test,Predicted_Profit)\nround(r2, 4)","deea1ffc":"# Check the mean error for predicted value and actual value\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, Predicted_Profit)\nprint(f\"Mean Squared Error = {round(mse, 4)}\")","93981b67":"# Adjusted R-squared\n\n'''n is number of rows in X_test'''\nn = X_test_lm2.shape[0]\n\n'''Number of features'''\np = X_test_lm2.shape[1]\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nround(adjusted_r2, 4)","64b0ca13":"mse_percent = round(mse, 4)\nr2_percent = round(r2*100, 2)\nadjusted_r2_percent = round(adjusted_r2*100, 2)\nresult = pd.DataFrame(data={\"MSE%\":[mse_percent], \"R-squared%\":[r2_percent], \n                            \"Adjusted R-squared%\":[adjusted_r2_percent]})\nresult","defac21a":"# Parameters from regression model\nparameter = lr_2.params\n\nconst  = round(parameter[0], 2)\ncoeff1 = round(parameter[1], 2)\ncoeff2 = round(parameter[2], 2)\neqn = pd.DataFrame(data={\"coeff Constant\":[const], \"coeff R&D\":[coeff1], \"coeff Marketing\":[coeff2]})\neqn","808bafa6":"# Convert X and y to arrays\nimport numpy as np\nX = X_train_lm2\ny = y_train\n\nX = np.array(X)\ny = np.array(y)","3b1a5bc1":"# Theta is the vector representing coefficients (intercept, area, bedrooms)\ntheta = np.matrix(np.array([0,0,0])) \nalpha = 0.5\niterations = 1000","94a24f1a":"def compute_cost(X, y, theta):\n    return np.sum(np.square(np.matmul(X, theta) - y)) \/ (2 * len(y))","300bea7d":"# gradient descent\n'''Takes in current X, y, learning rate alpha, num_iters\n    returns cost (notice it uses the cost function defined above)'''\n\ndef gradient_descent_multi(X, y, theta, alpha, iterations):\n    theta = np.zeros(X.shape[1])\n    m = len(X)\n    gdm_df = pd.DataFrame( columns = ['Bets','cost'])\n\n    for i in range(iterations):\n        gradient = (1\/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n        theta = theta - alpha * gradient\n        cost = compute_cost(X, y, theta)\n        gdm_df.loc[i] = [theta,cost]\n\n    return gdm_df","f24b3ff0":"# print costs with various values of coefficients b0, b1, b2\ngradient_descent_multi(X, y, theta, alpha, iterations)","e0954d31":"Coeff = gradient_descent_multi(X, y, theta, alpha, iterations).iloc[999,:]","e9066287":"const  = round(Coeff[0][0], 2)\ncoeff1 = round(Coeff[0][1], 2)\ncoeff2 = round(Coeff[0][2], 2)\neqn_gradient = pd.DataFrame(data={\"coeff Constant\":[const], \"coeff R&D\":[coeff1], \"coeff Marketing\":[coeff2]})\neqn_gradient","e31420f0":"### Data Preparation","9a181027":"### Reading and Understanding the Data","17ebfb7c":"### Dividing into X_test and y_test","c24f0434":"### Dropping the Variable and Updating the Model\n<!--  -->\n     As some of the variable have high p-values. Such variables are insignificant and should be dropped.\n     Hence, the model is stable for R&D, Marketing.","fbbefb1c":"#### Visualising Categorical Variables","18c9bb73":"### Residual analysis ","c8cce171":"Inference: There are no outliers for predictor variable.\n\n    X = [\"Administration\",\"Marketing Spend\",\"R&D Spend\"]","74663c68":"<b>Inference:<\/b> \n\n     1. At \"New York\" profit returns are worse and startup, might end up in loss and its marketing expenditure is \n        high than other states.\n     2. Company profit is higly dependent on the R&D expense as R&D expense is highly correlated with Profit.\n     3. Profits from the states doesn't make a significant change to the company.\n     4. Company spends more on R&D, quality of marketing for the comapany increases. Hence, increase in overall profit.","5beadc8b":"### Gradient descent","56f0620a":"<b>Inference:<\/b> As the model coefficients obtained are same in both cases.","a54768b8":"#### Visualising Numeric Variables","a034eac0":"### Building a linear model","e906ed21":"<b>Inference:<\/b> \n    The features- \"R&D Spend\" and \"Marketing Spend\" are in linear relation with target variable.","e24d5b76":"<b>Inference:<\/b>\n<!--  -->\n    1. The p-value is significant for co-efficients of predictor variable and constant of the regression line. So \n    the association is not purely by chance.\n    2. Prob (F-statistic) tells the model fit is statistically significant, and the explained variance isn't purely \n    by  chance.","f5333273":"### Rescaling the Features","21c80a10":"### Making Predictions Using the Final Model","f01f8b7b":"### Visualise the entire dataset","da021d6a":"<b>Inference:<\/b> On an average return profits are maximum at Florida than other states.","547ef0aa":"## Equation of model","794f200c":"###  Model Evaluation","8372fbb7":"<b>Inference:<\/b> On an average company's administration is same for all states.","550c917b":"### Dividing into X and Y sets for the model building","0a4b5b28":"<b>Inference:<\/b> On an average marketing expense are minimum at California than other states.","e6235f05":"### Splitting the Data into Training and Testing Sets","089f456a":"$ Profit =  0.08 + 0.79  \\times  RD + 0.12  \\times  Marketing $","55c23db4":"### Check for Outliers"}}