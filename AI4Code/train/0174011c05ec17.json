{"cell_type":{"1cd24fd8":"code","097946f6":"code","34dc6fe4":"code","f9543d0c":"code","d83bb28a":"code","c6d79b2e":"code","547bca0d":"code","6e1e7f5e":"code","3153b9a8":"code","c49bf738":"code","cc6ff421":"code","5dd189a6":"code","bc3727b4":"code","61427222":"code","254cfef3":"code","b765cd7a":"code","a3822fd3":"code","935d1be8":"markdown","d7f110f6":"markdown","685f7be3":"markdown","e39b6ded":"markdown","a3989afc":"markdown","18fa250b":"markdown"},"source":{"1cd24fd8":"!pip install -qq timm\n!pip install -qq albumentations==1.0.3","097946f6":"!nvidia-smi","34dc6fe4":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2 as cv\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport timm\nimport glob\nfrom xgboost import XGBClassifier\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport gc\ngc.enable()\n\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","f9543d0c":"def train_transform_object(DIM = 384):\n    return albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.5),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1, 0.1),\n                contrast_limit=(-0.1, 0.1), p=0.5\n            ),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef valid_transform_object(DIM = 384):\n    return albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )","d83bb28a":"class PetDataset(Dataset):\n    \n    def __init__(self, image_paths, dense_features, targets, transform=None):\n        self.image_paths = image_paths\n        self.dense_feats = dense_features\n        self.targets = targets\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        #read the image using the path.\n        img = cv.imread(self.image_paths[index], 1)\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = cv.resize(img, (224, 224), interpolation = cv.INTER_AREA)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n            \n        img = img.float()\n            \n        #get the dense features.\n        dense = self.dense_feats[index, :]\n        \n        #get the label and convert it to 0 to 1.\n        label = torch.tensor(self.targets[index]).float()\n        \n        return (img, dense, label)\n        ","c6d79b2e":"class PetTestset(Dataset):\n    \n    def __init__(self, image_paths, dense_features, transform=None):\n        self.image_paths = image_paths\n        self.dense_feats = dense_features\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        #read the image using the path.\n        img = cv.imread(self.image_paths[index], 1)\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = cv.resize(img, (224, 224), interpolation = cv.INTER_AREA)\n        \n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n            \n        img = img.float()\n            \n        #get the dense features.\n        dense = self.dense_feats[index, :]\n        \n        return (img, dense)","547bca0d":"class PetNet(nn.Module):\n    def __init__(self, model_name, out_features, inp_channels, pretrained, num_dense):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n#         n_features = self.model.classifier.in_features\n#         self.model.classifier = nn.Linear(n_features, 128)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, 128)\n        self.fc = nn.Sequential(\n            nn.Linear(128 + num_dense, 64),\n            nn.ReLU(),\n            nn.Linear(64, out_features)\n        )\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, image, dense):\n        embeddings = self.model(image)\n        x = self.dropout(embeddings)\n        x = torch.cat([x, dense], dim=1)\n        output = self.fc(x)\n        return output","6e1e7f5e":"def usr_rmse_score(output, target):\n    y_pred = torch.sigmoid(output).cpu()\n    y_pred = y_pred.detach().numpy()*100\n    target = target.cpu()*100\n    \n    return mean_squared_error(target, y_pred, squared=False)","3153b9a8":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","c49bf738":"def train_fn(train_loader, model, loss_fn, optimizer, epoch, device, scheduler=None):\n    model.train()\n    stream = tqdm(train_loader)\n    \n    for i, (image, dense, target) in enumerate(stream, start=1):\n        image = image.to(device, non_blocking=True)\n        dense = dense.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True).float().view(-1, 1)\n        \n        output = model(image, dense)\n        loss = loss_fn(output, target)\n        rmse = usr_rmse_score(output, target)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        stream.set_description(f\"Epoch {epoch:02}. Train. Loss {loss}. RMSE {rmse}\")","cc6ff421":"def validation_fn(validation_loader, model, loss_fn, epoch, device):\n    model.eval()\n    stream = tqdm(validation_loader)\n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        for i, (image, dense, target) in enumerate(stream, start=1):\n            image = image.to(device, non_blocking=True)\n            dense = dense.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True).float().view(-1, 1)\n            \n            output = model(image, dense)\n            loss = loss_fn(output, target)\n            rmse_score = usr_rmse_score(output, target)\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. Loss {loss}. RMSE {rmse_score}\")\n            \n            targets = (target.detach().cpu().numpy()*100).tolist()\n            outputs = (torch.sigmoid(output).detach().cpu().numpy()*100).tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n        \n    return final_targets, final_outputs","5dd189a6":"def test_fn(test_loader, model, device):\n    model.eval()\n    stream = tqdm(test_loader)\n    final_outputs = []\n    \n    with torch.no_grad():\n        for i, (image, dense) in enumerate(stream, start=1):\n            image = image.to(device, non_blocking=True)\n            dense = dense.to(device, non_blocking=True)\n            output = model(image, dense)\n            outputs = (torch.sigmoid(output).detach().cpu().numpy()*100).tolist()\n            final_outputs.extend(outputs)\n        \n    return final_outputs","bc3727b4":"best_models_of_each_fold = []\nrmse_tracker = []\nFOLDS = 10\nEPOCHS = 10\n\ndef get_dataset(df, images, state='training'):\n    ids = list(df['Id'])\n    image_paths = [os.path.join(images, idx + '.jpg') for idx in ids]\n    df['Pawpularity'] = df['Pawpularity']\/100\n    target = df['Pawpularity'].values\n    df.drop(['Id', 'Pawpularity', 'kfold'], inplace=True, axis=1)\n    dense_feats = df.values\n\n    if state == 'training':\n        transform = train_transform_object(224)\n    elif state == 'validation' or state == 'testing':\n        transform = valid_transform_object(224)\n    else:\n        transform = None\n\n    return PetDataset(image_paths, dense_feats, target, transform)\n\nfor fold in range(FOLDS):    \n    data = pd.read_csv('..\/input\/kfolddatasets\/train_{}folds.csv'.format(FOLDS))\n\n    train = data[data['kfold'] != fold]\n    val = data[data['kfold'] == fold]\n    images = '..\/input\/petfinder-pawpularity-score\/train'\n\n    train_dataset = get_dataset(train, images)\n    val_dataset = get_dataset(val, images, state='validation')\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n    model_params = {\n        'model_name' : 'swin_small_patch4_window7_224',\n        'out_features' : 1,\n        'inp_channels' : 3,\n        'pretrained' : True,\n        'num_dense' : 12,\n    }\n    model = PetNet(**model_params)\n    model = model.to(device)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-6, amsgrad=False)\n\n    best_rmse = np.inf\n    best_epoch = np.inf\n    best_model_name = None\n    for epoch in range(EPOCHS):\n        train_fn(train_loader, model, loss_fn, optimizer, epoch, device)\n        valid_targets, predictions = validation_fn(val_loader, model, loss_fn, epoch, device)\n        rmse = round(mean_squared_error(valid_targets, predictions, squared=False), 3)\n\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_epoch = epoch\n            if best_model_name is not None:\n                os.remove(best_model_name)\n            torch.save(model.state_dict(),f\"{model_params['model_name']}_{fold}_fold_{epoch}_epoch_{rmse}_rmse.pth\")\n            best_model_name = f\"{model_params['model_name']}_{fold}_fold_{epoch}_epoch_{rmse}_rmse.pth\"\n\n            print(f'The Best saved model is: {best_model_name}')\n            \n    best_models_of_each_fold.append(best_model_name)\n    rmse_tracker.append(best_rmse)\n    print(''.join(['#']*50))\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n        \n        ","61427222":"predicted_labels = None\nmodels_dir = '.\/'\nmodel_params = {\n    'model_name' : 'swin_small_patch4_window7_224',\n    'out_features' : 1,\n    'inp_channels' : 3,\n    'num_dense' : 12,\n    'pretrained' : False\n}\n\ndef get_testset(df, images):\n    ids = list(df['Id'])\n    image_paths = [os.path.join(images, idx + '.jpg') for idx in ids]\n    df.drop(['Id'], inplace=True, axis=1)\n    dense_feats = df.values\n    test_transform = valid_transform_object()\n    return PetTestset(image_paths, dense_feats, test_transform)\n\noutputs = None\nfor model_name in glob.glob(models_dir + '\/*.pth'):\n    model = PetNet(**model_params)\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(device)\n    \n    test_images = '..\/input\/petfinder-pawpularity-score\/test'\n    test_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\n    testset = get_testset(test_df, test_images)\n    test_loader = DataLoader(testset, batch_size=16, shuffle=False)\n    \n    if outputs is None:\n        outputs = test_fn(test_loader, model, device)\n    else:\n        temp = test_fn(test_loader, model, device)\n        for i in range(len(temp)):\n            outputs[i].append(temp[i][0])\n            \nfor i in range(len(outputs)):\n    outputs[i] = [sum(outputs[i]) \/ (len(glob.glob(models_dir + '\/*.pth')))]","254cfef3":"sub_csv = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\nfor i in range(len(outputs)):\n    sub_csv.loc[i, 'Pawpularity'] = outputs[i][0]","b765cd7a":"sub_csv.to_csv('submission.csv', index=False)","a3822fd3":"sub_csv.head()","935d1be8":"## Dataset","d7f110f6":"# Training","685f7be3":"This notebook walks you through the training process of the model. Efficient net version 2 proved to be better than visual transformers. Hence I have used efficient net from timm python library.\n\nFor the inference part checkout [this](https:\/\/www.kaggle.com\/nayakroshan\/fork-of-inference-pawpularity) particular notebook.","e39b6ded":"## Model","a3989afc":"## Metric","18fa250b":"# Augmentation"}}