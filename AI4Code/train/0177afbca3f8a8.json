{"cell_type":{"30536efd":"code","5b13e279":"code","daa279c1":"code","7f5d3456":"code","0f7fae86":"code","5dba45c1":"code","544f3ac8":"code","76b0191c":"code","dd478c70":"code","36d2591e":"markdown","9cd8389a":"markdown","532238b9":"markdown","4f07c21e":"markdown","0155880e":"markdown","1a6c639d":"markdown"},"source":{"30536efd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5b13e279":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import auc,matthews_corrcoef,make_scorer\nfrom sklearn.svm import SVC","daa279c1":"# Read the train and test datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\",index_col=0)","7f5d3456":"def count_aa(seq,\n             aa_order=[\"A\",\"R\",\"N\",\"D\",\"C\",\"E\",\"Q\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"Y\",\"V\"],\n             normalize_count=True):\n    \"\"\"\n    This function counts the occurrence of characters defined in 'aa_order'. The parameter \n    'normalize_count' defines if we normalize (divide by string length) the occurrence of\n    a character.\n    \n    The function returns a pandas series for each character defined in 'aa_order' in the \n    specified order of this list.\n    \"\"\"\n    \n    # Do we need to normalize?\n    if normalize_count:\n        # List containing the character counts - normalized\n        counted_aa = [seq.count(aa)\/len(seq) for aa in aa_order]\n    else:\n        # List containing the character counts\n        counted_aa = [seq.count(aa) for aa in aa_order]\n    \n    # Return a pandas Series, with the 'aa_order' as names\n    return pd.Series(counted_aa,index=aa_order)\n\ndef extract_rolling_features(seq,lib={}, lib_name=\"feat\", num_aas=5):\n    \"\"\"\n    This function first converts the sequence to a numeric vector; as described in the\n    provided dict library. Then it divides the vector into n splits (= seq length) with\n    a specific amount (= num aas) looking ahead. For each split we take the sum and\n    for these summed values of each split we return the maximum in a pandas series.\n    \"\"\"\n    # Get the vector with numbers according to lib (so: seq to --> numeric vector of lib)\n    feature_seqs = pd.Series([lib[aa] for aa in seq if aa in lib.keys()])\n    # Return maximum window according to lib\n    return pd.Series(feature_seqs.rolling(num_aas).sum().max(),index=[\"%s_%s\" % (lib_name,num_aas)])\n\nlib = {\n        \"A\" : 1.800,\n        \"R\" : -4.500,\n        \"N\" : -3.500,\n        \"D\" : -3.500,\n        \"C\" : 2.500,\n        \"E\" : -3.500,\n        \"Q\" : -3.500,\n        \"G\" : -0.400,\n        \"H\" : -3.200,\n        \"I\" : 4.500,\n        \"L\" : 3.800,\n        \"K\" : -3.900,\n        \"M\" : 1.900,\n        \"F\" : 2.800,\n        \"P\" : -1.600,\n        \"S\" : -0.800,\n        \"T\" : -0.700,\n        \"W\" : -0.900,\n        \"Y\" : -1.300,\n        \"V\" : 4.200\n}\n\n# We might need to scale features, for now this is not performed\nX = []\nX.append(train[\"sequence\"].apply(extract_rolling_features,lib=lib,lib_name=\"hydrophob_rolling\"))\nX.append(train[\"sequence\"].apply(count_aa))\nX = pd.concat(X,axis=1)\n\ny = train[\"target\"]\n\n# We might need to scale features, for now this is not performed\nX_test = []\nX_test.append(test[\"sequence\"].apply(extract_rolling_features,lib=lib,lib_name=\"hydrophob_rolling\"))\nX_test.append(test[\"sequence\"].apply(count_aa))\nX_test = pd.concat(X_test,axis=1)","0f7fae86":"print(\"\\nThe head of the X-matrix (training set):\\n\")\nprint(X.head(5))\nprint(\"\\nThe shape of the X-matrix (training set):\\n\")\nprint(X.shape)\nprint(\"\\n\\n\\nThe head of the y-vector:\\n\")\nprint(y.head(5))\nprint(\"\\nThe shape of the y-vector (training set):\\n\")\nprint(y.shape)","5dba45c1":"print(\"\\nThe head of the X-matrix (test set):\\n\")\nprint(X_test.head(5))","544f3ac8":"# Define some hyperparameters we are going to test\nparam_dist = {  \n        \"C\": [0.01,0.1,1.0,10.0,50.0,100.0,150.0],\n        \"solver\" : [\"lbfgs\"]\n    }\n\n# Use MCC scoring\nmcc = make_scorer(matthews_corrcoef)\n\n# Make a handle for our model (initialization or our model without fitted parameters)\nlogreg_model_handle = LogisticRegression()\n\n# Define a CV strategy\ncv = StratifiedKFold(n_splits=10,\n                     shuffle=True,\n                     random_state=42)\n\n# Define how we are going to fit our model parameters using the CV, hyperparameters and what evaluation metric\ngrid_search = GridSearchCV(logreg_model_handle,\n                           param_grid=param_dist,\n                           verbose=0,\n                           scoring=mcc,\n                           n_jobs=1,\n                           refit=True,\n                           cv=cv)\n\n# Fit the model parameters using the earlier defined search\nrandom_search_res = grid_search.fit(X,y)\n\nprint(\"Best performance: %s\" % (random_search_res.best_score_))\nprint(\"With the hyperparameters: %s\" % (random_search_res.best_params_))","76b0191c":"lr_model = random_search_res.best_estimator_","dd478c70":"# Predict probabilities of belonging to either classes \npredictions_test = lr_model.predict_proba(X_test)[:,1]\n\n# Create a dataframe with predictions\n# Only problem is that we need to convert it to integers. This is now done by rounding and converting to integers.\n# In future it might be interesting to determine the optimal threshold for the probability and assigning either of\n# the two classes.\npredictions_test_df = pd.DataFrame({\"index\":X_test.index,\"target\":list(map(int,map(round,predictions_test)))})\n\n# Write predictions to a file\npredictions_test_df.to_csv(\"predictions.csv\",index=False)","36d2591e":"With the fitted model lets make some predictions on the test set.","9cd8389a":"# Test set predictions","532238b9":"# Feature extraction","4f07c21e":"# Fitting the model","0155880e":"Now that we have our feature matrix (X-matrix) and our targets we can start fitting our model parameters.","1a6c639d":"Now that we extracted the features. Lets have a look at the results. Looking at the shape of you matrix or vector can give a lot of clues if something went wrong. "}}