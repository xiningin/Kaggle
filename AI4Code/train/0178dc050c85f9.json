{"cell_type":{"ff5c5df1":"code","6737f990":"code","56fa3d9c":"code","88dc696f":"code","3c46a710":"code","efeea86e":"code","bde178a5":"code","3cbed223":"code","f29913b7":"code","54b608e4":"code","7c35d7a1":"code","a2e07ebc":"code","8d22fd64":"code","33032c1b":"code","f69c9b95":"code","e2f3f58e":"code","93905edd":"code","6d292e63":"code","83e1677a":"code","e9b2a813":"code","f1cb1ce6":"code","bb19abc8":"code","98ac139d":"code","12c6e884":"code","cbae5db8":"code","8083a4c3":"code","d0b21539":"code","aadce69c":"code","7325a2f9":"code","ed477fee":"code","5886d475":"code","a475d1cf":"code","eafeef2f":"code","de1b1a5a":"code","c60a0da3":"code","f6012bb8":"code","8653291f":"code","dbda0296":"code","cc61d359":"code","5fc1aae4":"code","df81c9e3":"markdown","909d7b0a":"markdown","81e2f62e":"markdown","fb3a120e":"markdown"},"source":{"ff5c5df1":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport numpy as np\nimport io\nimport tensorflow as tf\n\nfrom keras import initializers\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras import layers\nfrom sklearn.feature_selection import SelectKBest, f_regression, SelectPercentile\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler","6737f990":"cityAttributes = pd.read_csv('..\/input\/historical-hourly-weather-data\/city_attributes.csv')\ncityAttributes.iloc[:5]","56fa3d9c":"humidity = pd.read_csv('..\/input\/historical-hourly-weather-data\/humidity.csv')\nhumidity.iloc[:5]","88dc696f":"pressure = pd.read_csv('..\/input\/historical-hourly-weather-data\/pressure.csv')\npressure.iloc[:5]","3c46a710":"temperature = pd.read_csv('..\/input\/historical-hourly-weather-data\/temperature.csv')\ntemperature.iloc[:5]","efeea86e":"weather = pd.read_csv('..\/input\/historical-hourly-weather-data\/weather_description.csv')\nBsmtQual = {'heavy shower snow': 0, 'heavy snow': 0, 'light shower snow': 1, 'light snow': 1, 'shower snow': 2, 'sleet': 2, 'light rain and snow': 2, ' light shower sleet': 2, \n            'fog': 3, 'haze':3, 'mist': 3, 'thunderstorm with heavy rain': 5, 'heavy intensity shower rain': 5, 'thunderstorm with rain': 5, 'very heavy rain': 5, \n            'ragged thunderstorm': 6, 'proximity thunderstorm': 6, 'smoke': 6, 'moderate rain': 6, 'heavy intensity rain': 6, 'thunderstorm with light rain': 6, \n            'shower rain': 7, 'thunderstorm': 7, 'proximity shower rain': 7, 'light intensity drizzle rain': 8, 'light intensity drizzle': 8, ' volcanic ash': 8, \n            'dust': 8, 'overcast clouds': 10, 'light intensity shower rain': 6, 'light rain': 7, 'broken clouds': 11, 'scattered clouds': 11,'few clouds': 11, 'sky is clear': 13}\nfor i in weather.columns.tolist():\n    weather[i] = weather[i].map(BsmtQual)\nweather\nweather.iloc[:5]","bde178a5":"windSpeed = pd.read_csv('..\/input\/historical-hourly-weather-data\/wind_speed.csv')\nwindSpeed.iloc[:5]","3cbed223":"# \u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0442\u0430\u0431\u043b\u0438\u0446\nprint(cityAttributes.shape)\nprint(humidity.shape)\nprint(pressure.shape)\nprint(temperature.shape)\nprint(weather.shape)\nprint(windSpeed.shape)","f29913b7":"# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044e \u0433\u043e\u0440\u043e\u0434\u043e\u0432 \u0438 \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \"datetime\"\ndel humidity['datetime']\nhumidity = humidity.rename(columns = lambda x: x.replace(x, x + 'H'))\ndel pressure['datetime']\npressure = pressure.rename(columns = lambda x: x.replace(x, x + 'P'))\ndel temperature['datetime']\ntemperature = temperature.rename(columns = lambda x: x.replace(x, x + 'T'))\ndel weather['datetime']\nweather = weather.rename(columns = lambda x: x.replace(x, x + 'W'))\ndel windSpeed['datetime']\nwindSpeed = windSpeed.rename(columns = lambda x: x.replace(x, x + 'Wd'))\n# \u0421\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\ntemperature = temperature - 273\ntemperature","54b608e4":"# \u0421\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u0435\nallData = pd.concat([humidity, pressure, temperature, weather], axis = 1)\nallData = allData.iloc[37000:42000].reset_index()\ndel allData['index']\nallData","7c35d7a1":"# \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\nallDataNa = allData.isnull().sum()\nallDataNA = allDataNa.drop(allDataNa[allDataNa == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :allDataNa})\nmissing_data.iloc[0:5]","a2e07ebc":"# \u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\nallData = allData.fillna(method='ffill')\nallData = allData.fillna(method='bfill')\nallDataNa = allData.isnull().sum()\nallDataNA = allDataNa.drop(allDataNa[allDataNa == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :allDataNa})\nmissing_data.iloc[0:5]","8d22fd64":"# \u0420\u0430\u0437\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435\ntrainData, testData = train_test_split(allData, test_size = 0.2, random_state = 42, shuffle = False)","33032c1b":"# \u041f\u0435\u0440\u0435\u0438\u043d\u0434\u0435\u043a\u0441\u0430\u0446\u0438\u044f\ntrainData = trainData.reset_index()\ndel trainData['index']\ntestData = testData.reset_index()\ndel testData['index']","f69c9b95":"# \u0420\u0430\u0437\u0431\u0438\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u0446\u0435\u043b\u044c\ntrainDataX = trainData.drop(['VancouverT'], axis = 1)\ntrainDataY = trainData['VancouverT']\ntestDataX = testData.drop(['VancouverT'], axis = 1)\ntestDataY = testData['VancouverT']","e2f3f58e":"# \u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435\nmean = trainDataX.mean(axis=0)\nstd = trainDataX.std(axis=0)\n# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\ntrainDataXN = (trainDataX - mean)\/std\ntestDataXN = (testDataX - mean)\/std","93905edd":"# \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u0430 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\npcaX = PCA(n_components = 15)\npcaX.fit(trainDataXScal)\ntrainDataXP = pd.DataFrame(pcaX.transform(trainDataX))\ntestDataXP = pd.DataFrame(pcaX.transform(testDataX))\n# \u0412\u044b\u0432\u043e\u0434 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u043e\u0432\nsum((pcaX.explained_variance_ratio_[i] for i in range(0, len(pcaX.explained_variance_ratio_))))","6d292e63":"fSelect = SelectPercentile(f_regression, percentile = 15)\nfSelect.fit(trainDataXScal, trainDataY)\ntrainFData = pd.DataFrame(fSelect.transform(trainDataXN))\ntestFData = pd.DataFrame(fSelect.transform(testDataXN))","83e1677a":"trainDataXCor = pd.concat([trainDataXP, trainFData], axis = 1)\ntestDataXCor = pd.concat([testDataXP, testFData], axis = 1)","e9b2a813":"# \u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438\ncorrelation = trainDataXCor.corr()\nplt.figure(figsize = (20,20))\nsns.heatmap(correlation, vmax = 1, square = True, annot = True, cmap = 'Blues')","f1cb1ce6":"# \u041a\u043b\u0430\u0441\u0441 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0445\u043e\u0434\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 30 == 0: print('')\n        print('.', end = '')","bb19abc8":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\nfrom keras.optimizers import RMSprop, Adam, SGD, Nadam\nfrom keras.layers.advanced_activations import *\nfrom numpy.random import seed\nseed(1)\ntf.compat.v1.set_random_seed(1)","98ac139d":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043a\u0440\u043e\u0441\u0441 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ndef crossValdation(modelTF, dataSet, arg, printD):\n    trainDataX = trainData.drop([arg], axis = 1)\n    trainDataY = trainData[arg]\n    epohs = 200\n    batch = 100\n    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n    histMean = pd.DataFrame({'loss':[], 'mse':[], 'mae':[], 'val_loss':[], 'val_mse':[], 'val_mae':[], 'epoch':[]})\n    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n    for trainIndex, validIndex in skf.split(trainDataX, trainDataY.astype(int)):\n        trainX = trainDataX.iloc[trainIndex]\n        trainY = trainDataY.iloc[trainIndex]\n        validX = trainDataX.iloc[validIndex]\n        validY = trainDataY.iloc[validIndex]\n        modelG = modelTF(trainX.shape[1])\n        history = modelG.fit(trainX, trainY, validation_data = (validX, validY), batch_size = batch, epochs = epohs, verbose = 0, callbacks = [early_stop, printD])\n        hist = pd.DataFrame(history.history)\n        hist['epoch'] = history.epoch\n        histMean = histMean.append(hist.iloc[[-1]], ignore_index = True)\n    \n    del histMean['epoch']    \n    tir = pd.DataFrame({'loss':['---'], 'mse':['---'], 'mae':['---'], 'val_loss':['---'], 'val_mse':['---'], 'val_mae':['---']})\n    histM = pd.DataFrame(histMean.mean()).T\n    histMean = histMean.append(tir)\n    histMean = histMean.append(histM)\n    return histMean, modelG, hist","12c6e884":"def createModelG(inputShape):\n    model = Sequential()\n    model.add(Dense(4096, input_dim = inputShape, \n                    kernel_initializer = initializers.glorot_uniform(seed = 1), \n                    kernel_regularizer = keras.regularizers.l2(0.01), activation = \"relu\")) \n    model.add(Dense(2048, \n                    kernel_initializer = initializers.glorot_uniform(seed = 1), activation = \"relu\"))\n    model.add(Dense(2048, \n                    kernel_initializer = initializers.glorot_uniform(seed = 1), activation = \"relu\"))\n    model.add(Dense(1024, \n                    kernel_initializer = initializers.glorot_uniform(seed = 1), activation = \"relu\"))\n    model.add(Dense(1024, \n                    kernel_initializer = initializers.glorot_uniform(seed = 1), activation = \"relu\"))\n    model.add(layers.Dropout(0.05))\n    model.add(Dense(1))\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.000001)\n    model.compile(loss = 'mse', optimizer = optimizer, metrics = [\"mse\", \"mae\"])\n    return model","cbae5db8":"startModelTest = crossValdation(createModelG, trainDataXCor, 'VancouverT', PrintDot())\nmodelTest = startModelTest[1]\nhist = startModelTest[2]\nstartModelTest[0]","8083a4c3":"pred = modelTest.predict(testDataX).flatten()\ndifference = pred - testDataY.values\nprint(sum(abs(difference))\/len(difference))","d0b21539":"def plot_history():\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Abs Error')\n    plt.plot(hist['epoch'], hist['mae'], label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error')\n    plt.legend()\n    plt.ylim([0,20])\n\nplot_history()","aadce69c":"for i in range(0, 1000, 50):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(pred[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","7325a2f9":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u043e\u0431\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435\u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445\nreg = LinearRegression().fit(trainDataXN, trainDataY)\npredL = reg.predict(testDataXN)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predL[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","ed477fee":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifference = predL - testDataY.values\nprint(sum(abs(difference))\/len(difference))","5886d475":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u043e\u0431\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445\nreg = LinearRegression().fit(trainDataXScal, trainDataY)\npredS = reg.predict(testDataXScal)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predS[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","a475d1cf":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifferenceS = predS - testDataY.values\nprint(sum(abs(differenceS))\/len(differenceS))","eafeef2f":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 F \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439\nreg = LinearRegression().fit(trainFData, trainDataY)\npredF = reg.predict(testFData)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predF[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","de1b1a5a":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifferenceF = predF - testDataY.values\nprint(sum(abs(differenceF))\/len(differenceF))","c60a0da3":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 PCA\nreg = LinearRegression().fit(trainDataXScalPCA, trainDataY)\npredP = reg.predict(testDataXScalPCA)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predP[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","f6012bb8":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifferenceP = predP - testDataY.values\nprint(sum(abs(differenceP))\/len(differenceP))","8653291f":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 PCA \u0441 F\nreg = LinearRegression().fit(correlationDataX, trainDataY)\npredPF = reg.predict(correlationDataY)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predPF[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","dbda0296":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifferencePF = predPF - testDataY.values\nprint(sum(abs(differencePF))\/len(differencePF))","cc61d359":"rfr = RandomForestRegressor(n_estimators = 5, criterion = 'mae', max_depth = 10).fit(trainDataX, trainDataY)\npredRFR = rfr.predict(testDataX)\nfor i in range(0, 1000, 100):\n    print(\"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(predRFR[i], 3), \", \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u0430\u044f \u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430:\", round(testDataY.values[i], 3))","5fc1aae4":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u0433\u0440\u0435\u0448\u043d\u043e\u0441\u0442\u0438 (\u0432 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u0445)\ndifferenceRFR = predRFR - testDataY.values\nprint(sum(abs(differenceRFR))\/len(differencePF))","df81c9e3":"# # **\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445**","909d7b0a":"# **\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0438**","81e2f62e":"# **\u0420\u0435\u0434\u0430\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445**","fb3a120e":"# **\u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a**"}}