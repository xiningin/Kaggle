{"cell_type":{"7aaffc2d":"code","8b4ded8b":"code","01b83d0c":"code","1be00279":"code","101425ba":"code","24549b07":"code","1ff77041":"code","8cdbcba7":"code","afff7333":"code","d0bd0a6b":"code","962cce53":"code","8f2a4670":"code","1d790deb":"code","58224c3b":"code","cdba1f75":"code","bee38ca9":"code","cd6d06d1":"code","49e97e7f":"code","14b8a836":"code","8ac05506":"code","9ce2b2cd":"code","c0004fe0":"code","9e639984":"code","c4b7b473":"code","a879f1dc":"code","3e58c6c2":"code","2f098b32":"code","f929eea1":"code","0222dd54":"code","64172d01":"code","b76c6fc3":"code","ed4cbb18":"code","80ac9c70":"code","025ea4a0":"code","15de6199":"code","5c7bd0b3":"code","5aa26b41":"code","b97895ea":"code","f3afc467":"code","f58419a2":"code","37298a70":"code","d87c4d47":"code","2d3d46b5":"code","97d35954":"code","4af03edf":"code","4803277f":"code","cfbdd1c9":"code","597e36e7":"code","1955cef7":"code","2ee8721a":"code","3c5251df":"code","a56c081f":"code","796e3ef9":"code","4284a267":"code","9cec44d4":"code","529598bb":"code","52bfe522":"code","a9fc5a4c":"code","41a8b1bf":"code","5a20f659":"code","2348af3c":"code","8e253ab9":"code","4b2306f8":"code","4f170020":"code","53a73e64":"code","7d79c32f":"code","89b0196d":"code","942d7377":"code","eff57e82":"code","9dfcccad":"code","09af45b2":"code","adeacb89":"code","fecf1ecf":"code","e0e0f9e7":"code","d0619f52":"code","4db8bc01":"code","e9d6d3f8":"code","09ec3371":"code","10a5a5b9":"code","be074175":"code","2b0f657b":"code","3f845078":"code","9597b3bf":"code","25f708ea":"code","1dbd8f1c":"code","ed6a8256":"code","4d10b5c5":"code","fdc6c828":"code","9f3b9277":"code","bbc1f67f":"code","8ea3c770":"code","7a4494d6":"code","2e7425f5":"code","7d2988bd":"code","441c7025":"code","8ad87edf":"code","b39bf0e6":"code","a5a982ac":"code","dd97c813":"code","d0d7ae9e":"code","69ffa7e7":"code","1bdb2638":"code","082e4016":"code","135fefc3":"code","0b3439c9":"code","f47d8a13":"code","03e44d49":"code","8bd17709":"code","a861279b":"code","56b77ca8":"code","8c62dea2":"code","904132c0":"code","3ecc94b5":"code","f728f9b6":"code","e9c9d031":"code","30456209":"code","3ceb84f8":"code","ff305893":"code","3f4f1ba2":"code","497ec8ee":"code","f12355fa":"code","97147155":"code","aa490cfb":"code","3a077e9d":"code","4f3d6fe8":"code","aeb60791":"code","1ae96617":"code","7583e93e":"code","02b0d51d":"code","bf920a19":"code","a43cfe41":"code","b7684066":"code","c29628a9":"code","d38f0060":"code","06720eed":"code","338b7d35":"code","cbf50668":"code","506e3217":"code","aa5cb342":"code","b7a73b71":"code","a4dfd040":"code","82db11be":"code","48b3e358":"code","3d4671db":"code","4c56abec":"code","cc34aa48":"code","1a560bf7":"code","ca8801b4":"code","9b021fb0":"code","f9b25785":"code","c4696c22":"code","88e36993":"code","b8f7dead":"code","6e174dd0":"code","c44888ed":"code","6ad61bbc":"code","93ebc9e9":"code","dd98cc15":"code","dfd5f951":"code","7dcabf1f":"code","7f475e03":"code","36946d8b":"code","3c96acb9":"code","ab6d5806":"code","86416a9b":"markdown","1e801f76":"markdown","cae97f72":"markdown","25ef4ff8":"markdown","9eb21854":"markdown","b5982f36":"markdown","f167ee3e":"markdown","946e2c02":"markdown","9920c1a1":"markdown","129bd71b":"markdown","0285600a":"markdown","fdc27506":"markdown","d6c0b4ba":"markdown","5cdb3c8d":"markdown","459017f6":"markdown","afa3c018":"markdown","2cb926d5":"markdown","b40747bc":"markdown","5b0ed4ff":"markdown","26def42b":"markdown","45f2fe34":"markdown","320a4e78":"markdown","dd6a9d23":"markdown","59f0b4c4":"markdown","6acabd38":"markdown","7b115621":"markdown","bacb4987":"markdown","91e72e3d":"markdown","afed7738":"markdown","a0bdd145":"markdown","45b35731":"markdown","f1fc135a":"markdown","9fac7d64":"markdown","a404614c":"markdown","b50fde6f":"markdown","4feb9f2a":"markdown","5433fc31":"markdown","66106a74":"markdown","a04daeea":"markdown","3701dfd8":"markdown","4cf2ccc2":"markdown","8af37eec":"markdown","b43f3800":"markdown","a9c3f922":"markdown","73d717cd":"markdown","5fa8d360":"markdown","d8c7ce4f":"markdown","f227de23":"markdown","2a8e786b":"markdown","90302363":"markdown","4ef6dcfe":"markdown","0bef7f84":"markdown","ca2ed8eb":"markdown","769be52d":"markdown","6dd49629":"markdown","630c9786":"markdown","693cf3e3":"markdown","bc45ff4d":"markdown","6793625c":"markdown","8236f81e":"markdown","21ca29bd":"markdown","8f249f54":"markdown","025d48a0":"markdown","71c88cfa":"markdown","769d108b":"markdown","b50de6f2":"markdown","94cefa7c":"markdown"},"source":{"7aaffc2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b4ded8b":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import skew","01b83d0c":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","1be00279":"sample_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","101425ba":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntrain_df","24549b07":"test_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\")\ntest_df","1ff77041":"train_df.info()","8cdbcba7":"test_df.info()","afff7333":"train_df.describe().T","d0bd0a6b":"train_df = train_df.sample(frac=0.4)","962cce53":"outliers_df = pd.DataFrame({\"skewness\": train_df.drop(\"id\", axis=1).skew(),\n                            \"kurtosis\": train_df.drop(\"id\", axis=1).kurtosis()})\n\noutliers_df = outliers_df.sort_values(by='skewness', ascending=False)\noutliers_df","8f2a4670":"fig, axes = plt.subplots(figsize=(14,4), nrows=1, ncols=2)\n\n#kurtosis = outliers_df['']\n\nsns.barplot(x=outliers_df.index, y=outliers_df['skewness'].values, palette=\"mako_r\", ax=axes[0])\nsns.barplot(x=outliers_df.index, y=outliers_df['kurtosis'].sort_values(ascending=False).values,\n            palette='mako_r',ax=axes[1])\n\n\naxes[0].set_title(\"Skewness\")\naxes[0].set_xticklabels(outliers_df.index,rotation=45)\n\n\naxes[1].set_title(\"Kurtosis\")\naxes[1].set_xticklabels(outliers_df.index, rotation=45);\n","1d790deb":"sns.distplot(train_df['target'], kde=True, color='r')","58224c3b":"sns.boxplot(x=train_df['target'])","cdba1f75":"ser = pd.Series(train_df['target'])\nser.describe()","bee38ca9":"q1 = np.quantile(train_df['target'],0.25)\nq3 = np.quantile(train_df['target'],0.75)\n\niqr = q3-q1\n\nlower_outlier = q1 - (1.5*iqr)\nupper_outlier = q3 + (1.5*iqr)","cd6d06d1":"train_df_without_outliers = train_df[train_df['target'] >= lower_outlier].copy()","49e97e7f":"len(train_df_without_outliers)","14b8a836":"sns.boxplot(x=train_df_without_outliers['target'])","8ac05506":"sns.distplot(train_df_without_outliers['target'], kde=True)","9ce2b2cd":"fig, ax = plt.subplots(figsize=(12,8), dpi=150)\ncorr_matrix = train_df.drop('id',axis=1).corr()\n\nsns.heatmap(corr_matrix, mask=corr_matrix < 0.8, annot=True, ax=ax, cmap='coolwarm')","c0004fe0":"corr_matrix['target'].sort_values(ascending=False)","9e639984":"train_df.drop(\"cont12\", axis=1, inplace=True)\ntest_df.drop(\"cont12\", axis=1, inplace=True)","c4b7b473":"corr_with_target = train_df.corr()['target'].sort_values(ascending=False)[1:]\ncorr_with_target = corr_with_target.drop('id')","a879f1dc":"import matplotlib as mpl","3e58c6c2":"colors = list(reversed(px.colors.qualitative.Dark24))\n#colors = list(reversed(['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']))\n\nfig = go.Figure(go.Bar(\n    x = corr_with_target.values,\n    y = corr_with_target.index,\n    text = corr_with_target.values,\n    textposition =\"auto\",\n    texttemplate = \"%{value:,s}\",\n    marker_color = colors,\n    orientation = \"h\",\n))\nfig.update_traces(\n    #marker_line_color = \"black\",\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Features Correlation to the Target Column\"\n)\nfig.show()","2f098b32":"fig, ax = plt.subplots(figsize=(12,8), dpi=150)\nsns.heatmap(train_df.drop('id',axis=1).corr(method='spearman'), annot=True, ax=ax, cmap='coolwarm')","f929eea1":"num_cols = train_df.drop([\"id\", \"target\"], axis=1).columns\n\nskewed_feat = train_df[num_cols].skew().sort_values(ascending=False)\nskewed_feat = skewed_feat[skewed_feat > 0.5]\nskewed_index = skewed_feat.index","0222dd54":"for col in skewed_index:\n    q3 = np.quantile(train_df[col], 0.75)\n    q1 = np.quantile(train_df[col], 0.25)\n    iqr = q3 - q1\n    upper_limit = q3 + (1.5*iqr)\n    lower_limit = q1 - (1.5*iqr)\n    \n    upper_col_bool = train_df[col].apply(lambda x: x <= upper_limit)\n    lower_col_bool = train_df[col].apply(lambda x: x >= lower_limit)\n    \n    clean_train_df = train_df[upper_col_bool]\n    clean_train_df = train_df[lower_col_bool]","64172d01":"X = clean_train_df.drop(['target','id'], axis=1)\ny = clean_train_df['target']\ntest = test_df.drop(\"id\", axis=1).values\nid_col = test_df['id'].values","b76c6fc3":"sc = StandardScaler()\nscaled_train = sc.fit_transform(X)\nscaled_train = pd.DataFrame(scaled_train, columns=train_df.drop(['target','id'], axis=1).columns)\n\nscaled_test = sc.transform(test)\nscaled_test = pd.DataFrame(scaled_test, columns=test_df.drop(\"id\", axis=1).columns)","ed4cbb18":"from sklearn.metrics import r2_score","80ac9c70":"def evaluateModel(model):\n    \"\"\"\n    This function evaluate the model with\n    mean absolute error and root mean squared error\n    \"\"\"\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    score_r2 = r2_score(y_test, y_pred)\n    \n    print(f\"MAE: {mae}\")\n    print(f\"RMSE: {rmse}\")\n    print(f\"R-square: {score_r2}\")\n    \n    return mae, rmse, score_r2, y_pred, model","025ea4a0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scaled_train, y, \n                                                    test_size=0.3, \n                                                    random_state=42)","15de6199":"from sklearn.linear_model import ElasticNetCV","5c7bd0b3":"elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], tol=0.01)","5aa26b41":"el_cv_mae, le_cv_rmse, lr_cv_r2, lr_y_pred, elastic_model = evaluateModel(elastic_model)","b97895ea":"from sklearn.linear_model import LassoCV","f3afc467":"lassoCV_model = LassoCV(eps=0.01, n_alphas=100,cv=10)","f58419a2":"la_cv_mae, la_cv_rmse, la_cv_r2, la_y_pred, lassoCV_model = evaluateModel(lassoCV_model)","37298a70":"lassoCV_model.get_params()","d87c4d47":"# Mean of target column\ntrain_df['target'].mean()","2d3d46b5":"residuals = pd.Series(y_test - la_y_pred,name='residuals')","97d35954":"residuals","4af03edf":"sns.scatterplot(x=y_test, y=residuals)\nplt.axhline(y=0, color='red', ls='--')","4803277f":"sns.distplot(residuals, bins=40, kde=True)","cfbdd1c9":"import scipy as sp","597e36e7":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals, plot=ax)","1955cef7":"lassoCV_model.coef_","2ee8721a":"coef_ser = pd.Series(lassoCV_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)","3c5251df":"colors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in LassoCV\"\n)\nfig.show()","a56c081f":"# Let's check if our model is the best model","796e3ef9":"from sklearn.linear_model import RidgeCV","4284a267":"ridgeCV_model = RidgeCV(alphas=(0.1, 1.0, 10.0),scoring='neg_mean_absolute_error', cv=10)","9cec44d4":"rid_cv_mae, rid_cv_rmse, rid_cv_r2, rid_y_pred, ridgeCV_model = evaluateModel(ridgeCV_model)","529598bb":"ridgeCV_model.alpha_","52bfe522":"ridgeCV_model.coef_","a9fc5a4c":"residuals_ridge = pd.Series(y_test - rid_y_pred, name='residuals')\nresiduals_ridge","41a8b1bf":"sns.distplot(residuals_ridge, bins=40, kde=True)","5a20f659":"sns.scatterplot(x=y_test, y=residuals_ridge)\nplt.axhline(y=0, color='red',ls='--')","2348af3c":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals_ridge, plot=ax)","8e253ab9":"coef_ser = pd.Series(ridgeCV_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)\n\ncolors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in RidgeCV\"\n)\nfig.show()","4b2306f8":"from sklearn.svm import LinearSVR\n\nsvr_model = LinearSVR(max_iter=1000000)\n\nsvr_mae, svr_rmse, svr_r2, svr_pred, svr_model = evaluateModel(svr_model)","4f170020":"residuals_svr = pd.Series(y_test - svr_pred,name=' LinearSVC residuals')\nresiduals_svr","53a73e64":"sns.distplot(residuals_svr, bins=40,kde=True)","7d79c32f":"sns.scatterplot(x=y_test, y=residuals_svr)\nplt.axhline(y=0, color='red',ls='--')\nplt.xlabel(\"y_actual\")","89b0196d":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals_svr, plot=ax)\n\nplt.title(\"Probability plot\")","942d7377":"svr_model.coef_","eff57e82":"coef_ser = pd.Series(svr_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)\n\ncolors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in LinearSVR\"\n)\nfig.show()","9dfcccad":"from sklearn.linear_model import Lasso, Ridge","09af45b2":"# lasso_model = Lasso(alpha=lassoCV_model.alpha_).fit(scaled_train, y)","adeacb89":"# ridge_model = Ridge(alpha=ridgeCV_model.alpha_).fit(scaled_train, y)","fecf1ecf":"# linearSVR_model = LinearSVR(max_iter=1000000).fit(scaled_train, y)","e0e0f9e7":"#lasso_sub = pd.DataFrame(data={'id': id_col,\n                               #'target':lassoCV_model.predict(scaled_test)})\n\n#ridge_sub = pd.DataFrame(data={'id':id_col,\n                               #'target':ridge_model.predict(scaled_test)})\n\n#linearSVR_sub = pd.DataFrame(data={'id':id_col,\n                                   #'target': linearSVR_model.predict(scaled_test)})\n\n\n#print(len(lasso_sub['id']) == len(sample_sub['id']))\n#print(len(ridge_sub['id']) == len(sample_sub['id']))\n#print(len(linearSVR_sub['id']) == len(sample_sub['id']))","d0619f52":"# lasso_sub.to_csv(\"submission_lasso.csv\", index=False)\n# ridge_sub.to_csv(\"submission_ridge.csv\", index=False)\n# linearSVR_sub.to_csv(\"submission_linearSVR.csv\")","4db8bc01":"from sklearn.model_selection import GridSearchCV\n\nlasso_model = Lasso(max_iter=1000000)\n\nparam_grid = {'alpha': [0.005, 0.02, 0.03, 0.05, 0.06, 0.1, 0.5, 1, 10, 100]}\n\nlasso_grid = GridSearchCV(lasso_model,param_grid, cv=10, scoring='neg_mean_squared_error')\nlasso_grid.fit(X_train, y_train)","e9d6d3f8":"lasso_grid.best_params_","09ec3371":"abs(lasso_grid.best_score_)","10a5a5b9":"lasso_y_grid = lasso_grid.predict(X_test)\n\nlasso_grid_mae = mean_absolute_error(y_test, lasso_y_grid)\n\nlasso_grid_rmse = np.sqrt(mean_squared_error(y_test, lasso_y_grid))","be074175":"lasso_grid_mae","2b0f657b":"lasso_grid_rmse","3f845078":"best_lasso_model = Lasso(alpha=0.005)","9597b3bf":"best_lasso_model.fit(X_train, y_train)","25f708ea":"best_lasso_y_pred = best_lasso_model.predict(X_test)","1dbd8f1c":"best_lasso_mae = mean_absolute_error(y_test, best_lasso_y_pred)\nbest_lasso_rmse = np.sqrt(mean_squared_error(y_test, best_lasso_y_pred))","ed6a8256":"best_lasso_mae","4d10b5c5":"best_lasso_rmse","fdc6c828":"final_best_lasso = Lasso(alpha=0.005)\n\nfinal_best_lasso.fit(scaled_train, y)\n\nbest_lasso_sub = pd.DataFrame({'id': id_col,'target': final_best_lasso.predict(scaled_test)})\n\nbest_lasso_sub.to_csv(\"submission_best_lasso.csv\", index=False)","9f3b9277":"param_grid = {'alpha':[0.01, 0.05, 0.1, 1.0, 10.0],\n              'solver':['auto', 'svd', 'cholesky', 'lsqr', 'saga']}\n\nbest_ridge_model = Ridge(max_iter=1000000)\n\ngrid_ridge = GridSearchCV(best_ridge_model, param_grid, cv=10, \n                          scoring='neg_mean_squared_error', verbose=0)\n\ngrid_ridge.fit(X_train, y_train)","bbc1f67f":"grid_ridge.best_params_","8ea3c770":"abs(grid_ridge.best_score_)","7a4494d6":"# Make predictions","2e7425f5":"ridge_grid_pred = grid_ridge.predict(X_test)\n\nridge_grid_mae = mean_absolute_error(y_test, ridge_grid_pred)\nridge_grid_rmse = np.sqrt(mean_squared_error(y_test, ridge_grid_pred))","7d2988bd":"ridge_grid_mae","441c7025":"ridge_grid_rmse","8ad87edf":"# No Submmission here","b39bf0e6":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression","a5a982ac":"# poly_converter = PolynomialFeatures(degree=3, include_bias=False)\n# poly_features = poly_converter.fit_transform(X)\n    \n# X_train, X_test, y_train, y_test = train_test_split(poly_features,\n                                                    # y,\n                                                    # test_size=0.3,\n                                                    # random_state=42)\n\n# poly_reg_model = LinearRegression()\n# poly_reg_model.fit(X_train, y_train)\n\n# poly_pred = poly_reg_model.predict(X_test)\n\n# poly_mae = mean_absolute_error(y_test, poly_pred)\n# poly_rmse = np.sqrt(mean_squared_error(y_test, poly_pred))\n","dd97c813":"# poly_mae","d0d7ae9e":"# poly_rmse","69ffa7e7":"# full_poly_reg = LinearRegression()\n# full_poly_reg.fit(poly_features, y)\n\n# poly_test_features = poly_converter.transform(test)\n\n# poly_sub = pd.DataFrame({'id': id_col,'target': full_poly_reg.predict(poly_test_features)})\n\n# poly_sub.to_csv(\"submission_poly.csv\", index=False)","1bdb2638":"#from sklearn.svm import SVR\n\n#svr_model = SVR()\n\n#param_grid = {'C':[0.001, 0.1, 1, 10],\n              #'max_iter':[1000, 10000]}\n\n#grid_svr = GridSearchCV(svr_model, param_grid, cv=2,verbose=1)\n\n#grid_svr.fit(X_train, y_train)\n\n#grid_svr_pred = grid_svr.predict(X_test)\n\n#grid_svr_mae = mean_absolute_error(y_test, grid_svr_pred)\n\n#grid_svr_rmse = np.sqrt(mean_squared_error(y_test, grid_svr_pred))","082e4016":"#grid_svr_mae","135fefc3":"#grid_svr_rmse","0b3439c9":"from sklearn.linear_model import SGDRegressor\n\nsgdr_model = SGDRegressor()","f47d8a13":"sgdr_mae, sgdr_rmse, sgdr_2r, sgdr_y_pred, sgdr_model = evaluateModel(sgdr_model)","03e44d49":"from sklearn.neighbors import KNeighborsRegressor\nknn_model = KNeighborsRegressor(n_neighbors=50)","8bd17709":"knn_mae, knn_rmse, knn_r2, knn_y_pred, knn_model = evaluateModel(knn_model)","a861279b":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor(n_estimators=200)","56b77ca8":"gbr_mae, gbr_rmse, gbr_r2, gbr_y_pred, gbr_model = evaluateModel(gbr_model)","8c62dea2":"#param_grid = {\"n_estimators\":[300,1000], # previously [10,100,200,300]\n              #\"learning_rate\":[0.01, 0.1, 1.0],\n              #\"max_features\":[\"auto\",\"sqrt\"]}\n\n#gr_boost_reg = GradientBoostingRegressor()\n\n#grid_gbr_model = GridSearchCV(gr_boost_reg, param_grid,cv=2)\n\n#grid_gbr_model.fit(X_train, y_train)","904132c0":"# grid_gbr_model.best_params_","3ecc94b5":"# grid_gbr_model.best_score_","f728f9b6":"#gbr_cv_mae, gbr_cv_rmse, gbr_cv_r2, gbr_y_pred, grid_gbr_model = evaluateModel(grid_gbr)","e9c9d031":"best_gbr = GradientBoostingRegressor(n_estimators=1000,\n                                     learning_rate=0.1,\n                                     loss=\"ls\",\n                                     max_features='sqrt')\n\ngbr_cv_mae, gbr_cv_rmse, gbr_cv_r2, gbr_y_pred, grid_gbr_model = evaluateModel(best_gbr)\n\n\n# full_data_gbr.fit(scaled_train, y)","30456209":"# gbr_sub = pd.DataFrame({\"id\":id_col,\n                        # \"target\":full_data_gbr.predict(scaled_test)})\n    \n# gbr_sub.to_csv(\"submission_gbr.csv\", index=False)","3ceb84f8":"# from sklearn.ensemble import RandomForestRegressor\n\n# rfc_model = RandomForestRegressor()\n\n# rfc_model.fit(X_train, y_train)\n\n# rfc_mae, rfc_rmse, rfc_r2, rfc_y_pred, rfc_model = evaluateModel(rfc_model)","ff305893":"# rfc_model.feature_importances_","3f4f1ba2":"#coef_ser = pd.Series(rfc_model.feature_importances_)\n#coef_ser = coef_ser.sort_values(ascending=False)\n\n#colors = list(reversed(px.colors.qualitative.Dark24))\n\n#fig = go.Figure(go.Bar(\n    #x = scaled_train.columns,\n    #y = coef_ser.values,\n    #text = coef_ser.values,\n    #textposition = 'auto',\n    #texttemplate = '%{value:,s}',\n    #marker_color = colors,\n    #orientation = 'v',\n#))\n#fig.update_traces(\n   # marker_line_width = 1,\n    #opacity = 0.8,\n#)\n#fig.update_layout(\n    #title = \"Feature importances via coefficients in LassoCV\"\n#)\n#fig.show()","497ec8ee":"# param_grid = {\"n_estimators\":[300, 500, 1000],\n              # \"max_depth\":[2,3,5],\n              # \"max_features\":[\"auto\",\"sqrt\"]}\n\n# rfr_model = RandomForestRegressor()\n\n# rfr_grid_model = GridSearchCV(rfr_model,param_grid, cv=2)\n\n# rfr_grid_mae, rfr_grid_rmse, rfr_grid_r2, rfr_grid_y, rfr_grid_model  = evaluateModel(grid_rfr)","f12355fa":"# No improvements with this algorithm.","97147155":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam","aa490cfb":"ann_model = Sequential()\n\n\n# input layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\n\n# hidden layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# hidden layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# hidden layer\nann_model.add(Dense(38, activation='relu',dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# output layer\nann_model.add(Dense(1, activation='relu', dtype='float32'))\n\nann_model.compile(optimizer='adam', loss='mae')  # rmsprop, adam","3a077e9d":"ann_model.fit(x=X_train, y=y_train,\n              validation_data=(X_test, y_test),\n              batch_size=64, epochs=15,verbose=0)","4f3d6fe8":"losses = pd.DataFrame(ann_model.history.history)","aeb60791":"losses.plot()","1ae96617":"# ann_model_pred = ann_model.predict(X_test)","7583e93e":"ann_mae, ann_rmse, ann_r2, ann_y_pred, ann_model  = evaluateModel(ann_model)","02b0d51d":"import xgboost as xgb","bf920a19":"data_dmatrix = xgb.DMatrix(data=scaled_train,\n                           label=y)","a43cfe41":"data_dmatrix","b7684066":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 500)\n\nxgb_mae, xgb_rmse, xgb_r2, xgb_y_pred, xgb_model = evaluateModel(xg_reg)","c29628a9":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","d38f0060":"# xgb_final = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n               #  max_depth = 5, alpha = 10, n_estimators = 500)\n\n# xgb_final.fit(scaled_train.values,y)","06720eed":"# xgb_pred = xgb_final.predict(scaled_test.values)\n\n# xgb_sub = pd.DataFrame({\"id\":id_col, \"target\":xgb_pred})\n# xgb_sub.to_csv(\"submission_xgboost.csv\", index=False)","338b7d35":"params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","cbf50668":"cv_results.describe().T","506e3217":"#param_grid = {'nthread':[1], #when use hyperthread, xgboost may become slower\n              #'objective':['reg:squarederror'],\n              #'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              # 'max_depth': [5, 6, 7],\n              #'alpha':[0.1, 1, 10],\n              #'colsample_bytree': [0.3, 0.7],\n              #'n_estimators': [500, 1000]}\n\n#xgb1 = xgb.XGBRegressor()\n\n#xgb_grid_model = GridSearchCV(xgb1, \n                              #param_grid,\n                              #cv=2,\n                              #verbose=1)\n\n#xgb_grid_mae , xgb_grid_rmse, xgb_grid_r2, xgb_grid_y, xgb_grid_model = evaluateModel(xgb_grid_model)","aa5cb342":"# xgb_grid.best_params_","b7a73b71":"xgb_best_model = xgb.XGBRegressor(n_estimators=1000,\n                                  learning_rate=0.03, \n                                  colsample_bytree=0.3, \n                                  alpha=10)\n\nxgb_best_mae , xgb_best_rmse, xgb_best_r2, xgb_best_y, xgb_best_model = evaluateModel(xgb_best_model)","a4dfd040":"xgb_best_model.fit(scaled_train, y)","82db11be":"xgb_best_sub = pd.DataFrame({\"id\": id_col,\n                             \"target\": xgb_best_model.predict(scaled_test)})","48b3e358":"xgb_best_sub.to_csv(\"submission_xgb_best.csv\", index=False)","3d4671db":"\nfrom sklearn.ensemble import VotingRegressor\nensemble_model = VotingRegressor(estimators=[\n    (\"svr_base\", svr_model),\n    (\"lasso_gr\", lasso_grid.estimator),\n    (\"ridge_gr\", grid_ridge.estimator),\n    (\"sgdr_base\", sgdr_model),\n    (\"knn_base\", knn_model),\n    (\"gbr_base\", gbr_model),\n    (\"gbr_best\", grid_gbr_model),\n    (\"xgb_best\", xgb_best_model),])","4c56abec":"ensemble_model.fit(scaled_train, y)","cc34aa48":"ensemble_pred = ensemble_model.predict(scaled_test)\n\nensemble_sub = pd.DataFrame({\"id\":id_col,\n                             \"target\": ensemble_pred})\n\nensemble_sub.to_csv(\"ensemble_tb_series_sub.csv\", index=False)","1a560bf7":"from sklearn.ensemble import StackingRegressor","ca8801b4":"reg = StackingRegressor(estimators=[(\"sgdr_base\", sgdr_model),\n                                    (\"knn_base\", knn_model),\n                                    (\"gbr_best\", grid_gbr_model)],\n                                    final_estimator = xgb_best_model)\n","9b021fb0":"reg.fit(scaled_train, y)","f9b25785":"reg_sub = pd.DataFrame({\"id\":id_col,\n                        \"target\": reg.predict(scaled_test)})\n\nreg_sub.to_csv(\"stacking_reg_tps_sub.csv\", index=False)","c4696c22":"rmse_score_df = pd.DataFrame({\n    \"Model\": [\"LassoCV\",\"RidgeCV\",\"LinearSVR\",\"Lasso GridSearchCV\",\"Ridge GridSearchCV\",\n              \"SGDRegressor\",\"KNeighborsRegressor\",\"GradientBoostingRegressor\",\n              \"GradientBoostingRegressor GridSearchCV\",\"RandomForestRegressor\",\n              \"RandomForestRegressor GridSearchCV\",\"ANN Regression\",\"XGBRegressor\",\n              \"XGBRegressor GridSearchCV\"],\n    \"RMSE\":[lasso_rmse, ridge_rmse,svr_rmse,\n            lasso_grid_rmse,ridge_grid_rmse,\n            sgdr_rmse,knn_rmse, gbr_rmse,\n            gbr_cv_rmse,rfc_rmse, rfr_rmse,\n            ann_rmse, xgb_rmse, xgb_grid_rmse]})","88e36993":"rmse_score_df = rmse_score_df.sort_values(by=['RMSE'], ascending=True).reset_index()\nrmse_score_df = rmse_score_df.drop('index', axis=1)\nrmse_score_df","b8f7dead":"pip install pytorch-tabnet==3.1.0","6e174dd0":"from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor","c44888ed":"# Load the data\n# X = train_df.drop(columns=['target']).values\n# sc = StandardScaler()\n# x=sc.fit_transform(x)\n# y = train_df['target'].values.reshape(-1, 1)","6ad61bbc":"# train models with AutoML\nre = TabNetRegressor()  #TabNetRegressor()\nre.fit(\n  p_X_train, y_train,\n  eval_set=[(p_X_val, y_val)],\n   eval_name=['train'],\n    eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n    max_epochs=1000,\n    patience=50,\n    batch_size=1024, virtual_batch_size=128,\n    num_workers=0,\n    drop_last=False\n)\npreds = re.predict(p_X_val)","93ebc9e9":"len(preds)","dd98cc15":"print(\"Test MSE:\", mean_squared_error(y_val, preds, squared=False))","dfd5f951":"preds","7dcabf1f":"preds = re.predict(p_test).reshape(-1)\nsubmission = pd.DataFrame({\"id\":id_col,\n                          \"target\":preds})\nsubmission.to_csv('submission_pytorch.csv', index = False)","7f475e03":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","36946d8b":"cont_features = [col for col in train_df.columns if col.startswith(\"cont\")]\nlen(cont_features)","3c96acb9":"#X = X.abs()\ny = train_df[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train_df))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train_df):\n    X_train, X_val = train_df.iloc[train_index], train_df.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n        \n        \n        oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","ab6d5806":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n","86416a9b":"As always we should check for multicollinearity. In this case we can try to use regularization methods like Ridge and Lasso or ElasticNet).","1e801f76":"It seems like this data set is not valid for linear regression.(If someone correct me if I am wrong). In other words if residuals plot shows clear pattern, Linear Regression is propably not a good choice.","cae97f72":"### GridSearch CV for Ridge Regression","25ef4ff8":"### Lasso CV","9eb21854":"Some of the  algorithms below run too long when I run it on kaggle notebook. So I am not going to do it again( no more time to do it).","b5982f36":"### Split the model","f167ee3e":"### Correlation","946e2c02":"Usually, at this point we would think about whether to drop outliers, but I did it already with previous ran and it had no effect on results.","9920c1a1":"I bolive there is still a room for improvement by better parameters tunning but this is not the purpose of this notebook, but for now my winner so far is Griedient Boosting Regressor. Finally, I would like to try last algorythm which is XGBoost.\n\n\nXGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library. XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art\u201d machine learning algorithm to deal with structured data.","129bd71b":"There is constant error between residuals and actual data which leads us to very sophisticated term  homoscesdasticity, the word I still struggle to pronounce it :).","0285600a":"For l1_ratio = 0 the penalty is an L2 penalty(Ridge). For l1_ratio = 1 it is an L1 penalty(Lasso). It looks like Lasso will be better model to choose for.","fdc27506":"**StackingRegressor**","d6c0b4ba":"### GridSearch CV for SVR","5cdb3c8d":"Kurtosis is a statistical measure that defines how heavily the tails of a distribution differ from the tails of a normal distribution. In other words, kurtosis identifies whether the tails of a given distribution contain extreme values.\n\nSkewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nSkewness essentially measures the relative size of the two tails. Kurtosis is a measure of the combined sizes of the two tails.","459017f6":"#### Submmit to Kaggle","afa3c018":"### Normalize data","2cb926d5":"#### Submmit to Kaggle","b40747bc":"Running this code in kaggle notebook could use most of you alocated memory and the notebook can stop responding, so if you want to run it make sure you have enough memory. This algorithm didn't improve root mean squared error when I run it, beside there is no chance of using elbow method to find if any degree can yield better results.","5b0ed4ff":"### Outliers","26def42b":"Hello Kagglers!\n\nIn this notebook I will try to find out which of the regression algorithms can improve the score  for this particular dataset and position in Kaggle competition. I found this competition challenging as this dataset is a bit tricky and I think there isn't too much to improve as the dataset has been preprocessed for us (correct me if I'm wrong). The only thing which comes to my mind is to choose the right algorithm and find the right hyperparametres.","45f2fe34":"### Best Lasso Submission","320a4e78":"### Table with models and their score","dd6a9d23":"## Linear regression","59f0b4c4":"#### Spearman\u2019s Correlation","6acabd38":"### Gradient Descent Algorithm for Regression ","7b115621":"This looks promising, let's tune hyperparameters and check if that improve our results.","bacb4987":"This is great. Submission scored 0.70400, which is an improvement of your previous score of 0.70565.\n654 position on Kaggle scoreboard on 20.01.2020. Let's see if we can tune our model and get better results.","91e72e3d":"#### KNeighborsRegressor","afed7738":"It seems to me that in this particular dataset gradient boosting and decision trees algorithms perform much better than linear models. I also realize that there might be more improvement if you go deeper into hyperparameters and tune chosen model. Another suggestion could be dealing with correlated data. One popular rank correlation method in ML is the Principal Component Analysis. It\u2019s a technique to find patterns in high dimensional data.\n\nI am sure there is more to explore to make predictions which would result with lower RMSE, but it is beyond my scope for now. There is still so much to learn...\n\nAfter this challange I found more question than the answers. One main question remains \"What else I could do to improve model score?, any suggestion please leave feedback.\n\nBe aware, that if you want to run this notebook it will take you 4 hours without the models I have comment.\n\nI hope you find this notebook interesting.","a0bdd145":"#### Feature importance","45b35731":"Submission scored 0.70774, which gave 727 from 1049 position in the scoreboard on 19.01.2020. Not bad, Let's se if we can tune it better with slightly different hyperparameters.\n\n1. MAE: 0.5956168406066421\n2. RMSE: 0.7110476866137042\n\n\nHyperparameters found by GridSearch CV:\n\n{'learning_rate': 0.1, 'loss': 'huber', 'n_estimators': 300}","f1fc135a":"### Polynomial Regression","9fac7d64":"#### Kaggle Submission","a404614c":"#### XGBoost tunning hyperparameters","b50fde6f":"#### GridSearch CV for Gradient Boosting Regressor","4feb9f2a":"Submission scored 0.70303, which is an improvement of your previous score of 0.70400. \n\nKaggle scorboard position:  628","5433fc31":"#### Residuals","66106a74":" ### ElasticNet CV","a04daeea":"## Make predictions and save it.","3701dfd8":"I found out that linear regression algorithms are not suited for this dataset, and there is little improvement in rmse for these models. I also find out that algorithms using loss function are performing better, therefore I try XGBoost. The algorithm is an implementation of the gradient boosting ensemble algorithm for classification and regression.","4cf2ccc2":"#### Feature importance","8af37eec":"#### Submmit to Kaggle","b43f3800":"#### Griedient Boosting Regressor","a9c3f922":"### Search for the best hyperparameters","73d717cd":"#### Take a sample of a dataset","5fa8d360":"#### GridSearch CV for RandomForestRegressor","d8c7ce4f":"Submission scored 0.70565, which is an improvement of your previous score of 0.70774. More tunning yielded small improvment and placed on scoreboard 706.\n\nHyperparameters:\n\n{'learning_rate': 0.1,\n 'loss': 'ls',\n 'max_features': 'sqrt',\n 'n_estimators': 1000}\n \nScore:\n\n1. MAE: 0.5937034963056688\n2. RMSE: 0.7088659238713398","f227de23":"## Conclusion","2a8e786b":"# Introduction","90302363":"## LGBM model","4ef6dcfe":"#### Feature importance","0bef7f84":"#### GradientBoostingRegressor with best hyperparameters found with GridSearchCV","ca2ed8eb":"### Residuals","769be52d":"### SVM model","6dd49629":"Good practice is to check for Variance-Bias Trade-Off by tunning in this case \"n_estimators\" hyperparameter and keep record of rmse for test and train test. This algorithm is fairly robust to overfitting so a large number usually results with better performance.","630c9786":"### Training final models","693cf3e3":"#### Submmit to Kaggle","bc45ff4d":"#### SGDRegressor","6793625c":"#### Residuals","8236f81e":"Linear Regression can not work on all datasets. For a linear regression algorithm to work properly, it has to pass at least the following five assumptions:\n\n1. Linear relationship - the relation between independent and dependent features should be linear. Scatter plot is a good way to visualize it.\n2. Multiviariate Normal - each variable seperatly needs to be bell shape curve. This can be tested by plotting a histogram.\n3. No Multicollinearity - Multicollinearity happens when the independent variables are highly correlated with each other. Can be tested with correlation matrix.\n4. No Autocorrelation - Autocorrelation means a single column data values are related to each other. Test it with scatterplot.\n5. Homoscedasticity - This means \u201csame variance\u201d .In other words residuals are equal across regression line. Homoscedasticity can also be tested using scatter plot.","21ca29bd":"A common aproach for highly correlated features is to do dimension reduction","8f249f54":"### Random Forest Regressor","025d48a0":"#### Evaluate the model","71c88cfa":"There is an issue with dataset because residuals are skewing from linear regression line.","769d108b":"This will takes ages in my computer to find the right hyperparameters. Searching for best parameters can be exhausting and in some cases can break down. So, purely because of the time needed for gridsearch and memory I create smaller datasets to reduce computing time. I will also search only for C values in this case ","b50de6f2":"### Artificial Neural Network","94cefa7c":"### Ridge CV"}}