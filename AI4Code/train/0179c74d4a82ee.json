{"cell_type":{"f6e240fb":"code","06063a61":"code","f5f169b4":"code","9bb20471":"code","d8393ea9":"code","b3ebe56e":"code","1c1e83bc":"code","7ad1b62a":"code","1c701dc5":"code","bc0bf983":"code","b0fdc946":"code","82e542b5":"code","f634007a":"code","fbf168fd":"code","4c30696d":"code","3775e2be":"code","9023a5b3":"code","dcf4e6e2":"code","5542a8d8":"code","b22396f0":"markdown","8a9c05d2":"markdown","b3c0cadc":"markdown","74f2c9ab":"markdown","1eadbc19":"markdown","9a21b395":"markdown","13314973":"markdown","aedc1dda":"markdown","3e7bccb2":"markdown","a8766005":"markdown","786795f3":"markdown","24ec39ea":"markdown","2dd65add":"markdown","a9657417":"markdown","7b38e009":"markdown","882f444a":"markdown","cf69b58a":"markdown","14257859":"markdown","068b39ae":"markdown","40044019":"markdown","25bbc2d8":"markdown","113d4f52":"markdown","4f4a0741":"markdown","802462df":"markdown"},"source":{"f6e240fb":"import tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntf.random.set_seed(1)\nEPOCHS = 20\nLR = 0.001\nOPT = tf.keras.optimizers.Adam(LR)\nplt.style.use('fivethirtyeight')\nplt.rcParams[\"figure.figsize\"] = (8,5)","06063a61":"(x_train , y_train) , (x_test , y_test ) = tf.keras.datasets.mnist.load_data()\nx_train = x_train \/255 \nx_test = x_test\/255","f5f169b4":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nmodel.summary()","9bb20471":"(x_train_partial , y_train_partial) = (x_train[:30000] ,  y_train[:30000])","d8393ea9":"partial_data = model.fit(x_train_partial , y_train_partial, \n                    validation_data=(x_test , y_test),\n                    epochs= EPOCHS ,\n                    verbose = 0 )","b3ebe56e":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nfull_data = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","1c1e83bc":"plt.plot(partial_data.history['accuracy'], label='Partial-Data'  )\nplt.plot(full_data.history['accuracy'], label='Full-Data' , )\nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Training Accuracy\")\nplt.legend(loc = 'lower right')","7ad1b62a":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\none_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","1c701dc5":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\ntwo_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","bc0bf983":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                             tf.keras.layers.Dense(40 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nthree_added_layers =  model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","b0fdc946":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                             tf.keras.layers.Dense(40 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nfive_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","82e542b5":"plt.plot(full_data.history['accuracy'], label='Simple Linear Model')\nplt.plot(one_added_layers.history['accuracy'], label='One Hideen Layer')\nplt.plot(two_added_layers.history['accuracy'], label='Two Hidden Layers')\nplt.plot(three_added_layers.history['accuracy'], label='Three Hidden Layers')\nplt.plot(five_added_layers.history['accuracy'], label='Five Hidden Layers')\nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Training Accuracy\")\nplt.legend(loc = 'lower right')","f634007a":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(80,activation = \"relu\"),\n                             tf.keras.layers.Dense(40,activation = \"relu\"),\n                             tf.keras.layers.Dense(20,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nsmall_units = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","fbf168fd":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nlarge_units = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","4c30696d":"plt.plot(full_data.history['accuracy'], label='Simple Linear Model')\nplt.plot(small_units.history['accuracy'], label='Smaller Units')\nplt.plot(large_units.history['accuracy'], label='Larger Units')\nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Training Accuracy\")\nplt.legend(loc = 'lower right')","3775e2be":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nbn = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","9023a5b3":"plt.plot(large_units.history['accuracy'], label='Best_model')\nplt.plot(bn.history['accuracy'], label='Batch Normalization')\nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Training Accuracy\")\nplt.legend(loc = 'lower right')","dcf4e6e2":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.Dropout(0.3),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.Dropout(0.2),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\ndropout = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","5542a8d8":"plt.plot(large_units.history['accuracy'], label='Best Model')\nplt.plot(dropout.history['accuracy'], label='Dropout')\nplt.xlabel(\"No. of iterations\")\nplt.ylabel(\"Training Accuracy\")\nplt.legend(loc = 'lower right')","b22396f0":"### Training on total training data","8a9c05d2":"# <center> FIXING HIGH BIAS IN NEURAL NETWORKS<\/center>","b3c0cadc":"# EFFECT OF DROPOUTS\n<a id=\"dropout\"><\/a>","74f2c9ab":"We will add Batch Norm Layers to our last best model to check its effect on training accuracy.","1eadbc19":"<a id=\"layers\"><\/a>\n# EFFECT OF INCREASING HIDDEN LAYERS","9a21b395":"In the next cell, the same model is created once more as a way to reset the weights from the previous trained data.","13314973":"## LOADING DATA","aedc1dda":"After training the same data on multiple models with different hyperparameters, we can conclude that the following changes can help us in fixing high bias:\n* Increasing the number of hidden layers.\n* Increasing the number of hidden units.\n* Training for a higher number of epochs.\n* Trying more neural networks.\n\nAlso, the following changes have not much impact on high bias :\n* Increasing the amount of training data.\n* Adding Batch Normalization\n* Adding Dropouts\n\nAlthough the last three updates in a neural network do not have a huge impact on fixing the problem of underfitting, but they certainly help in reducing high variance (or overfitting).\n\nFor a more detailed analysis of every factor, check out my blog on Fixing Underfitting [here](https:\/\/medium.com\/mlearning-ai\/hyperparameter-tuning-fixing-high-bias-underfitting-in-neural-networks-5184ead3cbed).","3e7bccb2":"**Created by Sanskar Hasija**\n\n**FIXING HIGH BIAS ( UNDERFITTING ) IN NUERAL NETWORKS**\n\n**11 August 2021**\n","a8766005":"### Training on 50% of the total training data","786795f3":"# EFFECT OF BATCH NORMALIZATION \n<a id=\"batch\"><\/a>","24ec39ea":"Adding large units to a 3 hidden-layers model","2dd65add":"### MODEL DESIGN ","a9657417":"### [1. Effect of increasing  data](#data) ###\n### [2. Effect of increasing hidden layers](#layers) ###\n### [3. Effect of nodes](#nodes) ###\n### [4. Effect of Batch Normalization](#batch) ###\n### [5. Effect of Dropouts](#dropout) ###\n\n##    [Conclusions](#conclusion) ##","7b38e009":"# CONCLUSION\n<a id=\"conclusion\"><\/a>","882f444a":"## IMPORTS","cf69b58a":"### Adding two extra hidden layers","14257859":"We will first start with a simple linear model with 1 input node and 10 ouptut nodes with softmax activation.","068b39ae":"### Adding five extra hidden layers","40044019":"Adding small units to a 3 hidden-layers model","25bbc2d8":"### Adding three extra hidden layers","113d4f52":"<a id=\"nodes\"><\/a>\n# EFFECT ON UNITS(NODES) IN LAYERS","4f4a0741":"<a id=\"data\"><\/a>\n# EFFECT OF INCREASING DATA","802462df":"### Adding one extra hidden layer"}}