{"cell_type":{"b070dc6a":"code","40881fd2":"code","583a799d":"code","5420298c":"code","944c80c5":"code","a395f4f3":"code","01a3d3fa":"code","d184baf5":"code","ec86c48d":"code","a07ca6fa":"code","004e2c32":"code","148e7580":"code","bb289d11":"markdown","d9ff11ed":"markdown","3b2a2d14":"markdown"},"source":{"b070dc6a":"context=open(\"\/kaggle\/input\/story-file\/story.txt\",\"r\")\ncontext=context.read()\ncontext","40881fd2":"import torch\nfrom transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer","583a799d":"model_list=[\"bert-large-cased-whole-word-masking-finetuned-squad\",\"distilbert-base-cased-distilled-squad\",\"bert-base-cased\"]\n\ncontext=context\n                                                                      \ndef load_model(model_name): #Fonction qui demande le mod\u00e8le a utiliser et qui permets de le charger.\n    if model_name in model_list:\n        global model\n        global tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_name) # Etape de tokenization\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        return True\n    else:\n        print(\"No model with that name\")\n        return None\n\ndef answer_function(question): #Fonction qui trouve la reponse a la question pos\u00e9e\n    #Encodage des tokens\n    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0] #Listes des encodages\n\n    #text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer_start_scores=model(**inputs)[\"start_logits\"]\n    answer_end_scores = model(**inputs)[\"end_logits\"]\n\n    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n    soft=torch.nn.Softmax(dim=1)\n    print('Start score : '+str(soft(answer_start_scores).max().item()))\n    print('End score : '+str(soft(answer_end_scores).max().item()))\n\n    return(answer)","5420298c":"#Script\ndef script_function():\n    model_loaded=None\n    while(True):\n        if model_loaded==None:\n            print(\"What model to use ?\\n\")\n            model_name=input()\n            model_loaded=load_model(model_name)\n        else:\n            print(\"Any question ?\")\n            text=input()\n            if text==\"No\":\n                print(\"Ok goodbye\")\n                break\n            else:\n                print(answer_function(text)+\"\\n\")","944c80c5":"script_function()","a395f4f3":"question=\"Who are you ?\"\ninputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"].tolist()[0]\n\ntext_tokens = tokenizer.convert_ids_to_tokens(input_ids)\nanswer_start_scores=model(**inputs)[\"start_logits\"]\nanswer_end_scores = model(**inputs)[\"end_logits\"]\n\nanswer_start = torch.argmax(answer_start_scores)  \nanswer_end = torch.argmax(answer_end_scores) + 1\n\nanswer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))","01a3d3fa":"import pandas as pd\niptid=pd.DataFrame(input_ids, columns=[\"encodage\"])\ntknz=pd.DataFrame(text_tokens,columns=[\"token\"])\ndt=pd.concat([iptid,tknz],axis=1)\ndt.head(20).T","d184baf5":"import matplotlib.pyplot as plt\n\nanswer_start_scores=model(**inputs)[\"start_logits\"]\nanswer_end_scores = model(**inputs)[\"end_logits\"]\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(30),answer_start_scores.tolist()[0][:30])\nfor i in range(30):\n    plt.text(i,0,text_tokens[i],fontsize=14)\n","ec86c48d":"answer_start = torch.argmax(answer_start_scores)  \nanswer_end = torch.argmax(answer_end_scores) + 1\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(170),answer_start_scores.tolist()[0])\nplt.scatter(range(170),answer_end_scores.tolist()[0])\nfor i in range(170):\n    plt.text(i,answer_start_scores.tolist()[0][i],text_tokens[i])\nplt.vlines([answer_start,answer_end],ymin=-12,ymax=12);\n\nplt.figure(figsize=(20,10))\n\nplt.scatter(range(30),answer_start_scores.tolist()[0][:30])\nplt.scatter(range(30),answer_end_scores.tolist()[0][:30])\nfor i in range(30):\n    plt.text(i,0,text_tokens[i],fontsize=14)\nplt.vlines([answer_start,answer_end],ymin=-12,ymax=12);","a07ca6fa":"nlp = pipeline('question-answering')","004e2c32":"context = context\nprint(nlp(question=\"Who are you ?\", context=context))","148e7580":"print(nlp(question=\"When was you born?\", context=context))","bb289d11":"### Petit Test avec la fonction pipeline","d9ff11ed":"# Step by step","3b2a2d14":"# TP7 - DeepLearning\n## Transformers for question answering\n> Elodie - Alexandra - Sonico - Raphael - Alissa\n### Objectif : \n> Extraire des r\u00e9ponses pertinentes \u00e0 des questions en se basant sur un texte.\n### Outils : \n> Biblioth\u00e8que **\u201ctransformers\u201d** de Huggingface : https:\/\/huggingface.co\/transformers\/\n### Importation du texte"}}