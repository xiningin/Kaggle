{"cell_type":{"fc11c223":"code","da3d43e4":"code","1dfd842d":"code","3a9cadb8":"code","30544cc2":"code","1c2e34ff":"code","362aa492":"code","ffa107a7":"code","b52b9539":"code","eed1ed01":"code","e97bff26":"code","5c8afefe":"code","15bf5d38":"code","a00e2b3f":"code","ceb26659":"code","0458a360":"code","b8a7c9cc":"markdown","5dc5eac7":"markdown","d94157ca":"markdown","22a6e291":"markdown","18092e81":"markdown","b17afef6":"markdown","83c7d95c":"markdown","98f61c24":"markdown","67f47345":"markdown","382eaea4":"markdown","f12ec142":"markdown","38135ab3":"markdown"},"source":{"fc11c223":"import os\nimport cv2\nimport time\nimport shutil\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras import layers\nfrom keras import models\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras import optimizers\nfrom math import ceil\nfrom sklearn.metrics import confusion_matrix\n\nBATCH_SIZE = 32\nEPOCHS = 200\nTRAIN_SAMPLE = 2500\nTEST_SAMPLE = 800\nIMG_SIZE = 440\nSEED = randint(0,100)","da3d43e4":"train_df = pd.read_csv('..\/input\/preprocessed-diabetic-retinopathy-trainset\/newTrainLabels.csv')\ntest_df = pd.read_csv('..\/input\/preprocessed-diabetic-retinopathy-trainset\/retinopathy_solution.csv')\n\ntrain_input_dir = '..\/input\/preprocessed-diabetic-retinopathy-trainset\/300_train\/300_train\/'\ntest_input_dir = '..\/input\/preprocessed-diabetic-retinopathy-trainset\/300_test\/300_test\/'\n\nprint(train_df.info())\n#binary\ntrain_df['level'] = 1*(train_df['level'] > 0)\ntest_df['level'] = 1*(test_df['level'] > 0)\n\n#Test out images can be found\nimage_path = train_input_dir + train_df['image'][0] +'.jpeg'\nif os.path.isfile(image_path) == False:\n    raise Exception('Uable to find train image file listed on dataframe')\nelse:\n    print('Train data frame and file path ready')\n    \nimage_path = test_input_dir + test_df['image'][0] +'.jpeg'\nif os.path.isfile(image_path) == False:\n    raise Exception('Uable to find test image file listed on dataframe')\nelse:\n    print('Test data frame and file path ready')","1dfd842d":"train_sample_df = train_df.sample(n=TRAIN_SAMPLE)\ntest_sample_df = test_df.sample(n=TEST_SAMPLE)\ntrain_sample_df[['level']].hist()","3a9cadb8":"def cropImages(dataFrame, inputDir, destDir):\n    try:\n        os.mkdir(destDir)\n        print('Created a directory:', destDir)\n    except:\n        print(destDir, 'already exists!')\n\n    # Crop and resize all images. Store them to train_dir\n    print('Cropping and rescaling the images:')\n    start = time.time()\n    for i, file in enumerate(dataFrame['image']):\n        try:\n            fname = inputDir + file + '.jpeg'\n            img = cv2.imread(fname)\n\n            # Crop the image to the height\n            h, w, c = img.shape\n            if w > h:\n                wc = int(w\/2)\n                w0 = wc - int(h\/2)\n                w1 = w0 + h\n                img = img[:, w0:w1, :]\n            # Rescale to N x N\n            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n            # Save\n            new_fname = destDir + file + '.png'\n            cv2.imwrite(new_fname, img)\n        except Exception as e:\n            # Display the image name having troubles\n            print(fname, str(e))\n            break\n\n        # Print the progress for every N images\n        if (i % 500 == 0) & (i > 0):\n            print('{:} images resized in {:.2f} seconds.'.format(i, time.time()-start))\n    print('Images cropped and saved to: ', destDir)","30544cc2":"train_dir = '\/tmp\/train\/'\ntest_dir = '\/tmp\/test\/'\ncropImages(train_sample_df, train_input_dir, train_dir)\ncropImages(test_sample_df, test_input_dir, test_dir)","1c2e34ff":"generator = ImageDataGenerator(rescale=1.\/255, validation_split=0.22, rotation_range=160)\n\n#add file extentions so generator can find them\ntrain_sample_df['image'] = train_sample_df['image'].apply(lambda x: str(x) + '.png')\ntest_sample_df['image'] = test_sample_df['image'].apply(lambda x: str(x) + '.png')\n\n#integer labels to strings so generator can read them\ntrain_sample_df['level'] = train_sample_df['level'].apply(str)\ntest_sample_df['level'] = test_sample_df['level'].apply(str)\n\ntrain_flow = generator.flow_from_dataframe(dataframe=train_sample_df,\n                                         directory=train_dir,\n                                         x_col='image',\n                                         y_col='level',\n                                         target_size=(IMG_SIZE, IMG_SIZE),\n                                         class_mode='binary',\n                                         batch_size=BATCH_SIZE, #default 32\n                                         seed=SEED,\n                                         subset='training')\n\nval_flow = generator.flow_from_dataframe(dataframe=train_sample_df,\n                                         directory=train_dir,\n                                         x_col='image',\n                                         y_col='level',\n                                         target_size=(IMG_SIZE, IMG_SIZE),\n                                         class_mode='binary',\n                                         batch_size=BATCH_SIZE, #default 32\n                                         seed=SEED,\n                                         subset='validation')\n\ntest_flow = generator.flow_from_dataframe(dataframe=test_sample_df,\n                                         directory=test_dir,\n                                         x_col='image',\n                                         y_col='level',\n                                         target_size=(IMG_SIZE, IMG_SIZE),\n                                         class_mode='binary',\n                                         batch_size=BATCH_SIZE, #default 32\n                                         seed=SEED)\n\n","362aa492":"test_batch = train_flow.next()\n\nprint(test_batch[0][0].shape)\n\nrows = 2\ncols = 2\nf, ax = plt.subplots(rows, cols, figsize=(rows*3, cols*3), squeeze=True)\nfor i in range(rows*cols):\n    a = ax.flat[i]\n    a.imshow(test_batch[0][i])\nplt.show()\n\n#reset the batch index of generator\ntrain_flow.reset()","ffa107a7":"ins_weights = '..\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#pre trained model\nbase_model = InceptionV3(weights=ins_weights,\n                         include_top=False,\n                         input_shape=(IMG_SIZE,IMG_SIZE,3))\n\nx = base_model.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.4, seed=SEED)(x)\nx = layers.Dense(32, activation='relu')(x)\nx = layers.Dropout(0.2, seed=SEED)(x)\npred = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = models.Model(inputs=base_model.input, outputs=pred)\n\nfor layer in model.layers[:99]:\n    layer.trainable = False\n\nmodel.summary()","b52b9539":"rmsprop = optimizers.RMSprop(lr=0.0005)\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer=rmsprop,\n             metrics=['acc'])","eed1ed01":"trainSteps = ceil(train_flow.n\/BATCH_SIZE)\nvalSteps = ceil(val_flow.n\/BATCH_SIZE)\n\nresults = model.fit_generator(\n    train_flow,\n    steps_per_epoch=trainSteps,\n    verbose=1,\n    epochs=EPOCHS,\n    validation_data= val_flow,\n    validation_steps=valSteps\n)\nmodel.save('ret_model.h5')\nwith open('trainHistory1', 'wb') as file_pi:\n        pickle.dump(results.history, file_pi)","e97bff26":"with open('trainHistory1', 'rb') as file:\n    history = pickle.load(file)\n#history = results.history\n\nacc = history['acc']\nval_acc = history['val_acc']\nloss = history['loss']\nval_loss = history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.savefig('results1_acc.png')\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.savefig('results1_loss.png')\nplt.show()","5c8afefe":"test_evals = model.evaluate_generator(test_flow,\n                                   steps=test_flow.n\/BATCH_SIZE,\n                                   verbose=1)\nprint(model.metrics_names[0], test_evals[0], model.metrics_names[1], test_evals[1])","15bf5d38":"test_pred = model.predict_generator(test_flow,\n                                   steps=test_flow.n\/BATCH_SIZE,\n                                   verbose=1)","a00e2b3f":"#delete processed images\nshutil.rmtree('\/tmp\/test')\nshutil.rmtree('\/tmp\/train')","ceb26659":"predictions = np.argmax(test_pred, axis=1)\ncm = confusion_matrix(test_flow.classes, predictions, )#labels=[0,1,2,3,4])\n\ncm_df = pd.DataFrame(cm)\nplt.figure(figsize=(7,5))\n\nhmap = sb.heatmap(cm_df, annot=True, fmt='d')\nhmap.set(xlabel='pred',ylabel='actual')\nplt.savefig('confusion_matrix.png', dpi=600)\nplt.show()","0458a360":"FP = (cm_df.sum(axis=0) - np.diag(cm_df)).values\nFN = (cm_df.sum(axis=1) - np.diag(cm_df)).values\nTP = np.diag(cm_df)\nTN = cm_df.values.sum() - (FP + FN + TP)\n\nprint('Sensitivity: ', sum(TP\/(TP+FN))\/2)\nprint('Specificity: ', sum(TN\/(TN+FP))\/2)\nprint('Accuracy: ', (TP+TN)\/(TP+FP+FN+TN))","b8a7c9cc":"See if image generator outputs images correctly","5dc5eac7":"## 3. Required libraries","d94157ca":"## 7. Previous steps\n- Created my own simple conv2D network.\n- Changed to InceptionV3 network with added dense layers.\n - Split training into two parts. First only the last dense layers then in the second round released more of Inception model's weights to be trained.\n- Add dropout to the end to combat overfitting and decreased the dense network size.\n- Removed first training step and released more weights to be trained in the second step.\n- Add rotation to images in data generator.\n## 8. Conclutions\nToo late in doing this case I realized that I used too little dataset to see any meaningful results. The model is able to learn the dataset after in this final version but overfits almost instantly. More steps should be added to combat it.\n","22a6e291":"## 4. Data desciption and preprocessing\nThe dataset is part of [Diabetic Retinopathy Detection](https:\/\/www.kaggle.com\/c\/diabetic-retinopathy-detection) challenge. The data has been preprocessed for easier training. The color palettes have been normailzed to match each other as mutch as possible.<br>\nIn the original dataset the is healthy persons and the diseased are devided by sevierty. Zero means healthy and numbers form 1-4 indicate the sevierty of the disease. I have grouped the diseased to one group becasue there is so few images of diseased.<br>\nIn addition images are cropped to be square so the images are not streched by the network.","18092e81":"Take subset of dataset and observe distribution","b17afef6":"Create sample generator combinig dataframe infromation and sample images","83c7d95c":"# Case 2. Diabetic Retinopathy Analysis\n## 1. Objective\nLearn to use convolutional neural networks to classify medical images\n## 2. References\n- [Kevin Mader's InceptionV3 for Retinopathy (GPU-HR) notebook](https:\/\/www.kaggle.com\/kmader\/inceptionv3-for-retinopathy-gpu-hr)\n- [Sakari Lukkarinen's Demo 11. Xception and Dense 2 notebook](https:\/\/www.kaggle.com\/sakarilukkarinen\/demo-11-xception-and-dense-2)\n","98f61c24":"##  6. Results","67f47345":"### Setup input data","382eaea4":"## 5. Model creation and training\nModel uses Google's Inception v3 model with pretrained weights. I have frosen the first 150 layers of the network. On top of the Inception I have added my own dropout to combat overfitting and dense layers. In the end is binary layer with sigmoids activation.","f12ec142":"Crop images for porcessing","38135ab3":"### Test validation"}}