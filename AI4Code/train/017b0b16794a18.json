{"cell_type":{"2ab3d3b0":"code","9ccc51d2":"code","72ae72df":"code","e976e1d1":"code","033bc0a0":"code","b6df3fa8":"code","40eb4a3c":"code","26f37d06":"code","9191fac3":"code","21383844":"code","2ed0fb59":"code","16d8babd":"code","a5f2667a":"code","eb1a7437":"code","af77956f":"code","55ea1f6a":"code","2582a500":"code","39e6d6ee":"code","836d2677":"code","0b81ac03":"code","8d8f7772":"code","640a33d4":"code","8c8e50e9":"code","3edcb340":"code","5a578f3a":"code","7be3dae3":"code","bccfe38f":"code","b64b9831":"code","6a850938":"code","59a8757d":"code","326e069c":"code","81a4e7f0":"code","f061dd0d":"code","e82b4c08":"code","9fd2c08f":"code","59a233b9":"code","90de3593":"code","25f9b9d8":"code","6da89008":"code","46f49fb7":"code","e7797e0a":"code","f1b9b791":"markdown","d49161f1":"markdown","01d3984a":"markdown","4023728b":"markdown","a0553092":"markdown","1d9195ca":"markdown","51db82c4":"markdown","7da0b6be":"markdown","50a92c1d":"markdown","21fcffb4":"markdown","a6742dce":"markdown","515b2c51":"markdown","20c1b6d9":"markdown","6ad1edef":"markdown","7ae95ca4":"markdown","351d99eb":"markdown","deb61641":"markdown","e922c1c0":"markdown","6700d1ed":"markdown","a07af917":"markdown","e8102b99":"markdown","83675051":"markdown","aada8a34":"markdown","eaf8d660":"markdown","92025ba9":"markdown","e12d8c2c":"markdown","53514aac":"markdown","2bd3722b":"markdown","e5c4ec90":"markdown","7b03ba84":"markdown","63664344":"markdown","326f8982":"markdown","20a83dcf":"markdown","be5ab500":"markdown","00062ac1":"markdown","8dbe4288":"markdown","bbed86f3":"markdown","fbc49217":"markdown","ba6ad898":"markdown","5ef4485c":"markdown","b49f5e3e":"markdown","28f8c976":"markdown","9b06cfd0":"markdown"},"source":{"2ab3d3b0":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, roc_auc_score, roc_curve\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9ccc51d2":"train_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","72ae72df":"print(\"K\u00edch th\u01b0\u1edbc t\u1eadp train: {}\".format(train_df.shape))\nprint(\"K\u00edch th\u01b0\u1edbc t\u1eadp test: {}\".format(test_df.shape))","e976e1d1":"train_df.head()","033bc0a0":"test_df.head()","b6df3fa8":"train_df.isnull().any().any()\n# T\u1eadp train kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u thi\u1ebfu","40eb4a3c":"test_df.isnull().any().any()\n# T\u1eadp test kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u thi\u1ebfu","26f37d06":"train_df.info()\nprint(train_df.dtypes)","9191fac3":"test_df.info()\nprint(test_df.dtypes)","21383844":"features = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","2ed0fb59":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max Duplicates', 'Value'])).\\\n            sort_values(by = 'Max Duplicates', ascending=False).head(15))","16d8babd":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max Duplicates', 'Value'])).\\\n            sort_values(by = 'Max Duplicates', ascending=False).head(15))","a5f2667a":"train_df.drop(['target', 'ID_code'], axis = 'columns').nunique()","eb1a7437":"test_df.nunique()","af77956f":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\ntrain_df.describe()","55ea1f6a":"plt.figure(figsize=(14,6))\n\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a mean c\u1ee7a m\u1ed7i h\u00e0ng c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].mean(axis=1),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].mean(axis=1),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","2582a500":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a mean c\u1ee7a m\u1ed7i c\u1ed9t (feature) c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].mean(axis=0),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].mean(axis=0),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","39e6d6ee":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a std c\u1ee7a m\u1ed7i h\u00e0ng c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].std(axis=1),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].std(axis=1),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","836d2677":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a std c\u1ee7a m\u1ed7i c\u1ed9t (feature) c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].std(axis=0),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].std(axis=0),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","0b81ac03":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a min c\u1ee7a m\u1ed7i h\u00e0ng c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].min(axis=1),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].min(axis=1),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","8d8f7772":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a min c\u1ee7a m\u1ed7i c\u1ed9t (feature) c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].min(axis=0),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].min(axis=0),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","640a33d4":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a max c\u1ee7a m\u1ed7i h\u00e0ng c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].max(axis=1),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].max(axis=1),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","8c8e50e9":"plt.figure(figsize=(14,6))\nplt.title(\"Ph\u00e2n ph\u1ed1i c\u1ee7a max c\u1ee7a m\u1ed7i c\u1ed9t (feature) c\u1ee7a t\u1eadp train v\u00e0 test\")\nsns.histplot(train_df[features].max(axis=0),color=\"yellow\", kde=True,bins=80, label='train')\nsns.histplot(test_df[features].max(axis=0),color=\"black\", kde=True,bins=80, label='test')\nplt.legend()\nplt.show()","3edcb340":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train_df.target.values, ax=ax[0], palette=\"husl\")\nsns.violinplot(x=train_df.target.values, y=train_df.index.values, ax=ax[1], palette=\"husl\")\nsns.stripplot(x=train_df.target.values, y=train_df.index.values,\n              jitter=True, ax=ax[1], color=\"black\", size=0.5, alpha=0.5)\nax[1].set_xlabel(\"Target\")\nax[1].set_ylabel(\"Index\");\nax[0].set_xlabel(\"Target\")\nax[0].set_ylabel(\"Counts\");","5a578f3a":"print(\"T\u1ed5ng s\u1ed1 target = 0 l\u00e0: \",train_df[\"target\"].value_counts()[0])\nprint(\"T\u1ed5ng s\u1ed1 target = 1 l\u00e0: \",train_df[\"target\"].value_counts()[1])\nprint(\"T\u1ed5ng s\u1ed1 target = 1 chi\u1ebfm {}% trong t\u1ea5t c\u1ea3 kh\u00e1ch h\u00e0ng.\".format(100 * train_df[\"target\"].value_counts()[1]\/train_df.shape[0]))","7be3dae3":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();\n    ","bccfe38f":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","b64b9831":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","6a850938":"features = train_df.columns.values[2:102]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)\n","59a8757d":"features = train_df.columns.values[102:202]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","326e069c":"sns.set(style=\"whitegrid\")\nfig, ax1 = plt.subplots(10,10, figsize=(50,50))\nk = 2\ncolumns = list(train_df.columns)\nfor i in range(10):\n    for j in range(10):\n            sns.boxplot(train_df['target'], train_df[columns[k]], ax = ax1[i][j], palette='pastel')\n            k += 1\nplt.show()","81a4e7f0":"sns.set(style=\"whitegrid\")\nfig, ax1 = plt.subplots(10,10, figsize=(50,50))\nk = 102\ncolumns = list(train_df.columns)\nfor i in range(10):\n    for j in range(10):\n            sns.boxplot(train_df['target'], train_df[columns[k]], ax = ax1[i][j], palette='pastel')\n            k += 1\nplt.show()","f061dd0d":"correlations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(20)","e82b4c08":"correlations.tail(20)","9fd2c08f":"plt.figure(figsize=(20,20))\ntrain_df.corr()['target'].apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:11][::-1].plot(kind='barh',color='pink') \nplt.title(\"Top 10 features c\u00f3 correlation v\u1edbi target cao nh\u1ea5t\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")","59a233b9":"train_correlations = train_df.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test_df.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation c\u1ee7a c\u00e1c features \u1edf train v\u00e0 test\")\nplt.ylabel(\"Density\")\nplt.title(\"C\u00e1c features c\u00f3 correlation n\u00e0o kh\u00f4ng?\"); \nplt.legend();","90de3593":"# LBGM with StratifiedKFold\ndef runModel(train_df, test_df):\n    features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    target = train_df['target']\n    lgbm_param = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38, \n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  #-1\n        'metric':'auc',\n        'min_data_in_leaf': 20,\n        'min_sum_hessian_in_leaf': 10.0, \n        'num_leaves': 3,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }\n    num_round = 1000000\n    folds = StratifiedKFold(n_splits = 5, shuffle = False)\n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n        print(\"Fold {}\".format(fold_))\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n        clf = lgb.train(lgbm_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3500)\n        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n    return predictions","25f9b9d8":"predictions = runModel(train_df, test_df)\npd.DataFrame({\"ID_code\": test_df.ID_code.values, 'target':predictions}).to_csv(\"submission_basicModel.csv\", index=False)","6da89008":"from imblearn.over_sampling import SMOTE\ndef augmentData(X, y, augment = 0):\n    if (augment == 0): # S\u1eed d\u1ee5ng SMOTE     \n        oversample = SMOTE()\n        X, y = oversample.fit_resample(X, y)\n        return X, y","46f49fb7":"def runModelWithAugmentation(train_df, test_df, augment = 0):\n    features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    target = train_df['target']\n    lgbm_param = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38, \n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  #-1\n        'metric':'auc',\n        'min_data_in_leaf': 20,\n        'min_sum_hessian_in_leaf': 10.0, \n        'num_leaves': 3,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }\n    num_round = 1000000\n    folds = StratifiedKFold(n_splits = 5, shuffle = False)\n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n        print(\"Fold {}\".format(fold_))\n        X_train, y_train = train_df.iloc[trn_idx][features], train_df.iloc[trn_idx]['target']\n        X_valid, y_valid = train_df.iloc[val_idx][features], train_df.iloc[val_idx]['target']\n        X_t, y_t = augmentData(X_train.values, y_train.values, augment)\n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n        \n        trn_data = lgb.Dataset(X_t, label=y_t)\n        val_data = lgb.Dataset(X_valid, label=y_valid)\n        clf = lgb.train(lgbm_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3500)\n        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n    return predictions","e7797e0a":"predictions = runModelWithAugmentation(train_df, test_df, 0) # 0 ngh\u0129a l\u00e0 s\u1eed d\u1ee5ng SMOTE \u0111\u1ec3 augment data\npd.DataFrame({\"ID_code\": test_df.ID_code.values, 'target':predictions}).to_csv(\"submission_AugmentBasicModel.csv\", index=False)","f1b9b791":"## <a id='3.3'>3.3. Ph\u00e2n t\u00edch correlation gi\u1eefa c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u <\/a> ","d49161f1":"### Th\u1eed nghi\u1ec7m Augment Data b\u1eb1ng SMOTE","01d3984a":"Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u missing, b\u00e2y gi\u1edd ta h\u00e3y ki\u1ec3m tra v\u1ec1 c\u00e1c d\u1eef li\u1ec7u c\u00f3 gi\u00e1 tr\u1ecb tr\u00f9ng nhau c\u1ee7a c\u00e1c features \u1edf t\u1eadp train v\u00e0 t\u1eadp test. H\u00e3y c\u00f9ng xem v\u1edbi t\u1eebng c\u1ed9t feature th\u00ec gi\u00e1 tr\u1ecb tr\u00f9ng l\u1eb7p nhi\u1ec1u nh\u1ea5t bao nhi\u1ec1u l\u1ea7n. (max duplicates)","4023728b":"#### B\u00e2y gi\u1edd ta s\u1ebd t\u00ecm hi\u1ec3u v\u1ec1 c\u00e1c features ","a0553092":"\u0110\u1ea7u ti\u00ean ta s\u1ebd xem bi\u1ec3u \u0111\u1ed3 m\u1eadt \u0111\u1ed9 c\u1ee7a 100 features \u0111\u1ea7u ti\u00ean (var_0 ~> var_99), sau \u0111\u00f3 l\u00e0 100 features sau (var_100 -> var_199).\n\n\u0110\u01b0\u1eddng m\u00e0u xanh l\u00e0 target = 1, \u0111\u01b0\u1eddng m\u00e0u cam l\u00e0 target = 0.","1d9195ca":"## <a id='4.1'>4.1. A Basic Model<\/a>","51db82c4":"Ta nh\u1eadn th\u1ea5y c\u00f3 m\u1ed9t s\u1ed1 feature c\u00f3 ph\u00e2n ph\u1ed1i c\u1ee7a target = 0 v\u00e0 target = 1 kh\u00f4ng \u0111\u1ed3ng \u0111\u1ec1u nhau nh\u01b0 **var_0**, **var_1**, **var_2**, **var_9**, **var_12**, **var_13**, **var_110**, **var_139**...\n\nTa s\u1ebd \u0111\u1ec3 \u00fd \u0111\u1ebfn nh\u1eefng features n\u00e0y c\u00e2n nh\u1eafc cho vi\u1ec7c features selection. \n\nB\u00e2y gi\u1edd, ta h\u00e3y xem bi\u1ec3u \u0111\u1ed3 v\u1ec1 ph\u00e2n b\u1ed1 c\u1ee7a 200 features so s\u00e1nh gi\u1eefa t\u1eadp train v\u00e0 test. (Xanh d\u01b0\u01a1ng l\u00e0 test v\u00e0 cam l\u00e0 train)","7da0b6be":"## <a id='4.2'>4.2. M\u1ed9t s\u1ed1 th\u1eed nghi\u1ec7m \u0111\u1ec3 c\u1ea3i ti\u1ebfn model<\/a>","50a92c1d":"Ta th\u1ea5y train v\u00e0 test r\u1ea5t \u0111\u1ed3ng \u0111\u1ec1u nhau v\u1ec1 ph\u00e2n ph\u1ed1i c\u1ee7a c\u00e1c features.","21fcffb4":"## Group 1\n* Hu\u1ef3nh Ti\u1ebfn D\u0169ng\n* Nguy\u1ec5n Thanh B\u1ea3o Danh\n* Nguy\u1ec5n T\u01b0 Th\u00e0nh Nh\u00e2n","a6742dce":"B\u00e2y gi\u1edd, ta s\u1ebd ki\u1ec3m tra c\u00e1c outliers c\u1ee7a 200 tr\u01b0\u1eddng var_0 ~> var_199","515b2c51":"## <a id='3.2'>3.2. Ph\u00e2n t\u00edch t\u1eebng tr\u01b0\u1eddng d\u1eef li\u1ec7u <\/a> ","20c1b6d9":"## <a id='3.1'> 3.1. T\u1ed5ng quan v\u1ec1 d\u1eef li\u1ec7u","6ad1edef":"# <a id='4'>4. Feature selection and Model building<\/a>","7ae95ca4":"B\u00e0i to\u00e1n n\u00e0y ta \u0111\u00e3 c\u00f3 s\u1eb5n 3 t\u1eadp data d\u01b0\u1edbi d\u1ea1ng file csv t\u1eeb <a href='https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data?select=train.csv'>Kaggle Santander Customer Transaction Prediction <\/a>, l\u00e0 train.csv, test.csv, sample_submission.csv (l\u1ea7n l\u01b0\u1ee3t l\u00e0 t\u1eadp train, t\u1eadp test v\u00e0 t\u1eadp ch\u1ea5m m\u1eabu cho m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y c\u1ee7a ta).","351d99eb":"Ta s\u1ebd t\u00ecm hi\u1ec3u v\u1ec1 ph\u00e2n ph\u1ed1i c\u1ee7a mean, std, min, max","deb61641":"# <a id='3'> 3. Khai ph\u00e1 d\u1eef li\u1ec7u (EDA)","e922c1c0":"C\u00f2n t\u1eadp test th\u00ec sao?","6700d1ed":"Ta s\u1ebd xem 10 features c\u00f3 correlation cao nh\u1ea5t v\u1edbi label target c\u1ee7a ch\u00fang ta.","a07af917":"#### Import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft","e8102b99":"B\u00e0i to\u00e1n c\u00f3 \u0111\u1ea7u v\u00e0o l\u00e0 d\u1eef li\u1ec7u s\u1ed1 c\u1ee7a 200 features c\u1ee7a c\u00e1c kh\u00e1ch h\u00e0ng (\u0111\u00e3 \u0111\u01b0\u1ee3c \u1ea9n danh v\u00e0 c\u00f3 t\u00ean t\u1eeb val_0 -> val_199), ch\u00fang ta c\u1ea7n d\u1ef1a v\u00e0o c\u00e1c ch\u1ec9 s\u1ed1 \u0111\u00f3 \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n xem kh\u00e1ch h\u00e0ng n\u00e0o s\u1ebd mua h\u00e0ng. V\u1edbi m\u1ed7i kh\u00e1ch h\u00e0ng d\u1ef1 \u0111o\u00e1n 0 n\u1ebfu kh\u00f4ng mua h\u00e0ng v\u00e0 1 n\u1ebfu mua h\u00e0ng.\n\nC\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u:\n\n1 - ID_code - Th\u1ee9 t\u1ef1 c\u1ee7a kh\u00e1ch h\u00e0ng - string\n\n2 - val_0 - numerical\n\n...\n\n201 - val_199 - numerical\n\nOutput variable:\n202 - target - D\u1ef1 \u0111o\u00e1n mua \/ kh\u00f4ng mua c\u1ee7a kh\u00e1ch h\u00e0ng - binary 0\/1 \n","83675051":"Ch\u1ea5m k\u1ebft qu\u1ea3 **submission_basicModel.csv** \u0111\u01b0\u1ee3c 0.90026 roc_auc_score","aada8a34":"# Outline\n\n- ### <a href='#1'>1. Problem defining<\/a>\n- ### <a href='#2'>2. Data collecting <\/a>  \n- ### <a href='#3'>3. Khai ph\u00e1 d\u1eef li\u1ec7u (EDA) <\/a>\n    * #### <a href='#3.1'>3.1. T\u1ed5ng quan v\u1ec1 d\u1eef li\u1ec7u<\/a>\n    * #### <a href='#3.2'>3.2. Ph\u00e2n t\u00edch t\u1eebng tr\u01b0\u1eddng d\u1eef li\u1ec7u <\/a> \n    * #### <a href='#3.3'>3.3. Ph\u00e2n t\u00edch correlation gi\u1eefa c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u <\/a> \n- ### <a href='#4'>4. Feature selection and Model building<\/a>\n    * #### <a href='#4.1'>4.1. A Basic Model<\/a>\n    * #### <a href='#4.2'>4.2. M\u1ed9t s\u1ed1 th\u1eed nghi\u1ec7m \u0111\u1ec3 c\u1ea3i ti\u1ebfn model<\/a>","eaf8d660":"#### \u0110\u1ea7u ti\u00ean h\u00e3y ph\u00e2n t\u00edch v\u1ec1 label \"target\" c\u1ee7a t\u1eadp train.","92025ba9":"Ta th\u1ea5y c\u00e1c tr\u01b0\u1eddng c\u1ee7a ta kh\u00f4ng c\u00f3 outliers n\u00e0o th\u1eadt s\u1ef1 n\u1ed5i b\u1eadt, d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c d\u00e0n tr\u1ea3i kh\u00e1 c\u00e2n \u0111\u1ed1i.","e12d8c2c":"C\u00e1c c\u1ed9t \u1edf t\u1eadp train l\u1ea1i c\u00f3 s\u1ed1 l\u1ea7n tr\u00f9ng nhau t\u01b0\u01a1ng t\u1ef1 nh\u01b0 t\u1eadp test. M\u1ed9t \u0111\u1eb7c t\u00ednh kh\u00e1 hay c\u00f3 th\u1ec3 c\u00f3 \u00edch trong ph\u1ea7n build model t\u01b0\u01a1ng lai. B\u00e2y gi\u1edd ta h\u00e3y ki\u1ec3m tra m\u1ed9t s\u1ed1 info v\u1ec1 c\u00e1c d\u1eef li\u1ec7u s\u1ed1.","53514aac":"Ta c\u00f3 m\u1ed9t s\u1ed1 nh\u1eadn x\u00e9t sau:\n* B\u00e0i to\u00e1n classification n\u00e0y kh\u00f4ng c\u00e2n \u0111\u1ed1i. S\u1ed1 kh\u00e1ch mua h\u00e0ng (target = 1) \u00edt h\u01a1n h\u1eb3n so v\u1edbi s\u1ed1 ng\u01b0\u1eddi kh\u00f4ng mua h\u00e0ng (target = 0). Ta c\u1ea7n \u0111\u1ec3 \u00fd \u0111i\u1ec1u n\u00e0y, v\u1ec1 sau c\u00f3 th\u1ec3 d\u00f9ng data augmentation (nh\u01b0 SMOTE ...).\n* C\u00e1c kh\u00e1ch h\u00e0ng mua v\u00e0 kh\u00f4ng mua \u0111\u01b0\u1ee3c ph\u00e2n b\u1ed1 \u0111\u1ed3ng \u0111\u1ec1u, kh\u00f4ng ph\u1ee5 thu\u1ed9c v\u00e0o index xu\u1ea5t hi\u1ec7n c\u1ee7a ch\u00fang.","2bd3722b":"![UMAP-subset-01.png](attachment:4559b359-7edb-4b62-8c79-377bfece750c.png)","e5c4ec90":"# <a id='1'>1. Problem defining","7b03ba84":"Ta s\u1ebd xem 20 c\u1eb7p features c\u00f3 correlation cao nh\u1ea5t.","63664344":"Th\u00f4ng th\u01b0\u1eddng, oversampling hay undersampling s\u1ebd c\u00f3 hi\u1ec7u qu\u1ea3 cao khi data c\u00f3 m\u1ed9t s\u1ef1 ph\u00e2n chia n\u00e0o \u0111\u00f3 \u0111\u1ed1i v\u1edbi c\u00e1c label. N\u00ean khi ta oversampling hay undersampling th\u00ec ta \u0111ang c\u1ed1 gi\u00fap model nh\u1eadn di\u1ec7n r\u00f5 h\u01a1n s\u1ef1 ph\u00e2n bi\u1ec7t \u0111\u00f3 \u0111\u1ec3 t\u0103ng s\u1ef1 ch\u00ednh x\u00e1c. \n\nC\u00f2n nh\u01b0 data c\u1ee7a b\u00e0i n\u00e0y ta th\u1ea5y 2 label 0 v\u00e0 1 d\u00e0n tr\u1ea3i v\u00e0 tr\u1ed9n l\u1eabn \u0111\u1ed3ng \u0111\u1ec1u v\u00e0o nhau. Vi\u1ec7c oversampling hay undersampling s\u1ebd ch\u1ec9 t\u1ea1o th\u00eam noise cho data, c\u00f3 th\u1ec3 n\u00f3 s\u1ebd gi\u00fap nh\u01b0ng s\u1ebd kh\u00f4ng th\u1ef1c s\u1ef1 hi\u1ec7u qu\u1ea3.","326f8982":"Ta s\u1ebd build m\u1ed9t model \u0111\u01a1n gi\u1ea3n tr\u1ef1c ti\u1ebfp v\u00e0o t\u1eadp train \u0111\u1ec3 d\u00f9ng \u0111\u1ec3 so s\u00e1nh v\u1edbi nh\u1eefng b\u01b0\u1edbc c\u1ea3i ti\u1ebfn model ti\u1ebfp theo. Ta s\u1ebd d\u00f9ng LightGBM, v\u00ec n\u00f3 kh\u00e1 nhanh, m\u1ea1nh v\u00e0 d\u1ec5 s\u1eed d\u1ee5ng. Do target c\u1ee7a ta kh\u00e1 imbalance, ta s\u1ebd d\u00f9ng Stratified K-Fold cho model LGBM n\u00e0y.","20a83dcf":"Ta th\u1ea5y s\u1ed1 gi\u00e1 tr\u1ecb unique c\u1ee7a c\u00e1c features l\u1ea1i r\u1ea5t kh\u00e1c nhau. \u0110\u00e2y l\u00e0 m\u1ed9t t\u00ednh ch\u1ea5t c\u1ea7n \u0111\u1ec3 \u00fd cho ph\u1ea7n feature selections. ","be5ab500":"# <a id='2'>2. Data collecting","00062ac1":"#### Load data","8dbe4288":"Sau khi s\u1eed d\u1ee5ng SMOTE \u0111\u1ec3 augment t\u1eadp train v\u00e0 \u0111\u00e1nh gi\u00e1 tr\u00ean t\u1eadp valid, ngay khi v\u1eeba ch\u1ea1y **Fold 0**, ta \u0111\u00e3 th\u1ea5y **training's auc** cao h\u01a1n so v\u1edbi khi ch\u01b0a augment data, nh\u01b0ng **valid_1's auc** l\u1ea1i r\u1ea5t th\u1ea5p so v\u1edbi khi ch\u01b0a augment trong khi model LBGM \u0111\u00e3 \u0111\u01b0\u1ee3c tuning kh\u00e1 nhi\u1ec1u \u0111\u1ec3 tr\u00e1nh overfiting. Ta nh\u1eadn x\u00e9t r\u1eb1ng model \u0111\u00e3 b\u1ecb overfit r\u1ea5t n\u1eb7ng v\u00e0 oversampling d\u01b0\u1eddng nh\u01b0 kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t ph\u01b0\u01a1ng \u00e1n hay \u0111\u1ec3 c\u1ea3i ti\u1ebfn data. \u0110i\u1ec1u n\u00e0y c\u0169ng ch\u1ec9 r\u1eb1ng undersampling c\u0169ng s\u1ebd kh\u00f4ng c\u1ea3i ti\u1ebfn data \u0111\u00e1ng k\u1ec3. \n\n\u0110\u1ec3 ki\u1ec3m ch\u1ee9ng cho assumption tr\u00ean th\u00ec ta s\u1ebd xem train data c\u1ee7a ta b\u1eb1ng UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction).","bbed86f3":"C\u00e1c correlation l\u1ea1i r\u1ea5t th\u1ea5p. H\u00e3y xem ph\u00e2n b\u1ed1 c\u1ee7a correlation \u0111\u1ec3 c\u00f3 c\u00e1i nh\u00ecn t\u1ed5ng qu\u00e1t h\u01a1n.","fbc49217":"Ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n 0 \/ 1 cho t\u1eebng kh\u00e1ch h\u00e0ng. \n\nTa s\u1ebd d\u00f9ng metric ROC AUC score \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 s\u1ef1 hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y, c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng th\u00eam confusion matrix, F1-Score \u0111\u1ec3 c\u00f3 c\u00e1i nh\u00ecn t\u1ed5ng quan h\u01a1n.","ba6ad898":"Ta c\u00f3 th\u1ec3 r\u00fat ra m\u1ed9t s\u1ed1 nh\u1eadn x\u00e9t sau:\n* standard deviation kh\u00e1 l\u1edbn cho c\u1ea3 train v\u00e0 test.\n* min, max, std, mean c\u1ee7a train v\u00e0 test kh\u00e1 gi\u1ed1ng nhau.\n* mean \u0111\u01b0\u1ee3c ph\u00e2n ph\u1ed1i d\u00e0n tr\u00ean m\u1ed9t kho\u1ea3ng l\u1edbn","5ef4485c":"# Santander Customer Transaction Prediction EDA ","b49f5e3e":"\u0110\u1ea7u ti\u00ean, h\u00e3y xem v\u1ec1 correlation gi\u1eefa c\u00e1c c\u1eb7p features v\u1edbi nhau.","28f8c976":"Ta nh\u1eadn x\u00e9t \u1edf c\u1ea3 t\u1eadp train v\u00e0 test ta th\u1ea5y m\u1ecdi features g\u1ea7n nh\u01b0 kh\u00f4ng c\u00f3 correlation n\u00e0o, c\u0169ng nh\u01b0 correlation gi\u1eefa c\u00e1c features v\u00e0 label target c\u0169ng r\u1ea5t th\u1ea5p.","9b06cfd0":"T\u1eadp train g\u1ed3m: \n* **ID_code** (string)\n* **target** (0 \/ 1)\n* **200** tr\u01b0\u1eddng d\u1eef li\u1ec7u s\u1ed1, \u0111\u1eb7t t\u00ean b\u1eaft \u0111\u1ea7u t\u1eeb **var_0** \u0111\u1ebfn **var_199**\n\nT\u1eadp test g\u1ed3m: \n* **ID_code** (string)\n* **200** tr\u01b0\u1eddng d\u1eef li\u1ec7u s\u1ed1, \u0111\u1eb7t t\u00ean b\u1eaft \u0111\u1ea7u t\u1eeb **var_0** \u0111\u1ebfn **var_199**\n\nTa h\u00e3y c\u00f9ng ki\u1ec3m tra c\u00e1c ki\u1ec3u d\u1eef li\u1ec7u v\u00e0 xem c\u00f3 d\u1eef li\u1ec7u thi\u1ebfu (NaN \/ NULL) n\u00e0o kh\u00f4ng."}}