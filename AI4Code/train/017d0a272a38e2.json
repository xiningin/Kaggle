{"cell_type":{"311860c1":"code","d5845204":"code","e2da59a3":"code","201cd2bb":"code","100d1d30":"code","eb938bb9":"code","6ac991f1":"code","bfabed29":"code","816a9fa9":"code","7d2c7f50":"code","655ab27d":"code","d515288f":"code","003fa784":"code","912d1fab":"markdown","d96ded69":"markdown","f249dbfd":"markdown","e91bcddf":"markdown","7ee1d78e":"markdown","f6ea6cda":"markdown"},"source":{"311860c1":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\nimport math\nimport itertools\npy.init_notebook_mode(connected=True)\nimport random\nimport seaborn as sns\n\n\"\"\"XGBoost and Other Helper Functions\"\"\"\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, RepeatedStratifiedKFold, KFold\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport gc\n\nLOOK_AT = 10\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)","d5845204":"data_by_artist = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/data_by_artist.csv\")\ndata_by_year = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/data_by_year.csv\")\nfull_music_data = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/full_music_data.csv\")\ninfluence_data = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/influence_data.csv\")\nmusic_df = pd.read_csv(\"..\/input\/icm-problem-d\/music_genre.csv\")","e2da59a3":"#music_df = music_df.loc[music_df['Genre'] != \"Pop\/Rock\"]","201cd2bb":"genre_list = list(music_df[\"Genre\"].unique())\ngenre_list","100d1d30":"fig = px.bar(music_df.groupby('Genre').size())\nfig.update_layout(title={'text': f\"Distribution of Each Song's Genre\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Count\", showlegend=False)","eb938bb9":"xnn = music_df.select_dtypes(np.number).drop([\"year\", \"Unnamed: 0\"], axis=1)\nX_train = xnn.to_numpy()\n\ny_train = []\nfor genre in music_df[\"Genre\"]:\n    y_train.append(genre_list.index(genre))\ny_train = np.array(y_train)","6ac991f1":"data_dmatrix = xgb.DMatrix(data=xnn, label=y_train)\nparams = {\"objective\":\"multi:softprob\", 'num_class': len(genre_list), 'min_child_weight': 3, 'colsample_bytree': 0.8, 'learning_rate': 0.17,\n                    'max_depth': 10, 'reg_lambda': 1.5, 'subsample': 0.8, 'reg_alpha': 0, 'gamma': 0, 'tree_method': 'gpu_hist'}","bfabed29":"%%time\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    clf = xgb.XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.17,\n        max_depth=10,\n        min_child_weight=3,\n        max_delta_step=0,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        missing=-999,\n        random_state=SEED,\n        n_jobs=-1,\n        reg_lambda=1.5,\n        reg_alpha=0,\n        gamma=0,\n        objective='multi:softprob',\n        tree_method='gpu_hist'\n    )\n\n    clf.fit(X_train[train_index], y_train[train_index], early_stopping_rounds=10, eval_set=[(X_train[test_index], y_train[test_index])], verbose=100)\n    print(np.count_nonzero(np.argmax(clf.predict_proba(X_train[test_index]), axis=1) == y_train[test_index])\/len(y_train[test_index]))\n    gc.collect()","816a9fa9":"np.argmax(clf.predict_proba(X_train[test_index]), axis=1)","7d2c7f50":"%%time\n\nperm = PermutationImportance(clf, random_state=SEED).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = xnn.columns.tolist())","655ab27d":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model","d515288f":"yy_train = []\nfor genre in music_df[\"Genre\"]:\n    my_list = [0 for x in range(len(genre_list))]\n    my_list[genre_list.index(genre)] = 1\n    yy_train.append(my_list)\nyy_train = np.array(yy_train)","003fa784":"batch_size = 256\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nclf = create_mlp(len(X_train[0]), len(yy_train[0]), hidden_units, dropout_rates, label_smoothing, learning_rate)\n\nFOLDS = 5\nes = tf.keras.callbacks.EarlyStopping(monitor='AUC', mode='max', verbose=0, patience=10, restore_best_weights=True)\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, yy_train):\n    %time clf.fit(X_train[train_index], yy_train[train_index], validation_data=(X_train[test_index], yy_train[test_index]), callbacks=[es], epochs=128, batch_size=batch_size, verbose=1)\n    print(np.count_nonzero(np.argmax(clf(X_train[test_index]), axis=1) == y_train[test_index])\/len(y_train[test_index]))\n    gc.collect()\n    break","912d1fab":"Drop pop\/rock potentially? Since there is a significant outlier in the amount of Pop\/Rock songs in the dataset compared to all other songs.","d96ded69":"<h3> Potential XGBoost CV Function <\/h3>","f249dbfd":"<h3> Simple Keras Neural Network <\/h3>\n\nThe following network architecture was copied from a Jane Street Market Prediction model. Although it is certainly not optimized for this task, it holds sufficient to test the efficacy of MLPs for this classification.","e91bcddf":"<h3> XGBoost Feature Importance <\/h3>","7ee1d78e":"# Problem D Prediction Models for Task #3.5\n\nIncluded is also feature importance analysis.","f6ea6cda":"<h3> XGBoost Implementation with some Hyperparameter Optimization <\/h3>"}}