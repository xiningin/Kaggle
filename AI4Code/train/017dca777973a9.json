{"cell_type":{"7aa74628":"code","16dda4d8":"code","33f7f9db":"code","c06bf6bc":"code","7f6a0c24":"code","51d3ca9f":"code","60b8e267":"code","d17678f0":"code","caaaf3f9":"code","d04d33d8":"code","074d461d":"code","c132181c":"code","c99e6064":"code","e32bf4cd":"code","f4a677c8":"code","c2900e1b":"code","b7315c4c":"code","47f1feb6":"code","1972a79b":"code","d51729e1":"code","182b7420":"code","da64c2eb":"code","e4fda00c":"code","618afce8":"code","f93270af":"code","9b569c27":"code","c0d4476f":"code","4e08980f":"code","da3fff7d":"code","3477830e":"code","87383329":"code","db8abc61":"code","e78b1b19":"code","f13607de":"code","25d8bdc1":"code","55f1e625":"code","b6eb8be9":"code","6dd74a7b":"code","b3698dc0":"code","2d7cf46d":"code","2366840e":"code","3007b1e4":"code","319a7b69":"code","b523599f":"code","b995223a":"code","b02fecf0":"code","2b8e3ff3":"code","6fc82986":"code","701bfb45":"code","07ea73c8":"code","4c947486":"code","6498bfd2":"markdown","b2d8923d":"markdown","a3668fd2":"markdown","f7ffd152":"markdown","6c09ce21":"markdown","b6efb640":"markdown","16eb67c2":"markdown","aa168fb2":"markdown","d6f3d24e":"markdown","3cfb8fef":"markdown","b4ea34b1":"markdown","b7b40b7d":"markdown","68f8f043":"markdown","1b6fbc81":"markdown","60359def":"markdown","a909ff71":"markdown","9ad731c4":"markdown","ab7ca773":"markdown","bb43cc1a":"markdown","b470bc04":"markdown","23144a76":"markdown","0c3b000f":"markdown","b90e9c5c":"markdown","01cba0fc":"markdown","d927ba7b":"markdown","6dd369c2":"markdown","79803fdf":"markdown","fa64cf68":"markdown","cf0d5b3b":"markdown","f0d13ce2":"markdown","cae8e3f7":"markdown","85e02cd7":"markdown","38b7cd6d":"markdown","fb96cf4a":"markdown","7da240a1":"markdown","55983708":"markdown","ccfa62d0":"markdown","8e649108":"markdown","b0373051":"markdown","06eba5f1":"markdown","a4b630b9":"markdown","06a0ed2f":"markdown","ce31eefe":"markdown","4afbe93a":"markdown","47188e19":"markdown","42476e63":"markdown","de543042":"markdown","fdf7f60c":"markdown","03d78810":"markdown","0986366e":"markdown","8b46748d":"markdown","77100d29":"markdown","46ca1474":"markdown","b7036896":"markdown","30cbc99b":"markdown","d967a4b9":"markdown","90f6e40c":"markdown","52964017":"markdown","8262dd84":"markdown","50aeb60c":"markdown","91502fcc":"markdown","1b446cba":"markdown","7b47fc0e":"markdown","fde7d731":"markdown","78d141b0":"markdown"},"source":{"7aa74628":"import matplotlib.pyplot as plt\nimport cv2\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 50,20\nimg=cv2.imread(\"..\/input\/private-score\/score.JPG\")\nplt.imshow(img)","16dda4d8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\/\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","33f7f9db":"train = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv')\ntest = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/test.csv')\ndf = train","c06bf6bc":"print(train.shape)\ntrain.head()","7f6a0c24":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_num = df.select_dtypes(include=numerics)       #  numeric dataframe\nobjects = ['O']\ndf_cat = df.select_dtypes(include=objects)\nprint(df_num.shape,df_cat.shape)\nprint(df_cat.columns,'\\n','--------------------------------------------------------------------------------','\\n',df_num.columns)","51d3ca9f":"for i in df_cat.columns:\n    print('The unique values in '+i+' are: ',df[i].nunique(),'\\n',df_cat[i].unique(),'\\n',\"--------------------------------------------------------------------------------\")","60b8e267":"print(df.isnull().sum().sum(axis=0))","d17678f0":"temp=df.y.values\ndf_cat['y']=temp\nprint(df_cat.head())","caaaf3f9":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(rc={'figure.figsize':(14,9)})\nplt.subplot(221)\nplt.title(\"Outlier Detection in target column via Boxplot\")\nplt.ylabel(\"Values of y\")\nplt.grid(True)\nsns.boxplot(y=df[\"y\"],color='gold')\nplt.subplot(222)\nplt.title(\"Outlier Detection in target column via Histogram\")\nplt.grid(True)\nax = sns.distplot(df.y,color='green',bins=22)\nplt.show()","d04d33d8":"sns.set(rc={'figure.figsize':(20,7)})\nplt.title(\"y Analysis\")\nplt.ylabel(\"Values of y\")\nplt.scatter(range(df.shape[0]),np.sort(df.y.values),color='orange')","074d461d":"print((df.loc[df.y>150,'y'].values))\ndf=df[df.y<150]\nprint(\"Removing outliers based on above information and setting 150 as a threshold value . . . . . . . . . . . . . . . . . . . . \")\nprint(df.shape)\ndf_cat=df_cat[df_cat.y<150]\ndf_num=df_num[df_num.y<150]","c132181c":"sns.set(rc={'figure.figsize':(20,7)})\nsns.regplot(x='ID', y='y', data=df,color='maroon')","c99e6064":"from scipy import stats\nrcParams['figure.figsize'] = 15, 7\nres = stats.probplot(df['y'], plot=plt)","e32bf4cd":"res = stats.probplot(np.log1p(train[\"y\"]), plot=plt)","f4a677c8":"rcParams['figure.figsize'] = 22, 8\nfor i in df_cat.columns:\n    if i not in 'y':\n        plt.figure()\n        plt.xlabel=i\n        sns.stripplot(x=i, y=\"y\", data=df,jitter=True, linewidth=1,order=np.sort(df[i].unique()))\n        sns.boxplot(x=i, y=\"y\", data=df, order=np.sort(df[i].unique()))\n        plt.show()","c2900e1b":"pd.crosstab([df_cat.X2], [df_cat.X0], margins=True).style.background_gradient(cmap='autumn_r')","b7315c4c":"temp = []\nfor i in df_num.columns:\n    if df[i].var()==0:\n        temp.append(i)\nprint(len(temp))\nprint(temp)","47f1feb6":"count=0\nlow_var_col=[]\nfor i in test.columns:\n    if test[i].dtype == 'int64':\n        if test[i].var()<0.01:\n            low_var_col.append(i)\n            count+=1\nprint(count)\n\ndf.drop(low_var_col,axis=1,inplace=True)\ndf_num.drop(low_var_col,axis=1,inplace=True)\ntest.drop(low_var_col,axis=1,inplace=True)","1972a79b":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_num = df.select_dtypes(include=numerics)","d51729e1":"dic={}\nfor i in df_num.columns:\n    if i!='y':\n        if df[i].corr(df.y)>0.25 or df[i].corr(df.y)<-0.25:\n            dic[i]=df[i].corr(df.y)\nprint(\"Important Features with there respective correlations are \",'\\n','---------------------------------------------------------','\\n',dic)","182b7420":"print(df.X119.corr(df.X118),'\\n', df.X29.corr(df.X54) ,'\\n', df.X54.corr(df.X76) ,'\\n', df.X263.corr(df.X279))","da64c2eb":"# Dublicate features\nd = {}; done = []\ncols = df.columns.values\nfor c in cols: d[c]=[]\nfor i in range(len(cols)):\n    if i not in done:\n        for j in range(i+1, len(cols)):\n            if all(df[cols[i]] == df[cols[j]]):\n                done.append(j)\n                d[cols[i]].append(cols[j])\ndub_cols = []\nfor k in d.keys():\n    if len(d[k]) > 0: \n        dub_cols += d[k]        \nprint('Dublicates:','\\n', dub_cols)","e4fda00c":"corrs=[]\nhigh_corr=[]\nfor i in range(0,len(dub_cols)):\n    for j in range(i+1,len(dub_cols)):\n        if df[dub_cols[i]].corr(df[dub_cols[j]]) >=0.90:\n            corrs.append(df[dub_cols[i]].corr(df[dub_cols[j]]))\n            high_corr.append((dub_cols[i],dub_cols[j]))\nprint(corrs)\nprint(\"\\n\")\nprint(high_corr)","618afce8":"df.drop(['X279','X76','X37','X134','X147','X222','X244','X326'] , axis=1 , inplace=True)","f93270af":"test.drop(['X279','X76','X37','X134','X147','X222','X244','X326'] , axis=1 , inplace=True)\ndf_num.drop(['X279','X76','X37','X134','X147','X222','X244','X326'] , axis=1 , inplace=True)","9b569c27":"from sklearn import preprocessing\ncategorical=[]\nfor i in df.columns:\n    if df[i].dtype=='object':\n        le = preprocessing.LabelEncoder()\n        le.fit(list(df[i].values) + list(test[i].values))\n        print(\"Categories in the encoded order from 1 to the size of \"+i+\" are : \")\n        print(le.classes_)\n        print(\"--------------------------------------------------------------------------\")\n        df[i] = le.transform(list(df[i].values))\n        test[i] = le.transform(list(test[i].values))\n        categorical.append(i)","c0d4476f":"correlation_map = df[df.columns[1:10]].corr()\nobj = np.array(correlation_map)\nobj[np.tril_indices_from(obj)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(9,10)\nsns.heatmap(correlation_map, mask=obj,vmax=.7, square=True,annot=True)","4e08980f":"import xgboost as xgb\ntrain_y = df[\"y\"].values\ntrain_X = df.drop(['y'], axis=1)\n\ndef xgb_r2_score(preds, final):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'n_trees': 520, \n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.98,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(train_y), # base prediction = mean(target)\n    'silent': 1\n}\n\nfinal = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params), final, num_boost_round=200, feval=xgb_r2_score, maximize=True)\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(model, max_num_features=40, height=0.8, ax=ax, color = 'coral')\nprint(\"Feature Importance by XGBoost\")\nplt.show()\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:40]\n\nplt.subplots(figsize=(10,10))\nplt.title(\"Feature importances by RandomForestRegressor\")\nplt.ylabel(\"Features\")\nplt.barh(range(len(indices)), importances[indices], color=\"green\", align=\"center\")\nplt.yticks(range(len(indices)), feat_names[indices], rotation='horizontal')\nplt.ylim([-1, len(indices)])\nplt.show()","da3fff7d":"df['X314_plus_X315'] = df.apply(lambda row: row.X314 + row.X315, axis=1)\ntest['X314_plus_X315'] = test.apply(lambda row: row.X314 + row.X315, axis=1)","3477830e":"print(\"Correalation between X314_plus_X315 and y is :  \",df.y.corr(df['X314_plus_X315']))\nprint(\"Which makes it pretty much high !! Awesome !!\")","87383329":"#df['X122_plus_X128'] = df.apply(lambda row: row.X122 + row.X128, axis=1)\n#test['X122_plus_X128'] = test.apply(lambda row: row.X122 + row.X128, axis=1)","db8abc61":"#print(\"Correlation between X122_plus_X128 and y is :  \",df.y.corr(df['X122_plus_X128']))","e78b1b19":"df['X118_plus_X314_plus_X315'] = df.apply(lambda row: row.X118 + row.X314 + row.X315, axis=1)\ntest['X118_plus_X314_plus_X315'] = test.apply(lambda row: row.X118 + row.X314 + row.X315, axis=1)","f13607de":"print(\"Correalation between X118_plus_X314_plus_X315 and y is :  \",df.y.corr(df['X118_plus_X314_plus_X315']))\nprint(\"Which makes it pretty much high !! Awesome !!\")","25d8bdc1":"df[\"X10_plus_X54\"] = df.apply(lambda row: row.X10 + row.X54, axis=1)\ntest[\"X10_plus_X54\"] = test.apply(lambda row: row.X10 + row.X54, axis=1)\nprint(\"Correalation between X10_plus_X54 and y is :  \",df.y.corr(df['X10_plus_X54']))","55f1e625":"df[\"X10_plus_X29\"] = df.apply(lambda row: row.X10 + row.X29, axis=1)\ntest[\"X10_plus_X29\"] = test.apply(lambda row: row.X10 + row.X29, axis=1)\nprint(\"Correalation between X10_plus_X29 and y is :  \",df.y.corr(df['X10_plus_X29']))","b6eb8be9":"train_X['X314_plus_X315']=df['X314_plus_X315']\n#train_X['X122_plus_X128']=df['X122_plus_X128']\ntrain_X['X118_plus_X314_plus_X315']=df['X118_plus_X314_plus_X315']\ntrain_X[\"X10_plus_X54\"] = df[\"X10_plus_X54\"]\ntrain_X[\"X10_plus_X29\"] = df[\"X10_plus_X29\"]","6dd74a7b":"corr_val=[]\nsame_features=[]\nfor i in range(0,len(df_num.columns)-1):\n    for j in range(i+1,len(df_num.columns)):\n        temp_corr=df[df_num.columns[i]].corr(df[df_num.columns[j]])\n        if temp_corr>=0.95 or temp_corr<=-0.95: \n            same_features.append((df_num.columns[i],df_num.columns[j]))\n            corr_val.append(temp_corr)\nprint(len(corr_val))\nprint(same_features)","b3698dc0":"booler = np.ones(400)\nfor i in same_features:\n    if booler[int(i[1][1:])]==1:\n        booler[int(i[1][1:])]=0\n        df_num.drop(i[1],axis=1,inplace=True)\n        df.drop(i[1],axis=1,inplace=True)\n        test.drop(i[1],axis=1,inplace=True)\n        train_X.drop(i[1],axis=1,inplace=True)\n    elif booler[int(i[0][1:])]==1:\n        booler[int(i[0][1:])]=0\n        df_num.drop(i[0],axis=1,inplace=True)\n        df.drop(i[0],axis=1,inplace=True)\n        test.drop(i[0],axis=1,inplace=True)\n        train_X.drop(i[0],axis=1,inplace=True)","2d7cf46d":"model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeature_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:40]\n\nplt.subplots(figsize=(10,10))\nplt.title(\"Feature importances by RandomForestRegressor\")\nplt.ylabel(\"Features\")\nplt.barh(range(len(indices)), importances[indices], color=\"green\", align=\"center\")\nplt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal')\nplt.ylim([-1, len(indices)])\nplt.show()\n\nfinal = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params), final, num_boost_round=1350, feval=xgb_r2_score, maximize=True)\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(model, max_num_features=40, height=0.8, ax=ax,color = 'coral')\nprint(\"Feature Importance by XGBoost\")\nplt.show()","2366840e":"print(train_X.shape , test.shape)","3007b1e4":"list(set(train_X.columns)-set(test.columns))","319a7b69":"'''from sklearn.preprocessing import OneHotEncoder\ntotal_hot=np.concatenate( (train_X.values[:,1:9], test.values[:,1:9]), axis=0)\nenc = OneHotEncoder()\nenc.fit(total_hot)\ntotal_hot=enc.transform(total_hot)'''","b523599f":"'''total_hot.todense().shape'''","b995223a":"'''train_hot=total_hot.todense()[:4194,:]\ntest_hot=total_hot.todense()[4194:8404,:]\nprint(train_hot.shape)\ntrain_X_hot=np.concatenate( (train_X.values[:,0].reshape(4194,1),train_hot) , axis=1)\ntest_hot=np.concatenate( (test.values[:,0].reshape(4209,1),test_hot) , axis=1)\ntrain_X_hot=np.concatenate( (train_X_hot,train_X.values[:,9:]) , axis=1)\ntest_hot=np.concatenate( (test_hot,test.values[:,9:]) , axis=1)'''","b02fecf0":"'''print(train_X_hot.shape, test_hot.shape)'''","2b8e3ff3":"'''from sklearn.decomposition import PCA\npca=PCA(n_components=6 , random_state=7)\npca.fit(train_X_hot)\npca_train_X = pca.transform(train_X_hot)\npca_test = pca.transform(test_hot)\n\nprint(pca.explained_variance_ratio_.sum())\nprint(\"--------------------------------------------------------------\")\nprint(pca.components_)\nprint(\"--------------------------------------------------------------\")\nprint(pca.components_.shape)\nprint(\"--------------------------------------------------------------\")\nprint(pca_train_X.shape , pca_test.shape)\n'''","6fc82986":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2, random_state=420)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(test)\n\nxgb_params = {\n    'n_trees': 500, \n    'eta': 0.0050,\n    'max_depth': 3,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(train_y), # base prediction = mean(target)\n    'silent': 1\n}\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=10)","701bfb45":"d_train = xgb.DMatrix(train_X, label=train_y)\n#d_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(test)\n\nxgb_params = {\n    'n_trees': 500, \n    'eta': 0.0050,\n    'max_depth': 3,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(train_y), \n    'silent': 1\n}\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train')]\n\nclf = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=10)","07ea73c8":"Answer = clf.predict(d_test)\n\nsub = pd.DataFrame()\nsub['ID'] = test.ID\nsub['y'] = Answer\nsub.to_csv('mercedes_benz_The_best_or_Nothing.csv', index=False)","4c947486":"sub.head()","6498bfd2":"Taking a total of 378 feature count to 185...","b2d8923d":"**STEPS for removing above redundancy . . .**","a3668fd2":"**But ,  YOU MUST SEE THAT SOME FEATURES ARE HAVING SAME CORRELATIONS THAT COULD INDICATE THE POSSIBLE DUPLICATE FEATURES. Lets check them too . . **","f7ffd152":"#### Now, finding correlations of each category with other . The increasing or decreasing class encoded value can be found from the categories written in the encoded order above.","6c09ce21":"Taking X10 and X54","b6efb640":"**Duplicate features. **","16eb67c2":"Getting the dictionary of important correlated features with target column y","aa168fb2":"<img src=\"https:\/\/media.giphy.com\/media\/fQoCOuFL7DlR6zYRnw\/giphy.gif\" \/>","d6f3d24e":"**This clearly shows the outliers are above a value of approx. 137.5. Well we will remove outliers after 150.**","3cfb8fef":"* Booler is the array of 1 and 0 for keeping the track of the features that we have dropped from the multiple dataframes and allowing the execution of cell without off any error of column not found. (Also the features are being repeated in multiple pairs.) \n* Initially the booler is taken as all of 1,  considering the fact that all features are present in the dataframe and later we would make them zeroes one by one. \n* The steps are like , we will target the 2nd feature of each pair and check for its existence, if its present then remove it or else if check for the 1st feature in the pair , and if this also is not present then simply skip that particular pair of feature. \n* The booler would be used to check the existence of that features in dataframes.","b4ea34b1":"Making a dense matrix from sparse","b7b40b7d":"## Please upvote if you like . . . ","68f8f043":"**Again, correlation threshold of 0.9 has been judged and taken after multiple experiments .....**","1b6fbc81":"**A very distinct and conspicuous point around 275 in boxplot and also the green area in histogram. This noise has to removed.**","60359def":"\nUpdating the df_num dataframe after droping the features from original dataframe df.","a909ff71":"# Lets do some Feature Engineering . . .\n\n* Following are some features that I have engineered after multiple trials  .\n* These are just the results (the new features engineered) of the work that I have done for long.\n* Since the features are anonymised, that makes pretty much difficult to do feature engineering.\n* I have made some 2 way and 3 way interactions of the features which prove to be pretty much useful.\n* Also there correlations are higher than their parent features which makes them even better.\n* Feel free to write new features engineered by yourself in the comment section below.","9ad731c4":"Removing columns with zero ovariance","ab7ca773":"** Taking 0.25 as threshold on grounds of experimental changes . . .**","bb43cc1a":"## Making the submission file ...Check yourself for the authentication of script claiming 78th place on private leaderboard with a score of 0.55282 which is Top 2 %. ","b470bc04":"Taking X122 and X128","23144a76":"\n* Removing the features from the main dataframe that are involving zero variance or are having constant value inorder to remove redundancy and increase model performance later.\n* Also checking the individual correlation of the features and getting some idea about individual feature importance.\n* There are total 13 variables with zero variance , therefore they must be dropped.\n* Checking for duplicate features in this large set.\n* Feature selection multiple times .....","0c3b000f":"## That look great the new features engineered have outperformed the existing features in the data in the RandomForrest feature importance plot.","b90e9c5c":"Taking X314 and X315","01cba0fc":"**Looking into each categorical feature **","d927ba7b":"**A separate dataframe to study only categorical features and there mutual relationship and also the one with target column y.**","6dd369c2":"## Check for output tab of the notebook and check the score after submitting it . . . ","79803fdf":"\n**Preparing the data for feature importance**","fa64cf68":"Updating the dataframe for feature importance , the one we used above.","cf0d5b3b":"Again setting a threshold of 0.01 for variance for each column and removing them too. The removed columns are also being removed from all the temporary dataframes.","f0d13ce2":"**This dataset has some real problem with the number of categories.**","cae8e3f7":"Appending the encoded categories to ID vector and then appending the rest dataframe of numerical features to this newly formed dataframe. Similarly doing this to test matrix.","85e02cd7":"### Outlier detection and removal .... A bit cleaning....","38b7cd6d":"Checking correlations among a set of duplicate features and preparing pairs who are highly correlated.","fb96cf4a":"* Taking the numeric dataframe df_num and finding all the features with very high correlation an dchecking for them. Also making the pairs of them as above.\n* Turns out to be a list of 63 features again that are highly correlated. \n**Again, the value of 0.95 has been experimentally judged and taken , there is no thumb rule to take the threshold value.**","7da240a1":"This also shows the gradient of change using colour change. As you can see as in X2 is the most popular category and leaving most of them with zero. Similarly it can be tested on any 2 more than two categories at the same time to check concurrent occurences of any pairs, triads, quadruplets , etc . . . . ","55983708":"# \u2666 Mercedes Benz - The Best or Nothing \u2666","ccfa62d0":"### Down is the full kernel . . . .","8e649108":"Github link : https:\/\/github.com\/deadskull7\/Mercedes-Benz-Challenge-78th-Place-Solution-Private-LB-0.55282-Top-2-Percent-","b0373051":"\nThis shows that are dataframe is containing some duplicate features which are having correlation of approx. 1. We will remove this redundancy also using some feature selection . .  .","06eba5f1":"**There seems a difference between the feature importances by the two models. You can check above, RandomForest is giving the feature importance more on the basis of the important correlations of target wrt numerical features that we have already figured out above.**","a4b630b9":"**y wrt ID of dataframe.**","06a0ed2f":"Taking X10 and X29","ce31eefe":"This dataset is very dirty !! Believe it ....We have to clean it to the utmost level we can to feed into our model to achieve the high accuracy that we aspire.","4afbe93a":"Validating our XGboost...Finding the best hyperparameters.","47188e19":"Though in XGboost they have earned a little less position but still higher enough to be considered as good work for model performance.","42476e63":"\n## Now lets see some jitter on boxplots . . .","de543042":"Using 12 as components so as to still retain a variance of ~98%.","fdf7f60c":"<img src=\"http:\/\/starchop.altervista.org\/wp-content\/uploads\/2015\/02\/Mercedes-Benz-Logo-Rain-HD-Wallpaper.jpg\"   \/>","03d78810":"Later tried to append the PCA , SVD ,sparse random projections to the dataframe but still got degraded model performance...Please do tell me if anybody knows the answer for this....","0986366e":"## Further data cleaning . . .  ","8b46748d":"**This shows a very slight decreasing trend of y wrt ID , maybe cars later in series took less time in test bench. This gives ID an importance while estimating y.**","77100d29":"**This states that X29, X54, X76, X127, X136, X162, X166, X178,  X232,  X250,  X261, X263, X272,  X276, X279, X313, X314, X328  are important features later we will select using some selection techniques. **","46ca1474":"Label encoding the categorical features ","b7036896":"\n\nIn these stripplots with boxplots superimposed,  we find the following:\n\n*     X0 and X2 have a large amount of diversity in their levels. Among those two, X0 shows the most obvious effect of grouping.\n\n*     The lowest y values (i.e. shortest times) appear to be predominantly caused by 6 feature levels: X0:az, X0:bc, X1:y, X2:n, X5:h, X5:x. Together, these ones are a pretty good predictor for having low y.\n\n*     Level X0:aa appears to have a notably higher average y than all other features, but consists only of two data points. This is very obvious with the jitter plots.\n\n*     X3, X5, X6, X8 and to a certain extent X1 show distributions that are largely similar among the different levels","30cbc99b":"\n**370 numerical features and 8 categorical features**","d967a4b9":"### Below is the code for one hot encoding and creating a sparse matrix of around 211 features. But commented it out because it gave me degraded performance. Don't know why ....if anybody knows the answer than please comment it below... I would love to listen.","90f6e40c":"The y values of the dataset appears to be skewed","52964017":"Turn out to be there are 146 columns for removal purpose.","8262dd84":"## Some important feature correlations with the target variable.","50aeb60c":"Taking Log Transformation","91502fcc":"Taking X118 , X314 and X315","1b446cba":"### Till here I was in 20 % on private leaderboard but after hyperparameter tuning I landed in top 2% .","7b47fc0e":"Now, training the whole dataset on selected parameters so as to avoid any data loss.","fde7d731":"\n**Bivariate analysis using Cross-tabulation**","78d141b0":"There are different number of categories in train and test datset. Encountered"}}