{"cell_type":{"743d976c":"code","b38343ac":"code","f7fe1d7d":"code","e4caf735":"code","1a5cfec0":"code","7372b6df":"code","e150f9b3":"code","0a676424":"code","9a647e75":"code","87a4e6ed":"code","bdc86148":"code","f476189a":"code","26c629fb":"code","fd009c93":"code","63ad5fba":"code","c204b764":"code","8c0a9f84":"code","f470e425":"code","07d62bcf":"code","3917c521":"code","a434111e":"code","779504ac":"code","e38d3aa5":"code","33a2386f":"code","19523fb1":"code","69635f54":"code","543fa025":"code","6bb8e2fd":"code","b1850af8":"code","6320d481":"code","bdce7b23":"code","9f6c5645":"code","b7667c00":"code","6f1edf45":"code","198fe44b":"code","e76cfcee":"code","5a2f82dd":"code","5abe904d":"code","23b7668d":"code","38e2fe8f":"code","f32a346f":"code","7fd10181":"code","3aab65b5":"code","5df07e33":"code","3f05408f":"code","05735a35":"code","a8531258":"code","90c2616e":"code","32660afc":"code","6362e336":"code","2f228340":"code","ccd9cfd3":"code","9b76d21f":"code","246bbb7e":"code","9ed1a342":"code","ca7fc45e":"code","b6eb91d3":"code","704f877d":"code","ec874d87":"code","f5253b49":"code","14356a6c":"code","a38a6b44":"code","fa3d5b16":"code","8de74f91":"code","46f56604":"code","4e0aaa59":"code","ed2a59bc":"code","46f76af6":"code","fa36a2bc":"code","b99cf860":"markdown","e21b9349":"markdown","6381ff35":"markdown","0a089c26":"markdown","7658e98b":"markdown","12b94140":"markdown","d6ca5b65":"markdown","5cb73c23":"markdown","a296fc58":"markdown","ac439bbb":"markdown","401b18c0":"markdown","d858fff4":"markdown","c99f15e4":"markdown","f657bc19":"markdown","2e917ed5":"markdown","ec575592":"markdown","9848b1a4":"markdown","cbba39d6":"markdown","d43a6694":"markdown","39b15b6b":"markdown","25a141ff":"markdown","cfcd71fc":"markdown","a96ba9f4":"markdown","553041da":"markdown","dfb18d2b":"markdown","c8986d4c":"markdown","de7a2c99":"markdown","96cb9a80":"markdown","79aefe39":"markdown","f417f81d":"markdown","740171b2":"markdown","057c341f":"markdown","3a037e56":"markdown","6618fa30":"markdown"},"source":{"743d976c":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","b38343ac":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/Santander\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","f7fe1d7d":"%%time\ntrain_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","e4caf735":"train_df.shape, test_df.shape","1a5cfec0":"train_df.head()","7372b6df":"test_df.head()","e150f9b3":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return (np.transpose(tt))","0a676424":"%%time\nmissing_data(train_df)","9a647e75":"%%time\nmissing_data(test_df)","87a4e6ed":"%%time\ntrain_df.describe()","bdc86148":"%%time\ntest_df.describe()","f476189a":"def plot_feature_scatter(df1,df2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig,ax=plt.subplots(4,4,figsize=[14,14])\n    \n    for feature in features:\n        i+=1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature],df2[feature],marker='+')\n        plt.xlabel(feature,fontsize=9)\n    plt.show()","26c629fb":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(train_df[::20],test_df[::20], features)","fd009c93":"sns.countplot(train_df['target'])","63ad5fba":"print(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]\/train_df.shape[0]))","c204b764":"def plot_feature_distribution(df1,df2,label1,label2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig,ax = plt.subplots(10,10,figsize=[18,22])\n    \n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature],bw=0.5,label=label1)\n        sns.kdeplot(df2[feature],bw=0.5,label=label2)\n        plt.xlabel(feature,fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n        plt.tick_params(axis='y',which='major',labelsize=6)\n    plt.show()","8c0a9f84":"t0 = train_df.loc[train_df['target']==0]\nt1 = train_df.loc[train_df['target']==1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0,t1,'0','1',features)","f470e425":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","07d62bcf":"features = train_df.columns.values[2:102]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","3917c521":"features = train_df.columns.values[102:202]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","a434111e":"plt.figure(figsize=[6,6])\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\",kde=True,bins=120,label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\",kde=True,bins=120,label='test')\nplt.legend()\nplt.show()","779504ac":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","e38d3aa5":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","33a2386f":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","19523fb1":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","69635f54":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","543fa025":"#I think this code is better than orgin code\n\ncorrelations = train_df[features].corr().where(np.triu(np.ones(train_df[features].corr().shape),k=1).astype(np.bool))\ncorrelations_df = correlations.abs().unstack().dropna().sort_values().reset_index()\ncorrelations_df.shape","6bb8e2fd":"correlations_df.head(5)","b1850af8":"correlations_df.tail(5)","6320d481":"[col for col in correlations.columns if any(abs(correlations[col])>0.95)]","bdce7b23":"%%time\nfeatures = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature,values.max(),values.idxmax()])\n\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature,values.max(),values.idxmax()])","9f6c5645":"np.transpose(pd.DataFrame(unique_max_train,columns=['Feature','Max duplicates','Values']).sort_values(by='Max duplicates',ascending=False).head(15))","b7667c00":"np.transpose(pd.DataFrame(unique_max_test,columns=['Feature','Max duplicates','Values']).sort_values(by='Max duplicates',ascending=False).head(15))","6f1edf45":"%%time\n\ni = 1\nfor df in [test_df, train_df]:\n    idx = df.columns.values[i:i+200]\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n    df['range'] = df['max']-df['min']\n    i = i + 1","198fe44b":"train_df[train_df.columns[202:]].head()","e76cfcee":"test_df[test_df.columns[201:]].head()","5a2f82dd":"features = train_df.columns.values[2:]\ncorrelations = train_df[features].corr().where(np.triu(np.ones(train_df[features].corr().shape),k=1).astype(np.bool))\ncorrelations_df = correlations.abs().stack().reset_index().rename(columns={0:'corr'}).sort_values(by='corr')\ncorrelations_df.shape","5abe904d":"correlations_df.head()","23b7668d":"correlations_df.tail()","38e2fe8f":"drop_cols = [col for col in correlations.columns if any(abs(correlations[col])>0.95)]","f32a346f":"print(\"Shape of train_df: {}, test_df: {}\".format(train_df.shape,test_df.shape))","7fd10181":"train_df = train_df.drop(columns=drop_cols)\ntest_df = test_df.drop(columns=drop_cols)","3aab65b5":"print(\"Shape of train_df: {}, test_df: {}\".format(train_df.shape,test_df.shape))","5df07e33":"def plot_new_feature_distribution(df1,df2,label1,label2,features):\n    i = 0\n    sns.set_style('whitegrid')\n    fig, ax = plt.subplots(2,4,figsize=[18,8])\n    \n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature],bw=0.5,label=label1)\n        sns.kdeplot(df2[feature],bw=0.5,label=label2)\n        plt.xlabel(feature,fontsize=11)\n        locs, lables = plt.xticks()\n        plt.tick_params(axis=\"x\",which=\"major\",labelsize=8)\n        plt.tick_params(axis=\"y\",which=\"major\",labelsize=8)\n    plt.show()","3f05408f":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","05735a35":"features = train_df.columns.values[202:]\nplot_new_feature_distribution(train_df, test_df, 'train', 'test', features)","a8531258":"print('Train and test columns: {} {}'.format(len(train_df.columns), len(test_df.columns)))","90c2616e":"train = train_df.drop(columns=['ID_code','target'])\ntrain_label = train_df['target']\ntest = test_df.drop(columns='ID_code')\ntest_ids = test_df['ID_code']","32660afc":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import make_scorer,roc_auc_score\n# from sklearn.model_selection import cross_val_score\n\n# scorer = make_scorer(roc_auc_score,greater_is_better=True)\n\n# rf = RandomForestClassifier(random_state=12,n_estimators=15000,n_jobs=-1)\n\n# rf.fit(train,train_label)\n\n# # cv_score = cross_val_score(rf,train,train_label,cv=5,scoring=scorer)\n# # print(f\"5 fold cv score is {cv_score.mean()}\")\n\n# indices = np.argsort(rf.feature_importances_)[::-1]\n# feature_names = train.columns\n# importances = rf.feature_importances_\n\n# df = pd.DataFrame(columns=['feature','importance'])\n# df['feature'] = feature_names\n# df['importance'] = importances\n\n\n# df.sort_values(by='importance',ascending=False).tail()","6362e336":"# from sklearn.feature_selection import RFECV\n\n# estimator = RandomForestClassifier(random_state=12,n_estimators=15000,n_jobs=-1)\n\n# selector = RFECV(estimator,step=1,cv=5,scoring=scorer,n_jobs=-1)\n\n# selector.fit(train,train_label)","2f228340":"import re\nimport string\n\ndef del_punct(one_list):\n    \n    return_list = []    \n    regex = re.compile('['+re.escape(\"'\")+']')\n    \n    for element in one_list:\n        return_list.append(regex.sub(\" \",element).strip())\n    \n    return return_list","ccd9cfd3":"def distinguish_str(value_list):\n    \n    output = []\n    \n    regex = re.compile('[0-9]')\n    \n    for i,element in enumerate(value_list):\n        if regex.search(element):\n            output.append(float(element))\n        else:\n            output.append(element)\n    \n    return output","9b76d21f":"def model_gbm(train,train_label,test,test_ids,nfolds=5,hyperparameters=None):\n    \n    feature_names = list(train.columns)\n    \n    valid_scores = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    \n    feature_importance_df = pd.DataFrame()\n    \n    max_iters_df = pd.DataFrame(columns=[\"folds\",\"iters\"])\n    \n    iters = []\n    folds = []\n    \n    if hyperparameters:\n        params = hyperparameters\n        \n#         If you guys get hyperparams below dataframe by hyperopt, the dictionary will be string type!! \n#         So You should change to dict following commented area. \n#         But, As I mentioned below, I'll put my hyperparams what tested at colab environment already!!\n        \n#         keys = []\n#         values = []\n        \n#         integer_elements = ['subsample_freq','max_depth','num_leaves','subsample_for_bin','min_child_samples','n_estimators']\n        \n#         for element in params[1:-1].split(\",\"):\n#             keys.append(element.split(\":\")[0])\n#             values.append(element.split(\":\")[1])\n            \n#         keys = del_punct(keys)\n#         values = distinguish_str(del_punct(values)) \n        \n#         params = dict(zip(keys,values))\n\n#         for element in integer_elements:\n#             params[element] = int(params[element])\n\n        del(params['n_estimators'])\n        \n        params['boost_from_average'] = True\n        params['seed'] = 31452\n        params['feature_fraction_seed'] = 31415\n        params['bagging_seed'] = 31415\n        params['drop_seed'] = 31415\n        params['data_random_seed'] =31415\n        params['metric'] = 'auc'\n    \n    #The hyperparams where I got from Gabriel's code\n    else:\n        params = {\n        'num_leaves': 6,\n        'max_bin': 63,\n        'min_data_in_leaf': 45,\n        'learning_rate': 0.01,\n        'min_sum_hessian_in_leaf': 0.000446,#min_child_weight\n        'bagging_fraction': 0.55, \n        'bagging_freq': 5, \n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 31452,\n        'feature_fraction_seed': 31415, \n        'feature_fraction': 0.51, #colsample_by_tree => \ub9e4 \ud2b8\ub9ac \uc0dd\uc131\uc2dc \uac00\uc838\uc624\ub294 \ud53c\uccd0\uc758 \uac1c\uc218\n        'bagging_seed': 31415, #\ubc30\uae45\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uba74 \uc4f0\ub294 \uc2dc\ub4dc\n        'drop_seed': 31415,\n        'data_random_seed': 31415,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False\n    }\n    \n    strfkold = StratifiedKFold(n_splits=nfolds,shuffle=True,random_state=12)\n    \n    for i,(train_indices,valid_indices) in enumerate(strfkold.split(train.values,train_label.values)):\n        \n        print(\"{} fold processing\".format(i+1)+\"#\"*20)\n        \n        d_train = lgb.Dataset(train.values[train_indices,:],label = train_label[train_indices])\n        d_valid = lgb.Dataset(train.values[valid_indices,:],label = train_label[valid_indices])\n        \n        n_rounds = 15000\n        \n        lgb_model = lgb.train(params,d_train,num_boost_round=n_rounds,valid_sets=[d_train,d_valid],valid_names=['train','valid'],verbose_eval=1000,early_stopping_rounds=250)\n        \n        valid_scores[valid_indices] = lgb_model.predict(train.values[valid_indices,:],num_iteration=lgb_model.best_iteration)\n        \n        fold_importance_df = pd.DataFrame(columns=[\"Feature\",\"importance\",\"fold\"])\n        fold_importance_df[\"Feature\"] = feature_names\n        fold_importance_df[\"importance\"] = lgb_model.feature_importance()\n        fold_importance_df[\"fold\"] = i + 1\n        \n        feature_importance_df = pd.concat([feature_importance_df,fold_importance_df],axis=0)\n        \n        folds.append(i+1)\n        iters.append(lgb_model.best_iteration)\n        \n        predictions += lgb_model.predict(test.values,num_iteration=lgb_model.best_iteration)\/nfolds    \n        \n        display(\"valid_set score is %f and best_iteration is %d of %d fold\"%(roc_auc_score(train_label[valid_indices],valid_scores[valid_indices]),lgb_model.best_iteration,i+1))\n        \n    max_iters_df[\"folds\"] = folds\n    max_iters_df[\"iters\"] = iters\n    \n    display(\"CV score of valid_set for %d fold is %f and maximum of best_iteration is %d of %d fold\"%(nfolds,roc_auc_score(train_label,valid_scores),max_iters_df['iters'].max(),max_iters_df['iters'].idxmax()+1))\n    \n    return valid_scores,predictions,feature_importance_df","246bbb7e":"from hyperopt import hp,tpe,Trials,fmin, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample","9ed1a342":"import csv\nimport ast\nfrom timeit import default_timer as timer","ca7fc45e":"def lgb_roc_auc(labels,predictions):\n#     print(predictions)\n#     predictions = predictions.reshape(len(np.unique(labels)),-1).argmax(axis=0)\n    \n    metric_value = roc_auc_score(labels,predictions)\n    \n    return 'roc_auc',metric_value,True","b6eb91d3":"def objective(hyperparameters, nfold=5):\n    \n    global ITERATION\n    ITERATION += 1\n    \n    for parameter_name in ['max_depth','num_leaves','subsample_for_bin','min_child_samples','subsample_freq']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    strkfold = StratifiedKFold(n_splits=nfold,shuffle=True)\n        \n    features = np.array(train)\n    labels = np.array(train_label).reshape((-1))\n        \n    valid_scores = []\n    best_estimators = []\n    run_times = []\n    \n    model = lgb.LGBMClassifier(**hyperparameters,n_jobs=-1,metric='None',n_estimators=1000)\n        \n    for i, (train_indices,valid_indices) in enumerate(strkfold.split(features,labels)):\n\n        print(\"#\"*20,\"%d fold of %d itertaion\"%(i+1,ITERATION))\n        \n        X_train,X_valid = features[train_indices],features[valid_indices]\n        y_train,y_valid = labels[train_indices], labels[valid_indices]\n            \n        start = timer()\n        #250 \/ 1000    \n        model.fit(X_train,y_train,early_stopping_rounds=50,\n                eval_metric=lgb_roc_auc,eval_set=[(X_train,y_train),(X_valid,y_valid)],\n                eval_names=['train','valid'],verbose=200)\n            \n        end = timer()\n            \n        valid_scores.append(model.best_score_['valid']['roc_auc'])\n            \n        best_estimators.append(model.best_iteration_)\n            \n        run_times.append(end-start)\n            \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1-score\n        \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n        \n    estimators = int(np.mean(best_estimators))\n    hyperparameters['n_estimators'] = estimators\n        \n    of_connection = open(OUT_FILE,'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss,hyperparameters,ITERATION,run_time,score,score_std])\n    of_connection.close()\n    \n    display(f'Iteration: {ITERATION}, Score: {round(score, 4)}.')\n    \n    if ITERATION % PROGRESS == 0:\n        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n    \n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}","704f877d":"space = {\n    'boosting_type':'gbdt',\n    'objective':'binary',\n    'is_unbalance':True,\n    'subsample': hp.uniform('gbdt_subsample',0.5,1),\n    'subsample_freq':hp.quniform('gbdt_subsample_freq',1,10,1),\n    'max_depth': hp.quniform('max_depth',5,20,3),\n    'num_leaves': hp.quniform('num_leaves',20,60,10),\n    'learning_rate':hp.loguniform('learning_rate',np.log(0.025),np.log(0.25)),\n    'subsample_for_bin':hp.quniform('subsample_for_bin',2000,100000,2000),\n    'min_child_samples': hp.quniform('min_child_samples',5,80,5),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0),\n    'min_child_weight':hp.uniform('min_child_weight',0.01,0.000001)\n}","ec874d87":"sample(space)","f5253b49":"algo = tpe.suggest","14356a6c":"# Record results\ntrials = Trials()\n\n# Create a file and open a connection\nOUT_FILE = 'optimization.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nMAX_EVALS = 10\nPROGRESS = 10\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\nwriter.writerow(headers)\nof_connection.close()","a38a6b44":"import datetime\n\nprint(\"beginning time is {}\".format(datetime.datetime.now()))\ndisplay(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,max_evals = MAX_EVALS)\n\nprint(\"end time is {}\".format(datetime.datetime.now()))","fa3d5b16":"import json\n\nwith open('trials.json','w') as f:\n    f.write(json.dumps(str(trials)))","8de74f91":"results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()\nresults.head()","46f56604":"plt.figure(figsize=[8,6])\nsns.regplot('iteration','score',data=results);\nplt.title('OPT Scores')\nplt.xticks(list(range(1,results.iteration.max()+1,3)))","4e0aaa59":"hyperparameters = results.hyperparameters[0]","ed2a59bc":"hyperparameters = {\n    'boosting_type': 'gbdt',\n    'colsample_bytree': 0.7812943473676428,\n    'is_unbalance': True,\n    'learning_rate': 0.012732207618246335,\n    'max_bin': 200,\n    'max_depth': 14,\n    'min_child_samples': 70,\n    'min_child_weight': 0.0010242091278688855,\n    'num_leaves': 10,\n    'objective': 'binary',\n    'subsample': 0.8026192939361728,\n    'subsample_for_bin': 72000,\n    'subsample_freq': 7,\n    'n_estimators': 6589}","46f76af6":"val_scores, predictions, gbm_fi= model_gbm(train,train_label,test,test_ids,hyperparameters=hyperparameters)","fa36a2bc":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv(\"submission.csv\",index=False)","b99cf860":"## <a id='34'>Features correlation<\/a>  \uceec\ub7fc\uac04 \uc0c1\uad00\uad00\uacc4\n\nWe calculate now the correlations between the features in train set.  \nThe following table shows the first 10 the least correlated features.\n\n\uc6b0\ub9ac\ub294 \ud6c8\ub828\uc138\ud2b8\uc5d0 \uceec\ub7fc\uac04\uc5d0 \uc0c1\uad00\uad00\uacc4\ub97c \uacc4\uc0b0\ud574\ubcf4\ub824\uace0\ud569\ub2c8\ub2e4. \n\uc544\ub798\uc758 \ud14c\uc774\ube14\uc740 \ucc98\uc74c 10\uac1c\uc758 \uc0c1\uad00\uad00\uacc4 \ud2b9\uc9d5\ub4e4\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n\n> Reference from about guidelines about correlations <br \/>https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\n\n#### The general guidelines for correlation values are below, but these will change depending on who you ask (source for these)\n\n* 00-.19 \u201cvery weak\u201d <br \/>\n* 20-.39 \u201cweak\u201d <br \/>\n* 40-.59 \u201cmoderate\u201d <br \/>\n* 60-.79 \u201cstrong\u201d <br \/>\n* 80-1.0 \u201cvery strong\u201d <br \/>\n\nWhat these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n\nIn that Kernel, he droped one of the columns what have high corrleation between them above 0.95\nSo, I'd like to drop them also here.\nBut we don't have any columns what I told above. So I don't delete anything about 200 coulmns","e21b9349":"### Final phase of hyperopt","6381ff35":"## make a model using our own hyperparameters!!","0a089c26":"The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n\n\ud6c8\ub828\uc14b\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc740 numeric \ubcc0\uc218\ub4e4\uc758 \ubd84\ud3ec\ub4e4\uc774 \uc798 \uade0\ud615\uc744 \uac16\ucd94\uace0 \uc788\ub294 \ub4ef \ud569\ub2c8\ub2e4.\n\n## <a id='33'>Distribution of mean and std<\/a>  \n\ud3c9\uade0\uacfc \ud45c\uc900\ud3b8\ucc28\uc758 \ubd84\ud3ec\n\nLet's check the distribution of the mean values per row in the train and test set.\n\n\ud589\ubcc4\ub85c \ud6c8\ub828\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc758 \ud3c9\uade0 \uac12\uc758 \ubd84\ud3ec\ub97c \uc54c\uc544\ubd05\uc2dc\ub2e4.","7658e98b":"# <a id='4'>Feature engineering<\/a>  \n\nThis section is under construction.  \n\nLet's calculate for starting few aggregated values for the existing features.","12b94140":"### Making user metric for objective function","d6ca5b65":"The correlation between the features is very small. \n\n## <a id='35'>Duplicate values<\/a>  \uc911\ubcf5\uac12 \ucc98\ub9ac\n\nLet's now check how many duplicate values exists per columns.\n\n\uceec\ub7fc\ub2f9 \uc5bc\ub9c8\ub098 \uc911\ubcf5\ub41c \uac12\ub4e4\uc774 \uc788\ub294\uc9c0 \ud655\uc778 \ud574\ubcf4\uc790","5cb73c23":"# Feature Selection\n\n**In here, I'd like to select features via SFM and REFCV but I couldn't. Because this data set is so huge as you guys know!! So this I'll try later...","a296fc58":"## So We could get some results about comparing two API \"lightgbm.train()\" and \"lightgbm.LGBMClassifier\"\n\n### common params btw two APIs\n\n* boosting_type': 'gbdt' ==  'boosting_type': 'gbdt' \n* 'feature_fraction' == 'colsample_bytree'\n* 'is_unbalance': True == 'class_weight': None \n* 'learning_rate' == 'learning_rate' \n* 'max_depth' == 'max_depth'\n* 'min_data_in_leaf' == 'min_child_samples'\n* 'min_sum_hessian_in_leaf' == 'min_child_weight'    \n* num_round == 'n_estimators'\n* 'num_leaves' ==  'num_leaves'\n* 'objective' == 'objective'\n* 'seed' == 'random_state'\n* 'subsample' == 'bagging_fraction'\n* 'subsample_freq' == 'baggin_freq'\n* 'subsample_for_bin' == 'bin_construct_sample_cnt' [**Gabriel didn't tuning it**]\n\n### only in lightgbm.LGBMClassifier()\n\n* 'importance_type'\n* 'min_split_gain'\n* 'silent'\n* 'class_weight'\n* 'reg_alpha'\n* 'reg_lambda'\n**(But, I don't know when I should tune about 'reg_xx' If someone knows it plz comment at this kernel)**","ac439bbb":"### For recording our result of hyperopt","401b18c0":"### Objective Function\n\nP.S) I do this process **briefly**, cuz this is **time-consumming process** as I mentioned before <br \/>\nSo, I recommend to set like this if you do yourself in own environment <br \/>\n\n* n_estimators => 15000\n* early_stopping_rounds => 250\n* verbose => 1000\n","d858fff4":"## REFCV","c99f15e4":"\n## <a id='32'>Density plots of features<\/a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**.","f657bc19":"Hi there guys. <br \/>\nI'm a **person who have jumped into kaggle three month ago**. During studying many enlighting expert's kernels, I've felt kind of **embarrassed feeling** about using hyperparameters of major algorithms such as Xgboost and LightGBM. You guys could reply my opinion like this, **\"Why you blame your fault to them?\"** \n### But, I definitely have **HUGE THANKS TO THEM!!** **Thanks to SUPER BRILLIANT EXPERTS OF KAGGLE** <br \/>\n\nThe reason why I make this kernel is that some people use **\"lightgbm.train\"** and the others use **\"lightgbm.LGBMClassifier\"** for their model. When I see the differences of them, It makes me insane!! Because the **parameters btw two kinds of kernel as I said above seem pretty different!!** <br \/>\n\nSo, In this kernel, I'll discover <br \/>\n**1. the true meaning of them and aliases of hyperparameters by looking official document of lightgbm.** <br \/>\n**2. parameter tuning by referring two website where I'll comment below notebooks** <br \/>\n\n### I hope that **two kinds of people** to see this kernel,\n\n1. **One is for people who have felt simliar feeling like me.** For them, I'll describe as detail as I can what I've learned and I'd like to share magnificant post for explaining what the Gradient Boosting and the Xgboost are!!<br \/>\n\nThe posts are below!!\n* Gradient Boost\n>  https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n* Xgboost\n> https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\n\n2. **I hope THE OTHERS are enlighting experts** who could give comment about what I understands through this kernel, I would really happy if you guys comment this kernel !! and Could you guys give me a great post about LightGBM parameter or parmeter tuning?? cuz I already have few posts about GBM and XGBoost but I don't have about LightGBM!! (I know generally it seems same one but I think there is regularization in LightGBM) \n\nThere is Korean comments for my studying for each sentences by Gabriel Preda's explaination. But I didn't only copy and paste this code. I've changed some code for my own!! \n\n# I'm staying to tune hyperparameters and I will frequently update this Kernel frequently!!.","2e917ed5":"## SFM","ec575592":"### Defining Space for Hyperparameters","9848b1a4":"## Hyperparameter Tunning using Hyperopt\n\n### The thing what I can do from below kernel is tunning parameters what we saw above through hyperopt!!\n> https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\n\n#### In this phase we need to comply following 4 phases\n1. making objective function\n2. defining space for parameters\n3. choosing algorithm for hyperopt\n4. using all of them through fmin of hyperopt\n\n### I'd like to complie all of precess using hyperopt but you guys know this process is pretty time-consuming!!!\n**So I'll post my hyperparameters via this process and finally I put in the gbm_model for making predictions!!**","cbba39d6":"## I could get the params below like that by changing params few times using colab, You guys shoud do that not getting someone's params!! \nIf Kaggle's session was long, I'll use hyperparameters what I got from hyporpot above. But you know, it's not! So, I'll use my own parameters to be gotten through hyperopt in colab environment!!","d43a6694":"We can observe that there is a considerable number of features with significant different distribution for the two target values.  \nFor example, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** and many others.\n\n\uc6b0\ub9ac\ub294 \ub450 \uac1c\uc758 \ud0c0\uac9f\uac12\uc5d0 \ub530\ub77c\uc11c \uc0c1\ub2f9\uc774 \ub2e4\ub978 \ubd84\ud3ec\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uc0c1\ub2f9\ud55c \uc218\uc758 \ud2b9\uc9d5\ub4e4\uc744 \uc0b4\ud3b4\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uba74, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** \uc640 \ub2e4\ub978 \uac83\ub4e4 \ub9d0\uc785\ub2c8\ub2e4.\n\nAlso some features, like **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196** shows a distribution that resambles to a bivariate distribution.\n\n\uadf8\ub9ac\uace0 \uba87\uba87 \ud2b9\uc9d5\ub4e4, **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196**, \uc740 \uc774\ubcc0\ub7c9\ubd84\ud3ec\uc640 \ub2ee\uc740 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\nWe will take this into consideration in the future for the selection of the features for our prediction model.  \n\n\uc6b0\ub9ac\ub294 \uc774\uac83\ub4e4\uc744 \uc6b0\ub9ac\uc758 \uc608\uce21\ubaa8\ub378\uc5d0 feature selection\uc2dc\uc5d0 \uace0\ub824\ud558\ub294 \ucc38\uace0\uc790\ub8cc\ub85c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\nLe't s now look to the distribution of the same features in parallel in train and test datasets. \n\n\uadf8\ub807\ub2e4\uba74 \uc774\uc81c\ub294 \ud6c8\ub828\uc14b\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc744 \ud3c9\ud589\uc801\uc73c\ub85c \uac19\uc774 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\nThe first 100 values are displayed in the following cell. Press <font color='red'>**Output**<\/font> to display the plots.\n\n\uccab \ubc88\uc9f8 100\uac1c\uc758 \uac12\ub4e4\uc740 \uc544\ub798\uc758 \uadf8\ub9bc\uacfc \uac19\uc774 \uc0dd\uacbc\uc2b5\ub2c8\ub2e4.","39b15b6b":"### Make a sample via space what we defined","25a141ff":"# <a id='5'>Model<\/a>  \n\nFrom the train columns list, we drop the ID and target to form the features list.","cfcd71fc":"We can make few observations here:   \n\uc6b0\ub9ac\uac00 \ucc3e\uc740 \uac83\ub4e4\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n* standard deviation is relatively large for both train and test variable data;\n\ud6c8\ub828 \ub370\uc774\ud130\uc640 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ubaa8\ub450 \ud45c\uc900\ud3b8\ucc28\uac00 \ud06c\ub2e4\ub294 \uac83\n* min, max, mean, sdt values for train and test data looks quite close;\n\ucd5c\uc18c,\ucd5c\ub300,\ud3c9\uade0,\ud45c\uc900\ud3b8\ucc28 \uac12\uc774 \ud6c8\ub828\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc5d0\uc11c \ubc00\uc811\ud574 \ubcf4\uc778\ub2e4\ub294 \uac83\n* mean values are distributed over a large range.\n\ud3c9\uade0\uac12\uc758 \ubcc0\ub3d9\uc774 \ud06c\ub2e4\ub294 \uac83\n\nThe number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n\ud6c8\ub828\uacfc \ud14c\uc2a4\ud2b8 \uc14b\uc5d0\uc11c\uc758 \uac12\uc758 \uc218\ub294 \ub3d9\uc77c\ud558\ub2e4. \uadf8\ub807\ub2e4\uba74 \uba87\uba87 \ud2b9\uc9d5\ub4e4\uc5d0 \ub300\ud574\uc11c \uc0b0\ud3ec\ub3c4\ub97c \uadf8\ub824\ubd05\uc2dc\ub2e4.\n","a96ba9f4":"### Selecting algorithm\nThis algorithm is called by Tree Parzen Estimators. but I don't know how it works.. **So I'll keep trying to understanding!!! Or if you guys have a good site for TPE plz comment below!!**","553041da":"## Load data   \n\nLet's check what data files are available.\n\n\uc6b0\ub9ac\uac00 \uc0ac\uc6a9\uac00\ub2a5\ud55c \ub370\uc774\ud130 \ud30c\uc77c\ub4e4\uc744 \uc54c\uc544 \ubd05\uc2dc\ub2e4.","dfb18d2b":"Train contains:  \n\n* **ID_code** (string);  \n* **target**;  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\n\ud6c8\ub828\uc138\ud2b8\ub294. ID, Target \uadf8\ub9ac\uace0 200\uac1c\uc758 \uc22b\uc790\uac12\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\nTest contains:  \n\n* **ID_code** (string);  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\n\ud14c\uc2a4\ud2b8 \uc14b\uc5d0\ub294 \ud0c0\uac9f\uac12\uc744 \uc81c\uc678\ud55c \uac83\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\nLet's check if there are any missing data. We will also chech(*k) the type of data.\n\n\uc190\uc2e4\uac12\ub4e4\uc5d0 \ub300\ud574\uc11c \ud55c\ubc88 \uc0b4\ud3b4\ubcfc\uae4c\uc694> \uadf8\ub9ac\uace0 \ub370\uc774\ud130\ub4e4\uc758 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc54c\uc544\ubd05\uc2dc\ub2e4.\n\nWe check first train.\n\n\uba3c\uc800 \ud6c8\ub828\uc138\ud2b8\uc785\ub2c8\ub2e4.","c8986d4c":"# Reference\n\nGabriel Preda's santander-eda-and-prediction\n> https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\nthis kernel uses lightgbm.train for prediction\n\nWill Koehrsen's A Complete Introduction and Walkthrough [Costa Rican Houshold] \n> https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\nthis kernel uses LGBMClassifier for prediction\n\nRudolph's Porto: xgb+lgb kfold LB 0.282 [Porto Seguro\u2019s Safe Driver Prediction]\n> https:\/\/www.kaggle.com\/rshally\/porto-xgb-lgb-kfold-lb-0-282\nthis kernel uses lightgbm.train for prediction","de7a2c99":"### making the plot using hyperopt","96cb9a80":"## INFOMATION ABOUT PARAMS\n\n### The params what used at Gabriel's code\n\n    params = {\n        'num_leaves': 6,\n        'max_bin': 63,\n        'min_data_in_leaf': 45,\n        'learning_rate': 0.01,\n        'min_sum_hessian_in_leaf': 0.000446,\n        'bagging_fraction': 0.55, \n        'bagging_freq': 5, \n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 31452,\n        'feature_fraction_seed': 31415,\n        'feature_fraction': 0.51,\n        'bagging_seed': 31415,\n        'drop_seed': 31415,\n        'data_random_seed': 31415,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }\n\n### The params when I make lightgbm.LGBMClassifier.get_params()\n\n    params = {   \n      'boosting_type': 'gbdt', \n      'class_weight': None,\n      'colsample_bytree': 1.0,\n      'importance_type': 'split',\n      'learning_rate': 0.1,\n      'max_depth': -1,\n      'min_child_samples': 20,\n      'min_child_weight': 0.001,\n      'min_split_gain': 0.0,\n      'n_estimators': 100,\n      'n_jobs': -1,\n      'num_leaves': 31,\n      'objective': None,\n     'random_state': None,\n     'reg_alpha': 0.0,\n     'reg_lambda': 0.0,\n     'silent': True,\n     'subsample': 1.0,\n     'subsample_for_bin': 200000,\n     'subsample_freq': 0\n        }\n\n> Reference from <br \/>\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html <br \/>\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n\n* 'boosting_type': 'gbdt' <br \/>\n **alias with boosting** (Default:gbdt, options gbdt,gbrt,rf,random_forest,dart,goss)\n \n* 'class_weight': None <br \/>\n(default=None) \u2013 Weights associated with classes in the form {class_label: weight}. **Use this parameter only for multi-class classification task**; **for binary classification task** you may use **is_unbalance or scale_pos_weight parameters.** The \u2018balanced\u2019 mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  \n \n* 'colsample_bytree': 1.0 <br \/>\n(Default=1.0 \/ constraints 0.0 < value <= 1.0) **alias: feature_fraction**[simliar with max_features of GBM]  <br \/>    \n lightgbm will randomly select iteration if feature_fraction smaller than 1.0.\n e.g if I set it 0.8 lightgbm will select 80% of features before training each tree.\n can be used to speed up training.\n can be used to deal with over-fitting.\n\n* 'importance_type': 'split' (default=\"split\") <br \/>\nHow the importance is calculated. If \u201csplit\u201d, result contains numbers of times the feature is used in a model. If \u201cgain\u201d, result contains total gains of splits which use the feature. <br \/>\n=> sort of method how to gain feature_importance\n\n* 'learning_rate': 0.1 <br \/>\n (Default=1.0 \/ constraints learning_rate > 0.0) **alias with with shrinkage_rate,eta**\n \n* 'max_depth': -1 <br \/>\n  limit the max depth for tree model. This is used to deal with overfitting when data is small\n \n* 'min_child_samples': 20 <br \/>\n  (Default = 20 \/ constraints min_data_in_leaf >= 0) **alias with min_data_in_leaf,min_data-per_leaf,min_data,min_child_samples** \n\n* 'min_child_weight': 0.001 <br \/>\n (Default = 1e-3 \/\/ Default min_sum_hessian_in_leaf >= 0.0) <br \/>\n **alias with min_sum_hessian_in_leaf,min_sum_hessian_per_leaf,min_sum_hessian,min_hessian,min_child_weight **<br \/>\n\n* 'min_split_gain': 0.0  **only in lightgbm.LGBMClassifier()[not in lightgbm.train()]<br \/>\n (Default =0.0 \/ constraints: min_gain_to_splot >= 0.0) **alias with min_gain_to_split,min_split_gain** <br \/>\n the minimal gain to perform split  <br \/>\n \n* 'n_estimators': 100 <br \/>\n (Default = 100 \/ constraints n_estimator >= 0) **alias with num_iteration,n_iter,num_tree,num_trees,num_round,num_rounds,num_boost_round,n_estimators** <br \/>\n number of boosting iterations\n\n* 'n_jobs': -1 <br \/>\n(Default = 0) **alias with num_thread,nthread,nthreads,n_jobs**\n\n* 'num_leaves': 31 <br \/>\n (Default = 31 \/ constraints: num_leaves > 1) **aliases: num_leaf, max_leaves, max_leaf** <br \/>  \n max number of leaves in one tree\n    \n* 'objective': None <br \/>\n(Default = regression \/ options: regression, regression_l1, huber, fair, poisson, quantile, mape, gammma, tweedie, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank)\n **aliases: objective_type, app, application**\n    \n* 'random_state': None <br \/>\n(Default = None) **aliases: random_seed, random_state** <br \/>\nthis seed is used to generate other seeds, e.g. data_random_seed, feature_fraction_seed, etc. <br \/>\nby default, this seed is unused in favor of default values of other seeds <br \/>\nthis seed has lower priority in comparison with other seeds, which means that it will be overridden, if you set other seeds explicitly <br \/>\n\n* 'reg_alpha': 0.0 <br \/>\n(Default = 0.0 \/ constraints: lambda_l1 >= 0.0) **aliases: reg_alpha** <br \/> \nL1 regularization <br \/>\n\n* 'reg_lambda': 0.0 <br \/>\n(Default = 0.0 \/  constraints: lambda_l2 >= 0.0) **aliases: reg_lambda, lambda** <br \/>\nL2 regularization <br \/>\n\n* 'silent': True **only in lightgbm.LGBMClassifier()(not in lightgbm.train())**<br \/>\nsilent (bool, optional (default=False)) \u2013 Whether to print messages during construction\n\n* 'subsample': 1.0 <br \/>\n(Default = 1.0 \/ constraints: 0.0 < bagging_fraction <= 1.0 ) **aliases: sub_row, subsample, bagging** <br \/> \nlike feature_fraction, but this will randomly select part of data without resampling <br \/> \ncan be used to speed up training <br \/> \ncan be used to deal with over-fitting <br \/> \nNote: to enable bagging, bagging_freq should be set to a non zero value as well <br \/> \n\n* 'subsample_for_bin': 200000 <br \/>\n(Default = 200000 \/ constraints: bin_construct_sample_cnt > 0)  **aliases: subsample_for_bin** <br \/> \nnumber of data that sampled to construct histogram bins <br \/>\nsetting this to larger value will give better training result, but will increase data loading time <br \/>\nset this to larger value if data is very sparse <br \/>\n\n* 'subsample_freq': 0\n(Default = 0) **aliases: subsample_freq, frequency for bagging** <br \/>\n0 means disable bagging; k means perform bagging at every k iteration <br \/>\nNote: to enable bagging, bagging_fraction should be set to value smaller than 1.0 as wel <br \/>l\n\n* 'reg_alpha': 0.0\n(default = 0.0) **aliases: reg_alpha**<br \/> \nconstraints: lambda_l1 >= 0.0 \/\/  L1 regularization<br \/>\n\n* 'reg_lambda': 0.0\n(default = 0.0) **aliases: reg_lambda, lambda** <br \/>\nconstraints: lambda_l2 >= 0.0 \/\/ L2 regularization","79aefe39":"sum has perfect correaltion wth mean. So, I'd like to delete sum instead of mean.","f417f81d":"Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.\n\n\ud6c8\ub828\uc138\ud2b8\uc640 \ud14c\uc2a4\ud2b8\uc138\ud2b8\uc5d0\uc11c \uac19\uc740 \uceec\ub7fc\ub4e4\uc774 \uac19\uac70\ub098 \uac00\uae4c\uc6b4 \uc591\uc758 \uc911\ubcf5\uac12\uc744 \uac00\uc9c0\uba70 \uc774 \uc911\ubcf5\uac12\uc758 \uac12 \ub610\ud55c \uac19\uac70\ub098 \ube44\uc2b7\ud588\ub2e4. \uc774\ub294 \ub098\uc911\uc5d0 \uc0ac\uc6a9\ud558\uae30\uc5d0\ub3c4 \ud765\ubbf8\ub85c\uc6b4 \ud328\ud134\uc774\ub2e4.","740171b2":"# <a id='1'>Introduction<\/a>  \n\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n\n\uc774\ubc88 \ucef4\ud53c\ud2f0\uc158\uc5d0\uc11c\ub294 \uc5b4\ub290 \uc18c\ube44\uc790\ub4e4\uc774 \ud6d7\ub0a0\uc5d0 \ud604\uae08\uc744 \uc778\ucd9c\ud560 \uac83\uc778\uc9c0\ub97c \uad6c\ubd84\ud558\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4. \uc774\ubc88 \ub300\ud68c\uc758 \ub370\uc774\ud130\ub294 \uc2e4\uc81c \ub370\uc774\ud130\uc640 \uac19\uc740 \uad6c\uc870\ub85c \uc81c\uacf5\ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n\n\ub370\uc774\ud130\ub294 \ubb34\uae30\uba85\uc73c\ub85c \ub418\uc5b4\uc788\uace0 \uac01\uac01\uc758 row\ub294 200\uac1c\uc758 \uc11c\ub85c \ub2e4\ub978 \uceec\ub7fc\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\nIn the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n\n\ub2e4\uc74c\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ub370\uc774\ud130\ub97c \uc0b4\ud3b4\ubcf4\uace0, \ubaa8\ub378\ub9c1 \uc900\ube44\ub97c\ud558\uace0, \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0 \ud0c0\uac9f \uac12\uc744 \ud14c\uc2a4\ud2b8\uc14b\uc5d0\uc11c \uc608\uce21\ud558\uace0 \uc81c\ucd9c\uae4c\uc9c0 \ud574\uc19d\uc2dc\ub2e4.","057c341f":"# <a id='3'>Data exploration<\/a>  \n\n## <a id='31'>Check the data<\/a>  \n\nLet's check the train and test set.\n\n\ud6c8\ub828\uc14b\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc744 \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.","3a037e56":"Let's load the train and test data files.","6618fa30":"Both train and test data have 200,000 entries and 202, respectivelly 201 columns. \n\n\ud6c8\ub828\uc14b\uacfc \ud14c\uc2a4\ud2b8\uc14b \ubaa8\ub450 200,000\uac1c\uc758 \ud589\uc744\uac00\uc9c0\uace0 \uac01\uac01 202, 201 \uac1c\uc758 \uceec\ub7fc\uc218\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\nLet's glimpse train and test dataset.\n\n\uac04\ub2e8\ud558\uac8c \ub450 \uc138\ud2b8\ub97c \uc0b4\ud3b4\ubcfc\uae4c\uc694.\n"}}