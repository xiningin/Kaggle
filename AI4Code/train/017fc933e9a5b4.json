{"cell_type":{"2725d92d":"code","d593c296":"code","ad7c0151":"code","12c03191":"code","b0327111":"code","04c3e7bb":"code","ce07f670":"code","6f49239f":"code","68ebd4d3":"markdown","4fa3086f":"markdown","7490e5cb":"markdown","8ab41e4c":"markdown","ce10ba14":"markdown","db6f5fe8":"markdown","84120e09":"markdown","7aef4f13":"markdown"},"source":{"2725d92d":"!pip install bs4\nimport requests\nimport re\nimport os\nimport bs4\nfrom pathlib import Path\nimport glob\nfrom zipfile import ZipFile\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import clear_output","d593c296":"# Set base values\ndownload_folder = Path.cwd()\nbase_url = 'https:\/\/opendata.dwd.de\/climate_environment\/CDC\/observations_germany\/climate\/hourly\/air_temperature\/historical\/'\n\n\n# Initiate Session and get the Index-Page\nwith requests.Session() as s:\n    resp = s.get(base_url)\n\n# Parse the Index-Page for all relevant <a href>\nsoup = bs4.BeautifulSoup(resp.content)\nlinks = soup.findAll(\"a\", href=re.compile(\"stundenwerte_TU_.*_hist.zip\"))\n\n# For testing, only download 10 files\nfile_max = 10\ndl_count = 0\n\n#Download the .zip files to the download_folder\nfor link in links:\n    zip_response = requests.get(base_url + link['href'], stream=True)\n    # Limit the downloads while testing\n    dl_count += 1\n    if dl_count > file_max:\n        break\n    with open(Path(download_folder) \/ link['href'], 'wb') as file:\n        for chunk in zip_response.iter_content(chunk_size=128):\n            file.write(chunk)\n\nprint('Done')","ad7c0151":"# make directory 'imports'\nos.mkdir('imports')","12c03191":"# Folder definitions\ndownload_folder = Path.cwd()\nimport_folder = Path.cwd() \/ 'imports'\n\n# Find all .zip files and generate a list\nunzip_files = glob.glob('stundenwerte_TU_*_hist.zip')\n\n# Set the name pattern of the file we need\nregex_name = re.compile('produkt.*')\n\n# Open all files, look for files that match ne regex pattern, extract to 'import'\nfor file in unzip_files:\n    with ZipFile(file, 'r') as zipObj:\n        list_of_filenames = zipObj.namelist()\n        extract_filename = list(filter(regex_name.match, list_of_filenames))[0]\n        zipObj.extract(extract_filename, import_folder)\n\ndisplay('Done')","b0327111":"os.makedirs('export_uncleaned\/to_clean')","04c3e7bb":"import_files = glob.glob('imports\/*')\nfor file in import_files:\n    df333 = pd.read_csv(file, delimiter=\";\")\ndf333.head()","ce07f670":"import_files = glob.glob('imports\/*')\nout_file = Path.cwd()\/'export_uncleaned'\/'to_clean'\n\nobsolete_columns = [\n                    'QN_9',\n                    'RF_TU',\n                    'eor']\n\nmain_df = pd.DataFrame()\ni = 1\n\nfor file in import_files:\n\n    # Read in the next file\n    df = pd.read_csv(file, delimiter=\";\")\n\n    # Prepare the df before merging (Drop obsolete, convert to datetime, filter to date, set index)\n    df.drop(columns=obsolete_columns, inplace=True)\n    df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H\")\n    df = df[df['MESS_DATUM']>= \"2007-01-01\"]\n    df.set_index(['MESS_DATUM', 'STATIONS_ID'], inplace=True)\n\n    # Merge to the main_df\n    main_df = pd.concat([main_df, df])\n\n    # Display some status messages\n    clear_output(wait=True)\n    display('Finished file: {}'.format(file), 'This is file {}'.format(i))\n    display('Shape of the main_df is: {}'.format(main_df.shape))\n    i+=1\n\n# Check if all types are correct\ndisplay(main_df['TT_TU'].apply(lambda x: type(x).__name__).value_counts())\n\n# Make sure that to files or observations a duplicates, eg. scan the index for duplicate entries.\n# The ~ is a bitwise operation, meaning it flips all bits.\nmain_df = main_df[~main_df.index.duplicated(keep='last')]\n\n\n# Unstack the main_df\nmain_df = main_df.unstack('STATIONS_ID')\ndisplay('Shape of the main_df is: {}'.format(main_df.shape))\n\n# Save main_df to a .csv file and a pickle to continue working in the next step\nmain_df.to_pickle(Path(out_file).with_suffix('.pkl'))\nmain_df.to_csv(Path(out_file).with_suffix('.csv'), sep=\";\")\n\ndisplay(main_df.head())\ndisplay(main_df.describe())\n","6f49239f":"# Import and export paths\npkl_file = '.\/export_uncleaned\/to_clean.pkl'\ncleaned_file = '.\/export_uncleaned\/to_clean.csv'\n\n# Read in the pickle file from the last cell\ncleaning_df = pd.read_pickle(pkl_file)\n\n# Replace all values with \"-999\", which indicate missing data\ncleaning_df.replace(to_replace=-999, value=np.nan, inplace=True)\n\n# Resample to daily frequency\ncleaning_df = cleaning_df.resample('D').mean().round(decimals=2)\n\n# Save as .csv\ncleaning_df.to_csv(cleaned_file, sep=\";\", decimal=\",\")\n\n# Show some results for verification\ndisplay(cleaning_df.loc['2011-12-31':'2012-01-04'])\ndisplay(cleaning_df.describe())\ndisplay(cleaning_df)","68ebd4d3":"## <span style=\"color:red\"> Hope you like this. ","4fa3086f":"## 3. Building the DataFrame <a class=\"anchor\" id=\"3\"><\/a> \nThere are three steps in this part\n1. Process Individual Files\nThe files are imported into a single DataFrame, stripped of unnecessary columns and filtered by date. Then we set a DateTimeIndex and concatenate them into the main_df . Because the loop takes a long time, we output some status messages, to ensure the process is still running.\n2. Process the concatenated main_df\nThen we display some info of the main_df so we can ensure that there are no errors, mainly to ensure all data-types are recognized correctly. Also, we drop duplicate entries, in case some of the .csv files were accidentally duplicated during the development process.\n3. Unstack and export\nFor the final step, we unstack the main_df and save it to a .csv and a .pkl file for the next step in the analysis process. Also, we display some output to get a grasp of what is going on.","7490e5cb":"## 1. Downloading the Data <a class=\"anchor\" id=\"1\"><\/a>    \nThis notebook pulls historical temperature data from the German Weather Service (DWD) server and formats it for future use in other projects. \n\nThe data is delivered in hourly frequencies in a .zip file for each of the available weather stations. To use the data, we need everything in a single .csv file with all stations side-by-side. Also, we need the daily average.    \nFor the purposes of this notebook, I have limited the download to only 10 files but the full data set is over 600 files.","8ab41e4c":"## 2. Extracting the Data<a class=\"anchor\" id=\"2\"><\/a> \n\nThe program will then extract each file and move to the import directory for further processing.","ce10ba14":"# Indroduction:\nThe main purpose of this notebook is to show how to use Python to solve real world problems. This notebook will show how a pipeline of Python programs is used to automate the process of collecting, cleaning and processing gigabytes of weather data in order to perform analysis.","db6f5fe8":"## 4. Final Processing <a class=\"anchor\" id=\"4\"><\/a> \nThe data contains some errors, which need to be cleaned. You can see, by looking at the output of main_df.describe(), that the minimum temperature on some stations is -999. That means that there is no plausible measurement for this particular hour. We change this to `np.nan`, so that we can safely calculate the average daily value in the next step.    \nOnce these values are corrected, we need to resample to daily measurements.","84120e09":"## Summary\n* This solution brings together many different concepts including web scraping, downloading files, working with zip files and cleaning & analyzing data with pandas.    \n* This solution brings together many different concepts including web scraping, downloading files, working with zip files and cleaning & analyzing data with pandas.","7aef4f13":"### Steps\n1. [Downloading the Data](#1)  \n2. [Extracting the Data](#2)    \n3. [Building the DataFrame](#3)    \n4. [Final Processing](#4)"}}