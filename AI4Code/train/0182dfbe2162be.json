{"cell_type":{"5aae988f":"code","f38c91e0":"code","51ae9ecf":"code","29b774ce":"code","54a11775":"code","0f914128":"code","4eadff33":"code","77eddeab":"code","bc8169dd":"code","0e62e102":"code","f7cd9c4e":"code","b1666282":"code","b240f54e":"code","ab27b3ea":"code","3a0fe73b":"code","ea20d9b6":"code","dd05bf9a":"markdown","e24a1b96":"markdown","7d91b521":"markdown","7c84a3f2":"markdown","bf22d2ad":"markdown","14b83758":"markdown","fb7e6284":"markdown","ef31950d":"markdown","93cd8da0":"markdown","64be088a":"markdown"},"source":{"5aae988f":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","f38c91e0":"dt = pd.read_pickle('..\/input\/cifar-100\/train')\ndf = pd.DataFrame.from_dict(dt, orient='index')\ndf = df.T\ndf.head()","51ae9ecf":"# Select an image and display\nimg = np.array(df[df['fine_labels'] == 15].iloc[10]['data']).reshape(3,32,32)\nplt.imshow(img.transpose(1,2,0).astype(\"uint8\"), interpolation='nearest')","29b774ce":"# convert to pytorch tensor\nimg = torch.from_numpy(img)","54a11775":"# add the batch_size\nimg = torch.reshape(img, (1, 3, 32, 32)).type(torch.FloatTensor)","0f914128":"batch_size, channels, h, w = img.shape\nprint('Batch size:{0}, Channels:{1}, Height:{2}, Width:{3}'.format(batch_size, channels, h, w))","4eadff33":"# setup the parameters for Conv2d\n\nkh, kw = 3, 3 # kernel size\ndh, dw = 3, 3 # stride","77eddeab":"# Create conv\nconv = nn.Conv2d(3, 3, (kh, kw), stride=(dh, dw), bias=False)\nconv_weight = conv.weight\nactual = conv(img)","bc8169dd":"patches = img.unfold(2, kh, dh).unfold(3, kw, dw)\n# batch_size, channels, h_windows, w_windows, kh, kw\nprint('Patches unfold shape: ', patches.shape)","0e62e102":"patches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\n# batch_size, channels, windows, kh, kw\nprint('Patches contiguous shape: ', patches.shape)","f7cd9c4e":"nb_windows = patches.size(2)","b1666282":"# Shift the windows into the batch dimension using permute\npatches = patches.permute(0, 2, 1, 3, 4)\n# batch_size, nb_windows, channels, kh, kw\nprint('Patches permutation shape', patches.shape)","b240f54e":"# Multiply the patches with the weights in order to calculate the conv\nresult = (patches.unsqueeze(2) * conv_weight.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\n# batch_size, output_pixels, out_channels\nprint('After conv operation', result.shape)\nresult = result.permute(0, 2, 1) # batch_size, out_channels, output_pixels","ab27b3ea":"# assuming h = w\nh = w = int(result.size(2)**0.5)\nresult = result.view(batch_size, -1, h, w)","3a0fe73b":"print('Result shape: ', result.shape, ' Actual shape:', actual.shape)","ea20d9b6":"# Verify the error between actual and result\nerror = (actual - result).abs().max().item()\nprint('Max Absolute Error : ', error)","dd05bf9a":"# Compare Results","e24a1b96":"# Goal","7d91b521":"# **Convolutional Neural Network 2D from Scratch using Pytorch**","7c84a3f2":"By looking at the Maximum Absolute Error since it's so low, it concludes that the manual Conv2d calculations worked.","bf22d2ad":"# Pytorch Manual Conv2d","14b83758":"# Data Preparation","fb7e6284":"# Pytorch Conv2d","ef31950d":"# Conclusion","93cd8da0":"The goal of this notebook is to do manual pytorch calculations so that the use of nn.Conv2d is avoided.<br>\nWe're going to calculate the Absolute Maximum Error in order to prove it.","64be088a":"# Prepare an Image for Conv2d"}}