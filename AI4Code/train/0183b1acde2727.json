{"cell_type":{"9c70feed":"code","fd4b1336":"code","024f6f3e":"code","8df97a02":"code","b5f37d7a":"code","e5932bc1":"code","91f29946":"code","b732c779":"code","0322bc69":"code","ebd413bc":"code","c1f3d622":"code","c76793e6":"code","4a79a2d3":"code","a8d35cc2":"code","99a72897":"code","28db5ece":"code","013189e7":"code","d7851574":"code","4b860ac6":"markdown","52714b09":"markdown"},"source":{"9c70feed":"#import required libraries\nimport pandas as pd\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor as xgr \nfrom xgboost import XGBClassifier as xgc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor","fd4b1336":"#function used for data treatment and transformation using pipeline\n#this function trains model on given model and also performs GridSearchCV\ndef pipeline_transform(data,target_variable,target_class, model,params):\n    \n    if target_class == 0:   # for continuous target variable\n        y = data[target_variable]\n    elif target_class == 1: # for multiclass target variable \n        y = pd.get_dummies(data[target_variable])\n    else:                   # for nominal encoding\n        y = LabelEncoder().fit_transform(data[target_variable])\n        \n    X_train, X_test, y_train, y_test = train_test_split(data.drop([target_variable],axis=1), y, test_size=0.3,random_state=777)\n    numeric_features     = X_train.select_dtypes(include = ['int64','float64']).columns\n    categorical_features = X_train.select_dtypes(include = ['object','category']).columns\n        \n    #developing transformer for each category of variables\n    categorical_transformer = Pipeline(\n                                steps = [\n                                    ('imputer',SimpleImputer(strategy='most_frequent')),\n                                    ('one_hot',OneHotEncoder())])\n\n    numeric_transformer =     Pipeline(\n                                steps = [\n                                    ('imputer',SimpleImputer(strategy='median')),\n                                    ('scaler',StandardScaler())])\n    #get preprocessor\n    preprocessor = ColumnTransformer(\n                    transformers = [\n                        ('num',numeric_transformer, numeric_features),\n                        ('cat',categorical_transformer, categorical_features)],\n                        remainder='passthrough')\n    \n    #making the pipeline\n    mymodel = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n    cv_inner = KFold(n_splits=5, shuffle=True)                    # choose cross validation technique\n\n    #developing for GridSearchCV\n    optimize_hparams = GridSearchCV(\n        estimator=mymodel, \n        param_grid=params, \n        cv=cv_inner, \n        scoring='neg_mean_absolute_error',\n        n_jobs=-1, )\n    \n    optimize_hparams.fit(X_train, y_train)\n    \n    \n    # fit the preprocessor \n    X_encoded  = optimize_hparams.best_estimator_['preprocessor'].fit_transform(X_train)\n    X_encoded2 = optimize_hparams.best_estimator_['preprocessor'].transform(X_test)\n\n\n    # fit the model \n    best_model = optimize_hparams.best_estimator_['model'].fit(X_encoded, y_train)\n\n    #get column names\n    numeric_features     = X_train.select_dtypes(include = ['int64','float64']).columns\n    \n    try:\n        cat_features = optimize_hparams.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['one_hot'].get_feature_names(categorical_features)\n        X_test_df = pd.DataFrame(X_encoded2,columns = [*numeric_features,*cat_features])\n    except NotFittedError as e:\n        X_test_df = pd.DataFrame(X_encoded2,columns = [*numeric_features])\n        print(repr(e))\n        pass\n    \n    return best_model,X_test_df","024f6f3e":"# here we give feature as input wrt which dependece plot need to be generated\ndef dependence_plot(best_model, X_test_df,explainer,feature,classification):    \n    shap.initjs()\n    shap_values = explainer(best_model).shap_values(X_test_df)\n    \n    if classification == 1:  # in case of classification problem\n        shap.dependence_plot(feature, shap_values[1], X_test_df,feature_names = list(X_test_df.columns))\n    else:                    # in case of regression problem\n        shap.dependence_plot(feature, shap_values, X_test_df,feature_names = list(X_test_df.columns))\n    return\n\ndef summary_plot(best_model, X_test_df,explainer,classification):\n    shap.initjs()\n    shap_values = explainer(best_model).shap_values(X_test_df) \n    if classification == 1:\n        class_names = list(data[4].unique())\n        print(class_names)\n        shap.summary_plot(shap_values, X_test_df, plot_type=\"bar\",class_names= class_names, feature_names = list(X_test_df.columns))\n    else:\n        shap.summary_plot(shap_values, X_test_df,feature_names = list(X_test_df.columns))\n    return\n\ndef force_plot(best_model, X_test_df,explainer,classification,index,index_2,category):    \n    shap.initjs()\n    explainer_2 = explainer(best_model)\n    shap_values = explainer_2.shap_values(X_test_df)\n    if classification == 1:\n        return shap.force_plot(explainer_2.expected_value[category],shap_values[category][index:index_2,:], X_test_df.iloc[index:index_2,:])         \n    else:\n        return shap.force_plot(explainer_2.expected_value,shap_values[index:index_2,:], X_test_df.iloc[index:index_2,:])     \n\ndef decision_plot(best_model, X_test_df,explainer,classification):    \n    shap.initjs()\n    shap_values = explainer(best_model).shap_values(X_test_df) \n    if classification == 1:\n        shap.decision_plot(explainer(best_model).expected_value[1], shap_values[1])\n    else:\n        shap.decision_plot(explainer(best_model).expected_value, shap_values)    \n    return\n\ndef partial_dependence_plot(best_model, X_test_df,explainer,feature,classification):\n    shap.initjs()\n    shap_values = explainer(best_model).shap_values(X_test_df)\n    \n    if classification == 1:  # in case of classification problem\n        shap.dependence_plot(feature, shap_values[1], X_test_df,interaction_index=None)\n    else:                    # in case of regression problem\n        shap.dependence_plot(feature, shap_values, X_test_df,interaction_index=None)\n    return\n","8df97a02":"# Sample Run 1: Using insurance dataset and predicting claim price\n\nclassification = 0                                      #declaring if function needs to perform regression\/classification\n\ndata = pd.read_csv('..\/input\/insurance\/insurance.csv')  #read data\ntarget_variable = 'charges'                             #target variable\ntarget_class = 0                                        # if 0:regression|1:multiclass|2:nominal\n\nmodel = xgr(booster='gbtree', random_state=13)         #choose a regressor model\nparams = {                                             #choose the parameter to be optimised on\n#     'model__learning_rate' : [0.3],\n}\n\nexplainer = shap.TreeExplainer                          # select shap explainer\nbest_model, X_test_df = pipeline_transform(data,target_variable,target_class, model,params)","b5f37d7a":"# DEPENDENCE PLOT\nfeature = 'age'\ndependence_plot(best_model, X_test_df,explainer,feature,classification) ","e5932bc1":"# SUMMARY PLOT\nsummary_plot(best_model, X_test_df,explainer,classification)","91f29946":"# # ## FORCE PLOT\nindex =2\nindex_2 = 5\ncategory = 0 #for multiclass classification\nforce_plot(best_model, X_test_df,explainer,classification,index,index_2,category)","b732c779":"# PARTIAL DEPDENDENCE PLOT\nfeature = 'age'\npartial_dependence_plot(best_model, X_test_df,explainer,feature,classification) ","0322bc69":"## DECISION PLOT\ndecision_plot(best_model, X_test_df,explainer,classification)","ebd413bc":"# Sample Run 2: Using iris dataset and predicting category\n\nclassification = 1                     #declaring if function needs to perform regression\/classification\n\ndata = pd.read_csv('..\/input\/iris-dataset\/iris.data.csv',header=None) #read data\ntarget_variable = 4                                                   #target variable\ntarget_class = 2                                                     #if 0:regression|1:multiclass|2:nominal\n\nmodel = RandomForestClassifier()                       #choose a regressor model\nparams = {                                             #choose the parameter to be optimised on\n#     'model__learning_rate' : [0.3],\n}\n\nexplainer = shap.TreeExplainer                          # select shap explainer\nbest_model, X_test_df = pipeline_transform(data,target_variable,target_class, model,params)","c1f3d622":"# SUMMARY PLOT\nsummary_plot(best_model, X_test_df,explainer,classification)","c76793e6":"## DEPENDENCE PLOT\nfeature = 1\ndependence_plot(best_model, X_test_df,explainer,feature,classification) ","4a79a2d3":"# ## FORCE PLOT\nindex =0\nindex_2 = 2\ncategory = 0 #for multiclass classification\nforce_plot(best_model, X_test_df,explainer,classification,index,index_2,category)","a8d35cc2":"# PARTIAL DEPDENDENCE PLOT\nfeature = 1\npartial_dependence_plot(best_model, X_test_df,explainer,feature,classification) ","99a72897":"# DECISION PLOT\ndecision_plot(best_model, X_test_df,explainer,classification)","28db5ece":"index =0\nindex_2 = 1\nforce_plot(best_model, X_test_df,explainer,classification,index,index_2,category)","013189e7":"#get plot type + partial dependence\n# granularity : local & global","d7851574":"# https:\/\/medium.com\/swlh\/decoding-customer-churn-6d1e941d0c7\nhttps:\/\/github.com\/breno-madruga\/machine_learning_exercises\/blob\/master\/Pipeline,%20Ensemble%20Models%20and%20XAI\/pipeline_ensemble_xai.ipynb\n    ","4b860ac6":"The following function takes care of data transformation and model estimation using pipeline. \nWe will be describing the function working in steps as follows:\n\n1. For treatment of target variable(y), 3 cases are dealt  : \n\n    a) if y is continuous variable, no transformation needed\n    \n    b) if y is binary or multiclass nominal data, we employ OneHotEncoding\n    \n    c) if y is ordinal variable, we emplot LabelEncoding\n    \n2. The data is splitted for train test split and then numeric & categorical features are extracted as per following:\n\n    a) int64,float64 : Numerical\n    \n>     b) object, category : Categorical | if we get a data that gives numeric data here can become issue\n\n3. Transformer for categorical and numeric data :\n\n    a) Numeric: Nan values filled with median   -> StandardScaler\n    \n    b) Categorical: Nan values filled with mode -> OneHotEncoder\n    \n4. Pipeline consisting of column_transformer and model\n\n5. Model fit & GridSearchCV\n\n6. Get feature names from one hot encoding and get train test dataframe","52714b09":"For each plot seperate function is generated so that individual functionalities can be added.\n\nThis code can produce 5 different kind of plots, namely:\n1. Dependence plot\n2. Summary plot\n3. Force plot\n4. Waterfall plot\n5. Decision plot\n\nFor every plot code: First of all, pipeline_transform function is called which handles data transformation, model training, grid search CV and finding best fit parameters. The function returns best model and transformed X_test dataframe.\n\nPost this, shap values for explainer for test dataframe is generated. and then depending on if classification or regression problem is being dealt. We get plot generated. \n\n> Note: For Classification, we have set index as 1 for binary classification| Need to handle for multiclass"}}