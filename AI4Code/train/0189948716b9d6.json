{"cell_type":{"2f4c989d":"code","80a02750":"code","ec86d151":"code","a5d9bbab":"code","bee202eb":"code","5891cc7a":"code","1db5b6e2":"code","3649a25c":"markdown","57125d6e":"markdown","ef0d0083":"markdown","8744d66d":"markdown","70eef9fa":"markdown","19459572":"markdown","b98e6ae9":"markdown","3cb2e7a7":"markdown","39701707":"markdown","2264aeba":"markdown"},"source":{"2f4c989d":"from numpy import *\nimport matplotlib.pyplot as plt","80a02750":"def plot_best_fit(intercept, slope):\n    axes = plt.gca()\n    x_vals = array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, 'r-')\n","ec86d151":"def mean_squared_error(b, m, points):\n    totalError = 0\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        totalError += (y - (m * x + b)) ** 2\n    return totalError \/ float(len(points))","a5d9bbab":"def batch_gradient(b_current, m_current, points, learningRate):\n    b_gradient = 0\n    m_gradient = 0\n    N = float(len(points))\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        b_gradient += -(2\/N) * (y - ((m_current * x) + b_current))\n        m_gradient += -(2\/N) * x * (y - ((m_current * x) + b_current))\n    new_b = b_current - (learningRate * b_gradient)\n    new_m = m_current - (learningRate * m_gradient)\n    return [new_b, new_m]\n","bee202eb":"def gradient_descent_runner(points, starting_b, starting_m, learning_rate):\n    b = starting_b\n    m = starting_m\n    checker = True\n    while(checker):\n        b_pre, m_pre = b,m\n        error_bef_grad = mean_squared_error(b, m, points)\n        b, m = batch_gradient(b, m, array(points), learning_rate)\n        error_aft_grad = mean_squared_error(b, m, points)\n        if error_aft_grad > error_bef_grad:\n            checker = False\n    plot_best_fit(b_pre,m_pre)\n    return[b_pre,m_pre]","5891cc7a":"def main_fun():\n    points = genfromtxt(\"..\/input\/distancecycledvscaloriesburned\/data.csv\", delimiter=\",\")\n    plt.plot(points[:,0], points[:,1], '.')\n    learning_rate = 0.0001\n    initial_b = 0 # initial y-intercept guess\n    initial_m = 0 # initial slope guess\n    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, mean_squared_error(initial_b, initial_m, points)))\n    print(\"Running...\")\n    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate)\n    print(\"After Gradient descent b = {0}, m = {1}, error = {2}\".format(b, m, mean_squared_error(b, m, points)))","1db5b6e2":"main_fun()","3649a25c":"**Function to implment the logic of the whole program:**\n\nAt one point we will reach a global minima (B) if we are descending our gradient contineously, but if we descend the gradient contineously even after reaching (B) we may end up in point (p) which is not the minima as shown in the below picture: \n\n![](https:\/\/i.imgur.com\/Jm0sqsn.png)\n\nAt this point (p) MSE will be more than (B). Here is were we will stop the program and the values (slope and intercept) calculated before (p) i.e, at point (B) will be the best slope and intecept with minimum Mean Squared Error\n\n**Note:** This logic is only applicable for concave models only.","57125d6e":"**Function to calculate MSE (Mean Squared Error):**","ef0d0083":"**Function to plot the best fit:**","8744d66d":"**PROGRAM:**","70eef9fa":"**Function to calculate gradient descent (b_gradient, m_gradient) and the respective value of slope (new_b) and intercept (new_m):**\n\nThe gradient descent formula is obtained by partial differentiation of cost function (MSE) with respect to slope (b) and intecept (m):\n\nMean squred error is the total sum of the difference between the predicted value yi and the expected value y'i \n\n![](https:\/\/i.imgur.com\/UTV23a0.png) Where, y'i = (MXi + b)\n![](https:\/\/i.imgur.com\/rpSB3vh.png)\n\n","19459572":"**GENERAL CONCEPTS:**","b98e6ae9":"**INTRODUCTION:**\n\n1. In the below program I have implemented a linear regression model for the given dataset.\n2. I Found the best fit (slope and intercept) using the batch gradient descent as an optimiser \n3. And the cost function as MSE (Mean Squared error)\n\n**Note:** I did not use any predefined functions to implement gradient descent, so that by going throught the program below will give a clear idea of how gradirnt descent works.","3cb2e7a7":"***What is Gradient Descent?***\n\nGradient is a point vector on any steep surface (example: mountain). It has both magnitude (measure of the steepness) and the direction of the steepness. We call it gradient descent because we are trying to descent the gradient by moving to the point where it is less steep (in our case the point where the cost function (MSE) is minimum). In the first image below we are trying to descent the gradient to reach from point A to B.\n\n***What is a Concave model?***\n\nWhen we plot a linear regression model along with its error in 3D we will always get a concav model (where there will be only one Global minima (B): \n\n![](https:\/\/i.imgur.com\/IHBZyN0.png)\n\nTo understand better below is not an example of Concave model (where there are many local minima (L1,L2,L3) and one global minima (B) as shown in below picture):\n\n![](https:\/\/i.imgur.com\/2NLAZ5G.png)","39701707":"\n\n**Importing required libraries:**","2264aeba":"**Function to Wrap and implement all the above fundctions:**"}}