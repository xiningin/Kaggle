{"cell_type":{"62ad114f":"code","c596f0ba":"code","225fea1c":"code","1e509708":"code","984ed67f":"code","226af752":"code","db66f0e1":"code","925f22e0":"code","f7433967":"code","63124801":"code","087343c5":"code","0f23efc0":"code","a1ae8236":"code","dbdabcb3":"code","e5e50098":"code","deaa4fd3":"code","72508938":"code","bc1695e4":"code","13677d5a":"code","34aa208a":"code","e96a73ac":"code","ed0396ba":"code","23a57698":"code","dd8f2fd0":"code","fd918e94":"code","ffd6ed9a":"code","45143700":"code","0260cf45":"code","74ffd1c7":"code","484c16b8":"code","2b9fdaa3":"code","149e5948":"code","6d454c30":"code","6161372c":"code","7398a46e":"code","018570a6":"code","e1d9b3fd":"code","e141c155":"code","93911020":"code","b860aa5c":"code","2353c3de":"code","284e508a":"code","6cfa70e3":"code","7b1ce5fc":"code","f8c4affe":"code","d3068d16":"code","56493d2b":"code","27bcf9d5":"code","7a32854a":"code","6d7fa4ae":"code","7ea06807":"code","cde572f2":"code","314d719c":"code","1c743c19":"code","ac3f10c8":"code","24e07bda":"code","b4eb1a99":"code","6ee43241":"code","8623ceaf":"code","3add5782":"code","e5fd105f":"code","27e8d0ef":"code","ecbd93ee":"code","4d882b27":"code","58d44bb6":"markdown","98120e52":"markdown","e3d24431":"markdown","469ee5b1":"markdown","5f5b39eb":"markdown","accbf56b":"markdown","b7250cce":"markdown","62ebd843":"markdown","f22e6739":"markdown","1581623f":"markdown","5d3e0152":"markdown","f7246e3d":"markdown","ffe675c9":"markdown","93df0307":"markdown","2263faef":"markdown","e0f7c800":"markdown","cc6220f7":"markdown","3d389031":"markdown","965bdf69":"markdown","d0ae2c54":"markdown","dba4fe30":"markdown","dfc570d0":"markdown","4de8b234":"markdown","1b30a895":"markdown","df30e5f4":"markdown","a78b5609":"markdown","bfaa0ad0":"markdown","6f007793":"markdown","a51fd838":"markdown","d03f043d":"markdown","4f749d2d":"markdown"},"source":{"62ad114f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c596f0ba":"#Import Statements\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport nltk","225fea1c":"# Importing the data into Notebook\nFlipkart_orignal_data = pd.read_csv('\/kaggle\/input\/flipkart-ecommerce-dataset\/flipkart_com-ecommerce_sample.csv')","1e509708":"Flipkart_orignal_data.info()","984ed67f":"#checking the head of the data \nFlipkart_orignal_data.head(n=10)","226af752":"#to check the number of rows and cols\nFlipkart_orignal_data.shape","db66f0e1":"Flipkart_orignal_data.isnull().sum()","925f22e0":"Flipkart_orignal_data.columns","f7433967":"# Making a copy of orginal data\nflipkart_data = Flipkart_orignal_data.copy()","63124801":"#Lets Drop the Un-required Coloumns.\n\nflipkart_data.columns","087343c5":"flipkart_data.drop(flipkart_data.columns[[0,1,2,5,6,7,8,9,11,12,14]], axis=1, inplace=True)\n\n#Lets View the data Now\nflipkart_data","0f23efc0":"# A function to Clean the product_category_tree and return Primary_Category\ndef clean_prod_category():\n  primary_category=[]\n  #iterate in col\n  for ele in flipkart_data['product_category_tree']: \n    ele=ele.replace('\"]',\"\")\n    category=ele[2:].split(\" >>\")\n    primary_category.append(category[0])\n  \n  #returinig the clean data\n  return primary_category","a1ae8236":"#Calling the function\nprimary_category=clean_prod_category()\n\n#Verfying the output\nprimary_category","dbdabcb3":"primary_category=pd.DataFrame(primary_category,columns=[\"Category\"])\nprint(\"Number of Unique Categories\",len(primary_category[\"Category\"].value_counts()))","e5e50098":"primary_category_top=Counter(primary_category[\"Category\"]).most_common(266)\nprimary_category_top","deaa4fd3":"# When we will try to plot the categories and its count , we will get a messy graph as there are 265 categories.\n\nplt.figure(figsize=(20,20))\nplt.title(\"Count Vs Category\")\nsns.countplot(x=\"Category\",data=primary_category,palette=\"rainbow\",\n              order=primary_category[\"Category\"].value_counts().index)\nplt.xticks(rotation=90,fontsize = 10, ha='left')\nplt.show()","72508938":"# A function which returns a output containg the top categories with more then 'n' counts.\n\n\n# Here, n stands for the count of categories , and bottom, if you want the categories with most\/least count.\n# and 'List', do we need a list or dataframe.\ndef count_of_category(n,bottom=False,List=False):\n    output=[]\n    primary_category_top=Counter(primary_category[\"Category\"]).most_common(266)\n    if bottom:\n      for i in primary_category_top:\n        if i[1]<=n:\n            output.append(i)\n    else:\n      for i in primary_category_top:\n        if i[1]>=n:\n          output.append(i)\n    if not List:\n      output=pd.DataFrame(output,columns=[\"Category\",\"Count\"])\n    return output","bc1695e4":"#Now lets try to find the categories with more then 500 counts \ntop_500_category = count_of_category(500) ","13677d5a":"sns.catplot(\"Category\", \"Count\", data=top_500_category, kind='bar',aspect=2,palette='rainbow')\nplt.xticks(rotation=90)\nplt.show()\n","34aa208a":"#Let's see if its even feasible to visualize the categories with least counts\nbottom_categories=count_of_category(500,bottom=True,List=True)","e96a73ac":"print(\"The number of categories with count less then 500 :\",len(bottom_categories))","ed0396ba":"bottom_categories","23a57698":"#making a new column \"primary_category\"  \nflipkart_data[\"primary_category\"]=primary_category\nflipkart_data.head()","dd8f2fd0":"#Now there is no need for product_category_tree , so we drop it.\nflipkart_data.drop(flipkart_data.columns[[1]], axis=1, inplace=True)\n#removing the 2 null values in 'description' column\nflipkart_data=flipkart_data.dropna(subset=['description'])","fd918e94":"#checking the remaning null values in description\nflipkart_data[flipkart_data['description'].isnull()]","ffd6ed9a":"#checking the remaning null values in the data.\nflipkart_data[flipkart_data.isnull().any(axis=1)]","45143700":"top_500_category = top_500_category['Category'][0:10]\ntop_10_categories = list(top_500_category)\ntop_10_categories","0260cf45":"flipkart_data","74ffd1c7":"# using 'isin' we filtered our flipkart data to have only top 10 categories\nflipkart_data= flipkart_data[flipkart_data['primary_category'].isin(top_10_categories)][['product_name','description','brand','primary_category']]","484c16b8":"flipkart_data","2b9fdaa3":"# Verification\nflipkart_data.describe()","149e5948":"#let's make a copy of orginal data \nflipkart_data_merged=flipkart_data.copy()\nflipkart_data_merged","6d454c30":"# Merging the columns using string property\nflipkart_data_merged['text']= flipkart_data_merged['brand'].astype(str) +\" \"+ flipkart_data_merged['product_name'].astype(str) + \" \" + flipkart_data_merged['description'].astype(str)\n\n#created a new single feature 'text'","6161372c":"#Dropping unrequired Columns\nflipkart_data_merged.drop(flipkart_data_merged.columns[[0,1,2]], axis=1, inplace=True)","7398a46e":"flipkart_data_merged","018570a6":"# reindex the 'text' column in front\nflipkart_data_merged=flipkart_data_merged.reindex(['text','primary_category'],axis=\"columns\")\nflipkart_data_merged","e1d9b3fd":"# Let's use only \"description\" column to predict the categories \nflipkart_data.drop(flipkart_data.columns[[0,2]], axis=1, inplace=True)","e141c155":"flipkart_data","93911020":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nimport string # for removing punctuations in string","b860aa5c":"#adding custom stopwords \n# custom stopwords are selected from manual inspection\nnew_stopwords = [\"buy\", \"features\", \"key\", \"specifications\",\"nan\",\"NaN\"]\n\n#extending the stopwords with custom stopwords\nstpwrd = nltk.corpus.stopwords.words('english')\nstpwrd.extend(new_stopwords)","2353c3de":"# This class preprocesses the text for us using tokenizer and stopwords.\n# it also removes punctuations if there are any.\nclass PreProcessText(object):\n    def __init__(self):\n        pass\n    \n    def __remove_punctuation(self, text):\n        \"\"\"\n        Takes a String \n        return : Return a String \n        \"\"\"\n        message = []\n        for x in text:\n            #Using String Library\n            if x in string.punctuation:\n                pass\n            else:\n                message.append(x)\n        message = ''.join(message)\n        \n        return message\n    \n    def __remove_stopwords(self, text):\n        \"\"\"\n        Takes a String\n        return List\n        \"\"\"\n        words= []\n        for x in text.split():\n\n            #our custom stpwrd\n            if x.lower() in stpwrd:\n                pass\n            else:\n                words.append(x)\n        return words\n    \n    \n    def token_words(self,text=''):\n        \"\"\"\n        Takes String\n        Return Token also called  list of words that is used to \n        Train the Model \n        \"\"\"\n        message = self.__remove_punctuation(text)\n        words = self.__remove_stopwords(message)\n        return words","284e508a":"obj = PreProcessText()\nflipkart_data[\"description\"]=flipkart_data[\"description\"].apply(obj.token_words)","6cfa70e3":"flipkart_data[\"description\"]","7b1ce5fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(flipkart_data[\"description\"],flipkart_data[\"primary_category\"], test_size=0.33)\n","f8c4affe":"X_train","d3068d16":"X_train=X_train.apply(lambda x: ' '.join(x))\nX_test=X_test.apply(lambda x: ' '.join(x))\n","56493d2b":"#check\nX_test","27bcf9d5":"from sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer\n#Importing the ML model\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","7a32854a":"# we will use pipeline to make our model of TfidfVectorizer and MultinomialNB\nmodel=make_pipeline(TfidfVectorizer(),MultinomialNB())\n#Training our model\nmodel.fit(X_train,y_train)\nprediction_tfid=model.predict(X_test)","6d7fa4ae":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for flipkart_Data using Tfid : \",accuracy_score(y_test,prediction_tfid))","7ea06807":"# we will use pipeline to make our model of TfidfVectorizer and MultinomialNB\nmodel_2 = make_pipeline(CountVectorizer(),MultinomialNB())\n#Training our model\nmodel_2.fit(X_train,y_train)\nprediction_countV = model_2.predict(X_test)","cde572f2":"print(\"Accuracy Score for flipkart_Data using Count : \",accuracy_score(y_test,prediction_countV))","314d719c":"print(classification_report(y_test,prediction_countV))","1c743c19":"flipkart_data_merged","ac3f10c8":"obj = PreProcessText()\nflipkart_data_merged[\"text\"]=flipkart_data_merged[\"text\"].apply(obj.token_words)","24e07bda":"flipkart_data_merged['text']","b4eb1a99":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(flipkart_data_merged[\"text\"],flipkart_data_merged[\"primary_category\"], test_size=0.33)","6ee43241":"X_train=X_train.apply(lambda x: ' '.join(x))\nX_test=X_test.apply(lambda x: ' '.join(x))","8623ceaf":"X_train","3add5782":"# we will use pipeline to make our model of TfidfVectorizer and MultinomialNB\nmodel=make_pipeline(TfidfVectorizer(),MultinomialNB())\n#Training our model\nmodel.fit(X_train,y_train)\nprediction_tfid=model.predict(X_test)","e5fd105f":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score for flipkart_data_merged using Tfid : \",accuracy_score(y_test,prediction_tfid)) # Expected higher result than flipkart_data","27e8d0ef":"# we will use pipeline to make our model of TfidfVectorizer and MultinomialNB\nmodel_2 = make_pipeline(CountVectorizer(),MultinomialNB())\n#Training our model\nmodel_2.fit(X_train,y_train)\nprediction_countV = model_2.predict(X_test)","ecbd93ee":"print(\"Accuracy Score for flipkart_data_mereged using Count : \",accuracy_score(y_test,prediction_countV)) \n#Expects Higher result than flipkart_data_mereged","4d882b27":"print(classification_report(y_test,prediction_countV))","58d44bb6":"### Observation -2\n\n\nWell, The data is unbalanced as ***Clothing*** has 6000+ counts while even ***Watches*** has less then 600. The proportions are bad. This will lead to a baised machine learning model. \n\n\n\n> To fix this problem without **Under-sampling**, we would require balanced data. Since we dont have that we can try **Over-sampling** the minorities.\n\nBut according the my analysis, we dont need to do that because even in the real-world, my data analysis says that the \"Clothing\" is generaly the most sold catergory for Flipkart and vice versa.\n\nNow since, the real-world test data would also contain most probably the same proportion, we can get away by doing nothing.\n\n*(This is my inital analysis, to have the correct approach, we would need to reasearch more about the shopping trends in Flipkart)*\n\n\n---\n\n\n\n\n\n","98120e52":"This is a list, but we need to send String not list to TfidfVectorizer and CountVectorizer in further code, otherwise it will break","e3d24431":"#### TfidfVectorizer ","469ee5b1":"# **Summary**\n\n\n---\n\n\nAs Expected, using **flipkart_data_merge** gave us better result than without merging the 'brand', 'product_name' and 'description'.\n\nBest Result:\n*Accuracy Score for flipkart_data_mereged using Count :  0.9865160349854227*\n\nWe obtained this result using **flipkart_data_merge** and **CountVectorizer**.\n\nWe don't need to explore other Machine learning models as Naive Bayes gave us 0.98 accuracy.\n\n\n\n---\n\n\n\n","5f5b39eb":"####  CountVectorizer","accbf56b":"After looking at value_counts(), we can observe there is some **Unnecesarry Categories**.","b7250cce":"\n# Data Analysis\n\nAfter importing the data, a simple analysis of the data is done.","62ebd843":"As, we can see we still have 254 rows i.e Unique Categories, it would not be possible to visualize , let's switch to old fashion way, **manually**.","f22e6739":"Now, we can go forward and split the data into test and train.","1581623f":"Its important to check how much null data we have to,so no problems occur later on.","5d3e0152":"####  CountVectorizer","f7246e3d":"Now, all the columns were enlisted so that we can decide which columns we can use as features for our Machine learning model.\n","ffe675c9":"\n\n---\n## Categories to use.\nWhen we looked at the plot createad using\n***top_500_category*** , We observed that the data was inbalanced,So our Machine Learning Model will give better accuracy and result if we use categories with large data only. For starters let's only take **Top 10 Categories**.\n","93df0307":"Cleaning the ***product_category_tree***\tcolumn.","2263faef":"## Attempt to Increase Accuracy\n\nNow, we can merge the product_name, description and brand column to create a single feature. This is an attempt to increase the accuracy of the model, using more than just the description.","e0f7c800":"### Now, to better Visualize the Data lets plot it.","cc6220f7":"## Rough Analysis\n\nBy glance, To predict the \"Primary Category\" of the given product we need \"product_category_tree\", \"brand\",\"product_name\" columns for our model. Talking about \"product_category_tree\" we do have to clean it up and create another table called \"primary_category\" where the tree is cleaned and the correct category is extracted. \"brand\" coloumn have 5864\nnull values, which we need to take care of.\n","3d389031":"### Observation-1 \nWe can now look at the plot and observe that there are way too many categories with less then 200 counts and a lot with just 1 and 2.\nTo Better visualize the data, Let's first observe the categories with high count.\n\n","965bdf69":"## Conclusion \nNow, we have two data, one with features megered into single column ***text*** \n\n```\nflipkart_data_merged\n```\nand other\n\n\n\n```\nflipkart_data\n```\n\n\n with just **description** column, we will now check which data performs better.\n\n\n (***Initial assumption*** : **flipkart_data_merged** will tend to perform better as Brand and product name is essential to define its category). \n\n** Things to keep in mind : **\n\n\n1.   *flipkart_data_merged* has word 'nan' due to empty brand columns. We can take care of that and other words like \"Buy\",\"Key Features\" etc in stopwords.\n2.   *flipkart_data* on the other hand has only \"description\" as a feature.\n\n","d0ae2c54":"# Data Cleaning\n\nSince we have done data analysis, we know that we need to clean our data.\n\n**Lets start!**","dba4fe30":"## Cleaning product_category_tree\nFurther Cleaning and setting up the data for Predictions.","dfc570d0":"Now, We will adjust our flipkart_data to only have these **Top 10 categories**","4de8b234":"#### TfidfVecorizer","1b30a895":"### **flipkart_data**\nFirst we will use flipkart_data for our model.","df30e5f4":"Now, we need to convert text data into vectors as model can process only numerical data.\n\n**Two methods that we can use are**\n\n1.   TfidfVectorizer \n2.   CountVectorizer \n\nTo decide which one is better for our data,we can simply use both and try.","a78b5609":"## Unrequired Data\nFor Now, Let's look at the ineffectual data and see if we can make something useful out of it.\n","bfaa0ad0":"# Machine Learning\n\nNow since our data is clean and ready to be used.\nWe can go ahead and decide the algorithm to use.\n\nI will use **Naive Bayes** algorithm based on Bayes\u2019 theorem.\n\n### Reason to use Naive Bayes.\nThis problem is of Multi-class classification,\nSo we need a classifer to define categories to the product based on text\/description, Naive bayes is well suited for that, It's easy to implement and works well on small data.\nOther Algorithm that we can try is Linear Support Vector Classifier.\n\ncite;https:\/\/analyticsindiamag.com\/7-types-classification-algorithms\/\n","6f007793":"### flipkart_data_merged\n\nNow we will use the flipkart_data_merged with brand, product_name and description merged into \"text\".","a51fd838":"### Manual Inspection Observation:\n\nIn this Inspection, I noticed that now the categories have product name instead of actual categories (in most cases).\nFurther, the data shows the same trend, large number of clothing and footwear items etc, meaning that even the un-required data speaks the proportion of the orignal and correct one. \n\nSince they are unique it would not be possible to clean this data and extract the Primary and Correct category from it. \n\nThis means, only thing now left is to remove the unrequired data and move forward to the Machine Learning Part.\n\n\n---\n\n","d03f043d":"# Task 3 NLP \n\nGoal: To predict the \"Primary Category\" of the product using given data.\n\n**By Atharv Jariath**","4f749d2d":"Now, we can go forward and split the data into test and train."}}