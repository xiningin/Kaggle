{"cell_type":{"475e5709":"code","a37db8c6":"code","4ebf728e":"code","aa1d8f51":"code","a198f8cb":"code","8aaccd88":"code","e6772b61":"code","cd45dd50":"code","958bc7c6":"code","5c498c00":"code","0388b93e":"code","bf38814b":"code","e29c141a":"code","87e94f61":"markdown","d9241421":"markdown","9bdb7fe6":"markdown","a649d3f0":"markdown","f76110e0":"markdown","13ec92a5":"markdown","3f070bc1":"markdown","d23466a9":"markdown","e42189b4":"markdown","bc5ae58c":"markdown"},"source":{"475e5709":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nfrom PIL import Image \nimport time,datetime,keras,cv2,shutil,keras_preprocessing,requests,math\nimport tensorflow as tf\nfrom keras import layers\nfrom keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, SpatialDropout2D \nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.datasets import load_files\nfrom io import BytesIO\nimport seaborn as sns\nfrom IPython.display import Image as IpyImage, display\n%load_ext tensorboard\nshutil.rmtree('.\/logs', ignore_errors=True)","a37db8c6":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\ndef plot_metric(history, metric):\n    train_metrics = history.history[metric]\n    val_metrics = history.history['val_'+metric]\n    epochs = range(1, len(train_metrics) + 1)\n    plt.plot(epochs, train_metrics)\n    plt.plot(epochs, val_metrics)\n    plt.title('Training and validation '+ metric)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([\"train_\"+metric, 'val_'+metric])\n    plt.show()","4ebf728e":"train_dir = '..\/input\/fruits\/fruits-360\/Training'\ntest_dir = '..\/input\/fruits\/fruits-360\/Test'\nimg_size = 100\nbatch_size_training = 512\nbatch_size_validation = 256\nbatch_size_test = 256\ninput_shape = (img_size,img_size,3)\n\n######## IMAGEDATAGENERATOR ########\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n######## FLOW FROM DIRECTORY - TRAINING ########\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_size, img_size),\n    batch_size=batch_size_training,\n    class_mode='categorical')\n\n######## FLOW FROM DIRECTORY - TESTING ########\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_size, img_size),\n    batch_size=batch_size_test,\n    class_mode='categorical',\n    shuffle=False)\n\nnum_classes = len(np.unique(train_generator.classes))","aa1d8f51":"model = Sequential([\n\n    Conv2D(filters = 32, kernel_size = 5, input_shape = input_shape, padding=\"same\"),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size = (2,2), strides=2),\n\n    Conv2D(filters = 64, kernel_size = 5, padding=\"same\"),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size = (2,2), strides=2),\n\n    Conv2D(filters = 128, kernel_size = 5, padding=\"same\"),\n    BatchNormalization(),\n    Activation('relu'),\n    MaxPooling2D(pool_size = (2,2), strides=2),\n\n    Conv2D(filters = 128, kernel_size = 5, padding=\"same\"),\n    BatchNormalization(),\n    Activation('relu'),\n    SpatialDropout2D(0.2),\n    MaxPooling2D(pool_size = (2,2), strides=2),\n\n    Flatten(),\n\n    Dense(1024),\n    BatchNormalization(),\n    Activation('relu'),\n    Dense(512),\n    BatchNormalization(),\n    Activation('relu'),\n    Dense(256, activation=\"relu\"),\n    Dense(num_classes, activation=\"softmax\"),\n])\nmodel.build(input_shape)\nmodel.summary()","a198f8cb":"######### LEARNING RATE DEFINITION #########\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=batch_size_training \/\/ 8,\n    decay_rate=0.95)\noptimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n\n######### MODEL COMPILATION #########\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nlog_dir = \".\/logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n######### CALLBACKS #########\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearly_stoppage_callback = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.0001, patience=15, restore_best_weights = True)\n\n######### TRAIN MODEL #########\nstart = time.time()\nhistory = model.fit(train_generator, \n            steps_per_epoch = math.ceil(train_generator.samples \/ batch_size_training),\n            epochs=50,\n            verbose=1,\n            validation_data=test_generator, \n            validation_steps = math.ceil(test_generator.samples \/ batch_size_test),\n            callbacks=[tensorboard_callback,early_stoppage_callback]\n            )\nend = time.time()\nprint(end - start,\"secs\")","8aaccd88":"plot_metric(history, \"loss\")\nplot_metric(history, \"accuracy\")","e6772b61":"#%tensorboard --logdir \".\/logs\/fit\/\"","cd45dd50":"preds = model.predict(test_generator)\npreds_cls_idx = preds.argmax(axis=-1)","958bc7c6":"score = model.evaluate(test_generator)\nscore","5c498c00":"plt.figure(figsize = (10,10))\nprint('Confusion Matrix')\nsns.heatmap(confusion_matrix(test_generator.classes, preds_cls_idx))\nprint('Classification Report')\nprint(classification_report(test_generator.classes, preds_cls_idx))","0388b93e":"urls=[]\nurls.append(\"https:\/\/p7.hiclipart.com\/preview\/743\/208\/896\/watermelon-fruit-food-vegetable-watermelon.jpg\")\nurls.append(\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQ4YA3FseQPkuf0TUlcm9WWfkfoae-yQXntQg&usqp=CAU\")\nurls.append(\"https:\/\/image.shutterstock.com\/image-photo\/one-ripe-yellow-banana-on-260nw-1713606277.jpg\")\nfor url in urls:\n    realfruit = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n    realfruit = realfruit.resize((img_size,img_size), Image.ANTIALIAS)\n    display(realfruit)\n    realfruit = np.asarray(realfruit)\/255\n    realfruit = realfruit.reshape(1,img_size,img_size,3)\n    predictedfruit = model.predict(realfruit)\n    predictedindex = predictedfruit.argmax(axis=-1)\n    my_dict = train_generator.class_indices\n    print('Prediction:', list(my_dict.keys())[list(my_dict.values()).index(predictedindex)])\n    print('Percentage:', '{0:.2f}'.format(float(predictedfruit[0][predictedindex]*100)), '%')","bf38814b":"model.save('fruit_model' + str(score[1]) + '.keras')","e29c141a":"layer_names = []\nfor layer in model.layers:\n        layer_names.append(layer.name)\n\nfor layer_name in layer_names:\n    if 'flatten' in layer_name: break\n    def compute_loss(input_image, filter_index):\n        activation = feature_extractor(input_image)\n        filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n        return tf.reduce_mean(filter_activation)\n    @tf.function\n    def gradient_ascent_step(img, filter_index, learning_rate):\n        with tf.GradientTape() as tape:\n            tape.watch(img)\n            loss = compute_loss(img, filter_index)\n        grads = tape.gradient(loss, img)\n        grads = tf.math.l2_normalize(grads)\n        img += learning_rate * grads\n        return loss, img\n    def initialize_image():\n        img = tf.random.uniform((1, img_size, img_size, 3))\n        return (img - 0.5) * 0.25\n    def visualize_filter(filter_index):\n        iterations = 1500\n        learning_rate = 1.0\n        img = initialize_image()\n        for iteration in range(iterations):\n            loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n        img = deprocess_image(img[0].numpy())\n        return loss, img\n\n    def deprocess_image(img):\n        img -= img.mean()\n        img \/= img.std() + 1e-5\n        img *= 0.15\n        img = img[25:-25, 25:-25, :]\n        img += 0.5\n        img = np.clip(img, 0, 1)\n        img *= 255\n        img = np.clip(img, 0, 255).astype(\"uint8\")\n        return img\n    layer = model.get_layer(name=layer_name)\n    feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)\n    all_imgs = []\n    for filter_index in range(31):\n        #print(\"Processing filter %d\" % (filter_index,))\n        loss, img = visualize_filter(filter_index)\n        all_imgs.append(img)\n\n    margin = 5\n    n = 5\n    cropped_width = img_size - 25 * 2\n    cropped_height = img_size - 25 * 2\n    width = n * cropped_width + (n - 1) * margin\n    height = n * cropped_height + (n - 1) * margin\n    stitched_filters = np.zeros((width, height, 3))\n\n    # Fill the picture with our saved filters\n    for i in range(n):\n        for j in range(n):\n            img = all_imgs[i * n + j]\n            stitched_filters[\n                (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n                (cropped_height + margin) * j : (cropped_height + margin) * j\n                + cropped_height,\n                :,\n            ] = img\n    keras.preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n\n    print(layer_name)\n    img = Image.open(\"stiched_filters.png\").resize((600,600))\n    display(img)","87e94f61":"# Display and Predict Real Fruits\nTested a few googled fruit images that seemed similarly placed to the dataset.","d9241421":"# Model Definition (CNN)\n4 layers of Conv2d and MaxPooling, and 3 Dense layers.\nFor regularization, I only used BatchNormalization and a 20% SpatialDropout2d. Tried Dropout and kernel regularization but it didn't improve. ","9bdb7fe6":"# Save Model","a649d3f0":"![](https:\/\/i.ibb.co\/VY6w6sS\/logo.png)","f76110e0":"# Visualize CNN Layers \/ Filters\nTried to visualize what activates some of the convolutional network layers weights. \n\nCode adapted from https:\/\/keras.io\/examples\/vision\/visualizing_what_convnets_learn\/ and tries to maximize filter activation through gradient ascent in order to visualize layer patterns.","13ec92a5":"# Accuracy and Loss\nDraw some lines for training and testing accuracy and loss.","3f070bc1":"# Imports and Declarations\nMainly uses **Keras** on **Tensorflow** backend to run a \"simple\" Convolutional Neural Network on the Fruits360 dataset.\nAlso uses **Sklearn** for post-run metrics, and a few other support libraries.","d23466a9":"# Classification Report and Confusion Matrix\n**Classification report** helps pinpoint which classes are failing more.\n**Confusion matrix** just helps visualize that differently.","e42189b4":"# Image Data Generator (Fruit 360 Dataset)\nThe ImageDataGenerator makes training a lot slower, and beyond 99% accuracy on the Test Data, doesn't really improve it since data was prepared in the same orientation as the training data. But for real-world fruit pictures, it would most likely perform better \"predicts\" if some image transformations are applied through the training generator.\n\nAlso, before running on the Test Data, I started out splitting training data into **Train** and **Validation** in order to simulate a case where I didn't have access to Test Data result labels. After a few runs, I decided to comment the split out, and run the whole training data to get more accurate results on the test data.","bc5ae58c":"# Model Training (CNN)\nNo major difference between using RMSprop and Adam accuracy wise.\nAdded an Early Stoppage callback to retain best weights during training and a TensorBoard callback for easier analysis when running locally."}}