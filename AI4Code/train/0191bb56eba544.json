{"cell_type":{"1e05b6e6":"code","d5288446":"code","c8c05012":"code","f19cd9e8":"code","2c25955c":"code","4be5ff2a":"code","4e611aae":"code","439d6d35":"code","ce2ee6a8":"code","9a248ecc":"code","db8810ac":"code","bfad1dad":"code","95a279d4":"code","dca7748f":"code","6241814e":"code","8471f259":"code","449136ca":"code","648bc3be":"code","b1675ab1":"code","8a35701b":"code","928ea884":"code","f7501468":"code","b5032f88":"code","717d2a71":"code","a5ac902e":"code","3b5ce677":"code","5e31f97a":"code","43bf5da3":"code","3c24223e":"code","6b0b46f5":"code","525162a4":"code","9471d191":"code","0235a81d":"code","9eea9b71":"code","09472283":"code","0672272f":"code","8877570e":"code","0b1482c6":"code","32d03496":"code","0aebf8ad":"code","98bb2c89":"code","5498b1e3":"code","cca02a24":"code","cb261a9e":"code","6724997e":"code","177db9ac":"code","cad668fc":"code","d7c436c9":"code","e33f0ca8":"code","2a873203":"code","359bcf65":"code","4c1c844b":"code","5e5d6e00":"code","2bd390cc":"code","ef3d68b1":"code","924ca276":"code","e1747886":"code","507cdfd6":"code","61ab5cf5":"code","9bea8cfe":"code","08e19991":"code","e88a68c7":"code","61bd3f4b":"code","b7d1e6b3":"code","253edf85":"code","465a39bb":"code","da1170a6":"code","f74f3b45":"code","f5899792":"code","241113f4":"code","176c062d":"code","a5296d2f":"code","be82e484":"code","18c64023":"code","d236e44f":"code","47c1e036":"code","bb774584":"code","aa8d27f2":"code","f309e330":"code","0a0d592a":"code","9fceec3d":"code","ea35353d":"code","771a2e14":"code","ca795302":"code","63a6441a":"code","878f73ce":"code","aadfdac9":"code","22ab1668":"code","3b9db791":"code","4b7e3ef1":"code","4bea1f83":"code","f6b94c64":"code","39aa488a":"code","fb48d858":"code","9bb8b220":"code","4d6edbf5":"code","df30f44c":"code","6ae4f4e9":"code","5659bbe5":"code","048c0c2c":"code","6f7e8ca0":"code","e21e9831":"code","c8d4cb06":"code","c8d4a544":"code","f826477e":"markdown","aeb4b4f1":"markdown","fa669e25":"markdown","55fd047d":"markdown","3a63fdde":"markdown","63eed870":"markdown","7640cd92":"markdown","8a750bb0":"markdown","d5b9927c":"markdown","40558639":"markdown","00606402":"markdown","16babe2d":"markdown","b2b73bbd":"markdown","1e7e6502":"markdown","db00f516":"markdown","fba10fb7":"markdown","0690e632":"markdown","c6f2b0d2":"markdown","f7cf58ba":"markdown","27baf12f":"markdown","d41112b6":"markdown","c91919a2":"markdown","906d3967":"markdown","60e7b4e8":"markdown","43f990e4":"markdown","5e709670":"markdown","a666829a":"markdown","182f8244":"markdown","b5c75b7a":"markdown","5a16ab05":"markdown","6b1d9acf":"markdown","5557f6f4":"markdown","936e3cb9":"markdown","03ceb69e":"markdown","42b8bc9b":"markdown","fbb3667e":"markdown","72aac86e":"markdown","60e6d56f":"markdown","735fd7c5":"markdown","ed760212":"markdown","4f1dc95f":"markdown","6f5f4494":"markdown","2c25b6ae":"markdown","4128ecb4":"markdown","87caa665":"markdown","d200b3fc":"markdown","67a1f446":"markdown","351827b1":"markdown","eefc93fc":"markdown","53d9de3d":"markdown","9c7665eb":"markdown","4b1b3211":"markdown","e8400516":"markdown","851b1c03":"markdown","336a3d9e":"markdown","5ed2d8f5":"markdown","56c6c0fa":"markdown","fbe3f8be":"markdown","5882752e":"markdown","fe8a3093":"markdown","efeea488":"markdown","b9ea07a8":"markdown","618adbe6":"markdown","78b9ad62":"markdown"},"source":{"1e05b6e6":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","d5288446":"sns.set_theme(rc = {'grid.linewidth': 0.6, 'grid.color': 'white',\n                    'axes.linewidth': 1, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': '#000000',\n                    'figure.facecolor': 'white',\n                    'xtick.color': '#000000', 'ytick.color': '#000000'})","c8c05012":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","f19cd9e8":"df_train.head(2)","2c25955c":"df_test.head(2)","4be5ff2a":"df_train.columns.difference(df_test.columns).tolist()","4e611aae":"train_info = pd.DataFrame(zip(df_train.columns, df_train.count(), \n                              df_train.nunique(), df_train.dtypes))\n\ntrain_info.columns = ['Column', 'Count', 'Unique values', 'Dtype']\n\ntest_info = pd.DataFrame(zip(df_test.columns, df_test.count(), \n                             df_test.nunique(), df_test.dtypes))\n\ntest_info.columns = ['Column', 'Count', 'Unique values', 'Dtype']\n\npd.concat([train_info, test_info], axis = 1, join = 'outer', \n           keys = ['Train', 'Test'], ignore_index = False)","439d6d35":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>')","ce2ee6a8":"df_nunique = {var: pd.DataFrame(df_train[var].value_counts()) \n              for var in {'Survived', 'Pclass', 'Sex', \n                          'SibSp', 'Parch', 'Embarked'}}","9a248ecc":"multi_table([df_nunique['Survived'], df_nunique['Pclass'], \n             df_nunique['Sex'], df_nunique['SibSp'], \n             df_nunique['Parch'], df_nunique['Embarked']])","db8810ac":"for column in {'Sex', 'SibSp', 'Parch', 'Embarked'}:\n    df_train[column] = df_train[column].astype('category')\n    \nfor column in {'Sex', 'SibSp', 'Parch', 'Embarked'}:\n    df_test[column] = df_test[column].astype('category')","bfad1dad":"round((df_train.isnull().sum()\/len(df_train)*100).sort_values(\n       ascending = False), 1)","95a279d4":"round((df_test.isnull().sum()\/len(df_test)*100).sort_values(\n       ascending = False), 1)","dca7748f":"df_train.drop('Cabin', axis = 1, inplace = True)\ndf_test.drop('Cabin', axis = 1, inplace = True)","6241814e":"train_no_NA = df_train.dropna()\n\ntrain_cat_visual_0 = train_no_NA[['Sex', 'SibSp', 'Parch', 'Embarked', \n                                  'Pclass', 'Survived']].columns.tolist()","8471f259":"with plt.rc_context(rc = {'figure.dpi': 300, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(2, 3, figsize = (10, 6))\n\n    for indx, (column, axes) in list(enumerate(list(zip(train_cat_visual_0, ax.flatten())))):\n    \n        sns.violinplot(ax = axes, x = train_no_NA[column], \n                       y = train_no_NA['Age'],\n                       scale = 'width', linewidth = 0.5, \n                       palette = 'viridis', inner = None)\n    \n        plt.setp(axes.collections, alpha = 0.3)\n    \n        sns.stripplot(ax = axes, x = train_no_NA[column], \n                  y = train_no_NA['Age'],\n                  palette = 'viridis', alpha = 0.9, \n                  s = 1.5, jitter = 0.1)\n    \n        sns.pointplot(ax = axes, x = train_no_NA[column],\n                  y = train_no_NA['Age'],\n                  color = '#ff5736', scale = 0.25,\n                  estimator = np.median, ci = 'sd',\n                  errwidth = 0.5, capsize = 0.15, join = True)\n    \n        plt.setp(axes.lines, zorder = 100)\n        plt.setp(axes.collections, zorder = 100)\n    \n    else:\n        \n        [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \nplt.tight_layout()\nplt.show()","449136ca":"Pclass_count_train = pd.DataFrame({'Count': df_train.groupby(['Pclass', 'Sex', 'SibSp']).size()})\n\nPclass_mean_std_train = round(pd.pivot_table(df_train, \n                              index = ['Pclass', 'Sex', 'SibSp'], \n                              values = 'Age', aggfunc = (np.median, np.std)), 0)\n\nPclass_count_train.join(Pclass_mean_std_train, how = 'outer')","648bc3be":"df_train.loc[df_train['Age'].isnull(), 'Age'] = df_train.groupby(['Pclass', 'Sex', 'SibSp'])['Age'].transform('median')","b1675ab1":"df_train.loc[df_train['Age'].isnull(), 'Age'] = df_train.groupby(['Pclass', 'Sex'])['Age'].transform('median')","8a35701b":"df_test.loc[df_test['Age'].isnull(), 'Age'] = df_test.groupby(['Pclass', 'Sex', 'SibSp'])['Age'].transform('median')","928ea884":"df_test.loc[df_test['Age'].isnull(), 'Age'] = df_test.groupby(['Pclass', 'Sex'])['Age'].transform('median')","f7501468":"df_train[df_train['Embarked'].isnull()]","b5032f88":"df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace = True)","717d2a71":"df_test[df_test['Fare'].isnull()]","a5ac902e":"df_test['Fare'].fillna(df_test['Fare'].mean(), inplace = True)","3b5ce677":"df_train.drop('PassengerId', axis = 1, inplace = True)\ndf_test.drop('PassengerId', axis = 1, inplace = True)\n\ndf_train.drop('Name', axis = 1, inplace = True)\ndf_test.drop('Name', axis = 1, inplace = True)","5e31f97a":"train_cat_visual_1 = df_train.select_dtypes(\n                     include = ['object', 'category']).columns.tolist()\n\ntrain_cat_visual_1.remove('Ticket')\ntrain_cat_visual_1.append('Pclass')","43bf5da3":"my_palette_0 = ['#481567FF', '#238A8DFF']\n\nwith plt.rc_context(rc = {'figure.dpi': 250, 'axes.labelsize': 6.5, \n                          'xtick.labelsize': 5.5, 'ytick.labelsize': 5.5,\n                          'legend.fontsize': 5.5, 'legend.title_fontsize': 6}):\n\n    fig, ax = plt.subplots(2, 3, figsize = (8, 5))\n\n    for indx, (column, axes) in list(enumerate(list(zip(train_cat_visual_1, ax.flatten())))):\n    \n        sns.countplot(ax = axes, x = df_train[column], hue = df_train['Survived'], \n                      palette = my_palette_0, alpha = 0.8)\n    \n    else:\n        \n        [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \n    axes_legend = ax.flatten()\n\n    axes_legend[1].legend(title = 'Survived', loc = 'upper right')\n    axes_legend[2].legend(title = 'Survived', loc = 'upper right')\n\nplt.tight_layout()\nplt.show()","3c24223e":"df_groupby = {var: pd.DataFrame(df_train.groupby([var, 'Survived']).size()) \n              for var in {'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'}}","6b0b46f5":"multi_table([df_groupby['Pclass'], df_groupby['Sex'], df_groupby['SibSp'], \n             df_groupby['Parch'], df_groupby['Embarked']])","525162a4":"df_train['Family_size'] = df_train['SibSp'].astype('int') + df_train['Parch'].astype('int') + 1\n\ndf_test['Family_size'] = df_test['SibSp'].astype('int') + df_test['Parch'].astype('int') + 1","9471d191":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (6, 3.5))\n\n    sns.countplot(x = df_train['Family_size'], hue = df_train['Survived'], \n                  palette = my_palette_0, alpha = 0.8)\n\n    ax.legend(loc = 'upper right', title = 'Survived')\n\nplt.show()","0235a81d":"conditions_0 = [(df_train['Family_size'] == 1),\n                (df_train['Family_size'] >= 2) & (df_train['Family_size'] < 4),\n                (df_train['Family_size'] == 4),\n                (df_train['Family_size'] > 4)]\n\nvalues_0 = ['Alone', 'Small', 'Medium', 'Large']","9eea9b71":"df_train['Family_s'] = np.select(conditions_0, values_0)","09472283":"conditions_1 = [(df_test['Family_size'] == 1),\n                (df_test['Family_size'] >= 2) & (df_test['Family_size'] < 4),\n                (df_test['Family_size'] == 4),\n                (df_test['Family_size'] > 4)]\n\nvalues_1 = ['Alone', 'Small', 'Medium', 'Large']","0672272f":"df_test['Family_s'] = np.select(conditions_1, values_1)","8877570e":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 3.5))\n\n    sns.countplot(x = df_train['Family_s'], hue = df_train['Survived'], \n                  palette = my_palette_0, alpha = 0.8)\n\n    plt.show()","0b1482c6":"df_train.drop(['SibSp', 'Parch', 'Family_size'], axis = 1, inplace = True)\ndf_test.drop(['SibSp', 'Parch', 'Family_size'], axis = 1, inplace = True)","32d03496":"df_train.head(2)","0aebf8ad":"Ticket_count_map_0 = df_train.groupby('Ticket').size().to_dict()\ndf_train['Ticket_count'] = df_train['Ticket'].map(Ticket_count_map_0)","98bb2c89":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 3.5))\n\n    sns.countplot(x = df_train['Ticket_count'], palette = 'viridis', alpha = 0.85)\n    \n    plt.show()","5498b1e3":"df_train['Ticket'].nunique()","cca02a24":"df_train['Ticket_count'].nunique()","cb261a9e":"Ticket_count_map_1 = df_test.groupby('Ticket').size().to_dict()\ndf_test['Ticket_count'] = df_test['Ticket'].map(Ticket_count_map_1)","6724997e":"df_train.drop('Ticket', axis = 1, inplace = True)\ndf_test.drop('Ticket', axis = 1, inplace = True)","177db9ac":"df_train.select_dtypes(include = ['float64']).describe().T.round(1)","cad668fc":"train_num_visual_0 = df_train.select_dtypes(include = ['float64']).columns.tolist()","d7c436c9":"with plt.rc_context(rc = {'figure.dpi': 200, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                         'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n\n    for indx, (column, axes) in list(enumerate(list(zip(train_num_visual_0, ax.flatten())))):\n    \n        sns.scatterplot(ax = axes, y = df_train[column].index, x = df_train[column], \n                        hue = df_train['Survived'], palette = my_palette_0, s = 15)\n    \n    else:\n        \n        [axes.set_visible(False) for axes in ax.flatten()[indx + 1:]]\n    \n    ax[1].legend(title = 'Survived', loc = 'upper right')\n    \n    plt.tight_layout()\n    plt.show()","e33f0ca8":"with plt.rc_context(rc = {'figure.dpi': 200, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n\n    for indx, (column, axes) in list(enumerate(list(zip(train_num_visual_0, ax.flatten())))):\n    \n        sns.histplot(ax = axes, x = np.log(df_train[column] + 1), hue = df_train['Survived'], \n                     palette = my_palette_0, alpha = 0.8, multiple = 'stack')\n    \n        legend = axes.get_legend() # sns.hisplot has some issues with legend\n        handles = legend.legendHandles\n        legend.remove()\n        axes.legend(handles, ['0', '1'], title = 'Survived', loc = 'upper right')\n    \n        Quantiles = np.quantile(np.log(df_train[column] + 1), [0, 0.25, 0.50, 0.75, 1])\n    \n        for q in Quantiles: axes.axvline(x = q, linewidth = 0.8, color = '#ff5736')\n        \nplt.tight_layout()\nplt.show()","2a873203":"from sklearn.preprocessing import OneHotEncoder","359bcf65":"X_train = df_train.copy()\nX_test = df_test.copy()","4c1c844b":"OHE =  OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n\nX_train_OHE = pd.DataFrame(pd.DataFrame(OHE.fit_transform(X_train[['Sex', \n                           'Embarked', 'Family_s']])))\n\nX_train_OHE.columns = OHE.get_feature_names(['Sex', 'Embarked', 'Family_s'])\n\nX_train.drop(['Sex', 'Embarked', 'Family_s', 'Survived'], axis = 1, \n             inplace = True)\n\nX_train = pd.concat([X_train, X_train_OHE ], axis = 1)","5e5d6e00":"y = df_train['Survived'].copy()","2bd390cc":"X_test_OHE = pd.DataFrame(pd.DataFrame(OHE.fit_transform(X_test[['Sex', \n                          'Embarked', 'Family_s']])))\n\nX_test_OHE.columns = OHE.get_feature_names(['Sex', 'Embarked', 'Family_s'])\n\nX_test.drop(['Sex', 'Embarked', 'Family_s'], axis = 1, inplace = True)\n\nX_test = pd.concat([X_test, X_test_OHE ], axis = 1)","ef3d68b1":"import xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RepeatedStratifiedKFold","924ca276":"x_train, x_test, y_train, y_test = train_test_split(X_train, y, \n                                   test_size= 0.2, random_state = 999)","e1747886":"CV = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 999)\n\nXGB_0 = xgb.XGBClassifier(use_label_encoder = False, \n                          objective = 'binary:logistic')\n\nXGB_param_0 = {'n_estimators': [20, 50, 100, 200, 300],\n               'learning_rate': [0.01, 0.05, 0.07, 0.1],\n               'max_depth': [4],\n               'min_child_weight': [4],\n               'gamma': [0.2],\n               'subsample': [0.9],\n               'colsample_bytree': [0.9]}\n\nXGB_grid_0 = GridSearchCV(XGB_0, XGB_param_0, verbose = False, \n                          scoring = 'neg_log_loss', cv = CV) # n_jobs = 6\n\nXGB_tuned_0 = XGB_grid_0.fit(x_train, y_train, early_stopping_rounds = 15, \n                              eval_set = [[x_test, y_test]], \n                              eval_metric = 'logloss', verbose = False)","507cdfd6":"round(-1*XGB_tuned_0.best_score_, 3)","61ab5cf5":"round(log_loss(y_test, XGB_tuned_0.predict_proba(x_test)), 3)","9bea8cfe":"XGB_tuned_0_results = pd.DataFrame(XGB_tuned_0.cv_results_)[['mean_test_score', 'param_n_estimators', 'param_learning_rate']]\n\nXGB_tuned_0_results['mean_test_score'] = -1*XGB_tuned_0_results['mean_test_score']","08e19991":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = XGB_tuned_0_results, x = 'param_n_estimators', \n                 y = 'mean_test_score', hue = 'param_learning_rate', \n                 marker = 'o', palette = 'viridis')\n\n    plt.show()","e88a68c7":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (4, 3))\n\n    sns.heatmap(confusion_matrix(y_test, XGB_tuned_0.predict(x_test)), \n                annot = True, fmt = 'd', cmap = 'YlGnBu', \n                annot_kws = {'fontsize': 8})\n\n    plt.show()","61bd3f4b":"XGB_tuned_0.best_params_","b7d1e6b3":"XGB_1 = xgb.XGBClassifier(use_label_encoder = False, \n                          objective = 'binary:logistic')\n\nXGB_param_1 = {'n_estimators': [XGB_tuned_0.best_params_.get('n_estimators')],\n         'learning_rate': [XGB_tuned_0.best_params_.get('learning_rate')],\n         'max_depth': [4, 5, 6],\n         'min_child_weight': [2, 3, 4, 5],\n         'gamma': [XGB_tuned_0.best_params_.get('gamma')],\n         'subsample': [XGB_tuned_0.best_params_.get('subsample')],\n         'colsample_bytree': [XGB_tuned_0.best_params_.get('colsample_bytree')]}\n\nXGB_grid_1 = GridSearchCV(XGB_1, XGB_param_1, verbose = False, \n                          scoring = 'neg_log_loss', cv = CV) # n_jobs = 6\n\nXGB_tuned_1 = XGB_grid_1.fit(x_train, y_train, early_stopping_rounds = 15, \n                              eval_set = [[x_test, y_test]], \n                              eval_metric = 'logloss', verbose = False)","253edf85":"round(-1*XGB_tuned_1.best_score_, 3)","465a39bb":"round(log_loss(y_test, XGB_tuned_1.predict_proba(x_test)), 3)","da1170a6":"XGB_tuned_1_results = pd.DataFrame(XGB_tuned_1.cv_results_)[['mean_test_score', 'param_max_depth', 'param_min_child_weight']]\n\nXGB_tuned_1_results['mean_test_score'] = -1*XGB_tuned_1_results['mean_test_score']","f74f3b45":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = XGB_tuned_1_results, x = 'param_max_depth', \n                 y = 'mean_test_score', \n                 hue = 'param_min_child_weight', marker = 'o', palette = 'viridis')\n\n    ax.legend(title = 'param_min_child_weight', ncol = 2)\n    plt.show()","f5899792":"XGB_tuned_1.best_params_","241113f4":"XGB_2 = xgb.XGBClassifier(use_label_encoder = False, \n                          objective = 'binary:logistic')\n\nXGB_param_2 = {'n_estimators': [XGB_tuned_1.best_params_.get('n_estimators')],\n          'learning_rate': [XGB_tuned_1.best_params_.get('learning_rate')],\n          'max_depth': [XGB_tuned_1.best_params_.get('max_depth')],\n          'min_child_weight': [XGB_tuned_1.best_params_.get('min_child_weight')],\n          'gamma': [0.2, 0.3, 0.5, 0.7, 1, 2],\n          'subsample': [0.8, 0.9],\n          'colsample_bytree': [0.8, 0.9]}\n\nXGB_grid_2 = GridSearchCV(XGB_2, XGB_param_2, verbose = False, \n                          scoring = 'neg_log_loss', cv = CV) # n_jobs = 6\n\nXGB_tuned_2 = XGB_grid_2.fit(x_train, y_train, early_stopping_rounds = 15, \n                              eval_set = [[x_test, y_test]], \n                              eval_metric = 'logloss', verbose = False)","176c062d":"round(-1*XGB_tuned_2.best_score_, 3)","a5296d2f":"round(log_loss(y_test, XGB_tuned_2.predict_proba(x_test)), 3)","be82e484":"XGB_tuned_2.best_params_","18c64023":"XGB_3 = xgb.XGBClassifier(use_label_encoder = False, \n                          objective = 'binary:logistic')\n\nXGB_param_3 = {'n_estimators': [XGB_tuned_2.best_params_.get('n_estimators')],\n          'learning_rate': [x \/ 100.0 for x in range(1, 20, 1)],\n          'max_depth': [XGB_tuned_2.best_params_.get('max_depth')],\n          'min_child_weight': [XGB_tuned_2.best_params_.get('min_child_weight')],\n          'gamma': [XGB_tuned_2.best_params_.get('gamma')],\n          'subsample': [XGB_tuned_2.best_params_.get('subsample')],\n          'colsample_bytree': [XGB_tuned_2.best_params_.get('colsample_bytree')]}\n\nXGB_grid_3 = GridSearchCV(XGB_3, XGB_param_3, verbose = False, \n                          scoring = 'neg_log_loss', cv = CV) # n_jobs = 6\n\nXGB_tuned_3 = XGB_grid_3.fit(x_train, y_train, early_stopping_rounds = 15, \n                              eval_set = [[x_test, y_test]], \n                              eval_metric = 'logloss', verbose = False)","d236e44f":"round(-1*XGB_tuned_3.best_score_, 3)","47c1e036":"round(log_loss(y_test, XGB_tuned_3.predict_proba(x_test)), 3)","bb774584":"XGB_tuned_3_results = pd.DataFrame(XGB_tuned_3.cv_results_)[['mean_test_score','param_learning_rate']]\n\nXGB_tuned_3_results['mean_test_score'] = -1*XGB_tuned_3_results['mean_test_score']","aa8d27f2":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = XGB_tuned_3_results, x = 'param_learning_rate', \n                 y = 'mean_test_score', marker = 'o', color = '#481567FF')\n\n    plt.show()","f309e330":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (4, 3))\n\n    sns.heatmap(confusion_matrix(y_test, XGB_tuned_3.predict(x_test)), \n                annot = True, fmt = 'd', cmap = 'YlGnBu',\n                annot_kws = {'fontsize': 8})\n\n    plt.show()","0a0d592a":"XGB_tuned_3.best_params_","9fceec3d":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score","ea35353d":"CV_final = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 999)\n\nXGB_final = xgb.XGBClassifier(\n        use_label_encoder = False, objective = 'binary:logistic',\n        eval_metric = 'logloss',\n        n_estimators = XGB_tuned_3.best_params_.get('n_estimators'),\n        learning_rate = XGB_tuned_3.best_params_.get('learning_rate'),\n        max_depth = XGB_tuned_3.best_params_.get('max_depth'),\n        min_child_weight = XGB_tuned_3.best_params_.get('min_child_weight'),\n        gamma = XGB_tuned_3.best_params_.get('gamma'),\n        subsample = XGB_tuned_3.best_params_.get('subsample'),\n        colsample_bytree = XGB_tuned_3.best_params_.get('colsample_bytree'))\n\nXGB_CV_scores = cross_val_score(XGB_final, X_train, y, \n                                scoring = 'neg_log_loss', cv = CV_final)\n\nXGB_fit_final = XGB_final.fit(X_train, y, verbose = False)","771a2e14":"round(-1*np.average(XGB_CV_scores), 3)","ca795302":"y_pred_XGB = XGB_fit_final.predict(X_test)","63a6441a":"from sklearn.ensemble import RandomForestClassifier","878f73ce":"RF_0 = RandomForestClassifier()\n\nRF_param_0 = {'n_estimators': [850, 900, 950, 1000],\n              'max_features': [6, 7],\n              'max_depth': [6, 7],\n              'criterion': ['gini']}\n\nRF_grid_0 = GridSearchCV(RF_0, RF_param_0, cv = CV, verbose = False, \n                         scoring = 'neg_log_loss') # n_jobs = 6\n\nRF_tuned_0 = RF_grid_0.fit(X_train, y)","aadfdac9":"round(-1*RF_tuned_0.best_score_, 3)","22ab1668":"RF_results_0 = pd.DataFrame(RF_tuned_0.cv_results_)[['mean_test_score', 'param_n_estimators', 'param_max_features', 'param_max_depth']]\n\nRF_results_0['mean_test_score'] = -1*RF_results_0['mean_test_score']","3b9db791":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7,\n                          'legend.fontsize': 7, 'legend.title_fontsize': 7.5}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = RF_results_0, x = 'param_n_estimators', \n                 y = 'mean_test_score', hue = 'param_max_features', \n                 style =  'param_max_depth', palette = 'viridis', ci = None)\n\n    ax.legend(ncol = 1, loc = 'upper left')\n    plt.show()","4b7e3ef1":"RF_tuned_0.best_params_","4bea1f83":"RF_1 = RandomForestClassifier()\n\nRF_param_1 = {'n_estimators': [RF_tuned_0.best_params_.get('n_estimators')],\n             'max_features': [RF_tuned_0.best_params_.get('max_features')],\n             'max_depth': [RF_tuned_0.best_params_.get('max_depth')],\n             'criterion': [RF_tuned_0.best_params_.get('criterion')],\n             'min_samples_split': [2, 3, 4, 5]}\n\nRF_grid_1 = GridSearchCV(RF_1, RF_param_1, cv = CV, verbose = False, \n                         scoring = 'neg_log_loss') # n_jobs = 6 \n\nRF_tuned_1 = RF_grid_1.fit(X_train, y)","f6b94c64":"round(-1*RF_tuned_1.best_score_, 3)","39aa488a":"RF_results_1 = pd.DataFrame(RF_tuned_1.cv_results_)[['mean_test_score', 'param_min_samples_split']]\n\nRF_results_1['mean_test_score'] = -1*RF_results_1['mean_test_score']","fb48d858":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = RF_results_1, x = 'param_min_samples_split', \n                 y = 'mean_test_score', marker = 'o', color = '#481567FF')\n\n    plt.show()","9bb8b220":"RF_tuned_1.best_params_","4d6edbf5":"RF_2 = RandomForestClassifier()\n\nRF_param_2 = {'n_estimators': [RF_tuned_1.best_params_.get('n_estimators')],\n              'max_features': [RF_tuned_1.best_params_.get('max_features')],\n              'max_depth': [RF_tuned_1.best_params_.get('max_depth')],\n              'criterion': [RF_tuned_1.best_params_.get('criterion')],\n              'min_samples_split': [RF_tuned_1.best_params_.get('min_samples_split')], \n              'max_leaf_nodes': list(range(20, 35, 1))}\n\nRF_grid_2 = GridSearchCV(RF_2, RF_param_2, cv = CV, verbose = False, \n                         scoring = 'neg_log_loss') # n_jobs = 6\n\nRF_tuned_2 = RF_grid_2.fit(X_train, y)","df30f44c":"round(-1*RF_tuned_2.best_score_, 3)","6ae4f4e9":"RF_results_2 = pd.DataFrame(RF_tuned_2.cv_results_)[['mean_test_score', 'param_max_leaf_nodes']]\n\nRF_results_2['mean_test_score'] = -1*RF_results_2['mean_test_score']","5659bbe5":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 7, 'ytick.labelsize': 7}):\n\n    fig, ax = plt.subplots(1, 1, figsize = (7, 4))\n\n    sns.lineplot(data = RF_results_2, x = 'param_max_leaf_nodes', \n                 y = 'mean_test_score', marker = 'o', color = '#481567FF')\n\n    plt.show()","048c0c2c":"RF_tuned_2.best_params_","6f7e8ca0":"RF_final = RandomForestClassifier(\n       n_estimators = RF_tuned_2.best_params_.get('n_estimators'),\n       max_features = RF_tuned_2.best_params_.get('max_features'),\n       max_depth = RF_tuned_2.best_params_.get('max_depth'),\n       criterion = RF_tuned_2.best_params_.get('criterion'),\n       min_samples_split = RF_tuned_2.best_params_.get('min_samples_split'), \n       max_leaf_nodes = RF_tuned_2.best_params_.get('max_leaf_nodes'))\n\nRF_CV_scores = cross_val_score(RF_final, X_train, y, scoring = 'neg_log_loss', \n                               cv = CV_final)\n\nRF_fit_final = RF_final.fit(X_train, y)","e21e9831":"round(-1*np.average(RF_CV_scores), 3)","c8d4cb06":"y_pred_RF = RF_fit_final.predict(X_test)","c8d4a544":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nmy_submission = pd.DataFrame({'PassengerId': df_test['PassengerId'], \n                              'Survived': y_pred_RF})\n\nmy_submission.to_csv('submission.csv', index = False)","f826477e":"It is quite handy when you can see all at once: column names, counts, unique counts and data types. I bet there is a function for it; however, I just wanted to practice a little bit.","aeb4b4f1":"Since some groups had NaN values, I repeated the process of aggregating twice. For the second time I removed \"SibSp\" before grouping.","fa669e25":"### 4. High cardinality features","55fd047d":"I could see that \"Fare\" was significantly skewed. Nonetheless, I didn\u2019t want to use binning in this case, as I didn\u2019t see a clear way of doing it. It seemed a little bit arbitrary. Frankly speaking, I thought that leaving these variables as they were would be a better idea than somehow \"rebuilding\" them.","3a63fdde":"### 1. Comparing column names","63eed870":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \n<span style=\"font-size: 15px\">Taking into account that XGB test scores (\"logloss\") were higher than that of RF, I was inclined to believe that XGB was overfitting. In order to prevent that, further feature engineering was necessary. Thus, out of 2 models I picked RF.\n<\/div>","7640cd92":"<h2><center> Getting rid of some columns <\/center><\/h2>","8a750bb0":"If you want to make sure that categorical features do not have any incorrectly labelled values, you should check what unique categories they have. In this case, it was done via creating tables with possible values for each variable.","d5b9927c":"### 4. Changing data type of some columns","40558639":"A decision was made to remove \"Cabin\" since it had plenty of NaN values (almost 80%). It was unlikely that this feature would be helpful.","00606402":"### 1. Basic libraries","16babe2d":"### 2. Dealing with \"Age\"","b2b73bbd":"#### 2.2 Tuning \"min_samples_split\"","1e7e6502":"### 1. Encoding ordinal features","db00f516":"Since there were only two NaN values, it made sense not to overthink it and replace values with the mode.","fba10fb7":"After exploring graphs and tables, I could clearly see that \"SibSp\" and \"Parch\" had plenty of levels that contained just few values. These features were imbalanced, so it made sense to first combine them and then regroup the new variable.","0690e632":"\"Pclass\", \"Sex\" and \"SibSp\" were chosen for predicting \"Age\". ","c6f2b0d2":"### 3.Unbalanced features","f7cf58ba":"#### 2.1 Testing set","27baf12f":"One more model that I decided to try was RF.","d41112b6":"Some global parametres for plots:","c91919a2":"### 3. Getting unique values of each categorical variable","906d3967":"#### 1.5 Training the final model","60e7b4e8":"When you encounter a variable with too many unique values, it can be useful to create a new variable with counts \/ frequencies instead of the categories. \"Ticket\" was transformed this way.","43f990e4":"#### 2.1 Tuning \"n_estimators\", \"max_features\" and \"max_depth\"","5e709670":"### 3. Dealing with \"Embarked\"","a666829a":"# \u2116 0. Getting started","182f8244":"### 2. Encoding nominal features","b5c75b7a":"### 4. Dealing with \"Fare\"","5a16ab05":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:<\/span> \n<span style=\"font-size: 15px\">I used a sequential grid search because the data set was quite small; nevertheless, it still took me quite some time to finish parameter tuning. On the other hand, a randomized search that I did not cover in this notebook (but heavily relied on in my other <a href=\"https:\/\/www.kaggle.com\/suprematism\/top-7-useful-graphs-and-encoding-techniques\">notebook<\/a>) gave me nearly the same leaderboard scores. So, by and large, I guess you should not opt for a grid search.<\/span>\n<\/div>","6b1d9acf":"# \u2116 2. Further analysis and feature engineering","5557f6f4":"Lastly, I dropped unnecessary features.","936e3cb9":"#### 1.2 Tuning \"max_depth\" and \"min_child_weight\"","03ceb69e":"#### 2.3 Tuning \"max_leaf_nodes\"","42b8bc9b":"# \u2116 1. Preliminary analysis and missing values","fbb3667e":"# \u2116 3. Training models","72aac86e":"#### 2.1 Training set ","60e6d56f":"### 2. Grouped tables for categorical variables","735fd7c5":"Analysing these graphs, I concluded that variables such as \"Survived\", \"Pclass\", \"Sex\" and \"SibSp\" were valuable in predicting \"Age\". These features established a more or less clear connection with \"Age\" and, on top of that, the standard deviations were not too high, compared to other variables, which was crucial as I could be more certain that imputed values would be accurate enough. Note, I didn\u2019t use \"Survived\" because it was not available in the test set.","ed760212":"The same was done for the test set.","4f1dc95f":"#### 1.3 Tuning \"gamma\", \"subsample\" and \"colsample_bytree\"","6f5f4494":"### 1. Plotting categorical variables","2c25b6ae":"### 1. XGBoost","4128ecb4":"<h2><center> Encoding <\/center><\/h2>","87caa665":"#### 2.1 Training set","d200b3fc":"### 1. Dealing with \"Cabin\"","67a1f446":"#### 1.4 Retuning \"learning_rate\"","351827b1":"#### 2.4 Training the final model","eefc93fc":"I went for the same strategy for \"Fare\". The only NaN value was replaced with the mean.","53d9de3d":"<h2><center> Getting some info about columns <\/center><\/h2>","9c7665eb":"So, instead of 681 unique values we got only 7:","4b1b3211":"I built XGBoost using early stopping and I also tried plotting some graphs. Hopefully, they can be helpful to readers.","e8400516":"I treated \"Pclass\" as an ordinal variable due to the fact that it represented socio-economic status. Taking into account that \"Pclass\" was already correctly mapped, I did not need to encode it.","851b1c03":"### 2. Random Forest","336a3d9e":"#### 2.2 Testing set ","5ed2d8f5":"<h2><center> Exploring numeric variables <\/center><\/h2>","56c6c0fa":"NaNs in \"Age\" were replaced with the mean value. In order to increase the accuracy, \"Age\" needed to be broken down into smaller categories which were yet to be defined. Needless to say, these categories had to establish a meaningful connection with \"Age\" to be useful. To make my analysis easier, I plotted some graphs, combining <span style=\"color:#E85E40\"> kdeplots <\/span> with <span style=\"color:#E85E40\"> stripplots <\/span>.","fbe3f8be":"<h2><center> Missing values <\/center><\/h2>","5882752e":"<h2><center> Exploring categorical variables <\/center><\/h2>","fe8a3093":"### 2. Data","efeea488":"### 2. A handmade <span style=\"color:#E85E40\"> info() <\/span> function","b9ea07a8":"#### 1.1 Tuning \"learning_rate\" and \"n_estimators\"","618adbe6":"## Writing this notebook helped me get started with Python. Tackling a great number of interesting and to some extent practical problems, I was able to structure my knowledge which essentially was my primary purpose. I hope that by sharing it, I will help someone out!","78b9ad62":"<h2><center> The first glimpse <\/center><\/h2>"}}