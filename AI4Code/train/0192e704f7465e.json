{"cell_type":{"cf9fe075":"code","2923d378":"code","66231d48":"code","aac102a1":"code","75e31376":"code","89fd9b61":"code","9c7666d6":"code","8ce94419":"code","d3b0aba9":"code","03b17408":"code","413b7a9a":"code","85c966fa":"code","cd299e7a":"code","542b5eb0":"code","a812ee8b":"code","b0e4b206":"code","76fadf77":"code","e8eee851":"code","c9d0c918":"code","9bc22bfd":"code","e7513767":"code","0da1d281":"code","e0627b3f":"code","e6542689":"code","1d7f5019":"code","cf73ec21":"code","4ac75516":"code","06eaad35":"code","21a66650":"code","60d2bf50":"code","7f7ee0a8":"code","9841028d":"code","40c13137":"code","1e594775":"code","b875098e":"code","e271ea75":"code","3ec2dcdc":"code","896d7164":"code","224e57c7":"code","7ce0dd58":"markdown","88c4b1ae":"markdown","0d08a86b":"markdown","75362eff":"markdown","01b4cbd5":"markdown","b1d54e8a":"markdown","4102c5eb":"markdown","c6da3bc5":"markdown","b5647d67":"markdown","c56a42c5":"markdown","477dac62":"markdown","fc15d865":"markdown","6988b60f":"markdown","9951dc4d":"markdown","72493c61":"markdown","96a14c3f":"markdown","aa4f39ba":"markdown"},"source":{"cf9fe075":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nimport scipy.stats as stats\nimport lightgbm as lgb\nimport seaborn as sns\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport json\nimport sys\nimport csv\nimport os","2923d378":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","66231d48":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\nsns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline","aac102a1":"# import Dataset to play with it\ntrain = pd.read_csv('..\/input\/train.csv')\ntest= pd.read_csv('..\/input\/test.csv')","75e31376":"\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","89fd9b61":"type(train),type(test)","9c7666d6":"# shape\nprint(train.shape)","8ce94419":"# shape\nprint(test.shape)","d3b0aba9":"print(train.info())","03b17408":"train[\"Fence\"].value_counts()\n","413b7a9a":"train_id=train['Id'].copy()\ntest_id=test['Id'].copy()","85c966fa":"train.isnull().sum().head(2)","cd299e7a":"train.groupby('SaleType').count()","542b5eb0":"train.columns","a812ee8b":"type((train.columns))","b0e4b206":"train[train['SalePrice']>700000]","76fadf77":"train['SalePrice'].describe()","e8eee851":"sns.set(rc={'figure.figsize':(9,7)})\nsns.distplot(train['SalePrice']);","c9d0c918":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","9bc22bfd":"# Modify the graph above by assigning each species an individual color.\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\ng=sns.FacetGrid(train[columns], hue=\"OverallQual\", size=5) \\\n   .map(plt.scatter, \"OverallQual\", \"SalePrice\") \\\n   .add_legend()\ng=g.map(plt.scatter, \"OverallQual\", \"SalePrice\",edgecolor=\"w\").add_legend();\nplt.show()\n","e7513767":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\n","0da1d281":"ax= sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns])\nax= sns.stripplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], jitter=True, edgecolor=\"gray\")\nplt.show()","e0627b3f":"mini_train=train[columns]\nf,ax=plt.subplots(1,2,figsize=(20,10))\nmini_train[mini_train['SalePrice']>100000].GarageArea.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('SalePrice>100000')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\nmini_train[mini_train['SalePrice']<100000].GarageArea.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('SalePrice<100000')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","e6542689":" \nmini_train[['SalePrice','OverallQual']].groupby(['OverallQual']).mean().plot.bar()\n ","1d7f5019":"train['OverallQual'].value_counts().plot(kind=\"bar\");","cf73ec21":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","4ac75516":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\nplt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\ntrain = train[train.GrLivArea < 4000]","06eaad35":"#deleting points\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)","21a66650":"#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","60d2bf50":"all_data = pd.get_dummies(all_data)","7f7ee0a8":"# Log transform the target for official scoring\n#The key point is to to log_transform the numeric variables since most of them are skewed.\ntrain.SalePrice = np.log1p(train.SalePrice)\ny = train.SalePrice","9841028d":"plt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","40c13137":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","1e594775":"X_train.info()","b875098e":"X=np.transpose(X_train)\n\n","e271ea75":"num_test = 0.3\nX_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=num_test, random_state=100)","3ec2dcdc":"# Fit Random Forest on Training Set\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=300, random_state=0)\nregressor.fit(X_train, y_train)\n\n# Score model\nregressor.score(X_train, y_train)","896d7164":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import utils","224e57c7":"from sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y)\nprint(training_scores_encoded)\nprint(utils.multiclass.type_of_target(y))\nprint(utils.multiclass.type_of_target(y.astype('int')))\nprint(utils.multiclass.type_of_target(training_scores_encoded))\n","7ce0dd58":"We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\n**You should see 1460 instances and  81  attributes for train and 1459 instances and  80 attributes for test**","88c4b1ae":"For getting some information about the dataset you can use **info()** command.","0d08a86b":"<a id=\"621\"><\/a> <br>\n### 6-2-1 Scatter plot\n\nScatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n\n\n","75362eff":"<a id=\"7\"><\/a> <br>\n## 7- Model Deployment\n","01b4cbd5":"Train has one column more than test why?   (yes ==>> **target value**)","b1d54e8a":"<a id=\"622\"><\/a> <br>\n### 6-2-2 Box\nIn descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]","4102c5eb":"<a id=\"51\"><\/a> <br>\n### 5-1 Import","c6da3bc5":"<a id=\"52\"><\/a> <br>\n### 5-2 Version","b5647d67":"The **concat** function does all of the heavy lifting of performing concatenation operations along an axis. Let us create all_data.","c56a42c5":"It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n\n","477dac62":"<a id=\"641\"><\/a> <br>\n## 6-4-1 Handle missing values\n","fc15d865":"<a id=\"553\"><\/a> <br>\n### 5-5-3 Setup\n\nA few tiny adjustments for better **code readability**","6988b60f":"to print dataset **columns**, we can use columns atribute","9951dc4d":"Copy Id for test and train data set","72493c61":"<a id=\"74\"><\/a> <br>\n## 7-4 RandomForestClassifier\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","96a14c3f":"<a id=\"613\"><\/a> <br>\n### 6-1-3 Skewness vs Kurtosis\n1. Skewness\n    1. It is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution. It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n<img src='https:\/\/www.parsmodir.com\/db\/statistic\/skewness.png'>\n1. Kurtosis\n    1. Kurtosis is all about the tails of the distribution\u200a\u2014\u200anot the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.\n","aa4f39ba":"Flexibly plot a univariate distribution of observations.\n\n"}}