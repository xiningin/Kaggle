{"cell_type":{"b8177a4e":"code","c280e477":"code","b2b6b1cd":"code","010b1cfe":"code","e7e12e68":"code","823567f4":"code","3ec3d869":"code","cb6eb99f":"code","05448cdd":"code","b17d1fa4":"code","6327890e":"code","be124f76":"code","cec114ac":"code","284af38c":"code","1464fa57":"code","88ee7c92":"code","15b7b703":"code","6cab4c0c":"code","7e207cd7":"code","4ff4e703":"code","8d9ac6f4":"code","638087fd":"code","07776952":"code","e93033a4":"code","dd6ef779":"code","12191954":"code","67fa7f51":"code","d8d95afd":"code","3b04e1eb":"code","ea417c7e":"code","b986cffd":"code","9fb0a396":"code","55b8222e":"code","92bdeb40":"markdown","b7fffeb7":"markdown","e4531321":"markdown","b5e24cbf":"markdown","b88bf02b":"markdown","5380f49a":"markdown","069aa71b":"markdown"},"source":{"b8177a4e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n%matplotlib inline\nimport cv2\nfrom scipy.stats import itemfreq\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random # for setting seed\nimport tensorflow\nimport keras\nimport IPython","c280e477":"MY_SEED = 42 # 480 could work too\nrandom.seed(MY_SEED)\nnp.random.seed(MY_SEED)\ntensorflow.set_random_seed(MY_SEED)\n\nprint(IPython.sys_info())\n# get module information\n!pip freeze > frozen-requirements.txt\n# append system information to file\nwith open(\"frozen-requirements.txt\", \"a\") as file:\n    file.write(IPython.sys_info())","b2b6b1cd":"from tensorflow.python.client import device_lib\n# print out the CPUs and GPUs\nprint(device_lib.list_local_devices())","010b1cfe":"# https:\/\/stackoverflow.com\/questions\/25705773\/image-cropping-tool-python\n# because painting images are hella big\nfrom PIL import Image\nImage.MAX_IMAGE_PIXELS = None","e7e12e68":"# globals\n\nDATA_DIR = '..\/input\/painters-train-part-1\/'\n\nTRAIN_1_DIR = '..\/input\/painters-train-part-1\/train_1\/train_1\/'\nTRAIN_2_DIR = '..\/input\/painters-train-part-1\/train_2\/train_2\/'\nTRAIN_3_DIR = '..\/input\/painters-train-part-1\/train_3\/train_3\/'\n\nTRAIN_4_DIR = '..\/input\/painters-train-part-2\/train_4\/train_4\/'\nTRAIN_5_DIR = '..\/input\/painters-train-part-2\/train_5\/train_5\/'\nTRAIN_6_DIR = '..\/input\/painters-train-part-2\/train_6\/train_6\/'\n\nTRAIN_7_DIR = '..\/input\/painters-train-part-3\/train_7\/train_7\/'\nTRAIN_8_DIR = '..\/input\/painters-train-part-3\/train_8\/train_8\/'\nTRAIN_9_DIR = '..\/input\/painters-train-part-3\/train_9\/train_9\/'\n\nTRAIN_DIRS = [TRAIN_1_DIR, TRAIN_2_DIR, TRAIN_3_DIR,\n             TRAIN_4_DIR, TRAIN_5_DIR, TRAIN_6_DIR,\n             TRAIN_7_DIR, TRAIN_8_DIR, TRAIN_9_DIR]\n\nTEST_DIR = '..\/input\/painter-test\/test\/test\/'","823567f4":"df = pd.read_csv(DATA_DIR + 'all_data_info.csv')\nprint(\"df.shape\", df.shape)","3ec3d869":"# quick fix for corrupted files\nlist_of_corrupted = ['3917.jpg','18649.jpg','20153.jpg','41945.jpg',\n'79499.jpg','91033.jpg','92899.jpg','95347.jpg',\n'100532.jpg','101947.jpg']\n# display the corrupted rows of dataset for context\ncorrupt_df = df[df[\"new_filename\"].isin(list_of_corrupted) == True]\nprint(corrupt_df.head(len(list_of_corrupted)))\n\n# completely get rid of them\ndf = df[df[\"new_filename\"].isin(list_of_corrupted) == False]\n\n# try to see if they are still there\nprint(df[df[\"new_filename\"].isin(list_of_corrupted) == True])\n\nprint(\"df.shape\", df.shape)","cb6eb99f":"\ntrain_df = df[df[\"in_train\"] == True]\ntest_df = df[df['in_train'] == False]\ntrain_df = train_df[['artist', 'new_filename']]\ntest_df = test_df[['artist', 'new_filename']]\n\nprint(\"test_df.shape\", test_df.shape)\nprint(\"train_df.shape\", train_df.shape)\n\nartists = {} # holds artist hash & the count\nfor a in train_df['artist']:\n    if (a not in artists):\n        artists[a] = 1\n    else:\n        artists[a] += 1\n\ntraining_set_artists = []\nfor a,count in artists.items():\n    if(int(count) >= 300):\n        training_set_artists.append(a)\n\nprint(\"number of artsits\",len(training_set_artists))\n\nprint(\"\\nlist of artists...\\n\", training_set_artists)\n","05448cdd":"t_df = train_df[train_df[\"artist\"].isin(training_set_artists)]\n\nt_df.head(5)","b17d1fa4":"t1_df = t_df[t_df['new_filename'].str.startswith('1')]\n\nt2_df = t_df[t_df['new_filename'].str.startswith('2')]\n\nt3_df = t_df[t_df['new_filename'].str.startswith('3')]\n\nt4_df = t_df[t_df['new_filename'].str.startswith('4')]\n\nt5_df = t_df[t_df['new_filename'].str.startswith('5')]\n\nt6_df = t_df[t_df['new_filename'].str.startswith('6')]\n\nt7_df = t_df[t_df['new_filename'].str.startswith('7')]\n\nt8_df = t_df[t_df['new_filename'].str.startswith('8')]\n\nt9_df = t_df[t_df['new_filename'].str.startswith('9')]\n\nall_train_dfs = [t1_df, t2_df, t3_df,\n                t4_df, t5_df, t6_df,\n                t7_df, t8_df, t9_df]\n\nt9_df.head(5)","6327890e":"from tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","be124f76":"len(training_set_artists)","cec114ac":"num_classes = len(training_set_artists) # one class per artist\nweights_notop_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = Sequential()\nmodel.add(ResNet50(\n  include_top=False,\n  weights=weights_notop_path,\n  pooling='avg'\n))\nmodel.add(Dense(\n  num_classes,\n  activation='softmax'\n))\n\nmodel.layers[0].trainable = False","284af38c":"model.compile(\n  optimizer='adam', # lots of people reccommend Adam optimizer\n  loss='categorical_crossentropy', # aka \"log loss\" -- the cost function to minimize \n  # so 'optimizer' algorithm will minimize 'loss' function\n  metrics=['accuracy'] # ask it to report % of correct predictions\n)","1464fa57":"# model globals\nIMAGE_SIZE = 224\nBATCH_SIZE = 96\nTEST_BATCH_SIZE = 17 # because test has 23817 images and factors of 23817 are 3*17*467\n                     # it is important that this number evenly divides the total num images \nVAL_SPLIT = 0.25","88ee7c92":"def setup_generators(\n    val_split, train_dataframe, train_dir,\n    img_size, batch_size, my_seed, list_of_classes,\n    test_dataframe, test_dir, test_batch_size\n):\n    print(\"-\"*20)\n    if not preprocess_input:\n          raise Exception(\"please do import call 'from tensorflow.python.keras.applications.resnet50 import preprocess_input'\")\n\n    # setup resnet50 preprocessing \n    data_gen = ImageDataGenerator(\n        preprocessing_function=preprocess_input,\n        validation_split=val_split)\n\n    print(len(train_dataframe), \"images in\", train_dir, \"and validation_split =\", val_split)\n    print(\"\\ntraining set ImageDataGenerator\")\n    train_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        target_size=(img_size, img_size),\n        subset=\"training\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    print(\"\\nvalidation set ImageDataGenerator\")\n    valid_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        subset=\"validation\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        target_size=(img_size,img_size),\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n    print(\"\\ntest set ImageDataGenerator\")\n    test_gen = test_data_gen.flow_from_dataframe(\n        dataframe=test_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=test_dir,\n        x_col='new_filename',\n        y_col=None,\n        has_ext=True,\n        batch_size=test_batch_size,\n        seed=my_seed,\n        shuffle=False, # dont shuffle test directory\n        class_mode=None,\n        target_size=(img_size,img_size)\n    )\n\n    return (train_gen, valid_gen, test_gen)\n\nprint(\"defined setup_generators()\")","15b7b703":"train_gens = [None]*len(TRAIN_DIRS)\nvalid_gens = [None]*len(TRAIN_DIRS)\ntest_gen  = None # only 1 test_gen\ni = 0\nfor i in range(0, len(TRAIN_DIRS)):\n    train_gens[i], valid_gens[i], test_gen = setup_generators(\n        train_dataframe=all_train_dfs[i], train_dir=TRAIN_DIRS[i],\n        val_split=VAL_SPLIT, img_size=IMAGE_SIZE, batch_size=BATCH_SIZE, my_seed=MY_SEED, \n        list_of_classes=training_set_artists, test_dataframe=test_df, \n        test_dir=TEST_DIR, test_batch_size=TEST_BATCH_SIZE\n    )\n    i += 1","6cab4c0c":"# the tutorial had 10 epochs... \nMAX_EPOCHS = 20 * len(train_gens) # should be a multiple of 9 because need evenly train each train_dir\nDIR_EPOCHS = 1 # fit each train_dir at least this many times before overfitting","7e207cd7":"histories = []\n\ne=0\nwhile ( e < MAX_EPOCHS):\n    for i in range(0, len(train_gens)):\n        # train_gen.n = number of images for training\n        STEP_SIZE_TRAIN = train_gens[i].n\/\/train_gens[i].batch_size\n        # train_gen.n = number of images for validation\n        STEP_SIZE_VALID = valid_gens[i].n\/\/valid_gens[i].batch_size\n        print(\"STEP_SIZE_TRAIN\",STEP_SIZE_TRAIN)\n        print(\"STEP_SIZE_VALID\",STEP_SIZE_VALID)\n        histories.append(\n            model.fit_generator(generator=train_gens[i],\n                                steps_per_epoch=STEP_SIZE_TRAIN,\n                                validation_data=valid_gens[i],\n                                validation_steps=STEP_SIZE_VALID,\n                                epochs=DIR_EPOCHS)\n        )\n        e+=1","4ff4e703":"# for i in range(0, len(valid_gens)):\n# model.evaluate_generator(generator=valid_gens[8])","8d9ac6f4":"accuracies = []\nval_accuracies = []\nlosses = []\nval_losses = []\nfor hist in histories:\n    if hist:\n        accuracies += hist.history['acc']\n        val_accuracies += hist.history['val_acc']\n        losses += hist.history['loss']\n        val_losses += hist.history['val_loss']","638087fd":"# Plot training & validation accuracy values\nplt.plot(accuracies)\nplt.plot(val_accuracies)\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(losses)\nplt.plot(val_losses)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","07776952":"model.save('painters_adam.h5')","e93033a4":"PRED_STEPS = len(test_gen) #100 # default would have been len(test_gen)","dd6ef779":"# Need to reset the test_gen before calling predict_generator\n# This is important because forgetting to reset the test_generator results in outputs with a weird order.\ntest_gen.reset()\npred=model.predict_generator(test_gen, verbose=1, steps=PRED_STEPS)","12191954":"print(len(pred),\"\\n\",pred)","67fa7f51":"predicted_class_indices=np.argmax(pred,axis=1)","d8d95afd":"print(len(predicted_class_indices),\"\\n\",predicted_class_indices)\nprint(\"it has values ranging from \",min(predicted_class_indices),\"...to...\",max(predicted_class_indices))","3b04e1eb":"labels = (train_gens[0].class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","ea417c7e":"print(\"*\"*20+\"\\nclass_indices\\n\"+\"*\"*20+\"\\n\",train_gens[0].class_indices,\"\\n\")\nprint(\"*\"*20+\"\\nlabels\\n\"+\"*\"*20+\"\\n\",labels,\"\\n\")\nprint(\"*\"*20+\"\\npredictions has\", len(predictions),\"values that look like\",\"'\"+str(predictions[0])+\"' which is the first prediction and corresponds to this index of the classes:\",train_gens[0].class_indices[predictions[0]])","b986cffd":"# Save the results to a CSV file.\nfilenames=test_gen.filenames[:len(predictions)] # because \"ValueError: arrays must all be same length\"\n\nreal_artists = []\nfor f in filenames:\n    real = test_df[test_df['new_filename'] == f].artist.get_values()[0]\n    real_artists.append(real)\n\nresults=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions,\n                      \"Real Values\":real_artists})\nresults.to_csv(\"results.csv\",index=False)","9fb0a396":"results.head()","55b8222e":"count = 0\nmatch = 0\nfor p, r in zip(results['Predictions'], results['Real Values']):\n    count += 1\n    if p == r:\n        match += 1\n\nprint(\"test accuracy on never before seen images\")\nprint(match,\"\/\",count,\"=\",\"{:.4f}\".format(match\/count))","92bdeb40":"# now we have the artists with over 300 paintings and are in the training set","b7fffeb7":"# TRAINING TIME!  \ud83c\udf89 \ud83c\udf8a \ud83c\udf81","e4531321":"#\u00a0setup the image data generator for train_1 with a dataframe\n### https:\/\/medium.com\/@vijayabhaskar96\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c","b5e24cbf":"# specify the model that classifies 38 artists \ud83c\udfa8 \ud83d\udd8c","b88bf02b":"# Evaluate the model \ud83e\uddd0 \ud83e\udd14","5380f49a":"# Compile Model","069aa71b":"# Predict the output \ud83d\udd2e \ud83c\udfa9"}}