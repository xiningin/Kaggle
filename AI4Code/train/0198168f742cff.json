{"cell_type":{"e81b6d5f":"code","753f617e":"code","ad456f8b":"code","3e03758c":"code","b83ae8a3":"code","6caac40b":"code","df4e4ae9":"code","b1b3db3f":"code","768c00f4":"code","9ff5a0e0":"code","4e56f5c5":"code","b7f34a73":"code","bbeb48c2":"code","e9c4efc6":"code","8541d349":"code","6bbc8eb8":"code","fb0d2738":"code","a2fa0afb":"code","6096aaf4":"code","57689ae6":"code","d56ea09a":"code","7bcc7777":"code","838f383d":"markdown","c8371544":"markdown","b981f6f7":"markdown","9e856436":"markdown","49dd7a50":"markdown","3160e1c7":"markdown","1a9c8c69":"markdown","a4c4163e":"markdown","658536ce":"markdown","349d52a8":"markdown","cbae74bd":"markdown","8e2184be":"markdown","2da70d8a":"markdown","9a8382ef":"markdown","ee72a457":"markdown","37ae87b5":"markdown","435d662c":"markdown","c8efcb50":"markdown","c9b02d23":"markdown","ebb5bd66":"markdown","7d9ddf86":"markdown","f23ea692":"markdown","f183229e":"markdown","9c9589e7":"markdown","4b9e1411":"markdown","3a8919ab":"markdown","e6fd4634":"markdown","06ff1ccc":"markdown","77fefd0e":"markdown","0babb737":"markdown","e6968377":"markdown","6ecaf5b3":"markdown"},"source":{"e81b6d5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","753f617e":"from IPython.display import Image\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nimport cv2","ad456f8b":"filepath =\"..\/input\/greyscale-image\/Image.jpg\"","3e03758c":"Image(filepath)","b83ae8a3":"img = cv2.imread(filepath)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nr, g, b = cv2.split(img)\nr = r.flatten()\ng = g.flatten()\nb = b.flatten()","6caac40b":"fig = plt.figure()\nax = Axes3D(fig)\nax.scatter(r, g, b)\nplt.show()","df4e4ae9":"vectorized = img.reshape((-1,3))\nvectorized = np.float32(vectorized)\nprint(vectorized.shape)\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)","b1b3db3f":"K=3\nattempts=10\nret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_RANDOM_CENTERS)\nlabel = label.flatten()","768c00f4":"center = np.uint8(center)\nres = center[label.flatten()]\nresult_image = res.reshape((img.shape))","9ff5a0e0":"plt.imshow(result_image)\nplt.show()","4e56f5c5":"figure_size = 10\nplt.figure(figsize=(figure_size,figure_size))\nplt.subplot(1, 2, 1),plt.imshow(img)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(1, 2, 2),plt.imshow(result_image)\nplt.title('Segmented Image when K = %i' % K), plt.xticks([]), plt.yticks([])\nplt.show()","b7f34a73":"K=4\nattempts=10\nret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_RANDOM_CENTERS)\nlabel = label.flatten()\n## Reshaping 2D array to 3D array\ncenter = np.uint8(center)\nres = center[label.flatten()]\nresult_image = res.reshape((img.shape))","bbeb48c2":"figure_size = 10\nplt.figure(figsize=(figure_size,figure_size))\nplt.subplot(1, 2, 1),plt.imshow(img)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(1, 2, 2),plt.imshow(result_image)\nplt.title('Segmented Image when K = %i' % K), plt.xticks([]), plt.yticks([])\nplt.show()","e9c4efc6":"K=5\nattempts=10\nret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_RANDOM_CENTERS)\nlabel = label.flatten()\n    \n## Reshaping 2D array to 3D array\n\ncenter = np.uint8(center)\nres = center[label.flatten()]\nresult_image = res.reshape((img.shape))","8541d349":"figure_size = 10\nplt.figure(figsize=(figure_size,figure_size))\nplt.subplot(1, 2, 1),plt.imshow(img)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(1, 2, 2),plt.imshow(result_image)\nplt.title('Segmented Image when K = %i' % K), plt.xticks([]), plt.yticks([])\nplt.show()","6bbc8eb8":"edges = cv2.Canny(img,150,200)\nplt.figure(figsize=(figure_size,figure_size))\nplt.subplot(1,2,1),plt.imshow(img)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(1,2,2),plt.imshow(edges,cmap = 'gray')\nplt.title('Edge Image'), plt.xticks([]), plt.yticks([])\nplt.show()","fb0d2738":"pic = np.float64(img\/256)\nnsamples, nx, ny = pic.shape\npic1 = pic.reshape((nsamples,nx*ny))\nkmeans = KMeans(n_clusters=20, random_state=0).fit(pic1)\npic2show = kmeans.cluster_centers_[kmeans.labels_]\nplt.imshow(pic2show)","a2fa0afb":"image = cv2.imread(filepath)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = image.reshape((image.shape[0] * image.shape[1], 3))","6096aaf4":"# cluster the pixel intensities\nclt = KMeans(n_clusters = 5)\nclt.fit(image)","57689ae6":"def centroid_histogram(clt):\n\t# grab the number of different clusters and create a histogram\n\t# based on the number of pixels assigned to each cluster\n\tnumLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n\t(hist, _) = np.histogram(clt.labels_, bins = numLabels)\n\t# normalize the histogram, such that it sums to one\n\thist = hist.astype(\"float\")\n\thist \/= hist.sum()\n\t# return the histogram\n\treturn hist","d56ea09a":"def plot_colors(hist, centroids):\n\t# initialize the bar chart representing the relative frequency\n\t# of each of the colors\n\tbar = np.zeros((50, 300, 3), dtype = \"uint8\")\n\tstartX = 0\n\t# loop over the percentage of each cluster and the color of\n\t# each cluster\n\tfor (percent, color) in zip(hist, centroids):\n\t\t# plot the relative percentage of each cluster\n\t\tendX = startX + (percent * 300)\n\t\tcv2.rectangle(bar, (int(startX), 0), (int(endX), 50),\n\t\t\tcolor.astype(\"uint8\").tolist(), -1)\n\t\tstartX = endX\n\t\n\t# return the bar chart\n\treturn bar","7bcc7777":"# build a histogram of clusters and then create a figure\n# representing the number of pixels labeled to each color\nhist = centroid_histogram(clt)\nbar = plot_colors(hist, clt.cluster_centers_)\n# show our color bart\nplt.figure()\nplt.axis(\"off\")\nplt.imshow(bar)\nplt.show()","838f383d":"## Applying Kmeans on a 256*256 greyscale image","c8371544":"**Functions to grab dominating colors**\n\nfor K = 5","b981f6f7":"# Image Segementation with Kmeans","9e856436":"***","49dd7a50":"***","3160e1c7":"### Introducing K-means ","1a9c8c69":"![](https:\/\/miro.medium.com\/max\/1104\/1*riInbzp5CiuMOOq8rldQ7w.png)","a4c4163e":"Plotting graph","658536ce":"**With K values as 4**","349d52a8":"**Reshaping np array in 2D to perform kmeans**","cbae74bd":"***","8e2184be":"**Using kmeans function from sklearn.cluster**","2da70d8a":"**Reshaping 2D array to 3D array**","9a8382ef":"**Comparing Original and Segmented image**","ee72a457":"**Keeping K value 3**","37ae87b5":"**FIlepath to images**","435d662c":"***","c8efcb50":"## <u>Algorithm :<\/u>\n\n\u039a-means clustering algorithm inputs are the number of clusters \u039a and the data set. Algorithm starts with initial estimates for the \u039a centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n\n1. <u>Data assigment step:<\/u>\n\nEach centroid defines one of the clusters. In this step, each data point based on the squared Euclidean distance is assigned to its nearest centroid. If \ud835\udc50\ud835\udc56\n\nis the collection of centroids in set C, then each data point x is assigned to a cluster based on\nmin\ud835\udc50\ud835\udc56\u2208\ud835\udc36\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc50\ud835\udc56,\ud835\udc65)2\n\nwhere dist( \u00b7 ) is the standard (L2) Euclidean distance.\n\n2. <u>Centroid update step:<\/u>\n\nCentroids are recomputed by taking the mean of all data points assigned to that centroid's cluster.\n\nThe algorithm iterates between step one and two until a stopping criteria is met (no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n\nThis algorithm may converge on a local optimum. Assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n\n### <u>Choosing K<\/u>\n\nIf the true label is not known in advance, then K-Means clustering can be evaluated using Elbow Criterion , Silhouette Coefficient , cross-validation, information criteria, the information theoretic jump method, and the G-means algorithm.\n***","c9b02d23":" ## Color to represent Cluster","ebb5bd66":"**Importing important libraries** ","7d9ddf86":"***","f23ea692":"# Displaying Image:","f183229e":"***","9c9589e7":"\n\nThe k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\n\n    The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n    Each point is closer to its own cluster center than to other cluster centers.\n\nThose two assumptions are the basis of the k-means model. We will soon dive into exactly how the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the k-means result.\n\nFirst, let's generate a two-dimensional dataset.\n\n***","4b9e1411":"This section will find the dominating Color from the segmented Image","3a8919ab":"**Converting 3D image numpy array to 2D **\n\nour image array is converted in 2d so Kmeans can be performed anad its done with np.reshape.","e6fd4634":"The good news is that the k-means algorithm assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points\u2014an exhaustive search would be very, very costly. Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to k-means involves an intuitive iterative approach known as expectation\u2013maximization.","06ff1ccc":"***","77fefd0e":"**With K values as 5**","0babb737":"**Identifying Edges in Image**","e6968377":"**Plotting Resultant Image **","6ecaf5b3":"### The objective of K-Means clustering is to minimize the sum of squared distances between all points and the cluster center."}}