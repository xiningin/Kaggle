{"cell_type":{"fd94fbb0":"code","5452ccd0":"code","0a1d83fa":"code","fe08b9e3":"code","6ce385ff":"code","bc04d8a8":"code","c99654c4":"code","66d18677":"code","d978dde9":"code","1b8819ee":"code","71c1e3a0":"code","9c8ccb3c":"code","6791760d":"code","a27e98fb":"code","f097d72a":"code","41a39587":"code","315c583f":"code","417533aa":"code","014c8e64":"code","570a5915":"code","69ab14df":"code","2a3e358d":"code","5165aa44":"code","05c622fe":"code","54a9f59b":"code","4da68d1f":"code","2f7bc857":"code","9038d699":"code","4b265904":"code","d9e86ff3":"code","e4eaae0a":"code","075b4b56":"markdown","952a20d7":"markdown","2f11d9f4":"markdown","dd9a9dd0":"markdown","d95e5f2c":"markdown","a8bd4f9b":"markdown","88d25749":"markdown","96da3002":"markdown","3ae79b58":"markdown","4608a9a3":"markdown","9ccbefc0":"markdown","a09644c5":"markdown","400536a5":"markdown","322c996f":"markdown","1af1f38d":"markdown","a8935aa5":"markdown","22f5facb":"markdown","a8ec3104":"markdown","fdecfbce":"markdown","01473b15":"markdown","62196c81":"markdown","5846887b":"markdown","eed6769b":"markdown","e59e070c":"markdown","6424e3f5":"markdown","9f03e4d4":"markdown","8d9ce2bc":"markdown","5888ace0":"markdown"},"source":{"fd94fbb0":"# Downloading the necesary libraries\n!pip install proplot\nimport proplot","5452ccd0":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\ndf = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf = df.iloc[:, :-2]  # Suggested by the author to remove last 2 columns\ndf.head()","0a1d83fa":"# Save and remove id from data\nclient_num = df['CLIENTNUM']\ndel df['CLIENTNUM']\n\n# Data shape\nprint('Data shape:', df.shape)\n\n# Do we have duplicates?\nprint('Number of Duplicates:', len(df[df.duplicated()]))\n\n# Do we have missing values?\nprint('Number of Missing Values:', df.isnull().sum().sum())","fe08b9e3":"print('Count of Features per Data Type:')\ndf.dtypes.value_counts()","6ce385ff":"# Defining plot design\ndef plot_design():\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.yticks(fontsize=13, color='black')\n    plt.box(False)\n    plt.title(i[1], fontsize=20, color='black')\n    plt.tight_layout(pad=5.0)\n    plt.grid(b=None)","bc04d8a8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\n# Select categorical variables\ncateg = df.select_dtypes(include=object).columns\n\n# Visualize\nfig, ax = plt.subplots(figsize =(20, 20))\nfig.patch.set_facecolor('white')\nmpl.rcParams['font.family'] = 'Hiragino Kaku Gothic Pro'\nmpl.rcParams['font.size'] = 12\n\ncolors = sns.color_palette(\"crest_r\", n_colors=7).as_hex()\n\n# Loop columns\nfor i in (enumerate(categ)):\n    plt.subplot(4, 2, i[0]+1)\n    \n    if df[i[1]].value_counts().count() > 2:\n        ax = sns.countplot(y = i[1], data = df, order=df[i[1]].value_counts().index, palette=colors)\n        pct = df[i[1]].value_counts(ascending=False, normalize=True).values * 100\n        ax.bar_label(container=ax.containers[0], labels=list(map('{:.2f}%'.format,pct)), padding=3, size=12, color='black')\n        ax.grid(False)\n        ax.grid(b=None)\n        ax.xaxis.set_ticks_position('none')\n        ax.yaxis.set_ticks_position('none')\n        ax.set_xticklabels('')\n        plot_design()\n\n    else:\n        _, texts, pcts = plt.pie(\n            df[i[1]].value_counts(), \n            labels=df[i[1]].value_counts().index, \n            colors= ['#00538F', '#6AADCC'],\n            autopct='%1.1f%%', \n            wedgeprops={'linewidth': 3.0, 'edgecolor': 'white'},\n            startangle=50)\n        for pcts in pcts:\n          pcts.set_color('white')\n        plt.title(i[1], fontsize=20, color='black')","c99654c4":"# Select numerical variables\nnumeric = df.select_dtypes(exclude=object).columns\n\n# Visualize\nfig, ax = plt.subplots(figsize =(20, 35))\nfig.patch.set_facecolor('white')\nmpl.rcParams['font.family'] = 'Hiragino Kaku Gothic Pro'\nmpl.rcParams['font.size'] = 12\n\ncolors = sns.color_palette(\"dark\", n_colors=14).as_hex()\n\n# Loop columns\nfor i in (enumerate(numeric)):\n    plt.subplot(7, 2, i[0]+1)\n    sns.kdeplot(x = i[1], data = df, color=colors[i[0]], fill=True)\n    plt.grid(b=None)\n    plot_design()","66d18677":"skew_limit = 0.5\nskew_vals = df[numeric].skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {0}'.format(skew_limit)))\n\nskew_cols","d978dde9":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Normalize skewed features\nfor col in skew_cols.index:\n    df[col] = boxcox1p(df[col], boxcox_normmax(df[col] + 1))","1b8819ee":"# Scaling features\nfrom sklearn.preprocessing import MinMaxScaler\n\nfor col in df[numeric]:\n    df[col] = MinMaxScaler().fit_transform(df[[col]])","71c1e3a0":"# One hot encoding\ndf['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})\ndf['Gender'] = df['Gender'].map({'M': 0, 'F': 1})\n\n# Select categorical variables\ncateg = df.select_dtypes(include=object).columns\n\n# Encoding with get dummies\ndf = pd.get_dummies(df, columns=categ)\n\n# Drop columns to avoid multicollinearity\ndf= df[df.columns.drop(list(df.filter(regex='Unknown')))]\ndf= df[df.columns.drop(list(df.filter(regex='Platinum')))]\n\ndf.head()","9c8ccb3c":"# Split target & features\nX = df.drop('Attrition_Flag', axis=1)\ny = df['Attrition_Flag']","6791760d":"# Dealing with imbalanced dataset\nfrom imblearn.over_sampling import SMOTE\n\n# Upsampling with SMOTE algorithm\nsmote = SMOTE(random_state=42)\n\nX_smote, y_smote = smote.fit_resample(X, y)\n\nprint(f'''Shape of X before SMOTE: {X.shape}\nShape of X after SMOTE: {X_smote.shape}''')\n\nprint('\\nBalance of positive and negative classes (%):')\ny_smote.value_counts(normalize=True) * 100","a27e98fb":"#Using elbow-plot variance\/dimensions\nfrom sklearn.decomposition import PCA\nimport matplotlib.ticker as mtick\n\npca = PCA()\npca.fit(X_smote)\n\ncumsum = np.cumsum(pca.explained_variance_ratio_)*100\nd = [n for n in range(len(cumsum))]\n\n# Visualize\nfig, ax = plt.subplots(figsize =(10, 10))\nfig.patch.set_facecolor('white')\nmpl.rcParams['font.family'] = 'Ubuntu'\nmpl.rcParams['font.size'] = 14\n\nax.plot(d,cumsum, color = '#00538F', label='Cumulative Explained Variance')\n\nax.axhline(y = 95, color='black', linestyle=':', label = '95% Explained Variance')\nax.legend(loc='best')\n\n# Remove ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Remove axes splines\nfor i in ['top','right']:\n    ax.spines[i].set_visible(False)\n\n# Set percentages\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n\n# annotation arrow\narrowprops = dict(arrowstyle=\"->\", connectionstyle=\"angle3,angleA=0,angleB=-90\")\nplt.annotate('Principal Component Number 19', \n             xy=(19, 95), \n             xytext=(19+5, 95+10), \n             arrowprops=arrowprops,\n             size = 14)\n\nplt.legend(bbox_to_anchor = (1, 0.2))\n\nplt.suptitle('Explained Variance vs Dimensions', size=26)\nplt.ylabel('Cumulative Explained Variance')\nplt.xlabel('PC');","f097d72a":"pca = PCA(.95) \npca.fit(X_smote)\n\nX_pca = pca.transform(X_smote)\nX_pca = pd.DataFrame(X_pca)\n\nprint(f'''Shape of X before PCA: {X_smote.shape}\nShape of X after PCA: {X_pca.shape}''')","41a39587":"# Merge y and X\ndata = pd.concat([y_smote, X_pca], axis=1)","315c583f":"# K-means \nfrom sklearn.cluster import KMeans\n\nkm_list = list()\n\nfor i in range(1,21):\n    km = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n    km = km.fit(X_pca)\n    \n    km_list.append(pd.Series({'clusters': i, \n                              'inertia': km.inertia_,\n                              'model': km}))","417533aa":"k = pd.concat(km_list, axis=1).T[['clusters','inertia']]\n\n# Visualize\nfig, ax = plt.subplots(figsize =(12, 8))\nfig.patch.set_facecolor('white')\nmpl.rcParams['font.family'] = 'Ubuntu'\nmpl.rcParams['font.size'] = 14\n\nplt.plot(k['clusters'], k['inertia'], 'bo-', color = '#00538F')\n\n# Remove ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Remove axes splines\nfor i in ['top','right']:\n    ax.spines[i].set_visible(False)\n\nax.set_xticks(range(0,21,2))\nax.set(xlabel='Cluster', ylabel='Inertia');\n\nplt.suptitle('The Elbow Method: Optimal Number of Clusters', size=26);\n","014c8e64":"# K-Means\nkm = KMeans(n_clusters=2, random_state=42)\nkm = km.fit(X_pca)\n\ndata_kmeans = data.copy()\n\ndata_kmeans['kmeans'] = km.predict(X_pca)","570a5915":"# Agglomerative\nfrom sklearn.cluster import AgglomerativeClustering\n\nag = AgglomerativeClustering(n_clusters=2, linkage='complete', compute_full_tree=True)\nag = ag.fit(X_pca)\n\ndata_agglom = data.copy()\n\ndata_agglom['agglom'] = ag.fit_predict(X_pca)","69ab14df":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y_smote, test_size=0.3, random_state=42)","2a3e358d":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV\n    \n# The parameters to be fit\nparam_grid = {\n    'learning_rate': [0.1, 0.125, 0.5],\n    'n_estimators':[300, 400, 500], \n    'max_depth':[7, 9, 11]       \n     }\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                      param_grid=param_grid, \n                      scoring='f1',\n                      cv = 3,\n                      verbose=0, \n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)\nprint(\"best score: \", GV_GBC.best_score_)\nprint(\"best param: \", GV_GBC.best_params_)\n","5165aa44":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","05c622fe":"from sklearn.model_selection import train_test_split\n\n# Split target & features\nX_kmeans = data_kmeans.drop('Attrition_Flag', axis=1)\ny_kmeans = data_kmeans['Attrition_Flag']\n\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X_kmeans, y_kmeans, test_size=0.3, random_state=42)","54a9f59b":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","4da68d1f":"# Try prediction with agglom\nfrom sklearn.model_selection import train_test_split\n\n# Split target & features\nX_agglom = data_agglom.drop('Attrition_Flag', axis=1)\ny_agglom = data_agglom['Attrition_Flag']\n\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X_agglom, y_agglom, test_size=0.3, random_state=42)","2f7bc857":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","9038d699":"# Try prediction with kmeans = 0\nfrom sklearn.model_selection import train_test_split\n\n# Cluster 0\ndata_kmeans0 = data_kmeans.loc[data_kmeans['kmeans'] == 0]\n\n# Split target & features\nX_kmeans = data_kmeans0.drop('Attrition_Flag', axis=1)\ny_kmeans = data_kmeans0['Attrition_Flag']\n\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X_kmeans, y_kmeans, test_size=0.3, random_state=42)","4b265904":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","d9e86ff3":"# Try prediction with kmeans = 1\nfrom sklearn.model_selection import train_test_split\n\n# Cluster 1\ndata_kmeans1 = data_kmeans.loc[data_kmeans['kmeans'] == 1]\n\n# Split target & features\nX_kmeans = data_kmeans1.drop('Attrition_Flag', axis=1)\ny_kmeans = data_kmeans1['Attrition_Flag']\n\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X_kmeans, y_kmeans, test_size=0.3, random_state=42)","e4eaae0a":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","075b4b56":"<a id=\"index3.2\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Scaling Features <\/center>\n","952a20d7":"<a id=\"index6.4\"><\/a>\n\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Experimentation<\/center><br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">In this section, I'll perform the prediction in separate datasets, where one of them contains all the observations within the first cluster and the other one in the second cluster.<br>\n    I took the idea of this experimentation from<a href=\"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html\" target=\"_blank\"> this article<\/a>.<\/center>","2f11d9f4":"<a id=\"index3.1\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Skew Features <\/center>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> If you want to know more about the problem of skew data you can check this article <a href=\"https:\/\/towardsdatascience.com\/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37\" target=\"_blank\"> here<\/a>.\n <\/center>","dd9a9dd0":"<a id=\"index6\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Modelling with Gradient Boosting<\/center>\n","d95e5f2c":"<a id=\"index3\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Data Preprocessing <\/center>\n","a8bd4f9b":"<a id=\"index4\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Dimensionality Reduction <\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Next, using the <b>Principal Component Analysis (PCA)<\/b>, we will reduce the dimensionality of the dataset as \"small\" as possible without losing any information.\n\n <\/center>","88d25749":"<a id=\"index6.2\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> K-Means + Model <\/center>","96da3002":"<a id=\"index1.1\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Importing the Data <\/center>","3ae79b58":"<a id=\"index3.3\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Encoding Categorical Features <\/center>\n","4608a9a3":"<a id=\"index5.2\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Hierarchical Clustering <\/center>","9ccbefc0":"<br><br>\n<center style=\"font-size:200%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Thanks a lot for your attention! If you liked this notebook, please let me know in the comments and upvote it if you found it helpful. Cheers!<\/center>\n\n","a09644c5":"<a id=\"index1\"><\/a>\n\n# <center style=\"font-size:200%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Introduction <\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> A manager at a bank is disturbed by the increasing number of customers leaving their credit card services.<br><br>\n    This notebook will predict who is most likely to get churned among the bank's customers; that way, the bank will act accordingly to solve the actual problem.<br><br>\n    While getting the most accurate prediction possible, this notebook will explain some unsupervised learning models (dimensional reduction and clustering) and demonstrate how they can improve the supervised learning problem.<br><br>\n    If you, the reader, want to know more about the dataset used for this report, you can click <a href=\"https:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers\" target=\"_blank\"> here<\/a>.\n <\/center>","400536a5":"<a id=\"index6.1\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Base Model (Not Using any Clustering) <\/center>","322c996f":"<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"><b>This is the best prediction so far!<\/b><\/center>","1af1f38d":"### <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Cluster = 1<\/center><br>","a8935aa5":"<a id=\"index5.1\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> K-Means <\/center>","22f5facb":"<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> In our case, we'll use the <b>Box-Cox transformation<\/b> to transform all the skew features into a normal distribution.<\/center>\n","a8ec3104":"<a id=\"index2.1\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Categorical Variables <\/center>","fdecfbce":"### <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Cluster = 0<\/center><br>","01473b15":"<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> The elbow method allows us to determine the optimal number of clusters. In this case, even though it will be ideal to have 4-8 clusters, we'll keep it in two clusters to have the same number as the classes we are trying to predict: churn or not churn.\n\n <\/center>","62196c81":"<p style=\"font-size:300%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Table of Contents<\/p>\n\n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Introduction<\/p>](#index1)\n\n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Importing the Data<\/p>](#index1.1)\n    \n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">EDA<\/p>](#index2)\n\n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Categorical Variables<\/p>](#index2.1)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Numerical Variables<\/p>](#index2.2)\n    \n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Data Preprocessing<\/p>](#index3)\n\n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Skew Features<\/p>](#index3.1)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Scaling Features<\/p>](#index3.2)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Encoding Categorical Features<\/p>](#index3.3)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Data Upsampling Using SMOTE Algorithm<\/p>](#index3.4)\n    \n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Dimensionality Reduction<\/p>](#index4)\n\n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Clustering<\/p>](#index5)\n\n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">K-Means<\/p>](#index5.1)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Hierarchical Clustering<\/p>](#index5.2)\n    \n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Modelling with Gradient Boosting<\/p>](#index6)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Base Model (Not Using any Clustering)<\/p>](#index6.1)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">K-Means + Model<\/p>](#index6.2)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Hierarchical Clustering + Model<\/p>](#index6.3)\n    \n    * [<p style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Experimentation<\/p>](#index6.4)\n    \n* [<p style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">Conclusion<\/p>](#index7)\n \n ","5846887b":"<a id=\"index2.2\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Numerical Variables <\/center>","eed6769b":"<a id=\"index3.4\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Data Upsampling Using SMOTE Algorithm <\/center>","e59e070c":"<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Here is important to see that the target variable (Attrition_Flag) is <b>imbalanced<\/b>. <br><br>\n    To deal with imbalanced data, we will explore the algorithm SMOTE to upsample the data and get a class balance.\n <\/center>","6424e3f5":"<a id=\"index2\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> EDA <\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> For the EDA, we will create a loop to plot every single categorical and numerical feature. This method will save us a considerable amount of energy and time. <br><br>\n    The more we automatize tasks, the easier it will be for us. \n <\/center>","9f03e4d4":"<a id=\"index6.3\"><\/a>\n\n## <center style=\"font-size:170%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Hierarchical Clustering + Model <\/center>","8d9ce2bc":"<a id=\"index5\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Clustering <\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Once used PCA method, we will group the observation into different groups. This process is known as clustering. There are multiple clustering methods, but in our case, we'll use K-Means and Hierarchical Clustering. To learn more about the different cluster methods and their characteristics, click <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/clustering.html\" target=\"_blank\"> here<\/a>.\n\n\n <\/center>","5888ace0":"<a id=\"index7\"><\/a>\n\n\n<a id=\"index2\"><\/a>\n\n# <center style=\"font-size:150%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> Conclusion <\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\"> To conclude this notebook, we'll summarize the key takeaways that we study in this project:<\/center>\n\n<br>\n<b>\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">1\ufe0f\u20e3 It is crucial to deal with imbalanced data. SMOTE is an excellent algorithm for that purpose.<\/center>\n\n<br>\n    \n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">2\ufe0f\u20e3 PCA can help you reduce the dimensions of your dataset without losing any relevant information.<\/center>\n\n<br>\n\n<center style=\"font-size:130%; font-family:Hiragino Kaku Gothic Pro, sans-serif\">3\ufe0f\u20e3 Clustering can help you increase the accuracy of your predictive model.<\/center>\n<b>\n\n"}}