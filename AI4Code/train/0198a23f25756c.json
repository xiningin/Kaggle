{"cell_type":{"467959c4":"code","780346f6":"code","0b926ee7":"code","1961d12b":"code","dc3eefe2":"code","e9168000":"code","d5f22d53":"code","03ff066a":"code","052aa3cf":"code","59f93c2b":"code","66dd6f9d":"code","e7304073":"code","870f3b4c":"code","6243ba3d":"code","113fdd9d":"markdown","de058e44":"markdown","835c85a8":"markdown","83018df8":"markdown","6143f5fa":"markdown","2b57940a":"markdown","1e0023e7":"markdown","3b00cb0b":"markdown","3f922a52":"markdown","75e03041":"markdown"},"source":{"467959c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install xlrd\n!pip install openpyxl\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.stats.api as sms\nfrom scipy.stats import ttest_1samp, shapiro, levene, ttest_ind, mannwhitneyu, pearsonr, spearmanr, kendalltau, f_oneway, kruskal\nfrom statsmodels.stats.proportion import proportions_ztest\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","780346f6":"control_group = pd.read_excel(\"\/kaggle\/input\/ab-testing\/ab_testing.xlsx\", sheet_name =\"Control Group\")\ntest_group = pd.read_excel(\"\/kaggle\/input\/ab-testing\/ab_testing.xlsx\", sheet_name =\"Test Group\")","0b926ee7":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","1961d12b":"check_df(control_group)","dc3eefe2":"check_df(test_group)","e9168000":"control_group.drop(['Unnamed: 4','Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9','Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13'],axis='columns', inplace=True)\ntest_group.drop(['Unnamed: 4','Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9','Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13'],axis='columns', inplace=True)","d5f22d53":"control_group.head()","03ff066a":"test_group.head()","052aa3cf":"control_group[\"Purchase\"].mean()","59f93c2b":"test_group[\"Purchase\"].mean()","66dd6f9d":"# Shapiro-Wilks Test for Control Group\n\ntest_stat, pvalue = shapiro(control_group[\"Purchase\"])\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))","e7304073":"# Shapiro-Wilks Test for Test Group\ntest_stat, pvalue = shapiro(test_group[\"Purchase\"])\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))","870f3b4c":"test_stat, pvalue = levene(control_group[\"Purchase\"], test_group[\"Purchase\"])\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))","6243ba3d":"test_stat, pvalue = ttest_ind(control_group[\"Purchase\"],\n                              test_group[\"Purchase\"],\n                              equal_var=True)\n\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))","113fdd9d":"**####### Can we conclude statistically significant results? ##########**\n\n# Indepented Two Sample T-Test\nThe Independent Samples t Test compares the means of two independent groups in order to determine whether there is statistical evidence that the associated population means are significantly different.\n\n**Requirement**\n\nNormal distribution:\nNon-normal population distributions, especially those that are thick-tailed or heavily skewed, considerably reduce the power of the test\n\nHomogeneity of variances:\nWhen this assumption is violated and the sample sizes for each group differ, the p value is not trustworthy.\n\n**Hypotheses**\n\nThe null hypothesis (H0) and alternative hypothesis (H1) of the Independent Samples t Test can be expressed in two different but equivalent ways:\n\n* H0: \u00b51 = \u00b52 (the two population means are equal)\n* H1: \u00b51 \u2260 \u00b52 (the two population means are not equal)\n\n**The Shapiro-Wilks Test for Normality**\n\nH0: There is no statistically significant difference between sample distribution and theoretical normal distribution\nH1: There is statistically significant difference between sample distribution and theoretical normal distribution\n\nThe test rejects the hypothesis of normality when the p-value is less than or equal to 0.05. Failing the normality test allows you to state with 95% confidence the data does not fit the normal distribution.\n\n* p-value < 0.05 (H0 rejected)\n* p-value > 0.05 (H0 not rejected)","de058e44":"# Let's Start!\n\n**########################### TASK 1 ############################**\n\nDefine the hypothesis of the A\/B test.\n\n* H0: M1 = M2 \/ There is no statistically significant difference between the Test and Control groups.\n* H1: M1!= M2 \/ There is a statistically significant difference between the Test and Control groups.\n\n\n**########################### TASK 2 ############################**\n\nPerform the hypothesis test\u2014comment on whether the results are statistically significant.\n\n* Maximum bidding campaign: Control Group (available)\n* Average bidding campaign: Test Group (new product)","835c85a8":"* p-value: 0.1083\n* p-value greater then 0.05 so H0 is not rejected.\n\nThe compared groups have equal variance. The assumptions of normality distribution and variance homogeneity were tested.Two assumptions are provided, we can now test for our main hypothesis.\n\n* H0: There is no statistically significant difference between the Control group that was served \u201cmaximum bidding\u201d campaign and Test group that was served \u201caverage bidding\u201d campaign.\n* H1: There is statistically significant difference between the Control group that was served \u201cmaximum bidding\u201d campaign and Test group that was served \u201caverage bidding\u201d campaign.","83018df8":"# What Is A\/B Testing?\n\nA\/B testing (also known as split testing or bucket testing) is a method of comparing two versions of a webpage or app against each other to determine which one performs better. AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.\n\n![image.png](attachment:12a2430d-b634-4007-b52a-6cf91f1c33e0.png)\n\nRunning an AB test that directly compares a variation against a current experience lets you ask focused questions about changes to your website or app, and then collect data about the impact of that change.\n\nTesting takes the guesswork out of website optimization and enables data-informed decisions that shift business conversations from \"we think\" to \"we know.\" By measuring the impact that changes have on your metrics, you can ensure that every change produces positive results.\n\n# How A\/B Testing Works?\n\nIn an A\/B test, you take a webpage or app screen and modify it to create a second version of the same page. This change can be as simple as a single headline or button, or be a complete redesign of the page. Then, half of your traffic is shown the original version of the page (known as the control) and half are shown the modified version of the page (the variation). As visitors are served either the control or variation, their engagement with each experience is measured and collected in an analytics dashboard and analyzed through a statistical engine. You can then determine whether changing the experience had a positive, negative, or no effect on visitor behavior.\n\n![image.png](attachment:2d8edbea-22c3-4925-9436-8352264a63c6.png)\n\n# Why You Should A\/B Test?\n\nA\/B testing allows individuals, teams, and companies to make careful changes to their user experiences while collecting data on the results. This allows them to construct hypotheses, and to learn better why certain elements of their experiences impact user behavior. In another way, they can be proven wrong\u2014their opinion about the best experience for a given goal can be proven wrong through an A\/B test.\n\nMore than just answering a one-off question or settling a disagreement, AB testing can be used consistently to continually improve a given experience, improving a single goal like conversion rate over time.\n\nFor instance, a B2B technology company may want to improve their sales lead quality and volume from campaign landing pages. In order to achieve that goal, the team would try A\/B testing changes to the headline, visual imagery, form fields, call to action, and overall layout of the page.\n\nTesting one change at a time helps them pinpoint which changes had an effect on their visitors\u2019 behavior, and which ones did not. Over time, they can combine the effect of multiple winning changes from experiments to demonstrate the measurable improvement of the new experience over the old one.\n","6143f5fa":"* p-value: 0.3493\n* P-value greater then 0.05 so H0 is not rejected. \n\nSo, There is no statistically significant difference between the Control group that was served \u201cmaximum bidding\u201d campaign and Test group that was served \u201caverage bidding\u201d campaign.\n\n\n**Which statistical test did we use, and why?**\n\nWe used independent t-test because we want to determine if there is a significant difference between the means of two indepented groups, which may be related in certain features.\n\n**What would be our recommendation to client?**\nThere is no statistically significant difference between the Control group that was served \u201cmaximum bidding\u201d campaign and Test group that was served \u201caverage bidding\u201d campaign. For this reason, we can recommend continuing with the maximum bidding campaign currently used.\n\n\n**Conclusion**\n\n* Hypothesis established and interpreted\n* The data was analyzed, outliers were observed\n* It was checked whether the assumptions were met for the statistical test to be applied\n* The assumptions were observed and tested\n* Commented based on -p-value\n* Suggestion offered to customer\n","2b57940a":"**Levene\u2019s Test for Homogeneity of variance**\n\nLevene\u2019s test is an equal variance test. It can be used to check if our data sets fulfill the homogeneity of variance assumption before we perform the t-test or Analysis of Variance\n\n* H0: the compared groups have equal variance.\n* H1: the compared groups do not have equal variance.","1e0023e7":"When we look at the purchase average of the two groups, we can observe that the test group, the average bidding campaign, is better. But this observation is not a statistically significant results.","3b00cb0b":"* p-value: 0.5891\n* p-value greater then 0.05 so H0 is not rejected","3f922a52":"# Dataset and Story\n\nIn this dataset, which includes the website information of XXXXXXXXXXXX.com, there is information such as the number of advertisements that users see and click and earnings information from here.\n\nThere are two separate data sets, Control and Test group. These datasets are in separate sheets of the ab_testing.xlsx excel.\n\n**Variables:**\n- Impression \u2013 Ad views count\n- Click \u2013 Indicates the number of clicks\/clicks on the displayed ad.\n- Purchase \u2013 Indicates the number of products purchased after the purchase \/ clicked ads.\n- Earning \u2013 Earnings \/ Earnings after purchased items","75e03041":"* p-value: 0.1541\n* p-value greater then 0.05 so H0 is not rejected\n\nThere is no statistically significant difference between sample distribution and theoretical normal distribution in groups Control and Test Group."}}