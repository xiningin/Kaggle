{"cell_type":{"5ca05f16":"code","e3525364":"code","d822b020":"code","cd7c0a8b":"code","ce5bc6c9":"code","d9c10bef":"code","f91850d6":"code","b0a62567":"code","62b17e03":"code","166737cc":"markdown","11858c7f":"markdown","b960bf9c":"markdown","a19b50be":"markdown","c0e0ae38":"markdown","4f22314c":"markdown","7d2d645f":"markdown","2edd3aeb":"markdown","ccf4f91b":"markdown"},"source":{"5ca05f16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n!pip install google_trans_new\nfrom google_trans_new import google_translator \ntranslator = google_translator()  \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3525364":"df= pd.read_csv(\"..\/input\/herbal-product-sales-ecommerce-20122013\/BitkiselUrunSatisiVerileri.csv\")\ndf.head()","d822b020":"df.info()","cd7c0a8b":"#correction of space chars in feature names.\ndf.columns=[(each.split()[0]+\"_\"+each.split()[1]) if len(each.split(\" \"))>1 else each for each in df.columns]\n\n#correction of MontH to lowercase.\ndf.columns=df.columns.str.lower()\n\n#drop id field. We do not need.\ndf.drop('id',  axis='columns', inplace=True)\nprint(df.columns)\n\n#sort by date ascending\ndf.sort_values(\"date\", axis=0, ascending=True, inplace=True)\n\n#set index for sorted value \ndf=df.reset_index(drop=True)\ndf.head()","ce5bc6c9":"# Two function for translate data and replacing translated data.\ndef translateToEnglish(word):\n    #translate from Turkish to English\n    translate_text = translator.translate(word, lang_src='tr', lang_tgt='en')  \n    return translate_text\n\ndef replaceColumnValues(dictionary,categorical_features):\n    \n    #lowercase all of the value of dictionary\n    dictionary={k:v.lower()\n        for k, v in\n            dictionary.items()\n                }\n    \n    #replace translated values\n    for each in categorical_features:\n        df[each].replace(dictionary, inplace=True)\n        \n    # lets correct some translate issue   \n    df[\"sex\"].replace({\"lady \":\"female\"}, inplace=True)\n    df[\"product_category\"].replace({\"hairdy \":\"hair care\"}, inplace=True)\n    df[\"product_category\"].replace({\"weakening \":\"obesity\"}, inplace=True)\n    df[\"product_category\"].replace({\"maintenance \":\"personal care\"}, inplace=True)\n    df[\"month\"].replace({\"engagement \":\"april\"}, inplace=True)\n    df[\"month\"].replace({\"hazy \":\"may\"}, inplace=True)\n    \n","d9c10bef":"dictionary={}\ncategories = {}\n#fetch only appropriate features\/columns of dataset\ncolumns=df.columns\ncategorical_features = [ each for each in columns if each not in [\"id\",\"date\",\"city\"] ]\n\n\nfor each in categorical_features:\n    categories[each] = df[each].unique()\n    print({each: categories[each]} )\n\n\nfor key,value in categories.items():\n    for index,value in enumerate(value):\n        #{Turkish keys, English values}\n        dictionary[value]=translateToEnglish(value)\n\nreplaceColumnValues(dictionary,categorical_features)\n\n  ","f91850d6":"\nprint({\"sex\": df[\"sex\"].unique()})\nprint({\"product_category\":df[\"product_category\"].unique()})\nprint({\"month\":df[\"month\"].unique()})\nprint({\"season\":df[\"season\"].unique()})\nprint({\"region\":df[\"region\"].unique()})\ndf.head()","b0a62567":"def bar_subplots(categorical_feature):\n    \"\"\"\n        input: categorical feature like \"sex\"\n        output: bar plot\n    \"\"\"\n    # get feature\n    var = df[categorical_feature]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    explode = (0.25,)* len(df[categorical_feature].unique())\n                       \n    print(varValue.index)\n    # visualize\n    plt.figure(figsize = (6,6))\n    varValue.plot(kind=\"pie\",explode=explode ,startangle=40,autopct='%1.1f%%',shadow=True)#,varValue.index, varValue)\n    #plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Sales Amount\")\n    plt.title(categorical_feature)\n    plt.show()\n   ","62b17e03":"from sklearn.compose import make_column_selector as selector\ncolumn_selector = selector(dtype_include=object)\ncategorical_features = column_selector(df)\nprint(\"all categorical features: \" ,categorical_features)\n\n#select suitable categorical features\ncategorical_features = [ each for each in categorical_features if each not in [\"id\",\"date\",\"city\"] ]\nprint(\"suitable categorical features: \",categorical_features)\n\n#1 longer method\n#for categorical_feature in categorical_features:\n#    bar_subplots(categorical_feature)\n\n#2 long method\n#y=map(lambda x:bar_subplots(x),categorical_features)\n#print(list(y))\n\n#3 short method\n#y=map(bar_subplots,categorical_features)\n#print(list(y))\n\n#4 shorter method\nlist(map(bar_subplots,categorical_features))","166737cc":"> **Univariate Variable Analysis**","11858c7f":"**Variable Description**\n1. id: Unique id number for purchase order of each customer.\n2. date: Date of purchase order.\n3. sex: male or female in Turkish.\n4. city: Turkey's cities.\n5. product category: Category of the purchased product.\n6. MontH: month of the year.\n7. season: Winter, summer and others.\n8. region: 7 region of Turkey.\n","b960bf9c":"> **Translate all data to English via Google Translate library of Python**","a19b50be":"# **INTRODUCTION**","c0e0ae38":"The dataset has over 10k rows, consisting sales order demand of a e-commerce company in Turkey. All order demands have arrived to its customers.\nLets hope customers do not need this orders again :)","4f22314c":"> **Load and Check Data**","7d2d645f":"> **Categorical Variabes**","2edd3aeb":"* Categorical Variables : sex,city,product category, season, region, MontH\n* Numerical Variables: id\n* Time : date","ccf4f91b":"> **Preprocessing Data**"}}