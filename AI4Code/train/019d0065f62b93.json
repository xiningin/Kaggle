{"cell_type":{"be2637f6":"code","a0c97f8b":"code","83eb7ccb":"code","89aa715d":"code","adcd28ed":"code","c00b2765":"code","e5c608da":"code","116fd408":"code","97c55717":"code","5db97ce4":"code","bcd8e4f7":"code","ffba2744":"code","bf4fdc94":"code","8da87288":"code","08c1b0fd":"code","4aa9dda3":"markdown","4a7cc8e4":"markdown","a2211121":"markdown","39ea651e":"markdown"},"source":{"be2637f6":"import numpy as np \nimport pandas as pd \nimport datatable as dt\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","a0c97f8b":"df = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ndf = df.to_pandas()\nprint(df.shape)","83eb7ccb":"## split data by first n dates and last n dates\nfirst_n = 188\nlast_n = 188\n\ntemp_df = df[(df['date'] < first_n) | (df['date'] > df['date'].max() - last_n)].copy()\n\nprint(temp_df.shape)\nprint(temp_df['date'].nunique())","89aa715d":"del df","adcd28ed":"## categorise first n dates and last n dates\ntemp_df.loc[:, 'target'] = 0\ntemp_df.loc[temp_df['date'] < first_n, 'target'] = 1","c00b2765":"features = [c for c in temp_df.columns if 'feature' in c] + ['resp']\n\nY = temp_df['target'].values\nX = temp_df[features]","e5c608da":"# train test split\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X,Y,test_size=0.33, random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n# prepare data for lgb\ntrain = lgb.Dataset(X_train, label=y_train)\ntest = lgb.Dataset(X_test, label=y_test)","116fd408":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.2,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","97c55717":"## fit an auxillary model\nnum_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","5db97ce4":"# shap values to explain impact of features on model's prediction\nshap.initjs()\nshap_values = shap.TreeExplainer(clf).shap_values(X_test[1000:])","bcd8e4f7":"shap.summary_plot(shap_values, X_test)","ffba2744":"cols_to_remove = ['feature_' + str(x) for x in range(41,46)]\n\nX_train.drop(cols_to_remove, axis=1, inplace = True)\nX_test.drop(cols_to_remove, axis=1, inplace = True)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n# prepare data for lgb\ntrain = lgb.Dataset(X_train, label=y_train)\ntest = lgb.Dataset(X_test, label=y_test)","bf4fdc94":"## fit an auxillary model\nnum_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","8da87288":"# shap values to explain impact of features on model's prediction\nshap.initjs()\nshap_values = shap.TreeExplainer(clf).shap_values(X_test[1000:])","08c1b0fd":"shap.summary_plot(shap_values, X_test)","4aa9dda3":"## My thoughts\n* This is just one way to look at difference in distributions between two datasets. Other methods include statistical test for difference in mean\/variance, structural breaks...\n* This kernel only looks at two distinct dates, time component is just one of the many reasons for differences. It could be different trades instruments altogether\n* Maybe this method could be used to study feature_0 too","4a7cc8e4":"The AUC is relative high with features 40-44 as main contributors. These could be features that are non-stationary (e.g computed based on historical\/lagged data).\n\nNote that I have included 'resp' as one of the features.\n\nLet's try removing the top 10 features and see how the AUC changes","a2211121":"AUC is reduced, but is still relatively high","39ea651e":"Adversarial validation is one method to test for differences in distributions between the training and test set. The idea is to the use an auxiliary model and evaluating it's predictive power in distinguishing whether a given observation belongs to the training or test set. \n\n--> If the auxiliary model predicts with good performance, it hints that there are some features that are very different across training and test set, which allowed for such good performance.\n\nFor this competition, although the test set is not given to us, we can proxy the test sets by splitting our training set according to dates. Let us mimic the private LB by creating a 6 month gap (~ 125 days):\n1. first 188 date \n2. last 188 dates. \n\nDoing so will give us a clue of how the features\/data is different across dates and thus make some inference on how the test set (private LB) will behave. \n\nReference kernel: \n[Adversarial Rainforest](https:\/\/www.kaggle.com\/tunguz\/adversarial-rainforest)"}}