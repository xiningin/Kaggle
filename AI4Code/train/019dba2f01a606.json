{"cell_type":{"714c1767":"code","3f1ef0aa":"code","d4ef281f":"code","e6779f67":"code","5eccae96":"code","49050848":"code","e06cc85a":"code","b903ab1d":"code","6aa271cf":"code","978c1d69":"code","498f0e38":"code","14d3b14b":"code","b6f44659":"code","76442625":"code","925e30eb":"markdown","1f2cd4d3":"markdown","071bde30":"markdown","1d1fad1c":"markdown","267ca0e7":"markdown"},"source":{"714c1767":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.lines import Line2D\nfrom IPython.display import HTML\nimport os\n\nimport skimage.io as io\nfrom skimage.transform import rotate, AffineTransform, warp\nfrom skimage.util import random_noise\nfrom skimage.filters import gaussian\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torchvision.utils as vutils\nfrom torch.optim import RMSprop, Adam\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f1ef0aa":"img = io.imread('..\/input\/monet-jpg-improved\/monet_jpg_improved\/05144e306fa.jpg')\nprint(img.shape)\nio.imshow(img)","d4ef281f":"data_path = '..\/input\/monet-jpg-improved'\n#check dimensions of all images\nsizes = []\nfor directory in os.listdir(data_path):\n    for file in os.listdir(data_path+'\/\/'+directory):\n        image = io.imread(data_path+'\/\/'+directory+'\/\/'+file)\n        sizes.append(image.shape)\n\nprint('Num Images: ' + str(len(sizes)))\nuniq_sizes = list(set(sizes))\nprint('Num Shapes: ' + str(len(uniq_sizes)))\nprint(uniq_sizes)","e6779f67":"#create dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbatch_size = 64\n\ndata = datasets.ImageFolder(root=data_path,\n                           transform=transforms.Compose([\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                           ])\n                           )\n\ndataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:batch_size], padding=2, normalize=True).cpu(),(1,2,0)))","5eccae96":"#info returned by dataloader in each batch: first element is batch of images, second is (useless) batch of labels\nprint(len(real_batch))\nprint(real_batch[0].size())\nprint(real_batch[1].size())","49050848":"#adapted from: https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html\n#set bias=False in conv layers as BatchNorm2d implicitly adds a bias paramter\nclass Critic(nn.Module):\n    def __init__(self, input_channels=3, n_filt=64):\n        super(Critic, self).__init__()\n        #defualt input size is 3 x 256 x 256\n        self.network = []\n        self.network.append(nn.Sequential(\n            nn.Conv2d(input_channels, n_filt, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True)\n        ))\n        \n        #iterate, downsampling by 2 at each step but doubling the number of channels\n        #by default depth=6 (5 hidden + output) results in a final size of n_filt*(2^5) x 4 x 4 prior to the final logit calculation\n        for i in range(5):\n            if i == 0:\n                prev_filt = n_filt\n            else:\n                prev_filt = n_filt*(2**i)\n                \n            self.network.append(nn.Sequential(\n            nn.Conv2d(prev_filt, n_filt*(2**(i+1)), kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True)\n        ))\n            \n        #output layer - 1x1 output\n        self.network.append(nn.Sequential(\n            nn.Conv2d(n_filt*(2**(5)), 1, kernel_size=4, stride=1, padding=0, bias=False)\n        ))\n        \n        self.network = nn.ModuleList(self.network)\n        \n    def forward(self, x):\n        o = x\n        for index, layer in enumerate(self.network):\n            o = layer(o)\n            #print(\"D \" + str(index))\n            #print(o.size())\n        \n        return o\n    \nclass Generator(nn.Module):\n    def __init__(self, z_dim=100, n_filt=64, num_channels=3):\n        super(Generator, self).__init__()\n        \n        self.network = []\n        depth=6\n        \n        self.network.append(nn.Sequential(\n            nn.ConvTranspose2d( z_dim, n_filt*(2**(depth-1)), kernel_size=4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(n_filt*(2**(depth-1))),\n            nn.LeakyReLU(0.2, inplace=True)\n        ))\n        \n        for i in range(depth-1):\n            self.network.append(nn.Sequential(\n                nn.ConvTranspose2d( n_filt*(2**(depth-i-1)), n_filt*(2**(depth-i-2)), kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(n_filt*(2**(depth-i-2))),\n                nn.LeakyReLU(0.2, inplace=True)\n            ))\n            \n        self.network.append(nn.Sequential(\n            nn.ConvTranspose2d(n_filt, num_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        ))\n        \n        self.network = nn.ModuleList(self.network)\n        \n    def forward(self, x):\n        o = x\n        for index, layer in enumerate(self.network):\n            o = layer(o)\n            #print(\"G \" + str(index))\n            #print(o.size())\n        \n        return o\n    \n#copied directly from DCGAN tutorial above\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n        \n#https:\/\/discuss.pytorch.org\/t\/check-gradient-flow-in-network\/15063  \ndef plot_grad_flow(named_parameters):\n    '''Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing \/ exploding problems.\n    \n    Usage: Plug this function in Trainer class after loss.backwards() as \n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n    ave_grads = []\n    max_grads= []\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean())\n            max_grads.append(p.grad.abs().max())\n    plt.figure()\n    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n    plt.xlim(left=0, right=len(ave_grads))\n    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\n    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n                Line2D([0], [0], color=\"b\", lw=4),\n                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n    plt.show()","e06cc85a":"#hyper params\nz_dim = 100\nlr = 1e-4\nbetas = (0.5, 0.9)\nLAMBDA = 1 #gradient penalty coefficient\n#c = 0.01 #clamping parameter\nn_critic = 20 # critic training iterations\nS = 500 # sampling interval\n\nnetG = Generator(z_dim=z_dim, n_filt=64, num_channels=3)\nnetD = Critic(input_channels=3, n_filt=64)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\noptimizerG = Adam(netG.parameters(), lr=lr, betas=betas) #RMSprop(netG.parameters(), lr=lr)\noptimizerD = Adam(netD.parameters(), lr=lr, betas=betas) #RMSprop(netD.parameters(), lr=lr)\n\nnetG.to(device)\nnetD.to(device)\n\n#fixed noise to predict on, visualize model progress\nfixed_noise = torch.randn(64, z_dim, 1, 1, device=device)","b903ab1d":"print(netG)\nprint(netD)","6aa271cf":"# Training Loop - adapted directly from DCGAN tutorial\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nnum_epochs = 700\n\n#torch.autograd.set_detect_anomaly(True)\n\nprint(\"Starting Training Loop...\")\nprint(device)\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        #train critic\n        for p in netD.parameters():  # reset requires_grad\n            p.requires_grad = True  # they are set to False below in netG update\n            \n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        # Forward pass real batch through D\n        #add decaying noise to real input\n        real_input = real_cpu \n        output_real = netD(real_input).view(-1)\n        \n\n        # Forward pass fake batch through D\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, z_dim, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        # Classify all fake batch with D\n        #add decaying noise to fake input\n        fake_input = fake \n        output_fake = netD(fake_input.detach()).view(-1)\n        \n        \n        #calculate gradient penalty\n        epsilon = torch.rand(b_size, 1,1,1).expand(real_input.size()).to(device)\n        interpolates = (epsilon * real_input) + (1.0 - epsilon) * fake_input\n        interpolates.requires_grad_(True)\n        \n        interp_output = netD(interpolates).view(-1)\n        interp_gradients = torch.autograd.grad(outputs=interp_output, inputs=interpolates, only_inputs=True,\n                                              grad_outputs=torch.ones(interp_output.size()).to(device),\n                                              create_graph=True, retain_graph=True)[0]\n        \n        #print(\"IG\")\n        #print(len(interp_gradients))\n        #print(interp_gradients)\n        #max(0,GradNorm) regularization term is from https:\/\/openreview.net\/forum?id=B1hYRMbCW\n        slopes = (interp_gradients.norm(2, dim=1) - 1)\n        #gradient_penalty = (torch.maximum(torch.zeros(slopes.size(), device=device), slopes)** 2).mean() * LAMBDA #LP\n        gradient_penalty = ((slopes)** 2).mean() * LAMBDA #GP\n        \n        # Calculate the gradients for this batch\n        errD = -(torch.mean(output_real) - torch.mean(output_fake)) + gradient_penalty\n        errD.backward()\n        optimizerD.step()\n        \n\n        #train generator\n        if iters % n_critic == 0:\n            for p in netD.parameters():\n                p.requires_grad = False  # to avoid computation\n                \n            netG.zero_grad()\n            # Since we just updated D, perform another forward pass of all-fake batch through D\n            noise = torch.randn(b_size, z_dim, 1, 1, device=device)\n            output = netD(netG(noise)).view(-1)\n            # Calculate G's loss based on this output\n            errG = -torch.mean(output)\n            # Calculate gradients for G\n            errG.backward()\n            # Update G\n            optimizerG.step()\n\n            # Output training stats\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item()))\n\n            # Save Losses for plotting later\n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n        \n        #plot gradient flow of generator\n        if iters == 10 or iters == 50 or iters % S == 0:\n            plot_grad_flow(netG.named_parameters())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % S == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1","978c1d69":"#save models\nmodel_path = 'WGANExp1.pt'\ntorch.save({\n    'generator_state': netG.state_dict(),\n    'discriminator_state': netD.state_dict(),\n    'generator_optimizer_state': optimizerG.state_dict(),\n    'discriminator_optimizer_state': optimizerD.state_dict()\n}, model_path)\n\nfrom IPython.display import FileLink\nFileLink(model_path)","498f0e38":"plt.figure()\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","14d3b14b":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","b6f44659":"#linear interpolation between two images in latent space\ndef lerp(start, end, num_steps):\n    weights = np.linspace(0,1, num=num_steps)\n    points = [start]\n    for w in weights:\n        points.append(torch.lerp(start, end, w))\n    points.append(end)\n    \n    return points\n\n\nnum_steps = 6\nnum_pairs = 8\npoints = []\nlerp_images = []\n\nfor i in range(num_pairs):\n    a = fixed_noise[random.randint(0,63)]\n    b = fixed_noise[random.randint(0,63)]\n    #get latent points in paths\n    points.append(lerp(a,b,num_steps))\n\nfor item in points:\n    for p in item:\n        lerp_images.append( torch.squeeze( netG(torch.reshape(p, (1,p.size()[0],p.size()[1],p.size()[2])) ).detach() ) )\n\nprint(type(lerp_images[0]))  \nprint(lerp_images[0].size())\nplt.figure(figsize=(8,num_steps*num_pairs))\nplt.axis(\"off\")\nplt.title(\"Lerp Images\")\nplt.imshow(np.transpose(vutils.make_grid(lerp_images, padding=2, normalize=True).cpu(),(1,2,0)))","76442625":"#spherical interpolation between two images in latent space\n#https:\/\/github.com\/soumith\/dcgan.torch\/issues\/14\ndef slerp(val, low, high):\n    omega = np.arccos(np.clip(np.dot(low\/np.linalg.norm(low), high\/np.linalg.norm(high)), -1, 1))\n    so = np.sin(omega)\n    if so == 0:\n        return (1.0-val) * low + val * high # L'Hopital's rule\/LERP\n    return np.sin((1.0-val)*omega) \/ so * low + np.sin(val*omega) \/ so * high\n\ndef spherical_interp(start, stop, num_steps):\n    weights = np.linspace(0,1, num=num_steps)\n    points = [start]\n    for w in weights:\n        points.append( slerp(w, start, stop) )\n    points.append(stop)\n    \n    return points\n    \nslerp_images = []\npoints = []\n\nfor i in range(num_pairs):\n    a = fixed_noise[random.randint(0,63)]\n    b = fixed_noise[random.randint(0,63)]\n    points.append(spherical_interp(a.cpu().squeeze(),b.cpu().squeeze(),num_steps))\n\nfor item in points:\n    for p in item:\n        slerp_images.append( torch.squeeze( netG(torch.reshape(p, (1,p.size()[0],1,1)).to(device)).detach().cpu() ) )\n\nprint(type(slerp_images[0]))\nprint(slerp_images[0].size())\nplt.figure(figsize=(8,num_steps*num_pairs))\nplt.axis(\"off\")\nplt.title(\"Slerp Images\")\nplt.imshow(np.transpose(vutils.make_grid(slerp_images, padding=2, normalize=True).cpu(),(1,2,0)))","925e30eb":"# Notes on Training\n\n1. Default WGAN architecture, parameters, and training loop\nGenerator learns but images are of very poor quality, Critic isn't trained to optimality\n\n2. Change n_critic from 5 -> 10,\nRemove BatchNorm from Critic Architecture\nStill bad results\n\n3. Change n_critic from 10->20, add batch norm to critic\nset lr from 5e-5 to 1e-4\n\n4. Observed vanishing gradients, increase clipping paramter from 0.01 to 0.03\nThis does not seem to help training, indeed it makes the loss far too noisy\n\n5. Set clipping param back to 0.01, replace BatchNorm in critic with GroupNorm(num_groups=1) which is equivalent to LayerNorm\nReplace ReLU with LeakyReLU in Generator\n\n6. Set clipping param to .1, remove normalization in critic, lr to 5e-5\n\n7. Set clipping param to 0.01, lr to 1e-3, add batchnorm\nThis produces the shadow of realistic images, better results than before but still some volatility in the loss colors are clearly off, black and yellow mostly. Critic loss sometimes jumps up to -6 from ~0.8\n\n8. Increase lr to 1e-2\n\n9. Switch to WGAN-GP Architecture\nRemove BatchNorm\nChange loss to GP loss function\nChange from RMSProp to Adam Optimizer w\/ lr=1e-4, B1 = 0.5, B2 = 0.9 as in \"Improved Training of Wasserstein GANs\"\n\\lambda = 10, n_critic = 5\nThis produces better results which approximate a passable image but nothing that constitutes success\n\n10. Reduce learning rate lr=5e-5\nn_critic 5->10\n\n11. \nnum_epochs from 250 -> 400\nMuch more passable results here but still blurry\n\n12. n_critic 10->20\nlambda 10->1\nworse with lambda = 1\n\n13. Try again with lr=1e-4 lambda = 1, n_critic = 20\nResults are not too bad, blurry and have stairway artifacts littering the images\n\n14. Try adding back layer norm - definitely severely inhibits training\n\n15. Remove layer norm\nAdd regularization term to the loss from https:\/\/openreview.net\/forum?id=B1hYRMbCW\nLoss is now -D(x) + D(G(z)) + [(max {0, ||\u2207f(\u02c6x)|| \u2212 1}) 2 ] (WGAN-LP)\nAbout comparable, maybe slightly worse\n\n16. Go back to GP loss, num_epochs -> 700\nBest results so far\n\n17. num_epochs -> 750","1f2cd4d3":"# WGAN Implementation","071bde30":"# Exploring the Latent Space","1d1fad1c":"# Training","267ca0e7":"# Generated Results"}}