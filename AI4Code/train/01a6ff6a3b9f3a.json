{"cell_type":{"c3572e1f":"code","0576cb54":"code","e2b70299":"code","fbfc7dad":"code","6927beef":"code","9b51d82a":"code","fe416450":"code","13e44b5d":"code","516ba019":"code","8517c352":"code","4ba850eb":"code","3664e84f":"code","4042e13d":"code","58d5d1d9":"code","53506489":"code","4050d308":"code","38ab2e57":"code","ec19a420":"code","d3211da9":"code","9ac61c7a":"code","4b8f9f20":"code","fe25abed":"code","e2c96d60":"code","33eb91ec":"code","4b47d2b4":"code","f10ad2e5":"code","e7561a6b":"code","e73f2fa1":"code","31b5278d":"code","30c4195a":"code","0e147dcc":"code","96c0f519":"code","ca3a65b4":"code","f2e1cc17":"code","52fcb144":"code","e7f0752f":"code","2b05d913":"code","4970de9d":"code","caac6746":"code","3f9048b1":"code","42bdd2b6":"code","a3079713":"code","7e7e223f":"markdown","f52a4652":"markdown","e8135a2c":"markdown","438d1cb7":"markdown","b6479539":"markdown","a73fd177":"markdown","4b9b9f70":"markdown","4fc9a224":"markdown","2234b86e":"markdown","4f7f99d6":"markdown","0a1441d4":"markdown","572a6e88":"markdown","0872df59":"markdown","ab2c5a5c":"markdown","ac3ae363":"markdown","56ebd5c7":"markdown","00289141":"markdown","32dca571":"markdown","4335b8bc":"markdown","51ecbdbc":"markdown","cef54a97":"markdown","2c8ee088":"markdown","3cff37e0":"markdown","6e95ce9f":"markdown","5aa6860c":"markdown","00b074d8":"markdown"},"source":{"c3572e1f":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nfrom sklearn.metrics import confusion_matrix\n\n# Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n","0576cb54":"#read the CSV file\ndf=pd.read_csv(\"..\/input\/spam-email-classification\/email.csv\")","e2b70299":"#Print top 5 Values\ndf.head()","fbfc7dad":"df.info()","6927beef":"df['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\ndf.head()","9b51d82a":"X=df['Message']\nY=df['spam']","fe416450":"X_train, X_test, y_train, y_test = train_test_split(X,Y)","13e44b5d":"#Defineing Naive Baised\nclf_NaiveBaised= Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('nd', MultinomialNB())\n])","516ba019":"#Fiting the algorithm\nclf_NaiveBaised.fit(X_train,y_train)","8517c352":"#Make prediction on X_test\ny_pred_NB=clf_NaiveBaised.predict(X_test)","4ba850eb":"conf_mat_NB=confusion_matrix(y_test, y_pred_NB)","3664e84f":"plt.figure(figsize=(10,8))\nsns.heatmap(conf_mat_NB,annot=True,fmt='d')","4042e13d":"naive_acc=accuracy_score(y_test,y_pred_NB)\nnaive_acc","58d5d1d9":"clf_svm= Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('svc', SVC(kernel=\"rbf\",C=1000,gamma=0.001))\n])","53506489":"clf_svm.fit(X_train,y_train)","4050d308":"y_pred_SVM=clf_svm.predict(X_test)","38ab2e57":"conf_mat_SVM=confusion_matrix(y_test, y_pred_SVM)","ec19a420":"plt.figure(figsize=(10,8))\nsns.heatmap(conf_mat_SVM,annot=True,fmt='d')","d3211da9":"svm_acc=accuracy_score(y_test,y_pred_SVM)\nsvm_acc","9ac61c7a":"clf_knn= Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('knn', KNeighborsClassifier(n_neighbors=3))\n])","4b8f9f20":"clf_knn.fit(X_train,y_train)","fe25abed":"y_pred_KNN=clf_knn.predict(X_test)","e2c96d60":"conf_mat_KNN=confusion_matrix(y_test, y_pred_KNN)","33eb91ec":"plt.figure(figsize=(10,8))\nsns.heatmap(conf_mat_KNN,annot=True,fmt='d')","4b47d2b4":"knn_acc=accuracy_score(y_test,y_pred_KNN)\nknn_acc","f10ad2e5":"clf_DecisionTree= Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('dt',DecisionTreeClassifier())\n])","e7561a6b":"clf_DecisionTree.fit(X_train,y_train)","e73f2fa1":"y_pred_DT=clf_DecisionTree.predict(X_test)","31b5278d":"conf_mat_DT=confusion_matrix(y_test, y_pred_DT)","30c4195a":"plt.figure(figsize=(10,8))\nsns.heatmap(conf_mat_DT,annot=True,fmt='d')","0e147dcc":"dt_acc=accuracy_score(y_test,y_pred_DT)\ndt_acc","96c0f519":"clf_rf= Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('rf', RandomForestClassifier(n_estimators=100))\n])","ca3a65b4":"clf_rf.fit(X_train,y_train)","f2e1cc17":"y_pred_RF=clf_rf.predict(X_test)","52fcb144":"conf_mat_RF=confusion_matrix(y_test, y_pred_RF)","e7f0752f":"plt.figure(figsize=(10,8))\nsns.heatmap(conf_mat_RF,annot=True,fmt='d')","2b05d913":"rf_acc=accuracy_score(y_test,y_pred_RF)\nrf_acc","4970de9d":"menMeans = np.array([naive_acc,svm_acc,knn_acc,dt_acc,rf_acc])*100\nind = ['Naive Bayes','SVM','KNN','DT','Random Forest']\nfig, ax = plt.subplots(figsize = (20,8))\nax.bar(ind,menMeans,width=0.3,color ='red')\nfor index,data in enumerate(menMeans):\n    plt.text(x=index , y =data+1 , s=\"{:.2f}\".format(data) , fontdict=dict(fontsize=20))\nplt.tight_layout()\nplt.show()","caac6746":"#Function for testing custome email\ndef spam_dect(clf,txt):\n    a=clf.predict([txt])\n    if a==1:\n        print(\"This is a Spam email\")\n    else:\n        print(\"This is a Real email\")","3f9048b1":"#Demo email\ntest_email_1=\"Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!\" #Spam Email from my mail box\ntest_email_2=\"Hey Ashfak, can we get together to watch footbal game tomorrow?\"   #Real Email from my mail box","42bdd2b6":"#Predict with Naive Bayes\nspam_dect(clf_NaiveBaised,test_email_1)","a3079713":"#Predict with Naive Bayes\nspam_dect(clf_NaiveBaised,test_email_2)","7e7e223f":"![Thankyouforreading.gif](attachment:a96ef779-c7e6-450e-9ada-cc703f6cceaf.gif)","f52a4652":"## Plot Confusion Matrix","e8135a2c":"# k nearest neighbor classifier\n\n![HyPUqwF.png](attachment:8d64edb7-eae0-4b29-99e6-3ea107cf81eb.png)\n\n![images.jpeg](attachment:bfa9b02b-a12f-40e2-9b71-eedbaa7e2546.jpeg)\n\n<b>K-Nearest Neighbors<\/b>\n\nKNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.\n\n\n<b>How does the KNN algorithm work?<\/b>\n\nIn KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1.<br>\n![Knn_k1_z96jba.png](attachment:6fd4b885-c751-4596-97e2-05ad14c46143.png)<br>\nSuppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n\n1. Calculate distance\n2. Find closest neighbors\n3. Vote for labels <br>\n\n![KNN_final1_ibdm8a.png](attachment:2b609647-42c6-4209-a52c-e9afae728ebc.png)\n\n","438d1cb7":"# Create Classifier for Random Forest","b6479539":"# Create Classifier for Naive Baised\n","a73fd177":"## Plot Confusion Matrix","4b9b9f70":"# Support Vector Machine Classifier\n\n![HyPUqwF.png](attachment:3dc7fe6f-1982-4969-ac78-421cf666765a.png)\n\n### Support Vector Machines\nGenerally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n\n![index3_souoaz.png](attachment:4060dd34-14a4-47b6-b4eb-5b7756120359.png)\n\n## Support Vectors\nSupport vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n\n## Hyperplane\nA hyperplane is a decision plane which separates between a set of objects having different class memberships.\n\n## Margin\nA margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n\n\n## How does SVM work?\nThe main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n\n1. Generate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n\n2. Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.<br>\n![index2_ub1uzd.png](attachment:827ffa0e-00d6-42cb-bb22-c8a565e03fa4.png)\n\n## SVM Kernels\nThe SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n\n* <b>Linear Kernel<\/b>  A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n> k(x,xi)=sum(x*xi)\n\n* <b>Radial Basis Function Kernel <\/b>The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n> K(x,xi) = exp(-gamma * sum((x \u2013 xi^2))\n\n\n## Generating Model\n\nLet's build support vector machine model. First, import the SVM module and create support vector classifier object by passing argument kernel as the linear kernel in SVC() function.\n\nThen, fit your model on train set using fit() and perform prediction on the test set using predict()\n\n> #Import svm model<br>\n> from sklearn import svm<br>\n> \n> #Create a svm Classifier<br>\n> clf = svm.SVC(kernel='linear') # Linear Kernel<br>\n> \n> #Train the model using the training sets<br>\n> clf.fit(X_train, y_train)<br>\n> \n> #Predict the response for test dataset<br>\n> y_pred = clf.predict(X_test)<br>\n\n![Graphical-presentation-of-the-support-vector-machine-classifier-with-a-non-linear-kernel_W640.jpg](attachment:fd2106f5-6fa4-46e4-8e09-1cca1b5875a9.jpg)","4fc9a224":"# Fun Fact\n## Lets test our model with custom email","2234b86e":"# Create Classifier for Support Vector Machine","4f7f99d6":"# \ud83d\udce9 Importing the Libraries\n","0a1441d4":"## Plot Confusion Matrix","572a6e88":"# \ud83d\udccc Intro Of This Notebook\n#### The aim of this study is to clacify spam email by using different machine learning algorithms. For this purpose, I will use,\n### * Multinomial Naive Bayes Classifier\n### * Support Vecrot Machine Classifier with Radial basis function kernel (RBF)\n### * k Nearest Neighbor Classifier(KNN)\n### * Decision Tree Classifier,\n### * Random Forest Classifier.\n#### I will give a short description about those algorithms.","0872df59":"## Plot Confusion Matrix","ab2c5a5c":"## Plot Confusion Matrix","ac3ae363":"# Create Classifier for KNeighborsClassifier\n","56ebd5c7":"# \ud83d\udcbb Load and Read DataSets\n","00289141":"# Catagorized the Dataframe","32dca571":"# Naive Bayes Algorithm\n\n![HyPUqwF.png](attachment:e9b19d10-e044-4d35-8295-33152f1d3245.png)\n\n![download (9).png](attachment:d07cee17-9c9f-4b0b-ba54-14d1f017ae67.png)\n<br>\n## <b>What is Naive Bayes algorithm?<\/b><br>\n### It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n## <b> What is Naive Bayes Classifier?<\/b>\n### Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.\n\n### Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his\/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n\n<center>\n<img src=\"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543836882\/image_3_ijznzs.png\">\n<\/center>\n<ul>\n<li>P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.<\/li>\n<li>P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability.<\/li>\n<li>P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability.<\/li>\n<li>P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability.<\/li>\n<\/ul>\n\n<br>\n<br>\n\n![5-Figure1-1.png](attachment:4730ebe9-0b2a-4c57-a0ec-efd3de366f63.png)\n\n\n\n","4335b8bc":"# Create Classifier for DecisionTreeClassifier\n","51ecbdbc":"# We Have finished basic concept of our algorithm\n\n![hedgehog-jumping-cartoon-happy-hurray-porcupine_125446-345.jpg](attachment:1e709e5e-5109-4c67-a820-f176192f548b.jpg)","cef54a97":"# Decision Tree Classifier\n![HyPUqwF.png](attachment:4a986081-b213-43c7-acee-cf05aeefded0.png)\n\n![maxresdefault.jpg](attachment:fb1ed5a3-2cd7-4f57-a087-78c9a3fa3fe6.jpg)\n\n\n<b>Decision Tree Algorithm<\/b>\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n\n![1_r5ikdb.jpeg](attachment:8ac5e896-41a7-4176-aa63-ea570131edda.jpeg)\n\nDecision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.\n\n\n![2_btay8n.jpeg](attachment:56fe881f-59a7-4bac-b82f-4147e2061335.jpeg)\n\n<b>Entropy<\/b>\n\n\nA decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n\n![Entropy.png](attachment:2a5ea16a-47b2-4ae8-b4c3-c27607a726f6.png)\n\n","2c8ee088":"# Split train and test data\n","3cff37e0":"# \ud83d\udcd1 About the Dataset:\n### - The used dataset is a CSV file.\n### - It contains 5573 individual emails.\n### - Each email has classified by Ham or Spam.\n","6e95ce9f":"# Compareing Accrucy of those algorithm","5aa6860c":"# \ud83d\udce5 Download Email Datasets:\n## Here is Dataset Download link: \ud83d\udc47\n### - From kaggle => \"https:\/\/www.kaggle.com\/ashfakyeafi\/spam-email-classification\"\n### - Form my Github => \"https:\/\/github.com\/AshfakYeafi\/Spam-Email-Classifier\/blob\/main\/emai.csv\"","00b074d8":"# Random Forest Classifier\n\n![HyPUqwF.png](attachment:79fece57-b42c-4000-99ec-e0985995a7ce.png)\n\n<b>What is a random forest?<\/b>\n\nA random forest is a machine learning technique that\u2019s used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems.\n\nA random forest algorithm consists of many decision trees. The \u2018forest\u2019 generated by the random forest algorithm is trained through bagging or bootstrap aggregating. Bagging is an ensemble meta-algorithm that improves the accuracy of machine learning algorithms.\n\nThe (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome.\n\nA random forest eradicates the limitations of a decision tree algorithm. It reduces the overfitting of datasets and increases precision.\n\n## Features of a Random Forest Algorithm\n* It\u2019s more accurate than the decision tree algorithm.\n* It provides an effective way of handling missing data.\n* It can produce a reasonable prediction without hyper-parameter tuning.\n* It solves the issue of overfitting in decision trees.\n* In every random forest tree, a subset of features is selected randomly at the node\u2019s splitting point.\n\n![decision-tree-nodes.png](attachment:ba1bd596-5363-4f7b-b6ed-dc6521c69cf8.png)\n\n## Classification in random forests\n\nClassification in random forests employs an ensemble methodology to attain the outcome. The training data is fed to train various decision trees. This dataset consists of observations and features that will be selected randomly during the splitting of nodes.\n\nA rain forest system relies on various decision trees. Every decision tree consists of decision nodes, leaf nodes, and a root node. The leaf node of each tree is the final output produced by that specific decision tree. The selection of the final output follows the majority-voting system. In this case, the output chosen by the majority of the decision trees becomes the final output of the rain forest system. The diagram below shows a simple random forest classifier.\n\n![random-forest-classifier.png](attachment:a4639a70-0952-4d92-8e17-d22f57ce0466.png)\n\n\n![example-of-random-forest-classifier.png](attachment:7cc7765d-2f69-4702-b3ee-8d85dfed1950.png)"}}