{"cell_type":{"bdecaf0c":"code","c952ae7c":"code","522f6922":"code","7b3b6ccb":"code","7bd22d2f":"code","b20e63d9":"code","e4a26f85":"code","76171da6":"code","221701e7":"code","cc265f08":"markdown","850ec1c7":"markdown","4fc54e96":"markdown","04cac3ab":"markdown","2c806a96":"markdown","6a745972":"markdown","849be36d":"markdown"},"source":{"bdecaf0c":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","c952ae7c":"# LOAD LIBRARIES\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","522f6922":"# INITIALIZE VARIABLES\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# BUILD 512 SEPARATE NON-LINEAR MODELS\nfor i in range(512):\n    \n    # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL WITH SUPPORT VECTOR MACHINE\n        clf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n    #if i%10==0: print(i)\n        \n# PRINT VALIDATION CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('CV score =',round(auc,5))","7b3b6ccb":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Test.csv predictions')\nplt.show()","7bd22d2f":"# LOAD LIBRARIES\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# PLOT FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    #plt.hist(train.iloc[:,i+1],bins=100)\n    sns.distplot(train.iloc[:,i+1],bins=100)\n    plt.title( train.columns[i+1] )\n    plt.xlabel('')\n    \n# PLOT GAUSSIAN FOR COMPARISON\nplt.subplot(3,3,9)\nstd = round(np.std(train.iloc[:,8]),2)\ndata = np.random.normal(0,std,len(train))\nsns.distplot(data,bins=100)\nplt.xlim((-17,17))\nplt.ylim((0,0.37))\nplt.title(\"Gaussian with m=0, std=\"+str(std))\n\nplt.subplots_adjust(hspace=0.3)\nplt.show()","b20e63d9":"# NORMALITY PLOTS FOR FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    stats.probplot(train.iloc[:,1], plot=plt)\n    plt.title( train.columns[i+1] )\n    \n# NORMALITY PLOT FOR GAUSSIAN\nplt.subplot(3,3,9)\nstats.probplot(data, plot=plt)   \nplt.title(\"Gaussian with m=0, std=\"+str(std))\n\nplt.subplots_adjust(hspace=0.4)\nplt.show()","e4a26f85":"train0 = train[ train['wheezy-copper-turtle-magic']==0 ]","76171da6":"# PLOT FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    #plt.hist(train0.iloc[:,i+1],bins=10)\n    sns.distplot(train0.iloc[:,i+1],bins=10)\n    plt.title( train.columns[i+1] )\n    plt.xlabel('')\n    \n# PLOT GAUSSIAN FOR COMPARISON\nplt.subplot(3,3,9)\nstd0 = round(np.std(train0.iloc[:,8]),2)\ndata0 = np.random.normal(0,std0,2*len(train0))\nsns.distplot(data0,bins=10)\nplt.xlim((-17,17))\nplt.ylim((0,0.1))\nplt.title(\"Gaussian with m=0, std=\"+str(std0))\n    \nplt.subplots_adjust(hspace=0.3)\nplt.show()","221701e7":"# NORMALITY PLOTS FOR FIRST 8 VARIABLES\nplt.figure(figsize=(15,15))\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    stats.probplot(train0.iloc[:,1], plot=plt)\n    plt.title( train.columns[i+1] )\n    \n# NORMALITY PLOT FOR GAUSSIAN\nplt.subplot(3,3,9)\nstats.probplot(data0, plot=plt)   \nplt.title(\"Gaussian with m=0, std=\"+str(std0))\n\nplt.subplots_adjust(hspace=0.4)\nplt.show()","cc265f08":"## Normality Plots\nNormality plots indicate that the variables are not Gaussian. If they were Gaussian, then we would see straight lines below. Instead we see piecewise straight lines indicating that we may have Gaussian mixture models. (Each variable is the sum of multiple Gaussians).","850ec1c7":"## Variables within partial datasets are Gaussian\nIf we only look at the partial datasets where `wheezy-copper-turtle-magic = k` for `0 <= k <= 511`, then the variables are Gaussian. This can be seen by the plots below. This suggests how the full dataset was made. Perhaps Kaggle made 512 different datasets and then combined them for this competition.","4fc54e96":"# Build 512 Support Vector Models\nUsing cross validation, we determined that SVC's polynomial kernel with degree=4 achieves the best CV.","04cac3ab":"# Support Vector Machine scores LB 0.925\nIn our previous kernel [here][1], we built 512 **linear** models using logistic regression in conjuction with the **magic feature** and scored LB 0.808. In this kernel we will build 512 **nonlinear** models using support vector machine with polynomial degree=4 kernel and score LB 0.926. Previously we performed feature selection with Lasso, aka LR's L1-penalty. Now we will perform feature selection with sklearn's VarianceThreshold selector (which will select more or less the same features). \n\nThe success of this kernel demonstrates the nature of \"Instant Gratification\" competition data. It appears that the data is actually 512 datasets combined together. Each dataset has rougly 512 observations. Thus the total training data has `262144 = 512 * 512` observations. Each partial dataset is identified by a unique `wheezy-copper-turtle-magic` value. This kernel shows that each dataset's target is a nonlinear function of approximately 40 important features (and each dataset uses a different 40 important features).  \n  \nThe next thing to investigate is whether there are interactions between the partial datasets that can improve prediction. If that is the case then instead of building 512 separate models, we need to build a single model that allows interactions. (Possibly NN with interesting architecture). \n\nAlso each model (in this kernel) only uses about 40 features. Within each partial dataset, are the other 215 features really useless, or can we use them to improve prediction?\n\n# Load Data\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800","2c806a96":"# Submit Predictions","6a745972":"## Partial dataset normality plots","849be36d":"# Conclusion\nIn conclusion, the success of using 512 separate models suggests that the 262144 rows of \"Instant Gratification\" dataset may actually be 512 partial datasets that were combined. Is this competition actually 512 models (competitions) in one? The appendix below investigates this further.\n\nThree suggestions to improve this kernel's accuracy are (1) identify and model interactions between partial datasets (2) extract information from the approx 215 variables that are not used in each model (3) build better partial models than SVC polynomial degree=4 via NN or LGBM.\n\n# Appendix\n## Variables are not Gaussian\nEach variable appears to be a nice bell shaped curve. However the curves are too tall and narrow. This is seen below by comparing the distribution of some variables with a Gaussian of the same mean and standard deviation. One can also verify that the variables are not Gaussian by making normality plots (pictured below)."}}