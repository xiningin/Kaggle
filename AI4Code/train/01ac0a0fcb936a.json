{"cell_type":{"910dfff8":"code","32837221":"code","88b177aa":"code","0aeaf198":"code","99a167ba":"code","f1addbd2":"code","2cbdd38e":"code","aaec36e3":"code","07f8d552":"code","805964fb":"code","a873af64":"code","304d288d":"code","5ec522fd":"code","991ebdee":"code","a643dcc2":"code","20c0f18d":"code","74e5d5d3":"code","481892b9":"code","0876e4ad":"code","7d5b6410":"markdown","24282152":"markdown","31b3dcca":"markdown","18e3fb7a":"markdown","2ad7256f":"markdown"},"source":{"910dfff8":"# Functions to read and show images.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n   \nd0 = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\n\nprint(d0.head(5)) # print first five rows of d0.\n\n# save the labels into a variable l.\nl = d0['label']\n\n# Drop the label feature and store the pixel data in d.\nd = d0.drop(\"label\",axis=1)\n\n    \n        \n","32837221":"print(d.shape)\nprint(l.shape)","88b177aa":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = d.iloc[idx].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","0aeaf198":"\nlabels = l.head(15000)\ndata = d.head(15000)\n\nprint(\"the shape of sample data = \", data.shape)\n","99a167ba":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)\n","f1addbd2":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)\n","2cbdd38e":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","aaec36e3":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","07f8d552":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","805964fb":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","a873af64":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()\n","304d288d":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n","5ec522fd":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","991ebdee":"pca_data.shape","a643dcc2":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","20c0f18d":"# TSNE\n\nfrom sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = standardized_data[0:1000,:]\nlabels_1000 = labels[0:1000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","74e5d5d3":"model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()","481892b9":"model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()","0876e4ad":"model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()","7d5b6410":"# t-SNE using Scikit-Learn","24282152":"# PCA for dimensionality redcution (not for visualization)","31b3dcca":"# PCA using Scikit-Learn","18e3fb7a":"Happy Learning!","2ad7256f":"#  2D Visualization using PCA "}}