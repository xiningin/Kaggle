{"cell_type":{"c87192d3":"code","50f183db":"code","eb1fd1d1":"code","1385acad":"code","9f63efdb":"code","46fdf47e":"code","0f2c37ff":"code","a32d2fc5":"code","20a161b9":"code","ab1be942":"code","3eb8db66":"code","fd01985e":"code","01335e3e":"code","2dc53027":"code","6274ff9b":"code","aa3fc076":"code","156074c2":"code","3e82bda2":"code","8f67658e":"code","c1e9072f":"code","0e18697a":"code","019e0bbc":"code","b725c667":"code","950e97cb":"code","a019fa95":"code","f08e1f7b":"code","a87fb073":"code","b2b1d7eb":"code","f15c7d9d":"code","7ec7bbde":"code","8be90e95":"code","816bc444":"code","4575d229":"code","652e29bb":"code","b425c9f4":"code","31c2800c":"code","a9ce1e73":"code","a22ab02d":"code","9cee58cd":"code","5fc3fa57":"code","eea3ecf3":"code","3616376c":"code","5fe40621":"code","4bd6486f":"markdown","1ef5e8e5":"markdown","752894e8":"markdown","2a279d58":"markdown"},"source":{"c87192d3":"import numpy as np   # linear algebra\nimport pandas as pd  # data processing\nimport seaborn as sns   # data visualization\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets.samples_generator import make_blobs\nimport argparse\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50f183db":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndTrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndTrain.columns","eb1fd1d1":"dTrain.drop(columns=[\"Name\",\"Ticket\",\"Cabin\"], inplace=True)\nl=dTrain.head(15)\nl","1385acad":"l=dTrain.describe()\nl","9f63efdb":"sns.countplot(x='Survived', hue='Pclass', data=dTrain)","46fdf47e":"sns.countplot(x='Survived', hue='Sex', data=dTrain)","0f2c37ff":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Pclass',y='Age',data=dTrain)","a32d2fc5":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = dTrain[dTrain['Sex']=='female']\nmen = dTrain[dTrain['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\nax.set_title('Male')","20a161b9":"sns.heatmap(dTrain.isnull(), yticklabels = False, cmap=\"YlGnBu\")","ab1be942":"dTrain.count()","3eb8db66":"total = dTrain.isnull().sum().sort_values(ascending=False)\npercent_1 = dTrain.isnull().sum()\/dTrain.isnull().count()*100\npercent_2 = (round(percent_1,1)).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent_2],axis=1,keys=['Total','%'])\nmissing_data","fd01985e":"sns.heatmap(dTrain.isnull(), yticklabels = False, cmap=\"YlGnBu\") # heat map for null values","01335e3e":"dTrain[\"Age\"][dTrain[\"Age\"].isna()] = dTrain[\"Age\"].mean()\nsns.heatmap(dTrain.isnull(), yticklabels = False, cmap=\"YlGnBu\") # heat map for null values","2dc53027":"# pd.get_dummies(dTrain[\"Sex\"])\nmale = pd.get_dummies(dTrain[\"Sex\"],drop_first=True)","6274ff9b":"embarked = pd.get_dummies(dTrain[\"Embarked\"],drop_first=True)\npclass= pd.get_dummies(dTrain[\"Pclass\"],drop_first=True)","aa3fc076":"dTrain= pd.concat([dTrain,pclass,male,embarked],axis=1)\ndTrain.head()","156074c2":"X=dTrain.drop([\"Survived\",\"Pclass\",\"Sex\",\"Embarked\"],axis=1)\ny = dTrain[\"Survived\"]\nX.head(10)","3e82bda2":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=0)","8f67658e":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","c1e9072f":"# Logistic Regression\nlogreg = LogisticRegression(tol=0.0001,solver='liblinear',max_iter=100)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","0e18697a":"# K Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","019e0bbc":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test) \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","b725c667":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","950e97cb":"# Linear Support Vector Machine\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","a019fa95":"# Decision Tree\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train) \nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","f08e1f7b":"# Which is the best Model ?\nresults = pd.DataFrame({'Model':['Support Vector Machines','KNN', 'Logistic Regression','Random Forest','Naive Bayes','Perceptron', \n              'Decision Tree'],'Score': [acc_linear_svc, acc_knn, acc_log,acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","a87fb073":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","b2b1d7eb":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","f15c7d9d":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","7ec7bbde":"X.head(5)","8be90e95":"test.head(5)","816bc444":"test.drop([\"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True)\ntest.head(5)","4575d229":"male= pd.get_dummies(test[\"Sex\"],drop_first=True)\nembarked = pd.get_dummies(test[\"Embarked\"],drop_first=True)\npclass= pd.get_dummies(test[\"Pclass\"],drop_first=True)\nxtest= pd.concat([test,pclass,male,embarked],axis=1)\nxtest.head(5)","652e29bb":"xtest=xtest.drop([\"Pclass\",\"Sex\",\"Embarked\"],axis=1)","b425c9f4":"xtest[\"Age\"][xtest[\"Age\"].isna()] = xtest[\"Age\"].mean()\nxtest[\"Fare\"][xtest[\"Fare\"].isna()] = xtest[\"Fare\"].mean()\nsns.heatmap(xtest.isnull(), yticklabels = False, cmap=\"YlGnBu\") ","31c2800c":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nprediction = random_forest.predict(xtest)","a9ce1e73":"prediction[:10]","a22ab02d":"# check structure with original submission \ng_submission=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ng_submission.head(10)","9cee58cd":"# make submission df as given gender_sub.csv\nsubmission=pd.DataFrame()\nsubmission['PassengerId']=test['PassengerId']\nsubmission['Survived']=prediction\nsubmission.head(10)","5fc3fa57":"model_score = random_forest.score(X_test, Y_test)\nprint(model_score)","eea3ecf3":"# Are our test and submission dataframe same length\nif len(submission) == len(xtest):\n    print(\"Same length\",len(submission))\nelse:\n    print(\"not match\")","3616376c":"submission.to_csv(\"randomForest_submission.csv\",index=False)","5fe40621":"#submission_check=pd.read_csv(\"randomForest_submission.csv\")\n#submission_check.head(5)","4bd6486f":"## Treatment for missing value","1ef5e8e5":"## Do  for test data set","752894e8":"## submission","2a279d58":"## Replace by Dummy variable"}}