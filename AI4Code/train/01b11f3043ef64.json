{"cell_type":{"81154d24":"code","eb6cff36":"code","6cb28e51":"code","1686b932":"code","530985e3":"code","40ced32c":"code","388d281c":"code","50d112ef":"code","92f475c9":"code","f8fafc31":"code","9a47c166":"code","b28c1204":"code","4ec16f2e":"code","e5b475eb":"code","53982bbd":"code","5cc34cd1":"markdown","19ece150":"markdown","162f0115":"markdown","64c9824f":"markdown","aab4321b":"markdown","cee1030a":"markdown","8421c633":"markdown","4d308b9e":"markdown","8153fa95":"markdown"},"source":{"81154d24":"%matplotlib inline\nfrom keras.models import Sequential, load_model\nfrom keras import regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.callbacks import ModelCheckpoint,History,EarlyStopping,LearningRateScheduler\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.optimizers import Adam, Adadelta, RMSprop\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eb6cff36":"data = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\nprint(data.shape)","6cb28e51":"test_data = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/test.csv')\nprint(test_data.shape)","1686b932":"train = data[:]\nval = data[55000:]\ntrain_label = np.float32(train.label)\nval_label = np.float32(val.label)\ntrain_image = np.float32(train[train.columns[1:]])\nval_image = np.float32(val[val.columns[1:]])\ntest_image = np.float32(test_data[test_data.columns[1:]])\nprint('train shape: %s'%str(train.shape))\nprint('val shape: %s'%str(val.shape))\nprint('train_label shape: %s'%str(train_label.shape))\nprint('val_label shape: %s'%str(val_label.shape))\nprint('train_image shape: %s'%str(train_image.shape))\nprint('val_image shape: %s'%str(val_image.shape))\nprint('test_image shape: %s'%str(test_image.shape))","530985e3":"datagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range = 10,\n    horizontal_flip = False,\n    zoom_range = 0.15)","40ced32c":"# one-hot coding\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False,categories='auto')\nyy = [[0],[1],[2],[3],[4],[5],[6],[7],[8],[9]]\nencoder.fit(yy)\n# transform\ntrain_label = train_label.reshape(-1,1)\nval_label = val_label.reshape(-1,1)\n\ntrain_label = encoder.transform(train_label)\nval_label = encoder.transform(val_label)\n\nprint('train_label shape: %s'%str(train_label.shape))\nprint('val_label shape: %s'%str(val_label.shape))","388d281c":"plt.imshow(train_image[13].reshape(28,28))\nplt.show()\nprint(train_image[13].shape)\n\ntrain_image = train_image\/255.0\nval_image = val_image\/255.0\ntest_image = test_image\/255.0\n\ntrain_image = train_image.reshape(train_image.shape[0],28,28,1)\nval_image = val_image.reshape(val_image.shape[0],28,28,1)\ntest_image = test_image.reshape(test_image.shape[0],28,28,1)\nprint('train_image shape: %s'%str(train_image.shape))\n\nprint('train_image shape: %s'%str(train_image.shape))\nprint('val_image shape: %s'%str(val_image.shape))","50d112ef":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1),padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(64, kernel_size=3, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(64, kernel_size=5, padding='same', activation='relu'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(128, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(128, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(256, kernel_size=5, activation='relu',padding='same'))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,kernel_regularizer=regularizers.l2(0.02)))\nmodel.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\nmodel.add(Dropout(0.3))\n# model.add(Dense(256,kernel_regularizer=regularizers.l2(0.02)))\n# model.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n# model.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","92f475c9":"BATCH_SIZE = 256\nEPOCHS = 53","f8fafc31":"model.compile(loss='categorical_crossentropy',optimizer=Adadelta(),metrics=['accuracy'])\n# fit data\ndatagen.fit(train_image)\n\n# training\nhistory = model.fit_generator(datagen.flow(train_image,train_label, batch_size=BATCH_SIZE),\n                              epochs = EPOCHS,\n                              shuffle=True,\n                              validation_data = (val_image,val_label),\n                              verbose = 1,\n                              steps_per_epoch=train_image.shape[0] \/\/ BATCH_SIZE)","9a47c166":"# \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u503c\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# \u7ed8\u5236\u8bad\u7ec3 & \u9a8c\u8bc1\u7684\u635f\u5931\u503c\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","b28c1204":"test_label = model.predict(test_image)\n# label = np.argmax(label_hot,1)\n# id_ = np.arange(0,label.shape[0])","4ec16f2e":"label_hot = model.predict(test_image)\nlabel = np.argmax(label_hot,1)\nid_ = np.arange(0,label.shape[0])","e5b475eb":"sim = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/sample_submission.csv')\nprint(sim.head(10))","53982bbd":"save = pd.DataFrame({'id':id_,'label':label})\nprint(save.head(10))\nsave.to_csv('submission.csv',index=False)","5cc34cd1":"# Training","19ece150":"# If this kernel helps you fortunately, please upvote it. Thanks you. :)","162f0115":"# One-hot encode","64c9824f":"# Divide data into training and validation sets","aab4321b":"# Load Data","cee1030a":"# Data enhancement","8421c633":"# Building the model","4d308b9e":"# Image transform","8153fa95":"# Visualization"}}