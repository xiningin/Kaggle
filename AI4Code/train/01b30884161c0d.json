{"cell_type":{"ea996b1f":"code","a99e56aa":"code","7952cba5":"code","82cec913":"code","23e96c09":"code","8af43094":"code","fd19e5f4":"code","519b7aa6":"code","4e70b6f0":"code","4695bb86":"code","017144f6":"code","e8cd1d7c":"code","fd5a471c":"code","b7c18f65":"code","d82a590f":"code","674e7411":"code","8a66ac8c":"code","306dd115":"code","898882c7":"code","17421d99":"code","ec335886":"code","bd3f72bb":"code","a3b76a08":"code","f6c1ed1c":"code","ae1da32a":"code","80a8e54d":"code","605a1479":"code","b511264b":"code","3ebe4d49":"code","fca8d5a1":"code","94a98b6f":"code","48658719":"code","df615cf7":"markdown","053da0c8":"markdown","0ff05e66":"markdown","58db1c5c":"markdown","b287f37f":"markdown","98b83f24":"markdown","6eed87cf":"markdown","2b3e1b9e":"markdown","fd360916":"markdown","2c08596f":"markdown","51f5dce5":"markdown","c86fbc58":"markdown"},"source":{"ea996b1f":"# importing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\n\n%matplotlib inline","a99e56aa":"df = pd.read_csv('..\/input\/the-boston-houseprice-data\/boston.csv')\ndf.shape","7952cba5":"df.head().T","82cec913":"df.info()","23e96c09":"df.dtypes.to_frame().rename(columns={0:'Data Type'})","8af43094":"# dealing with missing values\ndf.isnull().sum().sort_values(ascending= False)","fd19e5f4":"#unique values\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns],\n    columns=['Variable', 'Unique Values']).set_index('Variable')\n# df_uniques\ndf_uniques.sort_values(by=['Unique Values'],ascending=False)\n","519b7aa6":"# Create a list of float colums to check for skewing\nmask = df.dtypes == float\nfloat_cols = df.columns[mask]\n\nskew_limit = 0.75 # define a limit above which we will log transform\nskew_vals = df[float_cols].skew()","4e70b6f0":"# Showing the skewed columns\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n\nskew_cols","4695bb86":"viz = df[float_cols]\nviz.hist(figsize=(10,10))\nplt.show()","017144f6":"# Perform the skew transformation:\n\nfor col in skew_cols.index.values:\n    df[col] = df[col].apply(np.log1p)","e8cd1d7c":"viz = df[float_cols]\nviz.hist(figsize=(10,10))\nplt.show()","fd5a471c":"plt.figure(figsize=(24,20))\nplt.suptitle('Relationship between target and features variables', size=25)\n\nfor var , i in zip(float_cols,np.arange(1,len(float_cols))):\n    # if var == 'MEDV':\n    #     continue\n    plt.subplot(4,3,i)\n    plt.scatter(df[var], df.MEDV)\n    plt.xlabel(f\"{var.upper()}\")\n    plt.ylabel(\"MEDV\")\n","b7c18f65":"corr = df.corr()","d82a590f":"plt.figure(figsize=(14,10))\nsns.heatmap(corr)\nplt.title('Heatmap for all variables', size=15)","674e7411":"X = df.drop('MEDV', axis=1)\ny = df.MEDV\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=72018)","8a66ac8c":"s = StandardScaler()\nX_train_s = s.fit_transform(X_train)\nX_test_s = s.fit_transform(X_test)\n","306dd115":"def rmse(ytrue, ypredicted):\n    return np.sqrt(mean_squared_error(ytrue, ypredicted))","898882c7":"# First: LinearRegression\nlr = LinearRegression()\nlr.fit(X_train_s, y_train)\ny_pred = lr.predict(X_test_s)\nlinear_r2 = r2_score(y_test, y_pred)\n","17421d99":"kf = KFold(shuffle=True, random_state=72018, n_splits=3)\n","ec335886":"scores = []\n\nfor train_index, test_index in kf.split(df):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y[train_index], \n                                        y[test_index])\n    \n    X_train_s = s.fit_transform(X_train)\n    \n    lr.fit(X_train_s, y_train)\n    \n    X_test_s = s.transform(X_test)\n    \n    y_pred = lr.predict(X_test_s)\n\n    score = r2_score(y_test, y_pred)\n    \n    scores.append(score)\n\nscores","bd3f72bb":"estimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"lasso_regression\", Lasso())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'lasso_regression__alpha': np.geomspace(0.06, 6.0, 20)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","a3b76a08":"grid = grid.fit(X, y)\n","f6c1ed1c":"grid.best_score_, grid.best_params_","ae1da32a":"y_predict = grid.predict(X)\nlasso_r2 = r2_score(y, y_predict)\nprint(f'lasso_r2 = {lasso_r2}')\nlasso_rmse = rmse(y, y_predict)\nprint(f'lasso_rmse = {lasso_rmse}')\nlasso_mae = mean_absolute_error(y, y_predict)\nprint(f'lasso_mae = {lasso_mae}')\nlasso_mse = mean_squared_error(y, y_predict)\nprint(f'lasso_mse = {lasso_mse}')","80a8e54d":"estimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'ridge_regression__alpha': np.geomspace(0.06, 6.0, 20)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","605a1479":"grid = grid.fit(X, y)","b511264b":"grid.best_score_, grid.best_params_\n","3ebe4d49":"y_predict = grid.predict(X)\nr2_score(y, y_predict)","fca8d5a1":"linear_reg_rmse = rmse(y_test, y_pred)\nprint(linear_reg_rmse)\n\nlinear_mae = mean_absolute_error(y_test, y_pred)\nprint(linear_mae)\n\nlinear_mse = mean_squared_error(y_test, y_pred)\nprint(linear_mse)","94a98b6f":"\ny_predict = grid.predict(X)\nridge_r2 = r2_score(y, y_predict)\nprint(f'ridge_r2 = {ridge_r2}')\nridge_rmse = rmse(y, y_predict)\nprint(f'ridge_rmse = {ridge_rmse}')\nridge_mae = mean_absolute_error(y, y_predict)\nprint(f'ridge_mae = {ridge_mae}')\nridge_mse = mean_squared_error(y, y_predict)\nprint(f'ridge_mse = {ridge_mse}')\n","48658719":"rmse_vals = [linear_reg_rmse, ridge_rmse, lasso_rmse]\nr2_vals = [linear_r2, ridge_r2, lasso_r2]\nmae_vals = [linear_mae, ridge_mae, lasso_mae]\nmse_vals = [linear_mse, ridge_mse, lasso_mse]\n\nlabels = ['Linear', 'Ridge', 'Lasso']\n\nmetrics = {'MAE': mae_vals, 'MSE': mse_vals, 'RMSE': rmse_vals, 'R^2': r2_vals}\nmetrics_df = pd.DataFrame(metrics, index=labels)\n\nmetrics_df","df615cf7":"Now, let's plot each of these features against MEDV, to see how linear their relationship is:","053da0c8":"### spliting data","0ff05e66":"# understanding the data","58db1c5c":"### Ridge","b287f37f":"### scaling\n","98b83f24":"### Add Polynomial Features to Pipeline and use Grid Search CV","6eed87cf":"### Main objective:\nGoal is to predict the price of the Houses using regression models from the given multible features.\n\n","2b3e1b9e":"# `House Price Prediction` ","fd360916":"### Plan for data exploration:\n1. cleaning data \n    * removing unimportant data \n    * dealing with missing (NaN) values if found\n2. feature engineering \n    * visualizing the data and see the data distribution \n    * deal with skewed distribution if found and apply Log transforming on them\n3. Variable Selection\n    * encoding for categorical variables if found\n    * feature scalling for continuous variables\n4. Spliting the Data & implementing Cross Validation\n    * Train-Test split\n    * using k-fold with n=3\n5. regression models\n    * Linear regression\n    * regulation using Ridge and Lasso\n","2c08596f":"### Regression Model with k-fold","51f5dce5":"### Log transforming skewed variables","c86fbc58":"# `Conclusion`\nThis analysis shows that feature engineering can have a large effect on the model performance, and if the data are sufficiently large, cross-validation should be preferred over train-test-split to construct model evaluation. In my case, even though the predictors have high multicollinearity, their coefficients were not shrunk by the Lasso model, and it is shown that regularization does not always make big improvement on a given model. In the end, the Lasso regression has the highest $R^2$ when predicting on the test set, and categories of car model appear to be the most important features to predict houses prices. Also, Lasso did shrink some of the features that are not so important in terms of prediction."}}