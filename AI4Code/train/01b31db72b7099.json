{"cell_type":{"b4f6797a":"code","7874e54c":"code","aee72ae9":"code","882b87bf":"code","0be17cea":"code","3bd0e0ab":"code","1c29c819":"code","e9749802":"code","684ad3d3":"code","62d343c2":"code","6afb5da9":"code","39bd0b9f":"code","8247320b":"code","dd0e7073":"code","3eb96b8e":"code","6404531b":"code","27e078ca":"code","acf4d5e3":"code","5654fb0e":"code","2aa94f96":"code","2067517d":"code","f44b56b1":"code","7d8ecf9f":"code","234ed26d":"code","9835c3cc":"code","7168cc55":"code","9cdd1a12":"code","79bef0ba":"code","eacf1c98":"code","cd1407b0":"markdown","0a0693e0":"markdown","ca8de856":"markdown","cf394392":"markdown","11c3fba7":"markdown"},"source":{"b4f6797a":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-jan-2021\/')","7874e54c":"train = pd.read_csv(input_path \/ 'train.csv')\ndisplay(train.head())","aee72ae9":"test = pd.read_csv(input_path \/ 'test.csv')\ndisplay(test.head())","882b87bf":"submission = pd.read_csv(input_path \/ 'sample_submission.csv')\ndisplay(submission.head())","0be17cea":"### Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom  sklearn.model_selection import train_test_split , StratifiedKFold\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV , StratifiedKFold , KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\n\n\n# the scaler - for standardisation\nfrom sklearn.preprocessing import StandardScaler","3bd0e0ab":"#### Get Basic Details of the data files\nprint('Train data Shape : ' , train.shape)\nprint('Test data Shape : ' , test.shape)\nprint('Submission data Shape : ' , submission.shape)\n\nprint('#########################################')\nprint('Null Data Details - Train data')\nprint(train.isnull().sum())\n\nprint('#########################################')\nprint('Null Data Details - Test data')\nprint(test.isnull().sum())","1c29c819":"print('Train Data Set -- >')\nprint(train.info())\nprint('######################################')\nprint('Test Dataset ====> ')\nprint(test.info())","e9749802":"## Descriptive Statistics\ntrain.describe().T","684ad3d3":"### Lets check the distribution of features\nfor i in train.columns :\n    sns.distplot(train[i])\n    plt.show()","62d343c2":"target = 'target'\nId_Cols = ['id' ]\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\nX = train[features]\ny = train['target']\nX","6afb5da9":"#### Function to run alogorithm with Cross validation\ndef RunForAll(algo , k , train , test , features , params):\n    \n    ## Create matrix of zeros\n    val_set_pred = np.zeros(len(train))\n    test_set_pred = np.zeros(len(test))\n    #y_val = np.zeros(len(train))\n    \n    X = train[features]\n    y = train['target']\n    \n    kf = KFold(n_splits=k)\n    \n    for fold_, (train_index, val_index) in enumerate(kf.split(train , train['target'])):\n        print(f'\\n ---------------------Fold {fold_ + 1}-----------------')\n        \n        target = train['target']\n        X_train , y_train = train[features].iloc[train_index] , target.iloc[train_index]\n        X_val , y_val = train[features].iloc[val_index] , target.iloc[val_index]\n        \n        #New\n        X_train = X_train.abs()\n        X_test = test[features].abs()\n        \n        _ = algo.fit(X_train , y_train , eval_set = [(X_val , y_val)] , **params)\n        \n        prediction_val = algo.predict(X_val)\n        \n        kf_score = np.sqrt(mean_squared_error(y_val , prediction_val))\n        print(f'\\n Score For Validation Sample is {kf_score}')\n        \n        val_set_pred[val_index] = prediction_val\n        y_val = target.iloc[val_index]\n        #Predict for test \n        prediction_test = algo.predict(X_test)\n        test_set_pred += prediction_test \/ k\n        \n    val_score = np.sqrt(mean_squared_error(target,val_set_pred  ))\n    print(f'\\n Score for Validation set is {val_score}')\n    \n    return val_set_pred , test_set_pred , target        ","39bd0b9f":"xgb=XGBRegressor(n_estimators = 4000 , learning_rate = 0.011  )\nparams = {'verbose' : False , 'early_stopping_rounds' : 100}\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\nxgb_val_pred , xgb_test_preds , y_val_xgb = RunForAll(xgb,5, train , test , features , params)","8247320b":"coef1 = pd.Series(xgb.feature_importances_, features).sort_values()\ncoef1.plot(kind='bar' , title = 'Model Coefficient')\nplt.show()","dd0e7073":"xgb_val_pred , xgb_test_preds , y_val_xgb","3eb96b8e":"from lightgbm import LGBMRegressor\n\n#lgb = LGBMRegressor(n_estimators = 8000 , importance_type = 'gain' ,  learning_rate = 0.001 , tree_method= gpu_hist ,\n#             predictor= gpu_predictor)\n\nlgb = LGBMRegressor(n_estimators = 100000, metric = 'rmse' ,  learning_rate = 0.01 , boosting_type = 'gbdt' ,\n             num_leaves = 200 , feature_fraction = 0.6 , lambda_l1 = 2 , lambda_l2 = 2 , min_child_samples = 50 , bagging_fraction = 0.4 ,\n                   bagging_freq = 1 , verbosity=-1 , max_depth = 12 , max_bin = 200 ,\n                   objective = 'regression')\n\nparams = {'verbose' : 100 ,  'early_stopping_rounds' : 1000 }\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\nlgb_val_pred , lgb_test_preds , y_val_lgb = RunForAll(lgb,5, train , test , features , params)","6404531b":"coef1 = pd.Series(lgb.feature_importances_, features).sort_values()\ncoef1.plot(kind='bar' , title = 'Model Coefficient')\nplt.show()","27e078ca":"from catboost import CatBoostRegressor\n\ncb = CatBoostRegressor(n_estimators = 5000 ,  learning_rate = 0.005 )\n\nparams = {'verbose' : False , 'early_stopping_rounds' : 100}\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\ncb_val_pred , cb_test_preds , y_val_cb = RunForAll(cb,5, train , test , features , params)","acf4d5e3":"test['target'] = cb_test_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_cb.csv' , index=False)\noutput.head()","5654fb0e":"#target = train['target']\nensemble_df = pd.DataFrame(xgb_val_pred , columns=['xgboost'])\n\nensemble_df['lgboost'] = lgb_val_pred\nensemble_df['cbboost'] = cb_val_pred\n#ensemble_df['label'] = y_val_xgb\nensemble_df.head()","2aa94f96":"ensemble_test_df = pd.DataFrame(xgb_test_preds , columns=['xgboost'])\nensemble_test_df['lgboost'] = lgb_test_preds\nensemble_test_df['cbboost'] = cb_test_preds\nensemble_test_df.head()","2067517d":"from sklearn.linear_model import LinearRegression","f44b56b1":"X = ensemble_df\ny = y_val_xgb\nreg = LinearRegression().fit(X, y)\nprint(reg.score(X, y))\nprint(reg.coef_)\nprint(reg.intercept_)","7d8ecf9f":"ensemble_preds = reg.predict(ensemble_test_df)\nensemble_preds","234ed26d":"test['target'] = ensemble_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission.csv' , index=False)\noutput.head()","9835c3cc":"test['target'] = cb_test_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_1.csv' , index=False)\noutput.head()","7168cc55":"from sklearn.tree import DecisionTreeRegressor\nX = ensemble_df\ny = y_val_xgb\ndregr = DecisionTreeRegressor(max_depth=5)\ndregr.fit(X, y)\nensemble_preds_dr = dregr.predict(ensemble_test_df)\n","9cdd1a12":"test['target'] = ensemble_preds_dr\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_2.csv' , index=False)\noutput.head()","79bef0ba":"from sklearn.ensemble import RandomForestRegressor\nX = ensemble_df\ny = y_val_xgb\nrfregr = RandomForestRegressor(n_estimators = 200 , max_depth=5 , min_samples_leaf=100 , n_jobs=4)\nrfregr.fit(X, y)\nensemble_preds_rf = rfregr.predict(ensemble_test_df)","eacf1c98":"test['target'] = ensemble_preds_rf\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_3.csv' , index=False)\noutput.head()","cd1407b0":"# Read in the data files","0a0693e0":"In this notebook, you will learn how to make your first submission to the **[Tabular Playground Series - Jan 2021](https:\/\/admin.kaggle.com\/c\/tabular-playground-series-jan-2021\/overview)** competition. \n\nThis notebook will help get into top 30% solutions.","ca8de856":"##### Observation \n1. Some features have negative values\n2. Most features have values ranging from 0 to 1 \n3. Target feature has some records with 0 value. ","cf394392":"###### Lets Try to use Ensemble technique","11c3fba7":"#### Observations\n1. There is no null data in train or test dataset\n2. All features are numerical"}}