{"cell_type":{"e1f1f109":"code","33f64a66":"code","2b9e1dc3":"code","8a83f1e5":"code","c12cbcb6":"code","b668fba3":"code","6dae347d":"code","bbdfb3a7":"code","8f5ef451":"code","5bf63e98":"code","13956af1":"code","92b5d08a":"code","e9aa0e8a":"code","5b630a5f":"code","24d79085":"code","7934ec1b":"code","b425a573":"code","7f9e2518":"code","e7d2231b":"code","cdade99a":"code","6f3ca6bd":"code","681bdf0b":"code","96974f4f":"code","375b150c":"code","3b7399a2":"code","d0a7612b":"code","843f0c56":"markdown","431cbece":"markdown","2c0854f8":"markdown","289ad4a4":"markdown","96bb07a5":"markdown","f0292d7b":"markdown","73951d46":"markdown","ed759244":"markdown","6ee2ee5d":"markdown","81c4a498":"markdown","4a719a7c":"markdown","42ff78df":"markdown","4ba252d2":"markdown","c1fc2d8c":"markdown","fce9140d":"markdown","e20ca022":"markdown","dc6eddc8":"markdown","b53de2b8":"markdown","0d8488e7":"markdown","46baaedb":"markdown","a985e2b1":"markdown","14a7d535":"markdown","cf072798":"markdown","a8681ca3":"markdown","0297aabe":"markdown","eab081b9":"markdown","e1377328":"markdown","8919a244":"markdown","b65c3fca":"markdown","75d2c90f":"markdown","36f674d4":"markdown","cc2f7a7d":"markdown","17e09a9e":"markdown","4c78cff3":"markdown","05b0902c":"markdown","78cce9a8":"markdown","ec0d7a41":"markdown","86d2f433":"markdown"},"source":{"e1f1f109":"!pip install plotly==2.3.0","33f64a66":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import cm\nsns.set_style('ticks')\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","2b9e1dc3":"# Import Dataset\ndf_dup = pd.read_csv('..\/input\/pdb_data_no_dups.csv')\ndf_seq = pd.read_csv('..\/input\/pdb_data_seq.csv')","8a83f1e5":"# Merge the two Data set together\ndf_merge = df_dup.merge(df_seq,how='inner',on='structureId')\ndf_merge.rename({'macromoleculeType_x':'macromoleculeType',\n                                            'residueCount_y':'residueCount'},axis=1,inplace=True)\ndf_merge.drop(['macromoleculeType_y','residueCount_x'],axis=1,inplace=True)","c12cbcb6":"df_isnull = pd.DataFrame(round((df_merge.isnull().sum().sort_values(ascending=False)\/df_merge.shape[0])*100,1)).reset_index()\ndf_isnull.columns = ['Columns', '% of Missing Data']\ndf_isnull.style.format({'% of Missing Data': lambda x:'{:.1%}'.format(abs(x))})\ncm = sns.light_palette(\"skyblue\", as_cmap=True)\ndf_isnull = df_isnull.style.background_gradient(cmap=cm)\ndf_isnull","b668fba3":"print ('The publication of the Reserach on the Strutural Sequence of Protein have continually incresed over the last 4 decades')","6dae347d":"df_pub_year = df_merge.dropna(subset=['publicationYear']) #dropping the missing values from the publicationYear only\n#graph\nx1= df_pub_year.publicationYear.value_counts().sort_index().index\ny1 = df_pub_year.publicationYear.value_counts().sort_index().values\ntrace1 = go.Scatter(\n    x=x1,\n    y=y1,\n    mode = 'lines+markers',\n    text = x1,\n    marker=dict(\n    color='blue'),  \n)\nlayout = go.Layout(\n    xaxis=dict(\n        title = 'Years',\n        range = [1967.9,2018.5],\n        autotick=True,  \n    ),\n    yaxis = dict(\n    title = 'Frequency'\n    ),\n    title = 'Number of Publications Since 1968'\n    )\n\nfig = go.Figure(data=[trace1], layout=layout)\npy.iplot(fig)\n","bbdfb3a7":"# We will split the ph value into three according to the scientific categorization of the Ph such as Acidic if pH <7\n#basic if pH >7 and neutral if pH = 7\ndef ph_scale (ph):\n    if ph < 7 :\n        ph = 'Acidic'\n    elif ph > 7:\n        ph = 'Bacis'\n    else:\n        ph = 'Neutral'\n    return ph\nprint('The pH Scale are group into 3 Categories: BASIC if [ pH > 7 ], ACIDIC if [ pH < 7 ] and NEUTRAL if pH [ is equal to 7 ]')\n\n#Transform the dataset\ndf_ph = df_merge.dropna(subset=['phValue']) # dropping missing values in the phValue column only\ndf_ph['pH_scale'] = df_ph['phValue'].apply(ph_scale)\n#Graph\nlabels= df_ph['pH_scale'].value_counts().index\nvalues = df_ph['pH_scale'].value_counts().values\nfig = {\n      \"data\": [\n        {\n          \"values\":values,\n          \"labels\":labels,\n          \"text\":'pH Scale',\n          \"textposition\":\"inside\",\n          #\"domain\": {\"x\": [0, .33]},\n          \"textfont\": {'size':12,'color':'white'},  \n          \"name\": 'pH Scale',\n          \"hoverinfo\":\"label+percent+name\",\n          \"hole\": .4,\n          \"type\": \"pie\"\n        }],\n    \"layout\": {\n            \"title\":\"pH Distribution\",\n            \"annotations\": [\n                {\n                    \"font\": {\n                        \"size\": 20\n                    },\n                    \"showarrow\": False,\n                    \"text\": 'pH Scale',\n                    \"x\": 0.50,\n                    \"y\": 0.5\n                }]\n            }\n        }\npy.iplot(fig)                              ","8f5ef451":"# The result of this cell Show the Top 10 most used crystallization method\ndf_cry_meth = df_merge.dropna(subset=['crystallizationMethod']) # this will drop all missing values in\n#the crystallizationMethod column\n\ncry_me = pd.DataFrame(df_cry_meth.crystallizationMethod.value_counts(ascending=False).head(10)).reset_index()\ncry_me.columns = ['Crystallization Method','Values']\n\nf,ax = plt.subplots(figsize=(10,8))\ncry_me.plot(kind = 'barh',ax=ax,color='gray',legend=None,width= 0.8)\n# get_width pulls left or right; get_y pushes up or down\nfor i in ax.patches:\n    ax.text(i.get_width()+.1, i.get_y()+.40, \\\n            str(round((i.get_width()), 2)), fontsize=12, color='black',alpha=0.8)  \n#Set ylabel\nax.set_yticklabels(cry_me['Crystallization Method'])\n# invert for largest on top \nax.invert_yaxis()\nkwargs= {'length':3, 'width':1, 'colors':'black','labelsize':'large'}\nax.tick_params(**kwargs)\nx_axis = ax.axes.get_xaxis().set_visible(False)\nax.set_title ('Top 10 Crystallization Method',color='black',fontsize=16)\nsns.despine(bottom=True)","5bf63e98":"popular_exp_tech = df_merge.experimentalTechnique.value_counts()[:3] # Extract the 3 top used Exp Tech \npopular_exp_tech_df = pd.DataFrame(popular_exp_tech).reset_index()\npopular_exp_tech_df.columns=['Experimental Technique','values']\n# ADDING A ROW FOR THE ORTHER EXPERIMENTAL TECHNIQUE USED. PLEASE PUT IN MIND THAT TO ORTHER TECHNIQUES \n#IS JUST A GROUP OF THE REST OF THECNIQUES USED\npopular_exp_tech_df.loc[3]  = ['OTHER TECHNIQUE', 449]\nprint ('The X-RAY DIFFRACTION is by far the most used Experimental Technique during the Study of the Protein Sequences')\n\nlabels = popular_exp_tech_df['Experimental Technique']\nvalues = popular_exp_tech_df['values']\na = 'Exp Tech'\nfig = {\n      \"data\": [\n        {\n          \"values\":values,\n          \"labels\":labels,\n          \"text\":a,\n          \"textposition\":\"inside\",\n          #\"domain\": {\"x\": [0, .33]},\n          \"textfont\": {'size':12,'color':'white'},  \n          \"name\": a,\n          \"hoverinfo\":\"label+percent+name\",\n          \"hole\": .4,\n          \"type\": \"pie\"\n        }],\n    \"layout\": {\n            \"title\":\"Most Used Experimental Techniques\",\n            \"annotations\": [\n                {\n                    \"font\": {\n                        \"size\": 20\n                    },\n                    \"showarrow\": False,\n                    \"text\": a,\n                    \"x\": 0.50,\n                    \"y\": 0.5\n                }]\n            }\n        }\npy.iplot(fig)                              ","13956af1":"print ('There are more than 10 macro molecules used in this dataset but PROTEIN is widely used than the others')\n\nex = df_merge.macromoleculeType.value_counts()\na = 'Macro Mol Type'\ncolors = ['SlateGray','Orange','Green','DodgerBlue','DodgerBlue','DodgerBlue','DodgerBlue','DodgerBlue','DodgerBlue',\n        'DodgerBlue','DodgerBlue','DodgerBlue','DodgerBlue']\nfig = {\n      \"data\": [\n        {\n          \"values\":ex.values,\n          \"labels\":ex.index,\n          \"text\":a,\n          \"textposition\":\"inside\",\n          #\"domain\": {\"x\": [0, .33]},\n          \"textfont\": {'size':12,'color':'white'},  \n          \"name\": a,\n          \"hoverinfo\":\"label+percent+name\",\n          \"hole\": .4,\n          'marker':{'colors':colors\n                   },\n          \"type\": \"pie\"\n        }],\n    \"layout\": {\n            \"title\":\"Macro Molecule type Distribution\",\n            \"annotations\": [\n                {\n                    \"font\": {\n                        \"size\": 20\n                    },\n                    \"showarrow\": False,\n                    \"text\": a,\n                    \"x\": 0.50,\n                    \"y\": 0.5\n                }]\n            }\n        }\npy.iplot(fig)                              ","92b5d08a":"#classification distribution\nclasific =df_merge.classification.value_counts(ascending=False)\ndf_class = pd.DataFrame(round(((clasific\/df_merge.shape[0])*100),2).head(10)).reset_index()\ndf_class.columns = ['Classification', 'percent_value']\nprint('There are {} Unique Classification Types and the top 10 Classification type accounts for more than 50% of the classification in the dataset'.format(df_merge.classification.nunique()))\nf,ax = plt.subplots(figsize=(10,8))\n\ndf_class.plot(kind = 'barh',ax=ax,color='slategray',legend=None,width= 0.8)\n# get_width pulls left or right; get_y pushes up or down\nfor i in ax.patches:\n    ax.text(i.get_width()+.1, i.get_y()+.40, \\\n            str(round((i.get_width()), 2))+'%', fontsize=12, color='black',alpha=0.8)  \n#Set ylabel\nax.set_yticklabels(df_class['Classification'])\n# invert for largest on top \nax.invert_yaxis()\nkwargs= {'length':3, 'width':1, 'colors':'black','labelsize':'large'}\nax.tick_params(**kwargs)\nx_axis = ax.axes.get_xaxis().set_visible(False)\nax.set_title ('Top 10 Classification Types',color='black',fontsize=16)\nsns.despine(bottom=True)","e9aa0e8a":"df_class.Classification.values.tolist()[1:4]\n# Reduce the df_merge to df_protein which is compose of macromolecule type [Protein and Protein#RNA]\nmacrotype = ['Protein','Protein#RNA']\ndf_protein = df_merge[(df_merge['experimentalTechnique'] =='X-RAY DIFFRACTION') & \n                      (df_merge['macromoleculeType'].isin(macrotype))&\n                     (df_merge['classification'].isin(df_class.Classification.values.tolist()[1:4]))]\n\ndf_protein.reset_index(drop=True,inplace=True)\ncolumns = ['crystallizationMethod' ,'pdbxDetails', 'publicationYear','phValue','crystallizationTempK']\n#Dropping columns with missing value above 15%\ndf_protein.drop(columns=columns,inplace=True)\n# Classification Type that will be used from now on\nf,ax= plt.subplots(figsize=(10,5))\nsns.countplot('classification',data=df_protein, ax=ax)\nax.set_title('Classification Types Selected',fontsize=14,color='black')\nax.tick_params(length =3,labelsize=11,color='black')\nax.set_xlabel('Classification',color='black',fontsize=13)\nsns.despine()","5b630a5f":"from scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\ndef stat_kde_plot(input1,input2,input3):\n    f, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n    sns.kdeplot(df_protein[input1],ax = ax1,color ='blue',shade=True,\n                label=(\"Skewness : %.2f\"%(df_protein[input1].skew()),\n                       \"Kurtosis: %.2f\"%(df_protein[input1].kurtosis())))\n    sns.kdeplot(df_protein[input2], ax = ax2,color='r',shade=True,\n                label=(\"Skewness : %.2f\"%(df_protein[input2].skew()),\n                       \"Kurtosis: %.2f\"%(df_protein[input2].kurtosis())))\n    sns.kdeplot(df_protein[input3], ax = ax3,color='gray',shade=True,\n                label=(\"Skewness : %.2f\"%(df_protein[input3].skew()),\n                       \"Kurtosis: %.2f\"%(df_protein[input3].kurtosis())))\n    axes = [ax1,ax2,ax3]\n    input = [input1,input2,input3]\n    for j in range(len(axes)):\n        axes[j].set_xlabel(input[j],color='black',fontsize=12)\n        axes[j].set_title(input[j] + ' Kdeplot',fontsize=14)\n        axes[j].axvline(df_protein[input[j]].mean() , color ='g',linestyle = '--')\n        axes[j].legend(loc ='upper right',fontsize=12,ncol=2)\n    sns.despine()\n    return plt.show()\n\nstat_kde_plot('resolution','residueCount','structureMolecularWeight')","24d79085":"for i in ['resolution','residueCount','structureMolecularWeight']:\n    df_protein[i] = df_protein[i].map(lambda i: np.log(i) if i > 0 else 0)\nstat_kde_plot('resolution','residueCount','structureMolecularWeight')","7934ec1b":"# Drop all null values from this columns\ndef stat_plot (input):\n    (mu, sigma) = norm.fit(df_protein[input])\n    f, (ax1, ax2)= plt.subplots(1,2,figsize=(15,5))\n    # Apply the log transformation on the column\n    sns.distplot(df_protein[input],ax = ax1,fit=norm,color ='blue',hist=False)\n    ax1.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n    ax1.set_ylabel('Frequency')\n    ax1.set_title(input +' Distribution',color='black',fontsize=14)\n    #Get also the QQ-plot\n    res = stats.probplot(df_protein[input], plot=ax2)\n    sns.despine()\n    return plt.show()\nstat_plot('structureMolecularWeight')\nstat_plot('residueCount')\nstat_plot('resolution')","b425a573":"def box_plot(input):\n    g = sns.factorplot(x=\"classification\", y = input,data = df_protein, kind=\"box\",size =4,\n                  aspect=2)\n    plt.title(input, fontsize=14,color='black')\n    return plt.show()\n\nbox_plot('residueCount')\nbox_plot('resolution')\nbox_plot('structureMolecularWeight')","7f9e2518":"#class_dict = {'RIBOSOME':1,'HYDROLASE':2,'TRANSFERASE':3} \nclass_dict = {'HYDROLASE':1,'TRANSFERASE':2,'OXIDOREDUCTASE':3}\ndf_protein['class'] = df_protein.classification.map(class_dict)\n#Reduce the dataset to only numerical column and clssification column\ncolumns = ['resolution','structureMolecularWeight','densityMatthews','densityPercentSol',\n           'residueCount','class']\ndf_ml = df_protein[columns]\ndf_ml.dropna(inplace=True)\ndf_ml.head()","e7d2231b":"colormap = plt.cm.RdBu\nf, ax = plt.subplots(figsize=(18,7))\nsns.heatmap(df_ml.corr(),cmap= colormap,annot=True,ax=ax,annot_kws ={'fontsize':12})\nkwargs= {'length':3, 'width':1, 'colors':'black','labelsize':13}\nax.tick_params(**kwargs)\nax.tick_params(**kwargs,axis='x')\nplt.title ('Pearson Correlation Matrix', color = 'black',fontsize=18)\nplt.tight_layout()\nplt.show()","cdade99a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX = df_ml.drop('class',axis = 1)\ny = df_ml['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n# Standardizing the dataset\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6f3ca6bd":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.neural_network import MLPClassifier","681bdf0b":"def model_select(classifier):\n    cv_result = []\n    cv_means = []\n    # Cross validate model with Kfold stratified cross val\n    kfold = StratifiedKFold(n_splits=5)\n    cv_result.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    cv_means.append(np.mean(cv_result))\n    return cv_means\n# Fitting all the models \nmodel_type = [KNeighborsClassifier(),GaussianNB(),RandomForestClassifier(),\n              AdaBoostClassifier(),GradientBoostingClassifier(),DecisionTreeClassifier(),ExtraTreesClassifier()]\nmodel_score = [model_select(i) for i in model_type]","96974f4f":"classifier = ['KNeighbors','Naive Bayes','Random Forest', \n             'AdaBoost','Gradient Boosting','Decision Tree','Extra Trees']\n# Place result in a data Frame\nml_model = pd.DataFrame(model_score,classifier).reset_index()\nml_model.columns=['Model','acc_score']\nml_model.sort_values('acc_score',ascending = False,inplace=True)\nml_model.reset_index(drop=True,inplace = True)\nf, ax = plt.subplots(figsize=(10,8))\nsns.barplot('acc_score','Model',data=ml_model, ax=ax,palette='RdBu_r',edgecolor=\".2\")\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.01, i.get_y()+.55, \\\n        str(round((i.get_width()), 2)), fontsize=12, color='black') \nkwargs= {'length':3, 'width':1, 'colors':'black','labelsize':'large'}\nax.tick_params(**kwargs)\nx_axis = ax.axes.get_xaxis().set_visible(False)\nax.set_title('Model & Accuracy Score',fontsize=16)\nsns.despine(bottom=True)\nplt.show()","375b150c":"#Credit: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\nimport itertools\n# Compute confusion matrix\ndef single_model(model):\n    clf = model\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    conf_mx = confusion_matrix(y_pred,y_test)\n    return conf_mx\n\n#plot confusion matrix    \ndef plot_confusion_matrix(cm, classes,model_name):\n\n    plt.figure(figsize=(10,6))\n    cmap = plt.cm.Blues\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix: '+ model_name, fontsize=15)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\",fontsize=12)\n\n    plt.tight_layout()\n    plt.ylabel('True label',fontsize=12,color='black')\n    plt.xlabel('Predicted label',fontsize=12,color='black' )\n    np.set_printoptions(precision=2)\n    \n    return plt.show()\nclasses = ['HYDROLASE','TRANSFERASE','OXIDOREDUCTASE']\nplot_confusion_matrix(single_model(ExtraTreesClassifier()), classes,'Extra Trees Classifier Model')\nplot_confusion_matrix(single_model(RandomForestClassifier()), classes,'Random Forest Classifier Model')\n#plot_confusion_matrix(single_model(SVC()), classes,'Support Vector Classifier Model')","3b7399a2":"def sing_model(model,input):\n    clf = model\n    clf.fit(X_train,y_train)\n    model_fi = clf.feature_importances_\n    feat_imp = pd.DataFrame(model_fi,df_ml.columns[:-1]).reset_index()\n    feat_imp.columns = ['Features','Importance_score']\n    feat_imp.sort_values('Importance_score',ascending=False,inplace=True)\n    feat_imp.reset_index(drop=True,inplace = True)\n    f, ax = plt.subplots(figsize=(10,8))\n    sns.barplot('Importance_score','Features',data=feat_imp, ax=ax,palette='RdBu_r',edgecolor=\".2\")\n    for i in ax.patches:\n        # get_width pulls left or right; get_y pushes up or down\n        ax.text(i.get_width()+.002, i.get_y()+.45, \\\n            str(round((i.get_width()), 2)), fontsize=12, color='black') \n    kwargs= {'length':3, 'width':1, 'colors':'black','labelsize':'large'}\n    ax.tick_params(**kwargs)\n    x_axis = ax.axes.get_xaxis().set_visible(False)\n    ax.set_title(input +':'+ ' Features Importance Score',fontsize=16)\n    sns.despine(bottom=True)\n    return plt.show()\nsing_model(RandomForestClassifier(),'Random Forest Classifier')\nsing_model(ExtraTreesClassifier(),'Extra Trees Clas')","d0a7612b":"kfold = StratifiedKFold(n_splits=5)\n# Generate a simple plot of the test and training learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5)):\n\n    plt.figure(figsize = (10,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n\n    plt.grid()\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt.show()\n\nplot_learning_curve(ExtraTreesClassifier(),\"Extra Trees Classifier Learning curves\",X_train,y_train,cv=kfold)\nplot_learning_curve(RandomForestClassifier(),\"Random Forest Classifier Learning curves\",X_train,y_train,cv=kfold)\nplot_learning_curve(GradientBoostingClassifier(),\"Gradient Boosting Classifier mearning curves\",X_train,y_train,cv=kfold)","843f0c56":"This is a modified version of the Kaggle kernel: https:\/\/www.kaggle.com\/hnike25\/best-machine-learning-model-on-the-dataset","431cbece":"#### Normal Distribution Verification","2c0854f8":"### Most Used Crystallization Method","289ad4a4":"Interpretation of the figures above seems to follow a normal distribution reasonably well except in the extreme tails\n    1. the Residue Count and the Structure Molecular Weight are more likely to be normally distributed. After the   data transformation, the skewness is between [-1\/2,1\/2], \n    which means the distribution is approximately symmetrical\n    2. Also, the kurtosis of both distribution are close to zero, which can explained the absence of long tail on the graphes\n    3. the distribution of Resolution parameter can be observed to have a longer tail, which explained the bump at the end of the graph and a higher kurtosis","96bb07a5":"### Most Used Experimental Technique","f0292d7b":"I will reduce my dataset to only 3 classification type, which are HYDROLASE, TRANSFERASE AND OXIDOREDUCTASE","73951d46":"### Missing Data Analysis","ed759244":"### Merge Dataset ","6ee2ee5d":"### Analysis Results (part I)\n\nThis is what we have found So far\n    1. The most analyzed macromolecule Type is Protein\n    2. The most used Experimental Technique is the X_RAY DIFFRACTION\n    3. There are 5050 Uniques classification type in the dataset and the highest classification type is less than   15%\n    4. The macromolecule Type has less than 3% of missing values whereas the experimental Technique columns don't   any missing values\n    5. We will reduce our data set to Experimental Technique = 'X-RAY DIFFRACTION'  and drop all columns with missing values percentage greater than 15%\n    6. The Dataset is very large and contains **352125** classification types. I will reduce to classification to    the 3 most used classifications, which are **HYDROLASE, TRANSFERASE AND OXIDOREDUCTASE**. This is for learning purpose only. Also, I didn't select the RIBOSOME because its properties can be easily been differentiated. ","81c4a498":"#### My Data Interpretation","4a719a7c":"From the 7 models ran above, we will only performed the confusion matrix analysis on Two model, which are the Random Forest and the Extra tree Models Classifier.\nWhy those two?\nRandom Forest handle overfitting better than other model if there are enough data. Also the randomness of the algorithm during feature selection allow the model to perform better\nTo make it simple Random Forest is Decision tree on steroids.\nRandom Forest and Extra Tree and very similar but differ only on how they randomly splits the data. So if these Two models give me close result...I may conclude that my model is doing GREAT\n\n","42ff78df":"#### Learning Curve Analysis","4ba252d2":"### Dataset Reduction","c1fc2d8c":"### PH Scale Distribution","fce9140d":"### Pearson Correlation Matrix Analysis","e20ca022":"* This is a large dataset with a lot of Classification types. I reduce the dataset to three classification type to be able to analyze it on my PC. \n* I Do Not know if we apply my methodology to all the dataset on a powerful computer (may be cloud computing), my result will be valide. \n* For learning Purpose if you find my work helpful  or like what you see....... PLEASE GIVE ME A UPVOTE  :):):)","dc6eddc8":"#### Model Evaluation Using Confusion Matrix","b53de2b8":"We can observed that the , Resolution, Residue Count and Structure Molecular Weight distributions are very skewed. These can overweigth high values in the model, even after scaling them.\n\nA better approach to this problem is to transform the feature above using the log function to reduce the skew.","0d8488e7":"### Statistical Analysis ","46baaedb":"1. Training score (red line) is at its maximum regardless of training examples. This shows severe overfitting\n2. Cross-validation score (green line) increases as the training set sample increases but the huge gap between cross-validation score and training score indicates high variance problem\n3. To solve issue like this, may be  data collections or extra features can help with the high variance","a985e2b1":"CONTENT\n*  Merge Dataset\n*  Missing Data \n*  Publication Analysis\n*  PH Scale Distribution\n*  Most Used Crystallization Method\n*  Most Used Experimental Technique\n*  Macro Molecule Type Distribution\n* Classification Distribution\n* Analysis Results\n* Correlation Matrix Analysis\n* Statistical Analysis","14a7d535":"### Simple Machine Learning Analysis","cf072798":"The Kdeplots above do not give a clear interpretation of the data from the respective columns. we will use some statistical method to extract some useful meaning from the columns. we will play with some statistical analysis\n- plot the Normal Q-Q plot to verify if these distribution are normaly distributed\n- Interpret the plot","a8681ca3":"#### Summary","0297aabe":"### Feature Engineering","eab081b9":"Let check how the data from these columns are distributed. the columns are : Resolution, Residue Count and Struture Molecular Weights","e1377328":"#### Boxplot Analysis","8919a244":"Protein is the most macromoleclule type analyzed in this dataset","b65c3fca":"The Crystallization Method, Crystallization Temperatue,pH values,Publication Year and pdbxDetails have more than 15% of missing values. We will use the column for individual analysis only during the EDA process. We will also used the dataframe.dropna(subnet = [column_name]) method when this columns are used. This method willl drop all the missing values for the selected column only.","75d2c90f":"### Macro Molecule Type Distribution","36f674d4":"#### Features Importance & Plot","cc2f7a7d":"#### Apply Log Function Transformation","17e09a9e":"### What can we learn from the plot\nThere is a very strong Correlation between Density Matthews values and Density Percent Sol, ResidueCount and Structure Molecular Weight. Both parameters also have a weak correlation with **Resolution, Residue Count** and **Struture Molecular Weight**. Some features have low pearson correlation but I will keep and use them in the Machine learning model for learning purpose ","4c78cff3":" #### Learning Analysis","05b0902c":"### Accuracy Scores Plot","78cce9a8":"#### Fitting All Models One at the Time","ec0d7a41":"### Publication per Year","86d2f433":"### Classification Distribution"}}