{"cell_type":{"15d21582":"code","8ac5ca80":"code","142dc033":"code","e94b1cac":"code","3823669b":"code","587cabf1":"code","8fbb352b":"code","cf40f320":"code","0df6f95f":"code","9ffb4ede":"code","c03a3b5e":"code","8847afb8":"code","95004d1e":"code","477475cb":"code","324098b6":"code","0ddf8f2a":"code","edad8ac6":"code","456cff43":"code","397d9327":"code","eb56786d":"code","a4ce1221":"code","edf77b2d":"code","25f3bac3":"code","38ba7a13":"code","8e1b75fa":"code","ccdd1508":"code","2e4d0250":"code","42be3c29":"code","c73f0d90":"code","36983057":"code","9ab99f97":"code","df0ac8d8":"code","c7a9b2c1":"code","978bfafc":"code","a8b13271":"code","7cc2c471":"code","ca50b4bf":"code","b0623a8d":"code","49fdc00e":"code","3105a70a":"code","f119e2eb":"code","4541ea44":"code","a50e7dbe":"code","1d9d16a4":"code","75ec1cd0":"code","4b313077":"code","f1c8536d":"code","a60100ac":"code","16d1e77c":"code","f30248b3":"code","0bc2a1dd":"code","20a8eb59":"code","9e7a1791":"code","6e82799c":"code","9df2850f":"code","ed68497f":"code","0332a63a":"code","a805b5e8":"code","1b96da21":"markdown","fed0bb33":"markdown","067bb3fc":"markdown","94bb615e":"markdown","b33ee994":"markdown","85933f3b":"markdown","802eebac":"markdown","8375816c":"markdown","e709583f":"markdown","b7de776e":"markdown","9d030e4c":"markdown","4434c62d":"markdown","175ab2b9":"markdown","11df215a":"markdown","eb2babad":"markdown","76235c92":"markdown","a8ff9348":"markdown","615543fc":"markdown","1ef8629a":"markdown","f0b66033":"markdown","8bc5ffb8":"markdown","8dccc68c":"markdown","d5e37b72":"markdown","2c519dd7":"markdown","11e18364":"markdown","8cf4b6c0":"markdown","cf5d4ff0":"markdown","2506faf6":"markdown","7402f62a":"markdown","3e8c6b43":"markdown","160949da":"markdown","e34d910d":"markdown","8e4aa885":"markdown","ddda15c9":"markdown","bd978fb8":"markdown","ebf09547":"markdown","e6f6b750":"markdown","62918392":"markdown","940a0b2c":"markdown","53bb7c53":"markdown","5521e565":"markdown","03412385":"markdown","4a5aa0d8":"markdown","f07ce222":"markdown","c9a36450":"markdown","7d053b36":"markdown","a37b324a":"markdown","c4d99a79":"markdown","6f9b6bdd":"markdown","4b2ba883":"markdown","29fcd91c":"markdown","586d2fc2":"markdown"},"source":{"15d21582":"#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom subprocess import check_output\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","8ac5ca80":"#bring in the six packs\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\n","142dc033":"#check the decoration\ndf_train.columns","e94b1cac":"#descriptive statistics summary\ndf_train['SalePrice'].describe()","3823669b":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","587cabf1":"df_train.describe()","8fbb352b":"df_train.shape,df_test.shape","cf40f320":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","0df6f95f":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","9ffb4ede":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","c03a3b5e":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","8847afb8":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","95004d1e":"corrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","477475cb":"sns.barplot(df_train.OverallQual,df_train.SalePrice)","324098b6":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","0ddf8f2a":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","edad8ac6":"print(\"Find most important features relative to target\")\ncorr = df_train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)\n#this you can see at the time of heatmap also.","456cff43":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df_train.select_dtypes(include=['object']).columns\ncategorical_features","397d9327":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","eb56786d":"numerical_features = df_train.select_dtypes(exclude = [\"object\"]).columns","a4ce1221":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df_train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = df_train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = df_train[numerical_features]\ntrain_cat = df_train[categorical_features]","edf77b2d":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))\ntrain_num = train_num.fillna(train_num.median())\nprint(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))","25f3bac3":"from scipy.stats import skew \nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","38ba7a13":"skewness = skewness[abs(skewness)>0.5]\nskewness.index","8e1b75fa":"skew_features = df_train[skewness.index]\nskew_features.columns","ccdd1508":"#we can treat skewness of a feature with the help fof log transformation.so we'll apply the same here.\nskew_features = np.log1p(skew_features)","2e4d0250":"# Create dummy features for categorical values via one-hot encoding\ntrain_cat.shape ","42be3c29":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","c73f0d90":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","36983057":"#deleting points\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","9ab99f97":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","df0ac8d8":"#histogram and normal probability plot\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","c7a9b2c1":"#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","978bfafc":"#transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","a8b13271":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","7cc2c471":"#data transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])","ca50b4bf":"#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","b0623a8d":"#histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","49fdc00e":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1","3105a70a":"#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])","f119e2eb":"#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","4541ea44":"#scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice']);","a50e7dbe":"#scatter plot\nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","1d9d16a4":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","75ec1cd0":"train_cat.shape\ntrain_cat = pd.get_dummies(train_cat)\ntrain_cat.shape\nstr(train_cat.isnull().values.sum())","4b313077":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f1c8536d":"train = pd.concat([train_cat,train_num],axis=1)\ntrain.shape\ny = df_train.SalePrice","a60100ac":"#split the data to train the model \nX_train,X_test,y_train,y_test = train_test_split(df_train,y,test_size = 0.3,random_state= 0)","16d1e77c":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","f30248b3":"X_train.head(3)\nX_train.shape","0bc2a1dd":"n_folds = 5\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","20a8eb59":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)\nprint('rmse on train',rmse_CV_train(lr).mean())\nprint('rmse on train',rmse_CV_test(lr).mean())","9e7a1791":"#plot between predicted values and residuals\nplt.scatter(train_pre, train_pre - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre,test_pre - y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","6e82799c":"# Plot predictions - Real values\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","9df2850f":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train,y_train)\nalpha = ridge.alpha_\nprint('best alpha',alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],cv = 5)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\nprint(\"Ridge RMSE on Training set :\", rmse_CV_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_CV_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)","ed68497f":"coef = pd.Series(ridge.coef_, index = X_train.columns)\n\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n","0332a63a":"# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"black\", marker = \"v\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","a805b5e8":"# Plot predictions - Real values\nplt.scatter(y_train_rdg, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","1b96da21":"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!","fed0bb33":"# Out liars!\n\nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","067bb3fc":"The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n* <b>Histogram<\/b> - Kurtosis and skewness.\n* <b>Normal probability plot<\/b> - Data distribution should closely follow the diagonal that represents the normal distribution.","94bb615e":"![](http:\/\/intranet.tdmu.edu.ua\/data\/kafedra\/internal\/distance\/classes_stud\/english\/1course\/Medical%20statistics\/08.%20Types%20of%20correlation.files\/image013.gif)","b33ee994":"![](http:\/\/78.media.tumblr.com\/7efec0c9403dd3b9eeb181e1681cd3d2\/tumblr_inline_nii9j9oqS61rs8rb9.gif)\n\n[click here for residual info](https:\/\/www.google.co.in\/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiyqvzj_a7XAhVMuY8KHadPCu8QFgguMAI&url=http%3A%2F%2Fstattrek.com%2Fregression%2Fresidual-analysis.aspx%3FTutorial%3DAP&usg=AOvVaw17FeP5R2DKpLpQRyVYWvWG)","85933f3b":"*It is military wisdom to choose the terrain where you will fight. As soon as 'SalePrice' walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. It's just an intense research of an individual, if you know what I mean.*\n\n*According to her profile, we have some common friends. Besides Chuck Norris, we both know 'GrLivArea' and 'TotalBsmtSF'. Moreover, we also have common interests such as 'OverallQual' and 'YearBuilt'. This looks promising!*\n\n*To take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests.*","802eebac":"# 5. Getting hard core","8375816c":"We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. We can live with that, so we'll not do anything.","e709583f":"*'Amazing! If my love calculator is correct, our success probability is 97.834657%. I think we should meet again! Please, keep my number and give me a call if you're free next Friday. See you in a while, crocodile!'*","b7de776e":"### In summary\n\nStories aside, we can conclude that:\n\n* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n\nThat said, let's separate the wheat from the chaff.","9d030e4c":"*'Very well... It seems that your minimum price is larger than zero. Excellent! You don't have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don't know... like, you in the beach... or maybe a selfie in the gym?'*","4434c62d":"The primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","175ab2b9":"<b> Plan\n \n\n1. <b>Understand the problem<\/b>. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n2. <b>Univariable study<\/b>. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n3. <b>Multivariate study<\/b>. We'll try to understand how the dependent variable and independent variables relate.\n4. <b>Basic cleaning<\/b>. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n5. <b>Test assumptions<\/b>. We'll check if our data meets the assumptions required by most multivariate techniques.\n\nNow, it's time to have fun!","11df215a":"# 'SalePrice', her buddies and her interests","eb2babad":"#### Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)","76235c92":"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'...","a8ff9348":"#### Correlation matrix (heatmap style)","615543fc":"In Ayn Rand's novel, 'Atlas Shrugged', there is an often-repeated question: who is John Galt? A big part of the book is about the quest to discover the answer to this question.\n\nI feel Randian now. Who is 'SalePrice'?\n\nThe answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n\nAccording to [Hair et al. (2013)](https:\/\/amzn.to\/2uC3j9p), four assumptions should be tested:\n\n* <b>Normality<\/b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely  on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n* <b>Homoscedasticity<\/b> - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' [(Hair et al., 2013)](https:\/\/amzn.to\/2uC3j9p). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n* <b>Linearity<\/b>- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n* <b>Absence of correlated errors<\/b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\nWhat do you think Elvis would say about this long explanation? 'A little less conversation, a little more action please'? Probably... By the way, do you know what was Elvis's last great hit?\n\n(...)\n\nThe bathroom floor.","1ef8629a":"### In the search for writing 'homoscedasticity' right at the first attempt","f0b66033":"\n\nMost of the features are correlated with each other like Garage Cars and Garage Area, isnt it?\n\n    OverallQual is highly correlated with target feature SalePrice 0.79 can you see. we'll see how it effected the saleprice in below graph.\n\n","8bc5ffb8":"# 2. First things first: analysing 'SalePrice'\n\n'SalePrice' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)\n\nUsing the women analogy, let's build a little story, the story of 'How we met 'SalePrice''.\n\n*Everything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That's a sign that she's there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:*\n\n*'Hi, I'm Kaggly! And you? 'SalePrice'? What a beautiful name! You know 'SalePrice', could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I'd like to apply it to us!'*","8dccc68c":"# Last but not the least, dummy variables","d5e37b72":"Done! Let's check what's going on with 'GrLivArea'.","2c519dd7":"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships.\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.","11e18364":"Get ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!","8cf4b6c0":"### In the search for normality","cf5d4ff0":"*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*","2506faf6":"### Bivariate analysis","7402f62a":"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n* 'FullBath'?? Really? \n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\n\nLet's proceed to the scatter plots.","3e8c6b43":"# 3. Keep calm and work smart","160949da":"*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*\n\n<b>Note<\/b>: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years.","e34d910d":"Until now we just followed our intuition and analysed the variables we thought were important. In spite of our efforts to give an objective character to our analysis, we must say that our starting point was subjective. \n\nAs an engineer, I don't feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. There's a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.\n\nSo, let's overcome inertia and do a more objective analysis.","8e4aa885":"### Univariate analysis","ddda15c9":"It might sound confusing  what is validation data? \n\nwe are still working on train data only, which split was into x_train,x_test.  Here x_test is the validation set we call because \n\nwe are checking how model is performing on our own data(x_test). This explanation is for beginners because they might confuse \n\nwhat is the difference between test data and validation data.\n\nYou can see we plot the graph between residuals and predicated values.\nwhat is residual, you can the find the clear explanation [here ](http:\/\/stattrek.com\/regression\/residual-analysis.aspx)","bd978fb8":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell.\n\n*Avada kedavra!*","ebf09547":"Tastes like skewness... *Avada kedavra!*","e6f6b750":"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'.","62918392":"Ok, now we are dealing with the big boss. What do we have here?\n\n* Something that, in general, presents skewness.\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'.","940a0b2c":"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!","53bb7c53":"Easy mode.","5521e565":"### Relationship with categorical features","03412385":"# 4. Missing data\n\nImportant questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?\n\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.","4a5aa0d8":"Next, please...","f07ce222":"### Relationship with numerical variables","c9a36450":"### The 'plasma soup'\n\n'In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.' (source: http:\/\/umich.edu\/~gs265\/bigbang.htm) \n\nTo explore the universe, we will start with some practical recipes to make sense of our 'plasma soup':\n* Correlation matrix (heatmap style).\n* 'SalePrice' correlation matrix (zoomed heatmap style).\n* Scatter plots between the most correlated variables (move like Jagger style).","7d053b36":"*'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a <b>strong linear (exponential?)<\/b> reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.*","a37b324a":"*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a <b>linear relationship.<\/b>*\n\n*And what about 'TotalBsmtSF'?*","c4d99a79":"How 'SalePrice' looks with her new clothes:\n\n* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.","6f9b6bdd":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.","4b2ba883":"Variable transformation","29fcd91c":"#### 'SalePrice' correlation matrix (zoomed heatmap style)","586d2fc2":"We already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'."}}