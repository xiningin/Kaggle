{"cell_type":{"46db5289":"code","ee1897e9":"code","748b4d03":"code","936b7a07":"code","76041dcd":"code","32f27b1a":"code","26915f61":"code","74a16881":"code","89de0951":"code","d54ab0a0":"code","6eb92d06":"code","eed37f0c":"code","859272ae":"code","d3eb25da":"code","f4333599":"code","77754d89":"code","9f80e27b":"code","e25b8479":"code","8b0c7400":"code","8ecd1087":"code","ae4e07d6":"code","e94f7e24":"code","a69f3675":"code","9f19ce6c":"code","5a923239":"code","6b2c944d":"code","15185344":"code","fdea0d00":"code","4857c952":"code","281a4fa2":"code","f43c0c33":"code","93e0c632":"code","9ce0f217":"code","99fbf895":"code","a390f03d":"code","d14973ed":"code","f7cc6277":"code","16771b3f":"code","6c067060":"code","c1ace8ea":"code","543bb67d":"code","fab7597c":"code","1de5728e":"code","b4bb25bb":"code","3b833b6a":"code","75bb3d4e":"code","a3c8a29d":"code","ffbea49b":"code","76a1f1f1":"code","00c5f778":"code","0c1d8399":"code","6a12a852":"code","767c420b":"code","e63c22f9":"code","083152dd":"code","0a3dfff4":"code","62f29003":"code","d4d8e211":"code","26c4f645":"code","41f20364":"code","fa6d6c91":"code","85ff7ec9":"code","d7f73584":"code","66733395":"code","08ebeb70":"code","0e5c20d0":"code","329da84e":"code","fe90e6b3":"code","8ae57e7c":"code","0807ac7a":"code","2802f74f":"code","c7df8d8f":"code","53dc7c11":"markdown","af685343":"markdown","138fb753":"markdown","1ce68a88":"markdown","d299870f":"markdown","63865405":"markdown","f75d1f22":"markdown","1cd75252":"markdown","18c907bc":"markdown","6c842045":"markdown","1bc067e9":"markdown","ece8b8e9":"markdown","cab66c22":"markdown","59cb1522":"markdown","9a05477f":"markdown","42b6ae44":"markdown","8e7541c8":"markdown","50834c6e":"markdown","95fc74d7":"markdown","b764ed4b":"markdown","b1f8f077":"markdown","8f8f236b":"markdown","f65fa787":"markdown","b386c2d1":"markdown","a3982b5b":"markdown","7b3f96c2":"markdown","96c16830":"markdown","9e952dfd":"markdown","02828b85":"markdown","10aa3032":"markdown","ce1a7ee3":"markdown","7fb5941c":"markdown","515cb9f3":"markdown","3aa979b4":"markdown","54e7c7fb":"markdown","6cd2e5be":"markdown","e35a2d2d":"markdown","017b07ed":"markdown","87451df0":"markdown","b9916e6e":"markdown","af107f91":"markdown","26d9a866":"markdown","a3782965":"markdown","ac05ed9d":"markdown","85c8ac03":"markdown","b67fdcb1":"markdown","281646ce":"markdown"},"source":{"46db5289":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee1897e9":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nfrom statistics import mode\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","748b4d03":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","936b7a07":"train.head()","76041dcd":"#looking at target feature \nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train.Survived)\nplt.title('Number of passenger Survived');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Sex\", data=train)\nplt.title('Number of passenger Survived');","32f27b1a":"#realising null values on our training set\nplt.style.use('seaborn')\nplt.figure(figsize=(10,5))\nsns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')\nplt.title('Null Values in Training Set');","26915f61":"#Analysing Pclass\nplt.figure(figsize=(15,5))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1,2,1)\nsns.countplot(train['Pclass'])\nplt.title('Count Plot for PClass');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)\nplt.title('Number of passenger Survived');\n","74a16881":"pclass1 = train[train.Pclass == 1]['Survived'].value_counts(normalize=True).values[0]*100\npclass2 = train[train.Pclass == 2]['Survived'].value_counts(normalize=True).values[1]*100\npclass3 = train[train.Pclass == 3]['Survived'].value_counts(normalize=True).values[1]*100\n\nprint(\"View of some satistical data!\\n\")\nprint(\"Pclaas-1: {:.1f}% People Survived\".format(pclass1))\nprint(\"Pclaas-2: {:.1f}% People Survived\".format(pclass2))\nprint(\"Pclaas-3: {:.1f}% People Survived\".format(pclass3))\n\n","89de0951":"train['Age'].hist(bins=40)\nplt.title('Age Distribution');","d54ab0a0":"# set plot size\nplt.figure(figsize=(15, 3))\n\n# plot a univariate distribution of Age observations \nsns.distplot(train[(train[\"Age\"] > 0)].Age, kde_kws={\"lw\": 3}, bins = 50)\n\n# set titles and labels\nplt.title('Distrubution of passengers age',fontsize= 14)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n# clean layout\nplt.tight_layout()","6eb92d06":"plt.figure(figsize=(15, 3))\n\n# Draw a box plot to show Age distributions with respect to survival status.\nsns.boxplot(y = 'Survived', x = 'Age', data = train,\n     palette=[\"#3f3e6fd1\", \"#85c6a9\"], fliersize = 0, orient = 'h')\n\n# Add a scatterplot for each category.\nsns.stripplot(y = 'Survived', x = 'Age', data = train,\n     linewidth = 0.6, palette=[\"#3f3e6fd1\", \"#85c6a9\"], orient = 'h')\n\nplt.yticks( np.arange(2), ['drowned', 'survived'])\nplt.title('Age distribution grouped by surviving status (train data)',fontsize= 14)\nplt.ylabel('Passenger status after the tragedy')\nplt.tight_layout()\n","eed37f0c":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['SibSp'])\nplt.title('Number of siblings\/spouses aboard');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"SibSp\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');\n","859272ae":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.countplot(train['Embarked'])\nplt.title('Number of Port of embarkation');\n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Survived\", hue=\"Embarked\", data=train)\nplt.legend(loc='right')\nplt.title('Number of passenger Survived');","d3eb25da":"sns.heatmap(train.corr(), annot=True)\nplt.title('Corelation Matrix');","f4333599":"corr = train.corr()\nsns.heatmap(corr[((corr >= 0.3) | (corr <= -0.3)) & (corr != 1)], annot=True, linewidths=.5, fmt= '.2f')\nplt.title('Configured Corelation Matrix');\n","77754d89":"sns.catplot(x=\"Embarked\", y=\"Fare\", kind=\"violin\", inner=None,\n            data=train, height = 6, order = ['C', 'Q', 'S'])\nplt.title('Distribution of Fare by Embarked')\nplt.tight_layout()","9f80e27b":"sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"swarm\", data=train, height = 6)\n\nplt.tight_layout()","e25b8479":"sns.catplot(x=\"Pclass\", y=\"Fare\",  hue = \"Survived\", kind=\"swarm\", data=train, \n                                    palette=[\"#3f3e6fd1\", \"#85c6a9\"], height = 6)\nplt.tight_layout()","8b0c7400":"train.isnull().sum()","8ecd1087":"test.isnull().sum()","ae4e07d6":"sns.heatmap(train.corr(), annot=True)","e94f7e24":"train.loc[train.Age.isnull(), 'Age'] = train.groupby(\"Pclass\").Age.transform('median')\n\n\n#Same thing for test set\ntest.loc[test.Age.isnull(), 'Age'] = test.groupby(\"Pclass\").Age.transform('median')","a69f3675":"train.Embarked.value_counts()","9f19ce6c":"train['Embarked'] = train['Embarked'].fillna(mode(train['Embarked']))\n\n#Applying the same technique for test set\ntest['Embarked'] = test['Embarked'].fillna(mode(test['Embarked']))\n","5a923239":"train['Fare']  = train.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\ntest['Fare']  = test.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","6b2c944d":"train.Cabin.value_counts()","15185344":"train['Cabin'] = train['Cabin'].fillna('U')\ntest['Cabin'] = test['Cabin'].fillna('U')","fdea0d00":"train.Sex.unique()","4857c952":"train['Sex'][train['Sex'] == 'male'] = 0\ntrain['Sex'][train['Sex'] == 'female'] = 1\n\ntest['Sex'][test['Sex'] == 'male'] = 0\ntest['Sex'][test['Sex'] == 'female'] = 1","281a4fa2":"train.Embarked.unique()","f43c0c33":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntrain = train.join(temp)\ntrain.drop(columns='Embarked', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ntest = test.join(temp)\ntest.drop(columns='Embarked', inplace=True)","93e0c632":"train.columns","9ce0f217":"train.Cabin.tolist()[0:20]","99fbf895":"train['Cabin'] = train['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntest['Cabin'] = test['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","a390f03d":"train.Cabin.unique()","d14973ed":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ntrain['Cabin'] = train['Cabin'].map(cabin_category)\ntest['Cabin'] = test['Cabin'].map(cabin_category)","f7cc6277":"train.Name","16771b3f":"train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ntest['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","6c067060":"train['Name'].unique().tolist()","c1ace8ea":"train.rename(columns={'Name' : 'Title'}, inplace=True)\ntrain['Title'] = train['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n                                      \ntest.rename(columns={'Name' : 'Title'}, inplace=True)\ntest['Title'] = test['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","543bb67d":"train['Title'].value_counts(normalize = True) * 100","fab7597c":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(train[['Title']]).toarray())\ntrain = train.join(temp)\ntrain.drop(columns='Title', inplace=True)\n\ntemp = pd.DataFrame(encoder.transform(test[['Title']]).toarray())\ntest = test.join(temp)\ntest.drop(columns='Title', inplace=True)","1de5728e":"train['familySize'] = train['SibSp'] + train['Parch'] + 1\ntest['familySize'] = test['SibSp'] + test['Parch'] + 1","b4bb25bb":"# Drop redundant features\ntrain = train.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\ntest = test.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)","3b833b6a":"train.head()","75bb3d4e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'], test_size = 0.2, random_state=2)\n","a3c8a29d":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))","ffbea49b":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=10000, C=50)\nlogreg.fit(X_train, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test, y_test)))","76a1f1f1":"print(logreg.intercept_)\nprint(logreg.coef_)","00c5f778":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)","0c1d8399":"logreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train_scaled, y_train)\n\n#R-Squared Score\nprint(\"R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))","6a12a852":"from sklearn.neighbors import KNeighborsClassifier\n\nknnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Train the model using the training sets\nknnclf.fit(X_train, y_train)\ny_pred = knnclf.predict(X_test)","767c420b":"from sklearn.metrics import accuracy_score\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","e63c22f9":"knnclf = KNeighborsClassifier(n_neighbors=7)\n\n# Training the model using the scaled training sets\nknnclf.fit(X_train_scaled, y_train)\ny_pred = knnclf.predict(X_test_scaled)","083152dd":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","0a3dfff4":"from sklearn.svm import LinearSVC\n\nsvmclf = LinearSVC(C=50)\nsvmclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test, y_test)))\n","62f29003":"svmclf = LinearSVC()\nsvmclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))","d4d8e211":"from sklearn.svm import SVC\n\nsvcclf = SVC(gamma=0.1)\nsvcclf.fit(X_train, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test, y_test)))","26c4f645":"svcclf = SVC(gamma=50)\nsvcclf.fit(X_train_scaled, y_train)\n\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))\n","41f20364":"from sklearn.tree import DecisionTreeClassifier\n\ndtclf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))","fa6d6c91":"from sklearn.ensemble import RandomForestClassifier\nrfclf = RandomForestClassifier(random_state=2)","85ff7ec9":"# Set our parameter grid\nparam_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3, 5, 7]    \n}","d7f73584":"from sklearn.model_selection import GridSearchCV\n\nrandomForest_CV = GridSearchCV(estimator = rfclf, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)","66733395":"randomForest_CV.best_params_","08ebeb70":"rf_clf = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 100)\n\nrf_clf.fit(X_train, y_train)","0e5c20d0":"predictions = rf_clf.predict(X_test)","329da84e":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) * 100","fe90e6b3":"#Linear Model\nprint(\"Linear Model R-Squared for Train set: {:.3f}\".format(linreg.score(X_train, y_train)))\nprint(\"Linear Model R-Squared for test set: {:.3f}\" .format(linreg.score(X_test, y_test)))\nprint()\n\n#Logistic Regression\nprint(\"Logistic Regression R-Squared for Train set: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\nprint(\"Logistic Regression R-Squared for test set: {:.3f}\" .format(logreg.score(X_test_scaled, y_test)))\nprint()\n\n#KNN Classifier\nprint(\"KNN Classifier Accuracy:\",accuracy_score(y_test, y_pred))\nprint()\n\n#SVM\nprint('SVM Accuracy on training set: {:.2f}'\n     .format(svmclf.score(X_train_scaled, y_train)))\nprint('SVM Accuracy on test set: {:.2f}'\n     .format(svmclf.score(X_test_scaled, y_test)))\nprint()\n\n#Kerelize SVM\nprint('SVC Accuracy on training set: {:.2f}'\n     .format(svcclf.score(X_train_scaled, y_train)))\nprint('Accuracy on test set: {:.2f}'\n     .format(svcclf.score(X_test_scaled, y_test)))\nprint()\n\n#Decision Tree\nprint('Accuracy of Decision Tree on training set: {:.2f}'\n     .format(dtclf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree on test set: {:.2f}'\n     .format(dtclf.score(X_test, y_test)))\nprint()\n\n#Random Forest\nprint('Random Forest Accuracy:{:.3f}'.format(accuracy_score(y_test, predictions) * 100))","8ae57e7c":"scaler = MinMaxScaler()\n\ntrain_conv = scaler.fit_transform(train.drop(['Survived', 'PassengerId'], axis=1))\ntest_conv = scaler.transform(test.drop(['PassengerId'], axis = 1))\n","0807ac7a":"svcclf = SVC(gamma=50)\nsvcclf.fit(train_conv, train['Survived'])","2802f74f":"test['Survived'] = svcclf.predict(test_conv)","c7df8d8f":"test[['PassengerId', 'Survived']].to_csv('MySubmission1.csv', index = False)","53dc7c11":"Looks like single person Non-survived count is almost double than survived, while others have 50-50 % ratio","af685343":"it's clear from the score that linear regression doesn't makes sence","138fb753":"# MinMaxScaler","1ce68a88":"\nNow Looking at Port of embarkation","d299870f":"# Kernelize SVM\n","63865405":"So many different values let's place missing values with U as \"Unknown\"","f75d1f22":"Looking in to relationships among dataset","1cd75252":"# Linear Regression\n\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.","18c907bc":"looking at Number of siblings\/spouses aboard","6c842045":"Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class","1bc067e9":"Trying on scaled data","ece8b8e9":"# Feature Engineering","cab66c22":"Printing the optimal hyperparameters set","59cb1522":"Age column has non-uniform data and many outliers","9a05477f":"# KNN Classifier\n\nK Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms.KNN algorithm used for both classification and regression problems.","42b6ae44":"****y-intercept and coefficients","8e7541c8":"Each passenger Name value contains the title of the passenger which can be extracted and discovered. To create new variable \"Title\":\n\n* Using method 'split' by comma to divide Name in two parts and save the second part\n* Splitting saved part by dot and save first part of the result\n* To remove spaces around the title using 'split' method\n* Visualizing, how many passengers hold each title,by using countplot.","50834c6e":"As maximum values in train set is S let's replace it with the null values","95fc74d7":"But Sibsp is the number of siblings \/ spouses aboard the Titanic, and Parch is the number of parents \/ children aboard the Titanic. So, another straightforward feature to engineer is the size of each family aboard!","b764ed4b":"Also, corr(Fare, Pclass) is the highest correlation in absolute numbers for 'Fare', so using Pclass again to impute the missing values!","b1f8f077":"Converting to numeric","8f8f236b":"# Random Forest\nRandom Forest is an ensembling learning algorithm which combines decision trees in order to increase performance and avoid overfitting.","f65fa787":"Can't say much!","b386c2d1":"> Dateset is completely ready now!","a3982b5b":"Fare vs Embarked","7b3f96c2":"That's lot's of title. So, bundling them","96c16830":"# Decision Tree","9e952dfd":"On scaled data","02828b85":"# Logistic Regression\n\nAs our target variable is discrete value(i.e 0 and 1) logistic regression is more likely to fit well the model","10aa3032":"# Exploratory Data Analysis","ce1a7ee3":"We can get the alphabets by running regular expression","7fb5941c":"We will be dealling with null values later on.","515cb9f3":"# Scaling data and re-training the model","3aa979b4":"# Submitting the solutions\nSVC Model","54e7c7fb":"# Handling Missing Values","6cd2e5be":"# All model Accuracy Score","e35a2d2d":"To visualize two age distributions, grouped by surviving status I am using boxlot and stripplot showed together:\n","017b07ed":"Sex is categorical data so we can replace male to 0 and femail to 1","87451df0":"# Support Vector Machine(SVM)\n","b9916e6e":"That increases the accuracy a lot!","af107f91":"Encoding with OneHotEncoder technique","26d9a866":"Support Vector Machine with RBF kernel\u00b6","a3782965":"# Hyperparameter Tuning\u00b6\n\nBelow we set the hyperparameter grid of values with 4 lists of values:\n\n*  'criterion' : A function which measures the quality of a split.\n*  'n_estimators' : The number of trees of our random forest.\n* 'max_features' : The number of features to choose when looking for the best way of splitting.\n* 'max_depth' : the maximum depth of a decision tree.","ac05ed9d":"We can observe that the distribution of prices for the second and third class is very similar. The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.\n\nLet's add colours to our points to indicate surviving status of passenger (there will be only data from training part of the dataset):","85c8ac03":"Pclass is also a good feature to train our model.\nlooking at the Age column\n","b67fdcb1":"The wider fare distribution among passengers who embarked in Cherbourg. It makes scence - many first-class passengers boarded the ship here, but the share of third-class passengers is quite significant.\n\n\n\nThe smallest variation in the price of passengers who boarded in q. Also, the average price of these passengers is the smallest, I think this is due to the fact that the path was supposed to be the shortest + almost all third-class passengers.\n","281646ce":"Fare vs Pclass"}}