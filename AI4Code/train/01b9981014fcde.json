{"cell_type":{"d28db502":"code","79dcd8fe":"code","0e4aa042":"code","787fce34":"code","c24871ae":"code","db97fa2c":"code","ac4fc8c1":"code","ac0f6c52":"code","dc28d34e":"code","2b30518b":"code","332db475":"code","7e5f7de4":"code","009c12f4":"code","c8aef0d7":"markdown","4893359b":"markdown","7327c55e":"markdown"},"source":{"d28db502":"from math import sqrt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error #","79dcd8fe":"file = '..\/input\/solarpanelspower\/PV_Elec_Gas3.csv'\n#..\/input\/solarpanelspower\/PV_Elec_Gas3.csv\ndf = pd.read_csv(file)\nsolarpower = pd.read_csv(file ,skiprows=1 ,names = ['date','cum_power'], sep=',',usecols = [0,1])\nprint(solarpower.head(2))","0e4aa042":"# make a column with the daily power (stationary)\nsolarpower['day_power']=0.0\nfor index in range(solarpower.index[solarpower.shape[0]-1], 0, -1):\n    power = solarpower.cum_power[index] - solarpower.cum_power[index-1]\n    solarpower.at[index, 'day_power']= power\n#replace the day power of day 0 with the same power as day1\nsolarpower.at[0,'day_power'] = solarpower.at[1,'day_power']\nprint(solarpower.head(2))\nprint(solarpower.describe())","787fce34":"plt.plot(solarpower.day_power)\nplt.xticks(color='aqua')\nplt.yticks(color='aqua')\nplt.show()","c24871ae":"'''simple exponential smoothing go back to last N values\n y_t = a * y_t + a * (1-a)^1 * y_t-1 + a * (1-a)^2 * y_t-2 + ... + a*(1-a)^n * y_t-n  : c2018:Yogesh Chandra'''\n\n\ndef exponential_smoothing(panda_series, alpha_value):\n    ouput=sum([alpha_value * (1 - alpha_value) ** i * x for i, x in enumerate(reversed(panda_series))])\n    return ouput\n\ndef make_exp_smooth_col(my_df, alpha):\n    my_df['smooth_power'] = 0.0\n    for index in range(0, my_df.index[my_df.shape[0]-1], 1): \n        powers = my_df.day_power.head(index).values\n        new_power = exponential_smoothing(powers,alpha)   # set alpha-value!!!!!!!!!\n        my_df.at[index, 'smooth_power'] = new_power\n        my_df.dropna()\n    return my_df\n\nmy_df = solarpower.copy() #make a working copy\nRMSE_list = []\nsteps = np.arange(0.5,0.1,-0.05)\nfor alpha in steps:\n    make_exp_smooth_col(my_df, alpha)\n    RMSE = sqrt(mean_squared_error(my_df.day_power, my_df.smooth_power))\n    print('RMSE %.3f, alpha %.3f' %(RMSE, alpha))\n    RMSE_list.append(RMSE)\n    #print(my_df.smooth_power.describe())\n    \nplt.figure()\nplt.plot(steps, RMSE_list)\nplt.title('RMSE for line-up alpha in exponential smoothing')\nplt.xticks(color='crimson')\nplt.yticks(color='crimson')\nplt.xlabel('Freq (days)',color='crimson')\nplt.ylabel('amplitude |Y(freq)|',color='crimson')\nplt.show()\n    ","db97fa2c":"# The optimum alpha is 0.2\n'''simple exponential smoothing go back to last N values\n y_t = a * y_t + a * (1-a)^1 * y_t-1 + a * (1-a)^2 * y_t-2 + ... + a*(1-a)^n * y_t-n  : c2018:Yogesh Chandra'''\n\ndef exponential_smoothing(panda_series, alpha_value):\n    ouput=sum([alpha_value * (1 - alpha_value) ** i * x for i, x in enumerate(reversed(panda_series))])\n    return ouput\nsolarpower['smooth_power'] = 0.0\nfor index in range(0, solarpower.index[solarpower.shape[0]-1], 1): \n    powers = solarpower.day_power.head(index).values\n    new_power = exponential_smoothing(powers,0.2)   # set alpha-value 0.025!!!!!!!!!\n    solarpower.at[index, 'smooth_power'] = new_power","ac4fc8c1":"plt.figure(figsize=(15,7))\nplt.plot(solarpower.day_power[:730])\nplt.plot(solarpower.smooth_power[:730])\nplt.xticks(color='aqua')\nplt.yticks(color='aqua')\nplt.grid()\nplt.show()","ac0f6c52":"solarpower= solarpower.dropna()","dc28d34e":"#signal autocorrellation on day_power\nx = np.array(solarpower.day_power[:].values)\nacf = []\nfor i in range(1, len(x)-180):\n    acf.append(np.corrcoef(x[:-i], x[i:])[0,1])\nplt.plot(acf)\nplt.xticks(color='aqua')\nplt.yticks(color='aqua')\nplt.grid()\nplt.show()","2b30518b":"#stats autocorrellation day_power \ny = np.array(solarpower.day_power[:].values)\n\nyunbiased = y-np.mean(y)\nynorm = np.sum(yunbiased**2)\nacor = np.correlate(yunbiased, yunbiased, \"same\")\/ynorm\n# use only second half\nacor = acor[len(acor)\/\/2:]\n\nplt.plot(acor)\nplt.show()","332db475":"'''autocorrellation from https:\/\/stackoverflow.com\/questions\/643699\/how-can-i-use-np-correlate-to-do-autocorrelation\nsee Jason code'''\n\ndef autocorr1(x,lags):\n    '''np.corrcoef, partial'''\n\n    corr=[1. if l==0 else np.corrcoef(x[l:],x[:-l])[0][1] for l in lags]\n    return np.array(corr)\n\ndef autocorr2(x,lags):\n    '''manualy compute, non partial'''\n\n    mean=np.mean(x)\n    var=np.var(x)\n    xp=x-mean\n    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])\/len(x)\/var for l in lags]\n\n    return np.array(corr)\n\ndef autocorr3(x,lags):\n    '''fft, pad 0s, non partial'''\n\n    n=len(x)\n    # pad 0s to 2n-1\n    ext_size=2*n-1\n    # nearest power of 2\n    fsize=2**np.ceil(np.log2(ext_size)).astype('int')\n\n    xp=x-np.mean(x)\n    var=np.var(x)\n\n    # do fft and ifft\n    cf=np.fft.fft(xp,fsize)\n    sf=cf.conjugate()*cf\n    corr=np.fft.ifft(sf).real\n    corr=corr\/var\/n\n\n    return corr[:len(lags)]\n\ndef autocorr4(x,lags):\n    '''fft, don't pad 0s, non partial'''\n    mean=x.mean()\n    var=np.var(x)\n    xp=x-mean\n\n    cf=np.fft.fft(xp)\n    sf=cf.conjugate()*cf\n    corr=np.fft.ifft(sf).real\/var\/len(x)\n\n    return corr[:len(lags)]\n\ndef autocorr5(x,lags):\n    '''np.correlate, non partial'''\n    mean=x.mean()\n    var=np.var(x)\n    xp=x-mean\n    corr=np.correlate(xp,xp,'full')[len(x)-1:]\/var\/len(x)\n\n    return corr[:len(lags)]\n\n\nif __name__=='__main__':\n\n    y=np.array(solarpower.day_power[:].values)\n    #y=np.array(y).astype('float')\n\n    lags=range(7*365)\n    fig,ax=plt.subplots(figsize=(15,15))\n\n    for funcii, labelii in zip([autocorr1, autocorr2, autocorr3, autocorr4,\n        autocorr5], ['np.corrcoef, partial', 'manual, non-partial',\n            'fft, pad 0s, non-partial', 'fft, no padding, non-partial',\n            'np.correlate, non-partial']):\n\n        cii=funcii(y,lags)\n        #print(labelii)\n        #print(cii)\n        ax.plot(lags,cii,label=labelii)\n\n    ax.set_xlabel('lag')\n    ax.set_ylabel('correlation coefficient')\n    ax.legend()\n    plt.savefig('autocorrellation1.pdf')\n    plt.show()","7e5f7de4":"'''autocorrellation from https:\/\/stackoverflow.com\/questions\/643699\/how-can-i-use-np-correlate-to-do-autocorrelation\nsee Jason code'''\n\ndef autocorr1(x,lags):\n    '''np.corrcoef, partial'''\n\n    corr=[1. if l==0 else np.corrcoef(x[l:],x[:-l])[0][1] for l in lags]\n    return np.array(corr)\n\ndef autocorr2(x,lags):\n    '''manualy compute, non partial'''\n\n    mean=np.mean(x)\n    var=np.var(x)\n    xp=x-mean\n    corr=[1. if l==0 else np.sum(xp[l:]*xp[:-l])\/len(x)\/var for l in lags]\n\n    return np.array(corr)\n\ndef autocorr3(x,lags):\n    '''fft, pad 0s, non partial'''\n\n    n=len(x)\n    # pad 0s to 2n-1\n    ext_size=2*n-1\n    # nearest power of 2\n    fsize=2**np.ceil(np.log2(ext_size)).astype('int')\n\n    xp=x-np.mean(x)\n    var=np.var(x)\n\n    # do fft and ifft\n    cf=np.fft.fft(xp,fsize)\n    sf=cf.conjugate()*cf\n    corr=np.fft.ifft(sf).real\n    corr=corr\/var\/n\n\n    return corr[:len(lags)]\n\ndef autocorr4(x,lags):\n    '''fft, don't pad 0s, non partial'''\n    mean=x.mean()\n    var=np.var(x)\n    xp=x-mean\n\n    cf=np.fft.fft(xp)\n    sf=cf.conjugate()*cf\n    corr=np.fft.ifft(sf).real\/var\/len(x)\n\n    return corr[:len(lags)]\n\ndef autocorr5(x,lags):\n    '''np.correlate, non partial'''\n    mean=x.mean()\n    var=np.var(x)\n    xp=x-mean\n    corr=np.correlate(xp,xp,'full')[len(x)-1:]\/var\/len(x)\n\n    return corr[:len(lags)]\n\n\nif __name__=='__main__':\n\n    y=np.array(solarpower.smooth_power[:].values)\n    #y=np.array(y).astype('float')\n\n    lags=range(7*365)\n    fig,ax=plt.subplots(figsize=(15,15))\n\n    for funcii, labelii in zip([autocorr1, autocorr2, autocorr3, autocorr4,\n        autocorr5], ['np.corrcoef, partial', 'manual, non-partial',\n            'fft, pad 0s, non-partial', 'fft, no padding, non-partial',\n            'np.correlate, non-partial']):\n\n        cii=funcii(y,lags)\n        #print(labelii)\n        #print(cii)\n        ax.plot(lags,cii,label=labelii)\n\n    ax.set_xlabel('lag')\n    ax.set_ylabel('correlation coefficient')\n    ax.legend()\n    plt.savefig('autocorrellation1_smooth.pdf')\n    plt.show()","009c12f4":"# simple fft \nFs = 7*365.0;  # sampling rate 7\nTs = 1.0\/Fs; # sampling interval\nt = np.arange(0,1,Ts) # time vector\n\nff = 5;   # frequency of the reference signal\ny = np.sin(2*np.pi*ff*t)*5\ny1 = np.array(solarpower.day_power[:7*365].values)\ny2 = np.array(solarpower.smooth_power[:7*365].values)\nn = len(y) # length of the signal\nprint(n,'n')\nk = np.arange(n)\nT = n\/Fs\nfrq = k\/T # two sides frequency range\nfrq = frq[range(n\/\/2)] # one side frequency range\n\nY = np.fft.fft(y)\/n # fft computing and normalization\nyf1 = np.fft.fft(y1)\/(len(y1)) # fft computing and normalization\nyf2 = np.fft.fft(y2)\/(len(y2)) # fft computing and normalization\nprint('len(ffty)', len(Y))\nY = Y[range(n\/\/2)]\nprint('len(Y)',len(Y))\n\nplt.figure(figsize=(15,5))\nm = 20\n\nplt.plot(frq[1:m],abs(Y)[1:m], color='b') # plotting the spectrum\nplt.annotate('reference signal 5days', color='b',xy=(1.5, 4))\nplt.plot(frq[1:m],abs(yf1)[1:m],color='brown')\nplt.annotate('daily power', color='brown',xy=(1.5, 3.5))\nplt.plot(frq[1:m],abs(yf2)[1:m],color='purple')\nplt.annotate('exp. smooth power', color='purple',xy=(1.5, 3))\nplt.xticks(color='crimson')\nplt.yticks(color='crimson')\nplt.xlabel('Freq (days)',color='crimson')\nplt.ylabel('amplitude |Y(freq)|',color='crimson')\n\nplt.show()","c8aef0d7":"Exploring the data of solarpower ","4893359b":"We can see a lot of noise and can try exponential smoothing to filter out the noise. Exponential smoothing is primarily made for forecasting (see Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2014.). But it can be used to smooth data. First we try to find the optimum value of alpha. We do this by calculating the RMSE between day_power and the smoothed power.","7327c55e":"We see the obvious yearly cycle and a lot of noise on top. If we want to use some neural network based prediction then we need some kind of smoothing technique. Neural networks are stochastic and can vary a lot when fitting the model several times."}}