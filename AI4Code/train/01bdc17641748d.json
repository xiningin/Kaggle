{"cell_type":{"33b47fbb":"code","017bb856":"code","10f5dd5f":"code","da4561de":"code","0939f55e":"code","ed5eabc1":"code","1b86629b":"code","9ca44ea0":"code","875dcfda":"code","487e475f":"code","be11ec39":"code","7707daad":"code","28a0344e":"code","27e8348a":"code","15ce36b0":"code","2db71cc8":"code","ecbe43b1":"code","f9f86383":"code","f063607e":"code","66aee542":"code","ff598f64":"code","f0990229":"code","e97c7167":"code","7f73c576":"markdown","8d6a8fb6":"markdown","79578f7b":"markdown","583e6862":"markdown","e7b797a2":"markdown","4123b584":"markdown","56bae3e1":"markdown","f87343a5":"markdown","3c36c331":"markdown","6557ffcc":"markdown","9c8a63c9":"markdown","b1c889cb":"markdown","9d4aae0c":"markdown","32268e26":"markdown","0b8674da":"markdown","d1570db4":"markdown","08310ec5":"markdown","df797c4f":"markdown"},"source":{"33b47fbb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import confusion_matrix,f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import validation_curve","017bb856":"credit_card_dataset = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","10f5dd5f":"def understand_variables(dataset):\n    print(\"Type = \" +str(type(dataset))+\"\\n\")\n    print(\"Shape = \"+str(dataset.shape)+\"\\n\")\n    print(\"Head : \\n\\n\"+str(dataset.head())+\"\\n\\n\")\n    print(\"Columns:\\n\"+str(dataset.columns)+\"\\n\\n\")\n    print(\"No.of unique values :\\n\\n\"+str(dataset.nunique(axis=0))+\"\\n\\n\")\n    print(\"Description :\\n\\n\"+str(dataset.describe())+\"\\n\\n\")\n    \n    #print(dataset.describe(exclude=[np.number]))\n    #Since no categorical variables, no need to have the above line\n    \n    print(\"Null count :\\n\\n\"+str(dataset.isnull().sum()))\n    \nunderstand_variables(credit_card_dataset)","da4561de":"credit_card_dataset['Time'] = ((credit_card_dataset['Time']\/3600)%24).sort_values(ascending=False)","0939f55e":"def variable_distribution_analysis(dataset):\n\n        \n    numerical_features=[feature for feature in dataset.columns if dataset[feature].dtype!='O']\n\n    for feature in numerical_features:\n        plt.figure(figsize=(10,4))\n        try:\n            sns.distplot(dataset[feature])\n            plt.show()\n        except:\n            continue\n\n\n    #sns.pairplot(dataset,kind=\"reg\")\n    #plt.show()\n    \nvariable_distribution_analysis(credit_card_dataset)","ed5eabc1":"for col in list(credit_card_dataset.columns):\n    if col!='Class':\n        plt.figure(figsize=(10,4))\n        sns.boxplot(data=credit_card_dataset,x='Class',y=col)\n        plt.show()","1b86629b":"credit_card_corr = credit_card_dataset.corr()\nplt.figure(figsize=(25,25))\nsns.heatmap(data=credit_card_corr, annot=True,fmt='.1f')","9ca44ea0":"X = credit_card_dataset.drop([\"Class\"],axis=1)\ny = credit_card_dataset[\"Class\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=1,stratify=y)\nforest_model = RandomForestClassifier(n_estimators=20,random_state=1)\nforest_model.fit(X_train, y_train)\nY_pred = pd.Series(forest_model.predict(X_test))\n\nprint(\"Training Accuracy :\", forest_model.score(X_train, y_train))\nprint(\"Testing Accuracy :\", forest_model.score(X_test, Y_pred))\n\nconf = confusion_matrix(y_test, Y_pred)\nprint(\"\\nConfusion matrix\\n\"+str(conf))\nprint(\"\\nF1 score = \"+str(round(f1_score(y_test, Y_pred)*100,2))+\" %\")\nprint(\"\\nClassification report\\n\\n\"+str(classification_report(y_test, Y_pred)))","875dcfda":"features = pd.Series(forest_model.feature_importances_)\nfeatures.index = X_train.columns\nfeatures = features.sort_values(ascending=False)\nprint(\"Feature Importance in Random Forest:\\n\"+ str(features))","487e475f":"from sklearn import datasets\nimport xgboost as xgb\nxgb_classifier = xgb.XGBClassifier(scale_pos_weight=len(y_train[y_train==0])\/len(y_train[y_train==1]),random_state=1,base_score=0.3)","be11ec39":"X = credit_card_dataset.drop([\"Class\"],axis=1)\ny = credit_card_dataset[\"Class\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.245,random_state=1,stratify=y)\n\nxgb_model = xgb_classifier.fit(X_train, y_train)\nxgb_y_pred = xgb_model.predict(X_test)\n#best_preds = np.asarray([np.argmax(line) for line in xgb_y_pred])\nconf = confusion_matrix(y_test, xgb_y_pred)\nprint(\"\\nConfusion matrix\\n\"+str(conf))\nprint(\"\\nF1 score = \"+str(round(f1_score(y_test, xgb_y_pred)*100,2))+\" %\")\nprint(classification_report(y_test, xgb_y_pred))","7707daad":"f = 'gain'\nx = xgb_model.get_booster().get_score(importance_type= f)\n\nprint(\"Feature importance for xgboost model : \\n\")\n{k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)}","28a0344e":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc","27e8348a":"## Get predcition probabilities (range of 0 to 1) instead of binary values (0\/1)\nxgb_y_pred_prob = xgb_model.predict_proba(X_test)","15ce36b0":"y_pred_prob_df = pd.DataFrame(xgb_y_pred_prob)[[1]]\nprint(y_pred_prob_df.sort_values(by=[1],ascending=False).head(118)) # 117 predicted as fraud, and last probability in the list (index = 44160) is for a class predicted as non-fraud\n\n#This verifies the fact that probablity threshold is 0.5. We can look at how recall changes with change in probablity threshold\n\nxgb_y_pred_prob = pd.DataFrame(xgb_y_pred_prob)","2db71cc8":"import sklearn\nauprc = sklearn.metrics.average_precision_score(y_test, xgb_y_pred)\nprint(auprc)","ecbe43b1":"xgb_precision, xgb_recall,threshold = precision_recall_curve(y_test, y_pred_prob_df)\nxgb_f1, xgb_auc = f1_score(y_test, xgb_y_pred), auc(xgb_recall, xgb_precision)","f9f86383":"print(xgb_f1)\nprint(xgb_auc)","f063607e":"#threshold = np.append(threshold,[1])\nprecision_recall_threshold_df = pd.DataFrame([xgb_precision, xgb_recall,threshold]).transpose()\nprecision_recall_threshold_df.columns = ['Precision','Recall','Threshold']\nprecision_recall_threshold_df['F1-Score'] = 2*precision_recall_threshold_df['Precision']*precision_recall_threshold_df['Recall']\/(precision_recall_threshold_df['Precision']+precision_recall_threshold_df['Recall'])\nprint(precision_recall_threshold_df)","66aee542":"plt.figure(figsize=(20,4))\nsns.lineplot(x=\"Recall\", y=\"Precision\", data=precision_recall_threshold_df)\nplt.show()","ff598f64":"plt.figure(figsize=(20,4))\nsns.lineplot(x=\"Threshold\", y=\"F1-Score\", data=precision_recall_threshold_df)\nplt.show()\n\n## We look at thresholds where Precision is >=0.85 and Recall >= 0.9\n\nprint(precision_recall_threshold_df[(precision_recall_threshold_df['Precision']>=0.85) & (precision_recall_threshold_df['Recall']>=0.9)].sort_values(by='Recall',ascending=False))\n\n##Then, after the threshold observations, we select threshold with highest f1-score (since increasing recall will decrease precision drastically)\nmax_f1_threshold = (precision_recall_threshold_df[precision_recall_threshold_df[\"F1-Score\"]==max(precision_recall_threshold_df[\"F1-Score\"])][\"Threshold\"]).iloc[0]","f0990229":"## Classify as fraud for probability greater than max_f1_threshold (~0.33)\n\ny_pred_prob_df.columns = ['Prob of 1']\ny_pred_prob_df['class'] = [1 if float(prob)>=max_f1_threshold else 0 for prob in y_pred_prob_df['Prob of 1']]","e97c7167":"conf = confusion_matrix(y_test, y_pred_prob_df['class'])\nprint(\"\\nConfusion matrix\\n\"+str(conf))\nprint(\"\\nF1 score = \"+str(round(f1_score(y_test, y_pred_prob_df['class'])*100,2))+\" %\")\nprint(classification_report(y_test, y_pred_prob_df['class']))","7f73c576":"# Model training and validation","8d6a8fb6":"# Finally, we obtain a credit card fraud detection model which predicts fraud transactions with precision of 92% and recall of 92%\n\nFeel free to comment below wtth your thoughts, suggestions, feedback, improvements","79578f7b":"No strong correlations (>0.7 or <-0.7) found for any 2 variables","583e6862":"# Steps","e7b797a2":"# Now, let us create Precision-Recall Curve to get the ideal probablity threshold for positive classification.\n\nThe trade-off which we must do is that False Negative (Labelling fraud as non-fraud) should decrease at the expense of increasing False Positive (Labelling non-fraud as fraud) to some extent","4123b584":"xgboost has better f1 score than random forest. In xgboost, precision may be lower, but recall is much higher than random forest. \n\nRecall is more important than precision in case of credit card fraud detection. \n\nThis is because it is acceptable to an extent that a non-fraud transaction is labelled as fraud (can be resolved by calling up and asking for additional details from the customer\/bank). \n\nHowever, it is dangerous to label a fraud as non-fraud (in such scenario, a fraud transaction will go ahead without any checks)","56bae3e1":"Variable observations : \n\n* No missing values. \n* PCA variables seem to be normalized (mean is approx 0 for these)\n* Large data set size\n* All are numeric values","f87343a5":"Next, we attempt to understand individual variable distribution (Checking range, skewness etc..)","3c36c331":"Now, let's do Correlation check","6557ffcc":"Boxplot provides only 1 inference i.e. what is outlier for non-fraud (class=0) transaction can be an inlier for fraud (class=1) transaction","9c8a63c9":"# Now, lets focus on Feature Engineering #\n\nTime should be converted to hour of the day (24-hr clock), since there's a possibility that fraud transactions may occur more often at certain times of the day (maybe at 3am for example)","b1c889cb":"Now, let's use **random forest** to train and test model\n\nSince, we are working with imbalanced dataset, we will set stratify=y in train_test_split. It ensures that both training and testing datasets have same ratio of all label classes (0 and 1 in our case) as the complete imput dataset\n\nMoreover, due to class imbalance, we will use F1-score as measure of performance instead of testing accuracy","9d4aae0c":"We observe that using a threshold of 0.33 increases recall from 0.89 to 0.92, which precsion hasn't changed significantly (remains 0.92)\n\nHence, we will use this threshold for our model","32268e26":"First, let's import all require libraries and read the file","0b8674da":"This is good F1-score.\n\nNow, let's check out **xgboost** performance","d1570db4":"# Credit card fraud is when someone uses your credit card or credit account to make a purchase you didn't authorize.\n\nThe purpose may be to obtain goods or services, or to make payment to another account which is controlled by a criminal.\n\nIn 2018, unauthorised financial fraud losses across payment cards and remote banking totalled \u00a3844.8 million in the United Kingdom\n\nHence, we need to create a model which can detect fraud transactions and help prevent major financial loss","08310ec5":"# Function for variable understanding","df797c4f":"* Time makes sense (after conversion to hour of the day) since number of transactions are higher between 10am-11pm, compared to 12am-9am. \n\n* All the PCA variables (V1-V28) are centred at 0, not very skewed, have a lower range and hence seem normalized (as expected of a PCA variable) \n\n* From amount distribution, we can conclude that most transactions are of less amount (mean = 88 as per understanding_variable function), but there is atleast one instances when amount is around 25,000 (as per understanding_variable max is 25691.16). This distribution aligns with the understanding_variable function output\n\n* The target variable class (last plot) is highly imbalanced\n\nNow, let us check outlier for each variable (hue as class) using Boxplot"}}