{"cell_type":{"0f4bcaf4":"code","7d4fff5f":"code","739f47d7":"code","64fb92b7":"code","fa8be60f":"code","8f39e806":"code","2e4680dd":"code","e7a3148b":"code","ade114f1":"code","eeb8ad05":"code","7da56aeb":"code","37812e67":"code","fe1cc19c":"code","3cbbb028":"code","bb43f8e7":"code","706efc0b":"code","1b122733":"code","46f0dcf4":"code","df191118":"code","430dfe91":"code","bac9700d":"code","e33d1077":"code","cb22aacd":"code","4d514efe":"code","4f865855":"code","fd6f0086":"code","8a0609a7":"code","56271377":"code","31f41ec8":"code","5284d9d5":"code","72c2b17b":"code","750a02ea":"code","dc51f3eb":"code","85dc8527":"code","58201f00":"markdown","8733798a":"markdown","81243644":"markdown","9d59dd5f":"markdown","04960909":"markdown","ffb90f0b":"markdown","da840c06":"markdown","9617b77b":"markdown","03690536":"markdown","4a55a7be":"markdown","4d71b80d":"markdown","66058ae7":"markdown","785f6ed3":"markdown","4233c901":"markdown"},"source":{"0f4bcaf4":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport PIL\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","7d4fff5f":"autotune = tf.data.experimental.AUTOTUNE\nbatch = 32 * strategy.num_replicas_in_sync\ntarget = [176, 208]\nepochs = 100\nnum_classes=4","739f47d7":"train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\",\n    validation_split=0.2,\n    subset=\"training\",\n    seed=10,\n    image_size=target,\n    batch_size=batch,\n    color_mode=\"rgb\"\n)\n\nval_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\",\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=10,\n    image_size=target,\n    batch_size=batch,\n    color_mode=\"rgb\"\n)","64fb92b7":"class_names = train_dataset.class_names\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    #Greyscale\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","fa8be60f":"class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\npath=\"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/\"\nimport subprocess\nimport re\n\nplt.figure(figsize=(10,10))\n#subplot(r,c) provide the no. of rows and columns\n#f, axarr = plt.subplots(4,3)\ni=0\nj=0\n\nfor cls in class_names:\n    result = subprocess.run(['ls', path+cls], stdout=subprocess.PIPE)\n    res=result.stdout.decode('utf-8')\n    res=re.sub(r'\\n', ' ', res)\n    file_list=res.split(' ')\n    j=0\n    for x in range(0, 3):\n        # (nrows, ncols, index out of entire array of images)\n        ax=plt.subplot(4,3,(3*i)+j+1)\n        img=plt.imread(path+cls+'\/'+file_list[i])\n        plt.imshow(img)\n        plt.title(cls)\n        plt.axis(\"off\")\n        j+=1\n    i+=1","8f39e806":"# assigning one hot encoding of labels to train and val dataset\ndef one_hot_label(image, label):\n    label = tf.one_hot(label, num_classes)\n    return image, label\n\ntrain_dataset = train_dataset.map(one_hot_label, num_parallel_calls=autotune)\nval_dataset = val_dataset.map(one_hot_label, num_parallel_calls=autotune)","2e4680dd":"# preprocessing stage => rescaling pixel values of images\npreprocess_input = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.\/255.)","e7a3148b":"num_images_per_class = []\n\nfor label in class_names:\n    dir_name = \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/\" + label\n    num_images_per_class.append(len([name for name in os.listdir(dir_name)]))\nnum_images_per_class","ade114f1":"# y_train=list of train labels wrt to given directory format\ny_train=[]\nfor i in range(0, len(num_images_per_class)):\n    y_train+=[i]*num_images_per_class[i]","eeb8ad05":"from sklearn.utils.class_weight import compute_class_weight\nclass_weights=compute_class_weight(class_weight=\"balanced\",classes=[0,1,2,3], y=np.asarray(y_train))\nclass_weights\n# these weights will be used in fit procedure to compute class-weighted loss","7da56aeb":"# Create the base model from the pre-trained ResNet50V2 architecture\nbase_model = tf.keras.applications.ResNet50V2(input_shape=(*target, 3),\n                                               include_top=False,\n                                               weights='imagenet')\n\nimage_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","37812e67":"# freezing base_model layers\nbase_model.trainable = False\nbase_model.summary()","fe1cc19c":"# Global average pooling layer\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","3cbbb028":"# Classification head layer\nprediction_layer = tf.keras.layers.Dense(4, activation=\"softmax\")\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","bb43f8e7":"# TL model with frozen base\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.metrics import *\nimport tensorflow_addons as tfa\n\ninputs = tf.keras.Input(shape=(*target, 3))\n#x = data_augmentation(inputs)\nx = preprocess_input(inputs)        #rescaling\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)\n\n# using AUC and Weighted-F1Score as primary metrics\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adadelta\", metrics=[AUC(name=\"auc\"),\n                                                                            tfa.metrics.F1Score(name=\"f1\", num_classes=4,\n                                                                            average=\"weighted\"),\n                                                                            CategoricalAccuracy(name=\"acc\")\n                                                                            ])\nmodel.summary()","706efc0b":"# Callbacks\nimport numpy as np\nfrom keras.callbacks import Callback\n\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"dementia_model.h5\",\n                                                    save_best_only=True, monitor=\"val_loss\", mode='min')\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True, monitor=\"val_loss\", mode='min')\ncb=[checkpoint_cb, early_stopping_cb]\n# using 2 callbacks; \n# save best selected model with least val_loss, \n# terminate training run if val_loss doesnt change for 10 epochs","1b122733":"# Validation scores before training\nmodel.evaluate(val_dataset)","46f0dcf4":"# run_1 (training classification head after feature extraction)\nhistory1 = model.fit(train_dataset,\n                    epochs=50,\n                    validation_data=val_dataset, verbose=1, callbacks=cb, use_multiprocessing=True, workers=4, \n                    class_weight={\n                                    0: class_weights[0],\n                                    1: class_weights[1],\n                                    2: class_weights[2],\n                                    3: class_weights[3]\n                                  })","df191118":"base_model.trainable = True\nprint(\"Number of layers in the base model: \", len(base_model.layers))","430dfe91":"# Fine-tune from this layer onwards\nfine_tune_at = 125\n\n# Freezing all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False\n\n# 65 layers are set trainable in bottom up manner from classification head\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adadelta\", metrics=[AUC(name=\"auc\"),\n                                                                            tfa.metrics.F1Score(name=\"f1\", num_classes=4,\n                                                                            average=\"weighted\"),\n                                                                            CategoricalAccuracy(name=\"acc\")\n                                                                            ])\nmodel.summary()","bac9700d":"# validation scores before training\nmodel.evaluate(val_dataset)","e33d1077":"# run_2 (fine tuning the pretrained layers)\nhistory2 = model.fit(train_dataset,\n                    epochs=70,\n                    validation_data=val_dataset, verbose=1, callbacks=cb, use_multiprocessing=True, workers=4, \n                    class_weight={\n                                    0: class_weights[0],\n                                    1: class_weights[1],\n                                    2: class_weights[2],\n                                    3: class_weights[3]\n                                  })","cb22aacd":"# Plotting train_loss and val_loss wrt epochs\nimport matplotlib.pyplot as plt\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history1.history['f1'])\nplt.plot(history1.history['val_f1'])\nplt.title('model f1')\nplt.ylabel('f1')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4d514efe":"plt.plot(history1.history['auc'])\nplt.plot(history1.history['val_auc'])\nplt.title('model auc')\nplt.ylabel('auc')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model acc')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4f865855":"# Plotting train_loss and val_loss wrt epochs\nimport matplotlib.pyplot as plt\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history2.history['f1'])\nplt.plot(history2.history['val_f1'])\nplt.title('model f1')\nplt.ylabel('f1')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","fd6f0086":"plt.plot(history2.history['auc'])\nplt.plot(history2.history['val_auc'])\nplt.title('model auc')\nplt.ylabel('auc')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history2.history['acc'])\nplt.plot(history2.history['val_acc'])\nplt.title('model acc')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","8a0609a7":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/test\",\n    image_size=target,\n    batch_size=batch,\n    shuffle=False,\n)\n\ntest_ds = test_ds.map(one_hot_label, num_parallel_calls=autotune)\ntest_ds = test_ds.cache().prefetch(buffer_size=autotune)","56271377":"model.evaluate(test_ds)","31f41ec8":"predictions=model.predict(test_ds)","5284d9d5":"# list of number of images belonging to each class in test directory\ntest_images = []\n\nfor label in class_names:\n    dir_name = \"..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/test\/\" + label\n    test_images.append(len([name for name in os.listdir(dir_name)]))\ntest_images","72c2b17b":"# targets list\ntargets=[]\nfor i in range(0, len(test_images)):\n    targets+=[i]*test_images[i]\ntargets=np.asarray(targets)\ntest_images, class_names, targets","750a02ea":"from sklearn.metrics import classification_report\nprint(classification_report(np.asarray(targets), np.argmax(predictions, axis=1), target_names=class_names))","dc51f3eb":"model.save(\"dementia_tlmodel.h5\")","85dc8527":"# generating predictions csv file\nimport pandas as pd\ndf=pd.DataFrame(columns=class_names)\nfor i in range(0, 4):\n    df[class_names[i]]=predictions[:,i]\n\ndf.to_csv(\"dementia_tlmodel.csv\")","58201f00":"Run_1: Training classification head after feature extraction from pretrained base","8733798a":"Run_2: Fine tuning select layers of pretrained base model","81243644":"Forming Classification report","9d59dd5f":"# Evaluating model on test set","04960909":"# Saving best selected model","ffb90f0b":"# Data loading\nUsing [Kaggle Alzheimer's dataset](https:\/\/www.kaggle.com\/tourist55\/alzheimers-dataset-4-class-of-images)","da840c06":"# Visualizing Model Metrics","9617b77b":"# Dataset class distribution\n checking how many images are in each class for our training data","03690536":"Training runs\n","4a55a7be":"Visualizing images from train directory by accessing them directly through subprocess  (coloured)","4d71b80d":"# Transfer Learning\nMain steps involved:\n* Using Feature extraction from pretrained base model and training classification head\n* Fine tuning specific layers of pretrained base to suit to our classification task ","66058ae7":"# Training the Model","785f6ed3":"Visualizing images loaded into dataset instance","4233c901":"# Feature Engineering"}}