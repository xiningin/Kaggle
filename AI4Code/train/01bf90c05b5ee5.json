{"cell_type":{"7dee31c6":"code","8462a28e":"code","73de70fc":"code","f41841f7":"code","def89042":"code","afc20fbc":"code","da59ea94":"code","b8b6950b":"code","91f45add":"code","4d802743":"code","0cf462c1":"code","c9d9b0d0":"code","218c9ce7":"code","a06343a3":"code","9b1f39b7":"code","7d6cc8ab":"code","bccac2c6":"code","a0148a2f":"code","9322c6f2":"code","e99c275e":"code","d746008d":"code","a4940ca6":"code","642f2c4e":"code","5a5b4753":"code","0373f9d3":"markdown","522fcf9b":"markdown","4a1d4c12":"markdown"},"source":{"7dee31c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8462a28e":"import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nfrom keras.layers import Dense, Flatten, Conv1D, Dropout, Input\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n","73de70fc":"train_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","f41841f7":"train_df.head()","def89042":"test_df.head()","afc20fbc":"train_df.isnull().sum()","da59ea94":"train_df.dropna(inplace=True)\ntrain_df.reset_index(inplace=True)","b8b6950b":"train_df.isnull().sum()","91f45add":"def word(text):   \n    #line = re.findall(r'[a-zA-Z0-9]+', text)\n    review_word = BeautifulSoup(text).get_text()\n    review_word = re.sub(r'[^a-zA-Z]',' ', review_word)\n    words = review_word.lower().split()\n    stops = set(stopwords.words('english'))\n    words = [w for w in words if w not in stops]\n    \n    return ' '.join(words)\ntrain_df['clean_text']=train_df['text'].apply(lambda x : word(x))","4d802743":"import emoji\ndef f_emoji(text):\n    sentences=[]\n    emoji_text = emoji.demojize(text)\n  #  line = re.findall(r'\\:(.*?)\\:',emoji_text)\n    line = re.sub(\":\",\"\",emoji_text)\n    for sentence in line:\n  #      sett = re.findall(r'[^\\_]+',sentence)\n        sett = re.sub(\"_\",\" \",sentence)\n        sentences.append(' '.join(sett))\n  #      sentences.append(' '.join(sentence)) \n    return ''.join(sentences)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_email(text):  # text \uc548\uc5d0\uc11c email \uc81c\uac70\ud558\uae30\n    line = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n\n    return line.sub(r'',text)\n\ndef remove_hash(text):\n    line = re.sub(r'\\#','',text)\n    return ''.join(line)\n\ndef remove_phone_num(text):\n    line = re.sub(r'\\b\\d{10}\\b','', text)\n    line = re.sub(r'(\\d{3})\\-(\\d{3})\\-(\\d{4})','',line)\n    return ''.join(line)\n\ndef remove_year(text):\n    line=re.sub(r\"\\b(19[4-9][0-9]|20[0-1][0-9]|2020)\\b\",'',text)   \n    return ''.join(line)\n\ndef remove_url(text):\n    url = re.sub(r'http[s]?[\\:]?\/\/\\S+|www\\.\\S+\\.\\S+','',text)\n    return ''.join(url)\n\ntrain_df['c_s_text']= train_df['selected_text'].copy()\ntrain_df['c_s_text']= train_df['c_s_text'].apply(lambda x : remove_url(x))\ntrain_df['c_s_text']= train_df['c_s_text'].apply(lambda x : remove_email(x))\ntrain_df['c_s_text']= train_df['c_s_text'].apply(lambda x : remove_emoji(x))\ntrain_df['c_s_text']= train_df['c_s_text'].apply(lambda x : f_emoji(x))\ntrain_df['c_s_text']= train_df['c_s_text'].apply(lambda x : word(x))\n","0cf462c1":"sent_r=train_df.groupby('sentiment')['sentiment'].count()","c9d9b0d0":"sent_r.plot(kind='bar')","218c9ce7":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\nsent=['positive','neutral','negative']\nfig, ax = plt.subplots(1,3, figsize=(11,11))\nfor i,s in enumerate(sent):\n    \n    tot_token = ''\n    tot_token +=' '.join(train_df.loc[train_df['sentiment']==s,'c_s_text'])\n    wordcloud = WordCloud(width=300, height=300, background_color='white',\n                     stopwords = stopwords, min_font_size=6).generate(tot_token)\n\n    ax[i].imshow(wordcloud)  \n    ax[i].set_title(s)\n    ax[i].axis('off')\n \n","a06343a3":"train_df.loc[train_df['sentiment']=='negative','c_s_text']","9b1f39b7":"import seaborn as sns\n\ndef dist_unique_word(df):\n    fig,ax = plt.subplots(1,3, figsize=(12,5))\n    for i,s in enumerate(sent):\n        new = train_df[train_df['sentiment']==s]['c_s_text'].map(lambda x: len(set(x.split())))\n        sns.distplot(new.values, ax=ax[i])\n        ax[i].set_title(s)\n    fig.suptitle('Distribution of number of unique words')\n    fig.show()\n\ndist_unique_word(train_df)","7d6cc8ab":"neutral_df=train_df[train_df['sentiment']=='neutral']\nnegative_df = train_df[train_df['sentiment']=='negative']\npositive_df = train_df[train_df['sentiment']=='positive']\n\nprint('neutral : text vs selected text equivalent rate : {:.2f}'.format(sum(neutral_df['clean_text']==neutral_df['c_s_text'])\/len(neutral_df)*100))\nprint('negative : text vs selected text equivalent rate : {:.2f}'.format(sum(negative_df['clean_text']==negative_df['c_s_text'])\/len(negative_df)*100))\nprint('neutral : text vs selected text equivalent rate : {:.2f}'.format(sum(positive_df['clean_text']==positive_df['c_s_text'])\/len(positive_df)*100))","bccac2c6":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_ngram(corpus, n):\n    vec = CountVectorizer(ngram_range=(n,n), analyzer='word', max_features=5000)\n    bag_of_words = vec.fit_transform(train_df[train_df['sentiment']==corpus]['c_s_text'])\n    sum_words=bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0,idx])for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x:x[1], reverse=True)\n    return words_freq[:20]\n\ntop_pos=get_top_ngram('positive',3)\ntop_neu=get_top_ngram('neutral',3)\ntop_neg=get_top_ngram('negative',3)\nprint('Top Positive words: ',top_pos)\nprint('Top Neutral words: ', top_neu)\nprint('Top Negative words: ', top_neg)","a0148a2f":"max_len = 150\n\nvocab_path = '\/kaggle\/input\/roberta-base\/vocab.json'\nmerge_path = '\/kaggle\/input\/roberta-base\/merges.txt'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file = vocab_path,\n            merges_file = merge_path,\n            lowercase =True,\n            add_prefix_space=True\n)\n\nsentiment_id = {'positive':tokenizer.encode('positive').ids[0], 'negative':tokenizer.encode('negative').ids[0], 'neutral':tokenizer.encode('neutral').ids[0]}\n\ntrain_df.reset_index(inplace=True)","9322c6f2":"# input data formating for training\ntot_tw = train_df.shape[0]\n\ninput_ids = np.ones((tot_tw, max_len), dtype='int32')\nattention_mask = np.zeros((tot_tw, max_len), dtype='int32')\ntoken_type_ids = np.zeros((tot_tw, max_len), dtype='int32')\nstart_mask = np.zeros((tot_tw, max_len), dtype='int32')\nend_mask = np.zeros((tot_tw, max_len), dtype='int32')\n\nfor i in range(tot_tw):\n    set1 = \" \"+\" \".join(train_df.loc[i,'text'].split())\n    set2 = \" \".join(train_df.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)]=1\n    if set1[idx-1]==\" \":\n        set2_loc[idx-1]=1\n  \n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx=[]\n    for k,(a,b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    senti_token = sentiment_id[train_df.loc[i,'sentiment']]\n    input_ids[i,:len(enc_set1.ids)+5] = [0]+enc_set1.ids+[2,2]+[senti_token]+[2] \n    attention_mask[i,:len(enc_set1.ids)+5]=1\n\n    if len(selected_text_token_idx) > 0:\n        start_mask[i,selected_text_token_idx[0]+1]=1\n        end_mask[i, selected_text_token_idx[-1]+1]=1","e99c275e":"#input data formating for testing\n\ntot_test_tw = test_df.shape[0]\n\ninput_ids_t = np.ones((tot_test_tw,max_len), dtype='int32')\nattention_mask_t = np.zeros((tot_test_tw,max_len), dtype='int32')\ntoken_type_ids_t = np.zeros((tot_test_tw,max_len), dtype='int32')\n\nfor i in range(tot_test_tw):\n    set1 = \" \"+\" \".join(test_df.loc[i,'text'].split())\n    enc_set1 = tokenizer.encode(set1)\n\n    s_token = sentiment_id[test_df.loc[i,'sentiment']]\n    input_ids_t[i,:len(enc_set1.ids)+5]=[0]+enc_set1.ids+[2,2]+[s_token]+[2]\n    attention_mask_t[i,:len(enc_set1.ids)+5]=1","d746008d":"from keras.layers import Dense, Flatten, Conv1D, Dropout, Input\nfrom keras.models import Model\ndef build_model():\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok =  tf.keras.layers.Input((max_len,), dtype=tf.int32) \n\n    config_path = RobertaConfig.from_pretrained('\/kaggle\/input\/prerobertabase\/config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained('\/kaggle\/input\/prerobertabase\/pretrained-roberta-base.h5', config=config_path)\n    x = bert_model(ids, attention_mask = att, token_type_ids=tok)\n\n    \n    x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n    x1 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x1)\n    x1 =  tf.keras.layers.Conv1D(128,2, padding='same')(x1)\n    x1 =  tf.keras.layers.LeakyReLU()(x1)\n  #x1 =  tf.keras.layers.Conv1D(256,2, padding='same')(x1)\n  #x1 =  tf.keras.layers.LeakyReLU()(x1)\n  #x1 =  tf.keras.layers.Conv1D(128,2, padding='same')(x1)\n    x1 =  tf.keras.layers.Conv1D(16,2, padding='same')(x1)\n    x1 =  tf.keras.layers.LeakyReLU()(x1)\n  #x1 =  tf.keras.layers.Conv1D(1,1)(x1)\n    x1 =  tf.keras.layers.Dense(1)(x1)\n    x1 =  tf.keras.layers.Flatten()(x1)\n    x1 =  tf.keras.layers.Activation('softmax')(x1)\n\n    x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n    x2 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x2)\n    x2 =  tf.keras.layers.Conv1D(128,2, padding='same')(x2)\n    x2 =  tf.keras.layers.LeakyReLU()(x2)\n  #x2 =  tf.keras.layers.Conv1D(256,2, padding='same')(x2)\n  #x2 =  tf.keras.layers.LeakyReLU()(x2)\n  #x2 =  tf.keras.layers.Conv1D(128,2, padding='same')(x2)\n    x2 =  tf.keras.layers.Conv1D(16,2, padding='same')(x2)\n    x2 =  tf.keras.layers.LeakyReLU()(x2)\n  #x2 =  tf.keras.layers.Conv1D(1,1)(x2)\n    x2 =  tf.keras.layers.Dense(1)(x2)\n    x2 =  tf.keras.layers.Flatten()(x2)\n    x2 =  tf.keras.layers.Activation('softmax')(x2)\n    \n    \n    #x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n    #x1 =  tf.keras.layers.Conv1D(16,1, padding='same')(x1)\n    #x1 =  tf.keras.layers.Conv1D(16,1, padding='same')(x1)\n    #x1 =  tf.keras.layers.Activation('relu')(x1)\n    #x1 =  tf.keras.layers.Dropout(0.1)(x1)\n    #x1 =  tf.keras.layers.Conv1D(1,1)(x1)\n    #x1 =  tf.keras.layers.Activation('relu')(x1)\n    #x1 =  tf.keras.layers.Flatten()(x1)\n    #x1 =  tf.keras.layers.Activation('softmax')(x1)\n\n    #x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n    #x2 =  tf.keras.layers.Conv1D(16,1, padding='same')(x2)\n    #x2 =  tf.keras.layers.Conv1D(16,1, padding='same')(x2)\n    #x2 =  tf.keras.layers.Activation('relu')(x2)\n    #x2 =  tf.keras.layers.Dropout(0.1)(x2)\n    #x2 =  tf.keras.layers.Conv1D(1,1)(x2)\n    #x2 =  tf.keras.layers.Activation('relu')(x2)\n    #x2 =  tf.keras.layers.Flatten()(x2)\n    #x2 =  tf.keras.layers.Activation('softmax')(x2)\n\n    model =  tf.keras.models.Model(inputs=[ids,att,tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","a4940ca6":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","642f2c4e":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom transformers import TFRobertaModel\n\njac =[]; DISPLAY=1\n#oof_start = np.zeros((input_ids.shape[0],max_len))\n#oof_end = np.zeros((input_ids.shape[0],max_len))\npreds_start= np.zeros((input_ids_t.shape[0],max_len))\npreds_end= np.zeros((input_ids_t.shape[0],max_len))\n\n#skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\n#for fold,(idxT, idxV) in enumerate(skfold.split(input_ids, train_df.sentiment.values)):\n\n#    print('### FOLD %i'%(fold+1))\n\n    \n#    K.clear_session()\n#    model = build_model()\nmodel = build_model()\n#    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n#    mc = tf.keras.callbacks.ModelCheckpoint('sample_data\/pre-roberta.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto', save_freq='epoch', save_weights_only=True)\n#    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT]], [start_mask[idxT,],end_mask[idxT,]], epochs=12, batch_size=32, verbose=DISPLAY, callbacks=[es,mc], validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],[start_mask[idxV,],end_mask[idxV,]]))\n    \n#    print('Loading model ....')\n#    model.load_weights('sample_data\/pre-roberta.h5')\nprint('Loading model ....')\nmodel.load_weights('\/kaggle\/input\/preroberta8\/pre-roberta-v10.h5')\n\n#    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n#    preds_start += preds[0]\/skfold.n_splits\n#    preds_end += preds[1]\/skfold.n_splits\n    \n#    all = []\n#    for k in idxV:\n#        a = np.argmax(oof_start[k,])\n#        b = np.argmax(oof_end[k,])\n#        if a>b: \n#            st = train_df.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n#        else:\n#            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n#            enc = tokenizer.encode(text1)\n#            st = tokenizer.decode(enc.ids[a-1:b])\n#        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n#    jac.append(np.mean(all))\n#    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n#    print()\n\nprint('-'*15)\nprint('- RESULT -')\nprint('-'*15)\npreds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\npreds_start = preds[0]\npreds_end = preds[1]\n  \nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\ntest_df['selected_text']=all\ntest_df[['textID','selected_text']].to_csv('submission.csv', index=False)","5a5b4753":"model.summary()","0373f9d3":"- RoBerta -","522fcf9b":"About 90% of the selected text's in neutral were the same as the text's.","4a1d4c12":"Data Pre-processing for EDA"}}