{"cell_type":{"b2e1eb69":"code","0a37c115":"code","a04fe142":"code","3986186f":"code","d4999d40":"code","e341d941":"code","f3c4a04d":"code","41c1143c":"code","b834d8c7":"code","aa9bdd57":"code","cb43c557":"code","a73526dc":"code","267623a7":"code","b76e8f7a":"code","d87b1f20":"code","16bfc150":"markdown","5933f081":"markdown","f8bbcbd2":"markdown","a512d206":"markdown","89bd4d67":"markdown"},"source":{"b2e1eb69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a37c115":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np\nimport sys\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom sklearn.model_selection import train_test_split\nimport platform\nimport struct\nimport pandas as pd\nimport shutil\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\ntotal_class = len(class_names)\n# Parameters","a04fe142":"print(\"TensorFlow Version\\nRunning: \",tf.__version__)\nprint(\"Python Version\\nRunning: \", platform.python_version())","3986186f":"def train(hidden_unit_size=64,draw_plot=False,training_epochs = 50,intit_learning_rate = 0.0005,batch_size = 100,patience = 15):\n    print(\"+\" * 80)\n    print(\"Training Phase\")\n    print(\"+\" * 80)\n    learning_rate=intit_learning_rate\n    with open('\/kaggle\/input\/fashionmnist\/train-images-idx3-ubyte','rb') as f:\n        magic, size = struct.unpack(\">II\", f.read(8))\n        nrows, ncols = struct.unpack(\">II\", f.read(8))\n        X_train = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n        X_train = X_train.reshape((size, nrows, ncols))\n    with open('\/kaggle\/input\/fashionmnist\/train-labels-idx1-ubyte','rb') as f:\n        magic, size = struct.unpack(\">II\", f.read(8))\n        ##nrows, ncols = struct.unpack(\">II\", f.read(8))\n        y_train = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n        y_train = y_train.reshape((size,))\n\n    train_loss_list = []\n    validation_loss_list = []\n    train_accuracy_list = []\n    validation_accuracy_list = []\n#     y_true = np.unique(y_train,axis=0)\n#     print(y_true)\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,stratify=y_train ,test_size=(1\/10), random_state=48)\n    # Normalize input\n    X_train = -0.5 + (X_train \/ 255.0)\n    X_valid = -0.5 + (X_valid \/ 255.0)\n    X_train = X_train.reshape((-1,784))\n    X_valid = X_valid.reshape((-1,784))\n    # One Hot Encoding for Training and Validation Set\n    y_train = np.eye(total_class)[y_train]\n    y_valid = np.eye(total_class)[y_valid]\n    print(\"Length of Training Data:\", len(X_train))\n    print(\"Length of Validation Data\", len(X_valid))\n    # Logits\n    x = tf.placeholder(tf.float64, [None, 784])\n    w1 = tf.Variable(tf.random.normal(\n        [784, hidden_unit_size], mean=0.0, stddev=np.math.sqrt(2 \/ (hidden_unit_size)), dtype=tf.float64), name=\"w1\")\n    b1 = tf.Variable(tf.zeros([hidden_unit_size],\n                              dtype=tf.float64),  name=\"b1\")\n    layer1 = tf.add(tf.matmul(x, w1), b1)\n    layer1 = tf.nn.relu(layer1)\n    w2 = tf.Variable(tf.random.normal(\n        [hidden_unit_size, hidden_unit_size], mean=0.0, stddev=np.math.sqrt(2 \/ (hidden_unit_size)), dtype=tf.float64), name=\"w2\")\n    b2 = tf.Variable(tf.zeros(\n        [hidden_unit_size], dtype=tf.float64), name=\"b2\")\n    layer2 = tf.add(tf.matmul(layer1, w2), b2)\n    layer2 = tf.nn.relu(layer2)\n    w3 = tf.Variable(tf.random.normal(\n        [hidden_unit_size, hidden_unit_size],  mean=0.0, stddev=np.math.sqrt(2 \/ (hidden_unit_size)), dtype=tf.float64), name=\"w3\")\n    b3 = tf.Variable(tf.zeros([hidden_unit_size], dtype=tf.float64), name=\"b3\")\n    layer3 = tf.add(tf.matmul(layer2, w3), b3)\n    layer3 = tf.nn.relu(layer3)\n    w4 = tf.Variable(tf.random.normal(\n        [hidden_unit_size, 10],  mean=0.0, stddev=np.math.sqrt(2 \/ (hidden_unit_size)), dtype=tf.float64), name=\"w4\")\n    b4 = tf.Variable(tf.zeros([10], dtype=tf.float64), name=\"b4\")\n    layer4 = tf.add(tf.matmul(layer3, w4), b4)\n    y_ = tf.placeholder(tf.float64, [None, 10])\n    output = tf.nn.softmax(layer4)\n    # loss calculation\n    avgerageLoss = tf.reduce_mean(tf.compat.v1.losses.softmax_cross_entropy(y_, layer4)) + 0.0005 * (tf.reduce_sum(tf.multiply(\n        w1, w1)) + tf.reduce_sum(tf.multiply(w2, w2)) + tf.reduce_sum(tf.multiply(w3, w3)) + tf.reduce_sum(tf.multiply(w4, w4)))\n    # Minimize Cost\n    optimizer_adam = tf.compat.v1.train.AdamOptimizer(\n        learning_rate=learning_rate).minimize(avgerageLoss)\n    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n    validationLoss = tf.reduce_mean(\n        tf.compat.v1.losses.softmax_cross_entropy(y_, layer4))\n    # Initialize Variable\n    init = tf.compat.v1.global_variables_initializer()\n    saver = tf.compat.v1.train.Saver()\n    final_epoch_accuracy=()\n    with tf.compat.v1.Session() as sess:\n        sess.run(init)\n        best_val = 0\n        best_val_e = -1\n        for e in range(training_epochs):\n            print(\"Epoch\", e + 1)\n            epoch_loss = 0\n            for batch_index in range(0, X_train.shape[0], batch_size):\n                miniBatch_X = X_train[batch_index:batch_index + batch_size, :]\n                miniBatch_Y = y_train[batch_index:batch_index + batch_size, :]\n                _, batch_loss = sess.run([optimizer_adam, avgerageLoss], feed_dict={\n                    x: miniBatch_X, y_: miniBatch_Y})\n                epoch_loss += batch_loss\n            epoch_loss = epoch_loss \/ (X_train.shape[0] \/ batch_size)\n            print(\"Train_loss: \", epoch_loss, end=\" \")\n            trainAccuracy = sess.run(accuracy, feed_dict={\n                x: X_train, y_: y_train})\n            print(\"Train_accuracy:\", round(float(trainAccuracy * 100), 3), \"%\")\n            validLoss, validAccuracy = sess.run([validationLoss, accuracy], feed_dict={\n                x: X_valid, y_: y_valid})\n            print(\"Validation loss:\", validLoss, end=\" \")\n            print(\"Validation Accuracy:\", round(validAccuracy * 100, 3), \"%\")\n            train_loss_list.append(epoch_loss)\n            validation_loss_list.append(validLoss)\n            train_accuracy_list.append(round(float(trainAccuracy * 100), 3))\n            validation_accuracy_list.append(round(validAccuracy * 100, 3))\n            if(best_val < validAccuracy):\n                best_val = validAccuracy\n                best_val_e = e\n                save_path = saver.save(sess, \"weight\/model.ckpt\")\n            elif(best_val_e + patience <= e):\n                print(\"Early Stopping.\\nUsing Epoch\", best_val_e + 1)\n                print(\"Validation Accuracy\", round(best_val * 100, 3), \"%\")\n                final_epoch_accuracy=(e,round(best_val * 100, 3))\n                break\n            # Learning Rate Decay\n            learning_rate=intit_learning_rate\/(1+e)\n        if(draw_plot):\n            plt.plot(train_accuracy_list, label=\"Train Accuracy\")\n            plt.plot(validation_accuracy_list, label=\"Validation Accuracy\")\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy')\n            plt.legend()\n            plt.grid()\n            plt.show()\n            plt.plot(train_loss_list,label=\"Train Loss\")\n            plt.plot(validation_loss_list,label=\"Validation Loss\")\n            plt.xlabel('Epoch')\n            plt.ylabel('Loss')\n            plt.legend()\n            plt.grid()\n            plt.show()\n        return final_epoch_accuracy\n","d4999d40":"# neurons=[4,8,12,16,24,32,48,64,96,128,192,256,384,512,784,1024]\n# validAccuracy=[]\n# for i in neurons:\n#     print(\"Number of neurons:\",i)\n#     validAccuracy.append(train(i))","e341d941":"# plt.plot(neurons,testAccuracy,label=\"Accuracy\")\n# plt.xlabel('Neuron')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.grid()\n# plt.show()\n\n","f3c4a04d":"# learning_rates=[0.05,0.005,0.001,0.0005,0.0001]\n# learning_accuracy=[]\n# epoch_length=[]\n# for rate in learning_rates:\n#     epoch,accuracy=train(intit_learning_rate=rate)\n#     learning_accuracy.append(accuracy)\n#     epoch_length.append(epoch)","41c1143c":"# print(\"Learning Rate Accuracy Epoch Length\")\n# for i in range(len(learning_accuracy)):\n#     print(learning_rates[i],\"\\t\",learning_accuracy[i],\"\\t\",epoch_length[i])\n# plt.plot(learning_rates,learning_accuracy,label=\"Accuracy\")\n# plt.xlabel('Learning Rate')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.grid()\n# plt.show()\n# plt.plot(learning_rates,epoch_length,label=\"Epoch Length\")\n# plt.xlabel('Learning Rate')\n# plt.ylabel('Epoch')\n# plt.legend()\n# plt.grid()\n# plt.show()","b834d8c7":"train(draw_plot=True)","aa9bdd57":"print(\"Testing Phase\")\nwith open('\/kaggle\/input\/fashionmnist\/t10k-images-idx3-ubyte','rb') as f:\n    magic, size = struct.unpack(\">II\", f.read(8))\n    nrows, ncols = struct.unpack(\">II\", f.read(8))\n    X_test = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n    X_test = X_test.reshape((size, nrows, ncols))\nwith open('\/kaggle\/input\/fashionmnist\/t10k-labels-idx1-ubyte','rb') as f:\n    magic, size = struct.unpack(\">II\", f.read(8))\n    ##nrows, ncols = struct.unpack(\">II\", f.read(8))\n    y_test = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n    y_test = y_test.reshape((size,))\nX_test = X_test.reshape((-1,784))\nX_test = -0.5 + (X_test \/ 255.0)\ny_test = np.eye(total_class)[y_test]\nprint(\"Length of Training Data:\", len(X_test))\ntf.compat.v1.reset_default_graph()\nsaver = tf.compat.v1.train.import_meta_graph(\"weight\/model.ckpt.meta\")\nx = tf.compat.v1.placeholder(tf.float64, [None, 784])\ny_ = tf.compat.v1.placeholder(tf.float64, [None, 10])\nwith tf.compat.v1.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint(\"weight\/\"))\n    graph = tf.compat.v1.get_default_graph()\n    w1 = graph.get_tensor_by_name(\"w1:0\")\n    b1 = graph.get_tensor_by_name(\"b1:0\")\n    w2 = graph.get_tensor_by_name(\"w2:0\")\n    b2 = graph.get_tensor_by_name(\"b2:0\")\n    w3 = graph.get_tensor_by_name(\"w3:0\")\n    b3 = graph.get_tensor_by_name(\"b3:0\")\n    w4 = graph.get_tensor_by_name(\"w4:0\")\n    b4 = graph.get_tensor_by_name(\"b4:0\")\n    h1 = tf.add(tf.matmul(x, w1), b1)\n    h1 = tf.nn.relu(h1)\n    h2 = tf.add(tf.matmul(h1, w2), b2)\n    h2 = tf.nn.relu(h2)\n    h3 = tf.add(tf.matmul(h2, w3), b3)\n    h3 = tf.nn.relu(h3)\n    predicted = tf.add(tf.matmul(h3, w4), b4)\n    predicted = tf.argmax(predicted, 1)\n    actual = tf.argmax(y_, 1)\n    correct_prediction = tf.equal(predicted, actual)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n    test_accuracy,test_prediction,test_correct = sess.run([accuracy,predicted,actual], feed_dict={x: X_test, y_: y_test})\n    print(\"Test Accuracy:\", round(test_accuracy * 100, 3), \"%\")\n    print(\"*\"*80)","cb43c557":"test_correct_name=[class_names[i] for i in test_correct]\ntest_prediction_name=[class_names[i] for i in test_prediction]","a73526dc":"Length=5\nWidth=5\n\nfig,axes=plt.subplots(Length,Width,figsize=(12,12))\naxes=axes.ravel()\ni=0\nj=0\nwhile j<(Length*Width):\n    if(test_correct_name[i]!=test_prediction_name[i]):\n        axes[j].imshow(X_test[i].reshape(28,28))\n        axes[j].set_title(f'Prediction:{test_prediction_name[i]}({test_prediction[i]})\\n Actual:{test_correct_name[i]}({test_correct[i]})')\n        axes[j].axis('off')\n        j=j+1\n    i=i+1\n    \nplt.subplots_adjust(wspace=0.9)","267623a7":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(test_correct_name,test_prediction_name)\nplt.figure(figsize=(14,10))\nsns.heatmap(cm,annot=True)","b76e8f7a":"from sklearn.metrics import classification_report\ntarget_names=[f'{i}' for i in class_names]\nprint(classification_report(test_correct_name,test_prediction_name,target_names=target_names))","d87b1f20":"from sklearn.metrics import classification_report\n\nnum_classes=10\ntarget_names=[\"class {}\".format(i)for i in range (num_classes)]\n\nprint(classification_report(test_correct,test_prediction,target_names=target_names))","16bfc150":"# Precision Recall","5933f081":"## Test Accuracy for each Neuron","f8bbcbd2":"# Confusion Matrix","a512d206":"# Learning Rate Tunning","89bd4d67":"# Incorrect Classification"}}