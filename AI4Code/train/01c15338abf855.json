{"cell_type":{"dca8a31f":"code","257a0a5c":"code","3c62b63b":"code","596193c4":"code","f42dbca3":"code","a91631fb":"code","80d5fcd5":"code","52591d48":"code","4f804668":"code","22c7ed20":"code","488ea48d":"code","d64b2d0f":"code","16900d0a":"code","6825ea44":"code","91993450":"code","a0506bf6":"code","0206b426":"code","6038c46d":"code","2b587cbe":"code","935dc73b":"markdown","f6785e7f":"markdown","a4049b49":"markdown","67380a7f":"markdown"},"source":{"dca8a31f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","257a0a5c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score \nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nimport joblib\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings\nwarnings.simplefilter(action = 'ignore', category= FutureWarning)\n","3c62b63b":"data = pd.read_csv('..\/input\/tweetsdatacsv\/tweetsdata.csv', encoding='ISO-8859-1')\ndata.columns\ndata.head()","596193c4":"data = data.drop(['id',\n       'created_at', 'date', 'time',\n       'timezone', 'user_id', 'username',\n       'name', 'hashtags','link','depression_score'],axis=1)\ndata.head(10)","f42dbca3":"# check the number of negative and non-negative sentences\n\nnon_negatives = data['sentiment'][data.sentiment == 0]\nnegatives = data['sentiment'][data.sentiment == 1]\n\nprint('number of non depressed sentences is:  {}'.format(len(non_negatives)))\nprint('number of depressed tagged sentences is: {}'.format(len(negatives)))\nprint('total length of the data is:            {}'.format(data.shape[0]))\n","a91631fb":"# get a word count per sentence column\ndef word_count(sentence):\n    return len(sentence.split())\n    \ndata['word count'] = data['tweet'].apply(word_count)\ndata.head()","80d5fcd5":"import nltk\nsent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\nsent_tokenizer.tokenize(data['tweet'][1])","52591d48":"import re\nimport string\n\ndef processTweet(tweet):\n    # Remove HTML special entities (e.g. &amp;)\n    tweet = re.sub(r'\\&\\w*;', '', tweet)\n    #Convert @username to AT_USER\n    tweet = re.sub('@[^\\s]+','',tweet)\n    # Remove tickers\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # To lowercase\n    tweet = tweet.lower()\n    # Remove\n    tweet = re.sub(r'\\\u20ac\\w*','', tweet)\n    tweet = re.sub(r'\\\u20ac\u2122\\w*','', tweet)\n    tweet = re.sub(r'\\\u00e2\u20ac\u00a6\\w*','', tweet)\n    tweet = re.sub(r'\\\u20ac\u00a6\\w*','', tweet)\n    tweet = re.sub(r'\\\u00a6\\w*','', tweet)\n    tweet = re.sub(r'\\\u2122\\w*','', tweet)\n    \n    # Remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*\\\/\\w*', '', tweet)\n    # Remove hashtags\n    tweet = re.sub(r'#\\w*', '', tweet)\n    # Remove words with 2 or fewer letters\n    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n    # Remove whitespace (including new line characters)\n    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n    # Remove single space remaining at the front of the tweet.\n    tweet = tweet.lstrip(' ') \n    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n    return tweet\n# ______________________________________________________________\n# clean dataframe's text column\ndata['text'] = data['tweet'].apply(processTweet)\n# preview some cleaned tweets\ndata['text'].head()","4f804668":"def text_process(raw_text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in list(raw_text) if char not in string.punctuation]\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.lower().split() if word.lower() not in stopwords.words('english')]\n\ndef remove_words(word_list):\n    remove = ['...','\u201c','\u201d','\u2019','\u2026','\u20ac\u2122','it\u00e2\u20ac\u2122','it\u00e2\u2122']\n    return [w for w in word_list if w not in remove]\n# -------------------------------------------\n# tokenize message column and create a column for tokens\ndata = data.copy()\ndata['tokens'] = data['text'].apply(text_process) \ndata.head()","22c7ed20":"lemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text = [lemmatizer.lemmatize(i, pos=\"v\") for i in text]\n    return lem_text\ndata['lemmas1'] = data['tokens'].apply(lambda x: word_lemmatizer(x))\ndata.head()\n","488ea48d":"from nltk import pos_tag\ndef word_pos(text):\n    pos_tokens = [pos_tag(text)]\n    return pos_tokens\n\ndata['postag'] = data['lemmas1'].apply(lambda x: word_pos(x))\ndata['postag'].head()","d64b2d0f":"# split sentences to get individual words\nall_words = []\nwords = []\nfor line in data['lemmas1']:\n    all_words.extend(line)\n    \n    for word in words:\n        all_words.append(word.lower())","16900d0a":"from collections import Counter\n# create a word frequency dictionary\nwordfreq = Counter(all_words)\n# draw a Word Cloud with word frequencies\nfrom wordcloud import WordCloud,STOPWORDS\nwordcloud = WordCloud(width=900,\n                      height=500,\n                      max_words=500,\n                      max_font_size=100,\n                      relative_scaling=0.5,\n                      colormap='Blues',\n                      normalize_plurals=True).generate_from_frequencies(wordfreq)\nplt.figure(figsize=(14,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6825ea44":"#VECTORIZATION\n# vectorize\nbow_transformer = CountVectorizer(analyzer=text_process).fit(data['text'])\n# print total number of vocab words\nprint(len(bow_transformer.vocabulary_))\n","91993450":"# transform the entire DataFrame of texts\ntext_bow = bow_transformer.transform(data['text'])\n# check out the bag-of-words counts for the entire corpus as a large sparse matrix\nprint('Shape of Sparse Matrix: ', text_bow.shape)\nprint('Amount of Non-Zero occurences: ', text_bow.nnz)\n","a0506bf6":"tfidf_transformer = TfidfTransformer().fit(text_bow)\n\n# to transform the entire bag-of-words corpus\ntext_tfidf = tfidf_transformer.transform(text_bow)\nprint(text_tfidf.shape)","0206b426":"# We split the data into training and testing set:\ntrain, test = train_test_split(data, test_size=0.3, random_state=1)\nX_train = train['tweet'].values\nX_test = test['tweet'].values\ny_train = train['sentiment']\ny_test = test['sentiment']","6038c46d":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n# We are going to use cross validation and grid search to find good hyperparameters for our model. \n#We need to build a pipeline to don't get features from the validation folds when building each \n#training model.\nkfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nnp.random.seed(1)\n\npipeline = Pipeline([\n    ('bow', CountVectorizer(strip_accents='ascii',\n                            stop_words='english',\n                            lowercase=True)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n                        min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,\n                        objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)),  \n])\n# this is where we define the values for GridSearchCV to iterate over\nparameters = {'bow__ngram_range': [(1, 1), (1, 2)],\n              'tfidf__use_idf': (True, False),\n             }\n# k-fold cross validation\nxgboost = GridSearchCV(pipeline, cv=kfolds, param_grid=parameters, verbose=1)\nxgboost.fit(X_train,y_train)\nxgboost.score(X_test, y_test)\n\nprint(\"\\nBest Model: %f using %s\" % (xgboost.best_score_, xgboost.best_params_))\nprint('\\n')\nmeans = xgboost.cv_results_['mean_test_score']\nstds = xgboost.cv_results_['std_test_score']\nparams = xgboost.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"Mean: %f Stdev:(%f) with: %r\" % (mean, stdev, param))\n\n# save best model to current working directory\njoblib.dump(xgboost, \"twitter_sentiment.pkl\")\n\n# load from file and predict using the best configs found in the CV step\nmodel_xgboost = joblib.load(\"twitter_sentiment.pkl\" )\n\n# get predictions from best model above\ny_xgboost_preds = model_xgboost.predict(X_test)\nprint('xgboost_accuracy score: ',accuracy_score(y_test, y_xgboost_preds))\nprint('\\n')","2b587cbe":"import statistics\nxeval=[\"I had a chocolate today\",\"I went to the doctor today\",\"The sun is shining brightly\",\"The moon is out\",\"I don't feel happy\",\"I feel sad\",\"I don't have energy\",\"I went to the mall today\",\"I love the new movie track\"]\nprediction=model_xgboost.predict(xeval)\nif statistics.mode(prediction)==1:\n    print('Signs Of Depression')\nelse:\n    print('No Signs Of Depression')","935dc73b":"Train-Test Split","f6785e7f":"Bag of Words Model","a4049b49":"Word Cloud","67380a7f":"TOKENIZATION"}}