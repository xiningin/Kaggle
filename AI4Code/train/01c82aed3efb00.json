{"cell_type":{"0c09a46f":"code","cf8d6467":"code","8737e252":"code","8ad1504e":"code","fe01cf67":"code","a2541393":"code","839fe7c7":"code","3a82a0f4":"code","cdd219f6":"code","6c689f67":"code","87b93d73":"code","3ddc8690":"code","a61c6848":"code","fd2a7e86":"code","acd7ed33":"code","3f60bc88":"code","c76574a0":"code","2f20ddeb":"code","646c44b5":"code","7812144d":"code","4aef3659":"code","30b63fb5":"code","1236abf6":"code","242a0c2d":"code","7b03d07c":"code","42d71de2":"code","09764616":"code","a2d79030":"code","67db7d28":"code","09e84b95":"code","6d41b406":"code","5cea0e1e":"code","773ee1e0":"code","1c09338f":"code","9e50896f":"code","71914ce6":"code","881bc4ed":"code","59c95d23":"code","018e851b":"code","4746c9dc":"code","73290119":"code","9061e695":"code","bc8b8911":"code","b35c8b33":"code","5dd87ccd":"code","7bc80160":"code","27cc44ed":"code","4d1698db":"code","3ddde1d9":"code","e79ca7a7":"code","a12229a4":"code","696a8190":"code","d051ec61":"code","12b76629":"code","c653648f":"code","af851f98":"code","ceb35aa2":"code","773090af":"code","5589f71e":"code","10cf0190":"code","eb4f0a70":"markdown","ecd52c8b":"markdown","4a41eb9c":"markdown","95ba9fe1":"markdown","1eaf9bf5":"markdown","86f46889":"markdown","567325f6":"markdown","bcd2ea95":"markdown","fc827a52":"markdown","7832b6f5":"markdown","2667858e":"markdown","9ad9b7e3":"markdown","3ba358c0":"markdown","6fbdf280":"markdown","ffe0575e":"markdown","08d83f4c":"markdown","9f050c8c":"markdown","b13e66cb":"markdown","52da780b":"markdown"},"source":{"0c09a46f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf8d6467":"auto_df = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')\nauto_df.head()","8737e252":"auto_df.info()","8ad1504e":"auto_df = auto_df.drop(['car name'], axis = 1)","fe01cf67":"auto_df.describe()","a2541393":"mean = auto_df[auto_df['horsepower']!='?']['horsepower'].astype(int).mean()\nprint(int(mean))","839fe7c7":"def Fill_Data(data):\n    if data == '?':\n        return int(mean)\n    else:\n        return int(data)","3a82a0f4":"auto_df['horsepower'] = auto_df['horsepower'].apply(Fill_Data)","cdd219f6":"auto_df.describe()","6c689f67":"auto_df.info()","87b93d73":"auto_df_norm = (auto_df-auto_df.min())\/(auto_df.max()-auto_df.min())\nauto_df_norm.head()","3ddc8690":"f, axes = plt.subplots(2, 4, figsize=(40,40))\n\nsns.countplot(x = 'mpg', data = auto_df_norm, orient='v' ,ax = axes[0,0])\nsns.countplot(x = 'cylinders', data = auto_df_norm,ax = axes[0,1])\nsns.countplot(x = 'displacement', data = auto_df_norm,ax = axes[0,2])\nsns.countplot(x = 'horsepower', data = auto_df_norm,ax = axes[0,3])\nsns.countplot(x = 'weight', data = auto_df_norm,ax = axes[1,0])\nsns.countplot(x = 'acceleration', data = auto_df_norm,ax = axes[1,1])\nsns.countplot(x = 'model year', data = auto_df_norm,ax = axes[1,2])\nsns.countplot(x = 'origin', data = auto_df_norm,ax = axes[1,3])\n\n# plt.figure(figsize=(20,10))","a61c6848":"import statsmodels.api as sm","fd2a7e86":"f, axes = plt.subplots(2, 4, figsize=(40,40))\n\nsm.qqplot(data = auto_df_norm['mpg'], fit = True, line = 's', ax = axes[0,0])\nsm.qqplot(data = auto_df_norm['cylinders'], ax = axes[0,1])\nsm.qqplot(data = auto_df_norm['displacement'], fit = True, line = 's', ax = axes[0,2])\nsm.qqplot(data = auto_df_norm[auto_df['horsepower']!='?']['horsepower'].astype(int), fit = True, line = 's', ax = axes[0,3])\nsm.qqplot(data = auto_df_norm['weight'], fit = True, line = 's', ax = axes[1,0])\nsm.qqplot(data = auto_df_norm['acceleration'], fit = True, line = 's', ax = axes[1,1])\nsm.qqplot(data = auto_df_norm['model year'], ax = axes[1,2])\nsm.qqplot(data = auto_df_norm['origin'], ax = axes[1,3])","acd7ed33":"from scipy.stats import shapiro\n\ndef shapiro_test(df, col_list):\n    for x in col_list:\n        print(x)\n        data = df[x]\n        stat, p = shapiro(data)\n        print('Statistics=%.3f, p=%.3f' % (stat, p))\n        # interpret\n        alpha = 0.05\n        if p > alpha:\n            print('Sample looks Gaussian (fail to reject H0)')\n        else:\n            print('Sample does not look Gaussian (reject H0)')\n        print('\\n')","3f60bc88":"shapiro_test(auto_df_norm, list(auto_df_norm.columns))\n# print(\"MPG\"+\"\\n\")\n# shapiro_test(auto_df[\"mpg\"])\n# print(\"\\n\")\n# print(\"CYLINDERS\"+\"\\n\")\n# shapiro_test(auto_df[\"cylinders\"])\n# print(\"\\n\")\n# print(\"DISPLACEMENT\"+\"\\n\")\n# shapiro_test(auto_df[\"displacement\"])\n# print(\"\\n\")\n# print(\"HORSEPOWER\"+\"\\n\")\n# shapiro_test(auto_df['horsepower'])\n# print(\"\\n\")\n# print(\"WEIGHT\"+\"\\n\")\n# shapiro_test(auto_df[\"weight\"])\n# print(\"\\n\")\n# print(\"ACCELERATION\"+\"\\n\")\n# shapiro_test(auto_df[\"acceleration\"])\n# print(\"\\n\")\n# print(\"MODEL YEAR\"+\"\\n\")\n# shapiro_test(auto_df[\"model year\"])\n# print(\"\\n\")\n# print(\"ORIGIN\"+\"\\n\")\n# shapiro_test(auto_df[\"origin\"])\n# print(\"\\n\")","c76574a0":"from scipy.stats import normaltest\n\ndef D_A_test(df, col_list):\n    for x in col_list:\n        print(x)\n        data = df[x]\n        stat, p = normaltest(data)\n        print('Statistics=%.3f, p=%.3f' % (stat, p))\n        # interpret\n        alpha = 0.05\n        if p > alpha:\n            print('Sample looks Gaussian (fail to reject H0)')\n        else:\n            print('Sample does not look Gaussian (reject H0)')\n        print('\\n')","2f20ddeb":"D_A_test(auto_df_norm, list(auto_df_norm.columns))\n# print(\"MPG\"+\"\\n\")\n# D_A_test(auto_df[\"mpg\"])\n# print(\"\\n\")\n# print(\"CYLINDERS\"+\"\\n\")\n# D_A_test(auto_df[\"cylinders\"])\n# print(\"\\n\")\n# print(\"DISPLACEMENT\"+\"\\n\")\n# D_A_test(auto_df[\"displacement\"])\n# print(\"\\n\")\n# print(\"HORSEPOWER\"+\"\\n\")\n# D_A_test(auto_df['horsepower'])\n# print(\"\\n\")\n# print(\"WEIGHT\"+\"\\n\")\n# D_A_test(auto_df[\"weight\"])\n# print(\"\\n\")\n# print(\"ACCELERATION\"+\"\\n\")\n# D_A_test(auto_df[\"acceleration\"])\n# print(\"\\n\")\n# print(\"MODEL YEAR\"+\"\\n\")\n# D_A_test(auto_df[\"model year\"])\n# print(\"\\n\")\n# print(\"ORIGIN\"+\"\\n\")\n# D_A_test(auto_df[\"origin\"])\n# print(\"\\n\")","646c44b5":"from scipy.stats import anderson\n\ndef A_D_test(df, col_list):\n    for x in col_list:\n        print(x)\n        data = df[x]\n        result = anderson(data)\n        print('Statistic: %.3f' % result.statistic)\n        p = 0\n        for i in range(len(result.critical_values)):\n            sl, cv = result.significance_level[i], result.critical_values[i]\n            if result.statistic < result.critical_values[i]:\n                print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n            else:\n                print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n        print('\\n')","7812144d":"A_D_test(auto_df_norm, list(auto_df_norm.columns))\n# print(\"MPG\"+\"\\n\")\n# A_D_test(auto_df[\"mpg\"])\n# print(\"\\n\")\n# print(\"CYLINDERS\"+\"\\n\")\n# A_D_test(auto_df[\"cylinders\"])\n# print(\"\\n\")\n# print(\"DISPLACEMENT\"+\"\\n\")\n# A_D_test(auto_df[\"displacement\"])\n# print(\"\\n\")\n# print(\"HORSEPOWER\"+\"\\n\")\n# A_D_test(auto_df['horsepower'])\n# print(\"\\n\")\n# print(\"WEIGHT\"+\"\\n\")\n# A_D_test(auto_df[\"weight\"])\n# print(\"\\n\")\n# print(\"ACCELERATION\"+\"\\n\")\n# A_D_test(auto_df[\"acceleration\"])\n# print(\"\\n\")\n# print(\"MODEL YEAR\"+\"\\n\")\n# A_D_test(auto_df[\"model year\"])\n# print(\"\\n\")\n# print(\"ORIGIN\"+\"\\n\")\n# A_D_test(auto_df[\"origin\"])\n# print(\"\\n\")","4aef3659":"auto_df_new = auto_df\nauto_df.head()","30b63fb5":"auto_df_new['mpg'], bins = pd.cut(auto_df['mpg'], bins = 35, labels = False, retbins = True)\nauto_df_new.head()","1236abf6":"plt.figure(figsize = (10,10))\nsns.countplot(x = 'mpg', data = auto_df_new, orient='v')","242a0c2d":"D_A_test(auto_df_new, ['mpg'])","7b03d07c":"auto_df_new['acceleration'], bins = pd.cut(auto_df['acceleration'], bins = 18, labels = False, retbins = True)\n\nplt.figure(figsize = (10,10))\nsns.countplot(x = 'acceleration', data = auto_df_new, orient='v')","42d71de2":"sm.qqplot(data = auto_df_new['acceleration'], fit = True, line = 's')","09764616":"A_D_test(auto_df_new, ['acceleration'])","a2d79030":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(auto_df_norm, test_size = 0.3, random_state = 10)","67db7d28":"train","09e84b95":"test","6d41b406":"print(\"TRAIN SET STATISTICS\")\nprint(\"\\n\")\nprint(train.describe())\nprint(\"\\n\")\nprint(\"TEST SET STATISTICS\")\nprint(\"\\n\")\nprint(test.describe())","5cea0e1e":"from scipy.stats import skew\n\ndef skew_check(df, df_train, df_test, col_list):\n    for x in col_list:\n        print(x)\n        data0 = df[x]\n        data1 = df_train[x]\n        data2 = df_test[x]\n        skew0 = skew(data0)\n        skew1 = skew(data1)\n        skew2 = skew(data2)\n        print('orig = %.3f,train = %.3f, test = %.3f' % (skew0, skew1, skew2))\n        print('\\n')","773ee1e0":"skew_check(auto_df_norm, train, test, list(auto_df_norm.columns))","1c09338f":"from scipy.stats import kurtosis\n\ndef kurt_check(df, df_train, df_test, col_list):\n    for x in col_list:\n        print(x)\n        data0 = df[x]\n        data1 = df_train[x]\n        data2 = df_test[x]\n        kurt0 = kurtosis(data0)\n        kurt1 = kurtosis(data1)\n        kurt2 = kurtosis(data2)\n        print('orig = %.3f, train = %.3f, test=%.3f' % (kurt0, kurt1, kurt2))\n        print('\\n')","9e50896f":"kurt_check(auto_df_norm, train, test, list(auto_df_norm.columns))","71914ce6":"plt.figure(figsize = (12,10))\nsns.heatmap(auto_df_norm.corr(), annot = True)","881bc4ed":"from scipy.stats import chi2_contingency \n\ndef chi2_test(table):\n    stat, p, dof, expected = chi2_contingency(table)\n    print('stat=%.3f, p=%.3f' % (stat, p))\n    if p > 0.05:\n        print('Probably independent')\n    else:\n        print('Probably dependent')","59c95d23":"columns_list = list(auto_df_new.columns)","018e851b":"#columns_list.remove('car name')\n#print(columns_list)\ncont_columns = ['mpg', 'displacement', 'horsepower', 'weight', 'acceleration']","4746c9dc":"from scipy.stats import pearsonr\nfrom itertools import combinations\n\ndef pearson_test(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = pearsonr(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.1:\n            print('Probably independent')\n            print('\\n')\n        else:\n            print('Probably dependent')\n            print('\\n')","73290119":"pearson_test(auto_df, list(auto_df.columns))","9061e695":"pearson_test(auto_df_norm, list(auto_df_norm.columns))","bc8b8911":"from scipy.stats import ttest_ind\n\ndef t_test(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = ttest_ind(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n            print('\\n')\n        else:\n            print('Probably different distributions')\n            print('\\n')\n","b35c8b33":"t_test(auto_df, list(auto_df.columns))","5dd87ccd":"t_test(auto_df_norm, list(auto_df_norm.columns))","7bc80160":"from scipy.stats import f_oneway\n\ndef anova(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = f_oneway(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n        else:\n            print('Probably different distributions')\n        print('\\n')\n            ","27cc44ed":"anova(auto_df, list(auto_df.columns))","4d1698db":"anova(auto_df_norm, list(auto_df_norm.columns))","3ddde1d9":"from scipy.stats import mannwhitneyu\n\ndef mannwu_test(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = mannwhitneyu(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n        else:\n            print('Probably different distributions')\n        print('\\n')","e79ca7a7":"mannwu_test(auto_df, list(auto_df.columns))","a12229a4":"mannwu_test(auto_df_norm, list(auto_df_norm.columns))","696a8190":"from scipy.stats import wilcoxon\n\ndef wilc_test(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = wilcoxon(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n        else:\n            print('Probably different distributions')\n        print('\\n')","d051ec61":"wilc_test(auto_df, list(auto_df.columns))","12b76629":"wilc_test(auto_df_norm, list(auto_df_norm.columns))","c653648f":"from scipy.stats import kruskal\n\ndef kruskal_test(df, col_list):\n    dummy = list(combinations(col_list, 2))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #rint(data2)\n        stat, p = kruskal(data1, data2)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n        else:\n            print('Probably different distributions')\n        print('\\n')","af851f98":"kruskal_test(auto_df, list(auto_df.columns))","ceb35aa2":"kruskal_test(auto_df_norm, list(auto_df_norm.columns))","773090af":"from scipy.stats import friedmanchisquare\n\ndef fcs_test(df, col_list):\n    dummy = list(combinations(col_list, 3))\n    dummy = list((list(x) for x in dummy))\n    for x in dummy:\n        print(x)\n        data1 = df[x[0]].values\n        #print(data1)\n        data2 = df[x[1]].values\n        #print(data2)\n        data3 = df[x[2]].values\n        stat, p = friedmanchisquare(data1, data2, data3)\n        print('stat=%.3f, p=%.3f' % (stat, p))\n        if p > 0.05:\n            print('Probably the same distribution')\n        else:\n            print('Probably different distributions')\n        print('\\n')","5589f71e":"fcs_test(auto_df, list(auto_df.columns))","10cf0190":"fcs_test(auto_df_norm, list(auto_df_norm.columns))","eb4f0a70":"Kurtosis check","ecd52c8b":"# **Graphical Tests**","4a41eb9c":"Normalising the data","95ba9fe1":"AnOVa","1eaf9bf5":"Shapiro Wilk","86f46889":"D'Augustino's Test","567325f6":"Anderson's test","bcd2ea95":"# **Nonparametric Statistical Hypothesis Tests**","fc827a52":"Friedman chi square","7832b6f5":"Wilocoxon Test","2667858e":"# **Indepence of variables test**","9ad9b7e3":"# **Parametric Hypothesis Tests**","3ba358c0":"**References -**\n\n* https:\/\/machinelearningmastery.com\/statistical-hypothesis-tests-in-python-cheat-sheet\/\n* https:\/\/analyticsindiamag.com\/10-most-popular-statistical-hypothesis-testing-methods-using-python\/\n* https:\/\/towardsdatascience.com\/statistical-testing-understanding-how-to-select-the-best-test-for-your-data-52141c305168","6fbdf280":"Kruskal Wallis H test\n","ffe0575e":"# ****Statistical tests****","08d83f4c":"**Splitting data into train and test**","9f050c8c":"Mann Whitney U","b13e66cb":"Pearson Correlation","52da780b":"Skew check"}}