{"cell_type":{"fa53c00d":"code","4719aa85":"code","141731eb":"code","a2046549":"code","0e1228f7":"code","df40c0a9":"code","03c2b82b":"code","b85fa8ec":"code","f0d5deed":"code","1cd71e44":"code","68fd5792":"code","add28a26":"code","c8c2633f":"code","d731d85e":"code","dc36b540":"code","4b16ca8d":"code","96a49984":"code","b50df132":"code","206161f3":"code","838992a5":"code","a7ec1ba8":"code","dfc111d8":"code","edb582c8":"code","c2bea01e":"code","eba47f79":"code","cd5964b6":"code","a81c95b0":"code","91769cfb":"code","2e0d9838":"code","77f0feb6":"code","c3ab32c4":"code","64c3707a":"code","5e770f16":"code","f6a5fb05":"code","3e0fc1d1":"code","6eae2b97":"code","c1de00bc":"code","ca3245a1":"code","08c1b9dc":"code","6b6d3d81":"code","86da1e7d":"code","f3042f92":"code","3f934b55":"code","fadaeec5":"code","c81a3ddb":"code","c6048ea6":"code","91ddd64f":"code","a3668f76":"code","1a4ebfba":"code","50eeb7e7":"code","8d22a027":"markdown","ef788b8c":"markdown","577690a3":"markdown","10d24a3f":"markdown","a208a2d7":"markdown","cb988303":"markdown","aeb5e858":"markdown","15ff56cb":"markdown","23059bbc":"markdown","16351527":"markdown","d7941458":"markdown","f79a0988":"markdown","714712e9":"markdown","68ebff8c":"markdown","d3474a36":"markdown","fc6af4c1":"markdown"},"source":{"fa53c00d":"#install\n\n!python -m spacy download en_core_web_md\n!pip install wordcloud\n!pip install pyspellchecker\n!pip install contractions\n!pip install -U textblob\n!python -m textblob.download_corpora\n!pip install empath\n############################################\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport pandas_profiling as pdp\nimport gc\ngc.enable()\nimport spacy\nimport contractions\nimport en_core_web_md\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom spellchecker import SpellChecker\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport unicodedata\nimport re\nimport scattertext as st\nfrom textblob import TextBlob\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","4719aa85":"from IPython.display import IFrame\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:98% !important; }<\/style>\"))","141731eb":"data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","a2046549":"data.info()","0e1228f7":"data.head()","df40c0a9":"data.drop([\"id\",\"keyword\",\"location\"],axis=1,inplace=True)","03c2b82b":"reporte = pdp.ProfileReport(data, title=\"Pandas Profiling Reporte\",minimal=True)\nreporte","b85fa8ec":"data['contara'] = data[\"text\"].apply(lambda x: len(str(x).split()))\ndata.head()","f0d5deed":"plt.figure(figsize=(16,8))\nsns.barplot(data = data, x = \"target\", y = \"contara\")\nplt.title('Number of words in each target', fontsize= 25)\nplt.xlabel('predator')\nplt.ylabel('contara')","1cd71e44":"sns.countplot(data=data,x='target',orient=\"h\")\nplt.title(\"Disaster and fake\")\nplt.xlabel(\"target\")\nplt.ylabel(\"count\")\nplt.show()","68fd5792":"plt.figure(figsize=(10,10))\ndata.target.value_counts().plot(kind=\"pie\", autopct='%1.0f%%')\nplt.ylabel(\"target\")\n\nplt.title(\"fake or not\")\nplt.show","add28a26":"nlp = en_core_web_md.load()","c8c2633f":"stopwords_spacy = list(STOP_WORDS)\nprint(stopwords_spacy)\nlen(stopwords_spacy)","d731d85e":"en_stop_words=STOP_WORDS","dc36b540":"spell = SpellChecker(language='en',distance=1)","4b16ca8d":"def spell_check(x):\n    correct_word = []\n    mispelled_word = x\n    for word in mispelled_word:\n        correct_word.append(spell.correction(word))\n    return ' '.join(correct_word)","96a49984":"data['text']=data['text'].apply(str)","b50df132":"%%time\n\npattern = r\"[$&+,:;=_?@#|\\[\\]{}'<>.^*()%!-]\"\ntw_lemm=[]\nlm = WordNetLemmatizer()\nfor i in range(data.shape[0]):\n    tweet = data.iloc[i].text\n    tweet = ' '.join(re.sub(pattern, '', tweet).strip().split())\n    tweet = ' '.join([word for word in tweet.split() if word.isalpha()])\n    tweet = tweet.lower()\n    tweet = contractions.fix(tweet)\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    tweet = ' '.join(word for word in tweet.split() if word not in en_stop_words)\n    tweet = spell.correction(tweet)\n    tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n    tw_lemm.append(tweet)\n","206161f3":"data[\"tweet_lemms\"]=pd.Series(tw_lemm)","838992a5":"data.head()","a7ec1ba8":"test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","dfc111d8":"%%time\n\npattern = r\"[$&+,:;=_?@#|\\[\\]{}'<>.^*()%!-]\"\ntw_test_lemm=[]\nlm = WordNetLemmatizer()\nfor i in range(test.shape[0]):\n    tweet = test.iloc[i].text\n    tweet = ' '.join(re.sub(pattern, '', tweet).strip().split())\n    tweet = ' '.join([word for word in tweet.split() if word.isalpha()])\n    tweet = tweet.lower()\n    tweet = contractions.fix(tweet)\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    tweet = ' '.join(word for word in tweet.split() if word not in en_stop_words)\n    tweet = spell.correction(tweet)\n    tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n    tw_test_lemm.append(tweet)","edb582c8":"test[\"tweet_lemms\"]=pd.Series(tw_test_lemm)","c2bea01e":"data_real = data[data.target == 1]\ndata_fake = data[data.target == 0]","eba47f79":"real = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_real.shape[0]):\n    x = data_real.iloc[i].tweet_lemms\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1]\n    real.append(x) ","cd5964b6":"pred=[line for line in real for line in set(line)]\npred = Counter(pred)\npred = pred.most_common(20)\npred=pd.DataFrame(pred,columns = ['Words', 'Frequency'])\npred.head(10)","a81c95b0":"fake = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_fake.shape[0]):\n    x = data_fake.iloc[i].tweet_lemms\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    fake.append(x) ","91769cfb":"nopred=[line for line in fake for line in set(line)]\nnopred = Counter(nopred)\nnopred = nopred.most_common(20)\nnopred=pd.DataFrame(nopred,columns = ['Words', 'Frequency'])\nnopred.head(10)","2e0d9838":"plt.figure(figsize=(12,12))\n\nplt.subplot(321)\nreal=(\" \").join(pred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800 ).generate(real)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('Real disaster', fontsize=25)\n\nplt.subplot(322)\nfake=(\" \").join(nopred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(fake)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('fake disaster', fontsize=25)\n\nplt.tight_layout()\nplt.show()","77f0feb6":"data2 = data.copy()","c3ab32c4":"data2['cat']=data['target'].astype(\"category\").cat.rename_categories({0:'fake',1:'real'})","64c3707a":"%%time\nscatter_corpus_nlp = st.CorpusFromPandas(data2,\n                             category_col='cat',\n                             text_col='tweet_lemms',nlp=nlp).build()","5e770f16":"html = st.produce_scattertext_explorer(scatter_corpus_nlp,\n         category='real',category_name='real',         \n        not_category_name='fake',width_in_pixels=1000,\n          metadata=data2['cat'])\nopen(\"tweet-Visualization.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='tweet-Visualization.html', width = 1300, height=700)","f6a5fb05":"%%time\n\nfeat_builder = st.FeatsFromOnlyEmpath()\nempath_corpus = st.CorpusFromParsedDocuments(data2,\n                                              category_col='cat',\n                                              feats_from_spacy_doc=feat_builder,\n                                              parsed_col='tweet_lemms').build()\nhtml = st.produce_scattertext_explorer(empath_corpus,\n                                        category='real',\n                                        category_name='real',\n                                        not_category_name='fake',\n                                        width_in_pixels=1000,\n                                        metadata=data2['cat'],\n                                        use_non_text_features=True,\n                                        use_full_doc=True,\n                                        topic_model_term_lists=feat_builder.get_top_model_term_lists())\nopen(\"tweet-Visualization-Empath.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='tweet-Visualization-Empath.html', width = 1300, height=700)","3e0fc1d1":"blob = TextBlob(str(data['tweet_lemms']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.iplot(\n    kind='bar',\n    xTitle='POS',\n    yTitle='count', \n    title='Top 12 Part-of-speech tagging para tweet_lemms')","6eae2b97":"def counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","c1de00bc":"counter = counter_word(data[\"tweet_lemms\"])","ca3245a1":"vocab_size = len(counter)\nembedding_dim = 32\n\nmax_length = 20\ntrunc_type='post'\npadding_type='post'\n\noov_tok = \"<XXX>\"\nseq_len = 12","08c1b9dc":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","6b6d3d81":"X_train = data[\"tweet_lemms\"].values\nX_test = test[\"tweet_lemms\"].values\ny_train = data.target","86da1e7d":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)","f3042f92":"X_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)","3f934b55":"X_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)","fadaeec5":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import GaussianNoise\nimport keras","c81a3ddb":"custom_early_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=50,\n    restore_best_weights=True\n)","c6048ea6":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.GaussianNoise(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.GaussianNoise(0.6),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.GaussianNoise(0.6),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.GaussianNoise(0.5),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","91ddd64f":"model.summary()","a3668f76":"epochs = 100\nbatch_size = 32","1a4ebfba":"history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.3,callbacks=[custom_early_stopping])","50eeb7e7":"subm = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\npreds = model.predict(X_test)\nsubm['target'] = (preds > 0.5).astype(int)\nsubm.to_csv(\"LSTM.csv\", index=False, header=True)","8d22a027":"## Visualization of specific terms but that have low occurrence compared to general terms.","ef788b8c":"## Visualization of words in real \/ fake.","577690a3":"## Train","10d24a3f":"# Deep Learning.","a208a2d7":"## Earlystopping.","cb988303":"## Scattertext.","aeb5e858":"# EDA","15ff56cb":"### Train Test Split","23059bbc":"## Submission","16351527":"## LSTM.","d7941458":"## wordcloud.","f79a0988":"## Textblob.","714712e9":"## NLP and stopwords.","68ebff8c":"# Preprocessing","d3474a36":"## Test","fc6af4c1":"## Spellcheck Activation."}}