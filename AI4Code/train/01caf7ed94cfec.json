{"cell_type":{"fe6b4792":"code","40fd3859":"code","d1d54dbc":"code","b004cbd6":"code","c0a36f58":"code","97585f69":"code","3e79df55":"code","abf5ad56":"code","6607a7d5":"code","7231f382":"code","493e30af":"code","dec4247d":"code","c728eaa7":"code","e8184306":"code","857acc0e":"code","09b0eaf3":"markdown","f4c4067a":"markdown","69f74999":"markdown","ec324215":"markdown","389d3d85":"markdown","a7a38202":"markdown","ee783c40":"markdown","74d684eb":"markdown","79a8e8ac":"markdown","662ceb78":"markdown","07b23000":"markdown"},"source":{"fe6b4792":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.optimize import minimize","40fd3859":"train = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')\ntest = pd.read_csv('..\/input\/data-without-drift\/test_clean.csv')\ntt = pd.concat([train, test], sort=False)","d1d54dbc":"def add_model_groups(tt):\n    tt.loc[(tt['time'] > 0) & (tt['time'] <= 10), 'sbatch'] = 0\n    tt.loc[(tt['time'] > 10) & (tt['time'] <= 50), 'sbatch'] = 1\n    tt.loc[(tt['time'] > 50) & (tt['time'] <= 60), 'sbatch'] = 2\n    tt.loc[(tt['time'] > 60) & (tt['time'] <= 100), 'sbatch'] = 3\n    tt.loc[(tt['time'] > 100) & (tt['time'] <= 150), 'sbatch'] = 4\n    tt.loc[(tt['time'] > 150) & (tt['time'] <= 200), 'sbatch'] = 5\n    tt.loc[(tt['time'] > 200) & (tt['time'] <= 250), 'sbatch'] = 6\n    tt.loc[(tt['time'] > 250) & (tt['time'] <= 300), 'sbatch'] = 7\n    tt.loc[(tt['time'] > 300) & (tt['time'] <= 350), 'sbatch'] = 8\n    tt.loc[(tt['time'] > 350) & (tt['time'] <= 400), 'sbatch'] = 9\n    tt.loc[(tt['time'] > 400) & (tt['time'] <= 450), 'sbatch'] = 10\n    tt.loc[(tt['time'] > 450) & (tt['time'] <= 500), 'sbatch'] = 11\n    # Test\n    tt.loc[(tt['time'] > 500) & (tt['time'] <= 510), 'sbatch'] = 12\n    tt.loc[(tt['time'] > 510) & (tt['time'] <= 520), 'sbatch'] = 13\n    tt.loc[(tt['time'] > 520) & (tt['time'] <= 530), 'sbatch'] = 14\n    tt.loc[(tt['time'] > 530) & (tt['time'] <= 540), 'sbatch'] = 15\n    tt.loc[(tt['time'] > 540) & (tt['time'] <= 550), 'sbatch'] = 16\n    tt.loc[(tt['time'] > 550) & (tt['time'] <= 560), 'sbatch'] = 17\n    tt.loc[(tt['time'] > 560) & (tt['time'] <= 570), 'sbatch'] = 18\n    tt.loc[(tt['time'] > 570) & (tt['time'] <= 580), 'sbatch'] = 19\n    tt.loc[(tt['time'] > 580) & (tt['time'] <= 590), 'sbatch'] = 20\n    tt.loc[(tt['time'] > 590) & (tt['time'] <= 600), 'sbatch'] = 21\n    tt.loc[(tt['time'] > 600) & (tt['time'] <= 610), 'sbatch'] = 22\n    tt.loc[(tt['time'] > 610) & (tt['time'] <= 630), 'sbatch'] = 23\n    tt.loc[(tt['time'] > 630) & (tt['time'] <= 650), 'sbatch'] = 24\n    tt.loc[(tt['time'] > 650) & (tt['time'] <= 670), 'sbatch'] = 25\n    tt.loc[(tt['time'] > 670) & (tt['time'] <= 700), 'sbatch'] = 26\n    return tt\n\ndef had_drift(tt):\n    \"\"\"\n    I dentify if section had drift in the original dataset\n    \"\"\"\n    tt.loc[(tt['time'] > 0) & (tt['time'] <= 10), 'drift'] = False\n    tt.loc[(tt['time'] > 10) & (tt['time'] <= 50), 'drift'] = False\n    tt.loc[(tt['time'] > 50) & (tt['time'] <= 60), 'drift'] = True\n    tt.loc[(tt['time'] > 60) & (tt['time'] <= 100), 'drift'] = False\n    tt.loc[(tt['time'] > 100) & (tt['time'] <= 150), 'drift'] = False\n    tt.loc[(tt['time'] > 150) & (tt['time'] <= 200), 'drift'] = False\n    tt.loc[(tt['time'] > 200) & (tt['time'] <= 250), 'drift'] = False\n    tt.loc[(tt['time'] > 250) & (tt['time'] <= 300), 'drift'] = False\n    tt.loc[(tt['time'] > 300) & (tt['time'] <= 350), 'drift'] = True\n    tt.loc[(tt['time'] > 350) & (tt['time'] <= 400), 'drift'] = True\n    tt.loc[(tt['time'] > 400) & (tt['time'] <= 450), 'drift'] = True\n    tt.loc[(tt['time'] > 450) & (tt['time'] <= 500), 'drift'] = True\n    # Test\n    tt.loc[(tt['time'] > 500) & (tt['time'] <= 510), 'drift'] = True\n    tt.loc[(tt['time'] > 510) & (tt['time'] <= 520), 'drift'] = True\n    tt.loc[(tt['time'] > 520) & (tt['time'] <= 530), 'drift'] = False\n    tt.loc[(tt['time'] > 530) & (tt['time'] <= 540), 'drift'] = False\n    tt.loc[(tt['time'] > 540) & (tt['time'] <= 550), 'drift'] = True\n    tt.loc[(tt['time'] > 550) & (tt['time'] <= 560), 'drift'] = False\n    tt.loc[(tt['time'] > 560) & (tt['time'] <= 570), 'drift'] = True\n    tt.loc[(tt['time'] > 570) & (tt['time'] <= 580), 'drift'] = True\n    tt.loc[(tt['time'] > 580) & (tt['time'] <= 590), 'drift'] = True\n    tt.loc[(tt['time'] > 590) & (tt['time'] <= 600), 'drift'] = False\n    tt.loc[(tt['time'] > 600) & (tt['time'] <= 610), 'drift'] = True\n    tt.loc[(tt['time'] > 610) & (tt['time'] <= 630), 'drift'] = True\n    tt.loc[(tt['time'] > 630) & (tt['time'] <= 650), 'drift'] = True\n    tt.loc[(tt['time'] > 650) & (tt['time'] <= 670), 'drift'] = False\n    tt.loc[(tt['time'] > 670) & (tt['time'] <= 700), 'drift'] = False\n    return tt","b004cbd6":"tt = had_drift(tt)\ntt = add_model_groups(tt)\nFILTER_TRAIN = '(time <= 47.6 or time > 48) and (time <= 364 or time > 382.4)'\ntt = tt.query(FILTER_TRAIN)\ntt['drift'] = tt['drift'].astype('bool')","c0a36f58":"for i, d in tt.groupby('open_channels'):\n    d.query('not drift')['signal'].value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n              title='Value Counts by Signal (Excluding Drift Sections)')\nplt.legend()\nplt.show()","97585f69":"anchors = [-2.5002, -1.2502, -0.0002, 1.2498, 2.4998, 3.7498]\ntt.query('drift == False and signal in @anchors').groupby(['signal','open_channels'])[['time']].count()","3e79df55":"fig, ax = plt.subplots()\ntt.query('drift == False and signal in @anchors') \\\n    .groupby('open_channels') \\\n    .plot(x='time', y='signal', style='.', figsize=(15, 5), ax=ax)\nplt.show()","abf5ad56":"tt['signal_shift'] = np.nan\ntt.loc[~tt['drift'], 'signal_shift'] = tt.loc[~tt['drift']]['signal']","6607a7d5":"# Identify the batches with drift\ndrift_batches = tt.query('drift')['sbatch'].unique()\nprint(drift_batches)","7231f382":"for db in drift_batches:\n    d = tt.query('sbatch == @db')\n    def shift_and_anchor_count(shift):\n        anchor_count = 0\n        shifted_signal = (d['signal'] + shift).round(4)\n        for a in anchors:\n    #         print(a)\n            n_anchors = (shifted_signal == a).sum()\n    #         print(n_anchors)\n            anchor_count += n_anchors\n    #     print(f'Shift {shift} ---> anchor count: {anchor_count}')\n    \n\n        return -anchor_count\n    res = minimize(shift_and_anchor_count, [0], method='Powell', tol=1e-6)\n    opt_shift = res['x']\n    print(f'Drift batch {db} - optimal shift {opt_shift}')\n    tt.loc[tt['sbatch'] == db, 'signal_shift'] = (tt.loc[tt['sbatch'] == db]['signal'] - opt_shift).round(4)","493e30af":"for db in drift_batches:\n    d = tt.query('sbatch == @db')\n    def shift_and_anchor_count(shift):\n        shift = shift\/1000\n        anchor_count = 0\n        shifted_signal = (d['signal'] + shift).round(4)\n        for a in anchors:\n            n_anchors = (shifted_signal == a).sum()\n            anchor_count += n_anchors\n            # Penalize for high counts neighbors numbers being high\n            pprior = round(a - 0.0002, 4)\n            prior = round(a - 0.0001, 4)\n            post = round(a + 0.0001, 4)\n            ppost = round(a + 0.0002, 4)\n            n_anchor_prior = (shifted_signal == prior).sum()\n            n_anchor_pprior = (shifted_signal == pprior).sum()\n#             print(n_anchor_prior)\n            anchor_count -= (prior - pprior)\n            n_anchor_post = (shifted_signal == post).sum()\n            n_anchor_ppost = (shifted_signal == ppost).sum()\n            anchor_count -= (post - ppost)\n#         print()\n        return -anchor_count\n    res = minimize(shift_and_anchor_count, [0], method='Powell') #, bounds=(-0.0001, 0.0001))\n    opt_shift = res['x']\n    print(f'Drift batch {db} - optimal shift {opt_shift}')\n    tt.loc[tt['sbatch'] == db, 'signal_shift'] = (tt.loc[tt['sbatch'] == db]['signal'] + (opt_shift\/1000)).round(4)","dec4247d":"tt['signal_round4'] = tt['signal_shift'].round(4)","c728eaa7":"anchors = [-2.5002, -1.2502, -0.0002, 1.2498, 2.4998, 3.7498]\ntt.query('drift and signal_shift in @anchors').groupby(['signal_shift','open_channels'])[['time']].count()","e8184306":"for i, d in tt.groupby('open_channels'):\n    d.query('drift')['signal'].round(4) \\\n        .value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n              title='Unique Value Counts in Drift Segments before shifting')\nplt.show()","857acc0e":"for i, d in tt.groupby('open_channels'):\n    d.query('drift')['signal_round4'] \\\n        .round(4).value_counts() \\\n        .plot(figsize=(15, 5), style='.', label=i,\n             title='Unique Value Counts in Drift Data after Shifting to Optimize Anchors')\nplt.legend()\nplt.show()","09b0eaf3":"## First attempt: maximize anchor count.","f4c4067a":"## In the end this \"shifted\" data did not improve my CV\/LB Score and I'm still confused by why it exists.\n- Can you solve this mystery for me?","69f74999":"# We wee a high number of values with these signal values:\n- Each corresponding to open channels:\n```\n    open_channels -> signal\n    0 -> -2.5002\n    1 -> -1.2502\n    2 -> -0.0002\n    3 -> 1.2498\n    4 -> 2.4998\n    5 -> 3.7498\n```","ec324215":"Lets look at the value count of these in the data:","389d3d85":"This looks much cleaner, right?","a7a38202":"# Plotting only the anchor points","ee783c40":"# Anchor Point Mystery\n\nHelp me solve this mystery.\n\nDuring the ion challenge I found something interesting in the data that I still can't explain. Hopefully someone has a good explaination for it.\n\nMy attempt during the competition was to shift the \"drift\" sections of the data to maximize these points. I was successful but it didn't end up providing any fruitful to my model.","74d684eb":"# I attempted to \"shift\" the drift sections to maximize the number of these values","79a8e8ac":"## We now have a high value count of our anchor points in non-drift areas","662ceb78":"# Lets plot the drift sections before and after this shift:","07b23000":"## Second attempt + max and minimize surrounding values.\n- This was a better way is to also minize if surrounding values of \"anchors\" have high value counts:"}}