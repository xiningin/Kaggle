{"cell_type":{"d1da2962":"code","8d2bd359":"code","d838d404":"code","767bfc77":"code","dac82ef9":"code","189274d3":"code","6bd0044c":"code","874dcd6b":"code","1c136b9f":"code","fc87e5db":"code","8c3f9557":"code","fcbac93c":"code","8bde407b":"code","f5dae426":"code","0c351c14":"code","2db9a5b8":"code","b6d06dc0":"code","e4fa76ee":"code","31d08d71":"code","fb88d460":"code","16608abe":"code","12e1f7c8":"code","acf80bee":"code","c730ab8f":"code","2dbabe43":"code","e0d76666":"code","201a26ce":"code","40b1c3ff":"code","15895f02":"code","9c47cb28":"code","fae13693":"code","f8a3ef7d":"code","b59b5c28":"code","c2257335":"code","94fef5c5":"code","492093cf":"code","6efd0c02":"code","30bf57de":"code","1fc1b294":"code","60609745":"code","ffc5ec1c":"code","de44bbe8":"code","67f9576b":"code","f3450285":"code","31e36561":"code","80cc1f8f":"code","9237319c":"code","3fd143ee":"code","76f75469":"markdown","d2b418a2":"markdown","fb1dc6a0":"markdown","c9ad082d":"markdown","14f9a681":"markdown","0d8d4b78":"markdown","622881c8":"markdown","953df436":"markdown","8fe0111f":"markdown","a6ba5e89":"markdown","ea4c967d":"markdown","67540b4f":"markdown","ee14ff78":"markdown","ea8605f8":"markdown","a7ce3926":"markdown","7aa504f6":"markdown","a2014b33":"markdown","097bae4f":"markdown","4cf74ef9":"markdown","930ee412":"markdown","8e9fb645":"markdown","f6a796d9":"markdown","de0d3af4":"markdown","26b213b0":"markdown","bb3376e7":"markdown","18e2c40c":"markdown","20adaf24":"markdown","60c99a0c":"markdown","31858f09":"markdown","c7fd994a":"markdown","b1ac7d6f":"markdown","06d0d5a8":"markdown","a460242b":"markdown","7bed8b1d":"markdown","dbacdcae":"markdown","98ebcd99":"markdown","3852d4e8":"markdown","40308ae3":"markdown","1726631e":"markdown","28a064db":"markdown","bc3fe468":"markdown"},"source":{"d1da2962":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d2bd359":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as st\nimport seaborn as sb\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,PowerTransformer\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score\nfrom sklearn.decomposition import PCA\nimport warnings\nfrom xgboost import XGBClassifier\n# from empiricaldist import Cdf,Pmf\nimport missingno as msno\nfrom pprint import pprint\n!pip install impyute\nfrom impyute.imputation.cs import fast_knn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","d838d404":"heart=pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')\nheart.drop(['education'],inplace=True,axis=1)\nheart.head()","767bfc77":"def remove_index(df,info=False):\n    shape_0=df.shape[0]\n    for i in df.columns:\n        if len(df[i].unique())==shape_0:\n            df.drop([i],inplace=True,axis=1)\n    if (info == True):\n        print (df.info())\n    return df","dac82ef9":"heart=remove_index(heart,info=False)","189274d3":"heart_columns=heart.columns","6bd0044c":"def HandleCategory(df,info=False,describe=False):\n    \n    datatypes=dict(df.dtypes)\n    dependentVariable=len(datatypes)-1\n    countIndependentList=[]\n    countIndependent=-1\n    for key in datatypes:\n      countIndependent+=1\n      if (datatypes[key]==np.dtype('O')):\n        key_new=pd.get_dummies(df[key],drop_first=True)\n        df.drop([key],inplace=True,axis=1)\n        df=pd.concat([df,key_new],axis=1)\n        dependentVariable-=1\n    if (info==True):\n        pprint (df.info())\n    elif (describe==True):\n        pprint(df.describe())\n    return df","874dcd6b":"heart=HandleCategory(heart,info=True)","1c136b9f":"pprint(heart.isnull().sum())\nmsno.matrix(heart)","fc87e5db":"x=heart[heart['BPMeds'].isnull()].index.tolist()","8c3f9557":"heart=heart.drop(x,axis=0)\nheart.describe()","fcbac93c":"pprint(heart.isnull().sum())","8bde407b":"x=heart[heart['heartRate'].isnull()].index.tolist()\nheart=heart.drop(x,axis=0)","f5dae426":"missin=IterativeImputer()\nheart=pd.DataFrame(missin.fit_transform(heart),columns=heart_columns)","0c351c14":"msno.matrix(heart)","2db9a5b8":"counts=heart['TenYearCHD'].value_counts()\nplt.figure(figsize=(10,5))\nsb.barplot(counts.index, counts.values, alpha=0.8)\nplt.show()","b6d06dc0":"plt.figure(figsize=(20,15))\nheart_corr=heart.corr()\nsb.heatmap(heart_corr,cmap=\"Blues\", vmin= -2.0, vmax=1,\n           linewidth=0.1, cbar_kws={\"shrink\": .8},annot=True)\nplt.show()","e4fa76ee":"upper_tri = heart_corr.where(np.triu(np.ones(heart_corr.shape),k=1).astype(np.bool))\n# print (upper_tri['prevalentStroke'])\nprint ([i for i in upper_tri if any(upper_tri[i]>0.70)],\"columns would be dropped\")\nheart.drop([i for i in upper_tri if any(upper_tri[i]>0.70)],inplace=True,axis=1)","31d08d71":"heart.describe()","fb88d460":"# from statsmodels.tools import add_constant\n# heart_constant = add_constant(heart)\n# heart_constant.head()\n# heart=heart_constant.copy()","16608abe":"# sb.set_style('darkgrid')\n# sb.distplot(heart['male'])","12e1f7c8":"plt.figure(figsize=(20,10),dpi=80, facecolor='gray', edgecolor='yellow')\nheart.boxplot(column=[i for i in heart.columns])\nplt.show()\nprint(heart.shape)","acf80bee":"# plt.figure(figsize=(20,10),dpi=80, facecolor='gray', edgecolor='yellow')\n# sb.boxplot(y='age',x='TenYearCHD',data=heart,whis=10)\n# plt.yscale('log')  #to see data on log scale\n# plt.show()","c730ab8f":"# plt.figure(figsize=(20,10),dpi=80, facecolor='gray', edgecolor='yellow')\n# sb.kdeplot(data=heart['age'])\n# plt.show()\n# plt.figure(figsize=(20,10),dpi=80, facecolor='gray', edgecolor='yellow')\n# sb.violinplot(x='age',y='heartRate',data=heart,inner=None)\n# sb.despine(left=True,bottom=True)\n# plt.show()","2dbabe43":"def quantile_trans(string,minn,maxx,name):\n    max_threshold,min_threshold=name[str(string)].quantile([maxx,minn])\n    name=name[(heart[str(string)]<max_threshold) & (name[str(string)] > min_threshold)]\n    return name","e0d76666":"def iqrFunc(string,minn,maxx,name):\n    max_threshold,min_threshold=name[str(string)].quantile([maxx,minn])\n    iqr=max_threshold-min_threshold\n    upperLimit=max_threshold-1.5*iqr\n    lowerLimit=min_threshold-1.5-iqr\n    name=name[(heart[str(string)]<upperLimit) & (name[str(string)] > lowerLimit)]\n    return name","201a26ce":"TempIsolationHeart=heart","40b1c3ff":"from sklearn.ensemble import IsolationForest\nmodelIsolation=IsolationForest(contamination=0.14,random_state=0)\nmodelIsolation.fit(TempIsolationHeart)\npredictIsolation=modelIsolation.predict(TempIsolationHeart)","15895f02":"# mask=[]\n# for i in range(len(predictIsolation)):\n#     if (predictIsolation[i]==-1):\n#         mask.append(i)\n# TempIsolationHeart=TempIsolationHeart.drop(mask)\n# TempIsolationHeart.describe()","9c47cb28":"power=PowerTransformer(method=\"yeo-johnson\",standardize=True)","fae13693":"TempIsolationHeart=pd.DataFrame(TempIsolationHeart,columns=heart.columns)","f8a3ef7d":"counts=TempIsolationHeart['TenYearCHD'].value_counts()\nplt.figure(figsize=(10,5))\nsb.barplot(counts.index, counts.values, alpha=0.8)\nplt.show()","b59b5c28":"\n# heart=quantile_trans('totChol',0.01,0.83,heart)\n# #heart=iqrFunc('totChol',0.2,0.9,heart)\n# print(heart.shape)\n# plt.figure(figsize=(20,10),dpi=80, facecolor='gray', edgecolor='yellow')\n# heart.boxplot(column=[i for i in heart.columns])\n# plt.show()","c2257335":"from imblearn.combine import SMOTETomek\nsmt=SMOTETomek(random_state=42,sampling_strategy='auto')","94fef5c5":"from collections import Counter","492093cf":"heart_x=TempIsolationHeart.iloc[:,:-1].values\nheart_y=TempIsolationHeart.iloc[:,-1].values\nnew_heart_x,new_heart_y=smt.fit_resample(heart_x,heart_y)\n# sc=StandardScaler()\n# columnsHeart_x=TempIsolationHeart.columns\n# heart_x=sc.fit_transform(heart_x)\n# pca=PCA(n_components=9)\n# new_heart_x=pca.fit_transform(new_heart_x)\n# plt.figure()\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlim(0,7,1)\n# plt.xlabel('Number of components')\n# plt.ylabel('Cumulative explained variance')\n# plt.show()\n\ntrain_x,test_x,train_y,test_y=train_test_split(new_heart_x,new_heart_y,test_size=0.2,random_state=0)\n# train_x,train_y=smt.fit_resample(train_x,train_y)\n# train_x=power.fit_transform(train_x)\n# test_x=power.fit_transform(test_x)\nprint (\"test:\",Counter(test_y),\"train:\",Counter(train_y))\ntrain_y=train_y.reshape(-1,1)\ntest_y=test_y.reshape(-1,1)\n","6efd0c02":"counts=test_y\ncounts=pd.DataFrame(counts).value_counts()\nplt.figure(figsize=(10,5))\nsb.barplot(counts.index, counts.values, alpha=0.8)\nplt.show()","30bf57de":"counts=train_y\ncounts=pd.DataFrame(counts).value_counts()\nplt.figure(figsize=(10,5))\nsb.barplot(counts.index, counts.values, alpha=0.8)\nplt.show()","1fc1b294":"score=[]\n\ncv = KFold(n_splits=10, random_state=0)\nmaxx=0\nfor train_index, test_index in cv.split(train_x):\n    \n    regr=LogisticRegression(penalty='l2',solver='saga')\n    y=regr.fit(train_x[train_index], train_y[train_index])\n    prediction= y.predict(train_x[test_index])\n    train_score=y.score(train_x[train_index], train_y[train_index])\n    test_score=y.score(train_x[test_index],train_y[test_index])\n    test_prediction=y.predict(test_x)\n    accuracy_test=accuracy_score(test_y,test_prediction)\n    false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(test_y, test_prediction)\n\n    #mse_test=mean_squared_error(test_y,test_prediction)\n    \n    if (maxx<accuracy_test):\n        maxx=accuracy_test\n        intercept=y.intercept_\n        coef=pd.concat([pd.DataFrame(heart.columns),pd.DataFrame(np.transpose(y.coef_))], axis = 1)\n        trainScore=train_score\n        validationTestScore=test_score\n        roc_auc=roc_auc_score(test_y, test_prediction)\n        false_positive_rate=false_positive_rate1\n        true_positive_rate=true_positive_rate1","60609745":"print('training score:',trainScore,'\\n','Test Score: ',maxx,'\\n','intercept: ',intercept,'\\n','coeficient: ',coef,'\\n','Validation Test score:',validationTestScore,'\\n','ROC-AUC Score:',roc_auc)\n\nplt.figure(figsize=(15,10))\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint (confusion_matrix(test_y,test_prediction))","ffc5ec1c":"\nscore=[]\n\ncv = KFold(n_splits=10, random_state=42)\nmaxx=0\nfor train_index, test_index in cv.split(train_x):\n    \n    regr=RandomForestClassifier(n_estimators=30,criterion='gini',random_state=42)\n    y=regr.fit(train_x[train_index], train_y[train_index])\n    prediction= y.predict(train_x[test_index])\n    train_score=y.score(train_x[train_index], train_y[train_index])\n    test_score=y.score(train_x[test_index],train_y[test_index])\n    test_prediction=y.predict(test_x)\n    accuracy_test=accuracy_score(test_y,test_prediction)\n    false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(test_y, test_prediction)\n\n    #mse_test=mean_squared_error(test_y,test_prediction)\n    \n    if (maxx<accuracy_test):\n        maxx=accuracy_test\n        trainScore=train_score\n        validationTestScore=test_score\n        roc_auc=roc_auc_score(test_y, test_prediction)\n        false_positive_rate=false_positive_rate1\n        true_positive_rate=true_positive_rate1","de44bbe8":"print('training score:',trainScore,'\\n','Test Score: ',maxx,'\\n','intercept: ','intercept','\\n','coeficient: ','coef','\\n','Validation Test score:',validationTestScore,'\\n','ROC-AUC Score:',roc_auc)\n\nplt.figure(figsize=(15,10))\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint (confusion_matrix(test_y,test_prediction))","67f9576b":"score=[]\n\ncv = KFold(n_splits=10, random_state=0)\nmaxx=0\nfor train_index, test_index in cv.split(train_x):\n    \n    regr=make_pipeline(SVC(kernel=\"rbf\",gamma=\"auto\",degree=4,random_state=0))\n    y=regr.fit(train_x[train_index], train_y[train_index])\n    prediction= y.predict(train_x[test_index])\n    train_score=y.score(train_x[train_index], train_y[train_index])\n    test_score=y.score(train_x[test_index],train_y[test_index])\n    test_prediction=y.predict(test_x)\n    accuracy_test=accuracy_score(test_y,test_prediction)\n    false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(test_y, test_prediction)\n\n    #mse_test=mean_squared_error(test_y,test_prediction)\n    \n    if (maxx<accuracy_test):\n        maxx=accuracy_test\n        trainScore=train_score\n        validationTestScore=test_score\n        roc_auc=roc_auc_score(test_y, test_prediction)\n        false_positive_rate=false_positive_rate1\n        true_positive_rate=true_positive_rate1","f3450285":"print('training score:',trainScore,'\\n','Test Score: ',maxx,'\\n','intercept: ','intercept','\\n','coeficient: ','coef','\\n','Validation Test score:',validationTestScore,'\\n','ROC-AUC Score:',roc_auc)\n\nplt.figure(figsize=(15,10))\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint (confusion_matrix(test_y,test_prediction))","31e36561":"score=[]\n\ncv = KFold(n_splits=20, random_state=0)\nmaxx=0\nfor train_index, test_index in cv.split(train_x):\n    \n    regr=XGBClassifier(n_estimators=100,booster=\"gbtree\",learning_rate=0.3)\n    y=regr.fit(train_x[train_index], train_y[train_index])\n    prediction= y.predict(train_x[test_index])\n    train_score=y.score(train_x[train_index], train_y[train_index])\n    test_score=y.score(train_x[test_index],train_y[test_index])\n    test_prediction=y.predict(test_x)\n    accuracy_test=accuracy_score(test_y,test_prediction)\n    false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(test_y, test_prediction)\n\n    #mse_test=mean_squared_error(test_y,test_prediction)\n    \n    if (maxx<accuracy_test):\n        maxx=accuracy_test\n        trainScore=train_score\n        validationTestScore=test_score\n        roc_auc=roc_auc_score(test_y, test_prediction)\n        false_positive_rate=false_positive_rate1\n        true_positive_rate=true_positive_rate1","80cc1f8f":"print('training score:',trainScore,'\\n','Test Score: ',maxx,'\\n','intercept: ','intercept','\\n','coeficient: ','coef','\\n','Validation Test score:',validationTestScore,'\\n','ROC-AUC Score:',roc_auc)\n\nplt.figure(figsize=(15,10))\nplt.plot(false_positive_rate1, true_positive_rate1)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint (confusion_matrix(test_y,test_prediction))","9237319c":"# def doPrediction(train_x,test_x,train_y,test_y,meth=\"logistic\",LogisticPenalty='l2',):\n#     Meth={\"logistic\":LogisticRegression(penalty=LogisticPenalty)}\n#     score=[]\n\n#     cv = KFold(n_splits=10, random_state=42)\n#     maxx=0\n#     for train_index, test_index in cv.split(train_x):\n\n#         regr=LogisticRegression(penalty='l2',solver='saga',C=0.5,tol=0.1)\n#         y=regr.fit(train_x[train_index], train_y[train_index])\n#         prediction= y.predict(train_x[test_index])\n#         train_score=y.score(train_x[train_index], train_y[train_index])\n#         test_score=y.score(train_x[test_index],train_y[test_index])\n#         test_prediction=y.predict(test_x)\n#         accuracy_test=accuracy_score(test_y,test_prediction)\n#         false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(test_y, test_prediction)\n\n#         #mse_test=mean_squared_error(test_y,test_prediction)\n\n#         if (maxx<accuracy_test):\n#             maxx=accuracy_test\n#             intercept=y.intercept_\n#             coef=pd.concat([pd.DataFrame(heart.columns),pd.DataFrame(np.transpose(y.coef_))], axis = 1)\n#             trainScore=train_score\n#             validationTestScore=test_score\n#             roc_auc=roc_auc_score(test_y, test_prediction)\n#             false_positive_rate=false_positive_rate1\n#             true_positive_rate=true_positive_rate1","3fd143ee":"# from sklearn.manifold import TSNE\n# tsne=TSNE(n_components=2)\n# X_embedded = tsne.fit_transform(TempIsolationHeart)\n# sb.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=sb.color_palette(\"bright\"))","76f75469":"What we can do now is , we can either upsample our minority dataset values i.e *add a synthetic dataset having which is in minorty ,here its 'one'*, or we can downsample the majority dataset values ,here 'zeroes', but before that we have to remove outliers from our dataset","d2b418a2":"## Output","fb1dc6a0":"**Our dataset is quite unbalanced, as we can see there are more number of zero values than one**","c9ad082d":"# Reading Data","14f9a681":"- Function to remove all the index values column from dataset","0d8d4b78":"getting columns of the dataset\n","622881c8":"## Value Count Train Data","953df436":"# Logistic Regression Model","8fe0111f":"## FInding all the columns which are highly correlated and dropping them","a6ba5e89":"## Balancing Dataset","ea4c967d":"### Now we have to classify that under which category our dataset falls\n- **MCAR**(Missing completely at random)\n- **MAR**(Missing at random)\n- **NMAR**(Not missing at random)","67540b4f":"## There are some automatic outlier detector technique for multidimensional datsets","ee14ff78":"## Training and testing splitting","ea8605f8":"lets remove the rows where BPMeds is null, since its a binary class and filling missing value to it could be arduous task","a7ce3926":"## Value count test data","7aa504f6":"- info=True if you want dataframe info too","a2014b33":"## Checking missing values","097bae4f":"### 1. Isolation Forest","4cf74ef9":"# XgBoost","930ee412":"# Preprocessing","8e9fb645":"### It belong to NMAR since we can use other features to find the missing values and to find the missing values we can use Iterative Imputer module which basically iterate to find the best values for the missing data","f6a796d9":"1. ### Now we will upsample the minorty class and downsample the majority class in the data","de0d3af4":"## Value Count","26b213b0":"## Correlation Matrix","bb3376e7":"## Boxplot to search for outliers","18e2c40c":"# Support Vector Classifier","20adaf24":"## Outliers in number","60c99a0c":"### IQR method","31858f09":"## Handling categorical data","c7fd994a":"## Value Count","b1ac7d6f":"### Quantile Method","06d0d5a8":"- it takes in assumption that last column should be dependent variable","a460242b":"# Importing required libraries","7bed8b1d":"there is only one null value in heart rate now , lets drop that column also","dbacdcae":"## Adding a bias","98ebcd99":"## Validation set and Training set with model creation","3852d4e8":"select all the rows that are nor outliers","40308ae3":"# Random Forest","1726631e":"These are basically tree based anomaly detection techniques which take two quantitative properties in consideration\n1. anomalies that are fewer in number\n2. that have very different attributes than other data points ","28a064db":"## Quantile Method and Inter Quantile Method are basically used for Univariate datasets and not multidimensional datasets, on multivariate datasets they would breakdown","bc3fe468":"**Since \"cigsPerDay\", \"BPMeds\",\"totChol\",\"BMI\" and \"heart rate\" values have very few missing values , we can use interpolation**"}}