{"cell_type":{"f182b1ce":"code","9f1f398e":"code","810c29e8":"code","ac19fc20":"code","480fabf1":"code","c2ddd6a0":"code","ccb56b96":"code","926c52f7":"code","e6d097c6":"code","97da1e1b":"code","9c598eb4":"code","361caef5":"code","91c27329":"code","67a7f9e5":"code","e4e764e2":"code","1aab85da":"code","2cfee687":"code","89113b8b":"code","321fe41c":"code","968667a6":"code","68278bad":"code","f0b9d375":"code","71926671":"code","e0ab6f6c":"code","adc8f33d":"code","5b4272b6":"code","04d5413d":"code","87b0453f":"code","121c74b5":"code","79842bcb":"code","6c05487c":"code","74b2e166":"code","ad90da6a":"code","1ff1deed":"code","710f42d4":"code","e7b7bebc":"code","db54d338":"code","893c2abc":"code","f5fde5af":"code","21cf5ca7":"code","b26a16c1":"code","569512b2":"code","913840af":"code","ed251fcd":"code","9ab432d7":"code","40a70d1e":"code","d1bf0f67":"code","0e14658c":"code","f011c4f8":"code","02f76b4b":"code","4fd7be69":"code","5c84571b":"code","7ced93d9":"code","7d47b9ee":"code","50644705":"code","f3cdfa76":"code","a02677d4":"code","fc9aa6bf":"code","a3c06069":"code","b2515e38":"code","2d238243":"code","6eaab680":"code","9499bf46":"code","2520bf39":"code","7afdcf88":"code","307281dc":"code","4b73fedb":"code","a86cf723":"code","768b93a7":"code","d5903c4f":"code","f1481bb0":"code","1f9b5ed8":"code","e77e5632":"code","e8ad3bb5":"code","282be050":"code","ed2e4bdd":"code","ee347efb":"code","34c72e84":"code","a87a0c71":"code","892a42f6":"code","79499006":"code","18747f33":"code","0cd86c0c":"code","f7097f81":"code","5315e0bd":"code","928e69c5":"code","dac441ab":"code","3deeecd6":"code","d83ae929":"code","a2d567ef":"code","2d2b8e59":"code","825f1bf3":"code","f850bce3":"code","972a6f21":"code","bf4882d4":"code","c7280cae":"code","6ebf5ced":"code","f066e98a":"code","b3c660e1":"code","6c2c9914":"code","a346343b":"code","eedfac46":"code","5d92b1e1":"code","158e6508":"code","a4afb019":"code","84b012c8":"code","b3bb0677":"code","82c9f469":"code","fd0cdbe4":"code","407b4592":"code","5a22eeeb":"code","5765a57f":"code","78742091":"code","6efc34b7":"code","560a72d8":"code","8658f4ed":"code","cf2ce77c":"code","96c7cd1b":"code","ea4374bc":"code","a3b8470e":"code","b8b8a99f":"code","573d3072":"code","a90f2182":"code","85628c7a":"code","d88c9a64":"code","829f07f8":"code","cc6a4258":"code","9d2db5e4":"code","b01d3266":"code","e027b8fe":"code","f552b2ac":"code","e42b070a":"code","2a482df1":"code","600a293d":"code","375bfd35":"code","7dc754e0":"code","92f1c7fa":"code","1a4e4ef8":"code","bde6c24a":"code","d901afed":"code","d6290a10":"code","376bad5f":"code","d61842ae":"code","ffc324a1":"code","579cd14d":"markdown","5f81ac80":"markdown","b0ad4422":"markdown","af5a6e6f":"markdown","3e768dd3":"markdown","a4eead91":"markdown","ce0b9b61":"markdown","c69aff54":"markdown","95aeb6a8":"markdown","b242aaa0":"markdown","5edf9e12":"markdown","58750824":"markdown","79e6549c":"markdown","74ef471a":"markdown","300fdd82":"markdown","d9682b19":"markdown","df3ae988":"markdown","0d9d361d":"markdown","30621061":"markdown","ff548793":"markdown","41cdb6c5":"markdown","41cc59e4":"markdown","f1a6f371":"markdown","33528fd9":"markdown","8d469388":"markdown","1a61d4ae":"markdown","77ab8951":"markdown","15203526":"markdown","1b82549a":"markdown","3548a8a6":"markdown","17883874":"markdown","c70d50d2":"markdown","1dc3b2e4":"markdown","d5fbd419":"markdown","8393ff5d":"markdown","05d4d5d0":"markdown","659ff755":"markdown","b31abe31":"markdown","01ac4374":"markdown","a2a4990f":"markdown","635d001c":"markdown","04a4862d":"markdown","ba6e9723":"markdown","8c4f7f8a":"markdown","4709f2e3":"markdown","094c8c8d":"markdown","988c46f9":"markdown","01038811":"markdown","e2c1a113":"markdown","d8a9cebb":"markdown","ae115e36":"markdown","f2a38a33":"markdown","99f33636":"markdown","776f836a":"markdown","3792bfbb":"markdown","45adee02":"markdown","9ae3dc26":"markdown","7e84ca03":"markdown","9df55c5b":"markdown","f40ce24d":"markdown","b9c07a4d":"markdown","ae392925":"markdown","9cec3bfd":"markdown","12936175":"markdown","392ac939":"markdown"},"source":{"f182b1ce":"# Importing Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport itertools\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\nimport warnings","9f1f398e":"        \ntest = pd.read_csv('\/kaggle\/input\/vehicle-loan-default-prediction\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/vehicle-loan-default-prediction\/train.csv')\n","810c29e8":"df= pd.concat ([train, test])","ac19fc20":"#Shape of dataframe\nprint(\" Shape of training dataframe: \", train.shape)\nprint(\" Shape of testing dataframe: \", test.shape)\n# Drop duplicates\ntrain.drop_duplicates()\ntest.drop_duplicates()\nprint(train.shape)\nprint(test.shape)","480fabf1":"print(\"Names of columns \", list(train.columns))","c2ddd6a0":"#Null values in training dataset\n\nnull= train.isnull().sum().sort_values(ascending=False)\ntotal =train.shape[0]\npercent_missing= (train.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data.sort_values(by ='Total missing', ascending = False))","ccb56b96":"#Null values in test dataset\n\nnull= test.isnull().sum().sort_values(ascending=False)\ntotal =test.shape[0]\npercent_missing= (test.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data.sort_values(by ='Total missing', ascending = False))","926c52f7":"train_null_unique= train.EMPLOYMENT_TYPE .unique()\ntest_null_unique= test.EMPLOYMENT_TYPE .unique()\nprint(train_null_unique)\nprint (test_null_unique)","e6d097c6":"train.EMPLOYMENT_TYPE= train.EMPLOYMENT_TYPE.fillna(\"Missing\")\ntest.EMPLOYMENT_TYPE= test.EMPLOYMENT_TYPE .fillna(\"Missing\")\ntrain_null_unique= train.EMPLOYMENT_TYPE .unique()\ntest_null_unique= test.EMPLOYMENT_TYPE .unique()\nprint(train_null_unique)\nprint (test_null_unique)","97da1e1b":"print(train.info())","9c598eb4":"# Changing AVERAGE_ACCT_AGE & CREDIT_HISTORY_LENGTH\n\ntrain[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = train['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntrain[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = train['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntrain[\"AVERAGE_ACCT_AGE\"]= train[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((train[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))\/12)\ntrain= train.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntest[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = test['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntest[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = test['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntest[\"AVERAGE_ACCT_AGE\"]= test[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((test[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))\/12)\ntest= test.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntrain[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = train['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntrain[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = train['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntrain[\"CREDIT_HISTORY_LENGTH\"]= train[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((train[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))\/12)\ntrain= train.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])\n\ntest[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = test['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntest[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = test['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntest[\"CREDIT_HISTORY_LENGTH\"]= test[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((test[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))\/12)\ntest= test.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])","361caef5":"train['DATE_OF_BIRTH'] =  pd.to_datetime(train['DATE_OF_BIRTH'], format='%d-%m-%Y')\n#format='%d%b%Y:%H:%M:%S.%f'\ntest['DATE_OF_BIRTH'] =  pd.to_datetime(test['DATE_OF_BIRTH'], format='%d-%m-%Y')\ntrain['DISBURSAL_DATE'] =  pd.to_datetime(train['DISBURSAL_DATE'], format='%d-%m-%Y')\ntest['DISBURSAL_DATE'] =  pd.to_datetime(test['DISBURSAL_DATE'], format='%d-%m-%Y')","91c27329":"#remove_n = 128000\n#temp= train[train['LOAN_DEFAULT']==0]\n#drop_indices = np.random.choice(temp.index, remove_n, replace=False)\n#df_temp = temp.drop(drop_indices)\n#temp2= train[train['LOAN_DEFAULT']==1]\n#del train\n#train= pd.concat ([df_temp, temp2])","67a7f9e5":"class_df = train.groupby('LOAN_DEFAULT').count()['UNIQUEID'].reset_index().sort_values(by='UNIQUEID',ascending=False)\nclass_df.style.background_gradient(cmap='winter')\n","e4e764e2":"#Graph\nmy_pal = {0: 'deepskyblue', 1: 'deeppink'}\n\nplt.figure(figsize = (12, 6))\nax = sns.countplot(x = 'LOAN_DEFAULT', data = train, palette = my_pal)\nplt.title('Class Distribution')\nplt.show()\n\n# Count and %\nCount_Normal_transacation = len(train[train['LOAN_DEFAULT']==0])\nCount_Fraud_transacation = len(train[train['LOAN_DEFAULT']==1]) \nPercentage_of_Normal_transacation = Count_Normal_transacation\/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of no defaults       :', Percentage_of_Normal_transacation*100)\nprint('Number of no defaults     :', Count_Normal_transacation)\nPercentage_of_Fraud_transacation= Count_Fraud_transacation\/(Count_Normal_transacation+Count_Fraud_transacation)\nprint('% of defaults         :',Percentage_of_Fraud_transacation*100)\nprint('Number of defaults    :', Count_Fraud_transacation)","1aab85da":"print(\"Employment type\\n\")\nprint(train.groupby([\"EMPLOYMENT_TYPE\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Mobile Flag\\n\")\nprint(train.groupby([\"MOBILENO_AVL_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Aadhar Flag\\n\")\nprint(train.groupby([\"AADHAR_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Pan Flag\\n\")\nprint(train.groupby([\"PAN_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Voter ID Flag\\n\")\nprint(train.groupby([\"VOTERID_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Driving L Flag\\n\")\nprint(train.groupby([\"DRIVING_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))\nprint(\"##############\\n\")\nprint(\"Passport\\n\")\nprint(train.groupby([\"PASSPORT_FLAG\"]).LOAN_DEFAULT.value_counts(normalize=True))","2cfee687":"\nprint(train.groupby([\"LOAN_DEFAULT\",\"EMPLOYMENT_TYPE\",\"AADHAR_FLAG\",\"PAN_FLAG\",\"DRIVING_FLAG\",\"PASSPORT_FLAG\"]).VOTERID_FLAG.value_counts(normalize=False))\nprint(\"##############\\n\")\n","89113b8b":"train_0 = train[train[\"LOAN_DEFAULT\"]==0]\ntrain_1 = train[train[\"LOAN_DEFAULT\"]==1]","321fe41c":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))\n\nbins = 50\n\nax1.hist(train.DISBURSAL_DATE[train.LOAN_DEFAULT == 1], bins = bins, color = 'deeppink')\nax1.set_title('Default')\n\nax2.hist(train.DISBURSAL_DATE[train.LOAN_DEFAULT == 0], bins = bins, color = 'deepskyblue')\nax2.set_title('No default')\n\nplt.xlabel('DISBURSAL DATE')\nplt.ylabel('Number of Loans')\nplt.show()","968667a6":"# Plot distribution of one feature\ndef plot_distribution(feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(train[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()\n    \n# Plot distribution of multiple features, with TARGET = 1\/0 on the same graph\ndef plot_distribution_comp(var,nrow=2):\n    \n    i = 0\n    t1 = train.loc[train['LOAN_DEFAULT'] != 0]\n    t0 = train.loc[train['LOAN_DEFAULT'] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"LOAN_DEFAULT = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"LOAN_DEFAULT = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","68278bad":"def plot_bar_comp(var,nrow=2):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.countplot(train[feature])\n        sns.countplot(train[feature])\n        plt.ylabel('Count plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();\n    ","f0b9d375":"# Box Plot for one feature\ndef plot_box(feature, color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Box Plot of %s\" % feature)\n    sns.boxplot(train[feature].dropna(),color=color)\n    plt.show()\n","71926671":"# Bar Plot for one feature\ndef plot_bar(feature):\n    plt.figure(figsize=(10,50))\n    sns.catplot(y=feature, hue=\"LOAN_DEFAULT\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=train);","e0ab6f6c":"print(train.DISBURSED_AMOUNT.describe())\nplot_distribution('DISBURSED_AMOUNT','green')\n","adc8f33d":"plot_box(\"DISBURSED_AMOUNT\", \"green\")","5b4272b6":"#Number of observations in column\nobs = len(train.DISBURSED_AMOUNT)\nprint(\"No. of observations in column: \",obs)\n\n# calculate summary statistics\ndata_mean, data_std = mean(train.DISBURSED_AMOUNT), std(train.DISBURSED_AMOUNT)\nprint('Statistics: Mean=%.3f, Std dev=%.3f' % (data_mean, data_std))\n# identify outliers\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n# identify outliers\noutliers = [x for x in train.DISBURSED_AMOUNT if x < lower or x > upper]\nprint('Identified outliers: %d' % len(outliers))","04d5413d":"def impute_outlier(x):\n    if x <= lower:\n        return(data_mean)\n    elif x>= (upper):\n        return(data_mean)\n    else:\n        return(x)\ntrain[\"DISBURSED_AMOUNT_new\"]= train[\"DISBURSED_AMOUNT\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.DISBURSED_AMOUNT_new))","87b0453f":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['DISBURSED_AMOUNT_bins'] = pd.qcut(train['DISBURSED_AMOUNT'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['DISBURSED_AMOUNT_bins'].value_counts()\n","121c74b5":"plot_bar(\"DISBURSED_AMOUNT_bins\")","79842bcb":"print(train.ASSET_COST.describe().astype(str))\nplot_distribution('ASSET_COST','tomato')","6c05487c":"plot_box(\"ASSET_COST\", \"tomato\")","74b2e166":"#Number of observations in column\ndef outlier_data(df, feature):\n    #Number of observations in column\n    obs = len(df[feature])\n    print(\"No. of observations in column: \",obs)\n\n    # calculate summary statistics\n    data_mean, data_std = mean(df[feature]), std(df[feature])\n    print('Statistics: Mean=%.3f, Std dev=%.3f' % (data_mean, data_std))\n    # identify outliers\n    cut_off = data_std * 3\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    # identify outliers\n    outliers = [x for x in df[feature] if x < lower or x > upper]\n    print('Identified outliers: %d' % len(outliers))\n\ndef impute_outlier(x):\n    if x <= lower:\n        return(data_mean)\n    elif x>= (upper):\n        return(data_mean)\n    else:\n        return(x)\n\n    \n","ad90da6a":"outlier_data(train,\"ASSET_COST\")","1ff1deed":"train[\"ASSET_COST_new\"]= train[\"ASSET_COST\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.DISBURSED_AMOUNT_new))\noutlier_data(train,\"ASSET_COST_new\")","710f42d4":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['ASSET_COST_bins'] = pd.qcut(train['ASSET_COST'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['ASSET_COST_bins'].value_counts()","e7b7bebc":"plot_bar(\"ASSET_COST_bins\")","db54d338":"print(train.LTV.describe().astype(str))\nplot_distribution('LTV','blue')\n","893c2abc":"\nplot_box(\"LTV\", \"blue\")\n","f5fde5af":"outlier_data(train,\"LTV\")","21cf5ca7":"train[\"LTV_new\"]= train[\"LTV\"].apply(impute_outlier)\nprint(\"No. of observations in column: \",len(train.LTV_new))\noutlier_data(train,\"LTV_new\")","b26a16c1":"bin_labels = ['Low', 'Medium', 'High', 'Extreme']\ntrain['LTV_bins'] = pd.qcut(train['LTV'],\n                              q=[0, .25, .5, .75, 1],\n                              labels=bin_labels)\ntrain['LTV_bins'].value_counts()\n","569512b2":"plot_bar(\"LTV_bins\")\n","913840af":"print(train.PERFORM_CNS_SCORE.describe().astype(str))\nplot_distribution('PERFORM_CNS_SCORE','blue')\n","ed251fcd":"plot_box(\"PERFORM_CNS_SCORE\", \"blue\")","9ab432d7":"outlier_data(train,\"PERFORM_CNS_SCORE\")","40a70d1e":"\nbin_labels = [\"No History\",'Very Low', \"Low\" ,'Medium', 'High']\ncut_bins = [-1,10,150, 350, 650, 1000]\n\ntrain['PERFORM_CNS_SCORE_bins'] = pd.cut(train['PERFORM_CNS_SCORE'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PERFORM_CNS_SCORE_bins'].value_counts()\n","d1bf0f67":"plot_bar(\"PERFORM_CNS_SCORE_bins\")","0e14658c":"train.groupby([\"PERFORM_CNS_SCORE_DESCRIPTION\"]).PERFORM_CNS_SCORE_bins.value_counts()","f011c4f8":"train.PERFORM_CNS_SCORE_DESCRIPTION.value_counts()","02f76b4b":"g = train.groupby(\"PERFORM_CNS_SCORE_DESCRIPTION\")['LOAN_DEFAULT']\ngg = pd.concat([g.value_counts(), \n                g.value_counts(normalize=True).mul(100)],axis=1, keys=('counts','percentage'))\nprint (gg)\n\n#train.groupby(\"PERFORM_CNS_SCORE_DESCRIPTION\").LOAN_DEFAULT.value_counts(normalize=False)","4fd7be69":"print(train.PRI_NO_OF_ACCTS .describe().astype(str))\nplot_distribution('PRI_NO_OF_ACCTS','blue')\n","5c84571b":"plot_box(\"PRI_NO_OF_ACCTS\", \"blue\")","7ced93d9":"outlier_data(train,\"PRI_NO_OF_ACCTS\")","7d47b9ee":"train[\"PRI_NO_OF_ACCTS_new\"]= train[\"PRI_NO_OF_ACCTS\"].apply(impute_outlier)\noutlier_data(train,\"PRI_NO_OF_ACCTS_new\")\n","50644705":"\nbin_labels = [\"One\",'More than One']\ncut_bins = [-1,1, 1000]\n\ntrain['PRI_NO_OF_ACCTS_bins'] = pd.cut(train['PRI_NO_OF_ACCTS'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PRI_NO_OF_ACCTS_bins'].value_counts()","f3cdfa76":"plot_bar(\"PRI_NO_OF_ACCTS_bins\")","a02677d4":"print(train.PRI_OVERDUE_ACCTS.describe().astype(str))\nplot_box(\"PRI_OVERDUE_ACCTS\", \"blue\")","fc9aa6bf":"outlier_data(train,\"PRI_OVERDUE_ACCTS\")\n","a3c06069":"train[\"PRI_OVERDUE_ACCTS_new\"]= train[\"PRI_OVERDUE_ACCTS\"].apply(impute_outlier)\noutlier_data(train,\"PRI_OVERDUE_ACCTS_new\")\n","b2515e38":"bin_labels = [\"None\",'One (or more)']\ncut_bins = [-1,0, 1000]\n\ntrain['PRI_OVERDUE_ACCTS_bins'] = pd.cut(train['PRI_OVERDUE_ACCTS'],\n                              bins=cut_bins,\n                              labels=bin_labels)\ntrain['PRI_OVERDUE_ACCTS_bins'].value_counts()","2d238243":"plot_bar(\"PRI_OVERDUE_ACCTS_bins\")","6eaab680":"var = ['MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'PASSPORT_FLAG', 'DRIVING_FLAG']\nplot_bar_comp(var,nrow=3)","9499bf46":"# Employment Type\nsns.catplot(data=train,kind='count',x='EMPLOYMENT_TYPE',hue='LOAN_DEFAULT')","2520bf39":"now = pd.Timestamp('now')\n#train['DATE_OF_BIRTH'] = train['DATE_OF_BIRTH'].where(train['DATE_OF_BIRTH'] < now, train['DATE_OF_BIRTH'] -  np.timedelta64(100, 'Y'))   \ntrain['age'] = (now - train['DATE_OF_BIRTH'])  \n\ntrain['age']= train['age'].astype(str)\ntrain[['age','age_waste']] = train['age'].str.split(\"days\",expand=True)\ntrain['age']= train['age'].astype(str).astype(int)\ntrain= train.drop(columns= ['age_waste'])\n\nprint(train['age'].head())","7afdcf88":"train['disbursal_time'] = (now - train['DISBURSAL_DATE'])  \n\ntrain['disbursal_time']= train['disbursal_time'].astype(str)\ntrain[['disbursal_time','disbursal_time_waste']] = train['disbursal_time'].str.split(\"days\",expand=True)\ntrain['disbursal_time']= train['disbursal_time'].astype(str).astype(int)\ntrain= train.drop(columns= ['disbursal_time_waste'])\n\nprint(train['disbursal_time'].head())","307281dc":"# MANUFACTURER_ID\nsns.catplot(data=train,kind='count',x='MANUFACTURER_ID',hue='LOAN_DEFAULT')","4b73fedb":"sns.catplot(data=train,kind='count',x='BRANCH_ID',hue='LOAN_DEFAULT')","a86cf723":"var = ['PRI_NO_OF_ACCTS_new', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS_new', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT']\nplot_distribution_comp(var,nrow=3)","768b93a7":"var = ['SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT']\nplot_distribution_comp(var,nrow=3)","d5903c4f":"#Useless features\n#train = train.drop(['DISBURSED_AMOUNT','ASSET_COST', 'LTV', 'PRI_NO_OF_ACCTS','PRI_OVERDUE_ACCTS', 'DATE_OF_BIRTH', 'DISBURSAL_DATE', 'STATE_ID', 'EMPLOYEE_CODE_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID','BRANCH_ID'],axis=1)\ntrain = train.drop([ 'DATE_OF_BIRTH' , 'STATE_ID', 'EMPLOYEE_CODE_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID','BRANCH_ID'],axis=1)","f1481bb0":"#Highly Correlated\nsns.set()\n\ncols = train[['PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT', \n              'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n              'SEC_DISBURSED_AMOUNT',  'PRI_NO_OF_ACCTS_new', 'PRI_OVERDUE_ACCTS_new']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, annot=True, vmax=.8, square=True,cmap = 'YlGnBu');\n","1f9b5ed8":"# train = train.drop(['PRI_OVERDUE_ACCTS_new','SEC_ACTIVE_ACCTS','SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT' ],axis=1)","e77e5632":"#Highly Correlated\nsns.set()\n\ncols = train[['SEC_INSTAL_AMT', 'PERFORM_CNS_SCORE','NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n              'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES','age', 'disbursal_time']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, annot=True, vmax=.8, square=True,cmap = 'YlGnBu');","e8ad3bb5":"#train = train.drop(['AVERAGE_ACCT_AGE' ],axis=1)","282be050":"#Highly Correlated\nsns.set()\n\ncols = train[['PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PERFORM_CNS_SCORE',\n              'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',\n              'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', \n              'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES',\n              'DISBURSED_AMOUNT_new','ASSET_COST_new','LTV_new','PRI_NO_OF_ACCTS_new','age', 'disbursal_time']]\ncorr = cols.corr()\nf, ax = plt.subplots(figsize=(13, 10))\nsns.heatmap(corr, annot=True, vmax=.7, square=True,cmap = 'YlGnBu');","ed2e4bdd":"#train = train.drop(['PRI_SANCTIONED_AMOUNT','PRI_NO_OF_ACCTS_new','NEW_ACCTS_IN_LAST_SIX_MONTHS'],axis=1)","ee347efb":"train_con = train[['EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',\n                   'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE', 'PERFORM_CNS_SCORE_DESCRIPTION', \n                   'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', \n                   'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', \n                   'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES',\n                   'LOAN_DEFAULT', 'DISBURSED_AMOUNT_new', 'ASSET_COST_new', \n                   'LTV_new', 'age', 'disbursal_time']]","34c72e84":"train_bin = train [['UNIQUEID', 'EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', \n                    'VOTERID_FLAG', 'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE', \n                    'PERFORM_CNS_SCORE_DESCRIPTION', 'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE',\n                    'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',\n                    'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n                    'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES', 'LOAN_DEFAULT',\n                    'DISBURSED_AMOUNT_bins', 'ASSET_COST_bins', 'LTV_bins',\n                    'PERFORM_CNS_SCORE_bins', 'PRI_NO_OF_ACCTS_bins', 'PRI_OVERDUE_ACCTS_bins', 'age', 'disbursal_time']]","a87a0c71":"# Confusion Matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","892a42f6":"# Precision, Recall, F1 Score\ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","79499006":"# Precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","18747f33":"# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n    plt.xlim([0.0,0.001])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","0cd86c0c":"#feature importance plot\ndef plot_feature_importance(model):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (15,8))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()\n","f7097f81":"scaler_data = StandardScaler() \ndef scaleColumns(df, cols_to_scale):\n\n    for col in cols_to_scale:\n\n        df[col] = pd.DataFrame(scaler_data.fit_transform(pd.DataFrame(train_con[col])),columns=[col])\n\n    return df","5315e0bd":"scaled_df = scaleColumns(train_con,['PERFORM_CNS_SCORE','PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', \n                                 'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', \n                                 'SEC_CURRENT_BALANCE', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n                                 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH', \n                                 'NO_OF_INQUIRIES', 'DISBURSED_AMOUNT_new',\n                                 'ASSET_COST_new', 'LTV_new', 'age', 'disbursal_time'])\n\nscaled_df.head()","928e69c5":"train_dummy = pd.get_dummies(scaled_df, prefix_sep='_', drop_first=True)\ntrain_dummy.head()","dac441ab":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","3deeecd6":"np.any(np.isnan(X))","d83ae929":"X = X.fillna(0)\nX.shape","a2d567ef":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","2d2b8e59":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression() \nlogmodel.fit(X_train,y_train)\nlogpred = logmodel.predict(X_test)\n\n\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","825f1bf3":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, logpred))\nprint(\"F1 Score \",f1_score(y_test, logpred))\nprint(\"Recall Score \",recall_score(y_test, logpred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, logpred))","f850bce3":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","972a6f21":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","bf4882d4":"from sklearn.naive_bayes import GaussianNB\n\n# train model\nnb = GaussianNB().fit(X_train, y_train)\n\n# predict on test set\nnb_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test, nb_pred))\nprint(round(accuracy_score(y_test, nb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","c7280cae":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, nb_pred))\nprint(\"F1 Score \",f1_score(y_test, nb_pred))\nprint(\"Recall Score \",recall_score(y_test, nb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, nb_pred))","6ebf5ced":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","f066e98a":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","b3c660e1":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","6c2c9914":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, dtree_pred))\nprint(\"F1 Score \",f1_score(y_test, dtree_pred))\nprint(\"Recall Score \",recall_score(y_test, dtree_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, dtree_pred))","a346343b":"# XG Boost\n\nfrom xgboost import XGBClassifier\n\n# train model\nxgb = XGBClassifier().fit(X_train, y_train)\n\n# predict on test set\nxgb_pred = xgb.predict(X_test)\nprint(confusion_matrix(y_test, xgb_pred))\nprint(round(accuracy_score(y_test, xgb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","eedfac46":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, xgb_pred))\nprint(\"F1 Score \",f1_score(y_test, xgb_pred))\nprint(\"Recall Score \",recall_score(y_test, xgb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, xgb_pred))","5d92b1e1":"from imblearn.over_sampling import SMOTE\n\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_sample(X_train, y_train)","158e6508":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","a4afb019":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","84b012c8":"from sklearn.datasets import make_classification\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","b3bb0677":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, dtree_pred))\nprint(\"F1 Score \",f1_score(y_test, dtree_pred))\nprint(\"Recall Score \",recall_score(y_test, dtree_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, dtree_pred))","82c9f469":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","fd0cdbe4":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","407b4592":"\nfrom sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","5a22eeeb":"# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.LOAN_DEFAULT.value_counts()\n\ny_train = upsampled.LOAN_DEFAULT\nX_train = upsampled.drop('LOAN_DEFAULT', axis=1)","5765a57f":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","78742091":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","6efc34b7":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","560a72d8":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","8658f4ed":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","cf2ce77c":"#Downsample\nfrom sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]\n\n# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.LOAN_DEFAULT.value_counts()\n\ny_train = downsampled.LOAN_DEFAULT\nX_train = downsampled.drop('LOAN_DEFAULT', axis=1)","96c7cd1b":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","ea4374bc":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","a3b8470e":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","b8b8a99f":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","573d3072":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","a90f2182":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X)","85628c7a":"pca.explained_variance_ratio_.astype(str)","d88c9a64":"plt.figure(figsize= (12,9))\nplt.plot(range(1,45), pca.explained_variance_ratio_.cumsum(), marker= 'o', linestyle='--')\nplt.title(\"Explained variance by components\")\nplt.xlabel(\"No. of components\")\nplt.ylabel(\"Cumulative variance explained\")","829f07f8":"pca = PCA(n_components = 17)\npca.fit(X)","cc6a4258":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","9d2db5e4":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","b01d3266":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","e027b8fe":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","f552b2ac":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","e42b070a":"y = train_dummy[['LOAN_DEFAULT']]\nX= train_dummy.loc[:, train_dummy.columns != 'LOAN_DEFAULT']\nX.shape","2a482df1":"from sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","600a293d":"# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])","375bfd35":"# check new class counts\nupsampled.LOAN_DEFAULT.value_counts()","7dc754e0":"y_train = upsampled.LOAN_DEFAULT\nX_train = upsampled.drop('LOAN_DEFAULT', axis=1)","92f1c7fa":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","1a4e4ef8":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","bde6c24a":"from sklearn.utils import resample\n# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.LOAN_DEFAULT==0]\nfraud = X[X.LOAN_DEFAULT==1]","d901afed":"# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.LOAN_DEFAULT.value_counts()","d6290a10":"y_train = downsampled.LOAN_DEFAULT\nX_train = downsampled.drop('LOAN_DEFAULT', axis=1)","376bad5f":"from sklearn.tree import DecisionTreeClassifier\n\n# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","d61842ae":"from sklearn.linear_model import SGDClassifier\n\n# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","ffc324a1":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\nprint(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","579cd14d":" <a id=\"11\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Base Line Models<\/b><\/font><br>\n\n","5f81ac80":"SMOTE or Synthetic Minority Oversampling Technique is used to create synthetic data. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.","b0ad4422":"#### Not highly correlated with anyone: 'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE','PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT','SEC_OVERDUE_ACCTS'\n#### 'PRI_NO_OF_ACCTS_new', 'PRI_OVERDUE_ACCTS_new'are perfectly positively correlated and hence keeping one \n#### 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS' are highly positively correlated, hence keeping one\n####  'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT' are highly positively correlated, hence keeping one\n\n","af5a6e6f":"<font size=+4 color=\"Black\"><center><b>Vehicle Loan Default Prediction<\/b><\/center><\/font>\n<font size=-1 color=\"Black\"><center><b>Classification Models <\/b><\/right><\/font>","3e768dd3":" <a id=\"14\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.3 Naive Bayes<\/b><\/font><br>\n\n","a4eead91":"## Age is in days","ce0b9b61":"#### Let's see the new columns along with the less important continous variables","c69aff54":" <a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Importing data & libraries<\/b><\/font><br>","95aeb6a8":"### Binning\n#### mean     75865.06814380195\n#### std      18944.78128866517\n#### min                37000.0\n#### 25%                65717.0\n#### 50%                70946.0\n#### 75%               79201.75\n#### max              1628992.0","b242aaa0":"###  'DISBURSED_AMOUNT' : Amount of Loan disbursed","5edf9e12":"# Downsample","58750824":"### 'ASSET_COST' : Payment default in the first EMI on due date","79e6549c":"#### 'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH'are highly positively correlated and hence keeping one","74ef471a":"### PRI_OVERDUE_ACCTS: count of default accounts at the time of disbursement\n","300fdd82":" <a id=\"6\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.3 Univariate analysis<\/b><\/font><br>\n","d9682b19":"### LTV","df3ae988":"Accuracy score is good, however the model is not predicting the Defaults well","0d9d361d":" <a id=\"10\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5.2 Dummy insertion<\/b><\/font><br>\n\n","30621061":" <a id=\"9\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5.1 Standardization of data<\/b><\/font><br>\n\n","ff548793":"Upsampling can be defined as adding more copies of the minority class. Upsampling can be a good choice when you don\u2019t have a ton of data to work with. (Not a good choice here though)","41cdb6c5":" <a id=\"7\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.4 Outlier Treatment <\/b><\/font><br>\n","41cc59e4":" <a id=\"19\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.1 SMOTE<\/b><\/font><br>\n\n","f1a6f371":"### Binning\n#### mean      74.74653001879038\n#### std      11.456635738792304\n#### min                   10.03\n#### 25%                   68.88\n#### 50%                    76.8\n#### 75%                   83.67\n#### max                    95.0","33528fd9":"### Preparing Datasets 1) Binned Variables 2) Continous variables","8d469388":" <a id=\"2\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Variable Inspection<\/b><\/font><br>","1a61d4ae":"### PRI_NO_OF_ACCTS : count of total loans taken by the customer at the time of disbursement\n","77ab8951":"### Plenty of times we get data where there is imbalanced class distribution. This notebooks covers the existing methodologies to tackle such problems.[](http:\/\/)","15203526":"### Flag : Uneven class","1b82549a":" <a id=\"4\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.1 Class Distribution<\/b><\/font><br>","3548a8a6":" <a id=\"20\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.2 Upsampling<\/b><\/font><br>\n\n","17883874":" <a id=\"21\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.3 Downsampling<\/b><\/font><br>\n\n","c70d50d2":" <a id=\"5\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.2 Default vs Disbursal date<\/b><\/font><br>\n","1dc3b2e4":"Accuracy score is good, however the model is not predicting the Defaults well","d5fbd419":" <a id=\"8\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3.5 Feature Selection<\/b><\/font><br>\n\n","8393ff5d":"#### Flag 2:\n#### AVERAGE_ACCT_AGE, CREDIT_HISTORY_LENGTH are object, but they should be int. \n#### DATE_OF_BIRTH & DISBURSAL_DATE should be datetime type\n\n#### Lets' check...","05d4d5d0":"#### One out of 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT' \n#### One out of 'LTV_new', 'PRI_NO_OF_ACCTS_new'\n#### Eliminate 'NEW_ACCTS_IN_LAST_SIX_MONTHS'","659ff755":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work<\/b><\/font>","b31abe31":" <a id=\"22\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.4 PCA<\/b><\/font><br>\n\n","01ac4374":" <a id=\"16\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.5 Decision Tree Classifier<\/b><\/font><br>\n\n","a2a4990f":"<font size=\"+3\" color=\"Green\"><b>Related Work:<\/b><\/font>\n\n1. Deep diving into more methods to tackle the problem of Oversampling\n2. Stacking of algos\n3. Using deep learning models","635d001c":"### Let's look into variables with high importance\n### Loan information","04a4862d":"### The accuracy of RF might have fone down by 7% but is predicting defaults better now. ","ba6e9723":" <a id=\"13\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.2 Random Forest<\/b><\/font><br>\n\n","8c4f7f8a":"### PERFORM_CNS_SCORE_DESCRIPTION","4709f2e3":"#### Since there are outliers - either we can treat outliers or do the binning\n#### Here, we have the liberty of performing both and comparing their results","094c8c8d":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work<\/b><\/font>\n","988c46f9":"Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.","01038811":"### About this notebook\n\n## Banking industry is using classification models to predict the default or raise red flags on the accounts which are likely to default. One major problem is of Imbalanced Distribution of classes i.e. the available training data consists more of non-default accounts vs. default accounts. Here is my take on tackling such a problem.\n## This notebook covers various EDA, Outlier treatment, PCA, Variable Inspection, Classification models etc.\n\n\n![](https:\/\/media3.giphy.com\/media\/mnptBuEshROyk\/giphy.gif)\n\n","e2c1a113":"## Let's look into data with lesser importance\n#### MOBILENO_AVL_FLAG : if Mobile no. was shared by the customer then flagged as 1\n#### AADHAR_FLAG : if aadhar was shared by the customer then flagged as 1\n#### PAN_FLAG : if pan was shared by the customer then flagged as 1\n#### VOTERID_FLAG : if voter  was shared by the customer then flagged as 1\n#### PASSPORT_FLAG : if DL was shared by the customer then flagged as 1\n#### DRIVING_FLAG : if passport was shared by the customer then flagged as 1","d8a9cebb":"Accuracy score is good, however the model is not predicting the Defaults well","ae115e36":" <a id=\"17\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.6 XG Boost<\/b><\/font><br>\n\n","f2a38a33":" <a id=\"3\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. EDA<\/b><\/font><br>","99f33636":" #### Flag 1: 3443 missing values in employment type","776f836a":"Accuracy score is good, however the model is predicting the Defaults better than Logistic reg\n","3792bfbb":" <a id=\"15\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.4 Stochastic Gradient Descent<\/b><\/font><br>\n","45adee02":"Contents:\n\n* [1. Importing data & libraries](#1)\n* [2. Variable Inspection](#2)\n* [3. EDA](#3)\n    * [3.1 Class Distribution](#4)\n    * [3.2 Default Timeline](#5)\n    * [3.3 Univariate analysis](#6)  \n    * [3.4 Outlier Treatment](#7)\n    * [3.5 Feature Selection](#8)\n        * [3.5.1 Standardization of data](#9)\n        * [3.5.2 Dummy insertion](#10)\n* [4. Models](#11)\n    * [4.1 Logistic Regression](#12)\n    * [4.2 Random Forest](#13)\n    * [4.3 Naive Bayes](#14)\n    * [4.4 Stochastic Gradient Descent](#15)\n    * [4.5 Decision Tree Classifier](#16)\n    * [4.6 XG Boost](#17)\n* [5. Dealing with Imbalanced Data](#18)\n    * [5.1 SMOTE](#19)\n    * [5.2 Upsampling](#20)\n    * [5.3 Downsampling](#21)\n    * [5.4 PCA](#22)\n    * [5.1 Resampling](#23)","9ae3dc26":"## Best model is Random Forest till now","7e84ca03":"Accuracy score is good, however the model is not predicting the Defaults well","9df55c5b":" <a id=\"23\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5.5 Resampling<\/b><\/font><br>\n\n","f40ce24d":"### Binning\n#### mean      54356.993528\n#### std       12971.314171\n#### min       13320.000000\n#### 25%       47145.000000\n#### 50%       53803.000000\n#### 75%       60413.000000\n#### max      990572.000000","b9c07a4d":" <a id=\"18\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Dealing with Imbalanced data<\/b><\/font><br>\n\n","ae392925":" <a id=\"12\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4.1 Logistic Regression<\/b><\/font><br>\n\n","9cec3bfd":"### 'PERFORM_CNS_SCORE': Bureau Score\n","12936175":"# Best result is obtained by Random Forest after deploying [ SMOTE](#19).","392ac939":"Model accuracy is very poor"}}