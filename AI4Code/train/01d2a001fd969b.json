{"cell_type":{"3ab03ce1":"code","5e8e7a4b":"code","29bb6d9f":"code","a89e20cd":"code","10d5bcd3":"code","ebf9d227":"code","4cbadf0c":"code","5a2818d4":"code","ea2a5330":"code","87595e8d":"code","b45e5d22":"code","ea765c6f":"code","28f6203d":"code","6aff236e":"code","0aa68647":"code","b5df8705":"code","fc3fc663":"code","47b21343":"code","5f10ecad":"code","539ae6a4":"code","8874e223":"code","e45d9812":"code","2b7bdfa8":"code","d02eecda":"code","ba14b259":"code","fb8a4b55":"code","3dc50fb5":"code","853ac6dc":"code","f048b4d7":"code","82670266":"code","79a5a422":"code","e2fabffa":"code","2bd5f128":"code","45388e85":"code","a35e065d":"code","b17331b5":"code","549c883b":"code","cfa0c14b":"code","e64c01c6":"code","d36b453b":"code","32320e97":"code","dd6f755a":"code","312632bc":"code","f42ed5a8":"code","edb3a43e":"code","07c40bf0":"code","d8767b49":"code","0f5a783d":"code","eed5d3dc":"code","7e343f17":"code","f23ee4fb":"code","7d5916d3":"code","b71b457b":"code","b8fd3f31":"code","854dff2f":"code","0495b373":"code","ae34f071":"code","1f945e99":"code","8735c1b0":"code","bc630a36":"code","3958f29f":"code","a3ddc9ef":"code","1b23185c":"code","77c32e05":"code","3a28d85a":"markdown","d2686572":"markdown","90c278a3":"markdown","a20cbc53":"markdown","10e8140b":"markdown","946991a9":"markdown","805ede9c":"markdown","3ff03ded":"markdown","07630897":"markdown","4b043727":"markdown","ab461923":"markdown","2fea97ce":"markdown","253ce5d6":"markdown","62e81fc7":"markdown","93dd6c67":"markdown","c823da0e":"markdown","5cbe65c8":"markdown","16ce0c1f":"markdown","846a399d":"markdown","f42db198":"markdown","38914e60":"markdown","41165b00":"markdown","779d758e":"markdown","6cacd1ba":"markdown","3061da05":"markdown","463031bc":"markdown","bc52ee05":"markdown","c930f17b":"markdown","d75c27d0":"markdown","f8305d10":"markdown","7a009334":"markdown","c303e186":"markdown","c083309c":"markdown","7cbd2650":"markdown","1672fc3f":"markdown","461acd28":"markdown","9a5f4231":"markdown","179d9f0c":"markdown","2d5b3681":"markdown"},"source":{"3ab03ce1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n# Mute the setting wtih a copy warnings\npd.options.mode.chained_assignment = None","5e8e7a4b":"# Load in the dataset\ndata = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')","29bb6d9f":"# Print out the data shape\ndata.shape","a89e20cd":"# Drop duplicates and check the new data shape\ndata = data.drop_duplicates(keep='first').reset_index(drop=True)\ndata.shape","10d5bcd3":"# Split the data to use the training set only\n# Keep the test set unseen\nfrom sklearn.model_selection import train_test_split\ntrain_and_val, test = train_test_split(data, test_size=0.2, random_state=0)\nprint(f'Training and validation set size: {train_and_val.shape}')\nprint(f'Test set size: {test.shape}')","ebf9d227":"train, val = train_test_split(train_and_val, test_size=0.25, random_state=0)\nprint(f'Training set size: {train.shape}')\nprint(f'Validation set size: {val.shape}')","4cbadf0c":"# Print out first 5 rows\ntrain.head()","5a2818d4":"# Training set info\ntrain.info()","ea2a5330":"# Basic statistics\ntrain.describe()","87595e8d":"# Add in new feature age, and remove year\n# The data were collected in 2020, so the age column is calculated from this year\ntrain = train[train['year'] <= 2020]\ntrain['age'] = 2020 - train['year']\ntrain = train.drop(['year'], axis=1)","b45e5d22":"# Determine columns by data types\ncat_mask = (train.dtypes == np.object)\nnum_mask = (train.dtypes == np.float64) | (train.dtypes == np.int64)\n\ncat_cols = train.columns[cat_mask].tolist()\nnum_cols = train.columns[num_mask].tolist()\n\nprint(f'Categorical columns: {cat_cols}')\nprint(f'Numerical columns: {num_cols}')","ea765c6f":"# Create a plotting function\ndef hist_loop(data: pd.DataFrame,\n              rows: int,\n              cols: int,\n              figsize: tuple):\n\n    \"\"\" Returns multiple histograms as subplots\n    \"\"\"\n    fig, axes = plt.subplots(rows,cols, figsize=figsize)\n    for i, ax in enumerate(axes.flatten()):\n        if i < len(data.columns):\n            data[sorted(data.columns)[i]].plot.hist(bins=30, ax=ax)\n            ax.set_title(f'{sorted(data.columns)[i]} distribution')\n            ax.tick_params(axis='x')\n            ax.tick_params(axis='y')\n            ax.get_yaxis().get_label().set_visible(False)\n        else:\n            fig.delaxes(ax=ax)\n    fig.tight_layout()","28f6203d":"# Plot all numerical features\nhist_loop(data=train[num_cols],\n          rows=2,\n          cols=3,\n          figsize=(15,8))","6aff236e":"def skew_df(data: pd.DataFrame, skew_limit: float) -> pd.DataFrame:\n    # Define a limit above which we will transform\n    skew_vals = data.skew()\n\n    # Showing the skewed columns\n    skew_cols = (skew_vals\n                 .sort_values(ascending=False)\n                 .to_frame('Skew')\n                 .query('abs(Skew) > {}'.format(skew_limit))\n    )\n    return skew_cols\nskew_cols = skew_df(train[num_cols], 0.75)\nskew_cols","0aa68647":"# Apply square root transformation on predictors only\ntrain_sqrt = train[num_cols].drop('price', axis=1).copy()\nfor col in list(skew_cols.index):\n    if col != 'price':\n        train_sqrt[col] = train_sqrt[col].apply(np.sqrt)\n\n# Check again\nskew_df(train_sqrt, 0.75)","b5df8705":"# Pairplot of transformed features and the target\nsns.pairplot(train_sqrt.join(train['price']), plot_kws=dict(alpha=.2, edgecolor='none'));","fc3fc663":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","47b21343":"calc_vif(train_sqrt)","5f10ecad":"train.describe(include=np.object)","539ae6a4":"# Print out all unique model names\nlist(train['model'].unique())","8874e223":"# Remove leading spaces\ntrain['model'] = train['model'].str.strip(' ')","e45d9812":"# Boxplot of model and price\nfig, ax = plt.subplots(figsize=(15,8))\norder = sorted(list(train['model'].unique()))\nsns.boxplot(x='model', y='price', data=train, order=order, ax=ax)\nplt.xticks(rotation=45)\nplt.title('Price by Model', fontsize=16)\nplt.show()","2b7bdfa8":"list(train['transmission'].unique())","d02eecda":"list(train['fuelType'].unique())","ba14b259":"# Box plots of price and tranmission\/ fuel type\nfig, ax = plt.subplots(1, 2, figsize=(20,6), sharey=True)\norder0 = sorted(list(train['transmission'].unique()))\nsns.boxplot(x='transmission', y='price', data=train, order=order0, ax=ax[0])\nax[0].set_title('Price by Transmission', fontsize=16)\nax[0].tick_params('x', labelrotation=45)\n\norder1 = sorted(list(train['fuelType'].unique()))\nsns.boxplot(x='fuelType', y='price', data=train, order=order1, ax=ax[1])\nax[1].set_title('Price by Fuel Type', fontsize=16)\nax[1].tick_params('x', labelrotation=45)\nplt.show()","fb8a4b55":"# Copy the data from training and validation set\n# Clean the data\ndf = train_and_val.copy()","3dc50fb5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures","853ac6dc":"# Create a class for feature engineering and model fitting\nclass LR_model:\n\n    # Default values\n    target = 'price'\n    test_size = 0.25\n    random_state = 0\n    skew_cols = ['engineSize', 'mileage', 'age']\n\n    def __init__(self, data):\n        self.train, self.test = train_test_split(data, test_size=LR_model.test_size, random_state=LR_model.random_state)\n    \n    def clean_data(self, df):\n\n        \"\"\" Cleans the data\"\"\"\n\n        df = df[df['year'] <= 2020]\n        df['age'] = 2020 - df['year']\n        df = df.drop(['year'], axis=1)\n        df['model'] = df['model'].str.strip(' ')\n        \n        return df\n        \n    def oh_enc(self, X_train, X_test):\n\n        \"\"\" Performs one-hot encoding and drops the first category\n        \"\"\"\n\n        ENC = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n        # Filter categorical features only\n        X_train_cat = X_train.select_dtypes(include=['object'])\n        X_test_cat = X_test.select_dtypes(include=['object'])\n\n        # Fit one-hot encoding on training set\n        # Transform both training set and test set\n        X_train_enc = ENC.fit_transform(X_train_cat)\n        X_test_enc = ENC.transform(X_test_cat)\n\n        # Join dummy values with numerical features\n        X_train_enc_df = pd.DataFrame(X_train_enc,\n                                      index=X_train.index,\n                                      columns=ENC.get_feature_names(X_train_cat.columns.tolist()))\n        X_train = X_train_enc_df.join(X_train.select_dtypes(exclude=['object']))\n\n        # Drop one column of each category\n        for col in X_train_cat.columns.tolist():\n            cat_cols = X_train.columns[X_train.columns.str.startswith(col)].tolist()\n            if len(cat_cols) > 1:\n                X_train = X_train.drop(cat_cols[0], axis=1)\n        \n        # Same steps for the test set\n        X_test_enc_df = pd.DataFrame(X_test_enc,\n                                     index=X_test.index,\n                                     columns=ENC.get_feature_names(X_test_cat.columns.tolist()))\n        X_test = X_test_enc_df.join(X_test.select_dtypes(exclude=['object']))\n        for col in X_test_cat.columns.tolist():\n            cat_cols = X_test.columns[X_test.columns.str.startswith(col)].tolist()\n            if len(cat_cols) > 1:\n                X_test = X_test.drop(cat_cols[0], axis=1)\n\n        return X_train, X_test\n\n    def sqrt_trans(self, X_train, X_test):\n\n        \"\"\" Applies square root transformation for skewed features\n        \"\"\"\n        X_train[LR_model.skew_cols] = X_train[LR_model.skew_cols].apply(np.sqrt)\n        X_test[LR_model.skew_cols] = X_test[LR_model.skew_cols].apply(np.sqrt)\n\n        return X_train, X_test\n\n    def scale_X(self, X_train, X_test):\n\n        \"\"\" Applies standard scaling for all numerical features\n        \"\"\"\n\n        scaler = StandardScaler()\n\n        # Filter numerical features only (excluding binary values)\n        float_cols = X_train.columns[~X_train.isin([0,1]).all()].tolist()\n\n        # Fit features in traning set and transform to test set\n        X_train[float_cols] = scaler.fit_transform(X_train[float_cols])\n        X_test[float_cols] = scaler.transform(X_test[float_cols])\n\n        return X_train, X_test\n\n    def add_pf(self, X_train, X_test, degree=None):\n\n        \"\"\" Adds polynomial features into the dataset\n        \"\"\"\n        \n        PF = PolynomialFeatures(degree=degree, include_bias=False)\n\n        # Filter numerical features only (excluding binary values)\n        float_cols = X_train.columns[~X_train.isin([0,1]).all()].tolist()\n\n        # Fit features in traning set and transform to test set\n        X_train_pf = PF.fit_transform(X_train[float_cols])\n        X_test_pf = PF.transform(X_test[float_cols])\n        \n        # Add non-numerical features back into the transformed training set\n        X_train_pf_df = pd.DataFrame(X_train_pf,\n                                     index=X_train.index,\n                                     columns=PF.get_feature_names(input_features=float_cols))\n        X_train = X_train_pf_df.join(X_train[X_train.columns[~X_train.columns.isin(float_cols)].tolist()])\n        \n       # Add non-numerical features back into the transformed test set\n        X_test_pf_df = pd.DataFrame(X_test_pf,\n                                    index=X_test.index,\n                                    columns=PF.get_feature_names(input_features=float_cols))\n        X_test = X_test_pf_df.join(X_test[X_test.columns[~X_test.columns.isin(float_cols)].tolist()])\n        \n        return X_train, X_test\n\n    def rmse(self, y_true, y_predicted):\n\n        \"\"\" Returns root mean squared error\"\"\"\n        return np.sqrt(mean_squared_error(y_true, y_predicted))\n\n    def model_fit(self, label=None,\n                  encoding=False,\n                  squareroot=False,\n                  scaling=False,\n                  polynomial=False,\n                  degree=None):\n        \n        \"\"\" Fits linear regression model and returns \n        RMSE of training set and test set\n        \"\"\"\n\n        # Split data\n        train, test = self.clean_data(self.train), self.clean_data(self.test)\n        X_train, X_test = train.drop(LR_model.target, axis=1), test.drop(LR_model.target, axis=1)\n        y_train, y_test = train[LR_model.target], test[LR_model.target]\n\n        # Perform feature engineering\n        if encoding:\n            X_train, X_test = self.oh_enc(X_train, X_test)\n\n        if not encoding:\n           X_train, X_test = X_train.select_dtypes(exclude=['object']), X_test.select_dtypes(exclude=['object'])\n\n        if squareroot:\n            X_train, X_test = self.sqrt_trans(X_train, X_test)\n\n        if scaling:\n            X_train, X_test = self.scale_X(X_train, X_test)\n        \n        if polynomial:\n            X_train, X_test = self.add_pf(X_train, X_test, degree)\n        \n        num_features = X_train.shape[1]\n        # Fit model and predict the target\n        LR = LinearRegression()\n        LR.fit(X_train, y_train)\n        y_train_pred = LR.predict(X_train)\n        y_test_pred = LR.predict(X_test)\n        \n        # Compute RMSE and store in a dictionary\n        rmse_train = self.rmse(y_train, y_train_pred)\n        rmse_test = self.rmse(y_test,  y_test_pred)\n        scores = {\n            'Model': label,\n            'Number of features': num_features,\n            'RMSE train': rmse_train,\n            'RMSE test': rmse_test\n        }\n        \n        return scores","f048b4d7":"# Fit LR model in each set and print out RMSE\nerror_df = [] # Blank error list to create a data frame later\n\n# Fit non-encoded data\nnot_enc = LR_model(df).model_fit(label='not encoded')\n\n# Fit encoded data\nenc = LR_model(df).model_fit(label='one hot encoded', encoding=True)\n\n# Print out error table\nerror_df = pd.DataFrame.from_dict([not_enc])\nerror_df = error_df.append(enc, ignore_index=True)\nerror_df","82670266":"# Fit non-encoded data\nnot_enc_bc = LR_model(df).model_fit(label='not encoded + squareroot', squareroot=True)\n\n# Fit encoded data\nenc_bc = LR_model(df).model_fit(label='one hot encoded + squareroot', encoding=True, squareroot=True)\n\n# Print out error table\nerror_df = error_df.append([not_enc_bc, enc_bc], ignore_index=True)\nerror_df","79a5a422":"# Fit encoded data\nenc_bc_s = LR_model(df).model_fit(label='one hot encoded + squareroot + scaled', encoding=True, squareroot=True, scaling=True)\n\n# Print out error table\nerror_df = error_df.append(enc_bc_s, ignore_index=True)\nerror_df","e2fabffa":"# Create blank error table\nerror_pf_df = pd.DataFrame(columns=['Model', 'Number of features', 'RMSE train', 'RMSE test'])\n\n# Iterate different degree, 1 to 10\nfor d in list(range(1,11)):\n    error = LR_model(df).model_fit(label=f'Degree = {d}', encoding=True, scaling=True, polynomial=True, degree=d)\n    error_pf_df = error_pf_df.append(error, ignore_index=True)\n\n# Print out the error table\nerror_pf_df","2bd5f128":"# Import libraries\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn_pandas import DataFrameMapper, gen_features","45388e85":"# 5-fold cross validation\nkf = KFold(shuffle=True, random_state=0, n_splits=5)","a35e065d":"df = df[df['year'] <= 2020]\ndf['age'] = 2020 - df['year']\ndf = df.drop(['year'], axis=1)\ndf['model'] = df['model'].str.strip(' ')\n\nX = df.drop('price', axis=1)\ny = df['price']","b17331b5":"# Create a class for data pipeline\nclass XPipe:\n    # Skew features observed from the EDA\n    skew_cols = ['engineSize', 'mileage', 'age']\n\n    def __init__(self, X):\n        self.cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n        self.num_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n        \n    def drop_first(self, cat_values):\n\n        \"\"\" Drops one category after one-hot encoding\"\"\"\n\n        dummy_df = pd.DataFrame(cat_values)\n        dummy_df = dummy_df.drop(0, axis=1)\n\n        return dummy_df.values\n\n    def sqrt_trans(self, num_values):\n\n        \"\"\" Applies square root transformation to skewed features\"\"\"\n\n        num_df = pd.DataFrame(num_values, columns=self.num_cols)\n        num_df[XPipe.skew_cols] = num_df[XPipe.skew_cols].apply(np.sqrt)\n        \n        return num_df.values\n\n    def model_pipe(self, model, degree=2):\n\n        \"\"\" Returns a data pipeline\"\"\"\n\n        cat_list = [[col] for col in self.cat_cols]\n        cat_features = gen_features(\n            columns=cat_list,\n            classes=[{'class':OneHotEncoder, 'handle_unknown':'ignore', 'sparse':False},\n                     {'class':FunctionTransformer, 'func': self.drop_first}]\n        )\n        \n        mapper = DataFrameMapper((cat_features) + [  \n            (self.num_cols, [FunctionTransformer(self.sqrt_trans),\n                             StandardScaler(),\n                             PolynomialFeatures(degree=degree, include_bias=False)])\n        ])\n\n        return make_pipeline(mapper, model)","549c883b":"XPipe(X).model_pipe(LinearRegression())","cfa0c14b":"# Create a blank dictionary to store metrics of different models\nmetrics = {}","e64c01c6":"# Blank dictionary to add in metrics\nlr_scores = {}\n\n# Iterate over different degrees, 1 to 6\nfor d in list(range(1,7)):\n    score = cross_val_score(XPipe(X)\n                            .model_pipe(LinearRegression(), degree=d),\n                            X, y, cv=kf, scoring='neg_mean_squared_error')\n    lr_scores[f'Degree = {d}'] = np.mean(np.sqrt(-score))\n\n# Create an error table and print it out\nlr_rmse = pd.DataFrame.from_dict(lr_scores, orient='index', columns=['Average RMSE']).sort_values('Average RMSE')\nlr_rmse","d36b453b":"# Calculate average R squared\nlr_r2_scores = cross_val_score(XPipe(X)\n                              .model_pipe(LinearRegression(), degree=3),\n                              X, y, cv=kf)\nlr_r2_scores","32320e97":"# Store the metrics\nmetrics['Average RMSE'] = [lr_rmse['Average RMSE'].values[0]]\nmetrics['Average R2'] = [np.mean(lr_r2_scores)]","dd6f755a":"# Blank dictionary to add in metrics\nridge_scores = {}\n\n# Iterate over different alphas\nfor d in [1, 2, 3]:\n    for a in [0.005, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10]:\n        score = cross_val_score(XPipe(X)\n                                .model_pipe(Ridge(alpha=a, max_iter=100000), degree=d),\n                                X, y, cv=kf, scoring='neg_mean_squared_error')\n\n                                \n        ridge_scores[f'Degree = {d}, alpha = {a}'] = np.mean(np.sqrt(-score))\n\n# Create an error table and print it out (first 5 rows)\nridge_rmse = pd.DataFrame.from_dict(ridge_scores, orient='index', columns=['Average RMSE']).sort_values('Average RMSE')\nridge_rmse.head(5)","312632bc":"# Calculate average R squared\nridge_r2_scores = cross_val_score(XPipe(X)\n                                  .model_pipe(Ridge(alpha=0.005, max_iter=100000), degree=3),\n                                  X, y, cv=kf)\nridge_r2_scores","f42ed5a8":"# Store the metrics\nmetrics['Average RMSE'].append(ridge_rmse['Average RMSE'].values[0])\nmetrics['Average R2'].append(np.mean(ridge_r2_scores))","edb3a43e":"# Blank dictionary to add in metrics\nlas_scores = {}\n\n# Iterate over different alphas\nfor d in [1, 2, 3]:\n    for a in [0.005, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10]:\n        score = cross_val_score(XPipe(X)\n                                .model_pipe(Lasso(alpha=a, max_iter=100000), degree=d),\n                                X, y, cv=kf, scoring='neg_mean_squared_error')\n            \n        las_scores[f'Degree = {d}, alpha = {a}'] = np.mean(np.sqrt(-score))\n\n# Create an error table and print it out (first 5 rows)\nlas_rmse = pd.DataFrame.from_dict(las_scores, orient='index', columns=['Average RMSE']).sort_values('Average RMSE')\nlas_rmse.head(5)","07c40bf0":"# Calculate average R squared\nlas_r2_scores = cross_val_score(XPipe(X)\n                                .model_pipe(Lasso(alpha=0.3, max_iter=100000), degree=3),\n                                X, y, cv=kf)\nlas_r2_scores","d8767b49":"# Store the metrics\nmetrics['Average RMSE'].append(las_rmse['Average RMSE'].values[0])\nmetrics['Average R2'].append(np.mean(las_r2_scores))","0f5a783d":"# Blank dictionary to add in metrics\nelasticnet_scores = {}\n\n# Iterate over different alphas\nfor d in [1, 2, 3]:\n    for a in [0.005, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10]:\n        score = cross_val_score(XPipe(X)\n                                .model_pipe(ElasticNet(alpha=a, max_iter=100000), degree=d),\n                                X, y, cv=kf, scoring='neg_mean_squared_error')\n        elasticnet_scores[f'Degree = {d}, alpha = {a}'] = np.mean(np.sqrt(-score))\n\n# Create an error table and print it out (first 5 rows)\nelasticnet_rmse = pd.DataFrame.from_dict(elasticnet_scores, orient='index', columns=['Average RMSE']).sort_values('Average RMSE')\nelasticnet_rmse.head(5)","eed5d3dc":"# Calculate average R squared\nelasticnet_r2_scores = cross_val_score(XPipe(X)\n                                       .model_pipe(ElasticNet(alpha=0.005, max_iter=100000), degree=3),\n                                       X, y, cv=kf)\nelasticnet_r2_scores","7e343f17":"# Store the metrics\nmetrics['Average RMSE'].append(elasticnet_rmse['Average RMSE'].values[0])\nmetrics['Average R2'].append(np.mean(elasticnet_r2_scores))","f23ee4fb":"# Create a comparison table\nmetrics['Model'] = ['Linear', 'Ridge', 'Lasso', 'Elastic Net']\npd.DataFrame.from_dict(metrics).set_index('Model').sort_values('Average RMSE')","7d5916d3":"# Keep the original data\ntrain_df, test_df = train.copy(), test.copy()","b71b457b":"# Clean the training set\nX_train = train_df.drop('price', axis=1)\ny_train = train_df['price']","b8fd3f31":"# Clean the test set\ntest_df = test_df[test_df['year'] <= 2020]\ntest_df['age'] = 2020 - test_df['year']\ntest_df = test_df.drop(['year'], axis=1)\ntest_df['model'] = test_df['model'].str.strip(' ')\n\nX_test = test_df.drop('price', axis=1)\ny_test = test_df['price']","854dff2f":"# Make prediction on the test set and plot four scatter plots separately\n\nfig, axes = plt.subplots(2,2, figsize=(14,14), sharex=True, sharey=True)\nfig.text(0.5, 0.07, 'Predicted Price', ha='center', fontdict={'size': 16})\nfig.text(0.07, 0.5, 'Actual Price', va='center', rotation='vertical', fontdict={'size': 16})\n\nmodels = [LinearRegression(), Lasso(alpha=0.3, max_iter=100000),\n          Ridge(alpha=0.005, max_iter=100000), ElasticNet(alpha=0.005, max_iter=100000)]\n\nlabels = ['Linear', 'Lasso', 'Ridge', 'Elastic Net']\n\nfor i, ax in enumerate(axes.flatten()):\n    y_pred = (XPipe(X_train)\n              .model_pipe(models[i], degree=3)\n              .fit(X_train, y_train)\n              .predict(X_test))\n\n    r_squared = r2_score(y_test, y_pred)\n\n    ax.plot(y_test, y_pred, marker='o', ls='', ms=3.0)\n    ax.set(title=f'{labels[i]} Regression R2: {r_squared}')","0495b373":"# Make prediction on the test set and plot one combined scatter plot\n\nfig = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nmodels = [ElasticNet(alpha=0.005, max_iter=100000), LinearRegression(),\n          Ridge(alpha=0.005, max_iter=100000), Lasso(alpha=0.3, max_iter=100000)]\n         \nlabels = ['Elastic Net', 'Linear', 'Ridge', 'Lasso']\n\nfor mod, lab in zip(models, labels):\n    y_pred = (XPipe(X_train)\n              .model_pipe(mod, degree=3)\n              .fit(X_train, y_train)\n              .predict(X_test))\n    ax.plot(y_test, y_pred, marker='o', ls='', ms=3.0, label=lab)\n\nleg = plt.legend(frameon=True)\nleg.get_frame().set_edgecolor('black')\nleg.get_frame().set_linewidth(1.0)\n\nax.set(xlabel='Actual Price', \n       ylabel='Predicted Price', \n       title='Regression Results');","ae34f071":"# Have a look at the data pipeline\nestimator = XPipe(X_train).model_pipe(Lasso(alpha=0.3, max_iter=100000), degree=3)\nestimator.fit(X_train, y_train)","1f945e99":"# Get the transformer list and feature names\ntransformers = estimator.named_steps['dataframemapper'].features\ncat_cols = XPipe(X_train).cat_cols\nnum_cols = XPipe(X_train).num_cols","8735c1b0":"# Iterate over each transformer and get the feature names\nfeature_names = []\nfor f in transformers:\n    for col in cat_cols:\n        if f[0] == [col]:\n            names = f[1][0].get_feature_names([col]).tolist()\n            del names[0]\n            feature_names.append(names)\n    if f[0] == num_cols:\n        names = f[1][2].get_feature_names(num_cols)\n        feature_names.append(names)\n    else:\n        pass\nfeatures = [name for names in feature_names for name in names]","bc630a36":"beta_coef = estimator.named_steps['lasso'].coef_\nprint(f'Number of estimates: {len(beta_coef)}')\nprint(f'Number of features: {len(features)}')","3958f29f":"# Print out most importance features by the magnitude of the estimates\ndf_importances = pd.DataFrame(zip(features, beta_coef), columns=['feature', 'estimate'])\ndf_importances = df_importances.sort_values(by='estimate', ascending=False).reset_index(drop=True)\nprint(df_importances.head())\nprint('-'*50)\nprint(df_importances.tail())","a3ddc9ef":"# If Lasso shrank any coefficient\nany(df_importances['estimate'] == 0)","1b23185c":"# Features that Lasso eliminated\ndf_importances[df_importances['estimate'] == 0]","77c32e05":"# Visualize the feature importance\ndf_importances.set_index('feature').plot(kind='bar', legend=None, figsize=(25,10))\nplt.title('Feature Importance', fontsize=16);","3a28d85a":"### Elastic Net Regresstion (L1 + L2)","d2686572":"### Numerical features","90c278a3":"Lasso is slightly better than the Linear model. I highly doubt if it shrank any coefficient albeit the high multicollinearity among predictors.","a20cbc53":"Lasso is the best model even though the metrics among these models are not significantly different.","10e8140b":"### Compare the metrics","946991a9":"The main drivers of this model are features that indicate whether or not the car model is Edge, Mustang, Tourneo Custom, Galaxy, S-MAX, Puma, or Grand Tourneo Connect. These are all derived from the categorical feature - model. Among numerical features, age and mileage have the strongest predictive power. Most interaction terms and polynomial features have low estimates in comparison to others.","805ede9c":"It looks like the third polynomial degree transformation returns the best model. At degree 4 and above, as the model gets more and more complex, it starts overfitting.","3ff03ded":"## 3. Model variations\nIn this part, I will perform different feature engineering in an order, then fit the linear regression after each engineering step. The root mean square errors are stored and compared.","07630897":"## 4. Cross-validation and Regularization\nIn this part, I use cross validation to fit the linear regression model again, and then attempt to tune the hyperparameter to find a proper alpha and polynomial degree combination for regularization.","4b043727":"The third polynomial degree model still returns the lowest RMSE on average. Let's see the R2 score in each fold.","ab461923":"On average, car prices are different among models, transmission, and fuel types.","2fea97ce":"The model that has encoded features performs better, which is understandable because it has more information to predict the target. RMSEs of test sets are slightly higher than training sets, which is expected. There is no sign of overfitting.","253ce5d6":"### Linear Regression\nIn the train test split part, adding the third degree polynomial features returns the best model. Let's see if cross-validation returns different results.\n\n","62e81fc7":"### Scatter plots from four models","93dd6c67":"### Add Polynomial features\nAs shown in the pairplot, there is a polynomial relationship between the target and engineSize. Let's try adding polynomial features to the latest model (encoded, square root transformed, and scaled). ","c823da0e":"### Lasso Regression (L1)\nFind a combination of alpha and polynomial degree for Lasso regularization","5cbe65c8":"## 6. Conclusion\nThis analysis shows that feature engineering can have a large effect on the model performance, and if the data are sufficiently large, cross-validation should be preferred over train-test-split to construct model evaluation. In my case, even though the predictors have high multicollinearity, their coefficients were not shrunk by the Lasso model, and it is shown that regularization does not always make big improvement on a given model. In the end, the Lasso regression has the highest $R^2$ when predicting on the test set, and categories of car model appear to be the most important features to predict a car price. Also, Lasso did shrink some of the coefficients that are not so important in terms of prediction.\n\nWhile researching further analysis, I found a [suggestion](https:\/\/stats.stackexchange.com\/a\/326846) of using [grouped Lasso](http:\/\/cs229.stanford.edu\/proj2012\/ChoiParkSeo-LassoInCategoricalData.pdf) when a model have categorical features, which is worth trying in this case.\n\nVisit my [Github](https:\/\/github.com\/thuynh323\/IBM-Machine-Learning) for more details.","16ce0c1f":"The transformation improves all models. The one that has encoded features is the best so far.","846a399d":"## 1. Train test split","f42db198":"Now, let's check the skewness of all numerical features.","38914e60":"The features have high multicollinearity. This problem might be resolved by regularization later.","41165b00":"There is a leading space in each model name. This is not necessary but let's just remove them.","779d758e":"### Apply Square root transformation","6cacd1ba":"### Ridge Regression (L2)","3061da05":"### Apply One-hot encoding","463031bc":"Now, let's go over each model: Linear regression, Lasso regression, Ridge regression, and Elastic Net regression. Each model will be evaluated based on its average root mean squared error (from 5 folds).","bc52ee05":"## 5. Predict on the test set\nIn this part, I will use four models to make prediction on the unseen test set.","c930f17b":"### Apply Standard scaling\nScaling features is a preparation for regularization later. RMSE of both training set and test set should stay the same. From this point onwards, I will use the encoded data only.","d75c27d0":"Skewness is fixed. Let's use a pairplot to have an overview of these features and the target.","f8305d10":"## 2. Simple EDA\n### Descriptive statistics and data cleaning","7a009334":"This plot shows that:\n- age has a linear relationship with price. It looks quite like polynomial. \n- age also has a linear relationship with mileage (the older the more miles). This is multicollinearity.\n\nLet's see how severe the multicollinearity is.","c303e186":"Looking back at the box plots of price by model and fuel type, we can see that these are rare categories in our data set (or their prices do not vary much). I would say Lasso did a good job to eliminate them.","c083309c":"Lasso Regression has the best prediction on the test set. All these models can explain the target around 87% - 88%.","7cbd2650":"# Project 2 - Supervised Learning: Regression\nThis notebook is a part of my second project required by the IBM Machine Learning Program.\n\nData Source: Ford dataset from [100,000 UK Used Car Data set](https:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes)\n\nThe main objective of this analysis is to predict price(\u00a3) of used Ford cars using Linear Regressions.\n\nNotebook Contents:\n\n>1. Train test split<br>\n\n>2. Simple EDA<br>\nDescriptive statistics and data cleaning<br>\nNumerical features<br>\nCategorical features\n\n>3. Model variations<br>\nApply One-hot encoding<br>\nApply Square root transformation<br>\nApply Standard scaling<br>\nAdd Polynomial features\n\n>4. Cross-validation and Regularization<br>\nLinear Regression<br>\nLasso Regression (L1)<br>\nRidge Regression (L2)<br>\nElastic Net Regression (L1 + L2)<br>\nCompare the metrics\n\n>5. Predict on the test set<br>\nScatter plots of four models<br>\nFeature importance\n\n>6. Conclusion","1672fc3f":"This model performs the worst so far.","461acd28":"As expected, these plots are nearly identical.","9a5f4231":"### Feature importance\nAs shown above, the Lasso Regression is our best model. Let's see which feature is the most important in the model.","179d9f0c":"### Categorical features\nIn this part, I will check the data integrity and also plot some boxplots to see how price is different among these categories. But I will not perform much analysing to find combined relationship or correlation.\n\nThese plots are based on price values before transformation.","2d5b3681":"Except for tax and mpg, all features are right-skewed, and also there are zero values in engineSize (electric cars). Square root transformation might be a good choice to eliminate the skewness in this case."}}