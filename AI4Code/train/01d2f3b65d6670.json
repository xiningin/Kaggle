{"cell_type":{"5471af08":"code","8a50c2af":"code","22e2c0f2":"code","d0dc187e":"code","72d93177":"code","df40ec10":"code","73872531":"code","f103d223":"code","24ea9690":"code","9ee3adf9":"code","7115c11c":"code","b2380916":"code","4e1a4dae":"code","a615a541":"code","62418898":"code","68ea3b83":"code","1292f8a5":"code","77883a14":"code","b8cb4349":"code","fe88efc9":"code","ae7b3d93":"code","8c409dfe":"code","e532213f":"code","8d230f74":"code","2925ba32":"code","be75e92f":"code","43a8a087":"code","7894687f":"code","6c335b5a":"code","2fe166b9":"code","1d0f8b8f":"code","04435d86":"code","b5e7e415":"code","5c1cfeb8":"code","f59db341":"code","b097c48f":"code","bf18368c":"code","f9eb70a9":"code","a76a0296":"code","a90b6b9c":"code","91dd79cb":"code","023fcf0a":"code","45706397":"code","92864ef3":"code","cc07a46d":"code","8de1289d":"code","239d00cb":"code","4b01ce67":"code","7abf7a63":"code","38d7eeba":"code","89f8e541":"code","c753d4a3":"code","f42a7efa":"code","f0bb89cc":"code","fe8feca1":"code","39a3d2ba":"code","c88e02ae":"code","ee91ebf4":"code","691925fe":"code","3830ba1d":"code","15e34c4a":"code","df98184b":"code","84086cdc":"code","7cf2f06d":"code","a2b121cc":"code","126f1341":"code","c46c1170":"code","d980e4cc":"code","a3b496a2":"code","c9f3f646":"code","91c18a75":"code","d878ee6d":"code","d49855b0":"code","ee3ee6b5":"code","4ecec27e":"code","f4cac1ad":"code","77f65601":"code","6d7ce343":"code","dc043dc4":"code","83a69ab6":"code","e1687e30":"code","b597aadf":"code","3b403a2a":"code","7356b27f":"code","57ab592e":"code","3f2329a3":"code","19ef52b6":"code","215751b7":"code","7abc15e3":"code","0aab7764":"code","0c7f814c":"code","cb788aa2":"code","b713dae3":"code","33496899":"markdown","087335ee":"markdown","cc1e08a2":"markdown","dd775db8":"markdown","8f364038":"markdown","34937ee3":"markdown","d25755de":"markdown","62354ff7":"markdown","8af428d1":"markdown","52806545":"markdown","837e3803":"markdown","35e2475c":"markdown","e38c0c99":"markdown","6bbb6eb6":"markdown","0b85d5f9":"markdown","f9d431df":"markdown","96bd236b":"markdown","b6eccd5f":"markdown","c915db76":"markdown","72debc64":"markdown","e296bb30":"markdown","e0624b78":"markdown","19f76a6e":"markdown","1acabb3d":"markdown","8cf7c8f5":"markdown"},"source":{"5471af08":"import numpy as np\nimport pandas as pd\nimport missingno as no\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px","8a50c2af":"import warnings\nwarnings.filterwarnings(\"ignore\")","22e2c0f2":"sns.set_theme(style=\"whitegrid\")","d0dc187e":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nsub_sample_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","72d93177":"feat_cols = [col for col in train_df.columns if col.startswith(\"feature\")]","df40ec10":"train_df.head()","73872531":"no.matrix(train_df, figsize=(18,4))","f103d223":"train_df.info()","24ea9690":"train_df.shape, test_df.shape","9ee3adf9":"train_df.drop('id', axis=1, inplace=True)\ntest_df.drop('id', axis=1, inplace=True)","7115c11c":"train_df.describe().T.style.bar(subset=['mean'], color=px.colors.qualitative.Pastel1[1])\\\n                                .background_gradient(subset=['std'], cmap='Greens')","b2380916":"label_dict = {val:idx for idx, val in enumerate(train_df['target'].unique())}\ntrain_df['target_num'] = train_df['target'].map(label_dict)","4e1a4dae":"fig, ax = plt.subplots(figsize=(12,12))\n\ncorr_mat = train_df.corr()\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr_mat, mask=mask, square=True, ax=ax, linewidths=0.1,cmap='coolwarm', center=0)","a615a541":"plt.rcParams['axes.facecolor'] = 'black'","62418898":"target_order = sorted(train_df['target'].unique())\n\nplt.figure(figsize=(8, 4))\nsns.countplot(x=train_df[\"target\"], order=target_order, palette='coolwarm');","68ea3b83":"# Sorted correlation between features and the target\n# corr_target = corr_mat[\"target_num\"][:-1].sort_values(ascending=False)\n# fig = plt.figure(figsize=(16,5))\n# sns.barplot(x=corr_target.index, y=corr_target.values, palette=\"RdYlGn\")\n# plt.title(\"Features correlation to the target column\")\n# plt.xticks(rotation=45);","1292f8a5":"fig = plt.figure(figsize=(16,5))\nsns.barplot(x=corr_mat[\"target_num\"][:-1].index, y=corr_mat[\"target_num\"][:-1].values, palette=\"RdYlGn\")\nplt.title(\"Features correlation to the target column\")\nplt.xticks(rotation=90);","77883a14":"fig = plt.figure(figsize=(20,30))\n\nfor i, col in enumerate(train_df.drop(['target', 'target_num'], axis=1)):\n    df = train_df[[col, 'target']].groupby('target').mean()\n    plt.subplot(17,3, i+1)\n    sns.barplot(x=df.index, y=df[col], palette=\"RdYlGn\")\n    plt.tight_layout()","b8cb4349":"fig = plt.figure(figsize=(20,30))\nfig.patch.set_facecolor('black')\nsns.set_theme(style=\"dark\")\n\nfor i, col in enumerate(train_df.drop(['target', 'target_num'], axis=1)):\n    plt.subplot(17,3, i+1)\n    sns.kdeplot(train_df[col], fill=True, color='red')\n    sns.kdeplot(test_df[col], fill=True, color='blue')","fe88efc9":"feat_cols = train_df.drop([\"target\", \"target_num\"], axis=1).columns\ntrain_unique_list= []\ntest_nunique_list = []\n\nfor col in feat_cols:\n    train_unique_list.append(train_df[col].nunique())\n    test_nunique_list.append(test_df[col].nunique())\n\nunique_df = pd.DataFrame(data=train_unique_list, index=feat_cols, columns=[\"train_nunique\"])\nunique_df[\"test_nunique\"] = test_nunique_list","ae7b3d93":"print(\"Features cardinality\")\nunique_df.style.background_gradient(cmap=\"Blues\")","8c409dfe":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif[\"variables\"] = feat_cols\nvif[\"VIF_train\"] = [variance_inflation_factor(train_df[feat_cols].values, i)\\\n                    for i in range(train_df[feat_cols].shape[1])]\nvif[\"VIF_test\"] = [variance_inflation_factor(test_df[feat_cols].values, i)\\\n                   for i in range(test_df[feat_cols].shape[1])]\nvif.style.background_gradient(cmap=\"magma\")","e532213f":"sns.set_theme(style=\"whitegrid\")","8d230f74":"fig = plt.figure(figsize=(20,35))\nfig.patch.set_facecolor('white')\nplt.rcParams['axes.facecolor'] = 'white'\n\n\nfor i, col in enumerate(feat_cols):\n    plt.subplot(17, 3, i+1)\n    # Draw a nested violinplot and split the violins for easier comparison\n    sns.violinplot(data=train_df, x=\"target\", y=f\"feature_{i}\",\n                   split=True, inner=\"quart\", linewidth=1)\n    sns.despine(left=True)\n    plt.tight_layout()","2925ba32":"fig = plt.figure(figsize=(20,30))\n\nfor i, col in enumerate(train_df.drop(['target', 'target_num'], axis=1)):\n    plt.subplot(17,3, i+1)\n    sns.histplot(data=train_df[col], color='blue',bins=50)\n    sns.histplot(data=test_df[col], color='red', bins=50)\n    plt.xlim(0, 10)\n    plt.tight_layout()","be75e92f":"low_card_cols = [\"feature_0\",\"feature_2\",\"feature_5\",\"feature_13\",\n                 \"feature_22\",\"feature_36\",\"feature_44\"]","43a8a087":"train_df['train'] = 1\ntest_df['train'] = 0","7894687f":"ys = train_df[['target_num','target']]","6c335b5a":"def create_dummies(dftrain, dftest, cols):\n    ys = dftrain[[\"target_num\",\"target\"]]\n    dftrain = dftrain.drop(['target','target_num'], axis=1)\n    full_df = pd.concat([dftrain,dftest], axis=0)\n    temp_df = pd.get_dummies(full_df, columns=cols, drop_first=True)\n    return temp_df","2fe166b9":"new_full_df = create_dummies(train_df, test_df, low_card_cols)","1d0f8b8f":"new_train_df = new_full_df[new_full_df['train'] == 1].drop('train', axis=1)\nnew_test_df = new_full_df[new_full_df['train'] == 0].drop('train', axis=1)","04435d86":"new_train_df = pd.concat([new_train_df,ys], axis=1)","b5e7e415":"#for col in train_df.columns:\n    #print(f\"{col} unique values: {train_df[col].unique()}\")","5c1cfeb8":"train_df.drop(\"train\", axis=1, inplace=True)\ntest_df.drop(\"train\", axis=1, inplace=True)","f59db341":"for col in low_card_cols:\n    feat_cols = feat_cols.drop(col)","b097c48f":"def calculate_log(df, cols):\n    df = df.copy()\n    for col in cols:\n        df.loc[:,col] =  df[col].apply(lambda x: np.log(x) if x>0 else x) \n    return df","bf18368c":"train_transformed = calculate_log(new_train_df, feat_cols)\ntest_transformed = calculate_log(new_test_df, feat_cols)","f9eb70a9":"train_transformed.describe()","a76a0296":"import lightgbm as lgbm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score, log_loss, classification_report, confusion_matrix","a90b6b9c":"#seed = 1945\n#train_transformed = train_transformed.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n#train_transformed['kfold'] = -1\n#skf = StratifiedKFold(n_splits=5)\n\n#for fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_transformed, y=train_transformed['target_num'])):\n    #train_transformed.loc[valid_idx, \"kfold\"] = fold","91dd79cb":"seed = 1945\ntrain_df = train_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ntrain_df['kfold'] = -1\nskf = StratifiedKFold(n_splits=5)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df['target_num'])):\n    train_df.loc[valid_idx, \"kfold\"] = fold","023fcf0a":"seed = 1945\ntrain_df_shuffled = train_df.sample(frac=1, random_state=seed).reset_index(drop=True)\ntest_df_shuffled = test_df.sample(frac=1, random_state=seed).reset_index(drop=True)","45706397":"def run_training(algo, df, test, fold, oof):\n    t_df = df[df.kfold != fold].reset_index(drop=True)\n    v_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = t_df.drop([\"kfold\", \"target\", \"target_num\"], axis=1)\n    xvalid = v_df.drop([\"kfold\", \"target\", \"target_num\"], axis=1)\n    \n    ytrain = t_df['target_num'].values\n    yvalid = v_df['target_num'].values\n    \n    #sc = MinMaxScaler()\n    #xtrain = sc.fit_transform(xtrain)\n    #xvalid = sc.transform(xvalid)\n    #test = sc.transform(test)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    model = algo.train(\n            params={\n                \"objective\":\"multiclass\",\n                \"metrics\": \"multi_logloss\",\n                \"num_class\":4\n            },\n            train_set=dtrain,\n            num_boost_round=1000,\n            valid_sets=(dtrain,dvalid),\n            valid_names=('train','valid'),\n            early_stopping_rounds=100,\n            verbose_eval=100,\n    )\n    \n    ypred = model.predict(xvalid)\n    oof[valid_idx] = ypred\n    \n    test_ypred = model.predict(test)\n    \n    print()\n    print(f\"Valid's logloss: {log_loss(yvalid, ypred):.5f}\")\n    print(f\"Valid's ROC AUC: {roc_auc_score(yvalid, ypred, multi_class='ovo'):.5f}\")\n    \n    return model, oof, test_ypred","92864ef3":"NUM_CLASS=4\noof = np.zeros((len(train_transformed),NUM_CLASS))\ntest_oof = np.zeros((len(test_transformed), NUM_CLASS))\n\nfor fold in range(5):\n    model, oof, test_preds1 = run_training(lgbm, train_df_shuffled, test_df_shuffled, fold, oof)\n    test_oof += test_preds1 \/ NUM_CLASS","cc07a46d":"sub_df = pd.DataFrame(np.clip(test_oof, 0.025, 0.975))\nsub_df.columns = label_dict.keys()\nsub_df[\"id\"] = sub_sample_df[\"id\"].values","8de1289d":"sub_df.to_csv(\"sub_base_first.csv\", index=False)","239d00cb":"y_true = train_transformed[\"target_num\"].values\ny_pred_ser1 = pd.Series([np.argmax(line) for line in oof])\nprint(classification_report(y_true, y_pred_ser1))","4b01ce67":"class_labels = list(label_dict.keys())","7abf7a63":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_true, y_pred_ser1), \n            cmap='viridis', annot=True, fmt=\".0f\",\n            xticklabels=class_labels,yticklabels=class_labels);","38d7eeba":"#import optuna\n#from optuna.pruners import SuccessiveHalvingPruner\n#from sklearn.model_selection import train_test_split\n\n#X = train_transformed.drop([\"kfold\",\"target\", \"target_num\"], axis=1)\n#y = train_transformed[\"target_num\"].values\n\n#def objective(trial):\n    \n    #X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,\n                                                          #random_state=1945, stratify=y)\n    \n    #sc = MinMaxScaler()\n    #X_train = sc.fit_transform(X_train)\n    #X_valid = sc.transform(X_valid)\n    \n    #dtrain = lgbm.Dataset(X_train, label=y_train)\n    #dvalid = lgbm.Dataset(X_valid, label=y_valid)\n    \n    #params = {\n        #\"objective\":\"multiclass\",\n        #\"metrics\":\"multi_logloss\",\n        #\"num_class\": 4,\n        #\"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        #\"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 255),\n        #\"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n        #\"num_iterations\": trial.suggest_int(\"num_iterations\", 100, 1000),\n        #\"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        #\"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        #\"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        #\"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        #\"max_bin\": trial.suggest_int(\"max_bin\", 2, 255),\n    #}\n    \n    #model = lgbm.train(params,dtrain)\n    #preds = model.predict(X_valid)\n    #logloss = log_loss(y_valid, preds)\n    #return logloss\n\n#study1 = optuna.create_study(direction=\"minimize\",pruner=SuccessiveHalvingPruner())\n#study1.optimize(objective, n_trials=100)\n\n#print(\"Number of finished trials: \", len(study1.trials))\n#print(\"Best trial: \", study1.best_trial.params)","89f8e541":"# Hyperparameters found with train dataset without any transformation\n\nbest_params = {'learning_rate': 0.025845340173733224,\n 'num_leaves': 8,\n 'max_depth': 28,\n 'num_iterations': 754,\n 'boosting': 'gbdt',\n 'bagging_fraction': 0.8681224214865364,\n 'bagging_freq': 2,\n 'lambda_l1': 8.538579636440432e-08,\n 'lambda_l2': 3.164760963598122e-07,\n 'max_bin': 149,\n 'objective': 'multiclass',\n 'metrics': 'multi_logloss',\n 'num_class': 4}","c753d4a3":"# hyperparams found by optuna with transformed dataset\n#best_params={'learning_rate': 0.01559063647642801,\n #'num_leaves': 11,\n #'max_depth': 9,\n #'num_iterations': 830,\n #'bagging_fraction': 0.6535376526131755,\n #'bagging_freq': 1,\n #'lambda_l1': 3.5657610302806274e-08,\n #'lambda_l2': 0.47634999839812464,\n #'max_bin': 248}","f42a7efa":"# best_params = study1.best_params\nbest_params[\"objective\"] = \"multiclass\"\nbest_params[\"metrics\"] = \"multi_logloss\"\nbest_params[\"num_class\"] = 4","f0bb89cc":"lgbm_final_model = lgbm.LGBMClassifier(**best_params)","fe8feca1":"train_df_t = train_transformed.copy()\ntest_df_t = test_transformed.copy()","39a3d2ba":"#X = train_df_t.drop([\"kfold\",\"target\",\"target_num\"], axis=1)\n#y = train_df_t[\"target_num\"].values","c88e02ae":"X = train_df_shuffled.drop([\"kfold\",\"target\",\"target_num\"], axis=1)\ny = train_df_shuffled[\"target_num\"].values","ee91ebf4":"sc = MinMaxScaler()\nX_scaled = sc.fit_transform(X)\ntest_scaled = sc.transform(test_df)","691925fe":"lgbm_final_model.fit(X_scaled, y)\nfinal_preds = lgbm_final_model.predict_proba(test_scaled)","3830ba1d":"sub_df = pd.DataFrame(final_preds) # np.clip(final_preds, 0.025, 0.975)\nsub_df.columns = label_dict.keys()\nsub_df[\"id\"] = sub_sample_df[\"id\"].values","15e34c4a":"sub_df.to_csv(\"optuna_sub.csv\", index=False)","df98184b":"def run_training2(algo, df, test, fold, oof):\n    t_df = df[df.kfold != fold].reset_index(drop=True)\n    v_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = t_df.drop([\"kfold\", \"target\", \"target_num\"], axis=1)\n    xvalid = v_df.drop([\"kfold\", \"target\", \"target_num\"], axis=1)\n    \n    ytrain = t_df['target_num'].values\n    yvalid = v_df['target_num'].values\n    \n    sc = MinMaxScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    test = sc.transform(test)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    model = algo.train(best_params,\n            train_set=dtrain,\n            num_boost_round=1000,\n            valid_sets=[dtrain,dvalid],\n            valid_names=['train','valid'],\n            early_stopping_rounds=100,\n            verbose_eval=100\n    )\n    \n    ypred = model.predict(xvalid)\n    oof[valid_idx] = ypred\n    \n    test_ypred = model.predict(test)\n    logloss = log_loss(yvalid, ypred)\n    \n    print()\n    print(f\"Fold={fold+1}\")\n    print(f\"Valid's ROC AUC: {roc_auc_score(yvalid, ypred, multi_class='ovo'):.5f}\")\n    print(f\"Valid's logloss: {log_loss(yvalid, ypred):.5f}\")\n    \n    return model, oof, test_ypred, logloss\n\n\noof = np.zeros((len(train_transformed),NUM_CLASS))\ntest_oof2 = np.zeros((len(test_transformed), NUM_CLASS))\nlogloss_list = []\n\nfor fold in range(5):\n    lgbm_model, oof, test_preds, logloss = run_training2(lgbm, train_df_shuffled, test_df, fold, oof)\n    test_oof2 += test_preds \/ NUM_CLASS\n    logloss_list.append(logloss)\nprint(f\"Mean log_loss after {fold+1} folds: {np.mean(logloss_list)}\")","84086cdc":"sub_df = pd.DataFrame(np.clip(test_oof2, 0.025, 0.975)) # np.clip(final_preds, 0.025, 0.975)\nsub_df.columns = label_dict.keys()\nsub_df[\"id\"] = sub_sample_df[\"id\"].values\nsub_df.to_csv(\"sub_5kfold_optuna_hyperparams.csv\", index=False)","7cf2f06d":"y_true = train_df[\"target_num\"].values\ny_pred_ser2 = pd.Series([np.argmax(line) for line in oof])\nprint(classification_report(y_true, y_pred_ser2))","a2b121cc":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_true, y_pred_ser2),\n            cmap='viridis', annot=True, fmt=\".0f\",\n            xticklabels=class_labels,yticklabels=class_labels);","126f1341":"label_dict","c46c1170":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter","d980e4cc":"X = train_df_t.drop([\"target\",\"target_num\"], axis=1).copy()\ny = train_df_t['target_num'].values","a3b496a2":"counter = Counter(train_df_t['target_num'])\nprint(counter)","c9f3f646":"#def objective(trial):\n    \n    #X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,\n                                                          #random_state=1945, stratify=y)\n    \n    #sm = SMOTE(random_state=1945)\n    #X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train.ravel())\n    \n    #sc = MinMaxScaler()\n    #X_train_sm = sc.fit_transform(X_train_sm)\n    #X_valid = sc.transform(X_valid)\n    \n    #dtrain = lgbm.Dataset(X_train_sm, label=y_train_sm)\n    \n    #params = {\n        #\"boosting\": \"gbdt\",\n        #\"objective\":\"multiclass\",\n        #\"metrics\":\"multi_logloss\",\n        #\"num_class\": 4,\n        #\"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        #\"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 255),\n        #\"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n        #\"num_iterations\": trial.suggest_int(\"num_iterations\", 100, 1000),\n        #\"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        #\"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        #\"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        #\"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        #\"max_bin\": trial.suggest_int(\"max_bin\", 2, 255),\n    #}\n    \n    #model = lgbm.train(params,dtrain)\n    #preds = model.predict(X_valid)\n    #logloss = log_loss(y_valid, preds)\n    #return logloss\n\n#study2 = optuna.create_study(direction=\"minimize\",pruner=SuccessiveHalvingPruner())\n#study2.optimize(objective, n_trials=100)\n\n#print(\"Number of finished trials: \", len(study2.trials))\n#print(\"Best trial: \", study2.best_trial.params)","91c18a75":"#study2.best_params","d878ee6d":"# Hyperparameters found with training dataset without any transformation\n\n#best_params = {\n        #\"objective\": \"multiclass\",\n        #\"metrics\": \"multi_logloss\",\n        #\"num_class\": 4,\n        #\"learning_rate\": 0.09816514507767324,\n        #\"num_leaves\": 206,\n        #\"max_depth\": 30,\n        #\"num_iterations\":1000,\n        #\"bagging_fraction\": 0.6918190371064439,\n        #\"bagging_freq\": 6,\n        #\"lambda_l1\": 0.0007670842709700264,\n        #\"lambda_l2\": 0.019842881946747774,\n        #\"max_bin\": 127\n#}","d49855b0":"# Hyperparams for transformed train dataset\nbest_params = {'learning_rate': 0.01382837447462176,\n 'num_leaves': 117,\n 'max_depth': 29,\n 'num_iterations': 399,\n 'bagging_fraction': 0.4885473849475542,\n 'bagging_freq': 1,\n 'lambda_l1': 0.00039613672631532814,\n 'lambda_l2': 0.0005240426920058911,\n 'max_bin': 181}","ee3ee6b5":"#best_params = study2.best_params\nbest_params[\"objective\"] = \"multiclass\"\nbest_params[\"metrics\"] = \"multi_logloss\"\nbest_params[\"num_class\"] = 4","4ecec27e":"X['kfold'] = -1\nskf = StratifiedKFold(n_splits=5)\nvalid_idx_list = []\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=X, y=y)):\n    X.loc[valid_idx, \"kfold\"] = fold\n    valid_idx_list.append(valid_idx)","f4cac1ad":"X[\"target_num\"] = y","77f65601":"def run_training3(algo, df, test, fold, oof):\n    t_df = df[df.kfold != fold].reset_index(drop=True)\n    v_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = t_df.drop([\"kfold\",\"target_num\"], axis=1)\n    xvalid = v_df.drop([\"kfold\",\"target_num\"], axis=1)\n    \n    ytrain = t_df['target_num'].values\n    yvalid = v_df['target_num'].values\n    \n    sm = SMOTE(random_state=1945)\n    xtrain_sm, ytrain_sm = sm.fit_resample(xtrain, ytrain)\n    \n    sc = MinMaxScaler()\n    xtrain_sm = sc.fit_transform(xtrain_sm)\n    xvalid = sc.transform(xvalid)\n    test = sc.transform(test)\n    \n    dtrain = lgbm.Dataset(xtrain_sm, label=ytrain_sm)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    model = algo.train(best_params,\n            train_set=dtrain,\n            num_boost_round=5000,\n            valid_sets=[dtrain,dvalid],\n            valid_names=['train','valid'],\n            early_stopping_rounds=100,\n            verbose_eval=100\n    )\n    \n    ypred = model.predict(xvalid)\n    oof[valid_idx_list[fold]] = ypred\n    \n    test_ypred = model.predict(test)\n    logloss = log_loss(yvalid, ypred)\n    \n    print()\n    print(f\"Fold={fold+1}\")\n    print(f\"Valid's ROC AUC: {roc_auc_score(yvalid, ypred, multi_class='ovo'):.5f}\")\n    print(f\"Valid's logloss: {log_loss(yvalid, ypred):.5f}\")\n    \n    return model, oof, test_ypred, logloss\n\n\noof = np.zeros((len(X),NUM_CLASS))\ntest_oof3 = np.zeros((len(test_df), NUM_CLASS))\nlogloss_list = []\n\nfor fold in range(5):\n    lgbm_model, oof, test_preds, logloss = run_training3(lgbm, X, test_transformed, fold, oof)\n    test_oof3 += test_preds \/ NUM_CLASS\n    logloss_list.append(logloss)\nprint(f\"Mean log_loss after {fold+1} folds: {np.mean(logloss_list)}\")","6d7ce343":"#seed = 1945\n#train_transformed = train_transformed.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n#train_transformed['kfold'] = -1\n#skf = StratifiedKFold(n_splits=5)\n\n#for fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_transformed, y=train_transformed['target_num'])):\n    #train_transformed.loc[valid_idx, \"kfold\"] = foldy_true = X[\"target_num\"].values\ny_pred_ser3 = pd.Series([np.argmax(line) for line in oof])\nprint(classification_report(y_true, y_pred_ser3))","dc043dc4":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_true, y_pred_ser3),\n            cmap='viridis', annot=True, fmt=\".0f\",\n            xticklabels=class_labels, yticklabels=class_labels);","83a69ab6":"sub_df = pd.DataFrame(test_oof3) # np.clip(final_preds, 0.025, 0.975)\nsub_df.columns = label_dict.keys()\nsub_df[\"id\"] = sub_sample_df[\"id\"].values\nsub_df.to_csv(\"sub_SMOTE_transform_5folds.csv\", index=False)","e1687e30":"from collections import Counter","b597aadf":"def get_class_weights(y):\n    counter = Counter(y)\n    majority = max(counter.values())\n    return {cls: round(float(majority) \/ float(count), 2) for cls, count in counter.items()}\n\nclass_weights = get_class_weights(X[\"target_num\"].values)\nprint(class_weights)","3b403a2a":"pd.Series(y_true).unique()","7356b27f":"pd.Series(y).unique()","57ab592e":"X = train_df.drop([\"target\",\"target_num\"],axis=1)\ny = train_df[\"target_num\"]","3f2329a3":"#import optuna\n#from optuna.pruners import SuccessiveHalvingPruner\n#from sklearn.model_selection import train_test_split\n\n\n#def objective(trial):\n    \n    #X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,\n                                                          #random_state=1945, stratify=y)\n    \n    #sc = MinMaxScaler()\n    #X_train = sc.fit_transform(X_train)\n    #X_valid = sc.transform(X_valid)\n    \n    #params = {\n        #\"objective\":\"multiclass\",\n        #\"metrics\":\"multi_logloss\",\n        #\"num_class\": 4,\n        #\"class_weight\": class_weights,\n        #\"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        #\"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 255),\n        #\"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n        #\"num_iterations\": trial.suggest_int(\"num_iterations\", 100, 1000),\n        #\"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        #\"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        #\"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        #\"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        #\"max_bin\": trial.suggest_int(\"max_bin\", 2, 255),\n    #}\n    \n    #model = lgbm.LGBMClassifier(**params)\n    #model.fit(X_train, y_train)\n    #preds = model.predict_proba(X_valid)\n    #logloss = log_loss(y_valid, preds)\n    #return logloss\n\n#study3 = optuna.create_study(direction=\"minimize\",pruner=SuccessiveHalvingPruner())\n#study3.optimize(objective, n_trials=100)\n\n#print(\"Number of finished trials: \", len(study3.trials))\n#print(\"Best trial: \", study3.best_trial.params)","19ef52b6":"#best_params = study3.best_params","215751b7":"# hyperparams from optuna\nbest_params = {'learning_rate': 0.03491633418671592,\n               'num_leaves': 248,\n               'max_depth': 29,\n               'num_iterations': 852,\n               'bagging_fraction': 0.9713021482558285,\n               'bagging_freq': 7,\n               'lambda_l1': 0.013629154943194912,\n               'lambda_l2': 0.6536148007804358,\n                'max_bin': 6}\n\nbest_params[\"objective\"] = \"multiclass\"\nbest_params[\"metrics\"] = \"multi_logloss\"\nbest_params[\"num_class\"] = 4\nbest_params[\"class_weight\"] = class_weights","7abc15e3":"train_df['kfold'] = -1\nskf = StratifiedKFold(n_splits=5)\nvalid_idx_list = []\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df['target_num'])):\n    train_df.loc[valid_idx, \"kfold\"] = fold\n    valid_idx_list.append(valid_idx)","0aab7764":"def run_training4(algo, df, test, fold, oof):\n    t_df = df[df.kfold != fold].reset_index(drop=True)\n    v_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = t_df.drop([\"kfold\",\"target\",\"target_num\"], axis=1)\n    xvalid = v_df.drop([\"kfold\",\"target\",\"target_num\"], axis=1)\n    \n    ytrain = t_df['target_num'].values\n    yvalid = v_df['target_num'].values\n    \n    sc = MinMaxScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    test = sc.transform(test)\n    \n\n    model = lgbm.LGBMClassifier(**best_params)\n    model.fit(xtrain, ytrain)\n    \n    ypred = model.predict_proba(xvalid)\n    oof[valid_idx_list[fold]] = ypred\n    \n    test_ypred = model.predict_proba(test)\n    logloss = log_loss(yvalid, ypred)\n    \n    print()\n    print(f\"Fold={fold+1}\")\n    print(f\"Valid's ROC AUC: {roc_auc_score(yvalid, ypred, multi_class='ovo'):.5f}\")\n    print(f\"Valid's logloss: {log_loss(yvalid, ypred):.5f}\")\n    \n    return model, oof, test_ypred, logloss\n\n\noof = np.zeros((len(train_df),NUM_CLASS))\ntest_oof4 = np.zeros((len(test_df), NUM_CLASS))\nlogloss_list = []\n\nfor fold in range(5):\n    lgbm_model, oof, test_preds, logloss = run_training4(lgbm, train_df, test_df, fold, oof)\n    test_oof4 += test_preds \/ NUM_CLASS\n    logloss_list.append(logloss)\nprint(f\"Mean log_loss after {fold+1} folds: {np.mean(logloss_list)}\")","0c7f814c":"y_true = train_df[\"target_num\"].values\ny_pred_ser4 = pd.Series([np.argmax(line) for line in oof])\nprint(classification_report(y_true, y_pred_ser4))","cb788aa2":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_true, y_pred_ser4), \n            cmap='viridis', annot=True, fmt=\".0f\",\n            xticklabels=class_labels, yticklabels=class_labels);","b713dae3":"sub_df = pd.DataFrame(np.clip(test_oof4, 0.025, 0.975))\nsub_df.columns = label_dict.keys()\nsub_df[\"id\"] = sub_sample_df[\"id\"].values\nsub_df.to_csv(\"sub_class_weight.csv\", index=False)","33496899":"**Table of content**\n1. Training and testing datasets with their properties.\n2. Create dummies from features with low cardinality.\n3. Calculate logarithm for every instance in features.\n4. Base model prediction.\n5. Hyperparameters search using Optuna.\n6. Final model prediction and submission.\n7. Prediction and submission 5 kfold model with Optuna best hyperparams.\n8. SMOTE technique.\n9. Finding classes weights.","087335ee":"The best hyperparameters for SMOTE dataset:\n\n{'learning_rate': 0.09816514507767324,\n 'num_leaves': 206,\n 'max_depth': 30,\n 'num_iterations': 1000,\n 'bagging_fraction': 0.6918190371064439,\n 'bagging_freq': 6,\n 'lambda_l1': 0.0007670842709700264,\n 'lambda_l2': 0.019842881946747774,\n 'max_bin': 127}\n","cc1e08a2":"Classification report reveals one important advantage of using this technique, which is improve prediction for other classes. Just for remider, model without using SMOTE has predicted mostly class 2.","dd775db8":"Distribution between classes in features seems to be balanced.","8f364038":"best hyperparameters for model with weighted classes:\n{'learning_rate': 0.03491633418671592,\n 'num_leaves': 248,\n 'max_depth': 29,\n 'num_iterations': 852,\n 'bagging_fraction': 0.9713021482558285,\n 'bagging_freq': 7,\n 'lambda_l1': 0.013629154943194912,\n 'lambda_l2': 0.6536148007804358,\n 'max_bin': 6}","34937ee3":"A rule of thumb for interpreting the variance inflation factor:\n1 = not correlated.\nBetween 1 and 5 = moderately correlated.\nGreater than 5 = highly correlated.","d25755de":"### Variance Inflation Factor\n\nVariance Inflation Factor (VIF) is used to detect the presence of multicollinearity","62354ff7":"## Training and testing datasets with their properties","8af428d1":"The model trained on dataset without any transformation has recivied 1.08741 in LB but it is clear that the model with or without transformation doesn't predict class_1 and class_4 at all. Maybe SMOTE technique could help with that.","52806545":"### Dummies variablbe form features with low cardinality","837e3803":"Target column is unbalanced with the majority (57.5 %) of class_2, (21.5%) of class_3, (12.5%) of class_4 and (8.5%) of class_1. We should use StratifiedKFold to split our dataset.","35e2475c":"There are some differences in number of unique values in train and test sets. If we want to apply techniques like Label_encoding on features we have to take that for consideration.","e38c0c99":"## Final model prediction and submission","6bbb6eb6":"## Class Weights","0b85d5f9":"### Optuna","f9d431df":"Metrics with train dataset without any transformation:\n- Valid's ROC AUC: 0.57390\n- Valid's logloss: 1.09211\n- Mean log_loss after 5 folds: 1.09298154529417","96bd236b":"The best hyperparameters found with optuna:\n\n{'learning_rate': 0.025845340173733224,\n 'num_leaves': 8,\n 'max_depth': 28,\n 'num_iterations': 754,\n 'boosting': 'gbdt',\n 'bagging_fraction': 0.8681224214865364,\n 'bagging_freq': 2,\n 'lambda_l1': 8.538579636440432e-08,\n 'lambda_l2': 3.164760963598122e-07,\n 'max_bin': 149,\n 'objective': 'multiclass',\n 'metrics': 'multi_logloss',\n 'num_class': 4}","b6eccd5f":"## Base model","c915db76":"### Features cardinality.","72debc64":"## Optuna","e296bb30":"Both train and test set have very similar distribution and they all right skewed. We might have to deal with it and check if improves a model metrics.","e0624b78":"## SMOTE Technique","19f76a6e":"### Calculate log","1acabb3d":"The dataset has no missing values ","8cf7c8f5":"## Optuna best params with 5 kfolds submission"}}