{"cell_type":{"592aa136":"code","6de54f66":"code","54251080":"code","19e64e3d":"code","4eda7a26":"code","3425dab3":"code","10fd69f8":"code","7c2cd487":"code","652aa3f8":"code","fbf90db0":"code","16e05184":"code","7205f788":"code","9977a07d":"code","42dcb166":"code","f472501f":"code","2d73d653":"code","ed3584b4":"code","d8097244":"code","e136eb15":"code","292d613c":"code","a4fdbbbf":"code","fef84351":"code","cd140fdf":"code","a0bf9c7f":"code","dcaae810":"code","e54d8d8a":"code","b7b0f663":"code","2803ace9":"code","b28113ea":"code","619049f5":"code","dceffa38":"code","04391a21":"code","d21f9d4a":"code","ebc69160":"code","3b08f404":"code","a9e3e46e":"code","895f1ce0":"code","07ad53fc":"code","b13e6a62":"code","7ddffc19":"code","3fde850f":"code","60644ba3":"code","0142378e":"code","88e6ba27":"code","9078c935":"code","7a3c1eff":"code","1f443356":"code","0d9c6246":"code","e5a48dfa":"code","25234325":"code","38de0db7":"code","ffe10f33":"code","3430f3db":"code","0a375217":"code","5fc2f7fc":"code","51b91907":"code","d8c7cdd5":"code","635465c3":"code","b022f70e":"code","4b67a133":"code","b8ac283a":"code","25999a85":"code","afb5e8bb":"code","e44cca6b":"code","3760598a":"code","3f57e3ea":"code","6de14135":"code","15ed142a":"code","e82c1b27":"code","74529ac9":"code","831511b6":"code","30b1e937":"code","e9d2a8c9":"code","c454ccd8":"code","06ecf902":"code","af4997ee":"code","6dd1364d":"code","2f2718c6":"code","3f7d9a7f":"code","c46c24a0":"code","197312ed":"code","0145d03c":"code","e83957e5":"code","88dc0887":"markdown","9638ee06":"markdown","e3271ae5":"markdown","1edc7e09":"markdown","7be841ef":"markdown","e1179966":"markdown","7a54e3b0":"markdown","3edd9d73":"markdown","40eb7c1e":"markdown","125d4304":"markdown","27c7adfb":"markdown","a4e99fc1":"markdown","01c97b71":"markdown","de80aad7":"markdown","fde6c0bb":"markdown","a997faa8":"markdown","fb1b4e70":"markdown","2304d4ab":"markdown","cf4c4aaf":"markdown","88e2379d":"markdown","5dc864fa":"markdown","3cb148fe":"markdown","671339b4":"markdown"},"source":{"592aa136":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6de54f66":"#load libraries\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","54251080":"import cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)","19e64e3d":"pd.set_option('display.max_columns',None)","4eda7a26":"data = pd.read_csv('\/kaggle\/input\/employee-attrition\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n#copy the data to another df\nempdf = data.copy()","3425dab3":"#look over shape of data\nempdf.shape","10fd69f8":"#look at sample of data\nempdf.head()","7c2cd487":"#let's see what data types how many features we are having\nempdf.dtypes.value_counts()","652aa3f8":"empdf['Attrition'].value_counts()","fbf90db0":"#fig = go.Figure(figsize=(4,5))\n#trace = go.Pie(labels=empdf['Attrition'], values=empdf['Attrition'].value_counts())\n#iplot([trace])","16e05184":"# pie chart for attrition\nfig = go.Figure()\nfig.add_trace(go.Pie(labels=empdf['Attrition'], values=empdf['Attrition'].value_counts()))\nfig.update_layout(autosize=False, width=400, height=350)\nfig.show()","7205f788":"# let's see the total Missing values in dataset\nempdf.isnull().sum().sum()","9977a07d":"plt.figure(figsize=(13,9))\nsns.heatmap(empdf.corr(),vmax=0.8,linewidth=0.1,cmap='coolwarm')\nplt.show()","42dcb166":"categorical_features = empdf.select_dtypes(include=[np.object]).columns\nprint(\"total categorical_features\",len(categorical_features))","f472501f":"empdf[categorical_features].head()","2d73d653":"# Business Travel\nempdf['BusinessTravel'].value_counts()","ed3584b4":"plt.figure(figsize=(8,5))\nsns.countplot(x='BusinessTravel',hue='Attrition', data=empdf, palette='plasma')\nplt.title(\"Attrition wrt BusinessTravel\")\nplt.show()","d8097244":"# using plotly express.\nfig = px.histogram(empdf, x='BusinessTravel', color='Attrition', barmode=\"group\",\n            width=650,height=390)\nfig.show()","e136eb15":"#Department\nempdf['Department'].value_counts()","292d613c":"# Department wrt Attrition\nplt.figure(figsize=(8,5))\nsns.countplot(x='Department',hue='Attrition', data=empdf, palette='rocket')\nplt.title(\"Attrition w.r.t Department\")\nplt.show()","a4fdbbbf":"#EducationField\nempdf['EducationField'].value_counts()","fef84351":"# Department wrt Attrition\nplt.figure(figsize=(8,5))\nsns.countplot(x='EducationField',hue='Attrition', data=empdf, palette='seismic')\nplt.title(\"Attrition w.r.t EducationField\")\nplt.xticks(rotation=45)\nplt.show()","cd140fdf":"#Gender\nempdf['Gender'].value_counts()","a0bf9c7f":"# most male of female employes Attriate\n# Department wrt Attrition\nplt.figure(figsize=(7,5))\nsns.countplot(x='Gender',hue='Attrition', data=empdf, palette='plasma')\nplt.title(\"Gender w.r.t Attrition\")\nplt.legend(loc='best')\nplt.show()","dcaae810":"# let's also see, how any male and female are Married\npd.crosstab(empdf['MaritalStatus'],empdf['Gender'])","e54d8d8a":"#JobRole\nempdf['JobRole'].nunique()","b7b0f663":"# let's see at which post most people are leaving the jobs\n# JobRole\nplt.figure(figsize=(8,5))\nsns.countplot(x='JobRole',hue='Attrition', data=empdf, palette='plasma')\nplt.title(\"JobRole w.r.t Attrition\")\nplt.legend(loc='best')\nplt.xticks(rotation=45)\nplt.show()","2803ace9":"#OverTime\nempdf['OverTime'].value_counts()","b28113ea":"plt.figure(figsize=(7,5))\nsns.countplot(x='OverTime',hue='Attrition', data=empdf, palette='plasma')\nplt.title(\"OverTime w.r.t Attrition\")\nplt.legend(loc='best')\nplt.show()","619049f5":"# how many male and female do overTime\npd.crosstab(empdf['OverTime'], empdf['Gender'])","dceffa38":"numerical_features = [feature for feature in empdf.columns if empdf[feature].dtype != 'O']\nprint(\"Total numerical_features\", len(numerical_features))","04391a21":"empdf[numerical_features].head()","d21f9d4a":"# distribution of age\nsns.distplot(empdf['Age'],hist=False)\nplt.show()","ebc69160":"empdf['Age'].nunique()","3b08f404":"ordinal_features = ['Education','EnvironmentSatisfaction','JobInvolvement','JobSatisfaction',\n                    'PerformanceRating','RelationshipSatisfaction','WorkLifeBalance']\nempdf[ordinal_features].head()","a9e3e46e":"# Education\nempdf['Education'].value_counts()","895f1ce0":"edu_map = {1 :'Below College', 2: 'College', 3 :'Bachelor', 4 :'Master', 5: 'Doctor'}\nplt.figure(figsize=(7,5))\nsns.countplot(x=empdf['Education'].map(edu_map), hue='Attrition', data=empdf, palette='prism')\nplt.title(\"Education W.R.T Attrition\")\nplt.show()","07ad53fc":"#EnvironmentSatisfaction\nempdf['EnvironmentSatisfaction'].value_counts()   # it is very nice response from employees","b13e6a62":"#JobInvolvement\nempdf['JobInvolvement'].value_counts()","7ddffc19":"numerical_features = [feature for feature in empdf.columns if empdf[feature].dtype != 'O' and feature not in ordinal_features]\nprint(\"total numerical_features\",len(numerical_features))","3fde850f":"empdf[numerical_features].head()","60644ba3":"#MonthlyIncome\nsns.distplot(empdf['MonthlyIncome'],hist=False)\nplt.show()","0142378e":"plt.figure(figsize=(6,5))\nsns.boxplot(empdf['MonthlyIncome'])\nplt.show()","88e6ba27":"# MonthlyIncome with respect to Age of Employee\n\n#sns.lineplot(x='Age',y='MonthlyIncome', data=empdf)\ntrace = go.Scatter(x=empdf['Age'],y=empdf['MonthlyIncome'], mode=\"markers+lines\",\n                   marker=dict(size=12), line=dict(shape='spline'))\ndata=[trace]\n\nlayout = {\"title\":\"Monthly Income Variation wrt Age\",\n           \"xaxis\":{\"title\":\"Age\"},\n           \"yaxis\":{\"title\":\"MonthlyIncome\"}\n         }\n\niplot({\"data\":data, \"layout\":layout})","9078c935":"# employee count in age group and whicg age employees are more\nempdf.groupby('Age')['EmployeeCount'].sum().sort_values(ascending=False).head()","7a3c1eff":"#JobLevel\nempdf['JobLevel'].value_counts()","1f443356":"# JobLevel can have some effect on Attrition of Employees\nsns.countplot(x='JobLevel',hue='Attrition',data=empdf)\nplt.show()","0d9c6246":"#NumCompaniesWorked\nsns.countplot(x='JobLevel',hue='Attrition',data=empdf)\nplt.show()","e5a48dfa":"empdf['StockOptionLevel'].value_counts()","25234325":"#drop the columns which have save val in whole dataset or which are unrelevant for Attrition prediction. \nempdf.drop(['EmployeeCount','EmployeeNumber','StandardHours'],axis=1, inplace=True)","38de0db7":"empdf[categorical_features].head()","ffe10f33":"# Target Variable(Attrition)\nempdf['Attrition'] = empdf['Attrition'].replace({'No':0,'Yes':1})","3430f3db":"#encode binary variables\nempdf['OverTime'] = empdf['OverTime'].map({'No':0,'Yes':1})\nempdf['Gender'] = empdf['Gender'].map({'Male':0,'Female':1})","0a375217":"#encode categorical columns which are ordinal, use labelEncoding\ncat_cols = ['BusinessTravel','Department','EducationField','JobRole','MaritalStatus']\nfor col in cat_cols:\n    map_dict = {k:i for i, k in enumerate(empdf[col].value_counts().index,0)}\n    empdf[col] = empdf[col].map(map_dict)","5fc2f7fc":"# drop the Over18 column\nempdf.drop('Over18',axis=1,inplace=True)","51b91907":"empdf.corr()['Attrition'][:-1].sort_values(ascending=False)","d8c7cdd5":"plt.figure(figsize=(13,9))\nsns.heatmap(empdf.corr(),vmax=0.8,linewidth=0.1,cmap='coolwarm')\nplt.show()","635465c3":"empdf.head()","b022f70e":"x = empdf.drop('Attrition',axis=1)\ny = empdf['Attrition']","4b67a133":"from sklearn.ensemble import ExtraTreesClassifier\n\nextra_tree = ExtraTreesClassifier()\n\nextra_tree.fit(x,y)\n\nfeat_importance = extra_tree.feature_importances_","b8ac283a":"#plotting a feature importance\nplt.figure(figsize=(11,9))\nfeat_imp = pd.Series(extra_tree.feature_importances_, index=x.columns)\n\nfeat_imp.nlargest(20).plot(kind='barh')\nplt.show()","25999a85":"from sklearn.preprocessing import MinMaxScaler\n\nmin_max = MinMaxScaler()\n\nx_scaled = min_max.fit(x).transform(x)","afb5e8bb":"ExtraTree = ExtraTreesClassifier()\n\nExtraTree.fit(x_scaled, y)\n\nfeature_importance = pd.Series(ExtraTree.feature_importances_, index=x.columns)\nfeature_importance","e44cca6b":"plt.figure(figsize=(10,9))\nfeature_importance.nlargest(20).plot(kind='barh')\nplt.show()","3760598a":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#metrics\nfrom sklearn.metrics import accuracy_score, classification_report","3f57e3ea":"#split data into train and test set.\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)\nprint(\"training shape: \",x_train.shape)\nprint(\"testing shape: \",x_test.shape)","6de14135":"#models\nlog_clf = LogisticRegression()\nsvc_clf = SVC()\nknn_clf = KNeighborsClassifier()\ndt_clf = DecisionTreeClassifier()\nrf_clf = RandomForestClassifier()","15ed142a":"# first check the accuracy score on without scalled features\nfor clf in [log_clf, svc_clf, knn_clf, dt_clf, rf_clf]:\n    clf.fit(x_train, y_train)\n    \n    pred = clf.predict(x_test)\n    \n    print(clf.__class__.__name__, \" \", accuracy_score(y_test,pred))","e82c1b27":"# let's try out VotingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_clf =VotingClassifier([('lgclf',log_clf),('svc',svc_clf),('knn',knn_clf),('dt',dt_clf),('rf',rf_clf)])\n\nvoting_clf.fit(x_train,y_train)\n\ny_pred = voting_clf.predict(x_test)\nprint(\"acuracy: \",accuracy_score(y_test,y_pred))","74529ac9":"# if we do same thing on scaled features\n#split data into train and test set using scaled data.\nx_train_scaled,x_test_scaled,y_train_scaled,y_test_scaled = train_test_split(x_scaled,y,test_size=0.25,random_state=42)\nprint(\"training shape: \",x_train_scaled.shape)\nprint(\"testing shape: \",x_test_scaled.shape)","831511b6":"# Now check the accuracy score on scalled features\nfor clf in [log_clf, svc_clf, knn_clf, dt_clf, rf_clf]:\n    clf.fit(x_train_scaled, y_train_scaled)\n    \n    pred = clf.predict(x_test_scaled)\n    \n    print(clf.__class__.__name__, \" \", accuracy_score(y_test,pred))","30b1e937":"# try votingClf with scaled data\n\nvoting_clf =VotingClassifier([('lgclf',log_clf),('svc',svc_clf),('knn',knn_clf),('dt',dt_clf),('rf',rf_clf)])\n\nvoting_clf.fit(x_train_scaled,y_train_scaled)\n\ny_pred = voting_clf.predict(x_test_scaled)\nprint(\"acuracy: \",accuracy_score(y_test_scaled,y_pred))","e9d2a8c9":"from sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier","c454ccd8":"boost = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=1), n_estimators=500, algorithm='SAMME',learning_rate=0.01)\n\nboost.fit(x_train_scaled,y_train_scaled)\n\npredictions = boost.predict(x_test_scaled)\n\nprint(\"accuracy:\",accuracy_score(y_test,predictions))","06ecf902":"#training accuracy\nprint(\"training accuracy:\",boost.score(x_train_scaled,y_train_scaled))\nprint(\"testing accuracy:\",boost.score(x_test_scaled,y_test_scaled))","af4997ee":"# XGBOOST\nxgb = XGBClassifier()\n\nxgb.fit(x_train_scaled, y_train_scaled)\n\nprediction = xgb.predict(x_test_scaled)\n\nprint(\"accuracy: \",accuracy_score(y_test,prediction))","6dd1364d":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score","2f2718c6":"# let's first try hyperparameter tuning with RandomForest\n\n#number of trees in forest\nn_estimators = [int(x) for x in np.linspace(100,1200,12)]\n#max depth of tree\nmax_depth = [int(x) for x in np.linspace(5,30,6)]\n#quality of split\ncriterion = ['gini','entropy']\n#min no. of samples to consider for splitting a internal node\nmin_samples_split = [2,5,7,10]\n#min number of node can be as leaf node\nmin_samples_leaf = [2,5,8]\n# The number of features to consider when looking for the best split:\nmax_features = [\"auto\",\"sqrt\"]\n\nrandom_grid = dict(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion,\n                  min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,\n                   max_features=max_features)\nprint(random_grid)","3f7d9a7f":"rf_clf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(rf_clf, param_distributions=random_grid, cv=5, n_iter=10,\n                               random_state=42, n_jobs=-1,verbose=1,)\n\nrf_random.fit(x_train_scaled, y_train_scaled)","c46c24a0":"print(rf_random.best_score_)\nprint(rf_random.best_params_)","197312ed":"# hyperparameter tuning for XgBoost\n\nn_estimators = [int(x) for x in np.linspace(100,1000,10)]\n\nmax_depth = [int(x) for x in np.linspace(6,30,5)]\nlearning_rate = [0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01]\nmin_child_weight = list(range(1,10))\n\nxg_grid = dict(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n               min_child_weight=min_child_weight)\nprint(xg_grid)","0145d03c":"xgb = XGBClassifier()\nxgb_random = RandomizedSearchCV(xgb, param_distributions=xg_grid, cv=5, random_state=42,\n                               n_iter=10,scoring=\"accuracy\",n_jobs=-1,verbose=1)\n\nxgb_random.fit(x_train_scaled,y_train_scaled)","e83957e5":"print(\"score: \",xgb_random.best_score_)\nprint(\"best_params: \\n\",xgb_random.best_params_)","88dc0887":"# EDA\n\n### Univariate and Bivariate Analysis\n**Categorical Features**","9638ee06":"**OBSERVATIONS**\n- The employees are more who travel very rare, and the number of Attition of such employees are more. It is also one reason which creates one hypothesis in mind, that should emp got less chance to outing and thus they are not comfortable with job.\n- Employees working in R&D department are more, but employees from sales department or at position like sales executive,sale Representative leaves the job early.\n- Males are more under Attrition then Females\n- Male workers are more who do overtime.","e3271ae5":"### Feature Selection with Feature Scaling","1edc7e09":"\n### *Additinal Description Which will help to understand the features*\n\n**Hourly Rate:**  An hourly employee is paid a wage for each hour worked, unlike a salaried employee.\n\n**EmployeeNumber:** An Employee Number is a unique number that has been assigned to each current and former State employee and elected official in the Position and Personnel DataBase (PPDB).\n\n**JobLevel:** Job levels, also known as job grades and classifications, set the responsibility level and expectations of roles at your organization. They may be further defined by impact, seniority, knowledge, skills, or job title, and are often associated with a pay band. The way you structure your job levels should be dictated by the needs of your unique organization and teams.\n\n**StockOptionLevel:** Employee stock options, also known as ESOs, are stock options in the company\u2019s stock granted by an employer to certain employees. Typically they are granted to those in management or officer-level positions.\nStock options give the employee the right to buy a certain amount of stock at a specific price, during a specific period of time. Options typically have expiration dates as well, by which the options must have been exercised, otherwise they will become worthless.","7be841ef":"* JobLevel seems to be important feature, we will see that in further analysis.\n* TotalWorkingYears have positive relationship with JobLevel, and MonthlyIncome.\n* little bit of positive relationship can be seen between YearsAtCompany with YearsInCurrentRole, and YearsWithCurrentManager.","e1179966":"### Description about the data\nEducation 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\n\nEnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobInvolvement 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nJobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nPerformanceRating 1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\n\nRelationshipSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n\nWorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'","7a54e3b0":"**This is reason why Feature Scaling is used. by scaling down the features we are able to achieve better accuracy**","3edd9d73":"- Only a little bit of changed occured due to scaling the features that, can be proved strong from modelling part.","40eb7c1e":"## Categorical Encoding","125d4304":"**Observations**\n* we only have int and string data types features. there is no feature with float. 26 features are numerical and 9 features are categorical\n* Attrition in out target value which has no missing value. But, the quantity of data of emp having Attrition is less compared to employees whoch do not have Attrition. \n- It's very good that we are having a complete dataset, there is no any missing values in dataset.","27c7adfb":"## FeatureSelection","a4e99fc1":"## Hyperparameter(Performance) Tuning","01c97b71":"## Modelling","de80aad7":"**Analysis of exact numerical features except ordinal labels which are pre-encoded**","fde6c0bb":"**The graph is to much overlapper but it increases quadratic and It is clear that Income Increases with respect to Age and Experience**","a997faa8":"**Numerical Features**","fb1b4e70":"## Attrition:- ***company losing its customer base***\n\n**Attrition is a process in which the workforce dwindles at a company, following a period in which a number of people retire or resign, and are not replaced.**\n- A reduction in staff due to attrition is often called a hiring freeze and is seen as a less disruptive way to trim the workforce and reduce payroll than layoffs\n- In this NoteBook our Aim will be to analyze the datasets completely wrt each and feature and find the reasin behind Attrition of Employees.\n- And what the top factors which lead to employee attrition?","2304d4ab":"### **If you like this Notebook, Please Upvote. It gives motivation for making a new notebooks and positively move ahead with a Data science journey**\n\n**Thank You!..** ","cf4c4aaf":"**OBSERVATIONS:** ****Factors Responsible for Employee Attrition****\n\n- OverTime has highest relationship with Attrition, Employee who do OverTime, changes or leaves the Job early. we have seen this above also during categorical variable analysis.\n- Age is 2nd factor responsible, it can be that who are senior, are retiring or the employees who are bachelor has more expectations more organization.\n- MonthlyIncome is also a greatest factor for employee to Attritate\n- StockOptionLevel, we have seen can be determing factor, because employees are in chance that they should be given priority and power and consider to have involvement to buy Company stocks.\n- EnvironmentSatisfaction, JobSatisfaction, DistanceFromHome, WorkLifeBalance, JobLevel is at same level affects equally to the organization employees base.","88e2379d":"**ADABOOST:** Adaboost is a boosting algoritm, which works on concept of errror correction using the concept of SAMME(Stagwise Adaptive Multimodelling using Multiclass exponential loss function) algorithm. It works sequentially by building a sequential model that works on a error generated by previous model. Repeatedly it do this process untill iterations are over of error becomes 0. Thus, it exploits the dependency between the models and different from Bagging(RandomForest).\n\n**XGBoost:** Xgboost is an extension to GBM, for better speed and performance. It is trained in additive manner. It prevents the overfitting, and outliers. for xgboost we also no need to impute missing values, it takes care of that too.  ","5dc864fa":"### Making a Corelation","3cb148fe":"* Age column is very well normalized, most of employees are age between 25 to 40.\n- we are having some of the numerical columns which are lebel encoded for us, they are ordinal labels, so let's have a look at them first","671339b4":"**OBSERVATIONS**\n- Employees from Bachelor are more, then from Masters background. Attrition wrt to bachelor can be seem more because they have more and more expectation from companies and it will be interesting to see the reason behind this in this dataset.\n- EnvironmentSatisfaction can be rank as high or medium. means very less employees do not like the work environment which cannot be seem as great cause for changing a job.\n- JobInvolvement of employees is very much high."}}