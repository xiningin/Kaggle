{"cell_type":{"427c0fb6":"code","22632c2f":"code","9ead138b":"code","06fac3ac":"code","bdac63fe":"code","bc6e2301":"code","48c5deac":"code","a5c7ebc5":"code","925f0d91":"code","9e7b028b":"code","50ede0e0":"code","cafe718c":"code","f25bc947":"code","5c38356b":"code","c3512d34":"code","eb1c0f2a":"markdown","2ec88c82":"markdown"},"source":{"427c0fb6":"import pandas as pd\nimport numpy as np\nfrom tqdm import *\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline\n\n\ndf = pd.read_csv('..\/input\/homepage_actions.csv')\ndf.info() # getting the basic info about our dataset","22632c2f":"# total number of actions\ndf.action.count()\n","9ead138b":"# number of unique users\ndf.id.nunique()","06fac3ac":"# size of the control and the experiment group\ndf.groupby('group').nunique()['id']","bdac63fe":"# converting the timestamp column to datetime format\ndf['timestamp'] = pd.to_datetime(df['timestamp'])","bc6e2301":"# duration of experiment\ndf.timestamp.max() - df.timestamp.min()","48c5deac":"# experiment group users\nexperiment_gr = df.query(\"group == 'experiment'\")","a5c7ebc5":"# click through rate for experiment group users\nexperiment_ctr = experiment_gr.query(\"action == 'click'\").nunique()['id']\/experiment_gr.query(\"action == 'view'\").nunique()['id']\nexperiment_ctr","925f0d91":"# control group users\ncontrol_gr = df.query(\"group == 'control'\")","9e7b028b":"# click through rate for experiment group users\ncontrol_ctr = control_gr.query(\"action == 'click'\").nunique()['id']\/control_gr.query(\"action == 'view'\").nunique()['id']\ncontrol_ctr","50ede0e0":"# bootstrapping the sampling distribution\ndiffs = []\nfor i in tqdm(range(10000)):\n    sample = df.sample(4000 ,replace = True)\n    experiment_gr = sample.query(\"group == 'experiment'\")\n    control_gr = sample.query(\"group == 'control'\")\n    experiment_ctr = experiment_gr.query(\"action == 'click'\").nunique()['id']\/experiment_gr.query(\"action == 'view'\").nunique()['id']\n    control_ctr = control_gr.query(\"action == 'click'\").nunique()['id']\/control_gr.query(\"action == 'view'\").nunique()['id']\n    diffs.append( experiment_ctr - control_ctr)","cafe718c":"# ploting the sampling distribution\nplt.hist(diffs)","f25bc947":"# null values\nnull_vals = np.random.normal(0 ,np.array(diffs).std() ,10000)","5c38356b":"# ploting null values\n# with the observed stats\nplt.hist(null_vals)\nplt.axvline(x = np.array(diffs).mean() ,color = 'r') # we can see the observed stats are way out of the range of mean null vals","c3512d34":"# getting p-values\np_vals = (null_vals > np.array(diffs).mean()).mean()\np_vals # with a p-value of less then 1 % , we can safely reject the null","eb1c0f2a":"Thus we can conclude that the click through rate of the new page is definitely better  then old page. i recommend audacity to launch the new web page on the basis of the above reasoning","2ec88c82":"\n\nIn this case study, you\u2019ll analyze A\/B test results for Audacity. Here's the customer funnel for typical new users on their site:\n\nView home page > Explore courses > View course overview page > Enroll in course > Complete course\n\nAudacity loses users as they go down the stages of this funnel, with only a few making it to the end. To increase student engagement, Audacity is performing A\/B tests to try out changes that will hopefully increase conversion rates from one stage to the next.\n\nWe\u2019ll analyze test results for two changes they have in mind, and then make a recommendation on whether they should launch each change.\n\nThe first change Audacity wants to try is on their homepage. They hope that this new, more engaging design will increase the number of users that explore their courses, that is, move on to the second stage of the funnel.\n\nThe metric we will use is the click through rate for the Explore Courses button on the home page. Click through rate (CTR) is often defined as the the number of clicks divided by the number of views. Since Audacity uses cookies, we can identify unique users and make sure we don't count the same one multiple times. For this experiment, we'll define our click through rate as:\n\nCTR: # clicks by unique users \/ # views by unique users\n\nNow that we have our metric, let's set up our null and alternative hypotheses:\n\nH_0: CTR_{new} \\leq CTR _{old} H 0 \u200b :CTR new \u200b \u2264CTR old \u200b\n\nH_1: CTR_{new} > CTR _{old} H 1 \u200b :CTR new \u200b >CTR old \u200b\n\nOur alternative hypothesis is what we want to prove to be true, in this case, that the new homepage design has a higher click through rate than the old homepage design. And the null hypothesis is what we assume to be true before analyzing data, which is that the new homepage design has a click through rate that is less than or equal to that of the old homepage design. As you\u2019ve seen before, we can rearrange our hypotheses to look like this:\n\nH_0: CTR_{new} - CTR_{old} \\leq 0 H 0 \u200b :CTR new \u200b \u2212CTR old \u200b \u22640 H_1: CTR_{new} - CTR_{old} > 0 H 1 \u200b :CTR new \u200b \u2212CTR old \u200b >0"}}