{"cell_type":{"50e8bed9":"code","c410ca50":"code","7c39b93a":"code","74c15ab5":"code","a0beded1":"code","aa2bbc50":"code","db1e6ca6":"code","78cdb520":"code","56a6a938":"code","7fde0a0b":"code","734c15f9":"code","adf76251":"code","f184f375":"code","95de7303":"code","57a59ab3":"code","d3a257cc":"code","0eb90239":"code","c963b6e0":"code","0e0e07c3":"code","72034cf2":"code","bc89183f":"code","3314d694":"code","2b818804":"markdown","a046b403":"markdown","893ec73d":"markdown","6bf98618":"markdown","a623ad0e":"markdown","c9e838c3":"markdown","c9282f2f":"markdown","af1a95c9":"markdown"},"source":{"50e8bed9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c410ca50":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n!pip install pycaret","7c39b93a":"# Importing our dataset\ndf=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","74c15ab5":"df.columns","a0beded1":"# Toh doston,Chaliye shuru karte hain \n# Let us start our EDA\ndf.head()","aa2bbc50":"df","db1e6ca6":"df.info()","78cdb520":"# Checking for missing values\ndf.isnull().sum()\n","56a6a938":"# Let us seperate our target variable from the features so that we can build a model. We will be using a Supervised learning \n# approach in this notebook by making using of different classifiers, cross-validating,among other things.\n\n\nx=df.iloc[:,:-1].values\ny=df.target\n","7fde0a0b":"# let us look at x and y\n\nx","734c15f9":"y","adf76251":"# let us look at the correlation of different features with the target variable\n\nmatrix=df.corr()\nsns.heatmap(matrix)","f184f375":"print(matrix['target'])","95de7303":"profile = pp.ProfileReport(df, title='Heart Disease Dataset Report')","57a59ab3":"profile","d3a257cc":"# let us look at the skew of different features \nprint(df.skew())","0eb90239":"# Let us plot all the features except the one with binary values or repeating values ('I am doing this to viualise the variance in \n#  variance exhibited by other fetures')\n# Creating a new dataframe and dropping all the columns that are not required in the visual\ndf1=df.drop([ 'sex', 'cp',   'fbs', 'restecg', \n       'exang',  'slope', 'ca', 'thal'],axis=1)\nsns.pairplot(df1,hue='target')","c963b6e0":"df.dtypes","0e0e07c3":"from pycaret.classification import * \nexp = setup(data = df, target = 'target', session_id=1,\n                  normalize = True,\n                categorical_features = [ 'sex', 'cp',  'fbs', 'restecg', \n       'exang',  'slope' , 'thal', ],\n                numeric_features=['age','trestbps','chol','thalach','oldpeak','ca'],\n                categorical_imputation='mode',\n                numeric_imputation='mean',\n            remove_outliers=True,\n            outliers_threshold=0.1,\n            normalize_method='robust',\n            feature_selection=True,\n            feature_selection_threshold=0.9,\n            remove_multicollinearity=True,\n            train_size=0.8\n            \n                )","72034cf2":"compare_models()","bc89183f":"from pycaret.classification import * \nexp = setup(data = df, target = 'target', session_id=1,\n                  normalize = True,\n                categorical_features = [ 'sex', 'cp',  'fbs', 'restecg', \n       'exang',  'slope' , 'thal', ],\n                numeric_features=['age','trestbps','chol','thalach','oldpeak','ca'],\n                categorical_imputation='mode',\n                numeric_imputation='mean',\n            normalize_method='robust',\n            feature_selection=True,\n            feature_selection_threshold=0.9,\n            remove_multicollinearity=True,\n            train_size=0.8\n            \n                )","3314d694":"compare_models()","2b818804":"Let us now look at individual d=features in depth by using the pandas_profiling library. See below","a046b403":"So our PyCaret setup is now successful. I have set the train size at 80 % and teest size at 20%. You can play around with these values to find the ideal ratio. Let us now comapre our models.\n","893ec73d":"As you can see the results obtained were not that great.The maximum accuracy that we obtained was around 82 % with KNN classifier and a 10 fold cross validation. Let us see what happens if we dont remove the outliers.","6bf98618":"An instant increase of 2% in the accuracy . Let us now use only a select few features based on their correlation and see if this affects our model in anyway.","a623ad0e":"Ok so we have no missing values. ","c9e838c3":"We can use a log tranformation on the dataset to reduce the skew and try to transform the datset as closely as possible to a normal distribution. However let us first try by using some classification models and then let us see what we shall do next.","c9282f2f":"> Go through the above report generated using pandas-profiling to get better insight about individual features.\n\n","af1a95c9":"As you can see features are not highly correlated with the target varibale. The max correlation is 0.433 for the feature namely 'cp'"}}