{"cell_type":{"ca30eacd":"code","5b2e3c30":"code","11ed700a":"code","552b0237":"code","05403b7a":"code","97dd4883":"code","986142db":"code","59557e60":"code","73ede48d":"code","85e0b7d8":"code","d475412b":"code","a44522dc":"code","e516b281":"code","f9cf83be":"code","8d31aa00":"code","004c09ee":"code","d63e2262":"code","7613bbe6":"code","de2c1e36":"code","ae93c6ab":"code","203b1c9a":"code","bb733796":"code","e106d3a3":"code","f43e9295":"code","4926fdb8":"code","a79dc29c":"code","78c5a0f6":"code","22647ab4":"code","f0fd8efc":"code","9f59be83":"code","d521d47b":"code","914e5836":"code","b323a085":"code","95eeaee6":"code","98ad86c0":"code","770e68d6":"code","a41d4273":"code","063db26f":"code","955c97a8":"code","dba776bc":"code","a7ed9480":"code","2306203f":"code","1ece2d9c":"code","a56e02eb":"code","6b46afe0":"code","9ac850cc":"code","0e6533d2":"code","4c810f8e":"code","e00063f2":"code","4bd2f304":"code","4bb415a8":"code","f2f7b8e2":"code","d5a4c09e":"code","23320cfd":"code","5d3605d8":"code","c8ce626a":"code","2a7ed88c":"code","8a42c181":"code","ff9d05df":"code","65df8b10":"code","94236123":"code","1280cc8b":"code","67e8598b":"code","fb953776":"code","fd312896":"code","8edf5547":"code","63ef2d42":"code","f29c3729":"code","30ddc3d6":"code","6c6a32d2":"code","6f15a0e0":"code","b889714c":"code","975274d8":"code","63162f5a":"code","5322baf9":"code","a5573514":"code","3adf202c":"code","e35d0133":"code","4710bd87":"code","80f1a8c5":"markdown","189ed270":"markdown","13b1306c":"markdown","1b5c17bd":"markdown","7da6e7a1":"markdown","79a32c26":"markdown","387361e5":"markdown","e256077e":"markdown","b1c81175":"markdown","0f53a0c5":"markdown","a2ff6f3a":"markdown","d01c4310":"markdown","a3e3aca6":"markdown","15e5a82c":"markdown","ec83fc11":"markdown","b9dd526f":"markdown","a5e9516c":"markdown","ded46978":"markdown","8c62946e":"markdown","527ccd3f":"markdown","60417869":"markdown","a2421093":"markdown","df123f72":"markdown","cf3bc911":"markdown","0e9ab73a":"markdown","28f455ef":"markdown","cf9aaa51":"markdown","50144a10":"markdown","3a0524ca":"markdown","4f7cd15d":"markdown","fe59aaad":"markdown","2cd4fefd":"markdown","ac3682d6":"markdown","779d3838":"markdown","82800811":"markdown","83b442f3":"markdown","e0d17385":"markdown","adbabca7":"markdown","771eeed8":"markdown","cb79ef90":"markdown","dcf51fbf":"markdown","1ecb6850":"markdown","cb617204":"markdown","f57db41a":"markdown","df81bb05":"markdown","ae4db5cc":"markdown","ee9f5d07":"markdown"},"source":{"ca30eacd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt # visualization\n!pip install seaborn as sns -q # visualization with seaborn v0.11.1\nimport seaborn as sns # visualization\nimport missingno as msno # missing values pattern visualization\n\nimport warnings # supress warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nimport math\n\n\nplt.style.use('bmh')\n\n# set pandas display option\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\n# Load the data \nBooks_df = pd.read_csv('..\/input\/book-recommendation-dataset\/Books.csv')\nRatings_df = pd.read_csv('..\/input\/book-recommendation-dataset\/Ratings.csv')\nUsers_df = pd.read_csv('..\/input\/book-recommendation-dataset\/Users.csv')","5b2e3c30":"# display the dataset\nRatings_df.head().style.set_caption('Sample of Ratings data')","11ed700a":"# dimension of dataset\nprint(f'''\\t  Book_df shape is {Books_df.shape}\n          Ratings_df shape is {Ratings_df.shape}\n          Users_df shape is {Users_df.shape}''')","552b0237":"def missing_zero_values_table(df):\n    mis_val=df.isnull().sum()\n    mis_val_percent=round(df.isnull().mean().mul(100),2)\n    mz_table=pd.concat([mis_val,mis_val_percent],axis=1)\n    mz_table=mz_table.rename(\n    columns={df.index.name:'col_name',0:'Missing Values',1:'% of Total Values'})\n    mz_table['Data_type']=df.dtypes\n    mz_table=mz_table.sort_values('% of Total Values',ascending=False)\n    print(f\"Your selected dataframe has \"+str(df.shape[1])+\" columns and \"+str(df.shape[0])+\" Rows.\\n\"\n         \"There are \"+str(mz_table[mz_table.iloc[:,1] != 0].shape[0])+\n          \" columns that have missing values.\")\n    return mz_table.reset_index()","05403b7a":"missing_zero_values_table(Users_df)","97dd4883":"missing_zero_values_table(Ratings_df)","986142db":"missing_zero_values_table(Books_df)","59557e60":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.boxplot(y='Book-Rating', data=Ratings_df,ax=ax[0])\nax[0].set_title('Find outlier data in Rating Book column')\nsns.boxplot(y='Age', data=Users_df,ax=ax[1])\nax[1].set_title('Find outlier data in Age column')","73ede48d":"print(sorted(Users_df.Age.unique()))","85e0b7d8":"Users_df.Location.unique()","d475412b":"len(Users_df.Location.unique())","a44522dc":"Books_df['Book-Author'].describe()","e516b281":"print(Books_df['Year-Of-Publication'].unique().tolist())","f9cf83be":"1.0 - (np.count_nonzero(Ratings_df)\/float(Ratings_df.size))","8d31aa00":"sorted(Ratings_df['Book-Rating'].unique())","004c09ee":"Ratings_df.shape[0]","d63e2262":"usersCount=Users_df.shape[0]\nbooksCount=Books_df.shape[0]\nprint(f'Users : {usersCount}')\nprint(f'Books : {booksCount}')\nprint(f'Total : {usersCount*booksCount}')","7613bbe6":"ratings_new = Ratings_df[Ratings_df.ISBN.isin(Books_df.ISBN)]\nratings_new = ratings_new[Ratings_df['User-ID'].isin(Users_df['User-ID'])]","de2c1e36":"print(\"Users or books aren't in dataset\")\nprint(f'Total : {Ratings_df.shape[0] - ratings_new.shape[0]}')","ae93c6ab":"sparsity = round(1.0 - len(ratings_new)\/float(usersCount*booksCount),6)\nsparsity","203b1c9a":"Ratings_df.rename(columns={'User-ID':'user_id','Book-Rating':'book_rating'},inplace=True)\nUsers_df.rename(columns={'User-ID':'user_id'},inplace=True)\nBooks_df.rename(columns={'Book-Title':'Book_Title','Book-Author':'Book_Author',\n                         'Year-Of-Publication':'Year_Of_Publication'},inplace=True)","bb733796":"Users_df['Country']='Iran'\nfor i in Users_df:\n    Users_df['Country']=Users_df.Location.str.extract(r'\\,+\\s?(\\w*\\s?\\w*)\\\"*$')   ","e106d3a3":"len(Users_df.Country.unique())","f43e9295":"Users_df.isnull().sum()","4926fdb8":"Users_df.loc[Users_df.Country.isnull(),'Country']='other'","a79dc29c":"pd.crosstab(Users_df.Country,Ratings_df.book_rating).T.style.background_gradient()","78c5a0f6":"Users_df['Country'].replace(['','alachua','america','austria','autralia','cananda','geermany','italia','united kindgonm','united sates','united staes','united state','united states','us'],\n                           ['other','usa','usa','australia','australia','canada','germany','italy','united kingdom','usa','usa','usa','usa','usa'],inplace=True)","22647ab4":"# Create column Count_All_Rate\nRatings_df['Count_All_Rate']=Ratings_df.groupby('ISBN')['user_id'].transform('count')","f0fd8efc":"cm=sns.light_palette('green',as_cmap=True)\npopular=Users_df.Country.value_counts().to_frame()[:10]\npopular.rename(columns={'Country':'Count_Users_Country'},inplace=True)\npopular.style.background_gradient(cmap=cm)","9f59be83":"# outlier data became NaN\nUsers_df.loc[(Users_df.Age > 100 ) | (Users_df.Age < 5),'Age']=np.nan","d521d47b":"Users_df.Age.plot.hist(bins=20,edgecolor='black',color='red')","914e5836":"round(Users_df.Age.skew(axis=0,skipna=True),3)","b323a085":"# Series of users data live in which country \ncountryUsers = Users_df.Country.value_counts()","95eeaee6":"country=countryUsers[countryUsers>=5].index.tolist()","98ad86c0":"# Range of Age users in country register in this library and had participation\nRangeOfAge = Users_df.loc[Users_df.Country.isin(country)][['Country','Age']].groupby('Country').agg(np.mean).to_dict()","770e68d6":"\nfor k,v in RangeOfAge['Age'].items():\n    Users_df.loc[(Users_df.Age.isnull())&(Users_df.Country== k),'Age'] = v\n    ","a41d4273":"Users_df.isnull().sum()","063db26f":"medianAge = int(Users_df.Age.median())\nUsers_df.loc[Users_df.Age.isnull(),'Age']=medianAge","955c97a8":"Users_df.isnull().sum()","dba776bc":"Books_df[Books_df.Book_Author.isnull()]","a7ed9480":"Books_df.loc[(Books_df.ISBN=='9627982032'),'Book_Author']='other'","2306203f":"Books_df[Books_df.Publisher.isnull()]","1ece2d9c":"Books_df.loc[(Books_df.ISBN=='193169656X'),'Publisher']='other'\nBooks_df.loc[(Books_df.ISBN=='1931696993'),'Publisher']='other'","a56e02eb":"Books_df[Books_df.Year_Of_Publication=='Gallimard']","6b46afe0":"Books_df[Books_df.Year_Of_Publication=='DK Publishing Inc']","9ac850cc":"Books_df.loc[Books_df.ISBN=='2070426769','Year_Of_Publication']=2003\nBooks_df.loc[Books_df.ISBN=='2070426769','Book_Author']='Gallimard'","0e6533d2":"Books_df.loc[Books_df.ISBN=='0789466953','Year_Of_Publication']=2000\nBooks_df.loc[Books_df.ISBN=='0789466953','Book_Author']='DK Publishing Inc'\nBooks_df.loc[Books_df.ISBN=='078946697X','Year_Of_Publication']=2000\nBooks_df.loc[Books_df.ISBN=='078946697X','Book_Author']='DK Publishing Inc'","4c810f8e":"Books_df.Year_Of_Publication=Books_df.Year_Of_Publication.astype(np.int32)","e00063f2":"print(sorted(Books_df.Year_Of_Publication.unique()))","4bd2f304":"Books_df.loc[(Books_df.Year_Of_Publication>=2021)|(Books_df.Year_Of_Publication==0),'Year_Of_Publication']=np.NAN","4bb415a8":"Books_df.isnull().sum()","f2f7b8e2":"author=Books_df[Books_df.Year_Of_Publication.isnull()].Book_Author.unique().tolist()","d5a4c09e":"RangeYearOfPublication = Books_df.loc[Books_df.Book_Author.isin(author)][['Book_Author','Year_Of_Publication']].groupby('Book_Author').agg(np.mean).round(0).to_dict()","23320cfd":"meanYear=round(Books_df.Year_Of_Publication.mean())\nauthorNanYear={}\nauthorYear={}\nfor k,v in RangeYearOfPublication['Year_Of_Publication'].items():\n    if math.isnan(v) != True:\n        authorYear[k]=v\n    else:\n        authorNanYear[k] = meanYear","5d3605d8":"len(authorNanYear.keys())","c8ce626a":"len(authorYear.keys())","2a7ed88c":"# for k,v in authorYear.items():\n#     Books_df.loc[(Books_df.Year_Of_Publication.isnull())&(Books_df.Book_Author== k),'Year_Of_Publication'] = v","8a42c181":"Books_df.loc[Books_df.Year_Of_Publication.isnull(),'Year_Of_Publication'] = round(Books_df.Year_Of_Publication.mean())","ff9d05df":"ratings_new = Ratings_df[Ratings_df.ISBN.isin(Books_df.ISBN)]\nratings_new = ratings_new[ratings_new.user_id.isin(Users_df.user_id)]","65df8b10":"ratings_0 = ratings_new[ratings_new.book_rating ==0]\nratings_1to10 = ratings_new[ratings_new.book_rating !=0]\n# Create column Rating average \nratings_1to10['rating_Avg']=ratings_1to10.groupby('ISBN')['book_rating'].transform('mean')\n# Create column Rating sum\nratings_1to10['rating_sum']=ratings_1to10.groupby('ISBN')['book_rating'].transform('sum')","94236123":"ratings_0.shape[0]","1280cc8b":"ratings_1to10.shape[0]","67e8598b":"ratings_1to10.head()","fb953776":"dataset=Users_df.copy()\ndataset=pd.merge(dataset,ratings_1to10,on='user_id')\ndataset=pd.merge(dataset,Books_df,on='ISBN')","fd312896":"def skew_test(df):\n    col = df.skew(axis = 0, skipna = True)\n    val = df.skew(axis = 0, skipna = True) \n    sk_table = pd.concat([col, val], axis = 1)\n    sk_table = sk_table.rename(\n    columns = {0 : 'skewness'})\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n        \"There are \" + str(sk_table.shape[0]) +\n          \" columns that have skewed values - Non Gaussian distribution.\")\n    return sk_table.drop([1], axis = 1).sort_values('skewness',ascending = False).reset_index()","8edf5547":"skk = skew_test(dataset)\nskk.style.background_gradient(cmap='Blues')","63ef2d42":"fig, ax = plt.subplots(figsize=(18,8))\nsns.countplot(data=ratings_1to10,x='book_rating',ax=ax)","f29c3729":"print(dataset.columns.tolist())","30ddc3d6":"dataset=dataset[['user_id', 'Location', 'Age', 'Country', 'ISBN', 'book_rating', 'rating_Avg','rating_sum', 'Count_All_Rate', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher']]","6c6a32d2":"missing_zero_values_table(dataset)","6f15a0e0":"cm=sns.light_palette('red',as_cmap=True)\n# count all rate means include users rated 0 to book\npopular=dataset.groupby(['Book_Title','Count_All_Rate','rating_Avg','rating_sum']).size().reset_index().sort_values(['rating_sum','rating_Avg',0],\n                                                                                                            ascending=[False,False,True])[:20]\npopular.rename(columns={0:'Count_Rate'},inplace=True)\npopular.style.background_gradient(cmap=cm)","b889714c":"dataset.head()","975274d8":"def manhattan(rating1,rating2):\n    \"Computes the Manhattan distance. Both rating1 and rating2 are dictionaries\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += abs(user1[key] - user2[key])\n    return distance\nprint(f'Manhattan distance between user number 8 and 11676 : {manhattan(8,11676)}')","63162f5a":"def euclidean(rating1,rating2):\n    \"Computes the Euclidean distance. Both rating1 and rating2 are dictionaries\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += math.pow(abs(user1[key]-user2[key]),2)\n    return math.sqrt(distance)\nprint(f'Euclidean distance between user number 8 and 11676 : {euclidean(8,11676)}')  \n","5322baf9":"def minkowski(rating1,rating2,r):\n    \"\"\"Computes the Minkowski distance. Both rating1 and rating2 are dictionaries\"\"\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += math.pow(abs(user1[key]-user2[key]),r)\n    return math.pow(distance,1\/r)\nprint(f'Minkowski distance between user number 8 and 11676 : {minkowski(8,11676,2)}') ","a5573514":"counts1 = ratings_1to10['user_id'].value_counts()\nratings_1to10 = ratings_1to10[ratings_1to10['user_id'].isin(counts1[counts1 >= 100].index)]\ncounts = ratings_1to10['book_rating'].value_counts()\nratings_1to10 = ratings_1to10[ratings_1to10['book_rating'].isin(counts[counts >= 100].index)]","3adf202c":"dataset.user_id.unique().tolist()[500]","e35d0133":"def computeNearestNeighbor(username):\n    \"\"\"Creates a sorted list of users based on their distance \n    to username \"\"\"\n    #users = list(dataset.user_id.unique())\n    users=dataset.user_id.unique().tolist()[:500]\n    distances = []\n    for user in users:\n        if user != username:\n            distance = manhattan(user,username)\n            distances.append((distance,user))\n    # sort based on distance -- closest first\n    distances.sort()\n    return distances\ncomputeNearestNeighbor(192762)","4710bd87":"def recommend(username):\n    \"\"\"Give list of recommendations\"\"\"\n    # first find nearest neighbor\n    nearest=computeNearestNeighbor(username)[0][1]\n    recommendations=[]\n    # now find bands neighbor rated that user didn't\n    neighborRatings = dataset.loc[dataset.user_id==nearest].Book_Title.tolist()\n    userRatings = dataset.loc[dataset.user_id==username].Book_Title.tolist()\n    for artist in neighborRatings:\n        if not artist in userRatings:\n            recommendations.append((artist,int(dataset[(dataset.Book_Title==artist) & (dataset.user_id==nearest)].book_rating)))\n    return sorted(recommendations,key=lambda artistTuple : artistTuple[1],reverse=True)\nprint(recommend(192762))","80f1a8c5":"0 is an invalid number in the rated books  \nand rating value must be 1 to 10","189ed270":"**Country Column**","13b1306c":"The First step is to find  persons who are similar to user  \nso must be calculated distance   \nand distance can calculate by those methods    \n1. Manhattan distance \n2. Euclidean distance\n3. Minkowski distance","1b5c17bd":"Check outlier data in **Age** and **Book-Rating** column  ","7da6e7a1":"Create Column 'count rate'   \nuser participation in rated   \nand even users rated the books zero   ","79a32c26":"Age column has 39 percent null data   \nand age column has outlier data   \nand I can don't use Cosine Similarity   \nso let's do it together  \nif any things it's not correct I'm really become happy to tell me ","387361e5":"I don't like this method, but I force to use this solution","e256077e":"In the plot and in the unique value   \nwe understand we have outlier data   \nso for outlier data I convert it to Nan value  ","b1c81175":"We don't need 3 columns : 'Image-URL-S', 'Image-URL-M', 'Image-URL-L'","0f53a0c5":"**new Ratings_book dataset**","a2ff6f3a":"Age : 244 :))","d01c4310":"1959 authors don't have year of publication of them books    \nand they return value   \nbut it's take long time to fill Nan value   \nI would like to find a fast way :))  \nbut now I don't know   \nif you know please tell me in the comment  ","a3e3aca6":"Say us Miss [Agatha Christie](https:\/\/www.biography.com\/writer\/agatha-christie) is top in Books data frame","15e5a82c":"There are 20 most popular books in dataset  \nand they bought and rated it","ec83fc11":"What !!  \nWhy it's recommended 'Wild Animus' book   \navg rate is low, but sum rate is high    \nthis is one problem of that    \nDo you know how can I fix this bug ??  \nIf you know say in the comment box  ","b9dd526f":"368 of users Country column is Nan so must be fill it ","a5e9516c":"OK let's find our unique value in Location column ","ded46978":"**Year of Publication**","8c62946e":"# Summarize the dataset","527ccd3f":"Ok we have Outlier data in Age    \nso must be fixed it   ","60417869":"POF again we have 330 null Value   \nfor fill in it   \nAge has **positive Skewness** (right tail)        \nso we I have one idea to fill Na value from **Median**     ","a2421093":"This method it's not helpful     \nI must find another way      ","df123f72":"Rating Average and","cf3bc911":"Users rated **1149780** books, but there are **271360** books        \nso users did not rate all books      \nand users participation that rated make two question   \n1. Are the books they rated part of the book's data frame ?  \n2. Are the users they rated part of the user's data frame ?  \n\n","0e9ab73a":"Dataset has a lot of users had rated lower than ten books   \nand  users don't paid attention to some books  \nso I will drop it  ","28f455ef":"Age has **positive Skewness** (right tail)      \nso we I have one idea to fill Na value from **Median**   \nfor this we don't like to fill Na value **just for one range of age** for handle it I use **country column** to fill Na ","cf9aaa51":"1355 authors don't have a year of publication and the average of them is Nan   \nand I forced filling Nan value with mean of all year of publication authors","50144a10":"**Country and Users**","3a0524ca":"I don't have great knowledge, but I try to create best :))","4f7cd15d":"**Publisher column has Nan value**","fe59aaad":"**Book Author** column has **Nan** value","2cd4fefd":"# Collaborative Filtering ","ac3682d6":"**Age Columns**","779d3838":"<hr>","82800811":"Some data has Misspellings ","83b442f3":"Ok everything's ok  ","e0d17385":"**Steps**\n1. rename columns names :))\n2. Create country column to analyze better \n3. Fill Na value in  Country column \n4. Some data in Country Column has Misspellings \n5. Create rating_Avg and number_of_rating to analyze better\n6. users more in which countries\n7. Age column has outlier data \n8. Fill Na value in Age column \n9. Fill Na value in Book data frame's Author column \n10. Fill Na value in Book data frame's Publisher column \n11. Book data frame's Year of Publication column has two string value and some integer value  type is string\n12. Book data frame's Year of Publication has outlier data \n13. Fill Na value in Book data frame's Year of Publication \n14. join three data frames together\n15. Delete user and book columns they rated but aren't in the dataset\n16. Rating_book value must be 1 to 10\n17. drop three unhelpful columns 'Image-URL-S', 'Image-URL-M', 'Image-URL-L' ","adbabca7":"Separate 1 to 10 and 0 rated value","771eeed8":"In the below chart there is one row has named 'other' it's mean    \nlocation is Nan, or regex it's not able to read","cb79ef90":"57339 unique Value it's really hard to understand  \nso use regex and create column country","dcf51fbf":"Years of publication after 2021 and 0 it's not normal   \nso must be converted to Nan value","1ecb6850":"15 percent sparse","cb617204":"Year of publication **2037** !!  \n'**Gallimard**' , '**DK Publishing Inc**' , type of sum year is **string**  ","f57db41a":"# Simple Popularity based Recommendation System","df81bb05":"# Visualization and Modeling","ae4db5cc":"So I don't have any idea Location column has 57339 unique value    \nfor this I use Regex and create country column   \nbut we have [195 Countries in the World !!](https:\/\/www.worldometers.info\/geography\/how-many-countries-are-there-in-the-world\/)  \nBut it's better than 57339 unique Location value :))  \n","ee9f5d07":"It shows us Manhattan distance between user 192762 with 500 other users distance to suggest book   \nand it's not helpful for high count users   \nso must find another solution   "}}