{"cell_type":{"22dce1c3":"code","0689622b":"code","eab4d781":"code","64cba353":"code","70886637":"code","ee6cda57":"code","7fd81d19":"code","06a8716b":"code","496583d0":"code","e41d55d8":"code","b6785c1a":"code","0bb70219":"code","2640ad62":"code","f68ba300":"code","6527e34b":"code","3a2190cb":"code","f852fb83":"code","886253dd":"code","88a4027a":"code","9733d5fa":"code","cf534071":"code","8e531c75":"code","b2e768f6":"code","a9447dc4":"code","68feb3b2":"code","bdd07179":"code","e96adccc":"code","e5f7ac43":"code","9f3402a3":"code","2957f5c2":"code","88e88a21":"code","d617f74d":"code","39375a2d":"markdown","ed91c420":"markdown","47fe8258":"markdown","52d672cb":"markdown","8f8b0e80":"markdown","9fa01155":"markdown","2ddaeb1f":"markdown","e4147355":"markdown"},"source":{"22dce1c3":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n# from pandas.tools.plotting import scatter_matrix\n\n\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport random\nimport keras\n\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import chi2_contingency, ttest_ind\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nimport xgboost as xgb\nimport pprint\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n#Configure Visualization Defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\npd.set_option('max_columns', None)","0689622b":"train_csv = \"..\/input\/dataquest-2021-level-1\/train.csv\"\ntest_csv = \"..\/input\/dataquest-2021-level-1\/test.csv\"\nSS_csv = \"..\/input\/dataquest-2021-level-1\/submission_lvl_1.csv\"\n\nX = pd.read_csv(train_csv)\ntest_data = pd.read_csv(test_csv)\nss = pd.read_csv(SS_csv)\n\nCategorical_cols = ['Gender', 'Travel', 'Mode of reservation', 'Class',\n       'Departure\/Arrival time convenient',\n       'Ease of Online booking', 'Platform Number', 'Food Service',\n       'Railway service', 'Seat comfort', 'Ticket-collector service',\n       'Washroom service', 'Baggage security score',\n       'Compartment safety score', 'Cleanliness']\n\nContinious_cols = ['Age','Distance Travelled', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n\nNon_numerical_cols = ['Gender','Travel','Mode of reservation','Class']\n\nTarget_col = ['satisfaction']","eab4d781":"X.head()","64cba353":"for var in Categorical_cols: \n    print(X[var].value_counts())\n\n# view frequency distribution of categorical variables\nfor var in Categorical_cols: \n    print(X[var].value_counts()\/np.float(len(X)))","70886637":"#check null\nprint(X.info())\nprint(\"\\n\\n\",test_data.info())","ee6cda57":"#check cardinality\nfor var in Categorical_cols:\n    print(var, ' : ', len(X[var].unique()), ' labels')\n    \n#self note : cardinality is fine afterall ","7fd81d19":"X.describe()","06a8716b":"#replave NaN\nX['Arrival Delay in Minutes'].fillna(X['Arrival Delay in Minutes'].median(), inplace=True)\ntest_data['Arrival Delay in Minutes'].fillna(X['Arrival Delay in Minutes'].median(), inplace=True)","496583d0":"X","e41d55d8":"# def plot_f(col):\n#     city_count  = X[col].value_counts()\n# #     city_count = city_count[:10,]\n#     plt.figure(figsize=(10,5))\n#     sns.barplot(city_count.index, city_count.values, alpha=0.8)\n#     plt.ylabel('Number of Occurrences', fontsize=12)\n#     plt.xlabel(col, fontsize=24)\n#     plt.show()\n    \n# def plot_cont(col):\n#     plt.figure(figsize=(10,5))\n#     plt.xlabel(col, fontsize=24)\n#     plt.ylabel('Number of Occurrences', fontsize=12)\n#     sns.distplot(X[col],bins=100,kde=False)\n#     plt.show()\n    \n# for i in range(len(Categorical_cols)):\n#     plot_f(Categorical_cols[i])\n    \n# for i in range(len(Continious_cols)):\n#     plot_f(Continious_cols[i])\n\n# # extra hours, invested in stk market, \n# # level of edu, id proof, most prod hour, \n# # sport in XP, Trades done, tax category, income category,\n# # marital status handle -1s in test and train\n","b6785c1a":"# func1 = lambda r: r\/r.sum()\n\n# for i in Categorical_cols:\n#     plt.figure(figsize=(5, 10))\n#     stackCol = pd.crosstab(X[i], X[\"satisfaction\"])\n#     print(f\"Chi Square test b\/w {i} and Answer is: \", stats.chi2_contingency(np.array(stackCol))[1])\n#     stackCol = stackCol.apply(func1, axis=1)\n#     stackCol.plot(kind='bar', stacked=True)\n#     plt.show()","0bb70219":"LE = LabelEncoder()\n\ndef string_to_num(X):\n    for col in Non_numerical_cols:\n        X[col] = LE.fit_transform(X[col])\n    return X\n\nX = string_to_num(X)\n\ntest_data = string_to_num(test_data)\n","2640ad62":"X","f68ba300":"def adj_minority(X):\n    \n    \n    X[\"Railway service\"][X[\"Railway service\"] == 0] = 1.0\n    X[\"Seat comfort\"][X[\"Seat comfort\"] == 0] = 1.0\n    X[\"Baggage security score\"][X[\"Baggage security score\"] == 0] = 1.0\n    X[\"Compartment safety score\"][X[\"Compartment safety score\"] == 0] = 1.0\n    X[\"Ticket-collector service\"][X[\"Ticket-collector service\"] == 0] = 1.0\n    X[\"Cleanliness\"][X[\"Cleanliness\"] == 0] = 1.0\n    X[\"Washroom service\"][X[\"Washroom service\"] == 0] = 1.0\n    X[\"Food Service\"][X[\"Food Service\"] == 0] = 1.0\n\n    del X['id']       #useless\n    \n    return X\n\nX = adj_minority(X)\ntest_data = adj_minority(test_data)","6527e34b":"# for i in X.columns:\n#     sns.boxplot(x=i, hue=\"satisfaction\", data=X)\n#     plt.show()","3a2190cb":"# # remove outliers\n\n\n# range_lst = []\n# out_list = ['Trades Done','Pay']\n\n# for i in out_list:\n#     q25, q75 = X[i].quantile(0.25), X[i].quantile(0.75)\n#     iqr = q75-q25\n#     cut_off = iqr*1.5\n#     lower, upper = q25-cut_off, q75+cut_off\n#     range_lst.append((lower, upper))\n# print(range_lst)\n\n# print(X.shape)\n# X = X[(X[\"Trades Done\"]>range_lst[0][0]) & (X[\"Trades Done\"]<range_lst[0][1])]\n# # X = X[(X[\"Pay\"]>range_lst[1][0]) & (X[\"Pay\"]<range_lst[1][1])]\n# print(X.shape)","f852fb83":"print(X[\"satisfaction\"].value_counts())\nprint(X.shape)\n\nfactor = max(X.satisfaction.value_counts())\ndf_majority = X[X.satisfaction==0]\ndf_minority = X[X.satisfaction==1]\n#     Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                    replace = True,     # sample with replacement\n                                    n_samples = factor,    # to match majority class\n                                    random_state = 123) # reproducible results\nX =  pd.concat([df_majority, df_minority_upsampled])\n    \nprint(X.shape)\n\ny = X[\"satisfaction\"]\n\nX.drop(['satisfaction'],axis=1, inplace=True)\nX","886253dd":"# colormap = plt.cm.RdBu\n# plt.figure(figsize=(14,12))\n# plt.title('Pearson Correlation of Features', y=1.05, size=15)\n# sns.heatmap(X.astype(float).corr(),linewidths=0.1,vmax=1.0, \n#             square=True, cmap=colormap, linecolor='white', annot=True)","88a4027a":"ss_lst = Continious_cols\n\nstd_sc = StandardScaler()\nstd_sc.fit(X[ss_lst])\n\nX_temp = X.copy()\ntest_data_temp = test_data.copy()\n\nX_temp[ss_lst] = std_sc.transform(X[ss_lst])\ntest_data_temp[ss_lst] = std_sc.transform(test_data[ss_lst])\n\nX = X_temp\ntest_data = test_data_temp","9733d5fa":"X","cf534071":"del X[\"Cleanliness\"]\ndel test_data[\"Cleanliness\"]\n\ndel X['Departure Delay in Minutes']\ndel test_data['Departure Delay in Minutes']","8e531c75":"Categorical_cols = ['Gender', 'Travel', 'Mode of reservation', 'Class',\n       'Departure\/Arrival time convenient',\n       'Ease of Online booking', 'Platform Number', 'Food Service',\n       'Railway service', 'Seat comfort', 'Ticket-collector service',\n       'Washroom service', 'Baggage security score',\n       'Compartment safety score']\n\nContinious_cols = ['Age','Distance Travelled', 'Arrival Delay in Minutes']\n\nNon_numerical_cols = ['Gender','Travel','Mode of reservation','Class']\n\nTarget_col = ['satisfaction']","b2e768f6":"\ndef one_hot_enc(X):\n#     X = pd.concat([X,pd.get_dummies(X[\"Gender\"],prefix='Gender')],axis=1)\n    for a in Categorical_cols:\n        X = pd.concat([X,pd.get_dummies(X[a],prefix=a)],axis=1)\n    \n    X.drop(Categorical_cols,axis=1, inplace=True)\n    return X\n\nX_OHE = one_hot_enc(X)\ntest_data_OHE = one_hot_enc(test_data)\ny_OHE = pd.get_dummies(y)\n\nX_OHE","a9447dc4":"# from sklearn.preprocessing import RobustScaler\n# cols = X_OHE.columns\n\n# scaler = RobustScaler()\n\n# X_OHE = scaler.fit_transform(X_OHE)\n# test_data_OHE = scaler.transform(test_data_OHE)\n\n# X_OHE = pd.DataFrame(X_OHE, columns=[cols])\n# test_data_OHE = pd.DataFrame(test_data_OHE, columns=[cols])\n# X_OHE","68feb3b2":"X_OHE = X_OHE.values\ny_OHE = y_OHE.values\ntest_data_OHE = test_data_OHE.values\ny=y.values","bdd07179":"X_train_OHE, X_test_OHE, y_train, y_test = train_test_split(X_OHE, y, test_size=5, random_state=42, shuffle=True, stratify=y)\n","e96adccc":"from sklearn.metrics import roc_auc_score\n\ndef get_accuracy(y_pred, y):\n    return roc_auc_score(y_pred, y)","e5f7ac43":"from sklearn.metrics import accuracy_score\nfrom catboost import CatBoostClassifier\n\ndef run_model(MODEL):\n    MODEL.fit(X_train_OHE, y_train)\n\n    pred_train = MODEL.predict(X_train_OHE)\n    print( \"Train acc = \",get_accuracy(y_train, pred_train) )\n\n    pred_val = MODEL.predict(X_test_OHE)\n    print( \"Validation acc = \",get_accuracy(y_test, pred_val) )\n\n    pred_test = MODEL.predict(test_data_OHE)\n    ss[\"satisfaction\"] = pred_test\n    print(ss[\"satisfaction\"].value_counts())\n    print(ss.head())\n    ss.to_csv('level_01_data\/{}_Submission.csv'.format(MODEL), index=False) \n\n    return MODEL \n\n%time\nKNC = KNeighborsClassifier(n_neighbors=15)\nGNB = GaussianNB()\nRFC = RandomForestClassifier(n_estimators=300, criterion='entropy', max_depth=None, oob_score=True )\n# SVC_lin = SVC(kernel=\"linear\", random_state=101)\nCBC = CatBoostClassifier(iterations=10000, learning_rate=0.1) #loss_function='CrossEntropy')\n#9999:\tlearn: 0.0699153\n\nRFC = run_model(CBC)\n%time\n#0.91666 - VA test size = 12, any scaling\n\n","9f3402a3":"# grid_n_estimator = [10, 50, 100, 300, 500]\n# grid_max_depth = [2, 4, 6, 8, 10, None]\n# grid_criterion = ['gini', 'entropy']\n# grid_bool = [True, False]\n# grid_seed = [0]\n\n# param=[{\n# #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n# 'n_estimators': grid_n_estimator, #default=10\n# 'criterion': grid_criterion, #default=\u201dgini\u201d\n# 'max_depth': grid_max_depth, #default=None\n# 'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n# 'random_state': grid_seed\n# }]","2957f5c2":"# import time\n# start = time.perf_counter()\n# RFC_ = RandomForestClassifier()\n# best_search = model_selection.GridSearchCV(estimator = RFC_, param_grid = param, scoring = 'accuracy')\n# best_search.fit(X_train_OHE, y_train)\n# run = time.perf_counter() - start\n\n# best_param = best_search.best_params_\n# print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(RFC_.__class__.__name__, best_param, run))\n# RFC_.set_params(**best_param) ","88e88a21":"# print(best_param)","d617f74d":"# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\n# eclf1 = VotingClassifier(estimators=[('rfc1', RFC_1), ('rfc2', RFC_2), ('rfc', RFC)], voting='hard')\n# eclf1 = eclf1.fit(X_train_OHE, y_train)\n\n# pred_train = eclf1.predict(X_train_OHE)\n# print( \"Train acc = \",get_accuracy(y_train, pred_train) )\n\n# pred_val = eclf1.predict(X_test_OHE)\n# print( \"Validation acc = \",get_accuracy(y_test, pred_val) )\n\n# pred_test = eclf1.predict(test_data_OHE)\n# ss[\"Occupation\"] = pred_test\n# print(ss[\"Occupation\"].value_counts())\n# print(ss.head())\n# ss.to_csv('ENSEMBLE_RFC_2xSVM_hard.csv', index=False)  ","39375a2d":"* ## Upsampling. Could try smote, I didnt.","ed91c420":"# Ensemble, didnt try much but it will yeild the best results given you find a good combination","47fe8258":"## redefine cols","52d672cb":"# MODEL - CBC was used for my best submission","8f8b0e80":"### tried quantiles, didnt give good results","9fa01155":"## plotting takes time, uncomment if needed","2ddaeb1f":"## OHE","e4147355":"# For this my best submission was using catboost.\n\n### Unfortunately most EDA is unavaliale, not that there was much to begin with.\n### I'll try to comment at places I remember doing something different, as we couldn't track our submissions for this competition, take what i say with a grain of salt.\n\n### Do upvote !"}}