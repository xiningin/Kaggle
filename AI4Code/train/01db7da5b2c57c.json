{"cell_type":{"c6ee483a":"code","56394008":"code","f5169661":"code","a660aed3":"code","8f73ddd3":"code","e23caf85":"code","534b4780":"code","7c7e7f5c":"code","a17091e0":"code","0b6d87cb":"code","3cbe4079":"code","10f7e4e3":"code","6d277bee":"code","b0df4278":"markdown","a897e1e3":"markdown","385b89b0":"markdown"},"source":{"c6ee483a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56394008":"df = pd.read_csv('..\/input\/white-wine-quality\/winequallity-white.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf.head()","f5169661":"# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","a660aed3":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","8f73ddd3":"X = df.copy()\ny = X.pop(\"quality\")\n\n\n# YOUR CODE HERE: Define a list of the features to be used for the clustering\nfeatures = [\n    \"fixed acidity\",\n    \"residual sugar\",\n    \"free sulfur dioxide\",\n    \"alcohol\",\n    \"pH\",\n]\n\n\n# Standardize\nX_scaled = X.loc[:, features]\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n\n\n# YOUR CODE HERE: Fit the KMeans model to X_scaled and create the cluster labels\nkmeans = KMeans(n_clusters=10, random_state=0)\nX[\"Cluster\"] = kmeans.fit_predict(X_scaled)","e23caf85":"#The only change here is quality. In the original it was SalePrice\n\nXy = X.copy()\nXy[\"Cluster\"] = Xy.Cluster.astype(\"category\")\nXy[\"quality\"] = y\nsns.relplot(\n    x=\"value\", y=\"quality\", hue=\"Cluster\", col=\"variable\",\n    height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n    data=Xy.melt(\n        value_vars=features, id_vars=[\"quality\", \"Cluster\"],\n    ),\n);","534b4780":"score_dataset(X, y)","7c7e7f5c":"kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n\n\n# Create the cluster-distance features using `fit_transform`\nX_cd = kmeans.fit_transform(X_scaled)\n\n\n# Label features and join to dataset\nX_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\nX = X.join(X_cd)","a17091e0":"score_dataset(X, y)","0b6d87cb":"_ = sns.regplot(df['free sulfur dioxide'], df['quality'])","3cbe4079":"_ = sns.regplot(df['fixed acidity'], df['quality'], color= 'r')","10f7e4e3":"_ = sns.regplot(df['alcohol'], df['quality'], color= 'g')","6d277bee":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke Was here' )","b0df4278":"![](https:\/\/raw.githubusercontent.com\/Mephastophilis\/wine-quality-machine-learning-example\/master\/white_wine_plot.png)github.com","a897e1e3":"#Codes by Mar\u00edlia Prata aka @mpwolke  https:\/\/www.kaggle.com\/mpwolke\/seaborn-40-soiltypes","385b89b0":"#This notebook is an exercise in the Feature Engineering course by Ryan Holbrook.\n\nhttps:\/\/www.kaggle.com\/ryanholbrook\/clustering-with-k-means"}}