{"cell_type":{"430bb631":"code","631d77c9":"code","d49117fa":"code","8282ec4d":"code","eb8022d6":"code","afeac24d":"code","1545dcc9":"code","43734ad9":"code","f0e1cca0":"code","a313eb09":"markdown","06523e1e":"markdown","25b3929c":"markdown","754afc57":"markdown","914c0bac":"markdown","14a6f9de":"markdown","a35e3b44":"markdown","264c0c28":"markdown","eff36f1a":"markdown","3c92efa0":"markdown"},"source":{"430bb631":"#import libraries\nimport tensorflow as tf \nimport numpy as np \nimport pandas as pd \nimport keras \nimport matplotlib.pyplot as plt\nimport tensorflow as tf \n","631d77c9":"#Import the  Mnist data (images of handwritten digits and their labels)\n(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()","d49117fa":"normaliz = keras.layers.experimental.preprocessing.Normalization()\nnormaliz.adapt(training_images)\n\ninputs = keras.Input(shape=(28, 28,1))\nx = normaliz(inputs)\nx = keras.layers.experimental.preprocessing.Normalization()(x)\nx = keras.layers.Conv2D(32, 5, padding='same', activation='relu', input_shape=(28, 28, 1))(x)\nx = keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu')(x)\nx=  keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = keras.layers.Flatten(input_shape=(28, 28))(x)\nx = keras.layers.Dense(256, activation='relu')(x)\nx = keras.layers.Dense(10, activation='softmax')(x)\nmodel = keras.Model(inputs=inputs, outputs=x)","8282ec4d":"#compile model \nmodel.compile(optimizer=\"adam\",\n                loss=\"sparse_categorical_crossentropy\",\n                metrics=['accuracy'])\n\n#reshape training and testing \nx_train1= training_images.reshape(training_images.shape[0],28,28,1)\nx_test1= test_images.reshape(test_images.shape[0],28,28,1)\n","eb8022d6":"#call back  learning schedule\n\ndef scheduler(epoch, lr):\n   if epoch < 10:\n     return lr\n   else:\n     return lr * tf.math.exp(-0.1)\n    \ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","afeac24d":"#fit the model \nmodel.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n          callbacks = [callback],\n           validation_data=(x_test1, test_labels))\n#print learning rate\nround(model.optimizer.lr.numpy(), 5)","1545dcc9":"tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\".\/logs\")\n#fit the model \nmodel.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n          callbacks = [tensorboard_callback],\n           validation_data=(x_test1, test_labels))","43734ad9":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath = \".\/check\",\n    monitor='val_accuracy',\n    mode='auto',\n    save_best_only=True)\n\n\n#fit the model \nmodel.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n          callbacks = [model_checkpoint_callback],\n           validation_data=(x_test1, test_labels))\n\n#when you evulate it will get the best val acc","f0e1cca0":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\n\n#fit the model \nmodel.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n          callbacks = [reduce_lr],\n           validation_data=(x_test1, test_labels))","a313eb09":"# 5.ReduceLROnPlateau\n\n> tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=10,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0,\n    **kwargs\n)\n\n\nReduce learning rate when a metric has stopped improving.\n\nModels often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n\n\n","06523e1e":"# If you like this Notebook\n\n# please Vote and comment ,thanks","25b3929c":"# Callbacks \n\nA callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).","754afc57":"Arguments\n\n**log_dir**: the path of the directory where to save the log files to be parsed by TensorBoard.\n\n**histogram_freq**: frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. (frequency distribuation)\n\n**write_graph**: whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.\n\n**write_images**: whether to write model weights to visualize as image in TensorBoard.\n\n**update_freq**: 'batch' or 'epoch' or integer. When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000, the callback will write the metrics and losses to TensorBoard every 1000 batches. Note that writing too frequently to TensorBoard can slow down your training.\n\n**profile_batch**: Profile the batch(es) to sample compute characteristics. profile_batch must be a non-negative integer or a tuple of integers. A pair of positive integers signify a range of batches to profile. By default, it will profile the second batch. Set profile_batch=0 to disable profiling.\nembeddings_freq: frequency (in epochs) at which embedding layers will be visualized. If set to 0, embeddings won't be visualized.\n\n**embeddings_metadata**: a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.\n\n\n\n# to check the graps use : \ntensorboard --log dir='log\/'","914c0bac":"# 1.Base Callback\n\n> tf.keras.callbacks.Callback()\n\nAbstract base class used to build new callbacks.\n\nAttributes\n\n**params**: Dict. Training parameters (eg. verbosity, batch size, number of epochs...).\n**model**: Instance of keras.models.Model. Reference of the model being trained.\n\n\n\nyou could build your own callbacks using \n[build](https:\/\/keras.io\/guides\/writing_your_own_callbacks\/)","14a6f9de":"# 3.TensorBoard\n\nTensorBoard is a visualization tool provided with TensorFlow.\n\nThis callback logs events for TensorBoard, including:\n\n* Metrics summary plots\n* Training graph visualization\n* Activation histograms\n* Sampled profiling\n\n>tf.keras.callbacks.TensorBoard(\n    log_dir=\"logs\",\n    histogram_freq=0,\n    write_graph=True,\n    write_images=False,\n    update_freq=\"epoch\",\n    profile_batch=2,\n    embeddings_freq=0,\n    embeddings_metadata=None,\n    **kwargs\n)\n\n\n\n","a35e3b44":"**monitor**: quantity to be monitored.\n\n**factor**: factor by which the learning rate will be reduced. new_lr = lr * factor\n.\n**patience**: number of epochs with no improvement after which learning rate will be reduced.\n\n**verbose**: int. 0: quiet, 1: update messages.\n\n**mode**: one of {'auto', 'min', 'max'}. In 'min' mode, the learning rate will be reduced when the quantity monitored has stopped decreasing; in 'max' mode it will be reduced when the quantity monitored has stopped increasing; in 'auto' mode, the direction is automatically inferred from the name of the monitored quantity.\n\n**min_delta**: threshold for measuring the new optimum, to only focus on significant changes.\n\n**cooldown**: number of epochs to wait before resuming normal operation after lr has been reduced.\n\n**min_lr**: lower bound on the learning rate.","264c0c28":"# 2.LearningRateScheduler\n\nLearning rate scheduler.\n\nAt the beginning of every epoch, this callback gets the updated learning rate value\n\n> tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)\n\n\nArguments\n\nschedule: a function that takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).\n\nverbose: int. 0: quiet, 1: update messages.\u0627","eff36f1a":"# 4.Earlystopping\n\ncheck it frome here\n\n[Earlystopping](http:\/\/www.kaggle.com\/mahmoudreda55\/dropout-earlystopping)","3c92efa0":"# 3.ModelCheckpoint\n\nModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.\n\nA few options this callback provides include:\n\nWhether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance.\nDefinition of 'best'; which quantity to monitor and whether it should be maximized or minimized.\nThe frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches.\nWhether only weights are saved, or the whole model is saved.\n\n\n\n> tf.keras.callbacks.ModelCheckpoint(\n    filepath,\n    monitor=\"val_loss\",\n    verbose=0,\n    save_best_only=False,\n    save_weights_only=False,\n    mode=\"auto\",\n    save_freq=\"epoch\",\n    options=None,\n    **kwargs\n)\n\n"}}