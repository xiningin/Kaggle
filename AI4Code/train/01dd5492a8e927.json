{"cell_type":{"f377baa0":"code","c098d820":"code","b1deb6b2":"code","e0567428":"code","91d7dbfa":"code","8c07bf94":"code","ae194955":"code","f8148932":"code","6f8ef3b4":"code","7d28623d":"code","e221b223":"code","86a0090c":"code","d7349d7b":"code","0c548001":"code","513f6856":"code","7f0fa3a3":"code","dda8c72f":"code","6b530d5c":"code","0f4fb609":"code","359ad664":"code","b4903a6c":"code","4e341708":"code","ab1dfb33":"code","4c82ba73":"code","9281059f":"code","eee100ac":"code","8fcc0997":"code","b251c626":"code","e9eed416":"code","4aafbc08":"code","90a5c49c":"code","56dc76ee":"code","1f7c0c3b":"code","9b7274bb":"code","9b405d82":"code","c50bd025":"code","37c30b66":"code","da7783ea":"code","613db240":"code","5ad273c6":"code","902d71e1":"code","6b499bcc":"code","9e216354":"code","0abc77b9":"code","250fea6d":"code","1a3db08e":"code","98f99d48":"code","2232b0fd":"code","04e627d5":"code","aacafec4":"code","c1bdab27":"code","32ca1952":"code","eaf3960b":"code","622c5d6b":"code","3b961615":"code","1bd11539":"code","0ae14630":"code","9d12344c":"markdown","772f1676":"markdown","b408c7a1":"markdown","ed03b145":"markdown","c8cfdc00":"markdown","75e2f986":"markdown","2d3eccca":"markdown","60fa09cb":"markdown","a94d3522":"markdown","8e3ff9d3":"markdown","5979f706":"markdown","705c3687":"markdown","25378f4e":"markdown","e209435e":"markdown","49a0207d":"markdown","ac38bba9":"markdown","c58273ae":"markdown","abc21ad9":"markdown","07220b62":"markdown","0d32bdc7":"markdown","29bec4ff":"markdown","d1282d7d":"markdown","8b6cd1bb":"markdown","79e601eb":"markdown","cc4bbb45":"markdown","43ef52fb":"markdown","6470b127":"markdown","085035dd":"markdown","c920f8e0":"markdown","e308f71d":"markdown","2cedc549":"markdown","894e75ab":"markdown","0d115472":"markdown","a5ccde4b":"markdown","f199a5f4":"markdown","022b7aac":"markdown","38b24f1e":"markdown","ac8a7aeb":"markdown","5264977b":"markdown","9f0add6c":"markdown","9c57d100":"markdown","4762fc05":"markdown","cacdbd1b":"markdown"},"source":{"f377baa0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c098d820":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nfrom numpy import mean\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 46","b1deb6b2":"data = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","e0567428":"len(data)","91d7dbfa":"data.head()","8c07bf94":"data.tail()","ae194955":"df = data.copy()","f8148932":"df['Exited'].value_counts()","6f8ef3b4":"print('Class imbalance percentage: {}%'.format(len(df[df['Exited']==1])\/(len(df[df['Exited']==0]))*100))","7d28623d":"print(df.columns)","e221b223":"df.isnull().sum()","86a0090c":"plt.figure(figsize=(12,12))\nchurn_corr = df.corr()\nmatrix = np.triu(churn_corr)\nchurn_corr_top = churn_corr.index\nsns.heatmap(df[churn_corr_top].corr(), annot=True, vmin=1.0, vmax=-1.0, mask=matrix, cmap=\"cool\", alpha=0.55)","d7349d7b":"plt.figure(figsize=(15, 3))\nsns.countplot(y=df['Exited'], data=df, palette=\"cool\", alpha=0.55, saturation=1)","0c548001":"plt.figure(figsize=(15,5))\nsns.distplot(x=data[\"CreditScore\"], color=\"violet\", axlabel=\"Credit Scores\")\nplt.show()","513f6856":"df['CreditScoreBand'] = pd.cut(df['CreditScore'], 5)\ndf[['CreditScoreBand', 'Exited']].groupby(['CreditScoreBand'], as_index=False).mean().sort_values(by='CreditScoreBand', ascending=True)","7f0fa3a3":"df_ = [df] #making a list so that 'for loops' can be easily implemented","dda8c72f":"for dataset in df_:    \n    dataset.loc[(dataset['CreditScore'] > 349.5) & (dataset['CreditScore'] <= 450.0), 'CreditScore'] = 0\n    dataset.loc[(dataset['CreditScore'] > 450.0) & (dataset['CreditScore'] <= 550.0), 'CreditScore'] = 1\n    dataset.loc[(dataset['CreditScore'] > 550.0) & (dataset['CreditScore'] <= 650.0), 'CreditScore'] = 2\n    dataset.loc[(dataset['CreditScore'] > 650.0) & (dataset['CreditScore'] <= 750.0), 'CreditScore'] = 3\n    dataset.loc[(dataset['CreditScore'] > 750.0) & (dataset['CreditScore'] <= 850.0), 'CreditScore'] = 4\ndf.head()","6b530d5c":"df = df.drop(['CreditScoreBand'], axis=1)\ndf_ = [df]","0f4fb609":"fig, ax = plt.subplots(1, 2)\nsns.distplot(x=data[\"CreditScore\"], color=\"violet\", axlabel=\"Credit Scores\", ax=ax[0])\nsns.distplot(x=df[\"CreditScore\"], color=\"violet\", axlabel=\"Credit Scores\", ax=ax[1])\nfig.set_figheight(5)\nfig.set_figwidth(15)\nplt.show()","359ad664":"plt.figure(figsize=(15, 5))\nsns.countplot(y=\"Geography\", hue=\"Exited\", data=data, palette=\"cool\", alpha=0.55)\nplt.show()","b4903a6c":"for dataset in df_:\n    dataset['Geography'] = dataset['Geography'].map( {'France': 0, 'Germany': 1, 'Spain': 2} ).astype(int)\n\ndf.head()","4e341708":"plt.figure(figsize=(15,3))\nsns.countplot(y=df['Gender'], orient=\"v\", data=df, palette=\"cool\", alpha=0.55)\nplt.show()","ab1dfb33":"plt.figure(figsize=(15,5))\nsns.countplot(y=\"Exited\", hue=\"Gender\", data=data, orient=\"v\", palette=\"cool\", alpha=0.55)\nplt.show()","4c82ba73":"for dataset in df_:\n    dataset['Gender'] = dataset['Gender'].map( {'Female': 1, 'Male': 0} ).astype(int)\ndf.head()","9281059f":"plt.figure(figsize=(12,8))\nsns.distplot(x=data[\"Age\"], bins=30, axlabel=\"Age\", color='violet')\nplt.show()","eee100ac":"df['AgeBand'] = pd.cut(df['Age'], 5)\ndf[['AgeBand', 'Exited']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","8fcc0997":"for dataset in df_:    \n    dataset.loc[(dataset['Age'] > 17.926) & (dataset['Age'] <= 32.8), 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 32.8) & (dataset['Age'] <= 47.6), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 47.6) & (dataset['Age'] <= 62.4), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 62.4) & (dataset['Age'] <= 77.2), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 77.2, 'Age'] = 4\ndf.head()","b251c626":"df = df.drop(['AgeBand'], axis=1)\ndf.head()","e9eed416":"df['Tenure'].nunique()","4aafbc08":"df['Tenure'].unique()","90a5c49c":"plt.figure(figsize=(12, 8))\nsns.distplot(x=data[\"Balance\"], axlabel=\"Bank Balance\", color=\"violet\")\nplt.show()","56dc76ee":"df['BalanceBand'] = pd.cut(df['Balance'], 5)\ndf[['BalanceBand', 'Exited']].groupby(['BalanceBand'], as_index=False).mean().sort_values(by='BalanceBand', ascending=True)","1f7c0c3b":"df_ = [df]","9b7274bb":"for dataset in df_:    \n    dataset.loc[dataset['Balance'] <= 50179.618, 'Balance'] = 0\n    dataset.loc[(dataset['Balance'] > 50179.618) & (dataset['Balance'] <= 100359.236), 'Balance'] = 1\n    dataset.loc[(dataset['Balance'] > 100359.236) & (dataset['Balance'] <= 150538.854), 'Balance'] = 2\n    dataset.loc[(dataset['Balance'] > 150538.854) & (dataset['Balance'] <= 200718.472), 'Balance'] = 3\n    dataset.loc[(dataset['Balance'] > 200718.472) & (dataset['Balance'] <=  250898.09), 'Balance'] = 4\n    df['Balance'] = df['Balance'].astype(int)\ndf.head()","9b405d82":"df = df.drop(['BalanceBand'], axis=1)\ndf_ = [df]","c50bd025":"plt.figure(figsize=(12, 5))\nsns.distplot(x=data[\"EstimatedSalary\"], color=\"violet\")\nplt.show()","37c30b66":"df['EstimatedSalaryBand'] = pd.cut(df['EstimatedSalary'], 4)\ndf[['EstimatedSalaryBand', 'Exited']].groupby(['EstimatedSalaryBand'], as_index=False).mean().sort_values(by='EstimatedSalaryBand', ascending=True)","da7783ea":"for dataset in df_:    \n    dataset.loc[(dataset['EstimatedSalary'] > -188.401) & (dataset['EstimatedSalary'] <= 50006.805), 'EstimatedSalary'] = 0\n    dataset.loc[(dataset['EstimatedSalary'] > 50006.805) & (dataset['EstimatedSalary'] <= 100002.03), 'EstimatedSalary'] = 1\n    dataset.loc[(dataset['EstimatedSalary'] > 100002.03) & (dataset['EstimatedSalary'] <= 149997.255), 'EstimatedSalary'] = 2\n    dataset.loc[(dataset['EstimatedSalary'] > 149997.255) & (dataset['EstimatedSalary'] <= 199992.48), 'EstimatedSalary'] = 3\n    df['EstimatedSalary'] = df['EstimatedSalary'].astype(int)\ndf.head()","613db240":"df = df.drop(['EstimatedSalaryBand'], axis=1)\ndf.head()","5ad273c6":"df = df.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)\ndf.head()","902d71e1":"X_train, X_test, y_train, y_test = train_test_split(df, data['Exited'], test_size=0.15, random_state=seed)","6b499bcc":"LR_model = LogisticRegression(random_state=seed)\nLR_model.fit(X_train, y_train)","9e216354":"y_pred = LR_model.predict(X_test)","0abc77b9":"fig, (ax1, ax2) = plt.subplots(ncols=2)\nax1.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\nfig.set_figheight(5)\nfig.set_figwidth(12)\nax1.set_title('ROC Curve')\nax2.set_title('Precision Recall Curve')\nplot_roc_curve(LR_model, X_test, y_test, ax=ax1)\nplot_precision_recall_curve(LR_model, X_test, y_test, ax=ax2)","250fea6d":"print(classification_report(y_test, y_pred))","1a3db08e":"XGB_model = XGBClassifier(tree_method='gpu_hist')\nXGB_model.fit(X_train, y_train)\nprint('Your model is trained!')","98f99d48":"y_pred1 = XGB_model.predict(X_test)\nfig, (ax1, ax2) = plt.subplots(ncols=2)\nax1.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\nfig.set_figheight(5)\nfig.set_figwidth(12)\nax1.set_title('ROC Curve')\nax2.set_title('Precision Recall Curve')\nplot_roc_curve(XGB_model, X_test, y_test, ax=ax1)\nplot_precision_recall_curve(XGB_model, X_test, y_test, ax=ax2)","2232b0fd":"print(classification_report(y_test, y_pred1))","04e627d5":"MAX_TREE_DEPTH = [4]#complex>>>\nTREE_METHOD = ['gpu_hist']\nSUBSAMPLE = [0.7]\nREGULARIZATION = [0.1, 0.01]\nGAMMA = [0.2, 0.3]\nPOS_WEIGHT = [0.26]\nOBJ = ['binary:logistic']\nEVAL = ['auc']\nETA = [0.11]\n\nparam_grid = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': ETA, \n          'objective': OBJ, 'eval_metric': EVAL}","aacafec4":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)","c1bdab27":"%%time\ngrid = GridSearchCV(estimator=XGB_model, param_grid=param_grid, scoring='roc_auc', cv=cv, n_jobs=-1)\ngrid_fit = grid.fit(X_train,y_train)","32ca1952":"best_score = grid_fit.best_score_\nbest_params = grid_fit.best_params_\n\nprint('Best Score: {:.2f} % with these parameters: {})'.format(best_score*100, best_params))","eaf3960b":"#Passing our final model with the test data\ny_pred2 = grid_fit.predict(X_test)","622c5d6b":"fig, (ax1, ax2) = plt.subplots(ncols=2)\nax1.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\nfig.set_figheight(5)\nfig.set_figwidth(12)\nax1.set_title('ROC Curve')\nax2.set_title('Precision Recall Curve')\nplot_roc_curve(grid_fit, X_test, y_test, ax=ax1)\nplot_precision_recall_curve(grid_fit, X_test, y_test, ax=ax2)\nprint('Classification Report: \\n', classification_report(y_test, y_pred2))","3b961615":"fig, (ax1, ax2) = plt.subplots(ncols=2)\nfig.set_figheight(7.5)\nfig.set_figwidth(18)\nfig.suptitle('ROC Curve for Different models')\n\n#ROC curves for Training set\nax1.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.6)\nax1.set_title(\"Train Data Results\")\nplot_roc_curve(LR_model, X_train, y_train, ax=ax1)\nplot_roc_curve(XGB_model, X_train, y_train, ax=ax1)\nplot_roc_curve(grid_fit, X_train, y_train, ax=ax1)\n\n#ROC curves for Testing set\nax2.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.6)\nax2.set_title(\"Test Data Results\")\nplot_roc_curve(LR_model, X_test, y_test, ax=ax2)\nplot_roc_curve(XGB_model, X_test, y_test, ax=ax2)\nplot_roc_curve(grid_fit, X_test, y_test, ax=ax2)","1bd11539":"fig, (ax1, ax2) = plt.subplots(ncols=2)\nfig.set_figheight(7.5)\nfig.set_figwidth(18)\nfig.suptitle('Precision Recall Curve for Different models')\n\n#ROC curves for Training set\nax1.set_title(\"Train Data Results\")\nplot_precision_recall_curve(LR_model, X_train, y_train, ax=ax1)\nplot_precision_recall_curve(XGB_model, X_train, y_train, ax=ax1)\nplot_precision_recall_curve(grid_fit, X_train, y_train, ax=ax1)\n\n#ROC curves for Testing set\nax2.set_title(\"Test Data Results\")\nplot_precision_recall_curve(LR_model, X_test, y_test, ax=ax2)\nplot_precision_recall_curve(XGB_model, X_test, y_test, ax=ax2)\nplot_precision_recall_curve(grid_fit, X_test, y_test, ax=ax2)","0ae14630":"\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3)\nfig.set_figheight(5)\nfig.set_figwidth(20)\nfig.suptitle('Confusion Matrices for Different models')\n\nax1.set_title('Logistic Regression')\nLR_conf = confusion_matrix(y_test, y_pred)\nsns.heatmap(LR_conf, annot=True, fmt='d', ax=ax1, linewidths=3, linecolor='white')\n\nax2.set_title('XG-Boost Model')\nXG_conf_1 = confusion_matrix(y_test, y_pred1)\nsns.heatmap(XG_conf_1, annot=True, fmt='d', ax=ax2, linewidths=3, linecolor='white')\n\nax3.set_title('Hypertuned XG-Boost Model')\nXG_conf_2 = confusion_matrix(y_test, y_pred2)\nsns.heatmap(XG_conf_2, annot=True, fmt='d', ax=ax3, linewidths=3, linecolor='white')\n\nprint('Total no. of testing samples: ', len(y_test))","9d12344c":"# 3. <a id='modeltraining'>Model Training<\/a>\n## 3.1. <a id='lrmodel'>Treating a Logistic Regression model as a base model<\/a>","772f1676":"## 3.3. <a id='hyperparam'>HyperParameter Tuning<\/a>\n#### Determining the best hyperparameters using GridSearchCV","b408c7a1":"### `Credit Scores`:\n#### Distribution plot for credit scores:","ed03b145":">#### Now, let's drop the CreditScoreBand column which has now done its purpose.","c8cfdc00":"#### Heatmap for our data:","75e2f986":"> #### The class 0 has around 6000 more values than the class 1 making this dataset skewed and the classes imbalanced.","2d3eccca":"#### Here, we can conduct One Hot Encoding and replace the categories: France, Germany and Spain into numbers. But, I went on a more raw approach.","60fa09cb":"## 2.2. <a id='datacleaning'>Data Cleaning<\/a>\n### Let's first see if the target classes have equal number of entries or not.","a94d3522":"#### The above two plots give the ROC AUC curve for the training set and for the testing set. It can be observed that the curve for training set is more smooth as compared to the curves for the testing sets.\n#### For the XG-Boost model trained **without** cross validation and hyperparameter tuning, the training AUC score is **0.95** but while testing it falls to **0.81**, suggesting that the model must have overfit to the data. \n#### For the XG-Boost model which used cross validation during training and was hyper tuned, the training score was **0.88** and the testing score was **0.84**, suggesting that the model did not overfit to the training data and hence the testing score was the highest among all three models.\n#### Overall, the Hypertuned XG-Boost model performs the best among other models on testing set.","8e3ff9d3":"#### An excerpt from [this](http:\/\/acutecaretesting.org\/en\/articles\/precision-recall-curves-what-are-they-and-how-are-they-used) article referenced:  \n>#### The example presented above clearly shows that an imbalanced data set with a large fraction of persons without disease in the test population will make a ROC curve look better than it would in a balanced data set with fewer persons without disease.   \n\n#### This suggests that the ROC curves automatically shows better performance when the dataset is imbalanced as compared to when the dataset in not imbalanced. Hence, relying just on ROCs would not be good for understanding the model performance.  \n#### The above two figures show Precision Recall Curves for training and testing datasets on different models. The curve above the other curve has a better performance level.  \n#### First figure shows the XGBClassifier model as performing the best but it is clearly overfitting to the data as seen in the second figure where the GridSearchCV i.e. the hypertuned and cross-validated model comes on top.","5979f706":">To create a churn model I have selected the `churn-modelling` dataset. The dataset consists of various information on the customers and a target column called `'Exited'` that tells whether they have closed their bank accounts. Let's start!","705c3687":"> #### Features: CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary\n\n> #### Target: Exited","25378f4e":"### `Tenure`:\n>#### Tenure has 11 unique values that are just number of years from 0 to 10. Hence, I kept it as it is.","e209435e":"# Referenced:\n[1] https:\/\/machinelearningmastery.com\/xgboost-for-imbalanced-classification\/  \n[2] https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html?highlight=learning%20rate#learning-task-parameters  \n[3] https:\/\/acutecaretesting.org\/en\/articles\/precision-recall-curves-what-are-they-and-how-are-they-used","49a0207d":"<a id=\"imb\"><\/a>","ac38bba9":"### Thus, we can say after looking at the ROC and PRC charts that the performance improved substantially when XG-Boost was paired with fine tuning and cross validation. \n### Though the overall performance was not that great we can still improve it by using more advanced methods such as synthetic data creation to remove the imbalance in the dataset and see if the performance improves further.","c58273ae":">#### Here what I have done is that I have replaced each Credit score band with a value (from 0 to 4) so that now only these values will be displayed instead of the actual credit scores in the table. ","abc21ad9":">#### No missing values found so we move on to the next step.","07220b62":"### `Bank Balance`:\n#### Distribution plot for customer bank balance:","0d32bdc7":"## 2.1. <a id='datareading'>Data Reading<\/a>","29bec4ff":">#### Removing other columns with very little to no information. ","d1282d7d":"# 1. <a id='imports'>Imports<\/a>\n### Let us start by importing our usual libraries.","8b6cd1bb":">#### Here I have divided the Credit score column in 5 parts by using `pd.cut()`. The table below shows the 5 bands and the exiting value mean for those bands.","79e601eb":"### `Geography`\n#### Plot for Geography with Exited:","cc4bbb45":"# 4. <a id=\"results\">Results<\/a>\n### Thus, I have successfully trained the XG Boost model using cross validation and finally fine tuned the hyperparameters, with the best scores as shown below.","43ef52fb":"#### Here, we have a modified table with all the values converted into a format that our model can understand. Now, let's get it prepared to pass it into the model.","6470b127":"### Introducing Cross Validation:","085035dd":">`scale_pos_weight`: Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) \/ sum(positive instances) ==> already calculated over<a href=\"#imb\"> here.<\/a>  \n\n>`alpha`: L1 regularization term on weights. Increasing this value will make model more conservative.  \n\n>`max_depth`: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.  \n\n>`gamma`: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger `gamma` is, the more conservative the algorithm will be. \n\n>`subsample`: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \n\n>`learning_rate`: Step size shrinkage used in update to prevent overfitting. \n\n>`tree_method`: Basically to train XGBoost on cpu or gpu. Read more over in the <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/gpu\/index.html?highlight=tree_method\">documentation.<\/a>\n\n>`eval_metric`: Evaluation metrics for validation data, a default metric will be assigned according to objective. \n\n>`objective: 'binary:hinge'`: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. *I tried using this but the score went below 72%.*  \n\n>`objective: 'binary:logistic'`: logistic regression for binary classification, output probability.\n\n","c920f8e0":"# **BANK CUSTOMER CHURN PREDICTION**\n![Photo by <a href=\"https:\/\/unsplash.com\/@clemono?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Clem Onojeghuo<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/churn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n  ](https:\/\/images.unsplash.com\/photo-1465125159090-b104362936e1?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1190&q=80)\n> Image credits: Clem Onojeghuo from Unsplash\n\nChurn prediction models have become a very important asset in determining the customer behaviour for a company. The churn rate drives decision making and makes the company analyse itself and the way they provide its services to the customer. Churn prediction consists of detecting which customers are likely to cancel a subscription to a service based on how they use the service. As it is more costlier to get new customers than to keep the old ones, Churn models have become very necessary.\n<br\/>\nFor a bank, predicting the churn rate is to determine which customers are closing their bank accounts and what factors are leading to their decisions. A Churn Prediction Model might be useful in this case to predict way before a person might close their account. The factors leading to customer churn are also very important to analyse closely and make important decisions in changing or modifying the current workings of the company or a service provider.","e308f71d":">#### Creating a copy so that we can revert back to the original dataset whenever necessary.","2cedc549":"## 3.2. <a id='xgboost'>Training on XG-Boost<\/a>\n>#### Calling the XG-Boost model object and setting `tree_method='gpu_hist'` to be able to train the model on the GPU which is much faster than training it normally on CPU.","894e75ab":">#### The above parameters are what I found to give the greatest score (by trial and error).","0d115472":"### `Age`:\n#### Distribution plot for age of the customers:","a5ccde4b":"## 2.3. <a id='datawrangling'>Data Wrangling with EDA<\/a>\n### Here, we will conduct some Exploratory Data Analysis to determine which features are necessary and which are just useless. We will also convert categorical features into numerical ones to make it easier for the model to understand our data.","f199a5f4":"### Other parameters which I tried:\n- Best Score: **85.40%** with these parameters: {'alpha': 0.01, 'eval_metric': 'auc', 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'binary:logistic', 'scale_pos_weight': 0.26, 'subsample': 0.5, 'tree_method': 'gpu_hist'}), \n- Best Score: **85.43%** with these parameters: {'alpha': 0.01, 'eval_metric': 'auc', 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 4, 'objective': 'binary:logistic', 'scale_pos_weight': 0.26, 'subsample': 0.6, 'tree_method': 'gpu_hist'}),\n- Best Score: **85.44%** with these parameters: {'alpha': 0.01, 'eval_metric': 'auc', 'gamma': 0.2, 'learning_rate': 0.11, 'max_depth': 4, 'objective': 'binary:logistic', 'scale_pos_weight': 0.26, 'subsample': 0.7, 'tree_method': 'gpu_hist'}),  \n- Best Score: **85.44%** with these parameters: {'alpha': 0.1, 'eval_metric': 'auc', 'gamma': 0.3, 'learning_rate': 0.11, 'max_depth': 4, 'objective': 'binary:logistic', 'scale_pos_weight': 0.26, 'subsample': 0.7, 'tree_method': 'gpu_hist'})\n","022b7aac":"# Contents:\n1. <a href='#imports'>Imports<\/a>\n2. <a href='#dataanalysis'>Data Analysis<\/a>\n   1. <a href='#datareading'>Data Reading<\/a>\n   2. <a href='#datacleaning'>Data Cleaning<\/a>\n   3. <a href='#datawrangling'>Data Wrangling with EDA<\/a>\n3. <a href='#modeltraining'>Model Training<\/a>\n   1. <a href='#lrmodel'>Training a Logistic Regression model as a base model<\/a>\n   2. <a href='#xgboost'>Training on XG-Boost<\/a>\n   3. <a href='#hyperparam'>Hyperparameter tuning for XG-Boost<\/a>\n4. <a href='#results'>Results<\/a>","38b24f1e":"### `Estimated Salary`:\n#### Distribution plot for customer salary:","ac8a7aeb":"#### Exiting rate based on the gender:","5264977b":"#### Thus, we have replaced the continuous distribution of the Credit scores into 5 discrete values. Similarly we will conduct such modifications on all the continuous features.","9f0add6c":"### `Gender`:\n#### Plot for Gender:","9c57d100":">### The training process has been completed!!","4762fc05":"#### Class Imbalance shown visually:","cacdbd1b":"# 2. <a id='dataanalysis'>Data Analysis<\/a>\n### We will start by analysing the data. First we will read the dataset and see for any missing values using pandas. Then we will go through the dataset and replace all the values which are not in the right format."}}