{"cell_type":{"9ade5cf5":"code","5e570a19":"code","a914f969":"code","a9c15c1c":"code","52f2a31a":"code","9435d883":"code","7f364215":"code","161fa47b":"code","1aa470e3":"code","5afd53e8":"code","8e9d249c":"code","38530c12":"code","b595f5bf":"code","16dff96f":"code","6d4054a0":"code","523fa445":"code","a2afc870":"code","8987ba43":"code","43f03c23":"code","e57c8883":"code","0c54f336":"code","7b27f552":"code","15c67098":"code","3e9ca240":"code","358542b8":"code","b06ad19a":"code","85c501bb":"code","50fccbab":"code","f9529c01":"code","9f0991ac":"code","8da308b1":"code","88054cc7":"code","af50b488":"code","5b0a5ef1":"code","3f1eadef":"code","cba4b4a0":"code","c899f02d":"code","1c773284":"code","f040b857":"code","b2cf463d":"code","9cba1f5d":"code","da172912":"code","d5316525":"code","606b3a4c":"markdown","0f8aea8c":"markdown","fdbe14ff":"markdown","7992c51f":"markdown","cb6c83ea":"markdown","f6751a91":"markdown","847591e5":"markdown","dc659dbb":"markdown","fb1d1fb6":"markdown","5d4c3cd5":"markdown","589b8440":"markdown","bc08a3ca":"markdown","c20ac635":"markdown","d7df0256":"markdown","dd1b3e5a":"markdown","16f19858":"markdown","39f8fe0b":"markdown","7e3c6ee7":"markdown","abbf69ee":"markdown","cf22c901":"markdown","6bf5d8ef":"markdown","cdd55d3b":"markdown","f960fcaa":"markdown","a717c008":"markdown","8c65f1fc":"markdown","c6d58e81":"markdown","cc4c9788":"markdown","b55f659f":"markdown","0a1c0c4e":"markdown","ba525038":"markdown","ea101c19":"markdown","13038079":"markdown","e482e11f":"markdown","6693ec58":"markdown","58140f36":"markdown","584cee0c":"markdown","e0e1174e":"markdown","eaef792e":"markdown","a08dbc25":"markdown","4f5b393d":"markdown","96cb88be":"markdown","ab6083be":"markdown","274f195e":"markdown","2522da4e":"markdown"},"source":{"9ade5cf5":"# Basic\nimport numpy as np \nimport pandas as pd \n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Evaluation metrics\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score","5e570a19":"dataset = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv', index_col = False)\n\ndataset.head()","a914f969":"# Checking for missing values\n\ndataset.isna().sum()","a9c15c1c":"dataset.describe()","52f2a31a":"dataset.info()","9435d883":"cols = dataset.select_dtypes(include=['object']).columns\n\nfor col in cols:\n    print('----'+str(col).upper()+'----')\n    print(pd.crosstab(index = dataset[col], columns = dataset['status']))\n    print()\n","7f364215":"sns.countplot(x = 'status', data = dataset, hue = 'gender')\nplt.title('Gender vs Campus Placement')\nplt.xlabel('Status')\nplt.ylabel('Number of students')\nplt.show()","161fa47b":"sns.countplot(x='gender', data = dataset)\nplt.title('Gender')\nplt.xlabel('Gender')\nplt.ylabel('Number of students')\nplt.show()","1aa470e3":"# Secondary percentage vs campus placement\nax = sns.violinplot(x = 'status', y = 'ssc_p', data = dataset)\n\nmedians = dataset.groupby(['status'])['ssc_p'].median().values\nnobs = dataset['status'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n: ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Secondary Education percentage vs Campus Placement')\nplt.show()","5afd53e8":"sns.countplot(x='ssc_b', data = dataset)\nplt.title('Senior Secondary Board')\nplt.xlabel('Board')\nplt.ylabel('Number of students')\nplt.show()","8e9d249c":"sns.countplot(x = 'status', data = dataset, hue = 'ssc_b')\nplt.title('Senior Secondary Board vs Campus Placement')\nplt.xlabel('Status')\nplt.ylabel('Number of students')\nplt.show()","38530c12":"# Highesr Secondary percentage vs campus placement\nax = sns.violinplot(x = 'status', y = 'hsc_p', data = dataset)\n\nmedians = dataset.groupby(['status'])['hsc_p'].median().values\nnobs = dataset['status'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n: ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Higher Secondary percentage vs Campus Placement')\nplt.show()","b595f5bf":"sns.countplot(x='hsc_b', data = dataset)\nplt.title('Higher Secondary Board')\nplt.xlabel('Board')\nplt.ylabel('Number of students')\nplt.show()","16dff96f":"sns.countplot(x = 'status', data = dataset, hue = 'hsc_b')\nplt.title('Higher Secondary Board vs Campus Placement')\nplt.xlabel('Status')\nplt.ylabel('Number of students')\nplt.show()","6d4054a0":"sns.countplot(x='hsc_s', data = dataset)\nplt.title('Subject in HSC Board')\nplt.xlabel('Subjects')\nplt.ylabel('Number of students')\nplt.show()","523fa445":"sns.countplot(x = 'hsc_s', data = dataset, hue = 'status')\nplt.title('Subject in HSC vs Campus Placement')\nplt.xlabel('hsc_s')\nplt.ylabel('Number of students')\nplt.show()","a2afc870":"# Degree percentage vs campus placement\nax = sns.violinplot(x = 'status', y = 'degree_p', data = dataset)\n\nmedians = dataset.groupby(['status'])['degree_p'].median().values\nnobs = dataset['status'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n: ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Degree percentage vs Campus Placement')\nplt.show()","8987ba43":"sns.countplot(x='degree_t', data = dataset)\nplt.title('Graduation Degree')\nplt.xlabel('Graduation Degree')\nplt.ylabel('Number of students')\nplt.show()","43f03c23":"sns.countplot(x = 'degree_t', data = dataset, hue = 'status')\nplt.title('Graduation Degree vs Campus Placement')\nplt.xlabel('Graduation Degree')\nplt.ylabel('Number of students')\nplt.show()","e57c8883":"sns.countplot(x='workex', data = dataset)\nplt.title('Work Experience')\nplt.xlabel('Work Experience')\nplt.ylabel('Number of students')\nplt.show()","0c54f336":"# Work experience vs candidate placement\nsns.countplot(x = 'workex', data = dataset, hue = 'status')\nplt.title('Work Experience vs Campus Placement')\nplt.xlabel('Work Experience')\nplt.ylabel('Number of students')\nplt.show()","7b27f552":"# Employability test percentage vs campus placement\nax = sns.violinplot(x = 'status', y = 'etest_p', data = dataset)\n\nmedians = dataset.groupby(['status'])['etest_p'].median().values\nnobs = dataset['status'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n: ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Employability test percentage vs Campus Placement')\nplt.show()","15c67098":"sns.countplot(x='specialisation', data = dataset)\nplt.title('MBA Specialisation')\nplt.xlabel('MBA Specialisation')\nplt.ylabel('Number of students')\nplt.show()","3e9ca240":"# Degree specialization vs candidate placement\nsns.countplot(x = 'specialisation', data = dataset, hue = 'status')\nplt.title('Degree Specialisation vs Campus Placement')\nplt.xlabel('Specialisation')\nplt.ylabel('Number of students')\nplt.show()","358542b8":"# MBA percentage vs campus placement\nax = sns.violinplot(x = 'status', y = 'mba_p', data = dataset)\n\nmedians = dataset.groupby(['status'])['mba_p'].median().values\nnobs = dataset['status'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n: ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('MBA percentage vs Campus Placement')\nplt.show()","b06ad19a":"# Dropping the serial number column\ndataset.drop('sl_no', axis=1, inplace = True)\ndataset.head(3)","85c501bb":"# Dropping ssc_b and hsc_b\ndataset.drop('ssc_b', axis=1, inplace = True)\ndataset.drop('hsc_b', axis=1, inplace = True)\n\nprint(dataset.shape)\ndataset.head(3)\n","50fccbab":"# Gender: F coded as 0 and M as 1\ndummy_variable_1 = pd.get_dummies(dataset['gender'])\ndummy_variable_1.rename(columns={'M':'Gender'}, inplace=True)\n\n# drop original column \ndataset.drop(\"gender\", axis = 1, inplace=True)\n\n# merge data frame \"dataset\" and \"dummy_variable_1: Gender column\" \ndf = pd.concat([dummy_variable_1['Gender'], dataset], axis=1)\n\ndf.head(1)","f9529c01":"# Higher Secondary Specialisation: Science: 10 and Commerce: 01 and Arts: 00\ndummy = pd.get_dummies(df['hsc_s'])\ndummy.rename(columns={'Science': 'HS_Sci', 'Commerce': 'HS_Comm'}, inplace=True)\ndummy = pd.concat([dummy['HS_Sci'], dummy['HS_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('hsc_s', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:3], dummy, df.iloc[:, 3:]], axis=1)\n\ndf.head(1)","9f0991ac":"# Undergrad specialisation: Sci&Tech: 10 and Comm&Mgmt: 01 and Others: 00\ndummy = pd.get_dummies(df['degree_t'])\ndummy.rename(columns={'Sci&Tech': 'UG_Sci', 'Comm&Mgmt': 'UG_Comm'}, inplace=True)\ndummy = pd.concat([dummy['UG_Sci'], dummy['UG_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('degree_t', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:6], dummy, df.iloc[:, 6:]], axis=1)\n\ndf.head()","8da308b1":"# Work experience: Yes as 1 nd No as 0\ndummy = pd.get_dummies(df['workex'])\ndummy.rename(columns={'Yes': 'workex'}, inplace=True)\n# dummy.head()\n\n# drop original\ndf.drop('workex', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:8], dummy['workex'], df.iloc[:, 8:]], axis=1)\n\ndf.head(1)","88054cc7":"# Specialisation: Mkt&Fin as 1 and Mkt&HR as 0\ndummy = pd.get_dummies(df['specialisation'])\ndummy.rename(columns={'Mkt&Fin': 'specialisation'}, inplace=True)\n# dummy.head()\n\n# drop original data\ndf.drop('specialisation', axis=1, inplace=True)\n\n# merge data\ndf= pd.concat([df.iloc[:, 0:10], dummy['specialisation'], df.iloc[:, 10:]], axis=1)\n\ndf.head(2)","af50b488":"# Change Placed into dummy variables: Placed as 1 and Not Placed as 0\ndummy = pd.get_dummies(df['status'])\ndummy.rename(columns={'Placed': 'status'}, inplace=True)\n# dummy.head()\n\n# drop original\ndf.drop('status', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:12], dummy['status'], df.iloc[:, 12:]], axis=1)\n\ndf.head()","5b0a5ef1":"# spliting the dataset to get the independent and dependent variables separately\nX = df.iloc[:, :-2].values\ny = df.iloc[:, -2].values\n\nprint('X_shape {}'.format(X.shape))\nprint('y_shape {}'.format(y.shape))","3f1eadef":"# Splitting\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint('Shape of training set: {} and test set: {}'.format(X_train.shape, X_test.shape))","cba4b4a0":"# Feature Scaling\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nprint('Scaled Successfully')","c899f02d":"classifiers = [LogisticRegression(), KNeighborsClassifier(n_neighbors = 5, metric='minkowski', p=2), SVC(kernel = 'linear'), SVC(kernel = 'rbf'), DecisionTreeClassifier(criterion='entropy'), RandomForestClassifier(n_estimators = 10, criterion = 'entropy')]\n\nfor classifier in classifiers:\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    # print classifier name\n    print(str(type(classifier)).split('.')[-1][:-2])\n    \n    # Accuracy Score\n    print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred)))\n    \n    # jaccard Score\n    print('Jaccard Score: {}'.format(jaccard_score(y_test, y_pred)))\n    \n    # F1 score\n    print('F1 Score: {}'.format(f1_score(y_test, y_pred)))\n    \n    # Log Loss\n    print('Log Loss: {}'.format(log_loss(y_test, y_pred)))\n    \n    # confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, lw = 2, cbar=False)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix: {}'.format(str(type(classifier)).split('.')[-1][:-2]))\n    plt.show()","1c773284":"X = df.iloc[:, :-2].values\ny = df.iloc[:, -2].values\n\n# Finding out the significance of each feature in the dataset \nkb = SelectKBest(chi2, k = 'all')\nX_new = kb.fit_transform(X, y)\n\nprint(kb.pvalues_)","f040b857":"X = df.iloc[:, :-2].values\ny = df.iloc[:, -2].values\n\nacc=[]\nlogloss=[]\nf1=[]\njaccard=[]\n\nfor k in range(1, 13):\n    \n    # Selecting features\n    kb = SelectKBest(chi2, k = k)\n    X_new = kb.fit_transform(X, y)\n    \n    # Split dataset\n    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=0)\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.fit_transform(X_test)\n    \n    #classifier\n    classifier = LogisticRegression(random_state=0)\n    \n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    acc.append(accuracy_score(y_test, y_pred))\n    jaccard.append(jaccard_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    logloss.append(log_loss(y_test,y_pred))\n    \n\n    ","b2cf463d":"plt.figure()\nplt.plot(acc)\nplt.title('Accuracy Score: LR')\n\nplt.figure()\nplt.plot(jaccard)\nplt.title('Jaccard Score: LR')\n\nplt.figure()\nplt.plot(f1)\nplt.title('F1 Score: LR')\n\nplt.figure()\nplt.plot(logloss)\nplt.title('Log Loss Score: LR')","9cba1f5d":"X = df.iloc[:, :-2].values\ny = df.iloc[:, -2].values\n\n# Selecting 7 best features\nkb = SelectKBest(chi2, k = 6)\nX_new = kb.fit_transform(X, y)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=0)\n\n# Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n    \n#classifier\nclassifier = LogisticRegression(random_state=0)\n    \nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n    \nprint('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred)))\nprint('Jaccard Score: {}'.format(jaccard_score(y_test, y_pred)))\nprint('F1 Score: {}'.format(f1_score(y_test, y_pred)))\nprint('Log Loss: {}'.format(log_loss(y_test,y_pred)))","da172912":"accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies","d5316525":"print('Evaluation of our model performance: {}'.format(accuracies.mean()))\nprint('Variance check: {}'.format(accuracies.std()))","606b3a4c":"**We have the unique values of all the categorical variables and can see the distribution of the same with respect to the placement status of the student.**","0f8aea8c":"### 7. degree_p (Percentage in Graduation)","fdbe14ff":"**We will select the LOGISTIC REGRESSION Model as the performance of that model is better than SVC.**","7992c51f":"## Preprocessing\n","cb6c83ea":"OBSERVATION:\n\nStudents with a higher score in graduation are more likely to be placed. Students with scores <60 are much more likely to be un-placed.","f6751a91":"### 4. hsc_p (Higher Secondary Percentage)","847591e5":"### 12. mba_p (MBA percentage)","dc659dbb":"**NOTE: There are two SVC models that are printed above.**\n\n**The first one if a linear kernel SVC and the second one is a Gaussian Kernel SVC.**","fb1d1fb6":"OBSERVATION:\n\n1. Number of males are almost double than the number of females.\n2. The fraction of placed male are much more than the fraction of females. It is significant that the males are placed more often than the females.","5d4c3cd5":"## EDA","589b8440":"## One by one exploration of the columns","bc08a3ca":"### 10. etest_p (Employability test score)","c20ac635":"## Model Construction","d7df0256":"**Dropping ssc_b and hsc_b as it does not have much effect of the status of the student**","dd1b3e5a":"**The models tested: Logistic Regression, k-NN, SVC, Kernel SVC, Decision Tree, and Random Forest.**\n\n**The Logistic Regression and SVC models gave a better result than the other models:**\n\n**1. Higher F1 score**\n\n**2. Lower LogLoss**\n\n**3. Higher Accuracy Score**\n\n**4. Higher Jaccard Score**","16f19858":"**We will select the 6 best features to finalise our LOGISTIC REGRESSION model.**","39f8fe0b":"OBSERVATION:\n\n1. Number of students with no experience were almost double than the ones with some work experience.\n2. The percentage of students with or without work experience varies a lot when it comes to getting placed. The students with a work experience have a higher chance of getting placed.","7e3c6ee7":"### 3. ssc_b (Senior Secondary Board)","abbf69ee":"OBSERVATION:\n\nStudents with a higher HSC percentage are more likely to get placed. Student who scored <60 are most likely to be unplaced.","cf22c901":"**Changing categorical variables into dummy variables**","6bf5d8ef":"### 8. degree_t (Graduation level)","cdd55d3b":"### 9. workex (Work Experience)","f960fcaa":"OBSERVATION:\n\n1. The number of students from each Central and Other Boards sitting for placements is very comparable.\n2. The status of placement from either board is very comparable. The number do not show any specific preference to a certain Board type","a717c008":"OBSERVATION:\n\nStudents with a percentage >60 are more likely to be place and those with <60 are more likely to be unplaced.","8c65f1fc":"### 6. hsc_s (Subject in HSC)","c6d58e81":"### Feature Selection","cc4c9788":"OBSERVATION:\n\nThe percentage scored in MBA does not particularly reflect getting placed. There is a slight dependence with people scoring above 65% in terms of getting placed. \n\nHowever, we can also see that there is a decent number of placed students who got a percentage <60% in MBA.","b55f659f":"### Cross Validation","0a1c0c4e":"### 2. ssc_p (Senior Secondary Percentage)","ba525038":"OBSEVRATION:\n\n1. Majority of the students sitting for placements had opted for Commerce&Mgmt, followed by Science and then Others in their graduation degree.\n2. Majority of the students getting placed had opted for Commerce&Mgmt, followed by Science and then Others in their graduation degree.","ea101c19":"OBSERVATION:\n\nThere is no particular relation between getting a good employability score and getting placed. However, we can see a slight dependence between the two with students who scored >80 on the test. \n\nWe can also see that there is a decent number of students who scored <65 that are not placed yet.","13038079":"**We will not take in the salary to predict whether the student is placed or not.**","e482e11f":"OBSERVATION:\n\n1. There are more students who attended Other boards than Central.\n2. The percentage of students placed that belonged to each category is comparable. The numbers do not show a specific inclination towards a particular board category.","6693ec58":"We have 12 features in our model. Above, are the p values for all the features in our model.\n\nA **parameter sweep for the K best features** will give us the optimum number of features required for an optimum level of accuracy, jaccard score, logloss and f1 score.","58140f36":"**The Logistic Regression model gives the optimum result when it takes in 6 or 7 features to train the model. The performance drops when features are >7 and <6.**\n","584cee0c":"**The model will most of the time give an accuracy in the range: (88 - 8)% to (88 + 6)%, that is, 80% to 94%.**","e0e1174e":"### 5. hsc_b (Higher Secondary Board)","eaef792e":"### 11. specialisation (MBA Specialisation)","a08dbc25":"### 1. Gender","4f5b393d":"Applying **SelectKBest** to select the number of features that give the best result (that are most relevant to quesiton being asked.","96cb88be":"**Let's find out the specifics in categorical variables in relation to the placement status of the student.**","ab6083be":"OBSERVATION:\n\n1. Students opted for Mkt&Fin more than Mkt&HR in MBA.\n2. Students who pursued Mkt&Fin were more likely to get placed than students who pursued Mkt&HR.","274f195e":"OBSERVATION:\n\n1. The number of students who opted for the subjects in HSC are in the order: Commerce>Science>Arts\n2. Students with Science and Commerce in their HSC are the ones that get preference in placements.","2522da4e":"**There is no salary for 67 students. These many students were not placed yet.**"}}