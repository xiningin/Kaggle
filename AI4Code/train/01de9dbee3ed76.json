{"cell_type":{"423cf726":"code","b426f5c0":"code","5afeb4d1":"code","b35f6433":"code","89d0f400":"code","0f2de6ee":"code","be47ec69":"code","f171aa27":"code","9d88a147":"code","e61eb9c0":"code","42646899":"code","3636b189":"code","db1a54e0":"code","50e44316":"code","f12875a9":"code","8f38d1f4":"code","7d296160":"code","305b88b0":"code","68881319":"code","3be1363e":"code","484c2181":"code","4cda010f":"code","4752fb57":"code","5fc24da8":"code","fed6479e":"code","b4bd12aa":"code","c2e3ebac":"code","bd6b7c0c":"code","c0b6804f":"code","8c19e07b":"code","f6ec01d5":"code","92f0db15":"code","4a3b6c17":"code","7a4b83b4":"code","6fbfba85":"code","ddbdf210":"code","186b5e22":"code","6af88deb":"code","d0f9734f":"code","82aa430a":"code","3b011493":"code","e240294b":"code","da4a45dc":"code","930908a8":"code","0945cf14":"markdown","396e3a9d":"markdown","f8e8d133":"markdown","25608e91":"markdown","4480f3c2":"markdown","14935f21":"markdown","a313a8a6":"markdown","ad206af3":"markdown","4ce30289":"markdown","1e522556":"markdown","97adae8b":"markdown","60e06a57":"markdown","b4a7996a":"markdown","c9f59067":"markdown","b1249bc1":"markdown","8d468f22":"markdown","97915f3a":"markdown","5d9cf918":"markdown","3f571c72":"markdown","71409c60":"markdown"},"source":{"423cf726":"# base\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\n\n# pandas option\npd.set_option('display.max_columns', 500)\nplt.rcParams['figure.dpi'] = 100\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"","b426f5c0":"#gather our data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test.reset_index().drop('index',axis=1)['Id']\ntrain.head()","5afeb4d1":"train.shape, test.shape","b35f6433":"train.info()","89d0f400":"# correlation with label(SalePrice) \n\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>=0.4]\ncorrmat.loc[top_corr_features, 'SalePrice'].sort_values(ascending=False)[1:].to_frame().\\\nstyle.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","0f2de6ee":"# heatmap\nplt.figure(figsize=(13,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","be47ec69":"#Visualize columns have corr with SalePrice\n\nprint(len(top_corr_features))\nfig, axes = plt.subplots(5,3, figsize=(20, 20), sharey=True);\nplt.subplots_adjust(hspace = 0.7, wspace=0.1)\nfig.suptitle('Highest Correlation with sale price', fontsize=20);\n\nfor i,col in zip(range(len(top_corr_features)),top_corr_features):\n    sns.scatterplot(y=train['SalePrice'], x=train[col], ax=axes[i\/\/3][i%3])\n    axes[i\/\/3][i%3].set_title('SalesPrice with '+col)","f171aa27":"#select the index of outliers from the scatter plots above\ndrop_index = train[((train['GarageArea']>1200) & (train['SalePrice']<300000))|\n                  ((train['GrLivArea']>5000) & (train['SalePrice']<300000))|\n                  ((train['1stFlrSF']>4000) & (train['SalePrice']<300000))|\n                  ((train['TotalBsmtSF']>5000) & (train['SalePrice']<300000))|\n                  ((train['MasVnrArea']>1200) & (train['SalePrice']<700000))|\n                  ((train['SalePrice']>600000))\n                  ].index","9d88a147":"# the distribution of our target (Sale Price)\ntemp = pd.DataFrame()\ntemp['Sale Price'] = train['SalePrice']\ntemp['Log Sale Price'] = np.log1p(train['SalePrice'])\n\nfig, axs = plt.subplots(1, 2, figsize=(7, 7))\nsns.distplot(temp['Sale Price'], hist=True, kde=False, \n             bins=int(100\/5), color = 'blue',\n             hist_kws={'edgecolor':'black'}, ax=axs[0])\nsns.distplot(temp['Log Sale Price'], hist=True, kde=False, \n             bins=int(100\/5), color = 'blue',\n             hist_kws={'edgecolor':'black'}, ax=axs[1])\nplt.show()","e61eb9c0":"Y_train = train['SalePrice']\ndel train['SalePrice']\n\n#Converting the saleprice with Logarithms to over come the high skewness and the outliers\nY_train = np.log1p(Y_train) \n\nfull_df = pd.concat([train.iloc[:,1:], test.iloc[:,1:]])\nntrain = len(train)\nntest = len(test)","42646899":"#null percentage for each column\nnull_df = round(100*(full_df.isnull().sum().sort_values(ascending=False)\/len(full_df.index)),2)\\\n                    .to_frame().rename(columns={0:'Null values percentage'})[:15]\nnull_df","3636b189":"fig1, ax1 = plt.subplots()\nlabels = null_df.index.tolist()\nax1.pie(null_df['Null values percentage'], labels=labels, autopct='%.1f%%', shadow=True)\nax1.axis('equal')\nplt.show()","db1a54e0":"plt.figure(figsize=(20,6))\nplt.title('Heatmap for the null values in each column')\nsns.heatmap(full_df.isnull(),cmap='viridis');","50e44316":"num_cols = ['LotFrontage', 'LotArea', 'YearBuilt','YearRemodAdd', 'MasVnrArea', \n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n            'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n            '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\ncat_cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour','LotConfig', 'LandSlope', 'Neighborhood',\n            'Condition1', 'Condition2','BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st','Exterior2nd', \n            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2','Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual','TotRmsAbvGrd', 'Functional', \n            'FireplaceQu', 'GarageType','GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC','Fence', \n            'MiscFeature', 'MoSold', 'SaleType', 'SaleCondition','OverallCond', 'YrSold']","f12875a9":"null_cat = full_df[cat_cols].isnull().sum()[full_df[cat_cols].isnull().sum()>0].index\nprint('All Numeric coulmns that have null values are : \\n\\n{}'.format(null_cat.values))","8f38d1f4":"null_num = full_df[num_cols].isnull().sum()[full_df[num_cols].isnull().sum()>0].index\nprint('All categorical coulmns that have null values that are : \\n\\n{}'.format(null_num.values))","7d296160":"full_df['Utilities'].value_counts()","305b88b0":"del full_df['Utilities']","68881319":"# Basement columns\nBS_col = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n          'BsmtFullBath','MasVnrArea','BsmtHalfBath','BsmtFinSF1', \n          'BsmtFinSF2', 'BsmtUnfSF']\n\n# Meaning that No Basement\nfor col in BS_col:\n    full_df[col].fillna(0, inplace=True) \n    \nfull_df['BsmtCond'].fillna('NA', inplace=True)         # Meaning that No Basement(general condition)\nfull_df['BsmtExposure'].fillna('NA', inplace=True)     # Meaning that No Basement(walkout or garden level walls)\nfull_df['BsmtQual'].fillna('NA', inplace=True)         # Meaning that No Basement(height of the basement)\nfull_df['BsmtFinType1'].fillna('NA', inplace=True)     # Meaning that No Basement(basement finished area)\nfull_df['BsmtFinType2'].fillna('NA', inplace=True)\n\n#Garage Columns\ngr_col = ['GarageType','GarageYrBlt','GarageFinish','GarageCars',\n          'GarageArea','GarageQual','GarageCond']\n\n# Meaning that NO Garage\n# Numeric\nfull_df['GarageYrBlt'].fillna(0, inplace=True)\nfull_df['GarageCars'].fillna(0, inplace=True)          \nfull_df['GarageArea'].fillna(0, inplace=True)\n\n# Categorical\nfull_df['GarageCond'].fillna('NA', inplace=True)       \nfull_df['GarageQual'].fillna('NA', inplace=True)       \nfull_df['GarageType'].fillna('NA', inplace=True)       \nfull_df['GarageFinish'].fillna('NA', inplace=True)   \n\n\nfull_df['MasVnrType'].fillna('NA', inplace=True)     # Meaning that NO Masonry veneer\nfull_df['PoolQC'].fillna('NA', inplace=True)         # Meaning that NO Pool \nfull_df['Alley'].fillna('NA', inplace=True)          # Meaning that NO Alley ccess\nfull_df['Fence'].fillna('NA', inplace=True)          # Meaning that NO Fence\nfull_df['FireplaceQu'].fillna('NA', inplace=True)    # Meaning that NO Fireplace\nfull_df['MiscFeature'].fillna('NA', inplace=True)    # Meaning that NO Miscellaneous feature not covered in \n                                                     # other categories  \n    \n\nfull_df['Functional'].fillna('Typ' ,inplace=True)    # Typical Functionality\nfull_df['Electrical'].fillna('SBrkr' ,inplace=True)  # Standard Circuit Breakers & Romex\nfull_df['SaleType'].fillna('Oth' ,inplace=True)      # Other\nfull_df['KitchenQual'].fillna('TA' ,inplace=True)    # Typical\/Average\nfull_df['SaleType'].fillna('Oth' ,inplace=True)      # Other\nfull_df['Exterior1st'].fillna('Other' ,inplace=True) # Other\nfull_df['Exterior2nd'].fillna('Other' ,inplace=True) # Other\n \n# split the data before imputing the numeric data to avoid (Data lackage)\ntrain = full_df[:ntrain].reset_index().drop('index',axis=1)\ntest = full_df[ntrain:].reset_index().drop('index',axis=1)\n\nfor data in [train,test]:\n    # LotFrontage: Linear feet of street connected to property\n    data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n    \n    # Meaning that NO Masonry veneer\n    data['MSZoning'] = data['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n#combine all the data together again\nfull_df=pd.concat([train,test])","3be1363e":"#Check if still there are null values\nfull_df.isnull().sum().max()","484c2181":"#print the count of unique values in each categorical columns\nprint('Categorical columns      Unique values count\\n')\nfor col in cat_cols:\n    print(col,'-'*(30-len(col)),'>',len(full_df[col].unique()))","4cda010f":"rows, cols = full_df.shape\nprint('Our new data has {} rows and {} columns'.format(rows, cols))","4752fb57":"# Total Area\nfull_df['TotalSF'] = full_df['TotalBsmtSF'] + full_df['1stFlrSF'] + full_df['2ndFlrSF']\n\n# add the column to numeric columns to normalize\nnum_cols.append('TotalSF')","5fc24da8":"from sklearn.preprocessing  import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler","fed6479e":"# Convert YrSold, MoSold, MSSubClass, and OverallCond columns from numeric to string\n# We have to encode the data as categorical values, to benefit from increasing or decreasing to build the model\n\nfull_df['YrSold'] = full_df['YrSold'].astype(str)\nfull_df['MoSold'] = full_df['MoSold'].astype(str)\nfull_df['MSSubClass'] = full_df['MSSubClass'].apply(str)\nfull_df['OverallCond'] = full_df['OverallCond'].astype(str)","b4bd12aa":"# Convert all categorical collumns to numeric values\nfor c in cat_cols:\n    lbl = LabelEncoder() \n    full_df[c] = lbl.fit_transform(full_df[c])","c2e3ebac":"#Now we don't have any non-numeric columns\n\nfull_df.select_dtypes('O').columns","bd6b7c0c":"#lets create a dataframe for the numeric columns with high skewness\n\nskewness = pd.DataFrame()\n\nskewness[['Positive Columns','Skewness(+v)']] = full_df[num_cols].skew().sort_values(ascending=False)[:10].reset_index()\nskewness[['Negative Columns','Skewness(-v)']] = full_df[num_cols].skew().sort_values(ascending=True)[:10].reset_index()\n\nskewness.columns = pd.MultiIndex.from_tuples([('Positive Skewness', 'Columns'), ('Positive Skewness', 'Skewness'),\n                                              ('Negative Skewness', 'Columns'), ('Negative Skewness', 'Skewness')])\nskewness","c0b6804f":"#Visualize columns have highest Skewness\ncols = ['MiscVal','PoolArea','LotArea','LowQualFinSF','3SsnPorch','BsmtFinSF2','KitchenAbvGr',\n        'BsmtFinSF2','EnclosedPorch','ScreenPorch','GarageYrBlt','BsmtHalfBath']\n\n#Visualize columns have corr eith SalePrice\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(3,4, figsize=(18, 8));\nplt.subplots_adjust(hspace = 0.7, wspace=0.2)\nfig.suptitle('Highest Skewness', fontsize=20)\n\nfor i,col in zip(range(12),cols):\n    sns.kdeplot(full_df[col], ax=axes[i\/\/4][i%4], fill=True);\n    axes[i\/\/4][i%4].set_title(col+' Distribution')","8c19e07b":"df = full_df.copy()\n\n#convert all categorical columns to integer values\nfor col in cat_cols:\n    df[col] = df[col].astype(int)\n\n# convert our categorical columns to dummies instead of LabelEncoding\nfor col in cat_cols:\n    dumm = pd.get_dummies(df[col], prefix = col, dtype=int)\n    df = pd.concat([df,dumm], axis=1)\n    \ndf.drop(cat_cols, axis=1, inplace=True)\n\n#Normalize our numeric data\ndf[num_cols] = df[num_cols].apply(lambda x:np.log1p(x)) #Normalize the data with Logarithms\n\ntrain_set = df[:ntrain].reset_index().drop('index',axis=1)\ntest_set = df[ntrain:].reset_index().drop('index',axis=1)\n\n#dropping the outliers\ntrain_set = train_set.drop(drop_index)\nY_train = Y_train.drop(drop_index)","f6ec01d5":"from sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing  import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import (BaggingRegressor, AdaBoostRegressor,GradientBoostingRegressor, \n                              RandomForestRegressor,  GradientBoostingRegressor)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost as xgb","92f0db15":"base_models = {\"Random Forest\": RandomForestRegressor(n_estimators=300),      #Random Forest model\n               \"SVM\": SVR(),                                                  #Support Vector Machines\n               \"XGBoost\": XGBRegressor(),                                     #XGBoost model}\n               \"Gradient Boosting\":make_pipeline(StandardScaler(),\n                                                 GradientBoostingRegressor(n_estimators=3000, #GradientBoosting model\n                                                                           learning_rate=0.005,     \n                                                                           max_depth=4, max_features='sqrt',\n                                                                           min_samples_leaf=15, min_samples_split=10, \n                                                                           loss='huber', random_state =5))}","4a3b6c17":"# Preprocessing, fitting, making predictions and scoring for every model:\nmodels_data = {'R^2':{'Training':{},'Testing':{}},\n               'Adjusted R^2':{'Training':{},'Testing':{}},\n               'MAE':{'Training':{},'Testing':{}},\n               'MSE':{'Training':{},'Testing':{}},\n               'RMSE':{'Training':{},'Testing':{}}}\n\nX_train, X_test, y_train, y_test = train_test_split(train_set, Y_train, test_size=0.2, random_state=42)\np = train_set.shape[1]\ntrain_n = X_train.shape[0]\ntest_n = X_test.shape[0]\n\nfor name in base_models:\n    #fitting the model\n    model = base_models[name].fit(X_train, y_train)\n    #make predictions with train and test datasets\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    #calculate the R-Squared for training and testing\n    r2_train,r2_test = model.score(X_train, y_train), model.score(X_test, y_test)\n    models_data['R^2']['Training'][name], models_data['R^2']['Testing'][name] = r2_train, r2_test\n            \n    #calculate the Adjusted R-Squared for training and testing\n    adj_train, adj_test = (1-(1-r2_train)*(train_n-1)\/(train_n-p-1)) ,(1-(1-r2_test)*(train_n-1)\/(train_n-p-1))\n    models_data['Adjusted R^2']['Training'][name], models_data['Adjusted R^2']['Testing'][name] = adj_train, adj_test\n               \n    #calculate the Mean absolute error for training and testing\n    mae_train, mae_test = mean_absolute_error(y_train, y_pred_train), mean_squared_error(y_test, y_pred_test)         \n    models_data['MAE']['Training'][name], models_data['MAE']['Testing'][name] = mae_train, mae_test\n               \n    #calculate Mean square error for training and testing\n    mse_train, mse_test = mean_squared_error(y_train, y_pred_train), mean_squared_error(y_test, y_pred_test)\n    models_data['MSE']['Training'][name], models_data['MSE']['Testing'][name] = mse_train, mse_test\n\n    #calculate Root mean error for training and testing    \n    rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n    models_data['RMSE']['Training'][name], models_data['RMSE']['Testing'][name] = rmse_train, rmse_test\n    \n    print('\\n========================={}========================='.format(name))\n    print('**********Training**********************Testing********')\n    print('R^2    : ',r2_train,' '*(25-len(str(r2_train))),r2_test) \n    print('Adj R^2: ',adj_train,' '*(25-len(str(adj_train))),adj_test) \n    print('MAE    : ',mae_train,' '*(25-len(str(mae_train))),mae_test) \n    print('MSE    : ',mse_train,' '*(25-len(str(mse_train))),mse_test) \n    print('RMSE   : ',rmse_train,' '*(25-len(str(rmse_train))),rmse_test)","7a4b83b4":"R_2 = pd.DataFrame(models_data['R^2']).sort_values(by='Testing',ascending=False)\nAdjusted_R_2 = pd.DataFrame(models_data['Adjusted R^2']).sort_values(by='Testing',ascending=False)\nMAE = pd.DataFrame(models_data['MAE']).sort_values(by='Testing',ascending=True)\nMSE = pd.DataFrame(models_data['MSE']).sort_values(by='Testing',ascending=True)\nRMSE = pd.DataFrame(models_data['RMSE']).sort_values(by='Testing',ascending=True)","6fbfba85":"#order the results by testing values\n\npx.line(data_frame=R_2.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='R-Squared for training and testing')","ddbdf210":"#order the results by testing values\n\npx.line(data_frame=Adjusted_R_2.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Adjusted R-Squared for training and testing')","186b5e22":"#order the results by testing values\n\npx.line(data_frame=RMSE.reset_index(),\n        x='index',y=['Training','Testing'],\n        title='Root mean square error for training and testing')","6af88deb":"# prepare configuration for cross validation test\n\n#Create two dictionaries to store the results of R-Squared and RMSE \nr_2_results = {'R-Squared':{},'Mean':{},'std':{}}   \nrmse_results = {'RMSE':{},'Mean':{},'std':{}}\n\nn_folds = 5\nkfold = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_set)\n\nfor name in base_models:\n    model = base_models[name]\n    r_2 = cross_val_score(model, train_set, Y_train, scoring='r2', cv=kfold)   #R-Squared \n    rms = np.sqrt(-cross_val_score(model, train_set, Y_train, cv=kfold,        #RMSE\n                                   scoring='neg_mean_squared_error'))\n    \n    #save the R-Squared reults\n    r_2_results['R-Squared'][name] = r_2\n    r_2_results['Mean'][name] = r_2.mean()\n    r_2_results['std'][name] = r_2.std()\n    \n    #save the RMSE reults\n    rmse_results['RMSE'][name] = rms\n    rmse_results['Mean'][name] = rms.mean()\n    rmse_results['std'][name] = rms.std()","d0f9734f":"#visualizing the variance of R-Squared for each model\n\nr_2_cv_results = pd.DataFrame(index=r_2_results['R-Squared'].keys())\n\n#append the max R-Squared for each model to the dataframe\nr_2_cv_results['Max'] = [r_2_results['R-Squared'][m].max() for m in r_2_results['R-Squared'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nr_2_cv_results['Mean'] = [r_2_results['Mean'][m] for m in r_2_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nr_2_cv_results['Min'] = [r_2_results['R-Squared'][m].min() for m in r_2_results['R-Squared'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nr_2_cv_results['std'] = [r_2_results['std'][m] for m in r_2_results['std'].keys()]\n\nr_2_cv_results = r_2_cv_results.sort_values(by='Mean',ascending=False)\nr_2_cv_results","82aa430a":"#visualize the results of RMSE for each model\n\nrmse_cv_results = pd.DataFrame(index=rmse_results['RMSE'].keys())\n\n#append the max R-Squared for each model to the dataframe\nrmse_cv_results['Max'] = [rmse_results['RMSE'][m].max() for m in rmse_results['RMSE'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nrmse_cv_results['Mean'] = [rmse_results['Mean'][m] for m in rmse_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nrmse_cv_results['Min'] = [rmse_results['RMSE'][m].min() for m in rmse_results['RMSE'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nrmse_cv_results['std'] = [rmse_results['std'][m] for m in rmse_results['std'].keys()]\n\nrmse_cv_results = rmse_cv_results.sort_values(by='Mean',ascending=True)\nrmse_cv_results","3b011493":"parametersGrid = {'learning_rate': [0.001, 0.01, 0.1],\n                  'subsample'    : [0.9, 0.5, 0.2, 0.1],\n                  'n_estimators' : [100,500],\n                  'max_depth'    : [4,6,8,10]\n                 }\nkfold = KFold(n_splits=5)\n\ngbr  = GradientBoostingRegressor()\ngbr_grid = GridSearchCV(gbr, parametersGrid, scoring='r2', cv=kfold)\ngbr_grid.fit(train_set, Y_train)","e240294b":"base_model = base_models['Gradient Boosting']\nr2_1 = round(cross_val_score(base_model ,train_set, Y_train, scoring='r2',cv=kfold).mean()*100, 3)\nr2_2 = round(gbr_grid.score(train_set, Y_train)*100, 3)\n\nprint(f'Before tuning the hyperparmeters ---> {r2_1} \\nAfter tuning the hyperparmeters ---> {r2_2}')","da4a45dc":"#lets make the predictions for the submission \n\ny_pred = np.expm1(gbr_grid.predict(test_set)) #using expm1 (The inverse of log1p)","930908a8":"submission = pd.DataFrame({\n        \"Id\": test_id,\n        \"SalePrice\": y_pred })\nsubmission.to_csv('submission.csv', index=False)","0945cf14":"## **tunning parameters**","396e3a9d":"# 5) Make a Submission","f8e8d133":"### **Skewness**","25608e91":"### **Prepare the data for ML**","4480f3c2":"### **Null values**","14935f21":"## 1) Import Libraries","a313a8a6":"### **Data Cleaning**","ad206af3":"### **Contents:**\n1. Import Libraries\n2. Import Data\n3. Data Pre-processing\n4. Developing and Evaluating Model\n5. Make a Submission","4ce30289":"To overcome the high skewness issue, i will transform the data into logarithm values.","1e522556":"### **Adding a new feature**","97adae8b":"### **Correlation with SalesPrice**","60e06a57":"## **Cross validation**","b4a7996a":"## File descriptions\n* train.csv - the training set\n* test.csv - the test set\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n* sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n## Data fields\nHere's a brief version of what you'll find in the data description file.\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","c9f59067":"I choose Gradient Boosting\n* Highest score with testing data and reasonable socre with training with training data (low bias)","b1249bc1":"## 4) Developing and Evaluating Model","8d468f22":"## 3) Data Pre-processing","97915f3a":"### **imputating missing values**\nI will divide into two groups (categorical and numerical)","5d9cf918":"select model -> gridSearchCV -> evaluation ","3f571c72":"## 2) Import Data","71409c60":"### **Normalizing and encodning the data**"}}