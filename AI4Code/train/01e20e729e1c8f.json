{"cell_type":{"03d86a7c":"code","d7f917a5":"code","409c9d7c":"code","c25417b0":"code","d450a783":"code","b513b6be":"code","da4080ff":"code","afe99c24":"code","a1086ea5":"code","f96fe03a":"code","d757a12f":"code","02a339f2":"code","20f4e13b":"code","a583a603":"code","5872089d":"code","f2ed21c2":"code","8d522c88":"code","1a5e5193":"code","881660ee":"code","cab1e59f":"code","4eb58830":"code","346edce5":"code","f6430171":"code","a7deb58d":"code","fb0433ef":"code","1f91148b":"code","6a5d5f60":"code","7d51203a":"code","91ee3c86":"code","0ccf11d7":"code","e37514a2":"code","ccbe88a2":"code","2f8eb0ba":"code","fb7987fe":"code","8a997169":"code","778eb8cf":"code","c64debe9":"code","3b210666":"code","15258e86":"code","c456b6f0":"code","7b2e61d3":"code","18433eda":"code","e0a7f9ea":"code","def040c3":"code","ae113c5d":"code","283aa8c7":"code","170a415f":"code","d74a2e7d":"code","850de6bb":"code","a58677df":"code","70875075":"code","e79dfc86":"code","8022da42":"code","88e88894":"code","9219a520":"code","eee10113":"code","a6574f4d":"code","996850dd":"code","bbc15338":"code","fa5ba303":"code","14ec15e3":"code","1f107ec9":"code","034fec15":"code","dcd5cd86":"code","a14dbce3":"code","cccd191a":"code","42b6bbcb":"code","9a7135ca":"code","ccb80f3a":"code","b9ce0cd3":"code","f2fa95bc":"code","ae21607e":"code","90b04742":"code","5dd2d136":"code","9596454d":"code","9ab2b05f":"code","2e383e9c":"code","1663c690":"code","0455eba0":"code","6e54a471":"code","8df275e2":"code","d7cc00a7":"code","c25be93b":"code","58030a79":"code","7fc03df9":"code","6fa6fe0d":"code","477a1b57":"code","0c62dd96":"code","a5991a03":"code","5709720f":"code","f8deb0cb":"code","1c59d6df":"code","bfff3d15":"code","0cecfe8d":"code","f13630ca":"code","68d4e3ac":"code","466463ea":"code","8788f8dd":"code","19c70a20":"markdown","8e8b9ef1":"markdown","bf6548ab":"markdown","beeed672":"markdown","2ea74aad":"markdown","9ad97758":"markdown","46afb5b4":"markdown","fde5e932":"markdown","65cd8e9c":"markdown","61f39d57":"markdown","5d49cf41":"markdown","49a35b4e":"markdown","72dff75a":"markdown","24a63665":"markdown","a8987e2b":"markdown","df790649":"markdown","fc516972":"markdown","177604aa":"markdown","8b032cac":"markdown","3c56d4f3":"markdown","59935a43":"markdown","498822f7":"markdown","dbd86b8e":"markdown","f6301995":"markdown","8e877842":"markdown","cb9f2045":"markdown","1c9c240d":"markdown","ac4daf06":"markdown","1e30cbb0":"markdown","0101942b":"markdown","a48fa1ab":"markdown","49c308a3":"markdown","6d5a43b9":"markdown","7c333b09":"markdown","1aa56e10":"markdown","0ae624ab":"markdown","3e017071":"markdown","d6187b43":"markdown","30b8b5ba":"markdown","6d7916e9":"markdown","14d1ae52":"markdown","fa08e988":"markdown","df11c7ba":"markdown","855c3e00":"markdown","8585e0ad":"markdown"},"source":{"03d86a7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7f917a5":"#Data Analysis\nimport numpy as np\nimport pandas as pd\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n\n#For missing values\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Preprocessing\nfrom sklearn import preprocessing\n\n#Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\n#Machine learning \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport statsmodels.api as sm\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n#Random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n#Vif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","409c9d7c":"# Lets import the train and test data and look into it\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_valid = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nmerged = [train_df, test_df]\n\ntrain_df.head()","c25417b0":"# Looking into the datatype and count \n\ntrain_df.info()\nprint('-'*50)\ntest_df.info()","d450a783":"#Statistical Summary for numeric columns\n\ntrain_df.describe()","b513b6be":"#Statistical Summary for object columns columns\n\ntrain_df.describe(include=['O'])","da4080ff":"# Shape of dataset\n\nprint(\"Size of training data:{0}\".format(train_df.shape))\n\nprint(\"Size of test data:{0}\".format(test_df.shape))","afe99c24":"# Let's look into the missing value percentage \n\nprint(round(100*(train_df.isnull().sum()\/len(train_df)),2))\nprint('-'*40)\nprint(round(100*(test_df.isnull().sum()\/len(test_df)),2))","a1086ea5":"plt.figure(figsize = (10,8))\n\nsns.heatmap(train_df.corr(), annot= True, cmap = 'YlGnBu')","f96fe03a":"# we will drop Cabin Column from the data has it contain lot of missing values\n\n\ntrain_df.drop('Cabin', inplace = True, axis = 1)\ntest_df.drop('Cabin', inplace = True, axis =1)","d757a12f":"# Since we assume that the sex and Pclass is important let's look into the same\n\n# sex\n100*pd.crosstab(train_df.Survived, train_df.Sex, margins = True, margins_name = 'Total', normalize = True).round(4)","02a339f2":"# Cross tabulation to see the M and F distribution across different PClass\n\n100*pd.crosstab(train_df.Sex, train_df.Pclass, margins = True, margins_name = \"Total\", normalize = True).round(3)","20f4e13b":"100*pd.crosstab(train_df.Pclass,train_df.Survived, normalize = 'index').round(3)","a583a603":"# Survival rate of Siblings\/Spouses\n\n100*train_df[[\"SibSp\", 'Survived']].groupby([\"SibSp\"]).mean().sort_values(by = 'Survived', ascending = False).round(4)","5872089d":"# Survival rate of Parents\/Children\n\n100*train_df[[\"Parch\", 'Survived']].groupby([\"Parch\"]).mean().sort_values(by = 'Survived', ascending = False).round(4)","f2ed21c2":"# Visualizing for age group\n\ntrain_df['Age_Group'] = pd.cut(train_df.Age, bins = [0,16,32,48,64,100], labels = [0,1,2,3,4,])\n\nplt.figure(figsize = (8,6))\nsns.countplot('Age_Group', hue = 'Survived', data= train_df, palette=\"Set1\")\n\nplt.title(\"Survival distribution according to Age Group\", fontsize = 20)\nplt.ylabel('Frequency',fontsize = 15)\nplt.xlabel('Age Groups', fontsize = 15)\nplt.show()","8d522c88":"# We will drop the age group column has we dont need it\n\ntrain_df.drop('Age_Group', inplace = True, axis = 1)","1a5e5193":"# Visulizing for gender\n\nplt.figure(figsize = (8,6))\n\nsns.countplot('Sex', hue = 'Survived', data= train_df)\n\nplt.title(\"Survival distribution according to Gender\", fontsize = 20)\nplt.ylabel('Frequency',fontsize = 15)\nplt.xlabel('Gender', fontsize = 15)\nplt.show()","881660ee":"# Visualising for different Pclass\n\ng = sns.FacetGrid(train_df, row = 'Pclass', col = 'Survived', height=2.5, aspect=1.5)\ng.map(plt.hist, 'Age', alpha = 0.5, bins = 20,edgecolor=\"black\", color = 'g')","cab1e59f":"# we will fill the missing values in embarked with mode\n\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace = True)","4eb58830":"# Visualization for Embrakation\n\nplt.figure(figsize = (8,6))\n\nsns.countplot('Embarked', hue = 'Survived', data = train_df)\n\nplt.title(\"Survival distribution according to Embrakation\", fontsize = 20)\nplt.ylabel('Frequency',fontsize = 15)\nplt.xlabel('Port of Embarkation', fontsize = 15)\nplt.show()\n\n\n# C = Cherbourg, Q = Queenstown, S = Southampton","346edce5":"100*pd.crosstab(train_df.Embarked, train_df.Survived, normalize = 'index').round(3)","f6430171":"# Let's look into different fare prices for different embarkation port\n\nplt.figure(figsize = (8,6))\nsns.barplot(y = 'Embarked', x = 'Fare', data = train_df, hue = 'Pclass', palette = 'Set1', ci = None)\n\nplt.title(\"Fair prices for various Pclass from different Embrakation port\", fontsize = 20)\nplt.ylabel('Embrakation Port',fontsize = 15)\nplt.yticks([0,1,2], ['Southampton', 'Cherbourg','Queenstown'])\nplt.xlabel('Fare Price', fontsize = 15)\nplt.show()","a7deb58d":"# Looking into average fair price according to gender and port embarked`\n\npd.pivot_table(train_df, index = ['Sex','Embarked'], columns = 'Pclass', values = 'Fare', aggfunc = np.mean).round(2)","fb0433ef":"#Checking weather duplicate tickets were issued\n\nduplicate = train_df['Ticket'].duplicated().sum()\n\nprint(\"Number of duplicate tickets issued are {0} which contributes around {1}%\".format(duplicate, 100*round(duplicate\/len(train_df),2) ))","1f91148b":"# Dropping Ticket and Passenger ID from data frame as it doesnt contribute for analysis\n\ntrain_df.drop(['PassengerId', 'Ticket'], inplace = True, axis = 1)\n\ntest_df.drop(['PassengerId', 'Ticket'], inplace = True, axis = 1)","6a5d5f60":"# We will replace male to 1 and female to 0 & make sex column numeric\n\ntrain_df['Sex'].replace(['female', 'male'], [0,1], inplace = True)\ntest_df['Sex'].replace(['female','male'], [0,1], inplace = True)","7d51203a":"# Label encoding embracked column\n\nle = preprocessing.LabelEncoder()\n\ntrain_df['Embarked'] = le.fit_transform(train_df['Embarked'])\ntest_df['Embarked'] = le.fit_transform(test_df['Embarked'])","91ee3c86":"# We will extract a new feature called Title from name\n\nfor df in merged:\n    df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand = False)\n    \npd.crosstab(train_df['Title'], train_df['Sex'])","0ccf11d7":"#Replacing the least repeated keywords with others\n\nfor df in merged:\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')\n    \n    df['Title'] = df['Title'].replace(['Mlle','Ms'], 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    \n100*pd.crosstab(train_df.Title, train_df.Survived, normalize = 'index').round(3)","e37514a2":"# Encoding Title column\n\nfor df in merged:\n    df['Title'] = le.fit_transform(df['Title'])","ccbe88a2":"# Dropping the Name column from the dataset\n\nfor df in merged:\n    df.drop('Name', axis = 1, inplace = True)","2f8eb0ba":"# Filling the misssing value for fare in test dataset with median\n\ntest_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","fb7987fe":"# Stroting the column names\n\ntrain_columns = train_df.columns\ntest_columns = test_df.columns","8a997169":"# Filling the missing values for age with Iterative Imputer for train\n\nii = IterativeImputer(initial_strategy='median', min_value = 0, max_value = 80, random_state = 42)\n\ntrain_df_clean = pd.DataFrame(ii.fit_transform(train_df))\ntrain_df_clean.columns = train_columns","778eb8cf":"# Similiarly for test\n\ntest_df_clean = pd.DataFrame(ii.fit_transform(test_df))\ntest_df_clean.columns = test_columns","c64debe9":"# Restoring the datatype to there original format\n\nmain = [train_df_clean, test_df_clean]\n\nfor df in main:\n\n    for i in ['Pclass', 'Sex', 'SibSp', 'Parch','Embarked','Title']:\n        df[i] = pd.to_numeric(df[i])\n        df[i] = df[i].astype(int)","3b210666":"# Changing the datatype of survived in training dataset\n\ntrain_df_clean['Survived'] = pd.to_numeric(train_df_clean['Survived'])\ntrain_df_clean['Survived'] = train_df_clean['Survived'].astype(int)","15258e86":"train_df_clean.head()","c456b6f0":"# Creating a new feature called 'Familysize'\n\nfor df in main:\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1","7b2e61d3":"# Family size and surival chances\n\n100 * pd.pivot_table(data = train_df_clean, index = 'FamilySize', values = 'Survived', aggfunc = np.mean).sort_values(by = 'Survived', ascending = False).round(3)","18433eda":"#plotting graph to see surival rate and family size\n\nplt.figure(figsize = (8,6))\n\nsns.lineplot(data = train_df_clean, x = 'FamilySize', y = 'Survived',ci = None, marker=\"o\")\n\nplt.title(\"Family Size vs Survival Rate\", fontsize = 20)\nplt.ylabel('Survival Rate',fontsize = 15)\nplt.xlabel('Family Size', fontsize = 15)\nplt.show()","e0a7f9ea":"# Creating another attribute called Is_alone\n\nfor df in main:\n    df['Is_Alone'] = 0\n    df.loc[df['FamilySize']==1, 'Is_Alone'] = 1\n    \n# 1 = alone & 0 = Not_alone","def040c3":"# Let's look at survival rate of alone passaneger\n\n100 * pd.crosstab(train_df_clean['Is_Alone'], train_df_clean['Survived'], normalize = 'index').round(3)","ae113c5d":"plt.figure(figsize = (8,6))\n\nsns.barplot(data = train_df_clean, x = 'Is_Alone', y = 'Survived', ci = None)\n\nplt.title(\"Chances of Solo Passeneger Surviving\", fontsize = 20)\nplt.ylabel('Survival Rate',fontsize = 15)\nplt.xlabel('Type of Passeneger', fontsize = 15)\nplt.xticks([0,1], ['Family', 'Solo Passenger'])\nplt.show()","283aa8c7":"plt.figure(figsize = (15,10))\n\nsns.heatmap(train_df_clean.corr(), annot = True)","170a415f":"# based on the above correaltion we will drop SibSp, Parch, Family_size\n\nfor df in main:\n    df.drop(['SibSp','Parch','FamilySize'], inplace = True, axis =1)","d74a2e7d":"# Plotting box plot for all the variables and checking for outliers\n\n\nplt.figure\n\nfor i, col in enumerate(train_df_clean.columns):\n    plt.figure(i)\n    sns.boxplot(train_df_clean[col])","850de6bb":"# Doing small outlier treatment for fare attribute\n\ntrain_df_clean.drop(train_df_clean.index[train_df_clean['Fare'] > 300], inplace = True)","a58677df":"# This are the final data frame\n\ntrain_df_clean.head()","70875075":"test_df_clean.head()","e79dfc86":"# Let's look into class imbalance of our target varaible i.e. survived\n\npd.crosstab(train_df_clean['Survived'], train_df_clean['Survived'], normalize = True).round(4)*100","8022da42":"X_train = train_df_clean.drop('Survived', axis =1)\n\ny_train = train_df_clean['Survived']\n\nX_test = test_df_clean","88e88894":"# Storing the column names  for train and test\nX_train_col = X_train.columns\n\nX_test_col = X_test.columns","9219a520":"# We will convert the data into array as it will optimize more\n\nX_train, y_train = np.array(X_train), np.array(y_train)","eee10113":"scaler = MinMaxScaler()\n\n#for train data set\nX_train = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns = X_train_col)","a6574f4d":"#Scaling test dataset\n\nX_test = scaler.fit_transform(X_test)\nX_test = pd.DataFrame(X_test, columns = X_test_col)","996850dd":"#To use later for random forest\n\nrf_X_train = X_train.copy()\nrf_X_test = X_test.copy()","bbc15338":"# Finding the optimum hyper paramters\n\n## Different parameters to check\nmax_iter=[100,110,120,130,140]\nC_param_range = [0.001,0.01,0.1,1,10,100]\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n## Setting the paramters\nparam_grid = dict(max_iter = max_iter, C = C_param_range)\n\n## Setting model\nlog = LogisticRegression(penalty = 'l2')\n\n## Set up GridSearch for score metric\n\ngrid_search = GridSearchCV(estimator = log, param_grid = param_grid, cv = folds, n_jobs = -1, \n                           return_train_score = True, scoring = 'accuracy')\n\n## Fitting\ngrid_search.fit(X_train, y_train)","fa5ba303":"# Looking at the best parameters\n\nprint(\"The best accuracy score is {0:2.3} at {1}\".format(grid_search.best_score_, grid_search.best_params_))","14ec15e3":"# Setting model with optimum parameters\n\nlog = LogisticRegression(penalty = 'l2', C = 10, max_iter =100, class_weight = 'balanced')","1f107ec9":"# Fitting the model\n\nlog_fit = log.fit(X_train, y_train)","034fec15":"# Predicting on test data set\n\ny_test_pred = log_fit.predict(X_test)","dcd5cd86":"#Accuracy score for training data\n\nprint(\"Accuracy score for training data is: {0}\".format(round(log_fit.score(X_train, y_train) * 100, 2)))","a14dbce3":"X_train_sm = sm.add_constant(X_train)\nlog_sm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = log_sm.fit()\nprint(res.summary())","cccd191a":"X_train.shape[1]","42b6bbcb":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range (X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.reset_index(drop = True, inplace = True)\nvif","9a7135ca":"# Based on the above values we will drop Title\n\nX_train.drop('Title', axis = 1, inplace = True)","ccb80f3a":"X_train_sm = sm.add_constant(X_train)\nlog_sm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = log_sm.fit()\nprint(res.summary())","b9ce0cd3":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range (X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.reset_index(drop = True, inplace = True)\nvif","f2fa95bc":"# Based on the observation we will drop Is_alone\n\nX_train.drop('Is_Alone', axis =1, inplace = True)","ae21607e":"X_train_sm = sm.add_constant(X_train)\nlog_sm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = log_sm.fit()\nprint(res.summary())","90b04742":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range (X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.reset_index(drop = True, inplace = True)\nvif","5dd2d136":"#Dropping Fare\n\nX_train.drop('Fare', inplace = True, axis =1)","9596454d":"X_train_sm = sm.add_constant(X_train)\nlog_sm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = log_sm.fit()\nprint(res.summary())","9ab2b05f":"# Make a VIF dataframe for all the variables present\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range (X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.reset_index(drop = True, inplace = True)\nvif","2e383e9c":"#Prediciting the values of X train\n\ny_train_pred = res.predict(sm.add_constant(X_train))","1663c690":"y_train_pred_final = pd.DataFrame({'Survived': y_train, 'Survived_Proab':y_train_pred})\ny_train_pred_final['Survived_Proab'] = round(y_train_pred_final['Survived_Proab'],2)\ny_train_pred_final.head(2)","0455eba0":"# ROC function\n\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","6e54a471":"#Storing the values for FPR, TPR and thersolds\n\nfpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final['Survived'], y_train_pred_final['Survived_Proab'], drop_intermediate = False )","8df275e2":"# Call the ROC function\n\ndraw_roc(y_train_pred_final['Survived'], y_train_pred_final['Survived_Proab'])","d7cc00a7":"# Let's create columns with different probability cutoffs \n\nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final['Survived_Proab'].map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head(2)","c25be93b":"# Let's create a dataframe to see the values of accuracy, sensitivity, and specificity at different values of probabiity cutoffs\n\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final['Survived'], y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","58030a79":"# Plotting sensitivity, accuracy and specificity\n\nsns.set()\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","7fc03df9":"#Creating final predicated column\n\ny_train_pred_final['final_predicted'] = y_train_pred_final['Survived_Proab'].map( lambda x: 1 if x > 0.4 else 0)\n\ny_train_pred_final.head(2)","6fa6fe0d":"#Looking into the accuray of training data set\n\nprint(\"Accuracy : {:2.2}\".format(metrics.accuracy_score(y_train_pred_final['Survived'], y_train_pred_final['final_predicted'])))","477a1b57":"# Drop the required columns from X_test as well\n\nX_test.drop(['Fare', 'Is_Alone', 'Title'], axis =1, inplace = True)","0c62dd96":"# Making predictions on test data set\n\ny_test_pred1 = res.predict(sm.add_constant(X_test))","a5991a03":"# Converting y_pred to a dataframe\n\ny_pred1 = pd.DataFrame(y_test_pred1, columns = ['Survived_Proab'])\ny_pred1.reset_index(drop = True, inplace = True)\ny_pred1","5709720f":"# Make predictions on the test set using 0.4 as the cutoff\n\ny_pred1['final_predicted'] = y_pred1['Survived_Proab'].map(lambda x: 1 if x > 0.4 else 0)\ny_pred1.head()","f8deb0cb":"#Top features and there importance\n\nIP = pd.DataFrame(res.params , columns = ['Importance'])\nIP.reset_index(inplace = True)\nIP.columns = ['Features', 'Importance']\nIP.drop(IP.index[0], inplace = True)\nIP = IP.sort_values(by = 'Importance')\nIP.reset_index(drop = True, inplace =True)\nIP['Importance'] =  round(IP['Importance'], 2)\nIP.head(10)","1c59d6df":"# Instantiate\n\nrf = RandomForestClassifier()\n\n#Fitting \nrf.fit(rf_X_train, y_train)","bfff3d15":"# Setting up folds\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n#Setting up parameters to check\nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [4,5,6,7]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = folds, n_jobs = -1,verbose = 1)","0cecfe8d":"# Fit the grid search to the data\n\ngrid_search.fit(rf_X_train, y_train)","f13630ca":"# printing the optimal accuracy score and hyperparameters\n\nprint('We can get accuracy of',round(grid_search.best_score_,2),'using',grid_search.best_params_)","68d4e3ac":"# model with the best hyperparameters\n\nrf = RandomForestClassifier(bootstrap=True,class_weight = \"balanced\", criterion = 'gini',\n                             max_depth=4,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=4,\n                             n_estimators=200)\nrf_fit = rf.fit(rf_X_train, y_train)","466463ea":"#Predicitng on test\n\nrf_y_pred_test = rf_fit.predict(rf_X_test)","8788f8dd":"#File submission\n\nsubmission = pd.DataFrame({\"PassengerId\": test_valid[\"PassengerId\"], \"Survived\": rf_y_pred_test})\n\nsubmission.to_csv('submission_.csv', index=False)","19c70a20":"1. Importing the required libraries\n2. Loading and understanding the data\n3. Analysis of the data and missing value treatment:Used imputation technique such as iterative imputer\n4. Visualizing the data\n5. Feature Engineering (Creating new features)\n6. Outlier Analysis\n7. Train-Test split\n8. Scaling\n9. Model Building and Evaluation\n    - Logistic Regression\n    - Random Forest\n10. Submission","8e8b9ef1":"Now bucket the age variable into 5 groups defined as: \n- \"Age\" <= 16: 0\n- 16  & <= 32 :1\n- 32 & <= 48 :2\n- 48 & <= 64 :3\n- \"Age\" > 64 :4","bf6548ab":"**We will build 2 models:**\n1. Logestic Regression\n2. Random Forest","beeed672":"# EDA and Model Building on Titanic dataset","2ea74aad":"## 6. Outlier Analysis","9ad97758":"- Most of the Passenger for Pclass 3 didnt survive\n- Infants from Pclass 3 and Pclass 2 survived\n- Passenegers for Pclass have mostly survived","46afb5b4":"We can see that age group of 16-32 has the higest survival rate","fde5e932":"Since our main concern is regarding the Survival we will focusing on it more","65cd8e9c":"## 1. Logistic Regression","61f39d57":"- We eill drop Fare has it had p-value greater than 0.05","5d49cf41":"## 4.Visualizing the data","49a35b4e":"## 10. Submission","72dff75a":"- Survival rate of solo passeneger if 30.4%\n- Survival rate of family is 50.6%","24a63665":"## 2. Loading and Data Understanding","a8987e2b":"- To look at this we have slight class imbalance, we will handle this later using ``weight of class`` method while building model","df790649":"## Random Forest","fc516972":"**This are the final data frame**","177604aa":"#### Let's Gain some understanding\n\nSince this is a past event and many of us know about titanic<br>\nAccordingly the main criteria for survival would be \n1. Age\n2. Passenger's Class \n3. Sex","8b032cac":"#### Looking into p-values and vif and selecting the appropriate features for our model\n**NOTE**: This method will reduce the features in our model, I am just trying it out to see if this results in better accuracy","3c56d4f3":"- The fair prices are different for different embarkation port\n- Cherbourg has the highest number of Passenegers and that to from upper class segment","59935a43":"- We have high p-values for age and and Is_alone","498822f7":"- All the VIF values are less than 5 and are in the moderate range","dbd86b8e":"- Looking at this we can say 0.4 is the optimum threshold point","f6301995":"## 3. Analysis of the data and missing value treatment","8e877842":"Total survival rate is 38.4% out of which 26.2 % is of female","cb9f2045":"- All the p-values and VIF are less and we will procceed with this now for our predictions","1c9c240d":"- Prices at Cherbourg are maximum with average for Male 93.54 and Female 115.64 dollors","ac4daf06":"- We note that family size of 4 has highest survival rate whereas family greater than 8 has 0% survival rate","1e30cbb0":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated :)","0101942b":"- Major of the people are from Southampton, but highest people who didnt surive are also from Southampton","a48fa1ab":"Here we can see that major of the population belongs to Passenger Class 3 around 55%<br>\nAnd the ratio of male to female is 13:7","49c308a3":"- Looking at this we can say that major of the survived people are from Cherbourg ","6d5a43b9":"## Content","7c333b09":"- Here we can clearly see that the chances of solo passeneger survival is very low comapred to a family","1aa56e10":"## 7. Train-Test split","0ae624ab":"- Title and Age have high VIF","3e017071":"Clearly we can see that large number of females survived","d6187b43":"## 5. Feature Engineering","30b8b5ba":"Passengers belonging to the upper class have the highest rate of survival","6d7916e9":"#### Making predictions on Test data","14d1ae52":"Pclass have negative correlation with Fare i.e. as the fare price increases Pclass is lower","fa08e988":"- We can that we have high p-values for Fare, Title and Is_Alone","df11c7ba":"## 1. Importing libraries","855c3e00":"## Modeling Builidng and Evaluation","8585e0ad":"## 8. Scaling"}}