{"cell_type":{"ad5ae30a":"code","246efef0":"code","f8cc5b72":"code","27f98c8c":"code","b978a582":"code","404f70e3":"code","95baccf1":"code","e99a3efc":"code","dc698bfe":"code","1ff10019":"code","4d993e6a":"code","6cda96cd":"code","62f8c102":"code","fe7b0ef7":"code","e0fcabd0":"code","2d6da953":"code","52b8e7f3":"code","3bcab1b0":"code","ee4923f4":"code","007b5c06":"code","6875fec8":"markdown","eb357c01":"markdown","843c78a0":"markdown","5b6a4b8f":"markdown","4fcd078b":"markdown","736019b9":"markdown","530b0d33":"markdown","c7505bcc":"markdown","9e8710d2":"markdown","d7652717":"markdown","38c4a6f8":"markdown"},"source":{"ad5ae30a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","246efef0":"df=pd.read_csv(\"..\/input\/heart.csv\")","f8cc5b72":"df.head(5)","27f98c8c":"df.info()","b978a582":"df.describe()","404f70e3":"f, ax = plt.subplots(1, 2, figsize = (15, 7))\nf.suptitle(\"Heart disease?\", fontsize = 18.)\n_ = df.target.value_counts().plot.bar(ax = ax[0], rot = 0, color = (sns.color_palette()[0], sns.color_palette()[2])).set(xticklabels = [\"No\", \"Yes\"])\n_ = df.target.value_counts().plot.pie(labels = (\"No\", \"Yes\"), autopct = \"%.2f%%\", label = \"\", fontsize = 13., ax = ax[1],\\\ncolors = (sns.color_palette()[0], sns.color_palette()[2]), wedgeprops = {\"linewidth\": 1.5, \"edgecolor\": \"#F7F7F7\"}), ax[1].texts[1].set_color(\"#F7F7F7\"), ax[1].texts[3].set_color(\"#F7F7F7\")","95baccf1":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df.age, bins = 20, ax=ax[0,0]) \nsns.distplot(df.oldpeak, bins = 20, ax=ax[0,1]) \nsns.distplot(df.trestbps, bins = 20, ax=ax[1,0]) \nsns.distplot(df.chol, bins = 20, ax=ax[1,1]) \nsns.distplot(df.ca, bins = 20, ax=ax[2,0])\nsns.distplot(df.thal, bins = 20, ax=ax[2,1])\nsns.distplot(df.thalach, bins = 20, ax=ax[3,0]) \nsns.distplot(df.slope, bins = 20, ax=ax[3,1]) \nplt.show()","e99a3efc":"plt.figure(figsize=(16,12))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu',fmt='.2f',linewidths=2)","dc698bfe":"df.info()","1ff10019":"fig,ax = plt.subplots(nrows=4, ncols=2, figsize=(18,18))\nplt.suptitle('Violin Plots',fontsize=24)\nsns.violinplot(x=\"cp\", data=df,ax=ax[0,0],palette='Set3')\nsns.violinplot(x=\"trestbps\", data=df,ax=ax[0,1],palette='Set3')\nsns.violinplot (x ='chol', data=df, ax=ax[1,0], palette='Set3')\nsns.violinplot(x='fbs', data=df, ax=ax[1,1],palette='Set3')\nsns.violinplot(x='restecg', data=df, ax=ax[2,0], palette='Set3')\nsns.violinplot(x='thalach', data=df, ax=ax[2,1],palette='Set3')\nsns.violinplot(x='exang', data=df, ax=ax[3,0],palette='Set3')\nsns.violinplot(x='age', data=df, ax=ax[3,1],palette='Set3')\nplt.show()","4d993e6a":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","6cda96cd":"#Model\nLR = LogisticRegression()\n\n#fiting the model\nLR.fit(X_train, y_train)\n\n#prediction\ny_pred = LR.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", LR.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","62f8c102":"#Model\nDT = DecisionTreeClassifier()\n\n#fiting the model\nDT.fit(X_train, y_train)\n\n#prediction\ny_pred = DT.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", DT.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","fe7b0ef7":"from sklearn import tree\nimport graphviz","e0fcabd0":"#Plotting the graph\ntree_graph = tree.export_graphviz(DT, out_file=None)\ngraphviz.Source(tree_graph)","2d6da953":"feature_names = [i for i in df.columns if df[i].dtype in [np.int64]]","52b8e7f3":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=DT, dataset=df, model_features=feature_names, feature='age')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'age')\nplt.show()","3bcab1b0":"pdp_dist = pdp.pdp_isolate(model=DT, dataset=df, model_features=feature_names, feature='trestbps')\n\npdp.pdp_plot(pdp_dist, 'trestbps')\nplt.show()","ee4923f4":"features_to_plot = ['age', 'trestbps']\ninter1  =  pdp.pdp_interact(model=DT, dataset=df, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","007b5c06":"#Model\nmodel = GradientBoostingClassifier()\n\n#fiting the model\nmodel.fit(X_train, y_train)\n\n#prediction\ny_pred = model.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", model.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","6875fec8":"**Correlation between features**\n\nVariables within a dataset can be related for lots of reasons. It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n\nA correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated","eb357c01":"...stay tuned for updates","843c78a0":"This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.","5b6a4b8f":"# 2D Partial Dependence Plots\n\n2D partial dependence plots are very useful to understand interaction between features","4fcd078b":"# Predictive modelling ","736019b9":"**Decision Tree**\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.","530b0d33":"# Partial dependency plot\n\nWhile feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.","c7505bcc":"**Logistic Regression**\n\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary. Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","9e8710d2":"**Context**\n\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\nAttribute Information: \n 1. age \n 2. sex \n 3. chest pain type (4 values) \n 4. resting blood pressure \n 5. serum cholestoral in mg\/dl \n 6. fasting blood sugar > 120 mg\/dl\n 7. resting electrocardiographic results (values 0,1,2)\n 8. maximum heart rate achieved \n 9. exercise induced angina \n 10. oldpeak = ST depression induced by exercise relative to rest \n 11. the slope of the peak exercise ST segment \n 12. number of major vessels (0-3) colored by flourosopy \n 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","d7652717":"This graph shows predictions for any combination of age and resting blood pressure. For example, we see the highest predictions when age is around late 50's.\n\n**Gradient Boosting**\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","38c4a6f8":"**Violin Plots**\n\nA violin plot is a method of plotting numeric data. It is similar to box plot with a rotated kernel density plot on each side. Violin plots are similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram).\n\nA violin plot is more informative than a plain box plot. In fact while a box plot only shows summary statistics such as mean\/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot clearly shows the presence of different peaks, their position and relative amplitude. This information could not be represented with a simple box plot which only reports summary statistics. The inner part of a violin plot usually shows the mean (or median) and the interquartile range."}}