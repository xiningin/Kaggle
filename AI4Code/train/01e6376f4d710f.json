{"cell_type":{"8b0600e6":"code","d0d363fc":"code","c67ea37b":"code","9de54dac":"code","50545e59":"code","bd915311":"code","7a529873":"code","5710b217":"code","fbdc1db1":"code","7f713465":"code","94bab2e8":"code","fa94db55":"code","7d2b7422":"code","af635110":"code","83e27db1":"code","23c94a46":"code","d786db85":"code","5e228725":"code","cf812a9f":"code","f3123b45":"code","7de632fc":"code","09a9a8e4":"code","929aaaa9":"code","93417947":"code","646adfea":"code","4b6ca5fe":"code","658d33a8":"code","f7c7a846":"code","a9580eed":"code","9442b68f":"code","683edec9":"code","fc527b25":"code","ca77d8c0":"code","b966ad7f":"code","df44220f":"code","c9b59c3d":"code","23901916":"code","9f0d797c":"code","34a6ee3b":"code","794fd551":"code","fe0134db":"code","0faf5f57":"code","6893a911":"code","1e6a3eb4":"code","b702978e":"code","6ffb3a89":"code","be9f767d":"code","9651f0aa":"code","5886bb09":"code","963a0e5d":"code","ac8d3f47":"code","02bdfba7":"code","d28ff9d9":"code","5f46c49b":"code","55bdf820":"code","24fce33b":"code","3b13830b":"code","bad5ebc0":"code","4ad9ed05":"code","6b882c86":"code","3db80a45":"code","2476d6f4":"code","e6424ab1":"code","9f98eb59":"code","dead4b24":"code","66522cd0":"code","ee13f0b8":"code","9a12d11d":"code","6a192446":"code","57cf2635":"code","21aa2f6b":"code","2964ce5d":"code","99dfe866":"code","841793be":"code","76e4083d":"code","fd6e17aa":"code","328599d8":"code","032ff5f1":"code","ff018699":"code","32a1db60":"code","d3cc5513":"code","b3465572":"code","a36e3807":"code","1f0e4c72":"code","a6110c90":"code","6539254f":"code","7eae986e":"code","facb64b5":"code","ad7a518e":"code","d876ce27":"code","7e8c050f":"code","2ca69b51":"code","0c7c1e3b":"code","7da30c63":"code","43290ea5":"code","7c2fb1a1":"code","66145f7d":"code","a703f3fe":"code","709c12fb":"code","1fd8eea5":"code","2228acb4":"code","90ee90fe":"code","55c8f432":"code","6a5a7369":"code","4ce9c721":"code","a1a4c581":"code","c80395f1":"code","4d874056":"code","f8779e47":"code","471a3a72":"code","0761adfb":"code","d4153521":"code","fefb17be":"code","1d7a34d2":"code","e4e0ba49":"code","47dd14cc":"code","57e08abc":"code","e1b2127a":"code","a9dfb0b0":"code","8a3079fc":"code","1f04bb5f":"code","49d119fd":"code","b62e24bd":"code","1680eb98":"code","a60a867b":"code","c2feb5f3":"code","91a8dd2e":"code","a09862e7":"code","1d4c3b40":"code","0bd5cb1a":"code","8519d5a4":"code","5cd6e95b":"code","5d431329":"code","2737b458":"code","88277823":"code","3338deed":"code","c62b0d51":"code","fcb33adf":"code","a85e99e0":"code","f3f62760":"markdown","fc1d2fb4":"markdown","19fd9342":"markdown","a8677200":"markdown","e83da29f":"markdown","c5b53648":"markdown","763c2cfb":"markdown","23dfe557":"markdown","b3a486f2":"markdown","d49b437b":"markdown","9613516d":"markdown","889b5f88":"markdown","d04fbc3c":"markdown","e5df8be5":"markdown","823226a4":"markdown","76954f38":"markdown","dd4fb931":"markdown","5ef3c178":"markdown","6dbd9b3e":"markdown","a2d62014":"markdown","de711275":"markdown","b51527f3":"markdown","96ea20b9":"markdown","0753d414":"markdown","c87d8916":"markdown","7986b04a":"markdown","8bcca4e8":"markdown","85e3882c":"markdown","0db7946f":"markdown","f8b2a556":"markdown","e9af4a13":"markdown","e5e2eb7b":"markdown","6a17f05f":"markdown","9cd3f0e8":"markdown","47d34f93":"markdown","54bc2b57":"markdown","59e4d226":"markdown","fb5db46c":"markdown","ba0732fb":"markdown","bc88b4ac":"markdown","4fbe3717":"markdown"},"source":{"8b0600e6":"#This may take a while. \n#Please wait until the kernel is idle before you continue.\n!pip install --upgrade bamboolib>=1.4.1\n\n#then RELOAD the page","d0d363fc":"# RUN ME AFTER YOU HAVE RELOADED THE BROWSER PAGE\n\n# Uncomment and run lines below\nimport bamboolib as bam\n","c67ea37b":"import tensorflow as tf","9de54dac":"tf.__version__","50545e59":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nsns.set_style('whitegrid')\n\n","bd915311":"train_original = pd.read_csv('..\/input\/train.csv')\ntest_original = pd.read_csv('..\/input\/test.csv')\nsubmission_example = pd.read_csv('..\/input\/gender_submission.csv')","7a529873":"submission_example.head()","5710b217":"train_original.head()","fbdc1db1":"test_original.head()","7f713465":"train = train_original.copy()\ntest = test_original.copy()","94bab2e8":"bam.glimpse(train)","fa94db55":"bam.glimpse(test)","7d2b7422":"data = pd.concat([train.drop(['Survived'], axis=1), test])\ndata\n# bamboolib live code export\ndata_age = data.groupby(['Pclass', 'Sex']).agg({'Age': ['min', 'max', 'mean', 'median']})\ndata_age.columns = ['_'.join(multi_index) for multi_index in data_age.columns.ravel()]\ndata_age = data_age.reset_index()\ndata_age","af635110":"train\n# bamboolib live code export\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 1)) & (train['Sex'].isin(['male'])), 'Age'] = 42\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 1)) & (train['Sex'].isin(['female'])), 'Age'] = 36\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 2)) & (train['Sex'].isin(['male'])), 'Age'] = 29.5\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 2)) & (train['Sex'].isin(['female'])), 'Age'] = 28\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 3)) & (train['Sex'].isin(['male'])), 'Age'] = 25\ntrain.loc[((train['Age'].isna()) & (train['Pclass'] == 3)) & (train['Sex'].isin(['female'])), 'Age'] = 22\ntrain\n","83e27db1":"test\n# bamboolib live code export\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 1)) & (test['Sex'].isin(['male'])), 'Age'] = 42\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 1)) & (test['Sex'].isin(['female'])), 'Age'] = 36\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 2)) & (test['Sex'].isin(['male'])), 'Age'] = 29.5\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 2)) & (test['Sex'].isin(['female'])), 'Age'] = 28\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 3)) & (test['Sex'].isin(['male'])), 'Age'] = 25\ntest.loc[((test['Age'].isna()) & (test['Pclass'] == 3)) & (test['Sex'].isin(['female'])), 'Age'] = 22\ntest","23c94a46":"train\n\n\n# bamboolib live code export\nsplit_df = train[\"Name\"].str.split(',', expand=True)\nsplit_df.columns = [f\"Name_{id_}\" for id_ in range(len(split_df.columns))]\ntrain = pd.merge(train, split_df, how=\"left\", left_index=True, right_index=True)\nsplit_df = train[\"Name_1\"].str.split('.', expand=True)\nsplit_df.columns = [f\"Name_1_{id_}\" for id_ in range(len(split_df.columns))]\ntrain = pd.merge(train, split_df, how=\"left\", left_index=True, right_index=True)\ntrain = train.drop(columns=['PassengerId', 'Name', 'Name_0', 'Name_1', 'Name_1_1', 'Name_1_2'])\ntrain\n# bamboolib live code export\ntrain = train.drop(columns=['Cabin'])\nsplit_df = train[\"Ticket\"].str.split(' ', expand=True)\nsplit_df.columns = [f\"Ticket_{id_}\" for id_ in range(len(split_df.columns))]\ntrain = pd.merge(train, split_df, how=\"left\", left_index=True, right_index=True)\ntrain = train.drop(columns=['Ticket', 'Ticket_1', 'Ticket_2'])\ntrain\n# bamboolib live code export\ntrain = train.rename(columns={'Name_1_0': 'Title'})\ntrain = train.rename(columns={'Ticket_0': 'Ticket'})\ntrain\n# bamboolib live code export\ntrain[\"Title\"] = train[\"Title\"].str.strip()\ntrain\n# bamboolib live code export\ntrain = pd.get_dummies(train, columns=['Sex'], drop_first=True, dummy_na=False)\ntrain\n# bamboolib live code export\ntrain = train.loc[~(train['Embarked'].isna())]\ntrain","d786db85":"test\n# bamboolib live code export\nsplit_df = test[\"Name\"].str.split(',', expand=True)\nsplit_df.columns = [f\"Name_{id_}\" for id_ in range(len(split_df.columns))]\ntest = pd.merge(test, split_df, how=\"left\", left_index=True, right_index=True)\nsplit_df = test[\"Name_1\"].str.split('.', expand=True)\nsplit_df.columns = [f\"Name_1_{id_}\" for id_ in range(len(split_df.columns))]\ntest = pd.merge(test, split_df, how=\"left\", left_index=True, right_index=True)\ntest = test.drop(columns=['PassengerId', 'Name', 'Cabin', 'Name_0', 'Name_1', 'Name_1_1'])\nsplit_df = test[\"Ticket\"].str.split(' ', expand=True)\nsplit_df.columns = [f\"Ticket_{id_}\" for id_ in range(len(split_df.columns))]\ntest = pd.merge(test, split_df, how=\"left\", left_index=True, right_index=True)\ntest = test.drop(columns=['Ticket', 'Ticket_1', 'Ticket_2'])\ntest = test.rename(columns={'Ticket_0': 'Ticket'})\ntest = test.rename(columns={'Name_1_0': 'Title'})\ntest\n# bamboolib live code export\ntest = pd.get_dummies(test, columns=['Sex'], drop_first=True, dummy_na=False)\ntest\n# bamboolib live code export\ntest[\"Title\"] = test[\"Title\"].str.strip()\ntest","5e228725":"test['Fare'].fillna(value=7)","cf812a9f":"test\n# bamboolib live code export\ntest.loc[test['Fare'].isna(), 'Fare'] = 7\ntest","f3123b45":"data_Title = pd.concat([train, test])\ndata_Title","7de632fc":"print(\"train df legth :{}\".format(len(train)))\nprint(\"test df legth :{}\".format(len(test)))\nprint(\"data_Title df legth :{}\".format(len(data_Title)))","09a9a8e4":"data_Title['Title'].nunique(), data_Title['Title'].unique()","929aaaa9":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","93417947":"data_Title['Title']= le.fit_transform(data_Title['Title'])","646adfea":"data_Title\n# bamboolib live code export\ndata_Title = data_Title.drop(columns=['Ticket'])\ndata_Title","4b6ca5fe":"data_Title['Embarked'] = data_Title['Embarked'].astype(str, errors='raise')","658d33a8":"em = LabelEncoder()\ndata_Title['Embarked']= em.fit_transform(data_Title['Embarked'])","f7c7a846":"data_Title.info()","a9580eed":"train = data_Title.iloc[:889,:]\ntest = data_Title.iloc[889:,:]","9442b68f":"train","683edec9":"test\n# bamboolib live code export\ntest = test.drop(columns=['Survived'])\ntest","fc527b25":"X = train.drop('Survived',axis=1)\ny = train['Survived']","ca77d8c0":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X,y,test_size=0.15,\n                                                  random_state=101)\nx_test = test","b966ad7f":"x_train.keys()","df44220f":"categorical_vars = ['Pclass','Sex_male','SibSp','Parch','Embarked','Title']\nnumerical_vars=['Age','Fare']","c9b59c3d":"for c in train[categorical_vars]:\n    \n    print(c +\" : {}\".format(train[c].nunique()))","23901916":"X_train = [] #create empty arrays\nX_val = []\nX_test = []","9f0d797c":"for cat in categorical_vars:# append the data from every single category\n    X_train.append(x_train[cat].values)\n    X_val.append(x_val[cat].values)\n    X_test.append(x_test[cat].values)","34a6ee3b":"#standardize the numerical variables\nfrom sklearn.preprocessing import StandardScaler\n\n\nss = StandardScaler()\n\nx_train_num = x_train[numerical_vars].astype('float32').values\nx_val_num = x_val[numerical_vars].astype('float32').values\nx_test_num = x_test[numerical_vars].astype('float32').values\n\nX_scaled_train = ss.fit_transform(x_train_num)\nX_scaled_val = ss.transform(x_val_num)\nX_scaled_test = ss.transform(x_test_num)","794fd551":"X_train.append(X_scaled_train) #append continues values\nX_val.append(X_scaled_val)\nX_test.append(X_scaled_test)","fe0134db":"X_train","0faf5f57":"#size of embeddings\ncat_sizes = {}#empty list for category size and embedding size\ncat_embsizes = {}","6893a911":"for cat in categorical_vars:\n    cat_sizes[cat] = x_train[cat].nunique()\n    cat_embsizes[cat] = min(50, cat_sizes[cat]\/\/2+1)#rule for embedding","1e6a3eb4":"#imports\nfrom keras.layers import Dense, Dropout, Embedding, Input, Reshape, Concatenate\nfrom keras.models import Model","b702978e":"inputs = []\nconcat = []","6ffb3a89":"#create inputs and concatenate for categorical vars\nfor cat in categorical_vars:\n    x = Input((1,), name=cat)\n    inputs.append(x)\n    x = Embedding(cat_sizes[cat]+1, cat_embsizes[cat], input_length=1)(x)\n    x = Reshape((cat_embsizes[cat],))(x)\n    concat.append(x)","be9f767d":"#create inputs and concatenate for continuous variables\nnumerical_inputs = Input((len(numerical_vars),), name='continuous_vars')\ninputs.append(numerical_inputs)\nconcat.append(numerical_inputs)","9651f0aa":"y = Concatenate()(concat)","5886bb09":"#add the deep NN model and compile\np=0.2\ny = Dense(128, activation= 'relu')(y)\ny = Dropout(p)(y)\ny = Dense(64, activation= 'relu')(y)\ny = Dropout(p)(y)\ny = Dense(32, activation= 'relu')(y)\ny = Dropout(p)(y)\n\ny = Dense(1, activation='sigmoid')(y)\nmodel = Model(inputs=inputs, outputs=y)","963a0e5d":"model.compile(optimizer='adam', loss='binary_crossentropy')","ac8d3f47":"#callbacks earlystopping and checkpoint\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\n#checkpoint = ModelCheckpoint('best_test_embed.h5', monitor='val_loss',\n                             #save_best_only=True, verbose=2)\n\nearly_stopping = EarlyStopping(monitor='loss',mode='min', patience=10)","02bdfba7":"from keras.utils import plot_model","d28ff9d9":"plot_model(model,to_file='model.png',\n           show_shapes=True,\n           show_layer_names=True,\n           rankdir='TB')\nfrom IPython.display import Image\nImage('..\/working\/model.png')","5f46c49b":"model.fit(X_train, y_train, epochs=1000), \n          #validation_data=[X_val, y_val])#,\n          #callbacks=[early_stopping])#,checkpoint","55bdf820":"X_train","24fce33b":"sns.countplot(x='SibSp', data=train, palette='OrRd_r')","3b13830b":"train['Fare'].hist(bins=50, figsize=(12,6), color='darkred')","bad5ebc0":"import cufflinks as cf\ncf.go_offline()","4ad9ed05":"train['Fare'].iplot(kind='hist',bins=50, color='darkred',title='Fare')","6b882c86":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Pclass',y='Age',data=train, palette='OrRd_r')","3db80a45":"train[train['Pclass']==1]['Age'].mean()","2476d6f4":"train[train['Pclass']==2]['Age'].mean()","e6424ab1":"train[train['Pclass']==3]['Age'].mean()","9f98eb59":"# create a function\n\ndef impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 38\n        \n        elif Pclass == 2:\n            return 30\n        \n        elif Pclass == 3:\n            return 25\n        \n    else:\n        return Age\n        ","dead4b24":"train['Age'] = train[['Age','Pclass']].apply(impute_age, axis=1)\ntest['Age'] = test[['Age','Pclass']].apply(impute_age, axis=1)","66522cd0":"sns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='OrRd')","ee13f0b8":"sns.heatmap(test.isnull(),yticklabels=False, cbar=False, cmap='OrRd_r')","9a12d11d":"train.drop('Cabin', axis=1,inplace=True)\ntest.drop('Cabin', axis=1,inplace=True)","6a192446":"sns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='OrRd')","57cf2635":"sns.heatmap(test.isnull(),yticklabels=False, cbar=False, cmap='OrRd_r')","21aa2f6b":"train['Fare'].mean()","2964ce5d":"test[test['Fare'].isnull()] = 32","99dfe866":"sns.heatmap(test.isnull(),yticklabels=False, cbar=False, cmap='OrRd_r')","841793be":"train.dropna(inplace=True)","76e4083d":"sns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='OrRd')","fd6e17aa":"sns.heatmap(test.isnull(),yticklabels=False, cbar=False, cmap='OrRd_r')","328599d8":"train.head()","032ff5f1":"test.head()","ff018699":"# Store target variable of training data in a safe place\nsurvived_train = train.Survived","32a1db60":"survived_train.head()","d3cc5513":"# Concatenate training and test sets\ndata = pd.concat([train.drop(['Survived'], axis=1), test])","b3465572":"data.info()","a36e3807":"data.Name\n","1f0e4c72":"# Extract Title from Name, store in column and plot barplot\n#data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n#re.search\n#sns.countplot(x='Title', data=data);\n#plt.xticks(rotation=45);","a6110c90":"pd.get_dummies(train['Sex'], drop_first=True).head()","6539254f":"sex_train = pd.get_dummies(train['Sex'], drop_first=True)\nsex_train.head()","7eae986e":"pd.get_dummies(test['Sex'], drop_first=True).head()","facb64b5":"sex_test = pd.get_dummies(test['Sex'], drop_first=True)\nsex_test.head()","ad7a518e":"embark_train = pd.get_dummies(train['Embarked'],drop_first=True)\nembark_test = pd.get_dummies(test['Embarked'],drop_first=True)\n\nembark_train.head()","d876ce27":"embark_test.head()","7e8c050f":"train = pd.concat([train,sex_train,embark_train], axis=1)\ntest = pd.concat([test,sex_test,embark_test], axis=1)\n\ntrain.head()","2ca69b51":"sns.violinplot('Pclass','Age', hue='Survived', data=train,split=True)","0c7c1e3b":"train.drop(['PassengerId','Name','Sex','Ticket','Embarked'], axis=1, inplace=True)\ntrain.head()","7da30c63":"test.drop(['PassengerId','Name','Sex','Ticket','Embarked'], axis=1, inplace=True)\ntest.head()","43290ea5":"test.drop(['female','C'], axis=1, inplace=True)\ntest.head()","7c2fb1a1":"X = train.drop('Survived',axis=1)\ny = train['Survived']","66145f7d":"from sklearn.model_selection import train_test_split","a703f3fe":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11 )","709c12fb":"from sklearn.linear_model import LogisticRegression","1fd8eea5":"logmodel = LogisticRegression()","2228acb4":"logmodel.fit(X_train,y_train)","90ee90fe":"predictions_logmodel = logmodel.predict(X_test)","55c8f432":"from sklearn.metrics import classification_report","6a5a7369":"print(classification_report(y_test,predictions_logmodel))","4ce9c721":"from sklearn.metrics import confusion_matrix","a1a4c581":"confusion_matrix(y_test,predictions_logmodel)","c80395f1":"sns.heatmap(confusion_matrix(y_test,predictions_logmodel),cmap='OrRd_r',annot=True)","4d874056":"from sklearn.tree import DecisionTreeClassifier","f8779e47":"dtree = DecisionTreeClassifier()","471a3a72":"dtree.fit(X_train,y_train)","0761adfb":"prediction_d3 = dtree.predict(X_test)","d4153521":"print(classification_report(y_test,prediction_d3))\nprint(confusion_matrix(y_test,prediction_d3))","fefb17be":"sns.heatmap(confusion_matrix(y_test,prediction_d3),cmap='OrRd_r',annot=True)","1d7a34d2":"from sklearn.ensemble import RandomForestClassifier","e4e0ba49":"rfc = RandomForestClassifier(n_estimators=200)","47dd14cc":"rfc.fit(X_train,y_train)","57e08abc":"prediction_rfc = rfc.predict(X_test)","e1b2127a":"print(classification_report(y_test,prediction_rfc))\nprint(confusion_matrix(y_test,prediction_rfc))","a9dfb0b0":"sns.heatmap(confusion_matrix(y_test,prediction_rfc),cmap='OrRd_r',annot=True)","8a3079fc":"error_rate = []\nindex_error = []\n\nfor i in range(1,200):\n    \n    rfc = RandomForestClassifier(n_estimators=i)\n    \n    rfc.fit(X_train,y_train)\n    pred_i = rfc.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    index_error.append(i)","1f04bb5f":"error_rate_df = pd.DataFrame({'n_estimators': index_error,\n                                     'error_rate': error_rate})\nerror_rate_df.head()","49d119fd":"error_rate_df[error_rate_df['error_rate'].min()]['n_estimators']\ndf[df['A']==5].index.item()","b62e24bd":"plt.figure(figsize=(10,6))\nplt.plot(range(1,200),error_rate,color='orange', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. n_estimators')\nplt.xlabel('n_estimators')\nplt.ylabel('Error Rate')","1680eb98":"rfc = RandomForestClassifier(n_estimators=39)","a60a867b":"rfc.fit(X_train,y_train)","c2feb5f3":"prediction_rfc = rfc.predict(X_test)","91a8dd2e":"print(classification_report(y_test,prediction_rfc))\nprint(confusion_matrix(y_test,prediction_rfc))","a09862e7":"sns.heatmap(confusion_matrix(y_test,prediction_rfc),cmap='OrRd_r',annot=True)","1d4c3b40":"rfc = RandomForestClassifier(n_estimators=10)\nrfc.fit(X,y)\nprediction_rfc_kaggle = rfc.predict(test)","0bd5cb1a":"kaggle_submission = pd.DataFrame({\"PassengerId\": test_original[\"PassengerId\"],\n                                     \"Survived\": prediction_rfc_kaggle})","8519d5a4":"kaggle_submission.head()","5cd6e95b":"kaggle_submission.to_csv(\"kaggle_submission.csv\", index=False)","5d431329":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz","2737b458":"# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(64, \n                activation='relu',  \n                input_dim=8,\n                kernel_initializer='uniform'))\n\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.50))\n\n#adding second hidden layer \nmodel.add(Dense(32,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#With such a scalar sigmoid output on a binary classification problem, the loss\n#function you should use is binary_crossentropy\n\n#Visualizing the model\nmodel.summary()","88277823":"X_train.head()","3338deed":"#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.01, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, \n               batch_size = 60, \n               epochs = 300, verbose=1)","c62b0d51":"prediction_NN = model.predict(X_test)","fcb33adf":"def positive_sol(x):\n    if x>0.5:\n        return 1\n    else:\n        return 0\n    \npred = prediction_NN.apply(lambda x: positive_sol )","a85e99e0":"print(classification_report(y_test,prediction_NN))\nprint(confusion_matrix(y_test,prediction_NN))","f3f62760":"**Choosing a n_estimators Value**\n* Let's go ahead and use the elbow method to pick a good n_estimators Value:","fc1d2fb4":"* ### RELOAD THE BROWSER PAGE NOW","19fd9342":"the difference between the two dataset is only the 'Survived' column","a8677200":"# Decision Tree","e83da29f":"###### now we are going to impute the valuse of average age per class in the missing data of age","c5b53648":"we split numerical and categorical vars","763c2cfb":"###### now let's create a new DataFrame with only the answer and save it ","23dfe557":"###### this is the age distribution per class\n","b3a486f2":"###### Visualise the missing data from the diffent categories\n","d49b437b":"###### now we can add those new column to the df with concat\n","9613516d":"###### if now we check again the missing values we see that the column has been removed","889b5f88":"Examine the train and test dataset:","d04fbc3c":"# ML part","e5df8be5":"also here we drop one column because if it's not one of the two is then the third one","823226a4":"# Kaggle Titanic competition\nfirst competition","76954f38":"###### our dataframe is now clear from missing values","dd4fb931":"\n# ML model creation\n","5ef3c178":"and we assign this column to a new df","6dbd9b3e":"some age data missing and most of cabin data","a2d62014":"prepare the categorical data to feed","de711275":"###### if we feed this data to the model we have the problem that one column\nis the perfect predictor of the other column this is called 'Multicolinearity'\nso we get rid of one column with 'drop_first=True'","b51527f3":"Standardize the numerical vars","96ea20b9":"###### we drop the columns that we dont need for ML, we use only numerical data","0753d414":"## 3. EDA","c87d8916":"same missing data for the test df. Age and Cabin","7986b04a":"## 2. Import the train set","8bcca4e8":"###### change the valuse in the Age column applying th funtion","85e3882c":"# NN implementation\n","0db7946f":"###### now we drop any other missing values from the dataframe","f8b2a556":"###### most of the people were singles 0\n###### or had a spouse probably 1\n###### and very few brother and sisters","e9af4a13":"This is the format to submit for the Kaggle competition","e5e2eb7b":"* ## 1. Import the libraries","6a17f05f":"### number of siblings or spouse on board","9cd3f0e8":"We are now going to extract the Title from the names and drop the passenger_id column","47d34f93":"# Solution to send to Kaggle","54bc2b57":"# Logistic regression","59e4d226":"number of unique vars in the train df","fb5db46c":"### Create a new merged DF ","ba0732fb":"# Random Forest","bc88b4ac":"###### we need to create dummy values for categorical fatures e.g. sex category or embarked place","4fbe3717":"##### we are going to replace the median age in the missing values"}}