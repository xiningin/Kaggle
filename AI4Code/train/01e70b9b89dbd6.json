{"cell_type":{"81cbb2c9":"code","28fb84f0":"code","1ff03f7e":"code","2f9b1124":"code","88712326":"code","7b47f5e3":"code","ac812886":"code","8b9a11aa":"code","eb7ee27a":"code","3a3bc7c7":"code","6a9e16b1":"code","b9dcacf3":"code","c60dd141":"code","4c15cb58":"code","e8355f71":"code","ab909b35":"code","0ed354e6":"code","125d04d3":"code","75b717aa":"code","2127e41a":"code","ad556dba":"code","6ed384f5":"code","ea08e7a4":"code","7a067937":"code","b6a0b3cf":"code","46780175":"code","c1f1f55b":"code","7db70cfb":"code","f76b24f8":"code","dc87cd78":"code","aef6f8cc":"code","3d4160bb":"code","c8dd699a":"code","523dcf11":"code","6b8590b8":"code","fa78d1a3":"code","5517ae1a":"code","923fb1be":"code","67149ee4":"code","0cc1f877":"markdown","dea4c7c5":"markdown","fe2f58e7":"markdown","c1cf40f2":"markdown","dc7cec84":"markdown","eb2c3c74":"markdown","29b16c8f":"markdown","c5e092cd":"markdown","a7f17bb1":"markdown","57492c01":"markdown","96f30220":"markdown","33783cef":"markdown","3af1c228":"markdown","878e56db":"markdown","253363f5":"markdown","ca50f2f6":"markdown","714040fb":"markdown","dead7935":"markdown","cc997fbf":"markdown","283cd943":"markdown","0e86190b":"markdown","7ec82359":"markdown","2168316a":"markdown","cf1ffbd3":"markdown","bd02867f":"markdown","a6cc3185":"markdown","e5e2d86a":"markdown","9304a98f":"markdown","e59055b7":"markdown","b0286a56":"markdown","32fd6952":"markdown","824d92be":"markdown","88c79516":"markdown","c2600f8e":"markdown","2a1a941d":"markdown","c41cd58e":"markdown","fdbb03ad":"markdown","b4d33e79":"markdown","3d94a87b":"markdown","e3d9458d":"markdown","3321e0fb":"markdown","e7532c16":"markdown","6575098f":"markdown"},"source":{"81cbb2c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28fb84f0":"#importing libraries for dataset operations \nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statistics\n\n#importing modules for statistical operations and predictions\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n","1ff03f7e":"train_data= pd.read_csv('..\/input\/titanic\/train.csv') #this will be used to train the model\ntest_data= pd.read_csv('..\/input\/titanic\/test.csv') #this will be used to predict final values\ngender_data= pd.read_csv('..\/input\/titanic\/gender_submission.csv') #sample prediction ","2f9b1124":"train_data.head()\nprint(\"Training Dataset\")","88712326":"test_data.head()\nprint(\"Dataset which is to be used for predictions\")","7b47f5e3":"gender_data.head()\nprint(\"Sample submission\")","ac812886":"train_data_vs=train_data.drop(['Name','Ticket','Cabin'],axis=1)","8b9a11aa":"male_s= train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nprec_male_s= (sum(male_s)\/len(male_s))*100\n\nfemale_s= train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nprec_female_s= (sum(female_s)\/len(female_s))*100\n#classifying dataon the basis of gender","eb7ee27a":"#a piechart will help to visulaise this data in a perfect manner\nmy_data = [prec_female_s,(100-prec_female_s)]\nmy_labels = 'Female Survival percentage','Female Fatality Percentage'\nmy_colors=['lightpink','silver']\nplt.pie(my_data, labels=my_labels, autopct='%1.1f%%', startangle=15, shadow = True, colors=my_colors)\nplt.title('Female Survival')\nplt.axis('equal')\nplt.show()\nmy_data = [prec_male_s,(100-prec_male_s)]\nmy_labels = 'Male Survival Percentage','Male Fatality Percentage'\nmy_colors=['lightblue','silver']\nplt.pie(my_data, labels=my_labels, autopct='%1.1f%%', startangle=15, shadow = True, colors=my_colors)\nplt.title('Male Survival')\nplt.axis('equal')\nplt.show()","3a3bc7c7":"pclass_1= train_data.loc[train_data.Pclass == 1][\"Survived\"]\nprec_pclass_1= (sum(pclass_1)\/len(pclass_1))*100\n\npclass_2= train_data.loc[train_data.Pclass == 2][\"Survived\"]\nprec_pclass_2= (sum(pclass_2)\/len(pclass_2))*100\n\npclass_3= train_data.loc[train_data.Pclass == 3][\"Survived\"]\nprec_pclass_3= (sum(pclass_3)\/len(pclass_3))*100\n#classifying data on the basis of Cabin Class Type","6a9e16b1":"#I used Bar Plot in order to compare values\nsns.set_theme(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(6, 15))\nax. set(xlabel=\"Class\", ylabel=\"Survival Chance %\")\n\n# Loads data of mortality amongst classes\nmortality = [prec_pclass_3,prec_pclass_1,prec_pclass_2]\n\n# Plots the mortality distribution\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=train_data[\"Pclass\"].unique(),y=mortality,\n            label=\"Total\", color=\"r\")\n\n#sns.despine(left=True, bottom=True)\nplt.show()\n","b9dcacf3":"imputation=SimpleImputer(strategy='most_frequent')\nimputation.fit(train_data_vs)\nimputed_X_train = pd.DataFrame(imputation.transform(train_data_vs))\nimputed_X_train.columns = ['PassengerId', 'Survived', 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\nimputed_X_train","c60dd141":"\nsur=imputed_X_train[\"Survived\"] \nsns.displot(sur, x=imputed_X_train[\"Age\"], shrink=1)\n \n# Show plot\nplt.show()","4c15cb58":"ls=len(imputed_X_train.loc[imputed_X_train.Embarked=='S'][\"Survived\"])\nlc=len(imputed_X_train.loc[imputed_X_train.Embarked=='C'][\"Survived\"])\nlq=len(imputed_X_train.loc[imputed_X_train.Embarked=='Q'][\"Survived\"])\ns=sum(imputed_X_train.loc[imputed_X_train.Embarked=='S'][\"Survived\"])\/ls\nc=sum(imputed_X_train.loc[imputed_X_train.Embarked=='C'][\"Survived\"])\/lc\nq=sum(imputed_X_train.loc[imputed_X_train.Embarked=='Q'][\"Survived\"])\/lq\nsns.barplot(x=imputed_X_train[\"Embarked\"].unique(),y=[ls,lc,lq],palette='deep')\nplt.xticks(rotation=0)\nplt.xlabel('Embarkment Type')\nplt.ylabel('Survivor Count')\nplt.title('Embarked vs Survivor Count')\nplt.show()","e8355f71":"sns.barplot(x=imputed_X_train[\"Embarked\"].unique(),y=[s*100,c*100,q*100],palette='terrain_r')\nplt.xticks(rotation=0)\nplt.xlabel('Embarkment Type')\nplt.ylabel('Survivor Percentage')\nplt.title('Embarked vs Survivor Percentage')\nplt.show()","ab909b35":"\nplt.figure(figsize=(10, 8))\nsns.heatmap(train_data_vs.corr(), annot= True, cmap='cividis')","0ed354e6":"creamy_layer=statistics.mean(imputed_X_train[\"Fare\"])\nprint(\"The rich must've paid more than: $\",creamy_layer,\"\\nWhile the majority paid something around: $\",statistics.median(imputed_X_train[\"Fare\"]) )","125d04d3":"#changing Gender's textual values of passengers into numerical values\nimputed_X_train.loc[imputed_X_train.Sex=='female','Sex']=1\nimputed_X_train.loc[imputed_X_train.Sex=='male','Sex']=0\nimputed_X_train[\"Sex\"] = imputed_X_train[\"Sex\"].astype(str).astype(float)\n","75b717aa":"#changing embarked values to numerical ones\nimputed_X_train.loc[imputed_X_train.Embarked =='S','Embarked']= 3\nimputed_X_train.loc[imputed_X_train.Embarked =='C','Embarked']=2\nimputed_X_train.loc[imputed_X_train.Embarked =='Q','Embarked']=1\nimputed_X_train[\"Embarked\"] = imputed_X_train[\"Embarked\"].astype(str).astype(float)","2127e41a":"r_women=imputed_X_train[(imputed_X_train['Sex']==1) & (imputed_X_train['Fare']>34.21)]\np_men=imputed_X_train[(imputed_X_train['Sex']==0) & (imputed_X_train['Fare']<14.45)]\nr_women['r_women'] = 1\nr_women = r_women[['PassengerId','r_women']]\n\np_men['p_men'] = 1\np_men = p_men[['PassengerId','p_men']]\nmerged_feature= pd.merge(imputed_X_train, r_women, how='left', on='PassengerId')\nmergen_feature= pd.merge(merged_feature, p_men, how='left', on='PassengerId')\nfin_train_data=mergen_feature.fillna(0)\nfin_train_data = fin_train_data.drop(['PassengerId'],axis=1)\n","ad556dba":"fin_train_data.head()","6ed384f5":"X=fin_train_data.drop('Survived',axis=1)\ny=fin_train_data[\"Survived\"]\n\nstd_scal = StandardScaler()\nx = std_scal.fit_transform(X)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=34)","ea08e7a4":"\n#Decision Tree Classifier\ndtc_m = DecisionTreeClassifier( max_depth=3, random_state=42)\ndtc_m.fit(X_train,y_train)\npredict_dtc = dtc_m.predict(X_valid)\ndtc_score = accuracy_score(y_valid, predict_dtc)\nprint(dtc_score)","7a067937":"xgbc_m = XGBClassifier(max_depth= 4, n_estimators= 199, random_state= 0,use_label_encoder=False, learning_rate= 0.01, n_jobs=2)\nxgbc_m.fit(X_train,y_train)\npredict_xgbc = xgbc_m.predict(X_valid)\nxgbc_score = accuracy_score(y_valid, predict_xgbc)\nprint(xgbc_score)","b6a0b3cf":"rfc_m = RandomForestClassifier(n_estimators=250,min_samples_leaf=0.18, random_state=42)\nrfc_m.fit(X_train,y_train)\npredict_rfc = rfc_m.predict(X_valid)\nrfc_score = accuracy_score(y_valid, predict_rfc)\nprint(rfc_score)","46780175":"knc_m = KNeighborsClassifier(n_neighbors=22, leaf_size=10)\nknc_m.fit(X_train,y_train)\npredict_knc = knc_m.predict(X_valid)\nknc_score = accuracy_score(y_valid, predict_knc)\nprint(knc_score)","c1f1f55b":"adabc_m = AdaBoostClassifier(learning_rate= 0.01,n_estimators= 250,random_state=1)\nadabc_m.fit(X_train,y_train)\npredict_adabc = adabc_m.predict(X_valid)\nadabc_score = accuracy_score(y_valid, predict_adabc)\nprint(adabc_score)","7db70cfb":"comparison = pd.DataFrame({\n    'Model':['DecisionTreeClassifier','XGBClassifier', 'RandomForestClassifier' ,'KNeighborsClassifier', 'AdaBoostClassifier'],\n    'Accuracy_score' :[dtc_score, xgbc_score, rfc_score, knc_score, adabc_score]\n})","f76b24f8":"sns.barplot(x=comparison['Model'],y=comparison['Accuracy_score'],palette='winter')\nplt.xticks(rotation=60)\nplt.xlabel('Model Type')\nplt.ylabel('Accuracy Score')\nplt.title('Model and Accuracy')\nplt.show()\ncomparison","dc87cd78":"test = test_data.drop(['Name','Ticket','Cabin'],axis=1)\nimputation = SimpleImputer( strategy='most_frequent') \nimputation.fit(test)\nimp_test = pd.DataFrame(imputation.transform(test))\nimp_test.columns = ['PassengerId', 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']","aef6f8cc":"r_women=imp_test[(imp_test['Sex']=='female') & (imp_test['Fare']>34.21)]\np_men=imp_test[(imp_test['Sex']=='male') & (imp_test['Fare']<14.45)]\nr_women['r_women'] = 1\nr_women = r_women[['PassengerId','r_women']]\np_men['p_men'] = 1\np_men = p_men[['PassengerId','p_men']]","3d4160bb":"merged_test= pd.merge(imp_test, r_women, how='left', on='PassengerId')\nmergen_test= pd.merge(merged_test, p_men, how='left', on='PassengerId')\nfin_test_data=mergen_test.fillna(0)\nfin_test_data.head()","c8dd699a":"fin_test_data.loc[fin_test_data.Embarked =='S','Embarked']= 3\nfin_test_data.loc[fin_test_data.Embarked =='C','Embarked']=2\nfin_test_data.loc[fin_test_data.Embarked =='Q','Embarked']=1\nfin_test_data[\"Embarked\"] = fin_test_data[\"Embarked\"].astype(str).astype(float)\nfin_test_data.head()","523dcf11":"fin_test_data = fin_test_data.drop(['PassengerId'],axis=1)\nfin_test_data.head()","6b8590b8":"fin_test_data.loc[fin_test_data.Sex=='female','Sex']=1\nfin_test_data.loc[fin_test_data.Sex=='male','Sex']=0\nfin_test_data[\"Sex\"] = fin_test_data[\"Sex\"].astype(str).astype(float)\nfin_test_data.head()","fa78d1a3":"X_test = std_scal.fit_transform(fin_test_data)\n","5517ae1a":"xgbc_m.fit(X,y)\nfinal_prediction=xgbc_m.predict(X_test)\n","923fb1be":"prediction_sub = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': final_prediction})\nprediction_sub.head()","67149ee4":"prediction_sub.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0cc1f877":"# Model Fitting\nSplitting Data for traing and testing\nGetting the data ready for model fiiting ","dea4c7c5":"## Seeking trend in age and survival\n**As i managed to fill the missing Age values, now i can get back to Visualise it**","fe2f58e7":"The useful columns have been assigned numerical values in order to be helpful whie making predictions","c1cf40f2":"## **Using XG Boost Classifier as the final predictor**","dc7cec84":"# Creating CSV File of predictions","eb2c3c74":"# Drawing Conclusions:\n* **Females were more likely to survive as compared to males**\n* **Rich people were more likely to survive**\n> ### Hence one can say Rich Women were the most likely to survive. While Poor Men were amongst the people who were most likely to get succumbed by the disaster.","29b16c8f":"After going through all the data visualisations, we can observe that people who paid **hifh-fares**, were **females**, or belonged to **C\/Q** embarkment","c5e092cd":"As i was trying to plot mortality rate with respect to age, i found out it has nan values, hence preprpcessing the data first.","a7f17bb1":"> # Ada Boost Classifier and It's Accuracy Score","57492c01":"> # K Neighbours Classifier and It's Accuracy Score","96f30220":"# Titatnic Disaster\n## Here is an outline of how i will process the data and utilise its entities as information.\n1. Exploration of Dataset: What all entites does it has. \n2. Identification of Information amongst values.\n3. Reading Columns and its values in order to decide what all needs to be dropped.\n4. Data Visualisation: Summarising Data in form of graphs, and drawing conclusions from it.\n5. Imputation: Dropping all the columns which have majorly null values and changing the values of textual data which can be used for prediction.\n6. Deploying Various ML Models in order to get predictions. \n7. Identifying the bet suitable Model for this Dataset.\n8. Submission Of Final Scores","33783cef":"## Reading the dataset in order to explore:","3af1c228":"## Observing Survival Trend on the basis of Embarkment\nHere the distribution of survivors amongst different embarkment type have been put under observation in order to seek a pattern from the same","878e56db":"> # XG Boost Classifier and It's Accuracy Score","253363f5":"> **Dropping Passenger ID as it has no role in prediction**","ca50f2f6":"> # Random Forest Classifier and It's Accuracy Score","714040fb":"# Data Visualisation","dead7935":"## **Comparing Models' Accuracy and identifying the best fit**","cc997fbf":"Now one can clearly see that people from **S** were least likley to survive","283cd943":"## Mortality on the basis of wealth\n","0e86190b":"> **Preprocessing test.csv**","7ec82359":"> **Substituting Numerical values in \"Embarked\"**","2168316a":"**Observing the death ratio amongst varios cabin classes mentioned in \"Pclass\" column**","cf1ffbd3":"# Finding Correlation in the various columns ","bd02867f":"## Getting basic idea of what the data sets look like","a6cc3185":"**Now we are able to conclude that people in their 20s were the most amongst survivors**","e5e2d86a":"> **Adding Features**","9304a98f":"## Importing Libraries:","e59055b7":"# Preprocessing Data\n## This was done in order to fill the missing values in the Age column\nHere i am choosing the \"**strategy**\" as \"**most_frequent**\" in order to let **Median** values fill the missing ones while perfoming imputation using **SimpleImputer()**","b0286a56":"**Hence, it can be observed that females onboard were more likely to survive the disaster.**","32fd6952":"## Dropping Columns which are not relevant in terms of predicting values","824d92be":"**Conclusion**: XG Boost Classifier is the best performing model as compared to others here","88c79516":"Observing the **Fare** distribution, one can see that the majority paid something near to **14.46** while the **Creamy Layer** paid more than **32.2**, hence people who were travelling in **Q\/C classes** ","c2600f8e":"> **Substituting Numerical values in \"Sex\"**","2a1a941d":"# Predicting Final Submission Values","c41cd58e":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)","fdbb03ad":"> # Decision Tree Classifier and It's Accuracy Score","b4d33e79":"* **After going through this graph, one can directly say that people who were travelling First class were more likely to survive as compared to that of Second and Third class**\n","3d94a87b":"**Clearly visible that people from embarkement \"S\" were the majority amongst the survivors but the reality was different**","e3d9458d":"# Adding Self-Tailored Features \n**Based on the conclusions drawn earlier, i have decided to add two columns. These are classified on the basis of gender and the fare they paid**\n* > Rich Women (r_women)\n* > Poor Men (p_men)\n ","3321e0fb":"Preparing Test Dataset for predictions......","e7532c16":"## Preprocing Data \n> Now that trends have been identified, and conclusions have been drawn. Getting back to preprocessing of data in order to increase its usability.","6575098f":"## Classification of survivors based on Gender"}}