{"cell_type":{"6ec7261a":"code","c37ba2c4":"code","22bfa70f":"code","7d40182c":"code","08cacd6f":"code","d32d61d6":"code","22f89734":"code","3bb798cc":"code","83179616":"code","2fdadbaf":"code","7e68d987":"code","a94e3376":"code","010c8436":"code","d6527f7c":"code","46e13883":"code","3610221b":"code","e59d5b42":"code","fd2ad753":"code","34551a2d":"code","58fc7a48":"code","8bf88e46":"code","e33fbd45":"code","92d4227f":"code","1286d044":"code","dd2a9db9":"code","cb46e96c":"code","ed72da3e":"code","ee747d52":"code","73f15165":"code","a0879299":"code","bd80046b":"code","34b96671":"code","fb582044":"markdown","446a7d21":"markdown","a4b150c6":"markdown","34f05191":"markdown","01ddc760":"markdown","cf32b53a":"markdown","33632082":"markdown","a0c56f1f":"markdown","3ab0dd15":"markdown","dafce8d3":"markdown","120778af":"markdown","b90a5723":"markdown","da644620":"markdown","44c7bd11":"markdown","40e599d6":"markdown","08de83fc":"markdown","2ee43136":"markdown","444abf91":"markdown","bc7eb81a":"markdown","6a3be2a7":"markdown","8686a7e4":"markdown","bf6b62b2":"markdown","85a0e44f":"markdown","8d924b9f":"markdown","aace2a89":"markdown","a2866ea5":"markdown","5f0db8e3":"markdown","664a1c22":"markdown","f77302e6":"markdown","32f4de9b":"markdown"},"source":{"6ec7261a":"# Libraries for Data Analysis\nimport numpy as np\nimport pandas as pd\n# Libraries for Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c37ba2c4":"# Loading the dataset from datasets module of sklearn package\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(cancer.keys())","22bfa70f":"# Creating features dataframe\ndf_feature = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n# Creating target dataframe\ndf_target = pd.DataFrame(cancer.target, columns=['cancer'])\n# Concatenating dataframe\ndf = pd.concat([df_feature, df_target], axis=1)","7d40182c":"df.head()","08cacd6f":"df.info()","d32d61d6":"df.describe()","22f89734":"df.iloc[:,:-1].hist(figsize=(20,15), edgecolor='black')\nplt.show()","3bb798cc":"sns.set_style('whitegrid')\nsns.countplot(df.cancer)","83179616":"plt.figure(figsize=(10,8))\nx_axis = df.iloc[:,:-1].corrwith(df.cancer).values\ny_axis = df.iloc[:,:-1].corrwith(df.cancer).index\nplt.barh(y_axis, x_axis)\nplt.title('correlation with target(cancer)', fontsize=20)","2fdadbaf":"# Fixing the problem of imbalanced data by stratifying the data.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('cancer',axis=1), df.cancer, stratify=df.cancer, random_state=66)","7e68d987":"# Scaling the data for KNN model\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","a94e3376":"# Finding the best value for n_neighbors parameter\nfrom sklearn.neighbors import KNeighborsClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor i in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_scaled, y_train)\n    train_accuracy.append(knn.score(X_train_scaled, y_train))\n    test_accuracy.append(knn.score(X_test_scaled, y_test))\n\nplt.figure(figsize=(10,5))\nplt.plot(range(1,11), train_accuracy, label='train_accuracy')\nplt.plot(range(1,11), test_accuracy, label='test_accuracy')\nplt.legend()\nplt.xlabel('n_neighbors')\nplt.ylabel('accuracy')\n\nscore = pd.DataFrame({'n_neighbors':range(1,11),'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('n_neighbors')\nscore.transpose()","010c8436":"# Training the final model\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train_scaled,y_train)\nknn_score = knn.score(X_test_scaled, y_test)","d6527f7c":"from sklearn.linear_model import LogisticRegression\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [0.001,0.01, 0.1, 1, 100]:\n    logreg = LogisticRegression(C=i).fit(X_train, y_train)\n    train_accuracy.append(logreg.score(X_train, y_train))\n    test_accuracy.append(logreg.score(X_test, y_test))\n\nscore = pd.DataFrame({'C':[0.001,0.01, 0.1, 1, 100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('C').transpose()\nscore","46e13883":"# Building the best model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_score = logreg.score(X_test, y_test)","3610221b":"from sklearn.svm import LinearSVC\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [0.001, 0.01, 0.1, 1, 100]:\n    lsvc = LinearSVC(C=i).fit(X_train, y_train)\n    train_accuracy.append(lsvc.score(X_train, y_train))\n    test_accuracy.append(lsvc.score(X_test, y_test))\n    \npd.DataFrame({'C':[0.001,0.01,0.1,1,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('C').transpose()","e59d5b42":"# Building the best model\nlsvc = LinearSVC(C=0.001).fit(X_train, y_train)\nlsvc_score = lsvc.score(X_test, y_test)","fd2ad753":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(X_train, y_train)\ngnb_score = gnb.score(X_test,y_test)\nprint('train_accuracy: {}'.format(gnb.score(X_train, y_train)))\nprint('test_accuracy: {}'.format(gnb_score))","34551a2d":"from sklearn.tree import DecisionTreeClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [1,2,3,10,100]:\n    tree = DecisionTreeClassifier(max_depth=i).fit(X_train, y_train)\n    train_accuracy.append(tree.score(X_train, y_train))\n    test_accuracy.append(tree.score(X_test, y_test))\n    \npd.DataFrame({'max_depth':[1,2,3,10,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('max_depth').transpose()","58fc7a48":"from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file='tree_limited.dot', feature_names = X_train.columns,\n                class_names = cancer.target_names,\n                rounded = True, proportion = False, precision = 2, filled = True)\n\n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\nfrom IPython.display import Image\nImage(filename = 'tree_limited.png')","8bf88e46":"plt.figure(figsize=(10,8))\nplt.barh(X_train.columns, tree.feature_importances_)","e33fbd45":"# Building our final model\ntree = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\ntree_score = tree.score(X_test, y_test)","92d4227f":"from sklearn.ensemble import RandomForestClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [5, 20, 50, 75, 100]:\n    forest = RandomForestClassifier(n_estimators=i, random_state=43).fit(X_train, y_train)\n    train_accuracy.append(forest.score(X_train, y_train))\n    test_accuracy.append(forest.score(X_test, y_test))\n    \npd.DataFrame({'n_estimator':[5,20,50,75,100], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('n_estimator').transpose()","1286d044":"plt.figure(figsize=(10,8))\nplt.barh(X_train.columns, forest.feature_importances_)","dd2a9db9":"# Building our final model\nforest = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\nforest_score = forest.score(X_test, y_test)","cb46e96c":"from sklearn.ensemble import GradientBoostingClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [0.001,0.01,0.1,1]:\n    boost = GradientBoostingClassifier(learning_rate=i).fit(X_train, y_train)\n    train_accuracy.append(boost.score(X_train, y_train))\n    test_accuracy.append(boost.score(X_test, y_test))\n    \npd.DataFrame({'learning_rate':[0.001,0.01,0.1,1], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('learning_rate').transpose()","ed72da3e":"# Building the best model\nboost = GradientBoostingClassifier(learning_rate= 0.1).fit(X_train, y_train)\nboost_score = boost.score(X_test, y_test)\nplt.figure(figsize=(10,8))\nplt.barh(X_train.columns, boost.feature_importances_)","ee747d52":"from sklearn.svm import SVC\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [1, 10 ,100 ,1000]:\n    svc = SVC(C=i).fit(X_train, y_train)\n    train_accuracy.append(svc.score(X_train, y_train))\n    test_accuracy.append(svc.score(X_test, y_test))\n    \npd.DataFrame({'C':[1,10,100,1000], 'train_accuracy':train_accuracy,'test_accuracy':test_accuracy}).set_index('C').transpose()","73f15165":"# Building the best model\nsvc = SVC(C=1000).fit(X_train, y_train)\nsvc_score = svc.score(X_test, y_test)","a0879299":"from sklearn.neural_network import MLPClassifier\ntrain_accuracy = []\ntest_accuracy = []\nfor i in [[10],[10,10], [20,20]]:\n    mlp = MLPClassifier(activation='tanh', random_state=0, hidden_layer_sizes=i).fit(X_train, y_train)\n    train_accuracy.append(mlp.score(X_train, y_train))\n    test_accuracy.append(mlp.score(X_test, y_test))\n    \npd.DataFrame({'hidden_layers':['10','10,10','20,20'], 'train_accuracy':train_accuracy, 'test_accuracy':test_accuracy}).set_index('hidden_layers').transpose()","bd80046b":"# Building the best model\nmlp = MLPClassifier(activation='tanh', random_state=0, hidden_layer_sizes=[20,20]).fit(X_train, y_train)\nmlp_score = mlp.score(X_test, y_test)","34b96671":"scores = pd.DataFrame({'model':['LogisticRegression', 'LinearSVM', 'DecisionTree','RandomForest','GradientBoosting',\n                      'KernelSVM','NeuralNetwork','NaiveBayes'], 'accuracy':[logreg_score, lsvc_score, tree_score, forest_score,\n                                                               boost_score, svc_score, mlp_score, gnb_score]})\nscores.sort_values(by ='accuracy', ascending=False)","fb582044":"# Random Forest Model","446a7d21":"Above graph shows the correlation of features with target values","a4b150c6":"# Linear SVM Model","34f05191":"We can not show the decision process of random forest because there are so much tree involved in this model. Above graph show that which features are important to our model and how much.","01ddc760":"# Naive Bays ","cf32b53a":"Target values are a little imbalanced. We will fix this problem when splitting the dataset.","33632082":"We will use stratigy parameter to fix the problem of imbalance dataset","a0c56f1f":"# Kernel SVM Model","3ab0dd15":"There are not so much parameter to tune in case of Naive Bayes to using the baseline model of Naive bayes model.","dafce8d3":"# Decision Tree Model","120778af":"# Building model","b90a5723":"# Basic Imports","da644620":"# Splitting the dataset into train and test","44c7bd11":"Above graph shows that which features our tree model finds important and how much.","40e599d6":"So far the best models with parameter tuning are Random Forest and Gradient Boosting","08de83fc":"# MLP Classifier","2ee43136":"KNN works well when the data is scaled.","444abf91":"# Best Model","bc7eb81a":"Above graph shows which number of n_neighbors is best for best accuracy. In this case it is 7.","6a3be2a7":"# Data Import","8686a7e4":"Above graph shows the feature importance for this model.","bf6b62b2":"There is 31 columns and none of them has missing values.","85a0e44f":"This notebook is about exploring all important and popular classification machine learning alogrithms and tuning there most important parameters to found out their performance on Breast Cancer Dataset from sklearn.datasets.\nThe ML algorithms used are as follows.\n\n1. K-Nearest Neighbors\n2. Logistic Regression\n3. Linear SVM\n4. Naive Bayes\n5. Decision Tree\n6. Rendom Forest\n7. Gradient Boosting\n8. Kernel SVM\n9. Neural Network from sklearn (MLPClassifier)","8d924b9f":"Dataset loaded in dictionary format","aace2a89":"# Logistic Regression Model","a2866ea5":"# Gradient Boosting Model","5f0db8e3":"Above graph shows the decision making process of decision tree model","664a1c22":"# Quick look at data","f77302e6":"# Exploratory Data Analysis","32f4de9b":"# KNN Model"}}