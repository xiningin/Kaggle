{"cell_type":{"c159b17b":"code","bb1cebb8":"code","e58efecc":"code","e61be996":"code","ea962ba4":"code","47c46231":"code","3dfe6fa4":"code","b0ff427d":"code","a919e16d":"code","608daf6f":"code","6f7283cd":"code","735eea69":"code","fd75459e":"code","cd039422":"code","cc171111":"code","cb2574db":"code","9d3f8fd8":"code","b2100f4c":"code","23fd51cc":"code","7ad3b5e4":"code","7b680612":"code","465ac7eb":"code","3d3b4aae":"code","8163c4cd":"code","045c180f":"code","aad2553a":"code","9d6b153d":"code","4676c9d9":"code","5cc9f8d9":"code","6e96f2f4":"code","b9ae04a0":"code","968464e0":"code","87835df8":"code","f2e789af":"code","2f1f3f79":"code","129ad13a":"code","21ceb89b":"code","c53cad0b":"code","245313bf":"code","b404e955":"code","61b92112":"code","3bb0d0c9":"code","3df5bce9":"code","9bd53069":"code","463f3a3b":"code","0b99efeb":"code","812435ff":"code","98276976":"code","ca53079f":"code","83471bf2":"code","3b0dca7a":"code","b12383ba":"code","75b0d5b5":"code","a95d0bf1":"code","821e2bc2":"code","cff65759":"code","69a845bc":"code","7352cab9":"code","7f9d2a59":"code","79fa0705":"code","8bda58db":"code","44c8ff6a":"code","f25de468":"code","2e5fbb07":"code","5bd39bc8":"code","dbb546de":"code","558cd073":"code","573410a3":"code","dd840623":"markdown","d8689119":"markdown","2fde3c4f":"markdown","1eaba8ec":"markdown","97f3d05a":"markdown","4043987d":"markdown","88ab7601":"markdown","f0dae6a2":"markdown","696e05e0":"markdown","3a4a3867":"markdown","cbf99427":"markdown","6ea38bbf":"markdown","28402ff1":"markdown","832f70ce":"markdown","b0ed1659":"markdown","864b7c4d":"markdown","0db645f8":"markdown","13be4266":"markdown","7187537c":"markdown","e021a098":"markdown","28fb45b6":"markdown","61a5cd66":"markdown","7f8efbcf":"markdown","82dcb33f":"markdown","4b4b9fb4":"markdown","70235c3d":"markdown","b2f0527f":"markdown","92bb5cfe":"markdown"},"source":{"c159b17b":"#Importing Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","bb1cebb8":"#Loading the dataset\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv') \ndf_test  = pd.read_csv('..\/input\/titanic\/test.csv')","e58efecc":"#Viewing the dataset\n\ndisplay(df_train.head())\ndisplay()\ndisplay(df_test.head())","e61be996":"#Preprocessing needs to be done on both test and train data. So we will concatenate both test and train data.\n#Creating a new column in test data\ndf_test['Survived'] = 999      #Assigning some random value to the test dataset","ea962ba4":"#Now both test and train can be concatenated\n\ndf = pd.concat([df_train , df_test] , axis = 0)\nprint(df_train.shape)\nprint(df_test.shape)\nprint('Combined dataframe shape :',df.shape)","47c46231":"df.info()","3dfe6fa4":"plt.style.use('ggplot')\nprint('Null Values :',df['Pclass'].isnull().sum())  #To find the number of null values\ndf['Pclass'].value_counts(sort = True).plot(kind = 'barh' ,title = 'Passenger Class')","b0ff427d":"#Null values\nprint('The number of null values in age columns',df['Age'].isnull().sum())\nprint('The % of null values in age columns',round(df['Age'].isnull().mean()* 100,2)) ","a919e16d":"#Age column has lots of null values. \n#Around 20 % of the values are missing.","608daf6f":"df['Age'].plot(kind='box')\nprint('Range of age is between',df['Age'].min() , 'and' ,df['Age'].max())","6f7283cd":"#Eventhough the box plot shows outlier we wont remove any outliers in age column , because maximum age of the person in titanic was 80","735eea69":"#Null values\nprint('The number of null values in cabin columns',df['Cabin'].isnull().sum())\nprint('The % of null values in cabin columns',round(df['Cabin'].isnull().mean()* 100,2)) ","fd75459e":"#Dropping Cabin column\ndf.drop(columns = 'Cabin' , inplace = True)","cd039422":"df['Embarked'].value_counts().plot(kind=  'bar' , rot = 0 )","cc171111":"print('The number of null values in Embarked is ', df['Embarked'].isnull().sum())","cb2574db":"#NaN values are replaced with Southampton\ndf['Embarked'].replace({np.nan:'S'} , inplace = True)","9d3f8fd8":"#Only one value of fare is missing.\ndf[df['Fare'].isnull()]","b2100f4c":"plt.figure(figsize = (10,4))\nplt.subplot(1,2,1)\nsns.distplot(df['Fare'])\nplt.subplot(1,2,2)\nsns.boxplot(df['Fare'])","23fd51cc":"df['Name'].isnull().sum()\n#There are no missing values in name","7ad3b5e4":"df['Name'].head()\n#Name is useless feature , but Title in the name can give some additional information for prediction","7b680612":"def GetTitle_temp(name):\n    fname_title = name.split(',')[1]\n    title = fname_title.split('.')[0]\n    title = title.strip().lower()\n    return title\n\ndf.Name.map(GetTitle_temp).value_counts()","465ac7eb":"def GetTitle(name):\n    titles = {'mr' : 'Mr', \n               'mrs' : 'Mrs', \n               'miss' : 'Miss', \n               'master' : 'Master',\n               'don' : 'Mr',\n               'rev' : 'Mr',\n               'dr' : 'Mr',\n               'mme' : 'Mrs',\n               'ms' : 'Mrs',\n               'major' : 'Mr',\n               'lady' : 'Miss',\n               'sir' : 'Mr',\n               'mlle' : 'Miss',\n               'col' : 'Mr',\n               'capt' : 'Mr',\n               'the countess' : 'Miss',\n               'jonkheer' : 'Mr',\n               'dona' : 'Miss'\n                 }\n    fname_title = name.split(',')[1]\n    title = fname_title.split('.')[0]\n    title = title.strip().lower()\n    return titles[title]\n\ndf['Name'] = df.Name.map(GetTitle)","3d3b4aae":"sns.countplot(df['Name'])","8163c4cd":"df['Parch'].value_counts()","045c180f":"df['SibSp'].value_counts()","aad2553a":"#Combining Sibsp and Parch columns to form number of people accompanying\ndf['Accomp'] = df['SibSp'] + df['Parch']\ndf.drop(columns = ['SibSp' , 'Parch'] , inplace = True)","9d6b153d":"df['Sex'].value_counts()","4676c9d9":"sns.countplot(df['Sex'])","5cc9f8d9":"#Encoding the Sex column\ndf['Sex'] = df['Sex'].map({'female':0 , 'male':1 })","6e96f2f4":"#Dropping the ticket number\ndf.drop(columns = ['Ticket'] , inplace = True)","b9ae04a0":"#Viewing the Data\ndf.head()","968464e0":"cat = pd.get_dummies(df[['Embarked' , 'Name']] , drop_first=True)","87835df8":"df = pd.concat([df,cat] , axis = 1)\ndf.drop(columns = ['Embarked' , 'Name'] , inplace = True)    #Dropping the original columns","f2e789af":"#After Encoding\ndf.head()","2f1f3f79":"#Checking the percentage of missing values:\ndf.isnull().mean()*100","129ad13a":"import missingno as msno\nmsno.matrix(df)","21ceb89b":"msno.heatmap(df)   #Heatmap to find any correlation of missing values between other missing columns\n#From this heatpmap we can see that there is no correlation between missing values","c53cad0b":"msno.dendrogram(df)","245313bf":"#Imputing using KNN : \n\nfrom fancyimpute import KNN\nknn_imputer = KNN()\ndf_knn = df.copy()\ndf_knn.iloc[:,:] = knn_imputer.fit_transform(df_knn)","b404e955":"#Imputing using MICE\n\nfrom fancyimpute import IterativeImputer\nMICE_imputer = IterativeImputer()\ndf_mice = df.copy()\ndf_mice.iloc[:,:] = knn_imputer.fit_transform(df_mice)","61b92112":"sns.kdeplot(df['Age'] , c = 'r' , label = 'No imputation')\nsns.kdeplot(df_knn['Age'] , c = 'g' , label = 'KNN imputation')\nsns.kdeplot(df_mice['Age'] , c = 'b' , label = 'MICE imputation')\nsns.kdeplot(df['Age'].fillna(df['Age'].mean()) , c = 'k' , label = 'Fillna_Mean')\n#Distribution of the columns are maintained while using this fancy imputation techniques\n#The black kde plot shows how distribution when when we fill the null values with the mean value","3bb0d0c9":"df_mice.head()","3df5bce9":"df_mice.Survived.unique()","9bd53069":"#Spiltting Test and Train Datas:\ndfm_train = df_mice[df_mice['Survived'] != 999]\ndfm_test  = df_mice[df_mice['Survived'] == 999]\n\nprint('Train Shape :',dfm_train.shape)\nprint('Test Shape :',dfm_test.shape)\n\ndfm_test.drop(columns = 'Survived' , inplace = True)","463f3a3b":"dfm_train.head()","0b99efeb":"import xgboost as xgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX, y = dfm_train.drop(columns = ['Survived','PassengerId']) , dfm_train['Survived']\nX_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=123)\nxg_cl = xgb.XGBClassifier(objective='binary:logistic',\nn_estimators=20, seed=123)\n\nxg_cl.fit(X_train, y_train)\npreds = xg_cl.predict(X_test)\naccuracy = float(np.sum(preds==y_test))\/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","812435ff":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test , preds)","98276976":"y_pred =  xg_cl.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_pred})\nresults.to_csv('Titanic Prediction_XGB.csv', index=False)","ca53079f":"import xgboost as xgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nX, y = dfm_train.drop(columns = ['Survived','PassengerId']) , dfm_train['Survived']\nX_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=123)\n\n# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 3,5,7 , 10],\n        'gamma': [0.5, 1, 1.5, 2,3,4, 5],\n        'subsample': [0.6,0.7, 0.8,0.9, 1.0],\n        'colsample_bytree': [0.6,0.7, 0.8,0.9, 1.0],\n        'max_depth': [3, 4, 5 , 6]\n        }\n\nxgb = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\nrandom_search.fit(X, y)\n","83471bf2":"random_search.best_params_","3b0dca7a":"y_test = random_search.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_test})\nresults.to_csv('Titanic Prediction_XGB_hp.csv', index=False)\n\n#Got final score of 0.79904","b12383ba":"#Spiltting Test and Train Datas:\ndfm_train = df_knn[df_mice['Survived'] != 999]\ndfm_test  = df_knn[df_mice['Survived'] == 999]\n\nprint('Train Shape :',dfm_train.shape)\nprint('Test Shape :',dfm_test.shape)\n\ndfm_test.drop(columns = 'Survived' , inplace = True)","75b0d5b5":"import xgboost as xgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nX, y = dfm_train.drop(columns = ['Survived','PassengerId']) , dfm_train['Survived']\nX_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=123)\n\n# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 3,5,7 , 10],\n        'gamma': [0.5, 1, 1.5, 2,3,4, 5],\n        'subsample': [0.6,0.7, 0.8,0.9, 1.0],\n        'colsample_bytree': [0.6,0.7, 0.8,0.9, 1.0],\n        'max_depth': [3, 4, 5 , 6]\n        }\n\nxgb = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\nrandom_search.fit(X, y)\n","a95d0bf1":"y_test = random_search.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_test})\nresults.to_csv('Titanic Predictionkn.csv', index=False)\n\n#Got a score of 0.7666","821e2bc2":"#Spiltting Test and Train Datas:\ndfm_train = df_mice[df_mice['Survived'] != 999]\ndfm_test  = df_mice[df_mice['Survived'] == 999]\n\nprint('Train Shape :',dfm_train.shape)\nprint('Test Shape :',dfm_test.shape)\n\ndfm_test.drop(columns = 'Survived' , inplace = True)","cff65759":"#KNN randomized search\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn = KNeighborsClassifier()\n\nparams = {\n    'n_neighbors' : sp_randint(1 , 20) ,\n    'p' : sp_randint(1 , 5) ,\n}\n\nrsearch_knn = RandomizedSearchCV(knn , param_distributions = params , cv = 3 , random_state= 3  , n_jobs = -1 , return_train_score=True)\n\nrsearch_knn.fit(X , y)","69a845bc":"rsearch_knn.best_params_","7352cab9":"#Random Forest randomized search\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(random_state=3)\nparams = { 'n_estimators' : sp_randint(50 , 200) , \n           'max_features' : sp_randint(1 , 12) ,\n           'max_depth' : sp_randint(2,10) , \n           'min_samples_split' : sp_randint(2,20) ,\n           'min_samples_leaf' : sp_randint(1,20) ,\n           'criterion' : ['gini' , 'entropy']\n    \n}\n\nrsearch_rfc = RandomizedSearchCV(rfc , param_distributions= params , n_iter= 200 , cv = 3 , scoring='roc_auc' , random_state= 3 , return_train_score=True , n_jobs=-1)\n\nrsearch_rfc.fit(X,y)","7f9d2a59":"rsearch_rfc.best_params_","79fa0705":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver = 'liblinear')\nknn = KNeighborsClassifier(**rsearch_knn.best_params_)\nrfc = RandomForestClassifier(**rsearch_rfc.best_params_)\n\nclf = VotingClassifier(estimators=[('lr' ,lr) , ('knn' , knn) , ('rfc' , rfc)] , voting = 'soft')\n\nclf.fit(X , y)","8bda58db":"y_test = clf.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_test})\nresults.to_csv('Titanic Prediction_Stack.csv', index=False)\n","44c8ff6a":"X, y = dfm_train.drop(columns = ['Survived','PassengerId']) , dfm_train['Survived']\nX_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=123)","f25de468":"RFM = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=123,\n                                           n_jobs=-1,\n                                           verbose=1) \n\nRFM.fit(X,y)","2e5fbb07":"y_pred = RFM.predict(X_test)","5bd39bc8":"y_test = RFM.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_test})\nresults.to_csv('Titanic PredictionRFM.csv', index=False)\n","dbb546de":"X, y = dfm_train.drop(columns = ['Survived','PassengerId']) , dfm_train['Survived']\nX_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=123)","558cd073":"from sklearn.ensemble import ExtraTreesClassifier\nimport sklearn.model_selection as model_selection\n\nclf_ET = ExtraTreesClassifier(random_state=0, bootstrap=True, oob_score=True)\n\nsss = model_selection.StratifiedShuffleSplit(n_splits=10, test_size=0.33, random_state= 0)\nsss.get_n_splits(X, y)\n\nparameters = {'n_estimators' : np.r_[10:210:10],\n              'max_depth': np.r_[1:6]\n             }\n\ngrid = model_selection.GridSearchCV(clf_ET, param_grid=parameters, scoring = 'accuracy', cv = sss, return_train_score=True, n_jobs=4, verbose=2)\ngrid.fit(X,y)","573410a3":"y_test = RFM.predict(dfm_test.drop(columns='PassengerId')).astype('int')\n\nresults = pd.DataFrame(data={'PassengerId':dfm_test['PassengerId'].astype('int'), 'Survived':y_test})\nresults.to_csv('Titanic PredictionETC.csv', index=False)","dd840623":"**NAME : **","d8689119":"**Predicting the values on Test Dataset**","2fde3c4f":"**EMBARKED :**","1eaba8ec":"### **XGBoost with Hyperparameter Tuning using RandomizedSearchCV**","97f3d05a":"**CABIN : **\n\nC = Cherbourg, Q = Queenstown, S = Southampton","4043987d":"### **RANDOM FOREST :**","88ab7601":"**FANCYIMPUTE:**\n\nFancyimpute imputation techniques,\n\n1. KNN or K-Nearest Neighbor\n2. MICE or Multiple Imputation by Chained Equation\n\nKNN finds most similar points for imputing\n\nMICE performs multiple regression for imputing","f0dae6a2":"**MISSINGNO :**\n\nMissingno package is imported. It helps to easily visualize missing values\n\nMissing values can be visualized using 3 ways : \n\n1. Heatmap \n2. Finding Missing pattern using Matrix\n3. Dendrogram","696e05e0":"**Encoding Categorical Variables :**","3a4a3867":"**AGE**","cbf99427":"Importing missingno package to find the missing values and its patterns.\n\nMissing values can be classified into three types :\n\n1. Missing Completely at Random (MCAR)\n2. Missing at Random (MAR)\n3. Missing Not at Random (MNAR)\n\nIn our dataset we can find that Age column comes under the category of Missing completely at random\n","6ea38bbf":"### MISSING VALUE IMPUTATION :","28402ff1":"**Passenger Class**","832f70ce":"Around 77 % of the values are missing .So its better to drop Cabin column than imputing the values","b0ed1659":"**PARCH :**\n\nNumber of parents \/ children aboard the Titanic","864b7c4d":"**SEX : **","0db645f8":"**TICKET :**","13be4266":"Since most people embarked in Southampton , the two values will be imputed with Southampton.","7187537c":"### STACKING","e021a098":"# Preprocessing the data","28fb45b6":"**SibSp :**\n    \nNumber of siblings \/ spouses aboard the Titanic","61a5cd66":"**FARE : **","7f8efbcf":"There are no null values in the passenger class column .\n\nWe can see that most passengers travelled in 3rd Class , and 1st class comes second and least people travelled in second class.\n\nSo no preprocessing is required","82dcb33f":"Mice score was better compared to Knn","4b4b9fb4":"### Checking the accuracy on Fancyimpute (Knn) imputed dataframe :","70235c3d":"### Applying XGBoost, (Without Hyperparameter Tuning)","b2f0527f":"There are around 18 titles in the dataset. It can be combined to 4 titles Master,Mr,Miss and Mrs","92bb5cfe":"**EXTRA TREE CLASSIFIER :**"}}