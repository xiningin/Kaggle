{"cell_type":{"226f9aab":"code","79d3509e":"code","ea83e185":"code","b77ae512":"code","f77cef92":"code","e23985e7":"code","57e9ab55":"code","74c03ec7":"code","28123032":"code","9208dfa2":"markdown","c7647e0c":"markdown","a86bb619":"markdown","0f105cc3":"markdown","982f1c72":"markdown","11c5cee9":"markdown"},"source":{"226f9aab":"import os\nimport cv2\nimport glob\nimport imageio\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.animation import FuncAnimation\n\nfrom IPython.display import Image, display\n\n%matplotlib inline","79d3509e":"class InputVideo():\n    def __init__(self, video_file, video_name):\n        self.name_ = video_name\n        self.color_images_ = video_file['colorImages']\n        self.bounding_box_ = video_file['boundingBox']\n        self.landmarks_2D_ = video_file['landmarks2D']\n        self.landmarks_3D_ = video_file['landmarks3D']\n        \n    def name(self):\n        return self.name_\n    \n    def frames(self):\n        return self.color_images_\n    \n    def length(self):\n        return self.color_images_.shape[3]\n    \n    def frame(self, i):\n        return self.color_images_[:, :, :, i]\n    \n    def landmarks_2D(self):\n        return self.landmarks_2D_\n    \n    def landmarks_3D(self):\n        return self.landmarks_3D_\n    \n    def bounding_box(self):\n        return self.bounding_box_","ea83e185":"class HeadPoseDetector():\n    def detect(self, im, landmarks_2d, landmarks_3d):\n        h, w, c = im.shape\n        K = np.array([[w, 0, w\/2],\n                      [0, w, h\/2],\n                      [0, 0, 1]], dtype=np.double)\n        dist_coeffs = np.zeros((4,1)) \n\n        (_, R, t) = cv2.solvePnP(landmarks_3d, landmarks_2d, K, dist_coeffs)\n        return R, t, K, dist_coeffs","b77ae512":"base_path = '\/kaggle\/input\/youtube-faces-with-facial-keypoints\/'\ndf_videos = pd.read_csv(base_path + 'youtube_faces_with_keypoints_large.csv')\n\n# https:\/\/www.kaggle.com\/selfishgene\/exploring-youtube-faces-with-keypoints-dataset\n# Create a dictionary that maps videoIDs to full file paths\nnpz_file_paths = glob.glob(base_path + 'youtube_faces_*\/*.npz')\nvideo_ids = [x.split('\/')[-1].split('.')[0] for x in npz_file_paths]\n\nfull_video_paths = {}\nfor video_id, full_path in zip(video_ids, npz_file_paths):\n    full_video_paths[video_id] = full_path\n\n# Remove from the large csv file all videos that weren't uploaded yet\ndf_videos = df_videos.loc[df_videos.loc[:,'videoID']\n                          .isin(full_video_paths.keys()), :].reset_index(drop=True)","f77cef92":"num_samples = 3\nnp.random.seed(25)\n\nsample_indices = np.random.choice(df_videos.index, size=num_samples, replace=False)\nsample_video_ids = df_videos.loc[sample_indices, 'videoID']\n\nsample_videos = []\nfor i, video_id in enumerate(sample_video_ids):\n    sv = InputVideo(np.load(full_video_paths[video_id]), video_id)\n    sample_videos.append(sv)","e23985e7":"for i, video in enumerate(sample_videos):\n    title = video.name()\n    frames = [video.frame(f) for f in range(video.length())]\n    \n    kwargs_write = {'fps':1.0, 'quantizer':'nq'}\n    imageio.mimsave(f'.\/{title}_orig.gif', frames, fps=24)\n    display(Image(url=f'.\/{title}_orig.gif'))","57e9ab55":"def add_mask_overlay(overlay, landmarks_2D):\n    c = (10, 10, 10) # color\n    landmark_vertex_ids = [\n        3, 30, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4\n    ]\n    \n    vertices = np.array([[int(landmarks_2D[i-1,0]),\n                          int(landmarks_2D[i-1,1])] for i in landmark_vertex_ids])\n    \n    cv2.fillConvexPoly(overlay, np.int32([vertices]), color=c, lineType=cv2.LINE_AA)\n    \n    mask_link_left_1 = 15\n    mask_link_left_2 = 17\n    cv2.line(overlay,\n             (int(landmarks_2D[mask_link_left_1-1,0]),\n              int(landmarks_2D[mask_link_left_1-1,1])),\n             (int(landmarks_2D[mask_link_left_2-1,0]),\n              int(landmarks_2D[mask_link_left_2-1,1])),\n             color=c,\n             thickness=1)\n    \n    mask_link_right_1 = 1\n    mask_link_right_2 = 3\n    cv2.line(overlay,\n             (int(landmarks_2D[mask_link_right_1-1,0]),\n              int(landmarks_2D[mask_link_right_1-1,1])),\n             (int(landmarks_2D[mask_link_right_2-1,0]),\n              int(landmarks_2D[mask_link_right_2-1,1])),\n             color=c,\n             thickness=1)","74c03ec7":"def process_frame(frame, landmarks_2D):\n    processed_img = np.array(frame)\n\n    mask_overlay = np.zeros(processed_img.shape,dtype=np.uint8)\n    mask_overlay.fill(255)\n    add_mask_overlay(mask_overlay, landmarks_2D)\n    \n    # Smooth mask\n    blurred_img = cv2.GaussianBlur(mask_overlay, (21, 21), 0)\n    mask_mask = np.zeros(mask_overlay.shape, np.uint8)\n    gray_mask = cv2.cvtColor(mask_overlay, cv2.COLOR_BGR2GRAY)\n    thresh = cv2.threshold(gray_mask, 60, 255, cv2.THRESH_BINARY)[1]\n    contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cv2.drawContours(mask_mask, contours, -1, (255,255,255),5)\n    mask_overlay = np.where(mask_mask==np.array([255, 255, 255]), blurred_img, mask_overlay)\n    \n    # Apply overlay\n    processed_img = cv2.bitwise_and(processed_img, mask_overlay)\n        \n    return processed_img","28123032":"for i, video in enumerate(sample_videos):\n    title = video.name()\n    \n    processed_imgs = [\n        process_frame(video.frame(f),\n                      video.landmarks_2D()[:, :, f])\n                      for f in range(video.length())\n    ]\n\n    imageio.mimsave(f'.\/{title}_mask.gif', processed_imgs, fps=24)\n    display(Image(url=f'.\/{title}_mask.gif'))","9208dfa2":"For each frame I create a separate image with mask and then apply a bit of blur to its contours, to add extra smoothing on borders. The mask image has only black region on white background, so no additional contours will be detected. After processing mask image, I apply it to original frame with `cv2.bitwise_and` and return combined matrix.","c7647e0c":"This actually is not used in this notebook but could be used to estimate head pose from point correspondences (2D and 3D face points).","a86bb619":"InputVideo structure helps to organize video data from YouTube Faces dataframe. For each video of length $N$ frames we've got: $N$ images, $N$ bounding boxes (for face), $N \\times 68$ landmarks positions and $N \\times 68$ corresponding 3D object points of facial landmarks.","0f105cc3":"![](https:\/\/www.researchgate.net\/publication\/327500528\/figure\/fig9\/AS:668192443748352@1536320901358\/The-ibug-68-facial-landmark-points-mark-up.ppm)\n\nIn this notebook I play with face detection and applying masks to image. YouTube Faces dataset already provides detected 2D image keypoints (68 facial landmarks) and 3D object points for each frame in videos, so I didn't have to detect them manually. I use images and keypoints to localize the area where I want to apply the overlay and then fill the area with polygon using OpenCV. At the end, applying gaussian blur can make the mask look more smoothly.","982f1c72":"### Select sample images (videos) to process\nI'm selecting 3 pseudo-random images with fixed seed and loading them into `InputVideo` objects. In this notebook I use IPython and Image class to display image sequences as GIFs. Unfortunately I couldn't find any way to display them in-place in notebook, so I have to same them to my workspace and then display them from existing file.","11c5cee9":"### Display faces with maks overlay\n\nFunction `add_mask_overlay` adds mask overlay to existing frame. In *landmark_vertex_ids* list I hold IDs of consecutive points in a polygon that forms a mask region. I draw filled polygon with `cv2.fillConvexPoly` function and anti-aliasing lines. The mask also consists of two lines (going behind ears) that I draw separately using `cv2.line` calls."}}