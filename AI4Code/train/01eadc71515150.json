{"cell_type":{"cfac3a17":"code","b3966e42":"code","d495fe07":"code","e0d4ab49":"code","632c2229":"code","b8c548bf":"code","b06b4083":"code","fdfd3816":"code","75267a8b":"code","efe8745b":"code","a77447b9":"code","3456ab42":"code","8cdf49af":"code","39c26386":"code","fe597dfe":"code","0408cb33":"code","26184c57":"code","494fbd5f":"code","74d64201":"code","8d10e5d1":"code","70422370":"code","a16ff771":"code","3bea5fc5":"code","e57b1f04":"code","2ef8de26":"code","5fa732a4":"code","2398ef1d":"code","5a4507f5":"code","e2a995cb":"code","d979a061":"code","ca5a4fa4":"code","16680327":"code","3c2d7e65":"code","80af49bc":"code","7a5bbd4f":"code","f465644d":"code","5bdf5c25":"code","9647b92f":"code","3854d210":"code","545c36c5":"code","8ab9edc5":"code","78f33cae":"code","64ca5698":"code","584d7b8a":"code","63487f77":"code","49b0e0c4":"code","29c12c3b":"code","d2801eb4":"code","f6804871":"code","c15aa34f":"markdown","e2aaa542":"markdown","060c8306":"markdown","d13e752f":"markdown","01ab679d":"markdown","487195b6":"markdown","acecbab9":"markdown","7ad9a7bf":"markdown","d37b3c2a":"markdown","cc1ec017":"markdown","a1125ff4":"markdown","ed4c8cca":"markdown","072ba2d7":"markdown","94482ed5":"markdown"},"source":{"cfac3a17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import skew\nimport pylab \nimport itertools\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization\nfrom keras.layers.core import Dense, Flatten, Dropout, Lambda\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","b3966e42":"test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","d495fe07":"test_features.head()","e0d4ab49":"train_features.head()","632c2229":"print(train_features.shape[0])\nprint(test_features.shape[0])\n\nprint(train_features.shape[1])\nprint(test_features.shape[1])\n\ntrain_features.shape[0] \/ test_features.shape[0]","b8c548bf":"missing_vals_train  = train_features.isnull().sum() \/ train_features.shape[0]\nmissing_vals_train[missing_vals_train > 0].sort_values(ascending=False) ","b06b4083":"missing_vals_test  = test_features.isnull().sum() \/ test_features.shape[0]\nmissing_vals_test[missing_vals_test > 0].sort_values(ascending=False) ","fdfd3816":"train_features.info()","75267a8b":"test_features.info()","efe8745b":"train_features[['cp_time']] = train_features[['cp_time']].astype('object')\ntest_features[['cp_time']] = test_features[['cp_time']].astype('object')","a77447b9":"train_features_object = train_features.select_dtypes(include = ['object'])\ntest_features_object = test_features.select_dtypes(include = ['object'])","3456ab42":"print(train_features_object.shape)\ntrain_features_object.head()","8cdf49af":"train_features_object_group_cp_type = train_features_object.groupby('cp_type').aggregate({'sig_id': 'count'}).reset_index()\ntrain_features_object_group_cp_type['train\/test'] = ['train', 'train']\n\ntest_features_object_group_cp_type = test_features_object.groupby('cp_type').aggregate({'sig_id': 'count'}).reset_index()\ntest_features_object_group_cp_type['train\/test'] = ['test', 'test']\n\ngroup_cp_type = pd.concat([train_features_object_group_cp_type, test_features_object_group_cp_type])\ngroup_cp_type.head()","39c26386":"fig = px.bar(group_cp_type, x=\"cp_type\", y=\"sig_id\", color=\"train\/test\", title=\"cp_type\")\nfig.show()","fe597dfe":"train_features_object_group_cp_dose = train_features_object.groupby('cp_dose').aggregate({'sig_id': 'count'}).reset_index()\ntrain_features_object_group_cp_dose['train\/test'] = ['test', 'test']\n\ntest_features_object_group_cp_dose = test_features_object.groupby('cp_dose').aggregate({'sig_id': 'count'}).reset_index()\ntest_features_object_group_cp_dose['train\/test'] = ['train', 'train']\n\ngroup_cp_dose = pd.concat([train_features_object_group_cp_dose, test_features_object_group_cp_dose])\ngroup_cp_dose.head()","0408cb33":"fig = px.bar(group_cp_dose, x=\"cp_dose\", y=\"sig_id\", color=\"train\/test\", title=\"cp_dose\")\nfig.show()","26184c57":"train_features_object_group_cp_time = train_features_object.groupby('cp_time').aggregate({'sig_id': 'count'}).reset_index()\ntrain_features_object_group_cp_time['train\/test'] = ['train', 'train', 'train']\n\ntest_features_object_group_cp_time = test_features_object.groupby('cp_time').aggregate({'sig_id': 'count'}).reset_index()\ntest_features_object_group_cp_time['train\/test'] = ['test', 'test', 'test']\n\ngroup_cp_time = pd.concat([train_features_object_group_cp_time, test_features_object_group_cp_time])\ngroup_cp_time.head()","494fbd5f":"fig = px.bar(group_cp_time, x=\"cp_time\", y=\"sig_id\", color=\"train\/test\", title=\"cp_time\")\nfig.show()","74d64201":"train_features_number = train_features.select_dtypes(include = ['float64', 'int64'])\ntest_features_number = test_features.select_dtypes(include = ['float64', 'int64'])","8d10e5d1":"train_features_random = train_features_number[train_features_number.columns[\n    np.random.randint(0, train_features_number.shape[1], 10)]]\n\ntrain_features_random.hist(bins=40, figsize=(20,15))\nplt.show()","70422370":"skewed_feats = train_features_number.apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats_index = skewed_feats.index","a16ff771":"random_g_skewed_feats = train_features_number[skewed_feats_index][train_features_number[skewed_feats_index].columns[\n    np.random.randint(0, train_features_number[skewed_feats_index].shape[1], 10)]]","3bea5fc5":"random_g_skewed_feats.head()","e57b1f04":"sns.set(rc={'figure.figsize': (20, 10)})\nplt.subplot(251)\nstats.probplot(random_g_skewed_feats.iloc[:,1], dist=\"norm\", plot=pylab)\nplt.subplot (252)\nstats.probplot(random_g_skewed_feats.iloc[:,2], dist=\"norm\", plot=pylab)\nplt.subplot (253)\nstats.probplot(random_g_skewed_feats.iloc[:,3], dist=\"norm\", plot=pylab)\nplt.subplot (254)\nstats.probplot(random_g_skewed_feats.iloc[:,4], dist=\"norm\", plot=pylab)\nplt.subplot (255)\nstats.probplot(random_g_skewed_feats.iloc[:,5], dist=\"norm\", plot=pylab)\npylab.show()","2ef8de26":"colu = [[columns] * len(random_g_skewed_feats.iloc[:,ind]) for ind, columns in enumerate(random_g_skewed_feats.columns)]\n\nvalues_random_g_skewed_feats_sp_g = []\n\nfor columns_split_g in range(random_g_skewed_feats.shape[1]):\n    values_random_g_skewed_feats_sp_g.append(random_g_skewed_feats.iloc[:,columns_split_g])\n    \nd = {'g-': list(itertools.chain(*values_random_g_skewed_feats_sp_g)), 'indax_g': list(itertools.chain(*colu))}\ndf = pd.DataFrame(data=d)","5fa732a4":"df.head()","2398ef1d":"pca = PCA()\npca.fit(train_features_number[skewed_feats_index])\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1","5a4507f5":"plt.figure(figsize=(10,7))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 400, 0, 1])\nplt.xlabel(\"Dimensions\")\nplt.ylabel(\"Explained Variance\")\nplt.plot([d, d], [0, 0.95], \"k:\")\nplt.plot([0, d], [0.95, 0.95], \"k:\")\nplt.plot(d, 0.95, \"ko\")\nplt.show()","e2a995cb":"del train_features['sig_id']\ndel test_features['sig_id']\ndel train_features_object['sig_id']\ndel train_targets_scored['sig_id']","d979a061":"cat_attr = train_features_object.columns\n\nnum_attr = np.array(train_features_number.columns)   \nnum_attr = np.delete(num_attr, np.argmax(num_attr == np.array(skewed_feats_index)[:, np.newaxis], axis=1))\n\nn_skewed_PCA = np.random.randint(0, skewed_feats_index.shape, 200)\nskewed_feats_index = skewed_feats_index[n_skewed_PCA]","ca5a4fa4":"X_train, y_train, X_test, y_test = train_test_split(train_features, train_targets_scored, test_size=0.3, random_state=42)","16680327":"X_train = pd.DataFrame(X_train, columns=train_features.columns)\ny_train = pd.DataFrame(y_train, columns=train_features.columns)","3c2d7e65":"# Create a class to select numerical or categorical columns \nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attribute_names].values\n    \n    \n# Create a class to skewness numerical of a data set\nclass Skewness_numericalSelector(BaseEstimator, TransformerMixin):\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return np.exp2(X)","80af49bc":"old_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attr)),\n        (\"scaler\", StandardScaler())\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attr)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])\n\nskew_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(skewed_feats_index)),\n        (\"scalermm\", MinMaxScaler()),\n        ('PCA', PCA(n_components=200)),\n        ('skew_scaler', Skewness_numericalSelector())\n        \n    ])\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"cat_pipeline\", old_cat_pipeline),\n        (\"num_pipeline\", old_num_pipeline),\n        (\"skew_pipeline\", skew_num_pipeline)\n    ])\n","7a5bbd4f":"X_train = old_full_pipeline.fit_transform(X_train)\ny_train = old_full_pipeline.fit_transform(y_train)\ntest_features = old_full_pipeline.fit_transform(test_features)","f465644d":"!nvidia-smi","5bdf5c25":"param = {'n_estimators':  166,\n         'learning_rate': 0.0503,\n         'subsample': 0.8639,\n         'max_depth': 10, \n         'colsample_bytree': 0.6522,\n         'min_child_weight': 31,\n         'tree_method': 'gpu_hist' \n        }\n\nclassifier = MultiOutputClassifier(estimator = XGBClassifier(**param))\nclassifier.fit(X_train, X_test) ","9647b92f":"predictions_XGBC = classifier.predict_proba(test_features)\npredictions_XGBC = np.array(predictions_XGBC)[:,:,1].T","3854d210":"def ret(a):\n    return  a ","545c36c5":"#del model","8ab9edc5":"model= Sequential()\n\nmodel.add(Lambda(ret, input_shape = [795]))\n\nmodel.add(Dense(879, activation = 'relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(800, activation = 'relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(700, activation = 'relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(600, activation = 'relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(500, activation = 'elu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(400, activation = 'elu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation = 'elu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(206, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], \n              optimizer = tf.keras.optimizers.Adam(learning_rate=0.001))","78f33cae":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)","64ca5698":"model_fit = model.fit(X_train, X_test, validation_data=(y_train, y_test), epochs = 8, callbacks=[reduce_lr]) ","584d7b8a":"sns.set(rc={'figure.figsize': (15, 10)})\nplt.plot(model_fit.history['loss'], label='train')\nplt.plot(model_fit.history['val_loss'], label='test')\nplt.legend()\nplt.show()\nplt.plot(model_fit.history['accuracy'], label='train')\nplt.plot(model_fit.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","63487f77":"predictions_DNN = model.predict(test_features)","49b0e0c4":"predictions = np.mean([predictions_DNN, predictions_XGBC], axis=0)","29c12c3b":"sample_submission.iloc[:,1:] = predictions","d2801eb4":"sample_submission.head()","f6804871":"sample_submission.to_csv(\"submission.csv\", index=False, header=True) ","c15aa34f":"# Distribution of numerical data","e2aaa542":"# Categorical variables","060c8306":"# XGBClassifier","d13e752f":"# Model DNN","01ab679d":"# Missing values are missing","487195b6":"# Data loading","acecbab9":"# PCA","7ad9a7bf":"## cp_time","d37b3c2a":"Computes the skewness of a data set. For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution","cc1ec017":"## cp_type","a1125ff4":"# Pipeline","ed4c8cca":"## cp_dose","072ba2d7":"# Skewed data","94482ed5":"# predictions"}}