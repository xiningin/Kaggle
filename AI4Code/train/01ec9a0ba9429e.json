{"cell_type":{"844a3a6c":"code","d3698f22":"code","bf101032":"code","6fab9772":"code","07e3a8b6":"code","db512efe":"code","ceb1ee63":"code","b8e4f032":"code","cfbaad1a":"code","d50e132d":"code","e15a64a7":"code","67a1f442":"code","67bf5937":"code","6501646b":"code","a56be0ed":"code","c814ec84":"code","e5d32071":"code","8c8ee3a1":"code","3f1cb505":"code","b42b1c27":"code","e3666876":"code","e4249dcf":"code","cf6a1e89":"code","16072196":"code","26c252bc":"markdown","7e354e5d":"markdown","af1bf8bd":"markdown","15c12e72":"markdown","4e0a46b5":"markdown","c53f3b5b":"markdown","b77f4eec":"markdown","35036b9a":"markdown","d9f0e2a6":"markdown","5e9094b2":"markdown","a2e53b51":"markdown","4458c5f8":"markdown","3cee4068":"markdown","24ef5cb0":"markdown","48375e9d":"markdown","f6db2583":"markdown","306d669f":"markdown","c794841b":"markdown"},"source":{"844a3a6c":"\nimport pandas as pd\nimport io\n\n  \ndf=pd.read_csv(\"..\/input\/flight-take-off-data-jfk-airport\/M1_final.csv\")\nprint(df)","d3698f22":"#for i in range(df.shape[0]):\n # if()\n#print( type(df[0]))\ndf1=df.copy(deep=True)\nfor i in range(df.shape[0]):\n  s=str(df.iloc[[i],13])\n  for x in s :\n      if x =='\\xa0':\n        df1.drop(i,axis=0,inplace=True)\n        break\nfor i in range(df.shape[0]):\n  s=int(df.iloc[[i],13])\n  \ndata=df1.values\nX,Y=data[:,:-1],data[:,-1]\nprint(X.shape,Y.shape)\nfrom sklearn.model_selection import train_test_split","bf101032":"from sklearn import preprocessing as pre\n#import numpy as np\n\nlabel=pre.LabelEncoder()\nX[:,3]=label.fit_transform(X[:,3].astype(str))\nX[:,4]=label.fit_transform(X[:,4].astype(str))\nX[:,5]=label.fit_transform(X[:,5].astype(str))\nX[:,15]=label.fit_transform(X[:,15].astype(str))\nX[:,19]=label.fit_transform(X[:,19].astype(str))\nX[:,13]=label.fit_transform(X[:,13].astype(str))","6fab9772":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.10)\nprint(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)\n","07e3a8b6":"import numpy as np\nmsel=np.empty(8,dtype=float)\n","db512efe":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlreg=LinearRegression()\nlreg.fit(X_train,Y_train)\npred=lreg.predict(X_test)\nmsel[0]=mean_squared_error(Y_test,pred)**0.5\nprint(msel[0])","ceb1ee63":"from sklearn.linear_model import Ridge\nridgereg=Ridge(alpha=0.05, normalize=True)\nridgereg.fit(X_train,Y_train)\npred1=ridgereg.predict(X_test)\nmsel[1]=mean_squared_error(Y_test,pred1)**0.5\nprint(msel[1])","b8e4f032":"from sklearn.linear_model import Lasso\nlasreg=Lasso(alpha=0.05, normalize=True)\nlasreg.fit(X_train,Y_train)\npred2=lasreg.predict(X_test)\nmsel[2]=mean_squared_error(Y_test,pred2)**0.5\nprint(msel[2])","cfbaad1a":"from sklearn.neighbors import KNeighborsRegressor\nknnr=KNeighborsRegressor(n_neighbors=200)\nknnr.fit(X_train,Y_train)\npred3=knnr.predict(X_test)\nmsel[3]=mean_squared_error(Y_test,pred3)**0.5\nprint(msel[3])","d50e132d":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nsc1=StandardScaler()\nsc2=StandardScaler()\nx1=X_train.copy()\nx2=X_test.copy()\nx1=sc1.fit_transform(x1)\nx2=sc2.fit_transform(x2)\nsvr=SVR()\nsvr.fit(x1,Y_train)\npred4=svr.predict(x2)\nmsel[4]=mean_squared_error(Y_test,pred4)**0.5\nprint(msel[4])","e15a64a7":"from sklearn.ensemble import RandomForestRegressor\nrand=RandomForestRegressor()\nrand.fit(X_train,Y_train)\npred5=rand.predict(X_test)\nmsel[5]=mean_squared_error(Y_test,pred5)**0.5\nprint(msel[5])","67a1f442":"from lightgbm import LGBMRegressor\nlgbm=LGBMRegressor()\nlgbm.fit(X_train,Y_train)\npred6=lgbm.predict(X_test)\nmsel[6]=mean_squared_error(Y_test,pred6)**0.5\nprint(msel[6])","67bf5937":"from sklearn.linear_model import BayesianRidge\nbayreg=BayesianRidge()\nbayreg.fit(X_train,Y_train)\npred7=bayreg.predict(X_test)\nmsel[7]=mean_squared_error(Y_test,pred7)**0.5\nprint(msel[7])","6501646b":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nohe=OneHotEncoder()\ntrainx=X.copy()\n\ncols=[0,1,2,3,4,5,15,19]\n#hotenc=ohe.fit_transform(testx)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[0])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[3])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[34])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[41])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[51])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[125])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\ncoltrans=ColumnTransformer([(\"encoder\",OneHotEncoder(),[147])],remainder=\"passthrough\")\ntrainx=np.array(coltrans.fit_transform(trainx),dtype=np.int)\nprint(trainx.shape)\n\n#with np.printoptions(threshold=np.inf):\n #  print(trainx)","a56be0ed":"X_train,X_test,Y_train,Y_test=train_test_split(trainx,Y,test_size=0.10)\nprint(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)\nmseh=np.empty(8,dtype=float)","c814ec84":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlreg=LinearRegression()\nlreg.fit(X_train,Y_train)\npred=lreg.predict(X_test)\nmseh[0]=mean_squared_error(Y_test,pred)**0.5\nprint(mseh[0])","e5d32071":"from sklearn.linear_model import Ridge\nridgereg=Ridge(alpha=0.05, normalize=True)\nridgereg.fit(X_train,Y_train)\npred1=ridgereg.predict(X_test)\nmseh[1]=mean_squared_error(Y_test,pred1)**0.5\nprint(mseh[1])","8c8ee3a1":"from sklearn.linear_model import Lasso\nlasreg=Lasso(alpha=0.05, normalize=True)\nlasreg.fit(X_train,Y_train)\npred2=lasreg.predict(X_test)\nmseh[2]=mean_squared_error(Y_test,pred2)**0.5\nprint(mseh[2])","3f1cb505":"from sklearn.neighbors import KNeighborsRegressor\nknnr=KNeighborsRegressor(n_neighbors=200)\nknnr.fit(X_train,Y_train)\npred3=knnr.predict(X_test)\nmseh[3]=mean_squared_error(Y_test,pred3)**0.5\nprint(mseh[3])","b42b1c27":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nsc1=StandardScaler()\nsc2=StandardScaler()\nx1=X_train.copy()\nx2=X_test.copy()\nx1=sc1.fit_transform(x1)\nx2=sc2.fit_transform(x2)\nsvr=SVR()\nsvr.fit(x1,Y_train)\npred4=svr.predict(x2)\nmseh[4]=mean_squared_error(Y_test,pred4)**0.5\nprint(mseh[4])","e3666876":"from sklearn.ensemble import RandomForestRegressor\nrand=RandomForestRegressor()\nrand.fit(X_train,Y_train)\npred5=rand.predict(X_test)\nmseh[5]=mean_squared_error(Y_test,pred5)**0.5\nprint(mseh[5])","e4249dcf":"from lightgbm import LGBMRegressor\nlgbm=LGBMRegressor()\nlgbm.fit(X_train,Y_train)\npred6=lgbm.predict(X_test)\nmseh[6]=mean_squared_error(Y_test,pred6)**0.5\nprint(mseh[6])","cf6a1e89":"from sklearn.linear_model import BayesianRidge\nbayreg=BayesianRidge()\nbayreg.fit(X_train,Y_train)\npred7=bayreg.predict(X_test)\nmseh[7]=mean_squared_error(Y_test,pred7)**0.5\nprint(mseh[7])","16072196":"import matplotlib.pyplot as plt\nmodels=[\"Linear Regression\",\"Ridge Regression\",\"Lasso Regression\",\"KNN Regression\",\"Support Vector Regression\",\"Random forest regression\",\"Light GBM\",\"Naive Bayes\"]\nmod1=[\"LR\",\"RR\",\"LSR\",\"KNNR\",\"SVR\",\"RFR\",\"LGBM\",\"NBR\"]\nplt.plot(mod1,msel)\nplt.plot(mod1,mseh,'r')\nplt.legend([\"Label Encoding\",\"One Hot Encoding\"])\nfor i in range(8) :\n  print(mod1[i],\":\",models[i])\nplt.show()","26c252bc":"## 8. Naive Bayes (Bayesian Ridge) with Label Encoding","7e354e5d":"## 7. Light GBM model with Label Encoding\n","af1bf8bd":"## 8. Naive Bayes (Bayesian Ridge) with One Hot Encoding","15c12e72":"## 5. Support Vector Regression with Label Encoding","4e0a46b5":"## 4. KNN regression with One Hot Encoding","c53f3b5b":"## 3. Lasso Regression with One Hot Encoding","b77f4eec":"## 6. Random Forest with Label Encoding \n\n\n","35036b9a":"So I have used the shuffle option on the dataset which shuffles the data everytime we run the code. So generally One Hot encoded models work better and out of them the random forest regressor gives least error but sometimes Light GBM tree trumps it.\n\n","d9f0e2a6":"## 7. Light GBM model with One Hot Encoding","5e9094b2":"## 5. Support Vector Regression with One Hot Encoding","a2e53b51":"## 6. Random Forest Regression with One Hot Encoding","4458c5f8":"## One Hot Encoding","3cee4068":"## 2. Ridge Regression with One Hot Encoding","24ef5cb0":"## 4. KNN Regression with Label Encoding","48375e9d":"## 3. Lasso Regression with Label Encoding","f6db2583":"## 2. Ridge Regression  with Label Encoding","306d669f":"## 1. Linear Regression with One Hot Encoding\n","c794841b":"## 1. Linear Regression with Label Encoding\n"}}