{"cell_type":{"4dccd623":"code","cdd78b96":"code","a82a2921":"code","c4c9fc02":"code","633a4342":"code","2fa9f05d":"code","691d97db":"code","d3ea33a7":"code","3124e9ba":"code","77970cfe":"code","e21e4eac":"code","14a8daa0":"code","2520a05e":"code","0277a33f":"code","5b16a340":"code","5f3e6c9b":"code","774772de":"code","179873af":"code","46a8741c":"code","65d3a72f":"code","c37dcf44":"code","9317b36f":"code","0b756ad8":"code","eb2c101f":"code","def54f55":"code","4667f052":"code","ba80fe0b":"markdown","464ac32b":"markdown","19e985ac":"markdown","498f2766":"markdown","79a404e2":"markdown","a32d16de":"markdown","cca522f3":"markdown","de168b83":"markdown","dc35c2ff":"markdown","0eb8f7bc":"markdown"},"source":{"4dccd623":"import numpy as np\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","cdd78b96":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","a82a2921":"df=pd.read_json(\"\/kaggle\/input\/d\/rudra717\/software\/Software.json\/Software.json\", lines=\"true\")\n","c4c9fc02":"df.shape","633a4342":"df.columns","2fa9f05d":"for col in df.columns:\n    print(col, sum(df[col].isnull()))","691d97db":"df_review=df[[\"overall\",\"reviewText\"]] ","d3ea33a7":"df_review=df_review.dropna()","3124e9ba":"df_review.shape","77970cfe":"df_review.head()","e21e4eac":"from bs4 import BeautifulSoup\n\ndef remove_html_tag(text):\n    soup=BeautifulSoup(text,\"html.parser\")\n    clean_text=soup.get_text(separator=\" \")\n    return clean_text\n\nprint(remove_html_tag(\"<html>jaydip<head>bhanderi<\/>\"))","14a8daa0":"import re\n\ndef remove_url(text):\n    return re.sub(r'http\\S+','',text)\n\nprint(remove_url(\"hello there https:\\\\www.google.com\"))","2520a05e":"def remove_non_alphanumeric(text):\n    return re.sub('[^a-zA-Z0-9]',' ',text)\n\nprint(remove_non_alphanumeric(\"3 cup of coffee!#$.,\"))","0277a33f":"def to_lowercase(text):\n    return str(text).lower()\n\nprint(to_lowercase(\"JAYdip\"))","5b16a340":"from nltk.tokenize import wordpunct_tokenize\n\ndef tokenize(text):\n    return wordpunct_tokenize(text)\n\nprint(tokenize(\"Hello there I am imposter\"))","5f3e6c9b":"import nltk\nfrom nltk.corpus import stopwords","774772de":"stop_words=set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return [item for item in text if item not in stop_words]\n\nprint(remove_stopwords(tokenize(\"this is an green apple\")))","179873af":"from nltk.stem import WordNetLemmatizer\nlemma=WordNetLemmatizer()\n\ndef lemmatization(text):\n    return [lemma.lemmatize(word=w,pos='v') for w in text]\n\nprint(lemmatization(tokenize(\"took says played saying\")))","46a8741c":"import nltk  \nimport numpy as np  \nimport random  \nimport string\n\nimport bs4 as bs  \nimport urllib.request  \nimport re  \nfrom sklearn.feature_extraction.text import CountVectorizer\n\nphrases = [\"The quick brown fox jumped over the lazy dog\",\"education is what you have left over after forgetting everything\"]\nvect = CountVectorizer()\nvect.fit(phrases)\n","65d3a72f":"print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content: \\n{}\".format(vect.vocabulary_))\nbag_of_words=vect.transform(phrases)\nprint(bag_of_words)","c37dcf44":"import json\nimport numpy as np\nimport keras.backend as K\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\nplabels = []\nnlabels = []\n","9317b36f":"print(\"bag_of_words as an array: \\n {}\".format(bag_of_words.toarray()))\nvect.get_feature_names()","0b756ad8":"from bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words=set(stopwords.words('english'))\nlemma=WordNetLemmatizer()\n\ndef clean_review(text):\n    soup=BeautifulSoup(text,\"html.parser\")\n    text=soup.get_text(separator=\" \")\n    text=re.sub(r'http\\S+','',text)\n    text=re.sub('[^a-zA-Z0-9]',' ',text)\n    text=str(text).lower()\n    text=wordpunct_tokenize(text)\n    text=[item for item in text if item not in stop_words]\n    text=[lemma.lemmatize(word=w,pos='v') for w in text]\n    text=' '.join(text)\n    return text","eb2c101f":"df_review['cleanReview']=df_review['reviewText'].apply(clean_review)","def54f55":"df_review.head()","4667f052":"df_review['cleanReview'].to_csv('review.csv') ","ba80fe0b":" ### 3. Remove all irrelevant characters","464ac32b":"## Data Preprocessing","19e985ac":"### 4. convert all character into lowercase","498f2766":"### 1. Remove HTML tags","79a404e2":"### 2. Remove URLs","a32d16de":"### Now let's combine all functions and add new column 'cleanReview' to the dataset ","cca522f3":"# **8. Bag Of Words**","de168b83":"### 5. Tokenization","dc35c2ff":"### 6. Remove stop words","0eb8f7bc":"### 7. Lemmatization"}}