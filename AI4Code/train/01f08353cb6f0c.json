{"cell_type":{"e3e53819":"code","4e1ce10c":"code","ae0fa0ca":"code","b6eb49f6":"code","5dbdcb4e":"code","5d28427f":"code","d2453737":"code","5db6ef5d":"code","bc6b8e88":"code","261f5ac5":"code","8518bf39":"code","06266ea3":"code","6e7a375d":"code","5971be42":"code","29f08ad8":"code","9fa1925b":"code","06a083a8":"code","17632710":"code","5aa406a3":"code","e950f70f":"code","e462a683":"code","8eb7153c":"code","0133dfc6":"code","74ce06ca":"code","38296c17":"code","6a3163da":"code","d448e6f1":"code","6af3983a":"code","374540e0":"markdown","accefe1f":"markdown","8e28dc69":"markdown","07e6b64b":"markdown","6c22cec2":"markdown","b96a6243":"markdown","c1ee8182":"markdown","8850d9ac":"markdown","35379b7f":"markdown","76a30612":"markdown","8b923cc5":"markdown","5bc07dce":"markdown","2cfa258a":"markdown"},"source":{"e3e53819":"import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport seaborn as sns\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_formats=['svg']","4e1ce10c":"rng = np.random.RandomState(42)\n\neducation = 16 * rng.rand(200) # variavel ale\nwage = 500 * education + 800*rng.randn(200) + 1000 # y = ax + b\n\ndf = pd.DataFrame({'Educ':education, 'Salaries':wage})\n\ndf.plot.scatter('Educ', 'Salaries', figsize=(9,6))","ae0fa0ca":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(df[['Educ']], df[['Salaries']])\n\nxfit = np.linspace(0, 16, 100)\nyhat = model.predict(xfit[:, np.newaxis])\n\nplt.scatter(education, wage)\nplt.plot(xfit, yhat, color='red')\n\nprint(\"Intercepto beta_0: \", model.intercept_[0])\nprint(\"Inclina\u00e7\u00e3o beta_1: \", model.coef_[0][0])","b6eb49f6":"import statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\nfunction = '''\nSalaries ~ Educ\n'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","5dbdcb4e":"import statsmodels.api as sm\n\nfig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 1, ax=ax)\nax.set_ylabel(\"Salario\")\nax.set_xlabel(\"N\u00edvel da Educa\u00e7\u00e3o\")\nax.set_title(\"Linear Regression\")","5d28427f":"size = 200\n\nesfor\u00e7o = 16 * rng.normal(0, 1, size)\neduca\u00e7\u00e3o = (.6)*(16 * rng.normal(0, 1, size)) + (.4)*esfor\u00e7o\nexperiencia = 25 * rng.normal(0, 1, size)\n\nsalario = 500 * rng.randn(size) + \\\n          300 * educa\u00e7\u00e3o + \\\n          200 * experiencia + \\\n          200 * esfor\u00e7o + \\\n          1000\n\ndf = pd.DataFrame({'Salaries':salario,\n                   'Educ':educa\u00e7\u00e3o,\n                   'Expe': experiencia,\n                   'Grit': esfor\u00e7o})\n\ndf.corr().round(2)","d2453737":"import seaborn as sns\n\nsns.heatmap(df.corr(), annot=True)","5db6ef5d":"def plotCorrGraph(df):\n  # libraries\n  !pip install networkx\n  import pandas as pd\n  import numpy as np\n  import networkx as nx\n  import matplotlib.pyplot as plt\n\n\n  # Calculate the correlation between individuals. We have to transpose first, because the corr function calculate the pairwise correlations between columns.\n  corr = df.corr().abs()\n  corr\n  \n  # Transform it in a links data frame (3 columns only):\n  links = corr.stack().reset_index()\n  links.columns = ['var1', 'var2','value']\n  \n  # Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\n  links_filtered=links.loc[(links['var1'] != links['var2'])]\n  links_filtered\n\n  temp = []\n  checkvalues= []\n  for row in links_filtered.values:\n      if row[2]>.1 and row[2] not in checkvalues:\n          temp.append(row)\n          checkvalues.append(row[2])\n        \n  links_filtered = pd.DataFrame(temp, columns=['var1', 'var2','value'])\n\n  # Build your graph\n  G=nx.from_pandas_edgelist(links_filtered, 'var1', 'var2', )\n  \n  # Plot the network:\n  nx.draw(G, with_labels=True, font_size=10, alpha=.75, node_color='#A0CBE2',edge_color=links_filtered.value.values, width=4, edge_cmap=plt.cm.Blues)\n  plt.show()\n\nplotCorrGraph(df)","bc6b8e88":"# Import the library\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn3, venn3_circles # A, B, AB, C, AC, BC, ABC\n \nvalues = (1000, 900, 300, 900, 200, 300, 100)\nv=venn3(subsets = values, set_labels = ('Salaries', 'Educ', 'Grit'))\nc=venn3_circles(subsets = values, linestyle='dashed', linewidth=1, color=\"grey\")\nplt.show()","261f5ac5":"df","8518bf39":"function = ''' Salaries ~ Educ + Expe'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","06266ea3":"values = (1000, 900, 200, 900, 300, 0, 0) # A, B, AB, C, AC, BC, ABC\nv=venn3(subsets = values, set_labels = ('Salaries', 'Exp', 'Educ'))\nc=venn3_circles(subsets = values, linestyle='dashed', linewidth=1, color=\"grey\")\nplt.show()","6e7a375d":"function = ''' Salaries ~ Expe'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","5971be42":"size = 400\n\nesfor\u00e7o = 16 * rng.normal(0, 1, size)\n\neduca\u00e7\u00e3o = (.6)*(16 * rng.normal(0, 1, size)) + (.4)*esfor\u00e7o\n\nexperiencia = 25 * rng.normal(0, 1, size)\n\nnasceu_brasil = rng.choice([0, 1], size)\n\nsalario = 500 * rng.randn(size) + \\\n          300 * educa\u00e7\u00e3o + \\\n          200 * experiencia + \\\n          100 * esfor\u00e7o + \\\n         -5000 * nasceu_brasil + \\\n          1000\n\ndf = pd.DataFrame({'salario':salario,\n                   'educa\u00e7\u00e3o':educa\u00e7\u00e3o,\n                   'exp': experiencia,\n                   'esfor\u00e7o': esfor\u00e7o,\n                   'br': nasceu_brasil})\n\n\nfunction = '''salario ~ educa\u00e7\u00e3o + br'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())\n\n","29f08ad8":"fig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 1, ax=ax)\nax.set_ylabel(\"Salario\")\nax.set_xlabel(\"N\u00edvel da Educa\u00e7\u00e3o\")\nax.set_title(\"Linear Regression\")","9fa1925b":"size = 400\n\nesfor\u00e7o = 16 * rng.normal(0, 1, size)\n\neduca\u00e7\u00e3o = (.6)*(16 * rng.normal(0, 1, size)) + (.4)*esfor\u00e7o\n\nexperiencia = 25 * rng.normal(0, 1, size)\n\nnasceu_brasil = rng.choice([0, 1], size)\n\nsalario = 500 * rng.randn(size) + \\\n          600 * educa\u00e7\u00e3o + \\\n          -30 * educa\u00e7\u00e3o**2 + \\\n          200 * experiencia + \\\n          100 * esfor\u00e7o + \\\n         -5000 * nasceu_brasil + \\\n          1000\n\ndf = pd.DataFrame({'Salaries':salario,\n                   'Educ':educa\u00e7\u00e3o,\n                   'Exp': experiencia,\n                   'Grit': esfor\u00e7o,\n                   'br': nasceu_brasil})\n\n\nfunction = '''Salaries ~ Educ'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","06a083a8":"fig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 1, ax=ax)\nax.set_ylabel(\"Salaries\")\nax.set_xlabel(\"Education Level\")\nax.set_title(\"Linear Regression\")\nfig.set_size_inches(8, 5)","17632710":"function = '''salario ~ educa\u00e7\u00e3o + I(educa\u00e7\u00e3o**2)'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","5aa406a3":"fig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 1, ax=ax)\nax.set_ylabel(\"Salaries\")\nax.set_xlabel(\"Education Level\")\nax.set_title(\"Linear Regression\")\nfig.set_size_inches(8, 5)","e950f70f":"size = 1000\n\nesfor\u00e7o = 16 * rng.poisson(6, size)\n\neduca\u00e7\u00e3o = (.6)*(16 * rng.poisson(6, size)) + (.4)*esfor\u00e7o\n\nexperiencia = 25 * rng.normal(0, 1, size)\n\nnasceu_brasil = rng.choice([0, 1], size)\n\nsalario = rng.exponential(30000,size) + \\\n          300 * educa\u00e7\u00e3o + \\\n          1000\n\ndf = pd.DataFrame({'salario':salario,\n                   'educa\u00e7\u00e3o':educa\u00e7\u00e3o})\n\n\nfunction = '''salario ~ educa\u00e7\u00e3o'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())\n\ndf['salario'].plot.hist(bins=50)","e462a683":"df['salario'].median(), df['salario'].mean()","8eb7153c":"df.plot.scatter('salario', 'educa\u00e7\u00e3o')","0133dfc6":"df['log_salario'] = np.log1p(df['salario'])\ndf['log_salario'].plot.hist(bins=30)","74ce06ca":"function = '''log_salario ~ educa\u00e7\u00e3o'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","38296c17":"size = 400\n\nesfor\u00e7o = 16 * rng.normal(0, 1, size)\n\nexperiencia = 25 * rng.normal(0, 1, size)\n\nnasceu_brasil = rng.choice([0, 1], size)\n\neduca\u00e7\u00e3o = (.6)*(16 * rng.normal(0, 1, size)) + (.4)*esfor\u00e7o\n\nsalario = 500 * rng.randn(size) + \\\n          100 * educa\u00e7\u00e3o + \\\n         -1000 * nasceu_brasil + \\\n          50 * esfor\u00e7o * nasceu_brasil + \\\n          1000\n\ndf = pd.DataFrame({'salario':salario,\n                   'educa\u00e7\u00e3o':educa\u00e7\u00e3o,\n                   'exp': experiencia,\n                   'esfor\u00e7o': esfor\u00e7o,\n                   'br': nasceu_brasil})\n\n\nfunction = '''salario ~ educa\u00e7\u00e3o + br'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","6a3163da":"fig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 1, ax=ax)\nax.set_ylabel(\"Salaries\")\nax.set_xlabel(\"Education Level\")\nax.set_title(\"Linear Regression\")\nfig.set_size_inches(8, 5)","d448e6f1":"function = '''salario ~ educa\u00e7\u00e3o + educa\u00e7\u00e3o*br -1'''\n\nmodel = smf.ols(function, df).fit()\nprint(model.summary())","6af3983a":"fig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(model, 0, ax=ax)\nax.set_ylabel(\"Salaries\")\nax.set_xlabel(\"Education Level\")\nax.set_title(\"Linear Regression\")\nfig.set_size_inches(8, 5)","374540e0":"# Statistical modeling and its interpretations.\nAs we have repeated several times, this model has many prerequisites, so let's start breaking them down and interpreting them.\n\n## Nonlinear behaviors.\n\nOne of the strongest assumptions is that the return of variables (X) over targets (y) has to be constant,\nhence the name of the Linear model (since the estimated parameters are linear). But what if the behavior is not exactly linear? Do we have to resort to other models? The short answer is no, we can model the problem to understand nonlinearities. Let's go to the examples:\n\nLet's imagine that the return of education is no longer constant about the salary,\nthat it actually peaks and then begins to decrease. That is, not only does it not increase forever, but the speed with which it increases decreases until it reverses. This is a very acceptable hypothesis and can be observed with real data. In estimating a linear model of this new reality, we have a rather strange result:","accefe1f":"# Categorical Variables\nWe already know from other models how to add a categorical variable, we need to model it as a dummy (0 or 1) and run the regression with this new variable, let's create a variable like this in our salary model that will represent whether or not the individual was born in Brazil, as our institutions are not the best,\nFor the same individual with the same experience, education and effort, the \u201chatred\u201d of living in Brazil is (in our theoretical world) $ -1000.","8e28dc69":"When running a regression model, we get the line that produces the smallest possible error: yhat = x * 496 + 1084. That is, the model was able to \u201cunderstand\u201d the reality we created and estimated that the slope coefficient is ~ 496 (very around $ 500 that was created) and the ~ 1084 intercept and the interpretation in this case is quite straightforward.\n\nHe identified the minimum wage (when education equals zero) and when a year of education alters people's income, $ 500.","07e6b64b":"Note that the predicted values \u200b\u200bare parallel, indicating that the slope of the line is given entirely by education and that born or not in Brazil only shifts the entire curve downward.\n\nDummies are super powerful and can help control many complex effects, including time, but this is a matter for causal posting.\n\n\n# Interaction between variables.\nLastly, we need to correct a rather complex behavior of nature which is that variables interact with each other. An example of this is the previous case,\nIs the return of education the same regardless of the other variables? Should not living in Brazil or not affect this return? If so, this model, because it's so simple, doesn't understand this behavior, and again we need to model it so that the model comes to understand.\n\nIn our example,\nThe \u201cBrazil\u201d variable will interact with the Education variable. For the model to understand this behavior we just need to create a third variable that is the multiplication of the two. Note that we are multiplying by 1's and 0's,\ntherefore the new variable will have the repeated education column when the observation is equal to 1 and will have zero otherwise and the regression result is as follows:\n","6c22cec2":"As the model continues to estimate partial correlations, to interpret these variables we need to consider both parts of education simultaneously, the estimated linear and quadratic parts are: 648 and -32 when the actual data was 600 and -30. Thus we can, for example, calculate the education that maximizes wage by taking the maximum of the curve.\n\n# Elasticity Models.\nAnother very common case of nonlinear effects is given when variables instead of having a constant nominal effect have a constant percentage effect. An example would be to estimate the effect of headcount (X) on the production of a bakery (y). If there is only one employee, productivity is high.\nWhen you hire one more production increases a lot, they can take turns, while one meets the other replenish the stock, etc. As we add more employees, productivity is dropping and adding the tenth employee is no longer so productive but increases production. We call this a marginally diminishing effect,\n\n\nand one way to model this problem is by applying the natural logarithm (ln). An extra employee when you have 1 is a 100% increase whereas an extra employee when you have 10 is just a 10% increase.\nIn addition to correcting this percentage increase behavior, logging helps to mitigate the effects of left asymmetric distributions, outliners, and often transforms distributions like this into a distribution much like a normal one.\nHow do we interpret this new variable after passing the logarithmic transformation? Basically as a percentage change rather than nominal. Let's go to the examples:","b96a6243":"One way to look at these relationships between variables is through a correlation chart in a heatmap:","c1ee8182":"Doesn't look like a good fit right? This is very common in the nature of the problems, we have effects that get stronger or weaker along the variable and the way we handle it is by adding the education variable twice, a linear (original) part and a quadratic part, so a linear model can understand nonlinear behavior:","8850d9ac":"When we run the regression over the Salary ln two things happen. The first is that R2 increases from 0.065 to 0.125 (doubled!), Meaning our modeling is on the right track. But when we look at the estimated value for education we see that it went from 300 to 0.0062, how to interpret it? Percentage changes! The new interpretation will be,\none more year of education instead of raising $ 300 in salary increases this model by 0.0062% we call it log-level and the estimated value becomes a semi-elasticity. If we log the two variables, it would be a logistic model.\nlog and interpretation would be: For a 1% increase in education increase the estimated percentage value in variable y. We call this effect elasticity (equal to those price elasticity we always see on the pricing team).","35379b7f":"Let's create a theoretical world where we are interested in interpreting the effects of various variables on people's earnings. In our hypothetical world we have a minimum wage of $ 1000 and each year of education increases on average $ 500 in monthly salary. Because our world is stochastic (not deterministic), we have randomness.","76a30612":"Looking at the first column, we would think that the most important variable is Grit \/ Claw because its correlation with Salary is the highest, and we would say that Experience and education have almost equal effects.\n\nAn alternate way to display the behavior between the variables that I prefer but still can't make popular,\nIt is through graphs where each node is a variable and the color intensity of the edges is the \u201cstrength\u201d of their correlations:","8b923cc5":"# Statistical Modeling and Interpreting Linear Regressions\n\nprevious post: https:\/\/towardsdatascience.com\/interpreting-machine-learning-model-70fa49d20af1\n\nIn this post we will focus on the interpretation of high bias and low variance models, as we explained in the previous post, these algorithms are the easiest to interpret so assume several prerequisites in the data. Let's choose the linear regressions to represent this group of algorithms.\nIf you have no idea what Linear Models are, you might want to check out the article A Brief History of Statistics.\n\nAll codes for this post are available on the Kaggle notebook.\n\nThe goal here is not to explain what these linear models are or how they work, but how to interpret their parameters and estimates,\nbut a brief introduction may be helpful. Linear models can be simple regressions like OLS, regularized Regressions like Lasso and Ridge, can be classification models like Logistic Regressions and even time series like ARIMA filters. They all have in common the fact that they have linear parameters, that is,\nWhen we estimate the \"weights\" of the variables they are constant at any level. Interestingly, a neural network can also be a linear model if its activation layers are linear (f (x) = x), and such a one-layer network will be similar to our simple linear regression that we will use here, but incredibly less efficient.","5bc07dce":"Allowing the model to interact with two variables allows the return of education to be different depending on the case. Now the interpretation will be that although being born in Brazil has a negative effect (keeping everything more constant), the return of education in Brazil is higher (the line is more inclined) than outside Brazil. That is,\nI have an estimated education inclination (eg 300), an estimated birthrate in Brazil (eg -1000, and an estimated education value of Brazilians (eg 50), when we want to analyze the return of education in Brazil we need to add the 300 (from everyone) + the return for being Brazilian 50.\nAll codes and plots are available on the following notebook.","2cfa258a":"# Partial Correlations\n\nThe great trick to interpreting Linear Regressions is to understand how partial correlations work. If you understand this deeply, it will be halfway to start doing causal analysis that is the subject of another post. To do this, let's create a \u201cstatistical language\u201d with Venn diagrams as follows:\nEach circle represents a variable.\nThe size of the circle represents the variance of this variable;\nThe intersections between the circles represent the covariance of these variables. We can interpret as correlation without loss of generality."}}