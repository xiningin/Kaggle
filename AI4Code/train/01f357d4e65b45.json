{"cell_type":{"6232de8f":"code","1095e7b3":"code","50fed864":"code","98e6d70f":"code","d822c91a":"code","e4d7a544":"code","895cce53":"code","b29c852f":"code","c7036ed7":"code","13bf430d":"code","86dad0ef":"code","1627431d":"code","1aebe7fe":"code","94b69075":"code","26a9c9a0":"code","d841193f":"code","0cde2192":"code","f53bc61c":"code","bb69f652":"code","825e3989":"markdown","72c59455":"markdown","404ee1ec":"markdown","acf49416":"markdown","dead150b":"markdown","83c35ece":"markdown","bac14f85":"markdown","2701c147":"markdown","b395e0f7":"markdown","e731f70c":"markdown","c238fe1e":"markdown","84c55942":"markdown","ad067a4e":"markdown","a76fe3ba":"markdown","7b3e21ec":"markdown","84aca1bb":"markdown","d7f8b758":"markdown","4a0f1e8e":"markdown","2d7386a1":"markdown","1a995190":"markdown","b913d959":"markdown"},"source":{"6232de8f":"!apt-get update > \/dev\/null 2>&1\n!apt-get install cmake > \/dev\/null 2>&1\n!pip install --upgrade setuptools 2>&1\n!pip install ez_setup > \/dev\/null 2>&1\n!pip install gym[atari] > \/dev\/null 2>&1\n!pip install gym pyvirtualdisplay > \/dev\/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg > \/dev\/null 2>&1","1095e7b3":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten,MaxPooling2D ,Activation,Input\nfrom tensorflow.keras.models import Sequential,load_model,Model\nimport gym\nimport numpy as np\nimport random\nfrom collections import deque\nfrom tensorflow.keras.utils import normalize as normal_values\nimport cv2\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\ngymlogger.set_level(40) #error only\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nfrom IPython import display as ipythondisplay ","50fed864":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","98e6d70f":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n\ndef show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","d822c91a":"RANDOM_SEED=1\nN_EPISODES=500\n\n# random seed (reproduciblity)\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# set the env\nenv = (gym.make(\"KungFuMaster-v0\")) # env to import\nenv.seed(RANDOM_SEED)\nenv.reset() # reset to env ","e4d7a544":"class SumTree(object):\n\n  data_pointer = 0\n  \n  \"\"\"\n  Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n  \"\"\"\n  def __init__(self, capacity):\n      self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n      \n      # Generate the tree with all nodes values = 0\n      # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n      # Parent nodes = capacity - 1\n      # Leaf nodes = capacity\n      self.tree = np.zeros(2 * capacity - 1)\n      \n      \"\"\" tree:\n          0\n          \/ \\\n        0   0\n        \/ \\ \/ \\\n      0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n      \"\"\"\n      \n      # Contains the experiences (so the size of data is capacity)\n      self.data = np.zeros(capacity, dtype=object)\n  \n  \n  \"\"\"\n  Here we add our priority score in the sumtree leaf and add the experience in data\n  \"\"\"\n  def add(self, priority, data):\n      # Look at what index we want to put the experience\n      tree_index = self.data_pointer + self.capacity - 1\n      \n      \"\"\" tree:\n          0\n          \/ \\\n        0   0\n        \/ \\ \/ \\\ntree_index  0 0  0  We fill the leaves from left to right\n      \"\"\"\n      \n      # Update data frame\n      self.data[self.data_pointer] = data\n      \n      # Update the leaf\n      self.update (tree_index, priority)\n      \n      # Add 1 to data_pointer\n      self.data_pointer += 1\n      \n      if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n          self.data_pointer = 0\n          \n  \n  \"\"\"\n  Update the leaf priority score and propagate the change through tree\n  \"\"\"\n  def update(self, tree_index, priority):\n      # Change = new priority score - former priority score\n      change = priority - self.tree[tree_index]\n      self.tree[tree_index] = priority\n      \n      # then propagate the change through tree\n      while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n          \n          \"\"\"\n          Here we want to access the line above\n          THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n          \n              0\n              \/ \\\n            1   2\n            \/ \\ \/ \\\n          3  4 5  [6] \n          \n          If we are in leaf at index 6, we updated the priority score\n          We need then to update index 2 node\n          So tree_index = (tree_index - 1) \/\/ 2\n          tree_index = (6-1)\/\/2\n          tree_index = 2 (because \/\/ round the result)\n          \"\"\"\n          tree_index = (tree_index - 1) \/\/ 2\n          self.tree[tree_index] += change\n  \n  \n  \"\"\"\n  Here we get the leaf_index, priority value of that leaf and experience associated with that index\n  \"\"\"\n  def get_leaf(self, v):\n      \"\"\"\n      Tree structure and array storage:\n      Tree index:\n            0         -> storing priority sum\n          \/ \\\n        1     2\n        \/ \\   \/ \\\n      3   4 5   6    -> storing priority for experiences\n      Array type for storing:\n      [0,1,2,3,4,5,6]\n      \"\"\"\n      parent_index = 0\n      \n      while True: # the while loop is faster than the method in the reference code\n          left_child_index = 2 * parent_index + 1\n          right_child_index = left_child_index + 1\n          \n          # If we reach bottom, end the search\n          if left_child_index >= len(self.tree):\n              leaf_index = parent_index\n              break\n          \n          else: # downward search, always search for a higher priority node\n              \n              if v <= self.tree[left_child_index]:\n                  parent_index = left_child_index\n                  \n              else:\n                  v -= self.tree[left_child_index]\n                  parent_index = right_child_index\n          \n      data_index = leaf_index - self.capacity + 1\n\n      return leaf_index, self.tree[leaf_index], self.data[data_index]\n    \n  @property\n  def total_priority(self):\n    return self.tree[0] # Returns the root node","895cce53":"class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n\n    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n    \n    PER_b_increment_per_sampling = 0.001\n    \n    absolute_error_upper = 1.  # clipped abs error\n\n    def __init__(self, capacity):\n        # Making the tree \n        \"\"\"\n        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n        And also a data array\n        We don't use deque because it means that at each timestep our experiences change index by one.\n        We prefer to use a simple array and to overwrite when the memory is full.\n        \"\"\"\n        self.tree = SumTree(capacity)\n        \n    \"\"\"\n    Store a new experience in our tree\n    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n    \"\"\"\n    def store(self, experience):\n        # Find the max priority\n        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n        \n        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n        # So we use a minimum priority\n        if max_priority == 0:\n            max_priority = self.absolute_error_upper\n        \n        self.tree.add(max_priority, experience)   # set the max p for new p\n\n        \n    \"\"\"\n    - First, to sample a minibatch of k size, the range [0, priority_total] is \/ into k ranges.\n    - Then a value is uniformly sampled from each range\n    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n    - Then, we calculate IS weights for each minibatch element\n    \"\"\"\n    def sample(self, n):\n        # Create a sample array that will contains the minibatch\n        memory_b = []\n        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n      \n        # Calculate the priority segment\n        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n        priority_segment = self.tree.total_priority \/ n       # priority segment\n\n        # Here we increasing the PER_b each time we sample a new minibatch\n        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n        \n        for i in range(n):\n            \"\"\"\n            A value is uniformly sample from each range\n            \"\"\"\n            a, b = priority_segment * i, priority_segment * (i + 1)\n            value = np.random.uniform(a, b)\n            \"\"\"\n            Experience that correspond to each value is retrieved\n            \"\"\"\n            index, priority, data = self.tree.get_leaf(value)\n            #P(j)\n            sampling_probabilities = priority \/ self.tree.total_priority\n            #  IS = (1\/N * 1\/P(i))**b \/max wi == (N*P(i))**-b  \/max wi\n            b_ISWeights[i] = np.power(n * sampling_probabilities, -self.PER_b)\n                                   \n            b_idx[i]= index\n            \n            experience = [data]\n            \n            memory_b.append(experience)\n\n        \n        b_ISWeights=b_ISWeights\/tf.math.reduce_max(b_ISWeights)\n        \n        return b_idx,memory_b, b_ISWeights\n    \n    \"\"\"\n    Update the priorities on the tree\n    \"\"\"\n    def batch_update(self, tree_idx, abs_errors):\n        abs_errors += self.PER_e  # convert to abs and avoid 0\n        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n        ps = np.power(clipped_errors, self.PER_a)\n\n        for ti, p in zip(tree_idx, ps):\n            self.tree.update(ti, p)","b29c852f":"class DDQN:\n\n  def __init__(self, env,memory_size,path_1=None,path_2=None):\n    self.env=env #import env\n    self.memory=Memory(memory_size)\n    self.state_shape=70, 160, 4 # the state space\n    self.action_shape=env.action_space.n # the action space\n    self.gamma=[0.99] # decay rate of past observations\n    self.learning_rate= 0.001 # learning rate in deep learning\n    self.epsilon_initial_value=1.0 # initial value of epsilon\n    self.epsilon_current_value=1.0# current value of epsilon\n    self.epsilon_final_value=0.001 # final value of epsilon\n    self.nma=5 # No of Top actions to take while exploring \n    self.observing_episodes=10    #No of observations before training the training model\n    #self.temperature_parameter_initial_value=5.0 # initial value of epsilon\n    #self.temperature_parameter_current_value=5.0# current value of epsilon\n    #self.temperature_parameter_final_value=1.0 # final value of epsilon\n    self.observing_episodes_target_model=200\n    self.batch_size=64\n    if not path_1:\n      self.target_model=self._create_model()    #Target Model is model used to calculate target values\n      self.training_model=self._create_model()  #Training Model is model to predict q-values to be used.\n    else:\n      self.training_model=load_model(path_1)\n      self.target_model=load_model(path_2)","c7036ed7":"  def _create_model(self):\n    ''' builds the model using keras'''\n    \n    model=Sequential()\n    model.add(Conv2D(256, (8, 8), padding='same',strides=(4, 4),input_shape=(self.state_shape))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3),strides=(1, 1),  padding='same'))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dense(14))\n    model.add(Activation('softmax'))\n    model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n    return model","13bf430d":"  def get_action(self, state):\n        '''samples the next action based on the E-greedy policy'''\n\n        x=random.random()\n        if x < self.epsilon_current_value:                                    #Exlporation\n          #q_values=((self.target_model.predict(state))[0])**(1\/self.temperature_parameter_current_value)  #This is the step where we use Boltzmann exploration policy\n          #top_actions=q_values.argsort()[-self.nma:][::-1]\n          #action=random.choice(top_actions)\n          action=random.choice(list(range(14)))\n\n        else:\n          #q_values=((self.model.predict(state))**(1\/(self.temperature_parameter_current_value)))[0] #Exploitation\n          q_values=(self.training_model.predict(state))[0]\n          max_Q = np.argmax(q_values)\n          action = max_Q\n        return action","86dad0ef":"  def get_frame(self,frame):\n    frame=frame[95:-45,:]\n    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n    frame=(frame-frame.mean())\/frame.std()\n    return frame","1627431d":"  def update_training_model(self):\n\n    '''\n    Updates the policy network using the NN model.\n    '''\n\n\n    tree_idx, batch, ISWeights_mb = self.memory.sample(self.batch_size)\n    \n    states_mb=np.zeros((self.batch_size,self.state_shape))\n    targets = np.zeros((self.batch_size,self.action_shape))\n    absolute_errors=np.zeros((self.batch_size,1))\n\n    for i in range(self.batch_size):\n      state=batch[i][0][0]\n      states_mb[i]=state\n      preds_=self.training_model.predict(state)\n      targets[i]=preds_\n\n      reward=(batch[i][0][2])\n      \n      terminal=batch[i][0][4]\n\n     \n      action=batch[i][0][1]\n      if terminal:  # If we are in a terminal state, only equals reward\n        targets[i, action] = np.asarray(reward)\n      else:\n        next_state=batch[i][0][3]\n        preds_next_state_target_model=self.target_model.predict(next_state)[0,action]\n        targets[i, action] =  np.asarray(reward) + np.asarray(self.gamma)*np.asarray(preds_next_state_target_model)   # Take the Qtarget for action a'\n      \n      absolute_errors[i]=np.abs(np.sum(targets[i]-preds_,axis=1))\n  \n      # Update priority\n      absolute_errors=absolute_errors\/np.amax(absolute_errors)\n      self.memory.batch_update(tree_idx, absolute_errors)\n\n      optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n      def train_step(states, targets,ISWeights):\n        with tf.GradientTape() as tape:\n          preds= (self.training_model)(states,training=True)  #This is the \ud835\udc44@(\ud835\udc60,\ud835\udc4e)\n          loss= ISWeights*(targets-preds)                 \n          \n        grads = tape.gradient(loss,self.training_model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, self.training_model.trainable_variables))\n      train_step(states_mb,targets,ISWeights_mb)","1aebe7fe":"  def update_target_model(self):\n    self.target_model.set_weights(self.training_model.get_weights())","94b69075":"  def train(self, episodes):\n    '''\n    train the model\n    episodes - number of training iterations\n    ''' \n    env=self.env\n    for episode in range(episodes):\n      # each episode is a new game env\n      state=env.reset()\n      done=False\n      state= self.get_frame(state)\n      stacked_frame=np.stack((state,state,state,state),axis=2)\n      stacked_frame=stacked_frame.reshape(1,stacked_frame.shape[0],stacked_frame.shape[1],stacked_frame.shape[2])\n      state=stacked_frame\n      episode_reward=0 #record episode reward\n      print(\"Episode Started\")\n      while not done:\n        # play an action and record the game state & reward per episode\n        action=self.get_action(state)\n        print(\"Episode Going On.\"+\"\\n\"+\"Action taken:\",action)\n        next_state, reward, done, _=env.step(action)\n        next_state=self.get_frame(next_state)\n        next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n        stacked_frames_1 = np.append(next_state_, stacked_frame[:, :, :, :3], axis=3)\n        next_state=stacked_frames_1\n        experience = state, action, reward, next_state, 1*done\n        self.memory.store(experience)\n        state=next_state\n        episode_reward+=reward\n      print(\"Episode:{}  reward:{}\".format(episode,episode_reward))\n      print(\"Episode Ended\")\n      if episode%self.observing_episodes==0 and episode!=0:\n        self.update_training_model()\n      if episode%self.observing_episodes_target_model==0 and episode!=0:\n        self.update_target_model()\n        self.target_model.save('target_model_{}.h5'.format(episode))\n      if episode%500==0 and episode!=0:\n        self.training_model.save('training_model_{}.h5'.format(episode))\n      if self.epsilon_current_value > self.epsilon_final_value:\n        self.epsilon_current_value=self.epsilon_current_value-(self.epsilon_initial_value-self.epsilon_final_value)\/1000\n        print('Current Epsilon Value:',self.epsilon_current_value)\n      #if self.temperature_parameter_current_value > self.temperature_parameter_final_value:\n        #self.temperature_parameter_current_value=self.temperature_parameter_current_value-(self.temperature_parameter_initial_value-self.temperature_parameter_final_value)\/1000","26a9c9a0":"memory_size=10000\nno_of_episodes=2000\n\nAgent=DDQN(env,memory_size)\nAgent.train(no_of_episodes)","d841193f":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","0cde2192":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n\ndef show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","f53bc61c":"class tester:\n\n  def __init__(self,path):\n    self.model=load_model(path)\n      \n  \n  def get_action(self, state):\n        '''samples the next action based on the E-greedy policy'''\n        q_values=(self.model.predict(state))[0]    #Exploitation\n        action = np.argmax(q_values)\n        return action\n  \n  def get_frame(self,frame):\n    frame=frame[95:-45,:]\n    frame=cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n    frame=(frame-frame.mean())\/frame.std()\n    return frame","bb69f652":"env=(wrap_env(gym.make(\"KungFuMaster-v0\")))\nstate=env.reset()\ndone=False\ntest=tester(\"Actor.h5\")\nstate=test.get_frame(state)\nstacked_frames = np.stack((state,state,state,state),axis=2)\nstacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2]) \nstate_=stacked_frames\nwhile True:\n  env.render('ipython')\n  action = test.get_action(state_)\n  next_state, reward, done, _=env.step(action)\n  print(action,reward)\n  next_state=test.get_frame(next_state)\n  next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n  stacked_frames_1 = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3)\n  next_state_=stacked_frames_1\n  state_=next_state_\n  if done:\n    break\nenv.close()\nshow_video()","825e3989":"Below code setups the environment required to run and record the game and also loads the required library.","72c59455":"Training the model\n\nThis method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a timestep ends, the model is using the observations to update the policy.\n\nWe know that in a dynamic game we cannot predict action based on 1 observation(which is 1 frame of the game in this case) so we will use a stack of 4 frames to predict the output.\n\nWe can also clip the rewards to help model learn faster.","404ee1ec":"Next we construct a SumTree and Memory object that will contain our sumtree and data.","acf49416":"Action Selection\nThe get_action method guides out action choice. Initially, when training begins we use exploration policy but later we do exploitation.\n\nYou can uncomment the commented lines to use Boltzmann exploration policy instead of epsilon greedy policy.","dead150b":"This is the preprocessing we do to the image we obtained by interacting with the environment. This is the preprocessing we do to the image we obtained by interacting with the environment. Here I have done grayscaling and also cropped the image to remove game scores and area which I found was not necessary to train the agent. This speeds up the training process.","83c35ece":"# Why Target Networks help to stabilize training?\nEvery time $Q_{tar}^{\u03c0_\u03b8}(s, a)$ is calculated, the Q-function represented by the parameters \u03b8 will be slightly different, and so $Q_{tar}^{\u03c0_\u03b8}(s, a)$ may be different for the same (s, a). It is possible that between one training step and the next, $Q_{tar}^{\u03c0_\u03b8}(s, a)$ differs significantly from the previous estimate. This \u201cmoving target\u201d can de-stabilize training because it makes it much less clear what values a network should be trying to approach. Introducing a target network stops the target from moving. In between updating \u03c6 to \u03b8, \u03c6 is fixed and so the Q-function represented by \u03c6 doesn\u2019t change. This transforms the problem into a standard supervised regression in between the updates to \u03c6. Whilst this does not fundamentally change the underlying optimization problem, a target network helps to stabilize training, and make divergence or oscillation of the policy less likely. Periodically replacing the target network parameters \u03c6 with a copy of the network parameters \u03b8 is a common way to perform the update. Alternatively each time step, \u03c6 can be set to a\nweighted average of \u03c6 and \u03b8. This is known as a Polyak update and can be thought of as a \u201csoft update\u201d.The main advantage of a replacement update is that \u03c6 is fixed for several steps which temporarily eliminates the \u201cmoving target\u201d. In contrast, when using a Polyak update, \u03c6 changes each training iteration but more gradually than \u03b8. However, a replacement update has the quirk of introducing a dynamic lag between \u03c6 and \u03b8 which depends on the number of time steps since the last update of \u03c6. A Polyak update doesn\u2019t have this feature. The mix between \u03c6 and \u03b8 is constant. One disadvantage of target networks is that they can slow down training since $Q_{tar}^{\u03c0_\u03b8}(s, a)$ is generated from the older target network. If \u03c6 and \u03b8 are too close, then training may be unstable. However if \u03c6 changes too slowly then training may be unnecessarily slow. The hyper-parameter which controls how fast \u03c6 changes needs to be tuned to find a good balance between stability and training speed.\n\n\n","bac14f85":"# Implementation of PER\nFirst of all, we can\u2019t just implement PER by sorting all the Experience Replay Buffers according to their priorities. This will not be efficient at all due to O(nlogn) for insertion and O(n) for sampling. We need to use another data structure instead of sorting an array \u2014 an unsorted sumtree. A sumtree is a Binary Tree, which is a tree with only a maximum of two children for each node. The leaves (deepest nodes) contain the priority values, and a data array that points to leaves contains the experiences.\n\nUpdating the tree and sampling will be really efficient (O(log n)).\n\nIf you don't have knowledge about tress then check out this video: https:\/\/youtu.be\/oSWTXtMglKE\n\nLook at this https:\/\/jaromiru.com\/2016\/11\/07\/lets-make-a-dqn-double-learning-and-prioritized-experience-replay\/ article for more clear understanding we need to use sumtree.\n\nThen, we create a memory object that will contain our sumtree and data.\n\nNext, to sample a minibatch of size k, the range [0, total_priority] will be divided into k ranges. A value is uniformly sampled from each range.\n\nFinally, the transitions (experiences) that correspond to each of these sampled values are retrieved from the sumtree.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-19%20at%207.34.27%20PM.png?raw=true)\n\n\n","2701c147":"This is the Part-4 of Deep Reinforcement Learning Notebook series.***In this Notebook I have talked about some problems we have while using the Deep Q-Networks algorithm (DQN) and how we can improve DQN***.\n\n\nThe Notebook series is about Deep RL algorithms so it excludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only.\n\n","b395e0f7":"# DOUBLE DQN\nDouble DQNs, or double Learning, was introduced by Hado van Hasselt in the paper Deep Reinforcement Learning with Double Q- learning. This method handles the problem of the overestimation of Q-values. So let us first understand what is this problem?\n\nIn DQN, we construct $Q_{tar}(s,a)$ by selecting the maximum Q- value estimate in next-state.Now we face a simple problem: how are we sure that the best action for the next state is the action with the highest Q-value?\nWe know that the accuracy of q values depends on what action we tried and what neighboring states we explored. As a consequence, at the beginning of the training, we don\u2019t have enough information about the best action to take. Therefore, taking the maximum q value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly given a higher Q value than the optimal best action, the learning will be complicated.\n\nIn the paper Deep Reinforcement Learning with Double Q- learning van Hasselt et. al. showed that if $Q^{\u03c0_\u03b8}(s\u2032, a\u2032)$ contain any errors, then max $Q^{\u03c0_\u03b8}(s\u2032, a\u2032)$ will be positively biased and the resulting Q-values will be overestimated. Unfortunately, there are many reasons why $Q^{\u03c0_\u03b8}(s\u2032, a\u2032)$ will not be exactly correct. Function approximation using a neural network is not perfect, an agent may not fully explore the environment, and the environment itself may be noisy. We should, therefore, expect $Q^{\u03c0_\u03b8}(s\u2032, a\u2032)$ to contain some error and so the Q-values will be overestimated. Furthermore, the more actions there are to choose from in-state s\u2032, the greater the overestimation is likely to be. \n\nThe solution is: when we compute the Q target, we use two networks to decouple the action selected from the target Q value generation. We:\n\n\n*  use our DQN network to select what is the best action to take for the next state (the action with the highest Q value).\n*  use our target network to calculate the target Q value of taking that action at the next state.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-17%20at%207.20.34%20PM.png?raw=true)\n\nThe DQN algorithm uses the same network \u03b8 to select action a\u2032 and to evaluate the Q-function for that action. Double DQN uses two different networks, \u03b8, and \u03c6. \u03b8 is used to select a\u2032, and \u03c6 is used to calculate the Q-value for (s\u2032, a\u2032)\n\nIn the last section, we saw target networks so we already have two networks; the training network \u03b8 and the target network \u03c6.\u03b8 and \u03c6 have been trained with overlapping experiences, but if the number of time steps between setting \u03c6 = \u03b8 is large enough, then in practice they are sufficiently different to function as the two different networks for Double DQN. The training network \u03b8 is used to select the action. This is important to ensure that we are still learning the optimal policy after introducing the Double DQN modification. The target network \u03c6 is used to evaluate that action. This is how we can combine Double DQN with target networks","e731f70c":"# Implementing Double DQN with a target network and Prioritized Experience Replay","c238fe1e":"Defining the DQN Class.You can see that I have commented out few things like temperature_parameter .You can uncomment them if you can want to use Boltzmann policy.In this epsilon greedy works good. So I have used that.","84c55942":"# DQN with target Networks\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-17%20at%207.08.06%20PM.png?raw=true)\n","ad067a4e":"Updating the training_model\n\nThe update_training_model method updates the training model weights. ","a76fe3ba":"# Double DQN combined with target network\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-17%20at%207.31.26%20PM.png?raw=true)\n","7b3e21ec":"With the help of below code we run our algorithm and see the success of it.With the help of below code we run our algorithm and see the success of it.","84aca1bb":"# Target Networks\nWe saw in the last notebook that we calculate target q-values by the reward of taking that action at that state plus the discounted highest Q value for the next state. We know that during training the Q-network\nparameters \u03b8 are adjusted to minimize the difference between predicted and target q-values but this is difficult when target q-values changes each training step. So, we\u2019re getting closer to our target but the target is also moving. It\u2019s like chasing a moving target! This lead to a big oscillation in training. It\u2019s like if you were a cowboy (the Q estimation) and you want to catch the cow (the Q-target) you must get closer (reduce the error). At each time step, you\u2019re trying to approach the cow, which also moves at each time step (because you use the same parameters). This leads to a very strange path of chasing (a big oscillation in training) and hence results in unstable training.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-19%20at%2012.11.42%20PM.png?raw=true)\n\nNow here we see our first improvement technique for DQN i.e. target Networks. It was introduced by Mnih et. al. in Human-level Control through Deep Reinforcement Learning. To reduce the changes target Q-values in between training steps we use a target network. A target network is a second network with parameters \u03c6 and is a lagged copy of the Q-network(this is the network we are training and using for predicted q-values). The target network $Q^{\u03c0_\u03c6}(s, a)$ is used to calculate target q-values instead of the Q-network which is continuously changing. Periodically \u03c6 is updated to the current values for \u03b8. This is known as a replacement update. The update frequency for \u03c6 is problem-dependent.For example, in the Atari games, it is common to update \u03c6 every 1, 000 \u2013 10, 000 environment steps. For simpler problems, it is not necessary to wait as long. Updating \u03c6 every 100 \u2013 1, 000 will be sufficient.\n ","d7f8b758":"# Double DQN with a target network and Prioritized Experience Replay\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-19%20at%201.23.53%20PM.png?raw=true)","4a0f1e8e":"Updating the target_model\n\nThe update_target_model method sets the target model weights to training model weights.","2d7386a1":"#Prioritized Experience Replay\nPrioritized Experience Replay (PER) was introduced in 2015 by Tom Schaul. The idea is that some experiences may be more important than others for our training, but might occur less frequently.\nIf we can train an agent by sampling informative experiences more frequently than uninformative ones, the agent may learn faster. Because we sample the batch uniformly (selecting the experiences randomly) these rich experiences that occur rarely have practically no chance to be selected. That\u2019s why, with PER, we try to change the sampling distribution by using a criterion to define the priority of each tuple of experience. Consider the following example. A humanoid agent is trying to learn how to stand. At the beginning of each episode, the agent is always initialized sitting on the floor. Initially, most actions will result in the agent flailing around on the ground, and only a few experiences will convey meaningful information about how to combine joint movements so as to balance and stand up. These experiences are more important for learning how to stand than those in which the agent is stuck on the ground. They help the agent learn what to do instead of the many ways it can do the wrong thing.\n\n\nPrioritized Experience Replay is based on this simple and intuitive idea, but presents two implementation challenges. First, how to automatically assign each experience a priority. Second, how to sample efficiently from the replay memory using their priorities.\n\nA natural solution to the first problem is for the priority to be derived from the absolute difference between $Q^{\u03c0_\u03b8}(s, a)$ and $Q^\u03c0_{tar}(s, a)$\n, known as the TD error. The larger the difference between these two values, the larger the mismatch between an agent\u2019s expectations and what happened in the next step, and the more an agent should correct $Q^{\u03c0_\u03b8}$(s, a). The only remaining issue is what priority to assign to experiences initially since there is no TD error available. Typically this is solved by setting the score to a large constant value to encourage each experience to be sampled at least once.\n\nSchaul et al. propose two different options for sampling using the scores; rank-based, and proportional prioritization. Both approaches are based on an interpolation between greedy prioritization (always picking the experiences with the top n scores) and uniform random sampling. This ensures that experiences with higher scores are sampled more often whilst ensuring that each experience has a non zero probability of being sampled.\n\nWe define the probability of sampling transition i as\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-19%20at%201.07.28%20PM.png?raw=true)\n\nEquation 5.1\n\nThe first variant we consider is the direct, **proportional prioritization** where $p_i$ = |$\u03b4_i$| + \u03b5, where \u03b5\nis a small positive constant that prevents the edge-case of transitions not being revisited once their\nerror is zero. The second variant is an indirect, **rank-based prioritization** where $p_i$ = 1\/rank(i) , where \nrank(i) is the rank of transition i when the replay memory is sorted according to $|\u03b4_i|$. In this case, P becomes a power-law distribution with exponent \u03b1. Both distributions are monotonic in |\u03b4|, but the latter is likely to be more robust, as it is insensitive to outliers.\n\n# Importance Sampling\nPrioritizing certain examples changes the expectation of the entire data distribution, which introduces bias into the training process. This can be corrected by multiplying the TD error for each example by a set of weights, known as importance sampling. If the bias is small then it is unclear how effective importance sampling is because there are other factors such as the action noise and a highly non-stationary data distribution, that may dominate the effect of a small bias especially in the early stages of learning. Schaul et. al. hypothesize that correcting for the bias is only likely to matter towards the end of the training, and show that the effect of making the correction is mixed. In some cases adding importance sampling led to improved performance, in others, it made little difference or caused performance to deteriorate.\n\n\nImportance sampling has another benefit when combined with prioritized replay in the context of non-linear function approximation (e.g. deep neural networks): here large steps can be very disruptive, because the first-order approximation of the gradient is only reliable locally, and have to be prevented with a smaller global step-size. In our approach instead, prioritization makes sure high-error transitions are seen many times, while the IS correction reduces the gradient magnitudes (and thus the effective step size in parameter space), and allowing the algorithm to follow the curvature of highly non-linear optimization landscapes because the Taylor expansion is constantly re-approximated.\n\n\n\n","1a995190":"Creating a Neural Network Model.","b913d959":"This part ensures the reproducibility of the code below by using a random seed and setups the environment."}}