{"cell_type":{"d021e924":"code","632c5b9f":"code","be120b25":"code","ed0fdb26":"code","c733fd10":"code","dbfe45f6":"code","3532e03b":"code","1ba905d0":"code","c2114c25":"code","3635e705":"code","22ec51d9":"code","6b1cae8b":"code","5b62af4b":"code","ee793a3e":"code","873aa652":"code","917801fa":"code","afcd5c9a":"code","9f04e97e":"code","727c066b":"code","704b571e":"markdown","591c0ec1":"markdown","fecdf6c3":"markdown","ad0cb7bc":"markdown","f77a6e7e":"markdown","caed675f":"markdown","e1ac1af8":"markdown","d2a7557d":"markdown","cfdc24c5":"markdown","bfc6f535":"markdown","e01a87cf":"markdown","68c8fb15":"markdown","3fd75d92":"markdown","e86773ea":"markdown","1f8adcd5":"markdown"},"source":{"d021e924":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np","632c5b9f":"def recover_data(data_input_0, \n                 target = None,   # For prediction at the end\n                 verbose = 1,\n                 stacking = 0\n                 ):\n    \n    \n    import datetime\n    import random\n    import pandas as pd\n    import numpy as np\n\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n        \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    \n    import lightgbm as lgb\n    if stacking:\n        import catboost as ctb\n        import xgboost as xgb\n    \n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import classification_report\n    from sklearn.preprocessing import LabelEncoder\n\n\n    \n        \n    encoder_pool = {}\n    decoder_pool = {}\n    encoded_columns = []\n    counter_predicted_values = 0\n    CLASS_VALUE_COUNTS = 10\n    miss_indeces = None\n\n    \n    \n    def get_data_info(df):\n        data_info = pd.DataFrame(index=df.columns)\n        try:\n            data_info['NaN_counts'] = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            data_info['NaN_percent'] = data_info['NaN_counts'].apply(lambda x: round((x\/len(df))*100, 2))\n            data_info['col_type'] = df.dtypes\n            data_info = data_info.sort_values(by=['NaN_counts'], ascending=True)\n        except:\n            return data_info\n        return data_info\n    \n    \n    def encode_column(df, enc_pool, dec_pool, column):\n        enc_pool[column] = {}\n        dec_pool[column] = {}\n        not_nan_index=df[df[column].notnull()].index\n        values_set = list(set(list(df.loc[not_nan_index, column])))\n        value = 0.0\n        for el in values_set:\n            enc_pool[column][el] = value\n            dec_pool[column][value] = el\n            value += 1\n        df[column] = df[column].map(enc_pool[column])\n        df[column] = df[column].astype('float64')\n        return df\n    \n    \n    def get_columns(columns, target_column):\n        columns_now = columns[:]\n        if target_column in columns:\n            columns_now.remove(target_column)\n        return columns_now\n    \n    \n    def normalize_data(df_in, columns):\n        for col in columns:\n            min_x = df_in[col].min()\n            max_x = df_in[col].max()\n            df_in[col] = (df_in[col] - min_x) \/ (max_x - min_x)\n            df_in[col] = np.log1p(df_in[col])\n        return df_in\n    \n    \n    def get_class_weight(y):\n        global class_weight\n        class_weight={}\n        unique = []\n        counts = []\n        values_counted = (pd.DataFrame(y)).value_counts().sort_values(ascending=False)\n        for idx, val in values_counted.items():\n            unique.append(idx[0])\n            counts.append(val)\n\n        max_value = max(counts)\n        j = 0\n        for el in unique:\n            class_weight[el] = int(max_value\/counts[j])\n            j += 1\n        if len(unique) == 1:\n            class_weight = {}\n        return class_weight\n\n    \n    def predict(X__train, y__train, X__pred, all_algorithms, values_counted=None):\n        if len(pd.Series(y__train).value_counts()) == 1:\n            all_algorithms = [all_algorithms[0]]\n            \n        stacked_predicts = pd.DataFrame()\n        stacked_column_names = []\n        for alg in all_algorithms:\n            alg_name = str(alg.__class__.__name__)[:3]\n            model = alg.fit(X__train, y__train.ravel())\n            y_hat = model.predict(X__pred).ravel()\n            stacked_predicts[alg_name] = y_hat\n            stacked_column_names.append(alg_name)\n        if values_counted:\n            # classification\n            stacked_predicts['y_hat_final'] = stacked_predicts[stacked_column_names].mode(axis=1)[0].astype('int64')\n            y_hat = list(stacked_predicts.loc[:, 'y_hat_final'])\n        else:\n            # regression\n            stacked_predicts['y_hat_final'] = stacked_predicts[stacked_column_names].mean(axis=1)\n            y_hat = stacked_predicts.loc[:, 'y_hat_final']\n            y_hat[y_hat == -np.inf] = 0\n            y_hat[y_hat == np.inf] = 0\n#         print(stacked_predicts)\n        del stacked_predicts\n        return y_hat\n\n    \n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        df.loc[miss_indeces, el] = pred_miss[:]\n        return df\n    \n    \n    def decode_column(df, dec_pool, column):\n        df[column] = df[column].map(dec_pool[column])\n        df[column] = df[column].astype('object')\n        return df\n    \n    \n    \n    \n    # main\n    init_time = datetime.datetime.now()\n    \n    data_input = data_input_0.copy()\n\n    if verbose:\n        plt.figure(figsize=(20, 5))\n        sns.heatmap(data_input.isnull(), cbar=False)\n            \n    data_info = get_data_info(data_input)\n    if verbose:\n        print('\\n\\n\\n', data_info, '\\n\\n\\n')\n    \n    all_features = list(data_input.columns)\n    data_input_indeces = list(data_input.index)\n    data_input.reset_index(drop=True, inplace = True)\n    \n    all_miss_features = list(data_info.index[data_info['NaN_counts'] > 0])\n\n    \n    # move target_feature to end of the prediction\n    if target:\n        if target in all_miss_features:\n            all_miss_features.append(all_miss_features.pop(all_miss_features.index(target)))\n            \n            \n    # a simple encoding\n    for col in data_input.columns:\n        feature_type = data_info.loc[col, 'col_type']\n        if feature_type == 'object' or feature_type == 'bool':            \n            data_input[col] = encode_column(data_input[[col]], encoder_pool, decoder_pool, col)\n            encoded_columns.append(col)\n        else:\n            # object column with NaN sometimes has type float64\n            try:\n                data_input.loc[:, col] = data_input.loc[:, col] + 0\n            except:\n                data_input[col] = data_input[col].astype('object')\n                data_input[col] = encode_column(data_input[[col]], encoder_pool, decoder_pool, col)\n                encoded_columns.append(col)\n    \n    \n    # get continuous & discrete features\n    continuous_features = []\n    discrete_features = []\n    for col in data_input.columns:\n        count_val = len(data_input[col].value_counts())\n        if count_val > CLASS_VALUE_COUNTS:\n            continuous_features.append(col)\n        else:\n            discrete_features.append(col)\n\n    \n    # work with each column containing NaNs\n    for target_now in all_miss_features:\n        if verbose:\n            init_iter_time = datetime.datetime.now()\n            print('='*90,'\\n')\n        # predictors for iteration\n        predictors = all_features[:]\n        predictors.remove(target_now)\n        \n        continuous_features_now = get_columns(continuous_features, target_now)\n        discrete_features_now = get_columns(discrete_features, target_now)\n        \n        # indexes of missing data in target_now (data for prediction)\n        miss_indeces = list((data_input[pd.isnull(data_input[target_now])]).index)\n        count_miss_values = len(miss_indeces)\n\n        # data without NaN rows (X data for train & evaluation of model)\n        work_indeces = list(set(data_input_indeces) - set(miss_indeces))\n        \n        # X data for predict target NaNs\n        miss_df = data_input.loc[miss_indeces, predictors]\n        miss_df = normalize_data(miss_df, continuous_features_now)\n        \n        # X data for train and model evaluation \n        work_df = data_input.iloc[work_indeces, : ]\n        work_df = normalize_data(work_df, continuous_features_now)\n        \n        X = work_df[predictors]\n        y = work_df[target_now]\n        y[y == -np.inf] = 0\n        y[y == np.inf] = 0\n        \n        \n        target_values_counted = y.value_counts()\n        last_item = target_values_counted.tail(1).item()\n        len_target_values_counted = len(target_values_counted)\n        \n        \n        feature_type_target = data_info.loc[target_now, 'col_type']\n        if len_target_values_counted <= CLASS_VALUE_COUNTS or feature_type_target == 'object':\n            labelencoder = LabelEncoder()\n            y = labelencoder.fit_transform(y).astype('int64')\n        else:\n            # normalization\n            min_y = y.min()\n            max_y = y.max()\n            y = (y - min_y) \/ (max_y - min_y)\n            y = np.log1p(y)\n\n        \n            \n            \n        if feature_type_target == 'object' and last_item < 2:\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n        elif feature_type_target == 'object' and last_item < 3:\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0, stratify=y)\n        elif feature_type_target == 'object' and last_item < 5:\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n        elif feature_type_target == 'object':\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n        else:\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n        \n        # Info\n        if verbose:\n            percent_missing_data = data_info.loc[target_now, 'NaN_percent']\n            print(f'Feature: {target_now}, missing values: {percent_missing_data}%\\n')    \n            print(f'Shape for train & test: {(X.shape)}')\n            print('')\n            if len_target_values_counted < 11 or feature_type_target == 'object':\n                print(f'values_count:')\n                print(target_values_counted)\n                print('')\n\n        \n        # PREDICTIONS CLASSIFIER\n        if len_target_values_counted < CLASS_VALUE_COUNTS or feature_type_target == 'object':\n            class_weight = get_class_weight(y_train)\n            lgb_class = lgb.LGBMClassifier(class_weight=class_weight, random_state=0, n_jobs=-1)\n            algorithms = [lgb_class]\n            if stacking:\n                xgb_class = xgb.XGBClassifier(sample_weight=class_weight, random_state=0, n_jobs=-1, verbosity=0)\n                ctb_class = ctb.CatBoostClassifier(class_weights=class_weight, random_state=0, verbose=0)\n                algorithms = [lgb_class, xgb_class, ctb_class]\n                \n            pred_test = predict(X_train, y_train, X_test, algorithms, len_target_values_counted)\n\n            if verbose:\n                print('FAST CLASSIFIER:')\n                print(f'Weights transferred to the classifier: {class_weight}')\n                print('\\nEvaluations:')\n                print(f'first 20 y_test: {list(y_test[:20])}')\n                print(f'first 20 y_pred: {pred_test[:20]}\\n')\n                print(f'Classification Report:\\n')\n                print(classification_report(y_test, pred_test), '\\n')\n\n#             ADVANCED  alg = LGBMClassifier(n_jobs=-1, random_state=0, class_weight=class_weight)                \n            pred_miss = predict(X_train, y_train, miss_df, algorithms, len_target_values_counted)\n\n            pred_miss = [int(i) for i in pred_miss]\n            pred_miss = labelencoder.inverse_transform(pred_miss)\n            imput_missing_value_to_main_df(data_input, miss_indeces, pred_miss, target_now)\n            counter_predicted_values += len(miss_indeces)\n        \n        # PREDICTIONS REGRESSOR\n        elif feature_type_target == 'float64' or feature_type == 'int64':\n            lgb_reg = lgb.LGBMRegressor(n_jobs=-1, random_state=0)\n            algorithms = [lgb_reg]\n            if stacking:\n                xgb_reg = xgb.XGBRegressor(n_jobs=-1, random_state=0, verbosity=0)\n                ctb_reg = ctb.CatBoostRegressor(random_state=0, verbose=0)\n                algorithms = [lgb_reg, xgb_reg, ctb_reg]\n    \n            pred_test = predict(X_train, y_train, X_test, algorithms)\n            pred_test = np.expm1(pred_test)\n            pred_test = (pred_test * (max_y - min_y)) + min_y\n            y_test = (y_test * (max_y - min_y)) + min_y\n\n\n            MAE = mean_absolute_error(y_test,pred_test)\n            y_test = list(np.round(y_test[:10], 1))\n            y_pred = list(np.round(pred_test[:10], 1))    ##############\n            \n            if verbose:\n                print('FAST REGRESSOR:')\n                print('\\nEvaluations:')\n                print(f'first 10 y_test: {y_test}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'MAE for {target_now}: {MAE}')\n                print(f'min for {target_now}: {data_input[target_now].min()}')\n                print(f'avg for {target_now}: {data_input[target_now].mean()}')\n                print(f'max for {target_now}: {data_input[target_now].max()}\\n')\n                \n\n\n#             ADVANCED  alg = LGBMRegressor(n_jobs=-1, random_state=0)        \n            pred_miss = predict(X, y, miss_df, algorithms)\n            pred_miss = np.expm1(pred_miss)\n            pred_miss = (pred_miss * (max_y - min_y)) + min_y\n            \n            \n            imput_missing_value_to_main_df(data_input, miss_indeces, list(pred_miss), target_now)\n            counter_predicted_values += len(miss_indeces)\n            \n            \n\n        else:\n            if verbose:\n                print(f\"unprocessed feature: {target_now} - {feature_type} type\")\n\n\n        \n        del predictors\n        del miss_indeces\n        del miss_df        \n        del work_df\n        del X\n        del y\n        del X_train\n        del X_test\n        del y_train\n        del y_test\n        \n        if verbose:\n            finish_iter_time = datetime.datetime.now()\n            requared = finish_iter_time - init_iter_time\n            print(f'Imputed Values: {count_miss_values}')\n            print(f'Required time:  {str(requared)}\\n')\n\n        \n    # return states to their initial states\n    for col in encoded_columns:\n        data_input[col] = decode_column(data_input[[col]], decoder_pool, col)\n        \n    for col in data_input.columns:\n        data_input[col] = data_input[col].astype(data_info.loc[col, 'col_type'])\n                \n    data_input.index = data_input_indeces\n\n    if verbose:\n        plt.figure(figsize=(20, 5))\n        sns.heatmap(data_input.isnull(), cbar=False)\n        print('\\n\\n\\n')\n        data_info = get_data_info(data_input)\n        print(data_info)\n        print('\\n\\n\\n')\n        print(f'{counter_predicted_values} values have been predicted and replaced. \\\n        {(counter_predicted_values*100\/(data_input.shape[0]*data_input.shape[1]))} % of data')\n        print('\\n')\n        finish_time = datetime.datetime.now()\n        requared = finish_time - init_time\n        print(f'Required time totally: {str(requared)}\\n\\n')\n    \n\n    return data_input","be120b25":"import numpy as np\nimport pandas as pd\n\n\n# data = \"..\/input\/home-data-for-ml-course\/train.csv\"\n# target = \"SalePrice\"\n# target = \"LotFrontage\"\n\n\n# data = \"..\/input\/fe-course-data\/ames.csv\"\n# target = \"SalePrice\"\n\n\n# data = \"..\/input\/fe-course-data\/housing.csv\"\n# target = \"HouseAge\"\n\n\ndata = \"..\/input\/fe-course-data\/autos.csv\"\n# target = \"price\"\n\n# Classification\ntarget = \"aspiration\"\n# target = \"drive_wheels\"\n\n\n# data = \"..\/input\/fe-course-data\/concrete.csv\"\n# target = \"CompressiveStrength\"\n\n\n# data = \"..\/input\/fe-course-data\/accidents.csv\"\n# target = \"Distance\"\n\n# Classification\n# target = \"Severity\"\n\n\n\ndf = pd.read_csv(data)\npredictors = list(df.columns)\npredictors.remove(target)\ndf\n","ed0fdb26":"def trash_cleaner(df, target=None):\n    '''drop_duplicates, drop rows with nan in target, drop column with 1 unique value'''\n    \n    print(f'Start shape: {df.shape}\\n')\n    print('Drop_duplicates')\n    df.drop_duplicates(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    print(f'shape: {df.shape}\\n')\n    \n    print('Drop columns with 1 unique value')\n    for col in df.columns:\n        if len(df[col].unique()) == 1:\n            df.drop([col], inplace=True, axis=1)\n            print(f'column \"{col}\" cnontain 1 unique value and has been dropped out')\n    print(f'shape: {df.shape}\\n')\n            \n    \n    if target:\n        print('Drop rows with NaN in target feature')\n        nan = df[df[target].isnull()]\n        indeces = list(nan.index)\n        print(f'number of rows with NaN in target feature:\\\n        {len(indeces)} rows have been dropped out\\n')\n        df = df.drop(df.index[indeces])\n        df.reset_index(drop=True, inplace=True)\n    \n        print(f'Finish shape: {df.shape}\\n')\n    \n    return df\n\ndf = trash_cleaner(df, target)","c733fd10":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encoding(work_df, columns):\n    for col in columns:\n        feature_type = work_df[col].dtype\n        if feature_type == 'object' or feature_type == 'bool':\n            labelencoder = LabelEncoder()\n            work_df.loc[:, col] = labelencoder.fit_transform(work_df.loc[:, col])\n        # skew handling for continious features\n        else:\n            # object column with NaN has type float64\n            try:\n                work_df.loc[:, col] = work_df.loc[:, col] + 0\n            except:\n                work_df[col] = work_df[col].astype('object')\n                labelencoder = LabelEncoder()\n                work_df.loc[:, col] = labelencoder.fit_transform(work_df.loc[:, col])\n    return work_df\n\ndf = encoding(df, predictors)","dbfe45f6":"from sklearn.model_selection import train_test_split\n\n\nX = df[predictors]\ny = df[target]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=0)\n\ndf = pd.concat([train_X, train_y], axis=1)\ndf.reset_index(drop=True, inplace=True)\ndf","3532e03b":"def nan_create(df, fraction_NaN, predictors):\n    import random\n    random.seed(10)\n    \n    \n    df = df.copy()\n\n    for col in predictors:\n        randomlist = random.sample(range(0, len(df)), int(len(df)*fraction_NaN))\n        idx = [df.index[i] for i in randomlist]\n        \n        df.loc[idx,col] = np.nan\n\n    return df","1ba905d0":"# predictors.remove('StartTime')\n# predictors.remove('EndTime')\n# predictors.remove('Street')\n# predictors.remove('City')\n\n\ndf_1 = nan_create(df, 0.2, predictors[:])\n\n\nplt.figure(figsize=(20, 5))\nsns.heatmap(df_1.isnull(), cbar=False)","c2114c25":"df_2 = recover_data(df_1, stacking=0)","3635e705":"df","22ec51d9":"df_1","6b1cae8b":"df_2","5b62af4b":"plt.figure(figsize=(20, 5))\nsns.heatmap(df.isnull(), cbar=False)","ee793a3e":"plt.figure(figsize=(20, 5))\nsns.heatmap(df_1.isnull(), cbar=False)","873aa652":"plt.figure(figsize=(20, 5))\nsns.heatmap(df_2.isnull(), cbar=False)","917801fa":"from sklearn.metrics import f1_score, plot_roc_curve, plot_precision_recall_curve, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error\nfrom xgboost import XGBClassifier, XGBRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndef evaluate_model(train_X, train_y, test_X, test_y, params={}):\n    values_counted = train_y.value_counts()\n    if len(values_counted) < 11:\n        model = XGBClassifier(**params, verbosity=0, random_state=42, n_jobs=-1)\n        model.fit(train_X, train_y)\n        pred = model.predict(test_X)\n        f1 = f1_score(test_y, pred, average='macro')\n        print(f'\\nf1_score macro: {f1}\\n')\n        print(classification_report(test_y, pred), '\\n')\n\n\n        f, ax = plt.subplots(1, 3, figsize=(13, 4))\n\n        plt.subplot(1, 3, 1)\n        cm = confusion_matrix(test_y, pred)\n        sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f',  ax=ax[0])\n        ax[0].set_title(\"Confusion Matrix\")\n        \n        plt.subplot(1, 3, 2)\n        plt.plot([1,0], color='red')\n        plot_precision_recall_curve(model, test_X, test_y, ax=ax[1])\n        ax[1].set_title(\"PR AUC\")\n\n        plt.subplot(1, 3, 3)\n        plt.plot([0,1], color='red')\n        plot_roc_curve(model, test_X, test_y, ax=ax[2])\n        ax[2].set_title(\"ROC AUC\")\n\n        f.tight_layout()\n\n    else:\n        model = XGBRegressor(n_jobs=-1, random_state=0)\n        model = model.fit(train_X, train_y)\n        pred_test = model.predict(test_X)\n        MAE = mean_absolute_error(test_y, pred_test)\n        RMSE = np.sqrt(mean_squared_error(test_y, pred_test))\n        print(f'mean_absolute_error:     {MAE}')\n        print(f'root mean_squared_error: {RMSE}')\n\n    return 0","afcd5c9a":"X = df[predictors]\ny = df[target]\n\nevaluate_model(X, y, test_X, test_y)","9f04e97e":"X = df_1[predictors]\ny = df_1[target]\n\nevaluate_model(X, y, test_X, test_y) ","727c066b":"X = df_2[predictors]\ny = df_2[target]\n\nevaluate_model(X, y, test_X, test_y)","704b571e":"# Add NaNs ","591c0ec1":"# Prediction and imputing NaNs","fecdf6c3":"# df - baseline","ad0cb7bc":"## ML nan imputer","f77a6e7e":"# data with NaNs","caed675f":"# NaN prediction","e1ac1af8":"# Evaluate the model without NaN in data (after prediction)","d2a7557d":"# Baseline Data ","cfdc24c5":"# in df_2 NaNs were predicted and imputed","bfc6f535":"# Evaluate the model with NaNs in data","e01a87cf":"# Define evaluation function","68c8fb15":"# Let's split data to train and test.","3fd75d92":"# Let's choose data and the target feature \u00b6","e86773ea":"# df_1 contains NaNs","1f8adcd5":"# A Simple Encoding"}}