{"cell_type":{"ca15bbe5":"code","6985aa89":"code","1397aad3":"code","e5c71488":"markdown","d3916fef":"markdown"},"source":{"ca15bbe5":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmnist = keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.imshow(train_images[i])\nplt.show()\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape = (28, 28)),\n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(10, activation = 'softmax')\n])\n\nmodel.compile(optimizer = 'adam',\n              loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy'])\n\nmodel.fit(train_images, train_labels, epochs = 10)\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 2)\nprint('\\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:', test_acc)","6985aa89":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Wine\n\ndata = datasets.load_wine()\ndata_train, data_test, label_train, label_test = train_test_split(data['data'],\n                                                                  data['target'],\n                                                                  test_size=0.2)\n\nscaler = StandardScaler()\ndata_train = scaler.fit_transform(data_train)\ndata_test = scaler.fit_transform(data_test)\n\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(10, activation = 'softmax')\n])\n\nopt = keras.optimizers.SGD(learning_rate=0.005)\n\nmodel.compile(optimizer = opt,\n              loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy'])\n\nmodel.fit(data_train, label_train, epochs = 20)\n\ntest_loss, test_acc = model.evaluate(data_test, label_test, verbose = 2)\nprint('\\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:', test_acc)","1397aad3":"data = datasets.load_breast_cancer()\ndata_train, data_test, label_train, label_test = train_test_split(data['data'],\n                                                                  data['target'],\n                                                                  test_size=0.2)\n\nscaler = StandardScaler()\ndata_train = scaler.fit_transform(data_train)\ndata_test = scaler.fit_transform(data_test)\n\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation = 'tanh'),\n    keras.layers.Dense(2, activation = 'softmax')\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.1)\n\nmodel.compile(optimizer = opt,\n              loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy'])\n\nmodel.fit(data_train, label_train, epochs = 10)\n\ntest_loss, test_acc = model.evaluate(data_test, label_test, verbose = 2)\nprint('\\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:', test_acc)","e5c71488":"Relu: loss: 0.1605 - accuracy: 0.9298\n\nSigmoid: loss: 0.1306 - accuracy: 0.9474\n\nTanh: loss: 0.0662 - accuracy: 0.9825\n\n\u0412\u044b\u0432\u043e\u0434: \u0414\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u0443\u0447\u0430 \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u044f tanh.\n\nAdam: loss: 0.0662 - accuracy: 0.9825\n\nSGD: loss: 0.0804 - accuracy: 0.9912\n\nAdagrad: loss: 0.1734 - accuracy: 0.9649\n\n\u0414\u043b\u044f \u043c\u0435\u0442\u043e\u0434\u0430 Adam \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u0434\u0430\u0442\u044c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.\n\u0415\u0441\u043b\u0438 \u0437\u0430\u0434\u0430\u0442\u044c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c 0, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0438\u0437\u043a\u0430\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c loss: 0.7945 - accuracy: 0.3333.\n\u041f\u0440\u0438 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c 0.1 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442\u0441\u044f: loss: 1.1003 - accuracy: 0.9298\n\n","d3916fef":"**\u0424\u0443\u043d\u043a\u0446\u0438\u0438 \u0430\u043a\u0442\u0438\u0438\u0432\u0430\u0446\u0438\u0438:**\n*\u0413\u0438\u043f\u0435\u0440\u0431\u043e\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0430\u043d\u0433\u0435\u043d\u0441:* \n2\/2 - 0s - loss: 0.1980 - accuracy: 0.9167\n\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445: 0.9166666865348816\n\n*Relu:*\n2\/2 - 0s - loss: 0.2290 - accuracy: 0.9722\n\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445: 0.9722222089767456\n\n*Sigmoid:*\n2\/2 - 0s - loss: 0.4233 - accuracy: 0.9444\n\n\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445: 0.9444444179534912\n\n\u0411\u044b\u043b\u0430 \u0432\u044b\u0431\u0440\u0430\u043d\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f Relu, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0443\u044e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0438\u0445 \u043f\u043e\u0442\u0435\u0440\u044f\u0445.\n\n**\u041c\u0435\u0442\u043e\u0434\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438:**\n\n1. Adam: loss: 0.2278 - accuracy: 0.9444\n2. SGD: loss: 0.6508 - accuracy: 0.8611\n3. Adadelta: loss: 2.5955 - accuracy: 0.0833\n3. Adagrad: loss: 1.5919 - accuracy: 0.8333\n\n\u0412\u044b\u0432\u043e\u0434\u044b: \u041c\u0435\u0442\u043e\u0434  Adadelta  \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u0443\u0447\u0430\u044f \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442.  \u041d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0443\u044e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u0435\u0442 Adam.\n\n\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043b\u044f SGD\n\n0:      loss: 2.5073 - accuracy: 0.1667\n\n0.005 : loss: 1.1638 - accuracy: 0.8889\n\n0.01 :  loss: 0.4848 - accuracy: 1.0000\n\n0.1:    loss: 0.0788 - accuracy: 1.0000\n"}}