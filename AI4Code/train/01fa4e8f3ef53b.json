{"cell_type":{"f6781968":"code","ce534a32":"code","8d793c49":"code","7993ebb0":"code","aee644ba":"code","fa508873":"code","be530dd6":"code","f1dcedb1":"code","d4756501":"code","b852b4d7":"code","aaaffbe3":"code","e84bb472":"code","251b6596":"code","bd2f8815":"code","7cfa6084":"code","e2024db8":"code","8257f73e":"code","cee6f877":"code","c7fc369f":"code","cea6d6a5":"code","9127877e":"code","776f01f5":"code","59380338":"code","5f3e54f2":"code","d890b09b":"code","4df57be8":"code","2a337c68":"code","dbef0647":"code","fa5247ee":"code","82e5ebb2":"code","206580a4":"code","352d20af":"code","f3b29e67":"code","61fb38e7":"code","b75b6f43":"markdown","778f7f6c":"markdown","beb0dc3e":"markdown","641988aa":"markdown","d8885066":"markdown","ba023686":"markdown","3a57d9a7":"markdown","4d51400e":"markdown","02fa0865":"markdown","ac6ca8ba":"markdown","203b6944":"markdown","94fd7225":"markdown","6f09b209":"markdown","926c05c4":"markdown","4d188448":"markdown","8653d4e3":"markdown","4c09c44b":"markdown","7e7d0566":"markdown","6c5269f0":"markdown","d32d9311":"markdown","1291e5e0":"markdown","96c0d0a2":"markdown","9dbe1200":"markdown","5becb435":"markdown","99439725":"markdown"},"source":{"f6781968":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport seaborn as sns # data visualization\nimport matplotlib.pyplot as plt # data visualization\nfrom nltk.corpus import stopwords # text preprocessing - stopwords\nimport re, string, unicodedata # text preprocessing - regular expressions, string\nfrom bs4 import BeautifulSoup # html processing\nfrom wordcloud import WordCloud, STOPWORDS # visualizing word cloud from corpus & ignoring stopwords\nfrom collections import Counter # counter for most common words\nfrom sklearn.feature_extraction.text import CountVectorizer # feature-oriented counting of words\nfrom sklearn.model_selection import train_test_split # splitting the dataset into train & test sets\nfrom keras.preprocessing import text, sequence # word tokenization\nfrom keras.models import Sequential # class to construct the model\nfrom keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Flatten # RNN layers to be used\nfrom keras.optimizers import Adam # optimizer to be used\nfrom keras.callbacks import ReduceLROnPlateau # learning rate decay on plateau","ce534a32":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d793c49":"dataset = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndataset.head(5)","7993ebb0":"dataset.info()","aee644ba":"sns.set_style('darkgrid')\nsns.countplot(dataset.sentiment)","fa508873":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","be530dd6":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n            final_text.append(i.strip().lower())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndataset['review']=dataset['review'].apply(denoise_text)","f1dcedb1":"plt.figure(figsize=(20,20))\ncloud = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop).generate(\" \".join(dataset[dataset.sentiment == 'positive'].review))\nplt.grid(b=None)\nplt.imshow(cloud, interpolation='bilinear')","d4756501":"plt.figure(figsize=(20,20))\ncloud = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop).generate(\" \".join(dataset[dataset.sentiment == 'negative'].review))\nplt.grid(b=None)\nplt.imshow(cloud, interpolation='bilinear')","b852b4d7":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 8))\ntext_len = dataset[dataset['sentiment'] == 'positive']['review'].str.len()\nax1.set_title('Positive Reviews')\nax1.hist(text_len, color='green')\ntext_len = dataset[dataset['sentiment'] == 'negative']['review'].str.len()\nax2.set_title('Negative Reviews')\nax2.hist(text_len, color='red')\nfig.suptitle('Character Count in Reviews')","aaaffbe3":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 8))\ntext_len = dataset[dataset['sentiment'] == 'positive']['review'].str.split().map(lambda x: len(x))\nax1.set_title('Positive Reviews')\nax1.hist(text_len, color='green')\ntext_len = dataset[dataset['sentiment'] == 'negative']['review'].str.split().map(lambda x: len(x))\nax2.set_title('Negative Reviews')\nax2.hist(text_len, color='red')\nfig.suptitle('Word Count in Reviews')","e84bb472":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 8))\ntext_len = dataset[dataset['sentiment'] == 'positive']['review'].str.split().apply(lambda x: [len(i) for i in x])\nprint(\"Test\")\nax1.set_title('Positive Reviews')\nsns.distplot(text_len.map(lambda x: np.mean(x)), ax=ax1, color='green')\ntext_len = dataset[dataset['sentiment'] == 'negative']['review'].str.split().apply(lambda x: [len(i) for i in x])\nax2.set_title('Negative Reviews')\nsns.distplot(text_len.map(lambda x: np.mean(x)), ax=ax2, color='red')\nfig.suptitle('Average Word Length in Reviews')","251b6596":"dataset['sentiment'] = pd.get_dummies(dataset['sentiment']).drop(['negative'], axis=1)","bd2f8815":"def get_corpus(texts):\n    \n    words = []\n    \n    for text in texts:\n        for word in text.split():\n            words.append(word.strip())\n    \n    return words\n\ncorpus = get_corpus(dataset.review)\n\ncorpus[:5]","7cfa6084":"print(len(corpus))","e2024db8":"counter = Counter(corpus)\nmost_common = dict(counter.most_common(10))\nmost_common","8257f73e":"def get_top_ngrams(corpus, n, g):\n    \n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","cee6f877":"plt.figure(figsize=(16, 9))\nmost_common_uni = dict(get_top_ngrams(dataset.review, 20, 1))\nsns.barplot(x=list(most_common_uni.values()), y=list(most_common_uni.keys()))","c7fc369f":"plt.figure(figsize=(16, 9))\nmost_common_bi = dict(get_top_ngrams(dataset.review, 20, 2))\nsns.barplot(x=list(most_common_bi.values()), y=list(most_common_bi.keys()))","cea6d6a5":"plt.figure(figsize=(16, 9))\nmost_common_uni = dict(get_top_ngrams(dataset.review, 20, 3))\nsns.barplot(x=list(most_common_uni.values()), y=list(most_common_uni.keys()))","9127877e":"X_train, X_test, y_train, y_test = train_test_split(dataset.review, dataset.sentiment, train_size=0.9, random_state=0)\nX_test_temp = X_test\ny_test_temp = y_test","776f01f5":"max_features = 10000\nmax_len = 128","59380338":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)","5f3e54f2":"tokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=max_len)\n\ntokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=max_len) ","d890b09b":"EMBEDDING_FILE = \"..\/input\/glove840b300dtxt\/glove.840B.300d.txt\"","4df57be8":"def get_coeffs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\nembeddings_dict = dict(get_coeffs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","2a337c68":"all_embs = np.stack(embeddings_dict.values())\nemb_mean, emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnum_words = min(max_features, len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embed_size))\n\nfor word, i in word_index.items():\n    \n    if i >= num_words: continue\n    embedding_vector = embeddings_dict.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","dbef0647":"batch_size = 256\nepochs=10\nembed_size=300","fa5247ee":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.5, min_lr=0.00001)","82e5ebb2":"model = Sequential()\n\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=max_len, trainable=False))\nmodel.add(Bidirectional(LSTM(units=128)))\nmodel.add(Dropout(rate=0.8))\nmodel.add(Dense(units=16, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(lr=0.002), loss='binary_crossentropy', metrics=['accuracy'])","206580a4":"model.summary()","352d20af":"history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[learning_rate_reduction])","f3b29e67":"print(\"Model Accuracy on Training Data: \", round(model.evaluate(X_train, y_train)[1]*100), \"%\")\nprint(\"Model Accuraccy on Testing Data: \", round(model.evaluate(X_test, y_test)[1]*100), \"%\")","61fb38e7":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nax1.set_title('Training & Testing Accuracy')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Accuracy')\nax1.legend(['accuracy', 'val_accuracy'])\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nax2.set_title('Training & Testing Loss')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss')\nax2.legend(['loss', 'val_loss'])\n","b75b6f43":"# Dataset Balance\nFor the learning process to be consistent and effective, we have to make sure the dataset is balanced, luckily it turns out that this dataset is well balanced, with approximately 25K samples for each class (positive & negative)","778f7f6c":"# Text Preprocessing\nThe text data is always not perfect, and that's why we need to remove some parts of the text, such as URLs, HTML tags, words between square brackets, and frequent stop words such as (the, this, ..., etc) to make the learning process easier and less prone to errors","beb0dc3e":"The input data we have consists of 2 files, the dataset, and a \"word embedding\" file, but we'll get to this later.","641988aa":"# Creating & Training the Model\nFor this particular problem we're going to use a Recurrent Neural Network (RNN), which is suitable for sequence data, with Bidirectional Long-Short-Term-Memory (LSTM) units, which do a pretty good job at memorizing important parts of a sentence, so it does not \"forget\" an important negation for example if a movie fan said: \"The movie, despite being ............, didn't amuse me\".","d8885066":"**Creating the Embedding Matrix**","ba023686":"For this problem, we saw that the most frequent review length is about 100-120 words, and because the Network is fixed, we're going to use 128 as the max length of a training example, pad the shorter examples and truncate the longer ones.","3a57d9a7":"In this notebook, we're going to do some analysis on a 50K movie review dataset, containing both positive and negative reviews, to better understand how movie fans and critics describe what they like and what they don't like about movies, but before we get started, I'd like to acknowledge [Madz2000](https:\/\/www.kaggle.com\/madz2000) as I followed his style in performing analysis.","4d51400e":"# Importing Libraries\nFirst we're going to import the libraries we need for this task","02fa0865":"# Count Frequencies\nIn this section we're going to do some analysis to find out what's the most frequent character count, review length and even character count in a single word, and from this analysis, it turns out that people tend to write ***less*** about movies they didn't like ","ac6ca8ba":"**Word Count in Reviews**","203b6944":"**Character Count in Reviews**","94fd7225":"**Average Word Length in Reviews**","6f09b209":"# Analysis After Training\nWhen evaluating the sentiment, f a given text document, research shows that human analysts tend to agree around 80-85% of the time. This is the baseline we (usually) try to meet or beat when we\u2019re training a sentiment scoring system, so looking at our results, we've achieved 86% accuracy on the test set, which is pretty good, although there is a room for improvement.","926c05c4":"**Trigram Analysis**","4d188448":"**Bigram Analysis**","8653d4e3":"# Word Representations\nThere are many different ways to represent text for a model, because of course, models only understand numerical values. We're going to address 2 of them here: One-hot vectors and Word Embeddings\n\n**One-hot-vectors**\n\n![](https:\/\/static.bookstack.cn\/projects\/DeepLearning.ai-Summary\/5-%20Sequence%20Models\/Images\/27.png)\n\nOne-hot encoding is a naive way to represent words, by simply sorting the most frequent words in a vector, and for the word occurrence in a sentence, it is replaced by the vector with it's value set to 1 and all other values are 0s\n\nFor example, if the first word here is 'Movies', it will be represented as [1, 0, 0, 0]\n\nA big disadvantage of one-hot-encoding that it doesn't represent the difference and similarity between words, so it treats words such as 'Movie' and 'Film' as two completely different words, although the meaning is very close.\n\n**Word Embeddings**\n\n![](https:\/\/x-wei.github.io\/images\/Ng_DLMooc_c5wk2\/pasted_image.png)\n\nOn the other hand, a word embedding is a pre-trained feature representation of a word learned from a larger corpus, which helps the model to distinguish more between words and treat words like 'Movie' and 'Film' as more similar words than 'Movie' and 'Character'\n\nIn this notebook we're going to use a pre-trained word embeddings","4c09c44b":"**Setting up hyperparameters for training**","7e7d0566":"# IMDb Sentiment Analysis\n\n![IMDb](https:\/\/pmcvariety.files.wordpress.com\/2017\/02\/imdb1.png?w=600)\n\nIMDb (also known as the Internet Movie Database) is an online database owned by Amazon of information related to films, television programs, home videos, video games, and streaming content online \u2013 including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews.","6c5269f0":"# Word Clouds\nA Word cloud is a good informative image of the most frequent words in a text, with the most frequent words being the biggest ones, and the less frequent words being the small ones, this can lead us to know what this dataset is all about, which is, of course, Movies, Characters, Stories, Shows, etc.","d32d9311":"**Word Tokenization**","1291e5e0":"# N-Gram Analysis\nAn n-gram (unigram, bigram, trigram) is a way of analyzing what words and phrases are used in a text, which gives better insight about what was used most frequently, it turns out that the phrase \"Over the top\" is the most frequently used, maybe to describe something that is extremely good like a performance, a scenario, etc.","96c0d0a2":"# Conclusion\nTransformer models such as BERT have achieved a quite high (+90%) accuracy on this dataset, but that will be a topic for another time. \nIf you found this notebook helpful, an upvote is most appreciated :D","9dbe1200":"**Unigram Analysis**","5becb435":"# Train-Test-Split\nAn important step in the Learning process as it enables us to test the model's performance on data it has never seen it before","99439725":"# Data Analysis and Preprocessing\nFirst, we're going to view the dataset to better understand the columns"}}