{"cell_type":{"0afb20f4":"code","fbd189d3":"code","b65fddeb":"code","818d8a8a":"code","929c9a5f":"code","feef16d1":"code","2492e00d":"code","0c53d837":"code","0570ad4b":"code","e29a1dbb":"code","e57af76c":"code","005df631":"code","60d44790":"code","3ee2087d":"code","aaf9609a":"code","9d3b6ba3":"code","8bdbb6c8":"code","dd2e60d0":"code","b2ac4323":"code","d7746215":"code","4a055c43":"code","d877478c":"markdown","f6f176f7":"markdown","bc321f40":"markdown","e7534114":"markdown","eb167219":"markdown","406ea3bf":"markdown","4789f635":"markdown","47342bfe":"markdown","17474cb8":"markdown","d0c94ea0":"markdown","5df90148":"markdown","3e301f28":"markdown"},"source":{"0afb20f4":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage","fbd189d3":"from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()","b65fddeb":"df = pd.read_csv('..\/input\/indian-liver-patients-feature-selection-part-2\/liver_reduced_features.csv')\ndf.head()","818d8a8a":"X = df.values[:, :-1] # numpy array\ny = df.values[:,-1] # numpy array","929c9a5f":"scaler = StandardScaler() \nX_scaled = scaler.fit_transform(X.T).T \nX_scaled","feef16d1":"pca = PCA(n_components=2) #2-dimensional PCA transformation\nX_pca = pd.DataFrame(pca.fit_transform(X_scaled))","2492e00d":"var = pd.DataFrame({'var':pca.explained_variance_ratio_,'PC':['pc1','pc2']})\nsns.barplot(x='PC',y=\"var\",data=var, color=\"c\")\nplt.show()","0c53d837":"print(f'pca explains {round(pca.explained_variance_ratio_[0]+pca.explained_variance_ratio_[1],2)*100}% of the initial variation of the data')","0570ad4b":"plt.figure(figsize=(8,5))\nplt.title('Indian Liver Disease Data reduced in 2-D using PCA')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\n\nsns.scatterplot(x=X_pca[0], y=X_pca[1], color='purple', hue = df.iloc[:,-1], palette = 'Set2')\nplt.show()","e29a1dbb":"sns.set_theme()\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X_pca.iloc[:,:-1])\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss, marker = 'o')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","e57af76c":"kmeans = KMeans(n_clusters=3,  n_init=20,  precompute_distances=True, random_state=0, verbose=2)\nkmeans.fit(X_pca)\nk_means_labels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nX_pca['target'] = df['liver_disease']","005df631":"plt.figure(figsize = (9,7))\n## plot the clusters\n\nplt.scatter(x = X_pca[k_means_labels == 0][0], y = X_pca[k_means_labels == 0][1], c = 'r', label = 'class 1')\nplt.scatter(x = X_pca[k_means_labels == 1][0], y = X_pca[k_means_labels == 1][1], c = 'g', label = 'class 2')\nplt.scatter(x = X_pca[k_means_labels == 2][0], y = X_pca[k_means_labels == 2][1], c = 'b', label = 'class 3')\n\n##plot the centroids\n\nplt.scatter(x =centroids[0][0], y = centroids[0][1], c = 'black', marker = 'X', s=100 )\nplt.scatter(x =centroids[1][0], y = centroids[1][1], c = 'black' , marker = 'X',s=100)\nplt.scatter(x =centroids[2][0], y = centroids[2][1] ,c='black',  marker = 'X', s=100)\nplt.title('K Means for PCA dataset 2-D')\nplt.legend()\nplt.show()","60d44790":"from sklearn.neighbors import NearestNeighbors","3ee2087d":"neigh = NearestNeighbors(n_neighbors=3)\nnbrs = neigh.fit(X_pca.iloc[:,:-1])\ndistances, indices = nbrs.kneighbors(X_pca.iloc[:,:-1])","aaf9609a":"distances = np.sort(distances[:,-1], axis = 0)","9d3b6ba3":"plt.plot(distances)\nplt.xlabel('Number of points')\nplt.ylabel('Distance')\nplt.show()","8bdbb6c8":"db = DBSCAN(eps=0.12, min_samples=3)\ny_pred = db.fit_predict(X_pca.iloc[:,:-1])\nplt.figure(figsize=(10,5))\nsns.scatterplot(x = X_pca[0], y = X_pca[1],hue =y_pred, palette= 'Set2', color='purple')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title(\"DBSCAN\")\nplt.show()","dd2e60d0":"hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X_pca.iloc[:,:-1])","b2ac4323":"Z = linkage(X_pca.iloc[:,:-1], 'ward')","d7746215":"plt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()","4a055c43":"plt.figure(figsize = (8,6))\n## plot the clusters\nplt.scatter(x = X_pca[y_hc == 0][0], y = X_pca[y_hc == 0][1], c = 'r', label = 'class 1')\nplt.scatter(x = X_pca[y_hc == 1][0], y = X_pca[y_hc == 1][1], c = 'g', label = 'class 2')\n\n\nplt.title('Agglomerative Hierarchical Clustering for PCA dataset 2-D')\nplt.legend()\nplt.show()","d877478c":"### Import the modules","f6f176f7":"K means clustering seems to not distinguish the classes. \n## DBSCAN \nIn order to understand how DBSCAN works, we have to define the folloing:\n* ***MinPts***: minimun number of points (neighbors) that must exist within a circe of radius *eps* and center a specific point in order to classify it as core point.\n* ***eps*** defines the radius of neighborhood around a point x.\n\nWe classify all the points of a dataset as:\n1. **core point**: if there are at least ***MinPts*** points within a circle of radius ***eps***.\n1. **border point**: if it contains at least 1 core point in its neighbor.\n3. **noise point**: if it is neither a core point nor a border point.\n\n### DBSCAN Algorithm (simplified)\n1. Scan the database.\n2. Assign each point to one of the following categories: core point, border point or noise.\n3. Reject all the noise points.\n4. Connect all the core points within a distance \u03b5 and create a separate cluster.\n5. Assign each border point to the cluster which is reachable from a core point.\n","bc321f40":"## Determing the value of eps","e7534114":"## K Means Algorithm & Elbow Rule\nK means is a quite simple unsupervised machine learning algorithm. It works as follows:\n1. We define k, which is the number of clusters\n2. The algorithm selects randomly k-points of the dataset and use them as initial centroids\n3. The algorithm calculates the distance (we have to define a metric, a proximity measure) between the k centroids and all the other points of the dataset.\n4. Each point is assigned to the cluster, in which it has the smallest distance from the controid\n5. Reassignes the centroids. \n6. The algorith stops when the centroids do not change any more.\n","eb167219":"### Data Scaling ","406ea3bf":"### Principal Components Analysis\nIn order to visualize the results, we have to reduce the dimensions. Our dataset (after feature selection) contains 6 independent variables, so we are in the 6 dimensional space. We will use Principal Component Analysis (PCA), in order to create new components where each new component will be a linear composition of the initial coordinate system. Then, we are going to check how much of the variation, the new components explain.","4789f635":"## Indian Liver Disease Patients: Clustering (3\/5)\nWe implement clustering algorithms like K Means, DBSCAN and Agglomerative Hierarchical Clustering. Also, we visualize the data in 2-D, reducing the dimension with PCA.","47342bfe":"## Agglomerative Hierarchical Clustering\nLastly, we are going to implement Agglomerative Hierarchical Clustering. This type of algorithm starts by classify every single point as an individual cluster. In every step we calculate the so-called the proximity matrix which stores the distance between every cluster. In each step we merge the closest clusters, by implementing a distance metric. In the last step, all the points are assigned to one and only cluster.","17474cb8":"$ k $ is user defined parameter. We want to minimize an objective function known as intra cluster variation or within-cluster-sum of squares (WCSS) which is the sum of square distances of sample to their closest cluster center. <\/p>\n\nThe WCSS of $ k $ cluster is given by: $$ W(C_k) = \\sum_{xi\\ \\in C_k}{(x_i-m_k)}^2 $$ \nThe total WCSS for all the clusters: $$ \\sum_{k=1}^{n}\\sum_{xi\\ \\in C_k}{(x_i-m_k)}^2 $$ We want the last measure to be as small as possible.\n\n\n<p> The graph above plots number of clusters as a function of WCSS. We will opt the number of clusters so that if i add one more cluster, the wcss will not improve that much. So we will choose  $ k=3 $.\n    \n## Implementation of Kmeans algorithm with $ k = 3 $\nWe use Euclidean distance as distance metric\n    $$ \\sqrt{\\sum_{i=1}^{n}{(x_i-y_i)}^2} $$","d0c94ea0":"## Plot Data in 2-D","5df90148":"DBSCAN considers dense region as one cluster","3e301f28":"### Putting it all together:\nNone of the clustering algorithms seems to find the real classes. PCA was very informative because we understand that:\n* the classes are overlapping\n* there is noise\n\n### ~ end of clustering ~"}}