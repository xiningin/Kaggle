{"cell_type":{"c8edc8bb":"code","bb44ad72":"code","9acf4c66":"code","f0123bdd":"code","d8ab385e":"code","f5042802":"code","ae78c426":"code","68754055":"code","714dfa0f":"code","c528a7d2":"code","7d557988":"code","8cc2b8c9":"code","a65e9a94":"code","2365bf17":"code","adefaae4":"code","d589d726":"code","7b7cb7fe":"code","65c3847b":"code","4d8b3c5c":"code","6252083f":"code","432fa0e0":"markdown","6e730c8e":"markdown","d60bedbf":"markdown","e13bfdae":"markdown","f445f0c3":"markdown","802d57aa":"markdown","540a79f3":"markdown","549bca17":"markdown","35b06cd8":"markdown","465af5e0":"markdown","e4f31c98":"markdown","837b25f6":"markdown","ca52cbd1":"markdown","ea16ddc5":"markdown","5ec8a05a":"markdown","c1046247":"markdown","3f9b7db3":"markdown","9a40eb92":"markdown","45574d0e":"markdown","78614765":"markdown"},"source":{"c8edc8bb":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","bb44ad72":"df = pd.read_csv('..\/input\/boston-housing-price\/housing_price.csv')\ndf.head()","9acf4c66":"df.shape","f0123bdd":"df = df.fillna(df.mean())","d8ab385e":"X = pd.DataFrame(df.iloc[:, 0:13])\ny = pd.DataFrame(df.iloc[:, 13:14])","f5042802":"scaling = StandardScaler()\nX_std = scaling.fit_transform(df.iloc[:, 0:13])","ae78c426":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2, random_state = 1)","68754055":"lr1 = LinearRegression()\nmodel1 = lr1.fit(X_train, y_train)","714dfa0f":"coefs1 = dict(zip(X.columns, abs(lr1.coef_[0])))\ncoefs1","c528a7d2":"y_pred = lr1.predict(X_test)","7d557988":"mse1 = mean_squared_error(y_test, y_pred)\nmse1.round(4)","8cc2b8c9":"r2_score(y_test, y_pred).round(4)","a65e9a94":"lr1.score(X_test, y_test).round(4)","2365bf17":"rfe = RFE(estimator = LinearRegression(), n_features_to_select = 11, verbose = 1)\nrfe.fit(X_train, y_train)","adefaae4":"X.columns[rfe.support_]","d589d726":"print(dict(zip(X.columns, rfe.ranking_)))","7b7cb7fe":"y1_pred = rfe.predict(X_test)","65c3847b":"mse2 = mean_squared_error(y_test, y1_pred)\nmse2.round(4)","4d8b3c5c":"r2_score(y_test, y1_pred).round(4)","6252083f":"rfe.score(X_test, y_test).round(4)","432fa0e0":"## Shape of the dataset\nThere are 14 columns as well as variables in the dataset. Our main goal is to drop the unnecessary variables without reducing the model performance ","6e730c8e":"<br\/>\n\n## List of variables RFE choose for our model","d60bedbf":"<br\/>\n<br\/>\n\n# Here comes the RFE as a lifesaver!","e13bfdae":"<br\/>\n\n## Linear Regression Model with all the variables","f445f0c3":"<br\/>\n<br\/>\n\n## Conclusion:\nIn the first model, we worked with 13 variables and got mse value 23.448. But in the second model with applying RFE, we got 11 variables to work with. In this time, we got mse value 23.4293 which is almost same to the first one. More precisely speaking, the second one is a little bit better.\n<br\/>\n<br\/>\n\nSometimes we may have to adjust the 'n_features_to_select' parameters value to find the model performance close to the first one or a better one.","802d57aa":"## Prediction","540a79f3":"<br\/>\n\n## Observing coefficients of each variables","549bca17":"## Feature scaling\n\n#### Note:\nFeature scaling is very important in this regard. If we see how Recursive Feature Elimination (RFE) works, we will be able to understand it's importance.  \n<br\/>\n\nIn short, Firstly it calculates the coefficients of each variables and then removes the variable which coefficient is close to zero. Because when this coefficient will be multiplied by the variable, it won't bear that much importance to the model. But if we don't scale the variables, we can't compare the coefficients with each other.","35b06cd8":"## Importing necessary libraries","465af5e0":"## Load the dataset\nThis time I am using 'Boston Housing Price' dataset collected from kaggle. Hope you all are well acquainted with it.","e4f31c98":"<br\/>\n\n## Model Evaluation","837b25f6":"## Splitting into train and test set","ca52cbd1":"## Have a close look\n\nThe higher values mean that they were dropped at the early stage and the smaller values mean that they survive till the end and prove themselves fittest for the model. Survival of the fittest!","ea16ddc5":"## Dealing with missing values","5ec8a05a":"## Prediction","c1046247":"<br\/>\n<br\/>\n\n# One more time saving tips","3f9b7db3":"# Recursive Feature Elimination\n<br\/>\n<br\/>\n","9a40eb92":"<br\/>\n\n## Model Evaluation","45574d0e":"## Splitting into explanatory and response variables","78614765":"Don't try to measure the model performance with 'accuracy_score' like me. 'accuracy_score' is only for classification model. I spent almost half an hour with this problem and every time I got 'ValueError: continuous is not supported'. Thanks to Kaggle and Stake Overflow for helping me to solve this issue!\n<br\/>\n<br\/>\n\n## Feel free to share your thoughts and if you find it helpful, please upvote. Thanks!"}}