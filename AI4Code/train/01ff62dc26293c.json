{"cell_type":{"b0874275":"code","b22b40eb":"code","4824b2a5":"code","91e76bf5":"code","8d546d13":"code","28685028":"code","0b06ee7d":"code","5deddf20":"code","0a1aef38":"code","f0e9671b":"code","0a884824":"code","28527a7d":"code","809c5738":"code","82909b81":"code","a01f7111":"code","17114843":"code","86f24ae2":"code","5db6a614":"code","082709c6":"code","ead41500":"code","1e5d346f":"code","acf00bfd":"code","936f691c":"code","81791e31":"code","4b062c3a":"code","5cdc09b6":"markdown","803cbe04":"markdown","ad270da9":"markdown","8dd13600":"markdown","7c5d00b1":"markdown","7733bd14":"markdown","c980c87e":"markdown","26a30deb":"markdown"},"source":{"b0874275":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB tothe current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b22b40eb":"# Import Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import mode\n# Installing Facebook Prophet","4824b2a5":"# Load the data\ndata_path = \"..\/input\/daily-website-visitors\/daily-website-visitors.csv\"\ndata = pd.read_csv(data_path)","91e76bf5":"data.head()","8d546d13":"# Data preprocessing\n\n'''\n1.Convert Date into Datetime format.\n2.Removing ',' from Page.Loads, Unique.Visits, First.Time.Visits, Returning.Visits.\n3.Convert the above values into float.\n'''\n\ndata.info()","28685028":"# Function to remove commas\ndef remove_commas(x):\n    return float(x.replace(',', ''))","0b06ee7d":"# Apply the preprocessing functions\n\ndata['Date'] = pd.to_datetime(data['Date'])\ndata['Page.Loads'] = data['Page.Loads'].apply(lambda x : remove_commas(x))\ndata['Unique.Visits'] = data['Unique.Visits'].apply(lambda x : remove_commas(x))\ndata['First.Time.Visits'] = data['First.Time.Visits'].apply(lambda x : remove_commas(x))\ndata['Returning.Visits'] = data['Returning.Visits'].apply(lambda x : remove_commas(x))","5deddf20":"data.head()","0a1aef38":"# Frequency distribution of each continuous column\ncols_to_plot = ['Page.Loads', 'Unique.Visits', 'First.Time.Visits', 'Returning.Visits']\nplt.figure(figsize=(15, 15))\nfor i, col in enumerate(cols_to_plot):\n    plt.subplot(2, 2, i+1)\n    sns.histplot(data=data, x=col)","f0e9671b":"def check_normality(data, col):\n    \n    # Compute mean\n    mean = int(np.mean(data[col]))\n    median = int(np.median(data[col]))\n    mode_ = int(mode(data[col])[0][0])\n    \n    print(\"mean\", \":\", mean, \"median\", \":\", median, \"mode\", \":\", mode_)\n    if mean == median == mode_:\n        print(\"{} Distribution is Normal\".format(col))\n    elif mean > median and mean > mode_ and mode_ < median:\n        print(\"{} Distribution is skewed towards right\".format(col))\n    else:\n        print(\"{} Distribution is skewed towards left\".format(col))\n\n        ","0a884824":"for col in cols_to_plot:\n    check_normality(data, col)","28527a7d":"# Perform the EDA\n\nfigure, ax = plt.subplots(2, 2, figsize=(17, 15))\nplt.style.use('seaborn')\n\nax1 = ax[0]\nax2 = ax[1]\n\n# Plot the Number of Page Loads with time\nax1[0].plot(data['Date'], data['Page.Loads'])\nax1[0].set_xlabel(\"Date\")\nax1[0].set_ylabel(\"Number of Page Loads\")\n\n# Plot the Number of Unique Visits with time\nax1[1].plot(data['Date'], data['Unique.Visits'])\nax1[1].set_xlabel(\"Date\")\nax1[1].set_ylabel(\"Number of Unique Visits\")\n\n# Plot the Number of First Time visits with time\nax2[0].plot(data['Date'], data['First.Time.Visits'])\nax2[0].set_xlabel(\"Date\")\nax2[0].set_ylabel(\"Number of First Time visits\")\n\n# Plot the Number of Returning visits with time\nax2[1].plot(data['Date'], data['Returning.Visits'])\nax2[1].set_xlabel(\"Date\")\nax2[1].set_ylabel(\"Number of Returning visits\")\n\nfigure.show()","809c5738":"# group the data by day and draw insights\nday_grouped_data = data.groupby('Day')","82909b81":"# Perform the EDA\n\ndef day_wise_EDA(day):\n    sun_data = day_grouped_data.get_group(day)\n    figure, ax = plt.subplots(2, 2, figsize=(17, 15))\n    plt.style.use('seaborn')\n\n    ax1 = ax[0]\n    ax2 = ax[1]\n\n    # Plot the Number of Page Loads with time\n    print(\"=================================================================={} ANALYSIS======================================================\".format(day.upper()))\n    ax1[0].plot(sun_data['Date'], sun_data['Page.Loads'])\n    ax1[0].set_xlabel(\"Date\")\n    ax1[0].set_ylabel(\"Number of Page Loads\")\n\n    # Plot the Number of Unique Visits with time\n    ax1[1].plot(sun_data['Date'], sun_data['Unique.Visits'])\n    ax1[1].set_xlabel(\"Date\")\n    ax1[1].set_ylabel(\"Number of Unique Visits\")\n\n    # Plot the Number of First Time visits with time\n    ax2[0].plot(sun_data['Date'], sun_data['First.Time.Visits'])\n    ax2[0].set_xlabel(\"Date\")\n    ax2[0].set_ylabel(\"Number of First Time visits\")\n\n    # Plot the Number of Returning visits with time\n    ax2[1].plot(sun_data['Date'], sun_data['Returning.Visits'])\n    ax2[1].set_xlabel(\"Date\")\n    ax2[1].set_ylabel(\"Number of Returning visits\")\n\n    figure.show()","a01f7111":"# Call the above function for every day\n\n# 1. Sunday\nday_wise_EDA('Sunday')","17114843":"# 2. Monday\nday_wise_EDA('Monday')","86f24ae2":"# 3. Tuesday\nday_wise_EDA('Tuesday')","5db6a614":"# 4. Wednesday\nday_wise_EDA('Wednesday')","082709c6":"# 5. Thursday\nday_wise_EDA('Thursday')","ead41500":"# 6. Friday\nday_wise_EDA('Friday')","1e5d346f":"# 7. Saturday\nday_wise_EDA('Saturday')","acf00bfd":"avg_day_data = day_grouped_data.mean().reset_index().drop('Row', axis=1)\navg_day_data","936f691c":"# Plot the Bargraph for every continuous variable across day\ncols_to_plot = ['Page.Loads', 'Unique.Visits', 'First.Time.Visits', 'Returning.Visits']\nplt.figure(figsize=(15, 15))\nfor i, col in enumerate(cols_to_plot):\n    plt.subplot(2, 2, i+1)\n    sns.barplot(data=avg_day_data.sort_values(by=col, ascending=False), x='Day', y=col)","81791e31":"# Boxplots for all the continuous columns across day\ncols_to_plot = ['Page.Loads', 'Unique.Visits', 'First.Time.Visits', 'Returning.Visits']\nplt.figure(figsize=(15, 15))\nfor i, col in enumerate(cols_to_plot):\n    plt.subplot(2, 2, i+1)\n    sns.boxplot(data=data, x='Day', y=col)","4b062c3a":"#Plot the correlation heatmap\ncorr_matrix = data.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr_matrix, annot=True, cbar=False)\nplt.show()","5cdc09b6":"**Although, the distributions seem to look normal, but they fail the normality test. Hence, it can be concluded that the distributions are not normal.**","803cbe04":"## Average Value Counts for each day","ad270da9":"## Day wise analysis","8dd13600":"# EDA","7c5d00b1":"**Visually the distributions appear to be normal**","7733bd14":"**I hope you found this helpful. Please do upvote to support. Also, I will be soon uploading a new notebook where I try to generate forecasts using Facebook Prophet. Stay tuned for that and Thank you for viewing my notebook. Happy Learning! :)","c980c87e":"**High positive correlation can be observed between the following features:**\n1. Page.Loads and Returning.Visits\n2. Returning.Visits and Unique.Visits\n3. Returning.Visits and First.Time.Visits\n","26a30deb":"***In this notebook, I try to do draw insights and relations between variaous features from the dataset. I hope this notebook will help you to get a better insight of the data. If you find this helpful, please do upvote. In my next notebook, I will try to make forecasting using Facebook Prophet.***"}}