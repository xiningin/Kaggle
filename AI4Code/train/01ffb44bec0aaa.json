{"cell_type":{"7fe81ed6":"code","1aa008e0":"code","99c7d15a":"code","146f428c":"code","d749200f":"code","fe72c327":"code","be142596":"code","4ea90b74":"code","97789987":"code","b03bab85":"code","0385a83d":"code","c39fe342":"code","4927a556":"code","ed573b87":"code","a9ce581c":"code","18cbe684":"code","98bf4654":"code","5d80b199":"code","b25ccd34":"code","ffd5cc9a":"code","4437ef09":"code","ede18151":"code","cb63c8c4":"code","2724218d":"code","b0dc9280":"code","bf746be1":"code","9804b050":"code","2a09196b":"code","f3bd0998":"code","e09ed81f":"code","acaf8726":"code","7915d26f":"code","de820689":"code","b2087be7":"code","84c96b93":"code","6c8a6b6d":"code","fb2d17a6":"code","46d5a7e8":"code","0ca0d237":"markdown","a93d6764":"markdown","5e634a34":"markdown","8d9b9e2a":"markdown","ee94a7e3":"markdown","af443480":"markdown","067cceb6":"markdown","ac59dd69":"markdown","a757635f":"markdown","45b0d256":"markdown","54235789":"markdown","68ff2a29":"markdown","14266377":"markdown","ae94b4d6":"markdown","75dcda1f":"markdown","bae2d5ff":"markdown","9e46a107":"markdown","c1307000":"markdown","a7db7c5b":"markdown","50990eef":"markdown","fe03071f":"markdown","7e1a5542":"markdown","8e7d65af":"markdown","3b06e67e":"markdown","e2158afd":"markdown","58a65f60":"markdown","8e9961d5":"markdown","40c2f698":"markdown","16c3777e":"markdown","ef4dc886":"markdown","df7465ec":"markdown","c89fd007":"markdown"},"source":{"7fe81ed6":"import pandas as pd # \u0111\u1ecdc v\u00e0 nghi\u00ean c\u1ee9u data\nimport seaborn as sns # v\u1ebd ra m\u1ed9t ch\u00fat \u0111\u1ec3 nghi\u00ean c\u1ee9u m\u1ed1i quan h\u1ec7 c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb.\nimport numpy as np # l\u00e0m to\u00e1n","1aa008e0":"train = pd.read_csv(r'..\/input\/protonx-tf03-linear-regression\/trainDataset.csv') # \u0111\u1ecdc d\u1eef li\u1ec7u train. \u0111\u00e2y l\u00e0 b\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 t\u1ea1o m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n.\ntest = pd.read_csv(r'..\/input\/protonx-tf03-linear-regression\/submissionDataset.csv') # \u0111\u1ecdc d\u1eef li\u1ec7u test. \u0111\u00e2y l\u00e0 b\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3 v\u00e0 n\u1ed9p b\u00e0i.","99c7d15a":"train.head(5) # 5 d\u00f2ng \u0111\u1ea7u ti\u00ean c\u1ee7a b\u1ed9 train s\u1ebd c\u00f3 g\u00ec?","146f428c":"train.isnull().sum() # ki\u1ec3m tra nh\u1eb9 xem c\u00f3 \u00f4ng n\u00e0o b\u1ecb null h\u00f4ng.","d749200f":"train.dtypes # c\u00e1c d\u1ea1ng d\u1eef li\u1ec7u c\u1ee7a b\u1ed9 d\u1eef li\u1ec7u train g\u1ed3m c\u00f3...","fe72c327":"train = train.drop('id', axis = 1) # x\u00f3a c\u1ed9t id. axis b\u1eb1ng 1 ngh\u0129a l\u00e0 x\u00f3a c\u1ed9t.\ntest = test.drop('id',axis = 1) # nh\u01b0 em \u0111\u00e3 n\u00f3i, l\u00e0m g\u00ec v\u1edbi train th\u00ec test c\u0169ng ph\u1ea3i thay \u0111\u1ed5i theo.","be142596":"# b\u1ea3ng t\u01b0\u01a1ng quan gi\u1eefa c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng s\u1ed1. \nsns.pairplot(train[['temp', 'atemp', 'hum', 'windspeed', 'cnt']], diag_kind='kde')","4ea90b74":"# \u0111\u1ebfm xem d\u1eef li\u1ec7u trong 4 m\u00f9a c\u00f3 \u0111\u1ec1u nhau h\u00f4ng.\nsns.countplot(data=train, x ='season', color='dodgerblue')","97789987":"# t\u01b0\u01a1ng t\u1ef1 l\u00e0 ph\u00e2n ph\u1ed1i gi\u1edd tr\u00ean b\u1ed9 train.\nsns.countplot(data=train, x ='hr', color='dodgerblue')","b03bab85":"# t\u00ednh t\u1ed5ng s\u1ed1 xe thu\u00ea d\u1ef1a tr\u00ean gi\u1edd v\u00e0 ph\u00e2n lo\u1ea1i d\u1ef1a tr\u00ean ng\u00e0y \u0111i l\u00e0m.\nhours = train.groupby(['hr', 'workingday'])['cnt'].agg('sum').unstack()\nhours.plot(kind='bar', figsize=(15,5), width=0.8)","0385a83d":"train['peak'] = train[['hr', 'workingday']]\\\n    .apply(lambda df: 1 if ((df['workingday'] == 1 and (df['hr'] == 8 or 17 <= df['hr'] <= 18)) \\\n                            or (df['workingday'] == 0 and 10 <= df['workingday'] <= 19)) else 0, axis = 1)\n\ntest['peak'] = test[['hr', 'workingday']]\\\n    .apply(lambda df: 1 if ((df['workingday'] == 1 and (df['hr'] == 8 or 17 <= df['hr'] <= 18)) \\\n                            or (df['workingday'] == 0 and 10 <= df['workingday'] <= 19)) else 0, axis = 1)","c39fe342":"train['ideal'] = train[['temp', 'windspeed']]\\\n    .apply(lambda df: 1 if (df['temp'] > 0.68 and df['windspeed'] < 0.30) else 0, axis = 1)\n\ntest['ideal'] = test[['temp', 'windspeed']]\\\n    .apply(lambda df: 1 if (df['temp'] > 0.68 and df['windspeed'] < 0.30) else 0, axis = 1)\n    \ntrain['sticky'] = train[['hum', 'workingday']]\\\n    .apply(lambda df: 1 if (df['workingday'] == 1 and df['hum'] >= 0.60) else 0, axis = 1)\n\ntest['sticky'] = test[['hum', 'workingday']]\\\n    .apply(lambda df: 1 if (df['workingday'] == 1 and df['hum'] >= 0.60) else 0, axis = 1)","4927a556":"# yr. Thay \u0111\u1ed5i to\u00e0n b\u1ed9 0 b\u1eb1ng -1 \u0111\u1ec3 2012 \u0111\u01b0\u1ee3c xu\u1ea5t hi\u1ec7n.\ntrain['yr'].replace({0: -1}, inplace = True)\ntest['yr'].replace({0: -1}, inplace = True)\n\n# weathersit. T\u1ea1o 4 c\u1ed9t m\u1edbi d\u1ef1a tr\u00ean 4 tham s\u1ed1 cho s\u1eb5n.\ntrain['weathersit'] = train['weathersit'].map({1: 'good', 2: 'okay', 3: 'norm', 4: 'bad'})\ntrain = pd.get_dummies(train, columns=['weathersit'], prefix='', prefix_sep='')\n\ntest['weathersit'] = test['weathersit'].map({1: 'good', 2: 'okay', 3: 'norm', 4: 'bad'})\ntest = pd.get_dummies(test, columns=['weathersit'], prefix='', prefix_sep='')\n\n# check qua l\u1ea1i xem b\u1ed9 train \u1ed5n ch\u01b0a\ntrain.head()","ed573b87":"# b\u1ed9 test tr\u00f4ng c\u0169ng \u1ed5n r\u1ed3i.\ntest.head()","a9ce581c":"train_dataset = train.sample(frac=0.8, random_state=0)\ntest_dataset = train.drop(train.index)\n\ntrain_features = train_dataset.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = train_features.pop('cnt')\ntest_labels = test_features.pop('cnt')","18cbe684":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nprint(tf.__version__)","98bf4654":"normalizer = preprocessing.Normalization()\n\nnormalizer.adapt(np.array(train_features))\n\n# ki\u1ec3m tra mean v\u00e0 variance\nprint(normalizer.mean.numpy())","5d80b199":"first = np.array(train_features[:1])\n\nwith np.printoptions(precision=2, suppress=True):\n    print('First example:', first)\n    print()\n    print('Normalized:', normalizer(first).numpy())","b25ccd34":"temp = np.array(train_features['temp'])\n\ntemp_normalizer = preprocessing.Normalization(input_shape=[1,])\ntemp_normalizer.adapt(temp)","ffd5cc9a":"temp_model = tf.keras.Sequential([\n    temp_normalizer,\n    layers.Dense(units=1)\n])\n\ntemp_model.summary()","4437ef09":"temp_model.predict(temp[:10])","ede18151":"temp_model.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n    loss='mean_squared_error')\n\ntemp_model.fit(\n    # m\u00f4 h\u00ecnh fit c\u1ed9t temp v\u00e0 c\u1ed9t cnt, t\u1ee9c b\u00e0i to\u00e1n tuy\u1ebfn t\u00ednh \u0111\u01a1n\n    train_features['temp'], train_labels,\n    epochs=100,\n    verbose=0,\n    # t\u00ednh to\u00e1n \u0111\u1ed9 th\u1ea9m \u0111\u1ecbnh d\u1ef1a tr\u00ean 20% b\u1ed9 train\n    validation_split = 0.2)","cb63c8c4":"x = tf.linspace(0.0, 1, 251)\ny = temp_model.predict(x)\n\nimport matplotlib.pyplot as plt\ndef plot_temp(x, y):\n    plt.scatter(train_features['temp'], train_labels, label='Data')\n    plt.plot(x, y, color='k', label='Predictions')\n    plt.xlabel('temp')\n    plt.ylabel('cnt')\n    plt.legend()\n\nplot_temp(x,y)","2724218d":"linear_model = tf.keras.Sequential([\n    normalizer, # l\u1edbp chu\u1ea9n h\u00f3a\n    layers.Dense(units=1)\n])\n\nlinear_model.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n    loss='mean_squared_error')\n\nlinear_model.fit(\n    # h\u1ecdc to\u00e0n b\u1ed9 c\u00e1c features, aka multiple regression.\n    train_features, train_labels, \n    epochs=100,\n    verbose=0,\n    validation_split = 0.2)","b0dc9280":"y = linear_model.predict(test.drop('cnt', axis = 1))\ny","bf746be1":"# Hu\u1ea5n luy\u1ec7n l\u1ea1i v\u1edbi to\u00e0n b\u1ed9 features v\u00e0 weight \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a log.\nlinear_model.fit(\n    train_features, np.log1p(train_labels), # bi\u1ebfn \u0111\u1ed5i train_labels (c\u1ed9t cnt)\n    epochs=50,\n    verbose=0,\n    validation_split = 0.2)","9804b050":"y_log = linear_model.predict(test.drop('cnt', axis = 1))\n\n# sau khi log h\u00f3a xong th\u00ec s\u1ebd tr\u1ea3 l\u1ea1i gi\u00e1 tr\u1ecb ban \u0111\u1ea7u b\u1eb1ng c\u00e1ch m\u0169 e.\ny_1 = np.exp(y_log) - 1 # th\u00eam 1 b\u1edbt l\u1ea1i 1\n\ny_1","2a09196b":"def build_and_compile_model(norm):\n    model = keras.Sequential([\n      norm,\n      layers.Dense(64, activation='relu'),\n      layers.Dense(64, activation='relu'),\n      layers.Dense(1)\n  ])\n\n    model.compile(loss='mean_squared_error',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n    return model\n\ndnn_model = build_and_compile_model(normalizer)\ndnn_model.summary()","f3bd0998":"dnn_model.fit(\n    train_features, np.log1p(train_labels),\n    validation_split=0.2,\n    verbose=0, epochs=100)","e09ed81f":"y_log2 = dnn_model.predict(test.drop('cnt', axis = 1))\n\ny_2 = np.exp(y_log2) - 1\n\ny_2.round() # l\u00e0m tr\u00f2n s\u1ed1. xe \u0111\u1ea1p thu\u00ea li\u1ec1n 1 c\u00e1i, ph\u1ea9y bao nhi\u00eau ch\u1eafc l\u00e0 thu\u00ea xe theo b\u1ed9 ph\u1eadn.","acaf8726":"from sklearn.ensemble import RandomForestRegressor \nfrom sklearn.model_selection import train_test_split","7915d26f":"# Chia b\u1ed9. \u0110\u1ea7u ra \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a b\u1eb1ng np.log1p\nX_train, X_test, y_train, y_test = train_test_split(train.drop('cnt', axis = 1), np.log1p(train.cnt), test_size=0.3, random_state = 42)","de820689":"# random forest model\nparams = {'n_estimators': 1000, 'max_depth': 12, 'random_state': 0, 'min_samples_split' : 5, 'n_jobs': -1}\nrf_model = RandomForestRegressor(**params)\nrf_cols = [\n    'weathersit', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday', 'sticky',\n    'hr', 'peak', 'good', 'okay','norm', 'bad'\n]\n\nrf_model.fit(X_train.values, y_train)","b2087be7":"y_rf_log = rf_model.predict(test.drop('cnt', axis = 1))\n\ny_rf = np.exp(y_rf_log) - 1\n\ny_rf.round()","84c96b93":"import catboost as cb\n\ntrain_dataset = cb.Pool(X_train, y_train) \ntest_dataset = cb.Pool(X_test, y_test)\n\nmodel = cb.CatBoostRegressor(loss_function='RMSE',\n                             colsample_bylevel=0.85, \n                             verbose=False)\n\ngrid = {'iterations': [250, 300],\n        'learning_rate': [0.05, 0.07, 0.1],\n        'max_depth': [10, 11, 12],\n        'l2_leaf_reg': [0.2, 0.5, 0.7, 1, 3],\n        'max_bin' : [100]}\n\nmodel.grid_search(grid, train_dataset)\n","6c8a6b6d":"y_cat_log = model.predict(test.drop('cnt', axis = 1))\n\ny_cat = np.exp(y_cat_log) - 1 \n\ny_cat.round()","fb2d17a6":"predicted = X_test.index\n\npred = np.exp(np.array(model.predict(X_test))) - 1\n\noriginal = np.array(train.loc[predicted,'cnt'])\n\ntrain.loc[predicted,'cnt']\n\ndef average_gap(l1,l2):\n    resu=0\n    for i in range(len(l1)):\n        resu += np.abs(l1[i]-l2[i])\n    resu = resu\/len(l1)\n    return(resu)\n\nplt.figure(figsize=(15,7))\nsns.distplot(pred, color=\"blue\", label=\"Distrib Predictions\", hist = False)\nsns.distplot(original, color=\"red\", label=\"Distrib Original\", hist = False)\nplt.title(\"Distribution of pred and original cnt\")\nplt.legend()","46d5a7e8":"# N\u1ed9p b\u00e0i, n\u1ed9p t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb b\u00ean tr\u00ean :p\nsubmission = pd.read_csv(r'..\/input\/protonx-tf03-linear-regression\/sample_submission.csv')\n\nsubmission.cnt = y_cat.round()\nsubmission.to_csv(\"submission_cat.csv\", index = False)\n\nsubmission.cnt = y_rf.round()\nsubmission.to_csv(\"submission_rf.csv\", index = False)\n\n# S\u1eed d\u1ee5ng 2 m\u00f4 h\u00ecnh catboost v\u00e0 randomforest \u0111\u1ec3 n\u1ed9p\nsubmission.cnt = (y_cat * .6 + y_rf * .4).round()\nsubmission.to_csv(\"submission_cat_rf.csv\", index = False)\n\nsubmission.cnt = y_1.round()\nsubmission.to_csv(\"submission_DNN_1.csv\", index = False)\n\nsubmission.cnt = y_2.round()\nsubmission.to_csv(\"submission_DNN_2.csv\", index = False)","0ca0d237":"#### Thay \u0111\u1ed5i d\u1eef li\u1ec7u\n\nNgo\u00e0i vi\u1ec7c th\u00eam m\u1ed9t v\u00e0i c\u1ed9t, em c\u0169ng c\u1ea7n ph\u1ea3i chuy\u1ec3n \u0111\u1ed5i tham s\u1ed1 m\u1ed9t v\u00e0i c\u1ed9t. 2 v\u00ed d\u1ee5 em nh\u1eafm t\u1edbi trong b\u1ed9 d\u1eef li\u1ec7u n\u00e0y l\u00e0 `yr` v\u00e0 `weathersit`.\n\n* V\u1edbi `yr`: v\u00ec 2011 v\u00e0 2012 l\u00e0 2 category kh\u00e1c nhau, v\u00e0 \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n H\u1ed3i Quy, n\u00ean \u0111\u1ec3 1 v\u00e0 0 s\u1ebd l\u00e0m tham s\u1ed1 2012 m\u1ea5t \u0111i s\u1ef1 hi\u1ec7n di\u1ec7n trong ph\u01b0\u01a1ng tr\u00ecnh tuy\u1ebfn t\u00ednh khi m\u00f4 h\u00ecnh s\u1ebd t\u1ef1 \u0111\u1ed9ng x\u00f3a 2012 ra kh\u1ecfi ph\u01b0\u01a1ng tr\u00ecnh (nh\u00e2n v\u1edbi 0). Do \u0111\u00f3, tham s\u1ed1 2012 n\u00ean \u0111\u01b0\u1ee3c \u0111\u01b0\u1ee3c nh\u1eafm t\u1edbi v\u00e0 \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n \u0111\u1ea7y \u0111\u1ee7 trong ph\u01b0\u01a1ng tr\u00ecnh b\u1eb1ng -1. \n\n* V\u1edbi `weathersit`: 4 d\u1eef li\u1ec7u th\u1ec3 hi\u1ec7n 4 m\u1ee5c kh\u00e1c nhau. Thay v\u00ec s\u1eed d\u1ee5ng 1 2 3 4, em s\u1ebd d\u1ef1ng 4 c\u1ed9t kh\u00e1c nhau th\u1ec3 hi\u1ec7n s\u1ef1 c\u00f3 m\u1eb7t c\u1ee7a 4 tham s\u1ed1 n\u00f3i tr\u00ean \u0111\u1ec3 t\u0103ng s\u1ef1 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh khi hu\u1ea5n luy\u1ec7n.\n\n**L\u01b0u \u00fd**: 1 v\u00e0 0 n\u00ean s\u1eed d\u1ee5ng khi c\u1ed9t d\u1eef li\u1ec7u 2 tham s\u1ed1 *tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi c\u00f3 hay kh\u00f4ng*, 1 v\u00e0 -1 n\u00ean s\u1eed d\u1ee5ng khi c\u1ed9t d\u1eef li\u1ec7u (b\u1eaft bu\u1ed9c 2 tham s\u1ed1) *tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi theo 2 h\u01b0\u1edbng kh\u00e1c nhau*. (nam hay n\u1eef, h\u1ecdc gi\u1ecfi hay h\u1ecdc d\u1ed1t, v\u00e0 2011 hay 2012?)","a93d6764":"##### M\u00f4 h\u00ecnh sequential model 1 l\u1edbp Dense v\u00e0 l\u1edbp chu\u1ea9n h\u00f3a.","5e634a34":"Okay. Sau khi \u0111\u00e3 ph\u00e2n lo\u1ea1i nhanh xem c\u00e1c c\u1ed9t n\u00e0y \u0111\u00fang lo\u1ea1i \u0111\u00fang gi\u1edbi t\u00ednh r\u1ed3i th\u00ec em s\u1ebd v\u1ebd linh tinh m\u1ed9t ch\u00fat. M\u1ee5c \u0111\u00edch c\u1ee7a em l\u00e0 \u0111\u1ec3 hi\u1ec3u th\u00eam v\u1ec1 c\u00e1i b\u1ed9 d\u1eef li\u1ec7u n\u00e0y.\n\n\u0110\u1ea7u ti\u00ean l\u00e0 c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng s\u1ed1 m\u00e1 tr\u01b0\u1edbc. Seaborn l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 m\u1edf r\u1ed9ng kh\u00e1 \u0111\u1eb9p m\u1eaft c\u1ee7a matplotlib, \u01b0u \u0111i\u1ec3m l\u00e0 c\u00f4ng th\u1ee9c d\u1ec5 nh\u1edb (c\u1ea7n l\u00e0m nh\u1ec1u nh\u1ec1u 1 t\u1eb9o l\u00e0 s\u1ebd thu\u1ed9c) v\u00e0 v\u1ebd ra d\u1ec5 nh\u00ecn.","8d9b9e2a":"#### RandomForest","ee94a7e3":"Sau khi d\u1ef1ng \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh th\u00ec em s\u1ebd c\u1ea5u h\u00ecnh n\u00f3 v\u00e0 s\u1eed d\u1ee5ng h\u00e0m m\u1ea5t m\u00e1t MSE nh\u01b0 \u0111\u1ec1 b\u00e0i y\u00eau c\u1ea7u. Ngo\u00e0i ra kh\u00e1c v\u1edbi tr\u00ean l\u1edbp \u0111\u01b0\u1ee3c h\u1ecdc, em s\u1ebd s\u1eed d\u1ee5ng h\u00e0m t\u1ed1i \u01b0u `Adam`.","af443480":"### *B\u1eaft \u0111\u1ea7u train.*\n\nEm s\u1ebd s\u1eed d\u1ee5ng tensorflow tr\u01b0\u1edbc. ","067cceb6":"#### Th\u1eddi ti\u1ebft l\u00fd t\u01b0\u1edfng\n\nLi\u1ec7u \u0111\u00f3 c\u00f3 ph\u1ea3i l\u00e0 ng\u00e0y \u0111\u1eb9p tr\u1eddi \u0111\u1ec3 thu\u00ea xe kh\u00f4ng? H\u00e3y \u0111\u1ec3 `ideal` gi\u1ea3i \u0111\u00e1p. Ngo\u00e0i ra c\u1ed9t `sticky` s\u1ebd cho bi\u1ebft li\u1ec7u ng\u00e0y \u0111\u00f3 c\u00f3 ph\u1ea3i c\u00f3 \u0111\u1ed9 \u1ea9m cao kh\u00f4ng.","ac59dd69":"B\u00e2y gi\u1edd \u0111\u1ebfn th\u1eddi gian. Sau kha kh\u00e1 l\u1ea7n m\u00e0y m\u00f2 th\u00ec \u0111\u00e2y l\u00e0 m\u1ed9t c\u00e1i b\u1ea3ng ph\u00e2n t\u00edch kh\u00e1 hay v\u1ec1 th\u1eddi gian m\u00e0 em ph\u00e1t hi\u1ec7n ra \u0111\u01b0\u1ee3c.","a757635f":"### *S\u1eed d\u1ee5ng c\u00e1c th\u01b0 vi\u1ec7n kh\u00e1c: RandomForest v\u00e0 CatBoost*","45b0d256":"C\u00f3 v\u1ebb nh\u01b0 theo h\u00ecnh tr\u00ean m\u00f4 h\u00ecnh c\u1ee7a em c\u0169ng kh\u00e1 t\u1ed1t r\u1ed3i. Gi\u1edd th\u00ec n\u1ed9p to\u00e0n b\u1ed9 k\u1ebft qu\u1ea3 \u0111\u00e3 train th\u00f4i :D","54235789":"#### Ki\u1ec3m tra \u0111\u1ed9 hi\u1ec7u qu\u1ea3 m\u00f4 h\u00ecnh catboost d\u1ef1a tr\u00ean ph\u1ea7n test c\u1ee7a b\u1ed9 train.","68ff2a29":"#### H\u1ed3i quy tuy\u1ebfn t\u00ednh v\u1edbi Tensorflow\n\nTr\u01b0\u1edbc khi 'h\u1ecdc' c\u1ea3 b\u1ed9. Em s\u1ebd th\u1eed h\u1ecdc v\u1edbi 1 t\u00ednh n\u0103ng (feature) l\u00e0 temp xem \u0111\u1ebfm \u0111\u01b0\u1ee3c bao nhi\u00eau xe cho thu\u00ea.","14266377":"# M\u1ed9t l\u1eddi gi\u1ea3i \u0111\u01a1n gi\u1ea3n b\u1eb1ng tensorflow v\u00e0...","ae94b4d6":"### *Nh\u1eadp li\u1ec7u hoy*. ","75dcda1f":"Okeeee. C\u00f3 v\u1ebb l\u00e0 \u1ed5n d\u1ed3i. Em s\u1ebd chia b\u1ed9 \u0111\u1ec3 train :D","bae2d5ff":"Tuy nhi\u00ean, \u1edf ngay h\u00e0ng cu\u1ed1i c\u00f9ng c\u00f3 1 gi\u00e1 tr\u1ecb \u00e2m. Thu\u00ea xe th\u00ec kh\u00f4ng th\u1ec3 n\u00e0o c\u00f3 gi\u00e1 tr\u1ecb \u00e2m \u0111\u01b0\u1ee3c, n\u00ean *bi\u1ebfn \u0111\u1ed5i log* (log transformation) s\u1ebd gi\u00fap ch\u00fang ta tr\u00e1nh \u0111\u01b0\u1ee3c vi\u1ec7c s\u1ed1 xe d\u1ef1 \u0111o\u00e1n ra c\u00f3 gi\u00e1 tr\u1ecb \u00e2m. \n\nL\u01b0u \u00fd r\u1eb1ng bi\u1ebfn \u0111\u1ed5i log n\u00e0y c\u1ea7n ph\u1ea3i th\u00eam 1 v\u00e0o tr\u01b0\u1edbc khi bi\u1ebfn \u0111\u1ed5i b\u1edfi log(0) b\u1eb1ng v\u00f4 h\u1ea1n. Em mu\u1ed1n n\u00e9 gi\u00e1 tr\u1ecb n\u00e0y b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng `np.log1p`","9e46a107":"Theo c\u00e1i b\u1ea3ng n\u00e0y th\u00ec l\u01b0\u1ee3ng ng\u01b0\u1eddi thu\u00ea xe ch\u1ee7 y\u1ebfu l\u00e0 thu\u00ea t\u1ea7m gi\u1edd v\u00e0o l\u00e0m bu\u1ed5i s\u00e1ng v\u00e0 gi\u1edd tan t\u1ea7m bu\u1ed5i chi\u1ec1u. \u0110\u00eam h\u00f4m th\u00ec c\u00f3 m\u1ed9t v\u00e0i anh em \u0111am m\u00ea \u0111\u1ea1p xe v\u1eabn thu\u00ea xe v\u1edbi s\u1ed1 l\u01b0\u1ee3ng \u00edt h\u01a1n h\u1eb3n. C\u00f3 l\u1ebd m\u1ed9t c\u1ed9t data n\u1eefa \u0111\u1ec3 ph\u00e2n lo\u1ea1i vi\u1ec7c n\u00e0y n\u00ean c\u00f3 trong b\u1ed9 d\u1eef li\u1ec7u. \n\nC\u00f3 th\u1ec3 th\u1ea5y gi\u1edd cao \u0111i\u1ec3m \u0111\u1ec3 thu\u00ea xe l\u00e0 t\u1eeb 7-9am v\u00e0 5-7pm. B\u00e2y gi\u1edd l\u00e0 l\u00fac em s\u1ebd thay \u0111\u1ed5i d\u1eef li\u1ec7u m\u1ed9t ch\u00fat :D.","c1307000":"Em mu\u1ed1n l\u00e0m r\u00f5 c\u00e1c ki\u1ec3u d\u1eef li\u1ec7u \u1edf \u0111\u00e2y. \u1ede d\u00f2ng tr\u00ean, sau khi \u0111\u1ecdc d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o b\u1eb1ng pandas, th\u00ec c\u00f3 2 lo\u1ea1i d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c ch\u1ec9 ra l\u00e0 int v\u00e0 float. Tuy nhi\u00ean th\u00ec th\u1ef1c t\u1ebf khi train em s\u1ebd quan t\u00e2m xem c\u1ed9t (column) d\u1eef li\u1ec7u c\u1ee7a em c\u00f3 l\u00e0 d\u1ea1ng ph\u00e2n lo\u1ea1i (categorical) hay l\u00e0 d\u1ea1ng s\u1ed1 m\u00e1 (numerical). Theo nh\u01b0 t\u00e0i li\u1ec7u \u0111\u1ec1 b\u00e0i th\u00ec c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng nh\u1eadn ra qua 2 d\u00f2ng code \u1edf tr\u00ean th\u00ec:\n* C\u1ed9t `id` kh\u00e1 l\u00e0 v\u00f4 d\u1ee5ng. Ch\u00fang ta c\u00f3 th\u1ec3 lo\u1ea1i b\u1ecf n\u00f3 s\u1edbm kh\u1ecfi cu\u1ed9c ch\u01a1i.\n* C\u00e1c c\u1ed9t `season`, `yr`, `hr`, `mnth`, `holiday`, `weekday`, `workingday`, `weathersit` l\u00e0 c\u1ed9t c\u00f3 t\u00ednh ph\u00e2n lo\u1ea1i.\n* C\u00e1c c\u1ed9t c\u00f2n l\u1ea1i `temp`, `atemp`, `hum`, `windspeed`, v\u00e0 `cnt` l\u00e0 c\u1ed9t c\u00f3 t\u00ednh s\u1ed1 m\u00e1.\n\n\nVi\u1ec7c \u0111\u1ea7u ti\u00ean l\u00e0 s\u1ebd lo\u1ea1i b\u1ecf`id` ra kh\u1ecfi cu\u1ed9c ch\u01a1i. \n\n**L\u01b0u \u00fd** (nh\u1ecf): M\u1ed9t khi \u0111\u00e3 thay \u0111\u1ed5i c\u00e1i g\u00ec tr\u00ean b\u1ed9 train th\u00ec b\u1ed9 test c\u0169ng ph\u1ea3i thay \u0111\u1ed5i t\u01b0\u01a1ng t\u1ef1. C\u00e1c m\u00f4 h\u00ecnh s\u1ebd kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng, ho\u1eb7c \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n l\u00e0m sang ch\u1ea5n t\u00e2m l\u00fd **n\u1ebfu** hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh h\u1ecdc tr\u00ean b\u1ed9 train m\u1ed9t ki\u1ec3u v\u00e0 b\u1ed9 test kh\u00f4ng c\u00f3 s\u1ef1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1ec1 c\u1ed9t v\u00e0 v\u1ecb tr\u00ed c\u00e1c c\u1ed9t.","a7db7c5b":"M\u00f4 h\u00ecnh n\u00e0y s\u1ebd d\u1ef1 \u0111o\u00e1n `cnt` t\u1eeb `temp`. Em s\u1ebd th\u1eed v\u1edbi 10 gi\u00e1 tr\u1ecb \u0111\u1ea7u ti\u00ean, v\u00e0 \u0111\u1ea7u ra s\u1ebd x\u1ea5u ph\u1ebft, nh\u01b0ng m\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a n\u00f3 s\u1ebd l\u00e0 (10, 1):\n","50990eef":"#### CatBoost\n\nM\u00f4 h\u00ecnh CatBoost n\u00e0y l\u00e0 s\u1ef1 k\u1ebft h\u1ee3p gi\u1eefa vi\u1ec7c ph\u00e2n lo\u1ea1i (categorical) v\u00e0 boosting. \u1ede \u0111\u00e2y em c\u00f3 s\u1eed d\u1ee5ng `grid_search` \u0111\u1ec3 t\u00ecm ra \u0111\u01b0\u1ee3c hi\u1ec7u ch\u1ec9nh t\u1ed1i \u01b0u nh\u1ea5t cho thu\u1eadt to\u00e1n n\u00e0y.","fe03071f":"Em s\u1ebd d\u1ef1 \u0111o\u00e1n s\u1ed1 xe thu\u00ea \u0111\u01b0\u1ee3c b\u1eb1ng nhi\u1ec7t \u0111\u1ed9 v\u00e0 \u0111i k\u00e8m v\u1edbi 1 bi\u1ec3u \u0111\u1ed3 linear.","7e1a5542":"##### Neural Network nhi\u1ec1u l\u1edbp\n\nTr\u00f4ng c\u00f3 v\u1ebb h\u1ee9a h\u1eb9n h\u01a1n nhi\u1ec1u r\u1ed3i \u1ea1 :D. B\u00e2y gi\u1edd em s\u1ebd s\u1eed d\u1ee5ng m\u1ea1ng Deep-NN nhi\u1ec1u l\u1edbp.\n* L\u1edbp chu\u1ea9n h\u00f3a \u0111\u00e3 \u0111\u01b0\u1ee3c d\u1ef1ng t\u1eeb \u0111\u1ea7u\n* 2 l\u1edbp Neural Network ReLU\n* L\u1edbp gi\u00e1 tr\u1ecb \u0111\u1ea7u ra\n\nV\u1eabn nh\u01b0 \u1edf tr\u00ean, nh\u01b0ng em s\u1ebd g\u1ed9p l\u1ea1i t\u1ea1o th\u00e0nh 1 function v\u00e0 h\u00e0m t\u1ed1i \u01b0u Adam + MSE.","8e7d65af":"M\u1ed9t l\u1edbp Normalizer (chu\u1ea9n h\u00f3a) \u0111\u1ec3 tensorflow 'h\u1ecdc' qua c\u00f3 l\u1ebd s\u1ebd r\u1ea5t hay ho :D. Sau \u0111\u00f3 l\u1edbp n\u00e0y s\u1ebd 'th\u00edch nghi' (adapt) v\u1edbi b\u1ed9 d\u1eef li\u1ec7u.","3b06e67e":"Xin ch\u00e0o m\u1ecdi ng\u01b0\u1eddi! Em t\u00ean l\u00e0 Joee *(h\u01a1i d\u1ec5 th\u01b0\u01a1ng)*. May m\u1eafn thay em \u0111\u01b0\u1ee3c top 1 n\u00ean em mu\u1ed1n vi\u1ebft m\u1ed9t v\u00e0i c\u00e1i l\u1eddi gi\u1ea3i vui v\u1ebb. Mong m\u1ecdi ng\u01b0\u1eddi \u0111\u00f3n nh\u1eadn \u1ea1 :-s.\n\nB\u00e0i to\u00e1n th\u00ec kh\u00e1 \u0111\u01a1n gi\u1ea3n. Y\u00eau c\u1ea7u \u0111\u1ec1 b\u00e0i l\u00e0 H\u1ed3i Quy. V\u1eady th\u00ec t\u1ea1i sao ch\u00fang ta kh\u00f4ng th\u1eed x\u00e0i c\u00f4ng c\u1ee5 ch\u00ednh c\u1ee7a l\u1edbp m\u00ecnh - Tensorflow, v\u00e0 m\u1ed9t v\u00e0i c\u00f4ng c\u1ee5 kh\u00e1c, \u0111\u1ec3 \u00e1p d\u1ee5ng nh\u1ec9 :p\n\n\u0110\u1ea7u ti\u00ean th\u00ec d\u1eef li\u1ec7u nh\u1ea3y v\u00f4 kh\u00f4ng th\u1ec3 train lu\u00f4n \u0111\u01b0\u1ee3c. C\u00f3 m\u1ed9t v\u00e0i \u0111i\u1ec1u c\u1ea7n ph\u1ea3i kh\u00e1m ph\u00e1, v\u00e0 \u0111\u00f3 l\u00e0 Exploratory Data Analysis (EDA). Em s\u1ebd \u0111\u1ec3 caption chi ti\u1ebft \u1edf m\u1ed7i c\u00e2u l\u1ec7nh.","e2158afd":"### *M\u1ed9t ch\u00fat feature engineering.*\n\n#### Gi\u1edd cao \u0111i\u1ec3m\n\nLi\u1ec7u \u0111\u00f3 c\u00f3 ph\u1ea3i l\u00e0 ng\u00e0y \u0111i l\u00e0m v\u00e0 v\u00e0o gi\u1edd nhi\u1ec1u ng\u01b0\u1eddi thu\u00ea xe kh\u00f4ng? H\u00e3y \u0111\u1ec3 `peak` gi\u1ea3i \u0111\u00e1p.","58a65f60":"Khi l\u1edbp Normalizer \u0111\u01b0\u1ee3c g\u1ecdi, n\u00f3 tr\u1ea3 v\u1ec1 d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o, v\u1edbi m\u1ed7i t\u00ednh n\u0103ng (feature) \u0111\u01b0\u1ee3c chu\u1ea9n h\u00f3a \u0111\u1ed9c l\u1eadp: ","8e9961d5":"Ti\u1ebfp t\u1ee5c l\u00e0 Sequential 1 l\u1edbp, nh\u01b0ng b\u00e2y gi\u1edd em s\u1ebd s\u1eed d\u1ee5ng to\u00e0n b\u1ed9 c\u00e1c feature \u0111\u1ec3 m\u00f4 h\u00ecnh th\u1eed h\u1ecdc.\n","40c2f698":"Tua \u0111\u01b0\u1ee3c \u0111\u1ebfn \u0111\u00e2y c\u0169ng kh\u00e1 d\u00e0i r\u1ed3i. Em hy v\u1ecdng m\u1ecdi ng\u01b0\u1eddi enjoy c\u00e1i notebook n\u00e0y \u1ea1 :D.\n\nN\u1ebfu c\u00f3 b\u1ea5t c\u1ee9 c\u00e2u h\u1ecfi d\u00e0nh cho em, c\u00e1ch nhanh nh\u1ea5t l\u00e0 nh\u1eafn cho em qua [\u0111\u00e2y](https:\/\/m.me\/joeeislovely) \u1ea1! Em c\u1ea3m \u01a1n :p","16c3777e":"Okay. B\u00e2y gi\u1edd \u0111\u1ebfn nh\u1eefng anh b\u1ea1n l\u00e0m nhi\u1ec7m v\u1ee5 ph\u00e2n lo\u1ea1i. Em c\u00f3 2 v\u00ed d\u1ee5 l\u00e0 v\u1ec1 `season` v\u00e0 `hr`. K\u1ebft lu\u1eadn ng\u1eafn g\u1ecdn c\u1ee7a em l\u00e0 l\u01b0\u1ee3ng d\u1eef li\u1ec7u ph\u00e2n b\u1ed1 \u0111\u1ec1u, ngh\u0129a l\u00e0 vi\u1ec7c hu\u1ea5n luy\u1ec7n \u0111\u01a1n gi\u1ea3n h\u01a1n kh\u00e1 nhi\u1ec1u.","ef4dc886":"Tr\u00f4ng c\u00f3 v\u1ebb \u1ed5n \u00e1p h\u01a1n r\u1ed3i :p. S\u1ed1 m\u00e1 tr\u00f4ng c\u0169ng kh\u00e1 d\u1ec5 ch\u1ecbu. Tuy nhi\u00ean th\u00ec nh\u01b0 anh Ng\u1ecdc c\u00f3 n\u00f3i th\u00ec s\u1eed d\u1ee5ng Deep Learning cho b\u00e0i n\u00e0y ch\u01b0a th\u1eadt s\u1ef1 t\u1ed1i \u01b0u, ph\u1ea7n nhi\u1ec1u l\u00e0 v\u00ec \u0111\u00e2y l\u00e0 d\u1ea1ng Structured Data. \n\nKhi em nh\u00ecn v\u00e0o l\u01b0\u1ee3ng d\u1eef li\u1ec7u n\u00e0y, em ngh\u0129 r\u1eb1ng \u00e1p d\u1ee5ng m\u1ed9t thu\u1eadt to\u00e1n kh\u00e1c s\u1ebd t\u1ed1t h\u01a1n nhi\u1ec1u. \n\nThu\u1eadt to\u00e1n m\u00e0 em n\u00f3i \u0111\u1ebfn s\u1eed d\u1ee5ng Decision Tree g\u1ed3m c\u00f3 *RandomForest* v\u00e0 *GradientBoosting* (XGBoost, lightGBM, CatBoost). L\u00fd do ch\u00ednh cho vi\u1ec7c n\u00e0y l\u00e0 b\u1edfi b\u1ed9 data n\u00e0y c\u00f3 kh\u00e1 nhi\u1ec1u c\u1ed9t categorical, v\u00e0 vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c thu\u1eadt to\u00e1n Decision Tree s\u1ebd gi\u00fap cho vi\u1ec7c ph\u00e2n lo\u1ea1i thu\u1ed9c t\u00ednh c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c cao h\u01a1n.\n\nD\u01b0\u1edbi \u0111\u00e2y l\u00e0 2 thu\u1eadt to\u00e1n em s\u1eed d\u1ee5ng nhi\u1ec1u. RandomForest, m\u1ed9t thu\u1eadt to\u00e1n ch\u1ee7 y\u1ebfu s\u1eed d\u1ee5ng trong b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i, s\u1ebd gi\u00fap \u00edch nhi\u1ec1u trong vi\u1ec7c ph\u00e2n lo\u1ea1i c\u00e1c thu\u1ed9c t\u00ednh. CatBoost (categorical boost) l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n c\u1ee7a m\u1ea5y anh ng\u01b0\u1eddi Nga (Yandex) c\u0169ng r\u1ea5t m\u1ea1nh trong vi\u1ec7c ph\u00e2n lo\u1ea1i:)).","df7465ec":"C\u00f3 m\u1ed9t v\u00e0i nh\u1eadn x\u00e9t c\u1ee7a em v\u1ec1 bi\u1ec3u \u0111\u1ed3 n\u00e0y:\n* `temp` v\u00e0 `atemp` c\u00f3 \u0111\u1ed9 t\u01b0\u01a1ng quan r\u1ea5t cao. C\u00f3 th\u1ec3 th\u1ea5y c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u n\u1eb1m tr\u00ean m\u1ed9t \u0111\u01b0\u1eddng kh\u00e1 th\u1eb3ng t\u1eafp:)))\n* H\u1ea7u h\u1ebft c\u00e1c b\u1ea3ng \u0111\u1ec1u c\u00f3 ph\u00e2n ph\u1ed1i chu\u1ea9n.\n* Xe s\u1ebd \u0111\u01b0\u1ee3c thu\u00ea n\u1ebfu th\u1eddi ti\u1ebft l\u00fd t\u01b0\u1edfng. Em s\u1ebd t\u1ea1o m\u1ed9t v\u00e0i gi\u00e1 tr\u1ecb l\u00fd t\u01b0\u1edfng (data-centric) \u0111\u1ec3 khi\u1ebfn vi\u1ec7c hu\u1ea5n luy\u1ec7n ez h\u01a1n.","c89fd007":"V\u00e0 \u0111\u00e2y l\u00e0 k\u1ebft qu\u1ea3. M\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 n\u1ed9p th\u1eed k\u1ebft qu\u1ea3 n\u00e0y :D"}}