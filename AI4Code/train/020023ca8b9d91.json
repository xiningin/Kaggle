{"cell_type":{"cb347b5e":"code","90c08cce":"code","4b059c37":"code","76850f7c":"code","b842701d":"code","abeae17d":"code","f52efa3a":"code","41090efb":"code","37f9f06d":"code","b6a99d25":"code","92913844":"code","d96d44a2":"code","54ea8ccf":"code","08215d0e":"code","bc8d1a07":"code","30e82efd":"markdown","2a1536de":"markdown","ff3f714b":"markdown"},"source":{"cb347b5e":"import numpy as np \nimport pandas as pd \nfrom collections import Counter","90c08cce":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test  = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_sample= pd.read_csv('..\/input\/titanic\/gender_submission.csv')","4b059c37":"df_train.head()","76850f7c":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers ","b842701d":"Outliers_to_drop = detect_outliers(df_train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ndf_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","abeae17d":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df_train[\"Name\"]]\n\ndf_train[\"Title\"] = pd.Series(dataset_title)\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf_train[\"Title\"] = df_train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndf_train[\"Title\"] = df_train[\"Title\"].astype(int)\n\ndf_train.drop(labels = [\"Name\"], axis = 1, inplace = True)","f52efa3a":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age\n    \ndef impute_fare(cols):\n    Fare = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Fare):\n\n        if Pclass == 1:\n            return 84\n\n        elif Pclass == 2:\n            return 20\n\n        else:\n            return 13\n\n    else:\n        return Fare\n    \ndef get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex","41090efb":"df_train['Age'] = df_train[['Age','Pclass']].apply(impute_age,axis=1)\n\nsex    = pd.get_dummies(df_train['Sex'],drop_first=True)\nembark = pd.get_dummies(df_train['Embarked'],drop_first=True)\n\ndf_train = pd.concat([df_train,sex,embark],axis=1)\n\ndf_train[\"Family\"] = df_train[\"SibSp\"] + df_train[\"Parch\"] + 1\ndf_train['Single'] = df_train['Family'].map(lambda s: 1 if s == 1 else 0)\ndf_train['SmallF'] = df_train['Family'].map(lambda s: 1 if  s == 2  else 0)\ndf_train['MedF']   = df_train['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf_train['LargeF'] = df_train['Family'].map(lambda s: 1 if s >= 5 else 0)\ndf_train['Senior'] = df_train['Age'].map(lambda s:1 if s>60 else 0)\n\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df_test[\"Name\"]]\n\ndf_test[\"Title\"] = pd.Series(dataset_title)\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf_test[\"Title\"] = df_test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndf_test[\"Title\"] = df_test[\"Title\"].astype(int)\n\ndf_test.drop(labels = [\"Name\"], axis = 1, inplace = True)\ndf_test['Age'] = df_test[['Age','Pclass']].apply(impute_age,axis=1)\n\nsex    = pd.get_dummies(df_test['Sex'],drop_first=True)\nembark = pd.get_dummies(df_test['Embarked'],drop_first=True)\n\ndf_test = pd.concat([df_test,sex,embark],axis=1)\n\ndf_test['Fare'].fillna(value=df_test['Fare'].median(),inplace=True)\n\ndf_test['Fare'] = df_test[['Fare','Pclass']].apply(impute_fare,axis=1)\ndf_test[\"Fare\"] = df_test[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ndf_test[\"Family\"] = df_test[\"SibSp\"] + df_test[\"Parch\"] + 1\ndf_test['Single'] = df_test['Family'].map(lambda s: 1 if s == 1 else 0)\ndf_test['SmallF'] = df_test['Family'].map(lambda s: 1 if  s == 2  else 0)\ndf_test['MedF']   = df_test['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf_test['LargeF'] = df_test['Family'].map(lambda s: 1 if s >= 5 else 0)\ndf_test['Senior'] = df_test['Age'].map(lambda s:1 if s>60 else 0)\n\ndf_train['Person'] = df_train[['Age','Sex']].apply(get_person,axis=1)\ndf_test['Person']  = df_test[['Age','Sex']].apply(get_person,axis=1)\n\nperson_dummies_train  = pd.get_dummies(df_train['Person'])\nperson_dummies_train.columns = ['Child','Female','Male']\nperson_dummies_train.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_test  = pd.get_dummies(df_test['Person'])\nperson_dummies_test.columns = ['Child','Female','Male']\nperson_dummies_test.drop(['Male'], axis=1, inplace=True)\n\ndf_train = df_train.join(person_dummies_train)\ndf_test  = df_test.join(person_dummies_test)\n\ndf_train.drop(['Person'],axis=1,inplace=True)\ndf_test.drop(['Person'],axis=1,inplace=True)\n\ndf_train.drop('male',axis=1,inplace=True)\ndf_test.drop('male',axis=1,inplace=True)\n\ndf_train.drop(['Cabin','Ticket'],axis = 1, inplace= True)\ndf_test.drop(['Ticket','Cabin'],axis = 1, inplace= True)\n\ndf_train.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)","37f9f06d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_train.drop('Survived',axis=1), \n                                                    df_train['Survived'], test_size=0.20, \n                                                    random_state=101)","b6a99d25":"X = df_train.drop('Survived',axis=1)\ny = df_train['Survived']","92913844":"import matplotlib.pyplot as plt\nfrom yellowbrick.features import RadViz\n\nfig, ax = plt.subplots(figsize=(12, 12))\nrv = RadViz(classes=[\"died\", \"survived\"],features=X.columns)\nrv.fit(X, y)\n_ = rv.transform(X)\nrv.poof()","d96d44a2":"from pandas.plotting import radviz\nfig, ax = plt.subplots(figsize=(12, 12))\nnew_df = X.copy()\nnew_df[\"target\"] = y\nradviz(new_df, \"target\", ax=ax, colormap=\"PiYG\")","54ea8ccf":"from sklearn import ensemble\nfrom yellowbrick.features import RFECV\nfig, ax = plt.subplots(figsize=(12, 8))\nrfe = RFECV(\n    ensemble.RandomForestClassifier(n_estimators=100),cv=5)\nrfe.fit(X, y)\nrfe.rfe_estimator_.ranking_\nrfe.rfe_estimator_.n_features_\nrfe.rfe_estimator_.support_\nrfe.poof()","08215d0e":"from sklearn.ensemble import RandomForestClassifier\nfrom yellowbrick.model_selection import LearningCurve\nfig, ax = plt.subplots(figsize=(12, 8))\nlc3_viz = LearningCurve(RandomForestClassifier(n_estimators = 10),cv=5)\nlc3_viz.fit(X, y)\nlc3_viz.poof()","bc8d1a07":"from lime import lime_tabular\nexplainer = lime_tabular.LimeTabularExplainer(\n    X_train.values,\n    feature_names=X.columns,\n    class_names=[\"died\", \"survived\"]\n)\nexp = explainer.explain_instance(X_train.iloc[-1].values, rfe.predict_proba)\n\nfig = exp.as_pyplot_figure()\nfig.tight_layout()","30e82efd":"# Yellowbrick: Machine Learning Visualization\n\n![](https:\/\/www.scikit-yb.org\/en\/latest\/_images\/banner.png)\n\n### Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it\u2019s using Matplotlib.\n\n**Quick Start**\n\nIf you\u2019re new to Yellowbrick, this guide will get you started and help you include visualizers in your machine learning workflow. Before we begin, however, there are several notes about development environments that you should consider.\n\nYellowbrick has two primary dependencies: scikit-learn and matplotlib. If you do not have these Python packages, they will be installed alongside Yellowbrick. Note that Yellowbrick works best with scikit-learn version 0.20 or later and matplotlib version 3.0.1 or later. Both of these packages require some C code to be compiled, which can be difficult on some systems, like Windows. If you\u2019re having trouble, try using a distribution of Python that includes these packages like Anaconda.\n\nYellowbrick is also commonly used inside of a Jupyter Notebook alongside Pandas data frames. Notebooks make it especially easy to coordinate code and visualizations; however, you can also use Yellowbrick inside of regular Python scripts, either saving figures to disk or showing figures in a GUI window. If you\u2019re having trouble with this, please consult matplotlib\u2019s backends documentation.\n\nhttps:\/\/www.scikit-yb.org\/en\/latest\/quickstart.html\n\n","2a1536de":"to be continued! I'll try to add more interesting visuals over time!","ff3f714b":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/1280px-RMS_Titanic_3.jpg)\n\n## This kernel is intended for visuals only\n\n- If you are looking for a regular and easy classifier please check my other kernel \n\nhttps:\/\/www.kaggle.com\/frtgnn\/titanic-survival-classifier\n\n- you can also check the online competition introduction here\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/discussion\/111539\n\n- another kernel of mine to introduce pipelines using Titanic data\n\nhttps:\/\/www.kaggle.com\/frtgnn\/beginner-s-stop-pipeline-introduction\n\n## thanks"}}