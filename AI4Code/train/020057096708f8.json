{"cell_type":{"3d2b4f95":"code","924edfab":"code","eafa3cbd":"code","4f6495cd":"code","6d25444e":"code","c307e2d9":"code","58e2d24b":"code","4091403a":"code","a28959fc":"code","401d1069":"code","feaf89d0":"code","20478427":"code","7c48858b":"code","9fea2ecd":"code","b7e225c8":"code","d05adbe6":"code","57db688a":"code","e17b174c":"code","9680a880":"code","951a0d5b":"code","fc41374a":"code","a8da0997":"code","b1c2807a":"code","f4fdd37a":"code","81cb5086":"code","d85dbb5a":"code","bcf7647d":"code","2fbb0909":"code","ce6d5c9e":"code","8d6dbaa0":"code","679b9b08":"code","5553baa7":"markdown","f9c1fc5e":"markdown","70b5a88f":"markdown","3e7fc156":"markdown","1c1ce9b6":"markdown","a7a6c100":"markdown","3e04e140":"markdown","bd0cf0d8":"markdown","831ead6f":"markdown","b7d42649":"markdown","7a69a2e5":"markdown","e68e2643":"markdown","59da07be":"markdown","75e3aa81":"markdown","737cb0ab":"markdown","4df2e131":"markdown","96f0f71d":"markdown","d45239aa":"markdown","b05720bb":"markdown","f59c8497":"markdown"},"source":{"3d2b4f95":"# import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import os\n\nimport nltk\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport gc\nimport re\nimport spacy\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.datasets import make_classification\n\nfrom pandarallel import pandarallel\npandarallel.initialize()\nfrom tqdm.notebook import tqdm\nfrom IPython.core.display import display, HTML\ntqdm().pandas()\n","924edfab":"# \u0111\u1ecdc t\u1eadp d\u1eef li\u1ec7u csv v\u00e0o dataframe\ntrain_df = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\")\n\n# train_df.head(5)\ntrain_df.info()\ntest_df.info()","eafa3cbd":"# ph\u00e2n t\u00edch s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi sincere v\u00e0 insincere\ntrain_df['words_num'] = train_df.question_text.parallel_apply(lambda x: len(x.split()))\n\nsincere_ques = train_df[train_df.target==0]\ninsincere_ques = train_df[train_df.target==1]\n\n# x\u00e2y d\u1ef1ng b\u1ea3ng, bi\u1ec3u \u0111\u1ed3 ph\u00e2n t\u00edch d\u1eef li\u1ec7u\nfig, ax = plt.subplots(1,2)\nax[0] = fig.add_axes([0,0,1,1])\nax[1] = fig.add_axes([1.2,0,1,1])\n# bar chart\nax[0].set_title('Statistics of sincere and insincere question')\nax[0].bar(['sincere', 'insincere'], train_df.target.value_counts(),\n         color=(0.9, 0.4, 0.6, 0.6))\nfor p in ax[0].patches:\n    height = p.get_height()\n    ax[0].annotate('{}'.format(height),\n      xy=(p.get_x() + p.get_width() \/ 2, height),\n      xytext=(0, 3), # 3 points vertical offset\n      textcoords=\"offset points\",ha='center')\n# pie chart\nrates = [sincere_ques.shape[0]\/train_df.shape[0] * 100, \n        insincere_ques.shape[0]\/train_df.shape[0] * 100]\n\nplt.pie(rates, labels=['sincere', 'insincere'],autopct='%1.1f%%', shadow=False, \n        colors=(\"grey\",\"orange\"),startangle=90)\nplt.show()\n\n# train_df['target'].value_counts().plot(kind='bar')","4f6495cd":"# in ra m\u1ed9t s\u1ed1 m\u1eabu c\u00e2u h\u1ecfi sincere v\u00e0 insincere\nprint('sincere question: ')\ndisplay(sincere_ques.head(5))\nprint('insincere question: ')\ndisplay(insincere_ques.head(5))","6d25444e":"# ph\u00e2n t\u00edch s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u h\u1ecfi\nfig2, axes = plt.subplots(1,2)\naxes[0] = fig2.add_axes([0,0,1,1.2])\naxes[1] = fig2.add_axes([1.2,0,1,1])\n# table\nrowLabels = [\"min_len\",\"mean_len\",\"max_len\"]\ncolLabels = [\"sincere_question\",\"insincere_question\"]\ndata = [[np.min(sincere_ques['words_num']), np.min(insincere_ques['words_num'])],\n       [np.mean(sincere_ques['words_num']), np.mean(insincere_ques['words_num'])],\n       [np.max(sincere_ques['words_num']), np.max(insincere_ques['words_num'])]]\ndf = pd.DataFrame(data, columns=colLabels)\naxes[0].axis('off')\ntable=axes[0].table(cellText=df.values,colLabels=df.columns,rowLabels=rowLabels,bbox=[0, 0, 1, 0.8])\ntable.auto_set_font_size(False)\ntable.set_fontsize(14)\n# density plot\naxes[1].set_title('Density plot of number of words in every question') \nsns.set(style=\"darkgrid\")\nsns.kdeplot(train_df.words_num, shade=True, color='red')\nsns.kdeplot(sincere_ques.words_num, shade=True, color='blue')\nsns.kdeplot(insincere_ques.words_num, shade=True, color='orange')\nplt.legend([\"all\", \"sincere\", \"insincere\"])\nplt.show()","c307e2d9":"# t\u1ea1o sample data t\u1eeb train data \u0111\u1ec3 th\u1eed nghi\u1ec7m c\u00e1c h\u00e0m ti\u1ec1n x\u1eed l\u00fd\ntrain_suffle = train_df.sample(frac = 1,random_state=123).reset_index(drop=True)\nsample = train_suffle.sample(n=100, random_state=123)","58e2d24b":"# t\u1eeb \u0111i\u1ec3n t\u1eeb vi\u1ebft t\u1eaft\ncontraction_dict= {\"cant\": 'can not',\"doesnt\": 'does not',\"dont\": 'do not',\"i'm'a\": 'i am about to',\"i'm'o\": 'i am going to',\"i'm\": 'i am',\"i've\": 'i have',\"i'll\": 'i will',\"i'll've\": 'i will have',\"i'd've\": 'i would have',\"i'd\": 'i would',\"whatcha\": 'what are you',\"amn't\": 'am not',\"ain't\": 'are not',\"aren't\": 'are not',\"'cause\": 'because',\"can't\": 'can not',\"can't've\": 'can not have',\"could've\": 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have',\"daren't\": 'dare not',\"daresn't\": 'dare not',\"dasn't\": 'dare not',\"didn't\": 'did not','didn\u2019t': 'did not',\"don't\": 'do not','don\u2019t': 'do not',\"doesn't\": 'does not',\"e'er\": 'ever',\"everyone's\": 'everyone is',\"finna\": 'fixing to',\"gimme\": 'give me',\"gon't\": 'go not',\"gonna\": 'going to',\"gotta\": 'got to',\"hadn't\": 'had not',\"hadn't've\": 'had not have',\"hasn't\": 'has not',\"haven't\": 'have not',\"he've\": 'he have',\"he's\": 'he is',\"he'll\": 'he will',\"he'll've\": 'he will have',\"he'd\": 'he would',\"he'd've\": 'he would have',\"here's\": 'here is',\"how're\": 'how are',\"how'd\": 'how did',\"how'd'y\": 'how do you',\"how's\": 'how is',\"how'll\": 'how will',\"isn't\": 'is not',\"it's\": 'it is',\"'tis\": 'it is',\"'twas\": 'it was',\"it'll\": 'it will',\"it'll've\": 'it will have',\"it'd\": 'it would',\"it'd've\": 'it would have',\"kinda\": 'kind of',\"let's\": 'let us',\"luv\": 'love',\"ma'am\": 'madam',\"may've\": 'may have',\"mayn't\": 'may not',\"might've\": 'might have',\"mightn't\": 'might not',\"mightn't've\": 'might not have',\"must've\": 'must have',\"mustn't\": 'must not',\"mustn't've\": 'must not have',\"needn't\": 'need not',\"needn't've\": 'need not have',\"ne'er\": 'never',\"o'\": 'of',\"o'clock\": 'of the clock',\"ol'\": 'old',\"oughtn't\": 'ought not',\"oughtn't've\": 'ought not have',\"o'er\": 'over',\"shan't\": 'shall not',\"sha'n't\": 'shall not',\"shalln't\": 'shall not',\"shan't've\": 'shall not have',\"she's\": 'she is',\"she'll\": 'she will',\"she'd\": 'she would',\"she'd've\": 'she would have',\"should've\": 'should have',\"shouldn't\": 'should not',\"shouldn't've\": 'should not have',\"so've\": 'so have',\"so's\": 'so is',\"somebody's\": 'somebody is',\"someone's\": 'someone is',\"something's\": 'something is',\"sux\": 'sucks',\"that're\": 'that are',\"that's\": 'that is',\"that'll\": 'that will',\"that'd\": 'that would',\"that'd've\": 'that would have',\"em\": 'them',\"there're\": 'there are',\"there's\": 'there is',\"there'll\": 'there will',\"there'd\": 'there would',\"there'd've\": 'there would have',\"these're\": 'these are',\"they're\": 'they are',\"they've\": 'they have',\"they'll\": 'they will',\"they'll've\": 'they will have',\"they'd\": 'they would',\"they'd've\": 'they would have',\"this's\": 'this is',\"those're\": 'those are',\"to've\": 'to have',\"wanna\": 'want to',\"wasn't\": 'was not',\"we're\": 'we are',\"we've\": 'we have',\"we'll\": 'we will',\"we'll've\": 'we will have',\"we'd\": 'we would',\"we'd've\": 'we would have',\"weren't\": 'were not',\"what're\": 'what are',\"what'd\": 'what did',\"what've\": 'what have',\"what's\": 'what is',\"what'll\": 'what will',\"what'll've\": 'what will have',\"when've\": 'when have',\"when's\": 'when is',\"where're\": 'where are',\"where'd\": 'where did',\"where've\": 'where have',\"where's\": 'where is',\"which's\": 'which is',\"who're\": 'who are',\"who've\": 'who have',\"who's\": 'who is',\"who'll\": 'who will',\"who'll've\": 'who will have',\"who'd\": 'who would',\"who'd've\": 'who would have',\"why're\": 'why are',\"why'd\": 'why did',\"why've\": 'why have',\"why's\": 'why is',\"will've\": 'will have',\"won't\": 'will not',\"won't've\": 'will not have',\"would've\": 'would have',\"wouldn't\": 'would not',\"wouldn't've\": 'would not have',\"y'all\": 'you all',\"y'all're\": 'you all are',\"y'all've\": 'you all have',\"y'all'd\": 'you all would',\"y'all'd've\": 'you all would have',\"you're\": 'you are',\"you've\": 'you have',\"you'll've\": 'you shall have',\"you'll\": 'you will',\"you'd've\": 'you would have',\"you'd\": 'you would','jan.': 'january','feb.': 'february','mar.': 'march','apr.': 'april','jun.': 'june','jul.': 'july','aug.': 'august','sep.': 'september','oct.': 'october','nov.': 'november','dec.': 'december','I\u2019m': 'I am','I\u2019m\u2019a': 'I am about to','I\u2019m\u2019o': 'I am going to','I\u2019ve': 'I have','I\u2019ll': 'I will','I\u2019ll\u2019ve': 'I will have','I\u2019d': 'I would','I\u2019d\u2019ve': 'I would have','amn\u2019t': 'am not','ain\u2019t': 'are not','aren\u2019t': 'are not','\u2019cause': 'because','can\u2019t': 'can not','can\u2019t\u2019ve': 'can not have','could\u2019ve': 'could have','couldn\u2019t': 'could not','couldn\u2019t\u2019ve': 'could not have','daren\u2019t': 'dare not','daresn\u2019t': 'dare not','dasn\u2019t': 'dare not','doesn\u2019t': 'does not','e\u2019er': 'ever','everyone\u2019s': 'everyone is','gon\u2019t': 'go not','hadn\u2019t': 'had not','hadn\u2019t\u2019ve': 'had not have','hasn\u2019t': 'has not','haven\u2019t': 'have not','he\u2019ve': 'he have','he\u2019s': 'he is','he\u2019ll': 'he will','he\u2019ll\u2019ve': 'he will have','he\u2019d': 'he would','he\u2019d\u2019ve': 'he would have','here\u2019s': 'here is','how\u2019re': 'how are','how\u2019d': 'how did','how\u2019d\u2019y': 'how do you','how\u2019s': 'how is','how\u2019ll': 'how will','isn\u2019t': 'is not','it\u2019s': 'it is','\u2019tis': 'it is','\u2019twas': 'it was','it\u2019ll': 'it will','it\u2019ll\u2019ve': 'it will have','it\u2019d': 'it would','it\u2019d\u2019ve': 'it would have','let\u2019s': 'let us','ma\u2019am': 'madam','may\u2019ve': 'may have','mayn\u2019t': 'may not','might\u2019ve': 'might have','mightn\u2019t': 'might not','mightn\u2019t\u2019ve': 'might not have','must\u2019ve': 'must have','mustn\u2019t': 'must not','mustn\u2019t\u2019ve': 'must not have','needn\u2019t': 'need not','needn\u2019t\u2019ve': 'need not have','ne\u2019er': 'never','o\u2019': 'of','o\u2019clock': 'of the clock','ol\u2019': 'old','oughtn\u2019t': 'ought not','oughtn\u2019t\u2019ve': 'ought not have','o\u2019er': 'over','shan\u2019t': 'shall not','sha\u2019n\u2019t': 'shall not','shalln\u2019t': 'shall not','shan\u2019t\u2019ve': 'shall not have','she\u2019s': 'she is','she\u2019ll': 'she will','she\u2019d': 'she would','she\u2019d\u2019ve': 'she would have','should\u2019ve': 'should have','shouldn\u2019t': 'should not','shouldn\u2019t\u2019ve': 'should not have','so\u2019ve': 'so have','so\u2019s': 'so is','somebody\u2019s': 'somebody is','someone\u2019s': 'someone is','something\u2019s': 'something is','that\u2019re': 'that are','that\u2019s': 'that is','that\u2019ll': 'that will','that\u2019d': 'that would','that\u2019d\u2019ve': 'that would have','there\u2019re': 'there are','there\u2019s': 'there is','there\u2019ll': 'there will','there\u2019d': 'there would','there\u2019d\u2019ve': 'there would have','these\u2019re': 'these are','they\u2019re': 'they are','they\u2019ve': 'they have','they\u2019ll': 'they will','they\u2019ll\u2019ve': 'they will have','they\u2019d': 'they would','they\u2019d\u2019ve': 'they would have','this\u2019s': 'this is','those\u2019re': 'those are','to\u2019ve': 'to have','wasn\u2019t': 'was not','we\u2019re': 'we are','we\u2019ve': 'we have','we\u2019ll': 'we will','we\u2019ll\u2019ve': 'we will have','we\u2019d': 'we would','we\u2019d\u2019ve': 'we would have','weren\u2019t': 'were not','what\u2019re': 'what are','what\u2019d': 'what did','what\u2019ve': 'what have','what\u2019s': 'what is','what\u2019ll': 'what will','what\u2019ll\u2019ve': 'what will have','when\u2019ve': 'when have','when\u2019s': 'when is','where\u2019re': 'where are','where\u2019d': 'where did','where\u2019ve': 'where have','where\u2019s': 'where is','which\u2019s': 'which is','who\u2019re': 'who are','who\u2019ve': 'who have','who\u2019s': 'who is','who\u2019ll': 'who will','who\u2019ll\u2019ve': 'who will have','who\u2019d': 'who would','who\u2019d\u2019ve': 'who would have','why\u2019re': 'why are','why\u2019d': 'why did','why\u2019ve': 'why have','why\u2019s': 'why is','will\u2019ve': 'will have','won\u2019t': 'will not','won\u2019t\u2019ve': 'will not have','would\u2019ve': 'would have','wouldn\u2019t': 'would not','wouldn\u2019t\u2019ve': 'would not have','y\u2019all': 'you all','y\u2019all\u2019re': 'you all are','y\u2019all\u2019ve': 'you all have','y\u2019all\u2019d': 'you all would','y\u2019all\u2019d\u2019ve': 'you all would have','you\u2019re': 'you are','you\u2019ve': 'you have','you\u2019ll\u2019ve': 'you shall have','you\u2019ll': 'you will','you\u2019d': 'you would','you\u2019d\u2019ve': 'you would have', 'aka': 'also known as', 'approx.': 'approximately'}\n# c\u00e1c l\u1ed7i ch\u00ednh t\u1ea3 th\u01b0\u1eddng g\u1eb7p ho\u1eb7c c\u00e1c t\u1eeb c\u00e1ch vi\u1ebft kh\u00e1c nh\u01b0ng c\u00f9ng ngh\u0129a\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization','electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin','lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency','simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized','iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime','cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp','undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp','reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone','dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu','openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp','empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money','fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation','trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath','bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized','bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language','splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu','weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency'}","4091403a":"# remove tag------------------------------------------------------------------------------------\n# x\u00f3a c\u00e1c tag kh\u00f4ng mang nhi\u1ec1u \u00fd ngh\u0129a nh\u01b0 url, c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n\ndef remove_tag(x):\n    if '[math]' in x:\n        x = re.sub('\\[math\\].*?math\\]', 'math_equation', x)\n    if 'http' in x or 'www' in x:\n        x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'url', x)\n    return x\n\n# remove numbers--------------------------------------------------------------------------------\n# thay s\u1ed1 b\u1eb1ng # v\u00ec h\u1ea7u h\u1ebft c\u00e1c embedding \u0111\u1ec1u x\u1eed l\u00fd text nh\u01b0 v\u1eady, sau n\u00e0y khi k\u1ebft h\u1ee3p vs embedding\n# \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh s\u1ebd t\u1ed1t h\u01a1n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n# spell missing--------------------------------------------------------------------------------\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef misspell_fix(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\n# abbreviation - t\u1eeb vi\u1ebft t\u1eaft-------------------------------------------------------------------\ndef contraction_fix(word):\n    try:\n        a=contraction_dict[word]\n    except KeyError:\n        a=word\n    return a\n\ndef abbreviation_fix(text):\n    text = \" \".join([contraction_fix(w) for w in text.split()])\n    return text\n\n#stemming text--------------------------------------------------------------------------------\n# ex: studies --> studi, fishes-->fish\ndef stemming(text):\n    ls = LancasterStemmer()\n    text = \" \".join([ls.stem(w) for w in text.split()])\n    return text\n\n# example-----------------------------------\ntext = \"i am travelling around the world.\"\ntext2 = \"i'd've gone to japan\"\nprint('before\\n'+text+'\\n'+text2)\nprint('after')\nprint(misspell_fix(text))\nprint(stemming(text))\nprint(abbreviation_fix(text2))","a28959fc":"# s\u1eed d\u1ee5ng c\u00e1c h\u00e0m b\u00ean tr\u00ean \u0111\u1ec3 clean text\n# clean text g\u1ed3m: x\u00f3a tag, s\u1eeda l\u1ed7i misspell, t\u1eeb vi\u1ebft t\u1eaft,---------------------------------------------\n                # x\u00f3a k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, clean s\u1ed1\ndef clean_text(text, lowercase=True):\n    text = remove_tag(text)\n    if lowercase:\n        text = text.lower()\n    text = misspell_fix(text)\n    text = abbreviation_fix(text)\n    # x\u00f3a k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t kh\u00e1c ch\u1eef v\u00e0 s\u1ed1\n    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n    # clean s\u1ed1\n    text = clean_numbers(text)\n    # text = stemming(text)\n    return text\n\n# clean text cho to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u------------------------------------------------------------------\ndef clean_data(question_text, lowercase=True, preview=False):\n    data_clean = pd.DataFrame()\n    data_clean['question_text'] = question_text\n    data_clean['clean']= data_clean.question_text.parallel_apply(lambda x: clean_text(x, lowercase))\n    if preview:\n        display(data_clean.head(100))\n    return data_clean.clean\n\n# example---------------------------------------------------------\ntext='I really like may'\nprint(text.lower())\nsample['clean']=clean_data(sample[\"question_text\"], preview=True)\n#display(sample)\n","401d1069":"SEQUENCES_LENGTH = 70\n\n# tokenize text------------------------------------------------------------------------------------\ndef create_tokenizer(text_data, preview=False, amount=10):\n    tokenizer = Tokenizer(oov_token='<OOV>')\n    tokenizer.fit_on_texts(list(text_data))    \n    if preview:\n        print(\"Size of Vocab dictionary: \", len(tokenizer.word_index))\n        print(\"Preview: \"+str(amount))\n        print(list(tokenizer.word_index.items())[:amount])\n    return tokenizer\n\n# convert text to sequence by token dictionary----------------------------------------------------\ndef convert_to_sequences(tokenizer, text_data, padding=True, sequences_len=0):\n    # convert to sequences\n    word_sequences = tokenizer.texts_to_sequences(text_data)\n    # padding sequences \u0111\u1ec3 c\u00e1c chu\u1ed7i \u0111\u1ec1u c\u00f3 \u0111\u1ed9 d\u00e0i b\u1eb1ng nhau\n    if padding:\n        word_sequences= pad_sequences(word_sequences, maxlen=sequences_len, \n                          padding='post', truncating='post') \n    return word_sequences  \n    \n# example----------------------------------------------\ntokenizer=create_tokenizer(sample.clean, preview=True)\n#print(len(token_data.word_index)+1)\npadded_sequences = convert_to_sequences(tokenizer, sample['clean'],\n                                        sequences_len=SEQUENCES_LENGTH)\nprint(\"5 first sequences:\")\nfor sequence in padded_sequences[:5]:\n    print(sequence)","feaf89d0":"# f1 score\nf1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\nprint('success')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n                              patience=2, min_lr=0.0001, verbose=0)\nprint('success')\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, \n                              verbose=1, mode='auto', restore_best_weights=True)\nprint('success')\ncallbacks = [earlystopping, reduce_lr]\n\n# h\u00e0m \u0111\u1ec3 x\u00e2y d\u1ef1ng model-------------------------------------------------------------------------------\ndef build_Model_LSTM(features=120000, embedding_dim=200, lr=0.001, embedding_matrix=None):\n    input_x = Input(shape=(SEQUENCES_LENGTH))\n    if not(embedding_matrix is None):\n        embedding = Embedding(features, embedding_dim, input_length=SEQUENCES_LENGTH,\n                              weights=[embedding_matrix], trainable=False)(input_x)\n    else:\n        embedding = Embedding(features, embedding_dim, input_length=SEQUENCES_LENGTH)(input_x)\n    x = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=input_x, outputs=x)\n    opt = Adam(lr)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[f1])\n    return model","20478427":"EMBEDDING_DIM = 300\n\nbatch_size = 512\nnum_epochs = 10\nlearning_rate = 0.001\n\n\n# B1: preprocessing=====================================================================\n# clean t\u1eadp d\u1eef li\u1ec7u train v\u00e0 d\u1eef li\u1ec7u test\nprint('B1: clean text: wait about 40s...')\ntrain_text = clean_data(train_suffle.question_text, lowercase=True)\ntrain_target = train_suffle.target\n\ntest_X = clean_data(test_df.question_text, lowercase=True)\n\n# B2: chia train_df th\u00e0nh 2 t\u1eadp train v\u00e0 validate theo t\u1ec9 l\u1ec7 8:2========================\nprint('B2: splitting train_df')\ntrain_X, val_X, train_Y, val_Y = train_test_split(train_text, train_target,\n                                                  test_size=0.2, random_state=165)\n\n# B3: convert to sequences==============================================================\nprint('B3: convert to sequences....')\n\ntokenizer = create_tokenizer(train_X)\nprint('Create vocab dict success. Start converting..')\n\ntrain_X_seq = convert_to_sequences(tokenizer, train_X, sequences_len=SEQUENCES_LENGTH)\nval_X_seq = convert_to_sequences(tokenizer, val_X, sequences_len=SEQUENCES_LENGTH)\ntest_X_seq = convert_to_sequences(tokenizer, test_X, sequences_len=SEQUENCES_LENGTH)\nprint('Convert successfully.')\n\nfeatures = len(tokenizer.word_index)+1\nprint('features: '+str(features))","7c48858b":"# s\u1eed d\u1ee5ng TPU\nstrategy = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('CPU')","9fea2ecd":"with strategy.scope():\n    # B4: build model=====================================================================\n    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n    print('B4: bulding model....')\n    model = build_Model_LSTM()\n    model.summary()\n\n#     # B5: train model=====================================================================\n#     print('B5: start tranning....')\n#     model.fit(train_X_seq, train_Y, batch_size=batch_size, epochs=num_epochs, \n#               validation_data=(val_X_seq, val_Y), callbacks=callbacks)","b7e225c8":"# t\u00ecm threshold ph\u00f9 h\u1ee3p \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c f1_score cao nh\u1ea5t\ndef best_threshold(y_train,train_preds):\n    f1_score = [0,0,0] # idx, cur, max\n    threshold = 0\n    for f1_score[0] in tqdm(np.arange(0.1, 0.9, 0.01)):\n        f1_score[1] = metrics.f1_score(y_train, np.array(train_preds)>f1_score[0])\n        if f1_score[1] > f1_score[2]:\n            threshold = f1_score[0]\n            f1_score[2] = f1_score[1]\n    print('best threshold: ')\n    print(threshold)\n    print(f1_score[2])\n    return threshold, f1_score[2]","d05adbe6":"def predict_by(trained_model):\n    val_pred = trained_model.predict(val_X_seq, verbose=1, batch_size=256)\n    threshold, f1_score = best_threshold(val_Y, val_pred)\n\n    test_pred = trained_model.predict(test_X_seq, verbose=1, batch_size=256)\n\n    print(metrics.classification_report(val_Y,(val_pred>threshold).astype(int)))\n    return threshold, f1_score, val_pred, test_pred\n\n# predict by trained model========================================================\n# with strategy.scope():\n#     test_pred, threshold = predict_by(model)","57db688a":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n# l\u1ea5y s\u1ed1 d\u00f2ng c\u1ee7a file embedding\ndef get_lines_num(file_name): \n    return sum(1 for _ in open(file_name, encoding=\"utf8\", errors='ignore'))\n# chuy\u1ec3n file embedding th\u00e0nh dict\ndef load_embedding(file_name): \n    print('loading embedding file..')\n    return dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file_name, encoding=\"utf8\", errors='ignore'), \n                                                       total=get_lines_num(file_name)) if len(o) > 100)\n\ndef embedding_matrix(embeddings_vec, features, token_word_index):\n    print('load success. start embedding matrix..')\n    # features = len(tokenizer.word_index)+1\n    # embedding_vec is dict created by load_embedding()\n    all_embs = np.stack(embeddings_vec.values()) \n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    nb_words = features\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    print('yes1')\n    for word, i in token_word_index.items():\n        if i >= features: continue\n        embedding_vector = embeddings_vec.get(word) \n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    print('yes2')\n    return embedding_matrix","e17b174c":"GLOVE_PATH = 'glove.840B.300d\/glove.840B.300d.txt'\nPARA_PATH = 'paragram_300_sl999\/paragram_300_sl999.txt'\nWIKI_PATH = 'wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n\n!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {GLOVE_PATH} -d .\n\nglove_vec = load_embedding(GLOVE_PATH)","9680a880":"glove_emb_matrix = embedding_matrix(glove_vec, features, tokenizer.word_index)","951a0d5b":"# apply glove\n# TPU\nwith strategy.scope():\n# B4: build model========================================================================\n    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n    print('B4: bulding model with glove embedding....')\n    glove_model = build_Model_LSTM(features, EMBEDDING_DIM, learning_rate, glove_emb_matrix)\n    #glove_model.summary()\n\n# B5: train model========================================================================\n    print('B5: start tranning....')\n    glove_model.fit(train_X_seq, train_Y, batch_size=batch_size, epochs=num_epochs, \n              validation_data=(val_X_seq, val_Y), callbacks=callbacks)\n# predict by trained model===============================================================\n    glove_threshold, glove_f1_score, glove_val_pred, glove_test_pred = predict_by(glove_model)\n    \n    ","fc41374a":"# gi\u1ea3m t\u1ea3i cho b\u1ed9 nh\u1edb\ndel glove_vec\ngc.collect()","a8da0997":"!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {WIKI_PATH} -d .\n\nwiki_vec = load_embedding(WIKI_PATH)","b1c2807a":"wiki_emb_matrix = embedding_matrix(wiki_vec, features, tokenizer.word_index)","f4fdd37a":"# apply wiki\n# TPU\nwith strategy.scope():\n# B4: build model========================================================================\n    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n    print('B4: bulding model with glove embedding....')\n    wiki_model = build_Model_LSTM(features, EMBEDDING_DIM, learning_rate, wiki_emb_matrix)\n\n# B5: train model========================================================================\n    print('B5: start tranning....')\n    wiki_model.fit(train_X_seq, train_Y, batch_size=batch_size, epochs=num_epochs, \n              validation_data=(val_X_seq, val_Y), callbacks=callbacks)\n# predict by trained model===============================================================\n    wiki_threshold, wiki_f1_score, wiki_val_pred, wiki_test_pred = predict_by(wiki_model)","81cb5086":"# gi\u1ea3m t\u1ea3i cho b\u1ed9 nh\u1edb\ndel wiki_vec\ngc.collect()","d85dbb5a":"!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {PARA_PATH} -d .\n\npara_vec = load_embedding(PARA_PATH)","bcf7647d":"para_emb_matrix = embedding_matrix(para_vec, features, tokenizer.word_index)","2fbb0909":"# apply para\n# TPU\nwith strategy.scope():\n# B4: build model========================================================================\n    f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n    print('B4: bulding model with glove embedding....')\n    para_model = build_Model_LSTM(features, EMBEDDING_DIM, learning_rate, para_emb_matrix)\n\n# B5: train model========================================================================\n    print('B5: start tranning....')\n    para_model.fit(train_X_seq, train_Y, batch_size=batch_size, epochs=num_epochs, \n              validation_data=(val_X_seq, val_Y), callbacks=callbacks)\n# predict by trained model===============================================================\n    para_threshold, para_f1_score, para_val_pred, para_test_pred = predict_by(para_model)","ce6d5c9e":"# gi\u1ea3m t\u1ea3i cho b\u1ed9 nh\u1edb\ndel para_vec\ngc.collect()","8d6dbaa0":"val = np.zeros((len(val_X),), dtype=np.float32)\n\nval += 1\/3 * np.squeeze(glove_val_pred)\nval += 1\/3 * np.squeeze(para_val_pred)\nval += 1\/3 *np.squeeze(wiki_val_pred)\nthreshold_global, f1_global = best_threshold(val_Y, val)\nprint(metrics.classification_report(val_Y,(val>threshold_global).astype(int)))\n\npred = np.zeros((len(test_X),), dtype=np.float32)\npred += 1\/3 * np.squeeze(glove_test_pred)\npred += 1\/3 * np.squeeze(para_test_pred)\npred += 1\/3 * np.squeeze(wiki_test_pred)\ntest_pred=((pred>threshold_global).astype(int))\n","679b9b08":"#test_pred = (test_pred>threshold).astype(int)\n\nsubmit = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nsubmit['prediction'] = test_pred\nsubmit.to_csv(\"submission.csv\", index=False)","5553baa7":"# 4. M\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n","f9c1fc5e":"# B\u00e1o c\u00e1o b\u00e0i t\u1eadp l\u1edbn cu\u1ed1i k\u1ef3\n\nH\u1ecd v\u00e0 t\u00ean: L\u00ea Th\u1ecb B\u00edch Duy\u00ean\n\nMSV: 19021254\n\nL\u1edbp: 2122I_INT3405E_20 \n\n## N\u1ed9i dung\n1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\n2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n3. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u\n4. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh\n5. Hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n\n6. Submission\n\n# 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n\nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 nh\u1eadn l\u1ea1i nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng t\u1eeb c\u1ed9ng \u0111\u1ed3ng \u0111\u1ec3 gi\u00fap \u0111\u1ee1 v\u00e0 h\u1ecdc h\u1ecfi l\u1eabn nhau. \nB\u00e0i to\u00e1n \u0111\u1eb7t ra l\u00e0 l\u00e0m sao \u0111\u1ec3 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - d\u1ef1a tr\u00ean nh\u1eefng ti\u00ean \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch.\n- Input: c\u00e2u h\u1ecfi tr\u00ean quora \u1edf d\u1ea1ng text\n- Output: 0 ho\u1eb7c 1 (ch\u00e2n th\u00e0nh ho\u1eb7c kh\u00f4ng ch\u00e2n th\u00e0nh)\n\n# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n","70b5a88f":"Sau khi k\u1ebft h\u1ee3p c\u1ea3 3 embedding ta th\u1ea5y f1-score c\u00f3 t\u0103ng l\u00ean m\u1ed9t ch\u00fat so v\u1edbi ch\u1ea1y ri\u00eang l\u1ebb v\u1edbi t\u1eebng embedding.","3e7fc156":"# 3. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u\nV\u00ec d\u1eef li\u1ec7u d\u1ea1ng text th\u01b0\u1eddng ph\u1ee9c t\u1ea1p do b\u1ea3n th\u00e2n c\u00e1c t\u1eeb l\u00e0 d\u1eef li\u1ec7u r\u1eddi r\u1ea1c, nhi\u1ec1u nhi\u1ec5u n\u00ean \u0111\u1ec3 chuy\u1ec3n th\u00e0nh d\u1ea1ng m\u00e1y t\u00ednh c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c ta c\u1ea7n ph\u1ea3i x\u1eed l\u00fd tr\u01b0\u1edbc d\u1eef li\u1ec7u.\n\nC\u00e1c b\u01b0\u1edbc bao g\u1ed3m:\n\n**B1: Clean d\u1eef li\u1ec7u:**\n- b\u1ecf c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t (d\u1ea5u c\u00e2u...)\n- b\u1ecf c\u00e1c tag kh\u00f4ng c\u1ea7n thi\u1ebft (c\u00f4ng th\u1ee9c to\u00e1n, \u0111\u01b0\u1eddng link)\n- s\u1eeda c\u00e1c l\u1ed7i ch\u00ednh t\u1ea3 th\u01b0\u1eddng g\u1eb7p\n- lowercase\n- chuy\u1ec3n c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng nguy\u00ean b\u1ea3n\n- b\u1ecf c\u00e1c stopword (v\u00ed d\u1ee5 nh\u01b0 the, is,... kh\u00f4ng c\u00f3 ng\u1eef ngh\u0129a c\u1ee5 th\u1ec3) - \u1edf \u0111\u00e2y em kh\u00f4ng b\u1ecf c\u00e1c stopword v\u00ec tuy c\u00e1c t\u1eeb n\u00e0y \u0111\u1ee9ng ri\u00eang l\u1ebb th\u00ec ko c\u00f3 ng\u1eef ngh\u0129a nh\u01b0ng n\u1ebfu x\u00f3a h\u1ebft th\u00ec ngh\u0129a c\u1ee7a c\u00e2u s\u1ebd b\u1ecb \u1ea3nh h\u01b0\u1edfng kh\u00e1 nhi\u1ec1u, d\u1ef1 \u0111o\u00e1n c\u00f3 th\u1ec3 l\u00e0m gi\u1ea3m \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh.\n\n**B2: Tokenization** \n\n**B3: Convert to sequences** ","1c1ce9b6":"S\u1eed d\u1ee5ng TPU \u0111\u1ec3 c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 training, em th\u1ea5y nhanh h\u01a1n kho\u1ea3ng 3 l\u1ea7n so v\u1edbi GPU v\u00e0 kh\u00f4ng g\u1eb7p t\u00ecnh tr\u1ea1ng b\u1ecb d\u1eebng l\u1ea1i do truy c\u1eadp qu\u00e1 m\u1ee9c b\u1ed9 nh\u1edb\n\nhttps:\/\/www.kaggle.com\/kutaykutlu\/tpus-resnet50-leaf-disease#Detect-TPU","a7a6c100":"### Tokenization v\u00e0 Convert to sequences:\n- `create_tokenizer()`: \u0111\u1ec3 chuy\u1ec3n 1 c\u00e2u th\u00e0nh t\u1eadp c\u00e1c t\u1eeb ri\u00eang l\u1ebb, (m\u1ed7i token l\u00e0 m\u1ed9t t\u1eeb), s\u1eed d\u1ee5ng Tokenizer c\u1ee7a keras \u0111\u1ec3 t\u1ea1o m\u1ed9t b\u1ed9 t\u1eeb \u0111i\u1ec3n token, t\u1eeb xu\u1ea5t hi\u1ec7n c\u00e0ng nhi\u1ec1u th\u00ec index c\u1ee7a n\u00f3 c\u00e0ng nh\u1ecf\n    - khi s\u1eed d\u1ee5ng b\u1ed9 vocab tokenizer \u0111\u01b0\u1ee3c t\u1ea1o ra \u0111\u1ec3 chuy\u1ec3n v\u0103n b\u1ea3n th\u00e0nh chu\u1ed7i s\u1ed1 th\u00ec c\u00f3 th\u1ec3 c\u00f3 nh\u1eefng t\u1eeb \u1edf v\u0103n b\u1ea3n kh\u00f4ng n\u1eb1m trong tokenizer, v\u00ec v\u1eady ta d\u00f9ng `'<OOV>'` \u0111\u1ec3 thay th\u1ebf cho ch\u00fang.\n    VD: tokenizer t\u00ecm \u0111\u01b0\u1ee3c: `['<OOV>':1,'this': 2, 'fat': 3]`\n    \n    v\u0103n b\u1ea3n c\u1ea7n chuy\u1ec3n: `text` = `this cat is very fat`\n    \n    => sau khi s\u1eed d\u1ee5ng tokenizer: `text` = `[2 1 1 1 3]`\n- `convert_to_sequences()`: s\u1eed d\u1ee5ng t\u1eeb \u0111i\u1ec3n \u0111\u00e3 t\u1ea1o \u0111\u1ec3 chuy\u1ec3n text sang chu\u1ed7i s\u1ed1 \u0111\u1ec3 m\u00e1y t\u00ednh hi\u1ec3u \u0111\u01b0\u1ee3c, padding v\u00e0 truncating \u0111\u1ec3 \u0111\u1ed3ng b\u1ed9 chi\u1ec1u d\u00e0i c\u00e1c c\u00e2u th\u00e0nh m\u1ed9t gi\u00e1 tr\u1ecb c\u1ed1 \u0111\u1ecbnh\n\n    d\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 \u0111\u00e3 ph\u00e2n t\u00edch \u1edf ph\u1ea7n 2 ta nh\u1eadn th\u1ea5y h\u1ea7u h\u1ebft c\u00e1c c\u00e2u h\u1ecfi \u0111\u1ec1u c\u00f3 \u0111\u1ed9 d\u00e0i t\u1eeb 55 t\u1eeb tr\u1edf xu\u1ed1ng, c\u00e2u d\u00e0i h\u01a1n c\u00f3 s\u1ed1 l\u01b0\u1ee3ng r\u1ea5t \u00edt n\u00ean ta ch\u1ecdn \u0111\u1ed9 d\u00e0i chung cho c\u00e1c chu\u1ed7i l\u00e0 70 t\u1eeb\n","3e04e140":"X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n b\u1eb1ng Bidirectional LSTM (Long Short-Term Memory Networks)\n\nTr\u01b0\u1edbc khi hi\u1ec3u m\u00f4 h\u00ecnh LSTM l\u00e0 g\u00ec th\u00ec ta c\u1ea7n hi\u1ec3u qua v\u1ec1 m\u1ea1ng RNN - recurrent neural network.\nRNN c\u00f3 th\u1ec3 mang th\u00f4ng tin t\u1eeb c\u00e1c layer tr\u01b0\u1edbc \u0111\u1ebfn layer sau, n\u00ean n\u00f3 c\u00f3 th\u1ec3 d\u00f9ng \u0111\u1ec3 x\u1eed l\u00fd th\u00f4ng tin d\u1ea1ng chu\u1ed7i. M\u1ed9t v\u00ed d\u1ee5 c\u1ee7a RNN trong b\u00e0i to\u00e1n d\u1ef1 \u0111o\u00e1n video, RNN c\u00f3 th\u1ec3 mang th\u00f4ng tin c\u1ee7a frame \u1ea3nh t\u1eeb state tr\u01b0\u1edbc t\u1edbi state sau, tuy nhi\u00ean state \u1edf tr\u01b0\u1edbc \u0111\u00f3 c\u00e0ng xa th\u00ec c\u00e0ng b\u1ecb vanishing gradient, ngh\u0129a l\u00e0 th\u00f4ng tin ch\u1ec9 mang \u0111\u01b0\u1ee3c qua m\u1ed9t l\u01b0\u1ee3ng state nh\u1ea5t \u0111\u1ecbnh\nhay n\u00f3i c\u00e1ch kh\u00e1c l\u00e0 model ch\u1ec9 h\u1ecdc \u0111\u01b0\u1ee3c t\u1eeb c\u00e1c state g\u1ea7n n\u00f3 - short term memory\n\nV\u00ec v\u1eady, m\u00f4 h\u00ecnh Long short term memory ra \u0111\u1eddi \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 khi ta c\u1ea7n c\u00e1c th\u00f4ng tin t\u1eeb state \u1edf tr\u01b0\u1edbc \u0111\u00f3 r\u1ea5t xa v\u00e0 tr\u00e1nh \u0111\u01b0\u1ee3c vanishing gradient. N\u00f3 v\u1eabn gi\u1eef t\u01b0 t\u01b0\u1edfng ch\u00ednh c\u1ee7a RNN l\u00e0 s\u1ef1 sao ch\u00e9p ki\u1ebfn tr\u00fac theo d\u1ea1ng chu\u1ed7i nh\u01b0ng c\u00f3 ph\u1ea7n ph\u1ee9c t\u1ea1p h\u01a1n.\n\n- Bidirectional LSTM l\u00e0 m\u1ed9t bi\u1ebfn th\u1ec3 c\u1ee7a LSTM. Trong khi LSTM \u0111\u01a1n h\u01b0\u1edbng do \u0111\u1ea7u v\u00e0o duy nh\u1ea5t m\u00e0 n\u00f3 l\u1ea5y l\u00e0 t\u1eeb qu\u00e1 kh\u1ee9 n\u00ean n\u00f3 kh\u00f4ng th\u1ec3 l\u01b0u gi\u1eef c\u00e1c th\u00f4ng tin \u1edf t\u01b0\u01a1ng lai. BiLSTM s\u1ebd ch\u1ea1y th\u00f4ng tin \u0111\u1ea7u v\u00e0o theo hai c\u00e1ch: t\u1eeb qu\u00e1 kh\u1ee9 \u0111\u1ebfn t\u01b0\u01a1ng lai v\u00e0 t\u1eeb t\u01b0\u01a1ng lai tr\u1edf l\u1ea1i. Do \u0111\u00f3 n\u00f3 c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c ng\u1eef c\u1ea3nh c\u1ee7a v\u0103n b\u1ea3n t\u1ed1t h\u01a1n.\n\nVD: m\u1ed9t v\u00ed d\u1ee5 v\u1ec1 d\u1ef1 \u0111o\u00e1n c\u1ea7n th\u00f4ng tin t\u1eeb t\u01b0\u01a1ng lai trong \u0111o\u1ea1n text l\u00e0:\n\n   `T\u00f4i ph\u1ea3i \u0111i h\u1ecdc. H\u00f4m qua tr\u1eddi ... S\u00e1ng nay \u0111\u01b0\u1eddng \u01b0\u1edbt v\u00e0 tr\u01a1n tr\u01b0\u1ee3t`\n\nTa kh\u00f4ng th\u1ec3 \u0111o\u00e1n \u0111\u01b0\u1ee3c n\u1ebfu nh\u01b0 ch\u1ec9 s\u1eed d\u1ee5ng th\u00f4ng tin t\u1eeb qu\u00e1 kh\u1ee9, m\u00e0 c\u1ea7n s\u1eed d\u1ee5ng th\u00f4ng tin \u1edf c\u00e2u sau, t\u1ee9c l\u00e0 t\u1eeb t\u01b0\u01a1ng lai ` s\u00e1ng nay` \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n t\u1eeb c\u00f2n thi\u1ebfu l\u00e0 `m\u01b0a` \n\n","bd0cf0d8":"Nh\u01b0 v\u1eady l\u00e0 c\u00e1c h\u00e0m preprocessing \u0111\u00e3 x\u00e2y d\u1ef1ng v\u00e0 ho\u1ea1t \u0111\u1ed9ng t\u1ed1t, em s\u1ebd \u00e1p d\u1ee5ng \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u c\u1ee7a b\u00e0i to\u00e1n \u1edf ph\u1ea7n 5.","831ead6f":"D\u1eef li\u1ec7u train g\u1ed3m 1306122 h\u00e0ng v\u1edbi 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u\n- qid: m\u00e3 \u0111\u1ecbnh danh c\u00e2u h\u1ecfi\n- question_text: c\u00e2u h\u1ecfi d\u1ea1ng text tr\u00ean quora\n- target: nh\u00e3n c\u1ee7a c\u00e2u h\u1ecfi, 0 t\u01b0\u01a1ng \u1ee9ng v\u1edbi sincere, 1 t\u01b0\u01a1ng \u1ee9ng v\u1edbi insincere\n\nNh\u1eadn th\u1ea5y d\u1eef li\u1ec7u train v\u00e0 test c\u00f3 c\u00e1c gi\u00e1 tr\u1ecb \u0111\u1ec1u h\u1ee3p l\u1ec7, kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb n\u00e0o null","b7d42649":"### a. Hu\u1ea5n luy\u1ec7n model v\u1edbi GloVe embedding\nK\u1ebft qu\u1ea3 \u0111\u1ea1t \u0111\u01b0\u1ee3c l\u00e0 f1_score \u0111\u00e3 t\u0103ng l\u00ean 0.68 so v\u1edbi f1_score khi kh\u00f4ng s\u1eed d\u1ee5ng nh\u00fang l\u00e0 0.64. Khi hu\u1ea5n luy\u1ec7n c\u00f3 th\u00eam GloVe \u1edf version 8 th\u00ec Private score t\u0103ng l\u00ean 0.686 ","7a69a2e5":"### b. Hu\u1ea5n luy\u1ec7n model v\u1edbi wiki","e68e2643":"### C\u1ea3i thi\u1ec7n d\u1ef1 \u0111o\u00e1n b\u1eb1ng c\u00e1ch k\u1ebft h\u1ee3p c\u00e1c embedding cho s\u1eb5n\n\u1ede \u0111\u00e2y em th\u1eed nghi\u1ec7m v\u1edbi 3 lo\u1ea1i embedding l\u00e0:\n- Paragram\n- GloVe - Global Vectors: m\u00f4 h\u00ecnh l\u00e0 thu\u1eadt to\u00e1n h\u1ecdc t\u1eadp kh\u00f4ng gi\u00e1m s\u00e1t \u0111\u1ec3 t\u1ea1o ra c\u00e1c bi\u1ec3u di\u1ec5n vector cho c\u00e1c t\u1eeb, GloVe h\u1ecdc b\u1eb1ng c\u00e1ch x\u00e2y d\u1ef1ng ma tr\u1eadn \u0111\u1ed3ng xu\u1ea5t hi\u1ec7n, c\u01a1 b\u1ea3n l\u00e0 n\u00f3 t\u00ednh t\u1ea7n su\u1ea5t m\u1ed9t t\u1eeb xu\u1ea5t hi\u1ec7n trong ng\u1eef c\u1ea3nh\n- Wiki: hay FastText kh\u00e1 kh\u00e1c v\u1edbi nh\u00fang tr\u00ean, trong khi GloVe coi \u0111\u01a1n v\u1ecb nh\u1ecf nh\u1ea5t \u0111\u1ec3 \u0111\u00e0o t\u1ea1o l\u00e0 m\u1ed9t t\u1eeb th\u00ec FastText s\u1eed d\u1ee5ng c\u00e1c k\u00ed t\u1ef1 n-gram l\u00e0m \u0111\u01a1n v\u1ecb hu\u1ea5n luy\u1ec7n nh\u1ecf nh\u1ea5t. L\u1ee3i \u00edch l\u1edbn nh\u1ea5t c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng FastText l\u00e0 n\u00f3 t\u1ea1o ra c\u00e1c nh\u00fang t\u1eeb t\u1ed1t h\u01a1n cho c\u00e1c t\u1eeb hi\u1ebfm ho\u1eb7c th\u1eadm ch\u00ed l\u00e0 c\u00e1c t\u1eeb kh\u00f4ng \u0111\u01b0\u1ee3c t\u00ecm ra trong qu\u00e1 tr\u00ecnh trainning do c\u00e1c vector k\u00ed t\u1ef1 n-gram c\u00f3 kh\u1ea3 n\u0103ng xu\u1ea5t hi\u1ec7n trong c\u00e1c t\u1eeb kh\u00e1c nhi\u1ec1u h\u01a1n. \u0110\u00e2y l\u00e0 \u0111i\u1ec1u GloVe kh\u00f4ng th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c.\n    \n    VD: word vector `apple` c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c chia th\u00e0nh c\u00e1c word vector unit ri\u00eang bi\u1ec7t l\u00e0 `ap`, `app`, `ple`.\n","59da07be":"T\u1eeb bi\u1ec3u \u0111\u1ed3 tr\u00ean ta th\u1ea5y s\u1ed1 c\u00e2u h\u1ecfi sincere ch\u00eanh l\u1ec7ch kh\u00e1 l\u1edbn, g\u1ea5p kho\u1ea3ng 15 l\u1ea7n so v\u1edbi c\u00e2u h\u1ecfi insincere. V\u00ec k\u00edch th\u01b0\u1edbc d\u1eef li\u1ec7u \u1edf c\u00e1c l\u1edbp kh\u00f4ng c\u00e2n b\u1eb1ng nhau, n\u00ean ta s\u1eed d\u1ee5ng ph\u00e9p \u0111o Precision-Recall, l\u1ea5y F1-score (harmonic mean c\u1ee7a precision v\u00e0 recall) \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh. F1 c\u00e0ng cao th\u00ec b\u1ed9 ph\u00e2n l\u1edbp c\u00e0ng t\u1ed1t.\n\n$\\frac{2}{F_1}=\\frac{1}{precision}+\\frac{1}{recall}$\n\ntrong \u0111\u00f3: \n- precision - \u0111\u1ed9 ch\u00ednh x\u00e1c: t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m true positive (TP) trong s\u1ed1 nh\u1eefng \u0111i\u1ec3m \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i l\u00e0 positive (TP + FP).\n- recall - \u0111\u1ed9 bao ph\u1ee7: t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m true positive (TP) trong s\u1ed1 nh\u1eefng \u0111i\u1ec3m th\u1ef1c s\u1ef1 l\u00e0 positive (TP + FN).\n\n(https:\/\/exposedjunction.com\/f1-score-la-gi\/)\n\nKh\u00f4ng th\u1ec3 s\u1eed d\u1ee5ng accuracy (t\u00ednh b\u1eb1ng t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m d\u1ef1 \u0111o\u00e1n \u0111\u00fang\/t\u1ed5ng s\u1ed1 \u0111i\u1ec3m trong t\u1eadp ki\u1ec3m th\u1eed) \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh v\u00ec n\u00f3 s\u1ebd d\u1ef1 \u0111o\u00e1n c\u00e2u h\u1ecfi h\u1ea7u h\u1ebft thu\u1ed9c nh\u00f3m sincere, m\u00f4 h\u00ecnh s\u1ebd c\u00f3 sai s\u1ed1 l\u1edbn v\u00e0 kh\u00f4ng d\u00f9ng \u0111\u01b0\u1ee3c.\n","75e3aa81":"# 6. Submission\nK\u1ebft h\u1ee3p c\u00e1c k\u1ebft qu\u1ea3 t\u1eeb 3 m\u00f4 h\u00ecnh. f1_score \u1edf c\u1ea3 3 kh\u00f4ng c\u00f3 s\u1ef1 ch\u00eanh l\u1ec7ch qu\u00e1 l\u1edbn v\u00e0 \u0111\u1ec1u c\u1ea3i thi\u1ec7n \u0111\u1ed9 ch\u00ednh x\u00e1c h\u01a1n so v\u1edbi kh\u00f4ng s\u1eed d\u1ee5ng n\u00ean ta l\u1ea5y trung b\u00ecnh 3 k\u1ebft qu\u1ea3.","737cb0ab":"Nh\u1eadn x\u00e9t: \u0111\u1ed3 th\u1ecb c\u1ee7a c\u00e2u h\u1ecfi sincere (ch\u00e2n th\u00e0nh) h\u1eb9p v\u00e0 d\u1ed1c h\u01a1n c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh => c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh c\u00f3 xu h\u01b0\u1edbng \u00edt t\u1eeb h\u01a1n.\n- c\u00e2u h\u1ecfi sincere c\u00f3 \u0111\u1ed9 d\u00e0i trung b\u00ecnh l\u00e0 12.5 t\u1eeb tr\u00ean m\u1ed9t c\u00e2u, c\u00e2u d\u00e0i nh\u1ea5t l\u00e0 134 t\u1eeb, h\u1ea7u h\u1ebft \u0111\u1ed9 d\u00e0i \u0111\u1ec1u n\u1eb1m trong kho\u1ea3ng d\u01b0\u1edbi 55 t\u1eeb\n- c\u00e2u h\u1ecfi insincere c\u00f3 \u0111\u1ed9 d\u00e0i trung b\u00ecnh 17.3 t\u1eeb, c\u00e2u d\u00e0i nh\u1ea5t l\u00e0 64 t\u1eeb v\u00e0 \u0111\u1ed9 d\u00e0i h\u1ea7u h\u1ebft \u0111\u1ec1u n\u1eb1m trong kho\u1ea3ng 55 t\u1eeb tr\u1edf xu\u1ed1ng","4df2e131":"Khi kh\u00f4ng s\u1eed d\u1ee5ng th\u00eam c\u00e1c pretrain embedding th\u00ec private score \u0111\u1ea1t \u0111\u01b0\u1ee3c l\u00e0 0.644 c\u00f3 th\u1ec3 xem \u1edf version 4 v\u00e0 \u0111o\u1ea1n code d\u01b0\u1edbi \u0111\u00e2y.","96f0f71d":"# 5. Hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n\n## 5.1. Hu\u1ea5n luy\u1ec7n","d45239aa":"### c. Hu\u1ea5n luy\u1ec7n model v\u1edbi para","b05720bb":"## 5.2. S\u1eed d\u1ee5ng model \u0111\u00e3 hu\u1ea5n luy\u1ec7n \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n","f59c8497":"# Reference\n- clean data: https:\/\/www.exxactcorp.com\/blog\/Deep-Learning\/text-preprocessing-methods-for-deep-learning\n- model with embeddings: https:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge"}}