{"cell_type":{"64f0a657":"code","923844b1":"code","634f2037":"code","d5ea7fbf":"code","f6339f02":"code","53282728":"code","e56cf7bd":"code","76abeffb":"code","9edf33c8":"code","4f95d400":"code","c575ca0e":"code","c5b545b1":"markdown","54c2e414":"markdown","af4273fa":"markdown","d26679b0":"markdown","a6a6f64c":"markdown","3cd304f4":"markdown","25cf09f2":"markdown","9bb5125f":"markdown","b5640d28":"markdown","1522bebf":"markdown","c2351364":"markdown","13abefd7":"markdown","72a3c8be":"markdown","85294058":"markdown","107aa3ae":"markdown"},"source":{"64f0a657":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.signal\nfrom scipy.signal import savgol_filter\nfrom  scipy import ndimage\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize \nfrom scipy.ndimage.filters import uniform_filter1d\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import balanced_accuracy_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras import metrics, regularizers\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nimport keras\nimport seaborn as sn\nimport math\nimport timeit\nimport tensorflow as tf","923844b1":"rutatrain = '..\/input\/kepler-labelled-time-series-data\/exoTrain.csv'\nrutatest = '..\/input\/kepler-labelled-time-series-data\/exoTest.csv'\nextrain_df = pd.read_csv(rutatrain)\nextest_df = pd.read_csv(rutatest)\n\n# Separate the label from the rest of attributes\nY_train = extrain_df.LABEL\nX_train = extrain_df.drop('LABEL',axis=1)\nY_test = extest_df.LABEL\nX_test = extest_df.drop('LABEL',axis=1)","634f2037":"# Applying Fast Fourier Transform (FFT)\ndef fourier_transform(df):\n    df_fft = np.abs(np.fft.fft(df, axis=1))\n    return df_fft\n\n# Handling upper outliers: personalized function\ndef reduce_upper_outliers(df,reduce = 0.01, half_width=4):\n    length = len(df.iloc[0,:])\n    remove = int(length*reduce)\n    for i in df.index.values: #Para cada muestra\n        values = df.loc[i,:]\n        sorted_values = values.sort_values(ascending = False)\n        for j in range(remove): \n            idx = sorted_values.index[j]\n            new_val = 0\n            count = 0\n            idx_num = int(idx[5:])\n            for k in range(2*half_width+1):\n                idx2 = idx_num + k - half_width\n                if idx2 <1 or idx2 >= length or idx_num == idx2:\n                    continue\n                new_val += values['FLUX.'+str(idx2)]\n\n                count += 1\n            new_val \/= count # count will always be positive here\n            if new_val < values[idx]: # just in case there's a few persistently high adjacent values\n                df.at[i,idx] = new_val\n    return df\n\ndef iterar_ruo(df, n=2):\n    for i in range(n): \n        df2 = reduce_upper_outliers(df)\n    return df2\n\n# Handling outliers: Smoothing filters\ndef apply_filter(df,filternumber):\n    #UNIFORM FILTER == 0\n    if filternumber == 0:\n        filt = uniform_filter1d(df, axis=1, size=50)\n    #GAUSSIAN FILTER == 1\n    elif filternumber == 1:\n        filt = ndimage.filters.gaussian_filter(df, sigma=10)\n    #Savitzky-Golay FILTER == 2\n    elif filternumber == 2:\n        filt = savgol_filter(df,21,4,deriv=0)\n\n    return filt\n\n# Normalizing data\ndef apply_normalization(df_train, df_test, nnumber):\n    #MinMax Scaler\n    if nnumber == 0:\n        scaler = MinMaxScaler()\n        norm_train = scaler.fit_transform(df_train)\n        norm_test = scaler.transform(df_test)\n    #Normalize\n    elif nnumber == 1:\n        norm_train = normalize(df_train)\n        norm_test = normalize(df_test)\n    #Robust Scaler\n    elif nnumber == 2:\n        scaler = RobustScaler()\n        norm_train = scaler.fit_transform(df_train)\n        norm_test = scaler.transform(df_test)\n        \n    \n    norm_train = pd.DataFrame(norm_train)\n    norm_test = pd.DataFrame(norm_test)\n    return norm_train, norm_test\n\n# Standardizing data\ndef apply_standarization(df_train, df_test):\n    scaler = StandardScaler()\n    norm_train = scaler.fit_transform(df_train)\n    norm_test = scaler.transform(df_test)\n    \n    norm_train = pd.DataFrame(norm_train)\n    norm_test = pd.DataFrame(norm_test)\n    return norm_train, norm_test\n","d5ea7fbf":"class dataProcessor:\n\n    def __init__(self, outlier=False, smoothing=False, fourier=False, normalize=False, standardize=False):\n        self.outlier = outlier\n        self.smoothing = smoothing\n        self.normalize = normalize\n        self.standardize = standardize\n        self.fourier = fourier\n    \n    def process(self, df_train_x, df_test_x):\n        \n        # Handling outliers\n        if self.outlier:\n            print(\"Removing upper outliers...\")\n            df_train_x = iterar_ruo(df_train_x, P_OUTLIERS)\n            df_test_x = iterar_ruo(df_test_x, P_OUTLIERS)\n            \n        # Apply fourier transform\n        if self.fourier:\n            print(\"Applying Fourier...\")\n            df_train_x = fourier_transform(df_train_x)\n            df_test_x = fourier_transform(df_test_x)\n        \n        # Applying smoothing filters\n        if self.smoothing:\n            print(\"Applying smoothing filter...\")\n            df_train_x = pd.DataFrame(apply_filter(df_train_x, FILTER_NUMBER))\n            df_test_x = pd.DataFrame(apply_filter(df_test_x, FILTER_NUMBER))\n            \n        # Normalization\n        if self.normalize:\n            print(\"Normalizing...\")\n            df_train_x, df_test_x = apply_normalization(df_train_x, df_test_x, TIPO_NORM)\n            \n        # Normalization\n        if self.standardize:\n            print(\"Standardizing...\")\n            df_train_x, df_test_x = apply_standarization(df_train_x, df_test_x)\n        \n\n        print(\"Finished Processing!\")\n        return df_train_x, df_test_x","f6339f02":"# Change the value of P_OUTLIERS to apply reduce_upper_outliers over a specific percentage of the values of each sample\n# Change the value of FILTER_NUMBER to apply a specific smoothing filter\n    #\u00a0FILTER_NUMBER == 0 -> UNIFORM FILTER\n    #\u00a0FILTER_NUMBER == 1 -> GAUSSIAN FILTER\n    #\u00a0FILTER_NUMBER == 2 -> SAVITZKY-GOLAY FILTER\n# Change the value of TIPO_NORM to apply a specific normalization method\n    #\u00a0TIPO_NORM == 0 -> MaxMinScaler\n    #\u00a0TIPO_NORM == 1 -> normalize\n    #\u00a0TIPO_NORM == 2 -> RobustScaler\n    \nP_OUTLIERS = 2\nFILTER_NUMBER = 1\nTIPO_NORM = 1\n\nProcessor = dataProcessor(\n    outlier = True,\n    fourier = True,\n    smoothing = True,\n    normalize= True,\n    standardize= True\n)\n\ndf_train_x = X_train.copy()\ndf_test_x = X_test.copy()\ndf_train_x, df_test_x = Processor.process(df_train_x, df_test_x)","53282728":"from sklearn.utils import resample\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\n# UPSAMPLING: RandomOverSampler\nX_total = df_train_x.copy()\nos =  RandomOverSampler(sampling_strategy='minority')\nupsampling, upsampling_Y = os.fit_sample(X_total, Y_train)\n\nprint (\"Distribution before resampling {}\".format(Counter(Y_train)))\nprint (\"Distribution labels after resampling {}\".format(Counter(upsampling_Y)))\n\nupsampling_Y.value_counts().plot(kind='bar', title='Count (target)');","e56cf7bd":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef specificity_m(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n    specificity = true_negatives \/ (possible_negatives + K.epsilon())\n    return specificity\n\ndef imb_accuracy(y_true, y_pred):\n    recall = recall_m(y_true, y_pred)\n    specificity = specificity_m(y_true, y_pred)\n    score = (0.5 * recall) + (0.5 * specificity)\n    return score\n\n# Transformation of train labels to OneHotEncoder format\ndef transform_Y(y_samp, y_sampTest):\n    y_samp[y_samp < 2] = 0\n    y_samp[y_samp > 1] = 1\n    y_sampTest[y_sampTest < 2] = 0\n    y_sampTest[y_sampTest > 1] = 1\n    Y_train_ohe = np.zeros((y_samp.shape[0], 2))\n\n    for i in range(y_samp.shape[0]):\n        Y_train_ohe[i, int(y_samp[i])] = 1\n        \n    return Y_train_ohe, y_sampTest","76abeffb":"def create_model(X_samp, neurons=40, activation='relu', init_mode='uniform', dropout_rate = 0.5, learn_rate=0.001):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim = X_samp.shape[-1], kernel_initializer=init_mode, activation = activation, kernel_regularizer='l2'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(2, kernel_initializer=init_mode, activation = \"sigmoid\"))\n    \n    optimizer = Adam(lr=learn_rate)\n    #Adam is a optimized version of a SGD (Stochastic Gradient) optimizer.\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[imb_accuracy])\n    #binary_crossentropy is the go-to loss function for classification tasks, either balanced or imbalanced. \n    #It is the first choice when no preference is built from domain knowledge yet.\n    return model","9edf33c8":"EPOCHS = 100\nVAL_SPLIT = 0.2\nBATCH_SIZE = 75\nearly_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)","4f95d400":"\nTest_Y = Y_test.copy()\nX_sampTest = df_test_x.copy()\nupsamplingg_Y = upsampling_Y.copy()\nX_sampUp = upsampling.copy()\n\n# Creation and training of the NN\nmodelUp = create_model(X_sampUp)\nY_train_ohe, Test_Y = transform_Y(upsamplingg_Y, Test_Y)\nstart_time_train = timeit.default_timer()\nbaseline_historyUp = modelUp.fit(X_sampUp, Y_train_ohe, validation_split = VAL_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=[early_stop])\nupsampling_elapsed = timeit.default_timer() - start_time_train\nprint('Training time: ' + str(upsampling_elapsed))\n\n\n# Representation of the NN arquitecture\nprint(modelUp.summary())\n\n# Prediction\ny_test_pred = modelUp.predict_classes(np.array(X_sampTest))\ny_scores = modelUp.predict_proba(X_sampTest)[:,1]\n\n# Confussion matrix and classification_report\nprint(classification_report(Test_Y, y_test_pred))\n\nprint('Confussion matrix') \nmatrix = confusion_matrix(Test_Y, y_test_pred)\ndf_cm = pd.DataFrame(matrix, columns=np.unique(Test_Y), index = np.unique(Test_Y))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsn.set(font_scale=1.4) \nsn.heatmap(df_cm, cmap=\"PuRd\", annot=True,annot_kws={\"size\": 16})\nTN = matrix[0][0]\nFP = matrix[0][1]\nFN = matrix[1][0]\nTP = matrix[1][1]\n\n# Prediction metrics\nupsampling_accuracy = accuracy_score(Test_Y, y_test_pred)\nupsampling_imbaccuracy = balanced_accuracy_score(Test_Y, y_test_pred)\nupsampling_precision = precision_score(Test_Y, y_test_pred)\nupsampling_recall = recall_score(Test_Y, y_test_pred)\nupsampling_f1 = f1_score(Test_Y, y_test_pred)\nupsampling_auc = roc_auc_score(Test_Y, y_scores)\nupsampling_especifidad = TN \/ (TN + FP)\n","c575ca0e":"print('\\t\\t Upsampling\\n')\nprint(\"Accuracy:\\t\", \"{:0.10f}\".format(upsampling_accuracy))\nprint(\"Precision:\\t\", \"{:0.10f}\".format(upsampling_precision))\nprint(\"Recall:\\t\\t\", \"{:0.10f}\".format(upsampling_recall))\nprint(\"Specificity:\\t\", \"{:0.10f}\".format(upsampling_especifidad))\nprint(\"\\nF1 Score:\\t\", \"{:0.10f}\".format(upsampling_f1))\nprint(\"ROC AUC:\\t\", \"{:0.10f}\".format(upsampling_auc))\nprint(\"Balanced\\nAccuracy:\\t\", \"{:0.10f}\".format(upsampling_imbaccuracy))\nprint(\"Training time:\\t\", \"{:0.2f}s\".format(upsampling_elapsed))","c5b545b1":"This class is created in order to speed up and enhance our preprocess flow, providing the opportunity of trying the combination of techniques desired in a fast and easy way.","54c2e414":"###\u00a0Functions","af4273fa":"This test (with this specific preprocessing flow and this concrete MLP arquitecture) is part of a bigger group of tests carried out as a personal investigation for my bachelor's dissertation. Before reaching these metrics, the data has been explored, its problematics have been studied and a lot of different methods and combinations (both regarding preprocessing and model architecture) proposed to mitigate those problems have been tested.\n\nIn this case, the original data is preprocessed with the following techniques:\n- First, the function reduce_upper_outliers (https:\/\/www.kaggle.com\/muonneutrino\/exoplanet-data-visualization-and-exploration) is applied to reduce the 2% of the upper outliers that produce noise in the signals.\n- Second, the Fast Fourier Transform (FFT) is applied to our data in order to get the signals in the frequency domain.\n- After that, a gaussian filter with sigma=10 is applied. It is correct that a smoothing filter like this could be accounted as unnecessary because it could lead to information loss. It is also accurate that maybe other types of smoothing filters would be considered as a better option to reduce noise in the spacial domain, but the gaussian gilter provides a better performance for the treatment of frequency domain data because of its great skills to separate some frequencies from others.\n- Once we have our data filtered, we normalize (with normalize using l2 norm) and standardize (StandardScaler) it.\n- As a last step, we use an upsampling\/oversampling technique over the training set to fight the big imbalance presented in the data. Specifically, RandomOverSampler is used for this.\n","d26679b0":"# Model: Multilayer Perceptron\n\nArchitecture: \n- 1 hidden layer with 40 neurons.\n- 1 output layer with 2 neurons.\n- Activation function \"ReLU\" for the hidden layers, \"Sigmoid\" for the output layer\n- Optimizer: Adam\n- Dropout 0.5\n- Regularizer L2\n- Learning Rate 0.001\n- EarlyStopping\n","a6a6f64c":"### Imports","3cd304f4":"### Preprocess functions","25cf09f2":"### Preprocess class","9bb5125f":"#\u00a0Data Preprocessing","b5640d28":"### Training and prediction","1522bebf":"*Function reduce_upper_outliers was extracted from this notebook: https:\/\/www.kaggle.com\/muonneutrino\/exoplanet-data-visualization-and-exploration","c2351364":"If you have any opinions or comments, I would love to hear them!\n\nAlso, check out another notebook I uploaded, in which the results are the same, but the model used is CNN:\nhttps:\/\/www.kaggle.com\/danihinjos\/perfect-classification-with-cnn-100-metrics","13abefd7":"### Results","72a3c8be":"### Handling imbalanced data: upsampling\/oversampling with RandomOverSampler","85294058":"### Obtaining the data...","107aa3ae":"### Creation of the model"}}