{"cell_type":{"6b9db788":"code","659f49ce":"code","ab04655f":"code","1f3669a9":"code","57732646":"code","f84c6161":"code","a71c6f1a":"code","2cd6c621":"code","38b00639":"code","659488c7":"code","e1e070ed":"code","d01561ad":"code","edcd07a5":"code","a7886184":"code","22022bb9":"code","d8a13fef":"code","976a9a0c":"code","bebe99e3":"code","0c85999b":"code","164dfb97":"code","cbda4b80":"code","1e95c691":"code","e46894a8":"code","fba564d8":"code","16b8c0c4":"code","4bb86332":"code","34c3c178":"code","e8f6421f":"code","d4b5c448":"code","19bcee8c":"code","458ef7fa":"code","d59d2d79":"code","f9d9d49e":"code","794cdc4f":"code","b40fc4c6":"code","bb7de8ec":"code","b4379e97":"code","d0202be6":"code","83f7bfcd":"code","56841cb1":"code","3f5eed60":"code","8aeefd7f":"code","d3f47fb6":"code","22fd27b8":"code","720f0961":"code","667c82e8":"code","f6003725":"code","dac4d7e2":"code","f7bab80c":"markdown","6d1e4855":"markdown","efe21e23":"markdown","4ffe0bbd":"markdown","5cd2ef37":"markdown","d015cc74":"markdown","ee3901f3":"markdown","8ccbca39":"markdown","65784cb1":"markdown","5bd3a4c4":"markdown","a13ee553":"markdown","173d5786":"markdown"},"source":{"6b9db788":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","659f49ce":"import pandas as pd\nimport gzip\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')","ab04655f":"n = 40428967  #total number of records in the clickstream data \nsample_size = 1000000\nskip_values = sorted(random.sample(range(1,n), n-sample_size))\n\nparse_date = lambda val : pd.datetime.strptime(val, '%y%m%d%H')\nwith gzip.open('..\/input\/avazu-ctr-prediction\/train.gz') as f:\n    train = pd.read_csv(f,skiprows = skip_values)\ntrain['hour'] = pd.to_datetime(train['hour'],format = '%y%m%d%H')    ","1f3669a9":"train_data=train.copy()\ny_label=train_data.pop('click')\ntrain_data=train_data.drop(['hour','id'],axis=1)","57732646":"# Get categorical columns\ncategorical_cols = train_data.select_dtypes(\n  include = ['object']).columns.tolist()\nprint(\"Categorical columns: \")\nprint(categorical_cols)\n\n# Iterate over categorical columns and apply hash function\n","f84c6161":"for col in categorical_cols:\n\ttrain_data[col] = train_data[col].apply(lambda x: hash(x))","a71c6f1a":"train_data.head()","2cd6c621":"from sklearn.model_selection import train_test_split\n#X = train_data.drop(['id', 'device_id', 'device_ip', 'hour', 'click'], axis=1)\n#y = train_data.click\nx_train,x_test,y_train,y_test = train_test_split(train_data,y_label,test_size = 0.2,random_state=21)","38b00639":"from sklearn.preprocessing import StandardScaler\nsclr = StandardScaler()\nx_train.iloc[:,2:11] = sclr.fit_transform(x_train.iloc[:,2:11])\nx_test.iloc[:,2:11] = sclr.transform(x_test.iloc[:,2:11])","659488c7":"train_data['user_info']=train_data.device_ip+train_data.device_model+train_data.device_id\ntrain_data=train_data.drop(['device_id','device_ip','device_model'],axis=1)\ntrain_data['devtry']=train_data.device_type+train_data.banner_pos+train_data.device_conn_type\ntrain_data=train_data.drop(['banner_pos','device_conn_type','device_type'],axis=1)","e1e070ed":"import xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve,auc,confusion_matrix,precision_score,recall_score,roc_auc_score\n# intialize pca and logistic regression model\npca = PCA(n_components=2)\nparams = {\n    \"objective\": \"binary:logistic\",\n    \"booster\" : \"gbtree\",\n    \"eval_metric\": \"logloss\",\n    \"eta\":0.1,\n    \"max_depth\": 8,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"silent\": 1,\n}\nxgclf=xgb.XGBClassifier(**params)\n\n# fit and transform data\nX_train_pca = pca.fit_transform(x_train)\nX_test_pca = pca.transform(x_test)\nxgclf.fit(X_train_pca, y_train,\n        eval_set=[(X_test_pca, y_test)],\n        eval_metric='logloss',\n        verbose=False)\npca.explained_variance_ratio_\n","d01561ad":"from sklearn import metrics \nxgpred=xgclf.predict_proba(X_test_pca)\n#fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\nroc_auc_score = metrics.roc_auc_score(y_test,xgpred[:,1])\nprint(roc_auc_score)\nprint(xgclf.score(X_test_pca,y_test))","edcd07a5":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(train_data,y_label,discrete_features=True)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = train_data.columns\nmutual_info.sort_values(ascending=False)","a7886184":"import xgboost as xgb\nfrom sklearn.metrics import roc_curve,auc,confusion_matrix,precision_score,recall_score,roc_auc_score\nparams = {\n    \"objective\": \"binary:logistic\",\n    \"booster\" : \"gbtree\",\n    \"eval_metric\": \"logloss\",\n    \"eta\":0.1,\n    \"max_depth\": 8,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"silent\": 1,\n}\nxgclf=xgb.XGBClassifier(**params)\nxgclf.fit(x_train, y_train,\n        eval_set=[(x_test, y_test)],\n        eval_metric='logloss',\n        verbose=False)\nxgpred=xgclf.predict_proba(x_test)","22022bb9":"from sklearn import metrics \n#y_score = xgclf.predict_proba(x_test)\n#fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\nroc_auc_score = metrics.roc_auc_score(y_test,xgpred[:,1])\nprint(roc_auc_score)\nprint(xgclf.score(x_test,y_test))","d8a13fef":"from sklearn.model_selection import KFold,cross_val_score\n# Set up k-fold\nk_fold = KFold(n_splits = 5)\n\n# Evaluate precision and recall for each fold\nprecision = cross_val_score(\n  xgclf, x_train, y_train, cv = k_fold, scoring = 'precision_weighted')\nrecall = cross_val_score(\n  xgclf, x_train, y_train, cv = k_fold, scoring = 'recall_weighted')\nprint(\"Precision scores: %s\" %(precision)) \nprint(\"Recall scores: %s\" %(recall))\nprint(k_fold)","976a9a0c":"import seaborn as sns\nimport matplotlib.pyplot as plt","bebe99e3":"print(train.columns)\nprint(train.dtypes)","0c85999b":"print(train.info())\nprint(train.isnull().sum().sum())","164dfb97":"train.drop('id',axis=1,inplace = True)","cbda4b80":"train['day_week'] = train['hour'].apply(lambda x: x.dayofweek)\ntrain['hour_day'] = train['hour'].apply(lambda x: x.hour)","1e95c691":"train.groupby(['day_week','click']).size().unstack().plot(kind='bar')\nplt.title('Days of the week')\ntrain.groupby(['hour_day','click']).size().unstack().plot(kind='bar')\nplt.title('Hours of a day')","e46894a8":"train.groupby('hour_day')['click'].sum().plot.bar(figsize=(12,6))\nplt.ylabel('Number of clicks')\nplt.title('Number of clicks by hour of day')\nplt.show()","fba564d8":"sns.countplot(x='click',data=train)\nplt.show()\nprint(train.click.value_counts()\/len(train))","16b8c0c4":"train.groupby(['banner_pos', 'click']).size().unstack().plot(kind='barh')\ntrain.groupby(['banner_pos','click']).size().unstack().iloc[2:,:].plot(kind='barh')","4bb86332":"site_features = ['site_id', 'site_domain', 'site_category']\ntrain[site_features].describe()","34c3c178":"# Get categorical columns\ncategorical_cols = train.select_dtypes(\n  include = ['object']).columns.tolist()\nprint(\"Categorical columns: \")\nprint(categorical_cols)\n\n# Iterate over categorical columns and apply hash function\nfor col in categorical_cols:\n\ttrain[col] = train[col].apply(lambda x: hash(x))","e8f6421f":"ad_corr = train.corr(method = 'pearson')\nfig,ax = plt.subplots(figsize = (20,10))\nsns.heatmap(ad_corr,\n            xticklabels=ad_corr.columns,\n            yticklabels=ad_corr.columns,\n            cmap='RdBu_r',\n            annot=True,\n            linewidth=1,ax=ax)","d4b5c448":"from sklearn.model_selection import train_test_split\nX = train.drop(['device_id','device_ip','hour','click'],axis=1)\ny = train.click\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=21)","19bcee8c":"x_train.head()","458ef7fa":"from sklearn.preprocessing import StandardScaler\nsclr = StandardScaler()\nx_train.iloc[:,8:15] = sclr.fit_transform(x_train.iloc[:,8:15])\nx_test.iloc[:,8:15] = sclr.transform(x_test.iloc[:,8:15])","d59d2d79":"from sklearn.tree import DecisionTreeClassifier","f9d9d49e":"clf = DecisionTreeClassifier(max_depth=10)\nclf.fit(x_train,y_train)","794cdc4f":"print('Train Score:',clf.score(x_train,y_train))\nprint('Test Score:',clf.score(x_test,y_test))","b40fc4c6":"from sklearn.metrics import roc_curve,auc,confusion_matrix,precision_score,recall_score,roc_auc_score\ny_score = clf.predict_proba(x_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\nroc_auc_score = roc_auc_score(y_test,y_score[:,1])\nprint(roc_auc_score)","bb7de8ec":"random_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","b4379e97":"plt.style.use('seaborn')\nplt.plot(fpr,tpr,linestyle = '--',color = 'green',label='Decision Tree')\nplt.plot(p_fpr,p_tpr,linestyle='--',color = 'blue')\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('AUC-ROC Curve')","d0202be6":"y_pred = clf.predict(x_test)\nprecision = precision_score(y_test, y_pred, average = 'weighted')\nrecall = recall_score(y_test, y_pred, average = 'weighted')\nprint(\"Precision: %s, Recall: %s\" %(precision, recall))","83f7bfcd":"matrix = confusion_matrix(y_test,y_pred)\ntn, fp, fn, tp = matrix.ravel()\nprint(matrix)","56841cb1":"#For example:\n#Here total_return and cost values are examples\ntotal_return = 0.2\ncost = 0.05\ntotal_return = tp * total_return\ntotal_cost = (tp + fp) * cost \nroi = total_return \/ total_cost\nprint(\"Total return: %s, Total cost: %s, ROI: %s\" %(\n  total_return, total_cost, roi))","3f5eed60":"for max_depth_val in [2, 3, 5, 10, 15, 20]:\n    clf = DecisionTreeClassifier(max_depth = max_depth_val)\n    print(\"Evaluating tree with max_depth = %s\" %(max_depth_val))\n    y_pred = clf.fit(x_train,y_train).predict(x_test) \n    print(\"Confusion matrix: \")\n    print(confusion_matrix(y_test, y_pred))\n    prec = precision_score(y_test, y_pred, average = 'weighted')\n    recall = recall_score(y_test, y_pred, average = 'weighted')\n    print(\"Precision: %s, Recall: %s\" %(prec, recall))","8aeefd7f":"from sklearn.model_selection import KFold,cross_val_score\n# Set up k-fold\nk_fold = KFold(n_splits = 5)\n\n# Evaluate precision and recall for each fold\nprecision = cross_val_score(\n  clf, x_train, y_train, cv = k_fold, scoring = 'precision_weighted')\nrecall = cross_val_score(\n  clf, x_train, y_train, cv = k_fold, scoring = 'recall_weighted')\nprint(\"Precision scores: %s\" %(precision)) \nprint(\"Recall scores: %s\" %(recall))\nprint(k_fold)","d3f47fb6":"for max_depth_val in [3, 5, 10]:\n    k_fold = KFold(n_splits = 4)\n    clf = DecisionTreeClassifier(max_depth = max_depth_val)\n    print(\"Evaluating Decision Tree for max_depth = %s\" %(max_depth_val))\n    y_pred = clf.fit(x_train, y_train).predict(x_test) \n  \n    cv_precision = cross_val_score(clf, x_train, y_train, cv = k_fold, scoring = 'recall_weighted')\n    precision = recall_score(y_test, y_pred, average = 'weighted')\n    print(\"Cross validation Recall: %s\" %(cv_precision))\n    print(\"Test Recall: %s\" %(precision))\n  ","22fd27b8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [10], \n    'n_estimators':[2,5,10,20,50],\n    'min_samples_split': [2, 3, 4],\n    'max_features' : ['log2']\n}\n\nrf = RandomForestClassifier(random_state=42)\n\nrf_search = RandomizedSearchCV(estimator = rf, param_distributions=param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2, n_iter = 10)\n\nmodel = rf_search.fit(x_train,y_train)","720f0961":"rf_search.best_params_","667c82e8":"clf_rf = RandomForestClassifier(n_estimators= 50,\n min_samples_split= 2,\n max_features= 'log2',\n max_depth= 10,\n bootstrap= True,\n random_state=42)\nclf_rf.fit(x_train,y_train)","f6003725":"y_test_pred = clf_rf.predict(x_test)\n\nprint(confusion_matrix(y_test, y_test_pred))\nprec = precision_score(y_test, y_test_pred, average = 'weighted')\nrecall = recall_score(y_test, y_test_pred, average = 'weighted')\nprint(\"Precision: %s, Recall: %s\" %(prec, recall))","dac4d7e2":"from sklearn.tree import export_graphviz\nfrom six import StringIO\nimport pydot as pydot\nimport graphviz\nfrom IPython.display import Image\n\ndot_data = export_graphviz(clf,out_file=None,filled=True,rounded=True,special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph\n","f7bab80c":"## Click","6d1e4855":"## Feature Selection","efe21e23":"Nearly 17% is the CTR of the data provided.","4ffe0bbd":"## Timestamp - Hour","5cd2ef37":"## id","d015cc74":"We can remove the features which are irrelavant to the data, and not useful.","ee3901f3":"There are no missing values in the dataset.","8ccbca39":"# Exploratory Data Analysis\n","65784cb1":"## Banner Position","5bd3a4c4":"## Hashing\n","a13ee553":"## For missing values","173d5786":"## Model"}}