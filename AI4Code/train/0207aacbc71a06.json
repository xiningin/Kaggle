{"cell_type":{"1ca953f6":"code","527f1b3d":"code","c8703a13":"code","2cad4ee2":"code","088c7d83":"code","cd790d15":"code","bed74423":"code","70377a81":"code","ce4e8c6d":"code","72ae1bcd":"code","d80bd3d8":"code","df8d3dca":"code","da697241":"code","b21ee718":"code","8cad98ff":"code","d8edd85a":"code","029feee1":"code","99e1b6a1":"code","9c7d620c":"code","9b97f2cc":"code","8a05bf83":"code","32e50c9d":"code","0cbbc221":"code","b581d5ae":"code","e461aa3a":"code","c12fab7e":"code","00815e37":"code","9f133744":"code","54bd929f":"code","7255df93":"code","b86f5ddd":"code","8d7c89dd":"code","d7bb8f5c":"code","c7404f9d":"code","a8993dcc":"code","3df377f3":"code","2c8bee51":"code","71257419":"code","625e96d0":"code","89225436":"code","88d4bc8d":"code","8b6f45e4":"code","33d4f68b":"code","92765574":"code","bcbed548":"code","75357d12":"code","6a734795":"code","3c66212a":"code","968c84be":"code","ff5c352a":"code","98ce7b24":"code","c36f407f":"code","7b7c105a":"code","ae0aba08":"code","cedebfe1":"code","b4c3bc2c":"code","de251e2f":"code","28f05c80":"code","19395a55":"code","8b487272":"code","9966e622":"code","682d37a3":"code","18080428":"code","e09a5e47":"code","2b766778":"code","cfbeaf0f":"code","eebe1459":"code","c64bc182":"code","e03b22b9":"code","ab22f1e3":"code","8fc32a79":"code","8eb8169d":"code","99e3c100":"code","4da0016f":"code","50e41393":"code","b9fd6d57":"code","6ec6cf4c":"code","b69bafd9":"code","cf840676":"code","e5027902":"code","5b4b009b":"code","949c9304":"code","00eaae57":"code","53ed7d3c":"code","fd156bca":"code","01735d45":"code","0da4b95e":"code","fa29c419":"code","cd0dcf73":"code","8cd12dda":"code","0b6d4280":"code","8db19380":"code","8e7e22e5":"code","c3b00508":"code","f764fdf6":"code","fade43b4":"code","93e0eba8":"code","2a4cfafa":"code","8dd7f374":"markdown","abac6b42":"markdown","b8310344":"markdown","b4f9bc78":"markdown","71544a04":"markdown","584f50ca":"markdown","f0290fbb":"markdown","c9c5c138":"markdown","6ed52bfe":"markdown","0be71134":"markdown","878f7fe9":"markdown","560ac413":"markdown","66a48014":"markdown","384b9c4e":"markdown","a0db638a":"markdown","90ca26e2":"markdown","5ffebbd3":"markdown","15c5b3f8":"markdown","362f13ad":"markdown","e18d8003":"markdown","075a0b68":"markdown","f35676f2":"markdown","92b04fe6":"markdown","f225ebe4":"markdown","36ef0b1a":"markdown","6fc2ee1e":"markdown","4e3cd446":"markdown","52e22391":"markdown","9512eb7d":"markdown","478074c0":"markdown","3021ced7":"markdown","7b30aac0":"markdown","eae7925b":"markdown","fc68b315":"markdown","66a4a3b4":"markdown","e0babfe2":"markdown","b73048b6":"markdown","cacc22e6":"markdown","30882a74":"markdown","8f2d037a":"markdown","e46653b0":"markdown","8b72c23d":"markdown","f2057cfb":"markdown","801de0f2":"markdown","e5a9178f":"markdown","50c61c0e":"markdown","eff57665":"markdown","8d65a18d":"markdown","b6e6aee6":"markdown","5d6b55fd":"markdown","4b9f6094":"markdown","ae8081cf":"markdown","7e82546d":"markdown","28f5b818":"markdown","4adf5387":"markdown","5beaa991":"markdown","5ae3c8ec":"markdown"},"source":{"1ca953f6":"#import Useful Library\nimport pandas as pd\nimport numpy as np\n\n#for making graph\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#for warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","527f1b3d":"df = pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndf.head()","c8703a13":"df.info()","2cad4ee2":"df.Purchased.value_counts()","088c7d83":"df.Gender.value_counts()","cd790d15":"sns.boxplot(y='Age', x='Purchased', data=df)","bed74423":"sns.boxplot(y='EstimatedSalary', x='Purchased', data=df)","70377a81":"plt.hist(x=\"Purchased\", data=df);\nplt.title('Distribution of Purchase');\nplt.ylabel('Count');\nplt.xlabel('Purchase');","ce4e8c6d":"plt.figure(figsize=(20,5))\nbins_size = np.arange(15000,150000+10000,1000)\nplt.hist(x=\"EstimatedSalary\", data=df, bins= bins_size,rwidth=0.9);\nplt.title('Distribution of Salary');\nplt.ylabel('Count');\nplt.xlabel('Salary');","72ae1bcd":"df.EstimatedSalary.mean()","d80bd3d8":"plt.figure(figsize=(15,5))\nbins_size = np.arange(18,65,2)\nplt.hist(x=\"Age\", data=df, bins= bins_size,rwidth=0.9);\nplt.title('Distribution of Age');\nplt.ylabel('Count');\nplt.xlabel('Age');","df8d3dca":"sns.distplot(df.Purchased);","da697241":"df.describe()","b21ee718":"df.Age.unique()","8cad98ff":"df.Age.nunique()","d8edd85a":"df.EstimatedSalary.unique()","029feee1":"df.EstimatedSalary.nunique()","99e1b6a1":"df.Purchased.unique()","9c7d620c":"df.duplicated().sum()","9b97f2cc":"plt.figure(figsize =(8,8))\nax= sns.heatmap(df.corr(),square = True, annot = True,cmap= 'Spectral' )\nax.set_ylim(4.0, 0)","8a05bf83":"ax = sns.regplot(x=\"EstimatedSalary\", y=\"Purchased\", data=df)","32e50c9d":"ax = sns.regplot(x=\"Age\", y=\"Purchased\", data=df)","0cbbc221":"col = sns.color_palette()[0]","b581d5ae":"sns.barplot(x=\"Purchased\", y=\"EstimatedSalary\", data=df, color=col)","e461aa3a":"sns.barplot(x=\"Purchased\", y=\"EstimatedSalary\",hue='Gender', data=df)","c12fab7e":"sns.pairplot(df, vars=[\"Age\", \"EstimatedSalary\",\"Purchased\"])","00815e37":"sns.pairplot(df, vars=[\"Age\", \"EstimatedSalary\",\"Purchased\"], hue = \"Gender\")","9f133744":"df.drop_duplicates(inplace=True)","54bd929f":"df.drop(columns=['User ID'], inplace = True)","7255df93":"df.loc[((df.Age >58) & (df.Purchased==0)), 'Age'] = np.nan\ndf.fillna(53,inplace=True)","b86f5ddd":"df.loc[(df.EstimatedSalary>120000) & (df.Purchased==0), 'EstimatedSalary'] = np.nan\ndf.fillna(120000,inplace=True)","8d7c89dd":"from scipy import stats\nz = np.abs(stats.zscore(df['EstimatedSalary']))\nthreshold = 3\nprint(np.where(z > 3))","d7bb8f5c":"z = np.abs(stats.zscore(df['Age']))\nprint(np.where(z > 3))","c7404f9d":"df.Age = df.Age.astype(\"int64\")","a8993dcc":"df.EstimatedSalary = df.EstimatedSalary.astype(\"int64\")","3df377f3":"df.info()","2c8bee51":"a = df.groupby(['Gender', 'Age'])\na.first()","71257419":"a = df.groupby(['Purchased','EstimatedSalary'])\na.first()","625e96d0":"sns.scatterplot(y=\"EstimatedSalary\", x=\"Purchased\", data=df)","89225436":"plt.figure(figsize = (15,8))\nsns.scatterplot(y=\"EstimatedSalary\", x=\"Age\", data=df, hue = 'Purchased')","88d4bc8d":"sns.catplot(y=\"EstimatedSalary\", x=\"Purchased\", data=df, hue = 'Gender')","8b6f45e4":"sns.catplot(y=\"Age\", x=\"Purchased\", data=df, hue = 'Gender')","33d4f68b":"df.Gender.replace({'Male':1,\n                   'Female':0}, inplace=True)","92765574":"df.head()","bcbed548":"X = df.iloc[:, [1, 2]]\ny = df.iloc[:, 3]","75357d12":"X","6a734795":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","3c66212a":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","968c84be":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)","ff5c352a":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,knn_pred))\nprint(classification_report(y_test,knn_pred))","98ce7b24":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","c36f407f":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","7b7c105a":"knn = KNeighborsClassifier(n_neighbors=4)\n\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\n\nprint('WITH K=4')\nprint('\\n')\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","ae0aba08":"from sklearn.metrics import accuracy_score\nprint ('accuracy_score : ', accuracy_score(y_test,knn_pred))","cedebfe1":"from sklearn.model_selection import cross_val_score\nknn_accuracy = cross_val_score(knn,X,y, cv = 5)","b4c3bc2c":"knn_accuracy","de251e2f":"knn_accuracy.mean()","28f05c80":"from sklearn.metrics import roc_curve, auc\n\nknn_fpr, knn_tpr, threshold = roc_curve(y_test, knn_pred)\nauc_knn = auc(knn_fpr, knn_tpr)\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot(knn_fpr, knn_tpr, marker='.', label='Knn (auc = %0.3f)' % auc_knn)\n\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\n\nplt.legend()\n\nplt.show()","19395a55":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_pred = log_reg.predict(X_test)","8b487272":"print(confusion_matrix(y_test,log_pred))\nprint('\\n')\nprint(classification_report(y_test,log_pred))","9966e622":"log_accuracy = cross_val_score(log_reg,X,y, cv = 5)\nprint(log_accuracy)\nprint(\"mean value of accuracy\",log_accuracy.mean())","682d37a3":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'rbf', random_state = 0)\nsvc_classifier.fit(X_train, y_train)\nsvc_pred = svc_classifier.predict(X_test)","18080428":"print(confusion_matrix(y_test,svc_pred))\nprint('\\n')\nprint(classification_report(y_test,svc_pred))","e09a5e47":"svc_accuracy = cross_val_score(svc_classifier,X,y, cv = 5)\nprint(svc_accuracy)\nprint(\"mean value of accuracy\",svc_accuracy.mean())","2b766778":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","cfbeaf0f":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","eebe1459":"dt_accuracy = cross_val_score(dt_classifier,X,y, cv = 5)\nprint(dt_accuracy)\nprint(\"mean value of accuracy\",dt_accuracy.mean())","c64bc182":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(X_train, y_train)\nrf_pred = rf_classifier.predict(X_test)","e03b22b9":"print(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","ab22f1e3":"rf_accuracy = cross_val_score(rf_classifier,X,y, cv = 5)\nprint(rf_accuracy)\nprint(\"mean value of accuracy\",rf_accuracy.mean())","8fc32a79":"print(\"For Random Forest Classifier::\")\nprint(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","8eb8169d":"#Applying grid search\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{\"C\": [1, 10, 100, 1000], \"kernel\": ['linear']}, \n              {\"C\": [1, 10, 100, 1000], \"kernel\": ['rbf'], 'gamma': [0.5, 0.1, 0.01, 0.001]}]\n\n#Use this list to train\ngrid_search = GridSearchCV(estimator = svc_classifier, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\n\n#Use attributes of grid_search to get the results\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(\"Best accuracy: \",best_accuracy)\nprint(best_parameters)","99e3c100":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'rbf', random_state = 0, C =10, gamma=0.1)\nsvc_classifier.fit(X_train, y_train)\nsvc_pred = svc_classifier.predict(X_test)","4da0016f":"print(\"For SVM Classifier::\")\nprint(confusion_matrix(y_test,svc_pred))\nprint('\\n')\nprint(classification_report(y_test,svc_pred))","50e41393":"n_estimators = [100, 300, 500]\nmax_depth = [5, 8, 15]\nmin_samples_leaf = [1, 2] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(rf_classifier, hyperF, cv = 5, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","b9fd6d57":"bestF.best_estimator_","6ec6cf4c":"bestF.best_params_","b69bafd9":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0, max_depth=5,min_samples_leaf=1)\nrf_classifier.fit(X_train, y_train)\nrf_pred = rf_classifier.predict(X_test)","cf840676":"print(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","e5027902":"criterion = ['gini', 'entropy']\nmax_depth = [4,6,8,12]\n\nparameters = dict(criterion=criterion,max_depth=max_depth)\n\n  \nclf = GridSearchCV(rf_classifier, hyperF, cv = 5, verbose = 1, n_jobs = -1)\n\n# Fit the grid search\nclf.fit(X_train, y_train)","5b4b009b":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=5)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","949c9304":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","00eaae57":"param_grid = [    \n    {'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\nclf = GridSearchCV(log_reg, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\nbest_clf = clf.fit(X_train, y_train)","53ed7d3c":"a = best_clf.best_estimator_\na","fd156bca":"best_clf.best_params_","01735d45":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(C=1.0, solver='lbfgs',max_iter=100 )\nlog_reg.fit(X_train,y_train)\nlog_pred = log_reg.predict(X_test)","0da4b95e":"print(confusion_matrix(y_test,log_pred))\nprint('\\n')\nprint(classification_report(y_test,log_pred))","fa29c419":"#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,10))\nn_neighbors = list(range(1,10))\np=[1,2]\n\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n\n#Use GridSearch\nclf = GridSearchCV(knn, hyperparameters, cv=10, verbose=True, n_jobs=-1)\n#Fit the model\nbest_model = clf.fit(X_train,y_train)","cd0dcf73":"best_model.best_estimator_","8cd12dda":"best_model.best_params_","0b6d4280":"knn = KNeighborsClassifier(n_neighbors=9, p = 1, leaf_size=1)\n\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","8db19380":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","8e7e22e5":"df.head()","c3b00508":"X1 = df.iloc[:,0: 3]\ny1 = df.iloc[:, 3]","f764fdf6":"X1","fade43b4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.30, random_state = 0)","93e0eba8":"dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=5)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","2a4cfafa":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","8dd7f374":"- The chart is better explain the Distribution of Each Income level, Interesting there are customers in the mall with a very much comparable frequency with their Annual Income ranging from approx 75k US Dollars and 80K US Dollars. The average salary of the customers is 69742.5.","abac6b42":"## 7. Add your view of EDA to enhance understanding of data. i.e., Grouping data and observing the way data is distributed. Try to add as many layers of EDA as possible.","b8310344":"### Choosing a K Value","b4f9bc78":"## 11. Run hyperparameter tuning on all the models and pick the best parameters (A minimum of 2 Parameters should be tuned) and picked.","71544a04":"### d. Aggregation for all numerical Columns","584f50ca":"## 13. Try to Predict the target with maximum independent features.","f0290fbb":"- By this histogram we have clear idea that by the social media ads most of the are not purchased the product. In this notebook We will deep dive into the data and try to find the reason and explore it further. ","c9c5c138":"## 10. Compare the error and pick the ideal one with least errors.","6ed52bfe":"# Sales Prediction based on Social Media Ads","0be71134":"## Decision Tree Classifier","878f7fe9":"### f. Duplicate values across all columns","560ac413":"### i. Bar Plot","66a48014":"## Introduction\n> Using the fictional dataset of Gender, Age, Salary, Purchased (Target variable), the company wants to know whether a customer will buy its product or not.","384b9c4e":"### h. Regression Plot","a0db638a":"### Using KNN","90ca26e2":"## 3. Drop all non-essential features","5ffebbd3":"## K fold Cross Validation\n\n> This technique is useful to evaluate bias and variance more accurately. It splits the training set into k groups, and in each iteration, the algorithm chooses different test fold (individual section) for testing. This allows every part of the training set to be used for testing.","15c5b3f8":"- By looking at the above graph-, It can be seen that the Ages from 27 to 42 are very much frequent but there is no clear pattern, we can only find some group wise patterns such as the the older age groups are lesser frequent in comparison of youngsters.","362f13ad":"## Random Forest Classification","e18d8003":"## 4. Replace outliers with Nulls (if you find it essential) and replace all the nulls with respective approach of central tendencies (Mean\/Median\/Mode).","075a0b68":"## KNN","f35676f2":"## 2. Drop all duplicate rows","92b04fe6":"## Random Forest Classification","f225ebe4":"## K- Fold CV","36ef0b1a":"> By seeing the confusion matrix and accuracy score `Random Forest classifier` perform batter than others.","6fc2ee1e":"### e. Unique Values across all columns","4e3cd446":"## 12. Now, compare the models and pick the ideal one.","52e22391":"## Logistic Regression","9512eb7d":"## K- Fold CV","478074c0":"### By the hyperparameter tuning `Decision Tree` prefome best in above model ","3021ced7":"### b. Histogram \u2013 Distribution of Target Variable","7b30aac0":"- By this bar plot we can take the idea that the person who purchased the product have more salary then the average.","eae7925b":"## SVM","fc68b315":"## Logistic Regression","66a4a3b4":"### c. Distribution Plot \u2013 Target Variable","e0babfe2":"## Standardize the Variables\n- Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.","b73048b6":"## 6. Clean the data with formatting issues if any. (converting datatypes, replacing dollars, etc.)","cacc22e6":"## 8. Build a model of choice \u2013 Classification problem statement, hence build a classification model first and calculate Confusion Matrix, AUC, F1 Score, Precision, Recall and Accuracy.","30882a74":"### Train Test Split","8f2d037a":"- The Above Graph for Showing the linear correlation between the different attributes of the Mall Customer Segementation Dataset, This Heat map reflects the most correlated features with Blue Color and least correlated features with Red color.We can clearly see that only age has linearly related to the purchased. ","e46653b0":"## 5. Calculate Z score to validate whether outliers are still present or not.","8b72c23d":"- females have higher average salary.\n- females purchased more","f2057cfb":"## K- Fold CV","801de0f2":"### Predictions and Evaluations","e5a9178f":"## Steps\n- Data wrangling, which consists of:\n    - Gathering data\n    - Assessing data\n    - Cleaning data\n- Storing, analyzing, and visualizing our wrangled data\n- Making Report on\n    - Aboout data wrangling efforts and\n    - About data analyses and visualizations","50c61c0e":"## Decision Tree Classifier","eff57665":"## 9. Build at least a minimum of 4 different Classification models. All the models should use K-Fold cross Validation to train the model with at least 5-fold cross validation.","8d65a18d":"## HImanshu Sharma (DSOCT03)","b6e6aee6":"## Approach - Supervised Machine Learning (Classification)\n\n### Supervised Machine Learning:\n\n- The algorithms which learns from labeled data. After learning from the the data, the algorithm determines which label should be given to new data by their associating patterns to the unlabeled new data.\n\n- It can be divided into two categories: classification and regression. \n\n### Classification:\n\n- Classification is a technique which is useful to determining the class based on one or more independent variables.\n\n- I am going through the following classification algorithms in this notebook:\n\n> k-nearest neighbors\n\n> Logistic Regression\n\n> Decision Tree\n\n> Random Forest\n\n>Naive Bayes","5d6b55fd":"## KNN","4b9f6094":"### g. Correlation \u2013 Heatmap","ae8081cf":"### j. Pair plot","7e82546d":"## K- Fold CV","28f5b818":"## 1. Perform Basic EDA\n### a. Boxplot","4adf5387":"## Decision Tree","5beaa991":"Here we can see that that after arouns K>4 the error rate is decreasing and if we take those values that maked overfitting so we take k = 4. So Let's retrain the model with that and check the classification report!","5ae3c8ec":"## SVM"}}