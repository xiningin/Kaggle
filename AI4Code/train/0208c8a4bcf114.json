{"cell_type":{"fdaab06a":"code","b6f100dc":"code","869f9b64":"code","44408795":"code","99954110":"code","41263b6e":"code","500ac338":"code","999d8361":"code","69f2c2c3":"code","75d3d5f2":"code","b9cedd34":"code","ab1b8ae7":"code","1f175cc1":"code","a17364cc":"code","f8ee16bf":"code","e02f76d4":"code","cd3cf8c3":"code","f20cc5aa":"code","d61644d3":"code","4e15096f":"code","16d9a329":"code","9b268be8":"code","fb541a28":"code","5f294ce0":"code","5345f4ea":"code","7679158c":"code","11a548bf":"code","80e4703f":"code","4b1ee018":"code","09c95d20":"code","ade1999d":"code","e9727bd9":"code","e8b671cf":"code","12a49ed2":"code","c944ad93":"code","4e358af5":"code","ad2d0339":"code","788da45c":"code","23a33536":"code","6c93039d":"code","aede05a1":"code","e36b3f05":"code","a0635ea0":"code","a41a5f1f":"code","5fb6af07":"code","fe0445e7":"code","14799d43":"code","bb1214ad":"code","a60a31f9":"code","62aae7b1":"code","bd3737e6":"code","95376ce8":"code","962acdd8":"code","b012c39b":"code","1afc723e":"code","4a85cd40":"code","a16bd023":"code","7f4322ea":"code","11be6540":"code","40fc111c":"code","f2b2570c":"code","e49d564b":"code","b5ac9895":"code","aea7df4c":"code","65df5fed":"code","5132be93":"code","88f31e3a":"code","d6478c5f":"code","9b4dadfa":"code","a65ee772":"code","417ad7f5":"code","0ba8face":"code","0d6e895c":"code","3bc44259":"code","263af9d3":"code","2d788ee6":"code","002ebc8c":"code","c82f36b2":"markdown","3e7365cd":"markdown","7af07d8a":"markdown","7f6e6f37":"markdown","2183c05a":"markdown","e08b0879":"markdown","7e233582":"markdown","767631d1":"markdown","af729e75":"markdown","22bf58ae":"markdown","63869f59":"markdown","b3a158ab":"markdown","a375b5da":"markdown","7254a65b":"markdown","19c055d5":"markdown","e0d90443":"markdown","1ecf5769":"markdown","d9a32e26":"markdown","50270c1d":"markdown","fe11f59b":"markdown","a2d127d7":"markdown","467a2f3e":"markdown","305bb318":"markdown","4a91ae9e":"markdown","09f5e270":"markdown","faea9a6c":"markdown"},"source":{"fdaab06a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6f100dc":"#invite people for the Kaggle party\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nimport random\nfrom scipy import optimize\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nwarnings.filterwarnings('ignore')\n%matplotlib inline","869f9b64":"print('Dirname: %s' % dirname)\nprint('Filename: %s' % filename)\nprint('Some info: ')\ndf_train = pd.read_csv(dirname+ \"\/train.csv\")\nprint('Shape: %d %d' % df_train.shape)\nprint('Columns: %s' % df_train.columns)\nprint('Discribe SalePrice: \\n%s' % df_train['SalePrice'].describe())","44408795":"sns.displot(df_train['SalePrice'])","99954110":"print('Skewness: %f' % df_train['SalePrice'].skew())\nprint('Kurtosis: %f' % df_train['SalePrice'].kurt())","41263b6e":"def plotRelationship(fe_x, fe_y, onlyPositive = False):\n    if onlyPositive:\n        data = pd.concat([df_train[fe_y], df_train[df_train[fe_x]>0][fe_x]], axis = 1)\n        data.plot.scatter(x = fe_x, y = fe_y)\n    else:\n        data = pd.concat([df_train[fe_y], df_train[fe_x]], axis = 1)\n        data.plot.scatter(x = fe_x, y = fe_y)","500ac338":"plotRelationship('GrLivArea', 'SalePrice')\nplotRelationship('TotalBsmtSF', 'SalePrice')","999d8361":"data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis = 1)\nf, ax = plt.subplots(figsize=(8, 8))\nfig = sns.boxplot(x = 'OverallQual', y = 'SalePrice', data = data)\n","69f2c2c3":"data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y='SalePrice', data=data)\nplt.xticks(rotation=90);","75d3d5f2":"corr_matrix = df_train.corr()\n#print(corr_matrix)\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_matrix)","b9cedd34":"num_cols = 10\ncols = corr_matrix.nlargest(num_cols, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].T)\nhm = sns.heatmap(cm, vmin = 0, vmax = 1.0, annot=True, square = True, fmt = '.2f', xticklabels = cols, yticklabels = cols)\n","ab1b8ae7":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols])","1f175cc1":"total = df_train.isnull().sum(axis = 0).sort_values(ascending = False)\npercent = (df_train.isnull().sum(axis = 0) \/ df_train.isnull().count(axis = 0)).sort_values(ascending = False)\nmissing = pd.concat([total, percent], axis = 1)\nprint(missing.head(20))\n","a17364cc":"df_train = df_train.drop(missing[total > 1].index, axis = 1)\nprint(df_train.shape)","f8ee16bf":"print(df_train.columns)","e02f76d4":"df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index, axis = 0)\nprint(df_train.shape)","cd3cf8c3":"print(df_train.isnull().sum(axis = 0).max())","f20cc5aa":"df_test = pd.read_csv(dirname+ \"\/test.csv\")\ntotal = df_test.isnull().sum(axis = 0).sort_values(ascending = False)\npercent = (df_test.isnull().sum(axis = 0) \/ df_test.isnull().count(axis = 0)).sort_values(ascending = False)\nmissing = pd.concat([total, percent], axis = 1)\nprint(missing.head(40))\ndf_test['TotalBsmtSF'] = df_test['TotalBsmtSF'].fillna(df_train['TotalBsmtSF'].median())\ndf_test['BsmtHalfBath'] = df_test['BsmtHalfBath'].fillna(df_train['BsmtHalfBath'].median())\ndf_test['BsmtFullBath'] = df_test['BsmtFullBath'].fillna(df_train['BsmtFullBath'].median())\ndf_test['BsmtFinSF1'] = df_test['BsmtFinSF1'].fillna(df_train['BsmtFinSF1'].median())\ndf_test['BsmtFinSF2'] = df_test['BsmtFinSF2'].fillna(df_train['BsmtFinSF2'].median())\ndf_test['BsmtUnfSF'] = df_test['BsmtUnfSF'].fillna(df_train['BsmtUnfSF'].median())\ndf_test['GarageArea'] = df_test['GarageArea'].fillna(df_train['BsmtUnfSF'].median())\ndf_test['GarageCars'] = df_test['GarageCars'].fillna(df_train['GarageCars'].mode()[0])\ndf_test['MSZoning'] = df_test['MSZoning'].fillna(df_train['MSZoning'].mode()[0])\ndf_test['Utilities'] = df_test['Utilities'].fillna(df_train['Utilities'].mode()[0])\ndf_test['Functional'] = df_test['Functional'].fillna(df_train['Functional'].mode()[0])\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_train['KitchenQual'].mode()[0])\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(df_train['Exterior2nd'].mode()[0])\ndf_test['Exterior1st'] = df_test['Exterior1st'].fillna(df_train['Exterior1st'].mode()[0])\ndf_test['SaleType'] = df_test['SaleType'].fillna(df_train['SaleType'].mode()[0])\n","d61644d3":"total = df_test.isnull().sum(axis = 0).sort_values(ascending = False)\npercent = (df_test.isnull().sum(axis = 0) \/ df_test.isnull().count(axis = 0)).sort_values(ascending = False)\nmissing = pd.concat([total, percent], axis = 1)\nprint(missing.head(40))","4e15096f":"print(df_test.shape)","16d9a329":"print(missing[total > 1].shape)","9b268be8":"df_test = df_test.drop(missing[total > 1].index, axis = 1)\nprint(df_test.shape)\n","fb541a28":"print(df_test.isnull().sum(axis = 0).max())","5f294ce0":"#standardizing data\nsaleprice_scale = StandardScaler().fit_transform(df_train['SalePrice'][:, np.newaxis])\nsort_idx = np.argsort(saleprice_scale[:, 0])\nlow_range = saleprice_scale[sort_idx][:10]\nprint(low_range)\nhigh_range = saleprice_scale[sort_idx][-10:]\nprint(high_range)","5345f4ea":"print('Relationship of GrLivArea & TotalBsmtSF with SalePrice: \\n')\nplotRelationship('GrLivArea', 'SalePrice')\nplotRelationship('TotalBsmtSF', 'SalePrice')","7679158c":"liars = df_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\nprint(liars)\nprint(liars.index)\ndf_train = df_train.drop(df_train.loc[liars.index].index, axis = 0)","11a548bf":"liars = df_train.sort_values(by = 'TotalBsmtSF', ascending = False)[:1]\nprint(liars)\nprint(liars.index)\ndf_train = df_train.drop(df_train.loc[liars.index].index, axis = 0)","80e4703f":"print(df_train.shape, df_test.shape)","4b1ee018":"def norProbPlot(fe_x, onlyPositive = False):\n    if onlyPositive:\n        sns.distplot(df_train[df_train[fe_x]>0][fe_x], fit=norm)\n    else:\n        sns.distplot(df_train[fe_x], fit=norm)\n    fig = plt.figure(figsize = (8, 6))\n    if onlyPositive:\n        stats.probplot(df_train[df_train[fe_x]>0][fe_x], plot = plt, dist = norm );\n    else: \n        stats.probplot(df_train[fe_x], plot = plt, dist = norm );","09c95d20":"print('SalePrice: \\n')\nnorProbPlot('SalePrice')\n# log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])\nprint('SalePrice after log transformation: \\n')\nnorProbPlot('SalePrice')","ade1999d":"print('GrLivArea: \\n')\nnorProbPlot('GrLivArea')\n# log transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])\nprint('GrLivArea after log transformation: \\n')\nnorProbPlot('GrLivArea')","e9727bd9":"df_test['GrLivArea'] = np.log(df_test['GrLivArea'])","e8b671cf":"#histogram and normal probability plot\nnorProbPlot('TotalBsmtSF')\n#log transformation value > 0\ndf_train.loc[df_train['TotalBsmtSF'] > 0,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])\nprint('TotalBsmtSF after log transformation: \\n')\nnorProbPlot('TotalBsmtSF', True)","12a49ed2":"df_test.loc[df_test['TotalBsmtSF'] > 0,'TotalBsmtSF'] = np.log(df_test['TotalBsmtSF'])","c944ad93":"print(df_test.shape)","4e358af5":"print('Relationship GrLiveArea & SalePrice: \\n')\nplotRelationship('GrLivArea', 'SalePrice')\nprint('Relationship TotalBsmtSF & SalePrice: \\n')\nplotRelationship(fe_x = 'TotalBsmtSF', fe_y = 'SalePrice', onlyPositive = True)","ad2d0339":"df_train['SalePrice'].describe()","788da45c":"df_train = pd.get_dummies(df_train)\nprint(df_train.columns)\n","23a33536":"df_test = pd.get_dummies(df_test)\nprint(df_test.columns)","6c93039d":"missing_cols = set(df_train.columns) - set(df_test.columns)\nprint(missing_cols)\nfor c in missing_cols:\n    df_test[c] = 0\n# ensure same order\ndf_test = df_test[df_train.columns]\nprint(df_test.columns)","aede05a1":"m, n = df_train.shape\nfor column_name in df_train.columns:\n    column = df_train[column_name]\n    # Get the count of Zeros in column \n    count = (column == 0).sum()\n    if (count > m*0.99):\n        df_train = df_train.drop(column_name, axis = 1)\n        df_test = df_test.drop(column_name, axis = 1)\n        print('Count of zeros in column ', column_name, ' is : ', count)","e36b3f05":"print(df_train.shape)","a0635ea0":"df_Y = df_train['SalePrice']\ndf_train = df_train.drop('SalePrice', axis = 1)\ndf_test = df_test.drop('SalePrice', axis = 1)\ndf_X = df_train","a41a5f1f":"Y = np.array(df_Y) \nX = np.array(df_X)\nprint(X.shape, Y.shape)","5fb6af07":"seed = 19\nnp.random.seed(seed)\nnp.random.shuffle(X)\nnp.random.seed(seed)\nnp.random.shuffle(Y)\n","fe0445e7":"m = X.shape[0]\nm_test = round(m\/5)\nm_train = m - m_test\nX_train = X[:m_train, :]\nX_test = X[m_train:, :]\nY_train = Y[:m_train]\nY_test  = Y[m_train:]\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n","14799d43":"def featureNormalize(X):\n    mu = np.mean(X, axis = 0)\n    sigma = np.std(X, axis = 0)\n    X_norm = (X - mu)\/sigma\n    return X_norm, mu, sigma","bb1214ad":"def MSE(y, y_p):\n    return np.linalg.norm(y - y_p)**2","a60a31f9":"def costFunctionReg(X, y, theta, lambda_ = 0):\n    m, n = X.shape\n    y_p = X.dot(theta)\n    mse = MSE(y, y_p)\n    J = 1\/(2.0*m)*mse + lambda_\/(2.0*m)*np.sum(np.square(theta[1:]))\n    grad = 1\/m*X.T.dot(y_p-y)\n    grad[1:] += lambda_\/m*theta[1:]\n    return J, grad","62aae7b1":"def trainLinearReg(linearRegCostFunction, X, y, lambda_=0.0, maxiter=500):\n    # Initialize Theta\n    initial_theta = np.zeros(X.shape[1])\n\n    # Create \"short hand\" for the cost function to be minimized\n    costFunction = lambda t: linearRegCostFunction(X, y, t, lambda_)\n\n    # Now, costFunction is a function that takes in only one argument\n    options = {'maxiter': maxiter}\n\n    # Minimize using scipy\n    res = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)\n    return res.x\n","bd3737e6":"def learningCurve(X, y, Xtest, ytest, lambda_=0):\n    # Number of training examples\n    m = y.size\n    mtest = ytest.size\n    # You need to return these values correctly\n    error_train = np.zeros(m)\n    error_test   = np.zeros(m)\n\n    for i in range(1, m+1):\n        theta = trainLinearReg(costFunctionReg, X[:i, ], y[:i, ], lambda_)\n        error_train[i-1] = 1.0\/(2*i)*np.linalg.norm(X[:i, ].dot(theta)-y[:i, ])**2\n        error_test[i-1] = 1.0\/(2*mtest)*np.linalg.norm(Xtest.dot(theta)-ytest)**2\n    return error_train, error_test","95376ce8":"X_norm, mu, sigma = featureNormalize(X_train)\nX_train = np.concatenate([np.ones((m_train, 1)), X_norm], axis = 1)","962acdd8":"X_test = (X_test - mu)\/sigma\nX_test = np.concatenate([np.ones((m_test, 1)), X_test], axis = 1)","b012c39b":"error_train, error_test = learningCurve(X_train, Y_train, X_test, Y_test, lambda_=0)","1afc723e":"plt.plot(np.arange(1, X_train.shape[0]+1), error_train, np.arange(1, X_train.shape[0]+1), error_test, lw=2)\nplt.title('Learning curve for linear regression')\nplt.legend(['Train', 'Cross Validation'])\nplt.xlabel('Number of training examples')\nplt.ylabel('Error')\nplt.axis([0, m_train, 0, 0.01])\nprint('\\t error_train: %f \\t error_test: %f' % (error_train[m_train-1], error_test[m_train-1]))","4a85cd40":"def validationCurve(X, y, Xval, yval):\n    # Selected values of lambda (you should not change this)\n    lambda_vec = [50, 100, 150, 200, 250, 300]\n    # You need to return these variables correctly.\n    error_train = np.zeros(len(lambda_vec))\n    error_val = np.zeros(len(lambda_vec))\n    for i in range(len(lambda_vec)):\n        theta = trainLinearReg(costFunctionReg, X, y, lambda_vec[i])\n        error_train[i] = costFunctionReg(X, y, theta)[0]\n        error_val[i] = costFunctionReg(Xval, yval, theta)[0]\n    return lambda_vec, error_train, error_val","a16bd023":"lambda_vec, error_train, error_val = validationCurve(X_train, Y_train, X_test, Y_test)\n\nplt.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\nplt.legend(['Train', 'Cross Validation'])\nplt.xlabel('lambda')\nplt.ylabel('Error')","7f4322ea":"# choose lambda = 100\nerror_train, error_test = learningCurve(X_train, Y_train, X_test, Y_test, lambda_=100)","11be6540":"plt.plot(np.arange(1, X_train.shape[0]+1), error_train, np.arange(1, X_train.shape[0]+1), error_test, lw=2)\nplt.title('Learning curve for linear regression')\nplt.legend(['Train', 'Cross Validation'])\nplt.xlabel('Number of training examples')\nplt.ylabel('Error')\nplt.axis([0, m_train, 0, 0.01])\nprint('  \\t%d\\t\\t%f\\t%f' % (m_train, error_train[m_train-1], error_test[m_train-1]))","40fc111c":"Theta = np.linalg.pinv((X_train.T.dot(X_train))).dot(X_train.T).dot(Y_train)\nprint(X_train.shape, Theta.shape)\ny_pred = X_train.dot(Theta)\nprint('MSE Train data: %f' % (1\/(2*m_train)*MSE(Y_train, y_pred)))\ny_pred = X_test.dot(Theta)\nprint('MSE Train data: %f' % (1\/(2*m_test)*MSE(Y_test, y_pred)))","f2b2570c":"from sklearn.linear_model import Lasso\nmodel = Lasso(alpha = 0.01)\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\nprint(1\/(2*m_test)*MSE(Y_test, y_pred))","e49d564b":"def createFeature(X):\n    m, n = X.shape\n    out = np.ones((m, 1))\n    out = np.concatenate([out, X], axis = 1)\n    for i in range(n):\n        for j in range(i, n):\n            append = X[:, i] * X[:, j]\n            if np.sum(append) > 0:\n                out = np.concatenate([out, append[:, np.newaxis]], axis = 1)\n    return out","b5ac9895":"all_data = createFeature(X)","aea7df4c":"m = all_data.shape[0]\nm_test = round(m\/5)\nm_train = m - m_test\nX_train = all_data[:m_train, :]\nX_test = all_data[m_train:, :]\nY_train = Y[:m_train]\nY_test  = Y[m_train:]\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","65df5fed":"X_train = np.concatenate([np.ones((m_train, 1)), X_train], axis = 1)\nX_test = np.concatenate([np.ones((m_test, 1)), X_test], axis = 1)","5132be93":"lambda_vec, error_train, error_val = validationCurve(X_train, Y_train, X_test, Y_test)\n\nplt.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\nplt.legend(['Train', 'Cross Validation'])\nplt.xlabel('lambda')\nplt.ylabel('Error')","88f31e3a":"initial_theta = np.zeros(X_train.shape[1])\n\n# Create \"short hand\" for the cost function to be minimized\ncostFunction = lambda t: costFunctionReg(X_train, Y_train, t, lambda_)\nlambda_ = 200\n# Now, costFunction is a function that takes in only one argument\noptions = {'maxiter': 500}\n\n# Minimize using scipy\nres = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)\nprint(res.fun)\nprint(costFunctionReg(X_test, Y_test, res.x)[0])","d6478c5f":"print(df_train.shape, df_test.shape)","9b4dadfa":"X_submit = np.array(df_test)\nm = X.shape[0]\nm_test = round(m\/5)\nm_train = m - m_test\nX_train = X[:m_train, :]\nX_test = X[m_train:, :]\nY_train = Y[:m_train]\nY_test  = Y[m_train:]\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nX_norm, mu, sigma = featureNormalize(X_train)\nX_train = np.concatenate([np.ones((m_train, 1)), X_norm], axis = 1)\n","a65ee772":"X_submit = (X_submit - mu)\/sigma\nX_submit = np.concatenate([np.ones((X_submit.shape[0], 1)), X_submit], axis = 1)\nprint(X_submit)","417ad7f5":"print(X_submit)\nprint(np.sum(np.isnan(X_submit)))","0ba8face":"print(X_train)","0d6e895c":"theta = trainLinearReg(costFunctionReg, X_train, Y_train, 100)\nprint(costFunctionReg(X_train, Y_train , theta)[0])","3bc44259":"output = X_submit.dot(theta)\nprint(output)","263af9d3":"submit_price = (np.exp(output))\nprint(np.min(submit_price))","2d788ee6":"submit = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': submit_price})\nprint(submit)","002ebc8c":"submit.to_csv('submission.csv',index=False)","c82f36b2":"Zoom heatmap style: ","3e7365cd":"# Linear Regression","7af07d8a":"Getting Hard Code ","7f6e6f37":"Relationship with categorial features: ","2183c05a":"# Polynomial ","e08b0879":"Out Liars! \nUnivariate analysis","7e233582":"OverallQual: ","767631d1":"Get Dummies: ","af729e75":"# Submit","22bf58ae":"Missing value:","63869f59":"#### Use ridge\n","b3a158ab":"Normality","a375b5da":"Relationship with numerics features: ","7254a65b":"Log transformation usually works well with positive skewness","19c055d5":"GrLivArea vs SalePrice: ","e0d90443":"YearBUild: ","1ecf5769":"# Ridge Regression","d9a32e26":"Heatmap: ","50270c1d":"Histogram: ","fe11f59b":"### Handle Test data","a2d127d7":"Remove some variables: ","467a2f3e":"Delete outliers: 2 GrLivArea & 1 TotalBsmtSF","305bb318":"Detele 1 observation of missing values 'Electrical'","4a91ae9e":"# Normal Equation","09f5e270":"Delete some variable missing lots of values and not important","faea9a6c":"# Lasso "}}