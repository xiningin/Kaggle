{"cell_type":{"f5fb487f":"code","ceb1c74a":"code","711ac25a":"code","6be4cb8f":"code","2c4c00a7":"code","3b60f02d":"code","00eb5bcd":"code","4f98e97e":"code","b890cf83":"code","f5f5da13":"code","f97f4983":"code","7b579fd7":"code","d05cfb82":"code","fea3a81a":"code","b451910d":"code","0e5b5723":"code","916b8cb3":"code","bdffcf80":"code","a896adca":"code","cfa0f9c4":"code","77458b7b":"code","f817e7fa":"code","12e7e34e":"code","2e1e7036":"code","1ad45a12":"code","0b70ca58":"code","7cde4a7d":"markdown","17c9c98a":"markdown","ff811662":"markdown","e5f26c54":"markdown","1c244987":"markdown","01722f8f":"markdown","bc182d22":"markdown","36c53ebe":"markdown","91822bbe":"markdown","ffc75a28":"markdown","805eae47":"markdown","665fb49d":"markdown","305dbf8a":"markdown","c20d6edf":"markdown","31e3cb1d":"markdown","c6781bff":"markdown","8f53cc84":"markdown","e5fa6347":"markdown","8bf70a17":"markdown","76ea1ee8":"markdown","d0eed496":"markdown","e0b749a2":"markdown","f470a81c":"markdown","4cbe68f1":"markdown","5a6c6f69":"markdown","fe6a7fdf":"markdown","0b3d2cc3":"markdown"},"source":{"f5fb487f":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","ceb1c74a":"sea.set_style(\"darkgrid\")","711ac25a":"data = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/\"\n                   \"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\ndata.head(10).style.set_precision(2). \\\n                    set_properties(**{\"min-width\": \"80px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])","6be4cb8f":"data.drop(\"customerID\", axis=1, inplace=True)","2c4c00a7":"# disable SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"Churn\"]\ndata_Y = data[[\"Churn\"]]\n\nprint(\"\\ndata_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","3b60f02d":"for c in data_X.columns:\n    \n    print(\"Feature name: {}\".format(c))\n    print(\"Unique values:\\n\")\n    print(data_X[c].unique())\n    print(\"\\n--------------------------------------------------\\n\")","00eb5bcd":"data_X[\"TotalCharges\"] = [s.replace(\" \",\"\")\n                          for s in data_X[\"TotalCharges\"]]\ndata_X[\"TotalCharges\"] = pd.to_numeric(data_X[\"TotalCharges\"])","4f98e97e":"data_X[\"TotalCharges\"].isnull().sum()","b890cf83":"data_X[\"TotalCharges\"].fillna(data_X[\"TotalCharges\"].mean(), inplace=True)","f5f5da13":"cat = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\",\n       \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n       \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n       \"StreamingTV\", \"StreamingMovies\", \"Contract\",\n       \"PaperlessBilling\", \"PaymentMethod\"]\n\nnum = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]","f97f4983":"enc = OneHotEncoder(drop=\"first\")\nenc.fit(data_X[cat]);\n\ncat2 = enc.get_feature_names(cat)\ndata_X_C = pd.DataFrame(enc.transform(data_X[cat]).toarray(),\n                        columns = cat2)","7b579fd7":"data_X = pd.concat([data_X_C, data_X[num]], axis=1)","d05cfb82":"feature_names = data_X.columns\nprint(feature_names)","fea3a81a":"data_Y[\"Churn\"].unique()","b451910d":"lb = LabelBinarizer()\n\nlb.fit(data_Y[\"Churn\"]);\ndata_Y[\"Churn\"] = lb.transform(data_Y[\"Churn\"])","0e5b5723":"train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    shuffle = True,\n                                                    stratify=data_Y,\n                                                    random_state=0)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);","916b8cb3":"fig, axes = plt.subplots(1, 3, figsize=(10,6))\nfor i, c in enumerate(train_X[num]):\n    sea.boxplot(train_X[c], orient=\"v\", color = \"#6f7501\",\n                                width = 0.2, ax=axes[i])\n    \nfig.tight_layout(pad=3.0)","bdffcf80":"fig = plt.figure(figsize=(10,66))\ngs = gridspec.GridSpec(nrows=19, ncols=2, figure=fig)\n\nfor i, c in enumerate(train_X[cat2]):\n    y, x = np.int(i\/2), i%2 \n    ax = fig.add_subplot(gs[y,x])    \n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==0,c], kde = False,\n                 color = \"#004a4d\", hist_kws = dict(alpha=0.7),\n                 bins=10, label=\"Churn_No\", ax=ax);\n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==1,c], kde = False,\n                 color = \"#7d0101\", hist_kws = dict(alpha=0.7),\n                 bins=10, label=\"Churn_Yes\", ax=ax);\n\nax.legend(loc=\"center left\", bbox_to_anchor=(1.5,0.5),\n          prop={\"size\":12});","a896adca":"fig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(nrows=2, ncols=2, figure=fig)\n\nfor i, c in enumerate(train_X[num]):\n    y, x = np.int(i\/2), i%2 \n    ax = fig.add_subplot(gs[y,x])    \n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==0,c], kde = True,\n                 color = \"#004a4d\", hist_kws = dict(alpha=0.8),\n                 bins=20, label=\"Churn_No\", ax=ax);\n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==1,c], kde = True,\n                 color = \"#7d0101\", hist_kws = dict(alpha=0.5),\n                 bins=20, label=\"Churn_Yes\", ax=ax);\n\nax.legend(loc=\"center left\", bbox_to_anchor=(1.5,0.5),\n          prop={\"size\":12});","cfa0f9c4":"scaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","77458b7b":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=False, fmt=\".1f\", vmin=-1,\n            vmax=1, linewidth = 1,\n            center=0, mask=mask,cmap=\"RdBu_r\");","f817e7fa":"drop = [\"OnlineSecurity_No internet service\",\n        \"OnlineBackup_No internet service\",\n        \"DeviceProtection_No internet service\",\n        \"TechSupport_No internet service\",\n        \"StreamingTV_No internet service\",\n        \"StreamingMovies_No internet service\",\n        \"MultipleLines_No phone service\"]\n\nfor d in drop:\n    train_X.drop(d, axis=1, inplace=True)\n    test_X.drop(d, axis=1, inplace=True)\n    \nnp_train_X = train_X.values\nnp_train_Y = train_Y.values.ravel()\nnp_test_X = test_X.values\nnp_test_Y = test_Y.values.ravel()","12e7e34e":"knn_cls = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": range(30, 50, 2),\n    \"metric\": [\"minkowski\"],\n    \"p\": [1.0, 2.0],\n    \"algorithm\": [\"brute\"]\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\ngscv = GridSearchCV(\n    estimator=knn_cls,\n    param_grid=parameters,\n    scoring=\"f1\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=False\n)\n\ngscv.fit(np_train_X, np_train_Y)\nprint(\"Best parameters {}\".format(gscv.best_params_))","2e1e7036":"knn_cls = KNeighborsClassifier(**gscv.best_params_)\nknn_cls.fit(np_train_X, np_train_Y)\ny_pred = knn_cls.predict(np_test_X)\nprint(classification_report(np_test_Y, y_pred,\n                            target_names=[\"Churn No\", \"Churn Yes\"]))","1ad45a12":"knn_cls = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": range(40, 60, 2),\n    \"leaf_size\": [1, 2, 3],\n    \"metric\": [\"minkowski\"],\n    \"p\": [1.0, 2.0],\n    \"algorithm\": [\"kd_tree\"]\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\ngscv = GridSearchCV(\n    estimator=knn_cls,\n    param_grid=parameters,\n    scoring=\"f1\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=False\n)\n\ngscv.fit(np_train_X, np_train_Y)\nprint(\"Best parameters {}\".format(gscv.best_params_))","0b70ca58":"knn_cls = KNeighborsClassifier(**gscv.best_params_)\nknn_cls.fit(np_train_X, np_train_Y)\ny_pred = knn_cls.predict(np_test_X)\nprint(classification_report(np_test_Y, y_pred,\n                            target_names=[\"Churn No\", \"Churn Yes\"]))","7cde4a7d":"## Standardization\n\nStandardScaler is only fit to training data to prevent data leakage.","17c9c98a":"**customerID** has nothing to do with churn prediction, so it's dropped.","ff811662":"Numeric and one-hot-encoded categorical features are combined into a single dataframe.","e5f26c54":"Unique values of target variable:","1c244987":"Categorical and numeric features:","01722f8f":"Categorical features are converted to numeric. One of the categories is dropped to prevent correlation between features.","bc182d22":"Feature names are:","36c53ebe":"## Load Data\n\n**Telco Customer Churn** dataset is used. Following information is included:\n\n* Customers who left within the last month \u2013 target column **Churn**\n* Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support and streaming TV and movies\n* Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges and total charges\n* Demographic info about customers \u2013 gender, age range, and if they have partners and dependents\n\nDataset is loaded from input csv file. **Pandas** extracts the data and stores in a dataframe. Pandas styling is used to customize the look of the table.","91822bbe":"## Outlier Check with IQR\n\nNumeric features are analyzed for outliers using **interquartile range (IQR)**.","ffc75a28":"## Correlation Analysis","805eae47":"We train a new KNeighborsClassifier with the best parameters on np_train_X. Then we make predictions on np_test_X.","665fb49d":"There are 2 output classes as expected. Churn is converted to binary.","305dbf8a":"K Nearest Neighbor (KNN) is a supervised machine learning algorithm which can be used for both classification and regression. It does not use a model that generalizes on training data, that's why it's described as a **lazy learning method**. On the other hand, the methods using models to generalize on training data are called **eager learning methods**, e.g. neural network, SVM, tree based methods. On inference, lazy learning methods are slow and computationally expensive whereas eager learning methods do the hardwork during training. Lazy learning is especially suitable if training data is updated very often.\n\nKNN is also denoted as a nonparametric method which means it does not make any assumptions on data. On the other hand, a parametric method makes some strong assumptions. For example, if you want to fit a probability distribution to your data and assume Gaussian distribution, this is a parametric method. You only need to compute mean and standard deviation. If your assumption is consistent with your data, then your method gives good results, otherwise your method may fail. As a nonparametric method, KNN is suitable for both linear and nonlinear cases.\n\nNow, think about classification. How does KNN do classification without training? When the class of a test sample is queried, KNN inspects the similarity of all training samples with test sample. Degree of similarity is measured with a distance metric. It is assumed that if two samples are close to each other in feature space, they probably belong to same class. Searching for the closest point in a set is named as nearest neighbor search. KNN takes K closest samples. Final decision is made with majority voting. The mode of the classes of K nearest neighbors is the class of the queried test sample.\n\nK is a hyperparameter that determines the sensitivity of KNN. As K increases, the number of voting samples increases, decreasing the sensitivity. Large K results in low variance, high bias and small K results in high variance, low bias.\n\nOutline of the work is as follows:\n\n* Load Data\n* Feature Engineering\n* Split Data\n* Outlier Check with IQR\n* Visualization\n* Standardization\n* Correlation Analysis\n* KNN with Brute NN Search\n* KNN with KDTree","c20d6edf":"## Feature Engineering\n\nThe unique values each feature can take are inspected below.","31e3cb1d":"## KNN with KDTree\n\nIn nearest neighbor search, data structures like kdtree can be incorporated instead of using KNN in its original form with brute search. kdtree learns which training sample is residing on which part of the feature space. On inference, it takes you to the close proximity of test sample and gives you the neighbors. kdtree allows you to search multidimensional space efficiently.\n\nWhen creating kdtree, each node splits data using 1 dimension (1 feature). The split point is determined as the median of points along that dimension. The seperating hyperplane is orthogonal to dimension axis. The points on the left of hyperplane go to left child, the points on the right go to right child node. Choosing the number of points on each leaf, we slice the space into subspaces with the resolution we want. When a leaf is reached, we get a number of training points that we are interested in and KNeighborsClassifier switches to brute nearest neighbor search on this set.\n\nBelow, we do another grid search with KNeighborsClassifier to find optimal K, Minkowski p value when algorithm parameter is set to kd_tree. This time we have an extra hyperparameter, leaf_size denotes the number of points in each leaf.","c6781bff":"Numerical features","8f53cc84":"We have to check if there are any null values in TotalCharges.","e5fa6347":"Impute null values with mean.","8bf70a17":"## KNN with Brute NN Search\n\nWe will try KNN first with brute nearest neighbor search. We use grid search to find the optimal parameters and use stratified 5-fold for cross validation. Minkowski is used as distance metric and its value is searched (1 or 2). When p equals 1, Minkowski is Manhattan distance and when p equals 2, it is Euclidean distance. During grid search, model performance with each parameter combination is measured on cross validation folds. The parameters giving the highest performance is returned as best parameter set.","76ea1ee8":"Some features are dropped due to high correlation. Then, dataframes are converted to numpy arrays.","d0eed496":"## Visualization\n\nCategorical Features","e0b749a2":"Features gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling and PaymentMethod are categorical. Although datatype of SeniorCitizen is int64, it is categorical, it takes 0 and 1 values.\n\nFeatures tenure, MonthlyCharges and TotalCharges are numeric. Note that data type of TotalCharges is object (as info() function shows). But, if we look carefully, the entries of TotalCharges are float values converted to string.\n\nSpaces are removed from each entry of TotalCharges if there are any, then datatype is converted to float.","f470a81c":"Dataset has 7043 rows (training samples). There are 19 features. Target column **Churn** is also categorical.","4cbe68f1":"Features and corresponding labels are assigned to **data_X** and **data_Y**, respectively. Using Pandas info function, we inspect column data types and number of non-null values in data_X and data_Y.","5a6c6f69":"We train a new KNeighborsClassifier with the best parameters on np_train_X. Then we make predictions on np_test_X.","fe6a7fdf":"## Split Data\n\nDataset is split as training and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets.","0b3d2cc3":"There are no outliers as can be seen from the box plots."}}