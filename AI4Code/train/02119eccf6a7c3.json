{"cell_type":{"40d11b1f":"code","a9ec01fb":"code","2bb42a10":"code","926db984":"code","d675cb0b":"code","26c4d4a9":"code","8fc2d06f":"code","b5b5d69c":"code","2d370680":"code","1464822f":"code","03c48283":"code","c7977e50":"code","076418a2":"code","c7654eb9":"code","d59aba79":"code","59a13fcb":"code","10f5a9fd":"code","55e8a4c2":"code","1c46db42":"code","7cb16b90":"markdown","0978974a":"markdown","e3b96938":"markdown","e86a9b57":"markdown","c1a55eb8":"markdown","427f707c":"markdown","c529d156":"markdown","475ae34f":"markdown","eeaa4b47":"markdown","65a493bf":"markdown","f9ba3aa7":"markdown","f9bc07e6":"markdown","a9583a7f":"markdown"},"source":{"40d11b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nos.listdir(\"..\/input\/\")\ndf = pd.read_csv(\"..\/input\/housing.csv\")\n\n\n      \n      \n      \n\n# Any results you write to the current directory are saved as output.","a9ec01fb":"df.head\ndf.describe()","2bb42a10":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","926db984":"from sklearn.datasets import load_boston\nboston = load_boston()","d675cb0b":"print(boston.keys())","26c4d4a9":"boston.DESCR","8fc2d06f":"boston_ = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_.head()","b5b5d69c":"boston_['MEDV'] = boston.target","2d370680":"boston_.head()","1464822f":"sns.set(rc={'figure.figsize':(15,15)})\nsns.distplot(boston_['MEDV'], bins=100)\nplt.show()","03c48283":"correlation_matrix = boston_.corr().round(2)\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)","c7977e50":"plt.figure(figsize=(20, 5))\n\nfeatures = ['LSTAT', 'RM', 'RAD' , 'TAX']\ntarget = boston_['MEDV']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = boston_[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('MEDV')","076418a2":"X = pd.DataFrame(np.c_[boston_['LSTAT'], boston_['RM'] ,boston_['TAX'], boston_['RAD']], columns = ['LSTAT','RM', 'TAX', 'RAD'])\nY = boston_['MEDV']","c7654eb9":"\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","d59aba79":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)","59a13fcb":"y_pred = lin_model.predict(X_test)\ny_pred.astype('float64')\n","10f5a9fd":"y_pred.reshape(102)","55e8a4c2":"Y_test[] ","1c46db42":"# model evaluation for training set\nfrom sklearn.metrics import r2_score\ny_train_predict = lin_model.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\nr2 = r2_score(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\n\n# model evaluation for testing set\ny_test_predict = lin_model.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","7cb16b90":"We see that the values of MEDV are distributed normally with few **outliers** to know what are outliers click  [this](http:\/\/machinelearningmastery.com\/how-to-identify-outliers-in-your-data\/).","0978974a":"**Observations**:\n>The prices increase as the value of RM increases linearly. There are few outliers and the data seems to be capped at 50.\n\n>The prices tend to decrease with an increase in LSTAT. Though it doesn\u2019t look to be following exactly a linear line.\n\n>The prices tend to increase  with an increase in radial highways (RAD)but it is good at median. Though it doesn\u2019t look to be following exactly a linear line and having few outliers.\n\n>The prices tend to increase with an increase in TAX . Though it doesn\u2019t look to be following exactly a linear line and having few outliers at 700 \n","e3b96938":"**GIVE A SUPPORT BY UPVOTING THIS KERNEL IF YOU FIND IT USEFULL \n**\n\nFORK AND USE GRADIENT DESCENT TO BOOST THE PERFORMANCE IF YOU WANT\n","e86a9b57":"**Understanding the linear regression**\n\n\"linear regression is all about the concept of straight line y= mx +c \" (where c is intercept abou y axis)\nwhole linear regression is dependent on only this formula when its come to visualization and regresion and finding the relation between dependent and independent variables .\n\nIN DATA SCIENCE WORLD IT IS CLAIMED THAT ABOUT 70% PROBLEM CAN BE SOLVED BY LINEAR REGRESSION ALGORITHM AND IT IS MOST POPULAR AMON ANY OF ANOTHER ALGORITHM\n\n\n![lr.png](attachment:lr.png)\n\nthe best fitted line has least error value between estimated value and actual value\n---------------------------------------\ny axis shows  **DEPENDENT VARIABLE**\nx axis shows **INDEPENDENT VARIABLE**\n\n","c1a55eb8":"**REGRESSION ALGORITHM**\nthis deserve a right attention , because many problems , even intrinsically non-linear ones . can be easily solved with this model\n\n**Linear nmodel for regeression**\n1. Data set of real value vectors drawn from a data generating process data\n  X= {x1, x2, .. xn}\n  \n2. for each input vector is ac=soociate with real value yi\n  Y = {y1 , y2 , . . .  ,yn}  where yi implies set of real values.\n  \n  \n*Regression analysis is a form of predictive modelling technique which investigates the relationship between dependent and independent variable* \n\n**Three major uses for regression analysis**\n1. Determining the strength of prediction \n2. forecasting an effect , and\n3. Trends forecasting \n\n**why to select linear regression over others**\n> classification and regression capabilities \n> data quality easily to identify class outlier \n> computational complexity is less O(x^2) over any another algorithm\n> comprehnsible and transparent\n\n\n**Where is linear regression used **\n\n1. evaluating trends and sales estimation \n2. analyzing the impact of price change\n3. assesment of risk and financial services and insurance domains .\n\n\n\n\n\n\n\n\n","427f707c":"**LETS LEARN CODE **\n\n\n","c529d156":"FROM ABOVE RESULT OF Y-test and Y-pred WE CAN SAY THAT OUR MODEL IS PREDICTING GOOD\nNOW WE CAN USE RMSE (The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) (or sometimes root-mean-squared error) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.)\nAND R2 ( R-squared is a statistical measure that\u2019s used to assess the goodness of fit of our regression model ).\n\n","475ae34f":"**linear regression and R-squared **\n\n1. R-squared value is a statistical measures of how the close the data are the fitted regression line\n2. it is also known as coefficient of determination or the coefficient of multiple detetrmination.\n\n**CALCULATION OF R^2**\nhow to calculate of R-squared \n\nR^2 is nothing but it is = \u03b5(Yp - y' )^2\/ \u03b5(y - y' )^2 \n\n**Yp is predicted value\ny' is mean value \ny is dependent value **\n \n \n **ARE LOW R -squared values always bad**\n > well in some field it is entirely expected as the R^2 value to be low for foe example any field that attempts to predict human behaviour(psychology) R^2(50% , R^2 is low  statistically significant predicators , then we can still draw a conclusion about how changes , in predicator values in the changes in response value.","eeaa4b47":"**GRADIENT  DESCENT**(supervised learning)\n\n1. THE MATH BEHIND gradient descent requires two concept of calculus \n> PARTIAL DERIVATIVES, CHAIN RULES\n\n\n","65a493bf":">we create a correlation matrix that measures the linear relationships between the variables. \n>The correlation matrix can be formed by using the corr function from the pandas dataframe library. \n>We will use the heatmap function from the seaborn library to plot the correlation matrix.","f9ba3aa7":"1. **CRIM**: Per capita crime rate by town\n2. **ZN**: Proportion of residential land zoned for lots over 25,000 sq. ft\n3. **INDUS**: Proportion of non-retail business acres per town\n4. **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n5. **NOX**: Nitric oxide concentration (parts per 10 million)\n6. **RM**: Average number of rooms per dwelling\n7. **AGE**: Proportion of owner-occupied units built prior to 1940\n8. **DIS**: Weighted distances to five Boston employment centers\n9. **RAD**: Index of accessibility to radial highways\n10. **TAX**: Full-value property tax rate per $10,000\n11. **PTRATIO**: Pupil-teacher ratio by town\n12. B: 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town\n13. **LSTAT**: Percentage of lower status of the population\n14. MEDV: Median value of owner-occupied homes in $1000s","f9bc07e6":"Let\u2019s first plot the distribution of the target variable **MEDV**. We will use the distplot function from the seaborn library.\n\n\n","a9583a7f":"**Observations**:\n1. To fit a linear regression model, we select those features which have a high correlation with our target variable **MEDV**. By looking at the correlation matrix we can see that RM has a strong positive correlation with **MEDV** (0.7) where as **LSTAT** has a high negative correlation with MEDV(-0.74).\n\n1. An important point in selecting features for a linear regression model is to check for multi-co-linearity. The features **RAD**, **TAX** have a correlation of 0.91. These feature pairs are strongly correlated to each other. We should not select both these features together for training the model. Check this for an explanation. Same goes for the features **DIS** and **AGE** which have a correlation of -0.75."}}