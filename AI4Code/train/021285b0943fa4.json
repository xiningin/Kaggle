{"cell_type":{"15cc4b69":"code","c30fd0aa":"code","17a7f23c":"code","16beac26":"code","16750500":"code","5aadc198":"code","4a66ee8f":"code","c02fdfda":"code","1378f498":"code","0f10d928":"code","b966624f":"code","610ad9ee":"code","975a9cce":"code","498c627b":"code","2ace6f78":"code","9a6f0ec7":"code","454e7bf9":"code","56645f6f":"code","2745542a":"code","101f8a5c":"code","7b89eda0":"markdown","66fea09f":"markdown","854549f4":"markdown","2198c1a2":"markdown","f6c6f1b0":"markdown","9a241e68":"markdown","882e65fc":"markdown","fe742668":"markdown","a1ae16a2":"markdown","4aee5595":"markdown","f1c598bf":"markdown","8cb5be40":"markdown","daefdc90":"markdown","bda9f990":"markdown"},"source":{"15cc4b69":"# add it to G-research","c30fd0aa":"import pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns","17a7f23c":"n_step = 100\n\nw = np.zeros(n_step)\n        \nfor i in range(1,n_step):\n    yi = np.random.normal()\n    w[i] = w[i-1]+(yi\/np.sqrt(n_step))\n    \nplt.plot(w)\nplt.title('Simulated Brownian Motion')","16beac26":"n_step = 100\ndt = 0.01\nS0 = 1\nmu = 0\nsigma = 0.05\nS = np.ones(n_step)\n\nincrements = np.random.normal(0, np.sqrt(dt), size=(1, n_step))\nS = np.exp((mu - sigma ** 2 \/ 2) * dt + sigma * increments).T\nS = np.vstack([np.ones(1), S])\nS = S0 * S.cumprod(axis=0)\n\nplt.plot(S)\nplt.title( \"Realizations of Geometric Brownian Motion\")","16750500":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","5aadc198":"book_example = pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0')\ntrade_example =  pd.read_parquet('..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0')\n\nstock_id = '0'\ntime_id = book_example.time_id.unique()\n\nbook_example = book_example[book_example['time_id'].isin(time_id)]\nbook_example.loc[:,'stock_id'] = stock_id\n\ntrade_example = trade_example[trade_example['time_id'].isin(time_id)]\ntrade_example.loc[:,'stock_id'] = stock_id\n\nbook_example['wap'] = calc_wap(book_example)\nbook_example['log_wap'] = np.log(book_example['wap'])\nbook_example.loc[:,'log_return'] = book_example.groupby('time_id')['log_wap'].diff()\n\nbook_example = book_example.merge(trade_example, on=['seconds_in_bucket','time_id'],how='left', suffixes=('', '_y'))","4a66ee8f":"ts_example = book_example[book_example['time_id']==5]","c02fdfda":"plt.plot(ts_example.wap)\nplt.title('Price - Stock 0 - Time 5')","1378f498":"import torch\n\n# use cpu if gpu is not available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# get the increments\ndW = ts_example.log_return\ndW.loc[0] = 0\n\ndT = ts_example.seconds_in_bucket.diff()\ndT.loc[0] = 1\n\n# initialize sigma randomly\nsigma = torch.tensor(np.random.randn(1)*.01 + 1, device=device)\n\n# the actual parameters are the logarithms of sigma, to enforce positivity and have more stable convergence\n\nwith torch.no_grad():\n    sigma_log = torch.log(sigma).clone().requires_grad_(True)\n\n# load the data to device (increments)\ndW = torch.tensor(dW.values, device=device)\ndT_log = torch.log(torch.tensor(dT.values, device=device))","0f10d928":"%%time\n\nITERS = 15000\n\n# use Adam as it finds the right learning rates easily\nopt = torch.optim.Adam([sigma_log])\n\niteration = 0\n\nwhile iteration < ITERS:\n    \n    # reset the gradients to 0\n    opt.zero_grad() \n    \n    # compute the log-likelihood\n    logL = 1\/2 * torch.sum((dW**2 @ (1\/torch.exp(dT_log)).float())) * (1\/torch.exp(2*sigma_log)) + 1\/2*torch.sum(dT_log) + sigma_log\n        \n    # compute the gradient\n    logL.backward()\n\n    if iteration % 1000 == 0:\n        with torch.no_grad():\n            print(f'iter {iteration:8} {logL} {sigma_log}')\n\n    # execute one step of gradient descent\n    opt.step()\n    \n    iteration+=1","b966624f":"torch.exp(sigma_log)\nnp.sqrt(np.sum(log_return(ts_example.wap)[1:]**2))","610ad9ee":"%%time\n\ndW = ts_example.log_return\ndW.loc[0] = 0\n\ndT = ts_example.seconds_in_bucket.diff()\ndT.loc[0] = 1\n\n# initialize sigma randomly\nsigma_0 = np.random.randn(1)*.001\n\nlog_sigma_0 = np.log(sigma_0)\ndT_log = np.log(dT)\n\ndef neg_log_likelihood(log_sigma):\n     # compute the log-likelihood\n    logL = 1\/2 * np.sum((dW**2 @ (1\/np.exp(dT_log)))) * (1\/np.exp(2*log_sigma)) + 1\/2*np.sum(dT_log) + log_sigma\n    return logL\n\nres = minimize(neg_log_likelihood, log_sigma_0, method='nelder-mead',\n               options={'xatol': 1e-8, 'disp': True})","975a9cce":"# 55 ms x 112 stocks x 3830 time ids \/ 4 threads = 1h30\n(55\/1000)*112*3830*1\/4*1\/3600","498c627b":"np.exp(res.final_simplex[0][0][0])","2ace6f78":"def preprocessor_book(file_path_book):\n\n    df_book = pd.read_parquet(file_path_book)\n    stock_id = int(file_path_book.split('=')[1])\n\n    df_book['wap'] = calc_wap(df_book)\n    df_book['log_wap'] = np.log(df_book['wap'])\n    df_book['log_return'] = df_book.groupby('time_id')['log_wap'].diff()\n\n    unique_time_ids = df_book['time_id'].unique()\n\n    res_BS = []\n\n    for time_id in unique_time_ids:\n\n        ts_example = df_book[df_book['time_id']==time_id]\n\n        dW = ts_example.log_return.values\n        dW[0] = 0\n\n        dT = ts_example.seconds_in_bucket.diff().values\n        dT[0] = 1\n\n        # initialize sigma randomly\n        sigma_0 = 0.0001\n\n        log_sigma_0 = np.log(sigma_0)\n        dT_log = np.log(dT)\n\n        def neg_log_likelihood(log_sigma):\n             # compute the log-likelihood\n            logL = 1\/2 * np.sum((dW**2 @ (1\/np.exp(dT_log)))) * (1\/np.exp(2*log_sigma)) + 1\/2*np.sum(dT_log) + log_sigma\n            return logL\n\n        res = minimize(neg_log_likelihood, log_sigma_0, method='nelder-mead',\n                       options={'xatol': 1e-8, 'disp': False})\n\n        \n        rv = np.sqrt(np.sum(np.square(dW)))\n        \n        res_BS.append((stock_id,time_id,np.exp(res.final_simplex[0][0][0]),rv))\n\n    return pd.DataFrame(res_BS, columns=['stock_id', 'time_id', 'vol_BS','rv'])\n\n\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    def for_joblib(stock_id):\n\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = preprocessor_book(file_path_book)\n        \n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","9a6f0ec7":"%%time\n\ndata_dir  ='..\/input\/optiver-realized-volatility-prediction\/'\n\ntrain, test = read_train_test()\n\ntrain_stock_ids = train['stock_id'].unique()\ntest_stock_ids = test['stock_id'].unique()\n\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntest_ = preprocessor(test_stock_ids, is_train = False)\n\ntrain = train.merge(train_, on = ['time_id','stock_id'], how = 'left')\ntest = test.merge(test_, on = ['time_id','stock_id'], how = 'left')\n","454e7bf9":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","56645f6f":"print(rmspe(train.target, train.vol_BS))","2745542a":"print(rmspe(train.target, train.rv))","101f8a5c":"test['target'] = test['vol_BS']\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","7b89eda0":"# Estimated volatility v.s. realized volatility","66fea09f":"# No model Baseline","854549f4":"We might start by simplifying the model a bit, assuming no drift (and ignoring Ito's formula):\n\n$$ S_t = S_0 e^{\\sigma W_t} $$\n\n\nUnder our assumptions, $W_t$ increments follow the same law and are uncorrelated. Each increment follow a normal law of mean $0$ and variance $\\sigma^2 (T_i - T_{i-1})$. Then therefore have the joint probability density function of these increments, which is the product of pdf for Gaussian random variables. \n\nWe can obtain the likelihood :\n\n$$\nL(\\sigma, T) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2 (T_i - T_{i-1})}} \\exp \\left(-\\frac{{\\Delta W_i}^2}{2 \\sigma^2(T_i - T_{i-1})}\\right)\n$$\n\nBy maximizing the value of the likelihood function with respect to the parameters, we can find the parameters which are most likely to have generated the data.\n\nIn practice we have the time and price increments so we can estimate the volatility by finding which volatility maximize the likelihood. Equivalently we can minimize the negative log-likelihood\n\n$$\n\\mathcal{l}(\\sigma, T) = \\sum_{i = 1}^n \\left(\\frac{{\\Delta W_{i}}^2}{2 \\sigma^2 (T_i - T_{i-1})} + \\frac{1}{2}\\log(T_i - T_{i-1}) + \\log(\\sigma)\\right) + \\text{const}\n$$\n\nWe can optimize the log-likelihood numerically, to obtain the estimated values of $\\sigma$. The cool stuff here is using Pytorch to do so. \n\n\n## Caveats:\nThis approach assume a lot things (normal increments of log returns, no drift, independence of increments). We know these assumptions are all wrong, but the volatility estimation might still be usefull.","2198c1a2":"# Application to Optiver Data","f6c6f1b0":"# test on all stocks","9a241e68":"# Maximum Likelihood Estimation","882e65fc":"Allows for simple simulation:","fe742668":"# Pytorch for Maximum Likelihood Estimation of Volatility","a1ae16a2":"# Scipy version\n\nGoing Back to a basic optimisation library we might get faster results.","4aee5595":"# Stochastic Calculus Intro - Brownian Motion - Geometric Brownian Motion - Balck & Scholes model\nStochastic Calculus is a whole Field centered around stochastic processes and their integration. It is heavily used in industry to deal with financial time series. The main building block of stochastic calculus is Brownian Motion, a random process. Inspired from erratic movements of pollen particles on water (observed by Brown in the 30's ... 1830's). It can be caracterised (and simulated) by random normal increments.\n\nTaking a simple definition of Brownian motion:\n\n- $W_0 = 0$\n- $W_t$ is almost surely continuous\n- $W_t$ as random increments \n- $W_t - W_s = N(0,t-s)$ for $0 \\leq s \\le t$ \n\n\n","f1c598bf":"Cool but too slow. Even with parallelisation we need to estimate that for hundreds of thousands of time series.","8cb5be40":"With some basic assumptions the price of a stock can be modeled as the exponential of a random walk, usually called Geometric Brownian Motion. This is a standard of financial modelling and might be used for option pricing in the Black & Scholes model. Under these assumptions, at time $t$, the price $S_t$ equals :\n\n$$ S_0 e^{(\\mu -\\sigma^2)*t\/2 + \\sigma W_t} $$\n\nWhere $W_t$ is a Brownian motion. Of course assuming a constant volatility would be a problem for forecasting it. But we might use this model to estimate a smoothed volatility and use it as a feature. \n\nUsing a vectorised version of the precedent calculation:","daefdc90":"One of the few Quantitative contribution we had in the Jane Street competition was this notebook: https:\/\/www.kaggle.com\/pcarta\/jane-street-time-horizons-and-volatilities. We had returns at different time horizons. The idea was to use pytorch as a an optimisation solver for estimation of parameters of a Brownian Motion via Maximum Likelihood. Some winning teams mentionned it as decisive for their understanding of the competition and at least one team managed to use it for target engineering (using estimated trend as target). I figured this could be used for volatility estimation here. This notebook rely on the code in the linked notebook.\n\nFor the moment I've only managed to make it work on a given time_id x stock_id time series and it take 6 seconds... it might require some additional work (factorization, parallelisation) to be usable. Also the Geometric Brownian motion part require assuming a constant volatility. This might not be optimal for a volatility forecasting competition and might require an upgrade of the model. Worth mentionning too is that I added a bit of stochastic calculus explanation at the beginning to make it more palatable for non-quants.","bda9f990":"# Multivariate approach with scipy\n\nRefactoring the code to get a multivariate approach might help too."}}