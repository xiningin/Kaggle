{"cell_type":{"7814b163":"code","d28d5f78":"code","817eba0f":"code","2b969abb":"code","10afefe1":"code","71606f10":"code","6c5c069f":"code","46e58820":"code","b98bd17a":"code","9d612a99":"code","e5ef7bc5":"code","48aa1b99":"code","ef250fdb":"code","411e6ead":"code","642d981b":"code","b466388f":"code","1e98fa7b":"code","94cc23cf":"code","52bd5cd6":"code","2386cb65":"code","046413c4":"code","c7f921cb":"code","1b47d367":"code","ef661cfe":"code","460be877":"code","32e0e6c1":"code","f4e2ca25":"code","3bc59dee":"code","bd80fb41":"code","ac243deb":"code","cbafd510":"code","89597538":"code","aeb52e7c":"code","237e32f1":"code","70084751":"code","8c87ff9a":"code","bc5dacd3":"code","0abe3004":"code","ee63ae61":"code","6b718f45":"code","491eea32":"code","b9f73bd8":"code","a6156a94":"code","30524747":"code","aaa7acf8":"code","a010fab4":"code","d52fa946":"code","1a725d31":"code","1895b4cd":"code","06b5592f":"code","f9c7e705":"code","684d8fc9":"code","357ccce6":"code","12265367":"code","c3a30c34":"code","a5f571c8":"markdown","bfea5659":"markdown","afb9b38a":"markdown","736d2a11":"markdown","d215c137":"markdown","013bd99b":"markdown","90256ba1":"markdown","2517c6eb":"markdown","7ff38ddb":"markdown","b568dd99":"markdown","6bb83c36":"markdown","a8921399":"markdown","0d416673":"markdown","5da087ad":"markdown","3ab1fb88":"markdown","bcfd3d6b":"markdown","a07e0611":"markdown","d3d96379":"markdown","798dd04a":"markdown","7ea73501":"markdown","e600e93e":"markdown","04291937":"markdown","3f36008b":"markdown","dd4284dc":"markdown","65ddd455":"markdown","6e044474":"markdown","b99e3855":"markdown","8d97ab27":"markdown","07dc5c62":"markdown","698a818f":"markdown","3342e42a":"markdown","8e4eadba":"markdown","de972362":"markdown","783817c8":"markdown","e5b14c75":"markdown","f6006cb3":"markdown","6675b6dc":"markdown","fb14e222":"markdown","1a3237e1":"markdown","dc182707":"markdown","07199e69":"markdown","90f2344d":"markdown","b96df70f":"markdown","ddb5f99d":"markdown","5c5e630d":"markdown","6d908365":"markdown","d0221d45":"markdown","ceac56f6":"markdown","5a03d0c1":"markdown","e091cdc9":"markdown","aea648bd":"markdown","13688cd7":"markdown","ae55cb1d":"markdown","d9d1a6f8":"markdown","cf6b3f89":"markdown","7bfa41dc":"markdown","2adfe53e":"markdown","ecf977ce":"markdown","bfc8dfac":"markdown","99388817":"markdown","4209d4c3":"markdown","95a84e3d":"markdown","1dfb27ad":"markdown","c04e19bb":"markdown","cb9fc5b4":"markdown","90bb0867":"markdown","751b1493":"markdown","2e1e4346":"markdown","c3af816d":"markdown","b38d4947":"markdown","f7e5ebfe":"markdown","edb19dea":"markdown","00460722":"markdown"},"source":{"7814b163":"from sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n\nimport miner_a_de_datos_an_lisis_exploratorio_utilidad as utils\n\nseed = 27912","d28d5f78":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\" \n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)","817eba0f":"(X, y) = utils.divide_dataset(data, target=\"Outcome\")\n\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","2b969abb":"train = X_train.copy()\ntrain[target] = y_train","10afefe1":"train","71606f10":"train.info()","6c5c069f":"train.describe().T","46e58820":"train.describe(include='category')","b98bd17a":"utils.plot_barplot(train)","9d612a99":"utils.plot_histogram(train)","e5ef7bc5":"vs = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nntrain = train.replace({v: {0: np.NaN} for v in vs})\nutils.not_valid_values_plot(ntrain, vs)","48aa1b99":"train_clean = utils.imputar_valores(train)\nutils.plot_histogram(train_clean)","ef250fdb":"f = go.Figure(data=[{'type': 'box', 'y': train_clean[v], 'name': v} for v in set(train_clean.columns) - {\"Outcome\"}])\nf.show()","411e6ead":"fig = utils.plot_pairplot(train_clean, 'Outcome')\nfig.update_layout(width=1200, height=1000)","642d981b":"px.imshow(train_clean.corr())","b466388f":"disc = KBinsDiscretizer(n_bins=3, strategy='uniform')","1e98fa7b":"med_imp = SimpleImputer(missing_values=0, strategy='median')\nmea_imp = SimpleImputer(missing_values=0, strategy='mean')\npreproc = ColumnTransformer([('', 'drop', ['Insulin', 'SkinThickness', 'Pregnancies']), \n                             ('med_inp', med_imp, [\"Glucose\", \"BloodPressure\"]),\n                             ('mea_inp', mea_imp, [\"BMI\"])], remainder='passthrough')","94cc23cf":"zeror = DummyClassifier(strategy='most_frequent', random_state=seed)\n\nclean_zeror = make_pipeline(preproc, zeror)\nutils.evaluate(clean_zeror, X_train, X_test, y_train, y_test)\n\ncfs = [confusion_matrix(y_test, clean_zeror.predict(X_test))]\n\n\n","52bd5cd6":"ndt = DecisionTreeClassifier(random_state=seed)\n\ncln_ndt = make_pipeline(preproc, ndt)\n\nutils.evaluate(cln_ndt, X_train, X_test, y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, cln_ndt.predict(X_test)))","2386cb65":"cln_disc_ndt = make_pipeline(preproc, disc, ndt)\n\nutils.evaluate(cln_disc_ndt, X_train, X_test, y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, cln_disc_ndt.predict(X_test)))","046413c4":"go.Figure([go.Scatter(x=[0, 1], y=[0, 1], line={'dash': 'dash'}, name='Clasificador aleatorio'), go.Scatter(x=[cfs[i][0, 1] \/ (cfs[i][0, 0] + cfs[i][0, 1]) for i in range(len(cfs))], y=[cfs[i][1, 1] \/ (cfs[i][1, 1] + cfs[i][1, 0]) for i in range(len(cfs))], mode='markers', hovertext=['ZeroR', '\u00c1rbol sin discretizar', '\u00c1rbol discretizando'], name='Clasificadores propuestos')], layout={'title': 'Espacio ROC', 'xaxis': {'title': '1-specificity'}, 'yaxis': {'title': 'sensitivity'}})","c7f921cb":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index, target)","1b47d367":"data.sample(5, random_state=seed)","ef661cfe":"del data['Unnamed: 32']","460be877":"data.sample(5, random_state=seed)","32e0e6c1":"data.diagnosis.unique()","f4e2ca25":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","3bc59dee":"X.sample(5, random_state=seed)","bd80fb41":"y.sample(5, random_state=seed)","ac243deb":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","cbafd510":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","89597538":"train.sample(5, random_state=seed)","aeb52e7c":"test.sample(5, random_state=seed)","237e32f1":"data.shape","70084751":"train.shape","8c87ff9a":"test.shape","bc5dacd3":"train.info(memory_usage=False)","0abe3004":"y_train.cat.categories","ee63ae61":"train.describe(include=\"category\")","6b718f45":"utils.plot_barplot(train)","491eea32":"utils.plot_histogram(train)","b9f73bd8":"train_mean = train.loc[:,'radius_mean':'fractal_dimension_mean']\ntrain_mean[\"diagnosis\"] = train[\"diagnosis\"]\n\nfig = utils.plot_pairplot(train_mean, 'diagnosis')\nfig.update_layout(width=1600, height=1400)","a6156a94":"X_train_mean = X_train.loc[:,'radius_mean':'fractal_dimension_mean']\n\nfig = px.imshow(X_train_mean.corr(),title=\"Mapa de Correlaci\u00f3n variables 'mean'\")\nfig.show()\n\nX_train_mean.corr()","30524747":"X_train_se = X_train.loc[:,'radius_se':'fractal_dimension_se']\n\nfig = px.imshow(X_train_se.corr(),title=\"Mapa de Correlaci\u00f3n variables 'se'\")\nfig.show()\n\nX_train_se.corr()","aaa7acf8":"X_train_worst = X_train.loc[:,'radius_worst':'fractal_dimension_worst']\n\nfig = px.imshow(X_train_worst.corr(),title=\"Mapa de Correlaci\u00f3n variables 'worst'\")\nfig.show()\n\nX_train_worst.corr()","a010fab4":"f = go.Figure(data=[{'type': 'box', 'y': train[v], 'name': v} for v in set(train.columns)- {\"diagnosis\"}])\nf.show()","d52fa946":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"concavity_mean\", \"compactness_mean\", \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\", \"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \"area_worst\", \"perimeter_worst\"])], remainder=\"passthrough\")","1a725d31":"discretizer = KBinsDiscretizer(n_bins=3, strategy=\"uniform\")","1895b4cd":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","06b5592f":"pipeline = make_pipeline(preproc, zero_r_model)","f9c7e705":"utils.evaluate(pipeline,\n               X_train, X_test,\n               y_train, y_test)","684d8fc9":"tree_model = DecisionTreeClassifier(random_state=seed)","357ccce6":"preprocessed_tree_model = make_pipeline(preproc, tree_model)\npreprocessed_discretized_tree_model = make_pipeline(preproc, discretizer, tree_model)","12265367":"utils.evaluate(preprocessed_tree_model,\n               X_train, X_test,\n               y_train, y_test)","c3a30c34":"utils.evaluate(preprocessed_discretized_tree_model,\n               X_train, X_test,\n               y_train, y_test)","a5f571c8":"Mediante un **Boxplot** veremos de mejor forma como est\u00e1n distribuidos los datos de cada una, y as\u00ed ademas identificar de mejor forma aquellas que tengan valores at\u00edpicos (*outliers*), no solo fij\u00e1ndonos en el histograma. Ahora, podremos fijarnos en cada variable de forma individual para comprobar si cuentan con alg\u00fan outlier.","bfea5659":"###\u00a0Discretizaci\u00f3n","afb9b38a":"Podemos ver que Age y Pregnancies presentan cierta correlaci\u00f3n por lo que eliminaremos esta \u00faltima. Tambi\u00e9n observamos que las variables predictoras son en general bastante independientes de la variable clase ya que no se puede observar una diferencia clara en la distribuci\u00f3n de los puntos etiquetados como 0 y como 1.\n\nAl no poder observar puntos de corte claros, tomaremos arbitrariamente la decisi\u00f3n de realizar la discretizaci\u00f3n en tres intervalos de igual anchura.\n\n","736d2a11":"Podemos ver que no est\u00e1 balanceada.","d215c137":"Comenzaremos mostrando un **histograma** con todas las variables, para mostrar la densidad de ejemplos para los distintos valores de las variables num\u00e9ricas y analizar las tendencias que estas toman:","013bd99b":"## 2. Preprocesamiento de datos","90256ba1":"Observando los resultados de los tres clasificadores, podemos decir que los \u00e1rboles de decisi\u00f3n son superiores al ZeroR en *accuracy*, adem\u00e1s de alcanzar un mejor compromiso entre *sensitivity* y *specificity*.\n\nEntre los dos \u00e1rboles, la discretizaci\u00f3n previa parece que mejora ligeramente las medidas consideradas con respecto a no realizarla.","2517c6eb":"## Aprendizaje *Zero-R*","7ff38ddb":"Con el objeto de obtener unos datos m\u00e1s simples y convertir las variables num\u00e9ricas en intervalos vamos a discretizar esas variables, siendo en este caso, todas las variables predictoras.\n\nPara simplificar los datos realizaremos un proceso de discretizaci\u00f3n de los datos. Esto quiere decir que convertiremos las variables que son num\u00e9ricas (todas las variables predictoras que tengamos **despu\u00e9s de la selecci\u00f3n de variables** que hemos hecho previamente) en variables agrupadas por intervalos, lo que restar\u00e1 complejidad al modelo.\n\nAl no poder observar puntos de corte claros, tomaremos arbitrariamente la decisi\u00f3n de realizar la discretizaci\u00f3n en tres intervalos de igual anchura.","b568dd99":"Otra cosa que observamos gracias a aplicar el m\u00e9todo `info` sobre nuestro conjunto de datos de entrenamiento es que en realidad solo contamos con **10 variables** reales, pero que se convierten en 30 puesto que est\u00e1n divididas en 3 categor\u00edas `mean`, `se` y `worst`. Es importante considerar esto cuando pasemos a visualizar las variables posteriormente, puesto que al haber tal cantidad de variables ser\u00e1 mejor una representaci\u00f3n dividida por estas 3 categor\u00edas.","6bb83c36":"El an\u00e1lisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este an\u00e1lisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicaci\u00f3n de t\u00e9cnicas simples de resumen de datos y m\u00e9todos gr\u00e1ficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos.\n\nPara comenzar, veremos una descripci\u00f3n del conjunto de datos que vamos a emplear.","a8921399":"Y a continuaci\u00f3n la variable objetivo:","0d416673":"Lo primero que haremos es cargar los datos y dividirlos en conjunto de entrenamiento y de test estratificando.","5da087ad":"A continuaci\u00f3n, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). ","3ab1fb88":"Estudiando los tres mapas de correlaci\u00f3n podemos sacar una clara conclusi\u00f3n: **Las variables `perimeter y area` dependen directamente de `radius`.** Esto significa que m\u00e1s adelante en el preprocesamiento de datos podremos eliminar las variables `perimeter y area` sin problema.\n\nDe la misma manera, vemos c\u00f3mo las variables `concavity y compactness` dependen tambi\u00e9n en gran medida de `concave points`, por lo que ambas variables podr\u00e1n tambier ser eliminadas posteriormente, debido a su dependencia con esta \u00faltima.","bcfd3d6b":"Podemos ver que casi un 50% de los valores de la variable Insulin y un 30% de la variable SkinThickness no son v\u00e1lidos, por lo que hemos decidido eliminarlas. Para el resto de variables imputaremos los valores perdidos, por la mediana en el caso de Glucose y BloodPressure al ser variables enteras, y por la media en el caso de BMI.","a07e0611":"## Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","d3d96379":"Aplicamos el pipeline al \u00e1rbol de decisi\u00f3n creado. Diferenciaremos en dos:\n* Solo preprocesamiento (eliminaci\u00f3n de variables que no son necesarias)\n* Preprocesamiento y discretizaci\u00f3n","798dd04a":"##\u00a04. Aprendizaje y Evaluaci\u00f3n *\u00c1rbol de Decisi\u00f3n*","7ea73501":"##\u00a03. Aprendizaje y Evaluaci\u00f3n *Zero-R*","e600e93e":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la funci\u00f3n `sample` para comprobar que el proceso ha sido realizado correctamente","04291937":"## Evaluaci\u00f3n *Zero-R*","3f36008b":"##\u00a03. Aprendizaje y Evaluaci\u00f3n *Zero-R*","dd4284dc":"Primero recopilaremos el preprocesamiento en un pipeline.\n","65ddd455":"Comenzamos cargando el conjunto de datos `wisconsin`:","6e044474":"Se puede observar mejor de la siguiente manera, comprobando gr\u00e1ficamente qu\u00e9 clase es la que predomina:","b99e3855":"Comprobaremos que se hayan separado correctamente:","8d97ab27":"Comprobamos lo mismo para nuestro nuevo conjunto de prueba:","07dc5c62":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo bas\u00e1ndonos en las observaci\u00f3n que hemos realizado en el an\u00e1lisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selecci\u00f3n de variables adecuadas para conseguir reducir el n\u00famero de estas.\n\n\nPara realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nComo hemos decidido anteriormente graci\u00e1s al an\u00e1lisis exploratorio de los datos, debemos eliminar las columnas (variables) seleccionadas previamente:\n* Aquellas relacionadas con los `concave points`: `concavity_mean, compactness_mean, concavity_se, compactness_se, concavity_worst y compactness_worst`.\n* Aquellas relacionadas con `radius`, es decir: `area_mean, perimeter_mean, area_se, perimeter_se, area_worst y perimeter_worst`.\n\nLa respuesta a por qu\u00e9 estas columnas son las mencionadas previamente, porque son variables que dependen directamente de las 2 que vamos a dejar en nuestro conjunto de datos: `radius y concave points`.","698a818f":"## 1. An\u00e1lisis exploratorio de datos\n\nPrimero veamos c\u00f3mo se distribuyen las diferentes variables.","3342e42a":"## 2. Preprocesamiento de datos","8e4eadba":"De acuerdo con esto, todas las variables predictoras del conjunto de datos son num\u00e9ricas (`float64`). Sin embargo, la variable clase (`diagnosis`) es categ\u00f3rica y contiene dos estados, cuyos valores ser\u00e1n `M`y`B` como ya hemos comentado anteriormente:","de972362":"Para conocer cu\u00e1l es el tipo de las variables, recurrimos al m\u00e9todo `info`:","783817c8":"Lo primero que podemos observar es que existen muchos valores perdidos que vienen representados con 0 en variables en las que este valor no est\u00e1 entre los valores razonables para la variable. Es el caso de Glucose, BloodPressure, SkinThickness, Insulin y BMI. Veamos qu\u00e9 fracci\u00f3n del total de valores est\u00e1n perdidos por variable.","e5b14c75":"Como puede resultar evidente, los \u00e1rboles de decisi\u00f3n que hemos implementado obtienen mejores resultados que el algoritmo `Zero-R` puesto que no solo se quedan con la clase mayoritaria.\n\nA su vez, es importante comentar que el \u00e1rbol de decisi\u00f3n entrenado con el conjunto de datos discretizado (tras realizar las modificaciones necesarias que aprendimos en el an\u00e1lisis exploratorio) obtiene una tasa de acierto similar al \u00e1rbol de decisi\u00f3n con el conjunto de datos sin discretizar para este problema concreto.\n\nLa conclusi\u00f3n que podemos obtener es que con un sencillo preprocesamiento y empleando un modelo predictivo no muy complejo obtendremos unas predicciones con una precisi\u00f3n de un 92%, lo que nos demuestra la importancia de preprocesar nuestro conjunto de datos.","f6006cb3":"Empezamos mostrando las variables predictoras:","6675b6dc":"Antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","fb14e222":"## Conclusi\u00f3n","1a3237e1":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Comenzamos con las variables del conjunto de datos de entrenamiento, observando que la variable clase tambi\u00e9n aparece al final del conjunto:","dc182707":"## 4. Aprendizaje y Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","07199e69":"### Visualizaci\u00f3n de las variables","90f2344d":"Aplicamos nuestro preprocesamiento al conjunto de entrenamiento y al conjunto de test:","b96df70f":"#\u00a0Pr\u00e1ctica 1: An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\n\n###\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Realizado por:\n\n* Antonio Beltr\u00e1n Navarro\n* Ram\u00f3n Jes\u00fas Mart\u00ednez S\u00e1nchez","ddb5f99d":"Seguimos con el Mapa de Correlaci\u00f3n de las variables de tipo `se`:","5c5e630d":"Mediante el estudio de este histograma en el que se han representado todas las variables, podremos extraer diversas conclusiones que nos ser\u00e1n \u00fatiles a la hora de preprocesar los datos de nuestro conjunto:\n* Mirando las variables independientemente, encontramos outliers en las variables `radius, area, y perimeter` en sus tres variantes, lo que ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.\n* Estimaci\u00f3n de las distribuciones:\n    * Si analizamos las variables `mean`, veremos que `radius, perimeter y area` siguen una distribuci\u00f3n normal al igual que la mayor\u00eda de variables, excepto las relacionadas con la concavidad, que pueden llegar a tomar una distribuci\u00f3n exponencial.\n    * Si analizamos las variables `se`, veremos que `radius, perimeter y area` siguen ahora una distribuci\u00f3n exponencial, al igual que las relacionadas con la concavidad.\n    * Si analizamos las variables `worst`, veremos que `radius, perimeter y area` vuelven a seguir una distribuci\u00f3n normal, y las relacionadas con la concavidad pasan a ser distintas, ahora solo `concavity` sigue una tendencia exponencial, mientras que `concave_points` pasa a tomar una clara distribuci\u00f3n normal.\n* Tras este paso, vemos tambi\u00e9n que todas las distribuciones tienden a ir hacia la **derecha**.","6d908365":"----","d0221d45":"Empezaremos entrenando un Zero-R.","ceac56f6":"Por \u00faltimo, discretizaremos antes de entrenar el \u00e1rbol","5a03d0c1":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos pasar con el an\u00e1lisis exploratorio de datos.","e091cdc9":"Las conclusiones que podemos obtener de este diagrama es que encontramos **outliers** en las variables `radius, area, y perimeter` en sus tres variantes (como ya hab\u00edamos comentado en el histograma), y adem\u00e1s las variables `smoothness y concavity` cuentan con valores at\u00edpicos.\n\nEsto ser\u00e1 algo que ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.","aea648bd":"Comprobamos que se ha borrado correctamente haciendo uso del m\u00e9todo `sample` de nuevo:","13688cd7":"Para ver claramente las distribuciones, las visualizaremos mediante un diagrama de cajas.","ae55cb1d":"Ahora es el momento de entrenar y validar nuestros clasificadores. Para ello, vamos a usar una matriz de confusi\u00f3n y tasa de acierto.","d9d1a6f8":"Como era de esperar, el modelo *Zero-R* obtiene malos resultados, pues solo predice la clase mayoritaria en el conjunto de entrenamiento, en este caso 0 (B).","cf6b3f89":"A la hora de visualizar las variables, comenzaremos comprobando la distribuci\u00f3n de la variable clase de nuestro conjunto de entrenamiento. Como vemos, hay 250 instancias de la clase mayoritaria `B`, y 148 de la clase `M`.","7bfa41dc":"## 1. An\u00e1lisis exploratorio de datos","2adfe53e":"## Aprendizaje *Arbol de Decisi\u00f3n*","ecf977ce":"### Descripci\u00f3n del conjunto de datos\nEl n\u00famero de casos y variables (respectivamente) del conjunto de datos se puede obtener consultando el atributo `shape`:\nObservamos como de los 569 registros iniciales, tenemos **398** de ellos para el entrenamiento (un 70%), y **171** (un 30%) para el conjunto de prueba, ambos con 31 variables, las 30 predictoras y la variable objetivo.","bfc8dfac":"# Pima Diabetes","99388817":"Vamos a ver los resultados del \u00e1rbol de decisi\u00f3n sin el conjunto de datos discretizado:","4209d4c3":"Y con el conjunto de datos discretizado:","95a84e3d":"Ahora usaremos un \u00e1rbol de clasificaci\u00f3n sin discretizaci\u00f3n previa y con los hiperpar\u00e1metros por defecto:\n","1dfb27ad":"Ahora realizaremos un an\u00e1lisis multivariado para intentar encontrar correlaciones entre las variables.\n\nPara ello, obtenemos del conjunto de entrenamiento solo las variables de tipo `mean`, puesto que son las m\u00e1s representativas. Con ellas, realizaremos una matriz de dispersi\u00f3n por parejas para comprobar las relaciones entre ellas.\n\nComo podemos ver, la variable clase se diferencia tambi\u00e9n en nuestra gr\u00e1fica, siendo el color naranja el diagn\u00f3stico `M`, y su hom\u00f3logo azul el diagn\u00f3stico `B`:","c04e19bb":"Ahora realizaremos 3 distintos **Mapas de Correlaci\u00f3n**. La respuesta a esto es que separaremos los 3 tipos de variables `mean, se y worst` en mapas distintos, para poder visualizar de una manera m\u00e1s clara las correlaciones entre las variables de nuestro conjunto.\n\nComenzamos con el Mapa de Correlaci\u00f3n de las variables de tipo `mean`:","cb9fc5b4":"Es muy \u00fatil disponer del conjunto de datos separado dos subconjuntos, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). Se puede utilizar el siguiente fragmento de c\u00f3digo para dividirlo: ","90bb0867":"Como podemos observar, tenemos una columna, la \u00faltima, cuyo nombre es `Unnamed` y todo el contenido de sus filas `NaN`. Esto significa que antes de continuar trabajando con nuestra base de datos, debemos borrarla.","751b1493":"Ahora realizaremos un an\u00e1lisis multivariado para intentar encontrar correlaciones entre las variables","2e1e4346":"Una vez estudiado el algoritmo `Zero-R` probaremos un nuevo m\u00e9todo, la creaci\u00f3n de un \u00e1rbol de decisi\u00f3n.\n\nPara obtener este arbol de decisi\u00f3n, usaremos el estimador `DecisionTreeClassifier` de `scikit-learn`, sin olvidar fijar la semilla que definimos al principio de la libreta para asegurar que los experimentos sean reproducibles:","c3af816d":"Lo que este algoritmo har\u00e1 ser\u00e1 aprender un clasificador que asigne a los casos del conjunto de test la clase predominante en el conjunto de entrenamiento (ya vimos que el problema era desbalanceado). Como veremos, en nuestro conjunto de datos concreto no destaca por su efectividad.","b38d4947":"Estudiando la matriz de dispersi\u00f3n de las variables predictoras ordenadas por parejas, podemos sacar como conclusi\u00f3n que las variables `perimeter y area` dependen fuertemente de `radius`.\n\nIgualmente, podemos apreciar c\u00f3mo las variables `concavity y compactness` dependen tambi\u00e9n en gran medida de `concave points`.\n\nA continuaci\u00f3n, realizaremos diversas observaciones de estos datos para ver si realmente las dependencias ya mencionadas se ven reflejadas en el conjunto de datos.","f7e5ebfe":"# Wisconsin","edb19dea":"Lo que podemos observar es que las clases de la variable objetivo del problema no tienen el mismo n\u00famero de casos, es decir, el problema est\u00e1 desbalanceado (las frecuencias de las combinaciones de estados no aparecen en la misma proporci\u00f3n).\n\nAhora que sabemos que el problema es **desbalanceado**, es el turno de visualizar las variables predictoras del conjunto de datos.","00460722":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test."}}