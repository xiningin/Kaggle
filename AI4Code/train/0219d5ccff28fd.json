{"cell_type":{"4067a695":"code","889d06f5":"code","04477a45":"code","80448201":"code","fd3c5d3f":"code","bae92467":"code","e908b460":"code","d80aaaac":"code","83a3367f":"code","e53014b7":"code","00be5a9d":"code","68c306d4":"code","4e4e5d38":"code","da03fec4":"code","a455f424":"code","5701d1ec":"code","2b486fd9":"code","03b0f284":"code","b195fcbe":"code","02d9256a":"code","eff71248":"code","16727cb1":"code","9df09f42":"code","35087c64":"code","340a3c90":"code","6cc4ea7e":"code","7cbf2990":"code","82b454c8":"code","727e3014":"code","4286928f":"code","ead36ca6":"code","87280489":"code","96d00239":"code","27559c37":"code","c5263ef1":"code","82cc1ed0":"code","daa565fa":"code","df1cb404":"code","cbcee2b8":"code","cc4d8322":"code","240bb00a":"code","ff8844f9":"code","2d9f5c43":"code","4e2900e3":"code","de587454":"code","cf1e35fd":"code","82014e3e":"code","175fa842":"code","2aeea54e":"code","0e22627e":"code","255d7e28":"code","c6727b07":"code","ee5d6ebb":"code","3a90702d":"code","1aec4365":"code","5750b2b9":"code","89426c6b":"code","2234fcae":"code","ecab0e77":"code","4583d375":"code","9695a597":"code","91ce2177":"code","ee441414":"code","8b8d0091":"code","43498804":"code","6740f01a":"code","6eae9b18":"code","082bb3d8":"code","03a7631c":"code","9bf53776":"code","64b1f0a0":"code","e1992695":"code","cd38b065":"code","ad2bd498":"code","756522ca":"code","6d55d876":"code","86bbcf9a":"code","96c1b581":"code","b8222afb":"code","7f85a504":"code","c0160c4d":"code","d20ad6cd":"code","8fb35e8c":"code","29572a9f":"code","223e38c0":"code","6793beac":"code","1e56421d":"code","76745d85":"code","44bea5d6":"code","5c2f055b":"code","df666bff":"code","1ff167bd":"code","71d80788":"code","c663e679":"code","e8a04a01":"code","33618076":"code","1fee8d39":"code","34ce4b61":"code","6e8a8445":"code","b204cb6d":"code","1889bd75":"code","95010191":"code","ce641c5f":"code","fdec34db":"code","b48e592b":"code","47d70494":"code","98105c8b":"code","85731bd9":"code","6416d366":"code","4c78b850":"code","c210367f":"code","62b08291":"code","2bb1c5d3":"code","0f027e5a":"code","26d737bc":"code","a4c6afe3":"code","b5ed42b0":"code","466b8689":"code","8c5c9e13":"code","f723a130":"code","3a706f94":"code","7c90357c":"code","9cd161b3":"code","5d5e9256":"markdown","aac72e86":"markdown","40f10db7":"markdown","681482f1":"markdown","e6c25bd3":"markdown","df9f6e28":"markdown","4edef629":"markdown","b6bce803":"markdown","bdf1606f":"markdown","69c8f4f2":"markdown","b326efeb":"markdown","06b71686":"markdown","e450769a":"markdown","c579cbf5":"markdown","74c2a793":"markdown","7e458fee":"markdown","809f86ac":"markdown","b64fc1b3":"markdown","346efcc0":"markdown","0f8e69d3":"markdown","eac82c9f":"markdown","c81e357c":"markdown","cfcb6b0a":"markdown","e2e4f04a":"markdown","53a0837d":"markdown","e72bd920":"markdown","aa79be71":"markdown","42b8d676":"markdown","56de489d":"markdown","73cdcb13":"markdown","8f379331":"markdown","95ed4a1d":"markdown","fb894e6b":"markdown","838b8a59":"markdown","159e9703":"markdown","855929e3":"markdown","04c8a2e4":"markdown","c17bca83":"markdown","984e129f":"markdown","29b698a8":"markdown","8f73498b":"markdown","a4c881fa":"markdown","ad87f6b9":"markdown","dcebab49":"markdown","21169381":"markdown","2c81251e":"markdown","e8ffa418":"markdown","055eda75":"markdown","e97b6ce8":"markdown","bce994f1":"markdown"},"source":{"4067a695":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\n","889d06f5":"data=pd.read_csv('..\/input\/heart.csv')","04477a45":"#Now, our data is loaded. We're writing the following snippet to see the loaded data. The purpose here is to see the top five of the loaded data.\n\nprint('Data First 5 Rows Show\\n')\ndata.head()\n","80448201":"print('Data Last 5 Rows Show\\n')\ndata.tail()","fd3c5d3f":"print('Data Show Describe\\n')\ndata.describe()","bae92467":"print('Data Show Info\\n')\ndata.info()","e908b460":"#We will list all the columns for all data. We check all columns. Is there any spelling mistake?\nprint('Data Show Columns:\\n')\ndata.columns","d80aaaac":"data.sample(frac=0.01)","83a3367f":"#sample; random rows in dataset\ndata.sample(5)","e53014b7":"data=data.rename(columns={'age':'Age','sex':'Sex','cp':'Cp','trestbps':'Trestbps','chol':'Chol','fbs':'Fbs','restecg':'Restecg','thalach':'Thalach','exang':'Exang','oldpeak':'Oldpeak','slope':'Slope','ca':'Ca','thal':'Thal','target':'Target'})","00be5a9d":"#New show columns\ndata.columns","68c306d4":"#And, how many rows and columns are there for all data?\nprint('Data Shape Show\\n')\ndata.shape  #first one is rows, other is columns","4e4e5d38":"#Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.\nprint('Data Sum of Null Values \\n')\ndata.isnull().sum()","da03fec4":"#all rows control for null values\ndata.isnull().values.any()","a455f424":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True,fmt='.1f')\nplt.show()","5701d1ec":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\nplt.tight_layout()\nplt.show()","2b486fd9":"sns.pairplot(data)\nplt.show()","03b0f284":"data.Age.value_counts()[:10]\n#data age show value counts for age least 10","b195fcbe":"sns.barplot(x=data.Age.value_counts()[:10].index,y=data.Age.value_counts()[:10].values)\nplt.xlabel('Age')\nplt.ylabel('Age Counter')\nplt.title('Age Analysis System')\nplt.show()","02d9256a":"#firstly find min and max ages\nminAge=min(data.Age)\nmaxAge=max(data.Age)\nmeanAge=data.Age.mean()\nprint('Min Age :',minAge)\nprint('Max Age :',maxAge)\nprint('Mean Age :',meanAge)","eff71248":"young_ages=data[(data.Age>=29)&(data.Age<40)]\nmiddle_ages=data[(data.Age>=40)&(data.Age<55)]\nelderly_ages=data[(data.Age>55)]\nprint('Young Ages :',len(young_ages))\nprint('Middle Ages :',len(middle_ages))\nprint('Elderly Ages :',len(elderly_ages))","16727cb1":"sns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(young_ages),len(middle_ages),len(elderly_ages)])\nplt.xlabel('Age Range')\nplt.ylabel('Age Counts')\nplt.title('Ages State in Dataset')\nplt.show()","9df09f42":"data['AgeRange']=0\nyoungAge_index=data[(data.Age>=29)&(data.Age<40)].index\nmiddleAge_index=data[(data.Age>=40)&(data.Age<55)].index\nelderlyAge_index=data[(data.Age>55)].index","35087c64":"for index in elderlyAge_index:\n    data.loc[index,'AgeRange']=2\n    \nfor index in middleAge_index:\n    data.loc[index,'AgeRange']=1\n\nfor index in youngAge_index:\n    data.loc[index,'AgeRange']=0","340a3c90":"# Draw a categorical scatterplot to show each observation\nsns.swarmplot(x=\"AgeRange\", y=\"Age\",hue='Sex',\n              palette=[\"r\", \"c\", \"y\"], data=data)\nplt.show()","6cc4ea7e":"# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.barplot(y=\"AgeRange\", x=\"Sex\", data=data,\n            label=\"Total\", color=\"b\")\nplt.show()","7cbf2990":"sns.countplot(elderly_ages.Sex)\nplt.title(\"Elderly Sex Operations\")\nplt.show()","82b454c8":"elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum')","727e3014":"sns.barplot(x=elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum').index,y=elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum').values)\nplt.title(\"Gender Group Thalach Show Sum Time\")\nplt.show()","4286928f":"sns.violinplot(data.Age, palette=\"Set3\", bw=.2, cut=1, linewidth=1)\nplt.xticks(rotation=90)\nplt.title(\"Age Rates\")\nplt.show()","ead36ca6":"plt.figure(figsize=(15,7))\nsns.violinplot(x=data.Age,y=data.Target)\nplt.xticks(rotation=90)\nplt.legend()\nplt.title(\"Age & Target System\")\nplt.show()","87280489":"colors = ['blue','green','yellow']\nexplode = [0,0,0.1]\nplt.figure(figsize = (5,5))\n#plt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')\nplt.pie([len(young_ages),len(middle_ages),len(elderly_ages)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')\nplt.title('Age States',color = 'blue',fontsize = 15)\nplt.show()","96d00239":"data.Sex.value_counts()","27559c37":"#Sex (1 = male; 0 = female)\nsns.countplot(data.Sex)\nplt.show()","c5263ef1":"sns.countplot(data.Sex,hue=data.Slope)\nplt.title('Slope & Sex Rates Show')\nplt.show()","82cc1ed0":"total_genders_count=len(data.Sex)\nmale_count=len(data[data['Sex']==1])\nfemale_count=len(data[data['Sex']==0])\nprint('Total Genders :',total_genders_count)\nprint('Male Count    :',male_count)\nprint('Female Count  :',female_count)","daa565fa":"#Percentage ratios\nprint(\"Male State: {:.2f}%\".format((male_count \/ (total_genders_count)*100)))\nprint(\"Female State: {:.2f}%\".format((female_count \/ (total_genders_count)*100)))","df1cb404":"#Male State & target 1 & 0\nmale_andtarget_on=len(data[(data.Sex==1)&(data['Target']==1)])\nmale_andtarget_off=len(data[(data.Sex==1)&(data['Target']==0)])\n####\nsns.barplot(x=['Male Target On','Male Target Off'],y=[male_andtarget_on,male_andtarget_off])\nplt.xlabel('Male and Target State')\nplt.ylabel('Count')\nplt.title('State of the Gender')\nplt.show()","cbcee2b8":"#Female State & target 1 & 0\nfemale_andtarget_on=len(data[(data.Sex==0)&(data['Target']==1)])\nfemale_andtarget_off=len(data[(data.Sex==0)&(data['Target']==0)])\n####\nsns.barplot(x=['Female Target On','Female Target Off'],y=[female_andtarget_on,female_andtarget_off])\nplt.xlabel('Female and Target State')\nplt.ylabel('Count')\nplt.title('State of the Gender')\nplt.show()","cc4d8322":"\n# Plot miles per gallon against horsepower with other semantics\nsns.relplot(x=\"Trestbps\", y=\"Age\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=data)","240bb00a":"data.head()","ff8844f9":"#As seen, there are 4 types of chest pain.\ndata.Cp.value_counts()","2d9f5c43":"sns.countplot(data.Cp)\nplt.xlabel('Chest Type')\nplt.ylabel('Count')\nplt.title('Chest Type vs Count State')\nplt.show()\n#0 status at least\n#1 condition slightly distressed\n#2 condition medium problem\n#3 condition too bad","4e2900e3":"cp_zero_target_zero=len(data[(data.Cp==0)&(data.Target==0)])\ncp_zero_target_one=len(data[(data.Cp==0)&(data.Target==1)])","de587454":"sns.barplot(x=['cp_zero_target_zero','cp_zero_target_one'],y=[cp_zero_target_zero,cp_zero_target_one])\nplt.show()","cf1e35fd":"cp_one_target_zero=len(data[(data.Cp==1)&(data.Target==0)])\ncp_one_target_one=len(data[(data.Cp==1)&(data.Target==1)])","82014e3e":"sns.barplot(x=['cp_one_target_zero','cp_one_target_one'],y=[cp_one_target_zero,cp_one_target_one])\nplt.show()","175fa842":"cp_two_target_zero=len(data[(data.Cp==2)&(data.Target==0)])\ncp_two_target_one=len(data[(data.Cp==2)&(data.Target==1)])","2aeea54e":"sns.barplot(x=['cp_two_target_zero','cp_two_target_one'],y=[cp_two_target_zero,cp_two_target_one])\nplt.show()","0e22627e":"cp_three_target_zero=len(data[(data.Cp==3)&(data.Target==0)])\ncp_three_target_one=len(data[(data.Cp==3)&(data.Target==1)])","255d7e28":"sns.barplot(x=['cp_three_target_zero','cp_three_target_one'],y=[cp_three_target_zero,cp_three_target_one])\nplt.show()","c6727b07":"data.head(1)","ee5d6ebb":"# Show the results of a linear regression within each dataset\nsns.lmplot(x=\"Trestbps\", y=\"Chol\",data=data,hue=\"Cp\")\nplt.show()","3a90702d":"target_0_agerang_0=len(data[(data.Target==0)&(data.AgeRange==0)])\ntarget_1_agerang_0=len(data[(data.Target==1)&(data.AgeRange==0)])","1aec4365":"colors = ['blue','green']\nexplode = [0,0.1]\nplt.figure(figsize = (5,5))\nplt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Young Age ',color = 'blue',fontsize = 15)\nplt.show()","5750b2b9":"target_0_agerang_1=len(data[(data.Target==0)&(data.AgeRange==1)])\ntarget_1_agerang_1=len(data[(data.Target==1)&(data.AgeRange==1)])","89426c6b":"colors = ['blue','green']\nexplode = [0.1,0]\nplt.figure(figsize = (5,5))\nplt.pie([target_0_agerang_1,target_1_agerang_1], explode=explode, labels=['Target 0 Age Range 1','Target 1 Age Range 1'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Middle Age',color = 'blue',fontsize = 15)\nplt.show()","2234fcae":"target_0_agerang_2=len(data[(data.Target==0)&(data.AgeRange==2)])\ntarget_1_agerang_2=len(data[(data.Target==1)&(data.AgeRange==2)])","ecab0e77":"colors = ['blue','green']\nexplode = [0,0.1]\nplt.figure(figsize = (5,5))\nplt.pie([target_0_agerang_2,target_1_agerang_2], explode=explode, labels=['Target 0 Age Range 2','Target 1 Age Range 2'], colors=colors, autopct='%1.1f%%')\nplt.title('Target vs Age Range Elderly Age ',color = 'blue',fontsize = 15)\nplt.show()","4583d375":"data.Thalach.value_counts()[:20]\n#First show 20 rows","9695a597":"sns.barplot(x=data.Thalach.value_counts()[:20].index,y=data.Thalach.value_counts()[:20].values)\nplt.xlabel('Thalach')\nplt.ylabel('Count')\nplt.title('Thalach Counts')\nplt.xticks(rotation=45)\nplt.show()","91ce2177":"sns.swarmplot(x=data.Age)\nplt.title('Age Rates')\nplt.show()","ee441414":"age_unique=sorted(data.Age.unique())\nage_thalach_values=data.groupby('Age')['Thalach'].count().values\nmean_thalach=[]\nfor i,age in enumerate(age_unique):\n    mean_thalach.append(sum(data[data['Age']==age].Thalach)\/age_thalach_values[i])","8b8d0091":"#data_sorted=data.sort_values(by='Age',ascending=True)\nplt.figure(figsize=(10,5))\nsns.pointplot(x=age_unique,y=mean_thalach,color='red',alpha=0.8)\nplt.xlabel('Age',fontsize = 15,color='blue')\nplt.xticks(rotation=45)\nplt.ylabel('Thalach',fontsize = 15,color='blue')\nplt.title('Age vs Thalach',fontsize = 15,color='blue')\nplt.grid()\nplt.show()","43498804":"age_range_thalach=data.groupby('AgeRange')['Thalach'].mean()","6740f01a":"sns.barplot(x=age_range_thalach.index,y=age_range_thalach.values)\nplt.xlabel('Age Range Values')\nplt.ylabel('Maximum Thalach By Age Range')\nplt.title('illustration of the thalach to the age range')\nplt.show()\n#As shown in this graph, this rate decreases as the heart rate \n#is faster and in old age areas.","6eae9b18":"cp_thalach=data.groupby('Cp')['Thalach'].mean()","082bb3d8":"sns.barplot(x=cp_thalach.index,y=cp_thalach.values)\nplt.xlabel('Degree of Chest Pain (Cp)')\nplt.ylabel('Maximum Thalach By Cp Values')\nplt.title('Illustration of thalach to degree of chest pain')\nplt.show()\n#As seen in this graph, it is seen that the heart rate is less \n#when the chest pain is low. But in cases where chest pain is \n#1, it is observed that the area is more. 2 and 3 were found to \n#be of the same degree.","03a7631c":"data.Thal.value_counts()","9bf53776":"sns.countplot(data.Thal)\nplt.show()","64b1f0a0":"data[(data.Thal==0)]\n#as seen, only 50% was understood to be 50% target.","e1992695":"data[(data['Thal']==1)].Target.value_counts()\nsns.barplot(x=data[(data['Thal']==1)].Target.value_counts().index,y=data[(data['Thal']==1)].Target.value_counts().values)\nplt.xlabel('Thal Value')\nplt.ylabel('Count')\nplt.title('Counter for Thal')\nplt.show()","cd38b065":"#Target 1\na=len(data[(data['Target']==1)&(data['Thal']==0)])\nb=len(data[(data['Target']==1)&(data['Thal']==1)])\nc=len(data[(data['Target']==1)&(data['Thal']==2)])\nd=len(data[(data['Target']==1)&(data['Thal']==3)])\nprint('Target 1 Thal 0: ',a)\nprint('Target 1 Thal 1: ',b)\nprint('Target 1 Thal 2: ',c)\nprint('Target 1 Thal 3: ',d)\n\n#so,Apparently, there is a rate at Thal 2.Now, draw graph\nprint('*'*50)\n#Target 0\ne=len(data[(data['Target']==0)&(data['Thal']==0)])\nf=len(data[(data['Target']==0)&(data['Thal']==1)])\ng=len(data[(data['Target']==0)&(data['Thal']==2)])\nh=len(data[(data['Target']==0)&(data['Thal']==3)])\nprint('Target 0 Thal 0: ',e)\nprint('Target 0 Thal 1: ',f)\nprint('Target 0 Thal 2: ',g)\nprint('Target 0 Thal 3: ',h)","ad2bd498":"f,ax=plt.subplots(figsize=(7,7))\nsns.barplot(y=['T 1&0 Th 0','T 1&0 Th 1','T 1&0 Th 2','Ta 1&0 Th 3'],x=[1,6,130,28],color='green',alpha=0.5,label='Target 1 Thal State')\nsns.barplot(y=['T 1&0 Th 0','T 1&0 Th 1','T 1&0 Th 2','Ta 1&0 Th 3'],x=[1,12,36,89],color='red',alpha=0.7,label='Target 0 Thal State')\nax.legend(loc='lower right',frameon=True)\nax.set(xlabel='Target State and Thal Counter',ylabel='Target State and Thal State',title='Target VS Thal')\nplt.xticks(rotation=90)\nplt.show()\n#so, there has been a very nice graphic display. This is the situation that best describes the situation.","756522ca":"data.Target.unique()\n#only two values are shown.\n#A value of 1 is the value of patient 0.","6d55d876":"sns.countplot(data.Target)\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.title('Target Counter 1 & 0')\nplt.show()","86bbcf9a":"sns.countplot(data.Target,hue=data.Sex)\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.title('Target & Sex Counter 1 & 0')\nplt.show()","96c1b581":"#determine the age ranges of patients with and without sickness and make analyzes about them\nage_counter_target_1=[]\nage_counter_target_0=[]\nfor age in data.Age.unique():\n    age_counter_target_1.append(len(data[(data['Age']==age)&(data.Target==1)]))\n    age_counter_target_0.append(len(data[(data['Age']==age)&(data.Target==0)]))\n\n#now, draw show on graph    ","b8222afb":"#Target 1 & 0 show graph on scatter\nplt.scatter(x=data.Age.unique(),y=age_counter_target_1,color='blue',label='Target 1')\nplt.scatter(x=data.Age.unique(),y=age_counter_target_0,color='red',label='Target 0')\nplt.legend(loc='upper right',frameon=True)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Target 0 & Target 1 State')\nplt.show()","7f85a504":"sns.lineplot(x=\"Sex\", y=\"Oldpeak\",\n             hue=\"Target\",data=data)\nplt.show()","c0160c4d":"data.head()","d20ad6cd":"data.head()","8fb35e8c":"g = sns.catplot(x=\"AgeRange\", y=\"Chol\",\n                 hue=\"Sex\",\n                 data=data, kind=\"bar\")\nplt.show()","29572a9f":"ax = sns.barplot(\"Sex\", \"Chol\", data=data,\n                  linewidth=2.5, facecolor=(1, 1, 1, 0),\n                  errcolor=\".2\", edgecolor=\".2\")\nplt.show()","223e38c0":"male_young_t_1=data[(data['Sex']==1)&(data['AgeRange']==0)&(data['Target']==1)]\nmale_middle_t_1=data[(data['Sex']==1)&(data['AgeRange']==1)&(data['Target']==1)]\nmale_elderly_t_1=data[(data['Sex']==1)&(data['AgeRange']==2)&(data['Target']==1)]\nprint(len(male_young_t_1))\nprint(len(male_middle_t_1))\nprint(len(male_elderly_t_1))","6793beac":"f,ax1=plt.subplots(figsize=(20,10))\nsns.pointplot(x=np.arange(len(male_young_t_1)),y=male_young_t_1.Trestbps,color='lime',alpha=0.8,label='Young')\nsns.pointplot(x=np.arange(len(male_middle_t_1)),y=male_middle_t_1.Trestbps,color='black',alpha=0.8,label='Middle')\nsns.pointplot(x=np.arange(len(male_elderly_t_1)),y=male_elderly_t_1.Trestbps,color='red',alpha=0.8,label='Elderly')\nplt.xlabel('Range',fontsize = 15,color='blue')\nplt.xticks(rotation=90)\nplt.legend(loc='upper right',frameon=True)\nplt.ylabel('Trestbps',fontsize = 15,color='blue')\nplt.title('Age Range Values vs Trestbps',fontsize = 20,color='blue')\nplt.grid()\nplt.show()","1e56421d":"data.head()","76745d85":"data_filter_mean=data[(data['Target']==1)&(data['Age']>50)].groupby('Sex')[['Trestbps','Chol','Thalach']].mean()","44bea5d6":"data_filter_mean.unstack()","5c2f055b":"for i,col in enumerate(data.columns.values):\n    plt.subplot(5,3,i+1)\n    plt.scatter([i for i in range(303)],data[col].values.tolist())\n    plt.title(col)\n    fig,ax=plt.gcf(),plt.gca()\n    fig.set_size_inches(10,10)\n    plt.tight_layout()\nplt.show()","df666bff":"#Let's see how the correlation values between them\ndata.corr()","1ff167bd":"dataX=data.drop('Target',axis=1)\ndataY=data['Target']","71d80788":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.2,random_state=42)","c663e679":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","e8a04a01":"#Normalization as the first process\n# Normalize\nX_train=(X_train-np.min(X_train))\/(np.max(X_train)-np.min(X_train)).values\nX_test=(X_test-np.min(X_test))\/(np.max(X_test)-np.min(X_test)).values","33618076":"from sklearn.decomposition import PCA\npca=PCA().fit(X_train)\nprint(pca.explained_variance_ratio_)\nprint()\nprint(X_train.columns.values.tolist())\nprint(pca.components_)","1fee8d39":"cumulative=np.cumsum(pca.explained_variance_ratio_)\nplt.step([i for i in range(len(cumulative))],cumulative)\nplt.show()","34ce4b61":"pca = PCA(n_components=8)\npca.fit(X_train)\nreduced_data_train = pca.transform(X_train)\n#inverse_data = pca.inverse_transform(reduced_data)\nplt.scatter(reduced_data_train[:, 0], reduced_data_train[:, 1], label='reduced')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.show()","6e8a8445":"pca = PCA(n_components=8)\npca.fit(X_test)\nreduced_data_test = pca.transform(X_test)\n#inverse_data = pca.inverse_transform(reduced_data)\nplt.scatter(reduced_data_test[:, 0], reduced_data_test[:, 1], label='reduced')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.show()","b204cb6d":"reduced_data_train = pd.DataFrame(reduced_data_train, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8'])\nreduced_data_test = pd.DataFrame(reduced_data_test, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8'])\nX_train=reduced_data_train\nX_test=reduced_data_test","1889bd75":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \ndef plot_feature_importances(gbm):\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), gbm.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","95010191":"combine_features_list=[\n    ('Dim1','Dim2','Dim3'),\n    ('Dim4','Dim5','Dim5','Dim6'),\n    ('Dim7','Dim8','Dim1'),\n    ('Dim4','Dim8','Dim5')\n]","ce641c5f":"parameters=[\n{\n    'penalty':['l1','l2'],\n    'C':[0.1,0.4,0.5],\n    'random_state':[0]\n    },\n]\n\nfor features in combine_features_list:\n    print(features)\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n    \n    gslog=GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\n    gslog.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gslog.best_params_)\n    print()\n    predictions=[\n    (gslog.predict(X_train_set),y_train,'Train'),\n    (gslog.predict(X_test_set),y_test,'Test'),\n    ]\n    \n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1],pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n\n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=LogisticRegression(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50) \n   ","fdec34db":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l1',random_state=0)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oran\u0131 :',r2_score(y_test,y_pred))\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","b48e592b":"print('CoEf:\\n')\nprint(lr.coef_)\nprint('Intercept_\\n')\nprint(lr.intercept_)\nprint('Proba:\\n')\nprint(lr.predict_log_proba)","47d70494":"parameters=[\n{\n    'n_neighbors':np.arange(2,33),\n    'n_jobs':[2,6]\n    },\n]\nprint(\"*\"*50)\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n   \n    gsknn=GridSearchCV(KNeighborsClassifier(),parameters,scoring='accuracy')\n    gsknn.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gsknn.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gsknn.predict(X_train_set), y_train, 'Train'),\n    (gsknn.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=KNeighborsClassifier(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","98105c8b":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","85731bd9":"n_neighbors = range(1, 17)\ntrain_data_accuracy = []\ntest1_data_accuracy = []\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    knn.fit(X_train, y_train)\n    train_data_accuracy.append(knn.score(X_train, y_train))\n    test1_data_accuracy.append(knn.score(X_test, y_test))\nplt.plot(n_neighbors, train_data_accuracy, label=\"Train Data Set\")\nplt.plot(n_neighbors, test1_data_accuracy, label=\"Test1 Data Set\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Neighbors\")\nplt.legend()\nplt.show()","6416d366":"n_neighbors = range(1, 17)\nk_scores=[]\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    scores=cross_val_score(estimator=knn,X=X_train,y=y_train,cv=12)\n    k_scores.append(scores.mean())\nprint(k_scores)","4c78b850":"plt.plot(n_neighbors,k_scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel(\"Cross-Validated Accurancy\")\nplt.show()","c210367f":"print('Leaf Size :',knn.leaf_size)\nprint('Metric :',knn.metric_params)\nprint('Radius :',knn.radius)\nprint('Weights :',knn.weights)\nprint('Algorithms :',knn.algorithm)","62b08291":"parameters = [\n    {\n        'kernel': ['linear'],\n        'random_state': [2]\n    },\n    {\n        'kernel': ['rbf'],\n        'gamma':[0.9,0.06,0.3],\n        'random_state': [0],\n        'C':[1,2,3,4,5,6],\n        'degree':[2],\n        'probability':[True]\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n  \n    svc = GridSearchCV(SVC(), parameters,\n    scoring='accuracy')\n    svc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(svc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (svc.predict(X_train_set), y_train, 'Train'),\n    (svc.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","2bb1c5d3":"svc=SVC(C=5,degree=2,gamma=0.06,kernel='rbf',probability=True,random_state=0)\nsvc.fit(X_train,y_train)\n\ny_pred=svc.predict(X_test)\n\ny_proba=svc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"SVC TRAIN score with \",format(svc.score(X_train, y_train)))\nprint(\"SVC TEST score with \",format(svc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","0f027e5a":"print('Coef :',svc.coef0)\nprint('Dual Coef_',svc.dual_coef_)\nprint('Fit Status :',svc.fit_status_)\nprint('Gamma :',svc.gamma)\nprint('Kernel :',svc.kernel)\nprint('SV :',svc.support_vectors_)\nprint('Probablity :',svc.probability)\n","26d737bc":"parameters = [\n{\n    'learning_rate': [0.01, 0.02, 0.002],\n    'random_state': [0],\n    'n_estimators': np.arange(3, 20)\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n   \n    gbc = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='accuracy')\n    gbc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(gbc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gbc.predict(X_train_set), y_train, 'Train'),\n    (gbc.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=GradientBoostingClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","a4c6afe3":"gbc=GradientBoostingClassifier(learning_rate=0.02,n_estimators=18,random_state=0)\ngbc.fit(X_train,y_train)\n\ny_pred=gbc.predict(X_test)\n\ny_proba=gbc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"GradientBoostingClassifier TRAIN score with \",format(gbc.score(X_train, y_train)))\nprint(\"GradientBoostingClassifier TEST score with \",format(gbc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","b5ed42b0":"plot_feature_importances(gbc)\nplt.show()","466b8689":"parameters = [\n    {\n        'max_depth': np.arange(1, 10),\n        'min_samples_split': np.arange(2, 5),\n        'random_state': [3],\n        'n_estimators': np.arange(10, 20)\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    tree=GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\n    tree.fit(X_train_set, y_train)\n    \n    print('Best parameters set:')\n    print(tree.best_params_)\n    print(\"*\"*50)\n    predictions = [\n        (tree.predict(X_train_set), y_train, 'Train'),\n        (tree.predict(X_test1_set), y_test, 'Test1')\n    ]\n    \n    for pred in predictions:\n        \n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n    \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=RandomForestClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","8c5c9e13":"rfc=RandomForestClassifier(max_depth=7,min_samples_split=4,n_estimators=19,random_state=3)\nrfc.fit(X_train,y_train)\n\ny_pred=rfc.predict(X_test)\n\ny_proba=rfc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"RandomForestClassifier TRAIN score with \",format(rfc.score(X_train, y_train)))\nprint(\"RandomForestClassifier TEST score with \",format(rfc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","f723a130":"for i in range(1,11):\n    rf = RandomForestClassifier(n_estimators=i, random_state = 3, max_depth=7)\n    rf.fit(X_train, y_train)\n    print(\"TEST set score w\/ \" +str(i)+\" estimators: {:.5}\".format(rf.score(X_test, y_test)))","3a706f94":"plot_feature_importances(rf)\nplt.show()","7c90357c":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='accuracy')\n    \n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=DecisionTreeClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","9cd161b3":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(SVC(), parameters, scoring='accuracy')\n    \n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","5d5e9256":"<p>so,a new <b>feature<\/b> value can be removed from these age ranges will not affect this impact will see in the future.<\/p>","aac72e86":"<h3>HEART DISEASE ANALYSIS<\/h3>\n\n<h3>CONTENT<\/h3>\n\n<ul>\n    <a href='#1'><li>INTRODUCTION<\/li><\/a>\n    <a href='#2'><li>DATASET COLUMNS FEATURE EXPLAIN<\/li><\/a>\n    <a href='#3'><li>INVESTIGATING THE DATA and EXPLORATORY DATA ANALSIS<\/li><\/a>\n        <ul>\n            <a href='#4'><li>One Visualization to Rule Them All<\/li><\/a>\n            <a href='#5'><li>Age Analysis<\/li><\/a>\n            <a href='#6'><li>Sex (Gender) Analysis<\/li><\/a>\n            <a href='#7'><li>Chest Pain Type Analysis<\/li><\/a>\n            <a href='#8'><li>Age Range Analysis<\/li><\/a>\n            <a href='#9'><li>Thalach Analysis<\/li><\/a>\n            <a href='#10'><li>Thal Analysis<\/li><\/a>\n            <a href='#11'><li>Target Analysis<\/li><\/a>\n        <\/ul>\n    <a href='#12'><li>MODEL, TRAINING and TESTING<\/li><\/a>\n        <ul>\n            <a href='#13'><li>Logistic Regression<\/li><\/a>\n            <a href='#14'><li>K-Nearest Neighbors<\/li><\/a>\n             <a href='#15'><li>Naive Bayes<\/li><\/a>\n             <a href='#16'><li>Decision Tree<\/li><\/a>\n             <a href='#17'><li>Random Forest<\/li><\/a>\n             <a href='#18'><li>Gradient Boosting Machine<\/li><\/a>\n             <a href='#19'><li>Kernelized SVM<\/li><\/a>\n        <\/ul>\n    <a href='#20'><li>CONCLUSION<\/li><\/a>\n    <a href='#21'><li>REFERENCES<\/li><\/a> \n<\/ul>\n\n![](http:\/\/)<p>last updated : <b>05.07.2019<\/b><\/p>\n<p><h2>If you like it, please upvote.<\/h2><\/p>\n","40f10db7":"<p id='20'><h3><b>References<\/b><\/h3><\/p>\n<p>https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams<\/p>\n<p>https:\/\/seaborn.pydata.org\/<\/p>\n<p>https:\/\/www.kaggle.com\/kanncaa1\/seaborn-tutorial-for-beginners<\/p>\n<p>https:\/\/www.kaggle.com\/biphili\/seaborn-plot-to-visualize-iris-data<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/seaborn-tutorial-for-beginners<\/p>","681482f1":"<p id='3'><h3>INVESTIGATING THE DATA and EXPLORATORY DATA ANALSIS<\/h3><\/p>\n\n<p>First, I install all the libraries that I will use in our application. I install all the libraries in the first part because the algorithms I will use later and the analysis I will make more clearly will be done.Furthurmore, I have investigated the data, presented some visualization and analysed features. Let's write it. I will import necessary Python modules and read the data.<\/p>","e6c25bd3":"<p>The transactions we perform in this section mean an average age. In this part, taking the average of all transactions is performed.<\/p>","df9f6e28":"<p id='5'><h3>Age Analysis<\/h3><\/p>","4edef629":"<p>Another issue I am curious about in this section is the situation in which the value of the target is 1 and it is the maximum value of Thal.<\/p>","b6bce803":"<p>It seems that old people have a very hard job because their values are very high.<\/p>","bdf1606f":"<p id='13'><h1>Logistic Regression<\/h1><\/p>\n<p>First we need parameters to use our data more effectively. Hyperthermatic technique was used for this condition. This technique is used to express different features in the process.<\/p>","69c8f4f2":"<p id='4'><h3>One Visualization to Rule Them All<\/h3><\/p>","b326efeb":"<p id='14'> <h1>K-Nearest Neighbors<\/h1><\/p>","06b71686":"<p id=11><h3>Target Analysis<\/h3><\/p>\n<p>We will analyze this feature for people who are sick or not.<\/p>","e450769a":"<p id=10><h3>Thal Analysis<\/h3><\/p>\n<p>3 = normal; 6 = fixed defect; 7 = reversable defect<\/p>","c579cbf5":"<p id='6'><h3>Sex (Gender) Analysis<\/h3><\/p>","74c2a793":"<p>We have shown the P-value. The purpose of this value is to see the integration values between our data. The scale methods that we will do for this situation will increase this situation even higher.<\/p>","7e458fee":"<p>As is known, most of our data are categorized and not categorized only among certain feature values. We will perform scale operations to eliminate these situations. There are many scale operations for this. These are as follows.<\/p>\n<ul>\n    <li>Z-score<\/li>\n    <li>Normalization<\/li>\n<\/ul>","809f86ac":"<p id='7'><h3>Chest Pain Type Analysis<\/h3><\/p>\n<p>A wide range of chest pain is present in cases of heart failure. These pains will be analyzed according to their problems and age ranges in the analysis system.<\/p>","b64fc1b3":"<p>In the above output, I wish it would be good if there was an equal proportion of people. At the moment it seems to be imbalance value, but for this situation it applies to Gender.<\/p>","346efcc0":"<p><p>The difference in value between our data is too big. It increases both the volume and the results are very bad. To reduce this situation we need to use the StandardScaler function. A value of -1.1 will be obtained after using it.<\/p><\/p>","0f8e69d3":"<p id='2'><h3>DATASET COLUMNS FEATURE EXPLAIN<\/h3><\/p>\n<ul>\n    <li>Age (age in years)<\/li>\n    <li>Sex (1 = male; 0 = female)<\/li>\n    <li>CP (chest pain type)<\/li>\n    <li>TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))<\/li>\n    <li>CHOL (serum cholestoral in mg\/dl)<\/li>\n    <li>FPS (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)<\/li>\n    <li>RESTECH (resting electrocardiographic results)<\/li>\n    <li>THALACH (maximum heart rate achieved)<\/li>\n    <li>EXANG (exercise induced angina (1 = yes; 0 = no))<\/li>\n    <li>OLDPEAK (ST depression induced by exercise relative to rest)<\/li>\n    <li>SLOPE (the slope of the peak exercise ST segment)<\/li>\n    <li>CA (number of major vessels (0-3) colored by flourosopy)<\/li>\n    <li>THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)<\/li>\n    <li>TARGET (1 or 0)<\/li>\n<\/ul>","eac82c9f":"<p id='19'><h1>Kernelized SVM<\/h1><\/p>","c81e357c":"<p>As a result of the above analyzes, it can be seen that 0 cases with chest pain are less common with heart disease. But on the other hand, there are problems in all cases of chest pain, such as 1,2,3.<\/p>","cfcb6b0a":"<p>According to the analysis, Thal values are observed. Of these values, 0 is only 2. 1 showed a distribution of 18. In order to better understand this distribution, we need to take a look at the data set.<\/p>","e2e4f04a":"<p>All correlation values between the data are listed in the previous sections. As a result of this listing, it is aimed to ensure that these properties are used in different places by performing different operations. Thus, the p-value process determines a hypothesis and a hypothesis thesis is presented between each characteristic according to this hypothesis. In this process, after determining the Class property as hypothesis, the relations between all the other properties are checked. This results in a different number for each property. What is important here is that these numbers are not close to 1.00. If the number is close to 1.00 this is very bad.<\/p>","53a0837d":"<p id='1'><h3>INTRODUCTION<\/h3><\/p>\n<p>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).<\/p>\n<p>In addition, we will analyze for this dataset. We will use a wide range of tools for this part. If there's value in there, we'il do it there. Finally, machine learning algorithms are estimated.<\/p>\n\n<p>If you want to get detailed information about python programming you can look at my account.<\/p>\n\n<ul>\n    Python Programming for Beginner :<li>https:\/\/www.kaggle.com\/kralmachine\/python-programming-language-for-data-scientists<\/li>\n<\/ul>\n\n<p>Other kernels: https:\/\/www.kaggle.com\/kralmachine\/kernels<\/p>\n\n","e72bd920":"<p id='8'><h3>Age Range Analysis<\/h3><\/p>\n<p>In this section, age range analysis values are revealed. In this section we have used our new feature values.<\/p>","aa79be71":"<p id='9'><h3>Thalach Analysis<\/h3><\/p>\n<p>The analysis of the system we discussed in the case of maximum heart rate is shown.<\/p>","42b8d676":"<p>In this section, the best analysis can be divided into the <b>elderly,middle-aged, young<\/b> people by looking at the age ranges.<\/p>","56de489d":"<p>In the analysis system, which will be carried out now, an analysis shall be carried out for all persons or their status in the target process.<\/p>","73cdcb13":"<p>As you can see, we are making a single clue of the columns that the data set contains. This is the evaluation for this. Because it will be more effective in future analysis.<\/p>","8f379331":"<p>Now it is time to implement PCA to the data set.<\/p>\n\n<p>But, before that, I will investigate the explained variance ratio and resulting principal components:<\/p>","95ed4a1d":"<p id='16'><h1>Gradient Boosting Machine<\/h1><\/p>","fb894e6b":"<p>The diagonal values we see in these correlation values are always 1.0. That doesn't mean much to us. But the minus value is a big problem for me, but the value of the transactions are more valuable for us.<\/p>","838b8a59":"<p id='18'><h1>Decision Tree<\/h1>\t<\/p>","159e9703":"<p>In this section we will use the groupby function. Our aim here is to obtain the average values of Thalach according to age ranges. Because we're going to do chest pain.<\/p>","855929e3":"<p>Both the <b>head() and tail()<\/b> functions have a value of 5 by default. different values should be given as parameters to change these values.<\/p>","04c8a2e4":"<p>Now, we are uploading our data set to the data variable using the <b>read_csv<\/b> function in the pandas library. <\/p>","c17bca83":"<p>We will perform analysis on the training data. The relationship between the features found in the training data is observed. In this way, comments about the properties can be made\n<\/p>","984e129f":"<p>According to the principal components, data points with greater values on the x-axis represent the customers that are less likely to spend to Detergents_Paper category.<\/p>\n\n<p>Likewise, data points with greater values on the y-axis represent the customers that are less likely to spend to Fresh and Frozen categories.<\/p>\n\n<p>Now, I can constitute a DataFrame out of my reduced data with two dimensions:<\/p>","29b698a8":"<p>Describe function is a function that allows analysis between the numerical values contained in the data set. Using this function count, mean, std, min, max, 25%, 50%, 75%.<\/p>\n<p>As seen in this section, most values are generally categorized. This means that we need to integrate other values into this situation. These; age, trestbps, chol, thalach.<\/p>","8f73498b":"<p id='15'><h1>Naive Baes<\/h1><\/p>","a4c881fa":"<p>In the above analysis, the gender of people who are female is more common. In order to better understand this, we will make a more effective analysis in the following stages.<\/p>","ad87f6b9":"\/\/resim z-score\n\n<ul>\n    <li>X is the incoming data.<\/li>\n    <li>\u03bc is the average value.<\/li>\n    <li>The value of \u03c3 is the standard deviation.<\/li>\n<\/ul>\n\n\/\/sigma resim\n\n<p>Theoretically, it is understood that there is no big difference between standard and normalization. But when it comes to advice and usage, standard scaling comes out with a big difference. The reason for this is that the parsing process between the data is better. In the normalization system, the largest and smallest values within a group of data are considered. All other data is normalized according to these values. The aim here is to normalize the smallest value to 0 and the maximum value to 1, and all other data is a value between 0-1.<\/p>\n\n\/\/X normalization i\u015flemi resim\n\n<p>Of course, we need to remove the target property when doing this.<\/p>","dcebab49":"<p>In this section, the rate of disease is seen less when the gender value is male. This is the result of an analysis for us.<\/p>","21169381":"<p id='17'> <h1>Random Forest<\/h1><\/p>\t","2c81251e":"<p id='21'><h3><b>Conclusion<\/b><\/h3><\/p>\n<p>As a result, we have explained the seaborn library in a very detailed way and created a wide variety of graphs. If you like it, I expect your support. If you like <b>UPVOTED<\/b> I would be very happy if you do. If you have any questions, I am ready to answer your questions. At the bottom there are the kernel values that I have already done.<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/data-visualization-of-suicide-rates<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/gradient-admission-eda-ml-0-92<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/football-results-from-1872-to-2018-datavisulation<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/pandas-tutorial-for-beginner<\/p>\n<p>https:\/\/www.kaggle.com\/kralmachine\/visual-analysis-of-world-happiness-in-2015<\/p>","e8ffa418":"<p id='12'><h3>MODEL, TRAINING and TESTING<\/h3><\/p>\n<p>As a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values.<\/p>","055eda75":"<p>In a data set, the data that are distant from each other are made to scale between each other by making a specific scaling. As a result of this operation, the data takes a value of 0.1. This may change in some scaling operations. Standard and Normalization scale will be used for our operation. There is a big change between the data obtained. Therefore, we need to use this method for SVM algorithm.<\/p>","e97b6ce8":"<p>Now, we are going to analyze both the sex and the heart health situation.<\/p>","bce994f1":"<p>So, I will apply PCA to the data with number of components = 8.<\/p>\n\n<p>The reduced data can be seen on the plotting below.<\/p>"}}