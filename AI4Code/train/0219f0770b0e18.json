{"cell_type":{"f88d3f84":"code","42fe8a1b":"code","17d898f7":"code","d1dabdf4":"code","a65f7ecb":"code","0984a376":"code","8bca7f07":"code","01dc9c2e":"code","8ba77dcb":"code","4ab86ab6":"code","75ffde59":"code","991f4a8f":"code","5a98b07c":"code","b0fd59ee":"code","ca08cfbf":"code","2773daf4":"code","3faa6499":"code","0ea16163":"code","94b653cc":"code","d2cb4186":"code","8d4f1df1":"code","c550ec8d":"code","60c573ec":"code","f1df6c34":"code","1ef7ec9c":"code","7936f59e":"code","0dabbc84":"code","1c3522df":"code","6b174c7c":"code","06588aa8":"code","c8482bc4":"code","32a33639":"code","0fe8ba88":"code","6ed410fd":"code","e41a7ec4":"code","680bfc14":"code","7b59d199":"code","48901edb":"code","837b4c8a":"code","4aa6eced":"code","7b81ea34":"code","a79613ee":"code","2da3eb67":"code","0c2c0528":"code","b47d76f9":"code","8240074b":"code","5d20155c":"code","57a529a6":"code","7889a0c1":"code","476d5905":"code","1c53b430":"code","1c1918b5":"code","1b13bdb4":"markdown","0c7d3a8f":"markdown","881a6cde":"markdown"},"source":{"f88d3f84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42fe8a1b":"import pandas as pd\n\ntrain_df=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","17d898f7":"# let us try automatic EDA by autoviz\nimport pandas_profiling # library for automatic EDA\nfrom autoviz.AutoViz_Class import AutoViz_Class","d1dabdf4":"AV = AutoViz_Class()\nreport=AV.AutoViz(\"\/kaggle\/input\/titanic\/train.csv\")","a65f7ecb":"train_df.head()","0984a376":"train_df.info()","8bca7f07":"train_df.describe()","01dc9c2e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","8ba77dcb":"train_df[\"Survived\"].value_counts()\/len(train_df)","4ab86ab6":"# We are dropping Cabin as it misses more than 50 percent values.\ntrain_data=train_df.drop(\"Cabin\",axis=1)","75ffde59":"train_df[\"Pclass\"].value_counts().sort_values(ascending=False)\/len(train_df)    # most of the passengers are from Pclass 3","991f4a8f":"train_df[\"Sex\"].value_counts()\/len(train_df)          # majority is male","5a98b07c":"train_df[\"Embarked\"].value_counts()\/len(train_df)     # most of them embarked from S","b0fd59ee":"# Add some relevant features from the list of features\ntrain_data[\"Relatives\"]=train_data[\"SibSp\"]+train_data[\"Parch\"]\ntrain_data[[\"Relatives\",\"Survived\"]].groupby([\"Relatives\"]).mean()\n","ca08cfbf":"train_data[\"Fare\"]=pd.cut(train_data[\"Fare\"],bins=[0,150,300,520],labels=[1,2,3])\ntrain_data[\"Fare\"].value_counts()","2773daf4":"train_data[\"Age\"]=pd.cut(train_data[\"Age\"],bins=[0,15,30,45,60,80],labels=[1,2,3,4,5])\ntrain_data[\"Age\"].value_counts()","3faa6499":"# Let's groupby Sex with Survived\ntrain_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"]).mean()   # This proves the fact that more females survived in the mishap","0ea16163":"train_data[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"]).mean()  # This also sounds right.","94b653cc":"train_data=train_data.drop([\"SibSp\",\"Parch\"],axis=1)\ntrain_data.head()","d2cb4186":"from sklearn.base import TransformerMixin,BaseEstimator\n\nclass DataFrameSelector(TransformerMixin,BaseEstimator):\n    def __init__(self,attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        return X[self.attribute_names]","8d4f1df1":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\nnum_pipeline=Pipeline([(\"select_numeric\",DataFrameSelector([\"Relatives\"])),\n                      (\"imputer\",SimpleImputer(strategy=\"median\"))])\n","c550ec8d":"num_pipeline.fit_transform(train_data)","60c573ec":"from sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline=Pipeline([(\"select_cat\",DataFrameSelector([\"Pclass\",\"Age\",\"Sex\",\"Fare\",\"Embarked\"])),\n                      (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),\n                      (\"onehotenc\",OneHotEncoder(sparse=False))])","f1df6c34":"cat_pipeline.fit_transform(train_data)","1ef7ec9c":"from sklearn.pipeline import FeatureUnion\n\ncombined_pipeline=FeatureUnion([(\"num_pipeline\",num_pipeline),(\"cat_pipeline\",cat_pipeline)])","7936f59e":"X_train=combined_pipeline.fit_transform(train_data)","0dabbc84":"y_train=train_data[\"Survived\"]","1c3522df":"# Try some models\nfrom sklearn.svm import SVC\n\nsvc_clf=SVC(kernel='rbf',gamma=\"auto\")\nsvc_clf.fit(X_train,y_train)","6b174c7c":"test_data=test_df.drop(\"Cabin\",axis=1)\n\ntest_data[\"Relatives\"]=test_data[\"SibSp\"]+test_data[\"Parch\"]\ntest_data=test_data.drop([\"SibSp\",\"Parch\"],axis=1)","06588aa8":"test_data[\"Fare\"]=pd.cut(test_data[\"Fare\"],bins=[0,150,300,1000],labels=[1,2,3])\n#test_data[\"Fare\"].value_counts()","c8482bc4":"test_data[\"Fare\"]","32a33639":"test_data[\"Age\"]=pd.cut(test_data[\"Age\"],bins=[0,15,30,45,60,100],labels=[1,2,3,4,5])\n#test_data[\"Age\"].value_counts()","0fe8ba88":"test_data[\"Age\"]","6ed410fd":"test_data.head()","e41a7ec4":"X_test=combined_pipeline.transform(test_data)","680bfc14":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score\n\ny_train_pred=cross_val_predict(svc_clf,X_train,y_train,cv=5)\nsvc_acc=sum(y_train==y_train_pred)\/len(train_data)\nsvc_acc     ","7b59d199":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid={\"n_estimators\":[10,30,100,300,1000],\"max_features\":['auto', 'sqrt', 'log2']}\nforest_clf=RandomForestClassifier(random_state=42)\ngrid_search=GridSearchCV(forest_clf,param_grid,cv=5)\ngrid_search.fit(X_train,y_train)","48901edb":"grid_search.best_score_","837b4c8a":"grid_search.cv_results_[\"mean_test_score\"]","4aa6eced":"grid_search.best_params_","7b81ea34":"from sklearn.neighbors import KNeighborsClassifier\n\nparam_grid_knn={\"n_neighbors\":[3,4,5,6,7],\"weights\":['uniform', 'distance'] }\nknn_clf=KNeighborsClassifier()\ngrid_knn=GridSearchCV(knn_clf,param_grid_knn,cv=5)\ngrid_knn.fit(X_train,y_train)","a79613ee":"grid_knn.best_score_","2da3eb67":"grid_knn.cv_results_[\"mean_test_score\"]","0c2c0528":"grid_knn.best_params_","b47d76f9":"from sklearn.naive_bayes import GaussianNB\nnb_clf=GaussianNB()\ny_train_pred=cross_val_predict(nb_clf,X_train,y_train,cv=5)\nnb_acc=sum(y_train==y_train_pred)\/len(train_data)\nnb_acc                                            # Performs well","8240074b":"from sklearn.linear_model import SGDClassifier\nsgd_clf=SGDClassifier()\ny_train_pred=cross_val_predict(sgd_clf,X_train,y_train,cv=5)\nsgd_acc=sum(y_train==y_train_pred)\/len(train_data)\nsgd_acc                                                 # too bad","5d20155c":"''''from sklearn.svm import SVC\n\nsvc_clf=SVC()\nparam_grid_svc={\"kernel\":['linear'],\"C\":[0.1,1,10,100,1000]}\ngrid_linearSVC=GridSearchCV(svc_clf,param_grid_svc,cv=5)\ngrid_linearSVC.fit(X_train,y_train)'''","57a529a6":"from sklearn.svm import SVC\n# to see if data is linearly or non-linearly separable\nsvc_clf_def=SVC(kernel='linear',C=100)\ny_train_pred=cross_val_predict(svc_clf_def,X_train,y_train,cv=5)\nsvc_lin_acc=sum(y_train==y_train_pred)\/len(train_data)\nsvc_lin_acc     ","7889a0c1":"# Perceptron\nfrom sklearn.linear_model import Perceptron\nperceptron=Perceptron()\ny_train_pred=cross_val_predict(perceptron,X_train,y_train,cv=5)\nperceptron_acc=sum(y_train==y_train_pred)\/len(train_data)\nperceptron_acc ","476d5905":"from sklearn.ensemble import VotingClassifier\n\nclf=VotingClassifier(estimators=[('svc', svc_clf), ('knn',knn_clf), ('sgd', sgd_clf),('rf',forest_clf)], voting='hard')\nclf.fit(X_train,y_train)\ny_train_pred=clf.predict(X_train)\nvoting_acc=sum(y_train==y_train_pred)\/len(train_data)\nvoting_acc","1c53b430":"y_pred=clf.predict(X_test)","1c1918b5":"output = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\noutput.to_csv('submission.csv',index=False)","1b13bdb4":"****Now deploy the model for evaluation of test set.","0c7d3a8f":"****As expected,the voting classifier outplays all the other classifiers in terms of performance.","881a6cde":"Let us do ensemble learning combining all the models."}}