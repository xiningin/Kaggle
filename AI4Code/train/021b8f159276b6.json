{"cell_type":{"99a8d39e":"code","cd312121":"code","e8719e52":"code","4f2f6af0":"code","9e976083":"code","2c72cba5":"code","3da375e5":"code","006730b7":"code","272c3716":"code","e5670ced":"code","68ad732b":"code","87a8290f":"code","34d9cc2a":"code","2eeb80d6":"code","e4245c23":"markdown","d9c90ae2":"markdown","3bebdfa4":"markdown","3d1b466d":"markdown","9b8fb634":"markdown","6ea3d6b2":"markdown","283ae40a":"markdown","5f45fcf3":"markdown"},"source":{"99a8d39e":"!pip install PySastrawi","cd312121":"import numpy as np\nimport pandas as pd\n\n!ls '..\/input'","e8719e52":"data = pd.read_csv('..\/input\/indonesian-abusive-and-hate-speech-twitter-text\/data.csv', encoding='latin-1')\n\nalay_dict = pd.read_csv('..\/input\/indonesian-abusive-and-hate-speech-twitter-text\/new_kamusalay.csv', encoding='latin-1', header=None)\nalay_dict = alay_dict.rename(columns={0: 'original', \n                                      1: 'replacement'})\n\nid_stopword_dict = pd.read_csv('..\/input\/indonesian-stoplist\/stopwordbahasa.csv', header=None)\nid_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})","4f2f6af0":"print(\"Shape: \", data.shape)\ndata.head(15)","9e976083":"data.HS.value_counts()","2c72cba5":"data.Abusive.value_counts()","3da375e5":"print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\nprint(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)","006730b7":"print(\"Shape: \", alay_dict.shape)\nalay_dict.head(15)","272c3716":"print(\"Shape: \", id_stopword_dict.shape)\nid_stopword_dict.head()\n","e5670ced":"import re\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\ndef lowercase(text):\n    return text.lower()\n\ndef remove_unnecessary_char(text):\n    text = re.sub('\\n',' ',text) # Remove every '\\n'\n    text = re.sub('rt',' ',text) # Remove every retweet symbol\n    text = re.sub('user',' ',text) # Remove every username\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(http?:\/\/[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    return text\n    \ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text\n\nalay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\ndef normalize_alay(text):\n    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n\ndef remove_stopword(text):\n    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = text.strip()\n    return text\n\ndef stemming(text):\n    return stemmer.stem(text)\n\nprint(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\nprint(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\nprint(\"stemming: \", stemming(\"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"))\nprint(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe\"))\nprint(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))\nprint(\"remove_stopword: \", remove_stopword(\"ada hehe adalah huhu yang hehe\"))","68ad732b":"def preprocess(text):\n    text = lowercase(text) # 1\n    text = remove_nonaplhanumeric(text) # 2\n    text = remove_unnecessary_char(text) # 2\n    text = normalize_alay(text) # 3\n    text = stemming(text) # 4\n    text = remove_stopword(text) # 5\n    return text","87a8290f":"data['Tweet'] = data['Tweet'].apply(preprocess)","34d9cc2a":"print(\"Shape: \", data.shape)\ndata.head(15)","2eeb80d6":"data.to_csv('preprocessed_indonesian_toxic_tweet.csv', index=False)","e4245c23":"### ID Stopword","d9c90ae2":"# Load data","3bebdfa4":"# Preprocessing the Indonesian Hate & Abusive Text \nThe original paper [1] preprocess the data in 5 steps:\n1. Lower casing all text, \n2. Data cleaning by removing unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation\n3. Normalization using 'Alay' dictionary \n4. Stemming using PySastrawi [2]\n5. Stop words removal using list from [3]","3d1b466d":"# Preprocess","9b8fb634":"# References\n\n[1] Muhammad Okky Ibrohim and Indra Budi. 2019. Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter. In ALW3: 3rd Workshop on Abusive Language Online, 46-57.   \n[2] https:\/\/github.com\/har07\/PySastrawi\n[3] Tala, F. Z. (2003). A Study of Stemming Effects on Information Retrieval in Bahasa Indonesia. M.Sc. Thesis. Master of Logic Project. Institute for Logic, Language and Computation. Universiteit van Amsterdam, The Netherlands.  ","6ea3d6b2":"### Alay Dict","283ae40a":"# Save Preprocessed Data","5f45fcf3":"### Text Data"}}