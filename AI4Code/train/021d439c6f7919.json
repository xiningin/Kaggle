{"cell_type":{"5c4f21c6":"code","3f3674e8":"code","0cb22109":"code","dd3dc452":"code","b5847753":"code","466de34d":"code","27b4edaa":"code","2f1cc2ec":"code","580e615b":"code","a0277384":"code","64794a7e":"code","93711786":"code","38777d49":"code","c9cf0eb6":"code","ac4c89f0":"code","c42e7bf7":"code","c0387454":"code","4ab52cb5":"code","7196df22":"code","2d4453a3":"code","71458e2c":"code","23c7b673":"code","76665285":"code","290adaba":"code","fb303357":"code","36619fdb":"code","71c4a90f":"code","25e9c7a5":"code","3b2a4790":"code","fb26cd61":"code","e6a30e29":"code","042c7e5f":"code","2cc8303a":"code","90392a5c":"code","de214d36":"code","27ba2b51":"code","9b7f0cc9":"markdown","6cb9a7d9":"markdown","b72d3821":"markdown","bf116a2b":"markdown","bd1608e2":"markdown","ee3eb444":"markdown","74f5de7d":"markdown","db8dc0ef":"markdown","30f8406b":"markdown","7c816930":"markdown","a70a6533":"markdown","5d30c3ec":"markdown","47dec05d":"markdown","ee050512":"markdown","22947d27":"markdown","de51cbe0":"markdown","6f50c08f":"markdown","13885eb2":"markdown","fc129ae3":"markdown","b3066852":"markdown","42ea578a":"markdown"},"source":{"5c4f21c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f3674e8":"! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidiaapex\/repository\/NVIDIA-apex-39e153a","0cb22109":"import torch\nimport torch.nn as nn\nfrom torchvision.models import resnet50, resnet34\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nfrom torchvision import transforms\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom matplotlib.pyplot import figure\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\nfrom albumentations.torch import ToTensor\nfrom tqdm._tqdm_notebook import tqdm_notebook\nfrom sklearn.metrics import confusion_matrix\n\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport sys\nfrom sklearn.model_selection import train_test_split\nimport collections\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image, ImageFile\nfrom apex import amp","dd3dc452":"from albumentations import (\n    PadIfNeeded,\n    HorizontalFlip,\n    VerticalFlip,    \n    CenterCrop,    \n    Crop,\n    Compose,\n    Transpose,\n    RandomRotate90,\n    ElasticTransform,\n    GridDistortion, \n    OpticalDistortion,\n    RandomSizedCrop,\n    OneOf,\n    CLAHE,\n    RandomBrightnessContrast,    \n    RandomGamma    \n)","b5847753":"device = torch.device(\"cuda\" if torch.cuda.is_available() \n                                  else \"cpu\")","466de34d":"\nSEED = 2098\nEPOCH = 5\n\n","27b4edaa":"def seed_all(seed= SEED):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_all(SEED)","2f1cc2ec":"image_ids = pd.read_csv(\"..\/input\/trainsiimacr\/train_siimacr\/train-rle.csv\")\nimage_ids.head()","580e615b":"image_ids['LenMask'] =  image_ids[' EncodedPixels'].apply(lambda x: len(x))\nimage_ids['Mask'] = np.where(image_ids['LenMask'] == 3 , 0,1)\nimage_ids.head()","a0277384":"train , val =  train_test_split(image_ids ,test_size=0.15, random_state=SEED ,stratify =image_ids.Mask)\nval.shape","64794a7e":"def get_image(path, id_code, size,Aug):\n    img_path = path\n    \n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if Aug:\n        image = RandCenterCrop(image)\n        image = RandomBrightContrast(image)\n    image = cv2.resize(image, (size, size))\n    image = transforms.ToPILImage()(image)\n    return image","93711786":"def rle2mask(rle, width, height):\n    mask= np.zeros(width* height)\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        current_position += start\n        mask[current_position:current_position+lengths[index]] = 1\n        current_position += lengths[index]\n\n    return mask.reshape(width, height)","38777d49":"class ImageDataLoader(torch.utils.data.Dataset):\n\n    def __init__(self,train_file, image_dir , size =256,transform=None,Aug = False):\n        \n        self.df = train_file \n        self.image_dir = image_dir\n        self.size = size\n        self.name_frame = train_file['ImageId'].values\n        self.label_frame = train_file['Mask'].values\n        self.transform = transform\n        self.Aug = Aug\n        self.image_info = collections.defaultdict(dict)\n        \n        \n        counter = 0\n        for index, row in tqdm(self.df.iterrows(), total=len(self.df)):\n            image_id = row['ImageId']\n            image_path = os.path.join(self.image_dir, image_id)\n            if os.path.exists(image_path + '.png') :\n                self.image_info[counter][\"image_id\"] = image_id\n                self.image_info[counter][\"image_path\"] = image_path\n                self.image_info[counter][\"annotations\"] = row[\" EncodedPixels\"].strip()\n                counter += 1\n        \n\n    def __len__(self):\n        return len(self.name_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.image_dir, self.name_frame[idx]+str(\".png\"))\n        label = self.label_frame[idx]\n        image = get_image(img_name, idx, self.size,self.Aug)\n        width, height = image.size\n        \n        info = self.image_info[idx]\n        if info[\"annotations\"] != '-1':\n            mask = rle2mask(info['annotations'], width, height)\n            mask = Image.fromarray(mask.T)\n            mask = np.expand_dims(mask, axis=0)\n            masks = torch.as_tensor(mask, dtype=torch.float)\n           \n            \n        else:\n            mask = rle2mask('0 {}'.format(width*height), width, height)\n            mask = Image.fromarray(mask.T)\n            mask = np.expand_dims(mask, axis=0)\n            masks = torch.as_tensor(mask, dtype=torch.float)\n        \n        if self.transform:\n            image = self.transform(image);\n            \n                                \n        \n        return {'image': image,\n                'labels': label,\n                'Masks' : masks\n                }\n        ","c9cf0eb6":"IMAGE_TRAIN_DIR = \"..\/input\/siim-png-images\/input\/train_png\/\"\nDF_PATH = \"..\/input\/trainsiimacr\/train_siimacr\/train-rle.csv\"","ac4c89f0":"data_transf = transforms.Compose([transforms.ToTensor()])","c42e7bf7":"dataset = ImageDataLoader(train_file = image_ids ,image_dir = IMAGE_TRAIN_DIR  , size = 1024 , transform = data_transf)\nitem = dataset[2] # get some sample\nimage = item['image']\nmask = item['Masks']\n","c0387454":"mask = mask.numpy().squeeze()\nimage = transforms.ToPILImage()(image)\nimage = np.array(image)","4ab52cb5":"def visualize(image, mask, original_image=None, original_mask=None):\n    fontsize = 18\n    \n    if original_image is None and original_mask is None:\n        f, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n        ax[0].imshow(image)\n        ax[1].imshow(mask)\n    else:\n        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n\n        ax[0, 0].imshow(original_image)\n        ax[0, 0].set_title('Original image', fontsize=fontsize)\n        \n        ax[1, 0].imshow(original_mask)\n        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n        \n        ax[0, 1].imshow(image)\n        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n        \n        ax[1, 1].imshow(mask)\n        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)","7196df22":"visualize(image, mask)","2d4453a3":"def RandHorizontalFlip(image,mask = None,Prob = 1):\n    aug = HorizontalFlip(p=Prob)\n    augmented = aug(image=image, mask=mask)\n    image = augmented['image']\n    if mask is None : \n        return image \n    else : \n       \n        mask = augmented['mask']\n        return image,mask\n        \nimage_Hflip,mask_Hflip =  RandHorizontalFlip(image,mask,Prob = 1)\nvisualize(image_Hflip, mask_Hflip, original_image=image, original_mask=mask)","71458e2c":"def RandVerticalFlip(image,mask = None,Prob = 1):\n    aug = VerticalFlip(p=Prob)\n    augmented = aug(image=image, mask=mask)\n    image = augmented['image']\n    if mask is None : \n        return image \n    else : \n       \n        mask = augmented['mask']\n        return image,mask\n        \nimage_Vflip,mask_Vflip =  RandVerticalFlip(image,mask,Prob = 1)\nvisualize(image_Vflip, mask_Vflip, original_image=image, original_mask=mask)\n","23c7b673":"\ndef RandElasticTransform(image,mask = None,Prob = 1):\n    aug = ElasticTransform(p=1, alpha=150, sigma=120 * 0.05, alpha_affine=120 * 0.03)\n    augmented = aug(image=image, mask=mask)\n    image_elast = augmented['image']\n    if mask is None : \n        return image_elast \n    else : \n       \n        mask_elast = augmented['mask']\n        return image_elast,mask_elast\n        \nimage_elast,mask_elast =  RandElasticTransform(image,mask,Prob = 1)\nvisualize(image_elast, mask_elast, original_image=image, original_mask=mask)","76665285":"\ndef RandGridDistortion(image,mask = None,Prob = 1):\n    aug = GridDistortion(p=Prob,distort_limit=0.35)\n    \n    augmented = aug(image=image, mask=mask)\n    image_grid = augmented['image']\n    if mask is None : \n        return image_grid \n    else : \n       \n        mask_grid = augmented['mask']\n        return image_grid,mask_grid\n        \nimage_grid,mask_grid =  RandGridDistortion(image,mask,Prob = 1)\nvisualize(image_grid, mask_grid, original_image=image, original_mask=mask)","290adaba":"def RandRotate90(image,mask = None,Prob = 1):\n    aug = RandomRotate90(p=Prob)\n    \n    augmented = aug(image=image, mask=mask)\n    image_grid = augmented['image']\n    if mask is None : \n        return image_grid \n    else : \n       \n        mask_grid = augmented['mask']\n        return image_grid,mask_grid\n        \nimage_RR,mask_RR =  RandRotate90(image,mask,Prob = 1)\nvisualize(image_RR, mask_RR, original_image=image, original_mask=mask)\n\n\n","fb303357":"!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet","36619fdb":"model = EfficientNet.from_pretrained('efficientnet-b4') \n\n\n\nin_features = model._fc.in_features\n\n\nmodel.avg_pool = nn.AdaptiveAvgPool2d(1)\nmodel._fc = nn.Sequential(\n                          nn.BatchNorm1d(in_features, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n                          nn.Dropout(p=0.25),\n                          nn.Linear(in_features=in_features, out_features=in_features, bias=True),\n                          nn.ReLU(),\n                          nn.BatchNorm1d(in_features, eps=0.001, momentum=0.01, affine=True, track_running_stats=True),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(in_features=in_features, out_features=1, bias=True),\n                         )\nmodel = model.to(device)","71c4a90f":"def get_image(path, id_code, size,Aug):\n    img_path = path\n    \n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if Aug:\n        image = RandGridDistortion(image,mask = None,Prob = 0.4)\n        image = RandElasticTransform(image,mask = None,Prob = 0.4)\n        image = RandRotate90(image,mask = None,Prob = 0.4)\n        image = RandHorizontalFlip(image,mask = None,Prob = 0.4)\n        \n    image = cv2.resize(image, (size, size))\n    image = transforms.ToPILImage()(image)\n    return image","25e9c7a5":"class ImageDataLoaderClassifier(torch.utils.data.Dataset):\n\n    def __init__(self,train_file,root_dir,size =256,transform=None,Aug = False):\n        \n        self.root_dir = root_dir\n        self.size = size\n        self.name_frame = train_file['ImageId'].values\n        self.label_frame = train_file['Mask'].values\n        self.transform = transform\n        self.Aug = Aug\n\n    def __len__(self):\n        return len(self.name_frame)\n\n    def __getitem__(self, idx):\n        \n       \n       \n        img_name = os.path.join(self.root_dir, self.name_frame[idx]+str(\".png\"))\n        label = self.label_frame[idx]\n        \n        image = get_image(img_name, idx, self.size,self.Aug)\n                 \n              \n        \n        if self.transform:\n            image = self.transform(image);\n                                  \n        \n        return {'image': image,\n                'labels': label\n                }\n          \n","3b2a4790":"data_transf = transforms.Compose([transforms.ToTensor()])\nbatch_valid_size = 1\nbatch_size = 16\ntrain_dataset = ImageDataLoaderClassifier(train_file = train  ,root_dir = IMAGE_TRAIN_DIR, size = 512 , transform = data_transf,Aug = True)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nvalid_dataset = ImageDataLoaderClassifier(train_file = val  ,root_dir = IMAGE_TRAIN_DIR, size = 512 , transform = data_transf,Aug = False)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_valid_size, shuffle=False, num_workers=4,drop_last =True)\n","fb26cd61":"lr          = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\ncriterion = nn.BCEWithLogitsLoss()\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)","e6a30e29":"def train_model(epoch):\n    model.train() \n        \n    avg_loss = 0.\n    optimizer.zero_grad()\n    tk0 = tqdm_notebook(train_loader, total=int(len(train_loader)))\n        \n    for i, items in enumerate(tk0):\n        \n      imgs = items[\"image\"]\n      labels = items[\"labels\"].view(-1, 1)\n      imgs_train, labels_train = imgs.cuda(), labels.float().cuda()\n      output_train = model(imgs_train)\n      loss = criterion(output_train,labels_train)\n       \n      with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n         \n      optimizer.step() \n      optimizer.zero_grad() \n      avg_loss += loss.item() \/ len(train_loader)\n      tk0.set_postfix(loss= loss.item() \/ (batch_size))\n      global_loss = loss\n        \n      \n    return avg_loss\n\n\n\n\n\ndef test_model():\n    \n    avg_val_loss = 0.\n    model.eval()\n    with torch.no_grad():\n      tk0 = tqdm_notebook(valid_loader, total=int(len(valid_loader)))\n      for i, items in enumerate(tk0):\n        imgs = items[\"image\"]\n        labels = items[\"labels\"].view(-1, 1)\n        imgs_vaild, labels_vaild = imgs.cuda(), labels.float().cuda()\n        output_test = model(imgs_vaild)\n        avg_val_loss += criterion(output_test, labels_vaild).item() \/ len(valid_loader)\n        tk0.set_postfix(loss= criterion(output_test, labels_vaild).item() \/ len(valid_loader))       \n        \n    return avg_val_loss\n","042c7e5f":"agg_train_loss = []\nagg_val_loss = []\n\nnum_epochs = EPOCH\nbest_avg_loss = 100.0\n\nglobal_loss = []\n\n\n\nbatch_loss = [] \n\nseed = SEED\nseed_all(seed)\nfor epoch in range(num_epochs):\n        \n        \n    avg_loss     = train_model(epoch)\n    agg_train_loss.append(avg_loss)\n    avg_val_loss = test_model()\n    agg_val_loss.append(avg_val_loss)\n    \n    print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    print('lr:', scheduler.get_lr()[0]) \n       \n    print('Training Loss: {:.4f}'.format(avg_loss))\n    print('Validiation  Loss: {:.4f}'.format(avg_val_loss))\n    \n    scheduler.step()\n    if avg_val_loss < best_avg_loss:\n        best_avg_loss = avg_val_loss\n        print(\"epoch is:\",epoch,\"nodel saved\")\n        torch.save(model.state_dict(), 'weight_bestf' + str(epoch) + '.pt')\n        \n","2cc8303a":"figure(num=None, figsize=(15, 12), dpi=100, facecolor='w', edgecolor='k')\nplt.plot(agg_train_loss)\nplt.plot(agg_val_loss)\n#plt.xticks(np.arange(0, len(batch_loss), batch_size)) \nplt.legend(['y = agg_train_loss', 'y = agg_val_loss'], loc='upper left')\nplt.show()","90392a5c":"for param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","de214d36":"print((batch_size)) \npredict = np.zeros((batch_size)*len(valid_loader))\ntargets = np.zeros((batch_size)*len(valid_loader))\nwith torch.no_grad():\n  tk0 = tqdm_notebook(valid_loader, total=int(len(valid_loader)))\n  for i, items in enumerate(tk0):\n    imgs = items[\"image\"]\n    labels = items[\"labels\"].view(-1, 1)\n    imgs_vaild, labels_vaild = imgs.cuda(), labels.float().cuda()\n    output_test = model(imgs_vaild)\n    predict[i*batch_size:(i+1)*batch_size] = output_test.cpu().detach().numpy().flatten()\n    targets[i*batch_size:(i+1)*batch_size] = labels_vaild.cpu().detach().numpy().flatten()\n    tk0.set_postfix(loss= criterion(output_test, labels_vaild).item() \/ len(valid_loader))       \n        ","27ba2b51":"confusion_matrix(targets, predict> 0.5)","9b7f0cc9":"## Load Meta Data ","6cb9a7d9":"## Grid Distortion","b72d3821":"## Albumentations","bf116a2b":"## Predict ","bd1608e2":"## Train","ee3eb444":"## Global Variables ","74f5de7d":"## Data loader ","db8dc0ef":"## Create Labels\nCreate Mask label  - 1 if Mask exists, 0 if not ","30f8406b":"## Rle2mask","7c816930":"## RandomRotate90 ","a70a6533":"## Split Meta Data \nSplit Data to Train and Validation sets ","5d30c3ec":"## Random Seed ","47dec05d":"This kernel is a simple classification with augmentation examples (based on the Albumentations library)\n\nA classifier can be used as part of the entire model :\n\n *  (as a pre-processing before doing the segmentation or as a post-processing as a helper to decide if there is no mask)\n\nI have not tried this method yet, but you can try it.\n\nYou can also see the use of the Nvidia driver, It is using more efficiently the GPU memory. I think it also provides faster training \n\nThe kernel is divided into two \nFirst part augmentation illustration for both images and masks \nSecond part classification with augmentation \n\n\nCredits :  \n  For the Triple Grand Master - Abhishek Thakur  \n  ( https:\/\/www.kaggle.com\/abhishek\/mask-rcnn-with-augmentation-and-multiple-masks ) , As always learned a lot from him - the Data loader is coming from his kernels.\n  Thanks for also loading and allowing the use of the data \n   https:\/\/www.kaggle.com\/abhishek\/siim-png-images\n   \n   Augmentation is based on this Github \n   https:\/\/github.com\/albu\/albumentations\/blob\/master\/notebooks\/example_kaggle_salt.ipynb\n   ","ee050512":"## Visualization ","22947d27":"Define the Loader again without the masks and with augmentation  ","de51cbe0":"Credit Here for This kernel (for the Data loading) - made by Abhishek Thakur\nhttps:\/\/www.kaggle.com\/abhishek\/mask-rcnn-with-augmentation-and-multiple-masks","6f50c08f":"## HorizontalFlip","13885eb2":"## Vertical Flip","fc129ae3":"## Model","b3066852":"## Plot the loss ","42ea578a":"## Elastic Transform"}}