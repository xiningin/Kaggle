{"cell_type":{"99cf8871":"code","1c3f5303":"code","357ce682":"code","0b236770":"code","363300fa":"code","d4c66ef7":"code","1563b0a1":"code","43f8bc8b":"code","be29e141":"code","7279ff4d":"code","cf173c43":"code","f031a5b8":"code","ca626444":"code","7d3c0940":"code","81f2f707":"code","75f86ae6":"code","8b60a38e":"code","7bc19d7c":"code","19976a65":"code","af049ffb":"code","ab9d6ae4":"code","3012c2f1":"code","ae843123":"code","4666b3f9":"code","fb21ef6d":"code","778cd7a0":"code","78f4ee1f":"code","bb499955":"code","306f08c3":"code","1d6fd13c":"code","753f3cea":"code","a40fdaa6":"code","fccd264c":"code","3b0cc7ab":"code","9d0d5f74":"code","b3877f9c":"code","ed3cca9a":"code","818bbc46":"code","42db3a83":"code","e74bad7c":"code","6b0e3217":"code","7490f955":"code","e1f8dddf":"code","1c5d79fe":"code","b9ece668":"code","18f1b8b7":"markdown","655d4f47":"markdown","00f382a3":"markdown","f1115c99":"markdown","f0b93646":"markdown","eebbdc4a":"markdown","261cb02b":"markdown","6e135e38":"markdown","c0d827b7":"markdown","1054141f":"markdown","6571ac1f":"markdown","691b5752":"markdown","85eeb945":"markdown","11bbe54d":"markdown","2bbec387":"markdown","d6a8ae4e":"markdown","08462dc0":"markdown","92b0baff":"markdown","863dd00f":"markdown","56239aa0":"markdown","781f4dd6":"markdown","f25cb0fb":"markdown","d0dc41d6":"markdown","a44507a0":"markdown","5bc962e7":"markdown","709517e8":"markdown","251973e4":"markdown","c6a15495":"markdown","35066d14":"markdown","96eee249":"markdown","eb2cc9d5":"markdown","3e271a39":"markdown","2fe3a02f":"markdown"},"source":{"99cf8871":"import numpy as np\nimport pandas as pd\nimport torch \nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom IPython.core.debugger import set_trace","1c3f5303":"df = pd.read_csv('..\/input\/adult.csv')","357ce682":"train_df, valid_df = df[:-2000].copy(),df[-2000:].copy()\n","0b236770":"train_df.head()","363300fa":"train_df.isnull().sum() # check NaN","d4c66ef7":"full_col = list(train_df.columns)\nfull_col.remove('>=50k') # all the independent data","1563b0a1":"full_col","43f8bc8b":"cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\ncat_names # categorical variables","be29e141":"cont_names = [col for col in full_col if col not in cat_names]\ncont_names # continuous variables","7279ff4d":"for n in cat_names:\n    train_df[n] = train_df[n].astype('category').cat.as_ordered()","cf173c43":"train_df.dtypes # data types after categorifying","f031a5b8":"train_df['occupation'].cat.categories","ca626444":"train_df['occupation'].cat.codes","7d3c0940":"train_df['education-num'].median()","81f2f707":"train_df['education-num'].head()","75f86ae6":"for n in cont_names:\n    if pd.isnull(train_df[n]).sum():\n        filler = train_df[n].median()\n        train_df[n] = train_df[n].fillna(filler)","8b60a38e":"train_df['education-num'].head()","7bc19d7c":"from pandas.api.types import is_numeric_dtype, is_categorical_dtype","19976a65":"dep_var = '>=50k' # dependent variable","af049ffb":"if not is_numeric_dtype(df[dep_var]): train_df[dep_var] = train_df[dep_var].cat.codes\ny = torch.tensor(train_df[dep_var].values)","ab9d6ae4":"y","3012c2f1":"train_df['sex'].cat.codes.values","ae843123":"if cat_names and len(cat_names) >= 1: # categorical data\n    cats = np.stack([c.cat.codes.values for n,c in train_df[cat_names].items()], 1) + 1","4666b3f9":"cats.shape","fb21ef6d":"cats = torch.LongTensor(cats.astype(np.int64))","778cd7a0":"cont_names # continuous data","78f4ee1f":"if cont_names and len(cont_names) >= 1:\n    conts = np.stack([c.astype('float32').values for n,c in train_df[cont_names].items()], 1)\n    means, stds = (conts.mean(0), conts.std(0))\n    conts = (conts - means[None]) \/ stds[None]\n    stats = means,stds","bb499955":"conts = torch.FloatTensor(conts)","306f08c3":"bs = 64 #\nxb_cont = conts[0:bs]\nxb_cat = cats[0:bs]\nyb = y[:bs]","1d6fd13c":"cat_szs = [len(train_df[n].cat.categories)+1 for n in cat_names]\nemb_szs = [(c, min(50, (c+1)\/\/2)) for c in cat_szs]\nemb_szs","753f3cea":"def bn_drop_lin(n_in, n_out, bn, p, actn):\n    \"`n_in`->bn->dropout->linear(`n_in`,`n_out`)->`actn`\"\n    layers = [nn.BatchNorm1d(n_in)] if bn else []\n    if p != 0: layers.append(nn.Dropout(p))\n    layers.append(nn.Linear(n_in, n_out))\n    if actn is not None: layers.append(actn)\n    return layers","a40fdaa6":"class TabularModel(nn.Module):\n    \"Basic model for tabular data\"\n    \n    def __init__(self, emb_szs, n_cont, out_sz, layers, drops, \n                 emb_drop, use_bn, is_reg, is_multi):\n        super().__init__()\n        \n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n        layers = []\n        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+drops,actns)):\n            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n        self.layers = nn.Sequential(*layers)\n    \n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont) # why batch norm here ??\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        x = self.layers(x)\n        return x.squeeze()","fccd264c":"model = TabularModel(emb_szs, len(cont_names), 2, [200,100], [0.001,0.01], emb_drop=0.04, is_reg=False,is_multi=True, use_bn=True)","3b0cc7ab":"model","9d0d5f74":"model(xb_cat, xb_cont) # Test if model works","b3877f9c":"def accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds==yb).float().mean()","ed3cca9a":"loss_func = F.cross_entropy ","818bbc46":"loss_func(model(xb_cat, xb_cont), yb)","42db3a83":"accuracy(model(xb_cat, xb_cont), yb)","e74bad7c":"opt = optim.SGD(model.parameters(), lr=1e-2)","6b0e3217":"epochs = 15\n","7490f955":"n,c = cats.shape # number of sample and categorical variables","e1f8dddf":"for epoch in range(epochs):\n    for i in range((n-1)\/\/bs + 1):\n        start_i = i*bs\n        end_i = start_i+bs\n        xb_cont = conts[start_i:end_i]\n        xb_cat = cats[start_i:end_i]\n        yb = y[start_i:end_i]\n        pred = model(xb_cat, xb_cont)\n        loss = loss_func(model(xb_cat, xb_cont), yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()","1c5d79fe":"loss_func(model(xb_cat, xb_cont), yb)","b9ece668":"accuracy(model(xb_cat, xb_cont), yb)","18f1b8b7":"The rule of thumb for determining the embedding size is the cardinality size divided by 2, but no bigger than 50.","655d4f47":"To deal with missing data in continuous variables, we can fill it with its median","00f382a3":"To understand deeply how the model works, I recommend to use `set_trace` to enter the debug mode and run each line to play with the variables. I also did it to feel comfortable with the code","f1115c99":"### Optim","f0b93646":"Create a batch test data","eebbdc4a":"# Reverse Tabular Module of Fast.ai v1","261cb02b":"## Code","6e135e38":"NaN value in categorical data is converted to -1","c0d827b7":"Now each value of Day of Week is represented by a 4x1 vector. This vector will be feed to the input linear layer above","1054141f":"Tabular Module in fast.ai is used to solve problem with tabular data set or structured data set. Structured data set, unlike unstructured data set (Image, ...), each columns have different types of data. It can be text, number, ... continous data or categorical data. You can find the cell with `train_df.head()`, an example of Unstructured data.","6571ac1f":"As categorical can be text, we need to map it to some number. Pandas has category type which is very useful to categorify data","691b5752":"### Training","85eeb945":"If each column of the dataset is continous, for example: humidity, temperature, .... We can directly use a Linear Layers neural networks as below (the arrow represented fully connecter linear layer)","11bbe54d":"In the following section, I will solve a binary classification with tabular dataset and rewrite the tabular module with as least code as possible to understand the core concept\n\nThe problem is to predict whether a person have salary higher than 50k based on several structured information: age, education, occupation, relationship, ...","2bbec387":"#### Convert all data to torch tensor to used with pytorch","d6a8ae4e":"### Categorify categorical data","08462dc0":"### Tabular Model","92b0baff":"#### Embedding size","863dd00f":"### Entity Embedding","56239aa0":"In this notebook I will try to rewrite the Tabular module of Fast.ai based on fast.ai library with minimum code and as much as explanation as possible, inspired from the 001a_nn_basics notebook. This is to help the learner to understand the core concept of how to use deep learning with tabular data set. Reader is recommended to read the [001a_nn_basics](https:\/\/github.com\/fastai\/fastai_docs\/blob\/master\/dev_nb\/001a_nn_basics.ipynb) notebook first to get familiar with pytorch","781f4dd6":"We will make a Tabular Module class here base on fast.ai library","f25cb0fb":"We can see that the loss drop significantly  and the accuracy increase ","d0dc41d6":"But how to deal with categorical data ? From lesson 4 part 1 in fast.ai 2018, Jeremy said: \n\n`Numbers like Year , Month, although we could treat them as continuous, we do not have to. If we decide to make Year a categorical variable, we are telling our neural net that for every different \u201clevel\u201dof Year (2000, 2001, 2002), you can treat it totally differently; where-else if we say it is continuous, it has to come up with some kind of smooth function to fit them. So often things that actually are continuous but do not have many distinct levels (e.g. Year, DayOfWeek), it often works better to treat them as categorical.`\n\nYou can represent the categorical data with one-hot coding. It is a sparse vector with an only unique 1 and 0 else where. But it will make our input layer extreamly long, and increase the complexity. It also means that each categorical values are independant but sometime it is not true. For example, Saturday and Sunday should be closer to each other than Saturday and Thursday.\n\nWe can use entity embedding technique to solve the problem with categorical data. ","a44507a0":"![](https:\/\/i.imgur.com\/naHAacr.png)","5bc962e7":"*arguments*:\n- emb_szs: the embedding size of categorical data will created above\n- n_cont: number of continuous variables\n- out_sz: size of the output, 2 in our case which is a binary classification\n- layers: linear layer follow the input layer\n- drops: drop out percentage\n- emb_drop: drop out percentage at embedding layers\n- use_bn; use batch normalization\n- is_reg: is regression\n- is multi: is classification","709517e8":"## Getting data and preprocessing","251973e4":"For continuous data with need to normalize it to means zero and standard deviation 1 as below","c6a15495":"### Fill median - dealing with missing data in continuous variable","35066d14":"![](https:\/\/i.imgur.com\/DQDWjVl.png)","96eee249":"### What is Tabular Module in fast.ai ?","eb2cc9d5":"the following function is to simplify the repeadly sequence of layer \"`n_in`->bn->dropout->linear(`n_in`,`n_out`)->`actn`\"","3e271a39":"### Loss Function and Metrics","2fe3a02f":"![](https:\/\/i.imgur.com\/H3kEs0k.png)"}}