{"cell_type":{"ce343e99":"code","9c97d8db":"code","0c268ef2":"code","f489b58f":"code","1de9039c":"code","ad2737a2":"code","de4e31ab":"code","cdf6a577":"code","0cea0515":"code","9493c8d4":"code","a1c91401":"code","68050174":"code","164b4714":"code","def386bb":"code","06925658":"code","d43c6317":"code","be68edc0":"code","0f82694e":"code","95413b81":"code","dce5a728":"code","bbe6c2aa":"code","605b96cf":"code","17be4f89":"code","2a7d2a5d":"code","31c1007b":"code","d18b65ca":"code","d4f34345":"code","b60f3477":"code","61bbd2a2":"code","104f7b98":"code","73662a88":"code","bfe20006":"code","1551bbe6":"code","832c321d":"code","4c97bf03":"code","13ca8dc2":"code","78ac6477":"code","319a3a0c":"code","61452147":"code","4c32998a":"code","194d0462":"code","21712282":"code","f05f25ed":"code","c694e8af":"code","95edd10f":"code","48f83016":"code","7cdda67a":"code","dfb841da":"code","ec8a1b96":"code","691c239c":"code","4b12e5f3":"code","cb9897b3":"code","f72c1830":"code","a7d2cd38":"code","a0d703e9":"code","68035291":"code","c05dc935":"code","13e3f27a":"code","5a6e99a2":"code","31fc558f":"code","b06539e5":"code","b7436bfd":"code","5d4c97d5":"code","7ebb0d2d":"code","4a64c111":"code","5e26c867":"code","41d5403c":"code","aa914b0e":"code","2f5b7c5f":"code","1e11cc0b":"code","c5af03d3":"code","32900c62":"code","c2625fe8":"code","c770365f":"code","450b11b2":"code","4fcb9803":"code","31dabb7f":"code","2396ab13":"code","65f22247":"code","09e04c84":"code","6b90b1d2":"code","fa78a0d5":"code","4932baad":"code","3701abe6":"code","228885eb":"code","1bf42961":"code","178d60aa":"code","c623bd62":"code","7140c5dd":"code","89e1b455":"code","c7e83a4f":"code","198a906a":"code","c0c400d3":"code","fd08c664":"code","b9d203b4":"code","8891f3b8":"code","72967521":"code","50d2a7e3":"code","32b92999":"code","2395ac9f":"code","d8508900":"code","a7ccb236":"code","b08d16c3":"code","d833b093":"code","6da6ba74":"code","c9123526":"code","204edb42":"code","555f1e12":"code","5075dff0":"code","f3dce615":"code","a5decf06":"code","288d1db3":"markdown","c8594045":"markdown","ff68cbb4":"markdown","6e3a2480":"markdown","23f33be8":"markdown","2d19379f":"markdown","9728f976":"markdown","776337ac":"markdown","69df7218":"markdown","5cfc7564":"markdown","6c952e8d":"markdown","7b6aa71a":"markdown","d8a3e6c0":"markdown","46ea28db":"markdown","8a976676":"markdown","39742d90":"markdown","ee426170":"markdown","6fed0d60":"markdown","cf77823a":"markdown","a9071bc2":"markdown","a094424b":"markdown","ae1eb46f":"markdown","6dd3e7ff":"markdown","2b5bf1b0":"markdown","f8df5491":"markdown","0d64328d":"markdown","0c67c0e5":"markdown","18ff12d1":"markdown","7baec3d5":"markdown","bb7bf0e7":"markdown","e47d43d5":"markdown","ea238b97":"markdown","3dd55917":"markdown","520d8f62":"markdown","a09e1b9f":"markdown","9145b20a":"markdown","e861da70":"markdown","1ca8018c":"markdown","4a7570ce":"markdown","14a35ef2":"markdown","76ffbd54":"markdown","87dc9879":"markdown","d1cdafc8":"markdown"},"source":{"ce343e99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9c97d8db":"import re\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\nfrom gensim.models import Word2Vec\nfrom gensim.models.phrases import Phraser,Phrases\nfrom time import time\nimport multiprocessing\nfrom gensim.matutils import Dense2Corpus\nfrom gensim.similarities import MatrixSimilarity\nfrom gensim.models.doc2vec import Doc2Vec,TaggedDocument\nfrom gensim.models import LdaModel,KeyedVectors\nimport umap\nimport matplotlib.pyplot as plt\nfrom gensim.corpora import Dictionary","0c268ef2":"df = pd.read_json('..\/input\/arxivdataset\/arxivData.json',orient='records')\ndf.head()","f489b58f":"df2 = pd.DataFrame(df.author.str.split('}').tolist(),index = df.index).stack()\ndf2.head()","1de9039c":"def rem_unwanted(line):\n    return re.sub(\"\\'term'|\\'rel'|\\'href'|\\'type'|\\'title'|\\[|\\{|\\'name'|\\'|\\]|\\,|\\}\",'',line).strip(' ').strip(\"''\").strip(\":\")","ad2737a2":"df2 = pd.DataFrame(df2.apply(rem_unwanted))","de4e31ab":"df2 = pd.DataFrame(df2.unstack().iloc[:,0:2].to_records()).drop(columns={'index'})","cdf6a577":"df2.columns = ['Author1','Author2']","0cea0515":"df2.Author1 = df2.Author1.str.strip(' ')\ndf2.Author2 = df2.Author2.str.strip(' ')","9493c8d4":"df2[df2.Author2 == '']","a1c91401":"df2 = df2.reset_index().drop(columns='index')","68050174":"df2.head()","164b4714":"len(df2.Author1.unique())","def386bb":"df3 = pd.DataFrame(df.link.str.split(', ').tolist(),index = df.index).stack()","06925658":"df3 = pd.DataFrame(df3.apply(rem_unwanted,convert_dtype=True))","d43c6317":"df3.head()","be68edc0":"df3 = df3.unstack()","0f82694e":"links = df3.iloc[:,[1,4]]\nlinks.columns = ['textLink','pdfLink']\nlinks.head()","95413b81":"tags = pd.DataFrame(df['tag'].str.split(',').tolist())\n# tags = tags[tags.str.contains('term')]\n# tags.head()\ntags = tags.iloc[:,[0,3,6]].stack()\n# tags = tags.iloc[:,0]","dce5a728":"tags.head()","bbe6c2aa":"tags = tags.apply(rem_unwanted)","605b96cf":"tags = tags.unstack()","17be4f89":"tags[0] = tags[0].str.strip()\ntags.iloc[:,1] = tags.iloc[:,1].str.strip()\ntags.iloc[:,2] = tags.iloc[:,2].str.strip()","2a7d2a5d":"tags.columns = ['Topic1','Topic2','Topic3']","31c1007b":"tags.head()","d18b65ca":"pre0 = pd.merge(df,tags,how = 'inner',left_index=True,right_index=True).drop('tag',axis=1)\npre = pd.merge(pre0,df2,how = 'inner',left_index=True,right_index=True).drop('author',axis=1)\ndata = pd.merge(pre,links,how = 'inner',left_index=True,right_index=True).drop('link',axis=1)","d4f34345":"def rem_bracket(line):\n#     return re.sub(\"\\'term'|\\'rel'|\\'href'|\\'type'|\\'title'|\\[|\\{|\\'name'|\\'|\\]|\\)}\",'',line).strip(' ').strip(\"''\").strip(\":\")\n    return line.strip(')')","b60f3477":"df.head()","61bbd2a2":"data.head()","104f7b98":"tags = pd.read_csv('..\/input\/arxivtagsdescription\/tags.txt',sep='\/n',header=None,engine='python')","73662a88":"tags.head()","bfe20006":"tags.tail()","1551bbe6":"d1 = pd.DataFrame(tags.iloc[[i for i in tags.index if i%2==0]].reset_index().iloc[0:47][0].str.split(' - ').tolist())\nd2 = pd.DataFrame(tags.iloc[[i for i in tags.index if i%2==0]].reset_index().iloc[47:][0].str.split('(').tolist())","832c321d":"d1.head()","4c97bf03":"d2.head()","13ca8dc2":"d2[1] = d2[1].apply(rem_bracket)","78ac6477":"d2 = d2.set_index([1]).reset_index()","319a3a0c":"d2.columns = [0,1]","61452147":"d2.head()","4c32998a":"d3 = pd.concat([d1,d2])","194d0462":"d3 = d3.reset_index().drop(columns=['index'])","21712282":"d3['TopicExplain'] = tags.iloc[[i for i in tags.index if i%2!=0]][0].reset_index()[0]","f05f25ed":"d3.columns = ['Topic','FullTopic','TopicExplain']","c694e8af":"tags = d3.copy()","95edd10f":"tags.head()","48f83016":"data.fillna(' ',inplace=True)","7cdda67a":"data.isna().any()","dfb841da":"data['summary'][0]","ec8a1b96":"data['title'][0]","691c239c":"def rem_n(line):\n    return re.sub('\\\\n',' ',line)","4b12e5f3":"data['summary'] = data['summary'].apply(rem_n)\ndata['title'] = data['title'].apply(rem_n)","cb9897b3":"data['summary'][0]","f72c1830":"db1 = pd.merge(data,tags,how='left',left_on='Topic1',right_on='Topic').drop(columns=['Topic'])\ndb2 = pd.merge(db1,tags,how='left',left_on='Topic2',right_on='Topic').drop(columns=['Topic','TopicExplain_y'])\ndb3 = pd.merge(db2,tags,how='left',left_on='Topic3',right_on='Topic').drop(columns=['Topic','TopicExplain'])","a7d2cd38":"db3 = db3[['id', 'summary', 'title', 'year', 'FullTopic_x', 'FullTopic_y', 'FullTopic','TopicExplain_x', 'Topic1', 'Topic2', 'Topic3','Author1', 'Author2', 'textLink', 'pdfLink']]","a0d703e9":"db3.columns = ['id', 'summary', 'title', 'year','Topic1',\n       'Topic2', 'Topic3', 'Topic', 'DTopic1', 'DTopic2', 'DTopic3',\n       'Author1', 'Author2', 'textLink', 'pdfLink' ]","68035291":"db3.drop(columns=['DTopic1','DTopic2','DTopic3'],inplace=True)","c05dc935":"f1 = db3.copy()","13e3f27a":"f1.fillna(' ',inplace=True)","5a6e99a2":"f1.isna().sum()","31fc558f":"nlp = spacy.load('en',disable = ['ner','parser'])\nspacy.require_gpu()","b06539e5":"stopwords = list(STOP_WORDS)+list((''.join(string.punctuation)).strip(''))+['-pron-','-PRON-']\nlen(stopwords)","b7436bfd":"def lemmatizer(df):\n    texts = []\n    c=0\n    for text in df:\n        if c%1000==0:\n            print(\"Processed articles: \",c)\n        c+=1\n        doc = nlp(text)\n        lemma = [word.lemma_.lower().strip('') for word in doc]\n        words = [word for word in lemma if word not in stopwords]\n        texts.append(' '.join(words))\n    return pd.Series(texts)","5d4c97d5":"f1['Full'] = (f1['title']+\" \"+f1['summary']+' '+f1['Topic1']+' '+f1['Topic2']+' '+f1['Topic3']+' '+f1['Topic']+' '+f1['Author1']+' '+f1['Author2'])","7ebb0d2d":"t = time()\nprocessed_text = lemmatizer(f1['Full'])","4a64c111":"(time()-t)\/60","5e26c867":"processed_text.index = f1['id'].values","41d5403c":"processed_text = pd.DataFrame(processed_text)","aa914b0e":"processed_text.iloc[0:12].index","2f5b7c5f":"processed_text.iloc[:6].values","1e11cc0b":"phr = [i[0].split() for i in processed_text.values]","c5af03d3":"phrases = Phrases(phr,min_count=50,progress_per=1000)","32900c62":"bigram = Phraser(phrases)","c2625fe8":"sentences = bigram[phr]","c770365f":"phrases = Phrases(sentences,min_count=25,progress_per=1000)","450b11b2":"trigram = Phraser(phrases)","4fcb9803":"trigrams = trigram[sentences]","31dabb7f":"trigrams[123]","2396ab13":"tagged_data = [TaggedDocument(words=' '.join(i),tags=[j]) for i, j in zip(trigrams,processed_text.index)]","65f22247":"tagged_data[2]","09e04c84":"docmodel = Doc2Vec(dm=1,vector_size = 300,window=2,workers=4,negative=5,min_count=10, dbow_words=1)","6b90b1d2":"docmodel.build_vocab(tagged_data)","fa78a0d5":"docmodel.corpus_total_words","4932baad":"t = time()\ndocmodel.train(tagged_data,total_examples=docmodel.corpus_count,epochs=20)","3701abe6":"(time()-t)\/60","228885eb":"docmodel.init_sims(replace=True)\n# docmodel.save('model')","1bf42961":"docmodel = Doc2Vec.load('model')","178d60aa":" docmodel.docvecs.most_similar('1802.00209v1')","c623bd62":"# f2 = f1[['id','title','summary','FullTopic','TopicExplain','Author1','Author2']]","7140c5dd":"def get_recommendations(*n):\n    j = docmodel.docvecs.most_similar(positive=n)\n    r = f1[f1['id'].isin(list(n))]\n    p = ['Searched',]*len(r)\n    for i in j:\n        r = pd.concat([r,f1[f1['id']==i[0]]])\n        p.append(i[1])\n#     r = f2[f2['id'].isin(a)]\n    r['ProbabilityChance'] = p\n    return r","89e1b455":"rec = get_recommendations('1802.00209v1')\n# ('1305.3814v2')","c7e83a4f":"rec","198a906a":"mat = TfidfVectorizer().fit_transform(lemmatizer(rec['Full']))","c0c400d3":"(cosine_similarity(mat,mat)*100)[:,0]","fd08c664":"!yes Y | conda install faiss-gpu cudatoolkit=10.0 -c pytorch\n!wget https:\/\/anaconda.org\/CannyLab\/tsnecuda\/2.1.0\/download\/linux-64\/tsnecuda-2.1.0-cuda100.tar.bz2\n!tar xvjf tsnecuda-2.1.0-cuda100.tar.bz2 --wildcards 'lib\/*'\n!tar xvjf tsnecuda-2.1.0-cuda100.tar.bz2 --wildcards 'site-packages\/*'\n!cp -r site-packages\/* \/opt\/conda\/lib\/python3.6\/site-packages\/\n!cp \/kaggle\/working\/lib\/libfaiss.so \/usr\/local\/cuda\/lib64\/\n!apt search openblas\n!yes Y | apt install libopenblas-dev","b9d203b4":"!pip install tsnecuda","8891f3b8":"from tsnecuda import TSNE","72967521":"# from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n# from functions import make_tsne_subset","50d2a7e3":"doc_tags = docmodel.docvecs.doctags.keys()","32b92999":"X = docmodel[doc_tags]","2395ac9f":"print(X[0:5])","d8508900":"tSNE = TSNE(n_components=2)","a7ccb236":"X_tsne = tSNE.fit_transform(X)","b08d16c3":"df = pd.DataFrame(X_tsne, index=doc_tags, columns=['x', 'y'])","d833b093":"df.head()","6da6ba74":"plt.scatter(df['x'], df['y'], s=0.4, alpha=0.4)","c9123526":"def sub_tsne(topic):\n    ai = f1[f1['Topic1']==topic]['id'].values\n    return df[df.index.isin(ai)]","204edb42":"subplot = sub_tsne('Artificial Intelligence')","555f1e12":"plt.scatter(subplot['x'],subplot['y'],s=0.4, alpha=0.4)","5075dff0":"subplot = sub_tsne('Computation and Language')","f3dce615":"plt.scatter(subplot['x'],subplot['y'],s=0.4, alpha=0.4)","a5decf06":"tags[tags['FullTopic']=='Mathematical Software']['TopicExplain'].values","288d1db3":"> Creating Tagged data for Doc2Vec (tag = articleId)","c8594045":"As we can see there is uniform overlapping and separation takes place which is a good sign.","ff68cbb4":"Making Stop words and puctuations i.e. are, this, is, was etc.","6e3a2480":"Cleaning author names :","23f33be8":"As we can see similarity between recommendation and real article is very good.","2d19379f":"No empty\/Nan values. Good to go further.","9728f976":"> Similarity in Recommended Articles","776337ac":"> Loading spacy for NLP and tokenising.","69df7218":"> Cleaned Topics","5cfc7564":"Combining all cleaned data and their topics.","6c952e8d":"We can see *\/n* as impurity.","7b6aa71a":"Min count means words frequency, if a two or three words appears more than X times simultaneously then that words combined will be counted as bigrams and trigrams respectively.\nProgress_pre simply means batch size.","d8a3e6c0":"Takes 15 minutes to process the whole data.","46ea28db":"# ***Scholarly Articles Recommender Engine Using Doc2Vec***","8a976676":"* Web application based on flask is live on https:\/\/deviantpadam.pythonanywhere.com\/\n* Source Code is availabe on https:\/\/github.com\/DeviantPadam\/rec_system","39742d90":"**Here is the recommendations for id 1802.00209v1**\n* First one is 1802.00209v1 then recommendations are there with probability.","ee426170":"Lets check cluster seperation topic wise.","6fed0d60":"Merging all of them with original dataframe.","cf77823a":"* dm = 1 means using skip-grams\n* vector_size = 300, word embeddings will have shape of (vocab_size,300)\n* window = 2, model will try to predict every second word.\n* workers = 4, number of processors available for parallel processing.\n* negative = 5, 5% of noise words will be removed.\n* min_count = 10, words lower than frequency of 10 will be ignored.\n","a9071bc2":"> Creating Bigrams and Trigrams","a094424b":"Some have abbreviated topics at beginning and some have abbreviated topics at last.","ae1eb46f":"Installing GPU version of tSNE (for faster computation)","6dd3e7ff":"Cleaning links to obtain links of pdf and text version.","2b5bf1b0":"Model has overlapping which is due to some topics have vast number (like Artificial Intelligence is primary or secondary subjects of many articles) of articles but still there are separation between articles. Further tuning in model can introduce more separation.","f8df5491":"> **Data Cleaning**","0d64328d":"> Concat all text for tokenising","0c67c0e5":"# **Suggestions are heartily welcome.**","18ff12d1":"> Cleaning the topics text file","7baec3d5":"One more","bb7bf0e7":"> #### I have tried to make a recommendation engine using Doc2Vec using ArXiv research papers meta-data dataset and text of tags descreption is taken from https:\/\/arxiv.org\/archive\/cs.\n> #### Suggestions are most welcomed that can improve the recommendations.","e47d43d5":"### ***Some Drawbacks - ***\n* ### As I chose window size very less because many topics contains very little text. See below that topic contains literally no data so prediction will be hard.","ea238b97":"> Initial Data","3dd55917":"Authors' name is cleaned","520d8f62":"These are word embeddings.","a09e1b9f":"As we can see above there are many unwanted symbols and many columns have unwanted data.","9145b20a":"Fit and transform a t-SNE object with the vector data for dimensionality reduction and get a idea about cluster formation.","e861da70":"But checking relative articles are good or not is a difficult task so we will try to get more intuition on checking accuracy of recommendations","1ca8018c":"Training for 20 epochs.","4a7570ce":"These are the ids.","14a35ef2":"Cleaning topics to get subjects of articles:","76ffbd54":"Tokeniser function:","87dc9879":"> Cleaned Data","d1cdafc8":"As we can see some good bigrams and trigrams."}}