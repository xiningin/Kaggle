{"cell_type":{"1252917d":"code","e88994f0":"code","fd77064d":"code","0ddcb358":"code","757c6fc8":"code","131e8f1c":"code","aff90cfa":"code","5ad675e9":"code","2fb12e4f":"code","56d88ab0":"code","a9071380":"code","c9d01c2d":"code","4d19afad":"code","b78a6d36":"code","7325da4b":"code","f28282f5":"code","7226ca16":"code","fcfaf820":"code","22717b52":"code","05a9ea4c":"code","7683f7c3":"code","e7ef3c14":"code","4a3ec1e5":"code","7c411bcc":"code","c34400df":"code","76b7447d":"code","6033771e":"code","ff5fbab8":"code","aaed88a7":"code","4a9da96f":"code","764a1d4e":"code","54fab762":"code","d78f7326":"code","a16f2a21":"code","a5ac1d3a":"code","1feacd5e":"code","dc93a5b9":"code","deb2a835":"code","4decc148":"code","1fedbf6e":"markdown","d20818ef":"markdown","d55eeccd":"markdown","14d08177":"markdown","eb21e013":"markdown","3ee93f49":"markdown","71b491d9":"markdown","6fa8a98f":"markdown","83d5ad54":"markdown","1acf7659":"markdown","c3aff743":"markdown","79b7ec56":"markdown","bc9cace0":"markdown","97ff3253":"markdown","18b507f9":"markdown","d4d4dfd8":"markdown","0790a542":"markdown","0386d278":"markdown","56050746":"markdown","878de2e4":"markdown","12bb8ed8":"markdown"},"source":{"1252917d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e88994f0":"\ndf = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.head()","fd77064d":"#let's check the size of our data\ndf.shape","0ddcb358":"df.info()","757c6fc8":"df.describe(include = 'all')","131e8f1c":"#let's deal with the missing values first\nnull_data = pd.DataFrame(df.isna().sum().sort_values(ascending=False)).reset_index().rename(columns = {\"index\":\"columns\", 0:\"missing_values\"})\nnull_data[\"missing_percentage\"] = null_data[\"missing_values\"] \/ len(df) * 100\nnull_data[\"missing_percentage\"] = null_data[\"missing_percentage\"].round(2)\nnull_data","aff90cfa":"#### let's plot the missing values\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5ad675e9":"plt.figure(figsize=(20,5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"columns\", y=\"missing_percentage\", data = null_data)","2fb12e4f":"#Dealing with the missing value\ndf.bmi = df.bmi.fillna(df.bmi.mode()[0])\ndf.bmi.isna().sum()","56d88ab0":"#Let's first see the gender impact on stroke\ngender_df = pd.DataFrame(df.gender.value_counts()).reset_index().rename(columns = {\"index\":\"Gender\", \"gender\":\"count_of_gender\"})\ngender_df","a9071380":"df.head()","c9d01c2d":"#Count plot of gender\nax = sns.countplot(x=\"gender\", data= df, palette=\"tab10\", hue = \"gender\")\nax.set(xlabel=\"Gender\", ylabel=\"Count of specific gender\")\nplt.title(\"Count plot for visualizing gender\")\n\n#Pie Chart of type of gender\nsns.color_palette(\"hls\", 8)\ntype_counts = df.gender.value_counts()\npie, ax = plt.subplots(figsize=[10,6])\nplt.pie(type_counts, labels = type_counts.index,startangle=150, autopct=\"%1.1f%%\", shadow=True, explode=None, colors = ['Grey','Yellow', 'Red'])\nplt.title(\"Pie Chart for gender distribution\", fontsize=14);","4d19afad":"#Now let's try to visualize age and see which age group had more strokes\n#df.age = df.age.astype('int32')\nages_count = df.age.value_counts().sort_values(ascending=False)\nages_count.iloc[:10]","b78a6d36":"len(ages_count)","7325da4b":"#As we can see that we have 83 different age groups, we'll for now just show the top10 ages in which the most stroke has occurred\n\nx_ages = ages_count.index\ny_ages = ages_count\nplt.figure(figsize=(25,8))\nplt.title(\"Ages with most occurred attacks\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of strokes occurred\")\nplt.xticks(rotation = 45)\nsns.barplot(x= x_ages, y = y_ages, palette = 'rocket')\nplt.show()","f28282f5":"age_dff = pd.DataFrame(ages_count.iloc[:10]).reset_index().rename(columns = {\"index\":\"age\",\"age\":\"count\"})\nplt.figure(figsize=(10,5))\nsns.barplot(x=\"age\", y=\"count\",data = age_dff, palette = \"cubehelix\")\nplt.title(\"Top 10 age groups having strokes\")","7226ca16":"#Let's look at the more clearer view\nsns.distplot(ages_count, bins = 10, color='r')\nplt.title(\"Distance plot for ages vs strokes\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count of strokes\")","fcfaf820":"cols = df.columns\nnumerical_cols = []\ncategorical_cols = []\nfor column in cols:\n    if df[column].dtype == 'object':\n        categorical_cols.append(column)\n    else:\n        numerical_cols.append(column)\nprint(numerical_cols, len(numerical_cols), categorical_cols, len(categorical_cols), sep='\\n')","22717b52":"df[numerical_cols].describe()","05a9ea4c":"df[numerical_cols].hist(figsize=(15,7))","7683f7c3":"input_values = df.hypertension.value_counts()\ninput_values","e7ef3c14":"plt.figure(figsize=(7,5))\n\nplt.title(\"Smoking types and hypertension vs heart disease\")\nax = sns.barplot(x='smoking_status', y='heart_disease', hue='hypertension', palette=\"crest\", data=df, ci=None)\nfor patch in ax.patches:\n    clr = patch.get_facecolor()\n    patch.set_edgecolor(clr)\n\nsns.despine()\n# sns.barplot(x=input_values.index, y= input_values, palette = \"mako\")\n","4a3ec1e5":"#As we can see that we have 83 different age groups, we'll for now just show the top10 ages in which the most stroke has occurred\n\nwork_type = df.work_type.value_counts()\n\nx_ages = work_type.index\ny_ages = work_type\nplt.figure(figsize=(10,5))\nplt.title(\"Work Type vs Strokes\")\nplt.xlabel(\"Work Type\")\nplt.ylabel(\"Number of strokes occurred\")\nplt.xticks(rotation = 45)\nsns.barplot(x= x_ages, y = y_ages, palette = 'flare')\nplt.show()\n\n#Pie Chart of type of work\nsns.color_palette(\"hls\", 8)\npie, ax = plt.subplots(figsize=[10,6])\nplt.pie(work_type, labels = work_type.index,startangle=150, autopct=\"%1.1f%%\", shadow=True, explode=None)\nplt.title(\"Pie Chart for work-type\", fontsize=14);\n","7c411bcc":"plt.figure(figsize=(10,5))\n\nplt.title(\"Smoking types and hypertension vs heart disease\")\nax = sns.barplot(x='work_type', y='heart_disease', hue='smoking_status', palette=\"mako\", data=df, ci=None)\nfor patch in ax.patches:\n    clr = patch.get_facecolor()\n    patch.set_edgecolor(clr)\n\nsns.despine()","c34400df":"corr_mat = df.corr()\ncorr_mat","76b7447d":"plt.figure(figsize=(7,5))\nax = sns.heatmap(corr_mat, vmin=0, vmax=1, linewidths=.5, cmap=\"YlGnBu\")","6033771e":"#let's get all object features now\ncategorical_cols","ff5fbab8":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nle= LabelEncoder()\n\nfor col in categorical_cols:\n    df[col] = le.fit_transform(df[col])","aaed88a7":"df.head()","4a9da96f":"#seperating x and y data here\nX= df.drop(['stroke','id', 'bmi'], axis=1)\ny= df.stroke\n","764a1d4e":"scale = StandardScaler()\n\nscale.fit(X)\nx_scale = scale.transform(X)\nX = pd.DataFrame(x_scale, columns=X.columns)\nX.head()","54fab762":"from sklearn.model_selection import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d78f7326":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr = LogisticRegression(random_state=0).fit(X,y)","a16f2a21":"lr_predict = lr.predict(X_test)\nlr_score = lr.score(X_test, y_test)\nprint(lr_score)","a5ac1d3a":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=0).fit(X,y)","1feacd5e":"dtc_predict = dtc.predict(X_test)\ndtc_score = dtc.score(X_test, y_test)\nprint(dtc_score)\nprint(\"Accuracy Score:\",metrics.accuracy_score(y_test, dtc_predict))","dc93a5b9":"from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf.fit(X,y)","deb2a835":"clf_predict = clf.predict(X_test)\nclf_score = clf.score(X_test,y_test)\nprint(clf_score)","4decc148":"accuracy = [lr_score,dtc_score,clf_score]\nmodels = ['LogisticRegression','DecisonTreeClassifier','SVC']\n\nscore_df = pd.DataFrame({'Algorithms': models, 'Accuracy_Score': accuracy})\nscore_df.style.background_gradient(cmap=\"YlGnBu\",high=1,axis=0)","1fedbf6e":"**Results**: The results simply shows that the data has more records of private workers than any other and it doesn't tells anything specific about smoking status, let's explore it further\n","d20818ef":"Only the bmi has some missing values, rest do not have any none values.","d55eeccd":"### Logistic Regression","14d08177":"### Conclusion\nWe can say that we are getting the best results from DecisionTreeClassifier however since he accuracy is 100% it means there's a chance of overfitting here, we can try it with different parameters and different training sizes to see a slightly deviated results or more accurate results","eb21e013":"**Result:** We can see that the records provided shows that more number of Female had stroke then men","3ee93f49":"Note: We can also try other algorithms to achieve the different accuracy scores","71b491d9":"## Modelling","6fa8a98f":"**Results**: Accuracy Score from Decision Tree Classifier is 100%","83d5ad54":"**Results**: The graph shows that the smokers suffers more from hypertension than the rest","1acf7659":"**Results**: The age bracket between 77-79 has the most number of strokes","c3aff743":"It looks like the all the field have some relation with the stroke, we'll try to analyze as much columns as we can before we go for prediction.","79b7ec56":"**Results**: With SVMs, our accurracy score is upto 94%","bc9cace0":"**Results**: Accuracy Score from Logistic Regression is 94%","97ff3253":"### Support Vector Machines (SVM)","18b507f9":"### Decision Tree Classifier","d4d4dfd8":"Let's start with finding correlations and then will proceed with splitting and training the data","0790a542":"## Exploratory Data Analysis","0386d278":"**Results**: The distance plot shows the normal distribution between the ages and the strokes level","56050746":"**Results**: The heatmaps hows that the columns are weekly correlated with each other and only few columns have strong correlation such as age and bmi, age and hypertension, age and heart disease, and and stroke and etc.","878de2e4":"### Let's look at all the numerical data now","12bb8ed8":"**Results**: the self-employeed workers are found to me most smoking and there's a larege quantity of smoking_status that's unknown so it can be higher than what we have."}}