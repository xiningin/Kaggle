{"cell_type":{"6190f589":"code","95b1598e":"code","f2d22ec3":"code","92dd15ef":"code","5289ba26":"code","eacd1310":"code","d6bf9dad":"code","de96b525":"code","06c02f53":"code","b24bff2d":"code","7c0ebf08":"code","b4e09dad":"code","50443a03":"code","5be33ed3":"code","566b0799":"code","d13f9aeb":"code","7f8ec6cc":"code","72cafb32":"code","10d62182":"code","9e5f8e87":"code","f60382c9":"code","0fa38c7f":"code","76dd5a09":"code","c60e3b3a":"code","5e05ed08":"code","d8c11216":"code","1476fd72":"code","a4f5e818":"code","bb224b4b":"code","3458f8d5":"code","cd8ebbdc":"code","59410120":"code","b9949109":"code","9a3e1b61":"code","f9d9c133":"code","f8788f03":"code","f4875438":"code","46527573":"code","9f1fa423":"code","dfa93fae":"code","066a7b01":"code","e986c159":"code","16932681":"code","a856fcdf":"code","776fdccd":"code","74a36d13":"code","2db8659a":"code","d8114583":"code","221d27b8":"code","c5cac20a":"code","22a60743":"code","d02095fb":"code","a50968c6":"code","72a8cf6a":"code","2c4bbac2":"code","b993d895":"code","962d801b":"code","4eaae596":"code","355edad7":"code","bf1c1191":"code","29659202":"code","5c4f6b87":"code","b31d0800":"code","fa18a531":"markdown","21d65434":"markdown","54ad1e0a":"markdown","853f371b":"markdown","2fda201b":"markdown","ecee8d79":"markdown","b966fa0a":"markdown","654d4fd8":"markdown","11143f80":"markdown"},"source":{"6190f589":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport cv2\nimport random\n\nimport tensorflow as tf\nprint(f\"Tensorflow version: {tf.__version__}\")\n\nfrom sklearn.model_selection import train_test_split","95b1598e":"Images_path = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"","f2d22ec3":"train_df = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")\nsample_submission_df = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\")","92dd15ef":"print(\"train_df.shape : \",train_df.shape)\nprint(\"test_df.shape : \",test_df.shape)\nprint(\"sample_submission_df.shape : \",sample_submission_df.shape)","5289ba26":"sample_image = cv2.imread(Images_path+\"Train_0.jpg\")\nprint(\"sample Image shape : \",sample_image.shape)\n# sample_image = cv2.resize(sample_image, (512,256))\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\nplt.imshow(sample_image)\nplt.axis(\"off\")\nplt.show()","eacd1310":"train_df.head()","d6bf9dad":"test_df.head()","de96b525":"sample_submission_df.head()","06c02f53":"train_df['image_id'] = train_df['image_id'].apply(lambda x: x+'.jpg')\ntest_df['image_id'] = test_df['image_id'].apply(lambda x: x+'.jpg')","b24bff2d":"train_df.columns[1:5]","7c0ebf08":"image_shape_x1,image_shape_x2 = (400,400)","b4e09dad":"train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n                                                                    vertical_flip=True,\n                                                                    rotation_range=10,\n                                                                    width_shift_range=0.1,\n                                                                    height_shift_range=0.1,\n                                                                    zoom_range=.1,\n                                                                    fill_mode='nearest',\n                                                                    shear_range=0.1,\n                                                                    rescale=1\/255,\n                                                                    brightness_range=[0.5, 1.5],\n                                                                    validation_split=0.15)\n\ntrain_image_flow_dataframe = train_image_datagen.flow_from_dataframe(dataframe = train_df,\n                                                        directory = Images_path,\n                                                        x_col = 'image_id',\n                                                        y_col = train_df.columns[1:5],\n                                                        subset=\"training\",\n                                                        batch_size=32,\n                                                        shuffle=False,\n                                                        class_mode=\"raw\",\n                                                        target_size=(image_shape_x1,image_shape_x2)\n                                                        )\n\nvalidation_image_flow_dataframe = train_image_datagen.flow_from_dataframe(dataframe = train_df,\n                                                        directory = Images_path,\n                                                        x_col = 'image_id',\n                                                        y_col = train_df.columns[1:5],\n                                                        subset=\"validation\",\n                                                        batch_size=32,\n                                                        shuffle=False,\n                                                        class_mode=\"raw\",\n                                                        target_size=(image_shape_x1,image_shape_x2)\n                                                        )","50443a03":"test_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\n\ntest_image_flow_dataframe = test_image_datagen.flow_from_dataframe(dataframe = test_df,\n                                                                   directory = Images_path,\n                                                                   x_col = 'image_id',\n                                                                   y_col = None,\n                                                                   batch_size=32,\n                                                                   shuffle= False,\n                                                                   class_mode= None,\n                                                                   target_size=(image_shape_x1,image_shape_x2)\n                                                                  )","5be33ed3":"train_input , train_output = next(train_image_flow_dataframe)\n\nprint(\"train input shape(batchwise) : \",train_input.shape)\nprint(\"train output shape(batchwise) : \",train_output.shape)\n\nvalidation_input , validation_output = next(validation_image_flow_dataframe)\n\nprint(\"validation input shape(batchwise) : \",validation_input.shape)\nprint(\"validation output shape(batchwise) : \",validation_output.shape)\n\ntest_input = next(test_image_flow_dataframe)\n\nprint(\"test input shape(batchwise) : \",test_input.shape)","566b0799":"# Visulize Batch\n\ndef show_batch(input_data, output_data):\n    fig, ax = plt.subplots(nrows=input_data.shape[0]\/\/4, ncols=4,figsize=(20,20))\n    plt.axis('off')\n    label = ['healthy', 'multiple_diseases', 'rust', 'scab']\n    for i in range(input_data.shape[0]):\n        ax[i\/\/4,i%4].imshow(input_data[i])\n        ax[i\/\/4,i%4].axis('off')\n        ax[i\/\/4,i%4].set_title([p for p,q in zip(label,list(output_data[i])) if q==1][0])\n    plt.show()","d13f9aeb":"# Train Batch \nshow_batch(train_input, train_output)","7f8ec6cc":"# model Building\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(256,kernel_size=5,padding='same',strides=2,input_shape=[256,512,3]))\nmodel.add(tf.keras.layers.ReLU())\n\nmodel.add(tf.keras.layers.Conv2D(256,kernel_size=5,strides=2))\nmodel.add(tf.keras.layers.ReLU())\n\nmodel.add(tf.keras.layers.Conv2D(512,kernel_size=5,strides=2))\nmodel.add(tf.keras.layers.ReLU())\n\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(512))\nmodel.add(tf.keras.layers.ReLU())\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(256))\nmodel.add(tf.keras.layers.ReLU())\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(128))\nmodel.add(tf.keras.layers.ReLU())\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(4,activation='softmax'))\n\nmodel.compile(tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\",'mean_squared_error'])\n\nmodel.summary()","72cafb32":"LR_START = 0.0001\nLR_MAX = 0.005 \nLR_MIN = 0.0001\nLR_RAMPUP_EPOCHS = 4\nLR_SUSTAIN_EPOCHS = 6\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(20)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","10d62182":"STEP_SIZE_TRAIN=train_image_flow_dataframe.n\/\/train_image_flow_dataframe.batch_size \nSTEP_SIZE_VALID=validation_image_flow_dataframe.n\/\/validation_image_flow_dataframe.batch_size \nSTEP_SIZE_TEST=test_image_flow_dataframe.n\/\/test_image_flow_dataframe.batch_size \n\nprint(\"STEP_SIZE_TRAIN\",STEP_SIZE_TRAIN)\nprint(\"STEP_SIZE_VALID\",STEP_SIZE_VALID)\nprint(\"STEP_SIZE_TEST\",STEP_SIZE_TEST)","9e5f8e87":"H = model.fit_generator(generator=train_image_flow_dataframe,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=validation_image_flow_dataframe,\n                    validation_steps=STEP_SIZE_VALID,\n                    epochs=20)","f60382c9":"loss, acc = model.evaluate_generator(generator=validation_image_flow_dataframe,steps=STEP_SIZE_VALID,verbose=1)","0fa38c7f":"plt.plot(H.history['loss'], label='MAE (testing data)')\nplt.plot(H.history['val_loss'], label='MAE (validation data)')\nplt.title('MAE')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","76dd5a09":"model.save(\"basic_model\")","c60e3b3a":"pred_basic = model.predict_generator(test_image_flow_dataframe,verbose=1)","5e05ed08":"sample_submission_df.loc[:,'healthy':] = pred_basic\nsample_submission_df.to_csv('pred_basic_model.csv', index=False)\nsample_submission_df.head()","d8c11216":"# base models\n\ndef get_base_model(model_name):\n    \n    if model_name == \"ResNet50\":\n        base_model = tf.keras.applications.ResNet50(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n        \n    if model_name == \"ResNet101\":\n        base_model = tf.keras.applications.ResNet101(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n        \n    if model_name == \"ResNet152\":\n        base_model = tf.keras.applications.ResNet152(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n    \n    if model_name == \"DenseNet121\":\n        base_model = tf.keras.applications.DenseNet121(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n        \n    if model_name == \"DenseNet169\":\n        base_model = tf.keras.applications.DenseNet169(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n        \n    if model_name == \"DenseNet201\":\n        base_model = tf.keras.applications.DenseNet201(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n        \n    if model_name == \"MobileNet\":\n        base_model = tf.keras.applications.MobileNet(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n    \n    if model_name == \"VGG19\":\n        base_model = tf.keras.applications.VGG19(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n    \n    if model_name == \"Xception\":\n        base_model = tf.keras.applications.Xception(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n    \n    if model_name == \"InceptionV3\":\n        base_model = tf.keras.applications.InceptionV3(weights=\"imagenet\",\n                                                    input_shape = (image_shape_x1,image_shape_x2,3),\n                                                    include_top=False)\n    \n    return base_model","1476fd72":"def get_model_info(model):\n    print(\"Number of layer in model : \",len(model.layers))\n\n    trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n    non_trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n\n    print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n    print('Trainable params: {:,}'.format(trainable_count))\n    print('Non-trainable params: {:,}'.format(non_trainable_count))\n    \n    num_trainable_layer = np.sum([1 for layer in model.layers if layer.trainable])\n    print(\"Total trainable layers : \",num_trainable_layer)","a4f5e818":"base_model_ResNet50 = get_base_model(\"ResNet50\")","bb224b4b":"# Befor Fine Tune\nget_model_info(base_model_ResNet50)","3458f8d5":"base_model_ResNet50.trainable = True\n# all layers are trainable\n# fine_tune = np.floor(len(base_model_ResNet50.layers)*0.1)\nfor layer in base_model_ResNet50.layers:\n    layer.trainable =  True","cd8ebbdc":"# After Fine Tune\nget_model_info(base_model_ResNet50)","59410120":"model_ResNet50 = tf.keras.models.Sequential()\n\nmodel_ResNet50.add(base_model_ResNet50)\n\nmodel_ResNet50.add(tf.keras.layers.GlobalAveragePooling2D())\n\nmodel_ResNet50.add(tf.keras.layers.Dense(128))\nmodel_ResNet50.add(tf.keras.layers.ReLU())\n\nmodel_ResNet50.add(tf.keras.layers.Dense(64))\nmodel_ResNet50.add(tf.keras.layers.ReLU())\n\nmodel_ResNet50.add(tf.keras.layers.Dense(4,activation='softmax'))\n\nmodel_ResNet50.compile(optimizer=\"adam\",\n                       loss=tf.keras.losses.CategoricalCrossentropy(),\n                       metrics=['categorical_accuracy'])\n\nmodel_ResNet50.summary()","b9949109":"# After adding FC layers\nget_model_info(model_ResNet50)","9a3e1b61":"reduce_lr =  tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", \n                                                  factor = 0.3, \n                                                  patience = 3,\n                                                  verbose = 1,\n                                                  min_lr = 1e-6)","f9d9c133":"H_ResNet50 =  model_ResNet50.fit_generator(train_image_flow_dataframe, \n                     validation_data=validation_image_flow_dataframe,\n                     steps_per_epoch = STEP_SIZE_TRAIN,\n                     validation_steps=STEP_SIZE_VALID, \n                     epochs = 30, \n                     verbose = 1,\n                     callbacks = [reduce_lr],\n                     shuffle=True)","f8788f03":"loss, acc = model_ResNet50.evaluate_generator(generator=validation_image_flow_dataframe,steps=STEP_SIZE_VALID,verbose=1)","f4875438":"plt.plot(H_ResNet50.history['loss'], label='MAE (testing data)')\nplt.plot(H_ResNet50.history['val_loss'], label='MAE (validation data)')\nplt.title('MAE')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","46527573":"pred_ResNet50 = model_ResNet50.predict(test_image_flow_dataframe,verbose=1)","9f1fa423":"sample_submission_df.loc[:,'healthy':] = pred_ResNet50\nsample_submission_df.to_csv('Submission_ResNet50.csv', index=False)\nsample_submission_df.head()","dfa93fae":"model_ResNet50.save(\"model_ResNet50\")","066a7b01":"base_model_Xception = get_base_model(\"Xception\")","e986c159":"get_model_info(base_model_Xception)","16932681":"base_model_Xception.trainable = True\n# all layers are trainable\n# fine_tune = np.floor(len(base_model_Xception.layers)*0.1)\nfor layer in base_model_Xception.layers:\n    layer.trainable =  True","a856fcdf":"model_Xception = tf.keras.models.Sequential()\n\nmodel_Xception.add(base_model_Xception)\n\nmodel_Xception.add(tf.keras.layers.GlobalAveragePooling2D())\n\nmodel_Xception.add(tf.keras.layers.Dense(4,activation='softmax'))\n\nmodel_Xception.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n                       loss=tf.keras.losses.CategoricalCrossentropy(),\n                       metrics=['categorical_accuracy'])\n\nmodel_Xception.summary()","776fdccd":"reduce_lr =  tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", \n                                                  factor = 0.3, \n                                                  patience = 3,\n                                                  verbose = 1,\n                                                  min_lr = 1e-6)","74a36d13":"H_Xception =  model_Xception.fit_generator(train_image_flow_dataframe, \n                     validation_data=validation_image_flow_dataframe,\n                     steps_per_epoch = STEP_SIZE_TRAIN,\n                     validation_steps=STEP_SIZE_VALID, \n                     epochs = 30, \n                     verbose = 1,\n                     callbacks = [reduce_lr],\n                     shuffle=True)","2db8659a":"loss, acc = model_Xception.evaluate_generator(generator=validation_image_flow_dataframe,steps=STEP_SIZE_VALID,verbose=1)","d8114583":"plt.plot(H_Xception.history['loss'], label='MAE (testing data)')\nplt.plot(H_Xception.history['val_loss'], label='MAE (validation data)')\nplt.title('MAE')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","221d27b8":"pred_Xception = model_Xception.predict(test_image_flow_dataframe,verbose=1)","c5cac20a":"sample_submission_df.loc[:,'healthy':] = pred_Xception\nsample_submission_df.to_csv('model_Xception.csv', index=False)\nsample_submission_df.head()","22a60743":"model_Xception.save(\"model_Xception\")","d02095fb":"base_model_InceptionV3 = get_base_model(\"InceptionV3\")","a50968c6":"get_model_info(base_model_InceptionV3)","72a8cf6a":"base_model_InceptionV3.trainable = True\n# all layers are trainable\n# fine_tune = np.floor(len(base_model_InceptionV3.layers)*0.1)\nfor layer in base_model_InceptionV3.layers:\n    layer.trainable =  True","2c4bbac2":"model_InceptionV3 = tf.keras.models.Sequential()\n\nmodel_InceptionV3.add(base_model_InceptionV3)\n\nmodel_InceptionV3.add(tf.keras.layers.GlobalAveragePooling2D())\n\nmodel_InceptionV3.add(tf.keras.layers.Dense(4,activation='softmax'))\n\nmodel_InceptionV3.compile(optimizer=\"adam\",\n                       loss=tf.keras.losses.CategoricalCrossentropy(),\n                       metrics=['categorical_accuracy'])\n\nmodel_InceptionV3.summary()","b993d895":"reduce_lr =  tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", \n                                                  factor = 0.3, \n                                                  patience = 3,\n                                                  verbose = 1,\n                                                  min_lr = 1e-6)","962d801b":"H_InceptionV3 =  model_InceptionV3.fit_generator(train_image_flow_dataframe, \n                     validation_data=validation_image_flow_dataframe,\n                     steps_per_epoch = STEP_SIZE_TRAIN,\n                     validation_steps=STEP_SIZE_VALID, \n                     epochs = 30, \n                     verbose = 1,\n                     callbacks = [reduce_lr],\n                     shuffle=True)","4eaae596":"loss, acc = model_InceptionV3.evaluate_generator(generator=validation_image_flow_dataframe,steps=STEP_SIZE_VALID,verbose=1)","355edad7":"plt.plot(H_InceptionV3.history['loss'], label='MAE (testing data)')\nplt.plot(H_InceptionV3.history['val_loss'], label='MAE (validation data)')\nplt.title('MAE')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","bf1c1191":"pred_InceptionV3 = model_InceptionV3.predict(test_image_flow_dataframe,verbose=1)","29659202":"sample_submission_df.loc[:,'healthy':] = pred_InceptionV3\nsample_submission_df.to_csv('model_InceptionV3.csv', index=False)\nsample_submission_df.head()","5c4f6b87":"model_InceptionV3.save(\"model_InceptionV3\")","b31d0800":"pred = (pred_ResNet50 + pred_Xception + pred_InceptionV3)\/3\nsample_submission_df.loc[:,'healthy':] = pred\nsample_submission_df.to_csv('Final_prediction.csv', index=False)\nsample_submission_df.head()","fa18a531":"##  <font color='red'>Please upvote if you found this useful :)<\/font>","21d65434":"## Transfer Learning","54ad1e0a":"|model |accuracy|\n|----|:-----:|\n|ResNet50+Xception+InceptionV3 | o.967|\n|Xception+InceptionV3 | o.967|\n|ResNet50+Xception | 0.961|\n|ResNet50+InceptionV3 | o.962|\n|CNN model | 0.879 |","853f371b":"## ResNet50","2fda201b":"## Xception","ecee8d79":"## InceptionV3\n","b966fa0a":"## Final prediction","654d4fd8":"![Ball-chart-reporting-the-Top-1-and-Top-5-accuracy-vs-computational-complexity-Top-1-and.ppm.png](attachment:Ball-chart-reporting-the-Top-1-and-Top-5-accuracy-vs-computational-complexity-Top-1-and.ppm.png)","11143f80":"You always want to go for the smallest model that works well for your data. Up until earlier this year, people usually start with VGG16 or VGG19, but Resnet is also a great choice for fine tuning. Start with Resnet18, then to Resnet34 and Resnet50. You could also try the newer models in ResNext or Nascent nets.\n\nBigger models is not always better. They usually overfit your train data. What you really care about is Validation loss. Keeping it small, yet closer to training loss is tricky and may need some regularization like DropOut or WeightDecay etc.\n\nWhen working on a new problem, always start with simple networks - Resnet18 \/ 34 or 50 are good choices."}}