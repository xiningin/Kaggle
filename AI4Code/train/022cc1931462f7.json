{"cell_type":{"b913dee2":"code","5d995d68":"code","a6e0f8a4":"code","dd9c09e8":"code","a6a91543":"code","d6661547":"code","7a202a89":"code","8068a750":"code","d0ee6e91":"code","e1d00863":"code","660160e4":"code","deb77579":"code","091d39ae":"markdown","6766b428":"markdown","3673797c":"markdown","d5f63a37":"markdown","93f49cc5":"markdown","7ee5d576":"markdown","447ed73e":"markdown","1e6f6302":"markdown","6f43ecd1":"markdown","eef690de":"markdown","5620d6ae":"markdown","94d63dc5":"markdown","4ddf199f":"markdown","152f2c1e":"markdown"},"source":{"b913dee2":"import os, torch, librosa, sklearn, librosa.display\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\n\nfrom IPython.display import Audio\nfrom tqdm.auto import tqdm\n\nrandom_state = np.random.RandomState(0)\n\ndata_path = '\/kaggle\/input\/gtzan-dataset-music-genre-classification\/Data'","5d995d68":"# Load file data into a dataframe\ndf = pd.read_csv(os.path.join(data_path, 'features_30_sec.csv'))\n\n# Remove corrupted audio file\ndf.drop(df.loc[df.filename == 'jazz.00054.wav'].index, inplace=True)\n\n# Fetch file names and labels\nfiles, labels = df.filename, df.label","a6e0f8a4":"# Generate an 80-10-10 training, validation and test set split\nfiles_rest, files_test, labels_rest, labels_test = sklearn.model_selection.train_test_split(files, labels, stratify=labels, test_size=0.1, random_state=random_state)\nfiles_train, files_val, labels_train, labels_val = sklearn.model_selection.train_test_split(files_rest, labels_rest, stratify=labels_rest, test_size=0.1\/(1 - 0.1), random_state=random_state)\nlabels = {'train': labels_train, 'val': labels_val, 'test': labels_test}","dd9c09e8":"# Load the audio using LibROSA\nexample_file, example_label = files_train.iloc[2], labels_train.iloc[2]\nexample_path = os.path.join(data_path, 'genres_original', example_label, example_file)\nexample_audio, sample_rate = librosa.load(example_path)\nAudio(example_audio, rate=sample_rate)","a6a91543":"# Generate and plot the MFCCs for the sample audio\nexample_mfcc = librosa.feature.mfcc(example_audio, n_mfcc=13)[1:]\nfig, ax = plt.subplots(figsize=(7, 3))\nlibrosa.display.specshow(example_mfcc, ax=ax, x_axis='s')\nax.set(title=f'MFCCs \u2013 {example_file}', ylabel='MFCC')\nplt.tight_layout()\nplt.show()","d6661547":"# Generate and plot the chroma features for the sample audio\nexample_chroma = librosa.feature.chroma_stft(example_audio)\nfig, ax = plt.subplots(figsize=(7, 3))\nlibrosa.display.specshow(example_chroma, ax=ax, x_axis='s', y_axis='chroma')\nax.set(title=f'Chromagram \u2013 {example_file}')\nplt.tight_layout()\nplt.show()","7a202a89":"# Generate \u2206\u00a0and \u2206\u2206 for the sample MFCCs\nexample_mfcc_delta = librosa.feature.delta(example_mfcc) \nexample_mfcc_delta_delta = librosa.feature.delta(example_mfcc, order=2)\n\nfig, axs = plt.subplots(2, 1, figsize=(7, 6), sharex=True)\n\n# Plot MFCC \u2206s\nlibrosa.display.specshow(example_mfcc_delta, ax=axs[0], x_axis='s')\naxs[0].set(title=f'\u2206 (MFCCs) \u2013 {example_file}', xlabel=None, ylabel='MFCC')\n\n# Plot MFCC \u2206\u2206s\nlibrosa.display.specshow(example_mfcc_delta_delta, ax=axs[1], x_axis='s')\naxs[1].set(title=f'\u2206\u2206 (MFCCs) \u2013 {example_file}', ylabel='MFCC')\n\nplt.tight_layout()\nplt.show()","8068a750":"D = 12   # Number of MFCCs and chroma features\nT = 1290 # Number of frames (after windowing a 30-second recording when generating MFCCs and chroma features)\nC = 6    # Number of channels \u2013 feature types (MFCC + \u2206 + \u2206\u2206, Chromagram + \u2206 + \u2206\u2206)\n\nclass GTZAN(torch.utils.data.Dataset):\n    def __init__(self, files, labels, n_features, scaler=None, **args):\n        super().__init__()\n        self.files = files\n        self.labels = labels\n        self.n_features = n_features\n        self.scaler = scaler\n        self.args = args\n    \n    def fetch(self, index):\n        # Fetch the file path and corresponding label\n        file, label = self.files.iloc[index], self.labels.iloc[index]\n        file_path = os.path.join(data_path, 'genres_original', label, file)\n        \n        # Load the audio and encode the label\n        x, _ = librosa.load(file_path, **self.args)\n        \n        return x\n    \n    def transform(self, x):\n        # Generate D MFCCs & \u2206 + \u2206\u2206\n        mfcc = librosa.feature.mfcc(x, n_mfcc=(self.n_features+1), **self.args)[1:]\n        mfcc_d, mfcc_dd = librosa.feature.delta(mfcc), librosa.feature.delta(mfcc, order=2)\n        # Shape(s): D x T\n\n        # Generate chroma features & \u2206 + \u2206\u2206\n        chroma = librosa.feature.chroma_stft(x, **self.args)\n        chroma_d, chroma_dd = librosa.feature.delta(chroma), librosa.feature.delta(chroma, order=2)\n        # Shape(s): D x T\n        \n        # Only keep the first T frames (as there are some recordings that are slightly over 30s)\n        return (mfcc[:, :T], mfcc_d[:, :T], mfcc_dd[:, :T], chroma[:, :T], chroma_d[:, :T], chroma_dd[:, :T])\n        # Shape(s): [D x T]\n        \n    def scale(self, x):\n        if self.scaler == 'standardize':\n            # Standardizing to zero mean and unit std. dev.\n            return (x - x.mean(dim=(1, 2), keepdim=True)) \/ x.std(dim=(1, 2), keepdim=True)\n        elif self.scaler == 'min-max': \n            # Min-max scaling to [0, 1]\n            return (x - x.amin(dim=(1, 2), keepdim=True)) \/ (x.amax(dim=(1, 2), keepdim=True) - x.amin(dim=(1, 2), keepdim=True))\n        else:\n            # No scaling\n            return x\n    \n    def to_tensor(self, x):\n        # Scale these features and combine them into a multi-channel input\n        return torch.Tensor(x)\n        # Shape: C x D x T\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, index):\n        x = self.fetch(index)\n        x = self.transform(x)\n        x = self.to_tensor(x)\n        x = self.scale(x)\n        return x\n    \n    def plot(self, index, figsize=None, path=None):\n        x = self.fetch(index)\n        x = self.transform(x)\n        \n        fig, axs = plt.subplots(3, 2, figsize=figsize)\n        \n        # MFCC, \u2206 and \u2206\u2206\n        axs[0][0].set_title('MFCC')\n        librosa.display.specshow(x[0], ax=axs[0][0])\n        axs[1][0].set_title('\u2206')\n        librosa.display.specshow(x[1], ax=axs[1][0])\n        axs[2][0].set_title('\u2206\u2206')\n        librosa.display.specshow(x[2], ax=axs[2][0], x_axis='time')\n        \n        # Chroma, \u2206 and \u2206\u2206\n        axs[0][1].set_title('Chroma')\n        librosa.display.specshow(x[3], ax=axs[0][1], y_axis='chroma')\n        axs[1][1].set_title('\u2206')\n        librosa.display.specshow(x[4], ax=axs[1][1], y_axis='chroma')\n        axs[2][1].set_title('\u2206\u2206')\n        librosa.display.specshow(x[5], ax=axs[2][1], x_axis='time', y_axis='chroma')\n        \n        plt.tight_layout()\n        \n        if path:\n            fig.savefig(path, bbox_inches='tight')\n        \n    def plot_stacked(self, index, x_step=0.03, y_step=-0.06, figsize=None, path=None):\n        x = self.fetch(index)\n        x = self.transform(x)\n        \n        fig = plt.figure(figsize=figsize)\n        \n        for i in range(6):\n            ax = fig.add_axes([i * x_step, i * y_step, 1.0, 1.0])\n            librosa.display.specshow(x[5 - i], ax=ax, x_axis=('time' if i == 5 else None))\n            \n        if path:\n            fig.savefig(path, bbox_inches='tight')","d0ee6e91":"# Create Dataset objects for each split\nsplits = {\n    'train': GTZAN(files_train, labels_train, n_features=D, scaler=None),\n    'val': GTZAN(files_val, labels_val, n_features=D, scaler=None),\n    'test': GTZAN(files_test, labels_test, n_features=D, scaler=None)\n}","e1d00863":"splits['train'].plot(2, figsize=(8, 6))","660160e4":"splits['train'].plot_stacked(2, figsize=(4, 2))","deb77579":"for split in tqdm(('train', 'val', 'test'), desc='Split'):\n    # Create a sub-directory for each dataset split\n    split_data_path = os.path.join(split, 'data')\n    os.makedirs(split_data_path, exist_ok=True)\n    \n    # Store the labels for the dataset split\n    split_data = splits[split]\n    labels[split].reset_index(drop=True).to_csv(os.path.join(split, 'labels.csv'))\n    \n    # Preprocess each audio sample from the dataset split and store the resulting Tensor\n    for i, data in tqdm(enumerate(split_data), total=len(split_data), desc='Sample', leave=False):\n        torch.save(data, os.path.join(split_data_path, f'{i}.pt'))","091d39ae":"The GTZAN dataset on Kaggle actually has two variants \u2013 one with 3-second long recordings and one with 30-second long recordings.\n\nFor this task we will only use the 30-second long recordings, but if you are interested, you compare if your chosen method would also perform well on 3-second recordings. \n\n> **Example**: One could investigate whether an LSTM-RNN's ability to model long-term dependencies in a sequence makes them better suited towards 30-second long recordings than 3-second recordings.\n\nThe dataset information for the 30-second recordings is stored in `features_30_sec.csv`, where we can fetch the file names and labels from.\n\n**Note**: The [`jazz.00054.wav` recording is corrupted](https:\/\/www.kaggle.com\/andradaolteanu\/gtzan-dataset-music-genre-classification\/discussion\/158649) so we can drop this from the dataset. ","6766b428":"### Chromagrams\n\nWhile MFCCs are widely used for many machine learning tasks involving audio (speech in particular), chromagrams are most commonly used for music-related purposes.\n\n_Chromagrams_ have a much more straightforward interpretation than MFCCs \u2013 they simply show how much of each musical pitch class is present in each _frame_ (short time window) of an audio recording.\n\nBy default, LibROSA produces chromagrams using the 12 standard pitch classes in Western music: $C, C\\#, D, D\\#, E, F, F\\#, G, G\\#, A, A\\# ,B$. We stick with this default setting as most of the music genres in the GTZAN dataset usually only involve these 12 semitones. However, if for example we were to consider genres from other parts of the world, such as Arabic or Turkish music where microtonal scales form the basis of many songs, it might be worth trying 24 quartertones.\n\nIn our chromagrams, each frame is therefore represented by a 12-vector with values in $[0,1]$ depending on the intensity of each pitch class within that frame.","3673797c":"### Feature scaling\n\nAs with most machine learning problems, ensuring the inputs to the system are scaled correctly is a critical step. Proper scaling can also help improve and speed up convergence when optimizing network parameters.\n\nWhile chroma features take values in $[0,1]$, MFCCs can take on a large range of negative and positive numbers.\n\nThere are many approaches to feature scaling, but for this task I experimented with:\n\n- min-max scaling all features to be in $[0,1]$,\n- standardizing all features so that they have zero mean and unit standard deviation,\n- using the raw inputs without scaling.\n\n**Note**: Each audio sample was scaled individually, taking the mean & std. deviation or min & max value of each set of features individually, and scaling all features in every frame using the same values.\n\nUltimately it was **decided to use min-max scaling**, as standardization surprisingly performed worse than using raw inputs. Min-max scaling was also slightly worse than raw inputs, but it took much less time to train. \n\nThese results can be seen on the [Weights & Biases preprocessing report](https:\/\/wandb.ai\/eonu\/gtzan-cnn\/reports\/GTZAN-Preprocessing-Report--VmlldzoxNDk5NDg2).","d5f63a37":"### Visualizing the features\n\nWe can now visualize all of the 6 types of features for the example audio sample (training example \\#3).\n\n**Note**: The inputs have not been min-max scaled in this image.","93f49cc5":"### Dynamic features \u2013 delta (\u2206) and double-delta (\u2206\u2206)\n\nDelta (\u2206) and double-delta (\u2206\u2206) features are approximations of the first and second order derivatives of audio representations such as MFCCs or chroma features<sup><a href=\"#ref-3\">[3]<\/a><\/sup>. These features are used to capture how dynamic the changes in feature values are between frames.\n\n**Note**: \u2206 and \u2206\u2206 features are usually quite highly correlated with the original features, but can sometimes still result in better convergence and performance.\n\nWe generate \u2206 and \u2206\u2206 features for both the MFCCs and the chromagram, resulting in a **total of 6 types of features** we can use for each audio sample. Below we show an example of \u2206 and \u2206\u2206 features for the MFCCs of the previous audio sample.","7ee5d576":"## Resources\n\n- <span id=\"ref-1\">[1]<\/span> K. Sreenivasa Rao, Dipanjan Nandi. [**\"Language Identification Using Excitation Source Features\"**](https:\/\/link.springer.com\/content\/pdf\/bbm%3A978-3-319-03116-3%2F1.pdf), \\[2015\\]\n- <span id=\"ref-2\">[2]<\/span> [`pichenettes`](https:\/\/dsp.stackexchange.com\/users\/943\/pichenettes) (_Stack Exchange \u2013 DSP_) [**\"Help calculating \/ understanding the MFCCs: Mel-Frequency Cepstrum Coefficients\"**](https:\/\/dsp.stackexchange.com\/a\/6500), \\[Accessed 26\/01\/2022\\] \n- <span id=\"ref-3\">[3]<\/span> Tom B\u00e4ckstr\u00f6m (_Aalto University_). [**\"Deltas and Delta-deltas\"**](https:\/\/wiki.aalto.fi\/display\/ITSP\/Deltas+and+Delta-deltas),  \\[Accessed 26\/01\/2022\\]","447ed73e":"## Wrapping everything up\n\nTo make our lives easier when we start training our CNN, we will now iterate through each audio sample of all three dataset splits and apply all of the preprocessing steps now.\n\nWe can save the resulting preprocessed `torch.Tensor` objects so we don't have to recreate them each time in the next notebook.","1e6f6302":"## Loading and transforming the audio data\n\nNext, we define a [`torch.utils.data.Dataset`](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html) object which describes the steps taken to transform a **single** raw audio recording to a more effective representation for machine learning. But first, let's take a look at the features we will be using.\n\nWe use the popular music information retrieval package [LibROSA](https:\/\/github.com\/librosa\/librosa) for extracting the following features from audio.\n\nWe will use the third recording of the training dataset (which belongs to the **blues** genre) to illustrate the extracted audio features.","6f43ecd1":"### MFCCs\n\n_Mel Frequency Cepstral Coefficients (MFCCs)_ are obtained by applying a sequence of transformations to the usual spectrograms that are obtained through the short-time Fourier transform (STFT). \n\nThese transformations aim to:\n\n1. **Produce a representation that is more aligned with the way humans perceive audio.**<br\/>\n  By partitioning the frequencies in a signal into _bands_, and scaling them by using a set of _filter banks_ and a $\\log$-transform to emphasize frequency bands closer to the human range of hearing. \n2. **Obtain a low-dimensional audio representation.**<br\/>\n  By encoding the resulting into a chosen number of numerical coefficients to be used as features for machine learning.\n  \nThe above is a very basic description of MFCCs, as there are other steps involved such as pre-emphasis and a discrete cosine transform. I highly suggest reading <a href=\"#ref-1\">[1]<\/a> for a more detailed explanation of how MFCCs are calculated.\n\nIn order to match the dimensionality of the chromagrams, we use 12 MFCCs, which also happens to be quite a standard number of coefficients to use.\n\n**Note**: We usually skip the first coefficient, as it is a constant value that is not very useful<sup><a href=\"#ref-2\">[2]<\/a><\/sup>.","eef690de":"### Combining features\n\nAs we will be passing these features into a CNN, we stack them each as individual channels so that any particular frame on the time axis will contain information about the 12 features across all 6 channels.\n\nThis allows us to design our convolutional filters so that they can slide over the time axis while spanning all 6 channels and 12 features.\n\n---\n\nThe following code combines this whole feature preprocessing pipeline into a single [`torch.utils.data.Dataset`](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html) subclass, which represents a dataset split and the instructions for how to read a single audio sample from that split, and apply the transformations.","5620d6ae":"## Creating training, validation and test set splits\n\nWe now randomly split the dataset into separate training (80%), validation (10%) and test (10%) sets, but ensure to keep an equal distribution of genres within each split, by stratifying on the labels.","94d63dc5":"Below is what the stacked representation that will be fed into the CNN looks like.","4ddf199f":"## More exploration?\n\nIf you're feeling inquisitive, there are many more ways to preprocess audio that you can explore.\n\n- Exploring audio features more specifically related to music. \n  > **Examples**:\n  > - [Zero-crossing rate](https:\/\/librosa.org\/doc\/main\/generated\/librosa.feature.zero_crossing_rate.html) can be used as an indicator of percussive sounds in audio.\n  > - The [spectral centroid](https:\/\/librosa.org\/doc\/main\/generated\/librosa.feature.spectral_centroid.html) of an audio signal can be perceived as the 'brightness' of a sound.\n  > - LibROSA provides tools for  extracting the [percussive](https:\/\/librosa.org\/doc\/main\/generated\/librosa.effects.percussive.html#librosa.effects.percussive) and [harmonic](https:\/\/librosa.org\/doc\/main\/generated\/librosa.effects.harmonic.html#librosa.effects.harmonic) parts of an audio signal (an example of _source separation_).<br\/>Could it be more useful break up the original input into these components and to try and learn from them instead?\n  \n- Standardization and min-max scaling are common, but there are more robust methods such as scaling with the median and interquartile range rather than the mean and standard deviation (as implemented in [`sklearn.preprocessing.RobustScaler`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html)). \n- While perhaps challenging to do on 30-second long recordings, data augmentation has proven to be a useful regularization method in many applications of supervised learning.\n\n---\n\n**Let's get onto the machine learning! [\u00bb\u00bb\u00bb](https:\/\/www.kaggle.com\/eonuonga\/gtzan-music-genre-classification-cnn-2-2)**","152f2c1e":"# GTZAN Music Genre Classification \u2013 Preprocessing (1\/2)\n\n**This notebook is the first of a two-part set of notebooks that investigates how convolutional neural networks (CNNs) can be applied to the task of music genre classification, using the GTZAN dataset which is [available on Kaggle](https:\/\/www.kaggle.com\/andradaolteanu\/gtzan-dataset-music-genre-classification).**\n\nThe [Weights & Biases reports page](https:\/\/wandb.ai\/eonu\/gtzan-cnn\/reportlist) for this project is a useful way to visualize results. You can also see into more detail on the [main W&B project page](https:\/\/wandb.ai\/eonu\/gtzan-cnn), such as information about the use of computational resources. Runs prefixed with `prep` refer to preprocessing experiments, whereas `final` is the final model that was trained. `search` runs are for hyper-parameter optimization, and `summary` runs report the results of the best hyper-parameter configurations.\n\n**See part 2 here (creating and training a CNN for predicting genres) [>>>](https:\/\/www.kaggle.com\/eonuonga\/gtzan-music-genre-classification-cnn-2-2)**\n\n---\n\n## The GTZAN Dataset\n\nOur source of audio data is the GTZAN dataset, which consists of 30-second segments of music from 10 different genres, each having 100 recordings (for a total of 1000).\n\n**Genres**: Blues, Classical, Country, Disco, Hiphop, Jazz, Metal, Pop, Reggae, Rock\n\n## Package imports and fetching files\/labels"}}