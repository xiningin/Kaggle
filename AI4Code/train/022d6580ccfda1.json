{"cell_type":{"d1aca7ad":"code","6a8210fe":"code","187748e1":"code","afa49569":"code","1a83abac":"code","87210427":"code","e8c0e389":"code","a919263b":"code","cb46db2f":"code","b7e19dbe":"code","99fc8474":"code","b8a764ca":"code","5c5fc3b8":"code","5674ed8e":"code","332978f1":"code","c0de6817":"code","83daff1e":"code","bfa828fe":"markdown","16c23128":"markdown"},"source":{"d1aca7ad":"# import libraries\nimport os \n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom transformers import DistilBertTokenizerFast, TFTrainer, TFTrainingArguments, TFDistilBertForSequenceClassification\n\nimport warnings \nwarnings.filterwarnings('ignore')","6a8210fe":"# get the path of the dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","187748e1":"# load the data\npath = '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv'\ndf = pd.read_csv(path, encoding = \"ISO-8859-1\")\ndf.head()","afa49569":"# some pre-processing on the dataset \nprint(df.isna().sum())\ndf.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True, axis=1)\ndf.columns = ['label', 'comment']\ndf.head()","1a83abac":"# seprate dataframe in X and y\nX = list(df['comment'])\ny = list(df['label'])","87210427":"# convert to numerical data\ny = list(pd.get_dummies(y, drop_first=True)['spam'])","e8c0e389":"# seprate in train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","a919263b":"# initialize tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","cb46db2f":"# tokenize data\ntrain_encodings = tokenizer(X_train, truncation=True, padding=True)\ntest_encodings = tokenizer(X_test, truncation=True, padding=True)","b7e19dbe":"# craete dataset using from_tensor_slices\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))","99fc8474":"# set training arguments\ntraining_args = TFTrainingArguments(\n    output_dir='.\/results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs',            # directory for storing logs\n    logging_steps=10,\n)","b8a764ca":"# initialize model and trainer\nwith training_args.strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\ntrainer = TFTrainer(\n    model = model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset)\n\ntrainer.train()","5c5fc3b8":"# evalute\ntrainer.evaluate(test_dataset)","5674ed8e":"# predict the data\noutput = trainer.predict(test_dataset)[1]\noutput","332978f1":"# shape of data\ntrainer.predict(test_dataset)[1].shape","c0de6817":"# confusion matrix\ncm = confusion_matrix(y_test, output)\ncm","83daff1e":"# save custom model 'sentiment_model'(we can set any name)\ntrainer.save_model('sentiment_model')","bfa828fe":"**Huggingface have written a proper blog on fine tuning of pre-trained models**\n**link: [huggingface.co\/transformers\/training.html](http:\/\/huggingface.co\/transformers\/training.html)**","16c23128":"# **Customize \ud83e\udd17 pre-trained model on spam and ham dataset**"}}