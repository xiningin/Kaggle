{"cell_type":{"72ab2823":"code","fb515b9d":"code","f6b5a2e8":"code","00b42fef":"code","5c0fe948":"code","31ca8b15":"code","21e97d6c":"code","870a0a3f":"code","2fb0ac41":"code","054d25e1":"code","ab0410b6":"code","f4eb2789":"code","8c69b2ab":"code","c133b2b0":"code","6fb63c42":"code","62a864a0":"code","79dfb21a":"code","5b08b591":"code","a2405a33":"code","b67270d5":"code","87d872e0":"code","3444b9e2":"code","da42b888":"code","89f0d1d9":"code","5e72e816":"code","9cd54f6d":"code","f59d2680":"code","2c7c6d38":"code","a999bdd3":"code","fbdc0469":"code","ce457d4e":"code","b3314f07":"code","62104980":"code","fae5e753":"code","9f89432a":"code","9136916c":"code","0ae448da":"code","329bc6c3":"code","9af380a8":"code","f83743a9":"code","93dc654d":"code","fe795414":"code","6e11f578":"code","2a82b6f5":"code","a4662490":"code","184915f0":"code","b62debef":"code","21c0c0ef":"code","1e23e43a":"code","6816ec04":"code","1a276ec5":"code","e4e13843":"code","5d17410d":"code","a82a4e19":"code","0a5b1919":"code","0d7d4dc9":"code","34572692":"code","2abfa962":"code","f6475a5f":"code","86545122":"code","c1e6c57f":"code","2fd07d54":"code","7ddc86f5":"code","0691a9fc":"code","9d2208cc":"code","26cf445c":"code","69cab547":"code","4411b468":"code","4653a86f":"code","79fbae95":"code","f3604f54":"code","c40aa718":"code","16f2da75":"code","c4776ce3":"code","e2866799":"code","c50b9b64":"code","7df2f5a3":"code","d71f31bc":"code","cc05d38c":"code","126215c8":"code","f8b52da5":"code","4b4bcb19":"code","f7cbda44":"code","dfb9fe25":"code","aee330ba":"code","467d4806":"code","7a20a0a7":"code","3c73e863":"code","959fd8d3":"code","e740e0e4":"code","4f979832":"code","59db44a7":"code","c9e23b67":"code","d8ce6bed":"code","b8763708":"code","3e7641b3":"code","610571e8":"code","693e8f0d":"code","852b2f44":"code","3ebf2944":"code","43e44a1c":"code","6f668161":"code","55605928":"code","a61aaa4a":"code","33c7293a":"code","f0671a2f":"code","49ba162f":"code","f281db9d":"code","41820a1c":"code","19967d2e":"code","25a05ce1":"markdown","5d747026":"markdown","d70a54c3":"markdown","d03596bf":"markdown","79f0e271":"markdown","e6328c9b":"markdown","f3c407eb":"markdown","adc8a184":"markdown","09e86d2d":"markdown","7e5baee7":"markdown","7e61fc77":"markdown","e7e5f09a":"markdown","08f72ace":"markdown","e7727099":"markdown","16018398":"markdown","67ea18a6":"markdown","f2a5b133":"markdown","1b8f1bb7":"markdown","d6892989":"markdown","a1702e39":"markdown","d995e6ee":"markdown","2d361859":"markdown","edc4ecaa":"markdown","df2d054c":"markdown","11529e39":"markdown","6bd9d31c":"markdown","3a11e4a6":"markdown","a6133ebc":"markdown","761449c6":"markdown","9dc2ad76":"markdown","44d0111f":"markdown","db46f515":"markdown","874beefd":"markdown","1f2a27a3":"markdown","86671542":"markdown","40ca2261":"markdown","00e68be7":"markdown","dbba4893":"markdown","48ccc0aa":"markdown","5e6577c4":"markdown","540edd73":"markdown","4b991f0f":"markdown","6a9e3877":"markdown","d72d46b3":"markdown","084727a6":"markdown","ea8a4189":"markdown","88746388":"markdown","a9f1415e":"markdown","de700473":"markdown"},"source":{"72ab2823":"#Importing all the required libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 50)","fb515b9d":"audit_risk = pd.read_csv(\"..\/input\/audit-data\/audit_risk.csv\")\ntrial = pd.read_csv(\"..\/input\/audit-data\/trial.csv\")","f6b5a2e8":"audit_risk.describe()","00b42fef":"trial.describe()","5c0fe948":"#Renaming columns\ntrial.columns = ['Sector_score','LOCATION_ID', 'PARA_A', 'Score_A', 'PARA_B',\n       'Score_B',  'TOTAL', 'numbers', 'Marks',\n       'Money_Value', 'MONEY_Marks', 'District',\n       'Loss', 'LOSS_SCORE', 'History', 'History_score', 'Score', 'Risk_trial' ]","31ca8b15":"trial['Score_A'] = trial['Score_A']\/10\ntrial['Score_B'] = trial['Score_B']\/10","21e97d6c":"same_columns = np.intersect1d(audit_risk.columns, trial.columns)\nsame_columns","870a0a3f":"# Merge two Dataframes  on common columns  using outer join\nmerged_df = pd.merge(audit_risk, trial, how='outer', on = ['History', 'LOCATION_ID', 'Money_Value', 'PARA_A', 'PARA_B',\n       'Score', 'Score_A', 'Score_B', 'Sector_score', 'TOTAL', 'numbers'])\nmerged_df.columns","2fb0ac41":"df = merged_df.drop(['Risk_trial'], axis = 1)","054d25e1":"df.info()","ab0410b6":"#Replacing the missing value by the median of the column\ndf['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].median())","f4eb2789":"df.describe()","8c69b2ab":"df = df.drop(['Detection_Risk', 'Risk_F'], axis = 1) \ndf.info()","c133b2b0":"#Unique values in LOCATION_ID column\ndf[\"LOCATION_ID\"].unique()","6fb63c42":"print(\"These are the number of non-numeric values in LOCATION_ID: \", len(df[(df[\"LOCATION_ID\"] == 'LOHARU') | (df[\"LOCATION_ID\"] ==  'NUH') | (df[\"LOCATION_ID\"] == 'SAFIDON')]))","62a864a0":"df = df[(df.LOCATION_ID != 'LOHARU')]\ndf = df[(df.LOCATION_ID != 'NUH')]\ndf = df[(df.LOCATION_ID != 'SAFIDON')]\ndf = df.astype(float)\nprint(\"Updated number of rows in the dataset: \",len(df))","79dfb21a":"df = df.drop_duplicates(keep = 'first')\nprint(\"Updated number of rows in the dataset: \",len(df))","5b08b591":"#Number of unique values in each columns\nfor i in range(0, len(df.columns)):\n    print(df.columns[i], \":\", df.iloc[:,i].nunique())","a2405a33":"import seaborn as sns\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps\ncm = sns.diverging_palette(220, 20, sep=20, as_cmap=True) \ncorr.style.background_gradient(cmap=cm).set_precision(2)","b67270d5":"#Keeping just the columns that are correlated with the target variable and not with other independent variables.\ndf = df[['Risk_A', 'Risk_B', 'Risk_C', 'Risk_D','RiSk_E', 'Prob', 'Score', 'CONTROL_RISK',\n        'Audit_Risk', 'Risk', 'MONEY_Marks', 'Loss']]\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps\ncm = sns.diverging_palette(220, 20, sep=20, as_cmap=True) \ncorr.style.background_gradient(cmap=cm).set_precision(2)","87d872e0":"#Creating a new dataframe for classification by deleting the Audit_Risk column.\nclass_df = df.drop(\"Audit_Risk\", axis = 1)","3444b9e2":"classification_X = class_df.drop([\"Risk\"], axis = 1)\nclassification_y = class_df[\"Risk\"]","da42b888":"from sklearn.model_selection import train_test_split\n\nX_train_org, X_test_org, y_train, y_test = train_test_split(classification_X, classification_y, \n                                                            test_size = 0.25, random_state = 0)","89f0d1d9":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train_org)\nX_test  = scaler.transform(X_test_org)","5e72e816":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nlr_hard = LogisticRegression()\nlr_hard.fit(X_train, y_train)\nknn_hard = KNeighborsClassifier(7)\nknn_hard.fit(X_train, y_train)\nsvc_hard = SVC(C = 10, probability = True)\nsvc_hard.fit(X_train, y_train)\n\nvoting_clf_hard = VotingClassifier(estimators=[('lr', lr_hard), ('knn', knn_hard), ('svc', svc_hard)], voting='hard')\nvoting_clf_hard.fit(X_train, y_train)","9cd54f6d":"from sklearn.metrics import accuracy_score\nfor clf in (lr_hard, knn_hard, svc_hard, voting_clf_hard):\n    clf.fit(X_train, y_train)\n    y_test_pred = clf.predict(X_test)\n    y_train_pred = clf.predict(X_train)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_test_pred))","f59d2680":"report_table_1 = ['Hard Voting Classifier 1', '',accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)]","2c7c6d38":"knn_hard = KNeighborsClassifier(3)\nknn_hard.fit(X_train, y_train)\nsvc_hard = SVC(kernel='rbf', random_state= 0)\nsvc_hard.fit(X_train, y_train)\ndt_hard = DecisionTreeClassifier(max_depth = 5, random_state= 0)\ndt_hard.fit(X_train, y_train)\n\nvoting_clf_hard = VotingClassifier(estimators=[('knn', knn_hard), ('svc', svc_hard), ('dt', dt_hard)], voting='hard')\nvoting_clf_hard.fit(X_train, y_train)","a999bdd3":"for clf in (knn_hard, svc_hard, dt_hard, voting_clf_hard):\n    clf.fit(X_train, y_train)\n    y_test_pred = clf.predict(X_test)\n    y_train_pred = clf.predict(X_train)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_test_pred))","fbdc0469":"report_table_2 = ['Hard Voting Classifier 2', '',accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)]","ce457d4e":"lr_soft = LogisticRegression()\nlr_soft.fit(X_train, y_train)\nknn_soft = KNeighborsClassifier(7)\nknn_soft.fit(X_train, y_train)\nsvc_soft = SVC(C = 10, probability = True)\nsvc_soft.fit(X_train, y_train)\n\nvoting_clf_soft = VotingClassifier(estimators=[('lr', lr_soft), ('knn', knn_soft), ('svc', svc_soft)], voting='soft')\nvoting_clf_soft.fit(X_train, y_train)","b3314f07":"for clf in (knn_soft, svc_soft, lr_soft, voting_clf_soft):\n    clf.fit(X_train, y_train)\n    y_test_pred = clf.predict(X_test)\n    y_train_pred = clf.predict(X_train)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_test_pred))","62104980":"report_table_3 = ['Soft Voting Classifier 1', '',accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)]","fae5e753":"knn_soft = KNeighborsClassifier(5)\nknn_soft.fit(X_train, y_train)\nsvc_soft = SVC(kernel='rbf', random_state= 0, probability= True)\nsvc_soft.fit(X_train, y_train)\ndt_soft = DecisionTreeClassifier(max_depth = 7, random_state= 0)\ndt_soft.fit(X_train_org, y_train)\n\nvoting_clf_soft = VotingClassifier(estimators=[('knn', knn_soft), ('svc', svc_soft), ('dt', dt_soft)], voting='soft')\nvoting_clf_soft.fit(X_train, y_train)","9f89432a":"for clf in (knn_soft, svc_soft, dt_soft, voting_clf_soft):\n    clf.fit(X_train, y_train)\n    y_test_pred = clf.predict(X_test)\n    y_train_pred = clf.predict(X_train)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_test_pred))","9136916c":"report_table_4 = ['Soft Voting Classifier 2', '',accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)]","0ae448da":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\n\nknn_param ={'n_neighbors': [3,5,7,11,15]}\nknn_grid = GridSearchCV(knn, knn_param,cv = 5, n_jobs= -1)\nknn_grid.fit(X_train, y_train)","329bc6c3":"print(\"Best Parameters for KNN Classifier: \", knn_grid.best_params_)","9af380a8":"knn = KNeighborsClassifier(n_neighbors=3)\nbag = BaggingClassifier(knn, bootstrap=True, random_state = 0)\n#model param\ngrid_param = {'n_estimators': [100, 500, 1000],\n              'max_samples': [0.1, 0.5, 1.0]}\n\n#grid model\nbag_knn_grid = GridSearchCV(bag, grid_param, cv = 5, n_jobs = -1, return_train_score= True)\n\n#train grid model\nbag_knn_grid.fit(X_train, y_train)","f83743a9":"print(\"Best Parameters for Bagging Classifier: \", bag_knn_grid.best_params_)","93dc654d":"bag = BaggingClassifier(knn, n_estimators=100, max_samples=1.0, n_jobs = -1, bootstrap=True, random_state=0)\nbag.fit(X_train, y_train)","fe795414":"print(\"KNN with Bagging Training Score: \", bag.score(X_train, y_train))\nprint(\"KNN with Bagging Testing Score: \", bag.score(X_test, y_test))","6e11f578":"report_table_5 = ['KNN with Bagging', 'n_neighbors: 3, max_samples: 1.0, n_estimators: 100',bag.score(X_train, y_train), bag.score(X_test, y_test)]","2a82b6f5":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nbag = BaggingClassifier(lr, bootstrap=True, random_state = 0)\ngrid_param = {'n_estimators': [100, 500, 1000],\n              'max_samples': [0.1, 0.5, 1.0]}\n\n#grid model\nbag_lr_grid = GridSearchCV(bag, grid_param, cv = 5, n_jobs = -1, return_train_score= True)\n\n#train grid model\nbag_lr_grid.fit(X_train, y_train)","a4662490":"print(\"Best Parameters for Bagging Classifier: \", bag_lr_grid.best_params_)","184915f0":"bag = BaggingClassifier(lr, n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\nbag.fit(X_train, y_train)","b62debef":"print(\"Logistic Regression with Bagging Training Score: \", bag.score(X_train, y_train))\nprint(\"Logistic Regression with Bagging Testing Score: \", bag.score(X_test, y_test))","21c0c0ef":"report_table_6 = ['Logistic Regression with Bagging', 'max_samples: 1.0, n_estimators: 500',bag.score(X_train, y_train), bag.score(X_test, y_test)]","1e23e43a":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state = 0)\ngrid_param = {'max_depth': [3, 5, 7, 9, 11, 15]}\ndt_grid = GridSearchCV(dt, grid_param, cv = 5, n_jobs = -1)\ndt_grid.fit(X_train_org, y_train)","6816ec04":"print(\"Best Parameters for Decision Tree Classifier: \", dt_grid.best_params_)","1a276ec5":"#base model\ndt = DecisionTreeClassifier(max_depth = 9, random_state=0)\nbag = BaggingClassifier(dt, random_state = 0)\n#model param\ngrid_param = {'n_estimators': [100, 500, 1000],\n              'max_samples': [0.1, 0.5, 1.0]}\n\n#grid model\nbag_dt_grid = GridSearchCV(bag, grid_param, cv = 5, n_jobs = -1, return_train_score= True)\n\n#train grid model\nbag_dt_grid.fit(X_train_org, y_train)","e4e13843":"print(\"Best Parameters for Bagging Classifier with Pasting: \", bag_dt_grid.best_params_)","5d17410d":"bag = BaggingClassifier(dt, n_estimators=100, max_samples=1.0, n_jobs = -1, bootstrap=False, random_state=0)\nbag.fit(X_train_org, y_train)","a82a4e19":"print(\"Decision Tree Classifier with Pasting Training Score: \", bag.score(X_train_org, y_train))\nprint(\"Decision Tree Classifier with Pasting Testing Score: \", bag.score(X_test_org, y_test))","0a5b1919":"report_table_7 = ['Decision Tree Classifier with Pasting', 'max_depth: 9, max_samples: 1.0, n_estimators: 100',bag.score(X_train_org, y_train), bag.score(X_test_org, y_test)]","0d7d4dc9":"from sklearn.svm import LinearSVC\nsvc = LinearSVC(penalty = 'l2', random_state=0)\ngrid_param = {'C':[1, 10, 100, 1000]}\nsvc_grid = GridSearchCV(svc, grid_param, cv = 5)\nsvc_grid.fit(X_train, y_train)","34572692":"print(\"Best Parameters for Linear SVC: \", svc_grid.best_params_)","2abfa962":"#base model\nsvc = LinearSVC(C = 100, penalty = 'l2', random_state=0)\nbag = BaggingClassifier(svc, random_state = 0)\n#model param\ngrid_param = {'n_estimators': [100, 500, 1000],\n              'max_samples': [0.1, 0.5, 1.0]}\n\n#grid model\nbag_svc_grid = GridSearchCV(bag, grid_param, cv = 5, n_jobs = -1, return_train_score= True)\n\n#train grid model\nbag_svc_grid.fit(X_train, y_train)","f6475a5f":"print(\"Best Parameters for Bagging Classifier with Pasting: \", bag_svc_grid.best_params_)","86545122":"bag = BaggingClassifier(svc, n_estimators= 100, max_samples= 0.1, bootstrap=False, n_jobs=-1, random_state=0)\nbag.fit(X_train, y_train)","c1e6c57f":"print(\"Linear SVC with Pasting Training Score: \", bag.score(X_train, y_train))\nprint(\"Linear SVC with Pasting Testing Score: \", bag.score(X_test, y_test))","2fd07d54":"report_table_8 = ['Linear SVC with Pasting', 'C: 100, max_samples: 0.1, n_estimators: 100',bag.score(X_train, y_train), bag.score(X_test, y_test)]","7ddc86f5":"from sklearn.ensemble import AdaBoostClassifier\n\ndt = DecisionTreeClassifier(max_depth = 9, random_state=0)\nada = AdaBoostClassifier(dt)\n\nparam = {'n_estimators' : [100,500,1000],\n        'learning_rate': [0.1, 0.5, 1, 10]}\n\nada_grid = GridSearchCV(ada, param, cv=5, n_jobs= -1)\n\nada_grid.fit(X_train_org, y_train)","0691a9fc":"print(\"Best Parameters for Adaboost Classifier: \", ada_grid.best_params_)","9d2208cc":"ada = AdaBoostClassifier(dt, n_estimators= 100, learning_rate= 0.1, algorithm=\"SAMME.R\", random_state=0)\nada.fit(X_train_org, y_train)","26cf445c":"print(\"Decision Tree Classifier with Adaboost Training Score: \", ada.score(X_train_org, y_train))\nprint(\"Decision Tree Classifier with Adaboost Testing Score: \", ada.score(X_test_org, y_test))","69cab547":"report_table_9 = ['Decision Tree Classifier with Adaboost', 'max_depth = 9, learning_rate = 0.1, n_estimators = 1000',\n                  ada.score(X_train_org, y_train), ada.score(X_test_org, y_test)]","4411b468":"lr = LogisticRegression()\nada = AdaBoostClassifier(lr)\n\nparam = {'n_estimators' : [100,500,1000],\n        'learning_rate': [0.1, 0.5, 1]}\n\nada_grid = GridSearchCV(ada, param, cv=5, n_jobs= -1)\n\nada_grid.fit(X_train, y_train)","4653a86f":"print(\"Best Parameters for Adaboost Classifier: \", ada_grid.best_params_)","79fbae95":"ada = AdaBoostClassifier(dt, n_estimators= 1000, learning_rate= 1, algorithm=\"SAMME.R\", random_state=0)\nada.fit(X_train, y_train)","f3604f54":"print(\"Logistic Regression with Adaboost Training Score: \", ada.score(X_train, y_train))\nprint(\"Logistic Regression with Adaboost Testing Score: \", ada.score(X_test, y_test))","c40aa718":"report_table_10 = ['Logistic Regression with Adaboost', 'learning_rate = 1, n_estimators = 1000',\n                  ada.score(X_train, y_train), ada.score(X_test, y_test)]","16f2da75":"from  sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(random_state=0)\n\nparam = {'max_depth': [4,5,7,9,11,15],\n           'n_estimators': [100, 500, 1000],\n           'learning_rate': [0.01,0.1, 0.5, 1.0]}\ngb_grid = GridSearchCV(gb, param, cv = 5, return_train_score= True)\ngb_grid.fit(X_train, y_train)","c4776ce3":"print(\"Best Parameters for Gradient Boosting Classifier: \", gb_grid.best_params_)","e2866799":"gb = GradientBoostingClassifier(max_depth=4, n_estimators=100, learning_rate=1.0, random_state=0)\ngb.fit(X_train, y_train)","c50b9b64":"print(\"Gradient Boosting Classifier Training Score: \", gb.score(X_train, y_train))\nprint(\"Gradient Boosting Classifier Testing Score: \", gb.score(X_test, y_test))","7df2f5a3":"report_table_11 = ['Gradient Boosting Classifier', 'learning_rate = 1.0, max_depth = 4, n_estimators = 100',\n                  gb.score(X_train, y_train), gb.score(X_test, y_test)]","d71f31bc":"report_table = pd.DataFrame(list(zip(report_table_1,\n             report_table_2,\n             report_table_3,\n             report_table_4,\n             report_table_5,\n             report_table_6,\n             report_table_7,\n             report_table_8,\n             report_table_9,\n             report_table_10,\n             report_table_11))).transpose()","cc05d38c":"report_table.columns = ['Model Name', 'Model Parameter', 'Training Score', 'Testing Score']\nreport_table.index = report_table['Model Name']\nreport_table.head(10)","126215c8":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\n\nX_train_reduced = pca.fit_transform(X_train)\nX_test_reduced = pca.transform(X_test)","f8b52da5":"print(\"Number of PCA components: \", pca.n_components_)","4b4bcb19":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors':[3, 4, 5, 6, 7, 8, 9, 10, 15]}\n\ngrid_knn_clf = GridSearchCV(knn, param_grid=param_grid, cv = 10, scoring='roc_auc')\ngrid_knn_clf.fit(X_train_reduced, y_train)","f7cbda44":"print(\"Best Parameters for KNN Classifier with PCA: \", grid_knn_clf.best_params_)","dfb9fe25":"pca_knn = KNeighborsClassifier(n_neighbors=8)\npca_knn.fit(X_train_reduced, y_train)","aee330ba":"print(\"KNN Classifier with PCA Training Score: \", pca_knn.score(X_train_reduced, y_train))\nprint(\"KNN Classifier with PCA Testing Score: \", pca_knn.score(X_test_reduced, y_test))","467d4806":"pca_report_table_1 = ['KNN Classifier with PCA', 'n_neighbors = 8', \n                      pca_knn.score(X_train_reduced, y_train), pca_knn.score(X_test_reduced, y_test)]","7a20a0a7":"svc = SVC()\nparam_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n\ngrid_svc_clf = GridSearchCV(svc, param_grid, cv = 5, scoring='roc_auc', return_train_score=True)\ngrid_svc_clf.fit(X_train_reduced, y_train)","3c73e863":"print(\"Best Parameters for LinearSVC with PCA: \", grid_svc_clf.best_params_)","959fd8d3":"pca_svc = SVC(C= 10, kernel= 'rbf')\npca_svc.fit(X_train_reduced, y_train)","e740e0e4":"print(\"SVC with PCA Training Score: \", pca_svc.score(X_train_reduced, y_train))\nprint(\"SVC with PCA Testing Score: \", pca_svc.score(X_test_reduced, y_test))","4f979832":"pca_report_table_2 = ['SVC with PCA', 'C =10, kernel= rbf', \n                      pca_svc.score(X_train_reduced, y_train), pca_svc.score(X_test_reduced, y_test)]","59db44a7":"pca_lr = LogisticRegression(random_state=0)\n\nparam_grid = {'penalty':['l1', 'l2']}\n\ngrid_log_clf = GridSearchCV(pca_lr , param_grid, cv = 5, return_train_score=True, scoring='roc_auc')\ngrid_log_clf.fit(X_train_reduced, y_train)","c9e23b67":"print(\"Best Parameters for Logistic Regression with PCA: \", grid_log_clf.best_params_)","d8ce6bed":"pca_lr = LogisticRegression(penalty= 'l1')\npca_lr.fit(X_train_reduced, y_train)","b8763708":"print(\"Logistic Regression with PCA Training Score: \", pca_lr.score(X_train_reduced, y_train))\nprint(\"Logistic Regression with PCA Testing Score: \", pca_lr.score(X_test_reduced, y_test))","3e7641b3":"pca_report_table_3 = ['Logistic Regression with PCA', 'penalty = l1', \n                      pca_lr.score(X_train_reduced, y_train), pca_lr.score(X_test_reduced, y_test)]","610571e8":"#Base model\ndt = DecisionTreeClassifier(random_state = 0)\n\n#model param\ngrid_param = {'max_depth': [3, 5, 7, 9, 11, 15]}\n\n#grid model\ndt_grid = GridSearchCV(dt, grid_param, cv = 5, n_jobs = -1)\n\n#train grid model\ndt_grid.fit(X_train_reduced, y_train)","693e8f0d":"print(\"Best Parameters for Decision Tree Classifier: \", dt_grid.best_params_)","852b2f44":"pca_dt = DecisionTreeClassifier(max_depth= 5, random_state= 0)\npca_dt.fit(X_train_reduced, y_train)","3ebf2944":"print(\"Decision Tree Classifier with PCA Training Score: \", pca_dt.score(X_train_reduced, y_train))\nprint(\"Decision Tree Classifier with PCA Testing Score: \", pca_dt.score(X_test_reduced, y_test))","43e44a1c":"pca_report_table_4 = ['Decision Tree Classifier with PCA', 'max_depth: 5', \n                      pca_dt.score(X_train_reduced, y_train), pca_dt.score(X_test_reduced, y_test)]","6f668161":"pca_report_table = pd.DataFrame(list(zip(pca_report_table_1,\n             pca_report_table_2,\n             pca_report_table_3,\n             pca_report_table_4))).transpose()","55605928":"pca_report_table.columns = ['Model Name', 'Model Parameter', 'Training Score', 'Testing Score']\npca_report_table.index = pca_report_table['Model Name']","a61aaa4a":"pca_report_table.head(10)","33c7293a":"report_table_without_pca = pd.read_csv('..\/input\/classification-report\/Classification Report Table without PCA.csv')\nreport_table_without_pca.head(5)","f0671a2f":"import matplotlib.pyplot as plt\n\nax = pca_report_table[['Training Score','Testing Score']].plot(kind='bar',\n            title = \"Comparison of Accuracies of Different Models with PCA\", figsize=(8, 8), fontsize = 8)\nplt.show()","49ba162f":"from keras.models import Sequential\nfrom keras.layers import Dense,MaxPooling1D\nnp.random.seed(0)\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim = 10, activation = 'sigmoid'))\nmodel.add(Dense(1))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nmodel.fit(X_train, y_train, epochs = 40, batch_size = 50)","f281db9d":"from sklearn.metrics import accuracy_score\ny_train_pred = model.predict(X_train)\ny_train_pred = np.where(y_train_pred >= 0.5 , 1, 0)\ny_test_pred = model.predict(X_test)\ny_test_pred = np.where(y_test_pred >= 0.5 , 1, 0)\nprint(\"NN Train Score: \",accuracy_score(y_train, y_train_pred))\nprint(\"NN Test Score: \", accuracy_score(y_test, y_test_pred))","41820a1c":"print(\"Logistic Regression with PCA Training Score: \", pca_lr.score(X_train_reduced, y_train))\nprint(\"Logistic Regression with PCA Testing Score: \", pca_lr.score(X_test_reduced, y_test))","19967d2e":"y_predicted = pca_lr.predict(X_test_reduced)\nprint(\"Predicted value for 1st testing row: \", y_predicted[0])\nprint(\"Original value for 1st testing row: \", y_test.values[0])\nprint(\"\")\nprint(\"Predicted value for 5th testing row: \", y_predicted[4])\nprint(\"Original value for 5th testing row: \", y_test.values[4])","25a05ce1":"##  SVM Classifier with PCA","5d747026":"# Classification","d70a54c3":"Most columns from the 2 dataframes have similar feature names and also similar description. Some columns are in all capital letters, some have values in multiples of 10. The only columns in **'trial'** that are entirely differrent are **'Loss'** and **'Risk'**.","d03596bf":"## Merging Dataframes","79f0e271":"## Hard Voting Classifier 1","e6328c9b":"# Correlation Matrix","f3c407eb":"## Classification Models","adc8a184":"## Exploring Data","09e86d2d":"## Decision Tree with Adaboost","7e5baee7":"## Soft Voting Classifier 1","7e61fc77":"## Pasting","e7e5f09a":"## Gradient Boosting Classifier","08f72ace":"Deleting the rows with these 3 values as we have no information about the sequencing of the numbers present in the LOCATION_ID column.","e7727099":"## Logistic Regression with PCA","16018398":"## Importing Datasets","67ea18a6":"## KNN Classifier with PCA","f2a5b133":"LOCATION_ID has object datatype. However there are numerical values in the column. There must be non-numeric values present.","1b8f1bb7":"## Data Cleaning","d6892989":"As we can see our model works pretty well on the test data as well.\n\n**Training and testing score of around 0.95 was achieved using Logistic Regression.** ","a1702e39":"## Soft Voting Classifier 2","d995e6ee":"## Decision Tree with Pasting","2d361859":"## Voting Classifiers","edc4ecaa":"## Logistic Regression with Bagging","df2d054c":"## Bagging","11529e39":"## Generating a Report table: PCA\nFor comparing all the models, we will create a table and a plot.","6bd9d31c":"# <font color='green'>Audit Data Classification","3a11e4a6":"### Table of Contents:\n1. [Data Pre-processing](#Data-Pre-processing)\n    * [Importing Datasets](#Importing-Datasets)\n    * [Exploring Data](#Exploring-Data)\n    * [Merging Dataframes](#Merging-Dataframes)\n    * [Data Cleaning](#Data-Cleaning)\n    \n    \n2. [Correlation Matrix](#Correlation-Matrix)\n\n\n3. [Classification](#Classification)\n    * [Classification: Train-Test Split](#Classification:-Train-Test-Split)\n    * [Classification: Feature Scaling](#Classification:-Feature-Scaling)\n    * [Classification Models](#Classification:-Models)\n        * [Voting Classifiers](#Voting-Classifiers)\n            * [Hard Voting Classifier 1](#Hard-Voting-Classifier-1)\n            * [Hard Voting Classifier 2](#Hard-Voting-Classifier-2)\n            * [Soft Voting Classifier 1](#Soft-Voting-Classifier-1)\n            * [Soft Voting Classifier 2](#Soft-Voting-Classifier-2)\n        * [Bagging](#Bagging)\n            * [KNN with Bagging](#KNN-with-Bagging)\n            * [Logistic Regression with Bagging](#Logistic-Regression-with-Bagging)\n        * [Pasting](#Pasting)\n            * [Decision Tree with Pasting](#Decision-Tree-with-Pasting)\n            * [Linear SVC with Pasting](#Linear-SVC-with-Pasting)\n        * [Adaboost](#Adaboost)\n            * [Decision Tree with Adaboost](#Decision-Tree-with-Adaboost)\n            * [Logistic Regression with Adaboost](#Logistic-Regression-with-Adaboost)\n        * [Gradient Boosting Classifier](#Gradient-Boosting-Classifier)\n        * [Voting, Boosting, Pasting and Bagging: Generating a Report table](#Voting,-Boosting,-Pasting-and-Bagging:-Generating-a-Report-table)\n        * [Principal Component Analysis](#Principal-Component-Analysis)\n            * [KNN Classifier with PCA](#KNN-Classifier-with-PCA)\n            * [SVM Classifier with PCA](#SVM-Classifier-with-PCA)\n            * [Logistic Regression with PCA](#Logistic-Regression-with-PCA)\n            * [Decision Tree Classifier with PCA](#Decision-Tree-Classifier-with-PCA)\n        * [Generating a Report table: PCA](#Generating-a-Report-table:-PCA)\n        * [Neural Network Model](#Neural-Network-Model) \n        \n        \n4. [Model Selection](#Model-Selection)   \n        \n        \n        ","a6133ebc":"# Data Pre-processing","761449c6":"## Hard Voting Classifier 2","9dc2ad76":"## Neural Network Model","44d0111f":"MinMax scaling is used to avoid any feature to dominate the model. MinMax scaling scales all the data in the columns between 0 to 1.","db46f515":"## Logistic Regression with Adaboost","874beefd":"Here, we see some interesting correlations. Deleting some variables that are highly correlated (0.8 and more) with each other to avoid overfitting the models and to avoid multicollinearity.","1f2a27a3":"'Detection_Risk' and 'Risk_F' have the same values throughout the columns. Deleting these columns.","86671542":"Amongst all the models Logistic Regression model with PCA seems to be the best model. ","40ca2261":"# Model Selection","00e68be7":"## Classification: Train-Test Split","dbba4893":"## KNN with Bagging","48ccc0aa":"Dropping duplicate values if any. Here the rows which intersected with both dataframes get deleted.  ","5e6577c4":"Money_Value has 1 missing value.","540edd73":"Here, we deleted the 'Risk_trial' column which as originally 'Risk' from 'trial.csv' as it had some values that were different from the 'Risk' column from 'audit_risk.csv'. \n\nThe paper provided by professor states that the values of 'Audit_Risk' being greater or equal to 1 are classified as 1 and 0 otherwise. \n\nThis condition is being satisfied by the 'Risk' column in 'audit_risk.csv' and not by the 'Risk' column in 'trial.csv'.","4b991f0f":"## Adaboost ","6a9e3877":"## Decision Tree Classifier with PCA","d72d46b3":"## Voting, Boosting, Pasting and Bagging: Generating a Report table\nFor comparing all the models, we will create a table and a plot.","084727a6":"# Principal Component Analysis","ea8a4189":"## Classification: Feature Scaling","88746388":"## Linear SVC with Pasting","a9f1415e":"Here, ``max_depth = 9`` is taken as the best parameter for Decision Tree as it is seen in the above Decision Tree section.","de700473":"The dataframes have been succesfully merged."}}