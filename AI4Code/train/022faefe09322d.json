{"cell_type":{"9a82ac09":"code","cf6e3a5d":"code","2e0d340b":"code","db60daed":"code","fce069b4":"code","28407669":"code","a3fcef63":"code","5770fd78":"code","c8b547f8":"code","f01b1b0a":"code","ec1aa343":"code","6c68b64b":"code","4c25b31b":"code","c49c682a":"code","7c3100cc":"code","b9f6647e":"code","1668312f":"code","d41bd9b6":"code","0b0a81d7":"code","f00e3ce1":"code","00caf9ba":"code","57129a26":"code","959faad5":"code","71939e18":"markdown"},"source":{"9a82ac09":"%%capture\n!pip install git+https:\/\/github.com\/tensorflow\/examples.git","cf6e3a5d":"import numpy as np \nimport pandas as pd\nimport os\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","2e0d340b":"DEBUG = False\n\ntrain_path = '..\/input\/sartorius-cell-instance-segmentation\/train\/'\n\nSEED = 42\nWIDTH, HEIGHT = 704, 520\n# RESIZE_WIDTH, RESIZE_HEIGHT = 128, 128\nRESIZE_WIDTH, RESIZE_HEIGHT = 512, 512\nBATCH_SIZE = 32\nBUFFER_SIZE = 32\n\nVAL_SPLIT = 0.2\n\nAUTO = tf.data.AUTOTUNE","db60daed":"train = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/train.csv')\ntrain.head()\n\nn_ids = train.id.nunique()\n\nif DEBUG:\n    unique_ids_train = list(set(train['id'].tolist()))[:BATCH_SIZE]\n    unique_ids_valid = list(set(train['id'].tolist()))[BATCH_SIZE:2*BATCH_SIZE]\nelse:\n    unique_ids_train = list(set(train['id'].tolist()))[:int(n_ids * (1 - VAL_SPLIT))]\n    unique_ids_valid = list(set(train['id'].tolist()))[int(n_ids * (1 - VAL_SPLIT)):]\n\n\ntemp = pd.DataFrame()\nfor sample_id in unique_ids_train:\n    query = train[train.id == sample_id]\n    temp = pd.concat([temp, query])\ntrain = temp\ntrain = train.reset_index(drop=True)\n\ntemp = pd.DataFrame()\nfor sample_id in unique_ids_valid:\n    query = train[train.id == sample_id]\n    temp = pd.concat([temp, query])\nvalid = temp\nvalid = train.reset_index(drop=True)\n    \nTRAIN_LENGTH = train['id'].nunique()\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\n\nVALID_LENGTH = valid['id'].nunique()\nVALIDATION_STEPS = VALID_LENGTH \/\/ BATCH_SIZE","fce069b4":"# ref: https:\/\/www.kaggle.com\/inversion\/run-length-decoding-quick-start\n\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros((shape[0] * shape[1]), dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\ndef get_mask(image_id, df):\n    current = df[df[\"id\"] == image_id]\n    labels = current[\"annotation\"].tolist()\n    \n    mask = np.zeros((HEIGHT, WIDTH))\n    for label in labels:\n        mask += rle_decode(label, (HEIGHT, WIDTH))\n    mask = mask.clip(0, 1)\n    \n    return mask","28407669":"def train_generator(df):\n    image_ids = set(df['id'].tolist())\n    \n    for image_id in image_ids:\n        image = cv2.imread(os.path.join(train_path, image_id) + '.png') \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        mask = get_mask(image_id, df)\n        \n        image = cv2.resize(image, (RESIZE_HEIGHT, RESIZE_WIDTH))\n        mask = cv2.resize(mask, (RESIZE_HEIGHT, RESIZE_WIDTH))\n        mask = mask.reshape((*mask.shape, 1))\n        \n        image = image.astype(np.float32)\n        mask = mask.astype(np.int32)\n        \n        yield image, mask","a3fcef63":"train_ds = tf.data.Dataset.from_generator(\n    lambda : train_generator(train), \n    output_types=(tf.float32, tf.int32),\n    output_shapes=((RESIZE_HEIGHT, RESIZE_WIDTH, 3), (RESIZE_HEIGHT, RESIZE_WIDTH, 1)))\n\nvalid_ds = tf.data.Dataset.from_generator(\n    lambda : train_generator(valid), \n    output_types=(tf.float32, tf.int32),\n    output_shapes=((RESIZE_HEIGHT, RESIZE_WIDTH, 3), (RESIZE_HEIGHT, RESIZE_WIDTH, 1)))\n","5770fd78":"class Augment(tf.keras.layers.Layer):\n    def __init__(self, seed=SEED):\n        super().__init__()\n        \n        self.augment_inputs = preprocessing.RandomFlip('horizontal', seed=seed)\n        self.augment_labels = preprocessing.RandomFlip('horizontal', seed=seed)\n        \n    def call(self, inputs, labels):\n        inputs = self.augment_inputs(inputs)\n        labels = self.augment_labels(labels)\n        return inputs, labels","c8b547f8":"train_ds = (\n    train_ds\n    .shuffle(BUFFER_SIZE)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .map(Augment())\n    .prefetch(AUTO))\n\nvalid_ds = (\n    valid_ds\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","f01b1b0a":"def display(display_list):\n    plt.figure(figsize=(20, 20))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n","ec1aa343":"for images, masks in train_ds.take(2):\n    sample_image, sample_mask = images[0], masks[0]\n    display([sample_image, sample_mask])\n","6c68b64b":"np.max(sample_mask)","4c25b31b":"base_model = tf.keras.applications.MobileNetV2(input_shape=[RESIZE_HEIGHT, RESIZE_WIDTH, 3], include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\n\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\ndown_stack.trainable = False","c49c682a":"up_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","7c3100cc":"def unet_model(output_channels : int):\n    inputs = tf.keras.layers.Input(shape=[RESIZE_HEIGHT , RESIZE_WIDTH, 3])\n    \n    skips = down_stack(inputs)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n    \n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip])\n    \n    last = tf.keras.layers.Conv2DTranspose(\n        filters=output_channels, kernel_size=3, strides=2,\n        padding='same', activation='sigmoid') #64x64 -> 128x128\n    \n    x = last(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)","b9f6647e":"from keras.losses import binary_crossentropy\nimport tensorflow.keras.backend as K\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(tf.cast(y_true, tf.float32), y_pred) + 0.5 * dice_loss(tf.cast(y_true, tf.float32), y_pred)","1668312f":"OUTPUT_CLASSES = 1\n\nmodel = unet_model(output_channels=OUTPUT_CLASSES)\nmodel.compile(optimizer='adam',\n              loss=bce_dice_loss,\n              metrics=['accuracy'])","d41bd9b6":"tf.keras.utils.plot_model(model, show_shapes=True)\n","0b0a81d7":"def create_mask(pred_mask):\n    pred_mask = tf.where(pred_mask > 0.5,1,0)\n#     pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask\n","f00e3ce1":"def show_predictions(dataset=None, num=1):\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask[0])])\n    else:\n        display([sample_image, sample_mask,\n                 create_mask(model.predict(sample_image[tf.newaxis, ...])[0])])","00caf9ba":"show_predictions(train_ds)","57129a26":"class DisplayCallback(tf.keras.callbacks.Callback):\n    def __init__(self):\n        super().__init__()\n    \n    def on_epoch_end(self, epoch, logs=None):\n#         clear_output(wait=True)\n        show_predictions()\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n","959faad5":"EPOCHS = 100\n\ndisplay_cb = DisplayCallback()\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'best_model\/',\n    save_best_only=True,\n    save_weights_only=False,\n)\nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau()\nes = tf.keras.callbacks.EarlyStopping(patience=15)\n\nmodel_history = model.fit(train_ds, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=valid_ds,\n                          callbacks=[display_cb, model_checkpoint, lr_reduce, es])\n","71939e18":"https:\/\/www.tensorflow.org\/tutorials\/images\/segmentation"}}