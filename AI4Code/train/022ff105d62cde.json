{"cell_type":{"0c234827":"code","4eea63a2":"code","78ddef01":"code","bfe29348":"code","2840cfbc":"code","a426bf23":"code","a4001d86":"code","4a0ebd77":"code","9500afe5":"code","7137616e":"code","39fff188":"code","ef200bee":"code","22a12a0e":"code","b8009fbc":"code","15940ed0":"code","b8f94821":"code","59f61b49":"code","abcee427":"code","fb12c582":"code","203979b5":"code","9095db71":"code","853b35a6":"code","e991bbc2":"code","196e19b7":"code","e3c82c18":"code","53c2f362":"code","ddfc24b1":"code","0a5db06d":"code","af71756f":"code","b622aff9":"code","bdb0360e":"code","0a778469":"code","b2ce5409":"code","8c11f09c":"code","7ec2d6f6":"code","d17e9d0b":"code","61488442":"code","2403513b":"code","4bb8a880":"code","bdca7e1e":"code","11deada4":"code","bb797a62":"code","16004561":"code","25ebf678":"code","80b2e7d7":"code","69e249cb":"code","aaee550e":"code","18d5716c":"code","712ce0b1":"code","ca521ade":"code","621553f9":"code","57bd2668":"code","230d3eab":"code","638cd6bc":"code","ca638514":"code","0de21164":"code","04e62e41":"code","d19f1eab":"code","354f511d":"code","e0bb7209":"code","f5d148b8":"code","6a71ad33":"code","60543551":"code","eb9c24fb":"code","d6c6db9f":"code","2702585e":"code","8987fb4a":"code","a3c59cf1":"code","3373740d":"code","2943898f":"markdown","4677b06a":"markdown","03e550a0":"markdown","2f2b7bb7":"markdown","9ad3c9b7":"markdown","ae8973a1":"markdown","2ee034b4":"markdown","cf4ac0db":"markdown","770df112":"markdown","723de26b":"markdown"},"source":{"0c234827":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4eea63a2":"import nltk","78ddef01":"#nltk.download()","bfe29348":"from nltk.corpus import brown\nbrown.words()","2840cfbc":"print(brown.categories())\nprint(len(brown.categories()))\n","a426bf23":"data = brown.sents(categories='fiction')       #i want to get sentences from the brown corpus ","a4001d86":"data\n#len(data)\n\n          ","4a0ebd77":"data[1]    \n' '.join(data[1])","9500afe5":"document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. \nI went to the market to buy some fruits.\"\"\"\n\nsentence = \"Send all the 50 documents related to chapters 1,2,3 at astha@gm.com\"","7137616e":"from nltk.tokenize import sent_tokenize,word_tokenize","39fff188":"sents = sent_tokenize(document)      #Sentence Tokenization\nprint(sents)              # this a list with 3 elements jo string hai\nprint(len(sents))","ef200bee":"sents[0]               ","22a12a0e":"sentence.split()        #isse bhi kr skte h .. word_tokenize ki jagah ","b8009fbc":"words = word_tokenize(sentence)   # breaks about special characters also","15940ed0":"words","b8f94821":"from nltk.corpus import stopwords\n\n","59f61b49":"sw = set(stopwords.words('english'))        #english k stopwords    set func se curly bracket me return hota h list me nhi ","abcee427":"print(sw)","fb12c582":"def remove_stopwords(text,stopwords):\n    useful_words = [w for w in text if w not in stopwords]        #list comprehension\n    return useful_words","203979b5":"text = \"i am not bothered about her very much\".split()  # split lgaya becoz text string h.. lekin list of words ki tarah bhejn chahiye\nuseful_text = remove_stopwords(text,sw)          # aur jab string pe iterate krte h it by default goes over every character \nprint(useful_text)","9095db71":"sentence = \"Send all the 50 documents related to chapters 1,2,3 at astha@gm.com\"","853b35a6":"sentence.split()   # isme numbers aa re hai .. word-tokenize me b ayge","e991bbc2":"from nltk.tokenize import RegexpTokenizer","196e19b7":"\ntokenizer = RegexpTokenizer('[a-zA-Z@.]+')   #tokenizer is the object of class RegexpTokenizer\nuseful_text = tokenizer.tokenize(sentence)  ","e3c82c18":"useful_text","53c2f362":"text= \"\"\"Foxes love to make jumps.The quick brown fox was seen jumping over the \n        lovely dog from a 6ft feet high wall\"\"\"","ddfc24b1":"from nltk.stem.snowball import SnowballStemmer, PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n#Snowball Stemmer, Porter, Lancaster Stemmer","0a5db06d":"\nps = PorterStemmer()","af71756f":"ps.stem('jumping')","b622aff9":"ps.stem('jumping')","bdb0360e":"ps.stem('lovely')","0a778469":"# Snowball Stemmer\nss = SnowballStemmer('english')       # kyuki ye multilingual hai ","b2ce5409":"ss.stem('lovely')","8c11f09c":"ss.stem('jumping')\n","7ec2d6f6":"\n## Lemmatization\nfrom nltk.stem import WordNetLemmatizer\n\nwn = WordNetLemmatizer()\nwn.lemmatize('jumping')","d17e9d0b":"\n# Sample Corpus - Contains 4 Documents, each document can have 1 or more sentences\ncorpus = [\n        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n        'We will win next Lok Sabha Elections, says confident Indian PM',\n        'The nobel laurate won the hearts of the people.',\n        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n]\n","61488442":"from sklearn.feature_extraction.text import CountVectorizer","2403513b":"cv = CountVectorizer()","4bb8a880":"\nvectorized_corpus = cv.fit_transform(corpus)","bdca7e1e":"\nvectorized_corpus = vectorized_corpus.toarray()","11deada4":"vectorized_corpus    # iss array me bhi 6th index with freq=1","bb797a62":"len(vectorized_corpus[0])    # means there are 42 unique words that are there in th entire corpus\n\n","16004561":"print(vectorized_corpus[0])  # 6th index pe 1 hai(freq=1)","25ebf678":"print(cv.vocabulary_)    #gives mapping between words and their index ","80b2e7d7":"len(cv.vocabulary_.keys())","69e249cb":"#REVERSE MAPPING - given a list of words .. how do u find the sentence ?","aaee550e":"numbers=vectorized_corpus[0]\nnumbers","18d5716c":"s = cv.inverse_transform(numbers)     #thi is how we can find out what words are there in the sentence \nprint(s)                     #we get jumbled words from here  ","712ce0b1":"cv = CountVectorizer?        # we can add our custom tokenizer","ca521ade":"def myTokenizer(document):                           #first convert into lowercase\n    words = tokenizer.tokenize(document.lower())    #this one is the nltk tokenizer which we made earlier \n    # Remove Stopwords\n    words = remove_stopwords(words,sw)\n    return words","621553f9":"myTokenizer(sentence)\n\n#print(sentence)","57bd2668":"cv = CountVectorizer(tokenizer=myTokenizer)","230d3eab":"vectorized_corpus = cv.fit_transform(corpus).toarray()","638cd6bc":"\nprint(len(vectorized_corpus[0]))              #42 to 33","ca638514":"cv.inverse_transform(vectorized_corpus)        #to see if these  woords are retained  .. and all other words are discarded","0de21164":"# For Test Data\ntest_corpus = [\n        'Indian cricket rock !',        \n]","04e62e41":"cv.transform(test_corpus).toarray()  ","d19f1eab":"#remember\ncv.fit_transform(test_corpus).toarray()        #Dont do this","354f511d":"cv.vocabulary_   #dont do this as the vocab is changed now ","e0bb7209":"sent_1  = [\"this is good movie\"]\nsent_2 = [\"this is good movie but actor is not present\"]\nsent_3 = [\"this is not good movie\"]\n","f5d148b8":"cv = CountVectorizer(ngram_range=(3,3))  # (2,2) for bigrams  ...(.ngram )","6a71ad33":"docs = [sent_1[0],sent_2[0]]\ncv.fit_transform(docs).toarray()\n","60543551":"cv.vocabulary_","eb9c24fb":"sent_1  = \"this is good movie\"    #doc1\nsent_2 = \"this was good movie\"     #doc2\nsent_3 = \"this is not good movie\"   #doc3\n\ncorpus = [sent_1,sent_2,sent_3]","d6c6db9f":"from sklearn.feature_extraction.text import TfidfVectorizer","2702585e":"tfidf = TfidfVectorizer()\n","8987fb4a":"vc = tfidf.fit_transform(corpus).toarray()","a3c59cf1":"print(vc)    #3rd -5th index is 0.... 5th had \"was\" so 'was' is not in sent3 so tf=0 for 'was' ","3373740d":"\ntfidf.vocabulary_   # not is at 3rd index  and in 3rd sentence not should have a higher weight\n","2943898f":"# 1. Get the Data\n* Get the Data from NLTK Corpora\n\n\n* or Scrape the Data\/ Use API","4677b06a":"# Vectorization with Stopword Removal","03e550a0":"# Stopwords","2f2b7bb7":"# More ways to create features","9ad3c9b7":"# Tokenization using Regular Expression","ae8973a1":"Unigram - every word as a feature\n\n\nBigrams\n\n\nTrigrams\n\n\nn-grams\n\n\nTF-IDF Normalisation","2ee034b4":"# Building a Vocab & Vectorization","cf4ac0db":"# Tf-idf Normalisation\nAvoid features that occur very often, becauase they contain less information\n\nInformation decreases as the number of occurences increases across different type of documents\n\nSo we define another term - term-document-frequency which associates a weight with every term","770df112":"# Tokenisation & Stopword Removal","723de26b":"# Stemming\nProcess that transforms particular words(verbs,plurals)into their radical form\n\n\nPreserve the semantics of the sentence without increasing the number of unique tokens\n\n\nExample - jumps, jumping, jumped, jump ==> jump"}}