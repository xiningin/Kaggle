{"cell_type":{"a8125b6c":"code","771d515e":"code","7119dbf3":"code","476346bb":"code","ec953b48":"code","a7f64b19":"code","efcbdc13":"code","d712d985":"code","36e577ad":"code","6b16658b":"code","8dd86411":"code","539ca316":"code","e3301bf0":"code","d7c0a5b1":"code","0253aee3":"code","961f7570":"code","c4d714bc":"code","0979f609":"code","961c8407":"code","4a49afc1":"code","f5e84004":"code","5b7fd315":"code","fe7085a4":"code","65c06380":"code","67b5536d":"code","c8eb3698":"code","53b999dc":"code","a4dc267a":"code","b979cd61":"code","59dfade8":"code","b5409015":"code","13d42cb6":"code","abaca6ba":"code","4fc42872":"code","ba24c431":"code","a5c509b1":"code","05fa24b0":"code","1d3529ae":"code","a5891988":"code","bbb20821":"code","9226083e":"code","3f22a271":"code","3f87e1c5":"code","c34ed646":"code","74c8d785":"code","892a9d78":"code","830ea190":"code","4556a71d":"markdown","c42c7c4b":"markdown","f7224f62":"markdown","577d135f":"markdown","11d27796":"markdown","f232f012":"markdown","0dafaed4":"markdown"},"source":{"a8125b6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","771d515e":"#load the second version that is v2 csv file\ndataset = pd.read_csv(\"\/kaggle\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv\")","7119dbf3":"#check the contents of first 5 rows\ndataset.head()","476346bb":"#check the length or number of rows\nlen(dataset)","ec953b48":"#check the datatyoes\ndataset.dtypes","a7f64b19":"#check for null values\ndataset.isnull().sum()","efcbdc13":"#describe the dataset as a table\ndataset.describe().T","d712d985":"#visualizing the number of rooms\nsns.catplot(\"rooms\", data = dataset, kind = \"count\", height = 6)","36e577ad":"#visualizing animals\nsns.catplot(\"animal\", data = dataset, kind = \"count\", height = 6)","6b16658b":"#visualizing furniture\nsns.catplot(\"furniture\", data = dataset, kind = \"count\", height = 6)","8dd86411":"#renaming the total columns \ndataset = dataset.rename(columns = {'total (R$)' : 'Total'}, errors = 'raise')","539ca316":"#determine the price with respect to area \nplt.figure(figsize=(10,6))\nsns.distplot(dataset[dataset.city=='S\u00e3o Paulo'].Total ,color='maroon',hist=False,label='S\u00e3o Paulo')\nsns.distplot(dataset[dataset.city=='Porto Alegre'].Total ,color='black',hist=False,label='Porto Alegre')\nsns.distplot(dataset[dataset.city=='Rio de Janeiro'].Total ,color='green',hist=False,label='Rio de Janeiro')\nsns.distplot(dataset[dataset.city=='Belo Horizonte'].Total ,color='blue',hist=False,label='Belo Horizonte')\nsns.distplot(dataset[dataset.city=='Campinas'].Total ,color='orange',hist=False,label='Campinas')\nplt.xlim(0,20000)","e3301bf0":"#total with respect to animals\nplt.figure(figsize=(10,6))\nsns.distplot(dataset[dataset.animal=='acept'].Total ,color='maroon',hist=False,label='accept')\nsns.distplot(dataset[dataset.animal=='not acept'].Total ,color='black',hist=False,label='not accept')\nplt.xlim(0,20000)","d7c0a5b1":"#total with respect to furniture\nplt.figure(figsize=(10,6))\nsns.distplot(dataset[dataset.furniture=='furnished'].Total ,color='maroon',hist=False,label='Furnished')\nsns.distplot(dataset[dataset.furniture=='not furnished'].Total ,color='black',hist=False,label='Unfurnished')\nplt.xlim(0,20000)","0253aee3":"#Label Encode the objects\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset.city = le.fit_transform(dataset.city)\ndataset.floor = le.fit_transform(dataset.floor)\ndataset.animal = le.fit_transform(dataset.animal)\ndataset.furniture = le.fit_transform(dataset.furniture)","961f7570":"#check data type\ndataset.dtypes","c4d714bc":"#check correlation\ndataset.corr().style.background_gradient(cmap = 'coolwarm')","0979f609":"#dividing into X and y\nX = dataset.iloc[:,0:-1].values\ny = dataset.iloc[:,-1].values","961c8407":"#confirming with backward elimination \nimport statsmodels.api as sm\ndef backwardElimination(x, sl):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n    regressor_OLS.summary()\n    return x","4a49afc1":"#setting significance level to 0.05\n#Optimal values of X taken\nSL = 0.05\nX_opt = X[:, [0, 1, 2, 3, 4, 5,6,7]]","f5e84004":"#Obtain the features having most importance\nX_Modeled = backwardElimination(X_opt, SL)","5b7fd315":"X_Modeled","fe7085a4":"#load the values into X and y\n#all features except the (R$) features are being taken\nX = dataset.iloc[:,0:8].values\ny = dataset.iloc[:,12:13].values","65c06380":"#split into training and test set \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)","67b5536d":"#multivariate linear regression\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","c8eb3698":"#predict value\ny_pred = regressor.predict(X_test)","53b999dc":"#calculating the r2 score and mse\nfrom sklearn.metrics import r2_score, mean_squared_error","a4dc267a":"#r2 score\nr2_score(y_test, y_pred)","b979cd61":"#mse value\nmean_squared_error(y_test, y_pred)","59dfade8":"#loading X with updated features\nX = dataset.iloc[:,[3,5,6,7]].values","b5409015":"#splitting into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)","13d42cb6":"#multivariate linear regression\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","abaca6ba":"#predict value\ny_pred = regressor.predict(X_test)","4fc42872":"#r2 score\nr2_score(y_test, y_pred)","ba24c431":"#mse value\nmean_squared_error(y_test, y_pred)","a5c509b1":"#polynomial distribution\nX = dataset.iloc[:,[3,5,6,7]].values\ny = dataset.iloc[:,12:13].values","05fa24b0":"#fitting into polynomial equation\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 2)\nX_poly = poly.fit_transform(X)","1d3529ae":"#split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_poly,y,test_size = 0.2, random_state = 0)","a5891988":"#multivariate linear regression\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","bbb20821":"#predict value\ny_pred = regressor.predict(X_test)","9226083e":"#r2 score\nr2_score(y_test, y_pred)","3f22a271":"#mse value\nmean_squared_error(y_test, y_pred)","3f87e1c5":"#load X and y\nX = dataset.iloc[:,[3,5,6,7]].values\ny = dataset.iloc[:,-1].values","c34ed646":"#split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_poly,y,test_size = 0.2, random_state = 0)","74c8d785":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 5)\nregressor.fit(X_train, y_train)","892a9d78":"#predicting the value\ny_pred = regressor.predict(X_test)","830ea190":"#r2 score\nr2_score(y_test, y_pred)","4556a71d":"It can be seen that the r2 score has improved tremendously. Though it does not have a large value it is in accordance to the correlation value that we saw in the heatmap above.\n\nNext lets check if there is any improvement by using **Polynomial Regression.**","c42c7c4b":"![5d603a4300ef2b75f368a885.jpg](attachment:5d603a4300ef2b75f368a885.jpg)\n\nHello! In this notebook I shall analyze the dataset by using visualizations, then the features that affect the Total is determined by using correlation heatmap and by using Ordinary Least Square Method to determine the P value and elimintae the features that do not affect the Total by using Backward Elimination Method.\n\nOnce the necessary features are determined we shall run various regression models to determine the r2 score of each model. The regression models used are,\n1. Multivariate Regression\n2. Polynomial Regression\n3. Random Forest Regression\n\nSo lets begin!","f7224f62":"Here, the r2 score has infact fallen a bit from 0.16 to 0.1521. Maybe Multivariate Regression was better than Polynomial? Lets now check the **Random Forest Regression**.","577d135f":"The r2 score of the Random Forest Regressor is worse than that of the Multivariate and the Polynomial Regression. \n\nIt can be concluded that for this particular dataset the Multivariate Linear Regression shows the best behaviour of the Total with respect to select features. Ofcourse the R$ values werent taken as that would just some up to form the Total and there would not be any real predictions if the values were known.\n\nThis is my second notebook and I would love to hear some feedback and maybe even upvotes if you liked it!\nThank You!","11d27796":"As you can see above, apart from the (R$) values the features that make the most impact are rooms, bathrooms and parking spaces.\n\nTo further check the correlation of the features and the features that are most important to determine the Total, let us take the approach of **Backward Elimination Algorithm** by using the Ordinary Least Square method to find the summary of the dataset.","f232f012":"Clearly the r2 score is very bad, it is infact worse than the median value. This might be because all the features apart from R$ features are taken and some of them have inverse relation and negative coefficients.\n\nLets now select only the features that were determined by Backward Elimination.","0dafaed4":"By comapring the values of X modeled with the dataset it can be concluded that the features having the p value less than 0.05 are bathroom, floor, animal and furniture.\n\nNow, lets begin the **Regression**"}}