{"cell_type":{"fde24205":"code","3fe40e7b":"code","6b48b4dd":"code","e0c524dc":"code","505f945e":"code","9371b6a7":"code","584c0389":"code","27b07c43":"code","fee3756a":"code","a3965814":"code","3a6e66d2":"code","dd16f025":"code","c53a3c20":"code","a75e0dad":"code","0336b468":"code","0e9b6b36":"code","b9afd2d3":"code","715e4840":"code","4d28ca3e":"code","b1ee3253":"code","ef607101":"code","4db780b7":"code","a36233cb":"markdown"},"source":{"fde24205":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fe40e7b":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.pipeline import Pipeline\n\nimport seaborn as sns, matplotlib.pyplot as plt\n\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","6b48b4dd":"# Fake News loaded\nfake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake.head()","e0c524dc":"# Fake news are labelled as 1\nfake['label'] = 1\nfake.head()","505f945e":"# True news loaded\ntrue = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ntrue.head()","9371b6a7":"# True news labelled as 0\ntrue['label'] = 0\ntrue.head()","584c0389":"len(true), len(fake)","27b07c43":"# Both fake and true news are combined and first 2000 data points are only selected\ndf = pd.concat([fake.iloc[:1000,:],true.iloc[:1000,:]], axis=0)\ndf = df.sample(len(df))\ndf = df.reset_index(drop=True)\ndf.head()","fee3756a":"# Text & Title columns are combined and other columns except label are dropped\ndf.text = df.title + df.text\ndf = df[['text','label']]\ndf.head()","a3965814":"X = df.text\ny = df.label","3a6e66d2":"# Data processing is done\ncorpus = []\nps = PorterStemmer()\n\nfor i in X:\n    # Except alphabets, everything is removed\n    new = re.sub('[^a-zA-Z]', ' ',i)\n    # Converted to lower case\n    new = new.lower()\n    # Word tolenizing done\n    new = nltk.word_tokenize(new)\n    # English stop words are removed\n    new = [ps.stem(i) for i in new if i not in stopwords.words('english')]\n    new = ' '.join(new)\n    corpus.append(new)","dd16f025":"# Splitting into train and test data\nX_train,X_test, y_train,y_test = train_test_split(corpus,y, train_size=0.7, random_state=100, stratify=y)","c53a3c20":"# 2 models are considered included in a pipeline to produce bag of words\nmnb = Pipeline([('cnt_vec', CountVectorizer()),\n               ('mnb', MultinomialNB())])\n\nbnb = Pipeline([('cnt_vec', CountVectorizer()),\n               ('bnb', BernoulliNB())])\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)","a75e0dad":"# Cross validation for first model\ncv_mnb = cross_val_score(mnb,X_train,y_train,cv=folds)\ncv_mnb.mean()","0336b468":"# Cross validation for second model\ncv_bnb = cross_val_score(bnb,X_train,y_train,cv=folds)\ncv_bnb.mean()","0e9b6b36":"# Hyperparameter tuning to find the best model and parameters\nclassifier = Pipeline([('classifier', mnb)])\n\nhyp = [{'classifier':[mnb],\n       'classifier__cnt_vec__ngram_range':[(1,1),(1,2),(2,1),(2,2)]},\n      {'classifier':[bnb],\n       'classifier__cnt_vec__ngram_range':[(1,1),(1,2),(2,1),(2,2)]}]\n\ngrid = GridSearchCV(estimator=classifier, param_grid=hyp, cv=folds, n_jobs=-1, scoring='accuracy',\n                   verbose=3, return_train_score=True)\ngrid.fit(X_train,y_train)","b9afd2d3":"grid.best_estimator_, grid.best_score_","715e4840":"model = grid.best_estimator_","4d28ca3e":"# Final training of model\nmodel.fit(X_train,y_train)\ny_test_pred = model.predict(X_test)\nprint('Accuracy of test data =',100*accuracy_score(y_test, y_test_pred),'%')","b1ee3253":"conf = confusion_matrix(y_test, y_test_pred)\nsns.heatmap(conf, annot=True)\nplt.show()","ef607101":"y_test_proba = [i[1] for i in model.predict_proba(X_test)]\n\nauc = roc_auc_score(y_test,y_test_proba)\nfpr,tpr,thresh = roc_curve(y_test,y_test_proba)\nplt.plot(fpr,tpr)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend(('AUC Score = {}%'.format(round(auc,2)),),loc='lower right')\nplt.show()","4db780b7":"imp = pd.DataFrame({'features':model['classifier']['cnt_vec'].get_feature_names(), 'coef':model['classifier']['bnb'].coef_[0]})\n\ntop20 = imp.sort_values('coef',ascending=False).iloc[:20,:]\ndown20 = imp.sort_values('coef').iloc[:20,:]\n\nplt.figure(figsize=(20,8))\nplt.bar(top20.features,top20.coef)\nplt.title('Top 20 words in true news', fontsize=24)\nplt.show()\n\nplt.figure(figsize=(20,8))\nplt.bar(down20.features,down20.coef)\nplt.title('Top 20 words in false news', fontsize=24)\nplt.xticks(rotation=90)\nplt.show()","a36233cb":"## Processing"}}