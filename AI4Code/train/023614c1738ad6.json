{"cell_type":{"a52ee5b8":"code","9a569e4a":"code","ff3f356b":"code","74eb1151":"code","4a3950b0":"code","2b175bc1":"code","999f819c":"code","55aede3d":"code","e13d853b":"code","5dc95615":"code","7fcda6a4":"code","a670de7f":"code","03f63f24":"code","282bf22d":"code","9216b72f":"code","506ca305":"code","457f2670":"code","0f0d4e08":"code","1d00f950":"code","9882807b":"code","4db2b034":"code","54ae3f54":"code","7228e3fe":"code","f8215a42":"code","b9331fc2":"code","d442310c":"code","c3364c16":"markdown","979e437b":"markdown","81aa1fce":"markdown","92a43628":"markdown","949319d1":"markdown","45920544":"markdown","1b402734":"markdown","1c57a4cd":"markdown"},"source":{"a52ee5b8":"# General imports\nimport os\nimport cv2\nimport glob \nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import *\nfrom sklearn.tree import *\nfrom sklearn.metrics import *\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\nfrom sklearn.decomposition import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\n\nSEED = 42\nnp.random.seed(SEED)\nsns.set_style(\"dark\")\nmpl.rcParams['figure.dpi'] = 200\n%matplotlib inline","9a569e4a":"def rmse(preds, y):\n    mse = mean_squared_error(preds, y)\n    return np.sqrt(mse)\n\ndef write_subfile(reg, scaler=None, filename=\"submission.csv\"):\n    test_data = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\n    ids = test_data[\"id\"]\n    test_data = test_data.drop(columns=[\"id\"])\n    if scaler:\n        test_data = scaler.transform(test_data)\n    y = reg.predict(test_data)\n    df = pd.DataFrame({\n        \"id\" : ids,\n        \"loss\" : y\n    })\n    df.to_csv(filename, index=False)","ff3f356b":"data = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ndata.head()","74eb1151":"data.describe()","4a3950b0":"Y = data[\"loss\"]\nX = data.drop(columns=[\"id\", \"loss\"])\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X) \n\nX_scaled.shape, Y.shape","2b175bc1":"import optuna\nfrom optuna.samplers import TPESampler\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)","999f819c":"from xgboost import XGBRegressor\n\n# Setup XGB hyperparameters for exps\ndef get_xgb_hyperparams(trail):\n    xgb_params = {\n        'learning_rate': 0.01,\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'n_estimators': trail.suggest_int('n_estimators', 500, 2500, 100),\n        'reg_lambda': trail.suggest_int('reg_lambda', 1, 100),\n        'reg_alpha': trail.suggest_int('reg_alpha', 1, 100),\n        'subsample': trail.suggest_float('subsample', 0.2, 1.0, step=0.1),\n        'colsample_bytree': trail.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n        'max_depth': trail.suggest_int('max_depth', 3, 10), \n        'min_child_weight': trail.suggest_int('min_child_weight', 2, 10),\n        'gamma': trail.suggest_float('gamma', 0, 20)        \n    }\n    return xgb_params","55aede3d":"# Define objective function\ndef objective_xgb(trail, X, Y, n_splits=4):\n       \n    xgb_params = get_xgb_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        xgb_reg = XGBRegressor(**xgb_params)\n    \n        xgb_reg = xgb_reg.fit(x_train, y_train)\n        preds = xgb_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss \/ n_splits","e13d853b":"# Callback function to print log messages when the best trail is updated\n\ndef logging_callback(study, frozen_trail):\n    prev_best = study.user_attrs.get('prev_best', None)\n    if prev_best != study.best_value:\n        study.set_user_attr('prev_best', study.best_value)\n        print(f\"Trail {frozen_trail.number} finished with best value {frozen_trail.value}\")","5dc95615":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='xgb_tuning')\nobjc = lambda trail : objective_xgb(trail, X_scaled, Y)\n\nstudy.optimize(objc, n_trials=500, timeout=60*60, callbacks=[logging_callback])","7fcda6a4":"print(f\"Best rmse value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","a670de7f":"xgb_reg = XGBRegressor(**study.best_params,\n                      learning_rate= 0.01,\n                      tree_method= 'gpu_hist',\n                      booster= 'gbtree')\n\nxgb_reg.fit(X_scaled, Y, eval_metric='rmse', verbose=True)\n\nprint(f\"RMSE on training data : {rmse(Y, xgb_reg.predict(X_scaled))}\")","03f63f24":"write_subfile(xgb_reg, scaler, \"xgb_tuned.csv\") # score: 7.87652","282bf22d":"from catboost import CatBoostRegressor\n\n# Setup CatB hyperparameters for exps\ndef get_catb_hyperparams(trail):\n    catb_params = {\n        'loss_function': 'RMSE',\n        'task_type': 'GPU',\n        'eval_metric': 'RMSE',\n        'bootstrap_type': 'Bernoulli',\n        'iterations': trail.suggest_int('iterations', 1000, 5000),\n        'od_wait': trail.suggest_int('od_wait', 500, 2000),\n        'learning_rate': trail.suggest_uniform('learning_rate', 0.01, 0.5),\n        'reg_lambda': trail.suggest_uniform('reg_lambda', 1e-4, 100),\n        'subsample': trail.suggest_uniform('subsample', 0, 1),\n        'random_strength': trail.suggest_uniform('random_strength', 10, 50),\n        'depth': trail.suggest_int('depth', 1, 15),\n        'min_data_in_leaf': trail.suggest_int('min_data_in_leaf', 1, 30),\n        'leaf_estimation_iterations': trail.suggest_int('leaf_estimation_iterations', 1, 15)\n    }\n    return catb_params","9216b72f":"# Define objective function\ndef objective_catb(trail, X, Y, n_splits=4):\n    \n    catb_params = get_catb_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        catb_reg = CatBoostRegressor(**catb_params)\n    \n        catb_reg = catb_reg.fit(x_train, y_train)\n    \n        preds = catb_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss \/ n_splits","506ca305":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='catb_tuning')\nobjc = lambda trail : objective_catb(trail, X_scaled, Y)\n\nstudy.optimize(objc, timeout=60*5, callbacks=[logging_callback])","457f2670":"print(f\"Best rmse value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","0f0d4e08":"catb_reg = CatBoostRegressor(**study.best_params,\n                            loss_function='RMSE',\n                            task_type='GPU',\n                            eval_metric='RMSE',\n                            bootstrap_type='Bernoulli')\n\ncatb_reg.fit(X_scaled, Y)\n\nprint(f\"RMSE on training data : {rmse(Y, catb_reg.predict(X_scaled))}\")","1d00f950":"write_subfile(catb_reg, scaler, \"catb_tuned.csv\") # score: 7.89228","9882807b":"from lightgbm import LGBMRegressor\n\n# Setup lgbm hyperparameters for exps\ndef get_lgbm_hyperparams(trail):\n    lgbm_params = {\n        \"objective\": \"rmse\",\n        \"metric\": \"rmse\",\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.008,\n        'device': 'gpu',\n        'n_estimators': trail.suggest_int(\"n_estimators\", 500, 4000),\n        \"num_leaves\": trail.suggest_int(\"num_leaves\", 8, 256),\n        \"min_child_samples\": trail.suggest_int(\"min_child_samples\", 2, 3000),\n        'feature_fraction': trail.suggest_uniform('feature_fraction', 0.25, 0.7),\n        'bagging_fraction': trail.suggest_uniform('bagging_fraction', 0.7, 1.0),\n        'bagging_freq': trail.suggest_int('bagging_freq', 0, 5),\n        'reg_alpha': trail.suggest_int(\"reg_alpha\", 1, 100),\n        'reg_lambda': trail.suggest_int(\"reg_lambda\", 1, 100),\n    }\n    return lgbm_params","4db2b034":"# Define objective function\ndef objective_lgbm(trail, X, Y, n_splits=4):\n\n    lgbm_params = get_lgbm_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        lgbm_reg = LGBMRegressor(**lgbm_params)\n    \n        lgbm_reg.fit(x_train, y_train)\n    \n        preds = lgbm_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss \/ n_splits","54ae3f54":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='lgbm_tuning')\nobjc = lambda trail : objective_lgbm(trail, X_scaled, Y)\n\nstudy.optimize(objc, timeout=60*60, callbacks=[logging_callback])","7228e3fe":"print(f\"Best value (rmse): {study.best_value:.5f}\")\nprint(f\"Best params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t{key}: {value}\")","f8215a42":"lgbm_reg = LGBMRegressor(**study.best_params,\n                        objective=\"rmse\",\n                        metric=\"rmse\",\n                        boosting_type=\"gbdt\",\n                        learning_rate=0.008,\n                        device='gpu')\n\nlgbm_reg.fit(X_scaled, Y, eval_metric='rmse', verbose=True)\n\nprint(f\"RMSE on training data : {rmse(Y, lgbm_reg.predict(X_scaled))}\")","b9331fc2":"write_subfile(lgbm_reg, scaler, \"lgbm_tuned.csv\") # score: 7.89669","d442310c":"xgb_preds = pd.read_csv('xgb_tuned.csv')['loss']\ncatb_preds = pd.read_csv('catb_tuned.csv')['loss']\nlgbm_preds = pd.read_csv('lgbm_tuned.csv')['loss']\n\nscores = [7.87652, 7.89228, 7.87669]\n\nweights = [1 \/ score for score in scores]\nweights = np.array([weight \/ sum(weights) for weight in weights])\n\npreds = np.array([xgb_preds, catb_preds, lgbm_preds])\n\nensemble_preds = np.zeros_like(xgb_preds)\n\nfor pred, weight in zip(preds, weights):\n    ensemble_preds += weight * pred\n    \nensemble_csv = pd.read_csv('xgb_tuned.csv') # Score: 7.87595\nensemble_csv['loss'] = ensemble_preds\nensemble_csv.to_csv('ensemble.csv', index=False)","c3364c16":"## Helper functions","979e437b":"## Ensembling","81aa1fce":"## Tuning Cat Boost Regressor using Optuna","92a43628":"## Tuning XGB Regressor using Optuna","949319d1":"## General Imports","45920544":"## Hyperparameter tuning and ensemble of XGB, CatB, LGBM\n\nThis notebook contains \n* Hyperparamter tuning of XGBoost, CatBoost, LightBGM with KFold cross validation.\n* Weighted ensemble of predictions obtained from tuned models with weights as reciprocal of individual scores. (This gave a score of **7.87595** on public LB data)\n\nReferences:\n* https:\/\/www.kaggle.com\/c\/tabular-playground-series-aug-2021\/discussion\/258009","1b402734":"## Tuning Light GBM Regressor using Optuna","1c57a4cd":"## Reading and scaling data"}}