{"cell_type":{"59a98596":"code","3a337dd2":"code","4430a00d":"code","58708ecc":"code","cac11d2e":"code","5b9b6f2e":"code","736601c0":"code","b394c178":"code","0b685284":"code","c3f6ad54":"code","4aacfc56":"code","98645ff3":"code","0e7fb70d":"code","2f92c085":"code","a8ea28db":"code","312708ca":"code","5567d9c9":"code","01a20b19":"code","3a724d38":"code","8e49aeb4":"code","a5ecd4b2":"code","2ffea74f":"code","7cf991f1":"code","3d65198e":"code","e00c6733":"code","9330007a":"code","c111175d":"code","54d5f66a":"code","ad69f27b":"code","cdf2b8ab":"code","bef88622":"code","377cc627":"code","953da7e4":"code","2feb2458":"code","e7dc5c70":"code","03027090":"code","b69b650f":"code","efea7338":"code","c006e1a9":"code","841cf4c7":"code","961dbf16":"code","76196e0a":"code","786026ab":"code","e6353c88":"code","5fb195b6":"code","b3f288a8":"code","4ee0ce39":"code","3e2c6f1f":"code","6cd310b8":"code","a923e552":"code","4aed08b6":"code","c1e0bd05":"code","5420b32c":"code","c90fbd46":"code","d2e4e724":"code","f9ca2129":"code","62d1b892":"code","6b3ef6a1":"code","313cc4ae":"code","86569541":"code","601d2e46":"code","a4a350d4":"code","260fc1ea":"code","911fb4d6":"code","f77d1ce0":"code","54d94406":"code","5b82ae4f":"code","e79caa6f":"code","e3bf0564":"code","a656edec":"code","b4dfc844":"code","72d585ac":"markdown","e65ce38b":"markdown","c81ae520":"markdown","7bef7d9d":"markdown","e7fb6355":"markdown","b4d32265":"markdown","6e568097":"markdown","cf305acd":"markdown"},"source":{"59a98596":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import Series, DataFrame\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n#Display full output in each jupyter cell, not jut the last statement\nfrom IPython.core.interactiveshell import InteractiveShell\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n","3a337dd2":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n  ","4430a00d":"# Loading the datasets\n\ndf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","58708ecc":"df.head()","cac11d2e":"# remove ID column\n\ndf.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","5b9b6f2e":"df.shape","736601c0":"test.shape","b394c178":"df.info()","0b685284":"df.describe([0.25,0.5,0.75,0.99])","c3f6ad54":"df.isnull().sum().sort_values(ascending=False).head(20)","4aacfc56":"#Looking at the percentage of null values\nround(df.isnull().sum()*100\/df.shape[0],2).sort_values(ascending=False).head(20)","98645ff3":"# Considering 10% as threshold and dropping columns having more than threshold NaN values\nthreshold =10\ndrop_cols = round(df.isnull().sum()*100\/df.shape[0],2)[round(df.isnull().sum()*100\/df.shape[0],2)>threshold].index.tolist()\ndrop_cols","0e7fb70d":"df.drop(columns = drop_cols, axis=1, inplace=True)\ntest.drop(columns=drop_cols, axis=1, inplace=True)","2f92c085":"df.shape","a8ea28db":"test.shape","312708ca":"# Checking remaining columns with null values and imputing them\n\n# NA in GarageType, GarageFinish, GarageQual and GarageCond means 'No Garage. so we will replace 'NA' by 'No Garage'\ndf['GarageType'].fillna('No Garage', inplace=True)\ndf['GarageFinish'].fillna('No Garage', inplace=True)\ndf['GarageQual'].fillna('No Garage', inplace=True)\ndf['GarageCond'].fillna('No Garage', inplace=True)\n# NA in BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual means 'No Basement'. so we will replace 'NA' by 'No Basement'          \ndf['BsmtFinType2'].fillna('No Basement', inplace=True)\ndf['BsmtExposure'].fillna('No Basement', inplace=True)         \ndf['BsmtFinType1'].fillna('No Basement', inplace=True)         \ndf['BsmtCond'].fillna('No Basement', inplace=True)         \ndf['BsmtQual'].fillna('No Basement', inplace=True)\ndf['MasVnrType'].fillna('None', inplace=True)\ndf['MasVnrArea'].fillna(0, inplace=True)\n\n# Converting years to 'number of years' for GarageYrBlt, YearBuilt , YearRemodAdd & YrSold\ndf['GarageYrBlt'] = 2021-df['GarageYrBlt']\ndf['YearBuilt'] = 2021-df['YearBuilt']\ndf['YearRemodAdd'] = 2021-df['YearRemodAdd']\ndf['YrSold'] = 2021-df['YrSold']\n\n# Imputing GarageYrBlt with -1, since these houses don't have garage \ndf['GarageYrBlt'].fillna(-1, inplace=True)","5567d9c9":"# doing same for test dataset\ntest['GarageType'].fillna('No Garage', inplace=True)\ntest['GarageFinish'].fillna('No Garage', inplace=True)\ntest['GarageQual'].fillna('No Garage', inplace=True)\ntest['GarageCond'].fillna('No Garage', inplace=True)\ntest['BsmtFinType2'].fillna('No Basement', inplace=True)\ntest['BsmtExposure'].fillna('No Basement', inplace=True)         \ntest['BsmtFinType1'].fillna('No Basement', inplace=True)         \ntest['BsmtCond'].fillna('No Basement', inplace=True)         \ntest['BsmtQual'].fillna('No Basement', inplace=True)\ntest['MasVnrType'].fillna('None', inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)\n\ntest['GarageYrBlt'] = 2021-df['GarageYrBlt']\ntest['YearBuilt'] = 2021-df['YearBuilt']\ntest['YearRemodAdd'] = 2021-df['YearRemodAdd']\ntest['YrSold'] = 2021-df['YrSold']\n\ntest['GarageYrBlt'].fillna(-1, inplace=True)","01a20b19":"# Checking remaining columns with null values and imputing them\ndf.isnull().sum()","3a724d38":"test.isnull().sum().sum()\n# Checking remaining columns with null values and imputing them\nround(test.isnull().sum()*100\/test.shape[0],2)[round(test.isnull().sum()*100\/test.shape[0],2)>0].sort_values(ascending=False)","8e49aeb4":"# replacing null values columns having more than threshold NaN values\nthreshold =0.07\ncols = round(test.isnull().sum()*100\/test.shape[0],2)[round(test.isnull().sum()*100\/test.shape[0],2)>=threshold].index.tolist()\ncols","a5ecd4b2":"test['MSZoning'].fillna(0, inplace=True)\ntest['Functional'].fillna(0, inplace=True)\ntest['GarageCars'].fillna(0, inplace=True)\ntest['Exterior2nd'].fillna(0, inplace=True)\ntest['SaleType'].fillna(0, inplace=True)\ntest['BsmtFinSF2'].fillna(0, inplace=True)\ntest['BsmtUnfSF'].fillna(0, inplace=True)\ntest['TotalBsmtSF'].fillna(0, inplace=True)\ntest['KitchenQual'].fillna(0, inplace=True)\ntest['BsmtFinSF1'].fillna(0, inplace=True)\ntest['Exterior1st'].fillna(0, inplace=True)\ntest.shape","2ffea74f":"df.dropna(axis=0, inplace=True)\ndf.shape","7cf991f1":"# Outlier Treatment\n\ndf.describe()","3d65198e":"# Removing outliers using IQR technique, taking the lower and upper quantile as 0.25 & 0.99 respectively\nnum_col = list(df.dtypes[df.dtypes != 'object'].index)\n\ndef drop_outliers(x):\n    list = []\n    for col in num_col:\n        Q1 = x[col].quantile(.25)\n        Q3 = x[col].quantile(.99)\n        IQR = Q3 - Q1\n        x = x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))]\n    return x\ndf = drop_outliers(df)         \n","e00c6733":"df.describe()","9330007a":"# Dropping PoolArea column, since all values are 0 after removing outliers\ndf.drop(columns=['PoolArea'], inplace=True)\ntest.drop(columns=['PoolArea'], inplace=True)","c111175d":"# Visualizing thr target variable\n\nplt.title('SalePrice')\nsns.distplot(df['SalePrice'], bins=10)\nplt.show()","54d5f66a":"# Now we can see our target variable SalePrice is skewed, so doing log transformation\n\ndf['SalePrice'] = np.log1p(df['SalePrice'])\n\nplt.title('SalePrice')\nsns.distplot(df['SalePrice'])\nplt.show()","ad69f27b":"# visualizing numeric variables\n# get the numeric variables list\n\nnum_vars = list(df.dtypes[df.dtypes != 'object'].index)\n\ndf[num_vars].head()","cdf2b8ab":"plt.figure(figsize=(10,5))\nsns.pairplot(df, x_vars=['MSSubClass', 'LotArea', 'MasVnrArea', 'GrLivArea', 'GarageArea'], y_vars=['SalePrice'], kind='scatter')\nsns.pairplot(df, x_vars=['OverallQual', 'OverallCond', 'LowQualFinSF', 'MiscVal', 'MoSold'], y_vars=['SalePrice'], kind='scatter')\nsns.pairplot(df, x_vars=['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', '2ndFlrSF'], y_vars=['SalePrice'], kind='scatter')\nsns.pairplot(df, x_vars=['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'GarageCars'], y_vars=['SalePrice'], kind='scatter')\nsns.pairplot(df, x_vars=['BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces'], y_vars=['SalePrice'], kind='scatter')\nsns.pairplot(df, x_vars=['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch'], y_vars=['SalePrice'], kind='scatter')\nplt.show()","bef88622":"# Heatmap\nplt.figure(figsize = (20, 20))\nsns.heatmap(df[num_vars].corr(), annot=True, cmap=\"YlGnBu\")\nplt.show()","377cc627":"# Dropping and clipping variables\ndf.drop(columns=['MSSubClass', 'LowQualFinSF', 'MiscVal', '3SsnPorch', 'KitchenAbvGr','BsmtFullBath','BsmtHalfBath',], inplace=True)\ndf['LotArea'].clip(0,60000)\ndf['GrLivArea'].clip(0,3000)\ndf['TotalBsmtSF'].clip(0,3000)\ndf['BsmtFinSF1'].clip(0,2500)\ndf['1stFlrSF'].clip(0,3000)\ndf['GarageArea'].clip(0,1200)\ndf['OpenPorchSF'].clip(0,400)\ndf.shape","953da7e4":"test.drop(columns=['MSSubClass', 'LowQualFinSF', 'MiscVal', '3SsnPorch', 'KitchenAbvGr','BsmtFullBath','BsmtHalfBath'], inplace=True)\ntest['LotArea'].clip(0,60000)\ntest['GrLivArea'].clip(0,3000)\ntest['TotalBsmtSF'].clip(0,3000)\ntest['BsmtFinSF1'].clip(0,2500)\ntest['1stFlrSF'].clip(0,3000)\ntest['GarageArea'].clip(0,1200)\ntest['OpenPorchSF'].clip(0,400)\ntest.shape","2feb2458":"df.drop(columns=['GarageArea','TotRmsAbvGrd','1stFlrSF','2ndFlrSF','FullBath','GarageYrBlt', 'YearRemodAdd'], inplace=True)\ntest.drop(columns=['GarageArea','TotRmsAbvGrd','1stFlrSF','2ndFlrSF','FullBath','GarageYrBlt', 'YearRemodAdd'], inplace=True)","e7dc5c70":"# Checking for skewness\nfrom scipy.stats import kurtosis, skew\nnum_var = df.select_dtypes(include = ['int64', 'float64'])\nskewness_num = num_var.apply(lambda x: skew(x)).sort_values(ascending = False)\nprint(skewness_num)","03027090":"# Using log transformation for fixing skewness within variables\ndf['BsmtFinSF2'] = np.log1p(df['BsmtFinSF2'])\ndf['ScreenPorch'] = np.log1p(df['ScreenPorch'])\ndf['LotArea'] = np.log1p(df['LotArea'])\ndf['EnclosedPorch'] = np.log1p(df['EnclosedPorch'])\ndf['MasVnrArea'] = np.log1p(df['MasVnrArea'])\ndf['OpenPorchSF'] = np.log1p(df['OpenPorchSF'])\ndf['WoodDeckSF'] = np.log1p(df['WoodDeckSF'])\ndf['GrLivArea'] = np.log1p(df['GrLivArea'])\ndf['BsmtUnfSF'] = np.log1p(df['BsmtUnfSF'])\ndf.shape","b69b650f":"num_var = test.select_dtypes(include = ['int64', 'float64'])\nskewness_num = num_var.apply(lambda x: skew(x)).sort_values(ascending = False)\nprint(skewness_num)","efea7338":"# Using log transformation for fixing skewness within variables\ntest['BsmtFinSF2'] = np.log1p(test['BsmtFinSF2'])\ntest['ScreenPorch'] = np.log1p(test['ScreenPorch'])\ntest['LotArea'] = np.log1p(test['LotArea'])\ntest['EnclosedPorch'] = np.log1p(test['EnclosedPorch'])\ntest['MasVnrArea'] = np.log1p(test['MasVnrArea'])\ntest['OpenPorchSF'] = np.log1p(test['OpenPorchSF'])\ntest['WoodDeckSF'] = np.log1p(test['WoodDeckSF'])\ntest['GrLivArea'] = np.log1p(test['GrLivArea'])\ntest['BsmtUnfSF'] = np.log1p(test['BsmtUnfSF'])\ntest['BsmtFinSF1'] = np.log1p(test['BsmtFinSF1'])\ntest.shape","c006e1a9":"plt.figure(figsize = (15,20)) \nplt.subplot(3,3,1)\nsns.boxplot(x='MSZoning', y=\"SalePrice\", data=df)\nplt.subplot(3,3,2)\nsns.boxplot(x='BldgType', y=\"SalePrice\", data=df)\nplt.subplot(3,3,3)\nsns.boxplot(x='Street', y=\"SalePrice\", data=df)\nplt.subplot(3,3,4)\nsns.boxplot(x='LotShape', y=\"SalePrice\", data=df)\nplt.subplot(3,3,5)\nsns.boxplot(x='HouseStyle', y=\"SalePrice\", data=df)\nplt.subplot(3,3,6)\nsns.boxplot(x='Utilities', y=\"SalePrice\", data=df)\nplt.subplot(3,3,7)\nsns.boxplot(x='RoofStyle', y=\"SalePrice\", data=df)\nplt.subplot(3,3,8)\nsns.boxplot(x='LandSlope', y=\"SalePrice\", data=df)\nplt.subplot(3,3,9)\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=df)\nplt.show()","841cf4c7":"plt.figure(figsize = (15,15)) \nplt.subplot(3,3,1)\nsns.boxplot(x='ExterQual', y=\"SalePrice\", data=df)\nplt.subplot(3,3,2)\nsns.boxplot(x='Foundation', y=\"SalePrice\", data=df)\nplt.subplot(3,3,3)\nsns.boxplot(x='BsmtQual', y=\"SalePrice\", data=df)\nplt.subplot(3,3,4)\nsns.boxplot(x='Heating', y=\"SalePrice\", data=df)\nplt.subplot(3,3,5)\nsns.boxplot(x='CentralAir', y=\"SalePrice\", data=df)\nplt.subplot(3,3,6)\nsns.boxplot(x='Electrical', y=\"SalePrice\", data=df)\nplt.subplot(3,3,7)\nsns.boxplot(x='KitchenQual', y=\"SalePrice\", data=df)\nplt.subplot(3,3,8)\nsns.boxplot(x='GarageType', y=\"SalePrice\", data=df)\nplt.subplot(3,3,9)\nsns.boxplot(x='GarageQual', y=\"SalePrice\", data=df)\nplt.show()\nplt.figure(figsize = (15,5)) \nplt.subplot(1,2,1)\nsns.boxplot(x='SaleType', y=\"SalePrice\", data=df)","961dbf16":"df.drop(columns=['Utilities'], inplace=True) \ntest.drop(columns=['Utilities'], inplace=True) ","76196e0a":"# Converting categorical data into numerical data\n\ncat_vars = list(df.dtypes[df.dtypes == 'object'].index)\n\ndf[cat_vars].head()","786026ab":"# Feature Engineering\n\ndf['LandSlope'] = df.LandSlope.map({'Sev':0,'Mod':1,'Gtl':2})\ndf['ExterQual'] = df.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ndf['BsmtQual'] = df.BsmtQual.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ndf['BsmtCond'] = df.BsmtCond.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ndf['BsmtExposure'] = df.BsmtExposure.map({'No Basement':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ndf['BsmtFinType1'] = df.BsmtFinType1.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ndf['BsmtFinType2'] = df.BsmtFinType2.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ndf['HeatingQC'] = df.HeatingQC.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ndf['CentralAir'] = df.CentralAir.map({'N':0,'Y':1})\ndf['KitchenQual'] = df.KitchenQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ndf['GarageFinish'] = df.GarageFinish.map({'No Garage':0,'Unf':1,'RFn':2,'Fin':3})\ndf['GarageQual'] = df.GarageQual.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ndf['GarageCond'] = df.GarageCond.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ndf['ExterCond'] = df.ExterCond.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ndf['LotShape'] = df.LotShape.map({'IR1':0,'IR2':1,'IR3':2,'Reg':3})","e6353c88":"test['LandSlope'] = test.LandSlope.map({'Sev':0,'Mod':1,'Gtl':2})\ntest['ExterQual'] = test.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['BsmtQual'] = test.BsmtQual.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['BsmtCond'] = test.BsmtCond.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['BsmtExposure'] = test.BsmtExposure.map({'No Basement':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntest['BsmtFinType1'] = test.BsmtFinType1.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['BsmtFinType2'] = test.BsmtFinType2.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['HeatingQC'] = test.HeatingQC.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['CentralAir'] = test.CentralAir.map({'N':0,'Y':1})\ntest['KitchenQual'] = test.KitchenQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['GarageFinish'] = test.GarageFinish.map({'No Garage':0,'Unf':1,'RFn':2,'Fin':3})\ntest['GarageQual'] = test.GarageQual.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['GarageCond'] = test.GarageCond.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['ExterCond'] = test.ExterCond.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['LotShape'] = test.LotShape.map({'IR1':0,'IR2':1,'IR3':2,'Reg':3})","5fb195b6":"# encoding for remaining categorical varibles\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder()\nfor col in df:\n   if df[col].dtype==\"object\":\n       df[col] = enc.fit_transform(df[col].values.reshape(-1,1))\ndf","b3f288a8":"enc = OrdinalEncoder()\nfor col in test:\n   if test[col].dtype==\"object\":\n       test[col] = enc.fit_transform(test[col].astype(str).values.reshape(-1,1))\ntest","4ee0ce39":"# Ensure the order of column in the test set is in the same order than in train set\ndf, test = df.align(test, axis=1)","3e2c6f1f":"# Splitting data\n\nX = df.drop(['SalePrice'], axis=1)\nY = df['SalePrice']","6cd310b8":"# Rescaling the Features\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","a923e552":"#test = test.drop(['SalePrice'], axis=1)\ncols = test.columns\ntest = pd.DataFrame(scale(test))\ntest.columns = cols\ntest.columns","4aed08b6":"# Spliting of the datasets \nnp.random.seed(0)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=42)","c1e0bd05":"X_train.shape","5420b32c":"X_test.shape","c90fbd46":"# Build Linear Regression model\n\nlm = LinearRegression()\nlm.fit(X_train,Y_train)\n\n# RFE for LinearRegression\n\nrfe = RFE(lm, 28)\nlm_rfe = rfe.fit(X_train, Y_train)","d2e4e724":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","f9ca2129":"col = X_train.columns[rfe.support_]\ncol","62d1b892":"Y_pred_lm_rfe_train = lm_rfe.predict(X_train)\nY_pred_lm_rfe = lm_rfe.predict(X_test)","6b3ef6a1":"# calculate MSE and RMSE\nprint(\"MSE = \", mean_squared_error(Y_test, Y_pred_lm_rfe))\nprint(\"RMSE = \", np.sqrt(mean_squared_error(Y_test, Y_pred_lm_rfe)))\nprint(\"R2 Score = \", r2_score(Y_train, Y_pred_lm_rfe_train))\nprint(\"R2 Score = \", r2_score(Y_test, Y_pred_lm_rfe))","313cc4ae":"# Mean Absolute Percentage Error\n\ndef mape(actual, pred):\n    actual, pred = np.array(actual), np.array(pred)\n    return np.mean(np.abs((actual - pred) \/ actual))*100\n\nprint('MAPE score = ', mape(Y_test, Y_pred_lm_rfe))","86569541":"# Build Random Forest Model\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf_clf = RandomForestRegressor()\n\nrf_cv = GridSearchCV(estimator = RandomForestRegressor(),\n                    param_grid = {'max_depth': range(3,7),\n                                  'n_estimators' : (10, 20, 50, 100, 500, 1000 ),\n                                 },\n                    cv=5, return_train_score=True,\n                    scoring = 'neg_mean_squared_error', verbose=0,\n                    n_jobs=-1\n                    )\nrf_cv.fit(X_train, Y_train)","601d2e46":"Y_pred_rf_cv = rf_cv.predict(X_test)","a4a350d4":"print(\"MSE = \", mean_squared_error(Y_test, Y_pred_rf_cv))\nprint(\"RMSE = \", np.sqrt(mean_squared_error(Y_test, Y_pred_rf_cv)))\nprint(\"R2 Score = \", r2_score(Y_test, Y_pred_rf_cv))","260fc1ea":"best_params = rf_cv.fit(X_train, Y_train).best_params_\nbest_params","911fb4d6":"RF_best_parm = RandomForestRegressor(max_depth=best_params[\"max_depth\"],\n                                    n_estimators=best_params[\"n_estimators\"],\n                                    random_state=False, verbose=False)","f77d1ce0":"RF_best_parm.fit(X_train, Y_train)","54d94406":"RF_best_parm_pred_train = RF_best_parm.predict(X_train)\nRF_best_parm_pred = RF_best_parm.predict(X_test)","5b82ae4f":"print(\"MSE = \", mean_squared_error(Y_test, RF_best_parm_pred))\nprint(\"RMSE = \", np.sqrt(mean_squared_error(Y_test, RF_best_parm_pred)))\nprint(\"R2 Score = \", r2_score(Y_train, RF_best_parm_pred_train))\nprint(\"R2 Score = \", r2_score(Y_test, RF_best_parm_pred))","e79caa6f":"# Residual Analysis on the train data\n# i.e. are the error terms normally distributed\n\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((Y_train - Y_pred_lm_rfe_train), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)     \n","e3bf0564":"# Preserving Homoscedasticity\n# The probability distribution of the errors has constant variance. We can look at residual vs fitted values plot\n\nplt.scatter(Y_train, (Y_train - Y_pred_lm_rfe_train))\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n","a656edec":"#  Final Model\ntest = test.drop(['SalePrice'], axis=1)\ntest = test.fillna(test.interpolate())\npreds = rfe.predict(test)\nfinal_predictions = np.exp(preds)","b4dfc844":"test.index = test.index + 1461\nsubmission = pd.DataFrame({'Id': test.index ,'SalePrice': final_predictions })\nsubmission.to_csv(\"submission.csv\",index=False)","72d585ac":"Observation:\nAfter creating model in both Linear Regression and Random Forest we can see that the r2_scores is higher for Linear regression model and will penalize less on the dataset and can also help in feature elemination i am going to consider that as my final model.","e65ce38b":"**Spliting of the datasets **","c81ae520":"### Regression Analysis for Predicting Housing Price - Linear Regression and Random Forest\n\n##### INTRODUCTION \n\nIn this day and age, it is hard to predict the price of a house. There are a lot of variables that affects its price. People are having a hard time knowing the housing price when each house have different conditions. Generally, people want to buy houses that is worth the price. But they do not know how much a specific condition of the house affects it\u2019s price. For the sellers, they do not want to sell their houses in a lower than average cost. So, they need to be able to accurately predict housing prices so that they can get or give the best price.\n> To solve this problem we can use Regression analysis to analyze available data. These data can be used to predict housing price given certain variables or factors.\nRegression analysis is a set of statistical methods used for the estimation of relationships between a dependent variable and independent variables. It can be used to calculate the strength of the relationship between variables and for modeling the future relationship between them. \n\nThere are three requirements to do a regression analysis:\n* The relationship between the variables is linear\n* Both variables must be at least interval scale\n* The least squares criterion is used to determine the equation\n\nThe questions we attempt to solve:\n* Is the data structured ?\n* Does the data have Nulls or Duplicates or Outliers?\n* How is the distribution of various columns in the data ?\n* Are columns related to each other ?\n* Can we reduce the features without affecting the accuracy ?\n* What is the best sale price of any given house ?\n\n","7bef7d9d":"**Model Creation**","e7fb6355":"## Visualising the Data\n\nThe most important step - understanding the data.\n\nIf there is some obvious multicollinearity going on, this is the first place to catch it\nHere's where we'll also identify if some predictors directly have a strong association with the outcome variable","b4d32265":"**3 float type, 34 int type and 43 object type variables found**","6e568097":"* **Error terms are normally distributed with a mean of zero, so we can use this model to do predictions**\n* **The scatter plot doesn't show any funnel shape pattern, then we say that Homoscedasticity is well preserved**","cf305acd":"**Visualising Categorical Variables**"}}