{"cell_type":{"a552d1a8":"code","6ab2e952":"code","2ba0e74f":"code","c6a4a191":"code","4b8a5e20":"code","73a12284":"code","883c7dd8":"code","17273f20":"code","7515881e":"code","56e38af3":"code","7bf1d877":"code","a45eca09":"code","81aec2d7":"code","bdf026d2":"code","ec5b14d6":"code","3d3bf827":"code","84dba653":"code","c1e55a47":"code","9deb4763":"code","22e669ac":"code","4f8a3cd4":"code","284ca1d6":"code","4462f1ce":"code","446ecbf4":"code","27ac542a":"code","98cfe355":"code","44528619":"markdown","dbe26fc0":"markdown","19c1f7f7":"markdown","9f9e5f1b":"markdown","942e2e48":"markdown","c7802946":"markdown","3199f9ac":"markdown","a1450ec4":"markdown","26f28731":"markdown"},"source":{"a552d1a8":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nThe python packages for conducting experiment.\nDescription of each package is following.\n\n1. os & glob: for using system operations.\n2. numpy: for processing numbers. ex) int, float, and ...\n3. pandas: for handling informations by dataframe.\n4. seaborn & matplotlib: for visualizing something.\n5. sklearn (scikit-learn): for measuring the performance.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nimport os, glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import precision_recall_curve, auc","6ab2e952":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nThe deep learning package, Keras.\nDetail of the keras is following.\n\n1. keras: just keras\n2. tensorflow & numpy: for fixing random seed. \n    Fixing random seed gives reproducibility.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nimport keras\nimport tensorflow as tf\n\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom numpy.random import seed\nseed(32)\ntf.random.set_seed(32)","2ba0e74f":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*--*-*-*-\nDefine the custom functions for reducing redundancy.\n\n1. make_dir: for generating the directory.\n2. sorted_list: for getting the sorted list at the specific path.\n3. get_keys: for getting the keys from dataframe or dictionary.\n4. summarizing_df: for summarizing the dataframe.\n5. read_csv: to read and show the dataframe from specific path.\n6. encode_df: for encoding the string values to number.\n7. plot_histogram: for plotting histograms.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*--*-*-*- \"\"\"\n\ndef make_dir(path):\n    \n    try: os.mkdir(path)\n    except: pass\n    \ndef sorted_list(path):\n    \n    tmplist = glob.glob(path)\n    tmplist.sort()\n    \n    return tmplist\n\ndef get_keys(df):\n    \n    return list(df.keys())\n\ndef summarizing_df(df):\n    \n    print(\"\\n1. Keys\")\n    print(get_keys(df))\n    print(\"\\n2. Shape\")\n    print(df.shape)\n    print(\"\\n3. Head\")\n    print(df.head(5))\n    \ndef read_csv(path):\n    \n    print(\"\\n* Path: %s\" %(path))\n    df = pd.read_csv(path)\n    summarizing_df(df)\n    \n    return df, get_keys(df)\n\ndef encode_df(df, key_map):\n    \n    list_key = get_keys(key_map)\n    for idx_k, key in enumerate(list_key):\n        print(\"Key \\'%s\\' Encoding...\" %(key))\n        \n        list_ustr = key_map[key]\n        try:\n            for idx_d in range(df.shape[0]):\n                for idx_u, ustr in enumerate(list_ustr):\n                    if(df.loc[idx_d, key] == ustr):\n                        df.loc[idx_d, '%s_enc' %(key)] = idx_u\n                        break\n        except: pass\n    \n    for idx_k, key in enumerate(list_key):\n        try: df.drop(key, axis=1, inplace=True)\n        except: pass\n    \n    for idx_k, key in enumerate(get_keys(df)):\n        df[key] = df[key].fillna(df[key].median())\n\n    summarizing_df(df)\n    \n    return df, get_keys(df)\n\ndef plot_histogram(df):\n\n    plt.figure(figsize=(15, 10))\n    for idx_k, key in enumerate(get_keys(df)):\n        plt.subplot(4, 4, idx_k+1)\n        plt.title(key)\n        df[key].hist()\n        plt.grid()\n    plt.tight_layout()\n    plt.show()","c6a4a191":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nConfirmation of the provided dataset.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nsorted_list(path=os.path.join(\"\/kaggle\", \"input\", \"kakr-4th-competition\", \"*\"))","4b8a5e20":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nDefine the path for loading and using dataset.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\npath_tr = '\/kaggle\/input\/kakr-4th-competition\/train.csv'\npath_te = '\/kaggle\/input\/kakr-4th-competition\/test.csv'\npath_sb = '\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv'","73a12284":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nLoad the dataset via function 'read_csv'.\nThe above function returns the dataframe and key of them.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\ndf_tr, key_tr = read_csv(path=path_tr)\ndf_te, key_te = read_csv(path=path_te)","883c7dd8":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nFor data pre-processing, prior information is needed.\nIn this competition, the prior informations are column names \nthat includes string values and unique values of the above column.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nkey_map = {}\nfor idx_k, key in enumerate(key_tr):\n    if(type(df_tr[key][0]) is str):\n        list_ustr = list(set(df_tr[key]))\n        try: list_ustr.extend(list(set(df_te[key])))\n        except: pass\n        list_ustr = list(set(list_ustr))\n        list_ustr.sort()\n        key_map[key] = list_ustr","17273f20":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nPre-processing and visualization procedure with training data.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\ndf_tr, key_tr = encode_df(df_tr, key_map)\nplot_histogram(df_tr)\nprint(df_tr.describe())","7515881e":"plt.figure(figsize=(15, 10))\nplt.title(\"Correlation Coefficient Matrix\")\nsns.heatmap(df_tr.drop('id', axis=1).corr(), cmap='jet', annot=True, fmt='.3f')\nplt.show()\nplt.close()","56e38af3":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nPre-processing and visualization procedure with test data.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\ndf_te, key_te = encode_df(df_te, key_map)\nplot_histogram(df_te)\nprint(df_te.describe())","7bf1d877":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nAdditional pre-processing method, called min-max normalization.\nIn this notebook, min-max normalization uses min and max of the \nwhole dataset (inclusing training and test set).\nThen, apply those two values for normalization procedure.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nkey_tr = get_keys(df_tr)\nfor idx_k, key in enumerate(key_tr):\n    if('id' in key or 'income' in key): continue\n    val_max = max(df_tr[key].max(), df_te[key].max())\n    val_min = min(df_tr[key].min(), df_te[key].min())\n    df_tr[key] = (df_tr[key] - val_min) \/ (val_max - val_min)\n    df_te[key] = (df_te[key] - val_min) \/ (val_max - val_min)\n\nsummarizing_df(df_tr)\nsummarizing_df(df_te)","a45eca09":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nFor training, the column, named with 'id', is not useful.\nThus, trimming, as shown as follows, is needed.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nx_tr = df_tr.iloc[:, 1:-1]\ny_tr = df_tr.iloc[:, -1]\nprint(\"Training X:\", x_tr.shape, \"Y:\", y_tr.shape)\n\nx_te = df_te.iloc[:, 1:]\nprint(\"Test     X:\", x_te.shape)","81aec2d7":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nFinal pre-processing method!\nIn this notebook, label smoothing is applied before using dataset [1].\nIf you wonder effect of the above method, refer the following paper.\n[1] When Does Label Smoothing Help? (https:\/\/arxiv.org\/pdf\/1906.02629.pdf)\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\ny_tr = np.where(y_tr > 0.5, 0.85, 0.15)\ny_tr = np.expand_dims(y_tr, axis=-1)","bdf026d2":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nThe deep learning model for predicting the income via input data (feature).\nIn this notebook, the neural network has simple ensemble architecture.\nHowever, you can modify structure of the neural network, more deeply or more complexly.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\ndef get_model(num_feat, feat_dims, verebose=False):\n    \n    x = keras.layers.Input(shape=(num_feat))\n    \n    x_in = x\n    for dim in feat_dims:\n        x1 = keras.layers.Dense(dim, activation='swish')(x_in)\n        x2 = keras.layers.Dense(dim, activation='relu')(x_in)\n        x3 = keras.layers.Dense(dim, activation='elu')(x_in)\n        x_in = tf.keras.layers.Concatenate(axis=1)([x1, x2, x3])\n    \n    fc = keras.layers.Dense(16, activation='relu')(x_in)\n    y_hat = keras.layers.Dense(1, activation=None)(fc)\n    y_clip = K.clip(y_hat, 1e-12, 1-(1e-12))\n    \n    model = keras.models.Model(inputs=[x], outputs=y_clip)\n    if(verebose): print(model.summary())\n    \n    return model","ec5b14d6":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nSet the hyperparameters for training the neural network.\nFor improving the performance, you should find the best hyperparameter via adjusting them.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nfeat_dimensions = [64, 128, 32, 8]\nlearning_rate = 1e-5\ndecay_scaler = 1e+2\nepochs = 300\nbatch_size = 32","3d3bf827":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nCall the deep learning model via function 'get_model' and compile them.\nNow, we can training the neural network for predicting 'income'!\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nmodel = get_model(num_feat=14, feat_dims=feat_dimensions, verebose=True)\n\nmake_dir(path='model')\nmodel_path = 'model\/best_model.hdf5'\ncb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=2, save_best_only=True)\n\nopt = optimizers.Adam(lr=learning_rate, decay=learning_rate\/decay_scaler)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)","84dba653":"history = model.fit(x=x_tr, y=y_tr, validation_split=0.05, \\\n    epochs=epochs, batch_size=batch_size, callbacks=[cb_checkpoint], verbose=3)","c1e55a47":"plt.figure(figsize=(15, 6))\nplt.plot(history.history['loss'][1:], label='Training')\nplt.plot(history.history['val_loss'][1:], label='Validation')\nplt.ylabel(\"Binary Crossentropy\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc='upper right')\nplt.grid()\nplt.tight_layout()\nplt.show()","9deb4763":"y_tr_hat = model.predict(x_tr)","22e669ac":"income_l, income_h = [], []\nfor idx_i, income in enumerate(df_tr['income_enc']):\n    if(income < 0.5): income_l.append(y_tr_hat[idx_i])\n    else: income_h.append(y_tr_hat[idx_i])","4f8a3cd4":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nIn this block, we will find the optimal threshold for making the results.\nThe threshold will be set by the f1-score curve.\nF1-score curve is drawn using the tho curve, precision and recall.\nSelected threshold and the two graph, as mensioned the above, is shown in final.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nscore_l, score_h = np.asarray(income_l), np.asarray(income_h)\nlabel_l, label_h = np.zeros_like(score_l), np.ones_like(score_h)\n\nscores = np.append(score_l, score_h, axis=0)\nlabels = np.append(label_l, label_h, axis=0)\n\nprecision, recall, thresholds_pr = precision_recall_curve(labels, scores)\n\nf1_score, max_f1, val_threshold, val_idx = [], 0, thresholds_pr[0], 0\nfor idx in range(len(thresholds_pr)):\n    f1_score.append(2 * (precision[idx] * recall[idx]) \/ (precision[idx] + recall[idx]))\n    if(f1_score[-1] > max_f1):\n        max_f1 = f1_score[-1]\n        val_threshold = thresholds_pr[idx]\n        val_idx = idx\n    \nplt.figure(figsize=(15, 6))\nplt.title(\"F1-score %.5f  Threshold %.5f\" %(max_f1, val_threshold))\nplt.plot(precision, label=\"Precision\")\nplt.plot(recall, label=\"Recall\")\nplt.plot(f1_score, label=\"F1_score\")\nplt.axvline(x=val_idx, color='red', linestyle='--')\nplt.xticks(range(len(thresholds_pr))[::1000], thresholds_pr[::1000], rotation=270)\nplt.legend(loc='upper right')\nplt.grid()\nplt.tight_layout()\nplt.show()","284ca1d6":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nThe score map of predicted value of training set.\nEach label, low and high income, is colored with blue and orange respectively.\nAlso, the selected threshold is shown with the red dash.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nplt.figure(figsize=(20, 6))\nplt.scatter(range(0, len(income_l)), income_l, s=3, label=\"Low\")\nplt.scatter(range(len(income_l), len(income_l)+len(income_h)), income_h, s=3, label=\"High\")\nplt.axhline(y=val_threshold, color='white', linewidth=10, alpha=0.7)\nplt.axhline(y=val_threshold, color='red', linestyle='--')\nplt.ylabel(\"Income\")\nplt.xlabel(\"ID\")\nplt.legend(loc='upper right')\nplt.grid()\nplt.tight_layout()\nplt.show()","4462f1ce":"y_te_hat = model.predict(x_te)","446ecbf4":"\"\"\" -*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\nThe score map of predicted values of test set.\nInformation of the label that correspond with input data does not provided.\nThus, all the predicted valus are colored with blue.\nThe threshold, same as the above block, is also shown with the red dash.\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*- \"\"\"\n\nplt.figure(figsize=(20, 6))\nplt.scatter(range(y_te_hat.shape[0]), y_te_hat, s=3)\nplt.axhline(y=val_threshold, color='white', linewidth=10, alpha=0.7)\nplt.axhline(y=val_threshold, color='red', linestyle='--')\nplt.ylabel(\"Income\")\nplt.xlabel(\"ID\")\nplt.grid()\nplt.tight_layout()\nplt.show()","27ac542a":"df_sb, _ = read_csv(path=path_sb)\nfor idx_i, val_id in enumerate(df_te['id']):\n    answer = y_te_hat[idx_i]\n    if(answer > val_threshold): answer = 1\n    else: answer = 0\n    df_sb.loc[df_sb.id == val_id, 'prediction'] = answer\n    \nsummarizing_df(df_sb)\ndf_sb.to_csv(\"submission.csv\", index=False)","98cfe355":"plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Digitized Score\")\nplt.scatter(range(df_sb.shape[0]), df_sb['prediction'], s=3)\nplt.ylabel(\"Income\")\nplt.xlabel(\"ID\")\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.title(\"Prediction\")\ndf_sb['prediction'].hist()\nplt.grid()\n\nplt.tight_layout()\nplt.show()","44528619":"### Training","dbe26fc0":"# 4. Make submission\nFinally, we have reached the stage of making the result.  \nFor making a submission file, we can use the provided file 'sample_submission.csv'.  \nEach prediction value is digitized by threshold that selected before this section.","19c1f7f7":"### Predicting via Training set","9f9e5f1b":"# 3. Deep Learning Procedure\n\nUsing the deep learning model gives more efficiency for solving much of the problems.\nIn this notebook, we will use the '<a href=\"https:\/\/keras.io\/\">Keras<\/a>', as mentioned before, because it is easier (in my opinion...) to use than '<a href=\"tensorflow.org\">TensorFlow<\/a>' or '<a href=\"pytorch.org\">PyTorch<\/a>'.","942e2e48":"# 0. Introduction\nWelcome!  \nThis Kaggle notebook provides step by step and end to end package for submission the result if you want to join <a href=\"https:\/\/www.kaggle.com\/c\/kakr-4th-competition\">this competition<\/a>.  \nThis notebook includes essential EDA and focuses on easy to participate in the competition.","c7802946":"# 1. Environment Setting\nIn this section, we will set the environment for experiment.  \nThis notebook is written using Python with deep learning package, '<a href=\"https:\/\/keras.io\/\">Keras<\/a>'.  ","3199f9ac":"### Show Training History","a1450ec4":"### Predicting via Test set","26f28731":"# 2. Data Preparing (including EDA)\nThe data preparing process is handled in this section.  \nAlso, some essential Exploratory Data Analysis (EDA) is conducted with visualization and statistical summarization.  "}}