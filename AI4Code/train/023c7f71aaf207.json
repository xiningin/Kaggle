{"cell_type":{"7d28c0d1":"code","332e7c31":"code","183a9a24":"code","4309a525":"code","cc0665c2":"code","654d98d1":"code","a0904588":"code","be5aab43":"code","fbf20eb5":"code","c41ca33b":"code","85b545db":"code","dc4b5f79":"code","03d93a28":"code","1e04f30f":"code","2f384116":"code","f1e82f47":"code","5f60509d":"code","aec5d09a":"code","265fba12":"code","58a67466":"code","d89cf5f5":"code","cd5ef1dc":"code","c89176f0":"code","8c07fa78":"markdown","93dfbc3b":"markdown","c8188b3e":"markdown","842fd35c":"markdown","66989607":"markdown","a325e988":"markdown","7c17d08c":"markdown","c7b33b96":"markdown"},"source":{"7d28c0d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","332e7c31":"# Data paths\ndir_df = '..\/input\/home-data-for-ml-course'\ntrain_dir = os.path.join(dir_df, 'train.csv')\ntest_dir = os.path.join(dir_df, 'test.csv')\n\n# read in data\ndf_train = pd.read_csv(train_dir)\ndf_test = pd.read_csv(test_dir)\nprint('shape of df_train:', df_train.shape)\nprint('shape of df_test:', df_test.shape)\ndisplay(df_train.head())\ndisplay(df_test.head())","183a9a24":"# check for null values\nprint('null values of train:')\nprint(df_train.isnull().sum()[0:81])\nprint('\\n\\n null values of test:')\nprint(df_test.isnull().sum()[0:20])","4309a525":"# remove following features (columns), because they have so many null values:\n# ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n\ndf_train_drop = df_train.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)\ndf_test_drop = df_test.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)\n\nprint('shape of df_train_drop', df_train_drop.shape)\nprint('shape of df_test_drop', df_test_drop.shape)","cc0665c2":"# split df_train_drop in X (input) and y (output)\n# and call df_test_drop without Id X_test\nX, y = df_train_drop.iloc[ : ,1:75], df_train_drop['SalePrice']\nX_test = df_test_drop.drop('Id', axis = 1)\nprint('shape of X:', X.shape)\nprint('shape of y:', y.shape)\nprint('shape of X_test:', X_test.shape)","654d98d1":"# split in categorical and numerical data\n# because we like to transform them differently\n# Note: You can check the data type of the different features with X.dtypes\ncat_features = (X.dtypes == 'object')\ncat_list = [i for i in cat_features]\ncont_features = (X.dtypes != 'object')\ncont_list = [i for i in cont_features]\n\nX_cat = X.iloc[ : , cat_list]\nX_cont = X.iloc[ : , cont_list]\nX_test_cat = X_test.iloc[ : , cat_list]\nX_test_cont = X_test.iloc[ : , cont_list]\n\nprint('shape of X_cat:', X_cat.shape)\nprint('shape of X_cont:', X_cont.shape)\nprint('shape of X_test_cat:', X_test_cat.shape)\nprint('shape of X_test_cont:', X_test_cont.shape)","a0904588":"# Now we one hot encode the categorical data\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n# We fit the OneHotEncoder on training plus test data\n# and replace Null Values of our dataset with an empty string\n# in this way we make shure to get no error message, because\n# of unknown values (maybe not the most elegant way I do this)\nX_full_cat = np.vstack([X_cat.fillna(\"\"), X_test_cat.fillna(\"\")])\n\n# fit OneHotEncoder on concatenate dataset\nohe.fit(X_full_cat)\n\n# one hot transform training and test set\nX_cat_ohe = ohe.transform(X_cat.fillna(\"\")).toarray()\nX_test_cat_ohe = ohe.transform(X_test_cat.fillna(\"\")).toarray()\n\nprint('shape of X_cat_ohe:', X_cat_ohe.shape)\nprint('shape of X_test_cat_ohe:', X_test_cat_ohe.shape)","be5aab43":"# impute mean values to numerical data for NANs (Null Values) and normalize to std 1 and mean 0\n\n# SimpleImputer is a simple way to replace NANs with some apropriate value\n# by default with the mean value of the respective column\nfrom sklearn.impute import SimpleImputer\nsi = SimpleImputer()\nsi.fit(X_cont)\nX_cont_imp = si.transform(X_cont)\nX_test_cont_imp = si.transform(X_test_cont)\n\n# Normalize\n# StandardScalar is a simple way for scaling all features to standard deviation 1 and mean 0\n# but of course you can also do this 'by hand'\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nss.fit(X_cont_imp)\nX_cont_imp_std = ss.transform(X_cont_imp)\nX_test_cont_imp_std = ss.transform(X_test_cont_imp)\n\n# check if data is scaled correctly\nprint('mean and std of X:', np.mean(X_cont_imp_std, axis = 0), np.std(X_cont_imp_std, axis = 0))\nprint('mean and std of X_test:', np.mean(X_test_cont_imp_std, axis = 0), np.std(X_test_cont_imp_std, axis = 0))\n\nprint('shape of X_cont_imp:', X_cont_imp.shape)\nprint('shape of X_test_cont_imp:', X_test_cont_imp.shape)","fbf20eb5":"# finally we combine categorical and numerical data\nX_train = np.hstack([X_cat_ohe, X_cont_imp_std])\nX_final_test = np.hstack([X_test_cat_ohe, X_test_cont_imp_std])\nprint('shape of X_train', X_train.shape)\nprint('shape of X_final_test', X_final_test.shape)","c41ca33b":"# to perform a grid search, we use GridSearchCV which is a simple and good option\n# at first we have to define a Parameter grid, which contains the \n# parameters, which should be tested during Search\n\n# we try 4 different options for the kernel namly 'linear', 'rbf', \n# 'sigmoid' and 'poly'. The do not have all the same parameters. So\n# for example it would be meaningless to give the linear kernel a \n# degree element (for details about the parameters have a look at \n# the documentation of SVR)\n\n# Note: the parameters in the grid below are already adjusted well,\n# but normaly you have to adjust them a few (or many) times, until \n# you get good predictions.\n# You can try to adjust them more properly than I, if you like. \n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nsvr = SVR()\n\n# setup grid\ngrid = [{'kernel': ['linear'],\n        'C': [950, 1000, 1.15e3],\n        'epsilon': [40, 50, 60]},\n        {'kernel': ['rbf'],\n         'C': [3e5, 3.5e5, 4e5],\n         'epsilon': [40, 50, 60],\n         'gamma': [0.002, 0.0025, 0.003]},\n        {'kernel': ['sigmoid'],\n         'C': [700, 750, 800],\n         'epsilon': [0, 0.000001, 0.00001],\n         'gamma': [0.4, 0.5, 0.6],\n         'coef0': [-21, -20, -19]},\n         {'kernel': ['poly'],\n         'C': [1.2, 1.3, 1.4],\n         'epsilon': [3050, 3100, 3150],\n         'gamma': [0.09, 0.1, 0.11],\n         'coef0': [6, 7, 8],\n         'degree': [3, 4, 5]}]\n\n# initialize GridSearchCV, note that you have to specify:\n# how often cross validation should be done (cv) and a \n# scoring parameter (scoring) (look at GridSearchCV documentation\n# for details)\n# n_jobs = -1 just means that all available kernels should be used\ngs = GridSearchCV(svr,\n                 param_grid = grid,\n                 scoring = 'neg_mean_absolute_error',\n                 cv = 10,\n                 n_jobs = -1)\n\n# fitting Grid Search takes around 20 Minutes or so\ngs.fit(X_train,y)","85b545db":"# check the results of our grid search and may adjust parameters in the grid\ntmp = pd.DataFrame(gs.cv_results_).loc[gs.cv_results_['rank_test_score'] < 5, ['rank_test_score', 'mean_test_score', 'std_test_score',\n                                                                         'param_kernel', 'param_C', 'param_epsilon', \n                                                                         'param_gamma', 'param_coef0', 'param_degree']]\n\n# shows the different kernels seperately\n# is done, because I like to adjust the parameters\n# of all 4 kernels, not only for the best one\ndisplay(tmp[tmp['param_kernel'] == 'linear'])\ndisplay(tmp[tmp['param_kernel'] == 'rbf'])\ndisplay(tmp[tmp['param_kernel'] == 'sigmoid'])\ndisplay(tmp[tmp['param_kernel'] == 'poly'])\n# you may have to change the number of evaluated \n# models which should be shown","dc4b5f79":"# Best Parameters: (for each kernel respectively)\n# Kernel:          'linear', 'rbf',    'sigmoid', 'poly'\n# C:                1000,     350000,   750,       1.3\n# epsilon:          50,       60,       0,         3100\n# gamma:            NaN,      0.0025,   0.5,       0.1\n# coef0:            NaN,      NaN,      -20,       7\n# degree:           NaN,      NaN,      NaN,       4\n# best_score (mae): 15471,    14010,    25293,     13713","03d93a28":"#svr_best = gs.best_estimator_\n\n## prediction on test set\n#pred_best = svr_best.predict(X_final_test)\n#pred_best","1e04f30f":"C_linear = 1000\nepsilon_linear = 50\n\nC_rbf = 350000\nepsilon_rbf = 60\ngamma_rbf = 0.0025\n\nC_sig = 750\nepsilon_sig = 0\ngamma_sig = 0.5\ncoef0_sig = -20\n\nC_poly = 1.3\nepsilon_poly = 3100\ngamma_poly = 0.1\ncoef0_poly = 7\ndegree_poly = 4\n\nsvm_poly = SVR(kernel = 'poly', C = C_poly, epsilon = epsilon_poly,\n               gamma = gamma_poly, coef0 = coef0_poly, degree = degree_poly)\nsvm_rbf = SVR(kernel = 'rbf', C = C_rbf, epsilon = epsilon_rbf,\n              gamma = gamma_rbf)\nsvm_linear = SVR(kernel = 'linear', C = C_linear, epsilon = epsilon_linear)\nsvm_sig = SVR(kernel = 'sigmoid', C = C_sig, epsilon = epsilon_sig,\n              gamma = gamma_sig, coef0 = coef0_sig)","2f384116":"# split in tr and val set\nfrom sklearn.model_selection import train_test_split\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y, test_size = 0.2, random_state = 15)\nprint(y_tr)","f1e82f47":"# Fit Models on tr set\nsvm_poly.fit(X_tr, y_tr)\nsvm_rbf.fit(X_tr, y_tr)\nsvm_linear.fit(X_tr, y_tr)\nsvm_sig.fit(X_tr, y_tr)","5f60509d":"# Predict on val set\npred_svm_poly = svm_poly.predict(X_val)\npred_svm_rbf = svm_rbf.predict(X_val)\npred_svm_lin = svm_linear.predict(X_val)\npred_svm_sig = svm_sig.predict(X_val)\n# pred_svm_poly\n# pred_svm_rbf\n# pred_svm_lin\n# pred_svm_sig","aec5d09a":"# Determine best weights for ensemble with Nelder-Mead-Algorithm\nfrom scipy.optimize import minimize\n\ndef dummyfunction(x = [0.3, 0.3]):\n    w0 = x[0]\n    w1 = x[1]\n    w2 = np.abs(1-np.sum(x))\n    w3 = 0                   # gets a 0 weight, so set it 0 from beginning\n    tmp_pred = (w0*pred_svm_poly + w1*pred_svm_rbf + w2*pred_svm_lin + w3*pred_svm_sig)\n    return np.sum(np.abs(tmp_pred - y_val))        # use mean absolute error, can also use mean squared error\n\nres = minimize(dummyfunction, x0 = [0.25, 0.25], method = 'Nelder-Mead', tol = 1e-6)\nprint(res)\n1-np.sum(res.x)","265fba12":"# fit models on whole training data\nsvm_poly = SVR(kernel = 'poly', C = C_poly, epsilon = epsilon_poly,\n               gamma = gamma_poly, coef0 = coef0_poly, degree = degree_poly)\nsvm_rbf = SVR(kernel = 'rbf', C = C_rbf, epsilon = epsilon_rbf, gamma = gamma_rbf)\nsvm_lin = SVR(kernel = 'linear', C = C_linear, epsilon = epsilon_linear)\n\nsvm_poly.fit(X_train, y)\nsvm_rbf.fit(X_train, y)\nsvm_lin.fit(X_train, y)\n\n# make predictions on test set\npred_svm_poly = svm_poly.predict(X_final_test)\npred_svm_rbf = svm_rbf.predict(X_final_test)\npred_svm_lin = svm_lin.predict(X_final_test)\n#pred_svm_poly\n#pred_svm_rbf\n#pred_svm_lin","58a67466":"# average preditions with determined weights\nweight_svm_poly = res.x[0]\nweight_svm_rbf = res.x[1]\nweight_svm_lin = np.abs(1 - np.sum(res.x))\npred = (weight_svm_poly * pred_svm_poly + weight_svm_rbf * pred_svm_rbf + weight_svm_lin * pred_svm_lin)","d89cf5f5":"# submit without model averaging\n#output_ohne = pd.DataFrame({'Id': df_test.Id,\n#                            'SalePrice': pred_best})\n#output_ohne.to_csv('submission_ohne', index = False)","cd5ef1dc":"# submit prediction\noutput = pd.DataFrame({'Id': df_test.Id,\n                      'SalePrice': pred})\noutput.to_csv('submission_mit_averaging.csv', index = False)","c89176f0":"# check output\noutput","8c07fa78":"# 0. Intro\n\nIn this Kernel only Support Vector Regression with 4 different Kernels is performed to crack the score of 13000.","93dfbc3b":"# 3. Determine weights for Model Averaging","c8188b3e":"# 2. Determine Best Parameters by a Grid Search","842fd35c":"4. Submit Predictions","66989607":"Import Data, \n\nRemove Columns with much missing values, \n\nImpute remaining missing values, \n\nOne Hot Encode categorical data and \n\nscale numerical data to Standard Deviation 1 and Mean 0.","a325e988":"Now you have two Options, the simple one is to just take the best Model of our Grid Search and make predictions on the test set (beats already the 13000)\n\nFor this uncomment the following code cell and scip part 3. of this kernel","7c17d08c":"# 1. Data Preperation","c7b33b96":"Option two is to go on with Part 3 and build an ensemble of the best Models of your Grid Search (one model for each kernel)"}}