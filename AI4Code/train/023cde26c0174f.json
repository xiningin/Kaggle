{"cell_type":{"d8d64d30":"code","d312563e":"code","fa1da5cc":"code","4b3d24f2":"code","3d7d4fef":"code","2f92bb15":"code","7c5149a0":"code","643a9d94":"code","407575b9":"code","dcef2c28":"code","0ff5e8c9":"code","ff94e924":"code","b0d54015":"code","a861b4d7":"code","15b45f8a":"code","24156c56":"code","8e4b7c10":"code","eb071ece":"code","98ac62c5":"code","4b8d2d99":"code","a1c66327":"code","3d3ad2b0":"code","bb515e7e":"code","81b12e15":"code","78d629b8":"code","97b216e1":"code","6c03bf34":"code","e9ac2ea7":"code","ad9d4039":"code","ae32aa66":"code","e770580a":"code","f9d2ac60":"code","ac65cf17":"code","b21fc63a":"code","6a03073b":"markdown","25d1b685":"markdown","e0eb4d27":"markdown","a28f0798":"markdown","690ca622":"markdown","b54c0c96":"markdown","f9fba7cc":"markdown","7098eefa":"markdown","72aa7007":"markdown","0d7efd7f":"markdown","5e4e79bb":"markdown","124df693":"markdown","b0ff61b9":"markdown","9ffbfd63":"markdown","33855082":"markdown","76cf384b":"markdown","55fafd62":"markdown"},"source":{"d8d64d30":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import cluster\nfrom sklearn import tree\nfrom sklearn import naive_bayes\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nplt.style.use('classic')\nimport seaborn as sns\nfrom IPython.display import HTML, display\nimport tabulate\nimport graphviz \nfrom itertools import combinations\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os","d312563e":"students_df = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")","fa1da5cc":"def key_index(array):\n    for i in range(0,len(array)):\n        yield((array[i],i))\n\n# Dictionaries for encoding the cathegorical fields\nrace_ethnicity_enc = {key:i for (key,i) in key_index(students_df['race\/ethnicity'].unique())}\ngender_enc = {key:i for (key,i) in key_index(students_df['gender'].unique())}\nparental_education = ['some high school', 'high school','some college',\n                      \"associate's degree\", \"bachelor's degree\", \"master's degree\"]\nparental_education_enc = {key:i for (key,i) in key_index(parental_education)}\nlunch_enc = {key:i for (key,i) in key_index(students_df['lunch'].unique())}\ntest_prep_course_enc = {key:i for (key,i) in key_index(students_df['test preparation course'].unique())}\n\n# Disctionaries for decoding the cathegorical fields\nrace_ethnicity_val = {i:key for (key,i) in key_index(tuple(race_ethnicity_enc.keys()))}\ngender_val = {i:key for (key,i) in key_index(tuple(gender_enc.keys()))}\nparental_education_val = {i:key for (key,i) in key_index(tuple(parental_education_enc.keys()))}\nlunch_val = {i:key for (key,i) in key_index(tuple(lunch_enc.keys()))}\ntest_prep_course_val = {i:key for (key,i) in key_index(tuple(test_prep_course_enc.keys()))}","4b3d24f2":"table = [\n    ['race\/ethnicity']+['value']+[a for a in race_ethnicity_enc.keys()],\n    [' ']+['code']+[a for a in race_ethnicity_enc.values()], \n    ['gender']+['value']+[a for a in gender_enc.keys()],\n    [' ']+['code']+[a for a in gender_enc.values()],\n    ['parental level of education']+['value']+[a for a in parental_education_enc.keys()],\n    [' ']+['code']+[a for a in parental_education_enc.values()],\n    ['lunch']+['value']+[a for a in lunch_enc.keys()],\n    [' ']+['code']+[a for a in lunch_enc.values()],\n    ['test preparation course']+['value']+[a for a in test_prep_course_enc.keys()],\n    [' ']+['code']+[a for a in test_prep_course_enc.values()]\n]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","3d7d4fef":"students_df.loc[:,'race\/ethnicity'] = students_df['race\/ethnicity'].apply(lambda x: race_ethnicity_enc[x])\nstudents_df.loc[:,'gender'] = students_df['gender'].apply(lambda x: gender_enc[x])\nstudents_df.loc[:,'parental level of education'] = students_df['parental level of education'].apply(lambda x: parental_education_enc[x])\nstudents_df.loc[:,'lunch'] = students_df['lunch'].apply(lambda x: lunch_enc[x])\nstudents_df.loc[:,'test preparation course'] = students_df['test preparation course'].apply(lambda x: test_prep_course_enc[x])","2f92bb15":"students_df.describe()","7c5149a0":"sns.set()\nfig=plt.figure(figsize=(20,3))\ngs=gridspec.GridSpec(1,3) # 2 rows, 3 columns\n\nax00=fig.add_subplot(gs[0,0]) # First row, first column\nax01=fig.add_subplot(gs[0,1]) # First row, second column\nax02=fig.add_subplot(gs[0,2]) # First row, first column\n\nsns.boxplot(x=\"gender\", y=\"math score\", data=students_df, ax=ax00)\nsns.boxplot(x=\"gender\", y=\"reading score\", data=students_df, ax=ax01)\nsns.boxplot(x=\"gender\", y=\"writing score\", data=students_df, ax=ax02)\n\nax00.set_xticklabels(['Male', 'Female'])\nax01.set_xticklabels(['Male', 'Female'])\nax02.set_xticklabels(['Male', 'Female'])\n\nax00.set_title('Math scores')\nax01.set_title('Reading scores')\n_ = ax02.set_title('Writing scores')","643a9d94":"sns.set()\nfig=plt.figure(figsize=(20,3))\ngs=gridspec.GridSpec(1,3) # 2 rows, 3 columns\n\nax00=fig.add_subplot(gs[0,0]) # First row, first column\nax01=fig.add_subplot(gs[0,1]) # First row, second column\nax02=fig.add_subplot(gs[0,2]) # First row, first column\n\nsns.scatterplot(x=\"writing score\", y=\"math score\", data=students_df, ax=ax00, hue=\"gender\")\nsns.scatterplot(x=\"writing score\", y=\"reading score\", data=students_df, ax=ax01, hue=\"gender\")\nsns.scatterplot(x=\"math score\", y=\"reading score\", data=students_df, ax=ax02, hue=\"gender\")\n\nax00.set_title('Math scores')\nax01.set_title('Reading scores')\n_ = ax02.set_title('Writing scores')","407575b9":"sns.set()\ncorr = students_df.loc[:,['math score', 'reading score', 'writing score']].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nax = sns.heatmap(corr,annot=True,annot_kws={\"size\": 7.5},linewidths=.5)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\");","dcef2c28":"outliers = None\nfor col in ['math score', 'reading score', 'writing score']:\n    iq = students_df[col].quantile(.75) - students_df[col].quantile(.25)\n    mean = students_df[col].mean()\n    if (outliers is None):\n        outliers = students_df[col] < (mean - 1.5*iq)   \n    else: \n        outliers = outliers | (students_df[col] < (mean - 1.5*iq))\n        \noutliers_by_gender = {'0':None, '1':None}\nfor gender in ['0','1']:\n    gender_filter = students_df['gender'] == int(gender)\n    for col in ['math score', 'reading score', 'writing score']:\n        iq = students_df.loc[gender_filter, col].quantile(.75) - students_df.loc[gender_filter, col].quantile(.25)\n        mean = students_df.loc[gender_filter, col].mean()\n        out = students_df[col] < (mean - 1.5*iq)\n        \n        if (outliers_by_gender[gender] is None):\n            outliers_by_gender[gender] = gender_filter & out\n        else: \n            outliers_by_gender[gender] = gender_filter & (outliers_by_gender[gender] | out)","0ff5e8c9":"np.sum(outliers_by_gender['0'])","ff94e924":"len(students_df[outliers_by_gender['0'] | outliers_by_gender['1']])","b0d54015":"len(students_df[outliers])","a861b4d7":"students_df.loc[:,'outlier'] = outliers","15b45f8a":"sns.set()\nfig=plt.figure(figsize=(20,3))\ngs=gridspec.GridSpec(1,3) # 2 rows, 3 columns\n\nax00=fig.add_subplot(gs[0,0]) # First row, first column\nax01=fig.add_subplot(gs[0,1]) # First row, second column\nax02=fig.add_subplot(gs[0,2]) # First row, first column\n\nsns.catplot(x='parental level of education', kind=\"count\", ax=ax00, data=students_df, color='lightgrey')\nsns.catplot(x='parental level of education', kind=\"count\", ax=ax00, data=students_df[students_df['gender']==0], color='#cc8963')\nsns.catplot(x='parental level of education', kind=\"count\", ax=ax00, data=students_df[students_df['gender']==1], color='#5975a4')\n\na= sns.catplot(x='race\/ethnicity', kind=\"count\", data=students_df, ax=ax01, color='lightgrey')\nb= sns.catplot(x='race\/ethnicity', kind=\"count\", data=students_df[students_df['gender']==0], ax=ax01, color='#cc8963')\nc= sns.catplot(x='race\/ethnicity', kind=\"count\", data=students_df[students_df['gender']==1], ax=ax01, color=\"#5975a4\")\n\nsns.catplot(x='test preparation course', kind=\"count\", data=students_df, ax=ax02, color='lightgrey')\nsns.catplot(x='test preparation course', kind=\"count\", data=students_df[students_df['gender']==0], ax=ax02, color='#cc8963')\nsns.catplot(x='test preparation course', kind=\"count\", data=students_df[students_df['gender']==1], ax=ax02, color='#5975a4')\n\nax02.legend(['Total','Female','Male'])\n\nax01.set_xticklabels([race_ethnicity_val[i] for i in range(0, len(race_ethnicity_val.keys()))])\nax02.set_xticklabels([test_prep_course_val[i] for i in range(0, len(test_prep_course_val.keys()))])\nprint('Keys for the firstbar chart ',parental_education_val)\n\nplt.close(2)\nplt.close(3)\nplt.close(4)\nplt.close(5)\nplt.close(6)\nplt.close(7)\nplt.close(8)\nplt.close(9)\nplt.close(10)","24156c56":"a = np.array(students_df.loc[:,['math score', 'reading score', 'writing score']])\nb = np.mean(a, axis=1)\nb = b.reshape((len(a),1))\n\nstudents_df.loc[:,'score_class'] = (np.ceil(b \/ 20)).astype(int)\nstudents_df = students_df.drop('math score',axis=\"columns\")\nstudents_df = students_df.drop('reading score',axis=\"columns\")\nstudents_df = students_df.drop('writing score',axis=\"columns\")\nstudents_df.head()","8e4b7c10":"sns.set()\ncorr = students_df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nax = sns.heatmap(corr, mask=mask,annot=True,annot_kws={\"size\": 7.5},linewidths=.5)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\");","eb071ece":"def test_classifiers(classifiers, y_values, X_train, X_test, y_train, y_test):\n    sns.set(font_scale=1.1)\n    fig=plt.figure(figsize=(18,9))\n    gs=gridspec.GridSpec(2,2) # 2 rows, 3 columns\n    gs.update(wspace=0.15, hspace=0.35)\n\n    axis_heatmap =[\n        fig.add_subplot(gs[0,0]),\n        fig.add_subplot(gs[0,1])\n    ]\n\n    axis_violin =[\n        fig.add_subplot(gs[1,0]),\n        fig.add_subplot(gs[1,1])\n    ]\n\n    clf_index = 0\n    for clf in classifiers:\n        print(f'Training {clf}')\n        clf = clf.fit(X_train, y_train)\n        print(f'Score : {clf.score(X_test, y_test)}')\n\n        results = pd.DataFrame()\n        results.loc[:,'real'] = y_test.copy()\n        results.loc[:,'predicted'] = list(map(lambda x:clf.predict([x,])[0], X_test))\n\n        axis_heatmap[clf_index].set_title(str(clf).split('(')[0])\n        axis_violin[clf_index].set_title(str(clf).split('(')[0])\n\n        g = sns.catplot(x=\"real\", y=\"predicted\", data=results, kind=\"violin\", ax=axis_violin[clf_index])\n        confusion_matrix = np.zeros((len(y_values),len(y_values)))\n        for i in range(0, len(results)):\n            confusion_matrix[results.loc[i,'predicted']-1][results.loc[i,'real']-1] += 1\n\n        # Confusion matrix dispolayed with a heatmap        \n        fig = sns.heatmap(confusion_matrix, robust=True, annot=True, linewidths=.3, ax=axis_heatmap[clf_index])\n        fig.set_xlabel('real')\n        fig.set_ylabel('predicted')\n        fig.yaxis.set_ticklabels(y_values)\n        _ = fig.xaxis.set_ticklabels(y_values)\n\n        # Table with the summary for each of the stats\n        TP = np.array([confusion_matrix[i,i] for i in range(0,len(y_values))])\n        FP = np.sum(confusion_matrix, axis=1) - TP\n        FN = np.sum(confusion_matrix, axis=0) - TP\n        TN = np.array([len(results) for i in range(len(y_values))]) - TP - FP - FN\n\n        precision = TP \/ np.maximum(np.ones(len(y_values)), (TP + FP))\n        recall = TP \/ np.maximum(np.ones(len(y_values)), (TP + FN))\n\n        header = []\n        table = [\n            [' ']+y_values,\n            ['TP',] + list(TP),\n            ['FP',] + list(FP),\n            ['TN',] + list(TN),\n            ['FN',] + list(FN),\n            ['Precision'] + list(precision),\n            ['Recall'] + list(recall)\n        ]\n\n        display(HTML(tabulate.tabulate(table, tablefmt='html')))\n        clf_index += 1\n\n    plt.close(2)\n    plt.close(3)","98ac62c5":"X = np.array(students_df.loc[:,['parental level of education', \\\n                                             'test preparation course',\n                                             'lunch',\n                                             'race\/ethnicity']]).copy()\ny = np.array(students_df.loc[:,'score_class']).copy()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","4b8d2d99":"classifiers = []\nclassifiers.append(tree.DecisionTreeClassifier())\nclassifiers.append(naive_bayes.ComplementNB())","a1c66327":"test_classifiers(classifiers, [1,2,3,4,5], X_train, X_test, y_train, y_test)","3d3ad2b0":"from sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier","bb515e7e":"X = np.array(students_df.loc[:,['test preparation course', \\\n                                'gender', \\\n                                'race\/ethnicity', \\\n                                 'parental level of education'\\\n                                ,'score_class', ]]).copy()\ny = np.array(students_df.loc[:,'lunch']).copy()\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.23, random_state=42)","81b12e15":"classifiers = []\nclassifiers.append(naive_bayes.ComplementNB())\nclassifiers.append(RandomForestClassifier(n_estimators=30, max_depth=5,random_state=0))","78d629b8":"test_classifiers(classifiers, [1,0], X_train, X_test, y_train, y_test)","97b216e1":"X = np.array(students_df.loc[:,['parental level of education', \\\n                                             'test preparation course',\n                                             'lunch',\n                                             'race\/ethnicity']]).copy()\nY = np.array(students_df.loc[:,'outlier']).copy()\n\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,Y)","6c03bf34":"dot_data = tree.export_graphviz(clf, \n                                out_file=None, \n                                feature_names=['parental level of education', \n                                'test preparation course', \n                                'lunch', \n                                'race\/ethnicity'],  \n                                filled=True, rounded=True,  \n                                special_characters=True)  \ngraph = graphviz.Source(dot_data)  \n#graph # uncomment to show the decision tree","e9ac2ea7":"results = pd.DataFrame()\nresults.loc[:,'real'] = Y.copy()\nresults.loc[:,'predicted'] = list(map(lambda x:clf.predict([x,])[0], X))\n\nconfusion_matrix = np.zeros((2,2))\nfor i in range(0, len(results)):\n    confusion_matrix[results.loc[i,'predicted']-1][results.loc[i,'real']-1] += 1\n    \nTP = np.array([confusion_matrix[i,i] for i in range(0,2)])\nFP = np.sum(confusion_matrix, axis=1) - TP\nFN = np.sum(confusion_matrix, axis=0) - TP\nTN = np.array([len(results) for i in range(2)]) - TP - FP - FN\n\nprecision = TP \/ np.maximum(np.ones(2), (TP + FP))\nrecall = TP \/ np.maximum(np.ones(2), (TP + FN))\n\ntable = [\n    [' ', '1', '0'],\n    ['TP',] + list(TP),\n    ['FP',] + list(FP),\n    ['TN',] + list(TN),\n    ['FN',] + list(FN),\n    ['Precision'] + list(precision),\n    ['Recall'] + list(recall)\n]\n\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))\n\n","ad9d4039":"fig = sns.heatmap(confusion_matrix, robust=True, annot=True, linewidths=.3)\nfig.set_title('Confusion matrix')\nfig.set_xlabel('real')\nfig.set_ylabel('predicted')\nfig.yaxis.set_ticklabels(['True', 'False'])\n_ = fig.xaxis.set_ticklabels(['True', 'False'])","ae32aa66":"X = np.array(students_df.loc[:,['parental level of education', \\\n                                             'test preparation course',\n                                             'lunch',\n                                             'race\/ethnicity']]).copy()\nY = np.array(students_df.loc[:,'outlier']).copy()\n\nclf = tree.DecisionTreeClassifier(max_depth=7, \n                                  class_weight=\"balanced\", \n                                  presort=True, \n                                  max_leaf_nodes=50, \n                                  min_samples_leaf=4, \n                                  min_samples_split=6)\nclf = clf.fit(X,Y)","e770580a":"results = pd.DataFrame()\nresults.loc[:,'real'] = Y.copy()\nresults.loc[:,'predicted'] = list(map(lambda x:clf.predict([x,])[0], X))\n\nconfusion_matrix = np.zeros((2,2))\nfor i in range(0, len(results)):\n    confusion_matrix[results.loc[i,'predicted']-1][results.loc[i,'real']-1] += 1\n    \nTP = np.array([confusion_matrix[i,i] for i in range(0,2)])\nFP = np.sum(confusion_matrix, axis=1) - TP\nFN = np.sum(confusion_matrix, axis=0) - TP\nTN = np.array([len(results) for i in range(2)]) - TP - FP - FN\n\nprecision = TP \/ np.maximum(np.ones(2), (TP + FP))\nrecall = TP \/ np.maximum(np.ones(2), (TP + FN))\n\ntable = [\n    [' ', '1', '0'],\n    ['TP',] + list(TP),\n    ['FP',] + list(FP),\n    ['TN',] + list(TN),\n    ['FN',] + list(FN),\n    ['Precision'] + list(precision),\n    ['Recall'] + list(recall)\n]\n\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","f9d2ac60":"fig = sns.heatmap(confusion_matrix, robust=True, annot=True, linewidths=.3)\nfig.set_title('Confusion matrix')\nfig.set_xlabel('real')\nfig.set_ylabel('predicted')\nfig.yaxis.set_ticklabels(['True', 'False'])\n_ = fig.xaxis.set_ticklabels(['True', 'False'])","ac65cf17":"social_factors = np.array(students_df.loc[:,['parental level of education', \\\n                                             'test preparation course',\n                                             'lunch',\n                                             'race\/ethnicity']]).copy()","b21fc63a":"kmeans = cluster.MiniBatchKMeans(n_clusters=5, random_state=0, batch_size=200)\nfor i in range(5):\n    kmeans = kmeans.partial_fit(social_factors[i:(200 + i*200),:])\n    \n    \n    clusters = np.zeros((social_factors.shape[0],1))\n    clusters[:,0] = kmeans.predict(social_factors)\n    #social_factors = np.append(social_factors,clusters,axis=1)\n    sc_df = pd.DataFrame(np.append(social_factors,clusters,axis=1), columns=['parental level of education', 'test preparation course', 'lunch', 'race\/ethnicity','cluster'])\n    flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n    sns.catplot(x=\"parental level of education\", \n                y='race\/ethnicity', \n                hue='cluster', \n                data=sc_df)","6a03073b":"Something that could be anticipated, but that I wanted to see, is that given the small amount of outliers, it would be easy to get a dumb clasifier. Indeed the accuracy is very good, (96.2%); however we can see how the precision and recall for the outliers is null.\n\nThus, no valuable conclusion can be extracted from this. With much more data regarding students whose performance is quite under the normal one, information about risk factors could have been extracted.\n\nHowever, we can try to make another go with some differencies in the parameters of the decission tree classifier :\n","25d1b685":"### 3.3 Classification with decision trees (outlier)","e0eb4d27":"### 3.1 Classification with Decision Trees and Complement Naive Bayes (score_class)\n#### Complement Naive Bayes is supposed to work well in unbalanced datasets.","a28f0798":"### 3.3 Clustering with kmeans","690ca622":"##### Outliers","b54c0c96":"## 4. Conclusions\n\nAs we have seen, there is a good reason to believe that students may have different scores between each others, but the overall performance of a student is common to the different basic competencies assessed in this analysis (with a slight difference between men and women in math tendencies). \n\nFurthermore, in this dataset a small group of lesser skilled students have been found and machine learning techniques have prove that we can assess if a student is going to perform worse than most of his schoolmates; even if more students are though to be worse than they actually are we can ensure, at least for this dataset, that a 92% of this cases could have been identifyed before the exams and thus, some measures to help them reach a good enough level of education.","f9fba7cc":"### 3.2 Classification with decision trees ()","7098eefa":"In this three last bar graphs we can appreciate how gender is equally distributed for parental level of education, race or ethnicity and if the student did a test preparation course. No specific value of these attributes presents a clear decompensation in regards to gender.\n\n##### Data for the later analysis\nFor the rest of the analysis, each entry will be assigned a class based on the arithmetic mean of his\/her scores on the three competencies now analyzed. In order to reduce the number of classes, five possible ones can be used:\n\n| 0-2 | 2-4 | 4-6 | 6-8 | 8-10 |\n\nThere will be another class for distinguishing between those students who are out of the normal performance (the outliers in the box plots above), and the rest. The intention here is to try and see if there is some way to predict wether a student may have extra difficoulties with his basic competencies based on the social factors that are available in the dataset.","72aa7007":"These results better fit the necessity of identifying those cases where the student may have a worse than normal performance at the basic skills. \n\nWe have here a model with a very low precission indeed **but**, it is preferable to have a higher rate of false positives if that proves to be effective at identifying all cases where the performance is bad. \u00bfWhy? Because this implies that help could be given to all those students who need it, benefiting in that case both these students and others whose performance is not in risk but who could make use of these preventive strategies to ensure the correct learning and a good adquisition of basic competencies.","0d7efd7f":"##### Overall good and overall bad students\nTaking into account how low the standard deviation (and thus the variance) of the scores in each of the basic competencies, and how similar the ranges of those are, one may ask if it is possible to afirm that a student's skill is transversal; if we can talk about how a student's performance in general without the need to specify the subject.","5e4e79bb":"As we can see, not only the pattern shows a clear linear correlation between the scores for the different skill, but a good centering in the origin; this means that scores of a student will be not only related but very similar independently of the type of the subject.\n\nMoreover, we can se how men have a slightly better performance in math related problems whereas women do perform better at reading and writing. \n\nWhen comparing reading and writing scores, the less of variation can be seen; which probably means that those abilities are more tightly related than with math.","124df693":"## 2. Formulation of the questions to be answered with ML\n\nHaving a discrete version of the overall performance of each student (arithmetic mean on the scores), I will try to make it possible to and create a model to predict such performance based on the rest of attributes, and using machine learning algorithms.\n\nApply unsupervised ML algorithms to discover knowledge.\n_____\n\nCan we predict ho well a student would by taking into account the social factors that are here presented? About those students whose scores are distant from the rest, do they all belong to the same group? Do they share any attribute? Is the type of lunch taken conditioned? And, if so, is it possible to predict what type of lunch would a student take based on the rest of information available?\n\nI will try to create a machine learning model to answer these questions; hopefuly it possible to get better results than the ones that are to be expected by looking at the linear correlations between the different dimensions of thee data. Thhese values seem really low to make a basic approach with linear functions such the technique of support vector machines.","b0ff61b9":"Something that catches my eye in the boxplots is that there are more outliers for male students; there seems to be a higher number of outliers with low scores. It would be interesting to see wether students have similar scores for all three subjects.\nHowever, when browsing for these outliers by myself using mean +- 3\\*std (standar deviation), I see less outliers and decide that these are not enough to try and extract a conclusion from them. \n\nThe problem is that the Seaborn is using the interquartile range to define the limits; therefore the mean minus 1.5 times the interquartile range is the lower limit for considering an entry normal or an outlier. Although there are only 38 outliers, I will try to do some analysis to see where it leads us. _I will use the outliers when they are not obtained with gender taken into account_.","9ffbfd63":"## 3. Use of the ML algorithms","33855082":"It is good news that there are almost equal number of entries for male and female respondants. This will help making statements less biased.\n\nBy encoding the categorical attributes with numerical values it is possible to see some statistics that, even though they don't have a literal meaning, can give a general idea of the data we are\nworking with :\n+ Only about a third of the students did a test reparation course.\n+ Scores for the three of the subjects (math, reading, and writing) are similar although math have a the lowest score.\n+ The scores for the three subjects almost share equal quantiles. Does this mean that the skill level of a student is transversal to the subjects?\n+ There are less students whose parents have an education level of bachelor's degree or master's degree. This could be further investigated.\n+ There are more students with standard lunch. Does this influeence the student's scores? Is this related to some extent to gender, race\/ethnicity, or the education level of the parents?","76cf384b":"# Student performance\nThis analysis is focused on applying and evaluating machine learning techniques, with an enfasis on worst case students and identifying those before the exams leave them with few options. The analysis is structured as follows :\n1. Preparation, presentation and initial EDA of the data\n2. Formulation of the questions to be answered with ML\n3. Use of the ML algorithms\n4. Conclusions\n","55fafd62":"## 1. Preparation, presentation and initial EDA of the data\n\nCathegorical data will be encoded with numerical values, and some basic EDA will be done on the data in order to give a sense of the data. This will help the later formulation of the questions\nthat will motivate the analysis and use of machine learning techniques."}}