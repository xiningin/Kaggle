{"cell_type":{"c5601189":"code","13a73704":"code","20e5fcc2":"code","02f1d0e9":"code","fcea3049":"code","0d60ca00":"code","6971ac19":"code","0f628fe9":"code","49565dbb":"code","5e66891d":"code","ddce3608":"code","fbb77e0a":"code","3393c5aa":"code","a8f8523d":"code","cce80071":"code","0e488148":"code","794c596d":"code","03fcd03f":"code","c0d22059":"code","a6d9bf0b":"markdown","1c78020b":"markdown","c6a06f4f":"markdown","87439eda":"markdown","642dfa22":"markdown"},"source":{"c5601189":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io\nimport keras\n\n%matplotlib inline","13a73704":"keras_model = tf.keras.applications.vgg19.VGG19(weights = 'imagenet', include_top = False)\npreprocess_input = tf.keras.applications.vgg19.preprocess_input\nkeras_model.summary()","20e5fcc2":"def load_img(url, target_size = None):\n    image = io.imread(url)\n    image = image \/ 255.0\n    image = tf.convert_to_tensor(image, dtype=tf.float32)\n    image = tf.expand_dims(image, axis = 0)\n    return image\n\ndef show_image(image, name = ''):\n    image = tf.reshape(image, shape = (image.shape[1], image.shape[2], image.shape[3]))\n    image = image.numpy()\n    plt.imshow(image)\n    plt.xlabel(name)","02f1d0e9":"content_file = 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/YellowLabradorLooking_new.jpg'\nstyle_file = 'https:\/\/st2.depositphotos.com\/3224051\/6529\/i\/950\/depositphotos_65299155-stock-photo-abstract-artificial-computer-generated-iterative.jpg'\ncontent_image = load_img(content_file)\nstyle_image = load_img(style_file)","fcea3049":"images = { 'content_image' : content_image, 'style_image' : style_image }\n_ = plt.figure(figsize=(25, 7))\nfor ix, name in enumerate(images.keys()):\n    _ = plt.subplot(1, 2, ix + 1)\n    show_image(images[name], name)\n    _ = plt.xticks([])\n    _ = plt.yticks([])","0d60ca00":"content_layers = {\n    'block1_conv1' : 1\n} \n\nstyle_layers = {\n    'block1_conv1' : 1,\n                'block2_conv1' : 1,\n                'block3_conv1' : 1, \n                'block4_conv1' : 1, \n                'block5_conv1' : 1\n}","6971ac19":"def gram_matrix(a_s):\n    _, n_h, n_w, n_c = a_s.shape\n    a_s = tf.reshape(a_s, (n_h * n_w, n_c))\n    a_s = tf.matmul(tf.transpose(a_s), a_s)\n    a_s = a_s \/ (n_h * n_w)\n    return a_s","0f628fe9":"def get_model(model_layers):\n    inp = keras_model.input\n    output = [keras_model.get_layer(name).output for name in model_layers]\n    model = Model([keras_model.input], output)\n    model.trainable = False  \n    return model","49565dbb":"extractor = get_model(list(content_layers.keys()) + list(style_layers.keys()))\nextractor.summary()","5e66891d":"def get_style_content_activation(image):\n    preprocess_image = preprocess_input(image * 255)\n    output = extractor(preprocess_image)\n    content_output = output[:len(content_layers)]\n    style_output = output[len(content_layers):]\n    \n    content_output = {layer : value for layer , value in zip(content_layers.keys(), content_output)}\n    style_output = {layer : gram_matrix(value) for layer , value in zip(style_layers.keys(), style_output)}\n    \n    return {'content' : content_output, 'style' : style_output}","ddce3608":"content_targets = get_style_content_activation(content_image)['content']\nstyle_targets = get_style_content_activation(style_image)['style']","fbb77e0a":"style_weight = 1e-2\ncontent_weight = 1e4","3393c5aa":"def get_style_content_loss(image):\n    output = get_style_content_activation(image)\n    content_output = output['content']\n    style_output = output['style']\n\n    content_loss = tf.add_n([content_layers[name] * tf.reduce_mean((content_output[name] - content_targets[name])**2) \n                            for name in content_output.keys()])\n    style_loss = tf.add_n([style_layers[name] * tf.reduce_mean((style_output[name] - style_targets[name])**2) \n                        for name in style_output.keys()])\n\n    content_loss = content_loss * content_weight \/ len(content_layers)\n    style_loss = style_loss * style_weight \/ len(style_layers)\n    loss = content_loss + style_loss\n    return loss","a8f8523d":"tf_generated_image = tf.Variable(content_image)\nopt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","cce80071":"def clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","0e488148":"def train_step(tf_generated_image):\n    with tf.GradientTape() as tape:\n        loss = get_style_content_loss(tf_generated_image)\n        #print('{}. loss: {}'.format(ix,loss))\n    grad = tape.gradient(loss, tf_generated_image)\n    opt.apply_gradients([(grad, tf_generated_image)])\n    tf_generated_image.assign(clip_0_1(tf_generated_image))","794c596d":"for ix in range(10):\n    train_step(tf_generated_image)\n    \nimages = { 'content_image' : content_image, 'style_image' : style_image, 'generated_image' : tf_generated_image }\n_ = plt.figure(figsize=(25, 7))\nfor ix, name in enumerate(images.keys()):\n    _ = plt.subplot(1, len(images), ix + 1)\n    show_image(images[name], name)\n    _ = plt.xticks([])\n    _ = plt.yticks([])","03fcd03f":"epochs = 100\nsteps_per_epoch = 10\nplt.figure(figsize = (25, 140))\nfor epoch in range(epochs):\n    for _ in range(steps_per_epoch):\n        train_step(tf_generated_image)\n    plt.subplot(20, 5, epoch + 1)\n    show_image(tf_generated_image, str(epoch))\n    _ = plt.xticks([])\n    _ = plt.yticks([])    ","c0d22059":"images = { 'content_image' : content_image, 'style_image' : style_image, 'generated_image' : tf_generated_image}\n_ = plt.figure(figsize=(25, 7))\nfor ix, name in enumerate(images.keys()):\n    _ = plt.subplot(1, len(images), ix + 1)\n    show_image(images[name], name)\n    _ = plt.xticks([])\n    _ = plt.yticks([])","a6d9bf0b":"<h1>VGG 19 model<\/h1>\n<p>For neural style transfer VGG19 because VGG19 is working better with style transfer problem. For model to transfer we don't need fully connected layer from model. As not using fully connected layers so there is no bound in image size and can be used any size image. But to make transfer faster can be used optimized image shape<\/p>\n<img src = 'https:\/\/www.researchgate.net\/profile\/Michael_Wurm\/publication\/331258180\/figure\/fig1\/AS:728763826442243@1550762244632\/Architecture-of-the-FCN-VGG19-adapted-from-Long-et-al-2015-which-learns-to-combine.png'\/>","1c78020b":"* API used for load and show image from numpy array.\n* image can be load from any http url or locally","c6a06f4f":"### content and style image ","87439eda":"# Neural Style Transfer\n## Introduction\n* <p>Neural style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.<\/p>\n\n<table>\n    <tr>\n        <th>\n<img src = 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/YellowLabradorLooking_new.jpg', width = '400px', height = '400px'\/>\n        <\/th>\n        <th>\n            <h1>+<\/h1>\n        <\/th>\n        <th>\n<img src = 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg', width = '400px', height = '400px'\/>\n        <\/th>\n        <th>\n            <h1>=<\/h1>\n        <\/th>        \n        <th>\n<img src = 'https:\/\/www.tensorflow.org\/tutorials\/generative\/images\/stylized-image.png', width = '400px', height = '400px'\/>\n        <\/th>  \n    <\/tr>\n      <tr>\n        <th>\n<p style=\"text-align:center\">content image<\/p>\n        <\/th>\n        <th>\n        <\/th>\n        <th>\n<p style=\"text-align:center\">style image<\/p>\n        <\/th>\n        <th>\n        <\/th>        \n        <th>\n<p style=\"text-align:center\">merge image<\/p>\n        <\/th>  \n    <\/tr>\n  <\/table>","642dfa22":"# import require modules"}}