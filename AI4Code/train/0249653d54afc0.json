{"cell_type":{"c1d88388":"code","fc951c61":"code","da32686c":"code","5165b756":"code","0fa527f1":"code","e19e5978":"code","57e33bc5":"code","ca216272":"code","34ca512d":"code","37774c50":"code","333bd6ea":"code","1626870f":"code","898a4575":"code","be89af6b":"code","b04bf3a5":"code","4c1dd6e2":"code","1df77b57":"code","8cd5ef7b":"code","b7d994f2":"code","f21f858c":"code","adcb6e71":"code","8733fa28":"code","112aba85":"code","9998c5de":"code","1c7a6f9a":"code","dd62f72a":"code","9ca013b5":"code","f3be470f":"code","489cbf96":"code","6c75db72":"code","51900dde":"code","23b4c219":"code","5507d610":"code","9545938f":"code","efb1326f":"code","7f763662":"code","dd716453":"code","ad999fd3":"code","0ab3d7b4":"code","868ebc95":"code","a0865a99":"code","a8feaee1":"code","bcca9e59":"code","bba0bba3":"code","2840a663":"code","38259659":"code","54f4d4f7":"code","e902eef7":"code","8c82d70d":"code","2118d1c8":"code","d13b06eb":"code","aab4f449":"code","27d86fac":"code","12210f6a":"code","da2a26bd":"markdown","6190ebea":"markdown","1725a013":"markdown","5354a9ab":"markdown","d185e9e3":"markdown","264dc8c9":"markdown","685a259e":"markdown","66d8a950":"markdown","458e44e1":"markdown","ed3a0645":"markdown","c03b3044":"markdown","5e0c1012":"markdown","cc604a5a":"markdown","b365d754":"markdown","5c8556c9":"markdown","bde32cd3":"markdown","2b017dc8":"markdown","b532c6e0":"markdown","e190628e":"markdown"},"source":{"c1d88388":"import numpy as np\nimport pandas as pd \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns               # Provides a high level interface for drawing attractive and informative statistical graphics\n%matplotlib inline\nsns.set()\nfrom subprocess import check_output\n\nimport warnings                                            # Ignore warning related to pandas_profiling\nwarnings.filterwarnings('ignore') \n\ndef annot_plot_num(ax,w,h):                                    # function to add data to plot\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    for p in ax.patches:\n        ax.annotate('{0:.1f}'.format(p.get_height()), (p.get_x()+w, p.get_height()+h))\n        \ndef annot_plot(ax,w,h):                                    # function to add data to plot\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    for p in ax.patches:\n         ax.annotate(f\"{p.get_height() * 100 \/ df.shape[0]:.2f}%\", (p.get_x() + p.get_width() \/ 2., p.get_height()),\n         ha='center', va='center', fontsize=11, color='black', rotation=0, xytext=(0, 10),\n         textcoords='offset points')\n\nimport os\nprint(os.listdir(\"..\/input\"))","fc951c61":"df = pd.read_csv('..\/input\/german_credit_data.csv')","da32686c":"print('**********Shape of the Dataset.*******************************************')\nprint(df.shape)\nprint('**********Column names******************************************************')\nprint(df.columns)\nprint('**********Total number of null values in each column.**********************')\nprint(df.isnull().sum())\nprint('**********Total number of unique values in each column*********************')\nprint(df.nunique())\n","5165b756":"df.head()","0fa527f1":"df.rename(columns=lambda x:x.replace('Unnamed: 0','id'), inplace = True )","e19e5978":"df.info()","57e33bc5":"df['Checking account'].fillna('no-info', inplace = True)\ndf['Saving accounts'].fillna('no-info', inplace = True)","ca216272":"ax = sns.countplot('Risk', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot_num(ax,0.08,1)\nplt.show()","34ca512d":"ax = sns.countplot('Sex',hue='Risk', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot(ax,0.08,1)\nplt.show()","37774c50":"plt.figure(figsize=(12,7))\nax = sns.countplot('Duration',hue='Risk', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot(ax,0.08,1)\nplt.show()","333bd6ea":"gender_df = df.groupby(['Sex','Risk'])['Purpose'].value_counts()\ngender_df","1626870f":"plt.figure(figsize=(12,7))\nax = sns.countplot('Sex',hue='Job', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot_num(ax,0.008,1)\nplt.show()","898a4575":"plt.show()\nplt.figure(figsize=(12,7))\nax = sns.countplot('Risk',hue='Job', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot_num(ax,0.008,1)\nplt.show()","be89af6b":"plt.figure(figsize=(12,7))\nax = sns.countplot('Risk',hue='Housing', data = df)\nplt.ylabel('Total number of credit holders.')\nannot_plot_num(ax,0.008,1)\nplt.show()","b04bf3a5":"gender_df = df.groupby(['Purpose','Risk'])['Sex'].value_counts()\ngender_df","4c1dd6e2":"purpose_group = gender_df.groupby('Purpose')\nfig = plt.figure()\ncount =  1\n\nfor gender, group in purpose_group:\n    ax = fig.add_subplot(2,4,count)\n    ax.set_title(gender)\n    ax = group[gender].plot.bar(figsize = (10,5), width = 0.8)\n    \n    count+=1\n    \n    plt.xlabel('')\n    plt.yticks([])\n    plt.ylabel('Holders')\n    \n    total_of_holders = []\n    for i in ax.patches:\n        total_of_holders.append(i.get_height())\n        total = sum(total_of_holders)\n    for i in ax.patches:\n         ax.text(i.get_x()+0.2, i.get_height()-1.5,s= i.get_height(),color=\"black\",fontweight='bold')\nplt.tight_layout()\nplt.show()","1df77b57":"job_df = df.groupby(['Job','Sex'])['Risk'].value_counts()\njob_group = job_df.groupby('Job')\n\nfig = plt.figure()\ncount =  1\n\nfor gender, group in job_group:\n    ax = fig.add_subplot(2,4,count)\n    ax.set_title(gender)\n    ax = group[gender].plot.bar(figsize = (10,5), width = 0.8)\n    \n    count+=1\n    \n    plt.xlabel('')\n    plt.yticks([])\n    plt.ylabel('Holders')\n    \n    total_of_holders = []\n    for i in ax.patches:\n        total_of_holders.append(i.get_height())\n        total = sum(total_of_holders)\n    for i in ax.patches:\n         ax.text(i.get_x()+0.2, i.get_height()-1.5,s= i.get_height(),color=\"black\",fontweight='bold')\nplt.tight_layout()\nplt.show()","8cd5ef7b":"sns.pairplot(data = df)\nplt.show()","b7d994f2":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf['Sex'] = le.fit_transform(df['Sex'])\ndf['Housing'] = le.fit_transform(df['Housing'])\ndf['Purpose'] = le.fit_transform(df['Purpose'])\ndf['Risk'] = le.fit_transform(df['Risk'])\ndf['Saving accounts'] = le.fit_transform(df['Saving accounts'])\ndf['Checking account'] = le.fit_transform(df['Checking account'])\n","f21f858c":"df.head(2)","adcb6e71":"sns.countplot('Risk', data = df)\nplt.show()","8733fa28":"corr = df.corr()\nplt.figure(figsize=(18,10))\nsns.heatmap(corr, annot = True)\nplt.show()","112aba85":"df.columns","9998c5de":"features = ['Age', 'Sex', 'Job', 'Housing', 'Saving accounts',\n       'Checking account', 'Credit amount', 'Duration', 'Purpose']\nx = df.loc[:,features].values\ny = df.loc[:,['Risk']].values #target var","1c7a6f9a":"#Step 1: Standardize the data\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(x) ","dd62f72a":"pd.DataFrame(data = X, columns = features).head()","9ca013b5":"# Setp 2 : PCA Projection to 2D\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform (x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns=['PC1','PC2','PC3']) #PC = Principal component \nprincipalDf.head()","f3be470f":"finalDf = pd.concat([principalDf,df[['Risk']]], axis = 1)\nfinalDf.head()","489cbf96":"# Step 3 - Visualize the Data in 2D.\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\ntargets = [1,0]\n\ncolors = ['r','g']\n\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['Risk'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n               , finalDf.loc[indicesToKeep, 'PC2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nplt.show()","6c75db72":"pca.explained_variance_ratio_","51900dde":"y = df['Risk']\nX = df.drop(['Risk','id'], axis = 1)","23b4c219":"#Split the dataset into train and test dataset.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","5507d610":"from sklearn.preprocessing import StandardScaler\nScaler_X = StandardScaler()\nX_train = Scaler_X.fit_transform(X_train)\nX_test = Scaler_X.transform(X_test)","9545938f":"y_test.value_counts()","efb1326f":"#let's check what params will be best suitable for random forest classification.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score\n\nrfc_clf = RandomForestClassifier()\nparams = {'n_estimators':[25,50,100,150,200,500],'max_depth':[0.5,1,5,10],'random_state':[1,10,20,42],\n          'n_jobs':[1,2]}\ngrid_search_cv = GridSearchCV(rfc_clf, params, scoring='precision')\ngrid_search_cv.fit(X_train, y_train)\n","7f763662":"print(grid_search_cv.best_estimator_)\nprint(grid_search_cv.best_params_)","dd716453":"rfc_clf = grid_search_cv.best_estimator_\nrfc_clf.fit(X_train,y_train)\nrfc_clf_pred = rfc_clf.predict(X_test)\nprint('Accuracy:',accuracy_score(rfc_clf_pred,y_test) )\nprint('Confusion Matrix:', confusion_matrix(rfc_clf_pred,y_test).ravel()) #tn,fp,fn,tp\nprint('Classification report:')\nprint(classification_report(rfc_clf_pred,y_test))\n\n# Let's make sure the data is not overfitting\nscore_rfc = cross_val_score(rfc_clf,X_train,y_train,cv = 10).mean()\nprint('cross val score:', score_rfc)","ad999fd3":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n# Implement gridsearchcv to see which are our best p\n\nparams = {'C': [0.75, 0.85, 0.95, 1], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n          'degree': [3, 4, 5]}\n\nsvc_clf = SVC(random_state=42)\ngrid_search_cv = GridSearchCV(svc_clf, params)\ngrid_search_cv.fit(X_train, y_train)","0ab3d7b4":"print(grid_search_cv.best_estimator_)\nprint(grid_search_cv.best_params_)","868ebc95":"svc_clf = grid_search_cv.best_estimator_\nsvc_clf.fit(X_train,y_train)\nsvc_pred = svc_clf.predict(X_test)\n\nprint('Accuracy:',accuracy_score(svc_pred,y_test) )\nprint('Confusion Matrix:', confusion_matrix(svc_pred,y_test,labels=[0,1])) #tn,fp,fn,tp\nprint('Classification report:')\nprint(classification_report(svc_pred,y_test))\n\n# Let's make sure the data is not overfitting\nscore_svc = cross_val_score(svc_clf,X_train,y_train, cv = 10).mean()\nprint('cross val score:', score_svc)","a0865a99":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nlr_pred = lr.predict(X_test)\n\nprint('Accuracy:',accuracy_score(lr_pred,y_test) )\nprint('Confusion Matrix:', confusion_matrix(lr_pred,y_test,labels=[0,1])) #tn,fp,fn,tp\nprint('Classification report:')\nprint(classification_report(lr_pred,y_test))\n\n# Let's make sure the data is not overfitting\nscore_lr = cross_val_score(lr,X_train,y_train,cv=10).mean()\nprint('cross val score:', score_lr)","a8feaee1":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\ngbc_pred = gbc.predict(X_test)\n\nprint('Accuracy:',accuracy_score(gbc_pred,y_test) )\nprint('Confusion Matrix:', confusion_matrix(gbc_pred,y_test,labels=[0,1])) #tn,fp,fn,tp\nprint('Classification report:')\nprint(classification_report(gbc_pred,y_test))\n\n# Let's make sure the data is not overfitting\nscore_gbc = cross_val_score(gbc,X_train,y_train, scoring='accuracy', cv = 10).mean()\nprint('cross val score:', score_gbc)","bcca9e59":"models = pd.DataFrame({'Models':['Random Forest Classifier','Logistic Regression', \n                                 'Gradient Boost Classifier', 'Support Vector Classifier'],\n                      'Score':[score_rfc,score_lr,score_gbc,score_svc]})\nmodels.sort_values(by='Score', ascending = False)\n","bba0bba3":"fig, ax_arr = plt.subplots(nrows = 2, ncols = 2, figsize = (20,15))\n\nfrom sklearn import metrics\n\n#gbc\ngbc_prob = gbc.predict_proba(X_test)[:,1]\nfprgbc, tprgbc, thresholdsgbc = metrics.roc_curve(y_test, gbc_prob)\nroc_auc_gbc = metrics.auc(fprgbc,tprgbc)\nax_arr[0,0].plot(fprgbc, tprgbc,'b',label = 'AUC = %0.2f' % roc_auc_gbc,color = 'red')\nax_arr[0,0].plot([0, 1], [0, 1], 'k--')\nax_arr[0,0].set_xlabel('False Positive Rate')\nax_arr[0,0].set_ylabel('True Positive Rate')\nax_arr[0,0].set_title('ROC for GBC.', fontsize = 20)\nax_arr[0,0].legend(loc = 'lower right', prop={'size': 16})\n\n\n#Random forest\nrfc_prob = rfc_clf.predict_proba(X_test)[:,1]\nfprRfc, tprRfc, thresholdsRfc = metrics.roc_curve(y_test, rfc_prob)\nroc_auc_rfc = metrics.auc(fprRfc,tprRfc)\nax_arr[0,1].plot(fprRfc, tprRfc,'b',label = 'AUC = %0.2f' % roc_auc_rfc,color = 'green')\nax_arr[0,1].plot([0, 1], [0, 1], 'k--')\nax_arr[0,1].set_xlabel('False Positive Rate')\nax_arr[0,1].set_ylabel('True Positive Rate')\nax_arr[0,1].set_title('ROC for RFC.', fontsize = 20)\nax_arr[0,1].legend(loc = 'lower right', prop={'size': 16})\n\n\n#Logistic Regression\nlr_prob = lr.predict_proba(X_test)[:,1]\nfprLr, tprLr, thresholdsLr = metrics.roc_curve(y_test, lr_prob)\nroc_auc_lr = metrics.auc(fprLr,tprLr)\nax_arr[1,0].plot(fprLr, tprLr,'b',label = 'AUC = %0.2f' % roc_auc_lr,color = 'blue')\nax_arr[1,0].plot([0, 1], [0, 1], 'k--')\nax_arr[1,0].set_xlabel('False Positive Rate')\nax_arr[1,0].set_ylabel('True Positive Rate')\nax_arr[1,0].set_title('ROC for Logistic.', fontsize = 20)\nax_arr[1,0].legend(loc = 'lower right', prop={'size': 16})\n\n#For All\nax_arr[1,1].plot(fprgbc,tprgbc, label ='Gradient Boost', color = 'red')\nax_arr[1,1].plot(fprRfc,tprRfc, label ='Random Forest', color = 'green')\nax_arr[1,1].plot(fprLr,tprLr, label ='Logistic Regression', color = 'blue')\nax_arr[1,1].plot([0, 1], [0, 1], 'k--')\nax_arr[1,1].set_title('Receiver Operating Comparison ',fontsize=20)\nax_arr[1,1].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,1].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,1].legend(loc = 'lower right', prop={'size': 16})\n\nplt.subplots_adjust(wspace=0.2)\nplt.tight_layout() \nplt.show()","2840a663":"pca = PCA(n_components=5)\npca.fit(X_train)","38259659":"pca.n_components_","54f4d4f7":"#Apply the mapping (transform) to both the training set and the test set.\ntrain_X = pca.transform(X_train)\ntest_X = pca.transform(X_test)","e902eef7":"from sklearn.linear_model import LogisticRegression","8c82d70d":"# all parameters not specified are set to their defaults\n# default solver is incredibly slow thats why we change it\n# solver = 'lbfgs'\nlogisticRegr = LogisticRegression(solver = 'lbfgs')","2118d1c8":"logisticRegr.fit(train_X, y_train)","d13b06eb":"logR_pred = logisticRegr.predict(test_X)","aab4f449":"logisticRegr.score(test_X,y_test)","27d86fac":"confusion_matrix(logR_pred,y_test)","12210f6a":"logR_cross_val_score = cross_val_score(logisticRegr,train_X,y_train, cv = 10).mean()\nlogR_cross_val_score","da2a26bd":"## PCA to Speed up Machine Learning Algorithms (Logistic Regression)\nStep 0: Import and use PCA. After PCA you will apply a machine learning algorithm of your choice to the transformed data","6190ebea":"****Accuracy is measured by the area under the ROC curve. An area of 1 represents a perfect test; an area of .5 represents a worthless test.****\n\nA rough guide for classifying the accuracy of a diagnostic test is the traditional academic point system:****\n\n* .90-1 = excellent (A)\n\n* .80-.90 = good (B)\n\n* .70-.80 = fair (C)\n\n* .60-.70 = poor (D)\n\n* .50-.60 = fail (F)","1725a013":"# Preprocess the dataset:\n**We have to convert all categorical values into numerical values.**","5354a9ab":"# Models:","d185e9e3":"\n**Step 2: Make an instance of the Model**","264dc8c9":"# Table of content:\n\n 1. Data Cleaning.\n 2. Data Visualization.\n 3. PCA for visualization\n 4. Models\n>     4.1 Random Forest Classifier\n>     4.2 Support Vector Classifier\n>     4.3 Logistic Regression\n>     4.4 Gradient Boost Classifier\n\n 5. ROC and AUC curve.\n 6. PCA for speed up the Machine learning Algo: Logistic Regression ","685a259e":"## SupportVectorClassifier:","66d8a950":"**Don't forget to upvote if you find this kernal usefull :)**\n**Happy Learning.**","458e44e1":"## LogisticRegression","ed3a0645":"## Random Forest Classifier.\n** Let's choose the best estimator and parameters :GridSearchCV**","c03b3044":"## ANALYZING THE RESULTS","5e0c1012":"## EDA:Exploratory Data Analysis(Data Visualization)","cc604a5a":"## PCA for Data Visualization\n**Steps:**\n1. **Standardize the Data** - Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. Although, all features in the Iris dataset were measured in centimeters, let us continue with the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms.\n2. PCA Projection to 2D\n3. Visualize 2D Projection","b365d754":"## Explained Variance\nThe explained variance tells us how much information (variance) can be attributed to each of the principal components.","5c8556c9":"## Select the Dependent(target) and Independent Variables:","bde32cd3":"## ROC and AUC curve:\n* ROC - ROC make it easy to identify the best threshold for making a decision.\n* AUC - AUC help us to decide which categorization method is better.","2b017dc8":"## Gradient Boosting: Classifier","b532c6e0":"**Step 1: Import the model you want to use**\n\nIn sklearn, all machine learning models are implemented as Python classes","e190628e":"**So now we have to decide which one is the best model, and we have two types of wrong values:**\n\n* False Positive, means they won't pay the loan(Risk:Yes), but the model thinks they will.\n* False Negative, means they will pay the loan(Risk:No), but the model said they won't.\n\n**In my opinion:**\n\n* Length of the dataset isn't enough, we need more data for better accuracy."}}