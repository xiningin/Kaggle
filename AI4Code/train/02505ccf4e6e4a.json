{"cell_type":{"14aed4a2":"code","8b4afc6b":"code","53eea209":"code","9d2a40a4":"code","e1e43d55":"code","92fd75fa":"code","6cdcb479":"code","1c22aacd":"code","10708f30":"code","49166c02":"code","611be247":"code","7de3e6f3":"code","a0ec12ef":"code","b43893df":"code","ded78cfe":"code","4e76c0dc":"code","bb448f06":"code","c50eb211":"code","f3e40506":"code","576a4419":"code","ec0dcf22":"code","bd9737a0":"code","4d9a203a":"markdown","3b969fb6":"markdown","b47b0bb7":"markdown","d37e9ed7":"markdown","e93ff47b":"markdown","2b70a7cb":"markdown"},"source":{"14aed4a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom scipy.stats import shapiro\nfrom scipy.stats import levene\nimport missingno\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\n#nlp\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english'))\nimport nltk\nfrom nltk.stem.porter import *\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast, BertTokenizer\n\nfrom pathlib import Path\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b4afc6b":"train =pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n#test=pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","53eea209":"#assigning dataframe as df \ndf = train.copy()\ndft=test.copy()\n","9d2a40a4":"\nprint('SHAPE'.center(80,\"*\"))\nprint('rows:{} columns:{} '.format(df.shape[0],df.shape[1]))\nprint('datatypes'.center(80,\"*\"))\nprint(df.dtypes)\nprint('descriptions'.center(80,\"*\"))\nprint(df.describe(include=\"all\"))\nprint('Info'.center(80,\"*\"))\nprint(df.info())","e1e43d55":"for colmns in df.columns:\n    print('Column name:{}     Null values :{} '.format(colmns,df[colmns].isnull().sum()))\n  ","92fd75fa":"\ndf.duplicated().sum()\ndf[df.duplicated()]\ndf.drop_duplicates(keep='first',inplace=True)\ndf.duplicated().sum()","6cdcb479":"dfs=df[['id','excerpt','target','standard_error']]\ndfs.info()","1c22aacd":"df['target'].corr(df['standard_error'])","10708f30":"common_titles= [\"MR \", \"MRS \", \"MS \", \"MISS \", \"DR \", \"HERR \", \"MONSIEUR \", \"HR \", \"FRAU \", \"A V M \", \"ADMIRAAL \", \n                \"ADMIRAL \", \"ALDERMAN \", \"ALHAJI \", \"AMBASSADOR \", \"BARON \", \"BARONES \", \"BRIG \", \"BRIGADIER \", \"BROTHER \", \"CANON \", \"CAPT \", \"CAPTAIN \", \n                \"CARDINAL \", \"CDR \", \"CHIEF \", \"CIK \", \"CMDR \", \"COL \", \"COLONEL \", \"COMMANDANT \", \"COMMANDER \", \"COMMISSIONER \", \"COMMODORE \", \"COMTE \", \n                \"COMTESSA \", \"CONGRESSMAN \", \"CONSEILLER \", \"CONSUL \", \"CONTE \", \"CONTESSA \", \"CORPORAL \", \"COUNCILLOR \", \"COUNT \", \"COUNTESS \", \"AIR CDRE \", \n                \"AIR COMMODORE \", \"AIR MARSHAL \", \"AIR VICE MARSHAL \", \"BRIG GEN \", \"BRIG GENERAL \", \"BRIGADIER GENERAL \", \"CROWN PRINCE \", \"CROWN PRINCESS \", \n                \"DAME \", \"DATIN \", \"DATO \", \"DATUK \", \"DATUK SERI \", \"DEACON \", \"DEACONESS \", \"DEAN \", \"DHR \", \"DIPL ING \", \"DOCTOR \", \"DOTT \", \"DOTT SA \", \n                \"DR \", \"DR ING \", \"DRA \", \"DRS \", \"EMBAJADOR \", \"EMBAJADORA \", \"EN \", \"ENCIK \", \"ENG \", \"EUR ING \", \"EXMA SRA \", \"EXMO SR \", \"F O \", \n                \"FATHER \", \"FIRST LIEUTIENT \", \"FIRST OFFICER \", \"FLT LIEUT \", \"FLYING OFFICER \", \"FR \", \"FRAU \", \"FRAULEIN \", \"FRU \", \"GEN \", \"GENERAAL \", \n                \"GENERAL \", \"GOVERNOR \", \"GRAAF \", \"GRAVIN \", \"GROUP CAPTAIN \", \"GRP CAPT \", \"H E DR \", \"H H \", \"H M \", \"H R H \", \"HAJAH \", \"HAJI \", \n                \"HAJIM \", \"HER HIGHNESS \", \"HER MAJESTY \", \"HERR \", \"HIGH CHIEF \", \"HIS HIGHNESS \", \"HIS HOLINESS \", \"HIS MAJESTY \", \"HON \", \"HR \", \n                \"HRA \", \"ING \", \"IR \", \"JONKHEER \", \"JUDGE \", \"JUSTICE \", \"KHUN YING \", \"KOLONEL \", \"LADY \", \"LCDA \", \"LIC \", \"LIEUT \", \"LIEUT CDR \", \n                \"LIEUT COL \", \"LIEUT GEN \", \"LORD \", \"MADAME \", \"MADEMOISELLE \", \"MAJ GEN \", \"MAJOR \", \"MASTER \", \"MEVROUW \", \"MISS \", \"MLLE \", \"MME \", \n                \"MONSIEUR \", \"MONSIGNOR \", \"MSTR \", \"NTI \", \"PASTOR \", \"PRESIDENT \", \"PRINCE \", \"PRINCESS \", \"PRINCESSE \", \"PRINSES \", \"PROF \", \n                \"PROF DR \", \"PROF SIR \", \"PROFESSOR \", \"PUAN \", \"PUAN SRI \", \"RABBI \", \"REAR ADMIRAL \", \"REV \", \"REV CANON \", \"REV DR \", \"REV MOTHER \", \n                \"REVEREND \", \"RVA \", \"SENATOR \", \"SERGEANT \", \"SHEIKH \", \"SHEIKHA \", \"SIG \", \"SIG NA \", \"SIG RA \", \"SIR \", \"SISTER \", \"SQN LDR \", \"SR \", \n                \"SR D \", \"SRA \", \"SRTA \", \"SULTAN \", \"TAN SRI \", \"TAN SRI DATO \", \"TENGKU \", \"TEUKU \", \"THAN PUYING \", \"THE HON DR \", \"THE HON JUSTICE \", \n                \"THE HON MISS \", \"THE HON MR \", \"THE HON MRS \", \"THE HON MS \", \"THE HON SIR \", \"THE VERY REV \", \"TOH PUAN \", \"TUN \", \"VICE ADMIRAL \", \n                \"VISCOUNT \", \"VISCOUNTESS \", \"WG CDR \"]\n\ncommon_titles_new=[]\nfor i in common_titles:\n    common_titles_new.append(i.strip().lower())\n    \n\n    \n    \n    ","49166c02":"#preprocessing the text data \ndef swst(x):\n\n    text_sw=[]\n    text_final=[]\n    \n    text_wp=[]\n    people_name=[]\n    stopwords=list(stop_words) + common_titles_new\n    #listing person names\n    [people_name.append(w.text) for w in nlp(x).ents if w.label_ == 'PERSON' ]\n    [text_wp.append(w) for w in x.split() if w not in people_name]\n    text_wp=(\" \").join(text_wp)\n    punc = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    for word in text_wp:\n        if word in punc:\n            text_wp = text_wp.replace(word, \"\")\n    \n    \n    [text_sw.append(w) for w in (text_wp.lower()).split() if w not in stopwords]\n    text_sw=(\" \").join(text_sw)\n    \n            \n    stemmer = PorterStemmer()\n    for token in text_sw.split():\n        text_final.append((stemmer.stem(token)))\n    clean_txt=\" \".join(text_final)    \n        \n    return  clean_txt\n    ","611be247":"#traindata\ndf_train=df[['id','excerpt']]\ndf_train_target=df[['target']]\ndf_train['excerpt']=df_train['excerpt'].apply(lambda x: swst(x))\n#testdata\ndf_test=dft[['id','excerpt']]\ndf_test['excerpt']=df_test['excerpt'].apply(lambda x: swst(x))","7de3e6f3":"dfs.info()","a0ec12ef":"import json\n#bert_tokenizer\nif not os.path.exists('..\/input\/trained-weights\/vocab.txt'):\n        \n        tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n        tz.save_pretrained('.\/')\n        \n       \nelse:\n        print('loading the saved pretrained tokenizer')\n        tz = BertTokenizerFast.from_pretrained('..\/input\/trained-weights')\n#model configuration for bert-base-cased\n        \nif not os.path.exists('..\/input\/trained-weights\/config.json'):\n        model_config = BertConfig.from_pretrained('bert-base-cased')\n        model_config.output_hidden_states = True\n        model_config.save_pretrained('.\/')\n       \nelse:\n        print('loading the saved model_config')\n        model_config = BertConfig.from_pretrained('..\/input\/trained-weights')\n        model_config.output_hidden_states = True\n        \n\n\ndef tok(text):\n    \n        encoded = tz.encode_plus(\n        text=text,  # the sentence to be encoded\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length = 256,  # maximum length of a sentence\n        pad_to_max_length=True,  # Add [PAD]s\n        return_attention_mask = True,  # Generate the attention mask\n        truncation=True \n)\n        return encoded\n","b43893df":"X_train, X_test, y_train, y_test = train_test_split(df_train,df_train_target, test_size=0.2, random_state=42)","ded78cfe":"\ninput_ids = []\ntoken_type_ids = []\nattention_mask = []\n    \nfor x in X_train['excerpt']:\n       \n        input_ids.append(tok(x)['input_ids'])\n        token_type_ids.append(tok(x)['token_type_ids'])\n        attention_mask.append(tok(x)['attention_mask'])\n        \nx_tr=np.array(input_ids) \nx_tr_id=np.array(token_type_ids)\nx_tr_am=np.array(attention_mask)\n#making a tuple \nX_train=(x_tr,x_tr_id,x_tr_am)","4e76c0dc":"input_ids = []\ntoken_type_ids = []\nattention_mask = []\n    \nfor x in X_test['excerpt']:\n       \n        input_ids.append(tok(x)['input_ids'])\n        token_type_ids.append(tok(x)['token_type_ids'])\n        attention_mask.append(tok(x)['attention_mask'])\n        \nx_ts=np.array(input_ids) \nx_ts_id=np.array(token_type_ids)\nx_ts_am=np.array(attention_mask)\nX_test=(x_ts,x_ts_id,x_ts_am)","bb448f06":"input_ids = []\ntoken_type_ids = []\nattention_mask = []\n    \nfor x in df_test['excerpt']:\n       \n        input_ids.append(tok(x)['input_ids'])\n        token_type_ids.append(tok(x)['token_type_ids'])\n        attention_mask.append(tok(x)['attention_mask'])\n        \ndf_ts=np.array(input_ids) \ndf_ts_id=np.array(token_type_ids)\ndf_ts_am=np.array(attention_mask)\ndf_val=(df_ts,df_ts_id,df_ts_am)","c50eb211":"#loading and saving tfbert\ndef load_bert(config):\n    if not os.path.exists('..\/input\/trained-weights\/tf_model.h5'):\n        Path('..\/output\/tfbert-large-uncased').mkdir(parents=True, exist_ok=True)\n        bert_model = TFBertModel.from_pretrained(\"bert-base-cased\", config=config)\n        bert_model.save_pretrained('.\/')\n    else:\n        print('loading the saved pretrained model')\n        bert_model = TFBertModel.from_pretrained('..\/input\/trained-weights', config=config)\n    return bert_model","f3e40506":"#model building\nfrom keras.utils.vis_utils import plot_model\ndef build_model(bert_model, max_len=256):    \n    input_ids = Input(shape=(256,), dtype=tf.int32, name=\"input_ids\")\n    token_type_ids = Input(shape=(256,), dtype=tf.int32, name=\"token_type_ids\")\n    attention_mask = Input(shape=(256,), dtype=tf.int32, name=\"attention_mask\")\n\n    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    clf_output = Dropout(.1)(clf_output)\n    out = Dense(1, activation='linear')(clf_output)\n    \n    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError(),'accuracy'])\n    \n    return model\n","576a4419":"bert_model=load_bert(model_config)\nclf = build_model(bert_model, max_len=256)\nclf.summary()\n\nif not os.path.exists('..\/input\/trained-weights\/clf.h5'):\n        clf.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=15,batch_size=4,verbose=1)\n        clf.save_weights('.\/clf.h5')\nelse:\n        print('loading the saved pretrained model')\n        clf.load_weights('..\/input\/trained-weights\/clf.h5')\n\npredict=clf.predict(df_val).flatten()\n","ec0dcf22":"submission = pd.DataFrame()\nsubmission['id'] = df_test['id']\nsubmission['target'] = predict\nsubmission.to_csv(\"submission.csv\",index=False )","bd9737a0":"print(pd.read_csv('.\/submission.csv'))","4d9a203a":"**Deleting columns having null values**","3b969fb6":"**finding whether the target and error has any correlation**","b47b0bb7":"**Remove stopwords and punctuations**","d37e9ed7":"**Columns and null values**","e93ff47b":"**Understanding the data**","2b70a7cb":"**Finding duplicate rows in the data**"}}