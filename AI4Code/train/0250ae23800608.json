{"cell_type":{"575c1e7f":"code","bfd60967":"code","1317ea1e":"code","e00c353c":"code","42261bf0":"code","ec28593e":"code","6c8c2b65":"code","b6d4736a":"code","962d7e19":"code","77aa5acc":"code","f447491e":"code","7d89bd92":"code","eb4e716e":"code","f520f923":"code","f05150e3":"code","69ad9661":"code","e42b0c70":"code","96d1626c":"code","18b8f11d":"code","958299de":"markdown","a4e8e642":"markdown","a53754fb":"markdown","abf51a19":"markdown","efab6231":"markdown","97306fa8":"markdown","ef1e3ae8":"markdown","de57d640":"markdown","3db4cf7f":"markdown","d5d60c64":"markdown","def2aaa0":"markdown"},"source":{"575c1e7f":"!wget https:\/\/github.com\/eyaler\/word2vec-slim\/raw\/master\/GoogleNews-vectors-negative300-SLIM.bin.gz\n!gunzip GoogleNews-vectors-negative300-SLIM.bin.gz","bfd60967":"import numpy as np\nimport pandas as pd\n\n# This is for making some large tweets to be displayed\npd.options.display.max_colwidth = 100\n\n# I got some encoding issue, I didn't knew which one to use !\n# This post suggested an encoding that worked!\n# https:\/\/stackoverflow.com\/questions\/19699367\/unicodedecodeerror-utf-8-codec-cant-decode-byte\ntrain_data = pd.read_csv(\"..\/input\/train.csv\", encoding='ISO-8859-1')","1317ea1e":"train_data","e00c353c":"from gensim.models import KeyedVectors\n\nembed = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)","42261bf0":"# Get the vocabulary used by the embedding\n\n# dictionary for efficient search\nembed_vocab = {}\nfor word in embed.vocab:\n    embed_vocab[word] = True","ec28593e":"tweets = train_data['SentimentText']\ntweets_words = []\ntweets_vocab = {}\nfor tweet in tweets:\n    # Don't include words that can't actually be mapped to a vector using embeddings\n    words = tweet.split()\n    filtered_words = []\n    for word in words:\n        tweets_vocab[word] = True\n        if embed_vocab.get(word) is not None:\n            filtered_words.append(word)\n    tweets_words.append(filtered_words)","6c8c2b65":"# We check how many words from the dataset's vocabulary\n# are found in the embedding vocab\n\ndef vocab_coverage(compared, base):\n    hit, miss = 0, 0\n    for word in compared:\n        if base.get(word) is not None:\n            hit += 1\n        else:\n            miss += 1\n\n    print(\"{} words were found\".format(hit))\n    print(\"{} words weren't found\".format(miss))\n    \nvocab_coverage(tweets_vocab, embed_vocab)","b6d4736a":"# We check the distribution of length of tweets after word filtering\nfrom collections import Counter\n\nlen_counts = Counter([len(tweet) for tweet in tweets_words])\n\nprint(len_counts)\npd.DataFrame([len(tweet) for tweet in tweets_words]).hist(bins=list(range(30)))","962d7e19":"tweets_words_idx = []\nfor tweet in tweets_words:\n    indexs = []\n    for word in tweet:\n        idx = embed.vocab[word].index\n        indexs.append(idx)\n    tweets_words_idx.append(indexs)\n\nprint(\"An example tweet: {}\".format(tweets_words_idx[0]))","77aa5acc":"all_labels = np.array(train_data['Sentiment'])\ndef prepare_data(all_tweets, all_labels, max_words):\n    labels = []\n    tweets = []\n    for i in range(len(all_tweets)):\n        tweet = all_tweets[i]\n        if not tweet:\n            continue\n        diff = max_words - len(tweet)\n        if diff > 0: # need to pad\n            tweet = [0 for j in range(diff)] + tweet\n        elif diff < 0:\n            tweet = tweet[: max_words]\n        tweets.append(tweet)\n        labels.append(all_labels[i])\n    return (np.array(tweets), np.array(labels))\n\ntweets, labels = prepare_data(tweets_words_idx, all_labels, 20)\nassert len(tweets) == len(labels)\n        ","f447491e":"# Split train\/val\/test\nimport random\n\n# to reproduce\nrandom.seed(73)\n\nds_size = len(tweets)\ntrain_ds_ratio = 0.7\nidxs = list(range(ds_size))\nrandom.shuffle(idxs)\n\nend_train = int(ds_size * 0.7)\ntrain_idxs = idxs[:end_train]\nremaining_idxs = idxs[end_train:]\ntest_end = int(len(remaining_idxs) * 0.5)\ntest_idxs = remaining_idxs[:test_end]\nval_idxs = remaining_idxs[test_end:]\n\ntrain_data, train_label = tweets[train_idxs], labels[train_idxs]\ntest_data, test_label = tweets[test_idxs], labels[test_idxs]\nval_data, val_label = tweets[val_idxs], labels[val_idxs]\n","7d89bd92":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nbatch_size = 64\n\ndatasets = {\n    'train': TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_label)),\n    'val': TensorDataset(torch.from_numpy(val_data), torch.from_numpy(val_label)),\n    'test': TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_label)),\n}\n\ndataloaders = {\n    'train': DataLoader(datasets['train'], batch_size=batch_size),\n    'val': DataLoader(datasets['val'], batch_size=batch_size),\n    'test': DataLoader(datasets['test'], batch_size=batch_size),\n}","eb4e716e":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","f520f923":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass SentimentCNN(nn.Module):\n    \"\"\"\n    The embedding layer + CNN model that will be used to perform sentiment analysis.\n    \"\"\"\n\n    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentCNN, self).__init__()\n\n        # set class vars\n        self.num_filters = num_filters\n        self.embedding_dim = embedding_dim\n        \n        # 1. embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # set weights to pre-trained\n        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n        # (optional) freeze embedding weights\n        if freeze_embeddings:\n            self.embedding.requires_grad = False\n        \n        # 2. convolutional layers\n        self.convs_1d = nn.ModuleList([\n            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2, 0)) \n            for k in kernel_sizes])\n        \n        # 3. final, fully-connected layer for classification\n        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n        \n        # 4. dropout and sigmoid layers\n        self.dropout = nn.Dropout(drop_prob)\n        self.sig = nn.Sigmoid()\n        \n    \n    def conv_and_pool(self, x, conv):\n        \"\"\"\n        Convolutional + max pooling layer\n        \"\"\"\n        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n        # conv_seq_length will be ~ 200\n        x = F.relu(conv(x)).squeeze(3)\n        \n        # 1D pool over conv_seq_length\n        # squeeze to get size: (batch_size, num_filters)\n        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x_max\n\n    def forward(self, x):\n        \"\"\"\n        Defines how a batch of inputs, x, passes through the model layers.\n        Returns a single, sigmoid-activated class score as output.\n        \"\"\"\n        # embedded vectors\n        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n        embeds = embeds.unsqueeze(1)\n        \n        # get output of each conv-pool layer\n        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n        \n        # concatenate results and add dropout\n        x = torch.cat(conv_results, 1)\n        x = self.dropout(x)\n        \n        # final logit\n        logit = self.fc(x) \n        \n        # sigmoid-activated --> a class score\n        return self.sig(logit)\n\n","f05150e3":"# Instantiate the model w\/ hyperparams\n\nvocab_size = len(embed_vocab)\noutput_size = 1 # binary class (1 or 0)\nembedding_dim = len(embed['for']) # 300-dim vectors\nnum_filters = 100\nkernel_sizes = [3, 4, 5]\n\nnet = SentimentCNN(embed, vocab_size, output_size, embedding_dim,\n                   num_filters, kernel_sizes)\n\nprint(net)","69ad9661":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n# training loop\ndef train(net, train_loader, valid_loader, epochs, print_every=100):\n\n    # move model to GPU, if available\n    if(train_on_gpu):\n        net.cuda()\n\n    counter = 0 # for printing\n    min_loss = np.Inf\n    # train for some number of epochs\n    net.train()\n    for e in range(epochs):\n\n        # batch loop\n        for inputs, labels in train_loader:\n            counter += 1\n\n            if(train_on_gpu):\n                inputs, labels = inputs.cuda(), labels.cuda()\n\n            # zero accumulated gradients\n            net.zero_grad()\n\n            # get the output from the model\n            output = net(inputs)\n\n            # calculate the loss and perform backprop\n            loss = criterion(output.squeeze(), labels.float())\n            loss.backward()\n            optimizer.step()\n\n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_losses = []\n                net.eval()\n                for inputs, labels in valid_loader:\n\n                    if(train_on_gpu):\n                        inputs, labels = inputs.cuda(), labels.cuda()\n\n                    output = net(inputs)\n                    val_loss = criterion(output.squeeze(), labels.float())\n\n                    val_losses.append(val_loss.item())\n                \n                if np.mean(val_losses) < min_loss:\n                    min_loss = np.mean(val_losses)\n                    torch.save(net.state_dict(), \"model.pth\")\n                    print(\"New val loss... saving model\")\n                \n                net.train()\n                print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","e42b0c70":"# training params\n\nepochs = 5 # this is approx where I noticed the validation loss stop decreasing\nprint_every = 100\n\ntrain(net, dataloaders['train'], dataloaders['val'], epochs, print_every=print_every)","96d1626c":"net.load_state_dict(torch.load(\"model.pth\"))","18b8f11d":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in dataloaders['test']:\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output = net(inputs)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct \/ len(dataloaders['test'].dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","958299de":"## Load the data","a4e8e642":"We checked how well the vocab of our tweets datasets matches with the vocab of the embedding, looks like not a good fit, so we may get big improvement by training an embedding on this tweets.","a53754fb":"## Solution","abf51a19":"# Test","efab6231":"We have seen some applications of CNNs for text classification that are giving good results, so we try here to use CNNs for classifying sentiment of tweets as SAD or HAPPY.","97306fa8":"### Architecture and Training","ef1e3ae8":"# Prepare the data","de57d640":"# Twitter Sentiment Analysis Using CNN","3db4cf7f":"I think we should truncate to 20 words length and pad short tweets (filtered tweets here)","d5d60c64":"### Prepare datsets","def2aaa0":"Download dependencie file before starting"}}