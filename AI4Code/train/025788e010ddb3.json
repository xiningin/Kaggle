{"cell_type":{"3b2e0772":"code","49655e20":"code","f1dc57b4":"code","2f7ddbc7":"code","5e580151":"code","43d06c6b":"code","7452ad86":"code","0f425bc8":"code","7ff5571b":"code","d4282195":"code","b9525e8d":"code","1ebe07c1":"code","d341f5c8":"markdown","91f19616":"markdown","30db7d61":"markdown","89507667":"markdown","da72fc23":"markdown","ac927d1f":"markdown","17940562":"markdown"},"source":{"3b2e0772":"#basic\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) \nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n\n#transformer \nfrom sklearn.pipeline import Pipeline\nfrom category_encoders import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# estimator \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#util\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\n#evaluate\nfrom scipy import stats\nimport os\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer","49655e20":"dataset = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndataset.head()","f1dc57b4":"#for train target\ntarget = dataset['Survived']\n#for train data  \ndataset= dataset.drop(columns=['Survived','PassengerId','Ticket','Name','Cabin'])","2f7ddbc7":"def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n    # Splitting into Train and Test Set\n    x_train, x_valid, y_train, y_valid = train_test_split(df, target, test_size=0.20, random_state=42)\n\n# 1st -> Numeric Transformers\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n\n# 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n\n# 3rd -> Combining both numerical and categorical pipelines\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n# trying to find the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's respective parameters.\n    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1),('feature_eng', PCA()),('clf', SVC())])\n\n    params_grid = [\n        {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [KNeighborsClassifier()],\n                     'clf__n_neighbors': stats.randint(1, 50),\n                     'clf__metric': ['minkowski', 'euclidean']},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LogisticRegression()],\n                     'clf__penalty': ['l1', 'l2'],\n                     'clf__C': stats.uniform(0.01, 10)},\n\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [SVC()],\n                     'clf__C': stats.uniform(0.01, 1),\n                     'clf__gamma': stats.uniform(0.01, 1),\n                     'clf__kernel':['linear','rbf']},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [RandomForestClassifier()],\n                     'clf__n_estimators': stats.randint(10, 175),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)],\n                     'clf__random_state': stats.randint(1, 49)},\n\n    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [GradientBoostingClassifier()],\n                     'clf__n_estimators': stats.randint(10, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]}\n\n    ]\n# Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=500, \n                                             scoring=metrics, refit='accuracy', \n                                             n_jobs=-1, cv=5, random_state=21)\n\n    best_model_pipeline.fit(x_train, y_train)\n\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_valid, y_train, y_valid, best_model_pipeline","5e580151":"categorical_columns=['Pclass','Sex','Embarked'];\nnumeric_columns=['Age', 'Fare', 'SibSp', 'Parch'];\n# Calling the function above, returing train\/test data and best model's pipeline\nx_train, x_valid, y_train, y_valid, best_model_pipeline = defineBestModelPipeline(dataset, target, categorical_columns, numeric_columns)","43d06c6b":"# Function responsible for checking our model's performance on the test data\ndef testSetResultsClassifier(classifier, x_test, y_test):\n    predictions = classifier.predict(x_test)\n    \n    results = []\n    f1 = f1_score(y_test, predictions)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    \n    results.append(f1)\n    results.append(precision)\n    results.append(recall)\n    results.append(accuracy)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n    print(\"F1 score, Precision, Recall, Accuracy:\")\n    print(results)\n    \n    return results","7452ad86":"testSetResultsClassifier(best_model_pipeline, x_valid, y_valid)","0f425bc8":"df_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","7ff5571b":"test_set = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_set.head()","d4282195":"test_predictions = best_model_pipeline.predict(test_set)\nprint(test_predictions)","b9525e8d":"column_names = [\"PassengerId\", \"Survived\"]\nsubmission = pd.DataFrame(columns = column_names)\nsubmission[\"PassengerId\"] = test_set[\"PassengerId\"]\nsubmission[\"Survived\"] = test_predictions\n\nsubmission.to_csv(\"submission\",index = False)","1ebe07c1":"submission.head(10)","d341f5c8":"# Submission","91f19616":"# get best model pipeline","30db7d61":"# predict test data","89507667":"# load train data","da72fc23":"# refine train data","ac927d1f":"# evaluate validation set","17940562":"# get best model pipeline"}}