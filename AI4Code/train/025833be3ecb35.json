{"cell_type":{"3037f6cc":"code","8d1f7dd2":"code","1693ecdd":"code","830c947f":"code","ac94776a":"code","59b6dd2f":"code","614f5a20":"code","140bc947":"code","43b57f2b":"code","d7d9013e":"code","f9e277ec":"code","8cc66aa3":"code","d1e1c17a":"code","acb4379a":"code","18de4460":"code","77b6c71a":"code","b24cae9f":"code","cf95a320":"code","7600b7f5":"code","08a1c62c":"code","8a7e4808":"code","f6468181":"code","a9c361c5":"code","d927c816":"code","b07d53c9":"code","71a0f23a":"code","bf146f51":"code","0298b9d3":"code","84fdc585":"code","47c9f4e5":"code","dc2f6cd8":"code","450ffab6":"code","6b618552":"code","9cf03e13":"code","78821918":"code","758d75ce":"code","73e53422":"code","f577d7f0":"code","75c4bbc2":"code","c4fd00ff":"code","3d5f1e11":"code","2e0d4030":"code","7897674c":"code","0333393a":"code","e9d19d97":"code","396e5408":"code","58d25917":"code","d3debafe":"code","c32fc81a":"code","4d8cb3ed":"code","7d93e9c0":"code","84af733d":"code","7870ae75":"code","a1ab8cde":"code","4b2d6793":"code","fcb819b6":"code","fde91f24":"code","c284b46e":"code","79c0de6d":"code","3e63e740":"markdown","f43dca14":"markdown","e28f8937":"markdown","6c8a5cfb":"markdown","46507e9c":"markdown","d0af8df5":"markdown","03c230a2":"markdown","d9c9026e":"markdown","da1d5717":"markdown","78f7311c":"markdown","ad335840":"markdown","27cea6ac":"markdown","d39b6398":"markdown","847f4ddf":"markdown","fcbb7802":"markdown","ac2dba29":"markdown","a0ff73cc":"markdown","76127ffd":"markdown","591f443f":"markdown","b0520446":"markdown","ed627b8b":"markdown","8a0b5327":"markdown","53ba387d":"markdown","e7de6ada":"markdown","2f3ff27c":"markdown","544ce58a":"markdown","74162929":"markdown","02ce1f5c":"markdown","d77a8c77":"markdown","0b32b6d5":"markdown","c09a422c":"markdown","53416bc5":"markdown","27019591":"markdown","e9bd3b14":"markdown","f25e6b10":"markdown","873b19bb":"markdown","820ef296":"markdown","c35a5ed6":"markdown"},"source":{"3037f6cc":"import numpy as np\nimport pandas as pd\n\n#Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nimport plotly.offline\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#Natural Language Processing\n#Data Manipulation and Cleaning\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import Counter\nstop = set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\n#Modeling\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8d1f7dd2":"Train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n","1693ecdd":"train = Train.copy()","830c947f":"train.head()","ac94776a":"print('There are {} rows and {} columns in train'.format(train.shape[0], train.shape[1]))","59b6dd2f":"#Category counts for type of tweets\nCategory_count=np.array(train['target'].value_counts())\nTweet_type=sorted(train['target'].unique())","614f5a20":"fig = go.Figure(data=[go.Pie(labels=Tweet_type, values=Category_count, hole=.3)])\nfig.show()","140bc947":"train['target'].value_counts()","43b57f2b":"#Adding lenght column to dataset\ntrain['length']=train['text'].apply(len)\ntrain.head()","d7d9013e":"#checking length distribution\nimport plotly.express as px\nfig = px.histogram(train, x=\"length\", color=\"target\")\nfig.show()","f9e277ec":"train['word_count']=train['text'].str.split().map(lambda x: len(x))","8cc66aa3":"import plotly.express as px\nfig = px.histogram(train, x=\"word_count\", color=\"target\")\nfig.show()","d1e1c17a":"train.head()","acb4379a":"train.describe()","18de4460":"#Tweet with max length\ntrain[train['length']==157]['text'].iloc[0]","77b6c71a":"train[train['length']==7]['text'].iloc[0]","b24cae9f":"#Tweet with max word count\ntrain[train['word_count']==31]['text'].iloc[0]","cf95a320":"train[train['word_count']==1]['text'].iloc[0]","7600b7f5":"avg_word_length=train['text'].str.split().apply(lambda x : [len(i) for i in x])\ntrain['avg_word_length']=avg_word_length.map(lambda x: np.mean(x))","08a1c62c":"train.head()","8a7e4808":"import plotly.express as px\nfig = px.histogram(train, x=\"avg_word_length\", color=\"target\")\nfig.show()","f6468181":"#Creating Tweet Corpus function\ndef create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n            \n    return corpus        ","a9c361c5":"corpus = create_corpus(1)\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]        ","d927c816":"x,y = zip(*top)\nplt.bar(x,y, color = 'pink')","b07d53c9":"corpus = create_corpus(0)\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]        ","71a0f23a":"x,y = zip(*top)\nplt.bar(x,y, color = 'pink')","bf146f51":"plt.figure(figsize = (10,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color='purple')\n        ","0298b9d3":"plt.figure(figsize = (10,5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color = 'purple')\n        ","84fdc585":"corpus = create_corpus(1)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word, count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","47c9f4e5":"sns.barplot(x=y,y=x)","dc2f6cd8":"corpus = create_corpus(0)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word, count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","450ffab6":"sns.barplot(x=y,y=x)","6b618552":"def get_top_tweet_bigrams(corpus, n = None):\n    vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0)\n    words_freq = [(word, sum_words[0, idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n    ","9cf03e13":"plt.figure(figsize = (10,5))\ntop_tweet_bigrams = get_top_tweet_bigrams(train['text'])[:10]\nx,y = map(list, zip(*top_tweet_bigrams))\nsns.barplot(x=y, y=x)","78821918":"df = pd.concat([Train,test])\ndf.shape","758d75ce":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","73e53422":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\nremove_URL(example)","f577d7f0":"df['text'] = df['text'].apply(lambda x : remove_URL(x))","75c4bbc2":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","c4fd00ff":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","3d5f1e11":"df['text']=df['text'].apply(lambda x : remove_html(x))","2e0d4030":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","7897674c":"df['text'] = df['text'].apply(lambda x : remove_emoji(x))","0333393a":"def remove_punct(text):\n    table = str.maketrans('','', string.punctuation)\n    return text.translate(table)\n\nexample = \"I am a #king\"\nprint(remove_punct(example))","e9d19d97":"df['text']=df['text'].apply(lambda x : remove_punct(x))","396e5408":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus     ","58d25917":"corpus = create_corpus(df)","d3debafe":"embedding_dict = {}\nwith open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt', 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\nf.close()        ","c32fc81a":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen = MAX_LEN, truncating = 'post', padding = 'post')","4d8cb3ed":"word_index = tokenizer_obj.word_index\nprint('Number of unique words:', len(word_index))","7d93e9c0":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, 100))\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)   \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","84af733d":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","7870ae75":"model.summary()","a1ab8cde":"train_ = tweet_pad[:train.shape[0]]\ntest = tweet_pad[train.shape[0]:]","4b2d6793":"X_train, X_test, y_train, y_test = train_test_split(train_,train['target'].values, test_size = 0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","fcb819b6":"history = model.fit(X_train,y_train, batch_size = 4, epochs =15, validation_data = (X_test, y_test), verbose = 2)","fde91f24":"sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","c284b46e":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","79c0de6d":"sub.head()","3e63e740":"# **Number of words in tweets**","f43dca14":"Please upvote my work if it could help! Thank you!","e28f8937":"As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","6c8a5cfb":"# **Removing URLs**","46507e9c":"we will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets","d0af8df5":"# Removing punctuations","03c230a2":"First we will analyze stopwords in real tweets","d9c9026e":"**Target** :\n\nTo Predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","da1d5717":"# **Number of characters in tweets**","78f7311c":"# **Importing Libraries**","ad335840":"The Donut chart shows that 57 % of the Disaster Tweets are Fake. Let's check the actual counts.","27cea6ac":"Analyzing stopwords in Fake Tweets.","d39b6398":"# **Common stopwords in tweets**","847f4ddf":"# Loading Data","fcbb7802":"# Twitter Disaster Tweets check\n\nTwitter has become an important communication channel in times of emergency.The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).  This notebook is a basic demonstration of the process used in classification of fake versus real disaster tweets.","ac2dba29":"Analysis on character level, word level and sentence level.","a0ff73cc":"# Exploratory Data Analysis","76127ffd":"# **Analyzing punctuations.**","591f443f":"In the data the target column shows 1 for real disaster tweet and 0 for fake disaster tweet.","b0520446":"Now we'll look at punctuations of fake tweets ","ed627b8b":"First let's check tweets indicating real disaster tweets.","8a0b5327":"# Making our submission","53ba387d":"# Removing Emojis","e7de6ada":"# Data Cleaning","2f3ff27c":"# Glove for Vectorization","544ce58a":"Creating Train copy to perform Exploratory Data Analysis","74162929":"# **Average word length in a tweet**","02ce1f5c":"# Exploratory Data Analysis and Visualisation ","d77a8c77":"**Fake Disaster Tweets**","0b32b6d5":"# Removing HTML tags","c09a422c":"so, we have got 78% accuracy using LSTM Baseline Model","53416bc5":"# **Analyzing Common words**","27019591":"So the actual counts show that more than 4000+ Disasters tweets are fake and 3000+ disasters tweets are real. ","e9bd3b14":"# Baseline Model using LSTM","f25e6b10":"**Real Disaster Tweets**","873b19bb":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","820ef296":"**About Data:**\n\nFiles\n\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n* Columns\n\nColumns\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","c35a5ed6":"# **Ngram Analysis**"}}