{"cell_type":{"5e33029b":"code","fc9027c4":"code","1e504df5":"code","545bb1f2":"code","0a40c5de":"code","11874898":"code","1f7bde48":"code","25a8f1fe":"code","ff16e9c0":"code","d4cc6ae8":"code","d709ac8d":"code","39705bbb":"code","38f6dd61":"code","4c1d6e5b":"code","5f4afd7d":"code","12a7dcf2":"code","5ad39743":"code","d08bdd3e":"code","691e29ad":"code","73baa693":"code","879e3c74":"code","c4dd01bc":"code","b2693e6f":"markdown","391215cd":"markdown","9164caee":"markdown","b5348af7":"markdown","57333d86":"markdown","6f5dbd13":"markdown","1f611a61":"markdown"},"source":{"5e33029b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fc9027c4":"train = pd.read_csv(r'\/kaggle\/input\/random-linear-regression\/train.csv', encoding='iso-8859-1')\ntest = pd.read_csv(r'\/kaggle\/input\/random-linear-regression\/test.csv', encoding='iso-8859-1')","1e504df5":"train.head(2)\n","545bb1f2":"test.head(2)\n","0a40c5de":"from bokeh.plotting import *\nfrom bokeh.models import *\nfrom bokeh.layouts import *\nfrom bokeh.io import *\nfrom bokeh.embed import *\nfrom bokeh.resources import *\nimport pandas as pd\n\n\ndef bokeh_regg_plot(x,y):\n    output_notebook()\n    regg_plot = figure(plot_width=800,plot_height=400,title=\"Data Visualization Plot\")\n    regg_plot.circle(x, y)   \n    html_name = 'regression.html'\n    output_file(html_name,mode='inline')\n    show(regg_plot)\n    save(regg_plot)","11874898":"bokeh_regg_plot(train['x'],train['y'])\n","1f7bde48":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n# Create linear regression object\nregg = linear_model.LinearRegression()\n\nm = train['x'].shape[0]\n#nan_To_num will handle nan and large numbers\n#hsstack will add another column to train['x'] , np.newaxis increases the dimention of Y_train from (700,) to (700,1)\nX_train = np.nan_to_num(np.hstack((np.ones((m,1)), train['x'][:,np.newaxis])))\nY_train = np.nan_to_num(train['y'].to_numpy()[:,np.newaxis])\n\n#Fit the model to X and Y training set\nregg.fit(X_train,Y_train)\n\n","25a8f1fe":"# Make predictions using the testing set\nn = test['x'].shape[0]\nX_test = np.nan_to_num(np.hstack((np.ones((n,1)), test['x'][:,np.newaxis])))\nY_test = np.nan_to_num(test['y'].to_numpy()[:,np.newaxis])\n\n\nY_predict = regg.predict(X_test)","ff16e9c0":"# The coefficients\nprint('Coefficients: \\n', regg.coef_)\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(Y_test, Y_predict))\n\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(Y_test, Y_predict))","d4cc6ae8":"from bokeh.plotting import *\nfrom bokeh.models import *\nfrom bokeh.layouts import *\nfrom bokeh.io import *\nfrom bokeh.embed import *\nfrom bokeh.resources import *\nimport pandas as pd\n\n\ndef bokeh_regg_model_plot(x,y, x_test,y_pred):\n    output_notebook()\n#     df = pd.DataFrame(list(zip(x,y,x_test,y_pred)),columns =['X','Y','X_Test','Y_Pred']) \n#     source = ColumnDataSource(data=df)\n    regg_plot = figure(plot_width=800,plot_height=400,title=\"Regression Model Plot\")\n    regg_plot.circle(x, y)\n    \n    regg_plot.line(x=x_test, y=y_pred, line_width=4, line_color='red')\n\n\n    html_name = 'regression_model.html'\n    output_file(html_name,mode='inline')\n    show(regg_plot)\n    save(regg_plot)","d709ac8d":"bokeh_regg_model_plot(train['x'],train['y'], test['x'],Y_predict[:,0])\n","39705bbb":"bokeh_regg_plot(np.nan_to_num(train['x'].to_numpy()),np.nan_to_num(train['y'].to_numpy()))\n","38f6dd61":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n\nlogmodel = LogisticRegression()\n\n#Fit the data \n#Since Logistic Regression is a Classifier, it accepts only Categorical Data (i.e int and not float)\nlogmodel.fit(X_train,Y_train.astype(int))\n\nY_predict = logmodel.predict(X_test)","4c1d6e5b":"#Since Logistic Regression is a Classifier, it accepts only Categorical Data (i.e int and not float)\n#These are the metrics for the Classifier\nprint(classification_report(Y_test.astype(int),Y_predict.astype(int)))\n","5f4afd7d":"bokeh_regg_model_plot(train['x'],train['y'], test['x'],Y_predict)\n","12a7dcf2":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\n\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(5, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","5ad39743":"#Define a checkpoint callback :\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","d08bdd3e":"#Train the model :\nNN_model.fit(X_train, Y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n","691e29ad":"# Load wights file of the best model :\n# wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n# NN_model.load_weights(wights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n","73baa693":"#Predict Y with NN_model\nY_predict = NN_model.predict(X_test)\n","879e3c74":"bokeh_regg_model_plot(train['x'],train['y'], test['x'],Y_predict[:,0])\n","c4dd01bc":"bokeh_regg_model_plot(np.nan_to_num(train['x'].to_numpy()),np.nan_to_num(train['y'].to_numpy()), np.nan_to_num(test['x'].to_numpy()),Y_predict[:,0])\n","b2693e6f":"**To overcome this underfitting because of extremities , let us see if we can use Logistic regression. \nNOTE: Logistic regression is actually a Classification , it draws a desicion boundary\nBelow is the code for Logistic regression**","391215cd":"*We can see very low variance (underfit model) because of np.nan_to_num() , which assigns 0 to nan and inf to very large values,\nThus the line shifts or deviates largly because of this.\nBetter option would be to remove the nans and large values from data*","9164caee":"**Since there are some values which are very large , the Liniear regression gets heavily deviated. \nBelow is the graph where nan and other large values are filled with 0 and inf**","b5348af7":"**As you can see, the Nueral Model does a much better job at regression**","57333d86":"**Let us now try to use Nueral Network to solve this problem**\n\nResource - https:\/\/towardsdatascience.com\/deep-neural-networks-for-regression-problems-81321897ca33","6f5dbd13":"**Above is the graph of the data with extremities**\n\n\n*Thank you!!! , if you found this useful please UPVOTE*","1f611a61":"**The Logistic Regression has actually done a One vs All Classification for 300 classes!!!.\nSo dont be surprised that the Logistic regression did not work**"}}