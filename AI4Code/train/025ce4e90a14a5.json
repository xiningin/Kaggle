{"cell_type":{"9ae2fed9":"code","56b2b467":"code","851d4b06":"code","8ffcb672":"code","15ca4494":"code","159bbcb2":"code","21342d7d":"code","ffd350f4":"code","5f6db7f0":"code","40bb2790":"code","fd986736":"code","22104445":"markdown","76f5e32a":"markdown","617dad83":"markdown","a878fdd0":"markdown","e05a4563":"markdown","da513c22":"markdown","7b3a4e58":"markdown"},"source":{"9ae2fed9":"import numpy as np\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plot\nimport seaborn as seaborn\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = pd.read_csv('..\/input\/train.tsv', sep='\\t')\n\n# a glimpse at the training data\ntrain_df.head()","56b2b467":"figure = plot.figure(figsize=(10, 5))\nseaborn.countplot(data=train_df, x='Sentiment')\nplot.show()\n","851d4b06":"def get_count():\n    s0 = train_df[train_df.Sentiment == 0].Sentiment.count()\n    s1 = train_df[train_df.Sentiment == 1].Sentiment.count()\n    s2 = train_df[train_df.Sentiment == 2].Sentiment.count()\n    s3 = train_df[train_df.Sentiment == 3].Sentiment.count()\n    s4 = train_df[train_df.Sentiment == 4].Sentiment.count()\n    return s0, s1, s2, s3, s4\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)\n\ndf0 = s2 \/\/ s0 - 1\ndf1 = s2 \/\/ s1 - 1\ndf3 = s2 \/\/ s3 - 1\ndf4 = s2 \/\/ s4 - 1\n \ntrain_df = train_df.append([train_df[train_df.Sentiment == 0]] * df0, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 1]] * df1, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 3]] * df3, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 4]] * df4, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 0][0 : s2 % s0]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 1][0 : s2 % s1]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 3][0 : s2 % s3]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 4][0 : s2 % s4]], ignore_index=True)\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)\n","8ffcb672":"figure = plot.figure(figsize=(5, 2.5))\nseaborn.countplot(data=train_df, x='Sentiment')\nplot.show()","15ca4494":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nsentiments = [0, 1, 2, 3, 4]\ncloud = WordCloud(background_color=\"white\", max_words=20, stopwords=stopwords.words('english'))\n\ndef draw_word_clouds(dataframe):\n    for i in sentiments: \n        category = cloud.generate(dataframe.loc[dataframe['Sentiment'] == i, 'Phrase'].str.cat(sep='\\n'))\n        plot.figure(figsize=(5, 2.5))\n        plot.imshow(category)\n        plot.axis(\"off\")\n        plot.title(i)\n        plot.show()\n\ndraw_word_clouds(train_df)","159bbcb2":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom joblib import Parallel, delayed\nimport string \nimport time \n\nlemmatizer = WordNetLemmatizer() \nstop_words  = stopwords.words('english')\nstop_words.extend(['movie', 'film', 'series', 'story', 'one', 'like'])\n\ndef clean_review(review):\n#     review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    tokens = review.lower().split()\n    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n    return \" \".join(filtered_tokens)\n\nstart = time.time()\nclean_train_data = train_df.copy()\nclean_train_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in train_df['Phrase'])\nend = time.time()\nprint(\"Cleaning Training Data - Processing time = \", end - start)\n\n# remove missing values\nprint(\"Clean entries: \", clean_train_data.shape[0], \" out of \", train_df.shape[0])","21342d7d":"target = clean_train_data.Sentiment\ntrain_X_, validation_X_, train_y, validation_y = train_test_split(clean_train_data['Phrase'], target, test_size=0.25, random_state=21)\n","ffd350f4":"from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n\ntfidf_vec = tfidf(min_df=3,  max_features=None, \n        ngram_range=(1, 2), use_idf=1)\nstart = time.time()\ntrain_X = tfidf_vec.fit_transform(train_X_)\nend = time.time()\nprint(\"TFIDF finished in: \", end - start)","5f6db7f0":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nmodel = MultinomialNB()\nmodel.fit(train_X, train_y)\nvalidation_X = tfidf_vec.transform(validation_X_)\npredicted = model.predict(validation_X)\nexpected = validation_y\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.accuracy_score(expected, predicted))","40bb2790":"test_df = pd.read_csv('..\/input\/test.tsv', sep='\\t')\n\nclean_test_data = test_df.copy()\nclean_test_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in test_df['Phrase'])\nend = time.time()\nprint(\"Cleaning Testing Data - Processing time = \", end - start)\n\n# remove missing values\nprint(\"Clean entries: \", clean_test_data.shape[0], \" out of \", test_df.shape[0])\ntest_X = tfidf_vec.transform(clean_test_data['Phrase'])\n\ntest_predictions = model.predict(test_X)\n\n","fd986736":"output = pd.DataFrame({\n    'PhraseId': test_df['PhraseId'],\n    'Sentiment': test_predictions\n})\n\noutput.to_csv('submission.csv', index=False)","22104445":"# Split: training & validation data","76f5e32a":"Resample","617dad83":"# An insight into the data\n","a878fdd0":"# TFIDF","e05a4563":"# Clean the data\n","da513c22":"# Model","7b3a4e58":"Several word clouds, to emphasize the mose frequent words per category:"}}