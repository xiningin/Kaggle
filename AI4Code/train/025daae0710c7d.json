{"cell_type":{"08b62496":"code","d65a7c88":"code","1fb25e1a":"code","932df9b1":"code","96265162":"code","5344af9f":"code","2e4ae623":"code","bfb6fce2":"code","19a821c2":"code","8907e7e0":"code","c8c7cb8c":"markdown","bc2f74eb":"markdown","60664cad":"markdown","3e5fb214":"markdown"},"source":{"08b62496":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\nplt.style.use(\"seaborn-whitegrid\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d65a7c88":"raw_df = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')","1fb25e1a":"! pip install pandas-profiling\n! pip install hyperopt","932df9b1":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(raw_df, title='Pandas Profiling Report', explorative=True)\n\nprofile.to_widgets()","96265162":"X = raw_df.copy()\ny = X.pop(\"target\")","5344af9f":"df_train = raw_df.copy()\n\ndf_train[\"kfold\"] = -1\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train)):\n    df_train.loc[valid_indicies, \"kfold\"] = fold","2e4ae623":"import category_encoders as ce\n\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df_train.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df_train[df_train.kfold != fold].reset_index(drop=True)\n    xvalid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    bin_enc = ce.binary.BinaryEncoder(cols=object_cols)\n    X_encoded = bin_enc.fit_transform(xtrain[object_cols]).add_suffix('_target')\n    xtrain = pd.concat([xtrain, X_encoded], axis=1).drop(columns = object_cols)\n    \n    X_valid_encoded = bin_enc.transform(xvalid[object_cols]).add_suffix('_target')\n    xvalid = pd.concat([xvalid, X_valid_encoded], axis=1).drop(columns = object_cols)    \n    \n    X_test_encoded = bin_enc.fit_transform(xtest[object_cols]).add_suffix('_target')\n    xtest = pd.concat([xtest, X_test_encoded], axis=1).drop(columns = object_cols)    \n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n","bfb6fce2":"from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\nfrom hyperopt import hp\n\nspace={'max_depth': hp.quniform(\"max_depth\", 1, 10, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 200\n    }\n\ndef hyperparameter_tuning(space):\n    model=XGBRegressor(n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n                         colsample_bytree=space['colsample_bytree'], tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    \n    evaluation = [( xtrain, ytrain), ( xvalid, yvalid)]\n    \n    model.fit(xtrain, ytrain,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n\n    pred = model.predict(xvalid)\n    mse= mean_squared_error(yvalid, pred)\n    print (\"SCORE:\", mse)\n    #change the metric if you like\n    return {'loss':mse, 'status': STATUS_OK, 'model': model}","19a821c2":"trials = Trials()\n\nbest_hyperparams = fmin(fn = hyperparameter_tuning,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","8907e7e0":"print(best_hyperparams)","c8c7cb8c":"### Tuning xboost hyper parameters using hyperopt","bc2f74eb":"## Auto Profiling Using pandas-profiling\nhttps:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/","60664cad":"### creating k flod data set","3e5fb214":"### Baseline model with binary encoder"}}