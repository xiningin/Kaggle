{"cell_type":{"4c43a9e7":"code","38433ba0":"code","56d914aa":"code","04be3a4e":"code","ef8a2ed2":"code","da9fb742":"code","6553f891":"code","bc54b649":"code","4be4be65":"code","70d8d413":"code","2c737018":"code","d849f7e8":"code","15cdd14d":"code","c370a6c7":"code","a09ec42f":"code","b1c5e79f":"code","7521a945":"code","50c246e3":"code","60c99fdc":"code","eff6ac0e":"code","86255a9e":"code","a18c883b":"code","f8a33ecc":"code","650dd98b":"code","62a53733":"code","70bf8f93":"code","749a8a1c":"code","85e69a12":"code","20d823fc":"code","00b08adf":"code","968e3e2e":"code","7752ba77":"code","51174334":"code","43d7898c":"code","76c2d083":"code","7ddb5724":"code","8068dce7":"code","63ba4de6":"code","5bb68ffc":"code","d1832536":"code","e015e828":"code","d34d5869":"code","ba86f81a":"code","5ecf421c":"code","210afd63":"code","1432f250":"code","3bb49642":"code","21a96494":"code","aa9f1c0d":"code","776b094a":"code","32522177":"code","a3b1d895":"code","2bd8a5e2":"code","b1ad2e29":"code","4cbd7a0d":"code","e89c4f8a":"code","f9b95b45":"code","4766ffb6":"code","296e5188":"code","b0fdac42":"code","b7874cb9":"code","882ae53b":"code","63dabef2":"code","71edb1c3":"code","eb34ae5f":"code","6bf4f900":"code","f744f6e1":"code","47ed2f7c":"code","74c88246":"code","460868c5":"code","c42bfe9e":"code","975b37f5":"code","57043eca":"code","7c5b0681":"code","a8a1a5cd":"code","95b37493":"markdown","f75eafb9":"markdown","c2acecd3":"markdown","5694a0d1":"markdown","76d5fd17":"markdown","ab64a8d0":"markdown","95cfc32f":"markdown","5f322b4a":"markdown","91e16adf":"markdown","d58acff6":"markdown","e739ec7d":"markdown","7c41229e":"markdown","ef9f517a":"markdown","9f484174":"markdown","26775bc2":"markdown","bd32d588":"markdown","6a0d2a4d":"markdown","7149ac20":"markdown","911e9ae1":"markdown","f60420f6":"markdown","55c5623b":"markdown","87a7e765":"markdown","7cd0ca04":"markdown","1e8e5fa4":"markdown","81d459c8":"markdown","9a097b40":"markdown","3065c4e3":"markdown","07935de2":"markdown","17bd2c9b":"markdown","956fb494":"markdown","e6bc27f1":"markdown","925aff26":"markdown","27219c5b":"markdown","e9fdd6d7":"markdown","e90a2032":"markdown","413ee7f3":"markdown","44e77aa8":"markdown","f2994736":"markdown","59d54af2":"markdown","13af7e34":"markdown","87d1db3c":"markdown","fccd8748":"markdown","a1175ed8":"markdown","d3c3d7df":"markdown","46453bf0":"markdown","50ba29c4":"markdown","6cf65997":"markdown","98563365":"markdown","d3c69359":"markdown","98718d11":"markdown","dc31444c":"markdown","5f757dba":"markdown"},"source":{"4c43a9e7":"# Numeric \nfrom math import*\n \ndef jaccard_numeric_values(x,y):\n \n    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n    union_cardinality = len(set.union(*[set(x), set(y)]))\n    return intersection_cardinality\/float(union_cardinality)\n \njaccard_numeric_values([0,1,2,5,6],[0,2,3,5,7,9])","38433ba0":"def jaccard_string(str1,str2):\n    \n    x = set(str1.lower().split())\n    y = set(str2.lower().split())\n    # convert String with lowercase and then split it\n    \n    z = x.intersection(y)\n    t = x.union(y)\n    # Alternative  : instead of t = x.union(y) you can write (len(x) + len(y) - len(z))\n    return float(len(z)) \/ float(len(t))\n\n\nSentence_1 = 'I am a data scientist at Google'\nSentence_2 = 'I am a software engineer at Google'\nSentence_3 = 'I am a software engineer at Microsoft'\n\n    \nprint(jaccard_string(Sentence_1,Sentence_2))\nprint(jaccard_string(Sentence_1,Sentence_3))\nprint(jaccard_string(Sentence_2,Sentence_3))","56d914aa":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport os\nimport numpy as np\nimport pandas as pd\n\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","04be3a4e":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')","ef8a2ed2":"# First 5 rows of trains data\ntrain.head()","da9fb742":"# shape of train data\nrows = train.shape[0]\ncolumns = train.shape[1]\nprint('The number of rows in train data are {0} and columns are {1}'.format(rows,columns))","6553f891":"# First 5 rows of test data\ntest.head()","bc54b649":"# shape of train data\nrows = test.shape[0]\ncolumns = test.shape[1]\nprint('The number of rows in train data are {0} and columns are {1}'.format(rows,columns))","4be4be65":"train.isnull().sum()\n# checking null value in train data\ntest.isnull().sum()\n# checking null value in test data","70d8d413":"train.dropna(axis = 0, how ='any',inplace=True) ","2c737018":"print('The number of duplicated train data is:',sum(train.duplicated()))\nprint('The number of duplicated test data is:',sum(test.duplicated()))","d849f7e8":"train.info()","15cdd14d":"print('sample of positive sentiment: ',train[train['sentiment']=='positive']['selected_text'].sample())\nprint('sample of negative sentiment: ',train[train['sentiment']=='negative']['selected_text'].sample())\nprint('sample of neutral sentiment: ',train[train['sentiment']=='neutral']['selected_text'].sample())","c370a6c7":"# Pie charts \nplt.figure(figsize=(17,7)) \nsorted_counts = train['sentiment'].value_counts()\nax=plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n        counterclock = False,pctdistance=0.8 ,wedgeprops = {'width' : 0.4}, autopct='%1.0f%%');\n\nplt.title('Train data sentiment proportion',fontsize=20);","a09ec42f":"# Pie charts \nplt.figure(figsize=(17,7)) \nsorted_counts = test['sentiment'].value_counts()\nax=plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n        counterclock = False,pctdistance=0.8 ,wedgeprops = {'width' : 0.4}, autopct='%1.0f%%');\n\nplt.title('Test data sentiment proportion',fontsize=20);","b1c5e79f":"def jaccard_string(str1,str2):\n    \n    x = set(str1.lower().split())\n    y = set(str2.lower().split())\n    \n    z = x.intersection(y)\n    t = x.union(y)\n    return float(len(z)) \/ float(len(t))\n\n# Just remember the beginning of this kernel. I introduced the jaccard score with python code. Copy it !!","7521a945":"jaccard_score = []\n\nfor rows in train.itertuples():\n    # another option : for index,rows in iterrows(), but in this case itertuples is faster than iterrows\n    # Iterate over DataFrame rows as namedtuples.\n    sentence1 = rows.text\n    # first we saved text column as sentence 1 \n    sentence2 = rows.selected_text\n    # second we saved selected_text column as sentence 2\n\n    \n    jaccard_result = jaccard_string(sentence1,sentence2)\n    # Now we apply jaccard score algoritum in sentnce 1,2\n\n    jaccard_score.append([sentence1,sentence2,jaccard_result])\n    # append sentence","50c246e3":"jaccard = pd.DataFrame(jaccard_score,columns=['text','selected_text','jaccard score'])\n# save in dataframe","60c99fdc":"jaccard.head()","eff6ac0e":"train=train.merge(jaccard,how='outer')\ntrain.head()\n\n# jaccard dataframe is merged with train data","86255a9e":"train['number of word in T'] = train['text'].apply(lambda x : len(str(x).split() ))   # number of word in Text\ntrain['number of word in ST'] = train['selected_text'].apply(lambda x : len(str(x).split() )) # number of word in Selected_text\n\ntrain['difference'] = train['number of word in T'] - train['number of word in ST']","a18c883b":"train.head()","f8a33ecc":"positive = train[train['sentiment']=='positive']\nnegative = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']","650dd98b":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in T',bins=33,data=train,edgecolor='black',color='red')\nplt.title('Number of word in text',fontsize=20)\nplt.xlabel('Number of word in text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","62a53733":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in T',bins=33,data=positive,edgecolor='black',color='red')\nplt.title('Positive sentiment word in text',fontsize=20)\nplt.xlabel('number of word in text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","70bf8f93":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in T',bins=33,data=negative,edgecolor='black',color='red')\nplt.title('Negative sentiment word in text',fontsize=20)\nplt.xlabel('number of word in text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","749a8a1c":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in T',bins=33,data=neutral,edgecolor='black',color='red')\nplt.title('Neutral sentiment word in text',fontsize=20)\nplt.xlabel('number of word in text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","85e69a12":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in ST',bins=33,data=train,edgecolor='black',color='green')\nplt.title('Number of word in selected text',fontsize=20)\nplt.xlabel('Number of word in selected text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","20d823fc":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in ST',bins=33,data=positive,edgecolor='black',color='green')\nplt.title('Positive sentiment word in selected text',fontsize=20)\nplt.xlabel('Number of word in selected text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","00b08adf":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in ST',bins=33,data=negative,edgecolor='black',color='green')\nplt.title('Negative sentiment word in selected text',fontsize=20)\nplt.xlabel('Number of word in selected text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","968e3e2e":"plt.subplots(figsize=(15,8))\nplt.hist(x='number of word in ST',bins=33,data=neutral,edgecolor='black',color='green')\nplt.title('Neutral sentiment word in selected text',fontsize=20)\nplt.xlabel('Number of word in selected text')\nplt.ylabel('Counting')\nx1 = list(range(0,33,1))\nplt.xticks(x1);","7752ba77":"plt.figure(figsize=(15,8))\np1=sns.kdeplot(train['number of word in ST'], shade=True, color=\"r\")\np1.set_title('Distribution of Number Of words',fontsize=20)\np1=sns.kdeplot(train['number of word in T'], shade=True, color=\"b\");","51174334":"plt.figure(figsize=(15,8))\np1=sns.kdeplot(train['difference'], shade=True, color=\"y\")\np1.set_title('Distribution of Difference',fontsize=20)\nx1 = list(range(0,35,3))\nplt.xticks(x1);","43d7898c":"plt.figure(figsize=(20,8))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard score'], shade=True, color=\"r\").set_title('Distribution of Jaccrd score',fontsize=20)\np1=sns.kdeplot(train[train['sentiment']=='negative']['jaccard score'], shade=True, color=\"b\")\nplt.legend(['positive sentiment','negative sentiment'],title='Jaccard score');","76c2d083":"plt.figure(figsize=(15,15))\ng=sns.jointplot(\"number of word in T\", \"number of word in ST\", data=train,kind=\"kde\", space=0, color=\"g\");\ng.fig.suptitle(\"number of word in Text and selected Text\",fontsize=20)\n\n# Format nicely.\ng.fig.tight_layout()\n\n#Reduce plot to make room for suptitle\ng.fig.subplots_adjust(top=0.9)\nplt.show()","7ddb5724":"from nltk.stem import SnowballStemmer\n\ndef stemming(words):\n    s = SnowballStemmer('english')\n    s.stem(words)\n    return words ","8068dce7":"train['text'] = train['text'].apply(lambda x: stemming(x))\ntest['text'] = test['text'].apply(lambda x:stemming(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x: stemming(x))","63ba4de6":"\"\"\"\nExpanding contraction \n\"\"\"\ndic = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                   \"this's\": \"this is\",\n                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" };","5bb68ffc":"def mapping_replacer(x, dic): \n    for word in dic.keys(): \n        if \" \" + word + \" \" in x: \n            x = x.replace(\" \" + word + \" \", \" \" + dic[word] + \" \")\n    return x","d1832536":"train['text'] = train['text'].apply(lambda x:mapping_replacer(x,dic))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:mapping_replacer(x,dic))\ntest['text'] = test['text'].apply(lambda x:mapping_replacer(x,dic))","e015e828":"train","d34d5869":"misspell_data = pd.read_csv(\"\/kaggle\/input\/spelling\/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\nmisspell_data.misspell = misspell_data.misspell.str.strip()\nmisspell_data.misspell = misspell_data.misspell.str.split(\" \")\nmisspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\nmisspell_data.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))","ba86f81a":"def misspelled_correction(val):\n    for x in val.split(): \n        if x in miss_corr.keys(): \n            val = val.replace(x, miss_corr[x]) \n    return val\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x : misspelled_correction(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x : misspelled_correction(x))\ntrain[\"selected_text\"] = train['selected_text'].apply(lambda x : misspelled_correction(x))","5ecf421c":"import re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords","210afd63":"########################### Cleaning Corpus ####################################\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    # make a lowercase \n\n    # re.sub(pattern,repl,string) \n    # find corresponded pattern from string convert to repl\n    \n    \n    text = re.sub('\\[.*?\\]', '', text)\n    #remove text in square brackets\n    \n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    # remove Hyperlink,HTML \n    \n    \n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # remove punctuation\n    \n    text = re.sub('\\n', '', text)\n    # remove line change\n\n    text = re.sub('\\w*\\d\\w*', '', text)\n    # remove words containing numbers.\n\n    return text\n\n\n########################### Tokenization ####################################\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","1432f250":"\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest['text'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntrain['selected_text'] = train['selected_text'].apply(str).apply(lambda x: text_preprocessing(x))\n\n\n# Let's take a look at the updated text\ntrain.head()","3bb49642":"# Step 1 split word by word\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\ndef tokenizer(text):\n    word_tokens = word_tokenize(text)\n    return word_tokens\n\n'''\nAlternative of def tokenizer(text)\n\n# from collections import Counter\n# train['word'] = train['selected_text'].apply(lambda x:str(x).split())\n'''\n\ntrain['word from ST'] = train['selected_text'].apply(str).apply(lambda x: tokenizer(x))\ntrain['word from T'] = train['text'].apply(str).apply(lambda x: tokenizer(x))\n\n","21a96494":"# Step 2 remove stopwords \n\ndef remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['word from ST'] = train['word from ST'].apply(lambda x:remove_stopword(x))\ntrain['word from T'] = train['word from T'].apply(lambda x: remove_stopword(x))","aa9f1c0d":"train.head()","776b094a":"from collections import Counter","32522177":"counting = Counter([item for sublist in train['word from T'] for item in sublist])\ncounting_table = pd.DataFrame(counting.most_common(30))\ncounting_table.drop([0],inplace=True)\ncounting_table.columns = ['word','counting']\n\nplt.figure(figsize=(17,10))\nax= sns.barplot(data=counting_table,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 words from text '.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 words',fontsize=15);","a3b1d895":"counting = Counter([item for sublist in train['word from ST'] for item in sublist])\ncounting_table = pd.DataFrame(counting.most_common(30))\ncounting_table.drop([0],inplace=True)\ncounting_table.columns = ['word','counting']\n\nplt.figure(figsize=(17,10))\nax= sns.barplot(data=counting_table,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 words from selected text'.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 words',fontsize=15);","2bd8a5e2":"positive = train[train['sentiment']=='positive']\nnegative = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']","b1ad2e29":"counting_positive_ST = Counter([item for sublist in positive['word from ST'] for item in sublist])\ncounting_positive_ST = pd.DataFrame(counting_positive_ST.most_common(30))\ncounting_positive_ST.drop([0],inplace=True)\ncounting_positive_ST.columns = ['word','counting']\n\nplt.figure(figsize=(19,10))\nax= sns.barplot(data=counting_positive_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 positive words from selected text'.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 positive words',fontsize=15);","4cbd7a0d":"counting_negative_ST = Counter([item for sublist in negative['word from ST'] for item in sublist])\ncounting_negative_ST = pd.DataFrame(counting_negative_ST.most_common(30))\ncounting_negative_ST.drop([0],inplace=True)\ncounting_negative_ST.columns = ['word','counting']\n\nplt.figure(figsize=(19,10))\nax= sns.barplot(data=counting_negative_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 negative words from selected text'.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 negative words',fontsize=15);","e89c4f8a":"counting_neutral_ST = Counter([item for sublist in neutral['word from ST'] for item in sublist])\ncounting_neutral_ST = pd.DataFrame(counting_neutral_ST.most_common(30))\ncounting_neutral_ST.drop([0],inplace=True)\ncounting_neutral_ST.columns = ['word','counting']\n\nplt.figure(figsize=(19,10))\nax= sns.barplot(data=counting_neutral_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 neutral words from selected text'.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 neutral words',fontsize=15);","f9b95b45":"positive_text = train[train['sentiment']=='positive']['text']\nnegative_text = train[train['sentiment']=='negative']['text']\nneutral_text = train[train['sentiment']=='neutral']['text']","4766ffb6":"from wordcloud import WordCloud\nfrom PIL import Image","296e5188":"from wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=1500,\n                        height=800).generate(\" \".join(positive_text))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=20);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=1500,\n                        height=800).generate(\" \".join(negative_text))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=20);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=1500,\n                        height=800).generate(\" \".join(neutral_text))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=20);","b0fdac42":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(15.0,8.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    wordcloud = WordCloud(background_color=color,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/masks\/masks-wordclouds\/'","b7874cb9":"positive_selected_text = train[train['sentiment']=='positive']['selected_text']\nnegative_selected_text = train[train['sentiment']=='negative']['selected_text']\nneutral_selected_text = train[train['sentiment']=='neutral']['selected_text']","882ae53b":"pos_mask = np.array(Image.open(d+ 'comment.png'))\nplot_wordcloud(positive_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\" Positive selected text\")","63dabef2":"pos_mask = np.array(Image.open(d+ 'loc.png'))\nplot_wordcloud(negative_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\"Negative selected text\")","71edb1c3":"pos_mask = np.array(Image.open(d+ 'star.png'))\nplot_wordcloud(neutral_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\"Neutral selected text\")","eb34ae5f":"from sklearn.feature_extraction.text import CountVectorizer\n\n\n# How to list the most common words from text corpus using countvectorizer \ndef WordRanking(corpus,n_gram,n=None):\n   \n    vec = CountVectorizer(ngram_range=n_gram,stop_words = 'english').fit(corpus)\n    # Here we get a Bag of Word model \n    \n    bag_of_words = vec.transform(corpus)\n    # bag_of_words a matrix where each row represents a specific text in corpus and each column represents a word in vocabulary,\n    # that is, all words found in corpus. Note that bag_of_words[i,j] is the occurrence of word j in the text i.\n    \n    sum_words = bag_of_words.sum(axis=0) \n    # we are adding the elements for each column of bag_of_words matrix.\n    \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    #Finally we sort a list of tuples that contain the word and their occurrence in the corpus.\n    \n    return words_freq[:n]","6bf4f900":"postive_bigrams = WordRanking(positive_selected_text,(2,2),30)\nnegative_bigrams = WordRanking(negative_selected_text,(2,2),30)\nneutral_bigrams = WordRanking(neutral_selected_text,(2,2),30)\n\npostive_bigrams = pd.DataFrame(postive_bigrams,columns=['word','counting'])\nnegative_bigrams = pd.DataFrame(negative_bigrams,columns=['word','counting'])\nneutral_bigrams = pd.DataFrame(neutral_bigrams,columns=['word','counting'])\n","f744f6e1":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=postive_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 positive bigram words from selected text'.title(),fontsize=20)\n\nax.set_ylabel('Word counting',fontsize=15)\nax.set_xlabel('Top 30 positive bigram words',fontsize=15);","47ed2f7c":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=negative_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 negative bigram words from selected text'.title(),fontsize=20)\n\nax.set_xlabel('Word counting',fontsize=15)\nax.set_ylabel('Top 30 negative bigram words',fontsize=15);","74c88246":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=neutral_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 neutral bigram words from selected text'.title(),fontsize=20)\n\nax.set_xlabel('Word counting',fontsize=15)\nax.set_ylabel('Top 30 neutral bigram words',fontsize=15);","460868c5":"positive_trigrams = WordRanking(positive_selected_text,(3,3),30)\nnegative_trigrams = WordRanking(negative_selected_text,(3,3),30)\nneutral_trigrams = WordRanking(neutral_selected_text,(3,3),30)\n\npositive_trigrams = pd.DataFrame(positive_trigrams,columns=['word','counting'])\nnegative_trigrams = pd.DataFrame(negative_trigrams,columns=['word','counting'])\nneutral_trigrams = pd.DataFrame(neutral_trigrams,columns=['word','counting'])","c42bfe9e":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=positive_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 positive trigram words from selected text'.title(),fontsize=20)\n\nax.set_xlabel('Word counting',fontsize=15)\nax.set_ylabel('Top 30 positive trigram words',fontsize=15);","975b37f5":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=negative_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 negative trigram words from selected text'.title(),fontsize=20)\n\nax.set_xlabel('Word counting',fontsize=15)\nax.set_ylabel('Top 30 negative trigram words',fontsize=15);","57043eca":"plt.figure(figsize=(19,10))\nax= sns.barplot(data=neutral_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\nax.set_title('Top 30 neutral trigram words from selected text'.title(),fontsize=20)\n\nax.set_xlabel('Word counting',fontsize=15)\nax.set_ylabel('Top 30 neutral trigram words',fontsize=15);","7c5b0681":"'''\nimport re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\nWORDS = Counter(words(open('big.txt').read()))\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] \/ N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n    \n    \n'''","a8a1a5cd":"def remove_space(text): \n    text = text.strip() \n    text = text.split()\n    return \" \".join(text)","95b37493":"Observation \n\n> **Neutral : 40% > Positive : 31% > Negative : 28%**. Accroding to the train data 40% of selected text are not allocted into either positve or negative.That means there are a lot of cases which are not clear to judge whether Sentiment is expressed or not","f75eafb9":"#### <div align='Center'><font size=\"7\" color=\"#51ABDA\">First NLP Analysis<\/font><\/div>\n\n<div align='left'><font size=\"5\" color=\"#51ABDA\">Introduction<\/font><\/div>\nThis is my first NLP Analysis kernal.It was really hard journey to finish it. But I'm very proud of myself.\n<hr>\n\n\n<div align='left'><font size=\"5\" color=\"#51ABDA\">Goal<\/font><\/div>\nprediction of `selected_text` into submission file by modeling\n\n# Structure  <a class=\"anchor\" id=\"toc\"><\/a>\n\n* <a href=\"#sec1\">1. Preliminary step<\/a>\n * <a href=\"#sec1.1\"> 1.1. Jaccard score <\/a>\n * <a href=\"#sec1.2\"> 1.2. Importing prerequisite libraries<\/a>\n * <a href=\"#sec1.3\"> 1.3. Reading datasets<\/a>\n \n\n* <a href=\"#sec2\">2. Exploratory Data Analysis(EDA)<\/a>\n  * <a href=\"#sec2.1\"> 2.1. Missing values <\/a>\n  * <a href=\"#sec2.2\"> 2.2. Incorrect datatype<\/a>\n  * <a href=\"#sec2.3\"> 2.3. Sampling of Sentiment<\/a>\n  * <a href=\"#sec2.4\"> 2.4. Data Visualization<\/a>\n    * <a href=\"#sec2.4.1\"> sentiment proportion<\/a>   \n    * <a href=\"#sec2.4.2\"> Add new feature<\/a> \n    * <a href=\"#sec2.4.3\"> Text data analysis<\/a>   \n    \n    \n* <a href=\"#sec3\"> 3. Text data preprocessing<\/a>\n  * <a href=\"#sec3.1\"> 3.1. Stemming<\/a>\n  * <a href=\"#sec3.2\"> 3.2. Cotraction mapping<\/a>\n  * <a href=\"#sec3.3\"> 3.3. Spelling corrector<\/a>\n  * <a href=\"#sec3.4\"> 3.4. Cleanig corpus<\/a>\n  * <a href=\"#sec3.5\"> 3.5. Tokenization<\/a>\n  * <a href=\"#sec3.6\"> 3.6. Removing stopwords<\/a>\n\n* <a href=\"#sec4\"> 4. Word Analysis<\/a>\n  * <a href=\"#sec4.1\"> 4.1. Word Frequency<\/a>\n  * <a href=\"#sec4.2\"> 4.2. Word cloud<\/a> \n\n* <a href=\"#sec5\"> 5. N-gram Analysis<\/a>\n * <a href=\"#sec5.1\"> 5.1. bigrams <\/a> \n * <a href=\"#sec5.2\"> 5.2. trigrams <\/a> \n\n\n* <a href=\"#sec6\">6. PLUS<\/a>\n * <a href=\"#sec6.1\"> 6.1. Spelling corector<\/a>\n * <a href=\"#sec6.2\"> 6.2. Removing Weird Space<\/a>\n* <a href=\"#sec7\">7. Closing<\/a>","c2acecd3":"Observation\n> `get` >> `go` >> `got`...","5694a0d1":"<a name=\"sec3\"><\/a>\n# 3. Text data preprocessing\n\n<a href=\"#toc\">(Back to the structure)<\/a>","76d5fd17":"<a name=\"sec2.1\"><\/a>\n### 2.1 Missing Value & Duplication","ab64a8d0":"Observation \n>`good` >> `day` >> `love` ...","95cfc32f":"<a name=\"sec5.1\"><\/a>\n## 5.1 Bigrams analysis\n<a href=\"#toc\">(Back to the structure)<\/a>\n","5f322b4a":"<a name=\"sec4.2\"><\/a>\n## 4.2 Word Cloud\n<a href=\"#toc\">(Back to the structure)<\/a>","91e16adf":"<a id='sec1.2'><\/a>\n## 1.2 Importing prerequisite libraries","d58acff6":"Observation\n\n> Distribution without sentiment filtering and with filtering are showing very similar tendecy.","e739ec7d":"Observation\n> `happy` >> `love` >> `day`...","7c41229e":"**Notice**\n\nAs I mentioned before this code is not working because `big.txt` isn't in the repository.","ef9f517a":"<a name=\"sec6\"><\/a>\n# 6 Options \n<a href=\"#toc\">Back to the structure<\/a>\n\n<a name=\"sec6.1\"><\/a>\n## 6.1 Spelling corrector\n\nThere is another concept of spellig corrector.[How to Write a Spelling Corrector from Peter Norvig](http:\/\/norvig.com\/spell-correct.html).If you want to use this corrector, make sure that you have 'big.txt' file on your kernel. The reason why I didn't use this corrector is that my laptop can't reach the ability to compile the code.","9f484174":"Observation\n\n> `happy mothers day` >> `happy star wars` >> `star wars day` ...","26775bc2":"<a name=\"sec2.4\"><\/a>\n### 2.4 Data Visualization \n<a href=\"#toc\">(Back to the structure)<\/a>","bd32d588":"**First : Text**","6a0d2a4d":"<a name=\"sec4\"><\/a>\n# 4. Word data Analysis\n<a href=\"#toc\">(Back to the structure)<\/a>","7149ac20":"<a name=\"sec3.4\"><\/a>\n## 3.4 Cleaning Corpus \n<a href=\"#toc\">(Back to the structure)<\/a>\n\nPreprocessig is the process which cleans the data to get all in a consistent format and  accurate prediction score. So this process is directly connected with the model result. We need to clean,tokenize and convert data into the matric form \n\n* make a lowercase\n* remove text in square brackets\n* reomve hyperlin,HTML\n* remove punctuation\n* remove line change\n* remove words containig numbers\n\nFor that we need special libraries ","911e9ae1":"<a name=\"sec2.2\"><\/a>\n### 2.2 Incorrect datatype","f60420f6":"> Observation \n\nNo Duplication","55c5623b":"Observation\n\n> Remarkably different distribution of `number of word from text` and `number of word from selectec text`","87a7e765":"Observation\n> `miss` >> `sad` >> `sorry` ...","7cd0ca04":"<a name=\"sec2.4.3\"><\/a>\n### Text data analysis","1e8e5fa4":"Observation \n\n> `just got home` >> `happy mothers day` >>  `star wars day`...","81d459c8":"<a name=\"sec5.2\"><\/a>\n## 5.2 Trigrams analysis\n<a href=\"#toc\">(Back to the structure)<\/a>\n","9a097b40":"### Second : number of word and difference","3065c4e3":"<a name=\"sec3.6\"><\/a>\n## 3.6 Removing Stopwords\n<a href=\"#toc\">(Back to the structure)<\/a>\n\n\n- This part can be belonged to the `Part 4.1`.But personally I want to show it separately because for removing stopwords we need to use special libraires.\n\n- Stopword is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\n- The reason for ignoring is that those words are not cruial component of the **sentiment analysis**. In other words stopwords have no meaning ,which is related to sentiment or expression\n\n\n**Notice** \n\nIn order to remve stopswords we have to split word by word. If you compile function without spliting words. your text table will be splited by alphabet.So don't forget to do it !!! ","07935de2":"Observation \n\n> about 20% of word difference is zero.That mean 20% of selected text are 100% same as text.","17bd2c9b":"<a name=\"sec2.4.2\"><\/a>\n### Add new feature \n<a href=\"#toc\">(Back to the structure)<\/a>\n\nAs we know our goal is that we predict selected_text in submission file. So it's more meaningful that we add new feature on the dataframe.\n\n\n### First : Jaccard score","956fb494":"Observation \n\n> `mothers day` >> `happy mothers` >> `good morning` ...","e6bc27f1":"<a id='sec1'><\/a>\n# 1. Preliminary step\n<a href=\"#toc\">(Back to the structure)<\/a>\n\n<a id='sec1.1'><\/a>\n## 1.1 Jaccard score\n\nI got a lof of helps about `Jaccard score` from [Parul's kernal](https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert). Be sure to check out !!","925aff26":"Observation \n\nfinally we have jaccard and difference in number of word between `text` and `selected_text`","27219c5b":"<a name=\"sec4.1\"><\/a>\n## 4.1 Word Frequencey \n<a href=\"#toc\">(Back to the structure)<\/a>","e9fdd6d7":"<a id='sec7'><\/a>\n# 7. Closing \n\n<a href=\"#toc\">(Back to the structure)<\/a>\n\nThanks for your reading my kernal.Because I'm kaggle newcomer i need much more time to practice in order to improve my kernal.\nIn the next time I'll make a second part of `Tweet sentiment` which is modeling and prediction of NLP.\n\n### thank you all very much :) \n\n","e90a2032":"Observation\n\n> Now we are ready for Word data Analysis ","413ee7f3":"<a name=\"sec3.5\"><\/a>\n## 3.5 Tokenization\n<a href=\"#toc\">(Back to the structure)<\/a>\n\nTokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. I'll show 4 famous Tokenization methods\n\n<div align='left'><font size=\"3\" color=\"#51ABDA\">Wordtokenizer<\/font><\/div>\nsplit the token by white space\n\n<div align='left'><font size=\"3\" color=\"#51ABDA\">Treebanktokenizer<\/font><\/div>\n\n- split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n\n- treat most punctuation characters as separate tokens\n\n- split off commas and single quotes, when followed by whitespace\n\n- separate periods that appear at the end of line\n\n<div align='left'><font size=\"3\" color=\"#51ABDA\">WordPunctTokenizer<\/font><\/div>\nsplit the token by punctuation\n\n<div align='left'><font size=\"3\" color=\"#51ABDA\">RegexpTokenizer<\/font><\/div>\nA tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n\n<hr>\nIn this we'll use `RegexpTokenizer`","44e77aa8":"<a name=\"sec2\"><\/a>\n# 2. Exploratory Data Analysis (EDA)\n<a href=\"#toc\">(Back to the structure)<\/a>","f2994736":"<a id='sec1.3'><\/a>\n## 1.3 Reading datasets","59d54af2":"<a name=\"sec2.4.1\"><\/a>\n### Sentiment proportion","13af7e34":"Observation\n\n> `im sorry` >> `dont like` >> `feel like` ...","87d1db3c":"Observation \n\n> We have one null value in train data. We need to drop out it","fccd8748":"Observtion\n\n> right datatype : Object(String)","a1175ed8":"Observation \n\n> Unlike text column, selected text has one exception. Without neutral sentiment graphs represent strong right skewed distribution.But neutral sentiment is showing different shape which is almost same tendency like text column","d3c3d7df":"Observation \n\n> `dont feel good` >> `dont think im` >> `hate hate hate` ...","46453bf0":"<a name=\"sec3.3\"><\/a>\n## 3.3 Spelling corrector\n\n<a href=\"#toc\">(Back to the structure)<\/a>","50ba29c4":"<a name=\"sec5\"><\/a>\n# 5. N-gram analysis\n<a href=\"#toc\">(Back to the structure)<\/a>\n\nI think, a thousand  hearing are not worth one seeing. Chech out what is **Unigram(1-grams)**,**Bigram(2-gram)** and **Trigrams(3-gram)**\n[Source](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/n-gram)\n![n%20gram.JPG](attachment:n%20gram.JPG)\nN-grams are simply all combinations of words or letters of length n that you can find in your source text.\n\nI won't do Unigram because it's actually same as the tokenization of text and visualize it (Chapter 4 : Word data analysis)\n\nFor N-gram analysis we need countvectorizer from sci-kit learn [How to list the most common words from text corpus using countvectorizer ](https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d)\n\n**Notice** \nIn the part 4 We used Counter library to get a common word, but  this function is incapable to get a bigram or trigram. In other word that can get only unigram. That why we use another function ","6cf65997":"<a name=\"sec3.1\"><\/a>\n## 3.1 Stemming\n\nstemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n\nLike here\n\n- sleeping -> sleep\n- sleepy -> sleep","98563365":"Observation \n\n> Same as train data","d3c69359":"<a name=\"sec6.2\"><\/a>\n## 6.2 Removig weird spaces\n<a href=\"#toc\">(Back to the structure)<\/a>","98718d11":"<a name=\"sec3.2\"><\/a>\n## 3.2 Contraction mapping\n\n<a href=\"#toc\">(Back to the structure)<\/a>\n\nContraction are shortened version of words or syllables. By nature, contraction pose a problem for NLP and text analysis because we have a special apostrophe character in the word. Hence, there should be some definitve process by which we can deal with contractions when processing text. ","dc31444c":"Observation \n> `day` >> `good` >> `get` ...","5f757dba":"<a name=\"sec2.3\"><\/a>\n### 2.3 Sampling of sentiment"}}