{"cell_type":{"a13399b9":"code","8e10e4bb":"code","139ccd09":"code","243d082d":"code","1c98e8e3":"code","576c7aab":"code","067a180b":"code","9e8d3363":"code","f59fee81":"code","fec75b74":"code","b072d3b1":"code","95c8c977":"code","84ee74de":"code","b77f2ec0":"code","e58c6e91":"code","5466a962":"code","da1b66e0":"code","c5f9d768":"code","0f2375a2":"code","db171372":"markdown","5317c165":"markdown","9f726446":"markdown","9e22c9c3":"markdown"},"source":{"a13399b9":"import gc\nimport os \nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm # \uc9c4\ud589 \uc0c1\ud0dc \ud45c\uc2dcas\nfrom keras import backend as K\nK.image_data_format() # \ucc44\ub110 first \uc778\uc9c0 last\uc778\uc9c0 \uc5ec\ubd80 \ud310\ub2e8\n# Image visualization\n\nimport PIL\nfrom PIL import ImageDraw","8e10e4bb":"path = '..\/input\/2019-3rd-ml-month-with-kakr'\nos.listdir(path)\n# \uc774\ubbf8\uc9c0 \ud3f4\ub354 \uacbd\ub85c \ntrain_img_path = os.path.join(path,'train')\ntest_img_path = os.path.join(path,'test')\n# csv \ud30c\uc77c \uacbd\ub85c\ndf_train = pd.read_csv(os.path.join(path,'train.csv'))\ndf_test = pd.read_csv(os.path.join(path,'test.csv'))\ndf_class = pd.read_csv(os.path.join(path,'class.csv'))","139ccd09":"#crop\n\ndef crop_boxing_img(img_name, margin=-4, size=(224,224)):\n    if img_name.split('_')[0] == 'train':\n        PATH = train_img_path\n        data = df_train\n    else:\n        PATH = test_img_path\n        data = df_test\n\n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, ['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1, y1, x2, y2)).resize(size)","243d082d":"TRAIN_CROPPED_PATH = '..\/cropped_train'\nTEST_CROPPED_PATH = '..\/cropped_test'","1c98e8e3":"if (os.path.isdir(TRAIN_CROPPED_PATH) == False):\n    os.mkdir(TRAIN_CROPPED_PATH)\n\nif (os.path.isdir(TEST_CROPPED_PATH) == False):\n    os.mkdir(TEST_CROPPED_PATH)\n\nfor i, row in df_train.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TRAIN_CROPPED_PATH, row['img_file']))\n\nfor i, row in df_test.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TEST_CROPPED_PATH, row['img_file']))","576c7aab":"# Set Path of Preprocessed Train Images\nTRAIN_IMG_PREP_PATH = os.path.join('..', 'train_prep')\nif not os.path.exists(TRAIN_IMG_PREP_PATH):\n    os.makedirs(TRAIN_IMG_PREP_PATH, exist_ok=True)\n\n# Set Path of Preprocessed Test Images\nTEST_IMG_PREP_PATH = os.path.join('..', 'test_prep')\nif not os.path.exists(TEST_IMG_PREP_PATH):\n    os.makedirs(TEST_IMG_PREP_PATH, exist_ok=True)","067a180b":"def img_he_pad(img_file_name, add_padding=True):\n    if img_file_name.split('_')[0] == 'train':\n        IMG_CROP_PATH = TRAIN_CROPPED_PATH\n        data = df_train\n    elif img_file_name.split('_')[0] == 'test':\n        IMG_CROP_PATH = TEST_CROPPED_PATH\n        data = df_test\n        \n    # --- Histogram Equalization --- #\n    img = cv2.imread(os.path.join(IMG_CROP_PATH, img_file_name))\n    img_y_cr_cb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n    y, cr, cb = cv2.split(img_y_cr_cb)\n\n    # Equalize y (CLAHE (Contrast Limited Adaptive Histogram Equalization))\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n    y_eq = clahe.apply(y)\n    img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))\n    img_bgr_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)\n    img_prep = img_bgr_eq\n\n    # --- Convert BGR To RGB (Just On cv2) ---\n    # b, g, r = cv2.split(img_bgr_eq)\n    # img_prep = cv2.merge((r,g,b))\n    \n    # -------- Add Padding --------- #\n    if add_padding:\n        img_prep_h, img_prep_w = img_prep.shape[0], img_prep.shape[1]  # (height, width)\n        ratio = float(IMG_SIZE) \/ max(img_prep_h, img_prep_w)\n        shape_no_padding = (int(img_prep_h * ratio), int(img_prep_w * ratio))\n\n        img_prep_no_padding = cv2.resize(img_prep, shape_no_padding[::-1])\n        \n        size_h = IMG_SIZE - shape_no_padding[0]\n        size_w = IMG_SIZE - shape_no_padding[1]\n        \n        top, bottom = size_h \/\/ 2, size_h - (size_h \/\/ 2)\n        left, right = size_w \/\/ 2, size_w - (size_w \/\/ 2)\n\n        PADDING_COLOR = (0, 0, 0)  # black\n        img_prep = cv2.copyMakeBorder(\n            img_prep_no_padding,\n            top,\n            bottom,\n            left,\n            right,\n            cv2.BORDER_CONSTANT,\n            value=PADDING_COLOR\n        )\n    \n    return img_prep","9e8d3363":"import cv2\nIMG_SIZE = 224\n# Save Preprocessed Train Images (Path: ..\/train_prep)\nif not os.listdir(TRAIN_IMG_PREP_PATH):  # If PATH_IMG_TRAIN_PREP is empty\n    for idx, row in df_train.iterrows():\n        img_file_name = row['img_file']\n        img_prep = img_he_pad(img_file_name, add_padding=True)\n        cv2.imwrite(os.path.join(TRAIN_IMG_PREP_PATH, img_file_name), img_prep)\n\n# Save Preprocessed Test Images (Path: ..\/test_prep)\nif not os.listdir(TEST_IMG_PREP_PATH):\n    for idx, row in df_test.iterrows():\n        img_file_name = row['img_file']\n        img_prep = img_he_pad(img_file_name, add_padding=True)\n        cv2.imwrite(os.path.join(TEST_IMG_PREP_PATH, img_file_name), img_prep)","f59fee81":"# Define Function For Test\ndef test_he_padding(img_file_name):\n    # Show Cropped Image\n    img_crop = PIL.Image.open(os.path.join(TRAIN_CROPPED_PATH, img_file_name))\n    plt.figure(figsize=(12, 9))\n    plt.subplot(1, 2, 1)\n    plt.title(f'Cropped Image - {img_file_name}')\n    plt.imshow(img_crop)\n    plt.axis('off')\n\n    # Show Preprocessed Image\n    img_he_pad = PIL.Image.open(os.path.join(TRAIN_IMG_PREP_PATH, img_file_name))\n    plt.subplot(1, 2, 2)\n    plt.title(f'Historgram Equalized Cropped Image(Add Padding) - {img_file_name}')\n    plt.imshow(img_he_pad)\n    plt.axis('off')\n    \n    # Show Result\n    plt.show()\n    \n# Test Histogram Equalization & Add Padding\ntest_he_padding(img_file_name=df_train['img_file'].iloc[114])","fec75b74":"from sklearn.metrics import f1_score\n\ndef f1_metric(y_true, y_pred):\n\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = (true_positives + K.epsilon()) \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = (true_positives + K.epsilon()) \/ (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","b072d3b1":"from keras_applications.resnext import ResNeXt101#, preprocess_input\n# from keras.applications.resnet_v2. import ResNet50, preprocess_input\n# from keras_applications.resnet import ResNet101, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Parameter\nimg_size = (224, 224)\nepochs = 70\nbatch_size =16\n\n# Define Generator config\ntrain_datagen = ImageDataGenerator(\n    rotation_range=30,\n    horizontal_flip = True, \n    vertical_flip = False,\n    #zoom_range=0.30,\n    #width_shift_range=0.2,\n    #height_shift_range=0.2,\n    #shear_range=0.5,\n    brightness_range=[0.5, 1.5],\n    fill_mode='nearest',\n    rescale=1.\/255)\n    #preprocessing_function=preprocess_input)\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)#preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)#preprocessing_function=preprocess_input)","95c8c977":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, GlobalAveragePooling2D\nfrom keras import layers, models, optimizers, utils, backend,regularizers\n# import keras\ndef get_model(model_name='ResNeXt101'):\n    resNet_model = ResNeXt101(include_top= False, input_shape = (224,224,3)\n                            , backend =backend, layers=layers, models = models,\n                             utils = utils\n                            )\n    # resNet_model.summary()\n    \n    model = Sequential()\n    model.add(resNet_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.25))  # \uacfc\uc801\ud569 \uc904\uc5ec\ubcf4\uae30\n    model.add(Dense(196, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l=0.01)))\n#     model.add(Dense(196, activation='softmax', kernel_initializer='he_normal'))\n#     model.add(Dense(196, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l=0.01)))\n#     model.add(LeakyReLU(alpha=0.01))    \n    model.summary()\n    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n    model.compile(optimizer = adam,loss = 'categorical_crossentropy', metrics=['acc',f1_metric])\n    # compile \ud560\ub54c \ub123\uc5b4\uc918\uc57c\uc9c0 \uc544\ub798\uc5d0\uc11c early stopping \ud560\ub54c \uc0ac\uc6a9 \uac00\ub2a5\ud558\ub2e4\n    return model\n\n","84ee74de":"def get_steps(num_samples,batch_size):\n    if (num_samples % batch_size)>0:\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size","b77f2ec0":"from sklearn.model_selection import StratifiedKFold\nk_folds = 5\nkfold =StratifiedKFold(n_splits = k_folds, random_state = 1990)\ndf_train['class'] = df_train['class'].astype('str')\ndf_train= df_train[['img_file','class']]\ndf_test = df_test[['img_file']]","e58c6e91":"%%time\nfile=[]\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfor idx,(train_index, valid_index) in enumerate(kfold.split(\n                                df_train['img_file'],df_train['class'])):\n    if idx != 0 : continue # \uc5ec\ub7ec\ubc88 \ub098\ub220\uc11c \ub3cc\ub9ac\uae30 \uc704\ud568\n   # if idx == 1 : continue\n    #if idx == 2 : continue\n    #if idx == 3 : continue    \n    traindf = df_train.iloc[train_index,:].reset_index()\n#     validdf = df_train.iloc[valid_index,:].reset_index()\n#     traindf.to_csv('%s_traindf'%idx,index=False)\n#     validdf.to_csv('%s_validdf'%idx,index=False)\n    \n    nb_train_samples = len(traindf)\n#     nb_validation_samples = len(validdf)\n#     nb_test_samples = len(df_test)\n    # Make Generator\n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe=traindf, \n        directory=TRAIN_CROPPED_PATH,#'..\/input\/train\/',\n        x_col = 'img_file',\n        y_col = 'class',\n        target_size = img_size,\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=batch_size,\n        seed=42\n    )\n\n\n\n","5466a962":"\ntest_generator = test_datagen.flow_from_dataframe(\n            dataframe=df_test,\n            directory=TEST_IMG_PREP_PATH,#TEST_CROPPED_PATH,#'..\/input\/test',\n            x_col='img_file',\n            y_col=None,\n            target_size= img_size,\n            color_mode='rgb',\n            class_mode=None,\n            batch_size=batch_size,\n            shuffle=False\n            )","da1b66e0":"path = '..\/input\/models10\/'\nlst = os.listdir(path)\nprint(lst)","c5f9d768":"\n%%time\ntta_steps = 5 #models5 \uae30\uc900 1\ud68c 0.950 ,5\ud68c 0.949 , 10\ud68c 0.9507\nprediction = []\nfor i, name in enumerate(lst):\n    preds =[]\n    print(name)\n    model = get_model()\n    model.load_weights(os.path.join(path,name))\n    for j in tqdm(range(tta_steps)):                \n        test_generator.reset()\n        nb_test_samples = len(df_test)\n        pred = model.predict_generator(\n            generator = test_generator,\n            steps = get_steps(nb_test_samples, batch_size),\n            verbose=1\n            )\n        preds.append(pred)\n        \n        gc.collect()\n#         print(np.mean(preds,axis=0))    \n    pd.DataFrame(np.mean(preds,axis=0)).to_csv('%s.csv'%i, index= False)\n#     prediction.append(np.mean(preds,axis=0)) \n    del preds\n    gc.collect()\nfor i, name in enumerate(lst):\n    prediction.append(np.array(pd.read_csv('%s.csv'%i)))\n# print(prediction)\ny_pred = np.mean(prediction,axis=0)\nprint(y_pred)","0f2375a2":"path = '..\/input\/2019-3rd-ml-month-with-kakr'\npreds_class_indices = np.argmax(y_pred,axis =1)\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nfinal_pred = [labels[k] for k in preds_class_indices]\n\nsubmission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nsubmission[\"class\"] = final_pred\nsubmission.to_csv(\"submission_rev03.csv\", index=False)\nsubmission.head()\n              ","db171372":"### **Model Summary**\n- Resnext101 8 Folds\ub85c \ucd5c\uc885 \uc81c\ucd9c\ud588\uace0 \ub300\ud68c \uc885\ub8cc \uc774\ud2c0\uc804\uc5d0 \ud070 \uc624\ub958\uac00 \uc788\uc74c\uc744 \uae68\ub2eb\uace0 \uae09\ud558\uac8c \uc218\uc815\ud558\uace0<br>\n  \ud559\uc2b5\uc2dc\ud0a4\ub2e4\ubcf4\ub2c8 \ub2e4\ub978 \ubaa8\ub378\uc744 \uc81c\ub300\ub85c \uc559\uc0c1\ube14 \uc2dc\ud0ac \uc2dc\uac04\uc740 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4.\n- Public 27\uc704\u2192Private 18\uc704\ub85c \uc62c\ub77c\uc120 \uac83\uc740 TTA+8folds\ub85c Generalization\uc774 \ub410\uae30 \ub54c\ubb38\uc778\ub4ef\ud569\ub2c8\ub2e4.  \n       \u2460 \uc804\ucc98\ub9ac  : Cropping , Histogram Equalization\n       \u2461 Augmentation : cutout , rotation 30, horizonal=True, zoom:0.3, rescale :1\/255\n       \u2462 Loss Function : categorical_crossentropy\n       \u2463 optimizer : Adam optimizer\n       \u2464 Basic Model : Resnext101(imagenet pretrained), 8 folds (\ud3c9\uade0 5.5hr\/1fold \uc18c\uc694)\n       \u2465 Inference \uc2dc TTA(5) \uc801\uc6a9\n  ","5317c165":"# <center>3rd ML Month - 18th solution <\/center>","9f726446":"## <a id='0'><strong>\uc6b4\uc601\uc9c4\uc5d0 \uac10\uc0ac \ub9d0\uc500\ub4dc\ub9bd\ub2c8\ub2e4.<\/strong><\/a>\n\uc88b\uc740 \ub370\uc774\ud130\ub85c \uc7ac\ubbf8\ub09c \ub300\ud68c\ub97c \ub9cc\ub4e4\uace0 \uc774\ub04c\uc5b4\uc8fc\uc2e0 \uc6b4\uc601\uc9c4\uaed8 \uac10\uc0ac \ub9d0\uc500\ub4dc\ub9bd\ub2c8\ub2e4.\n\n\ub355\ubd84\uc5d0 \uae30\uc220\uc801\uc73c\ub85c \ub9ce\uc740 \uac83\uc744 \ubc30\uc6e0\uace0 \ud2b9\ud788 **<u>\"\uc138\uc0c1\uc740 \ub113\uace0 \uace0\uc218\ub294 \ub9ce\ub2e4\"<\/u>**\ub294 \uac83\uc744 \ub2e4\uc2dc \ud55c\ubc88 \ub290\ub07c\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\ud2b9\ud788 \uc5bc\ub9c8\uc804 \uce90\uae00 \uc785\ubb38 \uac15\uc758\ub85c \uc218\uace0\ud574\uc8fc\uc2e0 \ud0dc\uc9c4\ub2d8\uaed8 \ub2e4\uc2dc \ud55c\ubc88 \uac10\uc0ac \ub9d0\uc500\ub4dc\ub9bd\ub2c8\ub2e4.      \n       \n\uc55e\uc73c\ub85c\ub3c4 \uc774\ub7f0 \uc88b\uc740 \ub300\ud68c\uac00 \uc774\uc5b4\uc9c0\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.\n       \n(\ube44\ub85d \uc790\ub791\ud560\ub9cc\ud55c \ub4f1\uc218\ub294 \uc544\ub2c8\uc9c0\ub9cc \uace0\uc218\ub2d8\ub4e4\uc758 \ucee4\ub110\uc744 \ubcf4\uace0 \ubc30\uc6b0\uba70 \uc0bd\uc9c8\ud588\ub358 \ub0b4\uc6a9\uc744     \n\uacf5\uc720\ud558\uba74 \uc88b\uc744 \uac83 \uac19\uc544\uc11c \ub0a8\uae41\ub2c8\ub2e4.)\n","9e22c9c3":"## <strong>\uc0bd\uc9c8\uc758 \uae30\ub85d<\/strong>\n**1.  Resnet50 + cropping + sigle fold** (public score 0.8345) <br>\n**2.  Resnet50 + cropping + 5folds** (public score 0.91439) <br>\n       - \ud559\uc2b5\uc774 \uc81c\ub300\ub85c \ub418\uc9c0 \uc54a\uace0 \uc788\ub294 \uac83\uc73c\ub85c \ud310\ub2e8\ud558\uace0 \ub72f\uc5b4\ubcf4\uae30 \uc2dc\uc791\n           . learning rate : 0.001 \u2192 0.0001\n           . Dropout(0.25) \ucd94\uac00\n**3.  Resnet50 + cropping + 5folds** (public score 0.92061) <br>\n        - \uae4a\uc740 \ubaa8\ub378(resnet50 \u2192 resnext101)\ub85c \ubcc0\uacbd\ud558\uae30\ub85c \uacb0\uc815\n        - resnet101,resnext101 \ubaa8\ub378\uc740 keras.applications\uac00 \uc544\ub2cc keras_applications \ubaa8\ub4c8\uc5d0\uc11c\n          \uac00\uc838\uc640\uc57c\ud568. keras\uac00 \ucc98\uc74c\uc774\ub77c \uc774\uac78 \ubab0\ub77c\uc11c \uc0bd\uc9c8\uc744 \uc624\ub798\ud568.\n        - keras_applications \uc5d0\uc11c resnext101\ub97c \ubd80\ub974\uba74 \uc544\ub798 \uc5d0\ub7ec\uac00 \uc0dd\uae30\ub294\ub370,\n          \ubaa8\ub378\uc744 \ubd80\ub97c \ub54c arguments\ub97c \ub123\uc5b4 \uc918\uc57c\ud568(backend, layers, models, utils)         \n         \"AttributeError: 'NoneType' object has no attribute 'image_data_format'\"\n         \u2192\ud574\uacb0\ucc45 : http:\/\/donghao.org\/2019\/02\/22\/using-resnext-in-keras-2-2-4\/\n        - \ub2e4\uc74c\uc5d0 \ub9c8\uc8fc\ud558\uac8c \ub418\ub294 \uc5d0\ub7ec\uac00 scaling\uacfc \uad00\ub828\ub41c \uc5d0\ub7ec\uc778\ub370 \uc774\ub294 preprocess_input\uc744 \ud65c\uc6a9\ud558\uae30\n          \ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud558\ub294 \uac83\uc73c\ub85c Data Augmentation\uc744 \ud560 \ub54c scale : 1\/255\ub85c \ubcc0\uacbd\ud558\uba74 \ud574\uacb0\ub428.\n**4.  Resnext101 + cropping + 5folds** (public score 0.93591) <br>    \n        - early stopping\uc744 f1 score\uc5d0 \uac78\uc5b4\ub450\uc5b4\uc11c\uc778\uc9c0 val_loss\uac00 \ucda9\ubd84\ud788 \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc558\ub294\ub370\n          \ud559\uc2b5\uc774 \uc885\ub8cc\ub418\ub294 \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud568(underfitting) \n          . patience = 5 \u2192 10 \ubcc0\uacbd\n          . ReduceLROnPlateau(factor 0.5) \uc801\uc6a9\n              \u2192 \ub4a4\uc5d0 \uc124\uba85\ud558\uaca0\uc9c0\ub9cc \ub9c9\ud310\uc5d0 \uc5ec\uae30\uc5d0 \uc624\ub958\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c\uac8c\ub418\uc5b4 \ud559\uc2b5\uc744 \ub2e4\uc2dc\ud568.\n         - \ud559\uc2b5 \uc2dc\uac04\uc774 3\ubc30\ub85c \ub298\uc5b4\ub098\uba74\uc11c Training \/ Inference kernel\uc744 \ubd84\ub9ac\ud558\uae30 \uc2dc\uc791.\n         \n**5.  Resnext101 + cropping + 5folds+ReduceLR** (public score 0.94787)          \n         - train Loss\uac00 0\uc5d0 \uac00\uae5d\uace0 acc\ub3c4 100\uc5d0 \uac00\uae4c\uc6b4 \ubc18\uba74, Val acc\ub294 98% \uc218\uc900\uc73c\ub85c \n           Overfitting \ub418\uace0 \uc788\ub2e4\uace0 \ud310\ub2e8 (\uc2e4\uc81c Private score 0.943\uc73c\ub85c Drop)\n           . Mixup \uc801\uc6a9(0.2)\ud588\uc73c\ub098 \uc624\ud788\ub824 Public score 0.01\uc774\uc0c1 drop\n           . overfitting \ub41c model 5\uac1c(+ Xception, resnet101, resnet50, Efficient_v3)\uc758 \uacb0\uacfc\ubb3c\ub85c\n             Voting\uc744 \uc218\ud589\ud558\uc5ec Public score 0.95074\ub97c \ub9cc\ub4e4\uc5c8\uc73c\ub098 private score\ub294 \uc800\uc870(0.946)\n             \u2192 Voting \ubcf4\ub2e4 softmax \uacb0\uacfc\ubb3c\ub4e4\uc758 \ud3c9\uade0\ub0b4\uc5b4 argmax\ub97c \ucde8\ud558\ub294\uac8c \ub354 \uc131\ub2a5\uc774 \uc88b\uc74c\n         - ** 5\uc77c\uac04 \uc5ec\ub7ec \uc0bd\uc9c8\uc744 \ud558\ub294 \ub3c4\uc911 \ub450\uac00\uc9c0 \ud070 \uc624\ub958\uac00 \uc788\uc74c\uc744 \ud655\uc778 **\n            . ReduceLROnPlateau \uc624\ub958 : f1 score\ub97c \ubc14\ub77c\ubcf4\uac8c \ud574\ub450\uace0 mode\ub97c default(min)\uc73c\ub85c \uc124\uc815\ub418\uc5b4<br>\n            (patience 10) f1 score\uc640 \uad00\uacc4 \uc5c6\uc774 \uacc4\uc18d \uc808\ubc18\uc529 \uc904\uace0 \uc788\uc5c8\uc74c. \n            . Earlystopping \uc624\ub958 : monitoring\uc744 val_f1_score\uac00 \uc544\ub2cc f1_score\ub85c \uc124\uc815\ud574\ub450\uc5b4<br>\n              validation set\uc774 \uc544\ub2cc train set\uc758 f1 score\uac00 \ub354\uc774\uc0c1 \uac1c\uc120\uc774 \uc5c6\uc744\ub54c \ub05d\ub098\ub3c4\ub85d \ub418\uc5b4 \uc788\uc5c8\uc74c<br>\n              \uc0ac\uc2e4\uc0c1 Earlystopping \uae30\ub2a5\uc744 \uc218\ud589\ud558\uc9c0 \ubabb\ud558\uace0 Overfitting\ub418\uace0 \uc788\uc74c\n**6.  (\uc624\ub958\uc218\uc815) Resnext101 + cropping + 8folds+ReduceLR** (public score 0.95217)\n        - \uc704 \uc624\ub958\ub97c \ubaa8\ub450 \uc218\uc815\ud558\uc5ec \ud559\uc2b5\ud558\uae30 \uc2dc\uc791, \ub300\ud68c \uc885\ub8cc\uae4c\uc9c0 \ud558\ub8e8\ubc16\uc5d0 \ub0a8\uc9c0 \uc54a\uc740 \uc0c1\ud669\uc774\ub77c<br>\n            \ud55c\ubc88\uc5d0 \ud559\uc2b5 \uae30\ud68c\uac00 \uc788\uc744 \ub4ef\ud558\uc5ec folds \uc218\ub3c4 \ub298\ub824\uc11c \uc870\uae08\uc774\ub77c\ub3c4 overfitting\uc744 \uc904\uc5ec\ubcf4\uace0\uc790\ud568.\n        - \ud559\uc2b5\uc774 \uc885\ub8cc\ub41c \uc774\ud6c4\uc5d0 voting\uc744 \uc218\ud589\ud558\uc5ec score\ub97c \uc870\uae08\ub354 \uc62c\ub838\uc73c\ub098 private score\ub85c \ubcf4\uba74<br>\n          voting \ud558\uae30 \uc804\uc774 \ub354 \uc798 \ub098\uc634\n"}}