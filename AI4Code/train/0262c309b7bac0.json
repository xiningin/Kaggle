{"cell_type":{"50443467":"code","f8ed8856":"code","f3f2e95d":"code","c1d5df42":"code","a1c0441f":"code","b9d00f5d":"code","191ccd04":"code","13bb8469":"code","7da77798":"markdown","c71505a6":"markdown","7a8e4e13":"markdown","4953017b":"markdown","ac5660b3":"markdown","05584109":"markdown","4b6abb1c":"markdown","6409b652":"markdown","527a4792":"markdown","a128890a":"markdown","72727036":"markdown","578bec4c":"markdown","179e237b":"markdown","e0267314":"markdown"},"source":{"50443467":"import pandas as pd\nimport numpy as np\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom sklearn import linear_model\nfrom sklearn.model_selection import StratifiedKFold\nimport gc","f8ed8856":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n","f3f2e95d":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\nprint(train.target.value_counts()[0]\/300000, train.target.value_counts()[1]\/300000, )\ntrain.sort_index(inplace=True)\ntrain_y = train['target']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","c1d5df42":"train","a1c0441f":"HTML('<iframe width=\"680\" height=\"620\" src=\"https:\/\/www.youtube.com\/embed\/8odLEbSGXoI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')\n","b9d00f5d":"from sklearn.metrics import roc_auc_score\ncat_feat_to_encode = train.columns.tolist()\n# target =0  0.69412%, target =1 0.30588\nsmoothing=0.50\n\noof = pd.DataFrame([])\nfor tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=1, shuffle=True).split(train, train_y):\n    \n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n\n    \n    \n    \nce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\nce_target_encoder.fit(train, train_y)\ntrain = oof.sort_index() \ntest = ce_target_encoder.transform(test)","191ccd04":"glm = linear_model.LogisticRegression(\n  random_state=1, solver='lbfgs', max_iter=2019, fit_intercept=True, \n  penalty='none', verbose=0)\n\nglm.fit(train, train_y)","13bb8469":"from datetime import datetime\npd.DataFrame({'id': test_id, 'target': glm.predict_proba(test)[:,1]}).to_csv(\n    'sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', \n    index=False)","7da77798":"## Categorical Feature Encoding Challenge WITH PYTHON\n[Crisl\u00e2nio Mac\u00eado](https:\/\/medium.com\/sapere-aude-tech) -  December, 31th, 2019\n\n \ud83d\udc31 CatComp - Simple Target Encoding : [ \ud83d\udc31 CatComp - Simple Target Encoding ](https:\/\/www.kaggle.com\/caesarlupum\/catcomp-simple-target-encoding)\n\n----------\n----------","c71505a6":"# General Findings","7a8e4e13":"### Categorical Material\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/110924#latest-638837\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105512#latest-656503\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/111930#latest-666056\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/113213#latest-666299\n\n### Cyclic features\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/106630#latest-648493\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105610#latest-647944\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/108805#latest-629677\n\n\n### Techniques to handle categorical variables\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/108805#latest-629677\n\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/108805#latest-629677\n\n","4953017b":"# Final","ac5660b3":"## Target encoding\n \t\t\nTarget-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.\n\nfor example,\n![](http:\/\/www.renom.jp\/notebooks\/tutorial\/preprocessing\/category_encoding\/renom_cat_target.png)\n\n\n","05584109":"# Encoding Categories","4b6abb1c":"## Logistic Regression","6409b652":"#### Target encoding\u2014as implemented in contrib.scikit-learn.org\/categorical-encoding\u2014can prove powerful especially to encode high cardinality categorical features. \n> This implementation assumes that the target is ordinal (which is the case here as it is a binary outcome, but for many multiclass classification that is often not the case).\n\nHere we use it for all features as a starting point, but many of those features might better contribute to the overall predictive power when encoded with alternative techniques.\n\nWe use k-fold to mitigate data leaks that would otherwise almost certainly lead to overfitting. Alternatively, we could split the train set, but given the small size of it, a resampling technique sounds preferable.\n\n\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Blue\">\nIf you find this kernel useful or interesting, please don't forget to upvote the kernel =)\n<\/font><\/p>\n\n<\/body>\n<\/html>\n\n","527a4792":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"blue\">If you like my kernel please consider upvoting it<\/font><\/p>\n<p><font size=\"4\" color=\"purple\">Remember the upvote button is next to the fork button, and it's free too! ;)<\/font><\/p>\n\n<\/body>\n<\/html>\n","a128890a":"## Read datasets","72727036":"## Import libs","578bec4c":"## predict proba","179e237b":"## About this Competition\n\n![](https:\/\/imgflip.com\/s\/meme\/Smiling-Cat.jpg)\n\n> #### In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values (See this). (Of course, in real-world settings both of these factors are often important to consider!)\n\n#### Files\n- train.csv - the training set\n- test.csv - the test set; you must make predictions against this data\n- sample_submission.csv - a sample submission file in the correct format","e0267314":"# Encoding the Features"}}