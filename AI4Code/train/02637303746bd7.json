{"cell_type":{"66462255":"code","51252cf2":"code","67dbe112":"code","a78bff01":"code","7fa0f378":"code","23fa2c5b":"code","4b02b171":"markdown"},"source":{"66462255":"#!\/usr\/bin\/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndf = pd.read_csv('..\/input\/reviews\/5556-ar-reviews.csv')\ndf = df.sample(frac=1)\ndf.head()","51252cf2":"\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.l2 = nn.Linear(hidden_size, hidden_size)\n        self.l3 = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        out = self.relu(out)\n        out = self.l3(out)\n        # no activation and no softmax at the end\n        return out","67dbe112":"\n\ndef bag_of_words(tokenized_sentence, words):\n    \"\"\"\n    return bag of words array:\n    1 for each known word that exists in the sentence, 0 otherwise\n    example:\n    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n    \"\"\"\n    # stem each word\n    sentence_words = [word for word in tokenized_sentence]\n    # initialize bag with 0 for each word\n    bag = np.zeros(len(words), dtype=np.float32)\n    for idx, w in enumerate(words):\n        if w in sentence_words:\n            bag[idx] = 1\n\n    return bag\n\n\nstopwords = {'\u0641\u0625\u0630\u0627', '\u0623\u0646\u0649', '\u0628\u0645\u0646', '\u062d\u062a\u0649', '\u0644\u0645', '\u0623\u0646\u062a\u0645\u0627', '\u0647\u0646\u0627\u0643', '\u062a\u064a\u0646\u0643', '\u0628\u0644', '\u0625\u064a', '\u0639\u0646', '\u0648\u0644\u0643\u0646', '\u0648\u0625\u0630\u0627', '\u062f\u0648\u0646', '\u0625\u0646\u0627', '\u0625\u0630\u0646', '\u0628\u0643\u0645', '\u062d\u064a\u0646', '\u0639\u0646\u062f', '\u0647\u0644', '\u0625\u0644\u0627', '\u0647\u0627\u062a\u0647', '\u0630\u064a\u0646\u0643', '\u0627\u0644\u0644\u0648\u0627\u062a\u064a', '\u0643\u0630\u0627', '\u0644\u0633\u062a\u0645\u0627', '\u0647\u064a', '\u0627\u0644\u0644\u062a\u0627\u0646', '\u0623\u0643\u062b\u0631', '\u0643\u0644\u062a\u0627', '\u0644\u0643\u0646', '\u0644\u064a\u0633\u062a\u0627', '\u0647\u0643\u0630\u0627', '\u0639\u0633\u0649', '\u0625\u0630', '\u0625\u0646', '\u0627\u0644\u0644\u0627\u062a\u064a', '\u0625\u0630\u0627', '\u0628\u0647\u0645', '\u0646\u062d\u0646', '\u0641\u064a\u0645\u0627', '\u0630\u0627\u0643', '\u0628\u0643\u0646', '\u0628\u064a\u062f', '\u0644\u0647\u0646', '\u0647\u0630\u064a', '\u0643\u0623\u064a', '\u0630\u0648\u0627', '\u0623\u064a', '\u0643\u0644\u0627\u0647\u0645\u0627', '\u0647\u0630\u064a\u0646', '\u0623\u064a\u0646\u0645\u0627', '\u0643\u064a', '\u0625\u0644\u064a\u0643\u0646', '\u0645\u0627\u0630\u0627', '\u0647\u064a\u0627', '\u0647\u0646\u0627\u0644\u0643', '\u0628\u064a', '\u0628\u0645\u0627', '\u062a\u0644\u0643\u0645\u0627', '\u0628\u0639\u0636', '\u0628\u0647\u0646', '\u062a\u064a\u0646', '\u0631\u064a\u062b', '\u0639\u0644\u0649', '\u063a\u064a\u0631', '\u062d\u064a\u062b\u0645\u0627', '\u0643\u0623\u0646', '\u0628\u062e', '\u0647\u0627\u062a\u0627\u0646', '\u0647\u0627\u0647\u0646\u0627', '\u0645\u0627', '\u0647\u064a\u0647\u0627\u062a', '\u0644\u062f\u0649', '\u0634\u062a\u0627\u0646', '\u0644\u0633\u0646\u0627', '\u0643\u064a\u0641\u0645\u0627', '\u0645\u0639', '\u0645\u0645\u0646', '\u0643\u0645\u0627', '\u0625\u0646\u0645\u0627', '\u064a\u0627', '\u0639\u0644\u064a\u0647', '\u0644\u0643', '\u0630\u0647', '\u0630\u0627\u0646', '\u0644\u0647\u0645\u0627', '\u0644\u064a\u0633\u062a', '\u0644\u0646\u0627', '\u0645\u0647', '\u0623\u0646\u062a\u0646', '\u0641\u064a', '\u0644\u0648\u0644\u0627', '\u0628\u0633', '\u0644\u0647\u0627', '\u0623\u0642\u0644', '\u0639\u0644\u064a\u0643', '\u0641\u0644\u0627', '\u0645\u0647\u0645\u0627', '\u0644\u064a\u0633\u0627', '\u0630\u064a\u0646', '\u0630\u0627\u062a', '\u0643\u0644\u0645\u0627', '\u0630\u0627', '\u0630\u0648', '\u0641\u064a\u0647', '\u062a\u064a', '\u0647\u0646\u0627', '\u0647\u0627\u062a\u064a\u0646', '\u0647\u0627', '\u0647\u0645', '\u0623\u0644\u0627', '\u0644\u0627', '\u0633\u0648\u0649', '\u0648\u0625\u0630', '\u0643\u0645', '\u0644\u0633\u062a', '\u062d\u064a\u062b', '\u0625\u0644\u064a\u0643\u0645\u0627', '\u0644\u0648\u0645\u0627', '\u0627\u0644\u0630\u064a\u0646', '\u0643\u0644\u0627',\n             '\u0627\u0644\u062a\u064a', '\u0643\u0623\u064a\u0646', '\u0630\u0648\u0627\u062a\u064a', '\u0644\u0633\u062a\u0645', '\u0647\u0630\u0627', '\u0641\u0645\u0646', '\u0630\u0644\u0643\u0645', '\u0648\u0645\u0627', '\u0643\u064a\u0641', '\u0644\u0643\u0645', '\u062d\u0627\u0634\u0627', '\u0628\u0643', '\u0648\u0627\u0644\u0630\u064a', '\u0623\u0646', '\u0644\u0647\u0645', '\u0644\u0633\u0646', '\u062b\u0645\u0629', '\u0630\u064a', '\u0648\u0625\u0646', '\u0648\u0645\u0646', '\u0623\u064a\u0647\u0627', '\u0644\u0647', '\u0645\u062a\u0649', '\u0628\u0644\u0649', '\u0627\u0644\u0644\u062a\u064a\u0646', '\u0644\u0633\u062a\u0646', '\u0628\u0643\u0645\u0627', '\u0642\u062f', '\u0643\u0644\u064a\u0643\u0645\u0627', '\u0644\u0643\u0645\u0627', '\u0647\u0644\u0627', '\u0622\u064a', '\u0644\u0643\u0646\u0645\u0627', '\u0627\u0644\u0644\u0630\u064a\u0646', '\u0627\u0644\u0644\u0627\u0626\u064a', '\u0630\u0644\u0643\u0646', '\u0644\u0627\u0633\u064a\u0645\u0627', '\u0630\u0644\u0643', '\u0645\u0630', '\u0627\u0644\u0644\u062a\u064a\u0627', '\u0647\u0645\u0627', '\u0625\u0644\u064a\u0643', '\u0633\u0648\u0641', '\u0645\u0646\u0647\u0627', '\u0648\u0627\u0644\u0630\u064a\u0646', '\u0623\u0646\u062a\u0645', '\u0647\u0627\u062a\u064a', '\u0644\u0643\u064a', '\u0627\u0644\u0644\u0630\u0627\u0646', '\u0630\u0648\u0627\u062a\u0627', '\u0639\u0645\u0627', '\u0641\u064a\u0647\u0627', '\u0625\u0644\u0649', '\u062a\u0644\u0643', '\u0643\u0644', '\u0644\u064a', '\u0647\u0648', '\u0641\u064a\u0645', '\u0625\u0644\u064a\u0643\u0645', '\u0628\u0647\u0627', '\u0630\u0627\u0646\u0643', '\u0625\u0646\u0647', '\u0647\u0624\u0644\u0627\u0621', '\u0623\u0648\u0644\u0626\u0643', '\u0625\u0630\u0645\u0627', '\u0628\u0646\u0627', '\u0645\u0646', '\u062e\u0644\u0627', '\u0644\u064a\u0633\u0648\u0627', '\u062b\u0645', '\u0644\u0639\u0644', '\u0648\u0647\u0648', '\u0646\u062d\u0648', '\u0623\u064a\u0646', '\u0644\u0626\u0646', '\u0639\u062f\u0627', '\u0622\u0647', '\u0643\u0623\u0646\u0645\u0627', '\u0643\u0644\u064a\u0647\u0645\u0627', '\u0627\u0644\u0630\u064a', '\u0644\u0646', '\u0646\u0639\u0645', '\u0647\u0630\u0647', '\u0628\u0647\u0645\u0627', '\u0644\u064a\u062a', '\u062a\u0644\u0643\u0645', '\u0623\u0645\u0627', '\u0645\u0646\u0630', '\u0623\u0648', '\u0647\u0627\u0643', '\u0628\u0645\u0627\u0630\u0627', '\u0643\u0630\u0644\u0643', '\u0623\u0646\u0627', '\u0622\u0647\u0627', '\u0641\u0625\u0646', '\u0639\u0644', '\u0645\u0646\u0647', '\u0647\u064a\u062a', '\u0623\u0641', '\u0623\u0645', '\u0625\u064a\u0647', '\u0643\u064a\u062a', '\u062a\u0647', '\u0644\u0643\u064a\u0644\u0627', '\u0644\u064a\u0633', '\u0645\u0645\u0627', '\u0647\u0630\u0627\u0646', '\u0623\u0646\u062a', '\u062d\u0628\u0630\u0627', '\u0648\u0644\u0648', '\u0623\u0648\u0647', '\u0625\u0645\u0627', '\u0644\u0648', '\u0628\u064a\u0646', '\u0628\u0647', '\u0648\u0644\u0627', '\u0644\u0645\u0627', '\u0628\u0639\u062f', '\u0647\u0646', '\u0630\u0644\u0643\u0645\u0627', '\u0623\u0648\u0644\u0627\u0621', '\u0648'}\n\n\ndef tokenize(sentence):\n    tmpTokens = sentence.lower().split()\n    tokens = [token for token in tmpTokens if (\n        (token not in stopwords) and (len(token) > 0))]\n\n    return tokens","a78bff01":"\n\nall_words = []\ntags = [0, 1]\nxy = []\n# loop through each sentence in our intents patterns\nintents = df[[\"label\", \"text\"]].values\n\nfor pattern in intents:\n    tag = pattern[0]\n    # tokenize each word in the sentence\n    w = tokenize(pattern[1])\n    # add to our words list\n    all_words.extend(w)\n    # add to xy pair\n    xy.append((w, tag))\n\n# stem and lower each word\nignore_words = ['?', '.', '!']\nall_words = [w for w in all_words if w not in ignore_words]\n# remove duplicates and sort\nall_words = sorted(set(all_words))\ntags = sorted(set(tags))\n\nX_train = []\ny_train = []\nfor (pattern_sentence, tag) in xy:\n    # X: bag of words for each pattern_sentence\n    bag = bag_of_words(pattern_sentence, all_words)\n    X_train.append(bag)\n    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n    label = tags.index(tag)\n    y_train.append(label)\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\n# Hyper-parameters\nnum_epochs = 100\nbatch_size = 64\nlearning_rate = 0.001\ninput_size = len(X_train[0])\nhidden_size = 8\noutput_size = len(tags)\nprint(input_size, output_size)  # 59915 2\n\n","7fa0f378":"\nclass ReviewsDataset(Dataset):\n\n    def __init__(self):\n        self.n_samples = len(X_train)\n        self.x_data = X_train\n        self.y_data = y_train\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n\ndataset = ReviewsDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\nfor epoch in range(num_epochs):\n    for idx, (words, labels) in enumerate(train_loader):\n        words = words.to(device)\n        labels = labels.to(dtype=torch.long).to(device)\n\n        # Forward pass\n        outputs = model(words)\n        # if y would be one-hot, we must apply\n        # labels = torch.max(labels, 1)[1]\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 5 == 0:\n        print(f'Epoch [{epoch+1}\/{num_epochs}], Loss: {loss.item()\/idx:.4f}')\n\n\nprint(f'final loss: {loss.item():.4f}')\n\ndata = {\n    \"model_state\": model.state_dict(),\n    \"input_size\": input_size,\n    \"hidden_size\": hidden_size,\n    \"output_size\": output_size,\n    \"all_words\": all_words,\n    \"tags\": tags\n}\n\nFILE = \"data2.pth\"\ntorch.save(data, FILE)\n\nprint(f'training complete. file saved to {FILE}')\n","23fa2c5b":"sentence= \"\u0631\u062d\u0644\u0627\u062a\u064a \u0627\u0644\u0642\u0627\u062f\u0645\u0629 \u0627\u0644\u0649 \u0627\u0644\u0631\u064a\u0627\u0636 \u0639\u0646\u062f\u0647\u0645 \u0623\u0646 \u0634\u0627\u0621 \u0627\u0644\u0644\u0647 . . \u0637\u0627\u0642\u0645 \u0627\u0644\u0639\u0645\u0644 \u0631\u0627\u0626\u0639 \u0643\u0644\u0647\u0645 \u0627.\u0648\u0627\u0626\u0644 \u0648 \u0627\u062b\u0646\u064a\u0646 \u0634\u0628\u0627\u0628 \u0641\u064a \u0641\u062a\u0631\u0629 \u0627\u0644\u0635\u0628\u0627\u062d \u0646\u0627\u0633\u064a \u0627\u0633\u0645\u0647\u0645 . \u062a\u0642\u062f\u064a\u0645 \u0636\u064a\u0627\u0641\u0629 \u0639\u0646\u062f \u0627\u0644\u0642\u062f\u0648\u0645 \u0635\u0628\u0627\u062d\u0627 . \u062a\u0642\u062f\u064a\u0645 \u0636\u064a\u0627\u0641\u0629 \u0641\u064a \u0627\u0644\u0645\u0633\u0627\u0621 \u0634\u0627\u064a \u0648 \u0642\u0647\u0648\u0629 \u0639\u0631\u0628\u064a \u0645\u0639 \u062a\u0645\u0631 . \u0631\u0627\u062d\u0629 \u0648 \u0646\u0638\u0627\u0641\u0629 \u0627\u0644\u0645\u0643\u0627\u0646 \u062c\u062f\u0627 \u062c\u062f\u0627 \u0646\u0638\u064a\u0641 \u0648 \u062e\u062f\u0645\u0629 \u0627\u0644\u063a\u0631\u0641 \u0633\u0631\u064a\u0639\u0629 . \u0645\u0648\u0642\u0639\u0647 \u0645\u0646\u0627\u0633\u0628 \u0644\u0644\u0623\u0634\u062e\u0627\u0635 \u0627\u0647\u062a\u0645\u0627\u0645\u0647\u0645 \u0627\u0644\u0633\u0641\u0627\u0631\u0627\u062a \u0627\u0648 \u0627\u0644\u062c\u0627\u0645\u0639\u0629 \u0642\u0631\u064a\u0628 \u0645\u0646\u0647\u0627 \u0648 \u0644\u0643\u0646 \u0628\u0639\u064a\u062f \u0639\u0646 \u0634\u0631\u0642 \u0627\u0644\u0631\u064a\u0627\u0636 . \u0633\u0647\u0648\u0644\u0629 \u0627\u0644\u0648\u0635\u0648\u0644 \u0627\u0644\u0649 \u0627\u0644\u062f\u0627\u0626\u0631\u064a \u0627\u0644\u0634\u0645\u0627\u0644\u064a . \u0647\u062f\u0648\u0621 \u0627\u0644\u0645\u0643\u0627\u0646 . \u062c\u0645\u064a\u0639 \u0627\u0644\u062e\u062f\u0645\u0627\u062a \u062d\u0648\u0644\u0643 . \u0627\u0644\u0645\u0633\u062c\u062f \u0645\u0642\u0627\u0628\u0644 \u0627\u0644\u0641\u0646\u062f\u0642. \u0644\u0627 \u064a\u0648\u062c\u062f \u0634\u064a \u0633\u064a \u0627\u0628\u062f\u0627\" \n\nsentence = tokenize(sentence)\nX = bag_of_words(sentence, all_words)\nX = X.reshape(1, X.shape[0])\nX = torch.from_numpy(X).to(device)\n\noutput = model(X)\n_, predicted = torch.max(output, dim=1)\n\ntag = tags[predicted.item()]\n\nprobs = torch.softmax(output, dim=1)\nprob = probs[0][predicted.item()]\nif prob.item() > 0.75:\n    for intent in intents:\n        if tag == intent[0]:\n            print(f\" {tag} \")\n            break\nelse:\n    print(\"I do not understand...\")\n","4b02b171":"# Prediction"}}