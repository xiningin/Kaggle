{"cell_type":{"ffa19ea6":"code","6c2c30a1":"code","a48fb3b0":"code","b838efc5":"code","900c11eb":"code","e3383ddc":"code","ed2afa64":"code","84809be9":"code","5d5afd66":"code","8196d490":"code","ee2aca9f":"code","5e2456fd":"code","625584e7":"code","390cb9c2":"code","54e8a1e1":"code","e0cb7606":"code","f8cba4bf":"code","aa2eb357":"code","c964795a":"code","a46bac5d":"markdown","aabd22be":"markdown","30a1fb72":"markdown","b883d1e6":"markdown","9cc50d9d":"markdown","e96bb246":"markdown","5dc541e7":"markdown","c83c5157":"markdown","9ed5f768":"markdown","104c202c":"markdown","38ed2996":"markdown","1d435e71":"markdown","8ee2d17c":"markdown","c31fae87":"markdown","9a143d41":"markdown","d05f5c7e":"markdown"},"source":{"ffa19ea6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm, skew\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#Loading train and test_data\ndata_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","6c2c30a1":"#visualizing correlation matrix to find dependent features\nhousing = data_train.copy()\ncorr_matrix = housing.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Add title\nplt.title(\"Correlation matrix of numerical features\")\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","a48fb3b0":"# print out feature combinations with high correlation coefficient\nprint(\"Correlation coefficient 'GarageYrBlt' \/ 'YearBuilt': '{:.1%}\".format(corr_matrix.loc['GarageYrBlt', 'YearBuilt']))\nprint(\"Correlation coefficient '1stFlrSF' \/ 'TotalBsmtSF': '{:.1%}\".format(corr_matrix.loc['1stFlrSF', 'TotalBsmtSF']))\nprint(\"Correlation coefficient 'TotRmsAbvGrd' \/ 'GrLivArea': '{:.1%}\".format(corr_matrix.loc['TotRmsAbvGrd', 'GrLivArea']))\nprint(\"Correlation coefficient 'GarageArea' \/ 'GarageCars': '{:.1%}\".format(corr_matrix.loc['GarageArea', 'GarageCars']))","b838efc5":"# Visualize correlation of columns to SalePrice\ncorr_matrix = corr_matrix.drop('GarageYrBlt')\ncorr_matrix = corr_matrix.drop('1stFlrSF')\ncorr_matrix = corr_matrix.drop('TotRmsAbvGrd')\ncorr_matrix = corr_matrix.drop('GarageArea')\nplt.figure(figsize=(50,6))\nplt.xticks(rotation=45)\nsns.barplot(x=corr_matrix[\"SalePrice\"].sort_values(ascending=False).index, y=corr_matrix[\"SalePrice\"].sort_values(ascending=False))","900c11eb":"# Remaining relevant numerical features are saved in relevant_num_features\n# 'SalePrice' needs to be dropped!\ncorr_matrix = corr_matrix.drop('SalePrice')\nrelevant_num_features = corr_matrix['SalePrice'].sort_values(ascending=False).loc[:'BsmtUnfSF'].index\nrelevant_num_features","e3383ddc":"sns.distplot(housing['SalePrice'])\nprint(\"Skewness: %f\" % housing['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % housing['SalePrice'].kurt())","ed2afa64":"# Fixing skew by natural log-transformation of \u201cSalePrice\u201d\nhousing['SalePrice'] = np.log1p(housing['SalePrice'])\nsns.distplot(housing['SalePrice'], fit=norm);","84809be9":"#Calculating percentage of empty values in each column\nn_nan = []\nfor col in housing.columns:\n    n_nan.append(int(housing[col].isna().sum(axis=0))\/len(housing.index))\nnan_percentage = pd.DataFrame(n_nan, index = housing.columns)\n\n# showing the ten highest features with NaN values\nnan_percentage = nan_percentage.iloc[:,0].sort_values(ascending=False)\nnan_percentage[:10]","5d5afd66":"# Saving categorical features with >50% missing values in list\n# so they can be dropped later\nlow_relevant_cat = nan_percentage[:4].index\nlow_relevant_cat","8196d490":"data_train['TotalSF'] = data_train['TotalBsmtSF'] + data_train['1stFlrSF'] + data_train['2ndFlrSF']\ndata_test['TotalSF'] = data_test['TotalBsmtSF'] + data_test['1stFlrSF'] + data_test['2ndFlrSF']","ee2aca9f":"# Creating label vector y_train and remove it from the training set\n# Labels are natural log normalized in order to reduce scew of dataset\ny_train = np.log1p(data_train['SalePrice'])\ndata_train = data_train.drop('SalePrice', axis = 1)\n\n# Creating helper function to select relevant features dataset\ndef select_relevant_data(df,relevant_num_features,low_relevant_cat):\n    # devide data into numerical and categorical data\n    df_cat = df.select_dtypes(include=object)\n    df_num = df.select_dtypes(include=np.number)\n    # drop categorical features with low relevance (>50 % missing values)\n    df_cat = df_cat.drop(low_relevant_cat, axis = 1)\n    # pick numerical feautres with high relevance\n    df_num = df_num.loc[:,relevant_num_features]\n    # return transformed dataset (only relevant features)\n    return pd.concat([df_cat, df_num], axis=1)\n\n# cleaning data from train and test_set using helper function\nX_train = select_relevant_data(data_train,relevant_num_features,low_relevant_cat)\nX_test = select_relevant_data(data_test,relevant_num_features,low_relevant_cat)","5e2456fd":"# Prepare data for ML algorithm\n# Missing numerical values will be imputed with median. Standard scaling of numerical features.\n# Missing categorical values will be imputed with '-1'. One-hot encoding of categorical features.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='-1')),\n    ('OneHotEncoder', OneHotEncoder()),\n])\n\n# creating list of numerical features and list of categorical features\nX_train_cat = X_train.select_dtypes(include=object)\nX_train_num = X_train.select_dtypes(include=np.number)\nnum_attribs = list(X_train_num)\ncat_attribs = list(X_train_cat)\n\nfull_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", cat_pipeline, cat_attribs),\n])\n\n#pipeline has to be fitted to training and test set in order for the ordinal_encoder to work for for both sets\nX_fit = X_train.append(X_test, ignore_index=True)\nfull_pipeline.fit(X_fit)\nX_train_prepared = full_pipeline.transform(X_train)","625584e7":"# Using XGBoostRegressor to model SalePrice\n# Using GridSearchCV to test different parameter sets\nimport xgboost\nfrom sklearn.model_selection import GridSearchCV\n\nxgb_reg = xgboost.XGBRegressor()\n\n# A parameter grid for XGBoost\nparam_grid =  {\n        'max_depth': [4,5,6],\n#        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [1.0],\n        'colsample_bytree': [1.0],\n        'colsample_bylevel': [0.5],\n        'learning_rate': [0.01, 0.5, 0.1],\n        'n_estimators': [500, 1000, 3000, 5000] \n#        'min_child_weight': [1, 5, 10],       \n        }\n\ngrid_search = GridSearchCV(xgb_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X_train_prepared, y_train)","390cb9c2":"print(grid_search.best_params_)\nprint(grid_search.best_estimator_)","54e8a1e1":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","e0cb7606":"final_model = grid_search.best_estimator_","f8cba4bf":"X_test_prepeared = full_pipeline.transform(X_test)\npredictions = np.floor(np.expm1(final_model.predict(X_test_prepeared)))","aa2eb357":"ImageId = np.arange(1461,1461+len(predictions))\nSalePrice = np.array(predictions)\nnew_submission = pd.DataFrame({'Id': ImageId, 'SalePrice': SalePrice})\nnew_submission.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c964795a":"new_submission","a46bac5d":"Another important issue to check is if the dataset shows skew. It can be seen that the target variable **'SalePrice'**  had extreme high values (a long right-tail) and was not distributed normally. In other words it shows positive skew (Skewness = 1.88).","aabd22be":"## 5. Data transformation\nThe following transformations were performed on the train and test data:\n- Creating natural log normalized label vector **y_train** for the training set and remove 'SalePrice' from training\n- Drop low relevant categorical features and only pick highly relevant numerical features of the train and test dataset\n- Create pipeline to prepare data for ML algorithm. Impute missing numerical values with median. Standard scale numerical features. Impute missing categorical values with '-1'. One-hot-encode categorical features.","30a1fb72":"## Introduction\n\nThis is my first ML project that I mostly coded myself from start to finish. The goal for me was to create a full ML pipeline that includes:\n- Data manupulation with pandas. Data analysis and extraction of relevant features.\n- Dataset cleaning with scikit-learn for XGBoost algorithm.\n- GridSearchCV for hyperparamter tuning.\n\n\n##  Useful recources:\nDuring my research for this competition, I found the following references very useful. I'd advise every beginner to have a peek into these articles:\n- Amazing walkthrough and good overview over the most important issues to tackle: <br>https:\/\/towardsdatascience.com\/wrangling-through-dataland-modeling-house-prices-in-ames-iowa-75b9b4086c96\n- Hands-on explanation how to perform hyperparameter tuning for different ML algorithms (including XGBoost): <br>https:\/\/towardsdatascience.com\/beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d\n\n\n## Contents\n\n1. Importing libaries and loading Data\n2. Exploring the dataset (correlation and skew)\n3. Analyzing features with missing values\n4. Feature engineering\n5. Data transformation\n6. Modeling, training and hyperparameter tuning\n8. Inference","b883d1e6":"It turned out all features with many mising values are categorical features. The following features with more than 50% missing values will be collected in a list (**low_relevant_cat**) in order to drop them from the dataset later:\n- **'PoolQC'**\n- **'MiscFeature'**\n- **'Alley'**\n- **'Fence'**","9cc50d9d":"## 6. Modeling, training and hyperparameter tuning\nThis is the result of a couple of rounds of hyperparameter tuning with GridSearch and XGBoost. For a detailed manual how to do hyperparameter tuning with XGBoost, see the link in references.","e96bb246":"In order to speed up training later, I will remove some features. If the correlation between two features is high (>0.8) one of them can be dropped because they don't add much more information. <br>\n\nThe following features will be removed (correlation coefficient values see next cell): \n- **'GarageYrBlt'**\n- **'1stFlrSF'**\n- **'TotRmsAbvGrd'**\n- **'GarageArea'**","5dc541e7":"Printing out information about the best estimator and training scores:","c83c5157":"## 1. Importing libaries and loading Data","9ed5f768":"To improve the accuracy it is better to normalize the target variable to reach a normal distribution. A posible way is to use natural log-transformation. This wil be done later in this notebook at point **5. Data transformation**:","104c202c":"## 3. Analyzing features with missing values\nNext, I check which features of the dataset can be removed because they don't contain enough data. For this we calculate the percentage of missing values of each column (**nan_percentage**) and show the ten highest values:","38ed2996":"Next step is to check how strong the correlation between **'SalePrice'** and the other features is. All features with | correlation coefficient | < 0.2 are removed to speed up training:\n- **BedroomAbvGr**                  0.168213\n- **ScreenPorch**                   0.111447\n- **PoolArea**                      0.092404\n- **MoSold**                        0.046432\n- **3SsnPorch**                     0.044584\n- **BsmtFinSF2**                   -0.011378\n- **BsmtHalfBath**                 -0.016844\n- **MiscVal**                      -0.021190\n- **Id**                           -0.021917\n- **LowQualFinSF**                 -0.025606\n- **YrSold**                       -0.028923\n- **OverallCond**                  -0.077856\n- **MSSubClass**                   -0.084284\n- **EnclosedPorch**                -0.128578\n- **KitchenAbvGr**                 -0.135907","1d435e71":"## 4. Feature Engineering\n\nI read that adding a feature which sums the total area (= **'TotalSF'**) will improve the score. For this I have to add a feature to the train and test dataset:","8ee2d17c":"## 2. Exploring the train set (correlation and skew)","c31fae87":"**'SalePrice'** column needs to be removed from training set before training! All revelant feature indexes are saved in **relevant_num_features**.","9a143d41":"## 7. Inference","d05f5c7e":"It's also possible to fix the scew of the other numerical features. However, I'll have a look into it in the next version of this notebook."}}