{"cell_type":{"478061c8":"code","e2b2a69a":"code","87dc6f76":"code","bccb4864":"code","8beca74c":"code","174c5a82":"code","f53d3d6d":"code","8c745705":"code","34d5f389":"code","de9e4953":"code","e6da3d6a":"code","0f783d9a":"code","ed5d9efb":"markdown","5a043b2f":"markdown","06d8e3ed":"markdown","a827702b":"markdown","d42b4d9a":"markdown","04023875":"markdown","d9fb6d21":"markdown"},"source":{"478061c8":"# Standard Data Science libraries for mathematical operations and data management in Dataframes\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Sklearn allows us to preprocess data and contains the regression model classes\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing os allows us to print to screen the contents of the \"input\" folder in which our datasets are stored.\n# This makes it easy to identify our data and the path and names to use to recover it.\nimport os\nprint(os.listdir(\"..\/input\"))","e2b2a69a":"# We create a dataframe df and its copy df_train on which we will work.\ndf = pd.read_csv('..\/input\/train.csv')\ndf_train = df.copy()\n\n# Dropping target variable and Id\n#df_train.drop('SalePrice', axis=1, inplace=True)\ndf_train.drop('Id', axis=1, inplace=True)\n\n# Printing the first 5 rows of the dataframe\ndf_train.head(3)\n\n# This is our data at the moment:","87dc6f76":"# We replace all NaN cells in our columns with 0\ndf_train.fillna(0, inplace=True)\n\ndf.head(3)\n\n# This is the state of our data at this moment:","bccb4864":"# Categorical values management\ncategories = df_train.select_dtypes(include=['category', object]).columns\n# Uncomment the following line of code to see which of our columns contain categorical values\n# print(categories)\ndf_categorical = pd.get_dummies(df_train, columns=categories, drop_first=True)\ndf_train.drop(categories.tolist(), axis=1, inplace=True)\ndf_train = pd.concat([df_train, df_categorical], axis=1)\ndf_train.head(3)","8beca74c":"# Data standardization \n\n# Get column names first\nnames = df_train.columns\n\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n\n# Fit your data on the scaler object\nscaled_train_df = scaler.fit_transform(df_train)\nscaled_train_df = pd.DataFrame(scaled_train_df, columns=names)\nscaled_train_df.head(3)","174c5a82":"# Data Visualization\n\ndf_train = scaled_train_df\n\n# Removing duplicate columns\ndf_train = df_train.loc[:,~df_train.columns.duplicated()]\n\n#Seeing correlation\n# df_train.corr()['SalePrice'].sort_values(ascending=False)[120:-120]\n# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n#    print(df_train.corr()['SalePrice'].sort_values(ascending=False))\n\ndf_train.head(3)","f53d3d6d":"#correlation matrix\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nf, ax = plt.subplots(figsize=(20, 3))\n# sns.heatmap(df_train[top_corr_features].corr()[['SalePrice']].sort_values(by='SalePrice', ascending = False), vmax=.99, vmin=-0.99, annot=True, center=0, cmap=sns.diverging_palette(10, 133, as_cmap=False, n=10));\nsns.heatmap(df_train[top_corr_features].corr().loc['SalePrice', :].to_frame().T, vmax=.99, vmin=-0.99, annot=True, center=0, cmap=sns.diverging_palette(10, 133, as_cmap=True, n=10)).set_xticklabels(hm.get_xticklabels(), rotation=45);\n","8c745705":"# Fitting the Model\nX = df_train.drop('SalePrice', axis=1)\ny = df['SalePrice'].values.reshape(-1,1)\nlm = LinearRegression()\nlm.fit(X,y)\n\n# Viewing training set score\nlm.score(X,y)","34d5f389":"# Test Data Pipeline\ndf_test = pd.read_csv('..\/input\/test.csv')\n# Removing Id column\ndf_test.drop('Id', axis=1, inplace=True)\n# Null values management\ndf_test.fillna(0, inplace=True)\n# Categorical values management\ncategories = df_test.select_dtypes(include=['category', object]).columns\n#print(categories)\ndf_test_categorical = pd.get_dummies(df_test, columns=categories, drop_first=True)\ndf_test.drop(categories.tolist(), axis=1, inplace=True)\ndf_test = pd.concat([df_test, df_test_categorical], axis=1)\n\n\n# Managing different category values:\n# This block is not part of the pipeline we applied to the training data.\n# It is needed because different categorical values in the test data might generate different dummy variables (and columns) in the testing data\nmissing = list(set(X.columns.tolist()) - set(df_test.columns.tolist()))\ndf_test = pd.concat([df_test,pd.DataFrame(columns=missing)], axis = 1)\nsurplus = list(set(df_test.columns.tolist()) - set(X.columns.tolist()))\ndf_test = df_test.drop(surplus, axis=1)\ndf_test.fillna(0, inplace=True)\n\n\n\n\n# Data standardization \nnames = df_test.columns\nscaler = preprocessing.StandardScaler()\nscaled_test_df = scaler.fit_transform(df_test)\nscaled_test_df = pd.DataFrame(scaled_test_df, columns=names)\ndf_test = scaled_test_df\n# Removing duplicate columns\ndf_test = df_test.loc[:,~df_test.columns.duplicated()]\n\ndf_test.head(3)","de9e4953":"y_test = pd.read_csv('..\/input\/sample_submission.csv')[['SalePrice']]\n#y_test = scaler.fit_transform(y_test[['SalePrice']])\nX_test = df_test\nX_test.shape","e6da3d6a":"# adapting the test set to training set\n#missing = list(set(X.columns.tolist()) - set(X_test.columns.tolist()))\n#print(missing)\n#X_test = pd.concat([X_test,pd.DataFrame(columns=difference)])\n#surplus = list(set(X_test.columns.tolist()) - set(X.columns.tolist()))\n#X_test = X_test.drop(surplus, axis=1)\nprint(X.shape, X_test.shape)","0f783d9a":"# Predicting with Multiple Linear Regression\nyhat = lm.predict(X_test)\nlm.score(X_test,y_test.values.reshape(-1,1))\nyhat","ed5d9efb":"   <hr\/>\n\n<h1>Importing the necessary Python Libraries<\/h1>\n","5a043b2f":"\n<h1 style=\"font-size:38pt;text-align:center\">House Pricing Prediction<\/h1>\n<h3 style=\"font-weight:normal;text-align:center;margin:auto\">An example of Data Science methodology for Regression<\/h3>\n<p style=\"float:right;font-style:italic\">by Alessandro D'Angeli<\/p>\n","06d8e3ed":"<h2>Standardizing the Data<\/h2>\n<p>We apply the StandardScaler fit_transform function to the dataframe so that the data will have a mean and unit variance of 0.<\/p>\n<hr\/>","a827702b":"<h2>Importing Test Data<\/h2>\n<p>We import the Test data into a dataframe and put it through the pipepline to transform it in the same way as the training data.<\/p>\n<hr\/>","d42b4d9a":"<hr\/>\n<h1>Importing the training data<\/h1>","04023875":"<hr\/>\n<h1>Converting Categorical Values<\/h1>\n<p>As some of our variables are represented by text rather than numbers, and can't be used in the mathematical operations necessary for regression, we create dummy variables so that all categorical variables can be represented as 0s and 1s.<\/p>\n<p>We will be assuming that we are dealing with nominal categorical variables and apply the <b>One Hot Encoding<\/b> method. Ordinal categorical variables would be better suited by a different approach (such as integer encoding).<\/p>","d9fb6d21":"<hr\/>\n<h1>Managing Null Data<\/h1>\n<p>In this case, considering the nature of the data that is missing (garage\/pool size and similar), we chose to replace all NaN values with 0 to represent the lack of the features that these columns would measure.<\/p>"}}