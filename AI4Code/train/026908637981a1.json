{"cell_type":{"b9b0bc2b":"code","a393490e":"code","2bac76ec":"code","a3d1306d":"code","309bedea":"code","9ad0fbea":"code","d7a929b8":"code","f0c6aa96":"code","57284c7a":"code","543f4288":"code","fb01ae24":"code","f56c1701":"code","fa7142f3":"code","16967566":"code","73654184":"code","004a0fa9":"code","210ee0c1":"code","5314a8f7":"code","0bd7bafa":"code","e761cdc3":"code","adedeccd":"code","3b18b1e1":"code","6cef8dac":"code","178a3bb8":"code","e4eb19d8":"code","ec87bc60":"code","dbb87ae8":"code","0593b1a7":"code","b6d90727":"code","69bdd95b":"code","7c8fce0e":"code","354a8969":"code","6e93fb01":"code","d9f3d1de":"code","0c59abd0":"code","8c269d27":"code","4231e0c1":"code","066ea02a":"code","521a362d":"code","2fe12001":"code","15296410":"code","5de5a45b":"code","3127dc8b":"code","b41da981":"code","4198f1c2":"code","a1ec50fb":"code","d199ede8":"code","e90cad41":"code","3a37b260":"code","31c64cb1":"code","98e642eb":"code","e9a8b9d1":"code","36b706f0":"code","b5a9634f":"code","3f2aa77d":"code","98152e6e":"code","4bb34245":"code","0cd1e03a":"code","1b41f181":"code","b89a5cde":"code","d9a4ea20":"code","ceb13126":"code","ba6291db":"code","fe4a73ff":"code","372884a2":"code","7849372a":"code","5d9729b1":"code","a2eaff75":"code","7ceb9852":"code","1e9d26da":"markdown","1071209b":"markdown","7b270895":"markdown","cb9bd3c4":"markdown","91e7f461":"markdown","b2e7a066":"markdown","5553390f":"markdown","a2e537e0":"markdown","b56ae7cf":"markdown","d124a4b8":"markdown"},"source":{"b9b0bc2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # complex plotting\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a393490e":"# Load train data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndisplay(train_data.head())\n\n# Load test data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndisplay(test_data.head())","2bac76ec":"# Explore a pattern\ndef bar_chart(feature):\n    survived = train_data[train_data[\"Survived\"] == 1][feature].value_counts()\n    non_survived = train_data[train_data[\"Survived\"] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, non_survived])\n    df.index = ['Survived', 'None']\n    df.plot(kind ='bar', stacked=True, figsize=(10, 5))","a3d1306d":"# Problem Analysis\n# Explore a pattern\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)\n\nbar_chart('Sex')","309bedea":"# Explore a pattern\nyoung = train_data.loc[train_data.Age < 40][\"Survived\"]\nrate_young = sum(young)\/len(young)\nprint(\"% of young who survived:\", rate_young)\n\nold = train_data.loc[train_data.Age >= 40][\"Survived\"]\nrate_old = sum(old)\/len(old)\nprint(\"% of old who survived:\", rate_old)","9ad0fbea":"# Explore a pattern\npc1 = train_data.loc[train_data.Pclass == 1][\"Survived\"]\nrate_pc1 = sum(pc1)\/len(pc1)\nprint(\"% of Pclass 1 who survived:\", rate_pc1)\n\npc2 = train_data.loc[train_data.Pclass == 2][\"Survived\"]\nrate_pc2 = sum(pc2)\/len(pc2)\nprint(\"% of Pclass 2 who survived:\", rate_pc2)\n\npc3 = train_data.loc[train_data.Pclass == 3][\"Survived\"]\nrate_pc3 = sum(pc3)\/len(pc3)\nprint(\"% of Pclass 3 who survived:\", rate_pc3)\n\nbar_chart('Pclass')","d7a929b8":"# Explore a pattern\nfare = train_data.loc[train_data.Fare > 76][\"Survived\"]\nrate_fare = sum(fare)\/len(fare)\nprint(\"% of fare who survived:\", rate_fare)\n\nbar_chart('SibSp')","f0c6aa96":"bar_chart('Parch')","57284c7a":"bar_chart('Embarked')","543f4288":"# Data Integration\n# Inspect data to find inconsistency\ntrain_data.info()\n\nsns.heatmap(train_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')","fb01ae24":"# Inspect test data to find inconsistency\ntest_data.info()\n\nsns.heatmap(test_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')","f56c1701":"# Feature Engineering\n\n# Get name prefix\ndef get_titles(data):\n    return data.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n# update data (we no longer need full name)\ntrain_data['Title'] = get_titles(train_data['Name'])\ntest_data['Title'] = get_titles(test_data['Name'])\n\n# check\ntrain_data.head()","fa7142f3":"# Preprocessing & Feature Engineering\n\n# check unique name prefix from data for feature encoding\ndisplay(train_data['Title'].unique())\ndisplay(test_data['Title'].unique())\n\nbar_chart('Title')","16967566":"# Preprocessing & Feature Engineering\n# Ordinal encoding: create name prefix dictionary based on value\n# children gets most importance: Mr: 0, Miss: 1, Mrs: 2, Others: 0\nprefix_dict = {\n    'Mr': 0,\n    'Mrs': 2, \n    'Miss': 1,\n    'Master': 3,\n    'Don': 3,\n    'Rev': 3,\n    'Dr': 3,\n    'Mme': 3,\n    'Ms': 3,\n    'Major': 3,\n    'Lady': 3,\n    'Sir': 3,\n    'Mlle': 3,\n    'Col': 3, \n    'Capt': 3,\n    'the Countess': 3,\n    'Jonkheer': 3,\n    'Dona': 3 \n}\n\n# replace data\ntrain_data['Title'] = train_data['Title'].map(prefix_dict)\ntest_data['Title'] = test_data['Title'].map(prefix_dict)\n\ntrain_data.head()","73654184":"bar_chart('Title')","004a0fa9":"train_data.drop(['Name'], axis=1, inplace=True)\ntest_data.drop(['Name'], axis=1, inplace=True)","210ee0c1":"# Preprocessing & Feature Engineering\n# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle_train_data = LabelEncoder()\nle_test_data = LabelEncoder()\ntrain_data[\"Sex\"] = le_train_data.fit_transform(train_data[\"Sex\"])\ntest_data[\"Sex\"] = le_test_data.fit_transform(test_data[\"Sex\"])\n\ntrain_data.head()","5314a8f7":"# Age data\nplt.figure(figsize=(12, 7))\nsns.boxplot(x='Title',y='Age',data=train_data, palette='winter')","0bd7bafa":"# Preprocessing\n# Impute age columns based on median from passenger class\ntrain_data['Age'].fillna(train_data.groupby('Title')['Age'].transform('median'), inplace=True)\ntest_data['Age'].fillna(train_data.groupby('Title')['Age'].transform('median'), inplace=True)","e761cdc3":"sns.set_style('darkgrid')\nfacet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\n\nplt.show()","adedeccd":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","3b18b1e1":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\nplt.xlim(20, 30)","6cef8dac":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\nplt.xlim(30, 40)","178a3bb8":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\nplt.xlim(40, 60)","e4eb19d8":"# Binning\n# child = 0, young = 1, adult = 2, mid-age = 3, older = 4\ntrain_data['AgeGroup'] = pd.cut(train_data['Age'], bins=[0, 16, 26, 36, 62, 100], labels=False, precision=0)\ntest_data['AgeGroup'] = pd.cut(test_data['Age'], bins=[0, 16, 26, 36, 62, 100], labels=False, precision=0)\nbar_chart('AgeGroup')","ec87bc60":"bar_chart('Embarked')","dbb87ae8":"pc1 = train_data[train_data[\"Pclass\"] == 1][\"Embarked\"].value_counts()\npc2 = train_data[train_data[\"Pclass\"] == 2][\"Embarked\"].value_counts()\npc3 = train_data[train_data[\"Pclass\"] == 3][\"Embarked\"].value_counts()\ndf = pd.DataFrame([pc1, pc2, pc3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind ='bar', stacked=True, figsize=(10, 5))","0593b1a7":"# Preprocessing & Feature Engineering\n# Ordinal encoding of Embarked data\ntrain_data[\"Embarked\"].fillna('S', inplace=True)\ntest_data[\"Embarked\"].fillna('S', inplace = True)\n\nembarked_map = {\n    'S' : 0,\n    'C' : 1,\n    'Q' : 2\n}\n\n# replace data\ntrain_data['Embarked'] = train_data['Embarked'].map(embarked_map)\ntest_data['Embarked'] = test_data['Embarked'].map(embarked_map)\n\ntrain_data.head()","b6d90727":"# Fare\n# fill missing Fare data with mean value\ntest_data['Fare'].fillna(test_data.groupby('Pclass').transform('median')['Fare'], inplace=True)\n\nsns.heatmap(test_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')","69bdd95b":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train_data['Fare'].max()))\nfacet.add_legend()\n\nplt.show()","7c8fce0e":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train_data['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","354a8969":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train_data['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 30)","6e93fb01":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train_data['Fare'].max()))\nfacet.add_legend()\nplt.xlim(30, 100)","d9f3d1de":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train_data['Fare'].max()))\nfacet.add_legend()\nplt.xlim(100, 600)","0c59abd0":"# Binning\n# child = 0, young = 1, adult = 2, mid-age = 3, older = 4\ntrain_data['FareGroup'] = pd.cut(train_data['Fare'], bins=[-1, 17, 30, 100, 1000], labels=False, precision=0)\ntest_data['FareGroup'] = pd.cut(test_data['Fare'], bins=[-1, 17, 30, 100, 1000], labels=False, precision=0)\nbar_chart('FareGroup')","8c269d27":"train_data.drop(['Age', 'Fare'], axis=1, inplace=True)\ntest_data.drop(['Age', 'Fare'], axis=1, inplace=True)","4231e0c1":"# Preprocessing\n# Impute age column in train dataset with LinearRegression using other columns\n\"\"\"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\n\n# replace age\ndef ageRegression(data):\n    age_data = data.drop([\"Ticket\", \"Cabin\"], axis=1)\n\n    test_age_data = age_data[age_data['Age'].isna()].drop(['Age'], axis=1)\n\n    train_age_data = age_data[~age_data['Age'].isna()]\n    train_age = train_age_data[\"Age\"]\n    train_age_data_no_age = train_age_data.drop(['Age'], axis=1)\n\n    # Run Model to find \n    model = LinearRegression()\n    model.fit(train_age_data_no_age, train_age)\n    \n    # update test part\n    test_age_data['Age'] = np.abs(np.ceil(model.predict(test_age_data)))\n    # merge with train part and return\n    out = pd.concat([train_age_data, test_age_data], axis=0)\n    out[\"Ticket\"] = data[\"Ticket\"]\n    out[\"Cabin\"] = data[\"Cabin\"]\n    return out\n\ntrain_data_new = ageRegression(train_data)\ntest_data_new = ageRegression(test_data)\n\ndisplay(train_data_new.head())\ndisplay(test_data_new.head())\n\nsns.heatmap(train_data_new.isnull(), yticklabels=False, cbar=False, cmap='viridis')\"\"\"","066ea02a":"#!pip install datawig\n#import datawig","521a362d":"# Feature Engineering\n# Use only first character of Cabin for simplicity\ntrain_data['Cabin'] = train_data['Cabin'].str[:1]\ntest_data['Cabin'] = test_data['Cabin'].str[:1]\n\ntrain_data.head()","2fe12001":"pc1 = train_data[train_data[\"Pclass\"] == 1][\"Cabin\"].value_counts()\npc2 = train_data[train_data[\"Pclass\"] == 2][\"Cabin\"].value_counts()\npc3 = train_data[train_data[\"Pclass\"] == 3][\"Cabin\"].value_counts()\ndf = pd.DataFrame([pc1, pc2, pc3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind ='bar', stacked=True, figsize=(10, 5))","15296410":"# Feature Scaling\n# Ordinal encoding of Cabin data\ncabin_map = {\n    'A' : 0.0,\n    'B' : 0.4,\n    'C' : 0.8,\n    'D' : 1.2,\n    'E' : 1.6,\n    'F' : 2.0,\n    'G' : 2.4,\n    'T' : 2.8\n}\n\n# replace data\ntrain_data['Cabin'] = train_data['Cabin'].map(cabin_map)\ntest_data['Cabin'] = test_data['Cabin'].map(cabin_map)\n\ntrain_data.head()","5de5a45b":"# fill missing Cabin data with mean value\ntrain_data['Cabin'].fillna(train_data.groupby('Pclass')['Cabin'].transform('median'), inplace=True)\ntest_data['Cabin'].fillna(test_data.groupby('Pclass')['Cabin'].transform('median'), inplace=True)\n\ntrain_data.head()","3127dc8b":"# Preprocessing & Feature Engineering\n\n# add additional feature from: Parch & SibSp\ndef process_family(data):\n    data['Family'] = data['Parch'] + data['SibSp'] + 1 \n    return data\n\ntrain_data = process_family(train_data)\ntest_data = process_family(test_data)\n\ntrain_data.head()","b41da981":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Family', shade=True)\nfacet.set(xlim=(0, train_data['Family'].max()))\nfacet.add_legend()\n\nplt.show()","4198f1c2":"# Ordinal encoding of Family data\nfamily_map = { 1:0, 2:0.4, 3:0.8, 4:1.2, 5:1.6, 6:2, 7:2.4, 8:2.8, 9:3.2, 10:3.6, 11:4 }\n\n# replace data\ntrain_data['Family'] = train_data['Family'].map(family_map)\ntest_data['Family'] = test_data['Family'].map(family_map)\n\ntrain_data.head()","a1ec50fb":"train_data.drop(['Parch', 'SibSp'], axis=1, inplace=True)\ntest_data.drop(['Parch', 'SibSp'], axis=1, inplace=True)\n\ntrain_data.head()","d199ede8":"\"\"\"df_train, df_test = datawig.utils.random_split(train_data_new)\n\n# Initialize a SimpleImputer model\nimputer = datawig.SimpleImputer(\n    input_columns=['Survived','Pclass','Name','Age','SibSp','Parch','Fare','Sex_male','Embarked_Q','Embarked_S','Family'], # column(s) containing information about the column we want to impute\n    output_column= 'Cabin', # the column we'd like to impute values for\n    output_path = 'imputer_model' # stores model data and metrics\n    )\n\n# Fit an imputer model on the train data\nimputer.fit(train_df=df_train, num_epochs=10)\n\n# Impute missing train Cabin values and return original dataframe with predictions\nimputed_train = imputer.predict(train_data_new)\n\n# Impute missing test Cabin values\ndf_train, df_test = datawig.utils.random_split(test_data_new)\nimputer = datawig.SimpleImputer(\n    input_columns=['Pclass','Name','Age','SibSp','Parch','Fare','Sex_male','Embarked_Q','Embarked_S','Family'], # column(s) containing information about the column we want to impute\n    output_column= 'Cabin', # the column we'd like to impute values for\n    output_path = 'imputer_model' # stores model data and metrics\n    )\nimputer.fit(train_df=df_train, num_epochs=10)\nimputed_test = imputer.predict(test_data_new)\"\"\"","e90cad41":"\"\"\"# Cabin imputation using deep learning (Datawig)\ntrain_data_new['Cabin'].fillna(imputed_train['Cabin_imputed'], inplace=True)\ntest_data_new['Cabin'].fillna(imputed_test['Cabin_imputed'], inplace=True)\"\"\"","3a37b260":"\"\"\"# Preprocessing\n\n# Apply one hot encoding with k-1 columns\ntrain_data_Cabin = pd.get_dummies(train_data_new['Cabin'].str[0], drop_first=True, prefix='Cabin')\ntest_data_Cabin = pd.get_dummies(test_data_new['Cabin'].str[0], drop_first=True, prefix='Cabin')\n\n# Concat new columns to old datasets\ntrain_data_new = pd.concat([train_data_new.drop(['Cabin'], axis=1), train_data_Cabin], axis=1)\ntest_data_new = pd.concat([test_data_new.drop(['Cabin'], axis=1), test_data_Cabin], axis=1)\n\ndisplay(train_data_new.head())\n\ndisplay(test_data_new.head())\"\"\"","31c64cb1":"\"\"\"# Additional column to keep consistency\ntest_data_new['Cabin_T'] = 0\"\"\"","98e642eb":"# Preprocessing\n# Tag passsengers having special ticket numbers\ntrain_data['Special'] = train_data['Ticket'].apply(lambda x: 0 if x.split(' ')[0].isnumeric() else 1 )\ntest_data['Special'] = train_data['Ticket'].apply(lambda x: 0 if x.split(' ')[0].isnumeric() else 1 )\n\nbar_chart('Special')","e9a8b9d1":"# Extract first two digits of ticket data (Replace LINE with 0)\ntrain_data['Ticket'] = train_data['Ticket'].apply(lambda x: x.split(' ')[len(x.split(' ')) - 1]).apply(lambda x: 0 if x == \"LINE\" else x).apply(lambda x: str(x)[:2]).apply(lambda x: float(x)\/100)\ntest_data['Ticket'] = test_data['Ticket'].apply(lambda x: x.split(' ')[len(x.split(' ')) - 1]).apply(lambda x: str(x)[:2]).apply(lambda x: float(x)\/100)\n\n#train_data[~train_data['Ticket'].apply(lambda x: str(x).isnumeric())]\ntrain_data.head()","36b706f0":"facet = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nfacet.map(sns.kdeplot, 'Ticket', shade=True)\nfacet.set(xlim=(0, train_data['Ticket'].max()))\nfacet.add_legend()\n\nplt.show()","b5a9634f":"# binning\ntrain_data['TicketGroup'] = pd.cut(train_data['Ticket'], bins=[-1, 0.42, 0.84, 1], labels=False, precision=0)\ntest_data['TicketGroup'] = pd.cut(test_data['Ticket'], bins=[-1, 0.42, 0.84, 1], labels=False, precision=0)\n\nbar_chart('TicketGroup')","3f2aa77d":"passengerId = test_data['PassengerId']","98152e6e":"train_data.drop(['Ticket', 'PassengerId'], axis=1, inplace=True)\ntest_data.drop(['Ticket', 'PassengerId'], axis=1, inplace=True)\n\ntrain_data.head()","4bb34245":"train_data.info()","0cd1e03a":"# install packages uninstalled during datawig install\n\"\"\"!pip install 'scikit-learn==0.22.2.post1'\n!pip install 'typing==3.7.4.1'\n!pip install 'pandas==1.0.3'\n!pip install 'mxnet==1.6.0'\"\"\"\n","1b41f181":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier","b89a5cde":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","d9a4ea20":"# LogisticRegression\nclf = LogisticRegression(max_iter=1000)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# LogisticRegression Score\nprint(round(np.mean(score)*100, 2))","ceb13126":"# kNN\nclf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# kNN Score\nprint(round(np.mean(score)*100, 2))","ba6291db":"# DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# DecisionTreeClassifier Score\nprint(round(np.mean(score)*100, 2))","fe4a73ff":"# RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# RandomForestClassifier Score\nprint(round(np.mean(score)*100, 2))","372884a2":"# GradientBoostingClassifier\nclf = GradientBoostingClassifier(learning_rate = 0.1, \n                    max_depth = 2,\n                    min_samples_split = 10,\n                    n_estimators = 200,\n                    subsample = 0.6)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# GradientBoostingClassifier Score\nprint(round(np.mean(score)*100, 2))","7849372a":"# GaussianNB\nclf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# GaussianNB Score\nprint(round(np.mean(score)*100, 2))","5d9729b1":"# SVC\nclf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n\n# GaussianNB Score\nprint(round(np.mean(score)*100, 2))","a2eaff75":"clf = SVC()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)","7ceb9852":"output = pd.DataFrame({'PassengerId': passengerId, 'Survived': predictions})\noutput.to_csv('submission_svm.csv', index=False)\nprint(\"Submission data successfully saved!\")\n\nprint(\"Train accuracy: {}\".format(round(model.score(X_train, y_train), 4)))\n\noutput.head()","1e9d26da":"# Testing","1071209b":"# Logistic Regression","7b270895":"# Decision Tree","cb9bd3c4":"More than 50% of 1st class are from S embark\n\nMore than 50% of 2nd class are from S embark","91e7f461":"# Gradient Boosting","b2e7a066":"# **Cross Validation (K-fold)**","5553390f":"# Random Forest","a2e537e0":"# Naive Bayes","b56ae7cf":"# kNN","d124a4b8":"# SVM"}}