{"cell_type":{"756015b8":"code","ed4e478c":"code","18b6fe1a":"code","5a139d2f":"code","2b920ba4":"code","593cd42e":"code","75a2ad5e":"code","3fcd8e5b":"code","c2e8bc29":"code","f2a3f95b":"code","a6510d15":"code","a2b99843":"code","db89f490":"code","c12fb88b":"code","0168da7f":"code","32eba550":"code","f0b58193":"code","8c28b0f4":"code","0137c6d5":"code","5c54931e":"code","d7e1fa23":"code","d20fa495":"code","47ad694d":"code","f0324d4e":"code","f9b7012b":"code","e70502b2":"code","9c080442":"code","1583bd2a":"code","95014d78":"code","d8d8b469":"code","b52ba95f":"code","5143f5b5":"code","1b190a07":"code","98499f71":"code","f101aded":"code","6698ac5f":"markdown","05cc66df":"markdown","0a37b16a":"markdown","8bdf82e7":"markdown","1b10e467":"markdown","775f546d":"markdown","67f389fb":"markdown","7cf80dfa":"markdown","1c7701ff":"markdown","0496658d":"markdown","af62840c":"markdown","cda05749":"markdown","bbeef324":"markdown","4cb1c579":"markdown","2fa6f308":"markdown","d5b18c24":"markdown","9cc24aab":"markdown","4d1f72a6":"markdown","66e75e2e":"markdown","da99a735":"markdown","e9c44f9a":"markdown","41809b36":"markdown","bca64c8a":"markdown","16247bb3":"markdown","367a9f4a":"markdown","1e0cee3c":"markdown"},"source":{"756015b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ed4e478c":"\nimport numpy as np\nimport pandas as pd\nimport re as re\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier #KNN","18b6fe1a":"train_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")\ntrain_df.info()\n","5a139d2f":"train_df.describe()\n","2b920ba4":"train_df.head()","593cd42e":"test_df.head()","75a2ad5e":"train_df.columns.values","3fcd8e5b":"total=train_df.isnull().sum().sort_values()\npercent_1=train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values()\nmissing_data=pd.concat([total,percent_2],axis=1,keys=['total','percent'])\nprint(missing_data)\n","c2e8bc29":"total1=test_df.isnull().sum().sort_values()\npercent1_1=test_df.isnull().sum()\/test_df.isnull().count()*100\npercent1_2 = (round(percent1_1, 1)).sort_values()\nmissing_data1=pd.concat([total1,percent1_2],axis=1,keys=['total','percent'])\nprint(missing_data1)","f2a3f95b":"train_df[train_df['Embarked'].isnull()]","a6510d15":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna('S')\ntrain_df[train_df['Embarked'].isnull()]\n","a2b99843":"survived=\"Survived\"\nnot_survived=\"Not_survived\"\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title(\"Women\")\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Men')\n","db89f490":"train_df = train_df.drop(['PassengerId'], axis=1)\ntrain_df.head()","c12fb88b":"train_df.hist(bins=10,figsize=(9,7),grid=False);","0168da7f":"train_df.Embarked.value_counts().plot(kind='bar', alpha=0.55)\nplt.title(\"Passengers per boarding location\");","32eba550":"train_df.Age[train_df.Pclass == 1].plot(kind='kde')    \ntrain_df.Age[train_df.Pclass == 2].plot(kind='kde')\ntrain_df.Age[train_df.Pclass == 3].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') ;\n","f0b58193":"corr=train_df.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","8c28b0f4":"#train_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]+1\n#test_df[\"FamilySize\"]=test_df[\"SibSp\"] + test_df[\"Parch\"] +1\n#print(train_df[\"FamilySize\"].value_counts())","0137c6d5":"train_df = train_df.drop(['Name','Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Name','Ticket', 'Cabin'], axis=1)\ntrain_df.head()","5c54931e":"\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_df['Sex'] = train_df['Sex'].map(sex_mapping)\ntest_df['Sex'] = test_df['Sex'].map(sex_mapping)\n\ntrain_df.head()","d7e1fa23":"embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_df['Embarked'] = train_df['Embarked'].map(embarked_mapping)\ntest_df['Embarked'] = test_df['Embarked'].map(embarked_mapping)\n\ntrain_df.head()","d20fa495":"guess_ages = np.zeros((2,3))\nguess_ages\ncombine = [train_df, test_df]\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","47ad694d":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n","f0324d4e":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()\ntrain_df = train_df.drop(['AgeBand'], axis=1)","f9b7012b":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","e70502b2":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain_df['Fare'].value_counts()","9c080442":"train_df.head()\n","1583bd2a":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","95014d78":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","d8d8b469":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","b52ba95f":"\"\"\"knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\nknn.fit(X_train, Y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\"\"\"","5143f5b5":"#gradient boosting algorithm\nfrom sklearn.ensemble import GradientBoostingClassifier\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X_train, Y_train)\ny_pred = gradient_boost.predict(X_test)\n","1b190a07":"#extra trees classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X_train, Y_train)\ny_pred = ExtraTreesClassifier.predict(X_test)\n","98499f71":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","f101aded":"submission.to_csv('submission', index=False)","6698ac5f":"**ABOUT THE TITANIC**\n\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n**ABOUT THE DATA**\n\nThe data in the data set can be broadly classified into two types categorical and continous \nCategorical: Survived, Sex, and Embarked. Ordinal: Pclass.\nContinous: Age, Fare. Discrete: SibSp, Parch.\n","05cc66df":"we then classify the differnt ages of the passengers into age 7 groups\n","0a37b16a":"we try to compare the number of men and women who survived with the number f men and women. we can see that the number of men who survived are drastically less than women ","8bdf82e7":"\nImport the packages required while processing the data. the packages like numpy ,pandas are important while loading and preprocessing the data\nseaborn and matplotlib are some packages that are used to display the contents of the data set in a graphical\/visual manner.\n","1b10e467":"then we convert the other categorical variable embarked which has three values \"S\",\"C\" and \"Q\", as 1,2 and 3 respectively","775f546d":"we change the fare which is a float into a categorical variable by converting them between 6 differnt groups","67f389fb":"we find the number of missing values in each of the columns and we find out this in terms of % and display this in order to find the effect of missing values in the data set.","7cf80dfa":"**head():**\n\nPandas head() method is used to return top n (5 by default) rows of a data frame or series.","1c7701ff":"the passanger id is not necessarily required  fo perform analysis and the id does not provied any added value to the analysis","0496658d":"we fill the age column which we saw had 20% of the data missing by using random numbers between the mean ","af62840c":"after converting all the variables into categories and dropping the columns that are not required we have finished preprocessing our data","cda05749":"we find the coorelation between the values..What are the correlations useful for\nCorrelation can help in predicting one attribute from another (Great way to impute missing values).\nCorrelation can (sometimes) indicate the presence of a causal relationship.\nCorrelation is used as a basic quantity for many modelling techniques","bbeef324":"**describe()**\n\ndescribe() is used to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values.","4cb1c579":"we find where the most of the people in the titanic boarded and it shows most of the people boarded in \"S\" port","2fa6f308":"\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. \n\n**info()**\n\nusing info() displays the details of all the labels and the count of all the non null objects, (i.e).,Print a concise summary of a DataFrame.","d5b18c24":"**logistic regression** \nlogistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.","9cc24aab":"we find distribution of ages in different classes of passangers. \nthe most of the first class passangers were around 30-50\nthe people in the third class were much younger than the first and second class\n","4d1f72a6":"**columns:**\n\nreturns column names as an array (which is \u201can index\u201d)","66e75e2e":"we drop the column values like name ,tickets and cabin because we dont need them to build our model we drop them in both training and test sets","da99a735":"we can observe that the fare is 80.0 and it is a first class ticket category\nand most of the first class are embarked on port \"C\" so we assume they too were from port \"C\"\n","e9c44f9a":"**hist():**\n\nthe hist() calls the histogram function which displays all the details of columns each as a seperate histogram ","41809b36":"we convert the categorical variables into numerics and we first change the sex \nby considering male as 0 and female as 1","bca64c8a":"there are two columns \"SibSp\" , a combination of siblings and spouses which gives the number of siblings or spouses present and the \"parch\" which is the combination of the parents and childs accompanying a specific passanger\n","16247bb3":"we chcek whether the dimensions of the training set and test set match and if the match we use inbuilt sklearn packages to build models","367a9f4a":"we can see that nearly 77% of the data in the cabin column is missing so we must drop the column because filling it will result in errors\nthe age has 19% of its data missing but only 2 of the data from embarked are missing so we try to display the data","1e0cee3c":"**Decision tree**\nThis model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."}}