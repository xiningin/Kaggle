{"cell_type":{"f8e7f440":"code","2e266612":"code","3c630215":"code","a06024ac":"code","aa47c90a":"code","847b2ff2":"code","4cabba0b":"code","85094e3f":"code","582f1bca":"code","2fd399cf":"code","86dc7002":"code","820e32ce":"code","6c7b43e5":"code","95ba5562":"code","14c975ef":"code","65d15406":"code","4a02ba1d":"code","5d1c8c2c":"code","9aec88c9":"code","e445ad3c":"code","c4851fe1":"code","ba51b37d":"code","0f755272":"code","67cdcda1":"code","4c587ccc":"code","a4bdb708":"code","4a3d23b4":"code","5b733e4f":"code","d5a68ecf":"code","4086d99d":"code","1c41c147":"code","ea655157":"code","665819a6":"code","1521beb3":"code","958e0fe7":"code","d1a450bd":"code","6957e648":"code","d3e515ba":"code","75b32347":"code","237a9dc5":"code","01dba5e4":"code","4c9e4bdb":"code","5090f029":"code","ee9e9f39":"code","e929d574":"code","5991b46c":"code","d0663215":"code","f9c1345c":"code","ed382819":"code","529efe9d":"code","c7f4e9c6":"code","f50fedc0":"code","a52643ce":"code","29bf2bed":"code","e920eb09":"code","ab143236":"markdown","bb213b60":"markdown","e5e02a8a":"markdown","f0dab3d3":"markdown","3c79c92e":"markdown","73d853a6":"markdown","aabf53db":"markdown","3a2f8594":"markdown","2c213120":"markdown","9a9438e2":"markdown","c2e091f2":"markdown","87861a3d":"markdown","478b0ef2":"markdown","d244d1e0":"markdown","ad8593bb":"markdown","cddcc223":"markdown","2afb6a0d":"markdown","21eb52d1":"markdown","47949d27":"markdown","7170a535":"markdown","be50836b":"markdown","833df045":"markdown","45e11cf1":"markdown","ed34ed1a":"markdown","0ec86e19":"markdown","23f14c84":"markdown","2616b466":"markdown","a30f1a25":"markdown","88b8e941":"markdown"},"source":{"f8e7f440":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2e266612":"# Reading in the Test Data\ndf = pd.read_csv('..\/input\/train.csv')\ndf.head(2)","3c630215":"# Let's find out the shape\/size of our dataset\ndf.shape","a06024ac":"# Let's find out the data types and number of null values for each column\/feature\ndf.info()","aa47c90a":"df[df['Embarked'].isna()]","847b2ff2":"df['Age'].plot(kind='hist')","4cabba0b":"df['Age'].describe()","85094e3f":"# Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\nfig = plt.figure(figsize=(18,6))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((2,3), (0,0))\ndf['Survived'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Survival Distribution (Normalized)')\n\nplt.subplot2grid((2,3), (0,1))\ndf['Sex'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Gender Distribution (Normalized)')\n\nplt.subplot2grid((2,3), (0,2))\ndf['Pclass'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Pclass (Normalized)')\n\nplt.subplot2grid((2,3), (1,0))\ndf['SibSp'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('# of Siblings\/Spouses (Normalized)')\n\nplt.subplot2grid((2,3), (1,1))\ndf['Parch'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('# of Parents\/Children (Normalized)')\n\nplt.subplot2grid((2,3), (1,2))\ndf['Embarked'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Embarked')\nplt.show()","582f1bca":"fig = plt.figure(figsize=(18,15))\nplt.subplots_adjust(hspace=0.5)\n\nplt.subplot2grid((4,3), (0,0))\nmale = df[df['Sex']=='male']\nmale['Survived'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Survival for Men')\n\nplt.subplot2grid((4,3), (0,1))\nfemale = df[df['Sex']=='female']\nfemale['Survived'].value_counts(normalize=True, ascending=True).plot(kind='bar', alpha=0.7)\nplt.title('Survival for Women')\n\nplt.subplot2grid((4,3), (0,2))\ndf['Sex'][df['Survived']==1].value_counts(normalize=True, ascending=True).plot(kind='bar', alpha=0.7)\nplt.title('Survival based on Gender')\n\nplt.subplot2grid((4,3), (1,0), colspan=2)\nfor ticketclass in sorted(df['Pclass'].unique()):\n    df['Age'][df['Pclass']==ticketclass].plot(kind='kde')\nplt.legend(('1st','2nd','3rd'))\nplt.title('Ticket Class and Age')\n\nax = plt.subplot2grid((4,3), (1,2))\ndf.groupby(['Survived', 'Pclass']).size().unstack().plot(kind='bar', stacked=True, ax=ax)\nplt.title('Ticket Class and Survival')\n\nplt.subplot2grid((4,3), (2,0))\ndf['Survived'][(df['Sex']=='female') & (df['Pclass']==1)].value_counts(normalize=True, ascending=True).plot(kind='bar', alpha=0.7)\nplt.title('Rich Women Survival')\n\nplt.subplot2grid((4,3), (2,1))\ndf['Survived'][(df['Sex']=='male') & (df['Pclass']==3)].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Poor Men Survival')\n\nax = plt.subplot2grid((4,3), (2,2))\ndf.groupby(['Parch', 'Survived']).size().unstack().plot(kind='bar', stacked=True, ax=ax)\nplt.title('Family and Survival: Parent\/Child')\n\nplt.subplot2grid((4,3), (3,0), colspan=2)\nfor survived in sorted(df['Survived'].unique()):\n    df['Age'][df['Survived']==survived].plot(kind='kde')\nplt.legend(('Died','Survived'))\nplt.title('Survival and Age')\n\nax = plt.subplot2grid((4,3), (3,2))\ndf.groupby(['SibSp', 'Survived']).size().unstack().plot(kind='bar', stacked=True, ax=ax)\nplt.title('Family and Survival: Sibling\/Spouse')\n\nplt.show()","2fd399cf":"# Extract the titles from names\ntitles = df['Name'].str.extract(' ([A-za-z]+)\\.', expand=False)\n\n# Create a dataframe of titles and survival\ntitles_df = pd.DataFrame()\ntitles_df['Title'] = titles\ntitles_df['Survived'] = df['Survived']","86dc7002":"# Have a look at the raw numbers\ntitles_df.groupby(['Title', 'Survived']).size().unstack().T","820e32ce":"titles_df['Title'].value_counts().head(6)","6c7b43e5":"mask_titles = ['Mr', 'Miss', 'Mrs', 'Master', 'Rev', 'Dr']\ntitles_df = titles_df.loc[titles_df['Title'].isin(mask_titles), :]\ntitles_df.groupby(['Title', 'Survived']).size().unstack().plot(kind='bar')\nplt.show()","95ba5562":"age_eda = df.groupby(['Survived', 'Age']).size().rename('count').reset_index()","14c975ef":"sns.catplot(x=\"Survived\", y=\"Age\", hue=\"Survived\", kind=\"violin\", inner=\"stick\", data=age_eda);","65d15406":"del age_eda\nage_eda = df.copy()\nage_eda['Age'].dropna(inplace=True)\nage_eda['Age'] = age_eda['Age'].astype(int)\nage_eda.loc[ age_eda['Age'] <= 11, 'Age'] = 0\nage_eda.loc[(age_eda['Age'] > 11) & (age_eda['Age'] <= 18), 'Age'] = 1\nage_eda.loc[(age_eda['Age'] > 18) & (age_eda['Age'] <= 22), 'Age'] = 2\nage_eda.loc[(age_eda['Age'] > 22) & (age_eda['Age'] <= 27), 'Age'] = 3\nage_eda.loc[(age_eda['Age'] > 27) & (age_eda['Age'] <= 32), 'Age'] = 4\nage_eda.loc[(age_eda['Age'] > 32) & (age_eda['Age'] <= 40), 'Age'] = 5\nage_eda.loc[(age_eda['Age'] > 40), 'Age'] = 6","4a02ba1d":"age_eda.groupby(['Age', 'Survived']).size().unstack().plot(kind='bar')","5d1c8c2c":"# Imports for learning algorithms\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","9aec88c9":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain_test = train.append(test, sort=False)\n\ntrain_test.reset_index(inplace=True)\ntrain_test.drop(['index'], inplace=True, axis=1)\ntrain_test.head(2)","e445ad3c":"def add_titles(data):\n    # Extract titles from Name section\n    data['Title'] = data['Name'].str.extract(' ([A-za-z]+)\\.', expand=False)\n    \n    # Title dictionary based on EDA, key 5 effectively translates to 'other title'\n    title_mapping = {\"Mr\": 'Mr',\n                     \"Miss\": 'Miss',\n                     \"Mrs\": 'Mrs', \n                     \"Master\": 'Master',\n                     \"Rev\": 'Rev',\n                     \"Dr\": 'Other',\n                     \"Col\": 'Other',\n                     \"Major\": 'Other', \n                     \"Mlle\": 'Other',\n                     \"Countess\": 'Other',\n                     \"Ms\": 'Other',\n                     \"Lady\": 'Other',\n                     \"Jonkheer\": 'Other',\n                     \"Don\": 'Other',\n                     \"Dona\" : 'Other',\n                     \"Mme\": 'Other',\n                     \"Capt\": 'Other',\n                     \"Sir\": 'Other'}\n    \n    data['Title'] = data['Title'].map(title_mapping)","c4851fe1":"add_titles(train_test)","ba51b37d":"def clean_fare(data):\n    # Fare in the training set has no missing values but test set does\n    data['Fare'] = data['Fare'].fillna(data['Fare'].dropna().median())    ","0f755272":"clean_fare(train_test)","67cdcda1":"fig, ax = plt.subplots(figsize=(8,6))\nage_grouped = train_test.iloc[:891].groupby(['Sex','Pclass','Title'])\nage_grouped['Age'].median().unstack().plot(kind = 'bar', ax=ax)\nplt.show()","4c587ccc":"age_grouped = train_test.iloc[:891].groupby(['Sex','Pclass','Title'])\nage_grouped = age_grouped.median()\nage_grouped = age_grouped.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\nage_grouped.head(3)","a4bdb708":"def clean_age(data):\n    data[\"Age\"] = data.groupby(['Sex','Pclass','Title'])['Age'].transform(lambda x: x.fillna(x.median()))","4a3d23b4":"clean_age(train_test)","5b733e4f":"train_test.loc[train_test['Age'].isnull()]","d5a68ecf":"train_test.loc[979, 'Age'] = train_test['Age'].median()","4086d99d":"def bin_age(dataset):\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 32), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","1c41c147":"bin_age(train_test)","ea655157":"def clean_embarked(data):\n    # Fill missing values with most common embarkment point\n    data['Embarked'] = data['Embarked'].fillna('S')","665819a6":"clean_embarked(train_test)","1521beb3":"def clean_cabin(data):\n    # Fill na values with 'Unknown' or simply 'U'\n    data['Cabin'].fillna('U', inplace=True)\n    data['Cabin'] = data['Cabin'].map(lambda x: x[0])","958e0fe7":"clean_cabin(train_test)","d1a450bd":"def clean_family(data):\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)","6957e648":"clean_family(train_test)","d3e515ba":"# Add label and one-hot encoding for categorical lables\n\ndef encode(data, labels):\n    for label in labels:\n        data = data.join(pd.get_dummies(data[label], prefix = label))\n        data.drop(label, axis=1, inplace=True)\n    return data","75b32347":"store = train_test.copy()","237a9dc5":"train_test = encode(train_test, ['Pclass', 'Sex', 'Embarked', 'Title', 'Cabin'])\ntrain_test.head(1).T","01dba5e4":"train = train_test.loc[:890, :]\ntest = train_test.loc[891:, :]","4c9e4bdb":"def model(classifier, train, test):\n    target = train['Survived'].values\n    features = train.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis = 1).values\n    \n    scores = cross_val_score(classifier, features, target, cv=5)\n    print(f'Scores for 5 fold CV: {round(np.mean(scores*100))}')\n    \n    classifier_ = classifier.fit(features, target)\n    # print(classifier_.score(X_test, y_test))\n    test = test.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis = 1)\n    predictions = classifier_.predict(test).astype(int)\n    \n    return predictions","5090f029":"def submit(predictions):\n    submission = pd.read_csv('..\/input\/gender_submission.csv')\n    submission['Survived'] = predictions\n    submission.to_csv('submission.csv', index=False)\n    return submission","ee9e9f39":"classifier = linear_model.LogisticRegression(solver='liblinear')\npredictions = model(classifier, train, test)","e929d574":"classifier = SVC(gamma='auto', kernel='linear')\npredictions = model(classifier, train, test)","5991b46c":"classifier = DecisionTreeClassifier(random_state = 1, max_depth = 3)\npredictions = model(classifier, train, test)","d0663215":"classifier = GradientBoostingClassifier()\npredictions = model(classifier, train, test)","f9c1345c":"classifier = KNeighborsClassifier(3)\npredictions = model(classifier, train, test)","ed382819":"classifier = GaussianNB()\npredictions = model(classifier, train, test)","529efe9d":"classifier = AdaBoostClassifier()\npredictions = model(classifier, train, test)","c7f4e9c6":"classifier = RandomForestClassifier(n_estimators=100, max_depth = 7)\npredictions = model(classifier, train, test)","f50fedc0":"xg_test = test.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis = 1).as_matrix()\nxg_train = train.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis = 1).as_matrix()\ntarget = train['Survived'].values\n\n\nclassifier = XGBClassifier()\nclassifier_ = classifier.fit(xg_train, target)\nclassifier_.score(xg_train, target)","a52643ce":"predictions = classifier_.predict(xg_test).astype(int)","29bf2bed":"# X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1)","e920eb09":"submission = submit(predictions)\nsubmission['Survived'].value_counts()","ab143236":"Now, let's fill the missing values for the *Fare*","bb213b60":"Great, now let's write a function to fill the missing values of age in the training and test set with these 'smarter' statistics","e5e02a8a":"Beautiful, this shows us as that all passengers with title 'Rev' did not survived while passengers with title 'Mrs' and 'Miss' had a much higher likelihood  of survival. This added information could be useful for our model. Let's see if we can engineer some more features","f0dab3d3":"You could also use df['Age'].value_counts(). These functions do a great job of giving us more information regarding the most common values held, mean, standard deviation, etc. \n\nHowever, we humans (most of us?) are much better at getting a quick perception and summary of the data using visualizations. So now, let's create multiple plots so that we can observe the raw distribution of all relevant features in the same figure.","3c79c92e":"We find that there are missing values for 'Age', and 'Cabin'. 'Embarked' has a couple of missing values which shouldn't be an issue. \n\nI'm a bit curious so let's take a look at the rows that have the 'Embarked parameter missing. Perhaps we can use features in these rows and others to determine what the value of embarking point would have been.","73d853a6":"Great, but it will be better if we plot this information. Most of the titles here have only one or two values so let's use value_counts to find the most common titles ","aabf53db":"I have grouped the age by Gender, Ticket class, and Title to get the median age. We can use this information to fill the missing values in a smarter fashion.","3a2f8594":"The first step can be to write a function that adds titles for the passengers from the *Name* column, based on our findings in the EDA","2c213120":"First, let's import our models and helper functions. I will primarily be using the [sklearn](https:\/\/scikit-learn.org\/stable\/index.html) libray","9a9438e2":"* Now, let's write some functions to help clean our data. We want these to be generic and pragmatic. Note, that the test set also needs to go through the same steps. We should therefore, combine the training and test data\n\n* I will read the training data again and start modifying it. The reason for this is that I want to always be able to access the unmodified training data, stored in variable *df*. I could have just used *df.copy()* but this dataset is small and reading it again is a foolproof way of ensuring the data has not been maligned in any of the above steps. ","c2e091f2":"From the violon plot of Age and Survival, along with the KDE plot that was constructed during the bivariate analysis, we can conclude that there were certain age categories that fared worse. Let's bin the ages to see if we can get information that might help our model.","87861a3d":"Right then, so now we know that most of people died, the number of men are twice as many as the women, most people belonged to the Ticket Class 3, did not travel with their siblings\/spouses\/parents\/children, and embarked from Southampton. All this with a quick glance! You will notice I used percentage notion instead o just raw numbers, as this gives a better perception regarding the distribution :-)\n\nWe can expand this work establish relationships between independent features, if any. For example, what is the survival rate based on gender, or ticket class? To observe these bivariate relationships, let's use a similar subplot structure like we used to view the univariate distributions above.","478b0ef2":"Brilliant, time to test our first model. Let's create a function that separate the target\/dependent variable, drops meaningless rows and prints the k-fold cross-validation score","d244d1e0":"This tells us most of the people aboard the Titanic (or to be accurate, in this dataset) were in the age range of ~18-30, and we have a continuos decrease in frequency as we increase our age. Let us look at the distribution of the 'Age' feature in some more ways using methods available in Pandas.","ad8593bb":"Now, we have nu null values! We should convert these wide values held by age to categories based on our EDA above. The function below achieves this.","cddcc223":"Let's make sure there are no null values left","2afb6a0d":"Now, let's take a closer look at our features individually to observe their raw distribution.","21eb52d1":"There is a null value and it makes sense why! There was no one in the dataset that was a female, with a title as Ms. (Other in my mapping) and belonging to class 3. The plot confirms this too. Let's fill this passenger's age with the median age for the entire dataset.","47949d27":"Now let's work on the label encoding and one hot encoding for the categorical features in our dataset","7170a535":"From the above graphs, one can make a simple model, predict all women to survive. This will be an appropriate first submission before using any algorithms. It gives a score of 0.76555, a strong improvement on a coin toss. But our model wouldn't really be anything to boast about. For that, we must use some learning algorithms. But before that, we should try to perform some feature engineering on our dataset. The *Name* column is a good place to begin our experimentation at.","be50836b":"Let's work on filling the missing values for *Age*. But first, some more EDA to guide us!","833df045":"Run encoding on the combined train_test data","45e11cf1":"Before we fit our model to the training data, however, we must perform the following steps:\n\n1. Fill in missing values with an average representation of that feature in the dataset. \n2. Categorical variables like 'Sex', 'Embarked', etc. need to be converted to numeric values for the learning algorithm.\n3. Add the engineered features\n4. Drop meaningless features\/columns\n5. Perform [one hot encoding](https:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) so that the linear model can perform better.\n6. Import different classifiers","ed34ed1a":"Store a copy of the train_test data at this stage for workflow purposes","0ec86e19":"We are really starting to spot some trends with these graphs.\n\n1. From the first row of graphs, it is easy to see that the survival rate for men is much lower than that of women on the ship.\n2. The ticket class has a role to play as well. The older you are, the likelier it is that you were able to get a higher class seat. The plot adjacent to it shows that the first class had more people surviving than dying while quite the opposite was true for passengers of class 3.\n3. The first two plots of the third row show likelihood of survival for rich women (ticket class 1) and poor men (ticket class 3). An analysis based on fare would be interesting as well.\n4. From the kde plot, you can see that there if you were under a certain age, you had a higher chance for survival. The bar graphs in row 3 and 4 show that if you had some family, your chances of survival were better than the passenger who had no family on board","23f14c84":"Right then, let's plot the survival of passengers with the above titles and see if we can spot something","2616b466":"Separate train and test data!","a30f1a25":"Interesting, both passengers stayed were in the same cabin, belonged to first class, and survived! After some googling, I learnt that Miss Amelie was the maid to Mrs. Stone and they both boarded from Southamption.","88b8e941":"Let's extract information from the *Cabin* field by plucking the first character. For the plenty of missing values, we will replace the fields with unknowns."}}