{"cell_type":{"c535c6e4":"code","c4056a20":"code","69ce83a6":"code","0e9dbfdb":"code","89e874d0":"code","eaba6715":"code","3ae8bf56":"code","98609fda":"code","5fd507f5":"code","737dd36c":"code","a40abb4b":"code","44171744":"markdown"},"source":{"c535c6e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4056a20":"!pip install pyspark","69ce83a6":"import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()","0e9dbfdb":"dataTrain = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"\/kaggle\/input\/it2034ch1502-car-acceptability-prediction\/train.csv\")\ndataTest = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"\/kaggle\/input\/it2034ch1502-car-acceptability-prediction\/test.csv\")","89e874d0":"import pyspark.sql.functions as f\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.functions import regexp_replace\n\ndataTrain = dataTrain.withColumn('number_of_doors', regexp_replace(col(\"number_of_doors\"), \"5more\", \"5\"))\ndataTrain = dataTrain.withColumn('carry_capacity', regexp_replace(col(\"carry_capacity\"), \"more\", \"5\"))\n\ndataTrain = dataTrain.withColumn('number_of_doors', col(\"number_of_doors\").cast(\"Integer\"))\ndataTrain = dataTrain.withColumn('carry_capacity', col(\"carry_capacity\").cast(\"Integer\"))\n\ndataTrain.printSchema()\ndataTrain.show(5)","eaba6715":"dataTest = dataTest.withColumn('number_of_doors', regexp_replace(col(\"number_of_doors\"), \"5more\", \"5\"))\ndataTest = dataTest.withColumn('carry_capacity', regexp_replace(col(\"carry_capacity\"), \"more\", \"5\"))\n\ndataTest = dataTest.withColumn('number_of_doors', col(\"number_of_doors\").cast(\"Integer\"))\ndataTest = dataTest.withColumn('carry_capacity', col(\"carry_capacity\").cast(\"Integer\"))","3ae8bf56":"from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import VectorAssembler\n\nlabelIndexer = StringIndexer(inputCol=\"acceptability\",outputCol=\"label\")\nlabelIndexerModel = labelIndexer.fit(dataTrain)\n\nlabelIndexedTrainData = labelIndexerModel.transform(dataTrain)\nlabelIndexedTrainData.show(5)\nlabelIndexedTrainData.printSchema()","98609fda":"# defined indexer to collect features col\nbuyingIndexer = StringIndexer(inputCol=\"buying_price\",outputCol=\"buyingPriceIndexed\")\nmaintenancePriceIndexer = StringIndexer(inputCol=\"maintenance_price\",outputCol=\"maintenancePriceIndexed\")\nsafetyIndexer = StringIndexer(inputCol=\"safety\",outputCol=\"safetyIndexed\")\ntrunkSizeIndexer = StringIndexer(inputCol=\"trunk_size\",outputCol=\"trunkSizeIndexed\")\n\n#numberDoorsIndexer = StringIndexer(inputCol=\"number_of_doors\",outputCol=\"numberDoorsIndexed\")\n#carryCapacityIndexer = StringIndexer(inputCol=\"carry_capacity\",outputCol=\"carryCapacityIndexed\")\n\nnumericCols = [\"number_of_doors\", \"carry_capacity\"]\n\nindexedCols = [\"buyingPriceIndexed\", \"maintenancePriceIndexed\", \"safetyIndexed\", \"trunkSizeIndexed\"]\nvecCols = [\"buyingPriceVec\", \"maintenancePriceVec\", \"safetyVec\", \"trunkSizeVec\"]\n\n#encoder = OneHotEncoder(inputCols=indexedCols, outputCols=vecCols)\n#assembler = VectorAssembler(inputCols=vecCols + numericCols, outputCol=\"features\")\n\nassembler = VectorAssembler(inputCols=indexedCols + numericCols, outputCol=\"features\")","5fd507f5":"from pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import NaiveBayes\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nfrom pyspark.ml import Pipeline\n\ndecisionTreeClassifier = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth=30)\nrandomForestClassifier = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', maxDepth=30, numTrees=20,seed=5043)\nnaiveBayes = NaiveBayes(featuresCol = 'features', labelCol = 'label',smoothing=1.0, modelType=\"multinomial\")\n\npipeline = Pipeline(stages=[\n                buyingIndexer,\n                maintenancePriceIndexer,\n                safetyIndexer,\n                trunkSizeIndexer,\n                assembler,\n                randomForestClassifier])\n\nmodel = pipeline.fit(labelIndexedTrainData)\npredictions = model.transform(dataTest)","737dd36c":"# export file solution\nlabelArray = [\"unacc\", \"acc\", \"good\", \"vgood\"]\n\nsolution = predictions.select('car_id', 'prediction')\nsolution = IndexToString(inputCol=\"prediction\", outputCol=\"acceptability\", labels=labelArray).transform(solution)\nsolution.show()","a40abb4b":"dumpSolution = solution.select('car_id', 'acceptability')\ndumpSolution.toPandas().to_csv(\"dump_solution_2.csv\", header=True, index=False)","44171744":"# Nh\u00f3m: Duy + Hi\u1ec1n + Ph\u00e1n\nCH2002006 - Tr\u1ea7n Ph\u01b0\u1edbc Duy \\\nCH2001005 - Cao Nguy\u1ec5n Nam Hi\u1ec1n \\\nCH2004020 - Tr\u1ea7n L\u01b0\u01a1ng Ph\u00e1n"}}