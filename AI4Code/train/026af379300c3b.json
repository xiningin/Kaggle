{"cell_type":{"4ef95ca6":"code","60b93cd5":"code","f6eaa41c":"code","7c9e4cb6":"code","b86c0718":"code","c32bd438":"code","4ec0d625":"code","2c401e21":"code","21ce6d05":"code","17cbe677":"code","fb33ab14":"code","c41bc6ad":"code","300db278":"code","d9fc82a5":"code","bc3cc28c":"code","75d29d7d":"code","9b6f1069":"code","3b59e5dc":"code","ae8609f8":"code","784a233a":"code","ce1eba35":"code","5dae0a88":"code","18a46be1":"code","2877450e":"code","adab915f":"code","0950e4d5":"code","68ff9428":"code","83726d5e":"code","821ef5fc":"code","8cf76464":"code","9858922e":"code","0090c02d":"code","c3b741ce":"code","5695c952":"code","f4dbf9b3":"code","f1e7e04e":"code","c65f2de3":"code","b9be1e0a":"code","074599ad":"code","252042ac":"code","73178547":"code","8970247a":"code","f8bf6d79":"code","b1afe646":"code","14397d26":"code","2c6c7adc":"code","7bdb11f2":"code","791e5481":"code","280e0f5a":"code","d9a450ed":"code","833e399a":"code","63188f00":"code","9888112d":"code","17990bff":"code","a0f4ebae":"code","0fdfd67c":"code","7bb6488c":"code","85b48333":"code","4aaee5f9":"code","f6fe76a2":"code","4a77f80f":"code","daf1c233":"code","a544311d":"code","0e70f43b":"code","181687c8":"code","3730ec5b":"code","15778977":"code","d70d225f":"code","6e777080":"code","966226cc":"code","8e696e4c":"code","c9221e50":"code","768bb34d":"code","0c033203":"code","ee5c8849":"code","2a464d97":"code","7bc8cd05":"code","ad43fea0":"code","d271fea5":"code","b74e8f7a":"code","fea3dc50":"code","4b8d3c90":"code","1cd30371":"code","0e472ce5":"code","281ab833":"code","5f6273e8":"code","b0b1b4ab":"code","1f9c263a":"code","67cff6ef":"code","6e124451":"code","78b03063":"code","4b32259e":"code","63d645c3":"code","c8d0e36b":"code","8f338668":"code","d1650b63":"code","2339791d":"code","b3e2e9e1":"code","3a1f0e23":"code","b357db23":"markdown","e97f10b5":"markdown","807bf241":"markdown","2caac2ae":"markdown","55dec1b7":"markdown","55ab8d76":"markdown","2882be1b":"markdown","1252206c":"markdown","b08f51d7":"markdown","f33ba746":"markdown","e0ee47ac":"markdown","7d0e4b0a":"markdown","4488ae2d":"markdown","ce9c613e":"markdown","edc46c6f":"markdown","56483c92":"markdown","e25848b5":"markdown","1c83b696":"markdown","85d54843":"markdown","c79b7906":"markdown","80ebe1cf":"markdown","da6c25a1":"markdown","d7a42250":"markdown","2763e6a7":"markdown","3a4e3a9a":"markdown","10c8f04b":"markdown","df3a3aa6":"markdown","ce755d88":"markdown","b028e952":"markdown","52e4f469":"markdown","62f2933a":"markdown","e822efea":"markdown","8fe2703c":"markdown","5b8fd5cc":"markdown","df44873b":"markdown","f6ede2e8":"markdown","5f4e3daa":"markdown","acf21374":"markdown","ba6beca8":"markdown","a4ddfb8d":"markdown","c1f12b7f":"markdown","0803b392":"markdown","73dfb92c":"markdown","5035399c":"markdown","0dd05234":"markdown","e3c28417":"markdown","578a7da6":"markdown","00ae42bc":"markdown","64591100":"markdown","87407d3c":"markdown","556bcbe6":"markdown","66ed95b0":"markdown","44fdb771":"markdown","89a36779":"markdown","1441a9e8":"markdown","2b5173bd":"markdown","6fed4ebc":"markdown","c15aa1eb":"markdown","6118f5e7":"markdown","b896dc1e":"markdown"},"source":{"4ef95ca6":"# Setup\n# importing libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime, date\nfrom dateutil.relativedelta import relativedelta\n\n#Ignore Depreciation Warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n","60b93cd5":"#Load Dataset\ndf=pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep='\\t')","f6eaa41c":"#Get dimensions of dataset\ndf.shape","7c9e4cb6":"#View first 5 records\ndf.head()","b86c0718":"#Get statistical summary of columns in datasets\ndf.describe()","c32bd438":"#1)Checking for NaN 'NULL' values\ndf.isnull().sum()","4ec0d625":"df['Income'].fillna(df['Income'].mean(), inplace = True)\nprint(df.isnull().sum())","2c401e21":"#2)Checking for correctly assigned datatype (This helps in faster processing)\ndf.info()","21ce6d05":"#Converting datetime\ndf['Dt_Customer']= pd.to_datetime(df['Dt_Customer'])\ndf.info()","17cbe677":"#3) Checking for duplicate values\ndf.duplicated().sum()","fb33ab14":"#4)Checking for skewed data\ndf.skew()","c41bc6ad":"#Dropping columns Z_revenue and Z_CostContact\ndf.drop('Z_Revenue', inplace=True, axis=1)\ndf.drop('Z_CostContact', inplace=True, axis=1)\ndf.drop('Complain', inplace=True, axis=1)\ndf.info()","300db278":"#hist=df.hist(figsize=(17,15), color='crimson', bins=15)\ndf.hist(bins=20, figsize=(20,15))\nplt.show()","d9fc82a5":"#Computing correlation matrix\ndf.corr()","bc3cc28c":"\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,annot=True, cbar_kws={\"shrink\": .5})","75d29d7d":"df[\"Year_Birth\"] = 2021-df[\"Year_Birth\"]\ndf.rename(columns = {'Year_Birth':'Age'},inplace = True)\ndf[\"Age\"].head()","9b6f1069":"def Days_Since_enrollment(start_date):\n    start_date = start_date.date()\n    today = date.today()\n    difference_in_years = relativedelta(today, start_date).years\n    return  difference_in_years\n  \ndf['Years_Since_Customer'] = df['Dt_Customer'].apply(Days_Since_enrollment)\ndf[[\"Dt_Customer\",\"Years_Since_Customer\"]].head()\n","3b59e5dc":"df[\"Total_Amount_Spent\"] = df[\"MntWines\"]+ df[\"MntFruits\"]+ df[\"MntMeatProducts\"]+ df[\"MntFishProducts\"]+ df[\"MntSweetProducts\"]+ df[\"MntGoldProds\"]\ndf[\"Total_Amount_Spent\"].head()\ndf[\"MntWines_%\"] = (df[\"MntWines\"] \/ df[\"Total_Amount_Spent\"] ) \ndf[\"MntFruits_%\"] = (df[\"MntFruits\"] \/ df[\"Total_Amount_Spent\"] ) \ndf[\"MntMeatProducts_%\"] = (df[\"MntMeatProducts\"] \/ df[\"Total_Amount_Spent\"] ) \ndf[\"MntFishProducts_%\"] = (df[\"MntFishProducts\"] \/ df[\"Total_Amount_Spent\"] ) \ndf[\"MntSweetProducts_%\"] = (df[\"MntSweetProducts\"] \/ df[\"Total_Amount_Spent\"] )\ndf[\"MntGoldProds_%\"] = (df[\"MntGoldProds\"] \/ df[\"Total_Amount_Spent\"] ) \ndf[[\"MntWines_%\",\"MntFruits_%\",\"MntMeatProducts_%\",\"MntFishProducts_%\",\"MntSweetProducts_%\",\"MntGoldProds_%\"]].head()","ae8609f8":"df[\"Total_Purchase_Count\"] = df[\"NumDealsPurchases\"]+ df[\"NumWebPurchases\"]+ df[\"NumCatalogPurchases\"]+ df[\"NumStorePurchases\"]\ndf[\"Total_Purchase_Count\"].head()\ndf[\"NumDealsPurchases_%\"] = (df[\"NumDealsPurchases\"] \/ df[\"Total_Purchase_Count\"] )\ndf[\"NumWebPurchases_%\"] = (df[\"NumWebPurchases\"] \/ df[\"Total_Purchase_Count\"] )\ndf[\"NumCatalogPurchases_%\"] = (df[\"NumCatalogPurchases\"] \/ df[\"Total_Purchase_Count\"] )\ndf[\"NumStorePurchases_%\"] = (df[\"NumStorePurchases\"] \/ df[\"Total_Purchase_Count\"] )\ndf[[\"NumDealsPurchases_%\",\"NumWebPurchases_%\",\"NumCatalogPurchases_%\",\"NumStorePurchases_%\"]].head()","784a233a":"df[\"Campaign_Success_Degree\"] = (df[\"AcceptedCmp1\"] * 5) + (df[\"AcceptedCmp2\"] * 4) + (df[\"AcceptedCmp3\"]* 3) + (df[\"AcceptedCmp4\"] * 2) + (df[\"AcceptedCmp5\"] * 1)\ndf[\"Campaign_Success_Degree\"].head(100)","ce1eba35":"#Generalize Marital Status value into two categories\ndf['Marital_Status'].unique()","5dae0a88":"\ndf['Relationship'] = df[\"Marital_Status\"].replace({\"Married\":\"Married\", \"Together\":\"Married\", \"Absurd\":\"Alone\", \"Widow\":\"Married\", \"YOLO\":\"Alone\", \"Divorced\":\"Married\", \"Single\":\"Alone\",})","18a46be1":"#Find total number of children, and find if parent\ndf[\"Children\"] = df[\"Kidhome\"] + df[\"Teenhome\"]\n#df[\"Children\"] = df[\"Kidhome\"]\n#df[\"Children\"] = df[\"Kidhome\"]\ndf[\"Is_Parent\"] = np.where(df.Children> 0, 1, 0) # for data analysis purpose","2877450e":"#Convert category into ordinal values\ndf['Education'].unique()","adab915f":"\n#df['Education'] = df[\"Education\"].replace({\"Graduation\":\"Graduated\", \"PhD\":\"Graduated\", \"Basic\":\"Not_Graduated\", \"2n Cycle\":\"Graduated\"})\ndf['Education'] = df[\"Education\"].replace({\"Graduation\":2, \"PhD\":5, \"Basic\":1, \"2n Cycle\":3,\"Master\":4})\ndf['Education'] = df[\"Education\"].astype(str).astype(int)","0950e4d5":"#Convert income datatype to int\ndf['Income'] = df[\"Income\"].astype(int)","68ff9428":"df.info()","83726d5e":"y = df[[\"Age\",\"Income\",\"Years_Since_Customer\"]]\nfor i in y.columns:\n    sns.boxplot(x = i, data = y,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","821ef5fc":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in y.columns:\n    count_outliers(df,i)","8cf76464":"df = df[(df['Age']<100)]\ndf = df[(df['Income']<600000)]\ndf = df[(df['Years_Since_Customer']<9)]\ndf_all_features = df","9858922e":"sns.catplot(x=\"Age\",y=\"Marital_Status\",data=df,kind='box',height=5, aspect=2)","0090c02d":"sns.catplot(x=\"Marital_Status\",y=\"Income\",kind='box',data=df,height=5, aspect=2)","c3b741ce":"sns.catplot(x=\"Education\",y=\"Income\",kind='box',data=df,height=5, aspect=2)","5695c952":"sns.catplot(x=\"Age\",y=\"Total_Amount_Spent\",kind='box',data=df,height=5, aspect=5)","f4dbf9b3":"sns.catplot(x=\"Total_Purchase_Count\",y=\"Total_Amount_Spent\",kind='box',data=df,height=5, aspect=5)","f1e7e04e":"sns.catplot(x=\"NumWebVisitsMonth\",y=\"Total_Amount_Spent\",kind='box',data=df,height=5, aspect=5)","c65f2de3":"\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(30, 30))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,annot=True, cbar_kws={\"shrink\": .5})","b9be1e0a":"#Removing columns\ndf.drop(['ID','Dt_Customer','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5'], inplace=True, axis=1)\ndf.drop(['Recency','Marital_Status','Kidhome','Teenhome','Children'], inplace=True, axis=1)\n#df.drop(['Recency','Complain','Marital_Status','Kidhome','Children'], inplace=True, axis=1)\n","074599ad":"\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,annot=True, cbar_kws={\"shrink\": .5})","252042ac":"df.info()","73178547":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['Relationship']=le.fit_transform(df['Relationship'])\n#df['Education']=le.fit_transform(df['Education'])","8970247a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nsc = StandardScaler()\nscaler = MinMaxScaler()\ndf_copy = df.copy()\ncols = ['Income','Age','Total_Amount_Spent','Total_Purchase_Count']\n#df_copy[cols] = sc.fit_transform(df_copy[cols])\ndf_copy[cols] = scaler.fit_transform(df_copy[cols])","f8bf6d79":"\ncorr = df_copy.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,annot=True, cbar_kws={\"shrink\": .5})","b1afe646":"df_copy.info()","14397d26":"df_copy = df_copy.dropna()","2c6c7adc":"df_copy.isnull().sum()","7bdb11f2":"#sns.pairplot(df_copy,hue='Is_Parent')\nprepared_df = df_copy","791e5481":"#Identify dependenet and independent variables\n\ntarget1 = ['Is_Parent']\nfeatures1 = [ 'Income', 'NumWebVisitsMonth', 'Response', 'Total_Amount_Spent', 'MntWines_%',\n       'MntFruits_%', 'MntMeatProducts_%', 'MntFishProducts_%',\n       'MntSweetProducts_%', 'MntGoldProds_%', 'Total_Purchase_Count',\n       'NumDealsPurchases_%', 'NumWebPurchases_%', 'NumCatalogPurchases_%',\n       'NumStorePurchases_%', 'Campaign_Success_Degree', 'Relationship']","280e0f5a":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.20,random_state=0)","d9a450ed":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nKNN_model = KNeighborsClassifier(n_neighbors=5)\nSVC_model = SVC()\nKNN_model.fit(X_train1, y_train1)\nSVC_model.fit(X_train1, y_train1)\nKNN_prediction = KNN_model.predict(X_test1)\nSVC_prediction = SVC_model.predict(X_test1)\nprint(classification_report(KNN_prediction, y_test1))\nprint(classification_report(SVC_prediction, y_test1))","833e399a":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier().fit(X_train1, y_train1)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train1, y_train1)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test1, y_test1)))","63188f00":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train1, y_train1)\nprint('Accuracy of LDA classifier on training set: {:.2f}'\n     .format(lda.score(X_train1, y_train1)))\nprint('Accuracy of LDA classifier on test set: {:.2f}'\n     .format(lda.score(X_test1, y_test1)))","9888112d":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train1, y_train1)\nprint('Accuracy of GNB classifier on training set: {:.2f}'\n     .format(gnb.score(X_train1, y_train1)))\nprint('Accuracy of GNB classifier on test set: {:.2f}'\n     .format(gnb.score(X_test1, y_test1)))","17990bff":"from sklearn.linear_model import LogisticRegression\n#logisticRegr = LogisticRegression()\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train1, y_train1)\npredictions = model.predict(X_test1)\nscore = model.score(X_test1, y_test1)\nprint(score)","a0f4ebae":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nimport matplotlib.cm as cm\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib.colors import LogNorm\n","0fdfd67c":"df_copy.info()","7bb6488c":"\ncorr = df_copy.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,annot=True, cbar_kws={\"shrink\": .5})","85b48333":"df_wopca = df_copy\ndf_wpca = df_copy\n\n#WIthout PCA Decomposition\ncluster = KMeans(n_clusters = 2).fit_predict(df_wopca)\ndf_wopca['cluster'] =cluster\n\n#With PCA Decomposition\npcaDecom = PCA(n_components = 2)\ndf_PCA = pd.DataFrame(data = pcaDecom.fit_transform(df_wpca), columns = ['PC_1', 'PC_2'])\ncluster_PCA = KMeans(n_clusters = 2).fit_predict(df_PCA)\ndf_PCA['cluster_PCA'] = cluster_PCA\n\n\nprint(\"KMeans clustering without PCA [number of clusters as 2]\")\n#print(\"Cluster Score\",cluster.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_wopca['Total_Amount_Spent'], df_wopca['Income'], c=df_wopca[\"cluster\"], marker='o',s=50,cmap = 'brg' )\nplt.show()\n\nprint(\"KMeans clustering with PCA [number of clusters as 2]\")\n#print(\"Cluster Score\",cluster_PCA.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_PCA['PC_1'], df_PCA['PC_2'], c=df_PCA[\"cluster_PCA\"], marker='o',s=50,cmap = 'brg' )\nplt.show()","4aaee5f9":"df_wopca = df_copy\ndf_wpca = df_copy\n\n#WIthout PCA Decomposition\ncluster = KMeans(n_clusters = 3).fit_predict(df_wopca)\ndf_wopca['cluster'] =cluster\n\n#With PCA Decomposition\npcaDecom = PCA(n_components = 2)\ndf_PCA = pd.DataFrame(data = pcaDecom.fit_transform(df_wpca), columns = ['PC_1', 'PC_2'])\ncluster_PCA = KMeans(n_clusters = 3).fit_predict(df_PCA)\ndf_PCA['cluster_PCA'] = cluster_PCA\n\n\nprint(\"KMeans clustering without PCA [number of clusters as 3]\")\n#print(\"Cluster Score\",cluster.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_wopca['Total_Amount_Spent'], df_wopca['Income'], c=df_wopca[\"cluster\"], marker='o',s=50,cmap = 'brg' )\nplt.show()\n\nprint(\"KMeans clustering with PCA [number of clusters as 3]\")\n#print(\"Cluster Score\",cluster_PCA.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_PCA['PC_1'], df_PCA['PC_2'], c=df_PCA[\"cluster_PCA\"], marker='o',s=50,cmap = 'brg' )\nplt.show()","f6fe76a2":"df_wopca = df_copy\ndf_wpca = df_copy\n\n#WIthout PCA Decomposition\ncluster = KMeans(n_clusters = 4).fit_predict(df_wopca)\ndf_wopca['cluster'] =cluster\n\n#With PCA Decomposition\npcaDecom = PCA(n_components = 2)\ndf_PCA = pd.DataFrame(data = pcaDecom.fit_transform(df_wpca), columns = ['PC_1', 'PC_2'])\ncluster_PCA = KMeans(n_clusters = 4).fit_predict(df_PCA)\ndf_PCA['cluster_PCA'] = cluster_PCA\n\n\nprint(\"KMeans clustering without PCA [number of clusters as 4]\")\n#print(\"Cluster Score\",cluster.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_wopca['Total_Amount_Spent'], df_wopca['Income'], c=df_wopca[\"cluster\"], marker='o',s=50,cmap = 'brg' )\nplt.show()\n\nprint(\"KMeans clustering with PCA [number of clusters as 4]\")\n#print(\"Cluster Score\",cluster_PCA.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_PCA['PC_1'], df_PCA['PC_2'], c=df_PCA[\"cluster_PCA\"], marker='o',s=50,cmap = 'brg' )\nplt.show()","4a77f80f":"df_wopca = df_copy\ndf_wpca = df_copy\n\n#WIthout PCA Decomposition\ncluster = KMeans(n_clusters = 5).fit_predict(df_wopca)\ndf_wopca['cluster'] =cluster\n\n#With PCA Decomposition\npcaDecom = PCA(n_components = 2)\ndf_PCA = pd.DataFrame(data = pcaDecom.fit_transform(df_wpca), columns = ['PC_1', 'PC_2'])\ncluster_PCA = KMeans(n_clusters = 5).fit_predict(df_PCA)\ndf_PCA['cluster_PCA'] = cluster_PCA\n\n\nprint(\"KMeans clustering without PCA [number of clusters as 5]\")\n#print(\"Cluster Score\",cluster.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_wopca['Total_Amount_Spent'], df_wopca['Income'], c=df_wopca[\"cluster\"], marker='o',s=50,cmap = 'brg' )\nplt.show()\n\nprint(\"KMeans clustering with PCA [number of clusters as 5]\")\n#print(\"Cluster Score\",cluster_PCA.inertia_)\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111)\nax.scatter(df_PCA['PC_1'], df_PCA['PC_2'], c=df_PCA[\"cluster_PCA\"], marker='o',s=50,cmap = 'brg' )\nplt.show()","daf1c233":"elbow= KElbowVisualizer(KMeans(), k=10)\nelbow.fit(df_copy)\nelbow.show();","a544311d":"from yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nfor i in [2, 3, 4, 5]:\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(i, 2)\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    visualizer.fit(df_copy)","0e70f43b":"km = KMeans(n_clusters=2, random_state=42)\nkm.fit_predict(df_copy)\nscore = silhouette_score(df_copy, km.labels_, metric='euclidean')\nprint('Silhouetter Score [n=2]: %.3f' % score)\nkm = KMeans(n_clusters=3, random_state=42)\nkm.fit_predict(df_copy)\nscore = silhouette_score(df_copy, km.labels_, metric='euclidean')\nprint('Silhouetter Score [n=3]: %.3f' % score)\nkm = KMeans(n_clusters=4, random_state=42)\nkm.fit_predict(df_copy)\nscore = silhouette_score(df_copy, km.labels_, metric='euclidean')\nprint('Silhouetter Score [n=4]: %.3f' % score)\nkm = KMeans(n_clusters=5, random_state=42)\nkm.fit_predict(df_copy)\nscore = silhouette_score(df_copy, km.labels_, metric='euclidean')\nprint('Silhouetter Score [n=5]: %.3f' % score)","181687c8":"df_kmeanscluster = df_copy\n\ncluster = KMeans(n_clusters = 5).fit_predict(df_kmeanscluster)\ndf_kmeanscluster['cluster'] =cluster","3730ec5b":"fig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d')\nax.scatter(df_kmeanscluster['Age'], df_kmeanscluster['Education'],df_kmeanscluster['Income'], c=df_kmeanscluster[\"cluster\"], marker='o',s=50,cmap = 'brg' )\nplt.show()","15778977":"sns.boxenplot(x = 'cluster' , y ='Total_Amount_Spent' ,data = df_kmeanscluster);","d70d225f":"# Spent vs Products\nProduct_vars = ['MntWines_%',\n       'MntFruits_%', 'MntMeatProducts_%', 'MntFishProducts_%', 'MntSweetProducts_%',\n       'MntGoldProds_%']\n\nfor i in Product_vars:\n    plt.figure(figsize = (7,7))\n    sns.barplot(x  = 'cluster' , y = i,data = df_kmeanscluster)\n    plt.show()","6e777080":"df_kmeanscluster.info()","966226cc":"# Spent vs Products\npurchase_channel = ['NumDealsPurchases_%',\n'NumWebPurchases_%',\n'NumCatalogPurchases_%',\n'NumStorePurchases_%']\n\nfor i in purchase_channel:\n    plt.figure(figsize = (7,7))\n    sns.barplot(x  = 'cluster' , y = i,data = df_kmeanscluster)\n    plt.show()","8e696e4c":"df_kmeanscluster.info()","c9221e50":"# Spent vs Products\npersonal_attributes = ['Age','Education','Income','Years_Since_Customer','Is_Parent','Relationship','NumWebVisitsMonth','Total_Amount_Spent']\n\nfor i in personal_attributes:\n    plt.figure(figsize = (7,7))\n    sns.barplot(x  = 'cluster' , y = i,data = df_kmeanscluster)\n    plt.show()","768bb34d":"# Spent vs Products\ncampaign_success = ['Campaign_Success_Degree']\n\nfor i in campaign_success:\n    plt.figure(figsize = (7,7))\n    sns.barplot(x  = 'cluster' , y = i,data = df_kmeanscluster)\n    plt.show()","0c033203":"for i in df_kmeanscluster:\n    g = sns.FacetGrid(df_kmeanscluster, col = \"cluster\", hue = \"cluster\", palette = \"coolwarm\",sharey=False,sharex=False)\n    g.map(sns.histplot,i) \n    g.set_xticklabels(rotation=30)\n    g.set_yticklabels()\n    g.fig.set_figheight(5)\n    g.fig.set_figwidth(20)","ee5c8849":"X = np.array(df_copy.drop(\"cluster\", axis = 1))\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=5).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(\"Estimated number of clusters: %d\" % n_clusters_)\nplt.show()","2a464d97":"df_copy.head()","7bc8cd05":"df_copy.info()","ad43fea0":"# import DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\nfrom sklearn import tree\n\n\n#Identify dependenet and independent variables\n\ntarget1 = ['Is_Parent']\nfeatures1 = [ 'Income', 'NumWebVisitsMonth', 'Response', 'Total_Amount_Spent', 'MntWines_%',\n       'MntFruits_%', 'MntMeatProducts_%', 'MntFishProducts_%',\n       'MntSweetProducts_%', 'MntGoldProds_%', 'Total_Purchase_Count',\n       'NumDealsPurchases_%', 'NumWebPurchases_%', 'NumCatalogPurchases_%',\n       'NumStorePurchases_%', 'Campaign_Success_Degree', 'Relationship']\n\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.20,random_state=0)\n","d271fea5":"#clf_gini = DecisionTreeClassifier(criterion='gini')\nclf_gini = DecisionTreeClassifier()\n\n# fit the model\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\n","b74e8f7a":"from sklearn import tree\n\nplt.Figure(figsize=(25,10))\n#plot_tree(clf_gini);\n#fig = plt.figure(figsize=(25, 20))\nplot_tree(clf_gini,filled=True);\nclf_gini.get_depth()","fea3dc50":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test1, y_pred_gini)\n\nprint('Confusion matrix\\n\\n', cm)","4b8d3c90":"# K-fold Validation (10-fold)\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.metrics import classification_report\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\nprint(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))","1cd30371":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))","0e472ce5":"#decision_tree = DecisionTreeClassifier(max_depth=4,min_samples_leaf=4,min_samples_split=4,min_impurity_decrease=0.001,random_state=64)\n\n#Changing Max Depth\n\nclf_gini = DecisionTreeClassifier(max_depth=3,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 3 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=4,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 4 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=5,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 5 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=6,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 6 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=7,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 7 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=8,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n MAX DEPTH 8 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))","281ab833":"#Changing Min Samples Leaf\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=3,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Sample Leaf 3 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Sample Leaf 4 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=5,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Sample Leaf 5 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=6,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Sample Leaf 6 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n","5f6273e8":"#decision_tree = DecisionTreeClassifier(max_depth=4,min_samples_leaf=4,min_samples_split=4,min_impurity_decrease=0.001,random_state=64)\n\n#Changing min_samples_split\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,min_samples_split = 2 ,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Sample Split 2 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n","b0b1b4ab":"#decision_tree = DecisionTreeClassifier(max_depth=4,min_samples_leaf=4,min_samples_split=4,min_impurity_decrease=0.001,random_state=64)\n\n#Changing min_impurity_decrease\n\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,min_samples_split = 2 ,min_impurity_decrease = 0.001,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Min Impurity Decrease 0.001 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n","1f9c263a":"#X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=64)\nfrom sklearn.ensemble import RandomForestClassifier\nX_train2, X_test2, Y_train2, Y_test2 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.20,random_state=0)\nrandom_forest = RandomForestClassifier(max_depth=4,min_samples_leaf=4,min_samples_split=4,min_impurity_decrease=0.001,random_state=64)\n\nrandom_forest.fit(X_train2,Y_train2)\nfig = plt.figure(figsize=(25, 20))\nY_pred2 = random_forest.predict(X_test2)\n#print(Y_pred2)\nprint(classification_report(Y_test2, Y_pred2))\nKfold_score = cross_val_score(random_forest,X_train2,Y_train2,cv= 10)\nKfold_pred = cross_val_predict(random_forest,X_test2,Y_test2,cv=10)\nprint(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(Y_test2, Kfold_pred))","67cff6ef":"#decision_tree = DecisionTreeClassifier(max_depth=4,min_samples_leaf=4,min_samples_split=4,min_impurity_decrease=0.001,random_state=64)\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.20,random_state=0)\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,min_samples_split = 2 ,min_impurity_decrease = 0.001,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Split 20-80 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.40,random_state=0)\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,min_samples_split = 2 ,min_impurity_decrease = 0.001,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Split 40-60 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df_copy[features1],df_copy[target1], test_size= 0.50,random_state=0)\nclf_gini = DecisionTreeClassifier(max_depth=6,min_samples_leaf=4,min_samples_split = 2 ,min_impurity_decrease = 0.001,random_state=64)\nclf_gini.fit(X_train1, y_train1)\ny_pred_gini = clf_gini.predict(X_test1)\ny_pred_train_gini = clf_gini.predict(X_train1)\nKfold_score = cross_val_score(clf_gini,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(clf_gini,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"\\n Split 50-50 ------------------------------------------------------------------------------------------------------- \\n\")\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train1, y_pred_train_gini)))\nprint('Test-set score: {:.4f}'.format(clf_gini.score(X_test1, y_test1)))\n","6e124451":"plt.figure(figsize=(20,20))  # set plot size (denoted in inches)\ntree.plot_tree(clf_gini,filled = True, fontsize=10)\nplt.show()","78b03063":"\ntarget1 = ['Is_Parent']\nfeatures1 = [ 'Income', 'NumWebVisitsMonth', 'Response', 'Total_Amount_Spent', 'MntWines_%',\n       'MntFruits_%', 'MntMeatProducts_%', 'MntFishProducts_%',\n       'MntSweetProducts_%', 'MntGoldProds_%', 'Total_Purchase_Count',\n       'NumDealsPurchases_%', 'NumWebPurchases_%', 'NumCatalogPurchases_%',\n       'NumStorePurchases_%', 'Campaign_Success_Degree', 'Relationship']\n\t   \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nprepared_df.info()","4b32259e":"\nX_train1, X_test1, y_train1, y_test1 = train_test_split(prepared_df[features1],prepared_df[target1], test_size= 0.40,random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\n#logisticRegr = LogisticRegression()\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train1, y_train1)\npredictions = model.predict(X_test1)\nscore = model.score(X_test1, y_test1)\n#print(score)\nKfold_score = cross_val_score(model,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(model,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))\n","63d645c3":"\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.linear_model import Perceptron\n\nsingle_layer_perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\nsingle_layer_perceptron.fit(X_train1, y_train1)","c8d0e36b":"\nKfold_score = cross_val_score(single_layer_perceptron,X_train1,y_train1,cv= 10)\nKfold_pred = cross_val_predict(single_layer_perceptron,X_test1,y_test1,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test1, Kfold_pred))","8f338668":"import tensorflow as tf\nprint(\"LEGEND : HL - Hidden Layer | IL - Input Layer | OL - Output Layer \\n\\n\")\n\n#Configuration 1 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 1 (Effect of Sigmoid Activation in all layers) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 8 , Activation: Sigmoid\")\nprint(\"HL2 : Node Count - 7 , Activation: Sigmoid\")\nprint(\"Epoch : 100\")\nprint(\"Learning Rate : 0.0001\")\nepoch = 100\nopt = tf.keras.optimizers.Adam(lr=0.0001)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='sigmoid'),\n  tf.keras.layers.Dense(7, activation='sigmoid'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c1 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c1.history['accuracy'])\/len(model_hist_c1.history['accuracy']))\npd.DataFrame(model_hist_c1.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n\n#Configuration 2 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 2 (Effect of Relu Activation) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 8 , Activation: Relu\")\nprint(\"HL2 : Node Count - 7 , Activation: Relu\")\nprint(\"Epoch : 100\")\nprint(\"Learning Rate : 0.0001\")\nepoch = 100\nopt = tf.keras.optimizers.Adam(lr=0.0001)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(7, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c2 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c2.history['accuracy'])\/len(model_hist_c2.history['accuracy']))\npd.DataFrame(model_hist_c2.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n\n#Configuration 3 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 3 (Effect of Softmax Activation) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Softmax\")\nprint(\"HL1 : Node Count - 8 , Activation: Relu\")\nprint(\"HL2 : Node Count - 7 , Activation: Relu\")\nprint(\"Epoch : 100\")\nprint(\"Learning Rate : 0.0001\")\nepoch = 100\nopt = tf.keras.optimizers.Adam(lr=0.0001)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(7, activation='relu'),\n  tf.keras.layers.Dense(1, activation='softmax')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c3 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c3.history['accuracy'])\/len(model_hist_c3.history['accuracy']))\npd.DataFrame(model_hist_c3.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n\n#Configuration 4 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 4 (Effect of Higher Learning Rate) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 8 , Activation: Relu\")\nprint(\"HL2 : Node Count - 7 , Activation: Relu\")\nprint(\"Epoch : 100\")\nprint(\"Learning Rate : 0.01\")\nepoch = 100\nopt = tf.keras.optimizers.Adam(lr=0.01)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(7, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c4 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c4.history['accuracy'])\/len(model_hist_c4.history['accuracy']))\npd.DataFrame(model_hist_c4.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n#Configuration 5 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 5 (Effect of Additional Hidden Layer)\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 8 , Activation: Relu\")\nprint(\"HL2 : Node Count - 7 , Activation: Relu\")\nprint(\"HL3 : Node Count - 5 , Activation: Relu\")\nprint(\"Epoch : 100\")\nprint(\"Learning Rate : 0.01\")\nepoch = 100\nopt = tf.keras.optimizers.Adam(lr=0.01)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(7, activation='relu'),\n  tf.keras.layers.Dense(5, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c5 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c5.history['accuracy'])\/len(model_hist_c5.history['accuracy']))\npd.DataFrame(model_hist_c5.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n#Configuration 6 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 6 (Effect of higher Epoch) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 8 , Activation: Relu\")\nprint(\"HL2 : Node Count - 7 , Activation: Relu\")\nprint(\"HL3 : Node Count - 5 , Activation: Relu\")\nprint(\"Epoch : 500\")\nprint(\"Learning Rate : 0.01\")\nepoch = 500\nopt = tf.keras.optimizers.Adam(lr=0.01)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(8, activation='relu'),\n  tf.keras.layers.Dense(7, activation='relu'),\n  tf.keras.layers.Dense(5, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c6 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c6.history['accuracy'])\/len(model_hist_c6.history['accuracy']))\npd.DataFrame(model_hist_c6.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n\n#Configuration 7 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 7 (Effect of higher Neurons) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 15 , Activation: Relu\")\nprint(\"HL2 : Node Count - 10 , Activation: Relu\")\nprint(\"HL3 : Node Count - 10 , Activation: Relu\")\nprint(\"Epoch : 500\")\nprint(\"Learning Rate : 0.01\")\nepoch = 500\nopt = tf.keras.optimizers.Adam(lr=0.01)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(15, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c7 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c7.history['accuracy'])\/len(model_hist_c7.history['accuracy']))\npd.DataFrame(model_hist_c7.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n\n#Configuration 8 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nprint(\"Configuration 8 (Effect of optimizer ) \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\nprint(\"IL: Node Count - 17\")\nprint(\"OL: Node Count - 1 , Activation: Sigmoid\")\nprint(\"HL1 : Node Count - 15 , Activation: Relu\")\nprint(\"HL2 : Node Count - 10 , Activation: Relu\")\nprint(\"HL3 : Node Count - 10 , Activation: Relu\")\nprint(\"Epoch : 500\")\nprint(\"Learning Rate : 0.01\")\nepoch = 500\nopt = tf.keras.optimizers.SGD(lr=0.01, momentum = 0.9)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(15, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.build(input_shape=X_train1.shape)\nmodel.summary()\nmodel.compile(optimizer = opt,loss= 'binary_crossentropy', metrics=['accuracy'])\nmodel_hist_c8 = model.fit(X_train1,y_train1,epochs=epoch,batch_size=42, validation_data=(X_test1, y_test1),verbose=0)\nprint(\"Average accuracy for this configuration: \",sum(model_hist_c8.history['accuracy'])\/len(model_hist_c8.history['accuracy']))\npd.DataFrame(model_hist_c8.history).plot(figsize=(20,10))\n# plt.Figure(figsize=(20,10))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()\n","d1650b63":"#install PYPI package\n!pip install BorutaShap","2339791d":"from BorutaShap import BorutaShap\n\nfeature_sel = BorutaShap(importance_measure='shap', classification=True)\n\nfeature_sel.fit(X=X_train1, y=y_train1.to_numpy(), n_trials=50, random_state=0)","b3e2e9e1":"feature_sel.plot(which_features='all', figsize=(16,12))","3a1f0e23":"\ntarget2 = ['Is_Parent']\nfeatures2 = [ 'Income', 'NumWebVisitsMonth', 'Total_Amount_Spent', 'MntWines_%',\n       'MntFruits_%', 'MntMeatProducts_%', 'MntFishProducts_%', 'Total_Purchase_Count',\n       'NumDealsPurchases_%', 'NumWebPurchases_%', 'NumCatalogPurchases_%',\n       'NumStorePurchases_%', 'Campaign_Success_Degree']\n\t   \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(prepared_df[features2],prepared_df[target2], test_size= 0.20,random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\n#logisticRegr = LogisticRegression()\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train2, y_train2)\npredictions = model.predict(X_test2)\n\nKfold_score = cross_val_score(model,X_train2,y_train2,cv= 10)\nKfold_pred = cross_val_predict(model,X_test2,y_test2,cv=10)\n#print(\"10-Fold Accuracies : \", Kfold_score)\nprint(\"10-Fold Accuracy : \", np.mean(Kfold_score))\nprint(classification_report(y_test2, Kfold_pred))\n","b357db23":"**KMeans Clistering will be done in below two methods:**\n* Without PCA decomposition and plot by highest correlated features\n* With PCA decomposition and plot by PCA generated features","e97f10b5":"# **Performing some data analysis to find out relationship between certain variables**\n","807bf241":"# **Performing Initial Analysis**","2caac2ae":"# **Performing EDA**","55dec1b7":"# Building Logistic Regression Model Using Selected Features","55ab8d76":"# Dropping Columns based on features extracted and correlation","2882be1b":"**Single Layer Perceptron [Threshold Logic Unit Network]**\n\n\n![image.png](attachment:17d04ae5-c25a-49b6-b454-6dcfd9a00875.png)","1252206c":"# **CW Part 5 - Research Question : Feature Selection Using BorutaShap Package**","b08f51d7":"# Perform **Feature Engineering** to extract\/modify attributes","f33ba746":"Activation Methods:\n\n![image.png](attachment:fb742797-56f1-40ad-a950-2c40f6372a29.png)","e0ee47ac":"**Plot selected features along with their ZScores**","7d0e4b0a":"# **Elbow Method**","4488ae2d":"# N = 2","ce9c613e":"* Do more indepth Outlier detection and handling\n* Use Pearson\u2019s Correlation Coefficient to perform forward\/backward feature selection\n","edc46c6f":"# **Coming Back To KMeans Clustering for N=5**","56483c92":"# **Silhoutte Method**","e25848b5":"From quick observation of above result,we can make out the following:\n* Almost all columns have values within range (except few Eg. income, recency,etc.) \n* And the columns 'Z_CostContact' and 'Z_Revenue' have constant values throughout. \n\nThese statements can be furthered proved in following observations.","1c83b696":"We can see that the Z_revenue and Z_CostContact have 0 skew, so they have same values throughout. Also the complain is the column witht he highest skewed value. This can be observed more in following histograms.\nSo now , the columns Z_revenue and Z_CostContact will be removed.\nAlso the Complain column has a bvery large skew, and it mostly has values 1 (from table statistics), so we will remove this column too","85d54843":"# Let's try out this method for extracting important features from our dataset in hand","c79b7906":"# Conclusion:","80ebe1cf":" **Initiate KMeans Clustering [Hard, Centroid] for n=2,3,4,5,6**","da6c25a1":"**Name:** Muhammad Zashin Nadeer\n\n**ID:** H00392626\n\n# **Dataset Description:**\n\nThe dataset we have chosen for all activites in this portfolio is a customer dataset, containing personal as well as purchase information of a customer from an organisation. We use this dataset for performing following procedures:\n\n-> CW1 - Perform Feature Engineering, EDA and build simple binary classification model\n\n-> CW2 - Perform clustering of datasets to identify different groups of customers\n\n-> CW3 - Build DecisionTreeClassifier Model with Varying Parameters\n\n-> CW4- Build ANN Model Using MultiLevelPerceptron for Binary CLassification\n\n-> CW5 (Research Topic) - Alternative method for Feature Selection \n","d7a42250":"**Use below methods to find optimal number of clusters:**\n* Elbow\n* Silhoutte","2763e6a7":"# **Effect of Varying Train-Test Split Ratios**\n\n-> 80-20\n\n-> 60-40\n\n-> 50-50","3a4e3a9a":"We can better view the correlation using a diagonal heat map visualization","10c8f04b":"-> https:\/\/www.jstatsoft.org\/article\/view\/v036i11\n\n-> https:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677\n\n-> https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/select-important-variables-boruta-package\/\n\n-> https:\/\/shap.readthedocs.io\/en\/latest\/index.html#:~:text=SHAP%20(SHapley%20Additive%20exPlanations)%20is,papers%20for%20details%20and%20citations).\n\n-> https:\/\/towardsdatascience.com\/shap-shapley-additive-explanations-5a2a271ed9c3\n","df3a3aa6":"# **CW Part 2 - Customer Segmentation (Clustering)**","ce755d88":"# **Perform Encoding and Normalization**","b028e952":"# **CW Part 3 - Decision Tree Classifiers**","52e4f469":"# **Pruning - Min Sample Split**","62f2933a":"**Introduction:**\n\nFeature selection can be defined as a process wherein the most relevant features for building a model is selected from a raw dataset. This process plays an important role in determing the way we build our model and also ultimately on the final accuracy. Picking up noisy features brings down the overall accuracy , and leaving out important attributes can be proven costly towards our model. Also, reducing the dimensionality means faster processing of ML models. Not to forget, it plays an important role in reducing overfitting as well (by removing noisy features).\n\n**Problem Statement:**\n\nThe ideal feature selection method would involve selecting all possible combinations of features from a dataset, building models, and then comparing the accuracies. But the amount of time and resource involved in this varies exponentially as follows:\n\nNumber of subsets = 2^n - 1\n\nThe current methods of performing feature selection can be divided into follows:\n\n-> **Filter Methods**\n\nThis involves filtering out the attributes using specific logic , and these attributes are subsequently used to build the model.\nEg: Pearson's Coeeficient\n\nDisadvantage: This method fails to consider relatioship amongst the features, and also selected features are not tuned to our model.\n\n-> **Wrapper Methods**\n\nThis involves building models on predictive features subsets and then comparing the accuracies with a standard.\nEg: Boruta \n\nDisadvantage: This method is time and resource consuming as model is build on each subset of features\n\n-> **Embedded Methods**\n\nThis uses both the functionalities of Filter and Wrapper methods\nDisadvantage: This method is time and resource consuming\n\n**Resolution Statement**\n\n**Boruta Method**\n\nThis is a wrapper method which makes use of the Random Forest predictive model. To summarise its working, it generates shuffled copies of existing features in order to consider the effect of random probes, and then selects features by comparing metrics (ZScore) of randomized features (Features having Zscore higher than highest value of Zscore of random features are taken into consideration and the ones below are ignored)\n\nProblems with Boruta: Simply put, the computational complexity is high, and also it has low consistency and accuracy.\n\n**SHAP (SHapley Additive exPlanations):**\n\nThis method developed by Shapley, is based on concept of identifying the individual effect of components which work together to produce a result.\nIt play an important role in identifying the marginal effects that each individual features (across all possile permutations of features) have on the target feature. Eventually, the mean values of these marginal contributions are then used to rank feature importance.\n\n**BorutaShapley Method**\n\nBy default, the Boruta wrapper uses 'Permutation' Logic to identify important features\nHowever, the mentioned disadvantages of Boruta wrapper method can be resolved by considering 'TreeSHAP' Logic (from SHAP ) while evaluating features importance.","e822efea":"Only 'income' column has null values, so filling values using mean values","8fe2703c":"**MultiLayer Perceptron Model [MPL]**\n\n![image.png](attachment:58d5b952-b84a-4980-8b82-4669eab8fe39.png)","5b8fd5cc":"# **Checking for Outliers**","df44873b":"**Using Logistic Regression Linear Classification**","f6ede2e8":"Find Highest correlated features:","5f4e3daa":"# **Highest Accuracy Score: 90.18% [Logistic Regression]**","acf21374":"# **Pruning - Max Depth**","ba6beca8":"Selected features for plotting (based on above correlation plot):\n* Total_Amount_Spent\n* Income","a4ddfb8d":"# **Pruning - Min Samples Leaf**","c1f12b7f":"**Varying Parameters for MLP:**\n\n-> Number of Hidden Layers\n\n-> Number of Neurons in each layers\n\n-> Activation methods\n\n-> Epoch Value\n\n-> Learning Rate","0803b392":"# **Pruning - Min Impurity Decrease**","73dfb92c":"**Ways to improve this ML Classification Model**:","5035399c":"# References (For Research CW):","0dd05234":"# **CW Part 1 - Exploratory Data Analysis**","e3c28417":"The below inferences can be made from our findings:\n\n-> We are able to identify the important features of a dataset just by running few simple commands making use of BorutaShap library.\n\n-> The accuracy of the model build from selecting the identified features is similar to the accuracy we had earlier model which considers all features.\n\n-> This reduction in dimensions without any change in accuracy will help us in obtaining lesser computation time of models and even lesser computing resource.\n\nHence we can say that 'BorutaShap' method is an effective method in performing feature selection. If this topic is of your interest, more detailed information about its implementation can be found in below artice link:\nhttps:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677","578a7da6":"# **Random Forest**","00ae42bc":"# **Methods to Reduce Overfitting:**\n\n**Pruning Decision Tree**\n\n-> criterion \n\n-> splitter\n\n-> max_depth\n\n-> min_samples_leaf\n\n-> min_sample_split\n\n-> min_impurity_decrease\n\n**Random Forest**","64591100":"# **Now Performing Basic Data Checks**","87407d3c":"# **Identification of which ML Classification Model to Use**","556bcbe6":"# N = 5","66ed95b0":"**The following transformations will be applied to extract new features:**\n* Extract age of customer from 'Year_Birth'\n* Get number of days since customer enrolled in company 'Dt_Customer'\n* Convert Amount spent on items into percentages of item bought over total amount spent\n* Convert Number of different purchases made into percentages over total number of purchases made.\n* Convert columns 'AcceptedComp1','AcceptedComp2','AcceptedComp3','AcceptedComp4','AcceptedComp5' into single column with values 1-5\n* Generalize Marital Status into 2 values [married\/not married]\n* convett categorical variable 'Education' into ordinal variable by assigning prioratized values\n* generate TARGET DEPENDENT variable IS_PARENT","44fdb771":"Checking Silhoutte scores for different cluster sizes","89a36779":"**Plotting Histogram to observe frequency distribution of column values**","1441a9e8":"# N = 3","2b5173bd":"# **CW Part 4 - Building ANN Predictive Model**","6fed4ebc":"First Step is  to load Data","c15aa1eb":"# N = 4","6118f5e7":"**Data Aanlysis Points:**\n\nCluster 1 -> \n\n* Lowest Income customers with low spending trends\n* Comparatively High spending trends on Fruit\/Fish\/Sweet\/Gold products\n* HIghest Interest in Deals Purchases\n* Lowest catalogue purchases\n* Mostly customers with basic educational qualifications\n* High chances of being a Parent\n* Highest WebVisits\n\n**Customer Segment can be classifed as : Gold**\n\nCluster 2 ->\n\n* Median Income customers with lowest spending trends\n* Highest spending trends on Fruit products\n* Highest spending trends on Meat products\n* High spending trends on Sweet products\n\n**Customer Segment can be classifed as : Bronze**\n\nCluster 3 ->\n* Median Income customers with avg spending trends\n* High spending trends on Wine products\n* Interested in Deals Purchases\n* Highest Web Purchases\n* Most likely is a Parent\n* High Web Visits\n\n**Customer Segment can be classifed as : Silver**\n\nCluster 4 ->\n* Highest Income customers with high spending trends\n* High spending trends on Wine products\n* Seems to not give any interest to Deals\n* Most purchases done through Catalogue\n* Most likely not a Parent\n* High Campaign Success Degree\n\n**Customer Segment can be classifed as : Platinum**\n\nCluster 5 ->\n* Median Income customers with avg spending trends\n* Customers with high level of education\n\n**Customer Segment can be classifed as : Basic**\n\nDisclaimer: Please note that these observations were made for a specific instance of clustering, the observations can change on reclustering.","b896dc1e":"**# Trying DBSCAN Clustering [Soft Clustering]**"}}