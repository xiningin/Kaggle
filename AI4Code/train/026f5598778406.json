{"cell_type":{"644e559f":"code","081afed7":"code","a1031f3f":"code","89f00842":"code","66d7a6ab":"code","f0cb32f7":"code","b903e35f":"code","79e7b7f6":"code","f23afd1b":"code","729940d7":"code","9999b1a3":"code","25f60102":"code","1463186a":"code","3fc8c04e":"code","4a2e505a":"code","2ea5ce3e":"code","26c152f3":"code","238fcab5":"code","01503d66":"markdown","2b2be8f0":"markdown"},"source":{"644e559f":"train_path = '..\/input\/emotions\/images\/train'\nvalid_path = '..\/input\/emotions\/images\/validation'","081afed7":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.metrics import Recall,Precision\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import SVG, Image\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","a1031f3f":"!nvidia-smi","89f00842":"\n# Import the latest version of wandb\n!pip install -q --upgrade wandb","66d7a6ab":"import wandb\nfrom wandb.keras import WandbCallback\n#from wandb.jupyteragent import jupyteragent as _secretagent\nwandb.login()","f0cb32f7":"img_size = 48\nbatch_size = 64\ndatagen_train = ImageDataGenerator(horizontal_flip=True)\ntrain_generator = datagen_train.flow_from_directory(train_path,\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\ndatagen_validation = ImageDataGenerator(horizontal_flip=True)\nvalidation_generator = datagen_validation.flow_from_directory(valid_path,\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","b903e35f":"\nmodel = Sequential()\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n# Flattening\nmodel.add(Flatten())\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(7, activation='softmax'))\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","79e7b7f6":"checkpoint = ModelCheckpoint(\".\/model.h5\",monitor = \"val_accuracy\",save_best_only = True,verbose=1)","f23afd1b":"def train():\n    # Specify the hyperparameter to be tuned along with\n    # an initial value\n    config_defaults = {\n        'batch_size': 8,\n        'learning_rate': 0.01\n    }\n\n    # Initialize wandb with a sample project name\n    wandb.init(config=config_defaults)\n\n    # Specify the other hyperparameters to the configuration, if any\n    wandb.config.epochs = 25\n\n    # Iniialize model with hyperparameters\n    tensorflow.keras.backend.clear_session()\n    #model = Model()\n    \n    # Compile the model\n    opt = tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate) # optimizer with different learning rate specified by config\n    model.compile(opt, 'sparse_categorical_crossentropy', metrics=['acc'])\n    \n    # Train the model\n    _ = model.fit(train_generator,\n                  epochs=wandb.config.epochs, \n                  validation_data=validation_generator,\n                  callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n                            \n    # Evaluate    \n    loss, accuracy = model.evaluate(testloader, callbacks=[WandbCallback(),checkpoint])\n    print('Test Error Rate: ', round((1-accuracy)*100, 2))\n    wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)}) # wandb.log to track custom metrics","729940d7":"sweep_config = {\n  'method': 'bayes', \n  'metric': {\n      'name': 'val_loss',\n      'goal': 'minimize'\n  },\n  'early_terminate':{\n      'type': 'hyperband',\n      'min_iter': 5\n  },\n  'parameters': {\n      'batch_size': {\n          'values': [ 64,128]\n      },\n      'learning_rate':{\n          'values': [0.01]\n      }\n  }\n}","9999b1a3":"sweep_id = wandb.sweep(sweep_config, project=\"emotions-detection\")","25f60102":"WANDB_AGENT_DISABLE_FLAPPING=True","1463186a":"wandb.agent(sweep_id, function=train)","3fc8c04e":"model.save('.\/model.h5')\n#model = keras.models.load_model('.\/finalmodel.h5')","4a2e505a":"plt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label=\"Training loss\")\nplt.plot(history.history['val_loss'], label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs Validation Loss\")\nplt.show()","2ea5ce3e":"plt.figure(figsize=(12, 6))\nplt.plot(history.history['accuracy'], label=\"Training acc\")\nplt.plot(history.history['val_accuracy'], label=\"Validation acc\")\nplt.legend()\nplt.title(\"Training vs Validation Accuracy\")\nplt.show()","26c152f3":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import SVG, Image\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nImage('model.png',width=400, height=200)","238fcab5":"loss, accuracy , precision,recall=model.evaluate(validation_generator)\nprint('Test Accuracy: %.3f' % accuracy)\nprint('Test Precision: %.3f' % precision)\nprint('Test Recall: %.3f' % recall)\nprint('Test loss: %.3f' % loss)","01503d66":"# ImageDataGenerator","2b2be8f0":"# feel free to upvote this kernel"}}