{"cell_type":{"75ffcf28":"code","646527c4":"code","b213ce57":"code","0c914b62":"code","6e6945dd":"code","6e88e8d0":"code","c16ddd5f":"code","ade119ad":"code","cb9141d3":"code","d3786031":"code","f2ad93fe":"code","e2ab22f3":"code","07e977ad":"code","c90c90b9":"code","9872e266":"code","8604431b":"code","65f9a881":"code","92a6dcc5":"code","45e574cc":"code","c5770dce":"code","76a44af5":"code","a0ad00ce":"code","e301a3ee":"code","7f13b5f6":"code","b9c44a79":"code","2f02bbe4":"code","72d22839":"code","33314842":"code","95065428":"code","c361cc02":"code","c35ecbf7":"code","2f854076":"code","d95bf17a":"code","aba7b613":"code","5a701988":"code","06376e6c":"code","bc565e79":"code","44cab9da":"code","25ac14da":"code","ffe9fd5b":"code","6bb0ab6b":"code","7b0bd52e":"markdown","376ed088":"markdown","6743cf23":"markdown","4990f04b":"markdown","e4267697":"markdown","773f1912":"markdown","feb0a788":"markdown","68a87039":"markdown","c3fcd165":"markdown","a4346067":"markdown","a4067584":"markdown","a1b40f10":"markdown","934c4259":"markdown","e8092b31":"markdown","db5d47e4":"markdown","f20a5349":"markdown","b030d56f":"markdown","6d03fd2f":"markdown","4a1c4acb":"markdown","f81c3af8":"markdown","75bbd229":"markdown","a19e8fe4":"markdown","209c3cb0":"markdown"},"source":{"75ffcf28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","646527c4":"import matplotlib.pyplot as plt\nimport seaborn as sns","b213ce57":"df= pd.read_csv(\"..\/input\/social-network-ads\/Social_Network_Ads.csv\")\ndf","0c914b62":"df.corr() # Lets look at statistical correlation\n#There is positive high correlation between Age and Purchased items","6e6945dd":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"jet\",annot=True)\n#here we visualize the correlations","6e88e8d0":"plt.figure(figsize=(15,10))\nsns.countplot(data=df, x=\"Age\",hue=\"Purchased\")\n#We can see that the effects of ads is highest between ages 26 and 40\n#Therefore these ge groups are more suitable to be target group for the commercial ads","c16ddd5f":"df.isnull().sum() # we do not have any missing values","ade119ad":"df.info() # we do not have any non numerical values in the columns","cb9141d3":"X = df.drop(\"Purchased\",axis=1).values\nX.shape","d3786031":"y = df[\"Purchased\"].values\ny.shape","f2ad93fe":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","e2ab22f3":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)\nX_train[0]\n#We rescale all of the features with standart scaler which produces values between -1 and 1\n#This secures there is no value gap between features ","07e977ad":"from sklearn.linear_model import LogisticRegression\nlogistic= LogisticRegression()\nlogistic.fit(X_train, y_train)\npredictions_logistic= logistic.predict(X_test)","c90c90b9":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(predictions_logistic,columns=[ \"Predictions of Logistic regression\"])\npd.concat([df1,df2],axis=1).head()\n#Here we can compare the predictions of our model with the actual values","9872e266":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\nprint(classification_report(y_test, predictions_logistic))\nprint(confusion_matrix(y_test, predictions_logistic))\nprint(accuracy_score(y_test, predictions_logistic))","8604431b":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, logistic.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n#Visualization of the predictions of the Logistic Regression in Train Set","65f9a881":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, logistic.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n#Visualization of the predictions of the Logistic Regression in Test Set","92a6dcc5":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/knneigbor\/knn.PNG\"))","45e574cc":"from sklearn.neighbors import KNeighborsClassifier\n","c5770dce":"error_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))","76a44af5":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","a0ad00ce":"knn=KNeighborsClassifier(n_neighbors=5) # we choose 5 as neigbor parameter\nknn.fit(X_train,y_train)\nknn_predictions=knn.predict(X_test)","e301a3ee":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(knn_predictions,columns=[ \"Predictions of KNN\"])\npd.concat([df1,df2],axis=1).head()","7f13b5f6":"print(classification_report(y_test, knn_predictions))\nprint(confusion_matrix(y_test, knn_predictions))\nprint(accuracy_score(y_test, knn_predictions))\n#KNN has higher performance than Logistic Regression in this dataset","b9c44a79":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, knn.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('blue', 'purple')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('blue', 'purple'))(i), label = j)\nplt.title('K-NN (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","2f02bbe4":"from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\ndtree_predictions= dtree.predict(X_test)","72d22839":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(dtree_predictions,columns=[ \"Predictions of Decision Tree Classifier\"])\npd.concat([df1,df2],axis=1) ","33314842":"print(classification_report(y_test, dtree_predictions))\nprint(confusion_matrix(y_test, dtree_predictions))\nprint(accuracy_score(y_test, dtree_predictions))\n#Decision has higher performance than Logistic Regression, but lower than KNN  in this dataset","95065428":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, dtree.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","c361cc02":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(X_train,y_train)\nrandom_predictions= random.predict(X_test)","c35ecbf7":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(random_predictions,columns=[ \"Predictions of Random Forest Classifier\"])\npd.concat([df1,df2],axis=1).head()","2f854076":"print(classification_report(y_test, random_predictions))\nprint(confusion_matrix(y_test, random_predictions))\nprint(accuracy_score(y_test, random_predictions))\n#Random Forest has the second best position after K Nearest Neighbors Algorithm","d95bf17a":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, random.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","aba7b613":"from sklearn.naive_bayes import GaussianNB\nbayes=GaussianNB()\nbayes.fit(X_train, y_train)\nbayes_predictions=bayes.predict(X_test)","5a701988":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(random_predictions,columns=[ \"Predictions of Naive Bayes Classifier\"])\npd.concat([df1,df2],axis=1).head()","06376e6c":"print(classification_report(y_test, bayes_predictions))\nprint(confusion_matrix(y_test, bayes_predictions))\nprint(accuracy_score(y_test, bayes_predictions))\n#Naive Bayes has better predictions than logistic regression, but worse than the restof algorithms","bc565e79":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, bayes.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","44cab9da":"from sklearn.svm import SVC\nsvclassifier= SVC(kernel=\"linear\")\nsvclassifier.fit(X_train, y_train)\nsvc_predictions= svclassifier.predict(X_test)","25ac14da":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(svc_predictions,columns=[ \"Predictions of Support Vector Machines\"])\npd.concat([df1,df2],axis=1).head()","ffe9fd5b":"print(classification_report(y_test, svc_predictions))\nprint(confusion_matrix(y_test,svc_predictions))\nprint(accuracy_score(y_test, svc_predictions))\n#Support Vector Machines has almost the same results as Naive Bayes","6bb0ab6b":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, svclassifier.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","7b0bd52e":"## B. Preparing Data For Algorithms","376ed088":"Evaluation of the Performance of Logistic Regression","6743cf23":"Visualising the Test set results\n","4990f04b":"Visualization of Test Results","e4267697":"Visualising the Test set results","773f1912":"## A. Exploratory Data Analysis","feb0a788":"2.Feature Scaling","68a87039":"How Algorithm Works:\n\n*K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).\n\nFirstly we store all the dataSecondly we calculate the data from x to all points in our data set, x indicationg particular new data point Then we sort the points near data by increasing distance from xFinally we predict the majority label of K, which is number and represent closest points\n\n*Choosing a K will effect what class a new point is assigned to: if we choose k=3, then the algorithm looks at the three nearest neighbors to this new point if we set k=6, then the algorithm looks at the six nearest neighbors to this new point and decide according to the majority of these 6 neighbors. If we set larger k values,we get a cleaner cutoff at the expense of mislabelling some points","c3fcd165":"## 3. Decision Tree Classifier","a4346067":"Evaluation of the Performance of  K Neares Neighbors","a4067584":"## 1. Logistic Regression:","a1b40f10":"Visualising the Test set results","934c4259":"Visualising the Test set results","e8092b31":"## 6. Support Vector Machines","db5d47e4":"Visualising the Test set results","f20a5349":"Choosing true k value is very important. Instead of using different k vlaues which will be time consuming, we can use a function in order to choose the best k.","b030d56f":"## 2. K Nearest Neighbors:","6d03fd2f":"1. Splitting Data into Train and Test Sets","4a1c4acb":"In this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.Therefore, the value of k is very important for our model's success.","f81c3af8":"As we can see in the figure above, k between 5 and 37 gives the least error rate,so we will use it for better predictions","75bbd229":"## 4. Random Forest Classifier","a19e8fe4":"## C. Training Classification Algorithms","209c3cb0":"## 5. Naive Bayes Classifier"}}