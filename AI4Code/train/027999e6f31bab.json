{"cell_type":{"eb7dfd7a":"code","933f3b54":"code","66a927c2":"code","be4b23d8":"code","393191f9":"code","0c3c7e81":"code","87463fe7":"code","e7d716cc":"code","511d8d23":"markdown","8dfc4ae6":"markdown","9817473f":"markdown","7efe459f":"markdown","59b786a6":"markdown","ef0d30ae":"markdown","85445498":"markdown"},"source":{"eb7dfd7a":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"..\/input\/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('..\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","933f3b54":"# read in some helpful libraries\nimport nltk # the natural language toolkit, open-source NLP\nimport pandas as pd # dataframes\nimport zipfile\n\n### Read in the data\n\n# read our data into a dataframe\ndf = pd.DataFrame()\n#pd.read_csv() - df = pd.read_csv('..\/input\/spooky-author-identification\/train.zip', compression='zip', header=0, sep=',', quotechar='\"')\nDataset = 'train'\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"..\/input\/spooky-author-identification\/\"+Dataset+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n\ntexts = pd.read_csv(Dataset + '.csv')\n\n# look at the first few rows\ntexts.head()","66a927c2":"#print(os.path.join(dirname, filename))","be4b23d8":"#import os\n#print(os.listdir('..\/input\/spooky-author-identification\/train.zip'))","393191f9":"### Split data\n\n# split the data by author\nbyAuthor = texts.groupby('author')\n\n### Tokenize (split into individual words) our text\n\n# word frequency by author\nwordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n\n# for each author...\nfor name, group in byAuthor:\n    # get all of the sentences they wrote and collapse them into a\n    # single long string\n    sentences = group['text'].str.cat(sep = ' ')\n    \n    # convert everything to lower case (so 'The' and 'the' get counted as\n    # the same word rather than two different words)\n    sentences = sentences.lower()\n    \n    # split the text into individual tokens\n    tokens = nltk.tokenize.word_tokenize(sentences)\n    \n    # calculate the frequency of each token\n    frequency = nltk.FreqDist(tokens)\n    \n    # add the frequencies for each author to our dictionary\n    wordFreqByAuthor[name] = (frequency)\n    \n# now we have an dictionary where each entry is the frequency distribution\n# of words for a specific author","0c3c7e81":"# see how often each author says 'blood'\nfor i in wordFreqByAuthor.keys():\n    print('blood: ' + i)\n    print(wordFreqByAuthor[i].freq('blood'))\n\n# print a blank line\nprint()\n\n# see how often each author says 'scream'\nfor i in wordFreqByAuthor.keys():\n    print('scream: ' + i)\n    print(wordFreqByAuthor[i].freq('scream'))\n    \n# print a blank line\nprint()\n\n# see how often each author says 'fear'\nfor i in wordFreqByAuthor.keys():\n    print('fear: ' + i)\n    print(wordFreqByAuthor[i].freq('fear'))","87463fe7":"# One way to guess authorship is to use the joint probability that each\n# author used each word in a given sentence\n\n# first, let's start with a test sentence\ntestSentence = \"It was a dark and stormy night.\"\n\n# and then lowercase & tokenize our test sentence\npreProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n\n# create an empty dataframe to put our output in\ntestProbabilities = pd.DataFrame(columns = ['author', 'word','probability'])\n\n# For each author...\nfor i in wordFreqByAuthor.keys():\n    # for each word in our test sentence..\n    for j in preProcessedTestSentence:\n        # find out how frequentyly the author used that word\n        wordFreq = wordFreqByAuthor[i].freq(j)\n        # and add a very small amount to every prob. so none of them are 0\n        smoothedWordFreq = wordFreq + 0.000001\n        # add the author, word and smoothed freq. to our dataframe\n        output = pd.DataFrame([[i,j,smoothedWordFreq]], columns = ['author', 'word','probability'])\n        testProbabilities = testProbabilities.append(output, ignore_index = True)\n\n# empty dataframe for the probability that each author wrote the snetence\ntestProbabilitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n\n# now let's group the dataframe with our frequency by author\nfor i in wordFreqByAuthor.keys():\n    # get the joint probability that each author wrote each word\n    oneAuthor = testProbabilities.query('author == \"' + i + '\"')\n    jointProbability = oneAuthor.product(numeric_only = True)[0]\n    \n    # and add that to our dataframe\n    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n    testProbabilitiesByAuthor = testProbabilitiesByAuthor.append(output, ignore_index=True)\n    \n# and our winner is...\ntestProbabilitiesByAuthor.loc[testProbabilitiesByAuthor['jointProbability'].idxmax(),'author']","e7d716cc":"testProbabilitiesByAuthor.to_csv(\"testProbabilityByAuthor.csv\", index=False)","511d8d23":"\uc6b0\ub9ac\uac00 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0\uc11c \ubcf8 \uac83\uc744 \ubc14\ud0d5\uc73c\ub85c, \uc138 \uba85\uc758 \uc791\uac00\ub4e4 \uc911 H.P.Lovecraft\uac00 'It was a dark and stormy night\" \ub77c\ub294 \ubb38\uc7a5\uc744 \uc37c\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc740 \uac83\uc73c\ub85c \ub098\ud0c0\ub0a9\ub2c8\ub2e4.","8dfc4ae6":"# \uc5b4\ub290 \uc791\uac00\uac00 \ubb38\uc7a5\uc744 \uc37c\ub294\uc9c0 \ucd94\uce21\ud558\ub294 \ub370\uc5d0 \ub2e8\uc5b4 \ube48\ub3c4\uc218 \uc0ac\uc6a9\ud558\uae30\n\n\uc77c\ubc18\uc801\uc778 \uc544\uc774\ub514\uc5b4\ub294 \ub2e4\ub978 \uc0ac\ub78c\ub4e4\uc740 \ub2e4\ub978 \ub2e8\uc5b4\ub4e4\uc744 \ub354 \uc790\uc8fc \ub610\ub294 \ub35c \uc0ac\uc6a9\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. (\ub098\uc758 \uacbd\uc6b0\uc5d0\ub294 \ud2b9\ud788 'gestalt'\ub97c \uc88b\uc544\ud558\uc168\ub358 \uad50\uc218\ub2d8\uc774 \uacc4\uc168\uc2b5\ub2c8\ub2e4.) \ub204\uac00 \ubb34\uc5c7\uc744 \ub9d0\ud588\ub294\uc9c0 \ud655\uc2e4\ud558\uc9c0 \uc54a\uc9c0\ub9cc \uadf8 \uc548\uc5d0 \ud55c \uc0ac\ub78c\uc774 \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 \ub2e8\uc5b4\uac00 \ub9ce\uc744 \uacbd\uc6b0, \uadf8 \ub2e8\uc5b4\ub4e4\uc740 \uc5b4\ub5a4 \ud55c \uc0ac\ub78c\uc774 \uc37c\uc744 \uac83\uc774\ub77c\uace0 \ucd94\uce21 \uac00\ub2a5\ud569\ub2c8\ub2e4.\n\n\uc774 \uc77c\ubc18\uc801\uc778 \uc6d0\uce59\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub204\uac00 'It was a dark and stormy night.'\uc774\ub77c\ub294 \ubb38\uc7a5\uc744 \uc4f8 \uac00\ub2a5\uc131\uc774 \ub354 \ub192\uc744\uc9c0 \ucd94\uce21\ud574\ubd05\uc2dc\ub2e4.","9817473f":"\uc774\uc81c \uc6b0\ub9ac\ub294 \uac01 \uc791\uac00\uac00 \ud2b9\uc815 \ub2e8\uc5b4\ub97c \uc5b4\ub290 \ube48\ub3c4\ub85c \uc0ac\uc6a9\ud558\ub294\uc9c0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc774 \ud560\ub85c\uc708 \ub300\ud68c\uc6a9\uc774\ub2c8\uae4c 'blood', 'scream', 'fear' \uac19\uc740 \uac74 \uc5b4\ub5a8\uae4c\uc694?","7efe459f":"# \uac01 \uc791\uac00\uac00 \uac01 \ub2e8\uc5b4\ub97c \uc0ac\uc6a9\ud55c \ube48\ub3c4 \uc218 \uc54c\uc544\ubcf4\uae30\n\n\uac01 \uc791\uac00\uac00 \uac01 \ub2e8\uc5b4\ub97c \uc5bc\ub9c8\ub098 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294\uc9c0 \uc54c\uc544\ubcf4\uc2ed\uc2dc\uc624. \ub9ce\uc740 NLP \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc740 \ud2b9\uc815 \ub2e8\uc5b4\uac00 \uc5bc\ub9c8\ub098 \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294\uc9c0 \uacc4\uc0b0\ud558\ub294 \ub370 \uc758\uc874\ud569\ub2c8\ub2e4. (\uc774 \uc6a9\uc5b4\uc5d0 \ub300\ud55c \uba4b\uc9c4 \uc6a9\uc5b4\ub294 '\ub2e8\uc5b4 \ube48\ub3c4' \uc785\ub2c8\ub2e4.) \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uac01 \uc800\uc790\uc758 \ub2e8\uc5b4 \ube48\ub3c4\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. NLTK\uc5d0\ub294 \uc774\ub97c \uc704\ud574 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uba4b\uc9c4 \ub0b4\uc7a5\ud568\uc218\uc640 \ub370\uc774\ud130 \uad6c\uc870\uac00 \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4.","59b786a6":"# \uc720\uc6a9\ud55c NLP \ub77c\uc774\ube0c\ub7ec\ub9ac & \ub370\uc774\ud130\uc14b\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 'NLTK' \ub77c\uace0 \ud558\ub294 Natural Language Toolkit\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc5b8\uc5b4 \ub370\uc774\ud130\ub97c \ubd84\uc11d\ud558\uae30 \uc704\ud55c \uc624\ud508 \uc18c\uc2a4 \ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4. NLTK\uc758 \uc88b\uc740 \uc810\uc740 \ub9ce\uc740 \uc77c\ubc18\uc801\uc778 NLP \uc791\uc5c5\uc744 \ub2e8\uacc4\ubcc4\ub85c \uc9c4\ud589\ud558\ub294 \uc720\uc6a9\ud55c \ucc45\uc774 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \ub354 \uc88b\uc740 \uc810\uc740 \uc5ec\uae30\uc5d0\uc11c \ucc45\uc744 \ubb34\ub8cc\ub85c \uc5bb\uc744 \uc218 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.","ef0d30ae":"\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ubca0\uc774\uc9c1\ud55c \uc790\uc5f0\uc5b4\ucc98\ub9ac(NLP) \uae30\ubc95 \uba87 \uac1c\ub97c \ub2e4\ub8f0 \uac83\uc785\ub2c8\ub2e4. \ub2e4\ub8f0 \ubaa9\ub85d\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n* \uc720\uc6a9\ud55c \uc77c\ubd80 NLP \ub77c\uc774\ube0c\ub7ec\ub9ac & \ub370\uc774\ud130\uc14b \uc77d\uae30\n* \uac01 \uc791\uac00\ub4e4\uc774 \uac01 \ub2e8\uc5b4\ub97c \uc0ac\uc6a9\ud55c \ube48\ub3c4\uc218 \ucc3e\uae30\n* \uc5b4\ub290 \uc791\uac00\uac00 \ubb38\uc7a5\uc744 \uc37c\ub294\uc9c0 \ucd94\uce21\ud558\ub294 \ub370\uc5d0 \ub2e8\uc5b4 \ube48\ub3c4\uc218 \uc0ac\uc6a9\ud558\uae30\n\n\uc900\ube44\ub418\uc5c8\ub098\uc694? \uc2dc\uc791\ud574\ubd05\uc2dc\ub2e4! :D","85445498":"# \uac1c\uc694\n\ud574\ub2f9 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uc815\uaddc\ud654\ub41c \uc720\ub2c8\uadf8\ub7a8 \ube48\ub3c4\ub97c \uae30\uc900\uc73c\ub85c \uc5b4\ub290 \uc791\uac00\uac00 \ud14d\uc2a4\ud2b8 \ubb38\uc790\uc5f4\uc744 \uc791\uc131\ud588\ub294\uc9c0 \ucd94\uce21\ud569\ub2c8\ub2e4. \uc774\ub294 \uac01 \uc791\uac00\uac00 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0\uc11c \ubaa8\ub4e0 \ub2e8\uc5b4\ub97c \uc5bc\ub9c8\ub098 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294\uc9c0\ub97c \uc138\uace0 \uadf8\ub4e4\uc774 \uc4f4 \ucd1d \ub2e8\uc5b4 \uac2f\uc218\ub85c \ub098\ub204\ub294 \uba4b\uc9c4 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c, \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc5d0\uc11c \ud55c \uc791\uac00\uac00 \ub2e4\ub978 \uc791\uac00\ub4e4\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \ub2e8\uc5b4\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \ubcf8\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uc774 \uc791\uac00\uac00 \uadf8 \uc791\uac00\uc784\uc744 \ucd94\uce21\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub2e4\uc74c\uc744 \uc6b0\ub9ac\uc758 \ud2b8\ub808\uc774\ub2dd \ub9d0\ubb49\uce58\ub77c\uace0 \uac00\uc815\ud574\ubd05\uc2dc\ub2e4:\n\n* \uc791\uac00 1: \"A very spooky thing happened. The thing was so spooky I screamed.\"\n* \uc791\uac00 2: \"I ate a tasty candy apple. It was delicious\"\n\n\uadf8\ub9ac\uace0 \ub2e4\uc74c\uc740 \uc5b4\ub290 \uc791\uac00\uac00 \uc37c\ub294\uc9c0 \uc54c\uc544\ub0b4\uace0 \uc2f6\uc740 \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc785\ub2c8\ub2e4:\n\n* \uc791\uac00 ??? : \"What a spooky thing!\"\n\n\uadf8\ub0e5 \ubd10\uc11c\ub294, \uc774 \ubb38\uc7a5\uc740 \uc791\uac00 1\uc774 \uc4f4 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \uc791\uac00 1\uc740 'spooky'\uc640 'thing'\uc744 \ub458 \ub2e4 \ub9ce\uc774 \uc4f0\ub294 \ubc18\uba74, \uc791\uac00 2\ub294 \uadf8\ub807\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4(\uc801\uc5b4\ub3c4 \uc6b0\ub9ac\uc758 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ub530\ub974\uba74). \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc5d0\uc11c 'spooky'\uc640 'thing'\uc744 \ub458 \ub2e4 \ubcfc \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0, \uc774\ub294 \uc791\uac00 2\ubcf4\ub2e4\ub294 \uc791\uac00 1\uc774 \uc4f4 \uac83\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4 -- \ube44\ub85d \ud14c\uc2a4\ud2b8 \ubb38\uc7a5\uc5d0\ub294 'a'\ub77c\ub294 \ub2e8\uc5b4\uac00 \uc788\uace0 \uc791\uac00 2\ub3c4 \uc774\ub97c \uc0ac\uc6a9\ud588\uc9c0\ub9cc \ub9d0\uc785\ub2c8\ub2e4. \n\n\ud29c\ud1a0\ub9ac\uc5bc\uc758 \ub098\uba38\uc9c0 \ubd80\ubd84\uc5d0\uc11c\ub294 \uc774 \uc9c1\uad00\uc744 \ucf54\ub4dc\ub85c \ubcc0\ud658\ud560 \uc218 \uc788\ub294\uc9c0 \uc54c\uc544\ubcfc \uac83\uc785\ub2c8\ub2e4."}}