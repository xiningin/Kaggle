{"cell_type":{"27bc0199":"code","c831e48f":"code","25dcbdfd":"code","fa4a681b":"code","1d91341c":"code","28fd00a8":"code","c1826772":"code","c9a33add":"code","063a5839":"code","4d5c23a9":"code","bf85f6d0":"code","91034ecb":"code","7c40d47a":"code","baf9ec2b":"code","eceb55e3":"code","0534d027":"code","33ceba7e":"code","5db3c4af":"code","aa05d286":"code","14f4b4d3":"code","1b24a502":"code","a99a80c7":"code","f8a676f4":"code","1e1a5fa7":"code","ad63dcd5":"code","41902b2e":"code","13b1c23f":"code","bb6f877d":"code","5436b1ff":"code","f94a1b76":"code","92198890":"code","4039c25d":"code","d562e98f":"code","970f4888":"markdown","c9b89baa":"markdown","a88da35e":"markdown","9e0ba401":"markdown","051a8d9e":"markdown","1bd4c4c1":"markdown","a840a69a":"markdown","e669e68e":"markdown","9d3c06b9":"markdown","d576461d":"markdown","5b5381ed":"markdown","c56674c6":"markdown","52440067":"markdown","7a1fcbf7":"markdown","5731aa25":"markdown","a9619047":"markdown","29a62396":"markdown","8ebb33b4":"markdown","10671edf":"markdown","c8f09b96":"markdown","f24136e6":"markdown","e5e991a0":"markdown","45c2e226":"markdown","e6fd1943":"markdown","6bf862b0":"markdown","425726bc":"markdown","65ffdef9":"markdown","91c7b5dd":"markdown","c5e8d847":"markdown","56b87308":"markdown","92aaa810":"markdown","b3e01065":"markdown","d9d362d3":"markdown","fb329c43":"markdown","e52000d9":"markdown","588b4145":"markdown","8f1cd41f":"markdown","17f5d0d1":"markdown","c5837287":"markdown","6a359753":"markdown","36a7bcbc":"markdown","ab109b3b":"markdown","9bf421f7":"markdown","a5482ac5":"markdown","028bcb3f":"markdown","88192795":"markdown","aa01991a":"markdown","b1005dea":"markdown","56366695":"markdown"},"source":{"27bc0199":"import pandas as pd\nimport numpy as np\nimport math\n\nimport holidays\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    StandardScaler, OneHotEncoder, FunctionTransformer\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import (\n    train_test_split, KFold, GridSearchCV, ParameterGrid,\n)\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor, DMatrix, plot_importance\nfrom xgboost import cv as xgb_cv","c831e48f":"STUDY_START_DATE = pd.Timestamp(\"2015-01-01 00:00\", tz=\"utc\")\nSTUDY_END_DATE = pd.Timestamp(\"2020-01-31 23:00\", tz=\"utc\")","25dcbdfd":"de_load = pd.read_csv(\"..\/input\/western-europe-power-consumption\/de.csv\")\nde_load = de_load.drop(columns=\"end\").set_index(\"start\")\nde_load.index = pd.to_datetime(de_load.index)\nde_load.index.name = \"time\"\nde_load = de_load.groupby(pd.Grouper(freq=\"h\")).mean()\nde_load = de_load.loc[\n    (de_load.index >= STUDY_START_DATE) & (de_load.index <= STUDY_END_DATE), :\n]\nde_load.info()","fa4a681b":"def split_train_test(df, split_time):\n    df_train = df.loc[df.index < split_time]\n    df_test = df.loc[df.index >= split_time]\n    return df_train, df_test\n\ndf_train, df_test = split_train_test(\n    de_load, pd.Timestamp(\"2019-02-01\", tz=\"utc\")\n)","1d91341c":"ax = df_train[\"load\"].plot(figsize=(12, 4), color=\"tab:blue\")\n_ = df_test[\"load\"].plot(ax=ax, color=\"tab:orange\", ylabel=\"MW\")","28fd00a8":"df_train.loc[df_train[\"load\"].isna(), :].index","c1826772":"def add_time_features(df):\n    cet_index = df.index.tz_convert(\"CET\")\n    df[\"month\"] = cet_index.month\n    df[\"weekday\"] = cet_index.weekday\n    df[\"hour\"] = cet_index.hour\n    return df\n\ndef add_holiday_features(df):\n    de_holidays = holidays.Germany()\n    cet_dates = pd.Series(df.index.tz_convert(\"CET\"), index=df.index)\n    df[\"holiday\"] = cet_dates.apply(lambda d: d in de_holidays)\n    df[\"holiday\"] = df[\"holiday\"].astype(int)\n    return df\n\ndef add_lag_features(df, col=\"load\"):\n    for n_hours in range(24, 49):\n        shifted_col = df[col].shift(n_hours, \"h\")\n        shifted_col = shifted_col.loc[df.index.min(): df.index.max()]\n        label = f\"{col}_lag_{n_hours}\"\n        df[label] = np.nan\n        df.loc[shifted_col.index, label] = shifted_col\n    return df\n\ndef add_all_features(df, target_col=\"load\"):\n    df = df.copy()\n    df = add_time_features(df)\n    df = add_holiday_features(df)\n    df = add_lag_features(df, col=target_col)\n    return df\n","c9a33add":"df_train = add_all_features(df_train).dropna()\ndf_test = add_all_features(df_test).dropna()\ndf_train.info()","063a5839":"target_col = \"load\"\nX_train = df_train.drop(columns=target_col)\ny_train = df_train.loc[:, target_col]\nX_test = df_test.drop(columns=target_col)\ny_test = df_test.loc[:, target_col]","4d5c23a9":"def fit_prep_pipeline(df):\n    cat_features = [\"month\", \"weekday\", \"hour\"]  # categorical features\n    bool_features = [\"holiday\"]  # boolean features\n    num_features = [c for c in df.columns\n                    if c.startswith(\"load_lag\")]  # numerical features\n    prep_pipeline = ColumnTransformer([\n        (\"cat\", OneHotEncoder(), cat_features),\n        (\"bool\", FunctionTransformer(), bool_features),  # identity\n        (\"num\", StandardScaler(), num_features),\n    ])\n    prep_pipeline = prep_pipeline.fit(df)\n    \n    feature_names = []\n    one_hot_tf = prep_pipeline.transformers_[0][1]\n    for i, cat_feature in enumerate(cat_features):\n        categories = one_hot_tf.categories_[i]\n        cat_names = [f\"{cat_feature}_{c}\" for c in categories]\n        feature_names += cat_names\n    feature_names += (bool_features + num_features)\n    \n    return feature_names, prep_pipeline","bf85f6d0":"feature_names, prep_pipeline = fit_prep_pipeline(X_train)\n\nX_train_prep = prep_pipeline.transform(X_train)\nX_train_prep = pd.DataFrame(X_train_prep, columns=feature_names, index=df_train.index)\nX_test_prep = prep_pipeline.transform(X_test)\nX_test_prep = pd.DataFrame(X_test_prep, columns=feature_names, index=df_test.index)\n\nX_train_prep.info()","91034ecb":"lin_model = SGDRegressor(penalty=\"elasticnet\", tol=10, random_state=42)\nrf_model = RandomForestRegressor(\n    n_estimators=100, criterion='mse', min_samples_leaf=0.001, random_state=42\n)\nxgb_model = XGBRegressor(n_estimators=1000)","7c40d47a":"def compute_learning_curves(model, X, y, curve_step, verbose=False):\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    n_train_obs = X_train.shape[0]\n    n_iter = math.ceil(n_train_obs \/ curve_step)\n    train_errors, val_errors, steps = [], [], []\n    for i in range(n_iter):\n        n_obs = (i+1) * curve_step\n        n_obs = min(n_obs, n_train_obs)\n        model.fit(X_train[:n_obs], y_train[:n_obs])\n        y_train_predict = model.predict(X_train[:n_obs])\n        y_val_predict = model.predict(X_val)\n        train_mse = mean_squared_error(y_train[:n_obs], y_train_predict)\n        val_mse = mean_squared_error(y_val, y_val_predict)\n        train_errors.append(train_mse)\n        val_errors.append(val_mse)\n        steps.append(n_obs)\n        if verbose:\n            msg = \"Iteration {0}\/{1}: train_rmse={2:.2f}, val_rmse={3:.2f}\".format(\n                i+1, n_iter, np.sqrt(train_mse), np.sqrt(val_mse)\n            )\n            print(msg)\n    return steps, train_errors, val_errors\n\ndef plot_learning_curves(steps, train_errors, val_errors, ax=None, title=\"\"):\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=(6, 4))\n    train_rmse = np.sqrt(train_errors)\n    val_rmse = np.sqrt(val_errors)\n    ax.plot(steps, train_rmse, color=\"tab:blue\",\n            marker=\".\", label=\"training\")\n    ax.plot(steps, val_rmse, color=\"tab:orange\",\n            marker=\".\", label=\"validation\")\n    ylim = (0.8*np.median(train_rmse),\n            1.5*np.median(val_rmse))\n    ax.set_ylim(ylim)\n    ax.set_xlabel(\"Number of observations\")\n    ax.set_ylabel(\"RMSE (MW)\")\n    ax.set_title(title)\n    ax.legend()\n    ax.grid()\n    ","baf9ec2b":"steps, train_mse, val_mse = compute_learning_curves(\n    lin_model, X_train_prep, y_train, 500, verbose=True\n)","eceb55e3":"plot_learning_curves(steps, train_mse, val_mse, title=\"Linear model\")","0534d027":"rf_steps, rf_train_mse, rf_val_mse = compute_learning_curves(\n    rf_model, X_train_prep, y_train, 500, verbose=True\n)","33ceba7e":"plot_learning_curves(rf_steps, rf_train_mse, rf_val_mse, title=\"Random forest\")","5db3c4af":"xgb_steps, xgb_train_mse, xgb_val_mse = compute_learning_curves(\n    xgb_model, X_train_prep, y_train, 500, verbose=True\n)","aa05d286":"plot_learning_curves(xgb_steps, xgb_train_mse, xgb_val_mse, title=\"XGB\")","14f4b4d3":"xgb_model.fit(X_train_prep, y=y_train)\n_, ax = plt.subplots(1, 1, figsize=(6, 6))\n_ = plot_importance(xgb_model, ax=ax, max_num_features=30)","1b24a502":"def xgb_grid_search_cv(\n    params_grid, X, y, nfold,\n    num_boost_round=1000, early_stopping_rounds=10,\n):\n    params_grid = ParameterGrid(params_grid)\n    search_results = []\n    print(f\"Grid search CV : nfold={nfold}, \" +\n          f\"numb_boost_round={num_boost_round}, \" +\n          f\"early_stopping_round={early_stopping_rounds}\")\n    for params in params_grid:\n        print(f\"\\t{params}\")\n        cv_df = xgb_cv(\n            params=params, dtrain=DMatrix(X, y), nfold=nfold,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            shuffle=False, metrics={\"rmse\"},\n        )\n        cv_results = params.copy()\n        cv_results[\"train-rmse-mean\"] = cv_df[\"train-rmse-mean\"].min()\n        cv_results[\"test-rmse-mean\"] = cv_df[\"test-rmse-mean\"].min()\n        search_results.append(cv_results)\n    return pd.DataFrame(search_results)\n    ","a99a80c7":"params_grid = dict(\n    eta = [0.05, 0.1, 0.3],\n    max_depth = [2, 4, 6],\n    min_child_weight = [5, 1]\n)\nxgb_search_scores = xgb_grid_search_cv(\n    params_grid, X_train_prep, y_train, nfold=4, early_stopping_rounds=10\n)","f8a676f4":"xgb_search_scores.sort_values(by=\"test-rmse-mean\")","1e1a5fa7":"final_model = XGBRegressor(\n    n_estimators=1000, learning_rate=0.05, max_depth=6, min_child_weight=5\n)\nfinal_model.fit(\n    X_train_prep, y_train, early_stopping_rounds=10,\n    eval_set=[(X_train_prep, y_train), (X_test_prep, y_test)],\n    verbose=False,\n)","ad63dcd5":"final_model.best_score","41902b2e":"def compute_predictions_df(model, X, y):\n    y_pred = model.predict(X)\n    df = pd.DataFrame(dict(actual=y, prediction=y_pred), index=X.index)\n    df[\"squared_error\"] =  (df[\"actual\"] - df[\"prediction\"])**2\n    return df\n\npred_df = compute_predictions_df(\n    final_model, X_test_prep, y_test\n)\npred_df.head()","13b1c23f":"def plot_predictions(pred_df, start=None, end=None):\n    _, ax = plt.subplots(1, 1, figsize=(12, 5))\n    start = start or pred_df.index.min()\n    end = end or pred_df.index.max()\n    pred_df.loc[\n        (pred_df.index >= start) & (pred_df.index <= end),\n        [\"actual\", \"prediction\"]\n    ].plot.line(ax=ax)\n    ax.set_title(\"Predictions on test set\")\n    ax.set_ylabel(\"MW\")\n    ax.grid()\n\nplot_predictions(pred_df)","bb6f877d":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-04-15\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2019-05-06\", tz=\"utc\"))","5436b1ff":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-12-16\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2020-01-06\", tz=\"utc\"))","f94a1b76":"daily_pred_df = pred_df.groupby(pd.Grouper(freq=\"D\")).mean()\ndaily_pred_df.sort_values(by=\"squared_error\", ascending=False).head(5)","92198890":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-06-19\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2019-06-22\", tz=\"utc\"))","4039c25d":"daily_pred_df.sort_values(by=\"squared_error\", ascending=True).head(5)","d562e98f":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2020-01-23\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2020-01-26\", tz=\"utc\"))","970f4888":"# Forecasting hourly electricity consumption of Germany","c9b89baa":"Gradient boosting model achieves reasonable performance (RMSE of \u00b11740MW) at load forecasting at country level. The most relevant features for this task are lag features, especially at H-24 and H-48. Week-ends and holidays have less impact, but are still relevant to use.\n\nTo further improve the model, additional features could be taken into account such as\n* additional lag features (beyond H-48),\n* regional holidays,\n* temperature data.","a88da35e":"There are no missing observations in our training data (there actually were a few missing observations on 15-min granularity, but we took care of these with hourly aggregation when loading the data).","9e0ba401":"### Features' importance","051a8d9e":"### Grid search cross validation","1bd4c4c1":"It is interesting to note that linear model performance significantly improves once we feed it the first year data (up to \u00b18760 hours), but does not further improve significantly as we feed it the next years' data.\n\nLet's now see what performance we achieve with the random forest. Validation error drops to \u00b12360MW while training error lies around 1730MW, which would indicate that the model overfits the training data.","a840a69a":"Electricity grid and market have become increasingly challenging to operate and maintain in the recent years. In particular, one of the main responsibilities of transmission system operators and aggregators consists in maintaining balance between production and consumption. With the development and spread of renewable energy sources, production has become more intermittent which requires even more effort to maintain the balance.\n\nOne of the underlying task of maintaining grid balance, it to forecast the consumption. In this analysis, we train and test a few models to forecast total german load, on an hourly basis, with a lead time of 24 hours.\n\nThe data was retrieved from [ENTSO-E Transparency Platform](https:\/\/transparency.entsoe.eu\/), which provides access to electricity generation, transportation, and consumption data for the pan-European market.\n\nThis notebook follows a structure similar to [this nice tutorial](https:\/\/www.kaggle.com\/robikscube\/tutorial-time-series-forecasting-with-xgboost) from [Rob Mulla](https:\/\/www.kaggle.com\/robikscube). The main adaptations are the following :\n* it applies to German load instead of PJM data covering US east region,\n* it includes additional features such as holidays and lag features,\n* a linear model, and a random forest are used as baselines in addition to the XGB model,\n* the final XGB model is finetuned with some grid search CV.","e669e68e":"Let's define our test set as the last 12 months of data","9d3c06b9":"Our final XGB model achieves RMSE score of \u00b11740MW on test set","d576461d":"Let's estimate feature importance based on XGB model trained on the whole training set","5b5381ed":"We will work with consumption data ranging from Jan 2015 to Jan 2020.","c56674c6":"## Data preparation","52440067":"## Fine-tuning the model","7a1fcbf7":"We then fit pipeline on training data, and apply it on training and test sets","5731aa25":"### Create features for training","a9619047":"### Best and worst prediction days","29a62396":"We then separate target values from features into distinct data frames.","8ebb33b4":"Comparing actual and predicted curves on the test set :","10671edf":"Here we observe that our model is slightly overestimating consumption during Christmas period and New Year's Eve.","c8f09b96":"## Conclusion and next steps","f24136e6":"The lag features introduce a few missing values which we will move out of the analysis. The features of our training set are then the following :","e5e991a0":"XGBoost model pushes RMSE even further to 2050MW with training RMSE of only 260MW. This indicates once again that our model is overfitting the training data. We will try to handle the overfitting later on at model fine-tuning step.","45c2e226":"## Predictions on test set","e6fd1943":"Performance drops slightly around 21st of April and on May 2nd. For the 2nd of May, this could be explained by the fact that 1st of May is a national holiday in Germany, and that the model is using lag features to estimate May 2nd volume using May 1st without properly adapting scale (as May 2nd is not a holiday).\n\nAppart from these, our model does a reasonable job at predicting volumes.","6bf862b0":"Our model is largely overestimating consumption on 20th of June. This date is actually a *regional* holiday in Germany (Fronleichnam) which are currently not taken into account by our model. These only and happen in specific states, but still have an impact on the load at country level.\n\nLooking at the best predictions, we have the following days :","425726bc":"Three models will be trained for our prediction task : a simple linear models with L1 and L2 regularisation, a random forest, and gradient boosting model (based on XGBoost library).","65ffdef9":"The following function applies grid search CV with XGB models. In particular, it uses the parameter `early_stopping_rounds` to interrupt training when validation error stops improving for $n$ iterations where $n$ is the parameter's value.\n\nEarly stopping is a way to prevent overfitting and reduce computation time. We will also try to reduce overfitting by optimizing XGB hyperparameters `eta` (learning rate) and `max_depth`.","91c7b5dd":"The German load data is originally available with 15-min resolution. We have resampled it on an hourly basis for this analysis.","c5e8d847":"### Learning curves","56b87308":"Based on previous result, we can train our final model on the whole training set","92aaa810":"We find back the Labour Day's and end of year's periods in the above list. We also observe poor performance on the 20th of June :","b3e01065":"In this part, we will start from the XGB model that we previously trained, and try to optimize its hyperparameters. To do so, we'll use a straightforward grid search approach (grid searh CV).","d9d362d3":"### Special time intervals","fb329c43":"### Data preparation pipeline","e52000d9":"Results of the grid search are the following :","588b4145":"Predictions were particularly accurate on 23rd and 25th of January 2020, as we can see from the graph below.","8f1cd41f":"Our training data covers roughly 20% of the data set","17f5d0d1":"The following features are used for training our forecast models :\n* time features: month, weekday and hour\n* national holiday features, as a boolean time series\n* lag features: load data with a lag values ranging from 24 to 48 hours","c5837287":"The linear model achieves a validation RMSE of \u00b13230MW. The training RMSE is \u00b13100MW which is relatively close.","6a359753":"We'll use the following data preparation pipeline to apply one-hot encoders on categorical feratures (time features), and a standard scaler on numerical features (lag features).","36a7bcbc":"The behaviour and performance of the previous models can be represented with learning curves. These are showing the models performance evolution based on experience (the amount of training data that is fed to the algorithm).","ab109b3b":"Let's group predicted and actual test data into a data frame","9bf421f7":"## Training regression models","a5482ac5":"The following code runs a 4-fold cross-validation on a hyperparameter grid of 18 possible combinations. So it will train a total of 72 models.","028bcb3f":"The days with worst prediction performance are the following :","88192795":"### Training our final model","aa01991a":"## Loading the data","b1005dea":"Based on this model, all the lag features have higher importance than time and calendar features. The two most important features are lags H-24 and H-48, i.e. the consumption yesterday and the day before at the same hour.","56366695":"The intervals \n* 15th Apr. 2019 \u2013 6th May 2019, and\n* 16th Dec. 2019 \u2013 6th Jan 2020\n\nseem slightly irregular compared to the rest. This is most likely due to holiday periods. Let's zoom on these periods  to check how our predictions perform. "}}