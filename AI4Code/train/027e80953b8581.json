{"cell_type":{"31878204":"code","bce4fefe":"code","e4bf4299":"code","196ef12d":"code","abb1d1ed":"code","ff2b65e3":"code","37b78954":"code","b2fe912f":"code","5f616fd4":"code","79c2be67":"markdown","34726755":"markdown","91aec760":"markdown","a04cbfdc":"markdown","c5050492":"markdown","470ed835":"markdown","2da2b434":"markdown","63fe4737":"markdown","5c23a05f":"markdown"},"source":{"31878204":"import csv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nimport time\nimport random\nimport math\nimport matplotlib.pyplot as plt","bce4fefe":"# turns a one hot vector array into digit labels\ndef to_label(y):\n    arr = np.empty(0)\n    for i in y:\n        arr = np.append(arr, np.argmax(i)).astype(int)\n    return arr\n\n# turns a digit label array into a one hot vector array\ndef one_hot_vector(y):\n    reformat_y = np.zeros((len(y), 10)).astype(int)\n    j = 0\n    while j < len(y):\n        reformat_y[j][int(y[j][0])] = 1\n        j += 1\n    return reformat_y\n\n# standardizes and normalizes training and testing data\ndef preprocess_data(data_arr, train_set=False):\n    if train_set:\n        train_features = data_arr\n    else:\n        train_features = pd.read_csv('..\/input\/digit-recognizer\/train.csv', header=None, sep=',', skiprows=1).to_numpy()[:, 1:]\n\n    # center around the mean\n    avg = np.average(train_features.reshape(1, -1))\n    data_arr = data_arr - avg\n    train_features = train_features - avg\n\n    # normalize using MinMax scaling\n    max, min = np.amax(train_features), np.amin(train_features)\n    data_arr = (data_arr - min) \/ (max - min)\n    return data_arr\n\n# load training data into features and labels numpy array\ndef load_train(file_path):\n    data = pd.read_csv(file_path, header=None, sep=',', skiprows=1).to_numpy()\n    new_data = np.empty((0, 28 * 28))\n    for i in range(data.shape[0]):\n        new_data = np.append(new_data, rotate_random(data[:, 1:][i], data[:, :1][i][0]).reshape(-1, 28 * 28), axis=0)\n    new_data = np.append(data[:, 0].reshape(-1, 1), new_data, axis=1)\n    # combine\n    data = np.append(data, new_data, axis=0)\n\n    # shuffle data\n    np.random.shuffle(data)\n\n    # separate into X, y\n    labels = one_hot_vector(data[:, :1])\n    features = data[:, 1:]\n    print(features.shape, labels.shape)\n\n    # normalize and standardize\n    features = preprocess_data(features, train_set=True)\n    return features.reshape(-1, 28, 28), labels\n\n# load test data into numpy array\ndef load_test(file_path):\n    features = pd.read_csv(file_path, header=None, sep=',', skiprows=1).to_numpy()\n\n    # normalize and standardize\n    features = preprocess_data(features)\n    return features.reshape(-1, 28, 28)\n\ndef show_random_images(image_arr, row, col):\n    image_arr = image_arr.reshape(-1, 28, 28)\n    plt.figure(figsize=(row*2, col*2))\n    for i in range(row * col):\n        plt.subplot(row, col, i + 1)\n        plt.imshow(image_arr[i, :, :], cmap='Greens', interpolation='nearest')\n    plt.show()\n\ndef rotate_random(img_arr, label):\n    img_arr = img_arr.reshape(28, 28)\n    \n    # generate random rotation in intervals of 10 degrees (excluding 0 and 360 --> no rotation) for numbers 1-5, 7, 8\n    # generate random rotation in intervals of 10 degrees between 260 and 80 degrees (excluding 0 and 360 --> no rotation)\n    # for numbers 6 and 9\n    if label == 6 or label == 9:\n        num = random.randint(0, 1)\n        if num == 1:\n            angle_deg = random.randint(1, 8) * 10\n        else:\n            angle_deg = random.randint(26, 35) * 10\n    else:\n        angle_deg = random.randint(1, 35) * 10\n\n    angle_rad = math.radians(angle_deg)\n    rotation_matrix = np.array([[math.cos(angle_rad), math.sin(angle_rad)], [math.sin(angle_rad) * -1, math.cos(angle_rad)]])\n    new_img = np.zeros((28, 28))\n    for i in range(28):\n        for j in range(28):\n            center = 28 \/ 2\n            coord_arr = np.array([[-i + center], [-j + center]])\n            new_coordinates = rotation_matrix @ coord_arr\n            new_coordinates = new_coordinates.astype(int)\n            if abs(new_coordinates[0][0]) < center and abs(new_coordinates[1][0]) < center:\n                new_coordinates = abs(new_coordinates - center + 1)\n\n                new_img[int(new_coordinates[0][0])][int(new_coordinates[1][0])] = img_arr[i][j]\n    return new_img.ravel()","e4bf4299":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv', header=None, sep=',', skiprows=1).to_numpy()\nnew_data = np.empty((0, 28*28))\nfor i in range(data.shape[0]):\n    new_data = np.append(new_data, rotate_random(data[:, 1:][i], data[:, :1][i][0]).reshape(-1, 28*28), axis=0)\nnew_data = np.append(data[:, 0].reshape(-1, 1), new_data, axis=1)\n\nshow_random_images(new_data[:, 1:], 3, 4)","196ef12d":"LEARNING_RATE = 0.0001\nEPOCHS = 25\nBATCH_SIZE = 50\nTRAIN = True\nMODEL_NAME = f\"handwritten_digit_model-{int(time.time())}\"\nsign_totals = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n","abb1d1ed":"train_X, train_y = load_train('..\/input\/digit-recognizer\/train.csv')\n\nvalidate_X = train_X[:2000, :, :]\nvalidate_y = train_y[:2000, :]\ntrain_X = train_X[2000:, :, :]\ntrain_y = train_y[2000:, :]\n\n\nprint(train_X.shape, train_y.shape)\nprint(validate_X.shape, validate_y.shape)\n\ntrain_X = torch.from_numpy(train_X).type('torch.FloatTensor')\ntrain_y = torch.from_numpy(train_y)\n\nvalidate_X = torch.from_numpy(validate_X).type('torch.FloatTensor')\nvalidate_y = torch.from_numpy(validate_y)\n","ff2b65e3":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=2)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=2)\n        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2)\n\n        self.fc1 = nn.Linear(128*3*3, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n        x = F.relu(F.max_pool2d(self.conv4(x), 2))\n        x = F.relu(F.max_pool2d(self.conv5(x), 2))\n\n        x = x.view(-1, 3*3*128)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=0.3, training=self.training)\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n","37b78954":"def feed_model(X, y, train=False):\n    if train:\n        handwritten_digit_cnn.zero_grad()\n    outputs = handwritten_digit_cnn(X)\n    compare = zip(outputs, y)\n    num_correct = 0\n    for n, m in compare:\n        a = torch.argmax(n)\n        b = torch.argmax(m)\n        if a == b:\n            num_correct += 1\n    accuracy = num_correct\/len(y)\n    y = torch.from_numpy(to_label(y.cpu().numpy()))\n    y = y.to(device)\n    loss = loss_fn(outputs, y.long())\n    if train:\n        loss.backward()\n        optimizer.step()\n    return accuracy, loss\n\ndef test(size):\n    random_start = np.random.randint(len(test_X) - size)\n    X, y = test_X[random_start:random_start + size], test_y[random_start:random_start + size]\n    with torch.no_grad():\n        test_accuracy, test_loss = feed_model(X.view(-1, 1, 28, 28).to(device), y.to(device))\n    return test_accuracy, test_loss","b2fe912f":"# Check for GPU, if no GPU, use CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(\"Running on the GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on the CPU\")\n\n# create model, optimizer and loss function \nhandwritten_digit_cnn = Digit_net().to(device)\noptimizer = optim.Adam(handwritten_digit_cnn.parameters(), lr=LEARNING_RATE)\nloss_fn = nn.CrossEntropyLoss()\n\n# training method, includes a log file to track training progress\ndef train():\n    global LEARNING_RATE\n    NUM_BATCH = 100 # don't want to write to log file every batch, set \"NUM_BATCH\"  to test every NUM_BATCH pass\n    with open(\"handwritten_digit_model.log\", \"a+\") as f:\n        init_time = time.time()\n        for epoch in range(EPOCHS):\n            print(epoch)\n\n                        for i in range(0, len(train_X), BATCH_SIZE):\n                batch_x = train_X[i:i + BATCH_SIZE].view(-1, 1, 28, 28)\n                batch_y = train_y[i:i + BATCH_SIZE]\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                train_accuracy, train_loss = feed_model(batch_x, batch_y, train=True) #train model with batch data\n                if i % NUM_BATCH == 0:\n\n                    test_accuracy, test_loss = test(size=100)\n                    f.write(\n                        f\"{MODEL_NAME}, {round(time.time()-init_time, 4)}, {int(epoch)}, {round(float(test_accuracy), 5)}, {round(float(test_loss), 5)}, {round(float(train_accuracy), 5)}, {round(float(train_loss), 5)}\\n\")\n        \n# set TRAIN=True to train model\nif(TRAIN):\n    train()\n    torch.save(handwritten_digit_cnn.state_dict(), \"handwritten_digit_model.pt\")","5f616fd4":"# load trained model\nhandwritten_digit_cnn = Digit_net()\nhandwritten_digit_cnn.load_state_dict(torch.load(\".\/handwritten_digit_model.pt\", map_location=device))\n\n# method predicts for one image\ndef predict(input):\n    try:\n        w, l = input.shape\n    except ValueError:\n        _, w, l = input.shape\n\n    input_tensor = torch.from_numpy(input).view(-1, w, l).type('torch.FloatTensor').to(device)\n    predict_vect = handwritten_digit_cnn(input_tensor.view(-1, 1, w, l))\n    predict_vect = predict_vect.cpu()\n    return predict_vect.detach().numpy()\n\ntest_set = load_test('..\/input\/digit-recognizer\/test.csv')\nprint(test_set.shape)\npredictions = predict(test_set)\nwith open('submission.csv', mode='w') as file:\n    headings = ['ImageId','Label']\n    writer = csv.DictWriter(file, fieldnames=headings, lineterminator = '\\n')\n    writer.writeheader()\n    image = 1\n    for prediction in predictions:\n        value = np.argmax(prediction).astype(int)\n        writer.writerow({'ImageId': image, 'Label': value})\n        image += 1\n","79c2be67":"The train method in this cell does the model training.  The train method includes a logfile that writes training and testing accuracy every 100 batches to check if we're overfitting and keep track of progress","34726755":"<h2>CNN for Digit Recognizer<\/h2>\n\n<p>I made a CNN using pytorch for the handwritten digit MNIST dataset.  I'm interested in the effects of network architechtue changes and what effects modifications to the data and data pre processing will have on the output so I'm experimenting here and tracking the changes.  I'm pretty new to neural networks so any feedback\/suggestions on this mini project, comments on why some methods are working or where to go for future learning is welcome.<\/p>\n\n1. <span style=\"color:red\">accuracy = 0.97482<\/span>, 3 layers with learning rate 0.001, 3x3 kernels and 32, 64, 72 filters in each layer, respectively.\n2. <span style=\"color:red\">accuracy = 0.98557<\/span>, 3 layers with learning rate reduced to 0.0001, 3x3 kernels and 32, 64, 128 filters in each layer, respectively\n3. <span style=\"color:red\">accuracy = 0.97928<\/span>, 5 layers with learning rate 0.0001, 3x3 kernels and 64, 64, 64, 64, 128 filters in each layer, respectively.  I reduced the dropout layer from p=0.5 to p=0.3 in this architecture.  In this attempt I followed some simple CNN performance improvement suggestions outlined in [this medium article](https:\/\/medium.com\/@dipti.rohan.pawar\/improving-performance-of-convolutional-neural-network-2ecfe0207de7) and used data augmentation by creating a function that rotates each image by a random angle between 10 and 350 (in increments of 10) and adds the rotated images to the original training set, doubling the training data size. I plotted the confusion matrix for the validation set and it indicated a lower accuracy for number 6 and 9 but a higher accuracy for the other numbers than my previous models.  Based on this, the resulting lower score might be due to the fact that the image rotation is affecting the model's ability to distinguish between 6 and 9.\n4. <span style=\"color:red\">accuracy = 0.98864<\/span> In an effort to correct for the ambiguity I introduced into the predictions of 9 and 6, I kept the same architecture as in attempt 3 and limited rotation of the digits 9 and 6 to between 280 and 80 degrees, this did improve the results, slightly.  ","91aec760":"<h4>load data into torch tensors<\/h4>\nI'm separating out 2000 images from the training set that will never be used in training to be able to test if the model is overfitting or not learning by looking at the logfile that will be generated during training","a04cbfdc":"<h4>I have several functions that are useful in the data preprocessing in the following cell:<\/h4>\n\n* <span style=\"color:green\">**to_label**<\/span> and <span style=\"color:green\">**one_hot_vector**<\/span> are utility functions that convert labels from the one digit, numberical label from 0-9 to one hot vector format and from one hot vector format back to the one numerical label format again depending on which version is needed in the code\n*<span style=\"color:green\">**preprocess_data**<\/span> takes an input array and standardizes and normalizes the data.  Data is standardized around the mean pixel value over the entire training set and then normalized using min max scaling so that all of the pixel values are between 0 and 1.  This is applied to the training data and needs to be applied to any testing data that is fed through the resulting model\n* <span style=\"color:green\">**load_train**<\/span> and <span style=\"color:green\">**load_test**<\/span> (TODO: add description)\n* <span style=\"color:green\">**show_random_images**<\/span> lets you pass an array of images of shape (:, 28, 28) and how many images you want to visualize by specifying how many rows and columns of images you want displayed \n* <span style=\"color:green\">**rotate_random**<\/span> takes input as one flattened numpy image array and rotates it by a random angle using a rotation matrix to calculate the pixels new coordinates.  Any pixels rotated outside of the 28x28 frame size are cut off and any part of the frame that does not have existing image data to rotate in to it is padded with zeros. \n\nIn the current training script, the training dataset is doubled by taking the entire original training set and subjecting each image to random rotation and adding it to the training set.  Following this data augmentation step, the images are shuffled and pre processed as in previous versions.","c5050492":"<h4>Use trained model to write output<\/h4>","470ed835":"<h4>The class that defines the neural net architecture<\/h4>\n\n<p>I'm currently using 5 convolutional layers and two fully connected layers with max pooling, ReLU activations and 0.3 dropout after the first fully connected layer.","2da2b434":"<h4>Set some parameters for the model training, I find it easier to keep track of if they're all in one place<\/h4>","63fe4737":"<h3>Twelve randomly rotated images generated using the rotate_random function<\/h3>","5c23a05f":"The method <span style=\"color:green\">**feed_model**<\/span> feeds data through the model. It's designed to be used for both training and testing purposes.  The method <span style=\"color:green\">**test**<\/span> tests accuracy and loss on a random slice of the test data that I separated out from the training set at the beginning."}}