{"cell_type":{"3849b2e5":"code","a08424b1":"code","9010dca9":"code","5c48ca0c":"code","b25b6f34":"code","ba17a0e5":"code","62101146":"code","bfcc43ff":"code","cb210714":"code","cba70fa1":"code","506ac180":"code","726b33b8":"code","290f2fd1":"code","0d945da2":"code","10427901":"code","336bba33":"code","8ccd7325":"code","6b358a0b":"code","c5f7c815":"code","50ff6442":"code","756092a0":"code","6998c2a5":"code","05d44b46":"code","39428cee":"code","b6ba79f0":"code","ad366d6e":"code","beaf6df9":"markdown","41da052f":"markdown","68b4cdb6":"markdown"},"source":{"3849b2e5":"import numpy as np \nimport pandas as pd\nimport os \nimport matplotlib.pyplot as plt \nimport matplotlib.cm as cm\nimport seaborn as sns \nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA \nfrom sklearn.cluster import KMeans \nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\nimport warnings \n\nwarnings.simplefilter(\"ignore\")","a08424b1":"train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")\ntrain.head()","9010dca9":"\n'''\nPrepare the table in chronological order . \n'''\n\ntrain[\"time_step_class\"] = train.groupby(\"breath_id\").cumcount()\ntest[\"time_step_class\"] = test.groupby(\"breath_id\").cumcount()\n\npiv_train = train.pivot_table(values=\"u_in\", columns=\"time_step_class\", index=\"breath_id\")\npiv_test = test.pivot_table(values=\"u_in\", columns=\"time_step_class\", index=\"breath_id\")\n\nm = MinMaxScaler(feature_range=(0.0, 1.0)).fit(piv_train)\npiv_train = pd.DataFrame(m.transform(piv_train), columns=piv_train.columns, index=piv_train.index)\npiv_train = pd.DataFrame(m.transform(piv_train), columns=piv_train.columns, index=piv_train.index)\n\npiv_train.head()","5c48ca0c":"pca = PCA(n_components=3, random_state=42).fit(piv_train)\n\npca_train = pca.transform(piv_train)\npca_test = pca.transform(piv_test)\n\npca_train = pd.DataFrame(pca_train, columns=[\"c\"+str(c) for c in range(3)], index=piv_train.index)\npca_test = pd.DataFrame(pca_test, columns=[\"c\"+str(c) for c in range(3)], index=piv_test.index)\n\npca_train.head()","b25b6f34":"km = KMeans(n_clusters=2, random_state=42)\ny_km = km.fit_predict(pca_train)\ny_km_te = km.fit_predict(pca_test) ### \n\npca_train[\"cluster\"] = y_km\npca_test[\"cluster\"] = y_km_te ","ba17a0e5":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111 , projection='3d')\nsc = ax.scatter(pca_train.iloc[:, 0], \n                pca_train.iloc[:, 1],\n                zs=pca_train.iloc[:, 2],\n                zdir='z',\n                s=50,\n                vmin=0,\n                vmax=1,\n                c=pca_train.iloc[:, 3],\n                cmap=plt.cm.jet) \nplt.colorbar(sc)\nplt.show()","62101146":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111 , projection='3d')\nsc = ax.scatter(pca_test.iloc[:, 0], \n                pca_test.iloc[:, 1],\n                zs=pca_test.iloc[:, 2],\n                zdir='z',\n                s=50,\n                vmin=0,\n                vmax=1,\n                c=pca_test.iloc[:, 3],\n                cmap=plt.cm.jet) \nplt.colorbar(sc)\nplt.show()","bfcc43ff":"\n# merge \npca_train[\"breath_id\"] = pca_train.index \npca_train = pca_train.reset_index(drop=True)\npca_train = pca_train[[\"breath_id\", \"cluster\"]]\ntrain = pd.merge(train, pca_train, how=\"left\", on=\"breath_id\")\n\npca_test[\"breath_id\"] = pca_test.index \npca_test = pca_test.reset_index(drop=True)\npca_test = pca_test[[\"breath_id\", \"cluster\"]]\ntest = pd.merge(test, pca_test, how=\"left\", on=\"breath_id\")\n\n\n# helper \ndef find_cluster_r_c(df):\n    fig, ax = plt.subplots(2, 2, figsize=(15, 6))\n    for c in range(2):\n        for r_c in range(2):\n            x = df.loc[df.cluster == c, \"R\" if r_c == 0 else \"C\" ]\n            sns.countplot(x, ax=ax[c][r_c])\n            ax[c][r_c].set_title(f\"Cluster={c}\")\n    plt.tight_layout()\n    \n    \ndef find_cluster_transition(df, is_train=True):\n    fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n    for c in range(2):\n        x = df.loc[df.cluster == c]\n        breath = x.breath_id.unique()\n        for n in range(5):\n            if is_train:\n                xx = x.loc[x.breath_id == breath[n], [\"time_step\", \"u_in\", \"u_out\", \"pressure\"]]\n            else:\n                xx = x.loc[x.breath_id == breath[n], [\"time_step\", \"u_in\", \"u_out\"]]\n            xx.set_index(\"time_step\").plot(ax=ax[c][n])\n            ax[c][n].set_title(f\"breath_id={breath[n]}\")\n            ax[c][n].set_xticks([])\n            \n            if n == 0:\n                ax[c][n].set_ylabel(f\"Cluster={c}\")\n    plt.tight_layout()","cb210714":"find_cluster_r_c(train)","cba70fa1":"find_cluster_r_c(test)","506ac180":"\n'''\nThe difference in the distribution of u_in is clear by comparing the two.\n'''\n\nfind_cluster_transition(train)","726b33b8":"find_cluster_transition(test, False)","290f2fd1":"train[[\"time_step\", \"u_in\", \"pressure\", \"cluster\"]].groupby(\"cluster\").mean().T","0d945da2":"pd.crosstab(train.cluster, [train.R, train.C]).T","10427901":"# GroupFold \ntrain.drop([\"time_step_class\", \"id\"], axis=1, inplace=True)\ntest.drop([\"time_step_class\", \"id\"], axis=1, inplace=True)\n\ndef k_split(df):\n    kf = GroupKFold(n_splits=2)\n    for i, (v, t) in enumerate(kf.split(df, df.pressure, groups=df.breath_id)):\n        df.loc[v, \"fold\"] = int(i)\n    df[\"fold\"] = df.fold.astype(np.uint8)\n    return df \n\ntrain = k_split(train)\n","336bba33":"train.head()","8ccd7325":"def fit(train, test):\n    os.makedirs(\"models\", exist_ok=True)\n    x, y = train.drop([\"pressure\",\"breath_id\"], axis=1), train[[\"pressure\", \"fold\"]]\n    use_col = x.drop(\"fold\", axis=1).columns \n    x_test = test[use_col]\n    predict = []\n    models = []\n    for fold in range(2):\n        x_train, y_train = x[x.fold != fold].drop(\"fold\", axis=1), y[y.fold != fold].drop(\"fold\", axis=1)\n        x_val, y_val = x[x.fold == fold].drop(\"fold\", axis=1), y[y.fold == fold].drop(\"fold\", axis=1)   \n        \n        model = LGBMRegressor(random_state=42, n_estimators=1000).fit(x_train,\n                                                                      y_train, \n                                                                      eval_set=[(x_train, y_train), (x_val, y_val)], \n                                                                      early_stopping_rounds=10,\n                                                                      verbose=100)\n        pred_test = model.predict(x_test)\n        predict.append(pred_test)\n        models.append(model)\n    predict = np.mean(predict, axis=0)\n    return predict, models","6b358a0b":"pred, models = fit(train, test)\n\n# a simple submission \nsub = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\nsub[\"pressure\"] = pred\nsub.to_csv(\"submission.csv\", index=False)","c5f7c815":"\n'''\nLet's see the difference between each by separating each cluster when making a prediction. \nAnalyze how u_out affects forecast data.\n'''\n\ndef predict(models, df):\n    predict = []\n    for model in models:\n        pred = model.predict(df.drop([\"breath_id\", \"pressure\", \"fold\"], axis=1))\n        predict.append(pred)\n    predict = np.mean(predict, axis=0).tolist()\n    return predict \n\ndef predict_cluster(models, df):\n    df_list = []\n    for c in range(2):\n        c_df = df[df.cluster == c]\n        pred = predict(models, c_df)\n        c_df_ = c_df.copy()\n        c_df_[\"predict\"] = pred \n        df_list.append(c_df_)\n    return df_list[0], df_list[1]\n\n\ndef metrics(df):\n    out_0 = mean_absolute_error(df.loc[df.u_out == 0, \"pressure\"], df.loc[df.u_out == 0,\"predict\"])\n    out_1 = mean_absolute_error(df.loc[df.u_out == 1, \"pressure\"], df.loc[df.u_out == 1, \"predict\"])\n    alls = mean_absolute_error(df[\"pressure\"], df[\"predict\"])\n    return pd.DataFrame({\"u_out_0\": [out_0], \"u_out_1\": [out_1], \"all\": [alls]}, index=[\"Error\"]).T.style.background_gradient(cmap=\"coolwarm\")\n\ndef viz_predict(df, c_name=0):\n    breath = df.breath_id.unique()\n    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n    ax = axes.ravel()\n    for i in range(9):\n        x = df[df.breath_id == breath[i]]\n        x[[\"time_step\", \"u_in\", \"u_out\", \"pressure\", \"predict\"]].set_index(\"time_step\").plot(ax=ax[i])\n        ax[i].set_title(f\"Cluster={c_name}, breath_id={breath[i]}\")\n    plt.tight_layout()","50ff6442":"c0, c1 = predict_cluster(models, train)","756092a0":"plt.subplot(121)\nsns.histplot(c0.pressure)\nplt.title(\"Cluster=0\")\nplt.subplot(122)\nsns.histplot(c0.predict)","6998c2a5":"plt.subplot(121)\nsns.histplot(c1.pressure)\nplt.title(\"Cluster=1\")\nplt.subplot(122)\nsns.histplot(c1.predict)","05d44b46":"metrics(c0)","39428cee":"\n'''\nComparing the error of the predicted value around 1.0,\nit is inferred that the degree of dispersion is wider before 1.0 second. \nBy comparing with cluster 0, the distribution difference before 1.0 second is clear.\n\n\nMy guess is that the score will fluctuate depending on whether\nor not you can accurately predict 1.0 seconds ago from the data belonging to \ncluster 1 which is different from the normal distribution.\n'''\n\nmetrics(c1)","b6ba79f0":"viz_predict(c0)","ad366d6e":"viz_predict(c1, 1)","beaf6df9":"# Clustering ","41da052f":"# Train ","68b4cdb6":"# EDA Clustering"}}