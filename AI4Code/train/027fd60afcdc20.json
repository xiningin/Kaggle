{"cell_type":{"ffe96e86":"code","afb966c1":"code","00627a4b":"code","c5ad2788":"code","45958b74":"code","cc12b764":"code","6857abb0":"code","7ff9caca":"code","e60d681d":"code","2fd65040":"code","3f1365b1":"code","9e9ff3b5":"code","4965d92a":"code","1fff8deb":"code","9da25034":"code","b72f55c7":"code","83f36291":"code","582e5073":"code","6170c7dc":"code","761e2b4f":"code","251ac363":"code","1bddf080":"code","27f8af24":"code","32d66d65":"code","b4ed9381":"code","2624e46f":"code","194c6f19":"code","65b04940":"code","bc1d0d50":"markdown","3226c734":"markdown","e7b2c865":"markdown","318b028b":"markdown","b1c150c4":"markdown","ef77ba0b":"markdown","27f5c130":"markdown","09b61c9d":"markdown","0b74d396":"markdown","6211b361":"markdown","42a17387":"markdown","e44dbc14":"markdown","19ad4736":"markdown","1960b535":"markdown","60ed2aab":"markdown"},"source":{"ffe96e86":"# linear algebra\nimport numpy as np \n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","afb966c1":"house_sales = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\nhouse_sales.head(4)","00627a4b":"#Checking the null value count for each columns. There is no missing values\ntotal = house_sales.isnull().sum().sort_values(ascending=False)\ntotal","c5ad2788":"house_sales.columns","45958b74":"house_sales.shape","cc12b764":"house_sales.describe()","6857abb0":"house_sales.info()","7ff9caca":"house_sales['waterfront'].value_counts()","e60d681d":"house_sales['view'].value_counts()","2fd65040":"house_sales[\"yr_renovated\"].value_counts()","3f1365b1":"df_house = house_sales.copy()\ndf_house = df_house.drop([\"id\",\"waterfront\",\"view\",\"sqft_living15\", \"sqft_lot15\"],axis=1)","9e9ff3b5":"# Take the renovated years for the age of the building\ndf_house[\"year\"] = df_house[[\"yr_built\",\"yr_renovated\"]].max(axis=1)","4965d92a":"df_house[\"date\"] = df_house[\"date\"].str[:4].astype(int)","1fff8deb":"df_house.head(4)","9da25034":"# Instead of a year column, I create a new column called age of the built and drop the other unrequired columns\ndf_house[\"age\"] = df_house[\"date\"] - df_house[\"year\"]\ndf_house = df_house.drop([\"date\",\"yr_built\",\"yr_renovated\"], axis=1)","b72f55c7":"df_house.head(4)","83f36291":"corr = df_house.corr()\nf, ax = plt.subplots(figsize = (12,9))\nsns.heatmap(corr, vmax=.8, square = True); ","582e5073":"cols = corr.nlargest(15,'price')['price'].index\ncm = np.corrcoef(df_house[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize = (12,9))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","6170c7dc":"sns.set()\ncorrelated_columns = [\"price\",\"sqft_living\", \"grade\", \"sqft_above\",\"bathrooms\"]\nsns.pairplot(df_house[correlated_columns], size = 2.5)\nplt.show()","761e2b4f":"df_house = df_house.drop([\"year\",\"zipcode\",\"lat\", \"long\"], axis=1)\ndf_house.head(4)","251ac363":"X= df_house.iloc[:,1:].values\ny= df_house.iloc[:,0].values","1bddf080":"X","27f8af24":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)","32d66d65":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression() \nregressor.fit(X_train,y_train)","b4ed9381":"y_pred= regressor.predict(X_test)","2624e46f":"regressor.predict([[3, 2.25, 2570, 7242, 2, 3, 7,2170,400,23]])","194c6f19":"df_house.head(4)","65b04940":"import statsmodels.regression.linear_model as lm\n#The 0th column contains only 1 in each rows \nX= np.append(arr = np.ones((21613,1)).astype(int), values = X, axis=1) \nX_opt= X[:, [0,1,2,3,4,5,6,7,8,9,10]] \nregressor_OLS=lm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary() ","bc1d0d50":"**References**: \n\nhttps:\/\/medium.com\/pursuitnotes\/day-1-data-preprocessing-1c7d5b0b0eb7\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\nhttps:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices\n\nRegularization: https:\/\/en.wikipedia.org\/wiki\/Regularization_(mathematics)","3226c734":"**Backward Elimination Method**: \n\nIn some situations, some of the features have little impact on the result. So that they should be eliminated to prevent the shadow on the output. So building an optimal model can be achieved by using Backward Elimination. \nThe backward elimination function will give us the optimal variables from the dataset\n\n**Ordinary least squares:** \nThe regressor_OLS object will be created by statsmodels module, endog is the dependent variable, exog is the number of observations\n\n\nBeta0 has x^0=1. Add a column of for the the first term of the MultiLinear Regression equation.\n","e7b2c865":"Scatter plots between 'price' and correlated variables:","318b028b":"The pairplots reveals some outliers in the dataset. However,  I want to jump to build my multiple linear regressin model immediately. \n\n* LinearRegression fits a linear model with coefficients w = (w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n\n\n* Multiple linear regression attempts to model the relationship between two or more features and a response by fitting a linear equation to observed data.\n\nX (feature matrix) = a matrix of size n X p where x_{ij} denotes the values of jth feature for ith observation.\n\ny (response vector) = a vector of size n\n\n\n                                            4 Basic Assumptions for linear regression models: \n\n1.)  Linear relationship: Look at the scatter plot \n\n![image.png](attachment:image.png)\n\n2.)  Little or no multi-collinearity:  Multicollinearity occurs when the features (or independent variables) are not independent from each other.\n\n3.)  Little or no auto-correlation: Autocorrelation occurs when the residual errors are not independent from each other. \n\n4.) Homoscedasticity: Homoscedasticity describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables.\n\n![image.png](attachment:image.png)\n","b1c150c4":"> There are mainly 6 steps to build a multiple linear regression from such a dataset: \n> \n> 1. Dividing the dataset X (independent variables) vs y (dependent variables)\n> 2. Encoding the categorical variables. Avoiding the dummy variables\n> 3. Splitting the dataset into trainin and test datasets. \n> 4. Fitting the multiple linear regression model to our training set.\n> 5. Predicting the test dataset. Find the accuracy of the model\n> 6. Building the optimal Model using Backward Elimination","ef77ba0b":"> According to this magic HeatMap correlation plot: \n> \n> 1. sqft_living, grade, sqft_above and bathrooms are highly correlated to price of the houses\n> 2. sqrt_living and sqrt_above are also highly correlated with each other. \n> 3. Age of the built seems not correlated as anticipated due to the renovation date","27f5c130":"Look at the highest p-value again. It depends on the significance level determined. If your significance level is 0.05, it needs to remove this one. But in this case there seems no unrequired feature. \n\nPlease comment on this notebook especially how to deal with the Warning on OLS Regression result. ","09b61c9d":"> In this notebook, I want to reveal a comprehensive notebook to follow how to preprocess and analyze the dataset to make it ready for training. I will be appreciated if you **UPVOTE** this notebook to make it a popular one. \n> \n> \n> Let's start with importing the required packages and the dataset itself.","0b74d396":"Fitting multiple Linear Regression model to our Train set. Create an object called regressor in the LinearRegression class... \nFit the linear regression model to the training set... We use the fit method the arguments of the fit method will be training sets","6211b361":"# Understanding the Dataset","42a17387":"                                                          Analyze Steps:\n\n> Describe the dataset\n> \n> Handling null values\n> \n> Deal with categorical variables ?\n> \n> HeatMap\n> \n> Drop some columns\n> \n> Divide the dataset into x(input) and y(output)\n> \n> Analyze the output distribution\n> \n> Analyze the input variables\n> \n> Feature selection\n> \n> Split the dataset into training and test dataset\n> \n> Trainig the model with training dataset\n> \n> Predict the test dataset ","e44dbc14":"> **HeatMap** is the best way to get a quick overview of our dataset and its relationships. Plot rectangular data as a color-encoded matrix. This is an Axes-level function and will draw the heatmap into the currently-active \n\n\n**heatmap class:** \n\nvmin, vmaxfloats, optional: Values to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.","19ad4736":"> It seems more logical to delete the unnecessary columns before putting my dataset into a heatmap. My initial step is to handle some info and descriptions. ","1960b535":"> Those things reveal us to make some initial handling to prepare the data for the heatmap. \n> \n> 1- Drop the id, view and waterfront, sqft_living15, sqft_lot15 columns directly.\n> \n> 2- Combine the yr_built and yr_renovated into one column ","60ed2aab":"Splitting the dataset into the Training and Test dataset train_set_split: Split arrays or matrices into random train and test subsets random_state de\u011feri sonu\u00e7lar\u0131n her seferinde ayn\u0131 \u00e7\u0131kmas\u0131n\u0131 sa\u011flamak i\u00e7in kullan\u0131l\u0131yor.\n%20 of the dataset to the test set"}}