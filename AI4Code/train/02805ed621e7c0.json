{"cell_type":{"1e9ff5a0":"code","5d0b2bb2":"code","3098f226":"code","9a1f85d2":"code","770d9c50":"code","9e5d992d":"code","9fe90939":"code","74c67d48":"code","bc45e09c":"code","7417b673":"code","99b514a3":"code","c0047462":"code","9a7aca82":"code","7a04f9a9":"code","3bf6c8f1":"code","cff08ce7":"code","d27484a2":"code","35ed8e57":"code","d58a460d":"code","f5683f9d":"code","d14e220a":"code","c58506eb":"code","0efc6e56":"code","8ac53a22":"code","58c9d4d2":"code","87c5bc7a":"code","afdc359d":"code","670b391b":"code","a399e080":"code","1cca2db7":"code","f7c77f15":"code","1082d9be":"code","fb6bf19a":"code","64cbbf82":"code","95a2f47a":"code","7fd4027d":"code","5a4f21b0":"code","3d7b21ed":"code","54e5242b":"code","adf389f4":"code","88fec22d":"code","8b315370":"code","e5a2b7c9":"code","c12d81c1":"code","3514ccbd":"code","338b3dc8":"code","8792870c":"code","338a71ae":"code","24f153e7":"code","3f46b1b0":"code","e561dbdb":"code","a335978c":"code","69e02a61":"code","8ed6dd8e":"code","7109f439":"code","332daaa5":"code","14c857a9":"code","b0fe8f17":"code","b1b85108":"code","8a4b0dae":"code","a0e47efa":"code","0d81aca1":"markdown","80f464e5":"markdown","091c0b74":"markdown","7dc947cf":"markdown","4d760f67":"markdown","257e1a08":"markdown","e73a670a":"markdown","5b6d249b":"markdown","33da08a5":"markdown","5367539b":"markdown","c1f6fc60":"markdown","b584388c":"markdown"},"source":{"1e9ff5a0":"! conda install -c conda-forge gdcm -y","5d0b2bb2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom ast import literal_eval\nimport json","3098f226":"extract_path = \"\/kaggle\/input\/siim-covid-512-reshaped-corrected-bounded-box\/Extracted_Study_Series_Img.csv\"\ndf = pd.read_csv(extract_path)\ndf.head()","9a1f85d2":"df.tail()","770d9c50":"import os\ndata_dir = \"\/kaggle\/input\/siim-covid-512-reshaped-corrected-bounded-box\/resized_data\/resized_data\"\nNew_Image_Path = []\nfor index, row in df.iterrows():\n    img_path = row[\"Image_Name\"]\n    curr_set = row[\"Set_Name\"]\n    png_path = img_path.replace('.dcm','.png').zfill(16)\n    new_path = os.path.join(data_dir,curr_set,png_path)\n    New_Image_Path.append(new_path)\ndf[\"New_Image_Path\"] = New_Image_Path","9e5d992d":"len(\"0026720152f5.png\")","9fe90939":"n = 20\ntrain_df = df[df[\"Set_Name\"] == \"train\"]\nsample_train = train_df.sample(n)\nsample_train.reset_index(inplace = True)\n\ntest_df = df[df[\"Set_Name\"] == \"test\"]\nsample_test = test_df.sample(n)\nsample_test.reset_index(inplace = True)","74c67d48":"sample_train.head(2)","bc45e09c":"import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n%matplotlib inline\nimport cv2\nplt.style.use(\"dark_background\")","7417b673":"import pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport PIL\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\nviz_counter=0\n\ndef create_dir(dir, v=1):\n    \"\"\"\n    Creates a directory without throwing an error if directory already exists.\n    dir : The directory to be created.\n    v : Verbosity\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n        if v:\n            print(\"Created Directory : \", dir)\n        return 1\n    else:\n        if v:\n            print(\"Directory already existed : \", dir)\n        return 0\n\nvoi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename):\n    \"\"\"Credit: https:\/\/github.com\/pydicom\/pydicom\/issues\/319\n               https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    #====== DICOM IMAGE DATA ======\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n    else:\n        data = dicom_header.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    modified_image_data = (data * 255).astype(np.uint8)\n    \n    return dicom_dict, modified_image_data\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\"\"\"\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\"\"\"\ndef view_data(sample_train,view=\"original\"):\n    if view==\"resized\":\n        fig, axs = plt.subplots(4, 5, figsize=(20,20))\n        fig.subplots_adjust(hspace=.2, wspace=.2)\n        axs = axs.ravel()\n        for i in range(n):\n            img = cv2.imread(sample_train['New_Image_Path'][i])\n            axs[i].imshow(img,cmap='gray')\n            if type(sample_train['corrected_boxes'][i])==str and (not sample_train['corrected_boxes'][i]==\"\") and (not sample_train['corrected_boxes'][i]==np.nan) :\n                boxes = literal_eval(sample_train['corrected_boxes'][i])\n                for box in boxes:\n                    axs[i].add_patch(Rectangle((box['x'], box['y']), box['width'], box['height'], fill=0, color='y', linewidth=2))\n                axs[i].set_title(sample_train['study_label'][i])\n            else:\n                axs[i].set_title(sample_train['study_label'][i])\n    if view==\"original\":\n        fig, axs = plt.subplots(4, 5, figsize=(20,20))\n        fig.subplots_adjust(hspace=.2, wspace=.2)\n        axs = axs.ravel()\n        for i in range(n):\n            fpath = sample_train['Image_Path'][i]\n            dicom_dict, modified_image_data = dicom_dataset_to_dict(fpath)\n            # print(modified_image_data.shape)\n            # rgb2gray = lambda rgb : np.dot(rgb[... , :3] , [0.299 , 0.587, 0.114])   \n            img = modified_image_data\n            axs[i].imshow(img,cmap='gray')\n            if type(sample_train['boxes'][i])==str and (not sample_train['boxes'][i]==\"\") and (not sample_train['boxes'][i]==np.nan):\n                boxes = literal_eval(sample_train['boxes'][i])\n                for box in boxes:\n                    axs[i].add_patch(Rectangle((box['x'], box['y']), box['width'], box['height'], fill=0, color='y', linewidth=2))\n                axs[i].set_title(sample_train['study_label'][i])\n            else:\n                axs[i].set_title(sample_train['study_label'][i])\n        ","99b514a3":"view_data(sample_train,view=\"original\")","c0047462":"view_data(sample_train,view=\"resized\")","9a7aca82":"sample_test.head(2)","7a04f9a9":"view_data(sample_test,view=\"resized\")","3bf6c8f1":"df.tail(2)","cff08ce7":"!pip install openpyxl","d27484a2":"df.to_csv('Resized_MetaData.csv',index=False)\ndf.to_excel('Resized_MetaData.xlsx',index=False)","35ed8e57":"def subtract_lists(x,y):\n    \"\"\"Subtract Two Lists (List Difference)\"\"\"\n    return [item for item in x if item not in y]\ndef merge_list_to_dict(test_keys,test_values):\n    \"\"\"Using dictionary comprehension to merge two lists to dictionary\"\"\"\n    merged_dict = {test_keys[i]: test_values[i] for i in range(len(test_keys))}\n    return merged_dict\n# CLASSES = subtract_lists(list(set(df[\"study_label\"])),[np.nan])\nCLASSES = ['negative','indeterminate', 'typical',  'atypical'] # keep negative at start","d58a460d":"# IMAGE_LABELS = subtract_lists(list(set(df[\"image_label\"])),[np.nan])\nIMAGE_LABELS = ['none','opacity']","f5683f9d":"np.random.seed(42)","d14e220a":"sub_pth = \"..\/input\/siim-covid19-detection\/sample_submission.csv\"\ndf_sub = pd.read_csv(sub_pth)\ndf_sub.head()","c58506eb":"df_sub.tail()","0efc6e56":"idx_img = []\nidx_std = []\n\nfor index, row in df_sub.iterrows():\n    if row[\"id\"].endswith('_image'):\n        idx_img.append(int(index))\n    if row[\"id\"].endswith('_study'):\n        idx_std.append(int(index))","8ac53a22":"len(idx_img)","58c9d4d2":"len(idx_std)","87c5bc7a":"df_sub.shape","afdc359d":"df_sub_img = df_sub.iloc[idx_img]\ndf_sub_std = df_sub.iloc[idx_std]","670b391b":"df_sub_img.head()","a399e080":"df.head(2)","1cca2db7":"df_sub_img[\"Image_ID\"] = df_sub_img[\"id\"].str.replace(\"_image\",\"\")","f7c77f15":"df_img = df_sub_img.merge(df,on=\"Image_ID\")\ndf_img.head()","1082d9be":"df_sub_img.shape","fb6bf19a":"df_img.shape","64cbbf82":"CLASSES","95a2f47a":"CLASS_LABELLINGS = merge_list_to_dict(CLASSES,list(range(len(CLASSES))))\nCLASS_LABELLINGS","7fd4027d":"def get_preds(img = np.zeros((512,512))):\n    \"\"\"\n    Returns Classes and BBoxes\n    \"\"\"\n    Shape_X,Shape_Y = img.shape\n    Height_X = Shape_X\/4\n    Height_Y = Shape_Y\/4\n    num_preds = np.random.randint(low=0,high=4)\n    # print(num_preds4\n    CLASSES_IDX = []\n    CONFS = []\n    \n    BBOX = []\n    for i in range(num_preds):\n        CLS = np.random.randint(low=1,high=4) # 1,2,3\n        conf = np.random.randint(low=7,high=11)\/10 # Confidence > 7 - 7-10\n        x = np.random.randint(low=40,high=61) * Height_X\/100\n        y = np.random.randint(low=40,high=61) * Height_Y\/100\n\n        init_x = np.random.randint(low=Height_X,high=(Shape_X-Height_X))\n        init_y = np.random.randint(low=Height_Y,high=(Shape_Y-Height_Y))\n\n        data = {\"x\":init_x,\n                \"y\":init_y,\n                \"width\":x,\n                \"height\":y}\n        \n        BBOX.append(data)\n        CLASSES_IDX.append(CLS)\n        CONFS.append(conf)\n    \n    return CLASSES_IDX,BBOX,CONFS\n\n    \nget_preds()","5a4f21b0":"df_img[\"confidences\"] = str([])\ndf_img.head(2)","3d7b21ed":"std_value_cnts = df_img[\"Study_Name\"].value_counts()","54e5242b":"greater_than_one = std_value_cnts>1\ngreater_than_one.sum()","adf389f4":"CLASS_LABELLINGS","88fec22d":"CLASS_VALS = {v: k for k, v in CLASS_LABELLINGS.items()}\nCLASS_VALS","8b315370":"\"\"\"\nIMG_PREDS = []\nSTUDY_PREDS = []\n\"\"\"\nfor index, row in df_img.iterrows():\n    img_path = row[\"New_Image_Path\"]\n    img = cv2.imread(img_path,0)\n    # print(img.shape)\n    CLASSES_IDX,BBOX,CONFS = get_preds(img)\n    \n    if CLASSES_IDX==[]:\n        df_img.loc[index, \"image_label\"] = \"none\"\n        df_img.loc[index, \"study_label\"] = \"negative\"\n        df_img.loc[index, \"confidences\"] = str([])\n        df_img.loc[index, \"corrected_boxes\"] = str([])\n    else:\n        PRED_CLASSES = []\n        for cls_idx in CLASSES_IDX:\n            e_cls = CLASS_VALS[cls_idx]\n            PRED_CLASSES.append(e_cls)\n        df_img.loc[index, \"image_label\"] = \"opacity\"\n        df_img.loc[index, \"study_label\"] = \",\".join(PRED_CLASSES)\n        df_img.loc[index, \"confidences\"] = str(CONFS)\n        df_img.loc[index, \"corrected_boxes\"] = str(BBOX)","e5a2b7c9":"pd.set_option('display.max_columns', None)","c12d81c1":"df_img.head()","3514ccbd":"df_sub.head()","338b3dc8":"df_img['ecfy'] = 1\/df_img['cfy']\ndf_img['ecfx'] = 1\/df_img['cfx']","8792870c":"from ast import literal_eval\nfor index, row in df_img.iterrows():\n    cbbox =  literal_eval(row[\"corrected_boxes\"])\n    OLD_BOXES = []\n    for each_box in cbbox:\n        data = {\"x\":each_box[\"x\"]*row[\"ecfy\"],\n                \"y\":each_box[\"y\"]*row[\"ecfx\"],\n                \"width\":each_box[\"width\"]*row[\"ecfy\"],\n                \"height\":each_box[\"height\"]*row[\"ecfx\"]}\n        OLD_BOXES.append(data)\n    df_img.loc[index, \"boxes\"] = str(OLD_BOXES)","338a71ae":"df_img.head()","24f153e7":"view_data(df_img,view=\"resized\")","3f46b1b0":"view_data(df_img,view=\"original\")","e561dbdb":"df_img.head()","a335978c":"df_img[\"Pred_Img\"] = \"\"\ndf_img[\"Pred_Std\"] = \"\"","69e02a61":"df_img.tail(2)","8ed6dd8e":"for index, row in df_img.iterrows():\n    \n    Pred_Img = \"\"\n    Pred_Std = \"\"\n    study_label = row[\"study_label\"]\n    if study_label == \"negative\":\n        Pred_Img = \"negative 1 0 0 1 1\"\n        Pred_Std = \"none 1 0 0 1 1\"\n        # maybe insert confidences here (instead of the initial 1)!\n    else:\n        all_cls = study_label.split(\",\") if \",\" in study_label else [study_label]\n        bboxes =  literal_eval(row[\"boxes\"])\n        confs = literal_eval(row[\"confidences\"])\n        for each_class,each_box,each_conf in zip(all_cls,bboxes,confs):\n            # opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20\n            Pred_Img += \"opacity \"+str(round(each_conf,1))+\" \" + str(round(each_box[\"x\"],2))\n            Pred_Img +=  \" \"+ str(round(each_box[\"y\"],2))\n            Pred_Img +=  \" \" + str(round(each_box[\"width\"],2))+\" \" + str(round(each_box[\"height\"],2)) + \" \"\n            # indeterminate 1 0 0 1 1 atypical 1 0 0 1 1\n            Pred_Std += each_class + \" 1 0 0 1 1 \"\n    df_img.loc[index, \"Pred_Img\"] = str(Pred_Img)\n    df_img.loc[index, \"Pred_Std\"] = str(Pred_Std)","7109f439":"df_img[\"Pred_Img\"] = df_img[\"Pred_Img\"].str.strip()\ndf_img[\"Pred_Std\"] = df_img[\"Pred_Std\"].str.strip()","332daaa5":"df_img.head(10)","14c857a9":"df_img.to_csv('Prediction_Data.csv',index=False)\ndf_img.to_excel('Prediction_Data.xlsx',index=False)","b0fe8f17":"for index, row in df_sub.iterrows():\n    img_id = row[\"id\"]\n    try:\n        if img_id.endswith('_study'):\n            idx = df_img.index[df_img['Study_Name'] == img_id.replace(\"_study\",\"\")].tolist()\n            p = df_img[\"Pred_Std\"][idx[0]]\n        elif img_id.endswith('_image'):\n            idx = df_img.index[df_img['Image_ID'] == img_id.replace(\"_image\",\"\")].tolist()\n            p = df_img[\"Pred_Img\"][idx[0]]\n        df_sub.loc[index, \"PredictionString\"] = str(p)\n    except:\n        continue","b1b85108":"df_sub.head()","8a4b0dae":"df_sub.tail()","a0e47efa":"df_sub.to_csv('Random_Submission.csv',index=False)\ndf_sub.to_excel('Random_Submission.xlsx',index=False)","0d81aca1":"# Re-Verification of Scaling","80f464e5":"# Upscale the Predictions\n\nRemember that we have downscaled the image to a `(512,512)` shape before and we need to adjust the bounding boxes accordingly again.","091c0b74":"# Now go ahead and build some awesome Object Detection Models!\n\nHurray! It displays the same data! Also let's try to build a mock submission file that takes in predictions from a random model and then create the `submission.csv` file. ","7dc947cf":"Target:\n\n```\nId,PredictionString\n2b95d54e4be65_study,negative 1 0 0 1 1\n2b95d54e4be66_study,typical 1 0 0 1 1\n2b95d54e4be67_study,indeterminate 1 0 0 1 1 atypical 1 0 0 1 1\n2b95d54e4be68_image,none 1 0 0 1 1\n2b95d54e4be69_image,opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20\netc.\n```","4d760f67":"# View Resized Train Data","257e1a08":"### Ignoring these for now.\nWill be clarified once the confusion is dealt with.\n\nSee Discussion : https:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/244189 for more intricate details.","e73a670a":"### Hooray! The Scaled Images have been brought back to the original size","5b6d249b":"# View Original Train Data","33da08a5":"# Prepare Submission\n\n### Image Level","5367539b":"# Get Dummy Predictions","c1f6fc60":"```\nId,PredictionString\n2b95d54e4be65_study,negative 1 0 0 1 1\n2b95d54e4be66_study,typical 1 0 0 1 1\n2b95d54e4be67_study,indeterminate 1 0 0 1 1 atypical 1 0 0 1 1\n2b95d54e4be68_image,none 1 0 0 1 1\n2b95d54e4be69_image,opacity 0.5 100 100 200 200 opacity 0.7 10 10 20 20\netc.\n```","b584388c":"# Resized Data Paths"}}