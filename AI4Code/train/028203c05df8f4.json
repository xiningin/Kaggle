{"cell_type":{"b3d9ad9d":"code","0791a0c5":"code","ec158a7b":"code","7afcbd8b":"code","f91d3a55":"code","347e2c1c":"code","4bed92cd":"code","ba4f243c":"code","fc4a90a8":"code","452a9001":"code","4ad14011":"code","a7f13d3a":"code","7103c0bb":"code","102999ba":"code","465de7fc":"code","11b966a1":"code","2b9ad7f4":"code","1c00815d":"code","3a3f1b88":"code","78bcd295":"code","4e8dc2fb":"code","c823d65a":"code","48858c67":"code","4db86325":"code","f52479ef":"code","e36e3ff2":"code","6c878aae":"code","e0ec535b":"code","7f0c0c3b":"code","f6795d51":"code","c45f11a5":"code","58d8cfe0":"code","dc5a5d17":"code","dcaff70e":"code","f6ff006d":"code","2bc2ccd3":"code","fd6e3dc6":"code","d8be42cc":"code","5a8e4125":"code","32600f34":"code","e7b37457":"code","c8787c37":"code","60e61e9c":"code","11de8689":"code","6097d47e":"markdown","6216af2e":"markdown","f5f3d3c1":"markdown","66aed85e":"markdown","3b67a128":"markdown","4c7fb15e":"markdown","a8edb091":"markdown"},"source":{"b3d9ad9d":"!pip install catboost","0791a0c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import BaggingClassifier\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nimport lightgbm as lgb\nfrom mlens.ensemble import SuperLearner\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ec158a7b":"import pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","7afcbd8b":"train = pd.read_csv('..\/input\/train_LZdllcl.csv')\ntest = pd.read_csv('..\/input\/test_2umaH9m.csv')","f91d3a55":"train.head()","347e2c1c":"train.describe()","4bed92cd":"train.isnull().sum()","ba4f243c":"train.dtypes","fc4a90a8":"train['recruitment_channel'].value_counts()","452a9001":"train['education'].value_counts()","4ad14011":"plt.matshow(train.corr())\nplt.show()","a7f13d3a":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = train.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","7103c0bb":"plt.hist(train[train['KPIs_met >80%'] == 0]['previous_year_rating'])\nplt.show()","102999ba":"plt.hist(train[train['KPIs_met >80%'] == 1]['previous_year_rating'])\nplt.show()","465de7fc":"prev_rating_Three = train[(train['previous_year_rating'].isnull())][train['KPIs_met >80%'] == 0]['employee_id']\n\nfor empId in tqdm(prev_rating_Three):\n    train.loc[train['employee_id']==empId,'previous_year_rating'] = 3.0\n    \nprev_rating_Three = test[(test['previous_year_rating'].isnull())][test['KPIs_met >80%'] == 0]['employee_id']\n\nfor empId in tqdm(prev_rating_Three):\n    test.loc[test['employee_id']==empId,'previous_year_rating'] = 3.0","11b966a1":"prev_rating_Five = train[(train['previous_year_rating'].isnull())][train['KPIs_met >80%'] == 1]['employee_id']\n\nfor empId in tqdm(prev_rating_Five):\n    train.loc[train['employee_id']==empId,'previous_year_rating'] = 5.0\n    \nprev_rating_Five = test[(test['previous_year_rating'].isnull())][test['KPIs_met >80%'] == 1]['employee_id']\n\nfor empId in tqdm(prev_rating_Five):\n    test.loc[test['employee_id']==empId,'previous_year_rating'] = 5.0","2b9ad7f4":"train['education'] = train['education'].fillna('notGiven')\ntest['education'] = test['education'].fillna('notGiven')","1c00815d":"X = train.drop(columns=['employee_id','is_promoted'])\ny = train['is_promoted']\ntest.drop(columns=['employee_id'],inplace=True)","3a3f1b88":"X.head()","78bcd295":"X['no_of_trainings'].unique()","4e8dc2fb":"train.plot.hexbin('is_promoted','age',gridsize=15)","c823d65a":"def binAge(x):\n    age=''\n    if (x<26):\n        age='young'\n    elif (x>=26 and x<=36):\n        age='medium'\n    else:\n        age='old'\n    return age\n\ndef binAgeTwo(x):\n    age=''\n    if (x<=30):\n        age='young'\n    else:\n        age='medium'\n    return age","48858c67":"X['age_bin_Two'] = X['age'].apply(binAgeTwo)\nX['age_bin'] = X['age'].apply(binAge)\n\ntest['age_bin_Two'] = test['age'].apply(binAgeTwo)\ntest['age_bin'] = test['age'].apply(binAge)","4db86325":"X['frac_train'] = X['no_of_trainings']\/10\ntest['frac_train'] = test['no_of_trainings']\/10","f52479ef":"X['crit_score'] = ((X['previous_year_rating']*X['KPIs_met >80%'] + X['previous_year_rating']*X['awards_won?'] + X['previous_year_rating']*X['frac_train'] ) * (20) + X['avg_training_score'])\/400\ntest['crit_score'] = ((test['previous_year_rating']*test['KPIs_met >80%'] + test['previous_year_rating']*test['awards_won?'] + test['previous_year_rating']*test['frac_train'] ) * (20) + test['avg_training_score'])\/400   ","e36e3ff2":"sns.boxplot(y,X['crit_score'])","6c878aae":"contVar = set(X.columns) - set(['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two'])  \ncontVar = list(contVar)","e0ec535b":"flag = 1\niter = 0\nwhile(flag!=0):\n    iter = iter + 1\n    if(iter > 20):\n        break\n    skewed_feats = X[contVar].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    #print(\"\\nSkew in numerical features: \\n\")\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    print(skewness[np.abs(skewness['Skew'])>0.75])\n    skewnessBox = skewness[(skewness.Skew)>0.75]\n    skewnessSquare = skewness[(skewness.Skew)<-0.75]\n    if(skewnessBox.shape[0] == 0 and skewnessSquare.shape[0] == 0):\n        flag = 0\n    #print(\"There are {} skewed numerical features to Box Cox transform\".format(skewnessBox.shape[0]))\n    #print(\"There are {} skewed numerical features to Square transform\".format(skewnessSquare.shape[0]))\n    skewed_features1 = skewnessBox.index\n    skewed_features2 = skewnessSquare.index\n    #print(skewed_features1)\n    #print(skewed_features2)\n    lam = 0.15\n    for feat in skewed_features1:\n        X[feat] = boxcox1p(X[feat], lam)\n        test[feat] = boxcox1p(test[feat], lam)\n    for feat in skewed_features2:\n        X[feat] = np.square(X[feat])\n        test[feat] = np.square(test[feat])","7f0c0c3b":"X_dummies = X.copy()\nX_dummies = pd.get_dummies(X,columns=['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two'])    \ntest_dummies = pd.get_dummies(test,columns=['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two'])    ","f6795d51":"Xstand = X_dummies.copy()\nXmin = X_dummies.copy()\nXrob = X_dummies.copy()\n\nteststand = test_dummies.copy()\ntestmin = test_dummies.copy()\ntestrob = test_dummies.copy()","c45f11a5":"from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n\nstandScl = StandardScaler()\nminScl = MinMaxScaler()\nrobScl = RobustScaler()\n\nXstand[contVar] = standScl.fit_transform(X[contVar])\nXmin[contVar] = minScl.fit_transform(X[contVar])\nXrob[contVar] = robScl.fit_transform(X[contVar])\n\nteststand[contVar] = standScl.transform(test[contVar])\ntestmin[contVar] = minScl.transform(test[contVar])\ntestrob[contVar] = robScl.transform(test[contVar])","58d8cfe0":"def scoreOfModel(clf,X,y,flag,shuffleBool=False,nFolds=12):\n    score = 0\n    finalPreds = np.zeros(23490)\n    #trainPreds = np.zeros(54808)\n    folds = StratifiedKFold(n_splits=nFolds, shuffle=shuffleBool, random_state=42)\n    train_pred = cross_val_predict(clf, X, y, cv=12,method='predict_proba')\n    for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X,y))):\n        X_train,X_val = X.loc[trn_idx,:],X.loc[val_idx,:]\n        #X_train,X_val = X[trn_idx],X[val_idx]\n        y_train,y_val = y[trn_idx],y[val_idx]\n        clf.fit(X_train,y_train)\n        yPreds = clf.predict(X_val)\n        score += f1_score(y_val,yPreds)\n        if(flag==0):\n            finalPreds += clf.predict(teststand)\n        elif (flag==1):\n            finalPreds += clf.predict(testmin)\n        elif(flag==2):\n            finalPreds += clf.predict(testrob)\n        elif(flag==3):\n            p = clf.predict_proba(test)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        print(\"**********\"+ str(score\/(1+fold_)) + \"******************Iteration \"+str(fold_)+\" Done****************\")    \n    return str(score\/nFolds),(train_pred),(finalPreds\/nFolds)\n\ndef scoreOfModelTwo(clf,X,y,X_val,y_val):\n    clf.fit(X,y)\n    yPreds = clf.predict(X_val)\n    score = f1_score(y_val,yPreds)\n    finalPreds = clf.predict(test)\n    return str(score),(finalPreds)\n\n\ndef scoreOfModelLGB(clfr,X,y,flag,shuffleBool=False,nFolds=12):\n    score = 0\n    finalPreds = np.zeros(23490)\n    #trainPreds = np.zeros(54808)\n    folds = StratifiedKFold(n_splits=nFolds, shuffle=shuffleBool, random_state=42)\n    train_pred = cross_val_predict(clfr, X, y, cv=12,method='predict_proba')\n    for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X,y))):\n        X_train,X_val = X.loc[trn_idx,:],X.loc[val_idx,:]\n        #X_train,X_val = X[trn_idx],X[val_idx]\n        y_train,y_val = y[trn_idx],y[val_idx]\n        clf = clfr.fit(X_train,y_train)\n        yPreds = clf.predict(X_val)\n        score += f1_score(y_val,yPreds)\n        if(flag==0):\n            p = clf.predict_proba(teststand)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        elif (flag==1):\n            p = clf.predict_proba(testmin)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        elif(flag==2):\n            p = clf.predict_proba(testrob)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        elif(flag==3):\n            p = clf.predict_proba(test)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        elif(flag==4):\n            p = clf.predict_proba(test_dummies)\n            #q = clf.predict_proba(X)    \n            for k in range(len(p)):\n                finalPreds[k] += p[k][0]\n            #for l in range(len(q)):\n            #    trainPreds[l] += q[l][0]\n        print(\"**********\"+ str(score\/(1+fold_)) + \"******************Iteration \"+str(fold_)+\" Done****************\")    \n    return str(score\/nFolds),(train_pred),(finalPreds\/nFolds)\n","dc5a5d17":"catClf2 = CatBoostClassifier(learning_rate = 0.0353,iterations = 1500,eval_metric='F1',cat_features=['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two'])    \nclfDummies1 = lgb.LGBMClassifier(max_depth= 7, learning_rate=0.07532, n_estimators=402, num_leaves= 28, reg_alpha=2.154 , reg_lambda= 1.028)\nclfDummies2 = lgb.LGBMClassifier(max_depth= 10, learning_rate=0.1, n_estimators=308, num_leaves= 30, reg_alpha=1.0 , reg_lambda= 0.1)\nclfStand1 = lgb.LGBMClassifier(max_depth= 4, learning_rate=0.1, n_estimators=635, num_leaves= 30, reg_alpha=1.0 , reg_lambda= 0.1)\nclfmin1 = lgb.LGBMClassifier(max_depth= 4, learning_rate=0.1, n_estimators=635, num_leaves= 30, reg_alpha=1.0 , reg_lambda= 0.1)\n\nscr_clfDummies1,trainclfDummies1Preds,catclfDummies1Preds = scoreOfModelLGB(clfDummies1,X_dummies,y,4)\nscr_clfDummies2,trainclfDummies2Preds,catclfDummies2Preds = scoreOfModelLGB(clfDummies2,X_dummies,y,4)\nscr_clfStand1,trainclfStand1Preds,catclfStand1Preds = scoreOfModelLGB(clfStand1,Xstand,y,0)\nscr_clfmin1,trainclfmin1Preds,catclfmin1Preds = scoreOfModelLGB(clfmin1,Xmin,y,1)\nscr_catClf2,traincatClf2Preds,catClf2Preds = scoreOfModel(catClf2,X,y,3)","dcaff70e":"stackedDF = pd.DataFrame({'dummiesOne' : trainclfDummies1Preds[:,0], 'dummiesTwo' : trainclfDummies2Preds[:,0],\n                          'standOne' : trainclfStand1Preds[:,0], 'minOne' : trainclfmin1Preds[:,0],\n                          'catClf' : traincatClf2Preds[:,0]\n                         })\n\nstackedTest = pd.DataFrame({'dummiesOne' : catclfDummies1Preds , 'dummiesTwo' : catclfDummies2Preds,\n                          'standOne' : catclfStand1Preds, 'minOne' : catclfmin1Preds,\n                          'catClf' : catClf2Preds\n                         })","f6ff006d":"stackedDF.head()","2bc2ccd3":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = stackedDF.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","fd6e3dc6":"stackedTest.head()","d8be42cc":"# #bounds on different parameters \n# param_to_be_optimized = {'iterations':(600,1500),'learning_rate':(0.03,0.05),'depth':(3,10),\n#                         'l2_leaf_reg':(2,21)}\n\n\n# def param_handler_to_optimize(iterations,learning_rate,depth,l2_leaf_reg):\n#     \"\"\"\n#     To handle integer type parameters:\n    \n#     \"\"\"\n#     thread_count=-1\n#     iterations = int(iterations)\n#     depth = int(depth) #int type params\n#     #border_count = int(border_count)\n#     #ctr_border_count = int(ctr_border_count)\n    \n#     param = {\n#     'iterations': iterations,  # the maximum depth of each tree\n#     'learning_rate': learning_rate,  # the training step for each iteration\n#     'silent': True,  # logging mode - quiet\n#     'depth':depth,\n#     'l2_leaf_reg':l2_leaf_reg,\n#     #'border_count':border_count,\n#     #'thread_count':thread_count,\n#     'task_type':'GPU',\n#     'loss_function': 'CrossEntropy',  # error evaluation for multiclass training\n#     'cat_features':['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two']  \n#     }\n#     return func_to_be_optimized(param)\n\n# def func_to_be_optimized(param):\n    \n#     model = CatBoostClassifier(**param)    \n#     score = 0\n#     #finalPreds=np.zeros(23490)\n#     folds = StratifiedKFold(n_splits=12, shuffle=False, random_state=42)\n#     for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X,y))):\n#         X_train,X_val = X.loc[trn_idx,:],X.loc[val_idx,:]\n#         y_train,y_val = y[trn_idx],y[val_idx]\n#         model.fit(X_train,y_train)\n#         yPreds = model.predict(X_val)\n#         score += f1_score(y_val,yPreds)\n#     return (score\/12)\n\n\n\n# optimizer = BayesianOptimization(\n#     f=param_handler_to_optimize,\n#     pbounds=param_to_be_optimized,\n#     random_state=1,\n# )\n# optimizer.maximize(\n#     init_points=3,\n#     n_iter=75,\n# )\n#------------------------------------------------------------------------------------------------\n#bounds on different parameters \n# param_to_be_optimized = {'C':(0.001,20000)\n#                         }\n\n# def param_handler_to_optimize(C):\n#     \"\"\"\n#     To handle integer type parameters:\n    \n#     \"\"\"\n   \n#     param = {\n#     'C':C,\n#     #'max_depth':max_depth\n#     #'C':C,\n#     'solver':'liblinear'\n#     }\n#     return func_to_be_optimized(param)\n\n\n# param_to_be_optimized = {'max_depth': (2, 15),'learning_rate': (0.001, 0.5),'n_estimators': (10, 1000), \n#                          'num_leaves': (2,50),'reg_alpha': (0.01, 10),'reg_lambda': (0, 3)}                                          \n\n# def param_handler_to_optimize(max_depth,learning_rate,n_estimators,num_leaves,reg_alpha,reg_lambda):\n#     \"\"\"\n#     To handle integer type parameters:\n    \n#     \"\"\"\n#     #thread_count=-1\n#     max_depth = int(max_depth)\n#     n_estimators = int(n_estimators) #int type params\n#     num_leaves = int(num_leaves)  \n#     #border_count = int(border_count)\n#     #ctr_border_count = int(ctr_border_count)\n    \n#     param = {\n#     'max_depth' : max_depth,  # the maximum depth of each tree\n#     'learning_rate' : learning_rate,  # the training step for each iteration\n#     'n_estimators' : n_estimators,  # logging mode - quiet\n#     'num_leaves' : num_leaves,\n#     'reg_alpha' : reg_alpha,\n#     'reg_lamda' : reg_lambda,\n#     #'border_count':border_count,\n#     #'thread_count':thread_count,\n# #     'task_type':'GPU',\n# #     'loss_function': 'CrossEntropy',  # error evaluation for multiclass training\n# #     'cat_features':['previous_year_rating','department','region','education','gender','recruitment_channel','KPIs_met >80%','awards_won?','age_bin','age_bin_Two']  \n#     }\n#     return func_to_be_optimized(param)\n\n# def func_to_be_optimized(param):\n    \n#     #model = lgb.LGBMClassifier(**param)    \n#     train_pred_opt = cross_val_predict(LogisticRegression(**param), stackedDFL2, y, cv=12,method='predict_proba')\n#     train_pred_optTweaked = train_pred_opt[:,0]\n#     thresholds = np.linspace(0.01, 0.99, 50)\n#     mcc = np.array([f1_score(y, train_pred_optTweaked<thr) for thr in thresholds])\n#     #best_threshold = thresholds[mcc.argmax()]\n#     return (mcc.max())\n\n# optimizer = BayesianOptimization(\n#     f=param_handler_to_optimize,\n#     pbounds=param_to_be_optimized,\n#     random_state=1,\n# )\n# optimizer.maximize(\n#     init_points=3,\n#     n_iter=75,\n# )","5a8e4125":"metaLearner = RandomForestClassifier(max_depth=5,n_estimators =116) \n\nmetaLearner.fit(stackedDF,y)\npadRF = metaLearner.predict_proba(stackedDF)\npadRFTweaked = padRF[:,0]\n\nthresholds = np.linspace(0.01, 0.99, 50)\nmcc = np.array([f1_score(y, padRFTweaked<thr) for thr in thresholds])\nplt.plot(thresholds, mcc)\nbest_threshold = thresholds[mcc.argmax()]\nprint(mcc.max())\nprint(best_threshold)","32600f34":"# metaLearner = LogisticRegression(solver='liblinear',C=1.931)\n# metaLearner.fit(stackedDF,y)\n# padLR = metaLearner.predict_proba(stackedDF)\n# padLRtest = metaLearner.predict_proba(stackedTest)\n\n# padLR = cross_val_predict(metaLearner, stackedDF, y, cv=12,method='predict_proba')\n# padLRTweaked = padLR[:,0]\n\n# thresholds = np.linspace(0.01, 0.99, 50)\n# mcc = np.array([f1_score(y, padTweaked<thr) for thr in thresholds])\n# plt.plot(thresholds, mcc)\n# best_threshold = thresholds[mcc.argmax()]\n# print(mcc.max())\n# print(best_threshold)\n\n# metaLearner = lgb.LGBMClassifier(learning_rate=0.07149459085784728,\n#  max_depth= 4,\n#  n_estimators= 10,\n#  num_leaves= 49,\n#  reg_alpha= 9.71326474211533,\n#  reg_lambda= 0.36594150409622384) \n\n# metaLearner.fit(stackedDF,y)\n# padLGBM = metaLearner.predict_proba(stackedDF)\n# padLGBMtest = metaLearner.predict_proba(stackedTest)\n\n# padLGBM = cross_val_predict(metaLearner, stackedDF, y, cv=12,method='predict_proba')\n\n# padLGBMTweaked = padLGBM[:,0]\n\n# thresholds = np.linspace(0.01, 0.99, 50)\n# mcc = np.array([f1_score(y, padLGBMTweaked<thr) for thr in thresholds])\n# plt.plot(thresholds, mcc)\n# best_threshold = thresholds[mcc.argmax()]\n# print(mcc.max())\n# print(best_threshold)\n\n# stackedDFL2 = pd.DataFrame({'padRF' : padRF[:,0], 'padLR' : padLR[:,0],\n#                           'padLGBM' : padLGBM[:,0]\n#                          })\n\n# stackedTestL2 = pd.DataFrame({'padRF' : padRFtest[:,0] , 'padLR' : padLRtest[:,0],\n#                           'padLGBM' : padLGBMtest[:,0]\n#                          })\n\n#metaLearner = LogisticRegression(solver='liblinear',C=optimizer.max['params']['C'])\n# metaLearner = LogisticRegression(C=0.6064701322378264) \n\n# metaLearner.fit(stackedDFL2,y)\n# padRF2 = metaLearner.predict_proba(stackedDFL2)\n# padRF2Tweaked = padRF2[:,0]\n# #padRF2 = cross_val_predict(metaLearner, stackedDFL2, y, cv=12,method='predict_proba')\n# #padRF2Tweaked = padRF2[:,0]\n\n# thresholds = np.linspace(0.01, 0.99, 50)\n# mcc = np.array([f1_score(y, padRF2Tweaked<thr) for thr in thresholds])\n# plt.plot(thresholds, mcc)\n# best_threshold = thresholds[mcc.argmax()]\n# print(mcc.max())\n# print(best_threshold)","e7b37457":"sampleShuffle = pd.read_csv('..\/input\/sample_submission_M0L0uXE.csv')","c8787c37":"pad = metaLearner.predict_proba(stackedTest)\npadTweaked = pad[:,0]\n\nsampleShuffle['is_promoted'] = padTweaked<best_threshold\nsampleShuffle['is_promoted'] = np.where(sampleShuffle['is_promoted']==False,0,1)","60e61e9c":"sampleShuffle.to_csv('submissionShuffle.csv',index=False)","11de8689":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submissionShuffle.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\ncreate_download_link(sampleShuffle)","6097d47e":"**Predictions**","6216af2e":"> **Feature Engineering**","f5f3d3c1":"> **Preparing Data for Training**","66aed85e":"> **NAN Imputation**","3b67a128":"> **STACKING**","4c7fb15e":"**PARAMETER TUNING**","a8edb091":"> **Data PreProcessing**"}}