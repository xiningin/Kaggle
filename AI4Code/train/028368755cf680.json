{"cell_type":{"51f86d03":"code","2ac27863":"code","d833a035":"code","cb1bf5f9":"code","d664d026":"code","d42d0d29":"code","cf75a48a":"code","bd2beab3":"code","f486bab0":"code","1e86fd6e":"code","77623e99":"code","02c444a0":"code","892bf69a":"code","4c6a9f66":"code","6ba17780":"code","c4d8871b":"code","073e4cda":"code","cc80affc":"code","235fd593":"code","cd9bae5e":"code","cdcd655e":"code","f7a2936a":"code","8652be9f":"code","6f9dec61":"code","a42cdd45":"code","71b90ec9":"code","3733550d":"code","533de699":"code","e3275fac":"code","3a1d8cb9":"markdown","1cb5aebf":"markdown","720cac81":"markdown","bde1502a":"markdown","518a13e9":"markdown","4aa4dd26":"markdown","efe1cdce":"markdown","722d2440":"markdown","f0e17c33":"markdown","faee1ad1":"markdown","a3b2041d":"markdown"},"source":{"51f86d03":"!pip install praw -q #official python reddit API wrapper\n!python -m spacy download en_core_web_md","2ac27863":"import praw\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport string\nimport spacy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnlp = spacy.load(\"en_core_web_md\")\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import chi2\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"reddit_client_id\")\nsecret_value_1 = user_secrets.get_secret(\"reddit_client_secret\")","d833a035":"# Read-only instance\nreddit_read_only = praw.Reddit(client_id=secret_value_0, #your client id\n                               client_secret=secret_value_1, #your client secret\n                               user_agent=\"personal Reddit project by u\/Comfortable-Watch-92\")","cb1bf5f9":"subreddit = reddit_read_only.subreddit(\"EDAnonymous\")","d664d026":"sub_limit = 1000\nposts = subreddit.top(\"month\", limit=sub_limit)\n \nposts_dict = {\"Title\": [], \"Post Text\": [],\n              \"Flair\": [], \"Score\": [],\n              \"Total Comments\": [], \"Post URL\": []\n              }\n \nfor post in posts:\n    # Title of each post\n    posts_dict[\"Title\"].append(post.title)\n     \n    # Text inside a post\n    posts_dict[\"Post Text\"].append(post.selftext)\n     \n    # Unique ID of each post\n    posts_dict[\"Flair\"].append(post.link_flair_text)\n     \n    # The score of a post\n    posts_dict[\"Score\"].append(post.score)\n     \n    # Total number of comments inside the post\n    posts_dict[\"Total Comments\"].append(post.num_comments)\n     \n    # URL of each post\n    posts_dict[\"Post URL\"].append(post.url)\n \n# Saving the data in a pandas dataframe\ntop_posts = pd.DataFrame(posts_dict)","d42d0d29":"data = {'Text': top_posts['Title'] + ' ' + top_posts['Post Text'], 'Flair': top_posts['Flair']}\ndf = pd.DataFrame(data=data)\ndf.head()","cf75a48a":"df['Text'].tolist()[:5]","bd2beab3":"#drop rows (posts) with no flairs\ndf = df.dropna(subset=['Flair'])","f486bab0":"df['Flair'] = df['Flair'].apply(lambda x: re.sub(r'TW.*', 'TW', x))\ndf['Flair'] = df['Flair'].apply(lambda x: re.sub(r'Recovery.*', 'Recovery', x))\ndf['Flair'].value_counts(dropna=False)","1e86fd6e":"#select 5 largest categories and save the dataset to csv file\ncategories = ['Discussion', 'Rant \/ Rave', 'TW', 'Shitpost', 'Recovery']\ndf = df.loc[df['Flair'].isin(categories) == True,]\ndf.to_csv('raw_data.csv')","77623e99":"df = pd.read_csv('raw_data.csv') #export the dataset as a csv file","02c444a0":"def process(text):\n  nopunc = [char for char in text if char not in string.punctuation]\n  nopunc = ''.join(nopunc)\n  \n  clean = ' '.join(word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english'))\n  return clean\n\ndf['Text'] = df['Text'].map(lambda x: process(x))\n\ndf.head()","892bf69a":"X = df['Text']\ny = df['Flair']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nprint(len(X_train), len(X_test))","4c6a9f66":"def vectorize(text):\n  text = nlp(text)\n  vec = [word.vector for word in text]\n  return torch.tensor(sum(vec) \/ len(vec))","6ba17780":"X_train_tensor = torch.stack([vectorize(text) for text in X_train])\nX_test_tensor = torch.stack([vectorize(text) for text in X_test])\n\nmapping_dict = {'Discussion':0, 'Rant \/ Rave':1, 'TW':2, 'Shitpost':3, 'Recovery':4}\n\ny_train_tensor = torch.LongTensor(y_train.map(mapping_dict).values)\ny_test_tensor = torch.LongTensor(y_test.map(mapping_dict).values)","c4d8871b":"import torch.nn as nn\n\nclass SingleNN(nn.Module):\n  def __init__(self, input_size, output_size):\n    super().__init__()\n    self.fc = nn.Linear(input_size, output_size, bias=False)\n    nn.init.normal_(self.fc.weight, 0.0, 1.0)\n  \n  def forward(self,x):\n    x = self.fc(x)\n    return x","073e4cda":"model = SingleNN(input_size=X_train_tensor.size()[1], output_size=5)\ncriterion = nn.CrossEntropyLoss()\nmodel","cc80affc":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)","235fd593":"epochs = 300\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model(X_train_tensor)\n    loss = criterion(y_pred, y_train_tensor)\n    losses.append(loss)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # a neat trick to save screen space:\n    if i%10 == 1:\n        print(f'epoch: {i:3}  train_loss: {loss.item():10.8f}')","cd9bae5e":"rows = len(y_test_tensor)\ncorrect = 0\n\nwith torch.no_grad():\n    y_val = model(X_test_tensor)\n\nfor i in range(rows):\n    if y_val[i].argmax().item() == y_test_tensor[i]:\n        correct += 1\n\nprint(f'\\n{correct} out of {rows} = {100*correct\/rows:.2f}% correct')","cdcd655e":"cv = CountVectorizer()\n\nX_train_cv = cv.fit_transform(X_train)\nX_train_cv.shape","f7a2936a":"clf = LinearSVC()\nclf.fit(X_train_cv, y_train)","8652be9f":"X_test_cv = cv.transform(X_test)","6f9dec61":"predictions = clf.predict(X_test_cv)","a42cdd45":"# Report the confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))\n# Print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","71b90ec9":"clf_tfidf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC())])\n\n# Feed the training data through the pipeline\nclf_tfidf_lsvc.fit(X_train, y_train)\n\n# Form a prediction set\npredictions = clf_tfidf_lsvc.predict(X_test)\n# Print the overall accuracy\nprint('{}% correct'.format(round(metrics.accuracy_score(y_test,predictions)*100, 2)))","3733550d":"mapping_dict = {'Discussion':0, 'Rant \/ Rave':1, 'TW':2, 'Shitpost':3, 'Recovery':4}\n\ny = y.map(mapping_dict).values","533de699":"tfidf = TfidfVectorizer()\nfeat = tfidf.fit_transform(X).toarray()","e3275fac":"# chisq2 statistical test\nN = 5    # Number of examples to be listed\nfor f, i in sorted(mapping_dict.items()):\n    chi2_feat = chi2(feat, y == i)\n    indices = np.argsort(chi2_feat[0])\n    feat_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [w for w in feat_names if len(w.split(' ')) == 1]\n    print(\"\\nFlair '{}':\".format(f))\n    print(\"Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))","3a1d8cb9":"## linearSVC with count vectorizer","1cb5aebf":"### scraping\nYou can download the scraped dataset [here](https:\/\/www.kaggle.com\/matakahas\/reddit-redanomymous-dataset).","720cac81":"The code below scrapes the top 1000 posts of the current month, and stores relevant information to the dictionary","bde1502a":"### required packages","518a13e9":"## linearSVC with tf-idf vectorizer","4aa4dd26":"**Descriptive**: extract the top 5 words most correlated with each flair using tf-idf vectorizer (based on [this post](https:\/\/towardsdatascience.com\/predicting-reddit-flairs-using-machine-learning-and-deploying-the-model-using-heroku-part-2-d681e397f258))","efe1cdce":"split between train and test data","722d2440":"## single-layer neural network","f0e17c33":"# Reddit scraping project\nIn this project, I will scrape the subreddit [r\/EDAnonymous](https:\/\/www.reddit.com\/r\/EDAnonymous\/) and try different machine learning models to see whether they can predict the \"flair\" (tag) on posts. ","faee1ad1":"## to-dos\nMoving on, I'd like to collect more data (including comments in addition to post titles), and explore other algorithms to refine the model.","a3b2041d":"### pre-processing\nI'll rename some of the flairs with similar concepts to in order to reduce the number of categories. I will also remove stopwords and make the letters lower case."}}