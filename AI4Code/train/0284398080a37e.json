{"cell_type":{"d6e2db70":"code","0181dd64":"code","c99361d8":"code","3214cea7":"code","8ca1a034":"code","cbc46ff4":"code","7d0c4a37":"code","a591a24b":"code","e4a337ad":"code","ac9b4e63":"code","8fd9b028":"code","3b5d84c6":"code","9fd484a0":"code","7d501096":"code","4f13e4c2":"code","2eeac1e3":"code","475ab1eb":"code","0bd9ee3f":"code","797262d9":"code","28afb2c1":"markdown","532898d3":"markdown","e095c568":"markdown","49fa0596":"markdown","e588439a":"markdown","2d8c09d9":"markdown","7de0d5e4":"markdown","394c2c8c":"markdown"},"source":{"d6e2db70":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0181dd64":"import keras \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, Dropout, Embedding, Flatten\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n\nprint(pd.__version__)","c99361d8":"# read in csv file directly from Keras public IMBD dataset\n\ndf = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')\n","3214cea7":"df=pd.DataFrame(df)","8ca1a034":"df","cbc46ff4":"# df=df1.sample(n=50000, random_state=23)\n# Take a sample if necessary; for faster training\n\n#check data format\nprint(df.iloc[0])\n\n#check the labels in the dataset\ndf.label.value_counts()\n\n#converts label into integer values\ndf['label'] = df.label.astype('category').cat.codes\n\n#prints out dataframe\ndf","7d0c4a37":"#creates a new column for total words in each row, because we want to know the matrix dimenison.\ndf['total_words'] = df['comment'].str.count(' ') + 1\n\n#prints the dataframe at the index of it's longest review\nprint(df.loc[df.total_words.idxmax()])\n\n#prints the length of the longest view in the dataset\nprint(\"\\nThe longest comment is \" + str(df['total_words'].max()) + \" words.\\n\")","a591a24b":"# plot word frequency\n\nplt.figure(figsize=(15, 9))\nplt.hist([(df['total_words'])],bins=100,color = \"blue\")\nplt.xlabel('Number of word in a review')\nplt.ylabel('Frequency')\nplt.title('Review Words Count Distribution')\nplt.show()","e4a337ad":"# counts the number of classes\n# label the target feature\n\nnum_class = len(np.unique(df.label.values))\ny = df['label'].values\nprint(\"\\nThere are a total of \" + str(num_class) + \" classes.\")\n\n# evenly distributed dataset\ndf.groupby('label').count()","ac9b4e63":"# import CountVectorizer to find common words used in the dataset\n# the default CountVectorizer is unigram, which is what is needed in this situation to count the frequency of each words in the dataset\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# transform and vectorize the comments in a sparse matrix. The dimenion length is the number of instances, the width of the matrix is the\n# number of words in total corpse. The words that are not in the each review is padded with 0, hence the sparse matrix.\n\nvect_texts = vectorizer.fit_transform(list(df['comment']))\nall_ngrams = vectorizer.get_feature_names()\n\n# display top min(50,len(all_ngrams) of the most frequent words\nnum_ngrams = min(50, len(all_ngrams))\n\n# count the number of words in the total corpse\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\n# loop the words(features) with counts using zip function\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\n# Let's now plot a frequency distribution plot of the most seen words in the corpus.\nplt.figure(figsize=(15, 15))\nplt.bar(idx, counts, width=0.8, color = \"blue\")\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()\n","8fd9b028":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2,f_classif, mutual_info_classif\n\n# import a list of stopwords, when applied the stopwords in the corpose will not be tokenized, they will be skipped\nstopwords=stopwords.words('english')\n\n# min_df=2, any words that occurs less than 2 times in the total corpse will not be tokenized\n\nvectorizer = TfidfVectorizer(min_df=3, binary=True, analyzer='word',ngram_range= (1,2), stop_words=stopwords)\ndf_bigram = vectorizer.fit_transform(df['comment'])\n","3b5d84c6":"# There a few ways of selecting the number of words to be included in the training set\n# you can use Chi2, f_classif\n# specify the k features to be included in the training\n# the k features (the width of the matrix) will correspond to the number of unique words and\/or phrases (if using bigram or trigram etc)\n# to be included in the the training set\n# Chi2, f_classif and mutual_info_classif with all the same k-value will produce different sets of words to be included in the training dataset based on their calculation.\n# another way of putting it is that the dimensions are the same, but the sparseness or the number of features selected to be a part of the 1800 features will be different\n# personally I prefer chi2 (it's a measurement of feature dependency to the target, the higher the value the more relevant) as it produces a more dense matrix than f_classif\n\nk=26000\n\nselector = SelectKBest(chi2, k=min(k, df_bigram.shape[1]))\nselector.fit(df_bigram, df.label)\ntransformed_texts = selector.transform(df_bigram).astype('float32')","9fd484a0":"# there are 3.2 million elements (non zero elemets) in the matrix\n# they are words and phrases that are tokenized with idf weighting, this directly impacts training accuracy and speed\ntransformed_texts","7d501096":"# keep the matrix compressed in this case, you will get memorry error if you are trying to print the array. Kaggle only allocates 8gb of ram\ntransformed_texts=transformed_texts.toarray()","4f13e4c2":"#train test split\nX_train, X_test, y_train, y_test = train_test_split(transformed_texts, y, test_size=0.3)","2eeac1e3":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras import optimizers\n\n#builds input shape\n\n# This a fully connected network with number of total parameters = 1,668,354   \n# 2 hidden layers, with a droput of 0.5\n# relu is the activation function, sigmoid also works well in this case since there are not too many layers to diminish the learn rate\n# output function is sigmoid which is the same if you use softmax in a binary situation\n# training on entropy, RMS and accuracy\n\nmax_features = min(k, df_bigram.shape[1])\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=max_features, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='RMSprop',\n              metrics=['acc'])\n\n\n# prints out summary of model\nmodel.summary()\n\n# saves the model weights\n# the train\/validation\/test split is 26500\/8750\/15000, it's essential to hold out a decent chunck of unseen data\nfilepath=\"weights-simple.hdf5\"\ncheckpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory = model.fit([X_train], batch_size=64, y=to_categorical(y_train), verbose=1, validation_split=0.25, \n          shuffle=True, epochs=15, callbacks=[checkpointer])","475ab1eb":"#plot model\ndf_result = pd.DataFrame({'epochs':history.epoch, 'accuracy': history.history['acc'], 'validation_accuracy': history.history['val_acc']})\ng = sns.pointplot(x=\"epochs\", y=\"accuracy\", data=df_result, fit_reg=False)\ng = sns.pointplot(x=\"epochs\", y=\"validation_accuracy\", data=df_result, fit_reg=False, color='green')","0bd9ee3f":"#get prediction accuarcy for testing dataset 15000 samples\npredicted = model.predict(X_test)\npredicted_best = np.argmax(predicted, axis=1)\nprint (accuracy_score(predicted_best, y_test))\npredicted=pd.DataFrame(data=predicted)","797262d9":"print (classification_report(predicted_best, y_test))","28afb2c1":"without smoothing, if not seen in testing it will result in undefined idf value\n\n\n![image.png](attachment:image.png)","532898d3":"This is a demostration on how to use Keras to to perform sentiment classification on the IMDB review dataset\n\nMy code will use unigram and bigram to vectorize the comments. It is essential to understand the basic training methods of NLP before using more advance pre-trained NLP packages such as BERT or XLnet\n\nFor more on BERT- https:\/\/arxiv.org\/abs\/1810.04805\npublished in 2018\n\nin a nutshell, the traditional way of vectorizing language is using bigram, trigram etc. It is a forward looking conditional probability on predicting what the next word is going to be. However BERT pretrained model captures more context of a sentence by learning bidirectionally using the idea of MASKING the training matrix. However BERT lacks the forward or backward looking autoregressive mechanism.\n\nFor more on XLnet- https:\/\/arxiv.org\/abs\/1906.08237\npublished in 2019\n\nXLnet by standing on the shoulders of BERT, it allowed the model to capture even more context by calculating the maximum likehood of all permutation in a sentence. It is not restricted only learning bidirectional context. It also incorporates the idea of Two-stream self-attention stream which allows the model to aware of both the context and the position of each token\n\n","e095c568":"currently there are a couple of industy leading models in the space of NLP\n\nBERT and XLnet","49fa0596":"As we can see from the training validation chart the model is overfitting. The training set is approach nearly perfect accuracy. This is not necessary a bad thing. Initially we want the model to overfitt. Overfitting it good in the sense that there are a lot of tools to tapper overfitting. At least the availablity of data is not the barrier.\n\nIn this case we can even use:\n\n1. A even higher dropout rate, it seems that the neurons are still learning specific weights of some words\/phrases. But when dropout rate is too high, the model does lose accuracy\n\n2. Smaller batch size\n\n3. Higher L2 regulization, similar effect to dropout\n\n\nOverfitting it's a reflection of inefficiency in feature selection in this case, but as long as validation accuracy and testing accuracy are stable the model is a good shape.\n","e588439a":"We use TfidfVectorizer directly instead of countvectorizer to assign weights directly to each word based on their frequency. The more frequent the words are the less weight will be assigned to the word. This allows the Neural network to learn more context that matters\n\nFor more on inverse document-frequency equation:\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction\n\nn is the total frequency of a word in a particular document set (in one particular movie review)\n\ndf(t) is count of the word occuring in the entire document at least once ( for example, the number of time the word 'the' is **contained** in the entire 50000 review)\n\nthen the resulting vectors are nomralized by the Eucildean Norm, this is where the weight actually get reduced to 0, when it's very frequent.\n\nWith Smoothing, it allows the algothrim to calculate idf value even when in the testing dataset the word it is not seen.\n\n![image.png](attachment:image.png)\n","2d8c09d9":"Let's visualize the movie review words length distribution.\nBy looking at the distribution the data scientist should have an estimation or an idea of many features or the caughtoff length for training\nthe reason being that long corpse (a movie review consisting of 2000 words) does not necessarily provide more context than a movie review that\nconsists of 500 words perse\n","7de0d5e4":"1. total number of parameters in the network is 1,668,354 (not the number of inputs)\n\nfirst hidden layer\n\n(26000 x 64) + 64 which is first hidden layer itself = 1664064\n\nsecond hidden layer\n(64 x 64) + 64 which is the second hidden layer itself = 4160\n\noutput layer\n\n(64 x 2) + 2 which is the output layer itself = 130\n\nin total it's 1,668,354\n\nBut it reality the number of parameters should only be about a half because of the dropout rates\n","394c2c8c":"Some of these words are stop words, which provides no context or add any value in interpreting the sentiment of each review but be careful when deciding to manaually inserting stopwords. becareful not to just blindly remove top 50 frequent words, because words such as 'like', 'good', 'more' are a part of the top 50 most frequent words, however they do provide context to each review. stopwords do inflat the training matrix and should be removed for efficient training of the model. A common practise is to use nltk.corpus import stopwords package to skip tokenizing some of these words. The package is by no means perfec, data scientist should add additional stop words specific to the context and problem it's trying to solve."}}