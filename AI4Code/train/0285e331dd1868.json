{"cell_type":{"2cf25100":"code","da26a91f":"code","5b030f24":"code","a8825187":"code","34b6e65d":"code","2054dc36":"code","c905d7c8":"code","3d6d40b6":"code","52f71ddc":"code","4d52ed0f":"code","92c4c019":"code","a053ca40":"code","8c4f4254":"code","28771e29":"code","8f22d97b":"code","6e8c2200":"code","8087a586":"markdown","3ab53c3b":"markdown","59225ace":"markdown","2c2efd67":"markdown","79a9ebbf":"markdown","2be4704a":"markdown","4a00c296":"markdown","87ffe714":"markdown","3ee96da8":"markdown"},"source":{"2cf25100":"%matplotlib inline  \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\n","da26a91f":"data_raw = pd.read_csv('..\/input\/application_train.csv')\ndata_bureau = pd.read_csv('..\/input\/bureau.csv')\ndata_credit_card_balance = pd.read_csv('..\/input\/credit_card_balance.csv')\ndata_previous_application = pd.read_csv('..\/input\/previous_application.csv')\ndata_installments_payments = pd.read_csv('..\/input\/installments_payments.csv')\ndata_POS_CASH_balance = pd.read_csv('..\/input\/POS_CASH_balance.csv')","5b030f24":"# Group data by SK_ID_CURR to join all data in just one dataset\ndata_bureau_agg=data_bureau.groupby(by='SK_ID_CURR').mean()\ndata_credit_card_balance_agg=data_credit_card_balance.groupby(by='SK_ID_CURR').mean()\ndata_previous_application_agg=data_previous_application.groupby(by='SK_ID_CURR').mean()\ndata_installments_payments_agg=data_installments_payments.groupby(by='SK_ID_CURR').mean()\ndata_POS_CASH_balance_agg=data_POS_CASH_balance.groupby(by='SK_ID_CURR').mean()","a8825187":"data_raw = data_raw.join(data_bureau_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2')                \ndata_raw = data_raw.join(data_credit_card_balance_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \ndata_raw = data_raw.join(data_previous_application_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \ndata_raw = data_raw.join(data_installments_payments_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n","34b6e65d":"# keep sk_id_curr column and delete it from dataset\nsk_id_curr = data_raw['SK_ID_CURR']\ndata_raw.drop('SK_ID_CURR',axis=1,inplace=True)","2054dc36":"#Probability of scoring 0 is at least 91.51% \n#This means that our model must, at least, improve this percentage, otherwise \n#we don`t need a model and the bank will just assume that almost 9 of 100 will default.\n100*(1 - data_raw[data_raw['TARGET']==1].TARGET.count()\/data_raw.TARGET.count())","c905d7c8":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = data_raw.select_dtypes(include = [\"object\"]).columns\nnumerical_features = data_raw.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ndata_raw_num = data_raw[numerical_features]\ndata_raw_cat = data_raw[categorical_features]","3d6d40b6":"data_raw.replace({'NAME_CONTRACT_TYPE': {'Cash loans': 1, 'Revolving loans': 2}},  inplace = True)\ndata_raw.replace({'CODE_GENDER': {'M': 1, 'F': 2, 'XNA': 0}},  inplace = True)\ndata_raw.replace({'FLAG_OWN_CAR': {'Y': 1, 'N': 0}},  inplace = True)\ndata_raw.replace({'FLAG_OWN_REALTY': {'Y': 1, 'N': 0}},  inplace = True)\ndata_raw.replace({'NAME_INCOME_TYPE': {'Working': 0, 'State servant': 1, 'Commercial associate': 2,\n                                           'Pensioner': 3, 'Unemployed': 4, 'Student': 5, 'Businessman': 6}},  inplace = True)\ndata_raw.replace({'NAME_EDUCATION_TYPE': {'Secondary \/ secondary special': 0, 'Higher education': 1, 'Incomplete higher': 2,\n                                           'Lower secondary': 3, 'Academic degree': 4}},  inplace = True)\ndata_raw.replace({'NAME_FAMILY_STATUS': {'Single \/ not married': 0, 'Married': 1, 'Widow': 2,\n                                           'Civil marriage': 3, 'Separated': 4}},  inplace = True)\ndata_raw.replace({'NAME_HOUSING_TYPE': {'House \/ apartment': 0, 'Rented apartment': 1, 'Municipal apartment': 2,\n                                           'With parents': 3, 'Office apartment': 4, 'Co-op apartment': 5}},  inplace = True)\n","52f71ddc":"# convert rest of categorical variable into dummy\ndata_raw = pd.get_dummies(data_raw)","4d52ed0f":"from sklearn.preprocessing import MinMaxScaler, Imputer\nimputer = Imputer(strategy = 'median') # Median imputation of missing values\nscaler = MinMaxScaler(feature_range = (0, 1)) # Scale each feature to 0-1","92c4c019":"for column in data_raw_num.columns:\n    data_raw[[column]] = imputer.fit_transform(data_raw[[column]])\n    data_raw[[column]] = scaler.fit_transform(data_raw[[column]])","a053ca40":"# Find correlations with the target and sort\n# correlations = data_raw.corr()['TARGET'].sort_values()\n# print('Most Positive Correlations: \\n', correlations.tail(6))\n# print('\\nMost Negative Correlations: \\n', correlations.head(5))","8c4f4254":"train=data_raw","28771e29":"# Partition the dataset in train + validation sets\nX = train.iloc[:,1:]\ny = train.iloc[:,0]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nprint(\"X_train : \" + str(X_train.shape))\nprint(\"X_test : \" + str(X_test.shape))\nprint(\"Y_train : \" + str(Y_train.shape))\nprint(\"Y_test : \" + str(Y_test.shape))","8f22d97b":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nY_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(Y_test, Y_pred_proba)\nprint('Train\/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(Y_test, Y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(Y_test, Y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","6e8c2200":"#Light GBM \n\nLGB_clf = LGBMClassifier(n_estimators=100, \n                         boosting_type='gbdt', \n                         objective='binary', \n                         metric='binary_logloss')\nLGB_clf.fit(X_train, Y_train)\nY_pred_proba = LGB_clf.predict_proba(X_test)[:, 1]\n\nroc_auc_score(Y_test, Y_pred_proba)","8087a586":"## Group data ","3ab53c3b":"## Read all input data","59225ace":"## Filling missing values of features","2c2efd67":"## Join data","79a9ebbf":"## Finding most correlated features","2be4704a":"## Import libraries","4a00c296":"## Checking Logistic Regression","87ffe714":"## Differentiate numerical features from categorical features","3ee96da8":"## Calculate de probability of default "}}