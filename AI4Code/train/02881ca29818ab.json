{"cell_type":{"d9ba051e":"code","33c88a70":"code","679038a3":"code","a1a38606":"code","5eab13d7":"code","cb84e9eb":"code","81f2c019":"code","7c401cf4":"code","a2a7d902":"code","73deb339":"code","96d065fd":"code","ae99ca34":"code","5606d928":"code","ba34590e":"code","0d8c4cfa":"code","80acfa4b":"code","3a797b0b":"code","087b5885":"markdown","e90be210":"markdown","4834ded7":"markdown","0d133650":"markdown","dac47205":"markdown","06ddce44":"markdown","064b165d":"markdown","c342e332":"markdown","9433ca28":"markdown","d64114bd":"markdown","51c7bfc8":"markdown","2875ebdf":"markdown","f9dd32a8":"markdown","fd6618a0":"markdown","6015af01":"markdown","6fbe5c26":"markdown","88f5edf2":"markdown","28f062bf":"markdown","f2ecc9dc":"markdown","6c94220f":"markdown"},"source":{"d9ba051e":"import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nimport matplotlib as mpl\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.offline import iplot\nimport plotly.express as px\nimport json\nfrom matplotlib.colors import LinearSegmentedColormap\n\n\npio.renderers.default = 'kaggle'\nplt.style.use('ggplot')\nsns.set()\n\n#ignore all warnings\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nnepal_districts = json.load(open('..\/input\/tempdata\/nepal-districts.geojson', 'r'))\n\ndmapping = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/ward_vdcmun_district_name_mapping.csv')\n\ndistricts = pd.DataFrame(columns=['district_id', 'district_name', 'damage_level'])\ntemp =np.append(dmapping.district_name.unique(),['Kavre'])\n\nfor i in nepal_districts['features']:\n    districts = districts.append({'district_id': i['id'], 'district_name': i['properties']['DISTRICT'],\n                                  'damage_status': 'Damaged' if i['properties']['DISTRICT'].capitalize() in temp else 'Undamaged'},\n                                 ignore_index=True)\n\n\n#draw color on Nepali map\nfig = px.choropleth(\n    districts,\n    locations=\"district_name\",\n    geojson=nepal_districts,\n    color=\"damage_status\",\n    hover_name=\"district_name\",\n    color_discrete_sequence=['#588bed', \"#f24444\"],\n    featureidkey='properties.DISTRICT')\n\n#draw name\nfor i in [\"black\", \"#FFFFFF\"]:\n    fig.add_trace(\n        go.Scattergeo(text=['Nepal'], lat=[28.3947], lon=[84.1240] if i == 'black' else [84.1240 - 0.035], textfont={\n            \"color\": [i for j in range(2)],\n            'size': [22 for j in range(2)]\n        }, mode='text', textposition='bottom left'))\n\nfig.update_geos(fitbounds=\"locations\", visible=False, landcolor='#9c9c9c')\nfig.update_layout(showlegend=False)\niplot(fig)","33c88a70":"indvdemo = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_individual_demographics.csv')\nhdemo = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_household_demographics.csv')\nhresources = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_household_resources.csv')\nhimpact = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_household_earthquake_impact.csv')\nbstructure = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_building_structure.csv')\ndmapping = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/ward_vdcmun_district_name_mapping.csv')\nmapping = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/mapping.csv')\nbassessment = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_building_damage_assessment.csv')\nbuse = pd.read_csv('..\/input\/earthquake-magnitude-damage-and-impact\/csv_building_ownership_and_use.csv')","679038a3":"import matplotlib.patches as mpatches\n#draw two subplots and draw two  double vertical histograms on their axes\nplt.figure(figsize=(14, 11))\nmales = indvdemo.query('gender_individual == \"Male\" ')\nfemales = indvdemo.query('gender_individual == \"Female\" ')\nplt.ylim([0, 100])\ngrid = plt.GridSpec(1, 2, wspace=0.04)\nmaleaxis = plt.subplot(grid[0, 0])\nfemaleaxis = plt.subplot(grid[0, 1])\nmaleaxis.invert_xaxis()\nmaleaxis.yaxis.set_major_locator(plt.MaxNLocator(20))\nmaleaxis.hist(males['age_individual'], bins=[i for i in range(0, 100, 5)], ec='black', histtype='bar',\n       orientation='horizontal', color='#26abff', label='Male')\nmaleaxis.hist(hdemo[hdemo['gender_household_head'] == 'Male']['age_household_head'], bins=[i for i in range(0, 100, 5)],\n       ec='black', histtype='bar', color='#b9d4e4', orientation='horizontal', label='Male Household Heads')\nmaleaxis.margins(y=0)\nfemaleaxis.hist(females['age_individual'], bins=[i for i in range(0, 100, 5)], ec='black', histtype='bar',\n       orientation='horizontal', color='pink', label='Female')\nfemaleaxis.hist(hdemo[hdemo['gender_household_head'] == 'Female']['age_household_head'], bins=[i for i in range(0, 100, 5)],\n       ec='black', histtype='bar', color='#ebcacd', orientation='horizontal', label='Female Household Heads')\nfemaleaxis.margins(y=0)\nfemaleaxis.yaxis.set_major_locator(plt.MaxNLocator(20))\nfemaleaxis.yaxis.set_label_position(\"right\")\nfemaleaxis.yaxis.tick_right()\nmaleaxis.set_xlabel('Males')\nfemaleaxis.set_xlabel('Females')\nmaleaxis.set_ylabel('Age')\nfemaleaxis.set_ylabel('Age')\nfemaleaxis.tick_params(length=0)\nmaleaxis.tick_params(length=0)\n\nax = maleaxis.inset_axes([0.088, 0.844, 0.185, 0.185])\n\nmtof = list(indvdemo.gender_individual.value_counts())\nax.set_xlabel('Overall Ratio:\\n{:.2f} Males : 1 Female'.format(mtof[0] \/ mtof[1]))\nax.set_ylim([1500000, 2000000])\nbarlist = ax.pie(indvdemo.gender_individual.value_counts(), colors=['#26abff', 'pink'], startangle=90, shadow=True,\n                 wedgeprops={\"edgecolor\": \"grey\"})\nfmhh = mpatches.Patch(ec='black', color='#ebcacd', label='Female Household Heads')\nmhh = mpatches.Patch(ec='black', color='#b9d4e4', label='Male Household Heads')\nfemale = mpatches.Patch(ec='black', color='pink', label='Females')\nmale = mpatches.Patch(ec='black', color='#26abff', label='Males')\n\nfemaleaxis.legend(handles=[fmhh, mhh, female, male]);","a1a38606":"from ipywidgets import interact\n\ncolors = [LinearSegmentedColormap.from_list('', ['#e9fce9',\n                                                 '#32c285'])(i) for i in np.linspace(0, 1, 7)]\nclasses = dict(zip(hdemo.income_level_household.dropna().unique(), colors))\n\npcts = hdemo.income_level_household.value_counts(normalize=True)\n\n#draw horizontal frequency histogram and  a pie chart with a legend next to it\ndef show_household_distribution(Income):\n    plt.figure(figsize=(14, 8))\n    ax = plt.gca()\n    ax.set_xlim([0.45, 25])\n    ax.margins(x=0.02)\n    ax2 = ax.inset_axes([0.515, 0.390, 0.600, 0.600])\n    zeroes = np.zeros(5)\n    counter = 0\n    finalcounter = 0\n    for incomelevel, color in classes.items():\n        if incomelevel != Income:\n            ax.hist(hdemo.query(f'income_level_household == \"{incomelevel}\"').size_household, bins=np.arange(50)-0.5, ec='black',\n                    alpha=0.09, color=color, density=True)\n            counter += 1\n        else:\n            finalcounter = counter\n    zeroes[finalcounter] = 0.1\n    ax.hist(hdemo.query(f'income_level_household == \"{Income}\"').size_household, bins=np.arange(50)-0.5, ec='black', alpha=1,\n            density=True, color=classes[Income])\n    ax2.pie(hdemo.income_level_household.value_counts(),\n            colors=colors,\n            explode=zeroes,\n            shadow=True,\n            pctdistance=1.2,\n            wedgeprops={\n                \"edgecolor\": \"k\"\n                , 'width': 0.55\n            })\n    ax2.legend(labels=hdemo.income_level_household.dropna().unique(),\n               bbox_to_anchor=(0.755, 0.265), loc=\"center\"\n               , bbox_transform=plt.gcf().transFigure)\n    ax.set_xticks(np.arange(1, 26, 1))\n    ax.set_title(\"Household Size Distribution Among Different Levels of Income\", fontsize=24)\n    ax2.set_xlabel('Exactly {0:.2f}% of the households\\nhave an income of {1} '.\n                   format(pcts[Income] * 100, Income),\n                   fontsize=16)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xlabel(\"Household Size\")\n    ax2.text(0, -0.025, '{:.0f}%'.format(pcts[Income] * 100), horizontalalignment='center',\n             verticalalignment='center', fontsize=35, color='black',\n             # path_effects=[pe.withStroke(linewidth=3, foreground=classes[classtohightlight])]\n             )\n\n\n# interact(show_household_distribution, Income=hdemo.income_level_household.dropna().unique());\n#uncomment the line above and comment the line below for the interactive part of this plot\nshow_household_distribution(hdemo.income_level_household.dropna().unique()[4]);","5eab13d7":"#group educational levels to major levels of education\nedu = hdemo.education_level_household_head.value_counts()\nfor i in edu.items():\n    if i[0] in ['Illiterate', 'Non-formal education', 'Nursery\/K.G.\/Kindergarten', 'Other']:\n        edu.update(pd.Series({i[0]: 'No Formal School Experience'}))\n    elif i[0] in ['Class {0}'.format(i) for i in range(1, 5)]:\n        edu.update(pd.Series({i[0]: 'Below Primary School'}))\n    elif i[0] in ['Class {0}'.format(i) for i in range(5, 9)]:\n        edu.update(pd.Series({i[0]: 'Primary School'}))\n    elif i[0] in ['Class {0}'.format(i) for i in range(9, 11)]:\n        edu.update(pd.Series({i[0]: 'Upper Primary School'}))\n    elif i[0] == 'SLC or equivalent':\n        edu.update(pd.Series({i[0]: 'Secondary School'}))\n    elif i[0] == 'Intermediate or equivalent':\n        edu.update(pd.Series({i[0]: 'Higher Secondary'}))\n    elif i[0] in ['Bachelors or equivalent', 'Masters or equivalent', 'Ph.D. or equivalent']:\n        edu.update(pd.Series({i[0]: 'Bachelor\\'s or Higher'}))\n\n#make a new column and map it to the modified series above\nhdemo['grouped_education_level_household_head'] = hdemo['education_level_household_head'].map(edu)\n\ncounted_data = hdemo.groupby(['grouped_education_level_household_head', 'income_level_household']).agg(\n    'count').household_id.unstack()\n\nstacked_data = counted_data.T.apply(lambda x: x * 100 \/ sum(x), axis=1)\n#turn into a percentage and plot as aa stacked bar chart\nstacked_data[['No Formal School Experience',\n              'Below Primary School', 'Primary School',\n              'Upper Primary School', 'Secondary School',\n              'Higher Secondary', 'Bachelor\\'s or Higher']].plot(\n    cmap=LinearSegmentedColormap.from_list('', ['#71c7c1', '#180254']), kind=\"bar\", stacked=True, figsize=(17, 8));\nhandles, labels = plt.gca().get_legend_handles_labels()\norder = [i for i in range(6, -1, -1)]\nplt.legend([handles[idx] for idx in order], [labels[idx] for idx in order], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.title(\"Education of Household Head to Household Income Breakdown\", fontsize=18)\nplt.xlabel(\"Level of Income\")\nplt.ylabel(\"Presence of Each Education Level(%)\");\nplt.xticks(rotation=0);","cb84e9eb":"# aggregate all people into decades born in, divide each each groups illiterate population by the total and find ratio\nindvdemo['decade_born_in'] = (2016 - indvdemo['age_individual']) \/\/ 10 * 10\nmale = indvdemo.query('gender_individual == \"Male\" & age_individual>15')\nfemale = indvdemo.query('gender_individual == \"Female\"& age_individual>15')\n\niltm = male[['decade_born_in', 'education_level_individual']].query('education_level_individual==\"Illiterate\"').groupby(\n    'decade_born_in').agg('count').unstack()\niltm.name = 'illiterate'\n\niltfm = female[['decade_born_in', 'education_level_individual']].query(\n    'education_level_individual==\"Illiterate\"').groupby('decade_born_in').agg('count').unstack()\niltfm.name = 'illiterate'\n\nltm = male[['decade_born_in', 'education_level_individual']].groupby('decade_born_in').agg('count').unstack()\nltm.name = 'literate'\n\nltfm = female[['decade_born_in', 'education_level_individual']].groupby('decade_born_in').agg('count').unstack()\nltfm.name = 'literate'\n\nmaleliteracy = pd.concat([iltm, ltm], axis=1)\nfemaleliteracy = pd.concat([iltfm, ltfm], axis=1)\n\nfor i in maleliteracy, femaleliteracy:\n    i['literacy%'] = i['literate'] \/ (i['literate'] + i['illiterate']) * 100\n\nplt.figure(figsize=(13, 11))\nax = plt.gca()\nax.set_xlim([0, 100])\nax.set_ylim([0, 100])\nax.set_ylabel('Female Literacy Ratio (%)')\nax.set_xlabel('Male Literacy Ratio (%)')\nax.plot(np.linspace(0, 100), np.linspace(0, 100), color='black', linestyle='--', alpha=0.4)\nax.scatter(maleliteracy['literacy%'], femaleliteracy['literacy%'], c=[i for i in range(1890, 2010, 10)], cmap='cividis',\n           s=80, alpha=0.8)\nax = plt.gca()  #get the current axes\na = 0\n# for PCM in ax.get_children():\n#     if isinstance(PCM, mpl.cm.ScalarMappable):\n#         a= PCM\n#         break\ncmap = plt.get_cmap('cividis',12)\nnorm = mpl.colors.Normalize(vmin=1890,vmax=2010)\n\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\nc = range(1890, 2010, 10)\ncbar = plt.colorbar(sm,  \n             boundaries= c )\n\ntick_locs = np.linspace(c[0], c[-1], len(c)) + 5\ncbar.set_ticks(tick_locs)\ncbar.set_ticklabels(c)\nax.set_title(\"Comparing Adult Male and Female Literacy Throughout Generations\", fontsize=16);","81f2c019":"%%capture\n!pip install pywaffle;","7c401cf4":"from pywaffle import Waffle\n#make a waffle plot about disabled part of the Nepalese population\ndis = np.round(indvdemo.query('disability_individual !=\"No\"')\n               .disability_individual.value_counts(normalize=True) * 100).reindex(\n    ['Yes\/No card', 'Yes\/White card', 'Yes\/Yellow card', 'Yes\/Blue card', 'Yes\/Red card']\n)\ndiscolors = ['#8B837E','#fefef0' ,  '#dae02b', '#2830b8','#d42020']\nfig = plt.figure(\n    FigureClass=Waffle,\n    rows=5,\n    values=dis.to_dict(),\n    title={'label': 'ID Card Distribution Among Disabled People of the 11 Districts', 'loc': 'left', 'fontsize': 28}\n    , colors=discolors,\n    # legend={\n    #     'loc': 'upper left',\n    #     'bbox_to_anchor': (1, 0.75)},\n    # legend={'loc': 'lower left', 'bbox_to_anchor': (0, -0.4), 'ncol': len(dis), 'framealpha': 0},\n    #     icon_legend=True,\n    icons='address-card',\n    figsize=(12, 11),\n    font_size=31,\n)\nplt.gca().get_legend().remove()\nfig.set_facecolor('#ebebeb')\n\n","a2a7d902":"district_data = pd.read_csv('..\/input\/tempdata\/district_latlong.csv')\ndistrict_data['CAPS'] = district_data.district_name.str.upper()\ndistrict_data['CAPS'] = district_data['CAPS'].replace(\"KAVREPALANCHOK\", \"KAVRE\")\ndef show_on_nepal(figtodraw, colorscale, title):\n    base = px.choropleth(\n        districts,\n        locations=\"district_id\",\n        geojson=nepal_districts,\n        title=title,\n        color_discrete_sequence=['#848484'])\n\n    for i in [\"black\", \"#f5f5f5\"]:\n        base.add_trace(go.Scattergeo(lat=district_data.lat\n                                     , textfont={\n                \"color\": [i for j in range(district_data.district_name.size)],\n                'size': [10.5 for j in range(district_data.district_name.size)]},\n                                     lon=district_data.lon if i == 'black' else district_data.lon - 0.0055,\n                                     text=district_data.district_name\n                                     , mode='text'))\n    base.update_layout(showlegend=False)\n    base.add_trace(figtodraw.data[0])\n    base.update_coloraxes(showscale=True, colorscale=colorscale)\n    base.update_geos(fitbounds=\"geojson\", visible=False)\n    # base.update_geos(center_lat=27.9512)\n    # base.update_geos(center_lon=85.6846)\n\n    iplot(base, filename='jupyter-parametric_plot')\npopulations = indvdemo.groupby('district_id')[['individual_id']].agg('count').reset_index()\npopulations = populations.merge(dmapping.drop_duplicates('district_name')[['district_name', 'district_id']],\n                                on='district_id')\ndistrict_data = district_data.merge(populations[['district_name', 'individual_id']], on='district_name', how='inner')\ndistrict_data.rename(columns={'individual_id': 'Resident Count'},\n                     inplace=True)\nshow_on_nepal(px.choropleth(\n    district_data,\n    locations=\"CAPS\",\n    geojson=nepal_districts,\n    color=\"Resident Count\",\n    featureidkey='properties.DISTRICT',\n), 'dense', 'Population Density in the Affected Districts')\n","73deb339":"\nmapped = bstructure[['district_id', 'damage_grade']].merge(dmapping.drop_duplicates('district_id'), on='district_id',\n                                                           how='inner')\nmapped['damage_level'] = mapped.damage_grade.map(\n    {'Grade 1': 0, 'Grade 2': 0, 'Grade 3': 1, 'Grade 4': 1, 'Grade 5': 1, 'nan': 0})\nmapped['Raw_Damage_Level'] = mapped.damage_grade.map(\n    {'Grade 1': 1, 'Grade 2': 2, 'Grade 3': 3, 'Grade 4': 4, 'Grade 5': 5, 'nan': 0})\ndistrict_data = district_data.merge(\n    mapped.query('damage_level>0').groupby('district_name').agg('count').reset_index()[['district_name', 'vdcmun_id']],\n    on='district_name', how='inner')\ndistrict_data = district_data.merge(\n    mapped.query('Raw_Damage_Level>0').groupby('district_name').agg('sum').reset_index()[['district_name', 'Raw_Damage_Level']],\n    on='district_name', how='inner')\ndistrict_data = district_data.merge(\n    mapped.groupby('district_name').agg('count').reset_index()[['district_name', 'ward_id']], on='district_name',\n    how='inner')\ndistrict_data.rename(columns={'vdcmun_id': 'Damaged Building Count',\n                              'ward_id': 'Total Building Count'},\n                     inplace=True)\ndistrict_data['Percentage(%)'] = district_data['Damaged Building Count'] \/ district_data['Total Building Count']\n\ndef raw_or_normalized(Option):\n    show_on_nepal(px.choropleth(\n    district_data,\n    locations=\"CAPS\",\n    geojson=nepal_districts,\n    color=Option,\n    featureidkey='properties.DISTRICT',\n    ), 'sunsetdark','Percentage of Damaged Buildings per District' if Option=='Percentage(%)' else 'Raw Damage Score per District')\n\n#interact(raw_or_normalized, Option=['Percentage(%)','Raw_Damage_Level']);\n#uncomment the line above and comment the line below for the interactive part of this plot\nraw_or_normalized('Percentage(%)')","96d065fd":"from sklearn.model_selection import train_test_split,learning_curve,StratifiedKFold, KFold,GridSearchCV,cross_validate,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import recall_score,precision_score,f1_score,accuracy_score,confusion_matrix,make_scorer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom numpy import mean,std\n\ncv = StratifiedKFold(n_splits=4, random_state=1, shuffle=False)","ae99ca34":"#drop all id \/ irrelevant columns to our classification to get all building structure & building use attributes known prior to any disaster\nbstructure['damaged_status'] = bstructure.damage_grade.map({'Grade 1': 0,'Grade 2':0,'Grade 3':1, 'Grade 4':1, 'Grade 5': 1,'nan': 0})\ndata= bstructure.drop(['district_id','vdcmun_id','ward_id','count_floors_post_eq','height_ft_post_eq',\n                    'condition_post_eq','damage_grade','technical_solution_proposed'],axis =1)\ndata = data.merge(buse.drop(['district_id','vdcmun_id','ward_id'],axis = 1),how='inner',on='building_id')\ndata = data.drop(['building_id'],axis =1 )\ndata= data.dropna()\n\n\n\n\nimport plotly.express as px\nstats = data.damaged_status.value_counts()\nfig = px.pie( values=stats.values, names=['Damaged','Undamaged'], title='Building Status Breakdown', labels={'names':'Status','values':'Data Points'})\nfig.update_traces(textposition='inside', textinfo='percent+label',marker=dict(colors=['#d62728','nitrogen'], line=dict(color='#000000', width=1)),pull=[0.1,0])\n\nfig.show()","5606d928":"#One hot encoding all categorical variables to vectorize them to columns of 0s and 1s, then merging them with other numerical data to the 'all variable'\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\ndata.reset_index(drop=True, inplace=True)\ncategorical =  data.select_dtypes(include=['object'])\nnumerical = data.select_dtypes(exclude=['object'])   # Assume for simplicity all features are categorical.\nencoder.fit(categorical);\ntemp =   encoder.transform(categorical)\ncatvars = pd.DataFrame(data = temp,columns = encoder.get_feature_names(categorical.columns))\nall = pd.concat([numerical,catvars],axis = 1)\nprint(f'The DataFrame now consists of {all.shape[0]} rows\/buildings and {all.shape[1]} columns\/features!')","ba34590e":"classifiers = {'Decision Tree Classifier':DecisionTreeClassifier(),\n               'Random Forests Classifier':RandomForestClassifier(),\n               'Adaboost Classifier':AdaBoostClassifier(),\n               'Gradient Boosting Classifier':GradientBoostingClassifier()}\nfig,axlist = plt.subplots(1,4,figsize=(30,5))\nplt.subplots_adjust(hspace=0.45)\nx= all.drop('damaged_status',axis=1)\ny= all['damaged_status']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)\naxcounter = 0\nfor name, model in classifiers.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    f1 = f1_score(y_test, y_pred, average=None)\n    recall_scores =recall_score(y_test, y_pred, average=None)\n    prec_scores =precision_score(y_test, y_pred, average=None)\n    current_ax = axlist[axcounter]\n    sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='g', ax=current_ax,cmap='Blues');\n    current_ax.set_xlabel(f'Predicted labels\\n\\nScores (Undamaged | Damaged)\\n F1-score: {f1[0]*100:.2f}% | {f1[1]*100:.2f}%\\nRecall : {recall_scores[0]*100:.2f}% | {recall_scores[1]*100:.2f}%\\nPrecision: {prec_scores[0]*100:.2f}% | {prec_scores[1]*100:.2f}%');\n    current_ax.set_ylabel('True labels'); \n    current_ax.set_title(name); \n    current_ax.xaxis.set_ticklabels(['Undamaged', 'Damaged']); \n    current_ax.yaxis.set_ticklabels(['Undamaged', 'Damaged']);\n    axcounter+=1\nfig.suptitle('Confusion Matrix for Each Classifier',y=1.02,fontsize = 26);","0d8c4cfa":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import precision_score\n\ndef display(name,results):\n    print(f'Best parameters for {name} are: {results.best_params_}')\nallbad = all.query('damaged_status==1')\nallgood = all.query('damaged_status==0')\n\nsmallsample = pd.concat([allgood.sample(600),allbad.sample(2400)])\nx= smallsample.drop('damaged_status',axis=1)\ny= smallsample['damaged_status']\n\n\nclassifiers = [['Gradient Boosting Classifier',\n                GradientBoostingClassifier(), {\"n_estimators\":[25,50,250,500],\"max_depth\":[1,3,5,7],\"learning_rate\":[0.01,0.1,1,10]},\n               None],\n               ['Random Forests Classifier',\n                RandomForestClassifier(), {'n_estimators': [100,300, 500], 'max_features': ['auto', 'sqrt', 'log2'],'max_depth' : [4,5,6,7,9, None], 'criterion' :['gini', 'entropy']},\n                None],\n               ['Adaboost Classifier',\n                AdaBoostClassifier(), {\"n_estimators\":[50,250,500],\"learning_rate\":[0.01,0.1,1,10]}\n                ,None],\n              ]\n               \nfor i in range(len(classifiers)):\n    name,model,params,best_params = classifiers[i]\n    cv = RandomizedSearchCV(model,params,cv=4,n_jobs=-1,scoring = 'f1_micro')\n    cv.fit(x,y)\n    classifiers[i][3] = cv.best_params_\n    display(name,cv)\ncv = StratifiedKFold(n_splits=4, random_state=1, shuffle=False)\nfig,axlist = plt.subplots(1,3,figsize=(25,5))\nplt.subplots_adjust(hspace=0.45)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)\naxcounter = 0\nfor name, model,params,_ in classifiers:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    f1 = f1_score(y_test, y_pred, average=None)\n    recall_scores =recall_score(y_test, y_pred, average=None)\n    prec_scores =precision_score(y_test, y_pred, average=None)\n    current_ax = axlist[axcounter]\n    sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='g', ax=current_ax,cmap='Blues');\n    current_ax.set_xlabel(f'Predicted labels\\n\\nScores (Undamaged | Damaged)\\n F1-score: {f1[0]*100:.2f}% | {f1[1]*100:.2f}%\\nRecall : {recall_scores[0]*100:.2f}% | {recall_scores[1]*100:.2f}%\\nPrecision: {prec_scores[0]*100:.2f}% | {prec_scores[1]*100:.2f}%');\n    current_ax.set_ylabel('True labels'); \n    current_ax.set_title(name); \n    current_ax.xaxis.set_ticklabels(['Undamaged', 'Damaged']); \n    current_ax.yaxis.set_ticklabels(['Undamaged', 'Damaged']);\n    axcounter+=1\nfig.suptitle('Confusion Matrix for Each Classifier',y=1.02,fontsize = 26);","80acfa4b":"fig, ax = plt.subplots(1, 1, figsize=(16, 6))\nallbad = all.query('damaged_status==1')\nallgood = all.query('damaged_status==0')\n\nsmallsample = pd.concat([allgood.sample(10000),allbad.sample(10000)])\nx= smallsample.drop('damaged_status',axis=1)\ny= smallsample['damaged_status']\nflag = 0\nfor name, model,params,best_params in classifiers:\n    N, train_lc, val_lc = learning_curve(model,\n                                         x, y, cv=StratifiedKFold(n_splits=4, random_state=1, shuffle=False),\n                                         train_sizes=np.linspace(0.5, 1, 24),n_jobs=-1)\n    ax.plot(N, np.mean(train_lc, 1), color='blue', label='Training Scores' if flag == 0 else None)\n    ax.plot(N, np.mean(val_lc, 1), color='red', label='Validation Scores' if flag == 0 else None)\n    flag+=1\n\n\nax.hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n                 color='gray', linestyle='dashed')\n\nax.set_ylim(0, 1)\nax.set_xlim(N[0], N[-1])\nax.set_ylim(0, 1.01)\n\nax.set_xlabel('Training Data Size')\nax.set_ylabel('Model Score')\nax.set_title('Model Performance by Training Dataset Size',size = 18)\n# ax.set_title('degree = {0}'.format(degree), size=19)\nax.legend(loc='best');","3a797b0b":"smallsample = pd.concat([allgood.sample(7500),allbad.sample(7500)])\nx= smallsample.drop('damaged_status',axis=1)\ny= smallsample['damaged_status']\n\n\nfor i in range(len(classifiers)):\n    name,model,params,best_params = classifiers[i]\n    cv = GridSearchCV(model,params,cv=StratifiedKFold(n_splits=4, random_state=1, shuffle=False),n_jobs=-1,scoring = 'accuracy')\n    cv.fit(x,y)\n    classifiers[i][3] = cv.best_params_\n    display(name,cv)\nfrom sklearn.metrics import accuracy_score\n\nallgood = all.query('damaged_status == 0')\nallbad = all.query('damaged_status ==1')\nsampledamaged = allbad.sample(len(allgood))\nall = pd.concat([allgood,sampledamaged] )\nall = all.sample(frac=1)\nx= all.drop('damaged_status',axis=1)\ny= all['damaged_status']\n\n#Not very necessary here\ncv = StratifiedKFold(n_splits=4, random_state=1, shuffle=False)\n\nfig,axlist = plt.subplots(1,3,figsize=(25,5))\nplt.subplots_adjust(hspace=0.45)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)\ncounter = 0\nfor name, model,params,best_params in classifiers:\n    model = model.__class__\n    model = model (**best_params) \n    model.fit(X_train, y_train)\n    classifiers[counter][1]= model\n    y_pred = model.predict(X_test)\n    scoring = {\n       'recall0': make_scorer(recall_score, average = None, labels = [0]), \n       'recall1': make_scorer(recall_score, average = None, labels = [1]),\n       'precision0': make_scorer(precision_score, average = None, labels = [0]),\n       'precision1': make_scorer(precision_score, average = None, labels = [1]),\n       'f10': make_scorer(f1_score, average = None, labels = [0]),\n       'f11': make_scorer(f1_score, average = None, labels = [1])}\n    scores = cross_validate(model,x,y, scoring = scoring, cv = cv, return_train_score = False,n_jobs=-1)\n    current_ax = axlist[counter]\n    sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='g', ax=current_ax,cmap='Blues');\n    current_ax.set_xlabel(f'Predicted labels\\n\\nAvg. Cross Validated Scores (Undamaged | Damaged)\\n F1-score: {mean(scores[\"test_f10\"])*100:.2f}% | {mean(scores[\"test_f11\"])*100:.2f}%\\nRecall : {mean(scores[\"test_recall0\"])*100:.2f}% | {mean(scores[\"test_recall1\"])*100:.2f}%\\nPrecision: {mean(scores[\"test_precision0\"])*100:.2f}% | {mean(scores[\"test_precision1\"])*100:.2f}%');\n    current_ax.set_ylabel('True labels'); \n    current_ax.set_title(name); \n    current_ax.xaxis.set_ticklabels(['Undamaged', 'Damaged']); \n    current_ax.yaxis.set_ticklabels(['Undamaged', 'Damaged']);\n    counter+=1\nfig.suptitle('Confusion Matrix for Each Classifier',y=1.02,fontsize = 26);","087b5885":"Lastly, the final aspect we'll look at in the population is disability.\n\nThe disabled people of Nepal are entitled to a disability card issued by the Social Development Department of Kathmandu (Nepal's capital) Medical College.\n This card entitles them to social benefits and easier accessibility to services,\n facilities, etc. provided by the government.\n\nThe level of social benefits corresponds with how severe the disability is,\nwhich led to the cards distributed to be color coded based on the severity.\n\nCiting The Himalayan Times:\n\n> <i>\"The government will distribute four different categories of identity cards to the disabled persons. A person with complete disability (\u2018A\u2019 category), severe disability (\u2018\u2019B\u2019\u2019 category), moderate disability (\u2018C\u2019 category) and general disability (\u2018D\u2019 category) will obtain red, blue, yellow and white cards respectively.\"\n> <\/i>\n\n\n","e90be210":"As shown by the pie chart on the top right, 55% of the household's have an income of Rs. 10 thousand or possibly less.\nThis leaves an average of about 4-6 people, as suggested by the frequency histogram living on ($83.34 USD) per month assuming\n it's only source of income. In extreme cases, the number of people living in households with the lowest classifcation of income can reach up to 18 residents.\n\nAs you interact with the figure above, you will find that household size distribution in higher income households shifts more towards the right,\nmeaning more people reside in the households with larger incomes on average.You should also notice that these higher income households make up less and less of the population the more they earn a month.\n\n\nWith the highest level of income making up only about 1% of the population, what does it take to earn this much for the household heads in our dataset at least in terms of education?","4834ded7":"Moving on, we'll take a look at geographical data about the population. As well as the earthquakes impact on the districts and how much damage it dealt.\n\nThe 11 districts house over 3.6 million people in total, let's see how they're distributed among all of them","0d133650":"As shown by the heatmaps above, the decision tree model has by far the worst performance. Which is expected, as other models implement the same idea of a decision tree but on s\u0336t\u0336e\u0336r\u0336o\u0336i\u0336d\u0336s\u0336  boosters or a multitude of trees.\n\nThe better models however are not much better.You'll notice the F1, recall and precision scores shown at the bottom of each heatmap are always higher for the damaged class which is the majority. Since F1 is calculated using both precision and recall which are both heavily skewed to the data imbalance, the scores can be very misleading.\n\nTo put it in simpler terms, since the number of buildings that were damaged **and** were identified by the model as damaged give the model inflated scores. This is emphasized by the much lower scores for the undamaged predictions (true negatives) that borders on 50% recall\/precision and the heatmap that shares almost the same color at the top.\n\n\nAlso we should note that:\n\n1. Damaged building recall is  supposed to be the most important metric in this use case of a model, as recall represents the number of buildings that were actually damaged, but predicted to be undamaged. The higher the recall is, the better the model is, but not always!\n\n2. While recall is indeed important, it's not the only metric as we can get a perfect recall by labelling all buildings as damaged. A feasible model should also have a fairly low amount of precision, which this one does not at all. All default hyperparameter models trained with this unbalanced dataset leave the prediction of undamaged buildings almost up to chance.\n\nThe deceivingly high F1-score is only inflated by the very high recall and precision due to the large number of possible positives(damaged buildings) to identify. This is unwanted. What we want is to maximize the recall & percision with the least amount of false negatives possible.\n\nNow with our model selection narrowed down a small bit. Next, we will take a small subsample of the data and do a cross-validated search on it for the best hyperparameters of the last 3 models, hoping for a better performance.","dac47205":"The population pyramid of our dataset closely matches a <b>late-expanding population\npyramid<\/b> meaning it was  at one point rapidly expanding, but started contracting more in\nthe newer generations. This could be due to financial, cultural changes, political instability\nleading to lower birth rates, higher child mortality rates or a variety of other reasons.\n\nWith the total population's male to female ratio being almost equal,\nthere isn't much difference between the number of females to males throughout the age groups.\n Only in the much older ages (75+) we\ncan notice that the female's outnumber the males consistently, only proving the saying that women live longer than men!\n\nThe above graph also shows the distribution of the household heads in the population,\nthe main source of income for each household. With male household heads being visibly almost double to triple the number\nof female household heads in their respective age groups.\n\nEvery individual in the data belongs to a household lead by one of the mentioned household heads,\n but how many people occupy a household on average? And how much monthly income are they expected to live on?\n\nIs there a relationship between the two?\n\n###### Please note that kaggle doesn't support the interact feature, run on your local machine or a colab notebook to view","06ddce44":"In 2015, the country of Nepal suffered a catastrophic earthquake that affected millions of people residing in 11 of it's 77 districs, in this notebook we try to analyze data recorded by the government of Nepal, which carried out a massive household survey using mobile technology to assess building damage in the earthquake-affected districts for social aid purposes. The data recorded however, is not at all limited to just building make-up and damage data; on the contrary it's rich with socioeconomic data of the affected individuals and their families, which we will try to explore to understand more about nepali culture, as well as how this earthquake impacted their lives using Python.\n\nThe data was collected between January and May of 2016 and is readily available through the [Nepal Earthquake: Open Data Portal](https:\/\/eq2015.npc.gov.np\/#\/).\n\n\nStarting off with population demographics, a population pyramid could help us understand a lot more about the people of Nepal, their age-sex composition as well as future trends in a population.","064b165d":"With 40% of those making Rs. 50 thousand or more (which translates\n   to about $416.71 USD), it seems that being illiterate in nepal without any\n formal school experience doesn't at all prohibit you from making the\n  highest classification of monthly income in our dataset.\n\nHowever, there's a clear visual trend in the above bar charts that those with higher\nand higher educational levels increase in percentage the higher the income gets.\n\nThe graph above shows us that no less than 40% of the household heads are\n illiterate in every income level. Since we're talking about adult household heads here\n with an average age higher than the rest of the residents. What can we say about the future generations of the population?\n Are they getting more educated than their predecessors?\n","c342e332":"<h1><center>The Gorkha Earthquake<\/center><\/h1>","9433ca28":"Unfortunately, the least populated district was one the most damaged ones.\n There's a clear concentration or a \"path\" the earthquake took starting from Gorkha to Okhaldhunga.\n\nWith the most severely damaged district being Sindhupalchok with over 96% needing major repairs done to them.","d64114bd":"# **Classficiation**","51c7bfc8":"# Geographical Analysis","2875ebdf":"Unfortunately, it seems like there's no hope for our models with an unbalanced dataset. Even searching the grid of hyperparameters yields little to no result in improving the false negative rate.\n\nLet's undersample to tune the hyperparameters as well as to train our model, we'll aim for an equal sample of each class and visualize our performance. \n\nWe'll use only a sample of the data on the grid search to save time, to make sure our grid search is optimal and will give us the best results, we'll draw a learning curve to see at what point does the classifiers' performance stop increasing as data continues to increase","f9dd32a8":"\nFortunately, it seems like that the newer generations of the population\nare heading into the right direction, as it appears that there's a positive correlation with how recently a person was born and their tendency to be literate.\nWith the literacy rate increasing not only overall, but the ratios between the genders approaching the diagonal equality line meaning both genders are equally as literate in the most recent generations, this can only be good news.\n","fd6618a0":"Above, we find the best 3 version of the selected models. And fortunately, they're better in every way! The question remains, though, which is the best one?\n\nIn my opinion, the well-being of the citizens is the most important, which is why I think that despite the Adaboost classifier having a higher average F1-score,the random forest classifier is the best one between the three:\n\nFor all models, there's a trade off between classifying more building as damaged, and risking a lower precision but higher recall, or being more picky with the classification and having lower recall and higher precision. Random forests has the highest recall of all three models, meaning that while it might misclassify some building as damaged while they're just fine(lower precision), it will classify more of the actually damaged buildings as damaged(higher recall). \n\nThe random forest classifier has a higher recall score for damaged buildings, and not a whole lot worse of a recall score for the undamaged ones compared to the other models, wasting some resources to unnecessarily check structurally sound buildings if this model was ever applied to a real life scenario, but preventing disaster and saving lives.\n\nFinally, to recap, the random forest model we trained can:\n\n* Label around 90% of the buildings that are prone to damage as damaged \n* Label around 57% of the buildings that aren't prone to damage as undamaged \n\nKeep in mind that 68% if the buildings identified as damaged were actually damaged and 85% of the buildings labelled as undamaged were actually undamaged.\n\nWhile I tried to cover as much as I could about the dataset in this notebook, there's plenty more to explore! Feel free to try exploring the dataset yourself and I hope this was an enjoyable read!  ","6015af01":"It seems like our dataset is heavily unbalanced. Only 21.8% are undamaged unfortunately. This could cause the model to be very biased. To avoid any bias in comparing between different classifcation algorithms, this needs to be paid attention to when evaluating our models' performance metrics.\n\nBefore passing any of our data to the model, it needs to be prepared; we'll vectorize all categorical columns using a OneHotEncoder to turn them into columns of 0s and 1s:","6fbe5c26":"In the next part of this notebook, we will try to use statistical models to predict which buildings could've been classified as prone to being significantly damaged after an earthquake of the same magnitude as well as identifying ones that wouldn't. We'll use all information available regarding the buildings prior to the disaster, including but not limited to floor count, floor type, height and area of the buildings, and more.\n\nFor this classification problem, we will test out algorithms that use decision trees in one form or another as their estimators.\n\nWe'll be testing out:\n\n* Decision Tree Classifier\n* AdaBoost Classifier\n* Random Forest Classifier\n* Gradient Boosting Classifier\n\nand seeing which of them perform best in classifying our dataset.\n\nBefore classifying, let's look at how equal the classes in our dataset are. We will consider any building that's classified as \"Grade 3\" damage or above as Damaged enough to consider in the damaged class, while others are undamaged.\n\nLet's visualize all the damaged vs. undamaged buildings in the dataset. We'll also drop all irrelevant columns..","88f5edf2":"Makwanpur and Kavrepalanchok seem to be the most densely populated\ndistricts of all, both collectively housing almost 1 million of individuals affected by the earthquake.\nAnd with Rasuwa being the least populated district with almost 58 thousand residents,\n all other districts fall in between.\n\nThe unequal distribution of the population among the districts could\n cause a misinterpretation of the raw damage was drawn on the next map.\n\nTo fix this, the below illustration shows the percentage of the total buildings\nthat needed major repairs done to them as a result of the earthquake. The 3rd of 5 grades of damage classification available in our dataset.\n\n###### To observe raw damage for all districts, using an unnormalized damage score that's not as meaningful, choose the raw damage level option below. Please note that kaggle doesn't support the interact feature, run on your local machine or a colab notebook to view","28f062bf":"We'll now do an evaluating heatmap to help us better understand the performance as well as. We'll train the models with their default parameters which isn't really expected to give us the best results, especially that it's tuned for accuracy over anything else which isn't very good at all in our case due to the major inbalance.\n\nStill, however, it might help us drop the least fit model. ","f2ecc9dc":"# Population Analysis","6c94220f":"It seems like the training score for all the models seem to converge right below 0.8 (80%), this means that as we feed the model more and more data from our dataset, it's performance wouldn't improve any further. However, tuning the model is still an option, we will do a full grid search for the best hyper parameters to use, using the 15000 training data point limit where the model performance plateaus illustrated by the graph above to get the best possible hyperparameters."}}