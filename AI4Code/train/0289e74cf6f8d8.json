{"cell_type":{"bce6eff5":"code","b3beb5a4":"code","1451fe2c":"code","d826f214":"code","8ffbd320":"code","855b7506":"code","75635cd1":"code","9c60cf10":"code","c0f87aed":"code","d9d79327":"code","6ed67062":"code","8037cc15":"code","df71fabf":"code","cb8f1b86":"code","015ef183":"code","69053c99":"code","b2da4413":"code","f388b69a":"code","cf1cda0b":"code","edf82c83":"code","17c885ef":"code","774cbf74":"markdown","9a5ada02":"markdown"},"source":{"bce6eff5":"\n# # ADD1) jigsaw-toxic-severity-rating\n# # ADD2) jigsaw-toxic-comment-classification-challenge\n\n# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3beb5a4":"!pip install nltk\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#!pip install xgboost\nfrom xgboost import XGBRegressor ","1451fe2c":"dirname = '\/kaggle\/input\/jigsaw-toxic-severity-rating'\n# submission = pd.read_csv(f\"{dirname}\/sample_submission.csv\")\n# df_val = pd.read_csv(f\"{dirname}\/validation_data.csv\")\ndata = pd.read_csv(f\"{dirname}\/comments_to_score.csv\")\n\n\n# https:\/\/www.kaggle.com\/samarthagarwal23\/mega-b-ridge-to-the-top-lb-0-85\ndirname = '\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge'\ntrain_data = pd.read_csv(f\"{dirname}\/train.csv\")\ntext = data['text']\n# df_test = pd.read_csv(f\"{dirname}\/test.csv\")\n# df_test_l = pd.read_csv(f\"{dirname}\/test_labels.csv\")","d826f214":"data.head()","8ffbd320":"train_data.head()","855b7506":"%%time\nscore = []\nfor i in range(len(train_data)):\n    x = train_data.iloc[i]\n    s = sum(x['toxic':].values) # 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n    score.append(round(s\/6, 3))","75635cd1":"%%time\ntrain_data['score'] = score\ntrain_data.head()","9c60cf10":"#Stopwords\nimport nltk\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))","c0f87aed":"def lowercase(te):\n    tmp = []\n    #Complete \n    for i in te : \n        tmp.append(i.lower())\n    return tmp\ndef remove_symbols(text):\n    #commplete\n    tmp = []\n    for i in text: \n        #i = re.sub(r'\\n','',i)\n       # i = re.sub(r'\"\"', ' ',i)\n        i = re.sub(r'[^\\w]',' ',i) #Remove all types of symbols from string\n        tmp.append(i.replace(') ',''))\n    return tmp\ndef remove_stopwords(text): \n    tmp = []\n    for i in text: \n        te = str(i)\n        words = word_tokenize(te)\n        token_without = [word for word in words if not word in stop]\n        s = ''\n        for w in token_without : \n            s = s + w + ' '\n        tmp.append(s)\n    return tmp\ndef stemming(text): \n    tmp = []\n    for i in text : \n        te = str(i)\n        lemmatizer = WordNetLemmatizer()\n        words = word_tokenize(te)\n        s = ''\n        for w in words:\n            rw = lemmatizer.lemmatize(w)\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\ndef stemming_lem_stop(text):\n    tmp = []\n    for i in text:\n        te = str(i)\n        words = word_tokenize(te)\n        ps = PorterStemmer()\n        lemmatizer = WordNetLemmatizer()\n        token_without = [word for word in words if not word in stop] #Remove Stopwords from token\n        s = ''\n        for w in token_without: \n            rootword = ps.stem(w)\n            rw = lemmatizer.lemmatize(rootword) #Lemmatization\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp","d9d79327":"%%time\ntrain1 = lowercase(train_data['comment_text'])\ntrain2 = remove_symbols(train1)\ntrain4 = stemming(train2)\ntrain3 = remove_stopwords(train4)\ntest1 = lowercase(text)\ntest2 = remove_symbols(test1)\ntest4 = stemming(test2)\ntest3 = remove_stopwords(test4)","6ed67062":"## Training set\n\n%%time\npol = []\nfor i in train3:\n    analysis = TextBlob(i)\n    x = analysis.sentiment.polarity\n    NewValue = (((x - (-1)) * 1) \/ 2) + 0 #NewValue = (((OldValue - OldMin) * NewRange) \/ OldRange) + NewMin\n    if NewValue == 0.5 :\n        pol.append(0)\n    else : \n        pol.append(round(NewValue,2))","8037cc15":"len(train3)","df71fabf":"valid_x = train3[120000:]\nvalid_y = pol[120000:]","cb8f1b86":"com = []\ndi = {}\npol1 = []\nfor i in range(len(data['comment_id'])): \n    com.append(data['comment_id'][i])","015ef183":"tfid = TfidfVectorizer(max_features = 10)\nres = tfid.fit_transform(train3).toarray()\nres1 = tfid.fit_transform(test3).toarray()\nvalid_res = tfid.fit_transform(valid_x).toarray()\n# dic = {}\n# for e1, e2 in zip(tfid.get_feature_names(), tfid.idf_): \n#     dic[e1] = e2","69053c99":"%%time\nimport xgboost as xgb \nimport optuna \nfrom warnings import simplefilter\nsimplefilter(\"ignore\", category=RuntimeWarning)\nimport sklearn.metrics\ndef objective(trial):\n    dtrain = xgb.DMatrix(res, label=pol)\n    dvalid = xgb.DMatrix(valid_res, label=valid_y)\n    \n    param = {\n        #'silent': 1,\n        'n_estimators' : trial.suggest_int('n_estimators',10,1000, log=True),\n        'max_depth':trial.suggest_int('max_depth',2,32, log= True),\n        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n        #'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n        'eta': trial.suggest_float('eta', 0.01, 1, log=True)\n    }\n\n    bst = xgb.train(param,dtrain)\n    preds = bst.predict(dvalid)\n    #pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.mean_absolute_error(valid_y, preds)\n    \n    return accuracy \n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)","b2da4413":"trial = study.best_trial\nval = []\nfor key, value in trial.params.items():\n    val.append(value)\n    print(\"    {}: {}\".format(key, value))","f388b69a":"model = XGBRegressor(n_estimators = val[0],max_depth = val[1] ,booster = val[2], eta = val[3])\nmodel.fit(res,pol)\ny_pred = abs(model.predict(res1))","cf1cda0b":"di = {'comment_id': com, 'score': y_pred}\ndf = pd.DataFrame(di)","edf82c83":"#Output file\ndf.to_csv(\"submission.csv\", index=False)","17c885ef":"df","774cbf74":"### libraries","9a5ada02":"submit - sample code\n- https:\/\/www.kaggle.com\/kush1729\/xgboostregressor-with-optuna"}}