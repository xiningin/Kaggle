{"cell_type":{"364511fe":"code","ae8e68a8":"code","d23c4b6e":"code","50e49ad5":"code","cf1047b1":"code","b0805a9b":"code","d5aefac0":"code","aa96fabc":"code","0601b685":"code","5e86f7c0":"code","811d97c5":"code","26f30708":"code","067d9c70":"code","e2dcc994":"code","5c5a6553":"code","9c9140d8":"code","a209428b":"code","036f8a2a":"code","02c8a3d3":"code","c9be1feb":"code","3a730469":"code","8fd597bf":"code","c766e40a":"code","2d900d0a":"code","d510ef38":"code","e8658d9a":"code","748ec999":"code","13226e46":"code","b4bd751f":"code","b47064cb":"code","dbf6f19a":"code","6fb312ee":"code","70b0da4a":"code","8d788c28":"code","12ea567c":"code","a900d7b5":"code","47e48003":"code","1d31d3b1":"code","7778b6ec":"code","338e00d0":"code","9aa5d809":"code","f0f7c314":"code","814b10b3":"code","2a11073c":"code","7fd2a3f8":"code","ce77701b":"code","6752d1ef":"code","9d61c47d":"code","b8873077":"code","ac49640d":"code","96fce35b":"code","78f8b857":"code","dec7e35d":"code","a7c5d54d":"code","f3ed9d60":"code","9209453b":"code","f57591e4":"code","f95eb0c4":"code","6af0f693":"code","3c240224":"code","36860b35":"code","f1ae1e5b":"code","0a4e4a03":"code","ace94e92":"code","f601504b":"code","fca90198":"code","92576f97":"code","b05e3a33":"code","48a54c03":"code","c8a8ebbd":"code","631b4c00":"code","d5a3f793":"code","f1cabeba":"code","4f9e684d":"code","e23bb19f":"code","a00a8513":"code","538ff567":"code","f8b5f253":"code","34976e48":"code","40d6fb6a":"markdown","f8b5e7ab":"markdown","06540e66":"markdown","baed98e1":"markdown","f70889ed":"markdown","ac9f0be1":"markdown","cb2ca340":"markdown","aabdaadc":"markdown","b6515dc2":"markdown","81920256":"markdown","6534ad3f":"markdown","1914091e":"markdown","bb99efdb":"markdown","f22148e3":"markdown","df5b02f0":"markdown","be8316a6":"markdown","7a4e60e5":"markdown","07f026f4":"markdown","abb31827":"markdown","4cdc362f":"markdown","846b94bc":"markdown","acd5e1b4":"markdown","78ffaff6":"markdown","a88ce410":"markdown","ea56c92c":"markdown","3fabb2cb":"markdown","0562eb03":"markdown","c016e87f":"markdown","8a1f11e5":"markdown","cb9816f9":"markdown","7146ebbf":"markdown","c0c6a3a5":"markdown","48e57227":"markdown","92159a85":"markdown","63b32528":"markdown","97cf4866":"markdown","7214e8ce":"markdown"},"source":{"364511fe":"import pandas as pd\nfrom collections import Counter\nimport numpy as np\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re","ae8e68a8":"train_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')","d23c4b6e":"train_set.info()","50e49ad5":"test_set.info()","cf1047b1":"train_set.groupby([\"Embarked\"], as_index=False)['Survived'].mean()","b0805a9b":"xp=train_set.groupby([\"Pclass\",\"Embarked\"], as_index=False)['Survived'].mean()\nxp_pivot=pd.pivot_table(xp, values=\"Survived\", index=\"Embarked\", columns=\"Pclass\")\n\ncolors=(\"gold\",\"silver\",\"chocolate\")\nax=xp_pivot.plot(kind=\"bar\", color=colors, edgecolor='darkblue', linewidth=1)\nfig = ax.get_figure()\nfig.set_size_inches(12,8)\nax.set_xlabel(\"Port of Embarkment\", fontsize=14, color=\"navy\")\nax.set_ylabel(\"Percentage\", fontsize=14, color=\"navy\")\nax.set_title(\"Survival by PClass and Port of Embarkment\", fontsize=14, color=\"navy\")\nax.set_xticklabels([\"C\",\"Q\",\"S\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax.spines.values():\n    spine.set_edgecolor('navy')\nax.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\nplt.show()","d5aefac0":"Counter(train_set['Embarked'].values)","aa96fabc":"#We already see there are no missing \"Embarked\" values in the test dataset. Let's have a look at it, anyway:\nCounter(test_set['Embarked'].values) ","0601b685":"train_set['Embarked'] = train_set['Embarked'].fillna('S')","5e86f7c0":"# and we convert the embarkment port values into numbers (numerical categories):\n\ntrain_set[\"Embarked\"]=train_set[\"Embarked\"].map({\"C\":0, \"Q\":1, \"S\":2})\ntest_set[\"Embarked\"]=test_set[\"Embarked\"].map({\"C\":0, \"Q\":1, \"S\":2})","811d97c5":"train_set.groupby([\"Survived\"], as_index=False)['Age'].mean()","26f30708":"age1=train_set[train_set[\"Survived\"]==1].groupby([\"Pclass\",\"Sex\"], as_index=False)['Age'].mean()\nage0=train_set[train_set[\"Survived\"]==0].groupby([\"Pclass\",\"Sex\"], as_index=False)['Age'].mean()\n\nage1_pivot = pd.pivot_table(age1, values=\"Age\", index=\"Sex\", columns=\"Pclass\")\nage0_pivot = pd.pivot_table(age0, values=\"Age\", index=\"Sex\", columns=\"Pclass\")\n\nfig, axes= plt.subplots(1,2, figsize=(24,8))\n\ncolors=(\"royalblue\",\"purple\",\"violet\")\n\nax1=age1_pivot.plot(kind=\"bar\", ax=axes[0], color=colors, edgecolor='darkblue', linewidth=1.5)\nax1.set_xlabel(\"Survived\", fontsize=18, color=\"navy\")\nax1.set_ylabel(\"Age\", fontsize=18, color=\"navy\")\nax1.set_title(\"Survivor Average Age by PClass and Sex\", fontsize=18, color=\"navy\")\nax1.set_xticklabels([\"female\",\"male\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax1.spines.values():\n    spine.set_edgecolor('navy')\nax1.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\n\nax0=age0_pivot.plot(kind=\"bar\", ax=axes[1], color=colors, edgecolor='darkblue', linewidth=1)\nax0.set_xlabel(\"Deceased\", fontsize=18, color=\"navy\")\nax0.set_ylabel(\"Age\", fontsize=18, color=\"navy\")\nax0.set_title(\"Deceased Average Age by PClass and Sex\", fontsize=18, color=\"navy\")\nax0.set_xticklabels([\"female\",\"male\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax0.spines.values():\n    spine.set_edgecolor('navy')\nax0.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\nplt.show()","067d9c70":"s1=train_set[train_set.Age.isnull()].groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Pclass\"].count()\ns2=test_set[test_set.Age.isnull()].groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Pclass\"].count()\ns=pd.DataFrame()\ns[\"Sex\"]=s1[\"Sex\"]\ns[\"Pclass\"]=s1[\"Pclass\"]+s2[\"Pclass\"]\n\ns3=train_set.groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Age\"].mean()\ns4=test_set.groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Age\"].mean()\ns[\"Avg_age\"]=(s3[\"Age\"]+s4[\"Age\"])\/2\n\ns5=train_set.groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Age\"].std()\ns6=test_set.groupby([\"Pclass\",\"Sex\"], as_index=False)[\"Age\"].std()\ns[\"std\"]=(s5[\"Age\"]+s6[\"Age\"])\/2\n\ns[\"+std\"]=s[\"Avg_age\"] + s[\"std\"]\ns[\"-std\"]=s[\"Avg_age\"] - s[\"std\"]","e2dcc994":"s","5c5a6553":"# Imputation of the missing \"Age\" values by the class and the sex of the passenger:\n\nmissing_ages=[]\n\ndef set_age(pclass,sex):\n       \n    age=np.random.randint(s[s['Sex']==sex][\"-std\"].iloc[pclass-1], \n                                           s[s['Sex']==sex][\"+std\"].iloc[pclass-1])\n    return(age)\n\nfor entry in train_set[train_set.Age.isnull()].values:\n    missing_ages.append(set_age(entry[2],entry[4]))\ntrain_set[\"Age\"][np.isnan(train_set['Age'])]=missing_ages\nmissing_ages=[]\n\nfor entry in test_set[test_set.Age.isnull()].values:\n    missing_ages.append(set_age(entry[1],entry[3]))\ntest_set[\"Age\"][np.isnan(test_set['Age'])]=missing_ages\nmissing_ages=[]","9c9140d8":"a=train_set[train_set['Survived']==1][\"Age\"]\nb=train_set[train_set['Survived']==0][\"Age\"]\nx=(a,b)\n\nfig, ax= plt.subplots(figsize=(12,6))\ncolors = ['skyblue', 'royalblue']\nax.hist(x, bins=20,  color=colors, edgecolor='darkblue', linewidth=3, histtype='bar', stacked=True,  \n         label=(\"Survived\",\"Deceased\"))\nax.set_ylabel('Number of people', fontsize=14, color=\"navy\")\nax.set_title('Survival by Age Intervals', fontsize=14, color=\"navy\")\nax.legend(loc=\"upper right\")\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor('navy')\n\nplt.show()","a209428b":"import matplotlib.ticker as ticker\n\nsurvived=[]; size=[]; labels=[]\ni=0; j=4\n\nwhile j<100:\n\n    c=len(train_set[(train_set['Survived']==1)&(train_set['Age']>=i)&(train_set['Age']<j)][\"Age\"])\n    d=len(train_set[(train_set['Survived']==0)&(train_set['Age']>=i)&(train_set['Age']<j)][\"Age\"])\n    if c+d!=0:\n        survived.append(round(c\/(c+d)*100))\n        size.append(c+d)\n        labels.append(f\"{i}-{j}:\")\n    i=j; j+=4\n    \n\nfig, ax = plt.subplots(figsize=(12,9))\n\nax.bar(labels, survived, color=\"skyblue\", edgecolor='royalblue', linewidth=1, width=0.9, label='Survived')\n\nfor i, p in enumerate(ax.patches):\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy()\n    ax.annotate('{}%\\n of\\n {}\\n ppl'.format(height,size[i]), (p.get_x()+0.15*width, p.get_y()+.5*height), \n                        fontsize=8, fontweight='bold', color='blue')\n\nax.tick_params(axis='x', labelrotation=45, labelcolor=\"darkblue\", labelsize=10)\nax.tick_params(axis='y', labelcolor=\"darkblue\", labelsize=10)\nax.set_ylabel('Percentage', fontsize=14, color=\"navy\")\nax.set_title('Survival by Age Intervals', fontsize=14, color=\"navy\")\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor('royalblue')\n    \ntick_spacing=10\nax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nax.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\nplt.show()","036f8a2a":"a=train_set[(train_set['Pclass']==1)&(train_set['Survived']==1)][\"Age\"]\nb=train_set[(train_set['Pclass']==2)&(train_set['Survived']==1)][\"Age\"]\nc=train_set[(train_set['Pclass']==3)&(train_set['Survived']==1)][\"Age\"]\n\nd=train_set[(train_set['Pclass']==1)&(train_set['Survived']==0)][\"Age\"]\ne=train_set[(train_set['Pclass']==2)&(train_set['Survived']==0)][\"Age\"]\nf=train_set[(train_set['Pclass']==3)&(train_set['Survived']==0)][\"Age\"]\n\nx=[a,b,c]\ny=[d,e,f]\n\nplt.figure(figsize=(16,8))\n\nplt.subplot(121)\ncolors = ['red', 'blue', 'lightgreen']\nplt.hist(x, bins=20, histtype='bar', color=colors, edgecolor='royalblue', linewidth=1,stacked=True, \n         label=(\"1st class\",\"2nd class\",\"3rd class\"))\nplt.xlabel('Age', fontsize=14, color=\"navy\")\nplt.ylabel('Number of people', fontsize=14, color=\"navy\")\nplt.title('Survived', fontsize=14, color=\"navy\")\nplt.legend(loc=\"upper right\")\n\n\nplt.subplot(122)\nplt.hist(y, bins=20, histtype='bar', color=colors, edgecolor='royalblue', linewidth=1,stacked=True, \n         label=(\"1st class\",\"2nd class\",\"3rd class\"))\nplt.xlabel('Age', fontsize=14, color=\"navy\")\nplt.ylabel('Number of people', fontsize=14, color=\"navy\")\nplt.title('Deceased', fontsize=14, color=\"navy\")\nplt.legend(loc=\"upper right\")\n\nplt.show()","02c8a3d3":"# I firstly create percentage (age\/survival) bins and then add each passenger to the related percentage range\n\ni=0; j=4\n\nage_list=[]\n\nwhile i<81:\n\n    c=len(train_set[(train_set['Survived']==1)&(train_set['Age']>=i)&(train_set['Age']<j)][\"Age\"])\n    d=len(train_set[(train_set['Survived']==0)&(train_set['Age']>=i)&(train_set['Age']<j)][\"Age\"])\n    if c+d!=0:\n        age_list.append(round(c\/(c+d)*100))\n    else:\n        age_list.append(50)\n    i=j; j+=4\n\nprint(age_list)","c9be1feb":"import math\n\ntrain_set[\"Age_Cat\"]=0\ntest_set[\"Age_Cat\"]=0\n\ndef set_age_cat(age):\n    \n    a=int(age\/4)\n    lv=math.ceil(age_list[a]\/20)\n    if lv==5:           # There's only one \"5\" value in the train set (none in the test set), \n        lv=4            # so it will be wise to put it in \"4\" bin\n        \n    return(lv)    \n    \nfor dataset in (train_set, test_set):\n    for i,j in enumerate(dataset[\"Age\"]):\n        dataset[\"Age_Cat\"][i]=set_age_cat(j)","3a730469":"Counter(train_set[\"Age_Cat\"])","8fd597bf":"Counter(test_set[\"Age_Cat\"])","c766e40a":"# We manipulate the data and create a new  column \"IsAlone\". \n\ntrain_set['FamilySize'] = train_set['SibSp'] + train_set['Parch'] + 1\ntest_set['FamilySize'] = test_set['SibSp'] + test_set['Parch'] + 1\n\ntrain_set['IsAlone'] = 0\ntrain_set.loc[train_set['FamilySize'] == 1, 'IsAlone'] = 1\ntest_set['IsAlone'] = 0\ntest_set.loc[test_set['FamilySize'] == 1, 'IsAlone'] = 1","2d900d0a":"train_set.groupby([\"IsAlone\"], as_index=False)['Survived'].mean()","d510ef38":"train_set[\"Sex\"]=train_set[\"Sex\"].map({\"male\":0, \"female\":1})\ntest_set[\"Sex\"]=test_set[\"Sex\"].map({\"male\":0, \"female\":1})","e8658d9a":"train_set.groupby([\"Sex\"], as_index=False)['Survived'].mean()","748ec999":"train_set.groupby([\"Pclass\",\"Sex\"], as_index=False)['Survived'].mean()","13226e46":"train_set[\"Fare\"][np.isnan(train_set['Fare'])]=(train_set['Fare'].mean()+test_set['Fare'].mean())\/2\ntest_set[\"Fare\"][np.isnan(test_set['Fare'])]=(train_set['Fare'].mean()+test_set['Fare'].mean())\/2","b4bd751f":"train_set.groupby([\"Survived\"], as_index=False)['Fare'].mean()","b47064cb":"xp=train_set.groupby([\"Pclass\",\"Survived\"], as_index=False)['Fare'].mean()\nxp_pivot=pd.pivot_table(xp, values=\"Fare\", index=\"Survived\", columns=\"Pclass\")\n\ncolors=(\"skyblue\",\"royalblue\",\"darkblue\")\nax=xp_pivot.plot(kind=\"bar\", color=colors, edgecolor='darkblue', linewidth=1)\nfig = ax.get_figure()\nfig.set_size_inches(8,6)\nax.set_xlabel(\"Survival\", fontsize=14, color=\"navy\")\nax.set_ylabel(\"Fare Amount\", fontsize=14, color=\"navy\")\nax.set_title(\"Survival by PClass and Fare\", fontsize=14, color=\"navy\")\nax.set_xticklabels([\"Deceased\",\"Survived\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax.spines.values():\n    spine.set_edgecolor('navy')\nax.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\nplt.show()","dbf6f19a":"fare1=train_set[train_set[\"Survived\"]==1].groupby([\"Pclass\",\"Sex\"], as_index=False)['Fare'].mean()\nfare0=train_set[train_set[\"Survived\"]==0].groupby([\"Pclass\",\"Sex\"], as_index=False)['Fare'].mean()\n\nfare1_pivot = pd.pivot_table(fare1, values=\"Fare\", index=\"Sex\", columns=\"Pclass\")\nfare0_pivot = pd.pivot_table(fare0, values=\"Fare\", index=\"Sex\", columns=\"Pclass\")\n\nfig, axes= plt.subplots(1,2, figsize=(24,8))\n\ncolors=(\"royalblue\", \"purple\",\"violet\")\n\nax1=fare1_pivot.plot(kind=\"bar\", ax=axes[0], color=colors, edgecolor='darkblue', linewidth=1.2)\nax1.set_xlabel(\"Survived\", fontsize=18, color=\"navy\")\nax1.set_ylabel(\"Fare Amount\", fontsize=18, color=\"navy\")\nax1.set_title(\"Average Fares by PClass and Sex\", fontsize=18, color=\"navy\")\nax1.set_xticklabels([\"female\",\"male\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax1.spines.values():\n    spine.set_edgecolor('navy')\nax1.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\n\nax0=fare0_pivot.plot(kind=\"bar\", ax=axes[1], color=colors, edgecolor='darkblue', linewidth=1)\nax0.set_xlabel(\"Deceased\", fontsize=18, color=\"navy\")\nax0.set_ylabel(\"Fare Amount\", fontsize=18, color=\"navy\")\nax0.set_title(\"Average Fares by PClass and Sex\", fontsize=18, color=\"navy\")\nax0.set_xticklabels([\"female\",\"male\"], rotation=0, color=\"blue\", fontsize=12)\nfor spine in ax0.spines.values():\n    spine.set_edgecolor('navy')\nax0.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\nplt.show()","6fb312ee":"a=train_set[(train_set['Survived']==1)&(train_set['Fare']<=400)][\"Fare\"]\nb=train_set[(train_set['Survived']==0)&(train_set['Fare']<=400)][\"Fare\"]\nx=(a,b)\n\nfig, ax= plt.subplots(figsize=(12,6))\n\ncolors = ['skyblue', 'royalblue']\nax.hist(x, bins=25, histtype='bar', color=colors, edgecolor='blue', \n         linewidth=1, stacked=True, label=(\"Survived\",\"Deceased\"))\n\nax.set_xlabel('Fare Amount',fontsize=14, color=\"navy\")\nax.set_ylabel('Number of people', fontsize=14, color=\"navy\")\nax.set_title('Survival by Fare Amount', fontsize=14, color=\"navy\")\nax.legend(loc=\"upper right\")\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor('navy')\n\nax.legend(loc=\"upper right\")\n\nplt.show()","70b0da4a":"survived=[]; amount=[]; labels=[]\ni=0; j=5\n\nwhile j<520:\n\n    c=len(train_set[(train_set['Survived']==1)&(train_set['Fare']>=i)&(train_set['Fare']<j)][\"Fare\"])\n    d=len(train_set[(train_set['Survived']==0)&(train_set['Fare']>=i)&(train_set['Fare']<j)][\"Fare\"])\n    if c+d!=0:\n        survived.append(round(c\/(c+d)*100))\n        amount.append(c+d)\n        labels.append(f\"{i}-{j}:\")\n    i=j; j+=5\n    \nfig, ax = plt.subplots(figsize=(16,10))\n\nax.bar(labels, survived, color=\"skyblue\", edgecolor='royalblue', linewidth=1, width=0.9, label='Survived')\n\nfor i, p in enumerate(ax.patches):\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy()\n    ax.annotate('{}%\\n of\\n {}\\n ppl'.format(height,amount[i]), (p.get_x()+0.15*width, p.get_y()+.5*height), \n                        fontsize=8, fontweight='bold', color='blue')\n\nax.tick_params(axis='x', labelrotation=45, labelcolor=\"darkblue\", labelsize=10)\nax.tick_params(axis='y', labelcolor=\"darkblue\", labelsize=10)\nax.set_ylabel('Percentage', fontsize=14, color=\"navy\")\nax.set_title('Survival by Fare Intervals', fontsize=14, color=\"navy\")\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor('royalblue')\n    \ntick_spacing=10\nax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nax.grid(axis=\"y\", color=\"lightgray\", linestyle=\"-.\")\n\nplt.show()","8d788c28":"i=0; j=5\n\nfare_list=[]\n\nwhile j<520:\n\n    c=len(train_set[(train_set['Survived']==1)&(train_set['Fare']>=i)&(train_set['Fare']<j)][\"Fare\"])\n    d=len(train_set[(train_set['Survived']==0)&(train_set['Fare']>=i)&(train_set['Fare']<j)][\"Fare\"])\n    if c+d!=0:\n        fare_list.append(round(c\/(c+d)*100))\n    else:\n        fare_list.append(\"n\/a\")\n    i=j; j+=5\nprint(fare_list)","12ea567c":"train_set[\"Fare_Cat\"]=0\ntest_set[\"Fare_Cat\"]=0\n\ndef set_fare_cat(fare):\n    \n    f=int(fare\/5)\n    \n    if fare_list[f] ==\"n\/a\":\n        return(3)\n    elif fare_list[f] ==0:\n        return(1)      \n    else:\n        return(math.ceil(fare_list[f]\/20)) \n    \n    \nfor dataset in (train_set, test_set):\n    for i,j in enumerate(dataset[\"Fare\"]):\n        dataset[\"Fare_Cat\"][i]=set_fare_cat(j)","a900d7b5":"Counter(train_set[\"Fare_Cat\"])","47e48003":"Counter(test_set[\"Fare_Cat\"])","1d31d3b1":"# We'll need some regexp operation in order to get the titles from the name column. In these datasets \n# titles end in \".\" , which makes it easy to fetch them, but if there were any without \".\" at the end, \n# we wouldn't be able to get them very easily.\n\n# You can play with the small code below to understand this regexp code better. \n# re.search('([A-Za-z]+)\\.', name) begins with the first word in the sequence, but \n# re.search(' ([A-Za-z]+)\\.', name) would start with the second item. Please note that \n# in our datasets the first item in the name string is the family name, so we skip the first item.\n# .group() returns the whole string (\"Mr.\") searched, whereas .group(1) returns (\"Mr\")\n\ndef get_title(name):\n    title = re.search('([A-Za-z]+)\\.', name) \n    if title:\n        return title.group(1)\n    return\n\nname=\"Mr. Sean. Connerie\"\nprint(get_title(name))\n\nname=\"Mr Sean. Connerie\"\nprint(get_title(name))\n\nprint()\n\ndef get_title(name):\n    title = re.search(' ([A-Za-z]+)\\.', name) \n    if title:\n        return title.group(1)\n    return\n\nname=\"Mr. Sean. Connerie\"\nprint(get_title(name))\n\nname=\"Mr Sean. Connerie\"\nprint(get_title(name))","7778b6ec":"def get_title(name):\n    title = re.search(' ([A-Za-z]+)\\.', name) #Since the first item in the name string is the family name, we skip the first word\n\n    if title:\n        return title.group(1)\n    return\n\nfor ds in (train_set, test_set):\n    ds['Title'] = ds['Name'].apply(get_title)","338e00d0":"print(\"TrainSet Titles: \", train_set[\"Title\"].unique())\nprint(\"TestSet Titles: \", test_set[\"Title\"].unique())","9aa5d809":"#Let's check if everything's alright:\n\nprint(\"TrainSet Titles: \", train_set[\"Title\"].unique())\nprint(\"TestSet Titles: \", test_set[\"Title\"].unique())","f0f7c314":"for ds in (train_set, test_set):\n    ds['Title'] = ds['Title'].replace(['Lady', 'Countess','Capt', 'Don', \n                                                 'Major','Mme','Mlle', 'Sir', 'Jonkheer', 'Dona'], 'Other')","814b10b3":"#Let's check if everything's alright:\n\nprint(\"TrainSet Titles: \", train_set[\"Title\"].unique())\nprint(\"TestSet Titles: \", test_set[\"Title\"].unique())","2a11073c":"# We convert the titles to numerical values:\n\ntrain_set[\"Title\"]=train_set[\"Title\"].map({\"Other\":0, \"Mr\":1, \"Mrs\":2,\"Miss\":3, \"Master\":4,\"Ms\":5, \"Dr\":6, \"Rev\":7, \"Col\":8})\ntest_set[\"Title\"]=test_set[\"Title\"].map({\"Other\":0, \"Mr\":1, \"Mrs\":2,\"Miss\":3, \"Master\":4,\"Ms\":5, \"Dr\":6, \"Rev\":7, \"Col\":8})","7fd2a3f8":"trn=train_set.copy()\ntst=test_set.copy()\n\ntrn.drop([\"PassengerId\",\"Name\",\"SibSp\", \"Parch\", \"Ticket\", \"FamilySize\", \"Cabin\"], axis=1, inplace=True)\ntst.drop([\"PassengerId\",\"Name\",\"SibSp\", \"Parch\", \"Ticket\",\"FamilySize\",\"Cabin\"], axis=1, inplace=True)","ce77701b":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\nfrom category_encoders import LeaveOneOutEncoder, CatBoostEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, StackingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport xgboost as xgb\nfrom vecstack import stacking, StackingTransformer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom catboost import CatBoostClassifier\n\nss=StandardScaler()","6752d1ef":"Xtrn=trn.drop([\"Survived\"], axis=1)\nfeats_names=Xtrn.columns #for the feature importance plot we'll make below\nXtst=tst\nXtrn = ss.fit_transform(Xtrn)\nXtst = ss.fit_transform(Xtst)\n\ny_trn=trn[\"Survived\"]","9d61c47d":"feats_names","b8873077":"dmatrix = xgb.DMatrix(data=Xtrn,label=y_trn)","ac49640d":"X_train, X_test, y_train, y_test = train_test_split(Xtrn, y_trn, test_size=0.2, shuffle=True, random_state=8888)\n\nmodel1=xgb.XGBClassifier(learning_rate=0.005, max_depth=9, n_estimators= 500, random_state=8888, verbosity=0) \n\nmodel1.fit(X_train,y_train)","96fce35b":"model1.score(X_train,y_train)","78f8b857":"model1.score(X_test,y_test)","dec7e35d":"# I've calculated the variance for this model though I can't say how I could use it at the moment. \n# However, it may be possible to use variance or standard deviation for \"threshold moving\", which quite\n# often gives much better results.\n\npreds_train=model1.predict_proba(X_train)\n\ntrl=list(y_train)\ntsl=list(y_test)\n\navg=[]\n\nfor i,j in enumerate(preds_train):\n    if (trl[i]==1) & (j[1]<0.5):\n        avg.append((j[1]-0.5)**2)\n\nvar=(sum(avg)\/len(avg))\nprint(f\"The variance for the train set: {var}%\")\n\npreds_test=model1.predict_proba(X_test)\n\navg=[]\n\nfor i,j in enumerate(preds_test):\n    if (tsl[i]==1) & (j[1]<0.5):\n        avg.append((j[1]-0.5)**2)\n\nvar=(sum(avg)\/len(avg))\nprint(f\"The variance for the test set: {var}%\")","a7c5d54d":"preds_test=model1.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n\nprint('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, preds_test)))","f3ed9d60":"# xgboost cross validation:\n\nfrom xgboost import cv\n\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.008,\n                'max_depth': 8, 'alpha': 5}\n\nxgb_cv = cv(dtrain=dmatrix, params=params, nfold=5,\n                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)","9209453b":"xgb_cv.head()","f57591e4":"# We can see which features had the biggest weight on the result for this model:\n\nxgb.plot_importance(model1).set_yticklabels(feats_names)\nplt.figure(figsize = (16, 12))\nplt.show()","f95eb0c4":"ypreds1 = model1.predict_proba(Xtst)\npredicts1=[]\nfor i in ypreds1:\n    if i[1]<0.5:\n        predicts1.append(0)\n    else:\n        predicts1.append(1)\n        \nprint(predicts1)","6af0f693":"yc=trn[\"Survived\"]\nXc=trn.drop([\"Age\",\"Fare\",\"Survived\"], axis=1)\n\nXc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, shuffle=True, random_state=4444)\n\nparams= {'max_depth':6, 'n_estimators':150, 'use_best_model':True,  'l2_leaf_reg':6, \n         'learning_rate': 0.05, 'cat_features':list(range(6)), 'verbose':0}\nmodel2 = CatBoostClassifier(**params)\nmodel2.fit(Xc_train,yc_train,eval_set= (Xc_test, yc_test), plot=True)","3c240224":"model2.score(Xc_train,yc_train)","36860b35":"predictions_cat = model2.predict(Xc_test)\naccuracy_score(yc_test,predictions_cat ) * 100","f1ae1e5b":"Xc.head(1)","0a4e4a03":"# We can see the weights of the features for this CatBoostClassifier model\n# Observation: Family size is the least important, whereas sex is the most important (just an observation :)\n\nmodel2.get_feature_importance()","ace94e92":"Xctst=tst.drop([\"Age\",\"Fare\"], axis=1)\nypreds2=model2.predict_proba(Xctst)","f601504b":"predicts2=[]\nfor i in ypreds2:\n    if i[1]<0.5:\n        predicts2.append(0)\n    else:\n        predicts2.append(1)\n        \nprint(predicts2)","fca90198":"# dummification of the categorical variables:\n\ny_trn=trn[\"Survived\"]\ntrn_=trn.drop(\"Survived\", axis=1)\ntst_=tst\n\ntrn_ = pd.get_dummies(trn_, columns=[\"Sex\",\"Embarked\",\"Fare_Cat\",\"Age_Cat\",\"Pclass\",\"IsAlone\",\"Title\"],\n                          prefix=[\"Sex\",\"Emb\",\"Fare\",\"AgeCat\",\"Class\",\"Status\",\"Title\"])\n\ntst_ = pd.get_dummies(tst_, columns=[\"Sex\",\"Embarked\",\"Fare_Cat\",\"Age_Cat\",\"Pclass\",\"IsAlone\",\"Title\"],\n                         prefix=[\"Sex\",\"Emb\",\"Fare\",\"AgeCat\",\"Class\",\"Status\",\"Title\"])","92576f97":"trn_.columns","b05e3a33":"tst_.columns","48a54c03":"ss=StandardScaler()\n\nX_trn_ = ss.fit_transform(trn_)\nX_tst_ = ss.fit_transform(tst_)","c8a8ebbd":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras import optimizers","631b4c00":"model3 = Sequential()\n\nmodel3.add(Dense(30, activation='relu', input_dim=30, kernel_initializer='uniform'))\nmodel3.add(Dropout(0.5)) #used to pessimize overfitting\n\nmodel3.add(Dense(60, kernel_initializer='uniform', activation='relu'))\nmodel3.add(Dropout(0.5))\n\n#k_model.add(Dense(30, kernel_initializer='uniform', activation='relu')) #another layer may be added if desired\n#k_model.add(Dropout(0.5))\n\nmodel3.add(Dense(1,kernel_initializer='uniform', activation='sigmoid'))\n\nmodel3.summary()","d5a3f793":"adam = optimizers.Adam(lr=0.01)\n\nmodel3.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel3.fit(X_trn_, y_trn, batch_size = 60,  epochs = 30, verbose=2)","f1cabeba":"scores = model3.evaluate(X_trn_, y_trn, batch_size=30, verbose=0)\nscores[1]","4f9e684d":"ypreds=model3.predict(X_tst_)\n\npredicts3=[]\nfor i in ypreds:\n    if i<0.5:\n        predicts3.append(0)\n    else:\n        predicts3.append(1)\n        \nprint(predicts3)","e23bb19f":"X=trn.drop([\"Survived\"], axis=1)\ny=trn[\"Survived\"]\n\nX = ss.fit_transform(X)\ntst = ss.fit_transform(tst)","a00a8513":"rs=4949\nlr=0.05\nne=250\nmd=8\n\n# I've actually experimented with various techniques like the implementation of the pipelines \n# within the stacking classifiers and for this purpose I've created various transformers. However, \n# I'll make do without the pipelines in this final section of my notebook.\n\n#num_feats=[\"Age\",\"FarePclass\"]\n#cat_feats=[\"Sex\", \"IsAlone\",\"Embarked\",\"Title\",\"Pclass\"]\n#cat1_feats=[\"Age_Cat\",\"Fare_Cat\"]\n\n#num_pipe=make_pipeline(StandardScaler())\n#cat_pipe=make_pipeline(StandardScaler())\n#cat1_pipe=make_pipeline((StandardScaler()))\n\n#transformer0=make_column_transformer((cat_pipe, cat_feats),(cat1_pipe, cat1_feats))\n#transformer1=make_column_transformer((num_pipe, num_feats), (cat_pipe, cat_feats))\n#transformer2=make_column_transformer((num_pipe, num_feats), (cat1_pipe, cat1_feats))\n#transformer3=make_column_transformer((num_pipe, num_feats),(cat_pipe, cat_feats),(cat1_pipe, cat1_feats))\n\n\nfinal_layer_rfr = LogisticRegression(max_iter=1000,random_state=rs, penalty=\"l2\")\n\nfinal_layer_gbr = GradientBoostingClassifier(n_estimators=100, max_depth=6, \n                                             max_leaf_nodes=5, random_state=rs)\n\nfinal_layer = StackingClassifier(estimators=[('fl1', final_layer_rfr),('fl2', final_layer_gbr)],\n                                             final_estimator=\n                                                    SVC(probability=True))\n                                 \n\nmlc = StackingClassifier(\n    estimators=[\n        \n    ('est1', CatBoostClassifier(learning_rate=lr, n_estimators=ne, max_depth=md, random_state=rs, verbose=0)),\n    ('est2', xgb.XGBClassifier(learning_rate=lr, n_estimators=ne, max_depth=md, random_state=rs, verbosity=0)),\n    ('est3', RandomForestClassifier(n_estimators=ne, max_depth=md, random_state=rs))\n                    \n    #('est1', make_pipeline(transformer0,CatBoostClassifier(learning_rate=0.08, max_depth=6, \n    #                                              n_estimators=150, random_state=rs, verbose=0))),\n    #('est2', make_pipeline(transformer3, XGBClassifier(learning_rate=0.08, random_state=rs, \n    #                                                 n_estimators=150, max_depth=6, verbosity=0))),                                           \n    #('est3', make_pipeline(transformer3, GradientBoostingClassifier(n_estimators=150, max_depth=6, \n    #                                         max_leaf_nodes=5, random_state=rs)))\n                ],\n    \n    final_estimator=final_layer)\n\n#mlc.fit(X_train, y_train)\n#print('Score: {:.2f}'.format(mlc.score(X_test, y_test)))\n\nmlc.fit(X, y)\n\ncross_val = KFold(n_splits=5, shuffle=True, random_state=rs)\nscores = cross_val_score(mlc, X, y, cv=cross_val, scoring=\"accuracy\")\nprint(\"Mean Score for the model: \", scores.mean())","538ff567":"# We fit the model after cross validation again and get the final model\n\nf_model=mlc.fit(X,y)","f8b5f253":"ypreds4 = f_model.predict_proba(tst)\npredicts4=[]\nfor i in ypreds4:\n    if i[1]<0.5:\n        predicts4.append(0)\n    else:\n        predicts4.append(1)\nprint(predicts4)","34976e48":"# And a seemingly primitive but actually quite effective method of taking the average of the predictions\/\n# probabilities from various models and forming the final predictions. I'm not yet capable of using some more\n# advanced techniques, but my learning adventure is still quite young. :)\n\nypreds3=np.concatenate((1-ypreds,ypreds), axis=1) # Keras predictions were the True values only\n\nfpredicts=[]\n#pos_pred=(np.array(predicts)+np.array(predicts2)+np.array(predicts3)+np.array(predicts4))\/4\npos_pred=(ypreds1+ypreds2+ypreds3+ypreds4)\/4\nfor i in pos_pred:\n    if i[1]<0.5:\n        fpredicts.append(0)\n    else:\n        fpredicts.append(1)\nprint(fpredicts)","40d6fb6a":"### 1.3. Family Size\n\nIt may be a good idea to have a look at the effect of the family size on survival. By adding up the number of siblings, spouses, parents and children we can actually get this number (plus the passenger him\/herself):","f8b5e7ab":"We see that this is a true observation for all passenger classes.","06540e66":"Let's have a look at the Survival&Age relationship by the Passenger Class as well:","baed98e1":"Now we can create fare categories more reasonably by using the data we have got by now:","f70889ed":"**Observation:** \"Fare\" seems to have some correlation with \"survival\", but this is more true with Class 1, less true with Class 2 and almost false with Class 3. Thus, I've decided to manipulate the \"Fare\" data and create a new feature \"FarePclass\" which will be more or less independent. (It's possible to consider the sex of the passenger further)\n\n\nNow let's have a look at how this is reflected for each sex:","ac9f0be1":"Let's quickly check whether or not there are any missing values in the datasets:","cb2ca340":"**Observation:** it will be better to take the embarkment port into consideration, which is proven by the prediction models as well.\n\nNow let's have a look at the embarkment-survival relation by passenger class:","aabdaadc":"Observation: \"when a man loves a woman..\"","b6515dc2":"### 1.5. Fare","81920256":"Let's see what percentage of\/how many people survived by fare intervals:","6534ad3f":"### 1.1.6. Title\n\nI personally don't know whose idea it initially was to get some meaningful feature from the \"Title\" column. Some simple regexp operation would solve everything, but the original idea was sure worth praising. Maybe it was thanks to the simple fact that the \".\" added to titles made the initial idea owner to come to the conclusion that this data should also be used. Well.. after all, we don't always put \".\" after titles in English, but our dataset, thank God, had this convenience for us.. Anyway, let's get back to the job:\n\nWe'll use some small regexp code to fetch the titles from the name strings. Please see the note below if you'd like some more details about the next code. Or, please safely skip to the code.","1914091e":"and have a look at the correlation between the sex parameter and the survival:","bb99efdb":"**Observation:** Family Size indeed has some significant effect on survival, which is indeed confirmed by the the feature importance of a model below.","f22148e3":"There are 171 \"Age\", 687 \"Cabin\" and 2 \"Embarked\" values missing in the train set.","df5b02f0":"We can already see the age&survival relationship from the graph, but it'll be a good idea to have a look at the exact figures for each interval. (It is possible to make the intervals bigger or smaller, of course.)","be8316a6":"# 2. Models","7a4e60e5":"**Observation:** \"Age\" by itself doesn't seem to have very high effect on survival. However, it is an important parameter with respect to various other variables like pclass and sex. Let's have a look:","07f026f4":"Fare amount seems to have some significant impact on survival. Let's see how this is reflected by passenger class.","abb31827":"Thus, it will be wiser to impute the missing age values in accordance with the class and the sex of the passenger. The following code calculates mean and std for the six different pclass&sex combinations:","4cdc362f":"Now let's see how it looks:","846b94bc":"### 1.4. Sex\n\nAll we do is convert the object values to numbers:","acd5e1b4":"### 1.2. Age\n\nNow let's impute the missing ages. Throughout my learning adventure from \"Titanic learning from the disaster\", I've come across several methods dealing with the missing age valuess. The decision mostly depended on whether the data would be a useful feature and also whether the missing data could be predicted without making significant mistakes. \n\nLet's, first of all, see the effect of \"Age\" variable on survival:","78ffaff6":"Now let's have a look at the relationship between survival and fare intervals:","a88ce410":"___\n\n<img style=\"width: 700px; height:1000 px;\" src=\"https:\/\/www.linkpicture.com\/q\/disaster.png\" class=center>\n___\n\nI'll begin this notebook with a rather personal reflection that I find somehow bizarre, yet, at the same time, quite mysterious. As we all know, in this competition we're trying to analyze the tragic incident with the Titanic, the unsinkable, thus, trying to learn from a disaster. Back then for a lot of people it was simply impossible for the Titanic to sink. Perhaps, like in the etymological definition of the word [\"disaster\"](https:\/\/www.merriam-webster.com\/dictionary\/disaster) above, it was rather about being \"ill-starred\" than being \"ill-constructed\". Something else that strikes me is the following analogy:\n\nastronomical : astrological\nnumerical : numerological\n\nI must add that I've only been learning ML for a few months and there have been moments when I couldn't help thinking what we were doing was some sort of hocus pocus. I had the same feeling even after reading some about all these formulas, statistical calculations, concepts and ideas. No matter how logical everything might sound, I still had the same feeling of this \"mathematical magic\".\n\nThis is my third notebook on Kaggle. The first two were NLP related, the field in which I'd like to improve my skills and grow. Still, I found this topic interesting, too, and I decided to improve my machine learning skills by participating in this knowledge competition. Througout my learning journey I've had a look at quite many notebooks prepared by experienced Kagglers. I always upvoted those that I found useful. (I am genuinely sorry if I somehow forgot to.) I hope that I'll also be able to be a little bit useful for others with this notebook.\n\n\n    Outline:\n\n    * Data Preparation\n        * Imputation of the missing values\n        * Data manipulation based on some EDA\n        \n    * Feature selection and model creation\n        * Model1: XGBClassifier with Cross Validation\n        * Model2: CatboostClassifier\n        * Model3: Keras Sequential\n        * Model4: StackingClassifier (with Pipelines)                         \n","ea56c92c":"## 2.4. Stacking Classifier (with pipelines)","3fabb2cb":"### Creating representative categories for \"Age\" values\n\nIt's known that categorical data is quite effective with certain ML models (like, for example, a model with the CatBoostClassifier). Yet, it's more important to wisely divide the data range into best categories. \nHere is my take:","0562eb03":"### 1.1. Cabin and Embarked\n\nLet's begin with the easiest ones: \"Cabin\" and \"Embarked\"\n\nIt's obvious that there are too many missing \"Cabin\" values and it is almost impossible to safely calculate\/predict the correct cabin values. So, it will be wise to drop this column. (This is just an assumption, of course. One may claim that missing \"Cabin\" values can also be filled in after making some careful mathematical calculations and prediction.)\n\nAs for the \"Embarked\" column, we can see there are only 2 missing values, so it'll be good enough to replace them with the most frequent one of the three values:","c016e87f":"**Observation:** Outliers is an issue to be dealt with for the \"Fare\" values. ","8a1f11e5":"There are 86 \"Age\", 1 \"Fare\" and 327 \"Cabin\" values missing in the test set.","cb9816f9":"### Some data manipulation with \"Fare\" values:","7146ebbf":"### Some data manipulation with \"Age\" values:\n\nIt may be a good idea to create smart age groups as some models work much better with categorical variables than numerical ones. There are many different approaches to dividing the whole age range into various categories. I'll do this by considering the age-survival relationship: ","c0c6a3a5":"## 2.2. CatBoostClassifier with categorical features","48e57227":"## 2.3. Keras Sequential Model","92159a85":"## 2.1. XGBClassifier with cross validation","63b32528":"### Imputation of the missing Fare values:\n\nThere is only one missing fare value in the test set, so it will be sufficient to impute it with the mean of the all fare values from both sets:","97cf4866":"**Observation:** It's possible to get a lot of information for this graph. We can find out, for example, mostly 1st class passengers survived. Also, the very young (0-5) were more lucky to survive. However, unfortunately, 3rd class passengers mostly died.","7214e8ce":"# 1. Data Preparation"}}