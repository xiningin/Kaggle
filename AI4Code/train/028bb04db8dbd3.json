{"cell_type":{"8c2f454f":"code","a7098779":"code","9eecc14e":"code","e52babb6":"code","1cec4b8b":"code","369b6400":"code","54b44ac1":"code","db386129":"code","ac7779e4":"code","09f875ad":"code","5e77a0a1":"code","ff68f078":"code","33e6f206":"code","e1820efb":"code","24df1089":"code","7cacea5d":"code","ee44110c":"code","7ad783d1":"code","e899ffa5":"code","6bf8457d":"code","8331c24e":"code","a77fdf2b":"code","f0768bad":"code","b8966e62":"code","93e38f2d":"code","4577bb73":"code","57c34462":"code","47235ef5":"code","556a5c8a":"code","49f50f5e":"code","004231c4":"code","2f20bcdc":"code","2bae9080":"code","70c42150":"code","8414989e":"code","7983f20e":"code","7bd6044e":"code","efe1e332":"code","43a5277c":"code","c3a5fbc5":"code","2019d195":"code","94a02277":"code","c7479f9b":"code","93b80f35":"code","3249a501":"code","84857416":"code","54908dda":"code","0bbd360d":"code","936de111":"code","5641ce27":"code","5d30b337":"code","395f471b":"code","2bdb6f37":"code","8aca68dc":"code","ccb16f76":"code","755f4627":"code","98cd6ff4":"code","0d1c6c14":"code","d6e09f58":"code","0a091b27":"code","aefa0c53":"code","423ee04d":"code","0d965fa8":"code","ecfe2da6":"code","cd87d2a7":"code","8ce8a299":"code","88a2acaf":"code","07871efd":"code","8308e11c":"code","8b3a017a":"code","ceb49857":"code","83466ddd":"code","7303b95f":"code","cc05cce2":"code","135db4a3":"code","f3591337":"code","8096152b":"code","fbd1a1d0":"code","38102963":"code","c20fcfcc":"code","03effe94":"code","f747f75f":"code","0d7863a0":"code","fffd829d":"code","ca4823c5":"code","4f1dc9e6":"markdown","da68f124":"markdown","a9c43bcb":"markdown","b3a50d6c":"markdown","bfdf2e57":"markdown","9b7672e2":"markdown","cc6971ed":"markdown","00ec4281":"markdown","bfb053af":"markdown","4ae83f5f":"markdown","a5cbcb6e":"markdown","b53feadc":"markdown","efa183f6":"markdown","feb72237":"markdown","7de1c824":"markdown","b05c4b2a":"markdown","19e1f681":"markdown","5798fc68":"markdown","3d8b6731":"markdown","6634e0a2":"markdown","c59f4462":"markdown","661f32f4":"markdown","6dfab581":"markdown","2c004609":"markdown","ffe53d02":"markdown","dd577fe0":"markdown","9f6d8715":"markdown","393c88de":"markdown","7aad211c":"markdown"},"source":{"8c2f454f":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import PowerTransformer\nimport os\n%matplotlib inline","a7098779":"# Limits floats output to 3 decimal points\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) ","9eecc14e":"# Files in directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e52babb6":"#import data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col=0)\n\n#Check size and look\nprint(train.shape)\ntrain.head()","1cec4b8b":"# Check size and look of data\nprint(test.shape)\ntest.head()","369b6400":"# Combine train and test for pre-processing\ndf_all = pd.concat([train[train.columns[:-1]],test])\ndf_all.head(5)","54b44ac1":"# Save training observations for later\ny = train.SalePrice","db386129":"# Number and types of columns\ndf_all.info()","ac7779e4":"# Looking at distribution of house prices\nplt.figure(figsize=[20,5])\n\n# Histogram plot\nplt.subplot(1,2,1)\nsns.distplot(y)\nplt.title('Standard')\n\n# Skewness and kurtosis\nprint(\"Skewness: %f\" % y.skew())\nprint(\"Kurtosis: %f\" % y.kurt())\n\n# Due to skew (>1), we'll log it and show it now better approximates a normal distribution\nplt.subplot(1,2,2)\nsns.distplot(np.log(y))\nplt.title('Log transformation')","09f875ad":"# Convert y into log(y)\ny_original = y.copy()\ny = np.log(y)","5e77a0a1":"# Look for missing data\nplt.figure(figsize=[20,5])\nsns.heatmap(df_all.isnull(),yticklabels=False,cbar=False)","ff68f078":"# Dropping data that is heavily missing (will deal with partially missing later)\ndf_all.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","33e6f206":"# Values for feature 'MSSubClass'\ndf_all.MSSubClass.unique()","e1820efb":"# Use dictionaries to convert across\ndf_all = df_all.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})","24df1089":"# Values for feature 'MoSold'\ndf_all.MoSold.unique()","7cacea5d":"# Use dictionaries to convert into month strings\ndf_all = df_all.replace({\"MoSold\" : {1 : \"January\", 2 : \"February\", 3 : \"March\", 4 : \"April\", \n                                       5 : \"May\", 6 : \"June\", 7 : \"July\", 8 : \"August\", \n                                       9 : \"September\", 10 : \"October\", 11 : \"November\", 12 : \"December\"}})","ee44110c":"# Check it worked as expected\ndf_all[['MSSubClass','MoSold']].head(5)","7ad783d1":"# No. of categoric variables\ncat_feats = df_all.dtypes[df_all.dtypes == \"object\"].index.tolist()\nprint(str(len(cat_feats)) + ' categoric features')","e899ffa5":"# No. of numerical variables\nnum_feats = df_all.dtypes[df_all.dtypes != \"object\"].index.tolist()\nprint(str(len(num_feats)) + ' numeric features')","6bf8457d":"# Return list of categoric columns with missing variables\ncat_feats_missing = df_all[cat_feats].columns[df_all[cat_feats].isna().any()].tolist()\ncat_feats_missing","8331c24e":"# Show value occurences to determine appropriate NaN replacement\nfor i in cat_feats_missing:\n    print(i)\n    print(df_all[i].value_counts(dropna=False))\n    print(\"\")","a77fdf2b":"# Make replacements into most likely value\n\n# Likely RL\ndf_all.MSZoning.fillna('RL', inplace = True)\n# Drop utilities as only 1 is different which doesn't help\ndf_all.drop('Utilities',axis=1,inplace=True)\n# Likely VinylSd\ndf_all.Exterior1st.fillna('VinylSd', inplace = True)\n# Likely VinylSd\ndf_all.Exterior2nd.fillna('VinylSd', inplace = True)\n# Likely no masonary\ndf_all.MasVnrType.fillna('None', inplace = True)\n# Likely no basement\ndf_all.BsmtQual.fillna('No basement',inplace = True)\n# Likely no basement\ndf_all.BsmtCond.fillna('No basement',inplace = True)\n# Likely no basement\ndf_all.BsmtExposure.fillna('No basement',inplace = True)\n# Likely no basement\ndf_all.BsmtFinType1.fillna('No basement',inplace = True)\n# Likely no basement\ndf_all.BsmtFinType2.fillna('No basement',inplace = True)\n# Likely standard electrical\ndf_all.Electrical.fillna('SBrkr',inplace = True)\n# Likely typical kitchen\ndf_all.KitchenQual.fillna('TA',inplace = True)\n# Likely typical functionality\ndf_all.Functional.fillna('Typ',inplace = True)\n# Likely no garage\ndf_all.GarageType.fillna('No garage',inplace = True)\n# Likely no garage\ndf_all.GarageFinish.fillna('No garage',inplace = True)\n# Likely no garage\ndf_all.GarageQual.fillna('No garage',inplace = True)\n# Likely no garage\ndf_all.GarageCond.fillna('No garage',inplace = True)\n# Likely typical sale type\ndf_all.SaleType.fillna('WD',inplace = True)\n\n# Check it worked correctly\ndf_all.head(5)","f0768bad":"# Using descriptions in 'About this file' we can order some categoric variables\ndf_all = df_all.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No basement\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No basement\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No basement\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No basement\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No basement\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No garage\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No garage\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","b8966e62":"# Checking for collinearity between similar variables\nbasement_feats = ['BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual']\nexterior_feats = ['ExterCond','ExterQual']\ngarage_feats = ['GarageCond','GarageQual']\n\n\nplt.figure(figsize=[20,5])\n\n# basement_feats plot\nplt.subplot(1,3,1)\nsns.heatmap(df_all[basement_feats].corr(), vmin = -1, vmax=1, annot=True, cmap=\"coolwarm\")\nplt.title('basement_feats')\n\n# exterior_feats plot\nplt.subplot(1,3,2)\nsns.heatmap(df_all[exterior_feats].corr(), vmin = -1, vmax=1, annot=True, cmap=\"coolwarm\")\nplt.title('exterior_feats')\n\n# garage_feats plot\nplt.subplot(1,3,3)\nsns.heatmap(df_all[garage_feats].corr(), vmin = -1, vmax=1, annot=True, cmap=\"coolwarm\")\nplt.title('garage_feats')","93e38f2d":"# Although 'BsmtQual' and 'ExterQual' are highly correlated, they should be independant of each other so will both stay.\n\n# 'GarageCond'and 'GarageQual' are highly correlated. Check what's more correlated to SalePrice\ntrain_temp = pd.concat([df_all[:train.shape[0]],y],axis=1)\ntrain_temp[basement_feats+garage_feats + ['SalePrice']].corr()","4577bb73":"# We'll keep 'BsmtQual' as it's more related to 'SalePrice'\ndf_all.drop('BsmtCond',axis=1,inplace=True)\n\n# We'll keep 'GarageQual' as it's more related to 'SalePrice'\ndf_all.drop('GarageCond',axis=1,inplace=True)","57c34462":"# No. of categoric variables\ncat_feats = df_all.dtypes[df_all.dtypes == \"object\"].index.tolist()\nprint(str(len(cat_feats)) + ' categoric features')\nprint(\"\")\nfor i in cat_feats:\n    print(i)\n    print(df_all[i].value_counts(dropna=False))\n    print(\"\")","47235ef5":"# No. of categoric variables\ncat_feats = df_all.dtypes[df_all.dtypes == \"object\"].index.tolist()\nprint(str(len(cat_feats)) + ' categoric features')\ndf_all = pd.get_dummies(df_all)\ndf_all.head(5)","556a5c8a":"null_columns=df_all.columns[df_all.isnull().any()].tolist()\ndf_all[null_columns].describe()","49f50f5e":"df_all[null_columns].isnull().sum()","004231c4":"# Some (to me) make sense to likely be 0\nto_set_to_zero = ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']\ndf_all[to_set_to_zero] = df_all[to_set_to_zero].fillna(0)\n\n# Drop GarageYrBlt as unlikely to be able to fill years, correlated to YearBuilt & has a value 2207 so question marks on it's validity...\ndf_all.drop('GarageYrBlt',axis=1,inplace = True)\n\n# Replace the rest with the median of the column:\ndf_all = df_all.fillna(df_all.median())","2f20bcdc":"# Using original list of numerical features (with 'GarageYrBuilt' removed), check the skewness\nnum_feats.remove('GarageYrBlt')","2bae9080":"# Calculate skewness\nskewed_feats = df_all[num_feats].apply(lambda x: skew(x)) \nskewed_feats.sort_values()","70c42150":"# Return columns with high skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75].index\nskewed_feats","8414989e":"# Annoyingly, Box-Cox is breaking for 3 features due to a precision issue (I think, see https:\/\/github.com\/scipy\/scipy\/issues\/7534)\n# Therefore need to remove these 3 from list to transform\nskewed_feats=skewed_feats.tolist()\nskewed_feats = [e for e in skewed_feats if e not in ['GrLivArea','LotArea','1stFlrSF']]","7983f20e":"# Transform numerical variables through the box-cox method (optimises transformation to a gaussian distribution)\n# Requires a +1 as inputs need to be strictly positive\npt = PowerTransformer('yeo-johnson',standardize=False)\nprint(pt.fit(df_all[skewed_feats])) \nprint(\"\")\n\n# Show lambdas to see what transformation was applied\nprint(pt.lambdas_)","7bd6044e":"# Insert these back into the dataframe\ndf_all[skewed_feats] = pt.transform(df_all[skewed_feats])\n\n# Log the failed features\ndf_all[['GrLivArea','LotArea','1stFlrSF']]=df_all[['GrLivArea','LotArea','1stFlrSF']].apply(np.log)\n\n# Read to list\nskewed_feats = skewed_feats + ['GrLivArea','LotArea','1stFlrSF']\n\n# Check the new skews (still could be improved, but a lot better than before!)\ndf_all[skewed_feats].skew().sort_values()","efe1e332":"X_train = df_all[:train.shape[0]]\nX_test = df_all[train.shape[0]:]\n\n# Check they are the same shape as started. They are, great.\nprint(X_train.shape)\nprint(X_test.shape)","43a5277c":"# Correlations with SalePrice (look to make sense)\npd.concat([X_train,y],axis=1).corr().iloc[:,-1].sort_values(ascending=False).head(10)","c3a5fbc5":"# Look for outliers\nplt.figure(figsize=[20,5])\n\n# 'OverallQual' plot\nplt.subplot(1,2,1)\nsns.scatterplot(x = X_train['OverallQual'], y = y)\nplt.title('OverallQual')\nplt.ylabel('SalePrice')\nplt.xlabel('OverallQual')\n\n# 'GrLivArea' plot\nplt.subplot(1,2,2)\nsns.scatterplot(x = X_train['GrLivArea'], y = y)\nplt.title('GrLivArea')\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.grid(b=bool, which='major', axis='both')","2019d195":"# Will remove two values that don't match distribution on 'GrLivArea'. Turns out this is also the two outliers on OverallQual = 10 (see below)\nindex_to_drop = X_train[(X_train['GrLivArea']>8.3) & (y<12.5)].index.tolist()\n# Remove from training feature set\nX_train = X_train.drop(index_to_drop,axis=0)\n# Remove from training observation set\ny = y.drop(index_to_drop)\n\n# Will also remove three values at the bottom that don't fit the pattern.\nindex_to_drop = X_train[(X_train['GrLivArea']>6.5) & (y<10.7)].index.tolist()\n# Remove from training feature set\nX_train = X_train.drop(index_to_drop,axis=0)\n# Remove from training observation set\ny = y.drop(index_to_drop)","94a02277":"# As above, checking they're gone\nplt.figure(figsize=[20,5])\n\n# 'OverallQual' plot\nplt.subplot(1,2,1)\nsns.scatterplot(x = X_train['OverallQual'], y = y)\nplt.title('OverallQual')\nplt.ylabel('SalePrice')\nplt.xlabel('OverallQual')\n\n# 'GrLivArea' plot\nplt.subplot(1,2,2)\nsns.scatterplot(x = X_train['GrLivArea'], y = y)\nplt.title('GrLivArea')\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.grid(b=bool, which='major', axis='both')","c7479f9b":"# Check still the same\nprint(X_train.shape)\nprint(y.shape)","93b80f35":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Scale features for improved regularisation performance\n\n# Fit scaler (using RobustScaler to reduce effect of outliers) to training set mean and variance\nscaler = RobustScaler()\nscaler.fit(X_train)\n\n# Transform both the training and testing sets\nscaled_features_train = scaler.transform(X_train)\nscaled_features_test = scaler.transform(X_test)\n\n# Put scaled data back into a pandas dataframe\nX_train = pd.DataFrame(scaled_features_train,columns = X_train.columns)\nX_test = pd.DataFrame(scaled_features_test,index = X_test.index, columns = X_test.columns)\nX_train.head(5)","3249a501":"from sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNetCV","84857416":"# Check still the same\nprint(X_train.shape)\nprint(y.shape)","54908dda":"# Split train (t) into train (t_train) and test (t_test) sets.\n# This allows us to evaluate the model for 'unseen' data and check for overfitting \n\nXt_train, Xt_test, yt_train, yt_test, = train_test_split(X_train, y, test_size = 0.3)","0bbd360d":"from sklearn.metrics import mean_squared_error, make_scorer\n\n# Common method 1 across models\n\ndef evaluate_model(model):\n    \n    # Produce predictions for training and testing sets\n    yt_train_pred = model.predict(Xt_train)\n    yt_test_pred = model.predict(Xt_test)\n    \n    # Evaluate models\n    rmse_train = np.sqrt(mean_squared_error(yt_train,yt_train_pred))\n    rmse_test = np.sqrt(mean_squared_error(yt_test,yt_test_pred))\n    print(\"RMSE on Training set :\", rmse_train)\n    print(\"RMSE on Test set :\", rmse_test)\n    \n    # Graphically compare predictions on training and validation set\n    plt.figure(figsize=[20,8])\n    # Plot residuals\n    plt.subplot(1,2,1)\n    plt.scatter(yt_train_pred, yt_train_pred - yt_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(yt_test_pred, yt_test_pred - yt_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(model)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\n    # Plot predictions\n    plt.subplot(1,2,2)\n    plt.scatter(yt_train_pred, yt_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(yt_test_pred, yt_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(model)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")    ","936de111":"# Method 2 - less obvious to see what's happening, but better for parameter hypertuning\n# Decided to use the full X_train as cross validation shouldn't be testing on seen data and the more data the better the final parameter selection should be.\n\ndef cross_validation_evaluation(model):\n    scores = -cross_val_score(model, Xt_train, yt_train, cv=10,scoring=\"neg_mean_squared_error\")\n    return scores.mean()","5641ce27":"# Instantiate the linear regression model\nlinear_regression_model = LinearRegression()\nlinear_regression_model.fit(Xt_train, yt_train)","5d30b337":"# Evaluate model\nevaluate_model(linear_regression_model)","395f471b":"# Instantiate the linear regression model\nridge_regression_model = Ridge(alpha=1.0)\nridge_regression_model.fit(Xt_train, yt_train)","2bdb6f37":"# Evaluate model\nevaluate_model(ridge_regression_model)","8aca68dc":"# Test different alphas\nalphas = np.logspace(start=-2,stop=2,base=10,num=30)\ncv_ridge = [cross_validation_evaluation(Ridge(alpha = alpha)) \n            for alpha in alphas]\noptimised_alpha = alphas[cv_ridge.index(min(cv_ridge))]\nprint('Optimised alpha is: ' + str(optimised_alpha))\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"alpha hypertuning\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.xscale('log')","ccb16f76":"# Instantiate the improved ridge regression model\nridge_regression_improved_model = Ridge(alpha=optimised_alpha)\nridge_regression_improved_model.fit(Xt_train, yt_train)","755f4627":"# Evaluate model\nevaluate_model(ridge_regression_improved_model)","98cd6ff4":"# Seeing what were considered important features (positive and negative)\ncoef = pd.Series(ridge_regression_improved_model.coef_, index = X_train.columns)\nplt.figure(figsize=[20,8])\nimportant_features = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nsns.barplot(y=important_features.index,x=important_features)","0d1c6c14":"# Instantiate the lasso regression model\nlasso_regression_model = Lasso()\nlasso_regression_model.fit(Xt_train, yt_train)","d6e09f58":"# Evaluate model\nevaluate_model(lasso_regression_model)","0a091b27":"# Test different alphas\nalphas = np.logspace(start=-5,stop=-1,base=10,num=30)\ncv_lasso = [cross_validation_evaluation(Lasso(alpha = alpha)).mean() \n            for alpha in alphas]\noptimised_alpha = alphas[cv_lasso.index(min(cv_lasso))]\nprint('Optimised alpha is: ' + str(optimised_alpha))\n\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"alpha hypertuning\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.xscale('log')","aefa0c53":"# Instantiate the improved lasso regression model\nlasso_regression_improved_model = Lasso(alpha=optimised_alpha)\nlasso_regression_improved_model.fit(Xt_train, yt_train)","423ee04d":"# Evaluate model\nevaluate_model(lasso_regression_improved_model)","0d965fa8":"# Seeing how many features were removed by the Lasso model\ncoef = pd.Series(lasso_regression_improved_model.coef_, index = X_train.columns)\nprint(\"Lasso model picked \" + str(sum(coef != 0)) + \" variables and eliminated \" +  str(sum(coef == 0)) + \" variables\")","ecfe2da6":"# Seeing what were considered important features (positive and negative)\nplt.figure(figsize=[20,8])\nimportant_features = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nsns.barplot(y=important_features.index,x=important_features)","cd87d2a7":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV","8ce8a299":"xg_reg = xgb.XGBRegressor()","88a2acaf":"xg_reg.fit(Xt_train, yt_train)","07871efd":"# Evaluate model\nevaluate_model(xg_reg)","8308e11c":"# # Commented out to improve performance\n\n# # Set the parameters by cross-validation\n\n# xgb1 = XGBRegressor()\n\n# tuned_parameters = {'objective':['reg:linear'],\n#               'learning_rate': [0.01,0.03,0.1],\n#               'max_depth': [3,5],\n#               'min_child_weight': [2,4,6],\n#               'silent': [1],\n#               'subsample': [0.7],\n#               'colsample_bytree': [0.8,1],\n#               'n_estimators': [500,1000],\n#                    'gamma':[0]}\n\n# xgb_grid = GridSearchCV(xgb1,\n#                         tuned_parameters,\n#                         cv = 5,\n#                         n_jobs = 5,\n#                         verbose=True)\n\n# xgb_grid.fit(X_train,\n#          y)\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)\n# p_dict = xgb_grid.best_params_","8b3a017a":"# THIS IS THE RESULT OF THE GRIDSEARCH COMMENTED OUT\n\np_dict = {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.03, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 1000, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}","ceb49857":"# Set the new parameters into the model\nxg_reg_improved = xgb.XGBRegressor(objective = p_dict['objective'], colsample_bytree = p_dict['colsample_bytree'], \n                          learning_rate = p_dict['learning_rate'], max_depth = p_dict['max_depth'], min_child_weight = p_dict['min_child_weight'], \n                          n_estimators = p_dict['n_estimators'], silent=p_dict['silent'], subsample=p_dict['subsample'])","83466ddd":"# Fit the model\nxg_reg_improved.fit(Xt_train, yt_train)","7303b95f":"# Show feature importances\nimport_feats_pf = pd.DataFrame({'Variable':Xt_train.columns,\n              'Importance':xg_reg_improved.feature_importances_}).sort_values('Importance', ascending=False)\nimport_feats_pf.head(10)","cc05cce2":"# Evaluate model\nevaluate_model(xg_reg_improved)","135db4a3":"# Returns an array of prediction results for a given model & feature set\n\ndef return_predictions_model(model,inputs):\n    predictions = model.predict(inputs)\n    return predictions","f3591337":"# Model predictions\nlasso_regression_predict = return_predictions_model(lasso_regression_improved_model,Xt_test)\nxg_reg_predict = return_predictions_model(xg_reg_improved,Xt_test)\nridge_regression_predict = return_predictions_model(ridge_regression_improved_model,Xt_test)","8096152b":"# Average them together\nstacked_predictions = np.array([lasso_regression_predict,xg_reg_predict,ridge_regression_predict])\nstacked_predictions_avg = np.average(stacked_predictions, axis=0)\n\nrmse_test = np.sqrt(mean_squared_error(yt_test,stacked_predictions_avg))\nprint(\"RMSE on Test set (non-weighted) :\", rmse_test)\n\n# Weighted higher on the better performing models (lasso, xgboost, ridge)\nstacked_predictions_weighted = (3*lasso_regression_predict + 2*xg_reg_predict + ridge_regression_predict)\/6\n\nrmse_test = np.sqrt(mean_squared_error(yt_test,stacked_predictions_weighted))\nprint(\"RMSE on Test set (weighted) :\", rmse_test)\n","fbd1a1d0":"# Sensecheck against a) known observations b) predictions made off the training data\nplt.figure(figsize=[20,8])\nsns.distplot(y,hist=False,label= 'Known observations')\nsns.distplot(lasso_regression_predict,hist=False, label='lasso_regression')\nsns.distplot(xg_reg_predict,hist=False, label='xg_reg')\nsns.distplot(ridge_regression_predict,hist=False, label='ridge_regression')\nsns.distplot(stacked_predictions_avg,hist=False, label='stacked_predictions')\nsns.distplot(stacked_predictions_avg,hist=False, label='stacked_predictions_weighted')\nplt.legend()","38102963":"# Get as much information as possible\nlasso_regression_improved_model.fit(X_train, y)\nxg_reg_improved.fit(X_train, y)\nridge_regression_improved_model.fit(X_train, y)","c20fcfcc":"# Model predictions\nlasso_regression_predict = return_predictions_model(lasso_regression_improved_model,X_test)\nxg_reg_predict = return_predictions_model(xg_reg_improved,X_test)\nridge_regression_predict = return_predictions_model(ridge_regression_improved_model,X_test)","03effe94":"# Average them together (weighted)\nstacked_predictions_weighted = (3*lasso_regression_predict + 2*xg_reg_predict + ridge_regression_predict)\/6","f747f75f":"# Put into dataframe\nd = {'Id': test.index, 'SalePrice': stacked_predictions_weighted}\npredictions_df = pd.DataFrame(data=d)\npredictions_df.head(5)","0d7863a0":"# Convert \npredictions_df.SalePrice = predictions_df.SalePrice.apply(np.exp)\npredictions_df.head(5)","fffd829d":"# Sensecheck against a) known observations b) predictions made off the training data\nplt.figure(figsize=[20,8])\nsns.distplot(y_original,hist=False,label= 'Known observations')\nsns.distplot(predictions_df.SalePrice,hist=False, label='Unseen data observations')\nplt.legend()","ca4823c5":"# Export for testing\npredictions_df.to_csv('output.csv', index=False)\npredictions_df.head(5)","4f1dc9e6":"# Gradient boosting (xgboost)","da68f124":"## Tune parameters","a9c43bcb":"## Converting categorical features into numerical features","b3a50d6c":"# Remove outliers in the training set","bfdf2e57":"## Simple linear regression","9b7672e2":"## Create training and testing subsets from the training data","cc6971ed":"## Dealing with missing variables","00ec4281":"# Separate back into train and test","bfb053af":"# Fit models on full test data","4ae83f5f":"# Numerical features","a5cbcb6e":"## Lasso regression","b53feadc":"# Converting mislabelled 'numerical' features into categoric features","efa183f6":"## Checking for co-linearity between similar variables","feb72237":"## Ridge regression","7de1c824":"Best linear regression model is lasso","b05c4b2a":"# Categoric features","19e1f681":"## Correcting skewness","5798fc68":"# Scale features","3d8b6731":"# Identifying categoric and numerical variables","6634e0a2":"Not sure why it isn't working after scaling... we'll try to reduce this by using regularisation methods.","c59f4462":"## Containing null values","661f32f4":"# Testing time","6dfab581":"# Looking at final output 'SalePrice' on training data","2c004609":"# Checking for missing data","ffe53d02":"## Export results","dd577fe0":"Using some techniques from the following kernels\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n* https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n* https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* https:\/\/www.kaggle.com\/vikassingh1996\/comprehensive-data-preprocessing-and-modeling\n","9f6d8715":"## Define evaluation method","393c88de":"# Stacking models test","7aad211c":"# Test on full data"}}