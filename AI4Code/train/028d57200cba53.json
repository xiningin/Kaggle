{"cell_type":{"36215dd3":"code","2d636268":"code","f1867170":"code","9e27f5fd":"code","d62c9bfb":"code","9f3b06a6":"code","65b27d63":"code","765247fb":"code","8ce3d636":"code","54086572":"code","931b7278":"code","4ed89985":"code","876f0f8a":"code","b0b4b75b":"code","2158936c":"code","6fe500e4":"code","43e6531e":"code","f3f69772":"code","f03d7b9a":"code","9a485670":"code","3589506b":"code","eda5caba":"code","c06a431e":"code","7caedcfa":"markdown","0a0cdeae":"markdown","32b5aa75":"markdown","f9659c08":"markdown","2107d5e7":"markdown","13793718":"markdown"},"source":{"36215dd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport torch\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d636268":"! pip install pytorch-tabnet","f1867170":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","9e27f5fd":"train.head()\ntrain.iloc[:,1:76].shape","d62c9bfb":"X_train = train.iloc[:,1:76].to_numpy()\ny_train = train['target'].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()","9f3b06a6":"folds = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)","65b27d63":"from pytorch_tabnet.tab_model import TabNetClassifier\n\npreds = np.zeros((len(X_test),9))\n\nfor fold,(train_idx,val_idx) in enumerate(folds.split(X_train,y_train)):\n    print(fold)\n    trainX,trainY = X_train[train_idx],y_train[train_idx]\n    valX,valY = X_train[val_idx],y_train[val_idx]\n    clf = TabNetClassifier(verbose=1,seed=42)\n    clf.fit(X_train=trainX,y_train=trainY,eval_set=[(trainX,trainY),(valX,valY)],patience=7,max_epochs=40,drop_last=False,eval_metric=['logloss'])\n    #oof[val_idx] = clf.predict_proba(valX)\n    preds += clf.predict_proba(X_test)\/folds.n_splits","765247fb":"for i in range(1,10):\n    col_name = 'Class_'+str(i)\n    sub_df[col_name] = preds[:,i-1]","8ce3d636":"sub_df.to_csv(\"submission.csv\",index=False)","54086572":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","931b7278":"train.head()\ntrain.iloc[:,1:76].shape","4ed89985":"X_train = train.iloc[:,1:76].to_numpy()\ny_train = train['target'].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()","876f0f8a":"trans = QuantileTransformer(n_quantiles=100, output_distribution='normal')\nX_train = trans.fit_transform(X_train)\nX_test = trans.transform(X_test)","b0b4b75b":"#trainX,valX,trainy,valy = train_test_split(X_train,y_train,test_size=0.2,shuffle=True,random_state=42)","2158936c":"    #n_d=64, n_a=64, n_steps=5,\n    #gamma=1.5, n_independent=2, n_shared=2,\n    #lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n    #optimizer_fn=torch.optim.Adam,\n    #optimizer_params=dict(lr=2e-2),\n    #scheduler_params = {\"gamma\": 0.95,\n    #                 \"step_size\": 20},\n    #scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,","6fe500e4":"folds = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)","43e6531e":"from pytorch_tabnet.tab_model import TabNetClassifier\n\npreds = np.zeros((len(X_test),9))\n\nfor fold,(train_idx,val_idx) in enumerate(folds.split(X_train,y_train)):\n    print(fold)\n    trainX,trainY = X_train[train_idx],y_train[train_idx]\n    valX,valY = X_train[val_idx],y_train[val_idx]\n    clf = TabNetClassifier(verbose=1,seed=42)\n    clf.fit(X_train=trainX,y_train=trainY,eval_set=[(trainX,trainY),(valX,valY)],patience=7,max_epochs=40,drop_last=False,eval_metric=['logloss'])\n    #oof[val_idx] = clf.predict_proba(valX)\n    preds += clf.predict_proba(X_test)\/folds.n_splits","f3f69772":"#from pytorch_tabnet.tab_model import TabNetClassifier\n\n#classifier = TabNetClassifier(verbose=1,seed=42)\n#classifier.fit(X_train=trainX,y_train=trainy,eval_set=[(trainX,trainy),(valX,valy)],patience=7,max_epochs=40,drop_last=False,eval_metric=['logloss'])","f03d7b9a":"#preds = classifier.predict_proba(X_test)","9a485670":"preds.shape","3589506b":"for i in range(1,10):\n    col_name = 'Class_'+str(i)\n    sub_df[col_name] = preds[:,i-1]","eda5caba":"sub_df.head()","c06a431e":"sub_df.to_csv(\"submission_qt.csv\",index=False)","7caedcfa":"***QuantileTransformer** had made a big stir in **MOA competition** last year and I could remember that we more less everybody used it in our preprocessing pipeline. So , I just thought to use it or rather check its impact in this dataset. In that process, I would implement **TabNet** (another high impactful component from MOA) with and without QuantileTransformer. \nI have tried to keep this notebook clean and simple and anyone can simply segregate \"WITH\" and \"WITHOUT\" part and use them separately as well.\nPoint to be noted, I haven't used any out of fold prediction to compare the impact of QT between two implementation rather used separate submissions :)., hope that would be more intriguing.*","0a0cdeae":"# ***TabNet without QuantileTransformer***","32b5aa75":"***This gives me a score of 1.76739 in Public test dataset***","f9659c08":"# ***TabNet Using QuantileTransformer***","2107d5e7":"This gives me a score of 1.75099 in Public test set. So , we could see there is impact whether it's going to be same in Private Date set , we will know very soon don't we :)","13793718":"QuantileTransformer could be considered as a scaling\/normalization process to normalize your data. We know, StandardScaler and MinmaxScaler are there to handling scaling but let's see how QuantileTransformer works and how it effects our data."}}