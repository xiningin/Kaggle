{"cell_type":{"fda81311":"code","1d8afa6a":"code","3651f1b8":"code","6bf65e3b":"code","266c45d7":"code","b418fdc1":"code","d4bda202":"code","0ceedcc5":"code","b5084fe6":"code","97d16ee0":"code","5a674099":"code","cbd666b4":"code","d44f7433":"code","c11e5f77":"code","b8d500b1":"code","7147eb1f":"code","a889f392":"code","7a9e6c41":"code","21a5a3c1":"code","4f65aacf":"code","0aa01ea3":"code","be895da3":"code","b5c9cdfc":"code","1dfcbfca":"code","bae79d97":"code","660781e0":"code","5fe4ccea":"code","d2092e6d":"code","bbfc092b":"code","9bbb49e2":"code","5cf49b0e":"code","1e3368ec":"code","0f7e4160":"code","18908b0e":"code","c1b13fe3":"code","98d245dd":"code","d1f2ecf0":"code","46a6aba8":"code","bfdbad14":"markdown","e5dc9867":"markdown","8f9dfeb0":"markdown","449f8971":"markdown"},"source":{"fda81311":"# Dupla:\n# Francis Pimentel\n# Vinicius Vieira","1d8afa6a":"import numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.core import Activation, Dense, Dropout\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.core import Dense, Activation\nimport keras.utils as kutils","3651f1b8":"df1 = pd.read_table('..\/input\/SW_EpisodeIV.txt',delim_whitespace=True, header=0, escapechar='\\\\')\ndf2 = pd.read_table(\"..\/input\/SW_EpisodeV.txt\",delim_whitespace=True, header=0, escapechar='\\\\')\ndf3 = pd.read_table(\"..\/input\/SW_EpisodeVI.txt\",delim_whitespace=True, header=0, escapechar='\\\\')","6bf65e3b":"df1.info()","266c45d7":"df2.info()","b418fdc1":"df3.info()","d4bda202":"all_dialogues = list(pd.concat([df1, df2, df3]).dialogue.values)\nprint('Tamanho: ', len(all_dialogues))\nprint(all_dialogues[:10])","0ceedcc5":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nimport string\n\nall_sents = [[w.lower() for w in word_tokenize(sen) if not w in string.punctuation]\\\n            for sen in all_dialogues]\n\nx = []\ny = []\n\nprint(all_sents[:10])\n\nprint('\\n')\nfor sen in all_sents:\n    for i in range(1, len(sen)):\n        x.append(sen[:i])\n        y.append(sen[i])\n        \nprint(x[:10])\nprint('\\n')\nprint(y[:10])","b5084fe6":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\nall_text = [c for sen in x for c in sen]\nall_text += [c for c in y]\n\nall_text.append('UNK')\n\nprint(all_text[:10])","97d16ee0":"words = list(set(all_text))\nprint(words[:10])","5a674099":"word_indexes = {word: index for index, word in enumerate(words)}\n\nmax_features = len(word_indexes)\n\nprint(max_features)","cbd666b4":"x = [[word_indexes[c] for c in sen] for sen in x]\ny = [word_indexes[c] for c in y]\n\nprint(x[:10])\nprint(y[:10])","d44f7433":"y = kutils.to_categorical(y, num_classes=max_features)\nprint(y[:10])","c11e5f77":"maxlen = max([len(sen) for sen in x])\nprint(maxlen)\nx = pad_sequences(x, maxlen=maxlen)\n\nprint(x[:10, -10:])\nprint(x[:10, -10:])","b8d500b1":"embedding_size = 10\n\nmodel = Sequential()\n\nmodel.add(Embedding(max_features, embedding_size, \\\n                    input_length=maxlen))\n\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(max_features, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","7147eb1f":"model.summary()","a889f392":"model.fit(x, y, epochs=10, verbose=5)","7a9e6c41":"import pickle\n\nprint(\"Saving model...\")\nmodel.save('shak-nlg.h5')\n\nwith open('shak-nlg-dict.pkl', 'wb') as handle:\n    pickle.dump(word_indexes, handle)\n\nwith open('shak-nlg-maxlen.pkl', 'wb') as handle:\n    pickle.dump(maxlen, handle)\nprint(\"Model Saved!\")","21a5a3c1":"import pickle\n\nmodel = keras.models.load_model('shak-nlg.h5')\nmaxlen = pickle.load(open('shak-nlg-maxlen.pkl', 'rb'))\nword_indexes = pickle.load(open('shak-nlg-dict.pkl', 'rb'))","4f65aacf":"sample_seed = 'may the force be with'","0aa01ea3":"sample_seed_vect = np.array([[word_indexes[c] \\\n                              if c in word_indexes.keys() else \\\n                             word_indexes['UNK'] \\\n                             for c in word_tokenize(sample_seed)]])\n                             \nprint(sample_seed_vect)","be895da3":"sample_seed_vect = pad_sequences(sample_seed_vect, maxlen=maxlen)\nprint(sample_seed_vect)","b5c9cdfc":"predicted = model.predict_classes(sample_seed_vect, verbose=0)\nprint(predicted)","1dfcbfca":"def get_word_by_index(index, word_indexes):\n    for w, i in word_indexes.items():\n        if index == i:\n            return w\n        \n    return None","bae79d97":"for p in predicted:\n    print(get_word_by_index(p, word_indexes))","660781e0":"import string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","5fe4ccea":"stopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):\n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n\n    return lemmas","d2092e6d":"tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\ntfidf_matrix = tfidf_vectorizer.fit_transform(tuple(all_dialogues))\nprint(tfidf_matrix.shape)","bbfc092b":"# Dada uma entrada, retorna o proximo dialogo, limitado a 20 palavras\ndef find_closest_response(question):\n    query_vect = tfidf_vectorizer.transform([question])\n    similarity = cosine_similarity(query_vect, tfidf_matrix)\n    max_similarity = np.argmax(similarity, axis=None)\n    \n    print('Entrada do usu\u00e1rio:', question)\n    print('Di\u00e1logo mais pr\u00f3ximo encontrado:', all_dialogues[max_similarity])\n    print('Similaridade: {:.2%}'.format(similarity[0, max_similarity]))\n    print('Di\u00e1logo seguinte:', all_dialogues[max_similarity+1])\n    return ' '.join(all_dialogues[max_similarity+1].split(' ')[:20])","9bbb49e2":"sample_seed = find_closest_response('may the force be with you')","5cf49b0e":"sample_seed","1e3368ec":"def obter_resposta_chatbot(entrada):\n    sample_seed = find_closest_response(entrada)\n    sample_seed_vect = np.array([[word_indexes[c] \\\n                                  if c in word_indexes.keys() else \\\n                                 word_indexes['UNK'] \\\n                                 for c in word_tokenize(sample_seed)]])\n\n    test = pad_sequences(sample_seed_vect, \\\n                        maxlen=maxlen, \\\n                        padding='pre')\n    \n    predicted = []\n    i = 0\n    while i < 20:\n        predicted = model.predict_classes(\n                                pad_sequences(sample_seed_vect, \\\n                                               maxlen=maxlen, \\\n                                               padding='pre'),\\\n                                verbose=0)\n        new_word = get_word_by_index(predicted[0], word_indexes)\n        sample_seed += ' ' + new_word\n\n        sample_seed_vect = np.array([[word_indexes[c] \\\n                                  if c in word_indexes.keys() else \\\n                                 word_indexes['UNK'] \\\n                                 for c in word_tokenize(sample_seed)]])\n        i += 1\n        \n    gen_text = ''\n    for index in sample_seed_vect[0][:20]:\n        gen_text += get_word_by_index(index, word_indexes) + ' '\n        \n    output = ''\n    for i, gen in enumerate(gen_text.split(' ')):\n        if gen == 'UNK':\n            output += sample_seed.split(' ')[i] + ' '\n        else:\n            output += gen + ' '\n    print('RESPOSTA CHATBOT:', output)","0f7e4160":"obter_resposta_chatbot('may the force be with you')","18908b0e":"obter_resposta_chatbot('darth vader')","c1b13fe3":"obter_resposta_chatbot('The Force will be with you. Always')","98d245dd":"obter_resposta_chatbot('I find your lack of faith disturbing')","d1f2ecf0":"obter_resposta_chatbot('Now, young Skywalker, you will die.')","46a6aba8":"obter_resposta_chatbot('There\u2019s always a bigger fish.')","bfdbad14":"** COMO FUNCIONA O CHATBOT:**\n1. Entrada do usu\u00e1rio \u00e9 buscada nos dialogos atrav\u00e9s do TF-IDF;\n2. Considerando que os di\u00e1logos s\u00e3o sequenciais, uma resposta adequada seria o dialogo imediatamente seguinte ao mais similar da entrada do usu\u00e1rio;\n3. Caso o di\u00e1logo seguinte tenha menos de 20 palavras, \u00e9 ent\u00e3o usado o modelo LSTM para gerar as palavras restantes.","e5dc9867":"**CRIANDO MODELO LSTM PARA GERAR TEXTO:**","8f9dfeb0":"**CRIANDO MODELO TF-IDF PARA ENCONTRAR O DIALOGO MAIS PROXIMO DO QUE O USUARIO DIGITAR:****","449f8971":"Conversa com personagens do Star Wars: Crie um chatbot que, dado um texto que voc\u00ea digitou, gere uma resposta no estilo \"Star Wars\". A resposta deve conter 20 palavras.\nPara criar o modelo de gera\u00e7\u00e3o de texto, voc\u00ea pode utilizar o dataset https:\/\/www.kaggle.com\/xvivancos\/star-wars-movie-scripts\n\n \n\nDeve ser entregue at\u00e9 dia 03\/11 \u00e0s 23:59"}}