{"cell_type":{"ebae5573":"code","ece31f95":"code","b26038ae":"code","7547964e":"code","41320bec":"code","5bbc5af3":"code","3407ef89":"code","2c196966":"code","55e183ae":"code","b5bce310":"code","1c43fcd2":"code","852167ee":"code","84c9f327":"code","0c89f8da":"code","5a489aa5":"code","a50877e5":"code","dfeb2517":"code","a97eec87":"code","9a233215":"code","6422afb8":"code","eba27b01":"code","0ded0ba6":"code","b7fdb228":"code","c855f928":"code","22263cb3":"code","cd36dace":"code","b43b477b":"code","8b326cfa":"code","c640f4bd":"code","838420f3":"code","cd69aa6c":"code","1fc40449":"code","1d636303":"code","909afb27":"code","0980c510":"code","b2e5c098":"code","2fc6ee05":"code","b6842c39":"code","55e71406":"code","a3dab87c":"code","1d1149f1":"code","6124550e":"code","02b01cd3":"code","614622f1":"code","aad777e4":"code","d263e377":"code","539a1883":"code","3f6edaa3":"code","88b590e9":"code","5c1dced2":"code","b0925c83":"code","b017be68":"code","fea2b4c1":"code","a51c29ae":"markdown","9475d92a":"markdown","b4deeb48":"markdown","57b6ebcd":"markdown","b045ac6f":"markdown","618838ad":"markdown","d8a76773":"markdown","bf63bcb0":"markdown","a30a068b":"markdown","c07f4b02":"markdown","14ae617a":"markdown","4dbf6f8f":"markdown","793a57bb":"markdown","50dd443a":"markdown","c3355f1f":"markdown","8a47e943":"markdown","434306c0":"markdown","076b0dfc":"markdown","e792b1ae":"markdown","85eddae3":"markdown","da4ac054":"markdown","49b73fbf":"markdown","30fc44bf":"markdown","d9500269":"markdown","d6d234e8":"markdown","9907a26e":"markdown","ffd2d45d":"markdown","9e10db32":"markdown","caee4b2c":"markdown","b902232d":"markdown","755e7dd9":"markdown","af6e6bd9":"markdown","5eb26067":"markdown","855e9c1a":"markdown","226c3f95":"markdown","a57afbcb":"markdown","7c787f0c":"markdown","64a11504":"markdown","e25a1cc0":"markdown"},"source":{"ebae5573":"import datetime as dt\nimport os.path\nimport numpy as np\nimport pandas as pd\nimport operator\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nimport warnings\nimport pickle\nfrom matplotlib.gridspec import GridSpec\nfrom sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler, StandardScaler\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, SGDRegressor, HuberRegressor\nfrom hyperopt import hp, tpe, Trials, fmin, STATUS_OK, pyll\nfrom sklearn.model_selection import KFold, RepeatedKFold, cross_val_score, cross_val_predict\nfrom scipy.stats import skew\nfrom copy import deepcopy\nfrom tabulate import tabulate\nfrom scipy.special import boxcox1p, inv_boxcox1p\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom boruta import BorutaPy\nfrom joblib import Parallel, delayed\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom lightgbm import LGBMModel\nfrom xgboost import XGBRegressor\nfrom scipy.stats import levene\n\nNBNAME = 'immediate_results_kaggle_pub\/\/'\n\nif not os.path.exists(NBNAME):\n    os.makedirs(NBNAME)\n","ece31f95":"GLOBAL_START_TIME=dt.datetime.now()","b26038ae":"# these two methoid are used for saving objects, for later use\ndef save_obj(obj, filename):\n    \"\"\"\n    Store any picklable object in file\n    \"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef load_obj(filename):\n    \"\"\"\n    Loads previously stored object from file\n    \"\"\"\n    with open(filename, 'rb') as f:\n        return pickle.load(f)\n\n\ndef drop_HCF(data: pd.DataFrame, target: pd.Series, corr_threshold: float):\n    \"\"\"\n    Drops highly correlated features. \n    Dropping is done in two steps:\n    - at first columns with corr=1 are dropped. \n    - than columns with corr above threshold are dropped. \n      The selection is made on the basis of mean correlation of the feature\n    \"\"\"\n    print('Initial number of features {}'.format(data.shape[1]))\n    start_num = data.shape[1]\n    # dropping correlations==1\n    corrdata = np.abs(data.corr(method='spearman'))\n    correlated_pairs = [(x, y) for x in corrdata.index for y in corrdata.columns if (\n        (corrdata.loc[x, y] == 1) & (x != y))]\n    import networkx as nx\n    G = nx.Graph()\n    G.add_edges_from(correlated_pairs)\n    nxdg = nx.degree_centrality(G)\n    if len(nxdg) > 0:\n        nodes = pd.DataFrame.from_dict(\n            nxdg, orient='index').sort_values(by=0, ascending=False)\n        to_be_dropped = []\n        while nodes.iloc[0, 0] > 0:\n            to_be_dropped.append(nodes.index[0])\n            G.remove_node(nodes.index[0])\n            nodes = pd.DataFrame.from_dict(nx.degree_centrality(\n                G), orient='index').sort_values(by=0, ascending=False)\n        data = data.drop(columns=to_be_dropped)\n\n    # dropping correlations based on threshold for float features\n    data_ = data[[col for col in data.columns if np.issubdtype(\n        data[col].dtype, np.float)]]\n    corrdata = np.abs(data_.corr(method='pearson'))\n    correlated_pairs = [(x, y, corrdata.loc[x, y]) for x in corrdata.index \n                        for y in corrdata.columns if \n                        ((corrdata.loc[x, y] > corr_threshold) & (x != y))]\n    correlated_pairs.sort(key=operator.itemgetter(2), reverse=True)\n    while len(correlated_pairs) > 0:\n        correlated_ = pd.DataFrame(correlated_pairs)\n        f0 = correlated_.loc[0, 0]\n        f1 = correlated_.loc[0, 1]\n        corr_val = correlated_.loc[0, 2]\n        f0_type = data[f0].dtype\n        f1_type = data[f1].dtype\n        print(\"{} [{}] - {} [{}], correlation: {:.5f}\".format(\n            f0,f0_type, f1, f1_type, corr_val))\n        c0 = (corrdata.loc[:, f0].mean())\n        c1 = (corrdata.loc[:, f1].mean())\n        if c0 >= c1:\n            data = data.drop(f0, axis=1)\n            print('Dropped {} (mean correlation with others {:.5f}), other {} \\\n                  mean correlation with others {:.5f} )'.\n                format(f0, c0, f1, c1))\n        else:\n            data = data.drop(f1, axis=1)\n            print('Dropped {} (mean correlation with others {:.5f}), other {} \\\n                  mean correlation with others {:.5f} )'.\n                format(f1, c1, f0, c0))\n        data_ = data[[col for col in data.columns if np.issubdtype(\n            data[col].dtype, np.float)]]\n        corrdata = np.abs(data_.corr(method='spearman'))\n        correlated_pairs = [(x, y, corrdata.loc[x, y]) for x in corrdata.index \n                            for y in corrdata.columns \n                            if ((corrdata.loc[x, y] > corr_threshold) & (x != y))]\n        correlated_pairs.sort(key=operator.itemgetter(2), reverse=True)\n    print('Number of features after elimination {}'.format(data.shape[1]))\n    return data, start_num != data.shape[1]\n\n\ndef evaluate_feature_set(estimators: dict, X_: pd.DataFrame, y_: pd.Series,\n                         n_splits=5, n_repeats=1, n_jobs=4, scoring='neg_mean_squared_error', \n                         random_state=None, shuffle=False,show_progressbar=True, \n                         print_results=False, save_results=None,nested=False):\n    '''\n    This is a wrapper method for Hyperopt based optimization of a set of estimators.\n\n    Parameters\n    ----------\n        @estimators : dict()\n            a dictionary of estimators including estimator, \n            estimator params for hyperopt and number of trials to be performed.\n        @X_ : pd.DataFrame\n            dataframe of features.\n        @y_: pd.Series\n            series of response values.\n        @n_splits : int, default 5\n            number of splits to be performed.\n        @n_repeats : int, default 1\n            number of splits repetitions to be performed.\n        @n_jobs : int, default 4 - the number of cores on Kaggle platform.\n            number of paralel job to be started. \n            For the best performance set to the number of expected jobs and not higher that \n            the number of physical cores.\n        @scoring : str, default 'neg_mean_squared_error'.\n            A string determining scoring function. One of the metrics from sklearn.metrics.\n        @random_state: int, default None.\n            Random seed.\n            Might be set to make the results repeatable (use the same train test split).\n        @shuffle: bool, default False\n            flag wheather to shuffle data before splits. \n        @show_progressbar : bool, default False.\n            wheather to show Hyperopt progressbar after each iteration.\n        @print_results : bool, default False.\n            wheather to print table at the end.\n        @save_results : str: default None.\n            wheather to save results to file. Default None, filename may be provided.\n    Returns: dict.\n            A dictionary of best result for each estimator provided in >>estimators<<.\n    '''\n\n    def objective_minimum_local(params):\n        # cv_splits = n_splits\n        estimator.set_params(**params)\n        scores = cross_val_score(\n            estimator=estimator, X=X_, y=y_, scoring=scoring, cv=kfold, n_jobs=n_jobs)\n        loss = np.mean(np.sqrt(np.abs(scores)))\n        std = np.std(np.sqrt(np.abs(scores)))\n        return {'loss': loss, 'loss_std': std, 'params': params, \n                'scores': [np.sqrt(np.abs(x)) for x in scores], \n                'optimized_estimator': deepcopy(estimator), 'status': STATUS_OK}\n\n    # tpe_algo = tpe.suggest\n    results = {}\n    for est in estimators:\n        if show_progressbar:\n            print(est)\n        results[est] = {}\n        # iteration = 0\n        results[est] = {}\n        # predictor_name = est\n        estimator = estimators[est][0]\n        space = estimators[est][1]\n        max_evals = estimators[est][2]\n        bayes_trials = Trials()\n        try:\n            if n_repeats == 1:\n                kfold = KFold(n_splits=n_splits,\n                              random_state=random_state, shuffle=shuffle)\n            else:\n                kfold = RepeatedKFold(\n                    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n            best = fmin(fn=objective_minimum_local, space=space, algo=tpe.suggest, \n                        max_evals=max_evals,trials=bayes_trials, \n                        show_progressbar=show_progressbar, \n                        rstate=np.random.RandomState(random_state))\n            if nested:\n                if n_repeats == 1:\n                    o_kfold = KFold(n_splits=n_splits, shuffle=shuffle)\n                else:\n                    o_kfold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n                scores=cross_val_score(estimator=bayes_trials.best_trial['result']\n                                       ['optimized_estimator'],X=X_,y=y_,\n                                       scoring=scoring,cv=o_kfold,n_jobs=n_jobs)\n                o_score=np.mean(np.sqrt(np.abs(scores)))\n            else:\n                o_score=0\n            results[est] = {\n            'loss': bayes_trials.best_trial['result']['loss'],\n            'loss_std': bayes_trials.best_trial['result']['loss_std'],\n            'params': bayes_trials.best_trial['result']['params'],\n            'scores': bayes_trials.best_trial['result']['scores'],\n            'optimized_estimator': bayes_trials.best_trial['result']['optimized_estimator'],\n            'bayes_trials': deepcopy(bayes_trials),\n            'out_score':o_score\n            }\n            if save_results:\n                save_obj(results, save_results)\n        except Exception as e:\n            print('Estimator: {} unable to optimize: {}'.format(est, e))\n            return -1\n    if print_results:\n        res = pd.DataFrame.from_dict(results, orient='index')\n        print(tabulate(res[['loss','loss_std','out_score']], headers='keys'))\n    return results\n\n\ndef plot_optimized_estimators(optimized_estimators: dict(), X_: pd.DataFrame, y_: pd.DataFrame, \n                              scoring=\"neg_mean_squared_error\",n_splits=5, n_repeats=1, \n                              n_jobs=12, shuffle=False, random_state=None, \n                              print_results=False, plot_type=['detail', 'grid', 'corrplot']):\n    \"\"\"\n    A function for plotting the results of estimator in a form of regression results, \n    residuals plot and residuals histogram\n    THe scoring is obtained from cross_val_score while plotting from cross_val_predict\n\n    Parameters\n    ----------\n    @optimized_estimators: dict\n        A dictionary of estimators with params found and set.\n        Structure of a dictionary: estimator name: estimator object\n    @X_ : pandas.DataFrame\n        A dataframe of features.\n    @y_ : pandas.Series\n        A series of response variable\n    @scoring: metric string\n        One of sklearn.metrics for regression\n    @n_splits: int, default 5\n        Number of cross validation splits. Will be used for both scoring and plotting\n    @n_repeats: in, default 1\n        Number of crossvalidation repeats.\n    @shuffle: bool, False\n        Wheather to shuffle data before cross validation. Only valid when n_repeats==1\n    @random_state : int, defaul None\n        Value for random seed, may be set to obtain repeatable results.\n    @print_results : bool, default False\n        Wheather to print table after plot (not implemented yet.)\n    \"\"\"\n    if n_repeats == 1:\n        kfold = KFold(n_splits=n_splits,\n                      random_state=random_state, shuffle=shuffle)\n    else:\n        kfold = RepeatedKFold(\n            n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n    results = {}\n    for est in optimized_estimators:\n        estimator = optimized_estimators[est]\n        y_pred = cross_val_predict(\n            cv=kfold, estimator=estimator, X=X_, y=y_, n_jobs=n_jobs)\n        score = cross_val_score(estimator=estimator, X=X_,\n                                y=y_, scoring=scoring, cv=kfold, n_jobs=n_jobs)\n        score_mean = np.mean(np.sqrt(np.abs(score)))\n        score_std = np.std(np.sqrt(np.abs(score)))\n        y_pred = inv_boxcox1p(y_pred, 0)\n        y_true = inv_boxcox1p(y_, 0)\n        results[est] = {'score_mean': score_mean,\n                        'score_std': score_std, 'y_pred': y_pred, 'y_true': y_true}\n        if 'detail' in plot_type:\n            fig = plt.figure(figsize=(17, 7))\n            gs = GridSpec(1, 3, width_ratios=(4, 4, 2))\n            ax1 = fig.add_subplot(gs[0, 0])\n            sns.scatterplot(x=y_pred, y=y_true, hue=pd.qcut(\n                y_pred, 6, labels=False), palette='rainbow')\n            sns.lineplot(x=range(0, 900000, 100000), y=range(\n                0, 900000, 100000), color='grey', ax=ax1)\n            ax1.set_ylabel('y_true')\n            ax1.set_xlabel('y_pred')\n            # residuals\n            ax2 = fig.add_subplot(gs[0, 1])\n            sns.scatterplot(x=y_pred, y=y_pred-y_true, hue=pd.qcut(\n                y_pred,6, labels=False), ax=ax2, palette='rainbow')\n            sns.lineplot(x=range(10, 15), y=0, color='grey')\n            # # residuas histogram\n            ax3 = fig.add_subplot(gs[0, 2])\n            sns.distplot(a=y_pred-y_true, ax=ax3, bins=20,\n                         color='blue', kde=True, vertical=True)\n            fig.suptitle(t='{}: mean:{:.5f},std:{:.5f}'.format(\n                est, score_mean, score_std), y=0.95, fontsize=14, fontweight='bold')\n            plt.show()\n    if 'grid' in plot_type:\n        n_cols = 3\n        n_rows = math.ceil(len(results)\/n_cols)\n        print(n_rows)\n        fig, ax = plt.subplots(\n            ncols=n_cols, nrows=n_rows, figsize=(24, n_rows*6))\n        ax = ax.reshape(1, -1)[0]\n        for idx, est in enumerate(results):\n            sns.scatterplot(x=results[est]['y_pred'], y=results[est]['y_true'], \n                            hue=pd.qcut(results[est]['y_pred'], 4, labels=False), \n                            palette='rainbow_r', ax=ax[idx])\n            ax[idx].set_title(est)\n            sns.lineplot(x=range(0, 900000, 100000), y=range(\n                0, 900000, 100000), color='grey', ax=ax[idx])\n        plt.show()\n    if 'corrplot' in plot_type:\n        plt.figure(figsize=(math.ceil(len(results)*1.1), len(results)*1))\n        vals = pd.DataFrame(index=y_train.index)\n        for est in results:\n            vals = vals.join(\n                pd.Series(results[est]['y_pred'], name=est, index=y_train.index))\n        sns.heatmap(vals.corr(), annot=True, fmt=\".4f\", vmax=1,\n                    annot_kws={'fontsize': 12}, cmap='RdYlBu')\n        plt.show()\n    return score_mean\n\n\ndef quick_evaluate_estimator(_optimized_estimator, _nsplits, _nrepeats, _random_states, \n                             _X_train, _y_train, _njobs, print_res=True,copy_res=True, \n                             plot_res=False, return_results=False):\n    \"\"\"\n    The method perform rather simple task of calculating cross validated score \n    for several random states\n    @optimized_estimator: the estimator, must provide fit\/predict methods\n    @_nsplits: number of splits in repeated k-fold (int) \n    @_nrepeats: number of repetitions in repeated k-fold (int)\n    @_random_states: constants for random state generation (list of ints)\n    @_X_train : train dataset\n    @_y_train : response variable\n    @_njobs: number of jobs done in paralell\n    @print_res: wheather to print results\n    @copy_res: wheather to copy results to clipboard in excel-friendly format\n    @plot_res: wheather to plot results (not implemented)\n    @return_results: wheather to return results\n    \"\"\"\n    _evaluation_results = {}\n    for rs in RANDOM_STATES:\n        kfold = RepeatedKFold(\n            n_splits=_nsplits, n_repeats=_nrepeats, random_state=rs)\n        score = cross_val_score(_optimized_estimator, _X_train, _y_train,\n                                scoring='neg_mean_squared_error', cv=kfold, n_jobs=_njobs)\n        scores = np.sqrt(np.abs(score))\n        scores_mean = np.mean(scores)\n        scores_std = np.std(scores)\n        _evaluation_results[rs] = {'mean': scores_mean, 'std': scores_std}\n    res_df = pd.DataFrame.from_dict(_evaluation_results, orient='index').T\n    res_df['result'] = res_df.mean(axis=1)\n    if print_res:\n        print(tabulate(res_df, headers='keys'))\n#   does not work at Kaggle docker\n#     if copy_res:\n#         res_df.to_clipboard(excel=True, decimal=',', float_format='%.6f')\n    if plot_res:\n        pass\n    if return_results:\n        return _evaluation_results\n    else:\n        return None\n","7547964e":"# a dictionary to store optimized models\noptimized_estimators = {}  \n\n# number of splits used in cross_val_score,cross_val_predict etc.\nN_SPLITS = 5  \n\n# number of repeats used in RepeatedKFold\nHOPT_NREPEATS = 1  \n\n# wheather to shuffle samples (g.e. in KFold)\nSHUFFLE = True  \n\n# fixed random state just to make results repeatable. Hasn't been chosen in any specific way. \n# Just a random int.\nFIXED_RS = 143  \n\n# number of jobs used (set to number of physical cores to get the best performance, at least with \n# Threadripper and Win64)\nN_JOBS = 12\n\n# random states used to evaluate and compare models\/datasets\nRANDOM_STATES = [10, 38, 75, 123]\n","41320bec":"INPUT_DIR = \"..\/input\/\"\ntrain_data = pd.read_csv(INPUT_DIR+'train.csv')\ntest_data = pd.read_csv(INPUT_DIR+'test.csv')\ntrain_data.set_index('Id', inplace=True)\ntest_data.set_index('Id', inplace=True)\ntest_data['SalePrice'] = np.nan\ntest_data['Train'] = 0\ntrain_data['Train'] = 1\ndata = pd.concat([train_data, test_data], axis=0, ignore_index=False)\nprint('Train data shape: {}, test_data shape: {}, data shape: {}'.format(\n    train_data.shape, test_data.shape, data.shape))\n","5bbc5af3":"_START_TIME = dt.datetime.now()\nfeats_with_nans = data.loc[data['Train'] == 1, :].columns[data.isnull().any()]\nfig, ax = plt.subplots(figsize=(14, 8))\nsns.heatmap(data[feats_with_nans].isnull().T, cbar=False);","3407ef89":"# Alley - No alley access\n# there is no way to verify if all NaNs have the meaning no alley access\ndata['Alley'].fillna('None', inplace=True)\n\n# for Basement wee need to check if all variables are consistent with respect to NaNs\n# we may assume that if all nominal fields are NaN and total surface is 0 - the row is true NaN,\n# and can be filed\nbsmt_nans = data.loc[\n    (data[['BsmtCond', 'BsmtQual', 'BsmtExposure', \n           'BsmtFinType1','BsmtFinType2']].isnull().all(axis=1)) & (data['TotalBsmtSF'] == 0),\n    ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', \n     'BsmtFinType2','BsmtUnfSF', 'TotalBsmtSF']].shape\nprint('Number of records with any basement attribute is nan: {}'.format(\n    bsmt_nans[0]))\n\ntrue_bsmt_nans = data.loc[\n    (data[['BsmtCond', 'BsmtQual', 'BsmtExposure', \n           'BsmtFinType1','BsmtFinType2']].isnull().all(axis=1)) & (data['TotalBsmtSF'] == 0),\n    ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n     'BsmtUnfSF', 'TotalBsmtSF']]\nprint('Number of records where all basement related attributes are nans (or 0): {}'.format(\n    true_bsmt_nans.shape[0]))\n\ndata.loc[true_bsmt_nans.index.values, ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1',\n                                       'BsmtFinType2']] = data.loc[\n    true_bsmt_nans.index.values, ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2']].fillna('None')\n\nprint('Remaining basement related nans:')\n\ndata.loc[data[['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1',\n               'BsmtFinType2']].isnull().any(axis=1),\n         ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n          'BsmtUnfSF', 'TotalBsmtSF']]\n","2c196966":"# for fireplaces only records where FireplaceQU is nan and number of \n# fireplaces is 0 can be filled\ndata.loc[\n    (pd.isna(data['FireplaceQu'])) & (data['Fireplaces'] == 0), 'FireplaceQu'] = data.loc[(\n    pd.isna(data['FireplaceQu'])) & (data['Fireplaces'] == 0), 'FireplaceQu'].fillna('none')\n\n# Garage related nans should also be considered together\ngarage_feats = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n                'GarageCars', 'GarageArea']\ngarage_true_nans = data.loc[(data['GarageArea'] == 0) & (data[\n    ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']].\n    isnull().any(axis=1)), garage_feats]\nprint('Number of records where all garage related features are Nan or 0: {}'.\n      format(garage_true_nans.shape[0]))\n\ndata.loc[garage_true_nans.index.values,\n         ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']] = data.loc[\n    garage_true_nans.index.values, ['GarageType', 'GarageFinish', 'GarageQual',\n                                    'GarageCond']].fillna('none')\ndata.loc[garage_true_nans.index.values, ['GarageYrBlt']] = data.loc[\n    garage_true_nans.index.values, ['GarageYrBlt']].fillna(0)\n\nprint('remaining garage nans:')\ndata.loc[data[['GarageType', 'GarageYrBlt', 'GarageFinish',\n               'GarageQual', 'GarageCond']].\n         isnull().any(axis=1), garage_feats]\n","55e183ae":"# for PoolQC there are some records where PoolQC is nan, but pool area is provided, some \n# Nan's still remain\ndata.loc[\n    (pd.isna(data['PoolQC'])) & (data['PoolArea'] == 0), \n    ['PoolQC', 'PoolArea']] = data.loc[(\n    pd.isna(data['PoolQC'])) & (data['PoolArea'] == 0), \n    ['PoolQC', 'PoolArea']].fillna('none')\n\ndata.loc[(pd.isna(data['PoolQC'])) & (\n    data['PoolArea'] != 0), ['PoolQC', 'PoolArea']]\n","b5bce310":"# all Fence nans are just no fence\ndata.loc[pd.isna(data['Fence']), 'Fence'] = data.loc[pd.isna(\n    data['Fence']), 'Fence'].fillna('none')\n\n# for MiscFeatures, MiscFeature and MiscFeatureVal shuld be consistent\ndata.loc[(pd.isna(data['MiscFeature'])) & (data['MiscVal'] == 0), 'MiscFeature'] = data.loc[(\n    pd.isna(data['MiscFeature'])) & (data['MiscVal'] == 0), 'MiscFeature'].fillna('none')\nprint('MiscFeature nans remaining:')\ndata.loc[(pd.isna(data['MiscFeature'])) & (\n    data['MiscVal'] == 0), ['MiscFeature', 'MiscVal']]\n","1c43fcd2":"feats_with_nans = data.columns[data.isnull().any()]\nfeats_with_nans = feats_with_nans.drop('SalePrice')\nfig, ax = plt.subplots(figsize=(14, 8))\nsns.heatmap(data[feats_with_nans].isnull().T, cbar=False);\n","852167ee":"# MSZoning will be imputed with mode\ndata.loc[pd.isna(data['MSZoning']), 'MSZoning'] = data['MSZoning'].mode()[0]\n\n# LotFrontage has a lot of NaNs. The most reasonable association with another feature is \n# association with Neghorhood (assuming that in the same region LotFrontage is similar)\nneighb_frontages = data.groupby(by='Neighborhood')['LotFrontage'].median()\nfor n in data['Neighborhood'].unique():\n    nnans = data.loc[(pd.isna(data['LotFrontage'])) &\n                     (data['Neighborhood'] == n), ].shape[0]\n    if nnans > 0:\n        data.loc[(pd.isna(data['LotFrontage'])) & (data['Neighborhood'] == n), \n                 'LotFrontage'] = data.loc[(pd.isna(data['LotFrontage'])) & \n                                           (data['Neighborhood'] == n), \n                                           'LotFrontage'].fillna(neighb_frontages[n])\n\n# Utilities are highly dominated by a single value\ndata.loc[pd.isna(data['Utilities']), 'Utilities'] = data['Utilities'].mode()[0]\n\n# single nans in Exterior features will be imputed with most common value\ndata.loc[pd.isna(data['Exterior1st']),\n         'Exterior1st'] = data['Exterior1st'].mode()[0]\ndata.loc[pd.isna(data['Exterior2nd']),\n         'Exterior2nd'] = data['Exterior1st'].mode()[0]\n\n# for MasVnrType 60% is none and 30% is BrkFace. 23 of 24 Nans has MasVnrArea=0\ndata.loc[(pd.isna(data['MasVnrType'])) & (pd.isna(data['MasVnrArea'])), [\n    'MasVnrType', 'MasVnrArea']] = ('None', 0)\ndata.loc[(pd.isna(data['MasVnrType'])) & (\n    data['MasVnrArea'] != 0), 'MasVnrType'] = 'BrkFace'\n\n# for Bsmt features there is single record with all nans and some records with accidental nans\ndata.loc[pd.isna(data['BsmtCond']) & pd.isna(data['BsmtQual']),\n         ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFinSF1', \n          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']] = ('none', 'none', 'none', 'none', 'none', \n                                                        0, 0, 0, 0)\nfor a in ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType2']:\n    data.loc[pd.isna(data[a]), a] = data[a].mode()[0]\n\n# Electrical is highly dominated by a single value\ndata.loc[pd.isna(data['Electrical']),\n         'Electrical'] = data['Electrical'].mode()[0]\n\n# Nans for 'BsmtFullBath','BsmtHalfBath' can be derived from 'TotalBsmtSF' \n# - for both records it is 0 \ndata.loc[data[['BsmtFullBath', 'BsmtHalfBath']].isnull().any(\n    axis=1), ['BsmtFullBath', 'BsmtHalfBath']] = (0, 0)\n\n# KitchenQual will be imputed with mode\ndata.loc[pd.isna(data['KitchenQual']),\n         'KitchenQual'] = data['KitchenQual'].mode()[0]\n\n# functional is highly dominated by a single value\ndata.loc[pd.isna(data['Functional']),\n         'Functional'] = data['Functional'].mode()[0]\n\n# for garrage related feats there is one record with all nans, and detached garrage,\n# the second record has also detached garage for a single car\n# for the first record the mode by building type will be used\n# I also assume that in this case the garrage was built together with the house\ndata.loc[(pd.isna(data['GarageQual'])) & (data['GarageArea'] > 0), 'GarageYrBlt'] = data.loc[(\n    pd.isna(data['GarageQual'])) & (data['GarageArea'] > 0), 'YearBuilt']\ndata.loc[(pd.isna(data['GarageQual'])) & (data['GarageArea'] > 0), \n         ['GarageFinish', 'GarageQual', 'GarageCond']] = (\n    data.loc[data['BldgType'] == '2fmCon', 'GarageFinish'].mode()[0], \n    data.loc[data['BldgType'] == '2fmCon', 'GarageQual'].mode()[0],\n    data.loc[data['BldgType'] == '2fmCon', 'GarageCond'].mode()[0])\n\ndata.loc[pd.isna(data['GarageQual']), garage_feats]\n","84c9f327":"# for the second case I would assume that it is a mistake, and there is no garage\n\ndata.loc[pd.isna(data['GarageQual']), garage_feats] = [\n    'none', 0, 'none', 'none', 'none', 0, 0]\n\n# PoolQC nans are very hard to guess, because only limited number of properties have a pool\n# the most reasonable predictor is OverallQual\ndata.loc[pd.isna(data['PoolQC']), ['PoolQC', 'PoolArea', 'OverallQual']]\n","0c89f8da":"data.loc[(pd.isna(data['PoolQC'])) & (data['OverallQual'] <= 5),\n         'PoolQC'] = 'Fa'  # overallQual 4\/10\ndata.loc[(pd.isna(data['PoolQC'])) & (data['OverallQual'] > 5),\n         'PoolQC'] = 'Gd'  # overallQual 6\/10\n\n\n# MiscFeature is also a bit complex, because for the record ehere MiscFeature is nan, \n# Misc Val is 17000\n# The highest value of MiscFeature in the dataset is 15500 (Second Garage)\n# The only option seems to be qualify the nan as other\ndata.loc[pd.isna(data['MiscFeature']), 'MiscFeature'] = 'Othr'\n\n# And the last element - SaleType. To fill this record in the best possible way S\n# the house was sold in 2007, ubilt in 1958 and the sale condition were Normal\n# the vast majority (96,3%) of Sale condition=Normal were SaleType=ED\ndata.loc[pd.isna(data['SaleType']), 'SaleType'] = 'WD'\n\nfeats_with_nans = data.columns[data.isnull().any()]\nprint(feats_with_nans)\n","5a489aa5":"print('Data imputing performed in ', dt.datetime.now()-_START_TIME)\n","a50877e5":"_START_TIME = dt.datetime.now()\nprint('Train data shape before: {}, test data shape before {}'.format(\n    data[data['Train'] == 1].shape, data[data['Train'] == 0].shape))\n\n# at first let's drop all outliers with very high great living area and the price \n# below a certain limit\noutliers_to_drop = list(data.loc[(data['GrLivArea'] > 4000) & (\n    data['Train'] == 1) & (data['SalePrice'] < 700000)].index.values)\n\nfig, axes = plt.subplots(ncols=1, nrows=1, figsize=(8, 8))\noutliers = pd.Series(\n    [1 if x in outliers_to_drop else 0 for x in data.index.values], index=data.index)\nsns.scatterplot(x=data.loc[(data['Train'] == 1) & (outliers == 0), 'SalePrice'],\n                y=data.loc[data['Train'] == 1, 'GrLivArea'], color='grey', alpha=0.5)\nsns.scatterplot(x=data.loc[(data['Train'] == 1) & (outliers == 1), 'SalePrice'],\n                y=data.loc[data['Train'] == 1, 'GrLivArea'], color='red')\nplt.show()\nprint(tabulate(data.loc[(data['Train'] == 1) & (outliers == 1), [\n      'GrLivArea', 'SalePrice']], headers='keys'))\n","dfeb2517":"outliers_to_drop.extend([31, 89, 463, 633, 1325])\noutliers = pd.Series(\n    [1 if x in outliers_to_drop else 0 for x in data.index.values], index=data.index)\nfig, axes = plt.subplots(ncols=1, nrows=1, figsize=(8, 8))\noutliers = pd.Series(\n    [1 if x in outliers_to_drop else 0 for x in data.index.values], index=data.index)\nsns.scatterplot(x=data.loc[(data['Train'] == 1) & (outliers == 0), 'SalePrice'],\n                y=data.loc[data['Train'] == 1, 'GrLivArea'], color='grey', alpha=0.5)\nsns.scatterplot(x=data.loc[(data['Train'] == 1) & (outliers == 1), 'SalePrice'],\n                y=data.loc[data['Train'] == 1, 'GrLivArea'], color='red')\nplt.show()\nprint(tabulate(data.loc[(data['Train'] == 1) & (data['GrLivArea'] < 4000) & (\n    outliers == 1), ['GrLivArea', 'MSSubClass', 'SaleType', 'SalePrice']], headers='keys'))\n","a97eec87":"data = data.drop(outliers[outliers == 1].index.values, axis=0)\nprint('Train data shape before: {}, test data shape before {}'.format(\n    data[data['Train'] == 1].shape, data[data['Train'] == 0].shape))\n","9a233215":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\nax1.set_title('Before log transformation, skewness={:.4}'.format(\n    skew(data.loc[data['Train'] == 1, 'SalePrice'])))\nsns.distplot(data.loc[data['Train'] == 1, 'SalePrice'], ax=ax1)\ndata.loc[data['Train'] == 1, 'SalePrice'] = data.loc[\n    data['Train'] == 1, 'SalePrice'].apply(lambda x: boxcox1p(x, 0))\n\n\nax2.set_title('After log transformation, skew={:.2}, mean: {:.2f}, std: {:.2f}'.format(\n    skew(data.loc[data['Train'] == 1, 'SalePrice']), \n    data.loc[data['Train'] == 1, 'SalePrice'].mean(),\n    data.loc[data['Train'] == 1, 'SalePrice'].std()))\nsns.distplot(data.loc[data['Train'] == 1, 'SalePrice'], ax=ax2);\n","6422afb8":"# lists to store features names\nnominal_feats = []\ncont_feats = []\n\n\n# supporting functions to visualize feature or show the most import information\ndef plot_nominal(feature: str, title: str, ax, _data=data):\n    \"\"\"\n    plots boxplot (vs. SalePrice) and barchart for train and test data separately\n    @feature - the name of the feature to plot\n    @title - title of the diagram\n    @ax - array of three axis\n    @_data - dataset to plot ('Train' column must exsist)\n    \"\"\"\n\n    def plot_nominal_subset(feature: str, title: str, ax0, ax1=None, _data=data):\n        sns.countplot(_data[feature], ax=ax0)\n        ax0.set_title(title)\n        for p in ax0.patches:\n            ax0.text(p.get_x()+p.get_width()\/2, p.get_y() + p.get_height(), \n                     '%d' % int(p.get_height()),fontsize=12, color='black', \n                     ha='center', va='bottom')\n        if ax1:\n            sns.boxplot(data=_data, x=feature, y='SalePrice', ax=ax1)\n            ax1.set_title(title)\n\n    plot_nominal_subset(feature=feature, title=title+', train', ax0=ax[0], ax1=ax[1], \n                        _data=_data.loc[_data['Train'] == 1].sort_values(by=feature))\n    plot_nominal_subset(feature=feature, title=title+', test', ax0=ax[2], \n                        _data=_data.loc[_data['Train'] == 0].sort_values(by=feature))\n\n\ndef plot_cont(feature, title, ax, _data=data):\n    \"\"\"\n    Plots the distribution of continuous variable\n    @feature - name of the feature to plot\n    @ax - array of two axis\n    @_data - data to plot, 'Train' identificator must exist\n    \"\"\"\n    def plot_cont_subset(feature, title, ax, _data=data):\n        sns.distplot(_data[feature], ax=ax)\n        ax.set_title('{},mean: {:.2f}, skew:{:.2f}'.format(\n            title, _data[feature].mean(), _data[feature].skew()))\n        bax = ax.twinx()\n        sns.boxplot(x=_data[feature], ax=bax, orient='h',\n                    width=0.05, color='yellow')\n    plot_cont_subset(feature=feature, title=title+' ,train',\n                     ax=ax[0], _data=_data.loc[_data['Train'] == 1])\n    plot_cont_subset(feature=feature, title=title+' ,test',\n                     ax=ax[1], _data=_data.loc[_data['Train'] == 0])\n","eba27b01":"# an additionall variable will be created\ndata['TotalSF'] = data['GrLivArea']+data['TotalBsmtSF']\n\nnominal_building_related = ['MSSubClass', 'BldgType', 'HouseStyle']\ncont_building_related = ['OverallQual', '1stFlrSF', '2ndFlrSF',\n                         'GrLivArea', 'LowQualFinSF', 'TotRmsAbvGrd', 'TotalSF']\n\nfor feat in nominal_building_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n    plot_nominal(feat, feat, ax.reshape(1, -1)[0])\n\nfor feat in cont_building_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_building_related)\ncont_feats.extend(['OverallQual', '1stFlrSF', 'GrLivArea',\n                   'TotRmsAbvGrd', 'LowQualFinSF','TotalSF'])\n","0ded0ba6":"nominal_location_related = ['MSZoning', 'Neighborhood',\n                            'Condition1', 'Condition2', 'Street', 'Alley']\ncont_location_related = ['OverallCond']\n\nfor feat in nominal_location_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_location_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_location_related)\ncont_feats.extend(cont_location_related)\n","b7fdb228":"# an additional feature to indicate that there is a basement\ndata['HasBsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nnominal_basement_related = ['Foundation', 'BsmtQual', 'BsmtCond',\n                            'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HasBsmt']\ncont_basement_related = ['BsmtFinSF1',\n                         'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\n\nfor feat in nominal_basement_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_basement_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_basement_related)\ncont_feats.extend(['BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF'])\n","c855f928":"nominal_shape_related = ['LotShape', 'LotConfig', 'LandSlope', 'LandContour']\ncont_shape_related = ['LotFrontage', 'LotArea']\n\nfor feat in nominal_shape_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_shape_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_shape_related)\ncont_feats.extend(cont_shape_related)\n","22263cb3":"data['TotalBaths'] = data['FullBath'] + data['HalfBath'] * \\\n    0.5 + data[\"BsmtFullBath\"]+data[\"BsmtHalfBath\"]*0.5\ncont_bath_related = ['BsmtFullBath', 'BsmtHalfBath',\n                     'FullBath', 'HalfBath', 'TotalBaths']\n\nfor feat in cont_bath_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\ncont_feats.extend(cont_bath_related)\n","cd36dace":"nominal_exterior_related = ['ExterCond',\n                            'ExterQual', 'Exterior1st', 'Exterior2nd']\n\nfor feat in ['ExterCond', 'ExterQual']:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in ['Exterior1st', 'Exterior2nd']:\n    fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 15))\n    plot_nominal(feat, feat, ax)\n\n\nnominal_feats.extend(nominal_exterior_related)\n","b43b477b":"# additional feature to represent that a garage is present\ndata['HasGarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nnominal_garage_related = ['GarageType', 'GarageQual',\n                          'GarageFinish', 'GarageCond', 'HasGarage']\ncont_garage_related = ['GarageYrBlt', 'GarageCars', 'GarageArea']\n\nfor feat in nominal_garage_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_garage_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\n\nnominal_feats.extend(nominal_garage_related)\ncont_feats.extend(cont_garage_related)\n","8b326cfa":"nominal_sale_related = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\nfor feat in nominal_sale_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nnominal_feats.extend(nominal_sale_related)\n","c640f4bd":"cont_pd_related = ['WoodDeckSF', 'OpenPorchSF',\n                   'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\ndata['TotalPorchDeckSF'] = data[cont_pd_related].sum(axis=1).astype(float)\ncont_pd_related.append('TotalPorchDeckSF')\n\nfor feat in cont_pd_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\ncont_feats.extend(['EnclosedPorch', 'ScreenPorch','TotalPorchDeckSF'])\n","838420f3":"cont_age_related = ['YearBuilt', 'YearRemodAdd']\n\nfor feat in cont_age_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\ncont_feats.extend(cont_age_related)\n","cd69aa6c":"nominal_kitchen_related = ['KitchenQual']\ncont_kitchen_related = ['KitchenAbvGr']\n\nfor feat in nominal_kitchen_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_kitchen_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_kitchen_related)\ncont_feats.extend(cont_kitchen_related)\n","1fc40449":"nominal_roof_related = ['RoofStyle', 'RoofMatl']\n\nfor feat in nominal_roof_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nnominal_feats.extend(nominal_roof_related)\n","1d636303":"nominal_masnery_related = ['MasVnrType']\ncont_masnery_related = ['MasVnrArea'] \n# finally I decided to drop this feature due to different outliers distribution\n# for test and train sets\n\nfor feat in nominal_masnery_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_masnery_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\nnominal_feats.extend(nominal_masnery_related)\n\n","909afb27":"data['HasFireplace'] = data['Fireplaces'].apply(\n    lambda x: 1 if x > 0 else 0)  # to be removed\n\nnominal_util_related = ['Utilities', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n                        'Functional', 'PoolQC', 'MiscFeature', 'FireplaceQu', 'Fence', \n                        'PavedDrive', 'HasPool', 'HasFireplace']\n\n# additional feature to represent spike\ndata['HasPool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ncont_util_related = ['PoolArea', 'MiscVal', 'BedroomAbvGr', 'Fireplaces']\n\nfor feat in nominal_util_related:\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    plot_nominal(feat, feat, ax)\n\nfor feat in cont_util_related:\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n    plot_cont(feat, feat, ax)\n\n\nnominal_feats.extend(nominal_util_related)\ncont_feats.extend(cont_util_related)\n","0980c510":"data[cont_feats] = data[cont_feats].astype(float)\nprint('Nominal features: {}, Continuous features {}'.format(\n    len(nominal_feats), len(cont_feats)))\n","b2e5c098":"from sklearn.preprocessing import PowerTransformer\nSKEWNESS_THRESHOLD = 0.5\n\nskew_data = pd.DataFrame.from_dict(\n    {x: {\n        '|train_skewness|': np.abs(skew(data.loc[data['Train'] == 1, x])),\n        'train_mean': data.loc[data['Train'] == 1, x].mean(),\n        'train_median': data.loc[data['Train'] == 1, x].median(),\n        'train_std': data.loc[data['Train'] == 1, x].std(),\n        '|test_skewness|': np.abs(skew(data.loc[data['Train'] == 0, x])),\n        'test_mean': data.loc[data['Train'] == 0, x].mean(),\n        'test_median': data.loc[data['Train'] == 0, x].median(),\n        'test_std': data.loc[data['Train'] == 0, x].std(),\n        } for x in cont_feats}, orient='index').sort_values(by='|train_skewness|', \n                                                            ascending=False)\n\nprint('\\nSkewness before transformation:')\nprint(tabulate(skew_data[skew_data['|train_skewness|']\n                         > SKEWNESS_THRESHOLD], headers='keys'))\n\n# deskewing\nto_deskew = skew_data.loc[(skew_data['|train_skewness|'] > SKEWNESS_THRESHOLD) ].index.values\n\npt = PowerTransformer()\nfor feat in to_deskew:\n    pt.fit(data.loc[data['Train'] == 1, feat].values.reshape(-1, 1))\n    data.loc[data['Train'] == 1, feat] = pt.transform(\n        data.loc[data['Train'] == 1, feat].values.reshape(-1, 1)).reshape(1, -1)[0]\n    data.loc[data['Train'] == 0, feat] = pt.transform(\n        data.loc[data['Train'] == 0, feat].values.reshape(-1, 1)).reshape(1, -1)[0]\n\nskew_data_after = pd.DataFrame.from_dict(\n    {x: {\n        '|train_skewness|': np.abs(skew(data.loc[data['Train'] == 1, x])),\n          'train_mean': data.loc[data['Train'] == 1, x].mean(),\n          'train_median': data.loc[data['Train'] == 1, x].median(),\n          'train_std': data.loc[data['Train'] == 1, x].std(),\n          '|test_skewness|': np.abs(skew(data.loc[data['Train'] == 0, x])),\n          'test_mean': data.loc[data['Train'] == 0, x].mean(),\n          'test_median': data.loc[data['Train'] == 0, x].median(),\n          'test_std': data.loc[data['Train'] == 0, x].std(),\n          } for x in cont_feats}, orient='index').sort_values(by='|train_skewness|', \n                                                              ascending=False)\nprint('\\nSkewness after transformation')\nprint(tabulate(\n    skew_data_after, headers='keys'))\n","2fc6ee05":"X_train = data.loc[data['Train'] == 1, nominal_feats+cont_feats]\nX_test = data.loc[data['Train'] == 0, nominal_feats+cont_feats]\ny_train = data.loc[data['Train'] == 1, 'SalePrice']\ndata_ = data[nominal_feats+cont_feats]\n","b6842c39":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nohe = OneHotEncoder(categories='auto', sparse=False,handle_unknown='ignore', dtype='uint8')\nohe.fit(X_train[nominal_feats])\nX_train_ = pd.DataFrame(ohe.transform(\n    X_train[nominal_feats]), index=X_train.index, columns=ohe.get_feature_names(nominal_feats))\nX_test_ = pd.DataFrame(ohe.transform(\n    X_test[nominal_feats]), index=X_test.index, columns=ohe.get_feature_names(nominal_feats))\nX_train = X_train_.join(X_train[cont_feats])\nX_test = X_test_.join(X_test[cont_feats])\nprint('X_train shape: {}, X_test shape: {}'.format(X_train.shape, X_test.shape))\n","55e71406":"HIGH_CORR_DROP_THRESHOLD = 1 \nX_train_reduced, _ = drop_HCF(X_train, y_train, HIGH_CORR_DROP_THRESHOLD)\nX_train = X_train_reduced.copy()\nX_test = X_test[X_train.columns]\nprint('Data transformations processed in ', dt.datetime.now()-_START_TIME)\n","a3dab87c":"to_robust_scaling=['OverallQual', '1stFlrSF', 'GrLivArea', 'LowQualFinSF', \n                   'TotRmsAbvGrd', 'TotalSF', 'OverallCond', 'BsmtFinSF2', \n                   'BsmtUnfSF', 'TotalBsmtSF', 'LotFrontage', 'LotArea', 'BsmtFullBath']\nto_standard_scaling=['BsmtHalfBath', 'FullBath', 'HalfBath', 'TotalBaths', \n                     'GarageYrBlt', 'GarageCars', 'GarageArea']\nto_maxabs_scaling=['EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'TotalPorchDeckSF', \n                   'YearBuilt', 'YearRemodAdd', 'KitchenAbvGr', 'MiscVal', \n                   'BedroomAbvGr', 'Fireplaces']","1d1149f1":"print('Processed in ', dt.datetime.now()-GLOBAL_START_TIME)\n","6124550e":"opt_res={}\nmodels={\n    'Lasso_pipe': [Pipeline(steps=[('ct',ColumnTransformer(transformers=[\n                                ('robust_scaler',RobustScaler(),\n                                 [x for x in to_robust_scaling if x in X_train.columns]),\n                                ('standard_scaler',StandardScaler(),\n                                 [x for x in to_standard_scaling if x in X_train.columns]),\n                                ('maxabs_scaler',MaxAbsScaler(),\n                                 [x for x in to_maxabs_scaling if x in X_train.columns])],\n                                                    remainder='passthrough')),\n                             ('estimator',Lasso(max_iter=50000, tol=0.001, normalize=False))]),\n                            {'estimator__alpha': hp.uniform('alpha', 1e-5, 1)},300],\n    'Ridge_pipe':[Pipeline(steps=[\n                            ('ct',ColumnTransformer(transformers=[\n                                ('robust_scaler',RobustScaler(),\n                                 [x for x in to_robust_scaling if x in X_train.columns]),\n                                ('standard_scaler',StandardScaler(),\n                                 [x for x in to_standard_scaling if x in X_train.columns]),\n                                ('maxabs_scaler',MaxAbsScaler(),\n                                 [x for x in to_maxabs_scaling if x in X_train.columns])],\n                                                    remainder='passthrough')),\n                             ('estimator',Ridge(max_iter=100000,normalize=False))]),\n                            {'estimator__alpha': hp.uniform('alpha', 10, 20)},300]\n        }\n\n\nfor model in models:\n    opt_res[model]=evaluate_feature_set(estimators={model:models[model]},X_=X_train,y_=y_train,\n                                 n_jobs=N_JOBS,n_splits=N_SPLITS,n_repeats=HOPT_NREPEATS,\n                                 random_state=FIXED_RS,shuffle=True,\n                                 scoring='neg_mean_squared_error')\n    optimized_estimators[model]=opt_res[model][model]['optimized_estimator']\n    plot_optimized_estimators(optimized_estimators=\n                              {model:opt_res[model][model]['optimized_estimator']},\n                              X_=X_train,y_=y_train,n_splits=N_SPLITS,n_jobs=N_JOBS,\n                              scoring='neg_mean_squared_error',plot_type=['detail'])\n\nmodels_results={}\nfor model in optimized_estimators:\n    models_results[model]=quick_evaluate_estimator(\n        _optimized_estimator=optimized_estimators[model],_nsplits=N_SPLITS,_nrepeats=1,\n        _random_states=RANDOM_STATES,_X_train=X_train,_y_train=y_train,_njobs=N_JOBS,\n        return_results=True,print_res=False)\n    \nresult_df=pd.DataFrame.from_dict(\n    {model:{rs:models_results[model][rs]['mean'] for \n            rs in RANDOM_STATES} for model in models_results  }).T\n\nresult_df['mean']=result_df.mean(axis=1)\nresult_df","02b01cd3":"# boruta_estimator = {\n#     'Boruta_GBR': [GradientBoostingRegressor(loss='huber', warm_start=False, learning_rate=0.05, max_depth=8, n_estimators=1000),\n#                    {\n#         'max_leaf_nodes': pyll.scope.int(hp.quniform('max_leaf_nodes', 50, 550.5, 1)),\n#         'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0.0, 0.9),\n#         'min_samples_leaf': pyll.scope.int(hp.quniform('min_samples_leaf', 5, 20.5, 1)),\n#         'min_samples_split': pyll.scope.int(hp.quniform('min_samples_split', 10.5, 650.5, 1)),\n#         'subsample': hp.uniform('subsample', 0.1, 0.99)\n#     }, 50]}\n\n# full_opt_res = evaluate_feature_set(estimators={e: boruta_estimator[e] for e in ['Boruta_GBR']},\n#                                     X_=X_train, y_=y_train,\n#                                     n_jobs=N_JOBS, n_splits=5, n_repeats=1, random_state=123)\n\n# boruta_estimator = full_opt_res['Boruta_GBR']['optimized_estimator']\n# print(boruta_estimator)\n# --------------------------------\n\n# def perform_boruta_selection(X: pd.DataFrame, y: pd.Series, selector, boruta_perc=100,\n#                              boruta_alpha=0.05, boruta_max_iter=200, boruta_n_estimators='auto', level=1,random_state=123):\n#     start = dt.datetime.now()\n#     feature_selection_results = {}\n#     feat_selector = BorutaPy(max_iter=boruta_max_iter, perc=boruta_perc, estimator=selector,\n#                              alpha=boruta_alpha, n_estimators=boruta_n_estimators, verbose=0,)\n#     feat_selector.fit(X.values, y.values)\n#     if level == 1:\n#         # if one wants to consider also 2 - introduce change here\n#         selected_cols = X.columns[feat_selector.support_]\n#     else:\n#         selected_cols = X.columns[pd.Series(feat_selector.ranking_).apply(\n#             lambda x: False if x > 2 else True)]\n#     feature_selection_results[boruta_perc] = {\n#         'selected_cols_num': len(selected_cols),\n#         'selected_cols': list(selected_cols),\n#         'boruta_ranking': feat_selector.ranking_,\n#         'duration': dt.datetime.now()-start,\n#     }\n#     return feature_selection_results\n# -------------------------------\n\n# UPPER_SEARCH_LIMIT = 90\n# LOWER_SEARCH_LIMIT = 80\n# BORUTA_FEATURES_LEVEL = 2\n# boruta_results = Parallel(n_jobs=N_JOBS, backend='loky',\n#                           prefer='processes', verbose=10)(delayed(perform_boruta_selection)\n#                                                           (X=X_train, y=y_train, selector=boruta_estimator,\n#                                                            boruta_max_iter=100, boruta_perc=UPPER_SEARCH_LIMIT-i,\n#                                                            level=BORUTA_FEATURES_LEVEL)\n#                                                           for i in range(UPPER_SEARCH_LIMIT-LOWER_SEARCH_LIMIT))\n# boruta_res_ = {k: elem[k] for elem in boruta_results for k in elem}\n# ------------------------------\n\n# # all scaled features must be scaled here and models used for evaluation must be simple models, not pipelines  - to be able to restore features names easily\n\n# scaling_pipe = Pipeline(steps=[('ct', ColumnTransformer(transformers=[\n#     ('robust', RobustScaler(), [x for x in to_robust_scaling if x in X_train.columns]),\n#     ('standard', StandardScaler(), [x for x in to_standard_scaling if x in X_train.columns]),\n#     ('maxabs', MaxAbsScaler(), [x for x in to_maxabs_scaling if x in X_train.columns])],\n#     remainder='passthrough'))])\n\n# X_train_scaled = scaling_pipe.fit_transform(X_train)\n\n# # robust scaler does not provide feature_names, so column names must be restored manually\n# restored_column_names = []\n# restored_column_names.extend([x for x in to_robust_scaling if x in X_train.columns])\n# restored_column_names.extend([x for x in to_standard_scaling if x in X_train.columns])\n# restored_column_names.extend([x for x in to_maxabs_scaling if x in X_train.columns])\n# remaining_data = X_train.drop(restored_column_names, axis=1)\n# restored_column_names.extend(remaining_data.columns)\n# X_train_scaled = pd.DataFrame(\n#     X_train_scaled, index=X_train.index, columns=restored_column_names)\n# --------------------------------\n\n# iternum = 300\n# estimators = {\n#     'ENet_boruta_eval':[ElasticNet(max_iter=100000,normalize=False,warm_start=False),\n#                                  {\n#                                    'alpha': hp.uniform('alpha', 0.0001, 0.1),\n#                                    'l1_ratio': hp.uniform('l1_ratio', 0.05, 0.5),\n#                                    'selection': hp.choice('selection', ['cyclic', 'random']),\n#                                 }, iternum]\n# }\n\n\n# start_ = dt.datetime.now()\n# selected_estimators = ['ENet_boruta_eval'] \n# eval_res = {}\n# boruta_eval_scores = {}\n# old_cols = []\n# for k in boruta_res_:\n#     print(k)\n#     cols = boruta_res_[k]['selected_cols']\n#     if set(cols) == set(old_cols):\n#         print(k, len(cols), 'columns not changed')\n#     else:\n#         old_cols = cols\n#         evaluators = evaluate_feature_set(estimators={e: estimators[e] for e in selected_estimators}, X_=X_train_scaled[cols], y_=y_train,\n#                                                  n_jobs=N_JOBS, n_splits=5, n_repeats=1, random_state=None,)        \n        \n#         for rs in RANDOM_STATES:\n#             kfold=RepeatedKFold(n_splits=5,n_repeats=10,random_state=rs)\n#             for est in selected_estimators:\n#                 evaluator=evaluators[est]['optimized_estimator']\n#                 scores=cross_val_score(evaluator,X=X_train_scaled[cols],y=y_train,cv=kfold,n_jobs=N_JOBS,scoring='neg_mean_squared_error')\n#                 scores=np.mean(np.sqrt(np.abs(scores)))\n#                 boruta_eval_scores[(est, k, len(cols), rs)] = scores\n# print('Finished in {}'.format(dt.datetime.now()-start_))\n# --------------------------\n\n# boruta_eval_ = pd.DataFrame.from_dict(boruta_eval_scores,orient='index').reset_index()\n\n# boruta_eval_['model'] = boruta_eval_['index'].apply(lambda x: x[0])\n# boruta_eval_['nperc'] = boruta_eval_['index'].apply(lambda x: x[1])\n# boruta_eval_['ncols'] = boruta_eval_['index'].apply(lambda x: x[2])\n# boruta_eval_['rs'] = boruta_eval_['index'].apply(lambda x: x[3])\n\n# boruta_eval_ = boruta_eval_.drop('index', axis=1)\n# # print(tabulate(boruta_eval_.head(),headers='keys'))\n# boruta_eval_comp=pd.pivot_table(data=boruta_eval_,values=[0],index='nperc',columns='rs')\n# boruta_eval_comp['mean_score']=boruta_eval_comp.mean(axis=1)\n# print(tabulate(boruta_eval_comp.sort_values(by='mean_score')[:5],headers='keys'))\n\n# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 10))\n# sns.lineplot(x='nperc',y=0,data=boruta_eval_,ax=ax,)\n# ax.grid()\n# ax_=ax.twinx()\n# ax_.grid(color='green')\n\n# sns.lineplot(x='nperc', y='ncols', data=boruta_eval_,ax=ax_, color='green');\n\n\n# boruta_best_nperc = # select the value from diagram. I used second minimum from the right, 85\n# selected_columns=boruta_res_[boruta_best_nperc]['selected_cols']\n\n# X_train_bs = X_train[selected_columns]\n# X_test_bs = X_test[selected_columns]\n# selcols=pd.DataFrame.from_dict({col:{'coltype':X_train[col].dtype,\n#                              'train_mean':X_train[col].mean(),'train_skewness':X_train[col].skew(),\n#                              'test_mean':X_test[col].dtype,'test_skewness':X_test[col].skew()\n#                             } for col in X_train_bs.columns}).T.sort_index()\n# selcols['nunique']=[X_train_bs[cols].unique().shape[0] for cols in selcols.index]\n# lvn=selcols.reset_index().apply(lambda r: levene(X_train_bs[r['index']],X_test_bs[r['index']],center='mean').pvalue \n#                             if (r['train_skewness']<1 and r['test_skewness']<1) else \n#                             levene(X_train_bs[r['index']],X_test_bs[r['index']],center='median').pvalue,axis=1)\n# lvn.index=selcols.index\n# selcols['lev']=lvn\n# print('Features before dropping features with different variance')\n# print(tabulate(selcols,headers='keys'))\n# print('Number of features: ',selcols.shape[0])\n# print('Performed in ',dt.datetime.now()-GLOBAL_START_TIME)\n","614622f1":"sc=['1stFlrSF','Alley_Pave','BedroomAbvGr','BldgType_1Fam','BldgType_Duplex','BsmtCond_Fa',\n 'BsmtExposure_Gd','BsmtExposure_No','BsmtFinSF2','BsmtFinType1_GLQ','BsmtFinType1_Unf',\n 'BsmtQual_Ex','BsmtUnfSF','CentralAir_Y','Condition1_Artery','Condition1_Norm',\n 'ExterQual_Ex','Exterior1st_BrkFace','FireplaceQu_Gd','Fireplaces','Foundation_BrkTil',\n 'FullBath','Functional_Maj1','Functional_Typ','GarageArea','GarageCars','GarageFinish_Unf',\n 'GarageQual_TA','GarageType_Attchd','GarageType_BuiltIn','GarageYrBlt','GrLivArea',\n 'HasFireplace_0','HeatingQC_Ex','HeatingQC_Fa','HouseStyle_1Story','HouseStyle_2Story',\n 'KitchenAbvGr','KitchenQual_Ex','LotArea','LotConfig_CulDSac','LotConfig_FR2','LotFrontage',\n 'LowQualFinSF','MSSubClass_30','MSSubClass_60','MSSubClass_70','MSZoning_FV','MSZoning_RL',\n 'MSZoning_RM','Neighborhood_BrkSide','Neighborhood_ClearCr','Neighborhood_Crawfor',\n 'Neighborhood_Edwards','Neighborhood_MeadowV','Neighborhood_Mitchel','Neighborhood_NoRidge',\n 'Neighborhood_OldTown','Neighborhood_Somerst','Neighborhood_StoneBr','OverallCond',\n 'OverallQual','PavedDrive_Y','SaleCondition_Abnorml','SaleCondition_Normal',\n 'SaleCondition_Partial','SaleType_New','ScreenPorch','TotalBaths','TotalBsmtSF',\n 'TotalPorchDeckSF','TotalSF','YearBuilt','YearRemodAdd','YrSold_2009']\nX_train_bs = X_train[sc]\nX_test_bs = X_test[sc]","aad777e4":"# att first let's correct our scalers\nto_robust_scaling_bs = [\n    x for x in to_robust_scaling if x in X_train_bs.columns]\nto_standard_scaling_bs = [\n    x for x in to_standard_scaling if x in X_train_bs.columns]\nto_maxabs_scaling_bs = [\n    x for x in to_maxabs_scaling if x in X_train_bs.columns]\nopt_res={}","d263e377":"# Let's create pipelines definitions\n# All models will be optimized using narrowed feature set, with _bs suffix\noptimized_estimators_bs = {}\niternum = 200\nmodels = {\n    'SVRpoly3_pipe_bs': [Pipeline(steps=[('ct', ColumnTransformer(transformers=[\n        ('robust_scaler', RobustScaler(), to_robust_scaling_bs),\n        ('standard_scaler', StandardScaler(), to_standard_scaling_bs),\n        ('maxabs_scaler', MaxAbsScaler(), to_maxabs_scaling_bs)], remainder='passthrough')),\n            ('estimator', SVR(kernel='poly', gamma='auto', degree=3))]),{\n                    'estimator__coef0': hp.uniform('coef0', 0.001, 10.0),\n                    'estimator__C': hp.uniform('C', 0.01, 1.0),\n                    'estimator__epsilon': hp.uniform('epsilon', 0.001, 0.5)\n                }, 2*iternum],\n    \n    'KR_poly_pipe_bs': [Pipeline(steps=[('ct', ColumnTransformer(transformers=[\n        ('robust_scaler', RobustScaler(), to_robust_scaling_bs),\n        ('standard_scaler', StandardScaler(), to_standard_scaling_bs),\n        ('maxabs_scaler', MaxAbsScaler(), to_maxabs_scaling_bs)], remainder='passthrough')),\n        ('estimator', KernelRidge(kernel='poly'))]),\n                {\n                    'estimator__alpha': hp.uniform('alpha', 0.05, 0.9),\n                    'estimator__coef0': hp.uniform('coef0', 1, 8),\n                    'estimator__gamma': hp.uniform('gamma', 1e-3, 0.1),\n                }, 2*iternum],\n    #in fact a pipeline is not neccessary here\n    'LightGBM_bs': [\n        Pipeline(steps=[('estimator', LGBMModel(\n            objective='regression', boosting_type='gbdt',\n            n_estimators=3000, importance_type='split', learning_rate=0.01))]),\n            {\n                'estimator__bagging_fraction': hp.uniform('bagging_fraction', 0.2, 0.5),\n                'estimator__colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n#                 'estimator__bagging_freq': pyll.scope.int(\n#                     hp.quniform('bagging_freq', 2.5, 7, 1)),\n                'estimator__feature_fraction': hp.uniform('feature_fraction', 0.1, 0.3),\n                'estimator__max_bin': pyll.scope.int(hp.quniform('max_bin', 100, 350, 1)),\n                'estimator__min_child_samples': pyll.scope.int(\n                    hp.quniform('min_child_samples', 1.5, 10.5, 1)),\n                'estimator__min_child_weight': hp.uniform('min_child_weight', 0.001, 0.5),\n                'estimator__min_split_gain': hp.uniform('min_split_gain', 0.0001, 0.3),\n                'estimator__num_leaves': pyll.scope.int(hp.quniform('num_leaves', 20, 30, 1)),\n                'estimator__reg_alpha': hp.uniform('reg_alpha', 0.01, 0.09),\n                'estimator__max_depth': pyll.scope.int(hp.quniform('max_depth', 7, 12.5, 1)),\n                'estimator__reg_lambda': hp.uniform('reg_lambda', 0.01, 0.1),\n            }, 2*iternum],\n    }\n\n\nfor model in models:\n    opt_res[model] = evaluate_feature_set(estimators={model: models[model]}, \n                                          X_=X_train_bs, y_=y_train,\n                                          n_jobs=N_JOBS, n_splits=N_SPLITS, \n                                          n_repeats=HOPT_NREPEATS,\n                                          random_state=FIXED_RS, shuffle=True,\n                                          scoring='neg_mean_squared_error',\n                                          nested=True,print_results=True)\n    optimized_estimators_bs[model] = opt_res[model][model]['optimized_estimator']\n    plot_optimized_estimators(\n        optimized_estimators={model: opt_res[model][model]['optimized_estimator']},\n        X_=X_train_bs, y_=y_train, n_splits=N_SPLITS, n_jobs=N_JOBS,\n        scoring='neg_mean_squared_error', plot_type=['detail'])\n#     print(str(optimized_estimators_bs[model]))\n# save_obj(optimized_estimators_bs, NBNAME+\"optimized_estimators_bs.pkl\")\n","539a1883":"print('Processed in ', dt.datetime.now()-_START_TIME)\n","3f6edaa3":"import math\nmodels_results = {}\nfor model in optimized_estimators_bs:\n    models_results[model] = quick_evaluate_estimator(\n        _optimized_estimator=optimized_estimators_bs[model],\n        _nsplits=N_SPLITS, _nrepeats=1, _random_states=RANDOM_STATES,\n        _X_train=X_train_bs, _y_train=y_train, _njobs=N_JOBS, \n        return_results=True, print_res=False,)\n    \n_ = plot_optimized_estimators(optimized_estimators={\n    ename: optimized_estimators_bs[ename] for ename in optimized_estimators_bs},\n    X_=X_train_bs, y_=y_train, n_splits=N_SPLITS, n_jobs=N_JOBS, \n                              plot_type=['grid', 'corrplot'])\n\nresult_df = pd.DataFrame.from_dict(\n    {model: {rs: models_results[model][rs]['mean'] for rs in RANDOM_STATES} for model in models_results}).T\nresult_df['mean'] = result_df.mean(axis=1)\nresult_df.sort_values(by='mean')","88b590e9":"from sklearn.base import RegressorMixin, BaseEstimator, TransformerMixin\nfrom sklearn.base import clone\n\n\nclass EstimatorWeightedAverager(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models, weights):\n        \"\"\"\n        @models - alist of models to be aggregated\n        @weights - a list of weights. The weights must sum up to 1.\n        \"\"\"\n        self.models = models\n        self.weights = weights\n\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n\n    # Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_])\n        return np.sum(predictions*self.weights, axis=1)\n","5c1dced2":"subsets_to_aggregate=[\n    ('SVRpoly3_pipe_bs', 'KR_poly_pipe_bs', 'LightGBM_bs')\n]\n","b0925c83":"def objective_ewa(params):\n    weights_sum = np.sum([x for x in params['weights']])\n    weights = [x\/weights_sum for x in params['weights']]\n    aggregator = EstimatorWeightedAverager(models_to_aggregate, weights)\n    # use large number of repetitions to minimize accidental data sequence impact on the result\n    kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=None)\n    scores = cross_val_score(estimator=aggregator, X=X_train_bs, y=y_train, \n                             scoring=\"neg_mean_squared_error\", cv=kfold, n_jobs=N_JOBS)\n    loss = np.mean(np.sqrt(np.abs(scores)))\n    std = np.std(np.sqrt(np.abs(scores)))\n    return {'loss': loss, 'loss_std': std, 'params': params, \n            'status': STATUS_OK, 'estimator': aggregator}\n\n\naggregation_results = {}\nidx = 0\nfor subset in subsets_to_aggregate:\n    idx += 1\n    print('Aggregating {}\/{}, {}'.format(idx, len(subsets_to_aggregate), subset))\n    models_to_aggregate_names = subset\n    models_to_aggregate = [optimized_estimators_bs[m]\n                           for m in models_to_aggregate_names]\n    aggreagation_weights = [hp.quniform(\n        name, 0, 1, 0.001) for name in models_to_aggregate_names]\n    aggregation_space = {'weights': aggreagation_weights}\n    aggregator = EstimatorWeightedAverager(models_to_aggregate,  # init with equally balanced \n                                           [1\/len(models_to_aggregate_names)\n                                            for x in range(0, len(models_to_aggregate_names))])\n    max_evals = 300\n    bayes_trials = Trials()\n    best = fmin(fn=objective_ewa, space=aggregation_space, algo=tpe.suggest, max_evals=max_evals,\n                trials=bayes_trials, rstate=None)\n    aggregator = bayes_trials.best_trial['result']['estimator']\n\n    res = quick_evaluate_estimator(\n        _optimized_estimator=aggregator,_nsplits=N_SPLITS, _nrepeats=1, \n        _random_states=RANDOM_STATES,_X_train=X_train_bs, \n        _y_train=y_train, _njobs=N_JOBS, print_res=False, return_results=True)\n\n    aggregation_results[subset] = {\n        'aggregator': aggregator, 'evaluation': res, \n        'mean_score': np.mean([x for x in {x: res[x]['mean'] for x in res}.values()])}\n    \n    # predict and save\n    aggregator.fit(X_train_bs,y_train)\n    y_pred=aggregator.predict(X_test_bs)\n    y_pred=pd.Series(\n        inv_boxcox1p(y_pred,0),index=pd.Index([x for x in range(1461,2920)],\n                                              name='Id'),name='SalePrice')\n    y_pred.to_csv('agg_pred_{}.csv'.format(\n        str(subset).replace('(','').replace(')','').replace(\"'\",\"\").\n        replace('_pipe_bs','').replace('_bs','').replace(',','').replace(' ','')),header=True)","b017be68":"agg_results_df = pd.DataFrame.from_dict(\n    {x: {'mean_score': aggregation_results[x]['mean_score'],\n         x[0]: aggregation_results[x]['aggregator'].get_params()['weights'][0],\n         x[1]: aggregation_results[x]['aggregator'].get_params()['weights'][1],\n         x[2]: aggregation_results[x]['aggregator'].get_params()['weights'][2]} \n     for x in aggregation_results for i in range(len(x))}, orient='index')\n\nagg_results_df.sort_values(by='mean_score')\nagg_results_hm = agg_results_df.sort_values(by='mean_score').copy()\nplt.subplots(nrows=1, ncols=1, figsize=(10, 4))\nsns.heatmap(agg_results_hm.drop('mean_score', axis=1), annot=True, fmt=\".2f\")\nplot_optimized_estimators(\n    optimized_estimators={\n        ename: aggregation_results[ename]['aggregator'] for ename in aggregation_results},\n    X_=X_train_bs, y_=y_train, n_splits=N_SPLITS, n_jobs=N_JOBS, plot_type=['detail']\n)\n\nagg_results_hm['mean_score']\n","fea2b4c1":"print('Script finished in ',dt.datetime.now()-GLOBAL_START_TIME)","a51c29ae":"## Highly correlated features elimination\n<a id='hcf_elimination'><\/a>","9475d92a":"### Kitchen related features","b4deeb48":"## Weighted aggregation","57b6ebcd":"**outliers from data description**","b045ac6f":"**additional outliers**\n- additionall outliers were identified from the first iteration ","618838ad":"**splitting back**","d8a76773":"# **Feature selection**\n- The final set of features was identified using Boruta algorithm. For a description of the method, I would recommend the paper https:\/\/pdfs.semanticscholar.org\/85a8\/b1d9c52f9f795fda7e12376e751526953f38.pdf> \n- among others, Boruta selector requires setting the \"perc\" parameter. The right value was found searching the scope of possible values (70-95), and assessing each feature set with two algorithms: ElasticNet and LightGBM (one linear model and one tree-based). I selected the first local minimum common to both regressors.\n- This approach is computationally intensive, but can be done in less than hour using 12 cores, assuming that you parallelize computations.\n- below is commented code I used. The selected set of features was hardcoded to avoid computations","bf63bcb0":"### Boruta selection for a wide range of nperc","a30a068b":"## Transforming output variable\n<a id='transforming_output'><\/a>","c07f4b02":"## Data imputing\n<a id='data_imputing'><\/a>\n- Data imputing is performed in two steps.\n- At first systematic NaNs are imputed (mostly NaNs with special meaning).\n- In the second step accidental NaNs were imputed, sometimes considering inconstitencies in data <br>(or at least something I classified as an incosistency).\n","14ae617a":"### Exterior related","4dbf6f8f":"### Building related features","793a57bb":"# Baseline\n- before any modification, let's look how the model performs","50dd443a":"# **Single models optimization**\n<a id='data_preparation'><\/a>\n- tree based models were used without any scaling\n- for linear models scaling was applied\n","c3355f1f":"### Building age related features","8a47e943":"### Non-systematic NaNs","434306c0":"## Outliers removal\n<a id='outliers_removal'><\/a>","076b0dfc":"# **Weighted aggregation of selected models**\n- weighted aggregation can be performed using as many models as you want, but... this is very time-consuming (one night is enough \ud83d\ude0a assuming 12 base models and analysis of triades).\n- the number of models and their combinations was reduced here to be able to execute script using Kaggle environment, nonetheless a ground idea is clear\n- to select combinations to use for weights searching, I used \u201cstatic\u201d way:\n> - for each optimized model I calculated predictions in using several random states\n> - in a loop I computed the weighted average of predictions using a range of weights\nThe effect was far from the real performance of aggregated models, but the order of results allowed me to identify top-performing combinations (and the computations were fast (20 min) with parallelisation, and would be even faster if I use GPU based data frames)\nOf course one can select combination on the basis of single models residuals distribution and their performance.","e792b1ae":"## Data encoding\n<a id='data_encoding'><\/a>","85eddae3":"### Utilities and misc related features","da4ac054":"### Garage related features","49b73fbf":"## Individual features analysis\n<a id='features_analysis'><\/a>\n- Data will be first grouped into three sets:\n> - nominal features, to one hot encode\n> - continuous features \n- any data transformations except deskewing will be applied here","30fc44bf":"### Basement and foundation related","d9500269":"### Porch and deck related featrues","d6d234e8":"# Introduction\n\nThe general workflow of this kernel is as follows:\n1. Nans imputation -> 2. Outliers removal -> 3. Output variable transformation -> \n4. Individual features analysis -> 5. Adjustment skewness -> 6. Data encoding -> \n7. Highly correlated features elimination -> 8. Feature selection ->\n9. Optimization of individual regressors -> 10. Weighted aggregation of models \n\n**While working on this kernel I benefited signifficantly from several solutions, the following among others**:\n- https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n- https:\/\/www.kaggle.com\/mgmarques\/houses-prices-complete-solution\/notebook\n\n**Remarks**\n\n> Due to the limitations of Kaggle docker (4CPU, 4 hours of execution max) I had to skip some computations \nor made them simpler (f.e. finding an  optimal params for Boruta or reduce the number of iterations in RepeatedKFold).<br> \nI also decided to hardcode several elements to make things repeatable. <br>\nI skipped stacking of aggregated predictions (stacking was performed with MLXtend StackingRegressor and improved results slightly, but was computationally expensive.\n\n**It made the final result slightly worse, but critical concepts of the kernel was retained.**\n\n**I hope this kernel will be still useful as a complete study of obtaining relatively good score.**<br>\n**<font color='green'>If you found it useful, please do not forget to upvote :-)<\/font>**","9907a26e":"### Bath related features","ffd2d45d":"**dropping outliers**","9e10db32":"some supporting stuff used by the kernel","caee4b2c":"### Property shape related features","b902232d":"### Imputing special meaning\/systematic NaN","755e7dd9":"### Sale related features","af6e6bd9":"**loading oryginal datafiles**\n- Train and tests data are loaded and concatenated into single dataframe for preprocessing\n- Train\/test data are identified by \"Train\" column","5eb26067":"The kernel presented here allows to achive score slightly above 0,11, in top 5%.<br>\nTo get the score below 11.0 I added stacking of aggregated combinations, <br> \n(including Gradient Boosting Regressor with Huber loss, SGD Huber and LightGBM with dart boosting),<br> so the final model was too complex (and too computationally expensive to run on Kaggle platform).\n\nI focused on Hyperopt, aggregating and stacking in this kernel,<br> but I'm sure that there is a lot what can be done with features to improve the score<br>.\n(fe. target coding all categorical variables that were used as continuous).<br>\nAnother option which improves final score is adjustment of the aggregation by quantiles<br> (there is a systematic error visible on the residuals diagrams )\n\nFinally, sorry for the code layout.<br> I did my best to make it looking well in HTML export, but it seems <br>that Kaggle nbconvert requires a bit different width (maybe standard :-) ) than I use. <br>Next time I will set the editor properly at the beginning. \n\n\n","855e9c1a":"### Roof related features","226c3f95":"### Masonery related features","a57afbcb":"**global settings**","7c787f0c":"### Location related features","64a11504":"## Adjusting continuous features distributions\n<a id='adjusting_dists'><\/a>\nIn general, there are (at least) two approaches to correct skewness:\n1. prepare and apply transformation (log based, arcsinh based etc.) on both train and test set at once, with optimized parameters\n2. prepare and apply the transformation on a train set, then apply to test set \n\nI think the second approach is correct and was used below, although I saw that the first approach is sometimes also used, with good results.\nThe cons of the first approach are as follows:\n- it optimizes parameters using test data, that can be unknown while preparing the model\n- it can be misleading (the skewness of train data is very often different from the skewness of the whole set)\n","e25a1cc0":"# **Data preparation**\n<a id='data_preparation'><\/a>"}}