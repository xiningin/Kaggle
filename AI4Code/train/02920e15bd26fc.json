{"cell_type":{"0274e3b1":"code","ea92584c":"code","1231ca54":"code","433389c8":"code","34538baf":"code","f6bf2ff4":"code","fc60a59d":"code","6a6b0e25":"code","6c51fa9e":"code","7df79fa6":"code","afd16a1a":"code","346c7994":"code","8b83ed2d":"code","6053d7b9":"code","fb56e598":"code","75527d26":"code","f3d5fa93":"code","d448cd2a":"code","6af45cc7":"code","c6ce677a":"code","4309fdce":"code","4c7b8cb9":"code","1044e425":"code","2b2a6b35":"code","0a73a1e0":"code","2577cfa0":"code","844aeabe":"code","ecb031eb":"code","ca973467":"code","756a8d7b":"code","a2bba97f":"code","1b12af66":"code","24345278":"code","91da4593":"code","b6a3a74b":"code","85b830b2":"code","87797203":"code","342a8d96":"code","2a939a1c":"code","764c7203":"code","85285139":"code","89ac2438":"code","d6efab69":"code","faa784fb":"code","2d2599d2":"code","5a288355":"code","d8b20f02":"code","4bbbabf5":"code","65a370ac":"code","e44326cf":"code","5c1f8551":"code","4f4ccdaf":"code","4a07b0c4":"code","62b6f87c":"code","31977322":"code","06651ea2":"code","b85b5928":"code","4ec4a45f":"code","10524c1f":"code","db20421f":"code","f4b5cc07":"code","c43902f3":"code","e840d6b1":"code","80942439":"code","42d68378":"code","54a22ecf":"code","a3e0cd55":"code","5234af5e":"code","df16d35b":"code","509cc6d1":"code","39d64247":"code","91fac9cf":"code","c5e192e4":"code","77a21276":"code","1725b7fe":"code","965868c1":"code","c61688c5":"code","c6f5cc00":"code","0c20cf27":"code","2dd096e3":"code","003f4ea9":"code","cc2a9023":"code","1fc1a75a":"code","fe405feb":"code","d3fbdff0":"code","d7d46931":"code","2be2463f":"code","d590a1ea":"code","16184b85":"code","c35662c6":"code","1d9b6047":"code","0dad0b9c":"code","4fd2266b":"code","ed5631c5":"code","0be313d9":"code","a55d2f89":"markdown","9f327a8a":"markdown","f30391d6":"markdown","d8171835":"markdown","0fac74e4":"markdown","fd3b923e":"markdown","3e7f7f20":"markdown","b37f8d05":"markdown","87084a57":"markdown","9c560fc1":"markdown","4fc13d96":"markdown","d88a826e":"markdown","541fd76c":"markdown","c2b08621":"markdown","c2791f57":"markdown","3315aeda":"markdown","6daefea2":"markdown","8f657e57":"markdown","cd2e2a08":"markdown","d1610a97":"markdown","6ac5059a":"markdown","2d0dfc9b":"markdown","633c0da8":"markdown","df8f5e40":"markdown","a9826ef0":"markdown","2c0b37e7":"markdown","ec2be776":"markdown","8ede4f6f":"markdown","4253fa90":"markdown","59f633a4":"markdown","78066405":"markdown","46227cd0":"markdown","817d77a7":"markdown","b298de17":"markdown","e9b16dbf":"markdown","106aa2d4":"markdown","35a8311f":"markdown"},"source":{"0274e3b1":"# Installing external libraries\n\n!pip install xgboost\n!pip install lightgbm\n!pip install catboost","ea92584c":"# Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost\nimport warnings\nwarnings.simplefilter(action='ignore')\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import neighbors\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsRegressor\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n","1231ca54":"# Reading Hitters data from kaggle server\n\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")  ","433389c8":"df.head()","34538baf":"df.info()","f6bf2ff4":"df.describe().T","fc60a59d":"df.shape","6a6b0e25":"# detecting missing values \n\ndf.isnull().sum()","6c51fa9e":"#For visualizing missing values I need to install below package\n# When you are working with anaconda you may need this installation\n\nconda install -c conda-forge\/label\/cf202003 missingno\n","7df79fa6":"#Visualizing missing values\n\nimport missingno as msno\nmsno.bar(df);","afd16a1a":"#Correlation values more than 0.5 between features (Because of >0.5 I can only see the values above 0.5)\n\ncorrelation_matrix = df.corr().round(2)\nfiltre=np.abs(correlation_matrix['Salary'])>0.50\ncorr_features=correlation_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(),annot=True,fmt=\".2f\")\nplt.title('Correlation btw features')\nplt.show()","346c7994":"# Even though there are very high correlation between some of the variables I will not do anything. Normally this problem should be solved.\n# Here I will delete missing values\n\ndf = df.dropna()","8b83ed2d":"df.shape","6053d7b9":"df.sort_values('Salary', ascending = False).head()\n","fb56e598":"# I have 3 categorical variables\n\ndf['League'].value_counts()","75527d26":"df['NewLeague'].value_counts()","f3d5fa93":"df['Division'].value_counts()","d448cd2a":"# Transforming nominal variables with one hot encoding method. Normally label encoding variable can be applied for dummy variables. One hot encoding is appropriate for the nominal variables have 3 or more categories \n\ndf = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)","6af45cc7":"df.head()","c6ce677a":"# For detecting outliers I will use LocalOutlierFactor. I will use default values of 20 and 'auto'.\n\nclf=LocalOutlierFactor(n_neighbors=20, contamination='auto')\nclf.fit_predict(df)\ndf_scores=clf.negative_outlier_factor_\ndf_scores= np.sort(df_scores)\ndf_scores[0:20]","4309fdce":"?LocalOutlierFactor","4c7b8cb9":"# I will take the 5th value as  threshold while the values after fift values decreasing closely\n# However at first I will visualize this situation regarding outliers\n\nsns.boxplot(df_scores);","1044e425":"threshold=np.sort(df_scores)[5]\nprint(threshold)\ndf = df.loc[df_scores > threshold]\ndf = df.reset_index(drop=True)","2b2a6b35":"df.shape","0a73a1e0":"# Standardization\n# I will make some operations in the below rows.\n# Salary is my dependent variable, others are dummy variables. At first I will drop them from my independent variable set (X)\n#At last I will combine all of the independent variables\n\ndf_X=df.drop(['Salary','League_N','Division_W','NewLeague_N'], axis=1)\ndf_X.head()\n","2577cfa0":"from sklearn.preprocessing import StandardScaler\nscaled_cols=StandardScaler().fit_transform(df_X)\n\n\n\nscaled_cols=pd.DataFrame(scaled_cols, columns=df_X.columns)\nscaled_cols.head()","844aeabe":"cat_df=df.loc[:, \"League_N\":\"NewLeague_N\"]\ncat_df.head()","ecb031eb":"Salary=pd.DataFrame(df['Salary'])","ca973467":"df=pd.concat([Salary,scaled_cols, cat_df], axis=1)\ndf.head()","756a8d7b":"# Dependent variable y = Salary, independents variables x = the variables without salary\n\ny = df['Salary']\nX = df.drop('Salary', axis =1)","a2bba97f":"X","1b12af66":"y","24345278":"# We will evaluate our model results cccording to mean value of predicted variable (y) \n\ny.mean()","91da4593":"# Train and test separation process and determining train and test size\n#Test size will be %20 of the data and random state will be 46 for all of the models in order to compare the models\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)","b6a3a74b":"linreg = LinearRegression()\nmodel = linreg.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_linreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_linreg_rmse","85b830b2":"ridreg = Ridge()\nmodel = ridreg.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ndf_ridreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_ridreg_rmse ","87797203":"lasreg = Lasso()\nmodel = lasreg.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_lasreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_lasreg_rmse","342a8d96":"enet = ElasticNet()\nmodel = enet.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_enet_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_enet_rmse","2a939a1c":"knn = KNeighborsRegressor()\nknn_model = knn.fit(X_train, y_train)\ny_pred = knn_model.predict(X_test)\ndf_knn_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_knn_rmse","764c7203":"svr = SVR(\"linear\")\nsvr_model = svr.fit(X_train, y_train)\ny_pred = svr_model.predict(X_test)\ndf_svr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_svr_rmse","85285139":"mlp = MLPRegressor()\nmlp_model = mlp.fit(X_train, y_train)\ny_pred = mlp_model.predict(X_test)\ndf_mlp_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_mlp_rmse","89ac2438":"cart = DecisionTreeRegressor()\ncart_model = cart.fit(X_train, y_train)\ny_pred = cart_model.predict(X_test)\ndf_cart_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_cart_rmse","d6efab69":"rf = RandomForestRegressor()\nrf_model = rf.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\ndf_rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_rf_rmse","faa784fb":"gbm = GradientBoostingRegressor()\ngbm_model = gbm.fit(X_train, y_train)\ny_pred = gbm_model.predict(X_test)\ndf_gbm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_gbm_rmse","2d2599d2":"xgb = XGBRegressor()\nxgb_model = xgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\ndf_xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_xgb_rmse","5a288355":"lgbm = LGBMRegressor()\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\ndf_lgbm_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_lgbm_rmse","d8b20f02":"catb = CatBoostRegressor()\ncatb_model = catb.fit(X_train, y_train)\ny_pred = catb_model.predict(X_test)\ndf_catb_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_catb_rmse","4bbbabf5":"# Thirteen models' Root Mean Squared Errors (RMSE) \n# I will not include CatBoostRegressor since it takes about 2 hours when I include CatBoostRegressor. \n# I will report it seperately to save time\n\ndef compML(df, y, alg):\n    model = alg().fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_name = alg.__name__\n    print(model_name, \"Model RMSE:\", RMSE)","65a370ac":"models = [LinearRegression, Ridge, Lasso, ElasticNet, KNeighborsRegressor, SVR, MLPRegressor, DecisionTreeRegressor, \n          RandomForestRegressor, GradientBoostingRegressor, XGBRegressor, LGBMRegressor] ","e44326cf":"for model in models:\n    compML(df, 'Salary', model)","5c1f8551":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 in Ridge regression. We will try different values.\n# The best fit alpha value or parameter will be employed in the final model\n\nalpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nridreg_cv = RidgeCV(alphas = alpha, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridreg_cv.fit(X_train, y_train)\nridreg_cv.alpha_\n\n#Final Model \n\nridreg_tuned = Ridge(alpha = ridreg_cv.alpha_).fit(X_train,y_train)\ny_pred = ridreg_tuned.predict(X_test)\ndf_ridge_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_ridge_tuned_rmse","4f4ccdaf":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 in Lasso regression. We will try different values.\n# The best fit alpha value or parameter will be employed in the final model\n\nalpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nlasso_cv = LassoCV(alphas = alpha, cv = 10, normalize = True)\nlasso_cv.fit(X_train, y_train)\nlasso_cv.alpha_\n\n# Final Model \n\nlasso_tuned = Lasso(alpha = lasso_cv.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\ndf_lasso_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_lasso_tuned_rmse","4a07b0c4":"?Lasso","62b6f87c":"?ElasticNet","31977322":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 and default l1_ratio is 0.5 in ElesticNet regression. We will try different values.\n# The best fit  values or parameters will be employed in the final model\n\n\nenet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\nenet = ElasticNet()\nenet_model = enet.fit(X_train,y_train)\nenet_cv = GridSearchCV(enet_model, enet_params, cv = 10).fit(X, y)\nenet_cv.best_params_\n\n#Final Model \n\nenet_tuned = ElasticNet(**enet_cv.best_params_).fit(X_train,y_train)\ny_pred = enet_tuned.predict(X_test)\ndf_enet_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_enet_tuned_rmse ","06651ea2":"?knn","b85b5928":"# n_neighbors : int, default=5 Number of neighbors to use by default for :meth:`kneighbors` queries.\n\nknn_params = {\"n_neighbors\": np.arange(2,30,1)}\nknn_cv_model = GridSearchCV(knn_model, knn_params, cv = 10).fit(X_train, y_train)\nknn_cv_model.best_params_\nknn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)\n\n# Final Model\n\ny_pred = knn_tuned.predict(X_test)\ndf_knn_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_knn_tuned_rmse","4ec4a45f":"knn_cv_model.best_params_","10524c1f":"knn_cv_model.best_estimator_","db20421f":"?svr","f4b5cc07":"# C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n\nsvr_params = {'C': [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1]}\nsvr_cv_model = GridSearchCV(svr_model, svr_params, cv = 5, n_jobs = -1, verbose =  2).fit(X_train, y_train)\nsvr_tuned = SVR('linear', **svr_cv_model.best_params_).fit(X_train, y_train)\ny_pred = svr_tuned.predict(X_test)\ndf_svr_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_svr_tuned_rmse","c43902f3":"svr_cv_model.best_params_","e840d6b1":"svr_cv_model.best_estimator_","80942439":"?mlp","42d68378":"# hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,) The ith element represents the number of neurons in the ith hidden layer.\n#alpha : float, default=0.0001\n\nmlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}\nmlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train, y_train)\nmlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train, y_train)\ny_pred = mlp_tuned.predict(X_test)\ndf_mlp_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_mlp_tuned_rmse","54a22ecf":"mlp_cv_model.best_params_","a3e0cd55":"mlp_cv_model.best_estimator_","5234af5e":"?cart","df16d35b":"cart_params = {\"max_depth\": [2,3,4,5,10,20, 100, 1000],\n              \"min_samples_split\": [2,10,5,30,50,10]}\ncart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10).fit(X_train, y_train)\ncart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)\ny_pred = cart_tuned.predict(X_test)\ndf_cart_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_cart_tuned_rmse","509cc6d1":"cart_cv_model.best_params_","39d64247":"cart_cv_model.best_estimator_","91fac9cf":"?RandomForestRegressor","c5e192e4":"rf_params = {\"max_depth\": [5,8,10,None],\n            \"max_features\": [2,5,10,15,17],\n            \"n_estimators\": [100,200, 500, 1000],\n            \"min_samples_split\": [2,5,10,20,30]}\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\nrf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)\ny_pred = rf_tuned.predict(X_test)\ndf_rf_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_rf_tuned_rmse","77a21276":"rf_cv_model.best_params_","1725b7fe":"rf_cv_model.best_estimator_","965868c1":"?gbm","c61688c5":"# This process takes too much time therefore it would be better to run the code with cv =5 instead of cv = 10. \n\ngbm_params = {\"learning_rate\": [0.001,0.1,0.01, 0.05],\n             \"max_depth\": [1,2,3,5,8,9,10],\n             \"n_estimators\": [50,100,200,500,1000],\n             \"subsample\": [2,1.5,1,0.4,0.5,0.7],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}                  \ngbm_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\ngbm_tuned = GradientBoostingRegressor(**gbm_cv_model.best_params_).fit(X_train, y_train)                             \ny_pred = gbm_tuned.predict(X_test)                             \ndf_gbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                             \ndf_gbm_tuned_rmse      ","c6f5cc00":"gbm_cv_model.best_params_","0c20cf27":"gbm_cv_model.best_estimator_","2dd096e3":"?xgb","003f4ea9":"\nxgb_params = {\"learning_rate\": [0.1,0.01,0.5,0.7,0.8],\n             \"max_depth\": [3,4,5,6,7,8],\n             \"n_estimators\": [100,200,500,1000],\n             \"colsample_bytree\": [0.5,0.7,0.8,0.9]}\nxgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\nxgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred = xgb_tuned.predict(X_test)\ndf_xgb_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_xgb_tuned_rmse","cc2a9023":"xgb_cv_model.best_params_","1fc1a75a":"xgb_cv_model.best_estimator_","fe405feb":"?lgbm","d3fbdff0":"#learning_rate default = 0.1, n_estimators default = 100, colsample_bytree default = 1, max_depth default = -1, n_jobs default=-1)\n\n\nlgbm_params = {\"learning_rate\": [0.01,0.001, 0.1, 0.5, 1],\n              \"n_estimators\": [50,80,100,200,500,1000],\n              \"max_depth\": [-1.5, -1.3, -1, 0.3, 0.5,0.7,2,4,6,7,10],\n              \"colsample_bytree\": [0.1,0.3,0.5,0.7,1,1.3,1.5]}\nlgbm_cv_model = GridSearchCV(lgbm_model, lgbm_params, cv = 10, n_jobs = -1, verbose =2).fit(X_train, y_train)\nlgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)                              \ny_pred = lgbm_tuned.predict(X_test)                              \ndf_lgbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                              \ndf_lgbm_tuned_rmse  ","d7d46931":"lgbm_cv_model.best_params_","2be2463f":"lgbm_cv_model.best_estimator_","d590a1ea":"?catb","16184b85":"# I tried both 10 and 5 folds. Fitting with 5 folds (cv =5) gave (lower) better result. With 10 folds the rmse value was 255, however with 5 folds it became 240.\n\ncatb_params = {\"iterations\": [200,500,100],\n              \"learning_rate\": [0.01,0.1],\n              \"depth\": [3,6,8]}\ncatb_cv_model = GridSearchCV(catb_model, catb_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)\ncatb_tuned = CatBoostRegressor(**catb_cv_model.best_params_).fit(X_train, y_train)                            \ny_pred = catb_tuned.predict(X_test) \ndf_catb_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))                            \ndf_catb_tuned_rmse                            ","c35662c6":"catb_cv_model.best_params_","1d9b6047":"catb_cv_model.best_estimator_","0dad0b9c":"\nComparableResults_df = pd.DataFrame({\"LINEAR\":[df_linreg_rmse],\"RIDGE\":[df_ridreg_rmse],\"RIDGE TUNED\":[df_ridge_tuned_rmse],\n                             \"LASSO\":[df_lasreg_rmse],\"LASSO TUNED\":[df_lasso_tuned_rmse], \n                             \"ELASTIC NET\":[df_enet_rmse], \"ELASTIC NET TUNED\":[df_enet_tuned_rmse],\n                             \"KNN\":[df_knn_rmse], \"KNN TUNED\":[df_knn_tuned_rmse],\n                             \"SVR\":[df_svr_rmse], \"SVR TUNED\":[df_svr_tuned_rmse],\n                             \"MLP\":[df_mlp_rmse], \"MLP TUNED\":[df_mlp_tuned_rmse],\n                             \"CART\":[df_cart_rmse], \"CART TUNED\":[df_cart_tuned_rmse],\n                             \"RF\":[df_rf_rmse], \"RF TUNED\":[df_rf_tuned_rmse],\n                             \"GBM\":[df_gbm_rmse], \"GBM TUNED\":[df_gbm_tuned_rmse],\n                             \"XGBOOST\":[df_xgb_rmse], \"XGBOOST TUNED\":[df_xgb_tuned_rmse],\n                             \"LightGBM\":[df_lgbm_rmse], \"LightGBM TUNED\":[df_lgbm_tuned_rmse],\n                             \"CatBoost\":[df_catb_rmse], \"CatBoost TUNED\":[df_catb_tuned_rmse]})\n\nComparableResults_df\n","4fd2266b":"ComparableResults_df.min(axis = 1, skipna = True)","ed5631c5":"ComparableResults_df.idxmin(axis=1)","0be313d9":"ComparableResults_df.T","a55d2f89":"- Parameters\n----------\n- loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n\n    loss function to be optimized. 'ls' refers to least squares\n    regression. 'lad' (least absolute deviation) is a highly robust\n    loss function solely based on order information of the input\n    variables. 'huber' is a combination of the two. 'quantile'\n    allows quantile regression (use `alpha` to specify the quantile).\n\n- learning_rate : float, default=0.1\n\n    learning rate shrinks the contribution of each tree by `learning_rate`.\n    There is a trade-off between learning_rate and n_estimators.\n\n- n_estimators : int, default=100\n\n    The number of boosting stages to perform. Gradient boosting\n    is fairly robust to over-fitting so a large number usually\n    results in better performance.\n\n- subsample : float, default=1.0\n\n    The fraction of samples to be used for fitting the individual base\n    learners. If smaller than 1.0 this results in Stochastic Gradient\n    Boosting. `subsample` interacts with the parameter `n_estimators`.\n    Choosing `subsample < 1.0` leads to a reduction of variance\n    and an increase in bias.\n\n- max_depth : int, default=3\n\n    maximum depth of the individual regression estimators. The maximum\n    depth limits the number of nodes in the tree. Tune this parameter\n    for best performance; the best value depends on the interaction\n    of the input variables.\n","9f327a8a":"### CART (Classification and Regression Trees)","f30391d6":"### Among base machine learning models CatBoost (Category Boosting) model ist the best model to predict salary with its RMSE value of 258.90197830660554.","d8171835":"- RandomForestRegressor default values\n    \n   -n_estimators=100,\n   - criterion='mse',\n   - max_depth=None,\n   - min_samples_split=2\n   - n_jobs=None,\n   - verbose=0\n    ","0fac74e4":"### KNN (K-Nearest Neighbors)","fd3b923e":"##### Prediction value (rmse) for linear regression model is 382.00085575367274. y.mean value is 538.2316872586872\n","3e7f7f20":"Salary variable has 59 missing values","b37f8d05":"### Lasso Regression","87084a57":"### CatBoost (Category Boosting)","9c560fc1":"### Random Forests","4fc13d96":"### MLP (Multilayer Perceptron) Model Tuning\nOne of the Artificial Neural Network Models (ANN)","d88a826e":"### CatBoost (Category Boosting)","541fd76c":"* max_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\n* min_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.","c2b08621":"### Understanding Data","c2791f57":"### GBM (Gradient Boosting Machines)","3315aeda":"### MODELING","6daefea2":"### Lasso Regression Model Tuning","8f657e57":"### XGBoost (Extreme Gradient Boosting) Model Tuning","cd2e2a08":"### CART (Classification and Regression Trees) Model Tuning","d1610a97":"## Model Tuning","6ac5059a":"### Linear Regression","2d0dfc9b":"\n\n## Results and Conclusion \n\n\n\nIn this project, thirteen different machine learning models were employed to predict salary of any US Major Baseball League player. By using Linear Regression, Ridge Regression, Lasso Regression, ElasticNet Regression,  KNN (K-Nearest Neighbors), SVR (Support Vector Regression), MLP (Multilayer Perceptron), CART (Classification and Regression Trees), Random Forests, GBM (Gradient Boosting Machines), XGBoost (Extreme Gradient Boosting), LightGBM, and CatBoost (Category Boosting) Machine Learning Models the root mean squared errors (RMSE) values were calculated. The RMSE is a measure of the average deviation of the estimates from the observed values. Then, the RMSE values were tried to be decreased with the help of hyperparameter optimizations. All of the base models were tuned. The results showed that in both the base and the tuned model, the lowest RMSE value (258.901978 and 240.560824) obtained from the CatBoost (Category Boosting) Machine Learning model. The best Machine Learning model became tuned CatBoost model with its RMSE value of 240.560824. This error score is quite far away from the mean of predicted value (539.2295992217898).\nIn sum, analyses and predictions results explicitly revealed that tuned CatBoost (Category Boosting) Machine Learning model is the best model to predict a US Baseball Major League player's salary. \n","633c0da8":"### Elastic Net Regression Regression Model Tuning","df8f5e40":"### KNN (K-Nearest Neighbors) Model Tuning","a9826ef0":"### MLP (Multilayer Perceptron)\nOne of the Artificial Neural Network Models (ANN)","2c0b37e7":"### GBM (Gradient Boosting Machines) Model Tuning","ec2be776":"### LightGBM","8ede4f6f":"### LightGBM Model Tuning","4253fa90":"### SVR (Support Vector Regression) Model Tuning","59f633a4":"### XGBoost (Extreme Gradient Boosting)","78066405":"### Random Forests  Model Tuning","46227cd0":"# Salary Prediction Project of US Baseball Major League Players with Thirteen Different Machine Learning Models\n\nIn this project, thirteen different machine learning models will be employed to predict salary of any US Major Baseball League player. The Hitters data described below will be used to predict the salaries of baseball players. The data will be retrieved from \"https:\/\/www.kaggle.com\"\n\n \n \n \n### Description\n    \n#### Context\n\nThis dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) \"An Introduction to Statistical Learning with applications in R\" to demonstrate how Ridge regression and the LASSO are performed using R.\n\n#### Content\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n#### Format\n\nA data frame with 322 observations of major league players on the following 20 variables.\n\n- AtBat Number of times at bat in 1986\n- Hits Number of hits in 1986\n- HmRun Number of home runs in 1986\n- Runs Number of runs in 1986\n- RBI Number of runs batted in in 1986\n- Walks Number of walks in 1986\n- Years Number of years in the major leagues\n- CAtBat Number of times at bat during his career\n- CHits Number of hits during his career\n- CHmRun Number of home runs during his career\n- CRuns Number of runs during his career\n- CRBI Number of runs batted in during his career\n- CWalks Number of walks during his career\n- League A factor with levels A and N indicating player\u2019s league at the end of 1986\n- Division A factor with levels E and W indicating player\u2019s division at the end of 1986\n- PutOuts Number of put outs in 1986\n- Assists Number of assists in 1986\n- Errors Number of errors in 1986\n- Salary 1987 annual salary on opening day in thousands of dollars\n- NewLeague A factor with levels A and N indicating player\u2019s league at the beginning of 1987\n\nAcknowledgements\nPlease cite\/acknowledge: Games, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York. \n\n\n","817d77a7":"### Comparable Results of Four Basic and Tuned Models","b298de17":"### Elastic Net Regression","e9b16dbf":"### SVR (Support Vector Regression)","106aa2d4":"### Ridge Regression","35a8311f":"### Ridge Regression Model Tuning"}}