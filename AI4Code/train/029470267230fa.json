{"cell_type":{"ba5202d0":"code","7bbf8201":"code","5d91673e":"markdown","f89f1257":"markdown","e342ac01":"markdown"},"source":{"ba5202d0":"%%writefile 'example.py'\n#!\/usr\/bin\/python3\n\nimport math\nimport random\nimport numpy as np\n\nnaction  = 3\nhalfLife = 100.0\nalpha    = math.exp(- math.log(2) \/ halfLife)\n\nclass AGENT:\n  \n  def __init__(self, configuration):\n    self.hist = 2 * np.ones((naction, naction, naction))\n    self.lastAgentAction = 0\n    \n  # make a move\n  def move(self, lastOpponentAction, step, forceAgentAction):\n\n    # increment predictors\n    if step < 2:\n      agentAction = random.randrange(naction)\n    else:\n      self.hist[self.penuOpponentAction, self.penuAgentAction] *= alpha\n      self.hist[self.penuOpponentAction, self.penuAgentAction, lastOpponentAction] += 1\n      agentAction = (np.argmax(self.hist[self.lastAgentAction, lastOpponentAction]) + 1) % naction - 1\n      \n    # store the action so that both actions can be sent to bots\n    self.penuAgentAction = self.lastAgentAction\n    self.penuOpponentAction = lastOpponentAction\n    if forceAgentAction != None:\n      self.lastAgentAction = forceAgentAction\n    else:\n      self.lastAgentAction = agentAction\n\n    return(agentAction)\n    \nagent = None\n\ndef main(observation, configuration, forceAgentAction=None):\n  global agent\n\n  if agent == None:\n    agent = AGENT(configuration)\n    return agent.move(None, observation.step, forceAgentAction)\n  else:\n    return agent.move(observation.lastOpponentAction, observation.step, forceAgentAction)","7bbf8201":"#%%writefile 'eval.py'\n#!\/usr\/bin\/python3\n\nimport os\nimport sys\nimport glob\nimport json\nimport statistics\nimport matplotlib.pyplot as plt\nfrom multiprocessing import Pool\n\nnagent = 2\n\nif os.path.exists('\/kaggle'):\n  inputjson = '\/kaggle\/input\/rps-episode\/[0-9]*0.json'\n  agentFile = 'example.py'\nelse:\n  inputjson = 'episode\/[0-9]*0.json'\n  agentFile = sys.argv[1]\n\nclass Struct:\n  def __init__(self, **entries):\n    self.__dict__.update(entries)\n\ndef replayEpisode(episode):\n  replay = json.load(open(episode))\n  configuration = Struct(**(replay['configuration']))\n\n  result = []\n  for index in [[0, 1],[1, 0]]:\n    agentID, opponentID = index\n    namespace = {}\n    exec(open(agentFile).read(), namespace)\n    nwin = 0\n    # print(replay['steps'][0][agentID])\n    agentAction = None\n    for step in replay['steps']:\n        \n      # score the last move\n      if agentAction != None:\n        nwin += (agentAction - step[opponentID]['action'] + 1) % 3 - 1\n\n      # fudge those that are missing step[agentID]['observation']['step']\n      if agentID == 1:\n        step[agentID]['observation']['step'] = step[opponentID]['observation']['step']\n\n      # rerun this move\n      agentAction = namespace['main'](Struct(**step[agentID]['observation']), configuration, forceAgentAction=step[agentID]['action'])\n\n    # base = episode.lstrip('episode\/').rstrip('.json')\n    print(\"predict: '%s'\" % replay['info']['TeamNames'][opponentID], \"forcing from: '%s'\" % replay['info']['TeamNames'][agentID], 'nwin:', nwin)\n    result.append(nwin)\n\n  return result\n\n\ndef ternary(n):\n  thresh = 20\n  if n >= thresh:\n    return +1\n  if n <= -thresh:\n    return -1\n  else:\n    return 0\n\n\nif __name__ == '__main__':\n  result = []\n  with Pool(processes=14) as pool:\n    result2 = pool.map(replayEpisode, glob.glob(inputjson))\n    result = [ternary(item) for sublist in result2 for item in sublist]\n    \n  print(result)\n\n  # do some pretty stuff with the results here\n  print('mean:', statistics.mean(result))\n  print('stddev:', statistics.stdev(result))\n  print('sharpe:', statistics.mean(result) \/ statistics.stdev(result))\n\n  plt.hist(result, bins=1000)\n  plt.savefig('hist.png')\n  plt.show()\n","5d91673e":"The file below, example.py, is an agent that could be submitted to the competition.   It works by looking at pairs of games (steps) and predicting the distribution of opponent actions.   The agent action is then chosen as the move which beats the most frequent opponent action.  It's not supposed to be a compention winner, just something short but non-trivial for you to see that it works.  Note that the only significant difference to normal agents is the use of the optional parameter forceAgentAction.  When called in the competition this won't be set, but we need it here so that our agent keeps in step with the real gameplay.","f89f1257":"The aim of this notebook is to demonstrate how the RPS-episode dataset can be used to develop RPS agents.","e342ac01":"Now we need a feramework where we can call our agent for each of the episdodes in the dataset.\n\nWe are using a notebook here, but in general we want to pass a complete python agent in to the evaluation and run it over all data, this way we can get statistics from lots of possible agents and develop offline.\n\nThe code simply loops over all expisodes, fighting both the first and second agent in the episodes.  We keep track of the final scores, but of course there is much more you are probably interested in.  Expect this bit to take a long time (more than 30mins as a notebook, much faster locally), it's playing the best part of a million games - the first percentage indictes progress.\n\n"}}