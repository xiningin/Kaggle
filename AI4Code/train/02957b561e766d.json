{"cell_type":{"dccd530b":"code","e4d3f23d":"code","25615989":"code","5a7a58f3":"code","497dcd26":"code","65dce80a":"code","fdf29b0c":"code","16c1c1e0":"code","2ac22e75":"code","bcc55aee":"code","bb2763d3":"code","4f65cba4":"code","9073f89b":"code","4780903b":"code","c399a2d2":"code","93e293c3":"code","802aba48":"code","383db8c3":"code","fc7b8a42":"code","fc15b85a":"code","10334d62":"code","b1ef7dc4":"code","2667dc5a":"code","a226e3bd":"code","edb7f539":"code","2f09fa50":"code","8bfb5728":"code","83a042df":"code","fdc39067":"code","25b63b06":"code","f8dbfbb3":"code","3182b98e":"code","cec254ea":"code","bbcbba3b":"code","f23c8470":"code","33b252ed":"code","5558ef1b":"code","4b5b1411":"code","978a1837":"code","e955848f":"code","415384a4":"code","438b788d":"code","195e0e3a":"code","49f7d2fb":"code","233614dc":"code","5ac74760":"code","af100fc7":"code","3566f1af":"code","6580fd39":"code","27c8706f":"code","28aa4b71":"code","e29a73ab":"code","ba427f7a":"code","993fc391":"code","988f2624":"code","8d72da71":"code","54c4c905":"code","6114ae5a":"code","7871519a":"code","d71e94f0":"code","ce1ebe56":"code","fd71c68d":"code","60d342c9":"code","4ddd8cfa":"code","1d1b627c":"code","cb6de9bb":"code","0fb9b80a":"code","511e12dc":"code","8930b2a2":"code","96634c26":"code","640697f1":"code","03c436cd":"code","70c5b097":"code","e45974c7":"code","e4307aad":"code","bdd8a8fe":"code","a2f75173":"code","6d3cf18b":"code","ad33a94f":"code","93f7386c":"code","7b086de4":"code","486d64ba":"code","640d80bd":"code","7a0096d0":"code","66a4de50":"code","12816c1e":"code","6ce998d1":"code","1f2d8dfe":"markdown","2ff0e7f5":"markdown","3b035ade":"markdown","ab65bcb4":"markdown","72698dc1":"markdown","0d107f1d":"markdown","a28c9387":"markdown","6f37715d":"markdown","659b7212":"markdown","b105cd26":"markdown","73697d0c":"markdown","e4ffe7ec":"markdown","7020a25d":"markdown","3cf614b7":"markdown","b6f71ad1":"markdown","46e807ee":"markdown","accf8957":"markdown","47eb4173":"markdown","452aecd4":"markdown","15ae6181":"markdown","6d41da45":"markdown"},"source":{"dccd530b":"import sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport platform\nimport seaborn as sns\nfrom sklearn import datasets \nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nmessage=\"        Versions        \"\nprint(\"*\"*len(message))\nprint(message)\nprint(\"*\"*len(message))\nprint(\"Scikit-learn version={}\".format(sklearn.__version__))\nprint(\"Numpy version={}\".format(np.__version__))\nprint(\"Pandas version={}\".format(pd.__version__))\nprint(\"Matplotlib version={}\".format(matplotlib.__version__))\nprint(\"Python version={}\".format(platform.python_version()))\n\n# shift-tab to show docstring: highlight and shift-tab: format\n#?zip()\n#%lsmagic\n# Suppress Future Warnings\nimport warnings\n#warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","e4d3f23d":"titanictrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(titanictrain.shape)\ntitanictrain[:5]","25615989":"titanictest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nprint(titanictest.shape)\ntitanictest[:5]","5a7a58f3":"titanictrain.info()","497dcd26":"titanictest.info()","65dce80a":"titanictrain.isnull().sum()","fdf29b0c":"titanictest.isnull().sum()","16c1c1e0":"def bar_chart(feature):\n    survived = titanictrain[titanictrain['Survived']==1][feature].value_counts()\n    dead = titanictrain[titanictrain['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","2ac22e75":"bar_chart('Sex')\nprint('The Chart show Women more likely survivied than Men')","bcc55aee":"bar_chart('Pclass')\nprint('The Chart show 1st class more likely survivied than \\\nother classes and 3rd class more likely dead than other classes')","bb2763d3":"bar_chart('SibSp')\nprint('The Chart shows a person aboarded with more than 2 siblings or spouse (3 to 8) \\\nor a person aboarded  without siblings or spouse (0) is more likely dead')","4f65cba4":"bar_chart('Parch')\nprint('This chart shows a person aboarded with more than 3 parents or children (4 to 6) or \\\na person aboarded alone (0) is more likely dead')","9073f89b":"bar_chart('Embarked')\nprint('The Chart shows a person aboarded from Q or a person aboarded from S more likely dead')","4780903b":"train_test_data = [titanictrain, titanictest] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False) #extract using regular expression","c399a2d2":"titanictrain['Title'].value_counts()","93e293c3":"titanictest['Title'].value_counts()","802aba48":"title_mapping = {\"Capt\": 'Man',\"Don\": 'Man',\"Major\": 'Man',\"Col\": 'Man',\"Rev\": 'Man',\"Dr\": 'Man',\"Sir\": 'Man',\"Mr\": 'Man',\"Jonkheer\": 'Man', \n                 \"Dona\": 'Woman',\"Countess\": 'Woman',\"Mme\": 'Woman',\"Mlle\": 'Woman',\"Ms\": 'Woman',\"Miss\": 'Miss',\"Lady\": 'Woman',\"Mrs\": 'Mrs',\n                 \"Master\": 'Boy' }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","383db8c3":"titanictrain.head()","fc7b8a42":"titanictrain['Ticket'].value_counts() #Check if there are common tickets","fc15b85a":"n_ticket = titanictrain.sort_values('Ticket') #sort dataset by tickets","10334d62":"df1 = pd.DataFrame(titanictrain.groupby('Ticket')['Survived'].count())\ndf1 = df1[df1['Survived']<2] #keep tickets that has only 1 count\ndf1.shape","b1ef7dc4":"df1 = df1.index.values.tolist() #to create a list of 1 count tickets","2667dc5a":"n_ticket = n_ticket[~n_ticket.Ticket.isin(df1)] #to remove 1 count tickets from n_ticket","a226e3bd":"tick_surv = n_ticket.groupby('Ticket')['Survived'].mean() #to find mean of survival of duplicate tickets","edb7f539":"tick_surv","2f09fa50":"for dataset in train_test_data:\n    dataset['Ticket'] = dataset['Ticket'].map(tick_surv)","8bfb5728":"for dataset in train_test_data:\n    dataset['Ticket'] = dataset['Ticket'].replace(np.nan, 0.5)","83a042df":"titanictrain.head()","fdc39067":"titanictest.head(400)","25b63b06":"for dataset in train_test_data:\n    dataset['Surname'] = dataset['Name'].str.extract('([A-Za-z]+)', expand=False) #extract using regular expression","f8dbfbb3":"titanictrain.head()","3182b98e":"n_surname = titanictrain.sort_values('Surname')","cec254ea":"df1 = pd.DataFrame(titanictrain.groupby('Surname')['Survived'].count())\ndf1 = df1[df1['Survived']<2]\ndf1.shape","bbcbba3b":"df1 = df1.index.values.tolist()","f23c8470":"n_surname = n_surname[~n_surname.Surname.isin(df1)]","33b252ed":"surname_surv = n_surname.groupby('Surname')['Survived'].mean()","5558ef1b":"for dataset in train_test_data:\n    dataset['Surname'] = dataset['Surname'].map(surname_surv)","4b5b1411":"for dataset in train_test_data:\n    dataset['Surname'] = dataset['Surname'].replace(np.nan, 0.5)","978a1837":"titanictrain.head()","e955848f":"# delete unnecessary feature from dataset\ntitanictrain.drop('Name', axis=1, inplace=True)\ntitanictest.drop('Name', axis=1, inplace=True)","415384a4":"# fill missing age with median age for each title (Mr, Mrs, Miss, Master, Others)\ntitanictrain[\"Age\"].fillna(titanictrain.groupby(\"Title\")[\"Age\"].transform(\"mean\"), inplace=True)\ntitanictest[\"Age\"].fillna(titanictest.groupby(\"Title\")[\"Age\"].transform(\"mean\"), inplace=True)","438b788d":"print('This chart shows that from ages 0 to 15 more likely to survive and ages 27 to 37 more likely to die')\n\nfacet = sns.FacetGrid(titanictrain, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, titanictrain['Age'].max()))\nfacet.add_legend()\nplt.show()","195e0e3a":"Pclass1 = titanictrain[titanictrain['Pclass']==1]['Embarked'].value_counts()\nPclass2 = titanictrain[titanictrain['Pclass']==2]['Embarked'].value_counts()\nPclass3 = titanictrain[titanictrain['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\nprint('This chart shows that regardless of class, more than 50% will embark from S. \\\nHence, we will use S to replace missing values')","49f7d2fb":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","233614dc":"# fill missing Fare with median fare for each Pclass\ntitanictrain[\"Fare\"].fillna(titanictrain.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntitanictest[\"Fare\"].fillna(titanictest.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntitanictrain.head()","5ac74760":"facet = sns.FacetGrid(titanictrain, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, titanictrain['Fare'].max()))\nfacet.add_legend()\n \nplt.show()","af100fc7":"titanictrain['Cabin'].fillna('No', inplace=True)\ntitanictrain['Cabin'].replace(regex=r'^((?!No).)*$',value='Yes',inplace=True)\ntitanictrain.head(2)","3566f1af":"titanictest['Cabin'].fillna('No', inplace=True)\ntitanictest['Cabin'].replace(regex=r'^((?!No).)*$',value='Yes',inplace=True)\ntitanictest.head(2)","6580fd39":"titanictrain[\"FamilySize\"] = titanictrain[\"SibSp\"] + titanictrain[\"Parch\"] +1\ntitanictest[\"FamilySize\"] = titanictest[\"SibSp\"] + titanictest[\"Parch\"] +1","27c8706f":"facet = sns.FacetGrid(titanictrain, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, titanictrain['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","28aa4b71":"titanictrain[\"Alone\"] = titanictrain[\"FamilySize\"]-1==0 \ntitanictest[\"Alone\"] = titanictest[\"FamilySize\"]-1==0 ","e29a73ab":"titanictrain.head()","ba427f7a":"family_mapping = { True: 1, False: 0 }\nfor dataset in train_test_data:\n    dataset['Alone'] = dataset['Alone'].map(family_mapping)","993fc391":"class_mapping = { 1: 'First_Class', 2: 'Second_Class', 3: 'Third_Class' }\nfor dataset in train_test_data:\n    dataset['Pclass'] = dataset['Pclass'].map(class_mapping)","988f2624":"titanictrain.head()","8d72da71":"titanictest.head()","54c4c905":"titanictrain = pd.get_dummies(titanictrain)\ntitanictrain.head(5)","6114ae5a":"titanictest = pd.get_dummies(titanictest)\ntitanictest.head(5)","7871519a":"corr = titanictrain.corr()\ncorr.sort_values([\"Survived\"], ascending = False, inplace = True)\nprint(corr.Survived)","d71e94f0":"titanictrain.to_csv(\"azuretitanictrain.csv\",index=False)\ntitanictest.to_csv(\"azuretitanictest.csv\",index=False)","ce1ebe56":"submission1 = pd.read_csv('azuretitanictrain.csv')\nsubmission1.head()","fd71c68d":"features_drop = ['SibSp','Parch','Surname','FamilySize','Sex_female','Sex_male','Cabin_Yes','Cabin_No','Embarked_C','Embarked_Q','Embarked_S']\ntitanictrain = titanictrain.drop(features_drop, axis=1)\ntitanictest = titanictest.drop(features_drop, axis=1)\ntitanictrain = titanictrain.drop(['PassengerId'], axis=1)","60d342c9":"train_data = titanictrain.drop('Survived', axis=1)\ntarget = titanictrain['Survived']\n\ntrain_data.shape, target.shape","4ddd8cfa":"train_data.head(10)","1d1b627c":"train_data.info()","cb6de9bb":"titanictest.head(10)","0fb9b80a":"titanictest.info()","511e12dc":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression","8930b2a2":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","96634c26":"print('Train using KNN')\nclf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","640697f1":"# kNN Score\nround(np.mean(score)*100, 2)","03c436cd":"print('Train using Decision Tree')\nclf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","70c5b097":"# decision tree Score\nround(np.mean(score)*100, 2)","e45974c7":"print('Train using Random Forest')\nclf = RandomForestClassifier(n_estimators=500,max_depth=6)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","e4307aad":"# Random Forest Score\nround(np.mean(score)*100, 2)","bdd8a8fe":"print('Train using Naive Bayes')\nclf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","a2f75173":"# Naive Bayes Score\nround(np.mean(score)*100, 2)","6d3cf18b":"print('Train using SVM')\nclf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","ad33a94f":"round(np.mean(score)*100,2)","93f7386c":"clf = MLPClassifier(activation='logistic',\n                    hidden_layer_sizes=(200, 80))\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","7b086de4":"round(np.mean(score)*100,2)","486d64ba":"clf = LogisticRegression()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","640d80bd":"round(np.mean(score)*100,2)","7a0096d0":"train_data.head()","66a4de50":"clf = RandomForestClassifier(n_estimators=500,max_depth=6)\nclf.fit(train_data, target)\n\ntest_data = titanictest.drop(\"PassengerId\", axis=1)\nprediction = clf.predict(test_data)","12816c1e":"submission = pd.DataFrame({\n        \"PassengerId\": titanictest[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('titanicsubmission.csv', index=False)","6ce998d1":"submission = pd.read_csv('titanicsubmission.csv')\nsubmission.head()","1f2d8dfe":"# Train Model","2ff0e7f5":"#### In this section, I will import the various models and prepare to split Data into train and test Sets using Cross fold validation","3b035ade":"### Transforming Ticket and Surname Columns buy grouping them into Survival Probabilities\n\n##### In this section, I will transfrom the Ticket and Surname column into survival probabilities by finding the mean value of duplicates.\n\n##### After researching background of passengers, I have found out that families and friends are most likely to have the same Surname and Ticket number. During the disaster, families and friends are more likely to look out for each other and move together as a group, they either survive or die as a group.  Hence, if most of the familiy or friends within a specific duplicate group survives, it is safe to assume that a member of that group in the test data is also likely to survive.\n\n##### During the sinking of Titanic, woman and children are given priority to rescue boats. Therefore, if a woman or children in the test data has the same Surname or ticket of a specific duplicate group, that person most likely shares the same fate as the majority of the group.  For example, if in the training data, a woman, her 1st child and 2nd child survives but 3rd child dies. The mean survive rate for this particular group of duplicates is 0.75 (3\/4 x 100%).  If there is a 4th child in the test data, discovered by matching Surnames and young age, that child is a assumed to survive as most people in that particular group of duplicate (family) survives, that child will be given a score of 0.75 (Score of 1 means survive, Score of 0 means dead). If a group of friends who have the same ticket number survives, a passenger in the test data who also have the same number is most likely to survive.\n\n##### This process is done by 1st counting the value of duplicates, sorting the datasets by duplicates, creating a new panda dataframe for duplicate groups and finding their mean chance of survival. For passengers who do not have duplicate surnames or ticket, they are given 0.5, 50% chance of dying or surviving. The surname and ticket column is subsequently mapped into the trainning data and test data.\n","ab65bcb4":"## Score Model and Evaluate Model:\n\n##### I have decided to use RandomForestClassifier as it has the highest cross validation score. The classifer will have n_esimators of 500 and max depth of 6. My best result with AzureML Studio, using the Two-Class Decision Jungle Algo is 81.34%, my best score using RandomForestClassifier model here is 80.38%. As compared to the stupidbaseline, my model is at least 10% more accurate. I have also discovered that if we simply predict all female survive, it will output about 76% accuracy. Hence, my model is more accurate as compared to models with simpler feature designs.","72698dc1":"### References\n\n##### https:\/\/sites.google.com\/site\/hermaidenvoyage\/titanic-luxury\n##### https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818\n##### https:\/\/www.kaggle.com\/jack89roberts\/titanic-using-ticket-groupings\n##### https:\/\/www.dummies.com\/education\/history\/suites-and-cabins-for-passengers-on-the-titanic\/\n##### https:\/\/qz.com\/321827\/women-and-children-first-is-a-maritime-disaster-myth-its-really-every-man-for-himself\/\n##### https:\/\/www.kaggle.com\/minsukheo\/titanic-solution-with-sklearn-classifiers\n##### https:\/\/owlcation.com\/humanities\/Titanic-April-1912-3rd-class-passengers-survivors-died-1st-2nd-ship-maiden-voyage-iceberg-sinking-sank\n##### https:\/\/autumnmccordckp.weebly.com\/tickets-and-accomodations.html","0d107f1d":"# Score and Evaluate Model","a28c9387":"#### Add missing age values\n\n##### In this secion, I used the mean age according to each title group and fill in missing data.","6f37715d":"#### Mapping PClass","659b7212":"##### The features for trainning and testing are: Age, Ticket, Fare, Alone, PClass and Title.\n##### The rational for choosing these features are: \n##### Title and Age will seperate passengers into Man, Woman, Boys and girls\n##### Ticket, Fare and PClass will show family\/friends relationship\n##### Alone will show which passengers are unlikely to receive help from others\n##### The premise of this feature set is base on the assumptions that Women and Children have higher chances of survival and passengers survive in groups of families or friends.","b105cd26":"# Exploratory Data Analysis","73697d0c":"##### Add missing Embarked Values","e4ffe7ec":"### Extracting Title from Name\n\n#### Extracting the title from the Name column will allow us to group the passengers into specific types of Man and woman, particular, younger males and females. The most common titles are Mr, Miss, Mrs and Master. The other titles are very rare, therefore I will group them along other male and females.\n\n#### Here, I have grouped the passengers into 4 groups, Man for older male passengers, Master for younger Male passengers, Mrs for older female and Ms younger female. I then Map these groups into the dataset. I will also use the average age of these groups to fill missing age data.","7020a25d":"### Combine Family Size and Create Alone Column\n\n##### SibSp and Parch is combine with each individual to find out the combine family size. An alone column is also created to determine if the passengers is travelling alone. A passenger travelling alone is more likely to die as women and children family groups are prioritised for survival.","3cf614b7":"#### Encode all the categorical variables and Find Correlation","b6f71ad1":"# Data Importing","46e807ee":"### Replace missing cabin values\n\n##### In this section, we will replace missing cabin values with no and yes for those with cabin. As there is a large amount of missing cabin data, I have group the dataset into 2 groups, those with and those without cabin. Background research has also showed that cabins are very expensive, it is likely that a sizeble group of passengers most likey 3rd class did not travel with cabins.","accf8957":"### Drop non-utilised feature\n\n##### In this section, I exported the datasets to Microsoft AzureML Studio for testing and analysis.\n\n##### Through extensive trial and error and logistical corelation, I have determined my best combination of features to achieve highest accuracy.\n\n##### I then drop the features that are not in use. This new dataset is later used to create predictive models in this notebook.","47eb4173":"# Data Preparation","452aecd4":"## Hello W0rld\n\n### Welcome to my frist kernel. My name is Lester and I am an aspiring Data Scientist.\n\n### I am new to programming and my first competition here is Kaggle's Titanic Challenge. It took me awhile to achieve 80% accuracy. Hence, I would like to share my research and work to other beginners who would find this useful.\n\n### I would like to thank @MinsukHeo, @Jack Roberts, @Chris Deotte and SP Prof Leong and others who have inspired and helped me in this kaggle challenge.","15ae6181":"## Train Model with different algorithms: ","6d41da45":"#### Add missing fare values\n\n##### In this section, I will use the median class fare according to each Pclass to fill missing data. Median is used as there are a wide range of prices within each class, I feel that median is more appropriate."}}