{"cell_type":{"a7c39bfb":"code","6c45bfe1":"code","52f6cbc6":"code","70d81321":"code","eb8d5a30":"code","c79c452c":"code","6910e8b6":"code","aad92c36":"code","6013d510":"code","b4aba9fd":"code","2a966ec7":"code","15a58694":"code","ecc2b821":"code","853d48d0":"code","f8be3da9":"code","39952ed2":"code","60e07e3f":"code","87d6fa3c":"code","5bb8114a":"code","2bc52906":"code","9ea40db9":"code","0b47952a":"code","501fe08d":"code","156e1843":"markdown","41f063c2":"markdown","eff1392b":"markdown","d0a0684e":"markdown","95bec5e0":"markdown","1edb2a71":"markdown","f483dfeb":"markdown","256fdc51":"markdown","85c5deb8":"markdown","895e6bca":"markdown","004818a1":"markdown","fc0418ab":"markdown"},"source":{"a7c39bfb":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.initializers import TruncatedNormal, RandomUniform, RandomNormal\nfrom keras.constraints import unit_norm, max_norm\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, precision_score, \\\n            recall_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","6c45bfe1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","52f6cbc6":"print ('\\nRealizando carga de dados....\\n')\nX_train = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv\", index_col='ID_code')\nX_test = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\", index_col='ID_code')\nprint ('\\nCarga de dados conclu\u00edda....\\n')\n\n\nprint ('\\nPreparando os conjuntos de treinamento e teste....\\n')\ny_train = X_train.target\nX_train.drop(['target'], axis=1, inplace=True)\nX_test_index = X_test.index\n","70d81321":"X_train.head()","eb8d5a30":"X_test.head()","c79c452c":"sum(X_train.columns.isnull())","6910e8b6":"sum(X_test.columns.isnull())","aad92c36":"sns.set()\nplt.figure(figsize=(30,3))\npos = 0\nfor col in X_train.columns.values[0:9]:\n        pos+=1\n        plt.subplot(190+pos)\n        plt.hist(X_train[col], density=True, bins=60)\n        plt.title(col)\n        plt.ylabel('Probability')\n        plt.xlabel('Data')\nprint ('\\nGr\u00e1ficos das primeiras colunas...\\n')\nplt.show()","6013d510":"std_threshold = 8\nbins = 6","b4aba9fd":"print ('\\nEstabelecendo as metricas para cria\u00e7\u00e3o e features...\\n')\n\n    \n#n_unicos_train = X_train.nunique()\n#n_unicos_test = X_test.nunique()\n    \nstd_train = X_train.std(axis=0)\nstd_test = X_test.std(axis=0)\n    \ncol_train_interested = np.where(std_train >= std_threshold)\ncol_test_interested = np.where(std_test >= std_threshold)\n    \ncol_train_for_bins = np.where(std_train < std_threshold)\ncol_test_for_bins = np.where(std_test < std_threshold)","2a966ec7":"print ('\\nDiscricionando colunas com valores pequenos de Desvio Padr\u00e3o...\\n')\n     \nX_train_e = X_train[std_train.index[(col_train_for_bins)]]\nX_test_e = X_test[std_test.index[(col_test_for_bins)]]\n    \ndiscretizer = preprocessing.KBinsDiscretizer(n_bins=bins, encode='onehot', strategy='uniform')\n    \ndiscretizer.fit(X_train_e)\nsparse_matrix = discretizer.transform(X_train_e)\ntrain_onehot = sparse_matrix.todense()\n\ndiscretizer.fit(X_test_e)\nsparse_matrix = discretizer.transform(X_test_e)\ntest_onehot = sparse_matrix.todense()\n\n\nprint ('\\nEliminando colunas com std < std_threshold ...\\n')\n    \nX_train = X_train[std_train.index[(col_train_interested)]]\nX_test = X_test[std_test.index[(col_test_interested)]]","15a58694":"X_train.head()","ecc2b821":"# N\u00famero de caracter\u00edsticas com valores de desvio padr\u00e3o maiores que std_threshold\nX_train.shape","853d48d0":"scaler =  preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f8be3da9":"X_train","39952ed2":"print ('\\nConcatenando as matrizes...\\n')   \nX_train = np.concatenate((X_train, train_onehot), axis=1)\nX_test = np.concatenate((X_test, test_onehot), axis=1)","60e07e3f":"X_train.shape\n","87d6fa3c":"input_len = X_train.shape[1]\n\n\n    \nmodel = Sequential([\n#Dense(input_len, input_shape=(input_len,)),\nDense(256, input_shape=(input_len,)),\nActivation('relu'),\nDropout(0.5),\nDense(128, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.5),\nDense(64, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.3),\nDense(32, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.1),\nDense(1),\nActivation('sigmoid'),\n])\n    \n    \nopt = optimizers.Adam(learning_rate=0.00005)    \n#opt = optimizers.Adam(learning_rate=0.001)\n    \n        \nmodel.compile(optimizer=opt,\n              loss= 'binary_crossentropy',\n              metrics=['accuracy'])\n    \nprint(model.summary())\n\n\nhistory = model.fit(X_train, y_train, \\\n                epochs= 120,\\\n                batch_size=1024,\\\n                validation_split=0.1,\\\n                )\n           ","5bb8114a":"sns.set()\nplt.figure(figsize=(15, 7))\n\nplt.subplot(141)\nplt.plot(history.history['accuracy'], label='ACC')\nplt.plot(history.history['val_accuracy'], label='Val_ACC')\nplt.xlabel('Epochs')\nplt.ylabel('Acur\u00e1cia')\nplt.legend()\n\n\nplt.subplot(142)\nplt.plot(history.history['loss'], label='LOSS')\nplt.plot(history.history['val_loss'], label='Val_LOSS')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","2bc52906":"input_len = X_train.shape[1]\n\n\nmodel = Sequential([\n#Dense(input_len, input_shape=(input_len,)),\nDense(256, input_shape=(input_len,)),\nActivation('relu'),\nDropout(0.5),\nDense(128, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.5),\nDense(64, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.3),\nDense(32, kernel_initializer='random_normal', activation = 'relu',  kernel_constraint=unit_norm()),\nDropout(0.1),\nDense(1),\nActivation('sigmoid'),\n])\n    \n    \nopt = optimizers.Adam(learning_rate=0.00005)    \n#opt = optimizers.Adam(learning_rate=0.001)\n    \n        \nmodel.compile(optimizer=opt,\n             loss= 'binary_crossentropy',\n              metrics=['accuracy'])\n    \nprint(model.summary())\n\n\nhistory = model.fit(X_train, y_train, \\\n                epochs= 40,\\\n                batch_size=1024,\\\n                #validation_split=0.1,\\\n                #class_weight = compute_class_weight('balanced', y_train.sum(), len(y_train))\n                )","9ea40db9":"X_test.shape","0b47952a":"predictions = model.predict(X_test)\n   \npred = predictions.reshape((200000,))\noutput = pd.DataFrame({'ID_code': X_test_index, 'target': pred})\noutput.to_csv('ANN_Santander_GColab_fet_Eng_TPU_3.csv', index=False)\nprint(\"O arquivo para envio ao Kaggle foi salvo com sucesso!\")","501fe08d":"predictions[0:10]","156e1843":"A partir dos gr\u00e1ficos acima podemos concluir algumas quest\u00f5es:\n\n* Embora o modelo continua crescente na sua performance nos sucessivos 'epochs' o \nmesmo n\u00e3o acontece nos dados de valida\u00e7\u00e3o.\n* O 'loss' deixa de cair em torno de 30 a 40 epochs\nAs m\u00e9tricas nos dados de valida\u00e7\u00e3o demonstram queda crescente ap\u00f3s 10 epochs.\n\nFica claro que mesmo adotando um conjunto de a\u00e7\u00f5es para evitar o 'overfitting' o modelo claramente n\u00e3o consegue generalizar quando submetido aos dados de valida\u00e7\u00e3o. As melhores abordagens para solu\u00e7\u00e3o deste problema publicado em diversas documenta\u00e7\u00f5es de Machine Learning sugerem que devemos trabalhar em duas vertentes:\n\n* Acrescentar mais dados de treinamento ao modelo.\n* Repensar a estrutura da rede\n\nAntes de aplicarmos algumas das solu\u00e7\u00f5es acima, vamos aplicar uma t\u00e9cnica muito comum e f\u00e1cil de realizar - o 'early stopping'. Pelos gr\u00e1ficos vericamos que o modelo tem boa performance at\u00e9 40 epochs. No c\u00f3digo abaixo realizaremos novamente o treinamento e submeteremos o arquivo ao Kaggle para confrontar o resultado. Desta vez sem a valida\u00e7\u00e3o cruzada, claro!\n\n","41f063c2":"#Estudo de caso do Kaggle - Banco Santander\n#Uma abordagem com Redes Neurais constru\u00eddo no Keras\n\n**Henrique Dias Marques - Analista Master - Petrobras**\n<br\/>\n**Em meio a Pandemia de 2020**\n\n\nNesta competi\u00e7\u00e3o hospedada no Kaggle, o Banco Santander nos prop\u00f4s um problema muito comum em qualquer atividade comercial, cujo prop\u00f3sito \u00e9 avaliar os potenciais clientes nas ofertas de produtos e servi\u00e7os. Especificamente o banco apresenta 200 caracter\u00edsticas diferentes para cada cliente as quais ser\u00e3o consideradas para averiguar se os mesmos ir\u00e3o no futuro realizar determinada transa\u00e7\u00e3o com o banco. Sempre visando dar aten\u00e7\u00e3o aos potenciais clientes, a estrat\u00e9gia do banco foi procurar a comunidade de Cientistas da Dados do mundo para estabelecer o melhor modelo que respondesse esta quest\u00e3o.\n\nA base de dados \u00e9 bem extensa pois possui 200 mil exemplos de clientes para a constru\u00e7\u00e3o do modelo na fase de treinamento e apresenta a mesma quantidade para realizarmos os testes nos modelos. O problema \u00e9 de classifica\u00e7\u00e3o bin\u00e1ria [0, 1] com aprendizagem supervisionada, onde o Banco apresenta o resultado do que aconteceu com 200 mil clientes.\n\nUm aspecto interessante deste caso \u00e9 que as 200 caracter\u00edsticas dos clientes, apresentadas pelo Banco Santander, n\u00e3o possui a informa\u00e7\u00e3o de seu metadado. N\u00e3o sabemos o que significam as 200 caracter\u00edsticas! Isto mostra o qu\u00e3o interessante s\u00e3o os algor\u00edtmos de Machine Learning, capazes de lidar e aprender com n\u00fameros que expressam informa\u00e7\u00f5es de uma natureza s\u00f3 conhecida pelo seu resultado final: [1] - o cliente realizou a transa\u00e7\u00e3o no futuro e [0] - o cliente desistiu da transa\u00e7\u00e3o.","eff1392b":"#Normaliza\u00e7\u00e3o.\nA normaliza\u00e7\u00e3o \u00e9 um item essencial - para a performance da execu\u00e7\u00e3o do treinamento - quando trabalhamos com Redes Neurais e tamb\u00e9m com outros algor\u00edtmos de Machine Learning que utilizam fun\u00e7\u00f5es sigmoidais ou regress\u00f5es log\u00edsticas. Ela tamb\u00e9m visa permitir que determinadas caracter\u00edsticas dos dados n\u00e3o se sobreponham \u00e0s outras.\n\nPara facilitar nosso trabalho vamos utilizar bibliotecas do SciKit Learn que realizam tais opera\u00e7\u00f5es com facilidades. No c\u00f3digo abaixo utlizamos o m\u00e9todo StandardScaler do m\u00f3dulo preprocessing para este fim. \u00c9 importante notar que a sa\u00edda destas fun\u00e7\u00f5es retornam um conjunto de arrays (numpy arrays) e n\u00e3o mais dataframes.\n\n","d0a0684e":"#Aplicando o primeiro modelo com \"cross-validation\"\nNeste exerc\u00edcio inicial vamos aplicar uma estrutura de rede que estabeleci ap\u00f3s sucessivos testes para defini\u00e7\u00e3o dos melhores hyerpar\u00e2metros da Rede Neural. Utilizei no modelo as bibliotecas do Keras - uma poderosa ferramenta para constru\u00e7\u00e3o de Redes Neurais que executa instru\u00e7\u00f5es sobre o TensorFlow. O seu modelo sequencial \u00e9 mais simples de codifica\u00e7\u00e3o al\u00e9m do f\u00e1cil entendimento das diversas camadas que foram escolhidas, portanto, optei por esta abordagem. Para o entedimento deste c\u00f3digo pressuponho que o leitor j\u00e1 possui toda a base te\u00f3rica de constru\u00e7\u00e3o de Redes Neurais. O otimizador escolhido foi o Adam, depois que realizei sucessivos testes com outros otimizadores, dentre eles o SGD e RMSprop.\n\nA solu\u00e7\u00e3o neste momento tamb\u00e9m ser\u00e1 realizada com o uso de valida\u00e7\u00e3o cruzada - par\u00e2metro 'validation_split'. Este processo de divis\u00e3o de dados de treinamento \u00e9 essencial para avaliarmos a performance do modelo. \n\nO modelo utiliza duas t\u00e9cnicas para regulariza\u00e7\u00e3o que \u00e9 o \"dropout\" e o \"kernel_constraint\", pois verificamos nos sucessivos testes como facilmente nos defrontamos com \"overfitting\" durante a fase de treinamento. Verifique que no par\u00e2metro 'metrics' inserimos nossas fun\u00e7\u00e3o 'Roc_auc' e 'average_precision' citadas no inicio deste artigo. Com estas fun\u00e7\u00f5es \u00e9 poss\u00edvel availarmos o desempenho do processo de aprendizado ao longo da execu\u00e7\u00e3o do c\u00f3digo. Os resultados ficam armazenados no callback - history.","95bec5e0":"<br \/>\n\u00c9 importante verificarmos se h\u00e1 dados nulos ou ausentes nestes conjuntos. Os c\u00f3digos abaixo avaliam isto e conclui que n\u00e3o h\u00e1. Tamb\u00e9m logo a seguir vou \"plotar\" alguns gr\u00e1ficos que representam algumas das 200 caracter\u00edsticas dos dados de treinamento e que depois voc\u00ea tamb\u00e9m poder\u00e1 verificar nos dados de teste. \u00c9 f\u00e1cil notar que todas elas possuem uma distribui\u00e7\u00e3o normal. \n\n<br \/>","1edb2a71":"Os gr\u00e1ficos a seguir nos d\u00e3o uma melhor avalia\u00e7\u00e3o do que ocorreu durante o treinamento. Temos as vis\u00f5es de acur\u00e1cia e do 'loss' para as duas sess\u00f5es dos dados: treinamento e valida\u00e7\u00e3o.","f483dfeb":"#Carga de dados\n\nO c\u00f3digo abaixo realiza a carga dos dados em dataframes - Pandas - para uso no treinamento e predi\u00e7\u00e3o. Cria tamb\u00e9m vari\u00e1veis que ser\u00e3o utilizadas na constru\u00e7\u00e3o da solu\u00e7\u00e3o. O X_test_index se faz necess\u00e1rio porque na cria\u00e7\u00e3o do arquivo de submiss\u00e3o ao Kaggle ele ser\u00e1 usado como \u00edndice no arquivo.\n\nPara exercitar este exemplo, substitua o caminho da pasta para sua condi\u00e7\u00e3o espec\u00edfica.","256fdc51":"# Envio do arquivo ao kaggle.\nVamos agora aplicar o modelo treinado na base de teste e avaliar como ser\u00e1 o seu resultado. O c\u00f3digo abaixo envia as predi\u00e7\u00f5es do modelo de acordo com o formado estabelecido nas competi\u00e7\u00f5es.\n\n","85c5deb8":"#Carregando as bibliotecas\n\nVamos iniciar com a carga de bibliotecas necess\u00e1rias para o desenvolvimento do modelo. \u00c0 medida que o c\u00f3digo for sendo explicado voc\u00ea entender\u00e1 a raz\u00e3o das bibliotecas aqui importadas.","895e6bca":"Adicionando caracter\u00edsticas que foram discretizadas. J\u00e1 est\u00e3o no formato one-hot ","004818a1":"# Conclus\u00e3o\nA aplica\u00e7\u00e3o de uma Rede Neural para a solu\u00e7\u00e3o proposta pelo Banco Santander apresenta uma boa resposta para os dados originais como foram fornecidos. A performance ainda est\u00e1 distante das melhores resultados apresentados na competi\u00e7\u00e3o que se deu no ano passado, mas que utilizaram outras abordagens de Machine Learning - como o LightGBM que \u00e9 bastante usado como algoritmo venceder pelas equipes que participam do Kaggle - e um extenso trabalho de prepara\u00e7\u00e3o dos dados. Vale salientar que neste exemplo n\u00e3o adotei nenhuma destas abordagem, e portanto, poderemos melhorar ainda mais esta performance com a utiliza\u00e7\u00e3o de Feature Engineearing ou mesmo modificando a estrutura da Rede Neural.\n\nNa pr\u00f3xima publica\u00e7\u00e3o sobre esta competi\u00e7\u00e3o irei mostrar como um pouco de Feature Engeeneearing sobre os dados de Treinamento e Testes ser\u00e1 poss\u00edvel obter um ganho na performance.","fc0418ab":"#Feature Engineering\n<br\/>\nNesta sess\u00e3o vou colocar um insight que tive baseado na ideia de que dados com desvios padr\u00f5es pequenos possuir\u00e3o pouca interfer\u00eancia nos resultados, portanto ser\u00e1 melhor termos estes dados como valores ordinais. ent\u00e3o a ideia ser\u00e1 discretiz\u00e1-los e transform\u00e1-los em colunas one-hot. O std_threshold \u00e9 o desvio padr\u00e3o m\u00e1ximo que aceitaremos em uma catacter\u00edstica. Portanto todas as colunas com desvio padr\u00e3o menor que std_threshold ser\u00e3o convertidos em valores discretos de acordo com o n\u00famero de bins. \n\nOs valores discretos s\u00e3o transformados em caracter\u00edsticas \"dummies\" no padr\u00e3o one-hot(veja par\u00e2metro 'discretizer')\n\n"}}