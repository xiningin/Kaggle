{"cell_type":{"61d5d214":"code","813c58c8":"code","77684414":"code","6399f5be":"code","68b58199":"code","09565570":"code","9e33e1ff":"code","34c7aa42":"code","9f9993ce":"code","ec20ed6c":"markdown","337c931d":"markdown","d4ededd9":"markdown","01973ef3":"markdown","0f19324e":"markdown","46c69f06":"markdown","2623c0df":"markdown","64dbc06a":"markdown","37f10f8f":"markdown","8ce10467":"markdown"},"source":{"61d5d214":"import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","813c58c8":"# activation functions and derivatives\ndef ReLU(x, deriv=False):\n    if deriv:\n        return 1. * (x > 0)\n    else:\n        return x * (x > 0)\n\ndef LReLU(x, deriv=False, alpha=0.001):\n    if deriv:\n        return 1. * (x >= 0) + alpha * (x < 0)\n    else:\n        return x * (x >= 0) + (alpha * x) * (x < 0) \n    \ndef LReLU6(x, deriv=False, alpha=0.1):\n    x = np.clip(x, -6, 6)\n    if deriv:\n        return 1. * (x >= 0) + alpha * (x < 0)\n    else:\n        return x * (x >= 0) + (alpha * x) * (x < 0) \n\ndef sigmoid(x, deriv=False):\n    if deriv:\n        return sigmoid(x) * (1 - sigmoid(x))\n    else:\n        x = np.clip(x, -700, 700)\n        return 1 \/ (1 + np.exp(-x))\n\ndef tanh(x, deriv=False):\n    if deriv:\n        return 1.0 - (np.tanh(x) ** 2)\n    else:\n        return np.tanh(x)\n\ndef linear(x, deriv=False, alpha=1.0):\n    if deriv:\n        return alpha\n    else:\n        return alpha * x","77684414":"def split_to_minibatch(z, batch_size=32):\n    z_length = len(z)\n    current_batch_size = batch_size\n    batch_split_points = []\n    while current_batch_size < z_length:\n        batch_split_points.append(current_batch_size)\n        current_batch_size += batch_size\n    return np.split(z, batch_split_points)\n\ndef shuffle_split(X, y, test_split=0.2):\n    X, y = shuffle(X, y)\n    return train_test_split(X, y, test_size=test_split)","6399f5be":"class NNN(object):\n    \"\"\"N-layered neural network\"\"\"\n    def __init__(self, inputs, weights, outputs, alpha):\n        self.inputs = inputs\n        self.outputs = outputs\n        self._ALPHA = alpha\n        self._num_of_weights = len(weights)\n        self._LAYER_DEFS = {}\n        self.WEIGHT_DATA = {}\n        self.BIAS_DATA = {}\n        self.LAYER_FUNC = {}\n        self.LAYERS = {}\n        for i in range(self._num_of_weights):\n            #(in, out, nonlin)\n            self._LAYER_DEFS[i] = {'in': weights[i][0],\n                                   'out': weights[i][1],\n                                   'nonlin': weights[i][2]}\n        print(self._LAYER_DEFS)\n        self._init_layers()\n    \n    def _init_layers(self):\n        for i in range(self._num_of_weights):\n            _in = self._LAYER_DEFS[i]['in']\n            _out = self._LAYER_DEFS[i]['out']\n            _nonlin = self._LAYER_DEFS[i]['nonlin']\n            self.WEIGHT_DATA[i] = np.random.randn(_in, _out)\n            self.BIAS_DATA[i] = np.full((1,_out), 1.0)\n            self.LAYER_FUNC[i] = _nonlin\n    \n    def forward_layer(self, prev_layer, next_layer, bias_values, nonlin):\n        \"\"\"Does the actual calcs between layers :)\"\"\"\n        ret = nonlin(np.dot(prev_layer, next_layer)+bias_values)\n        return ret\n\n    def backward_layer(self, w, z, act, prev_delta):\n        w_error = prev_delta.dot(w.T)\n        cur_delta = w_error * act(z, deriv=True)\n        return cur_delta\n\n    def backprop_sgd(self, w, z, act, prev_delta, bias):\n        cur_delta = self.backward_layer(w, z, act, prev_delta)\n        w += self._ALPHA * z.T.dot(prev_delta)\n        bias += -self._ALPHA * np.sum(np.asarray(prev_delta), axis=0, keepdims=True)\n        return cur_delta\n\n    def loss_absolute(y, y_hat):\n        return y - y_hat\n\n    def fit(self, X_in, Y_in, train_loops=100, batch_size=32, error_metric=None):\n        error_history = []\n        for j in range(train_loops):\n            X_in, Y_in = shuffle(X_in, Y_in)\n            x_batches = split_to_minibatch(X_in, batch_size)\n            y_batches = split_to_minibatch(Y_in, batch_size)\n            epoch_error = 0\n            for x, y in zip(x_batches, y_batches):\n                # set up layers\n                prev_layer = self.LAYERS[0] = x\n                for i in range(self._num_of_weights):\n                    current_layer = self.forward_layer(prev_layer, \n                                                       self.WEIGHT_DATA[i],\n                                                       self.BIAS_DATA[i], \n                                                       self.LAYER_FUNC[i])\n                    self.LAYERS[i+1] = current_layer\n                    prev_layer = current_layer\n                last_layer = current_layer\n    \n                # calculate errors\n                error = y - last_layer\n                if error_metric:\n                    epoch_error = (epoch_error + error_metric(y, last_layer)) \/ 2\n                else:\n                    epoch_error += np.average(abs(error))\n                nonlin = self.LAYER_FUNC[self._num_of_weights - 1]\n                delta = error * nonlin(last_layer, deriv=True)\n    \n                prev_delta = delta\n                for i in reversed(range(self._num_of_weights)):\n                    prev_delta = self.backprop_sgd(self.WEIGHT_DATA[i],\n                                                   self.LAYERS[i], \n                                                   self.LAYER_FUNC[i], \n                                                   prev_delta, \n                                                   self.BIAS_DATA[i])\n            \n            error_history.append(epoch_error)\n            if (j % (train_loops\/10)) == 0:\n                print(\"loop: {}\".format(j))\n                print(\"Guess (rounded): \")\n                print(np.round(last_layer[0], 3))\n                print(\"Actual: \")\n                print(np.round(y[0], 3))\n        return error_history\n        \n    def evaluate(self, x, y, loss_function):\n        # set up layers\n        prev_layer = self.LAYERS[0] = x\n        for i in range(self._num_of_weights):\n            current_layer = self.forward_layer(prev_layer, \n                                               self.WEIGHT_DATA[i],\n                                               self.BIAS_DATA[i], \n                                               self.LAYER_FUNC[i])\n            self.LAYERS[i+1] = current_layer\n            prev_layer = current_layer\n        last_layer = current_layer\n        return loss_function(y, last_layer)","68b58199":"# load the data\ntrain_df = pd.read_csv('..\/input\/train.csv')\n\n\n# clean the data\n# borrowed from other kagglers: https:\/\/www.kaggle.com\/hmendonca\/testing-engineered-features-lb-1-42\n# Find and drop duplicate rows\nt = train_df.iloc[:,2:].duplicated(keep=False)\nduplicated_indices = t[t].index.values\nprint(\"Removed {} duplicated rows: {}\".format(len(duplicated_indices), duplicated_indices))\ntrain_df.iat[duplicated_indices[0], 1] = np.expm1(np.log1p(train_df.target.loc[duplicated_indices]).mean()) # keep and update first with log mean\ntrain_df.drop(duplicated_indices[1:], inplace=True) # drop remaining\n\n# Columns to drop because there is no variation in training set\nzero_std_cols = train_df.drop(\"ID\", axis=1).columns[train_df.std() == 0]\ntrain_df.drop(zero_std_cols, axis=1, inplace=True)\nprint(\"Removed {} constant columns\".format(len(zero_std_cols)))\n\n\n# log transform the data\ntrain_df = train_df.drop(['ID'], axis=1)\ntrain_df = np.log1p(train_df)\n\n\n# split the data\nX = train_df.drop(['target'], axis=1)\nY = train_df[['target']]\n# Y = np.array(Y).reshape(len(Y))\n\n# we will use only a subset of the train data so this kaggle kerenel will run quicker...\n#from sklearn.model_selection import train_test_split\n#X, _, Y, __ = train_test_split(X, Y, test_size=0.33)\nXt, Xv, Yt, Yv = shuffle_split(X, Y)","09565570":"# define the loss function and evaluate the network\ndef rmsle(y, y_hat):\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y_hat), 2)))\n\ndef rmse(y, y_hat):\n    return np.sqrt(np.mean(np.power(y-y_hat, 2)))","9e33e1ff":"# define the layers\ni_input = Xt.shape[1]\ni_out = 1 # regression, single continuos variable\n\nweights = ((i_input, 64, LReLU6),\n           (64, 32, LReLU6),\n           (32, 16, LReLU6),\n           (16, 8, LReLU6),\n           (8, i_out, linear))\n\n# init the network\nalpha = 0.0001\ntrain_epochs = 30\nbatch_size = 16\n\nnn = NNN(i_input, weights, i_out, alpha)\n\n# fit the network\nerror_history = nn.fit(np.array(Xt), np.array(Yt), train_epochs, batch_size, error_metric=rmse)","34c7aa42":"# evaluate the network\nval_loss = nn.evaluate(np.array(Xv), np.array(Yv), rmse)\nprint('validation loss: {}'.format(val_loss))","9f9993ce":"def plot_error_history(error_history):\n    from matplotlib import pyplot as plt\n    epochs = [i for i in range(len(error_history))]\n    plt.plot(epochs, error_history)\n    plt.title('Loss History')\n    plt.xlabel('Epochs')\n    plt.ylabel('RMSE loss')\n    plt.grid(True)\n    plt.figure(figsize=(10, 6))\n    plt.show()\n\nplot_error_history(error_history)","ec20ed6c":"This particular challenge uses the rmsle loss function, lets define a version here so we can see what sort of performance this network is giving us.","337c931d":"Time to setup up the hyperparameters for the network and train it!","d4ededd9":"Let's prepare our data using some methods taken from other Kaggler's notebooks.","01973ef3":"Now we can validate the neural network using the loss function we defined previously! We will use just plain RMSE instead of RMSLE becuase the graph of loss over epochs looks like a more typical lerning curve.","0f19324e":"It is finally time to define the neural network class, this is over kill really considering all we are really doing is a bit of matrix multiplication, but I find this to be more reusable. (Even if it is a bit of a mess hahaha...)","46c69f06":"This brings us to the end of playing around with neural networks, please feel free to reuse the above code as you see fit, hack to fit your needs! \n\nps. one day i may tidy it up and release it as a small library for toy datasets and learning how neural networks work, until then, it can live here in kaggle!","2623c0df":"We can also define a simple function for plotting the loss vs epochs","64dbc06a":"I'm sure we've all been grinding away at this for the last few weeks, now lets just have some fun implementing some simple toy ANN's for regressions. We'll use numpy because I'm sure most of us are familiar with it so it shouldn't be too hard to see how it all works. \n\nLets start by importing some librarys...","37f10f8f":"Activation functions are sometimes also referred to as non linear functions, they allow our neural network to exibit behaviour above and beyond what would be possible with purely linear functions. we can define a few simple functions using numpy to vectorise them and ensure they run reasonably fast.","8ce10467":"We're going to need some simple fucntions to split our data into batches. One for the mini batches inside of the training loop, and another to just set up the data set with a shuffle first. \n\nThe shuffle is actually not needed becuase the sklearn function already shuffles the data before it splits, however I added it here to show people who arent familiar with with train_test_split that you should actually shuffle your data before you split it!"}}