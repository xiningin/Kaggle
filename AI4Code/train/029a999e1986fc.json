{"cell_type":{"f9458530":"code","67aeb6a7":"code","43a5abad":"code","75dc107f":"code","72f73ea4":"code","d709b06e":"code","ea1f8f39":"code","8bc7671e":"code","f553d531":"code","1fe73f02":"code","3bd3a7f4":"code","3db2cda7":"code","84abfbcc":"code","0702e32b":"code","59359f67":"code","69343c16":"code","908b4a20":"code","ad20c415":"code","4d39d028":"code","be045e04":"code","8b6778d3":"code","a2f30abc":"code","b89e609d":"code","669f1693":"code","8e6fcf73":"code","29e9fc4f":"code","8ede9121":"code","a7772deb":"code","6fe447ea":"code","65e5f195":"code","9a71b48b":"code","4c7ff7c3":"code","18cb0edb":"code","a28914f4":"code","284c7ed4":"code","6ed30f72":"code","2398bfbf":"markdown","40dad935":"markdown","0e1ca986":"markdown","d933d7fd":"markdown","53dc72cc":"markdown","c4321bd4":"markdown","bb6b6eb9":"markdown","18e99510":"markdown","881ecced":"markdown","1ed519e4":"markdown","c388a3b2":"markdown"},"source":{"f9458530":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67aeb6a7":"import matplotlib \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\n\n\nfrom sklearn.linear_model import LinearRegression,SGDClassifier, RidgeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","43a5abad":"df = pd.read_csv('\/kaggle\/input\/wine-quality\/winequalityN.csv')\ndf.head()","75dc107f":"## check Nan value\nfor i in df.columns:\n    print (i+\": \"+str(df[i].isna().sum()))","72f73ea4":"# There are some Nan values are present in the data, however we can drop these value or we can fill these Nan value with alternate option like median,Mode.\n# There are other methods also used to define Nan values, I will go with median you can try with constant and check how it will perform.\ndf['pH'] = df['pH'].fillna(df['pH'].median())\ndf['sulphates'] = df['sulphates'].fillna(df['sulphates'].median())\ndf['chlorides'] = df['chlorides'].fillna(df['chlorides'].median())\ndf['residual sugar'] = df['residual sugar'].fillna(df['residual sugar'].median())\ndf['citric acid'] = df['citric acid'].fillna(df['citric acid'].median())\ndf['volatile acidity'] = df['volatile acidity'].fillna(df['volatile acidity'].median())\ndf['fixed acidity'] = df['fixed acidity'].fillna(df['fixed acidity'].median())","d709b06e":"# Now as we can see quality score is varies in between 3 to 8, as we know low quality wine having low score and high quality wine having high score accordingly we will going to assign class to score and try to predict classes.\ndef values(x):\n    if x <= 5:\n        x = 'low'\n    elif x >5 and x <7:\n        x = 'medium'\n    else:\n        x = 'high'\n    \n    return(x)\ndf['level'] = df['quality'].apply(lambda x: values(x))","ea1f8f39":"# For normalizing data we can use either nominal or ordinal encoding, I checked with both however ordinal encoding gives better result.\nlabel = LabelEncoder()\n\nquality_score  = label.fit_transform(df['level'])\n\nprint(quality_score)\nprint((label.classes_))","8bc7671e":"#correlation gives us relation between each varibale. how much each variable is contributing.\n#correlation shows how each feature is dependent on other. from this will find out colinearity between each function, if colinearity is more than 0.5 that leads to problem however we can avoid that problem by dropping feature which highly correlated to each feature.\ncorrelation = df.corr()","f553d531":"plt.figure(figsize = (15,8))\nsns.heatmap(correlation,annot = True, cmap = 'Blues')","1fe73f02":"df.hist(bins=10,figsize=(15,12))\nplt.show()","3bd3a7f4":"df.head()","3db2cda7":"# seaborn packages gives us nice visualitons where in barplot helps us to predict how much each classes having alcohol.\nplt.figure(figsize = (15,8))\nsns.catplot('level','alcohol',data = df,kind = 'bar',hue = 'type',col = 'type',palette = 'cividis')","84abfbcc":"#Again will check how much sulphates is used in each classes and which class had used more sulphate.\nplt.figure(figsize = (15,8))\nax = sns.barplot(x=\"level\", y=\"sulphates\", data=df,palette = 'cubehelix')","0702e32b":"ax = sns.countplot(x=\"level\", data=df, palette=\"Set3\")","59359f67":"plt.figure(figsize=(10,10))\nsns.displot(df.alcohol, color=\"red\", label=\"level\", kde= True)\nplt.legend()","69343c16":"plt.figure(figsize=(10,10))\nsns.displot(x= 'citric acid', data = df,color=\"red\", label=\"citric acid\", kde= True)\nplt.legend()","908b4a20":"# To deal with outlier we will use IQR method.\nplt.figure(figsize = (20,8))\nsns.boxplot(y = 'alcohol',data = df,palette = 'gist_ncar')","ad20c415":"# Deal with Outlier with the help of IQR method.\nQ1 = df['alcohol'].quantile(0.25)\nQ3 = df['alcohol'].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (df['alcohol'] >= Q1 - 1.5 * IQR) & (df['alcohol']<= Q3 + 1.5 *IQR)\ntrain2 = df.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(df) - len(train2))\/len(df))*100))","4d39d028":"#In introduction part we already discussed about type of wine is present in data, so will use dummuy encoding method for converting categorical feature into numerical.\ndf['type'] = pd.get_dummies(df['type'],drop_first = True)","be045e04":"ax = sns.countplot(x=\"type\", data=df, palette=\"Set3\")","8b6778d3":"df.head()","a2f30abc":"x = df.iloc[:,:-2]\nx.head()","b89e609d":"standard = StandardScaler()\n\nstd_x = standard.fit_transform(x)","669f1693":"# While splitting data I used train-test-split method to split data.\n# data split for validation purpose, model that we will build we have to validate with test data.\nx_train,x_test,y_train,y_test = train_test_split(std_x,quality_score,test_size = 0.20,random_state = 50)\n\n\nprint(\"Training data:{}\".format(x_train.shape))\nprint(\"Test data:{}\".format(x_test.shape))","8e6fcf73":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\nclf.fit(x_train,y_train)\ny_predicted = clf.predict(x_test)\nscore = clf.score(x_test,y_test)\n\nprint(score)\n#results.append(score)","29e9fc4f":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(x_train,y_train)\nscore = clf.score(x_test,y_test)\ny_predicted = clf.predict(x_test)\nscore","8ede9121":"from sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","a7772deb":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","6fe447ea":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()","65e5f195":"# Random search of parameters, using 3 fold cross validation, \n# search across 50 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='accuracy', n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = 1)\nrf_random.fit(x_train,y_train)","9a71b48b":"rf_random.best_params_","4c7ff7c3":"clf = RandomForestClassifier(n_estimators = 200,min_samples_split =  5,min_samples_leaf =  1,max_features =  'auto',max_depth = 20)\n\nclf.fit(x_train,y_train)\nscore = clf.score(x_test,y_test)\ny_predicted = clf.predict(x_test)","18cb0edb":"print(score)","a28914f4":"cnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","284c7ed4":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","6ed30f72":"classes = df['level'].value_counts()\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","2398bfbf":"**Steps involved in Model Building**\n\n* Setting up features and target\n* Build a pipeline of standard scalar and model for classification algorithim.\n* Fit the models on training data.\n* Get Accuracy of the model and check how it will perform on each algorithm.\n* Use hyperparameter and check accuracy score.","40dad935":"\n<h1 style='background:#c2abab; border:0; color:black'><center>VISUALIZE DATA<\/center><\/h1> ","0e1ca986":"**<span style=\"color:#65634a;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#65634a;\"> If you have any suggestions or questions, I am all ears!<\/span>**\n\n**<span style=\"color:#65634a;\">Best Wishes!<\/span>**","d933d7fd":"\n<h1 style='background:#c2abab; border:0; color:black'><center>IMPORT DATA<\/center><\/h1> \n","53dc72cc":"<h1 style='background:#c2abab; border:0; color:black'><center>TRAIN-TEST DATA<\/center><\/h1> ","c4321bd4":"<h1 style='background:#c2abab; border:0; color:black'><center>INDEX<\/center><\/h1>\n\n- Data is taken from kaggle (https:\/\/www.kaggle.com\/rajyellow46\/wine-quality).\n- Data contains two types of wine that is - White and Red wine and it also gives remaining parameter that helps us to find data quality.\n- For both type of wine, some important parameters are given and using these parameter, we will going to build model that helps us to find Quality of wine.\n- At The end will try with different algorithm and check how it will perform, I would request you to Try with other algorithm and play with parameters and check how it will give you better accuracy.\n\n[1. IMPORTING LIBRARIES](#1)     \n[2. IMPORTING DATA](#2)  \n[3. DATA VISUALIZATION](#3)\n[4. DATA PREPROCESSING](#4)  \n[5. TRAIN DATA](#5)  \n[6. MODEL BUILDING](#6)       \n[7. CONFUSION MATRIX](#7)       \n[8. CONCLUSION](#8)","bb6b6eb9":"<h1 style='background:#c2abab; border:0; color:black'><center>PREPROCESSING DATA<\/center><\/h1> ","18e99510":"<h1 style='background:#c2abab; border:0; color:black'><center>BUILT MODEL<\/center><\/h1> ","881ecced":"<h1 style='background:#c2abab; border:0; color:black'><center>CONCLUSION<\/center><\/h1> \n\n- Data perform excellent on random forest model and also I used hyperparameter tuning method to increase accuracy.\n- Before random forest,i check with decision tree and also I used another algorithm but I got less accuracy as compared to random forest.\n- for Increasing accuracy I Took precaution while dealing with outliers, normalizing data, validating data,and while building model.\n- Above parameter will help you to get excellent accuracy by playing with it.","1ed519e4":"<h1 style='background:#c2abab; border:0; color:black'><center>CONFUSION MATRIX<\/center><\/h1> ","c388a3b2":"\n<h1 style='background:#c2abab; border:0; color:black'><center>LIBRARIES<\/center><\/h1> \n"}}