{"cell_type":{"4753f20f":"code","b934c874":"code","de045406":"code","65d46a88":"code","f8899d17":"code","ce7fa339":"code","64e9385b":"code","7299d7d0":"code","8fb0a4bf":"code","7f3b94a1":"code","0dc59843":"code","1b9c9119":"code","18c6e01a":"code","13e8ee46":"code","4996d39a":"code","515a4923":"code","69f8e004":"code","56b67fde":"code","ee314526":"code","f5993c48":"code","f7a25de4":"code","af0469f5":"code","7ca41f3f":"code","6c04defc":"code","f73a3f6f":"code","e64bbade":"code","70dbd1f7":"code","5829d4c8":"code","eb5dc3d9":"code","3f949acb":"code","ea2b25dd":"code","031c56a8":"code","24ab9c5f":"code","c52d2e92":"code","7029e1be":"code","0d5d4c31":"code","4ff8c1dd":"code","a379a2f7":"code","f16f7873":"code","2d6163ac":"code","83e6dbe4":"code","605b6e1f":"code","db502f70":"code","d71d9165":"code","008fcd09":"markdown","aa4ee5c8":"markdown","191633b4":"markdown","555187de":"markdown","932d6c37":"markdown","7bf7de69":"markdown","4969f5e8":"markdown","be17e88a":"markdown"},"source":{"4753f20f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b934c874":"import pandas_datareader as pdr","de045406":"df = pdr.get_data_tiingo('AAPL',api_key='')","65d46a88":"df.head()","f8899d17":"df.to_csv('AAPL.csv')","ce7fa339":"df = pd.read_csv('AAPL.csv')","64e9385b":"df.head()","7299d7d0":"df.tail()","8fb0a4bf":"df1 = df.reset_index()['close']","7f3b94a1":"df1.shape","0dc59843":"import matplotlib.pyplot as plt","1b9c9119":"df1.head()","18c6e01a":"plt.plot(df1)","13e8ee46":"df1","4996d39a":"from sklearn.preprocessing import MinMaxScaler","515a4923":"scaler = MinMaxScaler(feature_range=(0,1))","69f8e004":"df1=scaler.fit_transform(np.array(df1).reshape(-1,1))","56b67fde":"df1","ee314526":"train_size = int(len(df1)*0.65)\ntrain_size","f5993c48":"test_size=len(df1)-train_size\ntest_size","f7a25de4":"train_data,test_data=df1[0:train_size,:],df1[train_size:len(df1),:1]","af0469f5":"# convert an array of values into a dataset matrix\nimport numpy as np\ndef create_dataset(dataset, time_step=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-time_step-1):\n        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n        dataX.append(a)\n        dataY.append(dataset[i + time_step, 0])\n    return np.array(dataX), np.array(dataY)","7ca41f3f":"# reshape into X=t,t+1,t+2,t+3 and Y=t+4\ntime_step = 100\nx_train, y_train = create_dataset(train_data, time_step)\nx_test, y_test = create_dataset(test_data, time_step)","6c04defc":"print(x_train.shape)\nprint(y_train.shape)","f73a3f6f":"print(x_test.shape)\nprint(y_test.shape)","e64bbade":"#We are reshaping to convert into 3D\nx_train =x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)\nx_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)","70dbd1f7":"from tensorflow.keras.models import Sequential","5829d4c8":"from tensorflow.keras.layers import Dense","eb5dc3d9":"from tensorflow.keras.layers import LSTM","3f949acb":"model = Sequential()\n\nmodel.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n\nmodel.add(LSTM(50,return_sequences=True))\n\nmodel.add(LSTM(50))\n\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error',optimizer='adam')","ea2b25dd":"model.summary()","031c56a8":"model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=100,batch_size=64,verbose=1)","24ab9c5f":"train_predict=model.predict(x_train)\ntest_predict = model.predict(x_test)","c52d2e92":"#Transfrom back to original data as we scaled the data before\ntrain_predict=scaler.inverse_transform(train_predict)\ntest_predict=scaler.inverse_transform(test_predict)","7029e1be":"import math\nfrom sklearn.metrics import mean_squared_error\n\nmath.sqrt(mean_squared_error(train_predict,y_train))","0d5d4c31":"math.sqrt(mean_squared_error(test_predict,y_test))","4ff8c1dd":"look_back=100\ntrainPredictPlot = np.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","a379a2f7":"len(test_data)","f16f7873":"x_input=test_data[340:].reshape(1,-1)\nx_input.shape","2d6163ac":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()\ntemp_input","83e6dbe4":"#Demonstration of next 30 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","605b6e1f":"day_new=np.arange(1,101)\nday_pred=np.arange(101,131)","db502f70":"len(df1)","d71d9165":"plt.plot(day_new,scaler.inverse_transform(df1[1157:]))\nplt.plot(day_pred,scaler.inverse_transform(lst_output))","008fcd09":"# Prediction","aa4ee5c8":"**In time series kind of data, we have divide the train test data before doing the preprocessing of the data**","191633b4":"**LSTM is sensitive to the scale of the data, so we are doing Min max scaler to scale the values in the range of 0 to 1**","555187de":"**RMSE performance metrics**","932d6c37":"**Train test split**","7bf7de69":"# Stacked LSTM model","4969f5e8":"**In the above cell, I removed the api key, to get the api key go to pandas_datareader->tingo->api under the authentication and use in the above method**","be17e88a":"**From the graph, we can able to see the predicted data for the next 30 days**"}}