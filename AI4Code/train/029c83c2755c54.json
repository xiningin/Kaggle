{"cell_type":{"bfd9c393":"code","058901fd":"code","d0a3eb8a":"code","15ab5e29":"code","a2bbae6c":"code","4bfa11b7":"code","9cbe6c1a":"code","eb233335":"code","f154faf5":"code","a8ed2cf8":"code","9772526c":"code","d8f2611c":"code","5322bfd0":"code","0b51ca0a":"code","c40eaf1a":"code","c156b7fd":"code","0b065e0b":"code","083cfc2b":"code","a5abf0c0":"code","6adf43d9":"code","319eec7e":"code","6fd8ec7f":"code","342561ae":"code","213800ad":"code","233df8ff":"code","ce6387df":"code","131a68ff":"code","5ae5d2f4":"code","123424ff":"code","9902f576":"code","ee11517c":"code","78fa179c":"markdown","4ad3aaef":"markdown","f4c563e2":"markdown","13abdeb2":"markdown","928ce9ba":"markdown","0a964ab6":"markdown","d19ff7b1":"markdown","87eb85a0":"markdown","4406a78a":"markdown","df2f4f15":"markdown","bc7f36f0":"markdown","b31522bd":"markdown","5afb47f6":"markdown","0cd9ba9b":"markdown","f2e13288":"markdown","7d525646":"markdown","a8a1268f":"markdown","de1d4990":"markdown","dc885cde":"markdown","1d1b2a7e":"markdown","1ba51f0e":"markdown","3677dbcc":"markdown","c342d734":"markdown","a6db98c1":"markdown"},"source":{"bfd9c393":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio, display\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras.utils import to_categorical\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n%matplotlib inline","058901fd":"# Let's define the train and the test paths\nmain_path = \"\/kaggle\/input\/moroccan-darija-trigger-word-classification-ed-2\/\"\ntrain_path = main_path + \"data\/train\/\"\ntest_path = main_path + \"data\/test\/\"","d0a3eb8a":"# Loading .csv files \ndf_train = pd.read_csv(main_path + \"train.csv\")\ndf_test = pd.read_csv(main_path + \"test.csv\")\nsubmission_file = pd.read_csv(main_path + \"sample_submission.csv\")\nprint(\"done \ud83d\udc4d\")","15ab5e29":"df_train.sample(5)","a2bbae6c":"def read_plot_audio(audio_id, data_name='train'):\n    \n    \"\"\" Plot and read audio file wih wav extension\"\"\"\n    \n    if data_name == 'train':\n        filePath = train_path + df_train.loc[audio_id]['id'] +'.wav'\n    else : \n        filePath = test_path + df_test.loc[audio_id]['id'] +'.wav'\n\n    data, sample_rate = librosa.load(filePath)\n    audio = Audio(filePath)\n    audioGraph = librosa.display.waveplot(data, sample_rate)\n    plt.ylabel('Amplitude')\n    plt.xlabel('Time')\n    \n    return filePath, audio, audioGraph    ","4bfa11b7":"# safi7bess \ud83d\uded1\nfilePath_1, audio_1, audioGraph_1 = read_plot_audio(5)\naudio_1","9cbe6c1a":"# yallahbda\ud83c\udfc1\nfilePath_2, audio_2, audioGraph_2 = read_plot_audio(10)\naudio_2","eb233335":"print(f\"Shape of training set {df_train.shape}\")\nprint(f\"Train set contains {len(df_train['id'].unique())} unique training examples\")","f154faf5":"def white_noise(soundData):\n    \"\"\" Adding white noise to the audio\"\"\"\n    soundData_ = soundData.copy()\n    noise = 0.05*np.random.uniform()*np.amax(soundData)\n    soundData_ =soundData_.astype('float64') + noise*np.random.normal(size=soundData_.shape[0]) \n    \n    return soundData_","a8ed2cf8":"sound1, sr_1 = librosa.load(filePath_1)\nsound1AfterWN = white_noise(sound1)\n\n\n\naudioAfterWN = Audio(sound1AfterWN, rate=sr_1)\nwnGraph = librosa.display.waveplot(sound1AfterWN, sr=sr_1)\n\noriginalGraph = librosa.display.waveplot(sound1,sr=sr_1)\noriginalAudio = Audio(sound1, rate=sr_1)\n\n\ndisplay(originalAudio)\ndisplay(audioAfterWN)\nplt.legend([\"After adding WN\", \"original Sound\"])\nplt.show()","9772526c":"def shift(soundData):\n    \"\"\"Random Shifting\"\"\"\n    shifting_range = int(np.random.uniform(low=-4, high=20)*1000)\n    return np.roll(soundData, shifting_range)","d8f2611c":"Audio1AfterShifting = shift(sound1)\ndisplay(Audio(Audio1AfterShifting, rate=sr_1))\ndisplay(Audio(sound1, rate=sr_1))\n\nlibrosa.display.waveplot(Audio1AfterShifting, sr=sr_1)\nlibrosa.display.waveplot(sound1,sr=sr_1, color='r')\nplt.legend([\"After Shifting\", \"original Sound\"])\nplt.show()","5322bfd0":"def speedPitch(soundData):\n    \"\"\"\n    peed and Pitch Tuning.\n    \"\"\"\n    data = soundData.copy()\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  \/ length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data","0b51ca0a":"soundAfterSP = speedPitch(sound1)\ndisplay(Audio(sound1, rate=sr_1))\ndisplay(Audio(soundAfterSP, rate=sr_1))\n\n\nlibrosa.display.waveplot(sound1,sr=sr_1)\nlibrosa.display.waveplot(soundAfterSP, sr=sr_1, color='r')\nplt.legend([\"original Sound\",\"After speed and Pitch\"])\nplt.show()","c40eaf1a":"sound1, _ = librosa.load(filePath_1,res_type=\"kaiser_fast\", duration=2.5 , offset=0.5)\nmfccs = librosa.feature.mfcc(y=sound1, sr=44100, n_mfcc=30)\nprint(mfccs.shape)\nmfccs","c156b7fd":"librosa.display.specshow(mfccs, x_axis='time')\nplt.colorbar()\nplt.tight_layout()\nplt.title('mfcc')\nplt.show()","0b065e0b":"def data_preparation(df, n, mfcc, aug, path):\n    \"\"\"    \"\"\"\n    X = np.zeros(shape=(df.shape[0], n, 216, 1))\n    input_length = sample_rate*audio_duration\n    \n    counter = 0\n    \n    for fileName in tqdm(df.id):\n        filePath = path + str(fileName) + \".wav\"\n        soundData, _ = librosa.load(filePath, sr=sample_rate, res_type=\"kaiser_fast\",duration=2.5 , offset=0.5)\n        \n        # Random Padding\/ offset    \n        if len(soundData) > input_length:\n            max_offset = len(soundData) - input_length\n            offset = np.random.randint(max_offset)\n            soundData = soundData[offset:int(input_length+offset)]\n        else:\n            if input_length > len(soundData):\n                max_offset = input_length - len(soundData)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n                \n            soundData = np.pad(soundData, (offset, int(input_length) - len(soundData) - offset), \"constant\")\n\n        # Data Augmentation \n        if aug == 1:\n            soundData = white_noise(soundData)\n            \n        elif aug == 2:\n            soundData = speedPitch(soundData)\n            \n        elif aug == 3:\n            soundData = shift(soundData)\n            \n        # Features Extraction\n        if mfcc == 1:\n            MFCC = librosa.feature.mfcc(soundData, sr=sample_rate, n_mfcc=n_mfcc)\n            MFCC = np.expand_dims(MFCC, axis=-1)\n            X[counter,] = MFCC\n        \n        counter +=1\n            \n    return X\n        ","083cfc2b":"sample_rate=44100\naudio_duration=2.5\nn_mfcc = 30\n\n# Features Extraction (MFCCS)\nX_mfccs = data_preparation(df_train, n_mfcc, 1, 0, train_path)\n#White noise augmentation\nX_WnAug = data_preparation(df_train, n_mfcc, 1, 1, train_path)\n# Speed and Pitch augmentation\nX_SPAug = data_preparation(df_train, n_mfcc, 1, 2, train_path)\n# Shift augmentation \nX_SfAug = data_preparation(df_train, n_mfcc, 1, 3, train_path)","a5abf0c0":"# Concatenate X_mfccs, X_WnAug, W_SPAug, W_SfAug\nX = np.concatenate([X_mfccs, X_WnAug, X_SPAug, X_SfAug])\ny = df_train['label'].values\ny = to_categorical(y)\ny = np.concatenate([y]*4)","6adf43d9":"# 2D matrix of 30 MFCC bands by 216 audio length.\nX.shape, y.shape","319eec7e":"# Features Extraction (MFCCS)\nX_test_mfccs = data_preparation(df_test, n_mfcc, 1, 0, test_path)","6fd8ec7f":"X_train, X_valid, y_train, y_valid = train_test_split(X\n                                                    , y\n                                                    , test_size=0.1\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\nprint(f'training set : {X_train.shape} , validation set :{X_valid.shape}, test set : {X_test_mfccs.shape}')","342561ae":"# Normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_valid = (X_valid - mean)\/std\nX_test = (X_test_mfccs - mean)\/std\nprint(\"done \ud83d\udc4d\")","213800ad":"from keras import losses, models\nfrom tensorflow.keras.optimizers import Adam \nfrom keras.activations import relu, softmax\nfrom keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\nfrom keras.models import Sequential, Model","233df8ff":"\ndef convNet_model(input_shape):\n    \n    input_ = Input(shape=input_shape)  #2D matrix of 30 MFCC bands by 216 audio length.\n    x = Convolution2D(32, (3,3), padding=\"same\")(input_)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(64, (3,3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(128, (3,3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(256, (3,3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dense(256)(x)\n    x = Dropout(rate=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    \n    out = Dense(2, activation='softmax')(x)\n    model = Model(inputs=input_, outputs=out)\n    \n    \n    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\n\ninput_shape = X[0].shape\nmodel = convNet_model(input_shape)\nmodel.summary()","ce6387df":"model_history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n                          batch_size=16, verbose = 2, shuffle=True, epochs=200)","131a68ff":"def plot_loss_function(history):\n    \"train and validation loss\"\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \nplot_loss_function(model_history)","5ae5d2f4":"predictions = model.predict(X_test).argmax(axis=1)\npredictions","123424ff":"#0 if the trigger word is \"yallahbda\" and 1 if it is \"safi7bess\".\ni = 72\nfile_path, audio, wave_graph = read_plot_audio(i, data_name='test')\nprint(\"prediction:\",predictions[i])\naudio","9902f576":"submission_file['label'] = predictions\nsubmission_file.sample(5)","ee11517c":"submission_file.to_csv('Final_submission.csv', index=False)\nprint('done \ud83d\udc4d')","78fa179c":" * [In signal processing, white noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density.](https:\/\/en.wikipedia.org\/wiki\/White_noise)","4ad3aaef":"`id` : Name of the audio file in the train folder.\n\n`label` : target variable, it takes two values 0 if the trigger word is `yallahbda` \ud83c\udfc1 and 1 if it is `safi7bess`\ud83d\uded1.","f4c563e2":"<a id = 6><\/a>\n# 6. Predictions & Submission \ud83d\udcdd\n","13abdeb2":"### 3.2 Shift","928ce9ba":"**Let's see an example** ","0a964ab6":"**Let's hear and see some Recordings**","d19ff7b1":"<a id=2><\/a>\n# 2. Loading the data \ud83d\udd0a","87eb85a0":"**Let's split train and validation sets**","4406a78a":"\n# <p style=\" text-align:center; fontsize:1000%\">\ud83d\udd0a Moroccan Darija Trigger Word Classification\ud83d\udd0a<\/p>","df2f4f15":"mel-frequency cepstrum[ is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum)","bc7f36f0":"* **Content :**\n\n1. [Importing libraries\ud83d\udcda](#1)\n2. [Loading the data \ud83d\udd0a](#2)\n3. [Data Augmentation \ud83d\udcc8](#3)\n4. [Extracting Features from Sounds \ud83d\udd27](#4)\n5. [Modeling \ud83c\udfcb\ufe0f](#5)\n6. [Predictions & Submission \ud83d\udcdd](#6)","b31522bd":"### 3.3 Speed and Pitch","5afb47f6":"**\u26a0\ufe0f Same Pre-Processing should be done on the Test set**","0cd9ba9b":" \ud83d\udccc `train.csv` & `test.csv` contains information about the audio files availble in `data` folder.","f2e13288":"* `X.shape[0] = 576` : Number of training examples after data augmentation\n* `X.shape[1] = 30`  : Number of Mel-frequency cepstral coefficients.\n* `X.shape[2] = 216` : audio length.\n\n","7d525646":"**Let's take a look to our training set**","a8a1268f":"**Submission**","de1d4990":"<a id=4><\/a>\n# 4. Extracting Features from Sounds \ud83d\udd27","dc885cde":"\ud83d\udcccNote:  the audio data is a combination of two elements :\n* **Sound** : sequence of vibrations in varying pressure strenghs\n* **Sample Rate** :is the number of samples of audio carried per second, measured in Hz or KHz","1d1b2a7e":"<a id = 5><\/a>\n# 5. Modeling \ud83c\udfcb\ufe0f","1ba51f0e":"<a id=1><\/a>\n# 1. Importing libraries  \ud83d\udcda\u2b07","3677dbcc":"\ud83d\udcccNote: The training set is too smal for a hard task like `Audio Classification`. **Let's make some Audio Data Augmentation**","c342d734":"<a id = 3><\/a>\n# 3. Data Augmentation \ud83d\udcc8","a6db98c1":"### 3.1 white Noise"}}