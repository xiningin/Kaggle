{"cell_type":{"d74fbf74":"code","98624e02":"code","0a312d63":"code","9648acae":"code","89adee82":"code","97ec319a":"code","b8237c98":"code","3a514db1":"code","d740811f":"code","d0be4c41":"code","b27bbd27":"code","44ea67a6":"code","d8efeaa9":"code","9f8fb1d7":"code","3ba460b4":"code","7cf66fd8":"code","208759bf":"code","62b41f07":"code","96d70630":"code","ff205c2e":"code","fd0f570b":"code","e48c9f87":"code","cf2986a0":"code","c24e7720":"code","721d2d6e":"code","65910cb5":"code","4f10042d":"code","84b9ce30":"code","be19cfae":"code","89a39067":"code","a26cadeb":"code","4e608be4":"code","dda86a2e":"code","f0fa11c4":"code","15d09c32":"code","5e6c6934":"code","94c5cfab":"code","8d5f5c29":"code","df2a3aa1":"code","da0f2e8f":"code","736ddeda":"code","06f39686":"code","fb2a28ab":"code","c1ebf1c0":"code","adb96fa8":"code","e1318eae":"code","7a574548":"code","2d539ddc":"code","a32179a9":"code","3db6b5f8":"code","56bbd50a":"code","3911e46e":"code","ebebafeb":"code","ee9a8798":"code","feb4f8fa":"code","2ed3c76b":"code","1ba1d923":"code","8b607d29":"code","c1ffc514":"code","518cdee4":"code","73335fb6":"code","f679437d":"code","5cb3170b":"code","d337e0cd":"code","772cd6e9":"code","eeaa29bc":"code","69c8c1fb":"code","2ddbfc11":"code","11dcfb1a":"code","6b8d09d4":"code","e1d5c27a":"code","2e139a86":"code","2cb01a35":"code","37cd46d6":"code","d40b0d9d":"code","5f639d2d":"markdown","1678e5be":"markdown","e49d6860":"markdown","170d02fb":"markdown","e8d07a35":"markdown","28b3313a":"markdown","12ea8197":"markdown","e3024419":"markdown","1ce21647":"markdown","0adedc9a":"markdown","428fab6f":"markdown","e7bfa762":"markdown","d1a56ed5":"markdown","5934611f":"markdown","234d04ee":"markdown","2365f575":"markdown","0babf37b":"markdown","f64eb22a":"markdown","cd364ef3":"markdown","4ef040b8":"markdown","78d0d806":"markdown","3e07069c":"markdown","1569f2ab":"markdown"},"source":{"d74fbf74":"%matplotlib notebook","98624e02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport re\n\n#needed for text processing and analytics\nimport nltk\nimport nltk.stem\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer\nfrom nltk import pos_tag, sent_tokenize, word_tokenize, BigramAssocMeasures,\\\n    BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n#visualization\nfrom plotly import tools\nimport plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n\n#to plot inside the document\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Any results you write to the current directory are saved as output.","0a312d63":"#load the dataset\ndebates = pd.read_csv(\"..\/input\/un-general-debates.csv\")\ndebates.head()","9648acae":"debates = debates.loc[debates['country'] == 'IND']","89adee82":"# create features from meta text data\ndebates['char_count'] = debates['text'].str.len()\ndebates['words'] = debates['text'].str.split(' ')\ndebates['sentences'] = debates['text'].str.split('.')\ndebates['word_count'] = debates['words'].str.len()\ndebates['sentence_count'] = debates['sentences'].str.len()\ndebates['word_length'] = debates['char_count'] \/ debates['word_count']\ndebates['sentence_length'] = debates['word_count'] \/ debates['sentence_count']","97ec319a":"#create feature with text processing\ndebates['text'] = debates['text'].str.lower().map(lambda x: re.sub('\\W+',' ', x))\ndebates['text'] = debates['text'].str.lower().map(lambda x: re.sub('united nations','united_nations', x))\ndebates['token'] = debates['text'].apply(word_tokenize)\nstop_words = set(stopwords.words('english'))\n# I noticed that \"'s\" is not included in stopwords, while I think it doesn't bring much meaning in a text, so I'll add it to the set to remove from the cleaned tokens.\nstop_words.add(\"'s\")\nstop_words.add(\"'\")\nstop_words.add(\"-\")\nstop_words.add(\"'\")\ndebates['clean'] = debates['token'].apply(lambda x: [w for w in x if not w in stop_words and not w in punctuation])\n\nimport nltk\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet as wn\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n\nstemmer = nltk.stem.PorterStemmer()\ndebates['stems'] = [[format(get_lemma(token)) for token in speech] for speech in debates['clean']]\ndebates = debates.sort_values('year')\n","b8237c98":"# Look at the meta text features' distributions\nplt.figure(figsize=(16, 8))\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\nall_data = [debates]\n\naxes[0, 0].violinplot(debates['word_count'], showmeans=True, showmedians=False)\naxes[0, 0].set_title('Word Count per Speech')\n\naxes[0, 1].violinplot(debates['word_length'], showmeans=True, showmedians=False)\naxes[0, 1].set_title('Avg. Word Length per Speech')\n\naxes[1, 0].violinplot(debates['sentence_count'], showmeans=True, showmedians=False)\naxes[1, 0].set_title('Sentence Count per Speech')\n\naxes[1, 1].violinplot(debates['sentence_length'], showmeans=True, showmedians=False)\naxes[1, 1].set_title('Avg. Sentence Length per Speech')\n\n# add x-tick labels\nplt.setp(axes, xticks=[y+1 for y in range(len(all_data))], xticklabels=['Frequency'])\nfig.subplots_adjust(wspace=.5, hspace=.5)\nplt.show()","3a514db1":"trace1 = go.Scatter(\n    x=debates['year'],\n    y=debates['char_count'],\n    fill='tonexty',\n    name = 'Character Count'\n)\ntrace2 = go.Scatter(\n    x=debates['year'],\n    y=debates['word_count'],\n    fill='tonexty',\n    name = 'Word Count'\n)\ntrace3 = go.Scatter(\n    x=debates['year'],\n    y=debates['sentence_count'],\n    fill='tonexty',\n    name = 'Sentence Count'\n)\ndata = [trace1, trace2,trace3]\niplot(data, filename='basic-area')","d740811f":"all_per_year = debates.groupby('session').agg({'year': 'mean', 'clean': 'sum'})","d0be4c41":"import nltk\nfrom wordcloud import WordCloud, STOPWORDS\n\nwords = []\nfor i, row in all_per_year.iterrows():\n    words = nltk.FreqDist(row['clean'])\n    \nwords = nltk.FreqDist(words)\nstopwords = nltk.corpus.stopwords.words('english')\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \nwordcloud = WordCloud(stopwords=STOPWORDS,background_color='white').generate(\" \".join(words_except_stop_dist))\nplt.imshow(wordcloud,interpolation='bilinear')\nfig=plt.gcf()\nfig.set_size_inches(16,8)\nplt.title(\"Most used words by India at UN Debate sessions\")\nplt.axis('off')\nplt.show()","b27bbd27":"#from https:\/\/www.kaggle.com\/peterwittek\/wordclouds-of-titles-and-abstracts-with-n-grams\n#definations for Bi-Trigrams for Wordcloud viz\ndef get_bitrigrams(full_text, threshold=30):\n    if isinstance(full_text, str):\n        text = full_text\n    else:\n        text = \" \".join(full_text)\n    bigram_measures = BigramAssocMeasures()\n    trigram_measures = TrigramAssocMeasures()\n    finder = BigramCollocationFinder.from_words(text.split())\n    finder.apply_freq_filter(3)\n    bigrams = {\" \".join(words): \"_\".join(words)\n               for words in finder.above_score(bigram_measures.likelihood_ratio, threshold)}\n    finder = TrigramCollocationFinder.from_words(text.split())\n    finder.apply_freq_filter(3)\n    trigrams = {\" \".join(words): \"_\".join(words)\n                for words in finder.above_score(trigram_measures.likelihood_ratio, threshold)}\n    return bigrams, trigrams\n\n\ndef replace_bitrigrams(text, bigrams, trigrams):\n    if isinstance(text, str):\n        texts = [text]\n    else:\n        texts = text\n    new_texts = []\n    for t in texts:\n        t_new = t\n        for k, v in trigrams.items():\n            t_new = t_new.replace(k, v)\n        for k, v in bigrams.items():\n            t_new = t_new.replace(\" \" + k + \" \", \" \" + v + \" \")\n        new_texts.append(t_new)\n    if len(new_texts) == 1:\n        return new_texts[0]\n    else:\n        return new_texts\n    \ndef process_text(text, lemmatizer, translate_table, stopwords):\n    processed_text = \"\"\n    for sentence in sent_tokenize(text):\n        tagged_sentence = pos_tag(word_tokenize(sentence.translate(translate_table)))\n        for word, tag in tagged_sentence:\n            word = word.lower()\n            if word not in stopwords:\n                if tag[0] != 'V':\n                    processed_text += lemmatizer.lemmatize(word) + \" \"\n    return processed_text\n\n\ndef get_all_processed_texts(texts, lemmatizer, translate_table, stopwords):\n    processed_texts = []\n    for index, doc in enumerate(texts):\n        processed_texts.append(process_text(doc, wordnet_lemmatizer, translate_table, stop))\n    bigrams, trigrams = get_bitrigrams(processed_texts)\n    very_processed_texts = replace_bitrigrams(processed_texts, bigrams, trigrams)\n    return \" \".join(very_processed_texts)\n","44ea67a6":"from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\nfrom nltk import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nimport string\n\nwordnet_lemmatizer = WordNetLemmatizer()\nstop = set(stopwords.words('english'))\ntranslate_table = dict((ord(char), \" \") for char in string.punctuation)\n\ndef use_ngrams_only(texts, lemmatizer, translate_table, stopwords):\n    processed_texts = []\n    for index, doc in enumerate(texts):\n        processed_texts.append(process_text(doc, wordnet_lemmatizer, translate_table, stop))\n    bigrams, trigrams = get_bitrigrams(processed_texts)\n    indexed_texts = []\n    for doc in processed_texts:\n        current_doc = []\n        for k, v in trigrams.items():\n            c = doc.count(k)\n            if c > 0:\n                current_doc += [v] * c\n                doc = doc.replace(k, v)\n        for k, v in bigrams.items():\n            current_doc += [v] * doc.count(\" \" + k + \" \")\n        indexed_texts.append(\" \".join(current_doc))\n    return \" \".join(indexed_texts)\n\nwordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").\\\n    generate(use_ngrams_only(debates['text'], wordnet_lemmatizer, translate_table, stop))\nplt.figure(figsize=(16, 8))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.title(\"Most used Bi-grams by India at UN Debate sessions\")\nplt.axis(\"off\")\nplt.show()\n","d8efeaa9":"#creating frequency of words \nfreqs = {}\nfor i, speech in debates.iterrows():\n    year = speech['year']\n    for token in speech['stems']:\n        if token not in freqs:\n            freqs[token] = {\"total_freq\":1, year:1}\n        else:\n            freqs[token][\"total_freq\"] += 1\n            if not freqs[token].get(year):\n                freqs[token][year] = 1\n            else:\n                freqs[token][year] += 1","9f8fb1d7":"#create a dataframe to store the words and their frequencies\nfreqs_df = pd.DataFrame.from_dict(freqs, orient='index')\nfreqs_df['word'] = freqs_df.index\nnew_cols = [\"total_freq\", \"word\"] + sorted(freqs_df.columns.tolist()[1:-1])\nfreqs_df = freqs_df[new_cols]\nfreqs_df = freqs_df.sort_values('total_freq', ascending=False)\nfreqs_df = freqs_df.fillna(0)\nfreqs_df.head()","3ba460b4":"#plot to visualize top 100 words over the years\ntrace = go.Scatter(\n    x=freqs_df.iloc[0:100]['word']   ,   \n    y=freqs_df['total_freq']          \n    )\nlayout = dict(title = 'Frequency of Words used over years',\n              xaxis = dict(title = 'Word'),\n              yaxis = dict(title = 'Count'),\n              )\nfig = dict(data= [trace], layout=layout)\niplot(fig, filename='basic-line')","7cf66fd8":"\n\nRANGE=(1971, 2016)\n\nplt.figure(figsize=(22,10))\nplt.xticks(range(*RANGE))\nplt.xlim(RANGE)\n\n\n\ndef show(year, n=5):\n    \"Add the top-n words for a year to the current plot\"\n    \n    top5 = freqs_df.nlargest(n, columns=year)    \n    plt.scatter([year]*n, top5[year],picker=True)\n    \n    for _,row in top5.iterrows(): \n        \n        plt.annotate(row['word'], (year, row[year]))\n         \nfor year in range(*RANGE):\n    show(year)\n    \n\n    \n\nplt.show()","208759bf":"text_data = list(debates['clean'])","62b41f07":"from gensim import corpora\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\n#import pickle\n#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n#dictionary.save('dictionary.gensim')","96d70630":"import gensim\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=10)\nfor topic in topics:\n    print(topic)","ff205c2e":"# import pyLDAvis.gensim\n# lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n# lda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)\n# pyLDAvis.display(lda_display10)","fd0f570b":"#print(text_data)","e48c9f87":"from gensim import corpora, models\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, \n                                    num_topics=10, passes=5, minimum_probability=0)\n\nhm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))], dtype=object)\nprint(hm)","cf2986a0":"freqs_df.iloc[0:5, 1:47].transpose().iloc[1:].plot(title=\"Most common words\",figsize=(16,8))","c24e7720":"freqs_df[freqs_df['word'].isin(['peace', 'war', 'security', 'cold', 'conflict', 'aggression'])].iloc[:, 1:47].transpose().iloc[1:].plot(title = \"War and Peace\",figsize=(16,8))","721d2d6e":"freqs_df[freqs_df['word'].isin(['economy', 'wealth', 'crisis', 'growth', 'inflation', 'trade', 'poverty', 'rich', 'income'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Economy\",figsize=(16,8))","65910cb5":"freqs_df[freqs_df['word'].isin(['people', 'refugee', 'humanitarian', 'freedom', 'right'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"People\",figsize=(16,8))","4f10042d":"freqs_df[freqs_df['word'].isin(['democracy', 'republic', 'dictator', 'sovereign', 'politics'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Politics\",figsize=(16,8))","84b9ce30":"freqs_df[freqs_df['word'].isin(['violence', 'kill', 'death'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Violence\",figsize=(16,8))","be19cfae":"freqs_df[freqs_df['word'].isin(['environment', 'sustain', 'green', 'energy', 'ecology', 'warm', 'temperature', 'pollution', 'planet'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Environment\",figsize=(16,8))","89a39067":"freqs_df[freqs_df['word'].isin(['terror','terrorism','terrorist'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Terrorism\",figsize=(16,8))","a26cadeb":"import pycountry\nlist_of_countries = []\nfor i in range(len(pycountry.countries)):\n    x = list(pycountry.countries)[i]\n    list_of_countries.append(x.name.lower())\nprint(list_of_countries)","4e608be4":"debates['countries_mentioned'] = debates['token'].apply(lambda token: {x:token.count(x) for x in token if x in list_of_countries})","dda86a2e":"country_mentions = pd.concat([debates[[\"year\"]],\n                              debates['countries_mentioned'].apply(pd.Series)], axis=1).dropna(axis=1, how='all')\ncountry_mentions = country_mentions.reset_index()\ndel country_mentions['index']\ncountry_mentions.head()","f0fa11c4":"country_mentions = country_mentions.transpose()\ncountry_mentions[\"total_mention\"] = country_mentions.sum(axis=1)","15d09c32":"country_mentions = country_mentions.sort_values('total_mention', ascending=False)\ncountry_mentions = country_mentions.transpose()\ncountry_mentions = country_mentions.drop(country_mentions.index[len(country_mentions)-1])\ncountry_mentions['year'] =country_mentions['year'].apply(np.int64)\ncountry_mentions = country_mentions.sort_values('year')\ncountry_mentions = country_mentions.fillna(0)\ncountry_mentions.head()","5e6c6934":"country_mentions.plot(x=\"year\", y=[\"india\",\"pakistan\",\"namibia\",\"afghanistan\",\"israel\",\"bangladesh\",\"cyprus\",\"china\",\"iraq\"],figsize=(16,8))\nplt.show()","94c5cfab":"country_mentions = country_mentions.rename_axis(None)\ncountry_mentions = country_mentions.set_index('year').T\ncountry_mentions['country'] = country_mentions.index","8d5f5c29":"\nRANGE=(1971, 2016)\n\nplt.figure(figsize=(22,10))\nplt.xticks(range(*RANGE))\nplt.xlim(RANGE)\n\ndef show(year, n=5):\n    \"Add the top-n words for a year to the current plot\"\n    top5 = country_mentions.nlargest(n, columns=year)\n    plt.scatter([year]*n, top5[year],picker=True)\n    for _,row in top5.iterrows():\n        plt.annotate(row['country'], (year, row[year]))\n         \nfor year in range(*RANGE):\n    show(year)\n    \n\nplt.show()\n","df2a3aa1":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef analize_sentiment(text):\n    sid = SentimentIntensityAnalyzer()\n    ss = sid.polarity_scores(text)\n    if ss[\"compound\"] == 0.0: \n        sent = 0.0\n    elif ss[\"compound\"] > 0.0:\n        sent = 1.0\n    else:\n        sent = -1.0\n    return ss[\"compound\"] \n\ndef analize_polarity(text):\n    sid = SentimentIntensityAnalyzer()\n    ss = sid.polarity_scores(text)\n    if ss[\"compound\"] == 0.0: \n        sent = \"Neutral\"\n    elif ss[\"compound\"] > 0.0:\n        sent = \"Postive\"\n    else:\n        sent = \"Negative\"\n    return sent","da0f2e8f":"debates['sentiment'] = np.array([ analize_sentiment(text) for text in debates['text'] ])\ndebates['polarity'] = np.array([ analize_polarity(text) for text in debates['text'] ])\ndebates.head()","736ddeda":"data = [go.Bar(\n           x=debates['year'],\n           y=debates['sentiment']\n    )]\n\niplot(data, filename='basic-bar')","06f39686":"deb_values = debates.groupby(['polarity'],axis=0).count()\ndeb_values.head()","fb2a28ab":"label = ['Negative','Postive']\nvalues = deb_values['text']\ntrace = go.Pie(labels= label,values=values,title='Polarity of Speeches by India')\niplot([trace], filename='basic_pie_chart')","c1ebf1c0":"data= pd.read_csv(\"..\/input\/un-general-debates.csv\")\ndata.head()","adb96fa8":"#create feature with text processing\ndata['text'] = data['text'].str.lower().map(lambda x: re.sub('\\W+',' ', x))\ndata['token'] = data['text'].apply(word_tokenize)","e1318eae":"data['india_mentioned'] = data.text.str.count('india')\ndata = data[data.country != 'IND']\ndata.head()","7a574548":"countries = dict((k, v.lower()) for k,v in {\n    'AFG': 'Afghanistan', \n    'ALA': 'Aland Islands', \n    'ALB': 'Albania', \n    'DZA': 'Algeria', \n    'ASM': 'American Samoa', \n    'AND': 'Andorra', \n    'AGO': 'Angola', \n    'AIA': 'Anguilla', \n    'ATA': 'Antarctica', \n    'ATG': 'Antigua and Barbuda', \n    'ARG': 'Argentina', \n    'ARM': 'Armenia', \n    'ABW': 'Aruba', \n    'AUS': 'Australia', \n    'AUT': 'Austria', \n    'AZE': 'Azerbaijan', \n    'BHS': 'Bahamas', \n    'BHR': 'Bahrain', \n    'BGD': 'Bangladesh', \n    'BRB': 'Barbados', \n    'BLR': 'Belarus', \n    'BEL': 'Belgium', \n    'BLZ': 'Belize', \n    'BEN': 'Benin', \n    'BMU': 'Bermuda', \n    'BTN': 'Bhutan', \n    'BOL': 'Bolivia', \n    'BIH': 'Bosnia and Herzegovina', \n    'BWA': 'Botswana', \n    'BVT': 'Bouvet Island', \n    'BRA': 'Brazil', \n    'VGB': 'Virgin Islands', \n    'IOT': 'British Indian Ocean Territory', \n    'BRN': 'Brunei', \n    'BGR': 'Bulgaria', \n    'BFA': 'Burkina Faso', \n    'BDI': 'Burundi', \n    'KHM': 'Cambodia', \n    'CMR': 'Cameroon', \n    'CAN': 'Canada', \n    'CPV': 'Cape Verde', \n    'CYM': 'Cayman Islands', \n    'CAF': 'Central Africa', \n    'TCD': 'Chad', \n    'CHL': 'Chile', \n    'CHN': 'China', \n    'HKG': 'Hong Kong', \n    'MAC': 'Macao', \n    'CXR': 'Christmas Island', \n    'CCK': 'Cocos Islands', \n    'COL': 'Colombia', \n    'COM': 'Comoros', \n    'COG': 'Congo', \n    'COD': 'Democratic Republic of Congo', \n    'COK': 'Cook Islands', \n    'CRI': 'Costa Rica', \n    'CIV': \"Cote d'Ivoire\", \n    'HRV': 'Croatia', \n    'CUB': 'Cuba', \n    'CYP': 'Cyprus', \n    'CZE': 'Czech Republic', \n    'DNK': 'Denmark', \n    'DJI': 'Djibouti', \n    'DMA': 'Dominica', \n    'DOM': 'Dominican Republic', \n    'ECU': 'Ecuador', \n    'EGY': 'Egypt', \n    'SLV': 'El Salvador', \n    'GNQ': 'Equatorial Guinea', \n    'ERI': 'Eritrea', \n    'EST': 'Estonia', \n    'ETH': 'Ethiopia', \n    'FLK': 'Falkland', \n    'FRO': 'Faroe', \n    'FJI': 'Fiji', \n    'FIN': 'Finland', \n    'FRA': 'France', \n    'GUF': 'French Guiana', \n    'PYF': 'French Polynesia', \n    'ATF': 'French Southern Territories', \n    'GAB': 'Gabon', \n    'GMB': 'Gambia', \n    'GEO': 'Georgia', \n    'DEU': 'Germany', \n    'GHA': 'Ghana', \n    'GIB': 'Gibraltar', \n    'GRC': 'Greece', \n    'GRL': 'Greenland', \n    'GRD': 'Grenada', \n    'GLP': 'Guadeloupe', \n    'GUM': 'Guam', \n    'GTM': 'Guatemala', \n    'GGY': 'Guernsey', \n    'GIN': 'Guinea', \n    'GNB': 'Guinea-Bissau', \n    'GUY': 'Guyana', \n    'HTI': 'Haiti', \n    'HMD': 'Heard and Mcdonald Islands', \n    'VAT': 'Vatican', \n    'HND': 'Honduras', \n    'HUN': 'Hungary', \n    'ISL': 'Iceland', \n    'IND': 'India', \n    'IDN': 'Indonesia', \n    'IRN': 'Iran', \n    'IRQ': 'Iraq', \n    'IRL': 'Ireland', \n    'IMN': 'Isle of Man', \n    'ISR': 'Israel', \n    'ITA': 'Italy', \n    'JAM': 'Jamaica', \n    'JPN': 'Japan', \n    'JEY': 'Jersey', \n    'JOR': 'Jordan', \n    'KAZ': 'Kazakhstan', \n    'KEN': 'Kenya', \n    'KIR': 'Kiribati', \n    'PRK': 'North Korea', \n    'KOR': 'South Korea', \n    'KWT': 'Kuwait', \n    'KGZ': 'Kyrgyzstan', \n    'LAO': 'Lao', \n    'LVA': 'Latvia', \n    'LBN': 'Lebanon', \n    'LSO': 'Lesotho', \n    'LBR': 'Liberia', \n    'LBY': 'Libya', \n    'LIE': 'Liechtenstein', \n    'LTU': 'Lithuania', \n    'LUX': 'Luxembourg', \n    'MKD': 'Macedonia', \n    'MDG': 'Madagascar', \n    'MWI': 'Malawi', \n    'MYS': 'Malaysia', \n    'MDV': 'Maldives', \n    'MLI': 'Mali', \n    'MLT': 'Malta', \n    'MHL': 'Marshall Islands', \n    'MTQ': 'Martinique', \n    'MRT': 'Mauritania', \n    'MUS': 'Mauritius', \n    'MYT': 'Mayotte', \n    'MEX': 'Mexico', \n    'FSM': 'Micronesia', \n    'MDA': 'Moldova', \n    'MCO': 'Monaco', \n    'MNG': 'Mongolia', \n    'MNE': 'Montenegro', \n    'MSR': 'Montserrat', \n    'MAR': 'Morocco', \n    'MOZ': 'Mozambique', \n    'MMR': 'Myanmar', \n    'NAM': 'Namibia', \n    'NRU': 'Nauru', \n    'NPL': 'Nepal', \n    'NLD': 'Netherlands', \n    'ANT': 'Netherlands Antilles', \n    'NCL': 'New Caledonia', \n    'NZL': 'New Zealand', \n    'NIC': 'Nicaragua', \n    'NER': 'Niger', \n    'NGA': 'Nigeria', \n    'NIU': 'Niue', \n    'NFK': 'Norfolk Island', \n    'MNP': 'Northern Mariana Islands', \n    'NOR': 'Norway', \n    'OMN': 'Oman', \n    'PAK': 'Pakistan', \n    'PLW': 'Palau', \n    'PSE': 'Palestine', \n    'PAN': 'Panama', \n    'PNG': 'Papua New Guinea', \n    'PRY': 'Paraguay', \n    'PER': 'Peru', \n    'PHL': 'Philippines', \n    'PCN': 'Pitcairn', \n    'POL': 'Poland', \n    'PRT': 'Portugal', \n    'PRI': 'Puerto Rico', \n    'QAT': 'Qatar', \n    'REU': 'Reunion', \n    'ROU': 'Romania', \n    'RUS': 'Russia', \n    'RWA': 'Rwanda', \n    'BLM': 'Saint-Barthelemy', \n    'SHN': 'Saint Helena', \n    'KNA': 'Saint Kitts', \n    'LCA': 'Saint Lucia', \n    'MAF': 'Saint-Martin', \n    'SPM': 'Saint Pierre and Miquelon', \n    'VCT': 'Saint Vincent and Grenadines', \n    'WSM': 'Samoa', \n    'SMR': 'San Marino', \n    'STP': 'Sao Tome and Principe', \n    'SAU': 'Saudi Arabia', \n    'SEN': 'Senegal', \n    'SRB': 'Serbia', \n    'SYC': 'Seychelles', \n    'SLE': 'Sierra Leone', \n    'SGP': 'Singapore', \n    'SVK': 'Slovakia', \n    'SVN': 'Slovenia', \n    'SLB': 'Solomon Islands', \n    'SOM': 'Somalia', \n    'ZAF': 'South Africa', \n    'SGS': 'South Georgia and the South Sandwich Islands', \n    'SSD': 'South Sudan', \n    'ESP': 'Spain', \n    'LKA': 'Sri Lanka', \n    'SDN': 'Sudan', \n    'SUR': 'Suriname', \n    'SJM': 'Svalbard', \n    'SWZ': 'Swaziland', \n    'SWE': 'Sweden', \n    'CHE': 'Switzerland', \n    'SYR': 'Syria', \n    'TWN': 'Taiwan', \n    'TJK': 'Tajikistan', \n    'TZA': 'Tanzania', \n    'THA': 'Thailand', \n    'TLS': 'Timor', \n    'TGO': 'Togo', \n    'TKL': 'Tokelau', \n    'TON': 'Tonga', \n    'TTO': 'Trinidad', \n    'TUN': 'Tunisia', \n    'TUR': 'Turkey', \n    'TKM': 'Turkmenistan', \n    'TCA': 'Turks and Caicos Islands', \n    'TUV': 'Tuvalu', \n    'UGA': 'Uganda', \n    'UKR': 'Ukraine', \n    'ARE': 'United Arab Emirates', \n    'GBR': 'United Kingdom', \n    'USA': 'United States', \n    'UMI': 'US Minor Outlying Islands', \n    'URY': 'Uruguay', \n    'UZB': 'Uzbekistan', \n    'VUT': 'Vanuatu', \n    'VEN': 'Venezuela', \n    'VNM': 'Viet Nam', \n    'VIR': 'Virgin Islands', \n    'WLF': 'Wallis and Futuna', \n    'ESH': 'Western Sahara', \n    'YEM': 'Yemen', \n    'ZMB': 'Zambia', \n    'ZWE': 'Zimbabwe'\n}.items())","2d539ddc":"data['country'] = data['country'].map(countries)\ndata.head()","a32179a9":"data_mention = data.groupby(\"country\")[data.columns[2:]].sum()\ndata_mention = data_mention.sort_values('india_mentioned',ascending=[False])\ndata_mention = data_mention.head(11)\ndata_mention","3db6b5f8":"countries_which_mention_india = data_mention.index.values.tolist() ","56bbd50a":"countries_which_mention_india","3911e46e":"data = data.loc[data['country'].isin(countries_which_mention_india)]\ndata_future = data\ndata.head()","ebebafeb":"data=data[['year','country','india_mentioned']]\ndata.head()","ee9a8798":"data = data.pivot(index='year', columns='country', values='india_mentioned')\ndata.head()","feb4f8fa":"data = data.fillna(0)\ndata.head()","2ed3c76b":"data = data.reset_index()\ndata.head()","1ba1d923":"data.set_index(data['year'])\ndata.head()","8b607d29":"data.plot(x=\"year\", y=countries_which_mention_india,figsize=(16,8))\nplt.show()\n","c1ffc514":"data_future = data_future.loc[data_future['country'] == 'pakistan']\ndata_future.head()","518cdee4":"data_future = data_future.loc[data_future['india_mentioned'] >= 10]","73335fb6":"import nltk\nfrom wordcloud import WordCloud, STOPWORDS\n\nwords = []\nfor i, row in data_future.iterrows():\n    words = nltk.FreqDist(row['token'])\n    \nwords = nltk.FreqDist(words)\nstopwords = nltk.corpus.stopwords.words('english')\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \nwordcloud = WordCloud(stopwords=STOPWORDS,background_color='white').generate(\" \".join(words_except_stop_dist))\nplt.imshow(wordcloud,interpolation='bilinear')\nfig=plt.gcf()\nfig.set_size_inches(16,8)\nplt.title(\"Most used words by Paksitan at UN Debate sessions (where they mentioned India more than 10 times)\")\nplt.axis('off')\nplt.show()","f679437d":"#creating frequency of words \nfreqs = {}\nfor i, speech in data_future.iterrows():\n    year = speech['year']\n    for token in speech['token']:\n        if token not in freqs:\n            freqs[token] = {\"total_freq\":1, year:1}\n        else:\n            freqs[token][\"total_freq\"] += 1\n            if not freqs[token].get(year):\n                freqs[token][year] = 1\n            else:\n                freqs[token][year] += 1","5cb3170b":"data_future['sentiment'] = np.array([ analize_sentiment(text) for text in data_future['text'] ])\ndata_future['polarity'] = np.array([ analize_polarity(text) for text in data_future['text'] ])\ndata_future.head()","d337e0cd":"data = [go.Bar(\n           x=data_future['year'],\n           y=data_future['sentiment']\n    )]\n\niplot(data, filename='basic-bar')","772cd6e9":"stop_words.add(\"india\")\nstop_words.add(\"pakistan\")\ndata_future['clean'] = data_future['token'].apply(lambda x: [w for w in x if not w in stop_words and not w in punctuation])\n","eeaa29bc":"data_future['stems'] = [[format(get_lemma(token)) for token in speech] for speech in data_future['clean']]","69c8c1fb":"data_future.head()","2ddbfc11":"#creating frequency of words \nfreqs = {}\nfor i, speech in data_future.iterrows():\n    year = speech['year']\n    for token in speech['stems']:\n        if token not in freqs:\n            freqs[token] = {\"total_freq\":1, year:1}\n        else:\n            freqs[token][\"total_freq\"] += 1\n            if not freqs[token].get(year):\n                freqs[token][year] = 1\n            else:\n                freqs[token][year] += 1","11dcfb1a":"freqs_df = pd.DataFrame.from_dict(freqs, orient='index')\nfreqs_df['word'] = freqs_df.index\nnew_cols = [\"total_freq\", \"word\"] + sorted(freqs_df.columns.tolist()[1:-1])\nfreqs_df = freqs_df[new_cols]\nfreqs_df = freqs_df.sort_values('total_freq', ascending=False)\nfreqs_df = freqs_df.fillna(0)\nfreqs_df.head()","6b8d09d4":"#plot to visualize top 100 words over the years\ntrace = go.Scatter(\n    x=freqs_df.iloc[0:100]['word']   ,   \n    y=freqs_df['total_freq']          \n    )\nlayout = dict(title = 'Frequency of Words used over years by Pakistan (India Obsessed Speches)',\n              xaxis = dict(title = 'Word'),\n              yaxis = dict(title = 'Count'),\n              )\nfig = dict(data= [trace], layout=layout)\niplot(fig, filename='basic-line')","e1d5c27a":"freqs_df[freqs_df['word'].isin(['nuclear', 'kashmir', 'weapon', 'kashmiri', 'afghanistan','muslim'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Word Frequency\",figsize=(16,8))","2e139a86":"text_data = list(data_future['clean'])","2cb01a35":"from gensim import corpora\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","37cd46d6":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","d40b0d9d":"# import pyLDAvis.gensim\n# lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n# lda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)\n# pyLDAvis.display(lda_display10)","5f639d2d":"I have commented the code the under mentioned. pyLDAvis changes the HTML too much and results in a weird view of the Kernel. While running locally one can uncomment an d visualize the topics, its very interesting. ","1678e5be":"**Visualzing most frequent Wordds using Word Clouds**\n\nA tag cloud is a novelty visual representation of text data, typically used to depict keyword metadata on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color. Here we visualize words from 1970-2016.","e49d6860":"What Pakistan says about India in UN ?","170d02fb":"Every year since 1947, representatives of UN member states gather at the annual sessions of the United Nations General Assembly. The centrepiece of each session is the General Debate. This is a forum at which leaders and other senior officials deliver statements that present their government\u2019s perspective on the major issues in world politics.\n\nThese statements are akin to the annual legislative state-of-the-union addresses in domestic politics. No other international forum provides all member states with the opportunity to deliver their state-of-the-world addresses in a comparable format.\n\n![UN General Debate](http:\/\/s3.amazonaws.com\/bucket.scribblelive.com\/333\/2016\/7\/12\/73533918-e02b-4d64-a89e-309500864d4b_1000.JPG)\n\nThis kernel focuses on debates by Indian delegation at the UN General Debates between 1970-2016.India was among the original members of the United Nations that signed the Declaration by United Nations at Washington, D.C. on 1944 October and also participated in the United Nations Conference on International Organization at San Francisco from 25 April to 26 June 1945. As a founding member of the United Nations, India strongly supports the purposes and principles of the UN and has made significant contributions in implementing the goals of the Charter, and the evolution of the UN's specialised programmes and agencies.\n\nIndia has been a member of the UN Security Council for seven terms (a total of 14 years), with the most recent being the 2011\u201312 term. India is a member of G4, group of nations who back each other in seeking a permanent seat on the Security Council and advocate in favour of the reformation of the UNSC. India is also part of the G-77.\n\nIndia is a charter member of the United Nations and participates in all of its specialised agencies and organizations. India has contributed troops to United Nations peacekeeping efforts in Korea, Egypt and the Congo in its earlier years and in Somalia, Angola, Haiti, Liberia, Lebanon and Rwanda in recent years, and more recently in the South Sudan conflict.\n\nThid kernal takes inspiration from [Word trends by LaurentBerder](https:\/\/www.kaggle.com\/lberder\/word-trends).","e8d07a35":"**Sentiment Analysis of Speech**\nIn this section we try analyze the sentiment of speeches given by Indian Delegation at the United Nations debates.  We make use of a simple Sentiment Analyzer named Vader from NLTK.  \n\nRefer the documentation [here](https:\/\/www.nltk.org\/_modules\/nltk\/sentiment\/vader.html)","28b3313a":"![](https:\/\/i.imgur.com\/rboi5A9.png)","12ea8197":"The above plot is a clutter,still figuring out how to enable hover function over scatter plots.","e3024419":"* Tokenize -  Tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n* Clean - Stop Word Removal is the process of removing the stop words. A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search queries.\n* Stemmer - Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\u2014generally a written word form.\n* Lemmatizer - Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .","1ce21647":"**Countries mentioned by India and related Visualization**\n\nIn this section, we try to find the countries mentioned by India in the speeches over the years. First we obtain the list of countries from an external package and later we find the mention of the countries in the text. We create a new dataframe to store the frequencies of countries mentioned by India.","0adedc9a":"**Finding Word Frequencies and Related Visualization**\n\nIn this section, first we find the frequency of different words used by the delegation in their speeches. Later, visualize them over to find particular patterns during the speeches over the years. Here, we take in consideration the stemmed words only because it makes more sense. For example, a stemmed word like terror make more sense than terrorist, terrorism, terrorising.","428fab6f":"Let's visualize the countries which mentioned India most times.","e7bfa762":"The kernal takes inspiration from two amazing kernals : https:\/\/www.kaggle.com\/joephilleo\/exploratory-data-analysis and https:\/\/www.kaggle.com\/lberder\/word-trends \n\nFeel free to fork and analyze for your country too.","d1a56ed5":"Lets, see which were the most frequent words except \"india\" in these speechs,","5934611f":"**Topic Extraction using Gensim**\n\nGensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Here, we use Gensim to get the topics in the text.\n","234d04ee":"Here we try to visualize the most frequent words ( top five ) every year from 1971 to 2016.","2365f575":"Lets, select only the talks where Pakistan mentioned \"India\" more than 10 times,","0babf37b":"Grouping the data per year ","f64eb22a":"Here's the topics Pakistan speaks about India at UN","cd364ef3":"**Bigram Word Cloud Visualization**\n\nRefer : [Wordclouds of titles and abstracts with n-grams\n by Peter Wittek](https:\/\/www.kaggle.com\/peterwittek\/wordclouds-of-titles-and-abstracts-with-n-grams)","4ef040b8":"We only select data which contains speeches given by Indian delegates at the UN General Debate sessions.","78d0d806":"**Mention of India by Other countries**","3e07069c":"**Pre-Processing**\n\nText Processing will be done in two levels:-\n* Extracting meta text data and creating new features\n* Process the text and create new features with tokens, cleaned text and stems.\n\n","1569f2ab":"![Pakistan Talks Topics](https:\/\/i.imgur.com\/cVcnkt8.png)"}}