{"cell_type":{"daa4e415":"code","556afc0b":"code","fe637ff5":"code","d5a919b3":"code","4aee8099":"code","8b51197c":"code","20f3d716":"code","a16850b2":"code","3b05015f":"code","bf73430c":"code","e0b30ca0":"code","14b66ec8":"code","0388aae8":"code","ff60230d":"code","4a406ff4":"code","e9c8df2b":"code","5a33410c":"code","deeefe9f":"code","142aeabc":"code","8f0e67c8":"code","af6a9d14":"code","ac7aa77b":"code","6087cea9":"code","ee5a6c10":"code","a5ab795d":"code","8910087c":"code","572075f9":"code","d02fb5b8":"code","e806ae33":"code","83fab584":"code","030e2c81":"code","6212fc31":"code","b9a15f74":"code","a79bcf04":"code","4f82691c":"code","a7c72d5c":"code","4e208667":"code","3d38dc60":"code","4593bb62":"code","9675ae79":"code","fc21cb05":"code","74518905":"code","51fefcc3":"code","eb53d711":"code","edd0e430":"code","342094ed":"code","0970723c":"code","3e73e70f":"code","3c47184e":"code","b088d218":"code","cdc38b33":"code","bb880f22":"code","6cc30319":"code","99ce08a6":"code","69f21900":"code","ac0da7d0":"code","5355c1eb":"code","bdbe36e3":"code","854abae6":"code","2ebb72f5":"code","9b935ef2":"code","5ce0e79c":"code","da407404":"code","864ddb65":"code","d9e06cbb":"code","db27890f":"code","fe19d6da":"code","e2242c6c":"code","1336583d":"code","889825b5":"code","58477082":"code","ef792f79":"code","ab4cbcff":"code","22f35aa6":"code","1ccb2c34":"code","c94e736c":"code","31de6446":"code","ee8b2eab":"code","99e8a758":"code","2d008afd":"code","b51b0f6c":"code","b82b50cd":"code","c3b44116":"code","bbbde608":"code","25e2d20f":"code","c15d84c6":"code","6b760a1e":"code","d5255bfa":"code","8d2d7e9b":"code","8a6d8f4f":"code","b821e291":"code","327607e1":"code","a628aced":"code","2848d7ab":"code","34443ed9":"code","33e1076e":"code","b8b9d7c9":"code","ea689f9f":"code","9f0baa00":"code","d51b01ae":"code","13f0aeb4":"code","0b55fa8f":"code","73c069a9":"code","c8e6baf4":"code","6f808691":"code","8455bb7e":"code","54331c64":"code","c4ba6cf0":"code","a63f67b2":"code","71eb23d4":"code","b2cc17c2":"code","294fb539":"code","452f5cbf":"code","631adddb":"code","fd8003e5":"code","c5794e28":"code","e70635b3":"code","92269a63":"code","3b70e8da":"markdown","e23e4796":"markdown","063c15b0":"markdown","332727d2":"markdown","43029285":"markdown","9036e3e7":"markdown","ad38f6e8":"markdown","ef60d00e":"markdown","36465143":"markdown","383a5024":"markdown","cdf03e79":"markdown","7b4bfd32":"markdown","df8d3183":"markdown","5f479fed":"markdown","45ab5086":"markdown","46dbca1c":"markdown","bc375da2":"markdown","bb689c9b":"markdown","3655fdd5":"markdown","0901f135":"markdown","ea5091b4":"markdown","dd05be64":"markdown","b996072b":"markdown","31a40887":"markdown","c77904ac":"markdown","9b7534ae":"markdown","42b03d3d":"markdown","ca2ef254":"markdown","f5525a65":"markdown","ac8ff2f1":"markdown","05bfc7df":"markdown","0ccff4d0":"markdown","748fd057":"markdown","59c78f70":"markdown"},"source":{"daa4e415":"# Importing the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings('ignore')","556afc0b":"# Importing the dataset\n\ndf = pd.read_csv('..\/input\/creditcard.csv')\n\n# Getting a look at our data\ndf.head()","fe637ff5":"df.describe()","d5a919b3":"# First of all let us check if we have any NULL values in the features that we are given. This is always a good practise to do before we start any problem.\ndf.isnull().sum().max()","4aee8099":"# Checking how skewed our dataset is : \n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","8b51197c":"# Visualising how skewed our data is:\n\nexplodeTuple = (0.1, 0)\npieLabels = 'Not Frauds', 'Frauds'\nfigureObject, axesObject = plt.subplots()\ncolors = (\"yellow\",\"red\")\nshares = [round(df['Class'].value_counts()[0]\/len(df) * 100,2),round(df['Class'].value_counts()[1]\/len(df) * 100,2) ]\naxesObject.pie(shares,explode = explodeTuple,\n        colors = colors,\n\n        labels=pieLabels,\n\n        autopct='%1.2f',\n\n        startangle=90)","20f3d716":"# All our features have been scaled except the time and amount column. Hence, first we plot them to get a better understanding of the distribution of each.\n\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()\n","a16850b2":"# Finding the number of fraud cases in our dataset\ndf['Class'].value_counts()[1]","3b05015f":"# Scaling the time and amount columns\nfrom sklearn.preprocessing import RobustScaler\n\n# We use RobustScaler because it is less prone to outliers.\n\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","bf73430c":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","e0b30ca0":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","14b66ec8":"# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\nnew_df.head()","0388aae8":"# Seeing the equally balanced distribution of classes\n\nexplodeTuple = (0.1, 0)\npieLabels = 'Not Frauds', 'Frauds'\nfigureObject, axesObject = plt.subplots()\ncolors = (\"yellow\",\"red\")\nshares = [round(new_df['Class'].value_counts()[0]\/len(new_df) * 100,2),round(new_df['Class'].value_counts()[1]\/len(new_df) * 100,2) ]\naxesObject.pie(shares,explode = explodeTuple,\n        colors = colors,\n\n        labels=pieLabels,\n\n        autopct='%1.2f',\n\n        startangle=90)","ff60230d":"# Creating the correlation matrix\n\ncorr_matrix = new_df.corr()\nprint(corr_matrix[\"Class\"].sort_values(ascending=False))","4a406ff4":"# Visualising the correlation matrix using a heatmap\n\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_matrix[['Class']].sort_values(by=['Class'],ascending=False),\n            vmin=-1,\n            cmap='coolwarm',\n            annot=True);","e9c8df2b":"# Finding the index of all the outliers\n\n# 1) V14\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nv14_q1, v14_q3 = np.percentile(v14_fraud, [25, 75])\nv14_iqr = v14_q3 - v14_q1\nv14_lower_bound = v14_q1 - (1.5 * v14_iqr)\nv14_upper_bound = v14_q3 + (1.5 * v14_iqr)\noutliers_v14 = [x for x in v14_fraud if x < v14_lower_bound or x > v14_upper_bound]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers_v14)))\nprint('V14 outliers:{}'.format(outliers_v14))\noutliers_index_v14 = []\nfor i in range(len(v14_fraud)):\n    if v14_fraud[i] < v14_lower_bound or v14_fraud[i] > v14_upper_bound:\n        outliers_index_v14.append(i)\nprint(outliers_index_v14)   \nprint('----' * 28)\n\n# 2) V12\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nv12_q1, v12_q3 = np.percentile(v12_fraud, [25, 75])\nv12_iqr = v12_q3 - v12_q1\nv12_lower_bound = v12_q1 - (1.5 * v12_iqr)\nv12_upper_bound = v12_q3 + (1.5 * v12_iqr)\noutliers_v12 = [x for x in v12_fraud if x < v12_lower_bound or x > v12_upper_bound]\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers_v12)))\nprint('V12 outliers:{}'.format(outliers_v12))\noutliers_index_v12 = []\nfor i in range(len(v12_fraud)):\n    if v12_fraud[i] < v12_lower_bound or v12_fraud[i] > v12_upper_bound:\n        outliers_index_v12.append(i)\nprint(outliers_index_v12) \nprint('----' * 28)\n\n# 3) V10\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nv10_q1, v10_q3 = np.percentile(v10_fraud, [25, 75])\nv10_iqr = v10_q3 - v10_q1\nv10_lower_bound = v10_q1 - (1.5 * v10_iqr)\nv10_upper_bound = v10_q3 + (1.5 * v10_iqr)\noutliers_v10 = [x for x in v10_fraud if x < v10_lower_bound or x > v10_upper_bound]\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers_v10)))\nprint('V10 outliers:{}'.format(outliers_v10))\noutliers_index_v10 = []\nfor i in range(len(v10_fraud)):\n    if v10_fraud[i] < v10_lower_bound or v10_fraud[i] > v10_upper_bound:\n        outliers_index_v10.append(i)\nprint(outliers_index_v10) \nprint('----' * 28)\n\n# 4) V11\nv11_fraud = new_df['V11'].loc[new_df['Class'] == 1].values\nv11_q1, v11_q3 = np.percentile(v11_fraud, [25, 75])\nv11_iqr = v11_q3 - v11_q1\nv11_lower_bound = v11_q1 - (1.5 * v11_iqr)\nv11_upper_bound = v11_q3 + (1.5 * v11_iqr)\noutliers_v11 = [x for x in v11_fraud if x < v11_lower_bound or x > v11_upper_bound]\nprint('Feature V11 Outliers for Fraud Cases: {}'.format(len(outliers_v11)))\nprint('V11 outliers:{}'.format(outliers_v11))\noutliers_index_v11 = []\nfor i in range(len(v11_fraud)):\n    if v11_fraud[i] < v11_lower_bound or v11_fraud[i] > v11_upper_bound:\n        outliers_index_v11.append(i)\nprint(outliers_index_v11)  \nprint('----' * 28)","5a33410c":"# Merging all the outliers index list into one list:\n\nmerged_list = outliers_index_v14 + outliers_index_v10 + outliers_index_v12 + outliers_index_v11\n\n# Function to find the unique index from te merged list:\n\ndef unique(merged_list): \n      \n    # insert the list to the set \n    list_set = set(merged_list) \n    # convert the set to the list \n    unique_list = (list(list_set)) \n    for x in unique_list: \n        print(x)\n    return unique_list    \n\nunique_list = unique(merged_list)    ","deeefe9f":"# Removing the outiers\n\nfor i in range(len(unique_list)):\n    new_df.drop(new_df.index[unique_list[i]], inplace = True)","142aeabc":"new_df.shape","8f0e67c8":"# Applying t-SNE\n\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nplt.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nplt.title('t-SNE', fontsize=14)\nplt.legend(handles=[blue_patch, red_patch])\nplt.show()","af6a9d14":"from sklearn.model_selection import train_test_split\n\n# Dividing the dataset nto X and y\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# Splitting into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ac7aa77b":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","6087cea9":"# Implementing the classifiers(with their default parameters):\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier()\n}","ee5a6c10":"# Finding the cross_val score for our simple default classification algorithms:\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", training_score.mean() * 100, \"% accuracy score\")","a5ab795d":"# Using GridSearchCV to find the best parameters for our classification models:\n\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n\n# Logistic Regression best classifier\nlog_reg = grid_log_reg.best_estimator_\nprint(grid_log_reg.best_params_)\nprint(\"-\" * 115)\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\nprint(grid_knears.best_params_)\nprint(\"-\" * 115)\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\nprint(grid_svc.best_params_)\nprint(\"-\" * 115)\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# Tree best estimator\ntree_clf = grid_tree.best_estimator_\nprint(grid_tree.best_params_)\nprint(\"-\" * 115)\n\n# Random Forest Classifier\nforest_params = {\"n_estimators\":[10, 100], \"criterion\":[\"gini\", \"entropy\"], \"min_samples_split\":list(range(2, 5, 1))}\ngrid_forest = GridSearchCV(RandomForestClassifier(), forest_params)\ngrid_forest.fit(X_train, y_train)\n\n# Forest best estimator\nforest_clf = grid_forest.best_estimator_\nprint(grid_forest.best_params_)","8910087c":"# Now let us see the cross_val score of our best models that we found using the GridSearchCV:\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv = 5)\nprint(\"Best Logistic Regression model has a cross-validation score of \",log_reg_score.mean() * 100, \"on the training set\")\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv = 5)\nprint(\"Best KNearest Neighbors model has a cross-validation score of \", knears_score.mean() * 100, \"on the training set\")\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv = 5)\nprint(\"Best SVC model has a cross-validation score of \", svc_score.mean() * 100, \"on the training set\")\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv = 5)\nprint(\"Best DecisionTree model has a cross-validation score of \", tree_score.mean() * 100, \"on the training set\")\n\nforest_score = cross_val_score(forest_clf, X_train, y_train, cv = 5)\nprint(\"Best RandomForest model has a cross-validation score of \", forest_score.mean() * 100, \"on the training set\")","572075f9":"# Now, we also find the cross-validation score on the testing data because we want our model to do well on both the training as well as the testing data:\n\nlog_reg_score_test = cross_val_score(log_reg, X_test, y_test, cv = 5)\nprint(\"Best Logistic Regression model has a cross-validation score of \",log_reg_score_test.mean() * 100, \"on the testing set\")\n\nknears_score_test = cross_val_score(knears_neighbors, X_test, y_test, cv = 5)\nprint(\"Best KNearest Neighbors model has a cross-validation score of \", knears_score_test.mean() * 100, \"on the testing set\")\n\nsvc_score_test = cross_val_score(svc, X_test, y_test, cv = 5)\nprint(\"Best SVC model has a cross-validation score of \", svc_score_test.mean() * 100, \"on the testing set\")\n\ntree_score_test = cross_val_score(tree_clf, X_test, y_test, cv = 5)\nprint(\"Best DecisionTree model has a cross-validation score of \", tree_score_test.mean() * 100, \"on the testing set\")\n\nforest_score_test = cross_val_score(forest_clf, X_test, y_test, cv = 5)\nprint(\"Best RandomForest model has a cross-validation score of \", forest_score_test.mean() * 100, \"on the testing set\")","d02fb5b8":"# Finding the accuracy score on the original testing dataset:\n\nlog_reg_score_test_orig = cross_val_score(log_reg, original_Xtest, original_ytest, cv = 2)\nprint(\"Best Logistic Regression model has a cross-validation score of \",log_reg_score_test_orig.mean() * 100, \"on the testing set\")\n\nknears_score_test_orig = cross_val_score(knears_neighbors, original_Xtest, original_ytest, cv = 2)\nprint(\"Best KNearest Neighbors model has a cross-validation score of \", knears_score_test_orig.mean() * 100, \"on the testing set\")\n\nsvc_score_test_orig = cross_val_score(svc, original_Xtest, original_ytest, cv = 2)\nprint(\"Best SVC model has a cross-validation score of \", svc_score_test_orig.mean() * 100, \"on the testing set\")\n\ntree_score_test_orig = cross_val_score(tree_clf, original_Xtest, original_ytest, cv = 2)\nprint(\"Best DecisionTree model has a cross-validation score of \", tree_score_test_orig.mean() * 100, \"on the testing set\")\n\nforest_score_test_orig = cross_val_score(forest_clf, original_Xtest, original_ytest, cv = 2)\nprint(\"Best RandomForest model has a cross-validation score of \", forest_score_test_orig.mean() * 100, \"on the testing set\")","e806ae33":"# Now let us plot the learning curve for our classifiers:\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # Logistic Regression Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # KNearestNeighbors Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # SVC Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Decision Tree Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size ')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    return plt\n\ndef plot_learning_curve2(estimator, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, (ax1) = plt.subplots(1, 1, figsize=(10,7), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # Random Forest Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.set_title(\"Random Forest Classifier Learning Curve\", fontsize = 14)\n    ax1.legend(loc=\"best\")\n    return plt","83fab584":"# Plotting the learning curves:\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=-1)\nplot_learning_curve2(forest_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=-1)","030e2c81":"# First, lets save all the predictions that our best classifiers make for the training set\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\n\nforest_pred = cross_val_predict(forest_clf, X_train, y_train, cv = 5)","6212fc31":"# Finding the roc_auc scores:\nfrom sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\nprint('Random Forest Classifier: ', roc_auc_score(y_train, forest_pred))","b9a15f74":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\ncross_val = KFold(n_splits=3, random_state=42)\nscores1 = cross_val_score(log_reg, X_train, y_train, cv=cross_val, scoring='roc_auc')\nprint(\"Mean AUC Score - Logistic Regression: \", scores1.mean())\nscores2 = cross_val_score(knears_neighbors, X_train, y_train, cv=cross_val, scoring='roc_auc')\nprint(\"Mean AUC Score - KNears Neigbors: \", scores2.mean())\nscores3 = cross_val_score(svc, X_train, y_train, cv=cross_val, scoring='roc_auc')\nprint(\"Mean AUC Score - SVC: \", scores3.mean())\nscores4 = cross_val_score(tree_clf, X_train, y_train, cv=cross_val, scoring='roc_auc')\nprint(\"Mean AUC Score - Decision Tree: \", scores4.mean())\nscores5 = cross_val_score(forest_clf, X_train, y_train, cv=cross_val, scoring='roc_auc')\nprint(\"Mean AUC Score - Random Forest: \", scores5.mean())","a79bcf04":"# Plotting the ROC curve:\nlog_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nforest_fpr, forest_tpr, forest_threshold = roc_curve(y_train, forest_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr, forest_fpr, forest_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 5 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(forest_fpr, forest_tpr, label = 'Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr, forest_fpr, forest_tpr)\nplt.show()","4f82691c":"# We first find the predictions of our best classifiers on the original test dataset because ultimately that is the one that we want to do well on. Also, it is unbalanced.\n\ny_score_log_reg = cross_val_predict(log_reg, original_Xtest, original_ytest, cv=2,\n                             method=\"decision_function\")\n\ny_score_knears = cross_val_predict(knears_neighbors, original_Xtest, original_ytest, cv=2)\n\ny_score_svc = cross_val_predict(svc, original_Xtest, original_ytest, cv=2,\n                             method=\"decision_function\")\n\ny_score_tree = cross_val_predict(tree_clf, original_Xtest, original_ytest, cv=2)\n\ny_score_forest = cross_val_predict(forest_clf, original_Xtest, original_ytest, cv = 2)","a7c72d5c":"from sklearn.metrics import average_precision_score\n\naverage_precision_log_reg = average_precision_score(original_ytest, y_score_log_reg)\nprint('Average precision-recall score for Logistic Regression: {0:0.2f}'.format(average_precision_log_reg))\n\naverage_precision_knears = average_precision_score(original_ytest, y_score_knears)\nprint('Average precision-recall score for KNears Neighbors: {0:0.2f}'.format(average_precision_knears))\n\naverage_precision_svc = average_precision_score(original_ytest, y_score_svc)\nprint('Average precision-recall score for SVC: {0:0.2f}'.format(average_precision_svc))\n\naverage_precision_tree = average_precision_score(original_ytest, y_score_tree)\nprint('Average precision-recall score for Decision Tree: {0:0.2f}'.format(average_precision_tree))\n\naverage_precision_forest = average_precision_score(original_ytest, y_score_forest)\nprint('Average precision-recall score for Random Forest: {0:0.2f}'.format(average_precision_forest))","4e208667":"from sklearn.metrics import precision_recall_curve\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\nprecision_log_reg, recall_log_reg, _ = precision_recall_curve(original_ytest, y_score_log_reg)\n\nax1.step(recall_log_reg, precision_log_reg, color='#004a93', alpha=0.2,\n         where='post')\nax1.fill_between(recall_log_reg, precision_log_reg, step='post', alpha=0.2,\n                 color='y')\n\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax1.set_ylim([0.0, 1.05])\nax1.set_xlim([0.0, 1.0])\nax1.set_title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score for Logistic Regression ', fontsize=16)\n\nprecision_knears, recall_knears, _ = precision_recall_curve(original_ytest, y_score_knears)\n\nax2.step(recall_knears, precision_knears, color='#004a93', alpha=0.2,\n         where='post')\nax2.fill_between(recall_knears, precision_knears, step='post', alpha=0.2,\n                 color='y')\n\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_ylim([0.0, 1.05])\nax2.set_xlim([0.0, 1.0])\nax2.set_title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score for KNears Neighbors ', fontsize=16)\n\nprecision_svc, recall_svc, _ = precision_recall_curve(original_ytest, y_score_svc)\n\nax3.step(recall_svc, precision_svc, color='#004a93', alpha=0.2,\n         where='post')\nax3.fill_between(recall_svc, precision_svc, step='post', alpha=0.2,\n                 color='y')\n\nax3.set_xlabel('Recall')\nax3.set_ylabel('Precision')\nax3.set_ylim([0.0, 1.05])\nax3.set_xlim([0.0, 1.0])\nax3.set_title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score for SVC ', fontsize=16)\n\nprecision_forest, recall_forest, _ = precision_recall_curve(original_ytest, y_score_forest)\n\nax4.step(recall_forest, precision_forest, color='#004a93', alpha=0.2,\n         where='post')\nax4.fill_between(recall_forest, precision_forest, step='post', alpha=0.2,\n                 color='y')\n\nax4.set_xlabel('Recall')\nax4.set_ylabel('Precision')\nax4.set_ylim([0.0, 1.05])\nax4.set_xlim([0.0, 1.0])\nax4.set_title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score for Random Forest ', fontsize=16)\n\nf, (ax1) = plt.subplots(1, 1, figsize=(8,5), sharey=True)\nprecision_tree, recall_tree, _ = precision_recall_curve(original_ytest, y_score_tree)\n\nax1.step(recall_tree, precision_tree, color='#004a93', alpha=0.2,\n         where='post')\nax1.fill_between(recall_tree, precision_tree, step='post', alpha=0.2,\n                 color='y')\n\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax1.set_ylim([0.0, 1.05])\nax1.set_xlim([0.0, 1.0])\nax1.set_title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score for Decision Tree ', fontsize=16)\n\n","3d38dc60":"# Finding predictions for all the classifiers:\n\ny_pred_log_reg = log_reg.predict(original_Xtest)\n\ny_pred_knears = knears_neighbors.predict(original_Xtest)\n\ny_pred_svc = svc.predict(original_Xtest)\n\ny_pred_forest = forest_clf.predict(original_Xtest)\n\ny_pred_tree = tree_clf.predict(original_Xtest)\n","4593bb62":"# Printing the classification report for all the classifiers:\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(\"Logistic Regression:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_log_reg))\nprint(classification_report(original_ytest, y_pred_log_reg))\n\nprint(\"KNearestNeighbors:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_knears))\nprint(classification_report(original_ytest, y_pred_knears))\n\nprint(\"Support Vector Classifier:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_svc))\nprint(classification_report(original_ytest, y_pred_svc))\n\nprint(\"Random Forest Calssifier:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_forest))\nprint(classification_report(original_ytest, y_pred_forest))\n\nprint(\"Decision Tree Classifer:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_tree))\nprint(classification_report(original_ytest, y_pred_tree))","9675ae79":"# Visualising the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n \ncm_log = confusion_matrix(original_ytest, y_pred_log_reg)\n\ncm_knears = confusion_matrix(original_ytest, y_pred_knears)\n\ncm_svc = confusion_matrix(original_ytest, y_pred_svc)\n\ncm_tree = confusion_matrix(original_ytest, y_pred_tree)\n\ncm_forest = confusion_matrix(original_ytest, y_pred_forest)\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n\n# Logistic Regresiion Confusion Matrix\n\nax1.imshow(cm_log, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for Logistic Regression')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_log[i][j]))\n  \n# KNearest Neighbors Confusion Matrix\n\nax2.imshow(cm_knears, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for KNearest Neighbors Classifier')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_knears[i][j]))\n        \n# Decision Tree Confusion Matrix\n\nax3.imshow(cm_tree, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax3.set_title('Confusion Matrix for Decision Tree Classifier')\nax3.set_ylabel('True label')\nax3.set_xlabel('Predicted label')\nax3.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax3.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax3.text(j,i, str(s[i][j])+\" = \"+str(cm_tree[i][j]))\n\n# Random Forest Confusion Matrix\n\nax4.imshow(cm_forest, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax4.set_title('Confusion Matrix for Random Forest Classifier')\nax4.set_ylabel('True label')\nax4.set_xlabel('Predicted label')\nax4.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax4.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax4.text(j,i, str(s[i][j])+\" = \"+str(cm_forest[i][j]))\n        \n# Support Vector Confusion Matrix\n\nf,(ax5) = plt.subplots(1,1, figsize = (6, 6), sharey = True)\nax5.imshow(cm_svc, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax5.set_title('Confusion Matrix for Support Vector Classifier')\nax5.set_ylabel('True label')\nax5.set_xlabel('Predicted label')\nax5.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax5.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax5.text(j,i, str(s[i][j])+\" = \"+str(cm_svc[i][j]))\n        \n\n","fc21cb05":"from sklearn.metrics import f1_score\n\nprint(\"Logisic Regression:\", f1_score(original_ytest, y_pred_log_reg, average='weighted'))  \nprint(\"KNearsNeighbors:\", f1_score(original_ytest, y_pred_knears, average='weighted')) \nprint(\"SVC:\", f1_score(original_ytest, y_pred_svc, average='weighted')) \nprint(\"DecisionTree Classifier:\", f1_score(original_ytest, y_pred_tree, average='weighted')) \nprint(\"RandomForest Classifier:\", f1_score(original_ytest, y_pred_forest, average='weighted')) ","74518905":"# Implementing SMOTE to create synthetic data points\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(original_ytrain==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(original_ytrain==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(original_Xtrain, original_ytrain.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","51fefcc3":"# Training our classification algorithms for the over-sampled data:\n\nclassifiers = {\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier()\n}","eb53d711":"# First we divide the over-sampled data into cross-validation and training data:\n\nX_training_res, X_val_res, y_training_res, y_val_res = train_test_split(X_train_res, y_train_res, test_size=0.2, random_state=42)","edd0e430":"# Finding the cross_val score for our simple default classification algorithms on our over-sampled data:\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_training_res, y_training_res)\n    cross_score = cross_val_score(classifier, X_val_res, y_val_res, cv = 2)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a Cross-Validation score of\", cross_score.mean() * 100, \"%\")","342094ed":"# Now we use GridSearchCv to find the best parameters for our Logisitc Regression classifier only as all the other models are already doing well with their default parameters:\n\"\"\"\"\"\"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg_smote = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg_smote.fit(X_training_res, y_training_res)\n\n# Logistic Regression best classifier\nlog_reg_smote = grid_log_reg_smote.best_estimator_\nprint(grid_log_reg_smote.best_params_)\n\"\"\"\"\"\"\"","0970723c":"# Seeing if our best Logistic Regression model has a better cross-validation score:\n\nlog_reg_smote = LogisticRegression(C = 100, penalty = 'l2')\nlog_reg_smote.fit(X_training_res, y_training_res)\ncross_score_log = cross_val_score(log_reg_smote, X_val_res, y_val_res, cv = 2)\nprint(\"Our best Logistic Regression model has a Cross-Validation score of\", cross_score_log.mean() * 100, \"%\")","3e73e70f":"classifiers = {\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"LogisiticRegression\": LogisticRegression(C = 100, penalty = 'l2'),\n    \"KNearest\": KNeighborsClassifier()\n}","3c47184e":"# Finding their cross_val score for the original testing data:\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_training_res, y_training_res)\n    testing_score = cross_val_score(classifier, original_Xtest, original_ytest, cv = 2)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a testing score of\", testing_score.mean() * 100, \"%\")","b088d218":"# Making objects for our classifiers:\nknears_smote = KNeighborsClassifier()\nknears_smote.fit(X_training_res, y_training_res)\n\nforest_clf_smote = RandomForestClassifier()\nforest_clf_smote.fit(X_training_res, y_training_res)\n\ntree_clf_smote = DecisionTreeClassifier()\ntree_clf_smote.fit(X_training_res, y_training_res)","cdc38b33":"# First, lets save all the predictions that our best classifiers make for the training set\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred_smote = cross_val_predict(log_reg_smote, X_training_res, y_training_res, cv=2,\n                             method=\"decision_function\")\n\nknears_pred_smote = cross_val_predict(knears_smote, X_training_res, y_training_res, cv=2)\n\ntree_pred_smote = cross_val_predict(tree_clf_smote, X_training_res, y_training_res, cv=2)\n\nforest_pred_smote = cross_val_predict(forest_clf_smote, X_training_res, y_training_res, cv = 2)","bb880f22":"# Finding the roc_auc scores:\nfrom sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_training_res, log_reg_pred_smote))\nprint('KNears Neighbors: ', roc_auc_score(y_training_res, knears_pred_smote))\nprint('Decision Tree Classifier: ', roc_auc_score(y_training_res, tree_pred_smote))\nprint('Random Forest Classifier: ', roc_auc_score(y_training_res, forest_pred_smote))","6cc30319":"# We first find the predictions of our best classifiers on the original test dataset because ultimately that is the one that we want to do well on. Also, it is unbalanced.\n\ny_score_log_reg_smote = cross_val_predict(log_reg_smote, original_Xtest, original_ytest, cv=2,\n                             method=\"decision_function\")\n\ny_score_knears_smote = cross_val_predict(knears_smote, original_Xtest, original_ytest, cv=2)\n\ny_score_tree_smote = cross_val_predict(tree_clf_smote, original_Xtest, original_ytest, cv=2)\n\ny_score_forest_smote = cross_val_predict(forest_clf_smote, original_Xtest, original_ytest, cv = 2)","99ce08a6":"# Now we use precision-recall curves and classification report to see which model actually performs good:\nfrom sklearn.metrics import average_precision_score\n\naverage_precision_log_reg_smote = average_precision_score(original_ytest, y_score_log_reg_smote)\nprint('Average precision-recall score for Logistic Regression: {0:0.2f}'.format(average_precision_log_reg_smote))\n\naverage_precision_knears_smote = average_precision_score(original_ytest, y_score_knears_smote)\nprint('Average precision-recall score for KNears Neighbors: {0:0.2f}'.format(average_precision_knears_smote))\n\naverage_precision_tree_smote = average_precision_score(original_ytest, y_score_tree_smote)\nprint('Average precision-recall score for Decision Tree: {0:0.2f}'.format(average_precision_tree_smote))\n\naverage_precision_forest_smote = average_precision_score(original_ytest, y_score_forest_smote)\nprint('Average precision-recall score for Random Forest: {0:0.2f}'.format(average_precision_forest_smote))","69f21900":"from sklearn.metrics import precision_recall_curve\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\nprecision_log_reg_smote, recall_log_reg_smote, _ = precision_recall_curve(original_ytest, y_score_log_reg_smote)\n\nax1.step(recall_log_reg_smote, precision_log_reg_smote, color='#004a93', alpha=0.2,\n         where='post')\nax1.fill_between(recall_log_reg_smote, precision_log_reg_smote, step='post', alpha=0.2,\n                 color='y')\n\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax1.set_ylim([0.0, 1.05])\nax1.set_xlim([0.0, 1.0])\nax1.set_title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score for Logistic Regression ', fontsize=16)\n\nprecision_knears_smote, recall_knears_smote, _ = precision_recall_curve(original_ytest, y_score_knears_smote)\n\nax2.step(recall_knears_smote, precision_knears_smote, color='#004a93', alpha=0.2,\n         where='post')\nax2.fill_between(recall_knears_smote, precision_knears_smote, step='post', alpha=0.2,\n                 color='y')\n\nax2.set_xlabel('Recall')\nax2.set_ylabel('Precision')\nax2.set_ylim([0.0, 1.05])\nax2.set_xlim([0.0, 1.0])\nax2.set_title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score for KNears Neighbors ', fontsize=16)\n\nprecision_tree_smote, recall_tree_smote, _ = precision_recall_curve(original_ytest, y_score_tree_smote)\n\nax3.step(recall_tree_smote, precision_tree_smote, color='#004a93', alpha=0.2,\n         where='post')\nax3.fill_between(recall_tree_smote, precision_tree_smote, step='post', alpha=0.2,\n                 color='y')\n\nax3.set_xlabel('Recall')\nax3.set_ylabel('Precision')\nax3.set_ylim([0.0, 1.05])\nax3.set_xlim([0.0, 1.0])\nax3.set_title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score for Decision Tree ', fontsize=16)\n\nprecision_forest_smote, recall_forest_smote, _ = precision_recall_curve(original_ytest, y_score_forest_smote)\n\nax4.step(recall_forest_smote, precision_forest_smote, color='#004a93', alpha=0.2,\n         where='post')\nax4.fill_between(recall_forest_smote, precision_forest_smote, step='post', alpha=0.2,\n                 color='y')\n\nax4.set_xlabel('Recall')\nax4.set_ylabel('Precision')\nax4.set_ylim([0.0, 1.05])\nax4.set_xlim([0.0, 1.0])\nax4.set_title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score for Random Forest ', fontsize=16)","ac0da7d0":"# Finding predictions for all the classifiers:\n\ny_pred_log_reg_smote = log_reg_smote.predict(original_Xtest)\n\ny_pred_knears_smote = knears_smote.predict(original_Xtest)\n\ny_pred_forest_smote = forest_clf_smote.predict(original_Xtest)\n\ny_pred_tree_smote = tree_clf_smote.predict(original_Xtest)\n","5355c1eb":"# Printing the classification report for all the classifiers:\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(\"Logistic Regression:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_log_reg_smote))\nprint(classification_report(original_ytest, y_pred_log_reg_smote))\n\nprint(\"KNearestNeighbors:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_knears_smote))\nprint(classification_report(original_ytest, y_pred_knears_smote))\n\nprint(\"Random Forest Calssifier:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_forest_smote))\nprint(classification_report(original_ytest, y_pred_forest_smote))\n\nprint(\"Decision Tree Classifer:\")\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, y_pred_tree_smote))\nprint(classification_report(original_ytest, y_pred_tree_smote))","bdbe36e3":"# Visualising the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n \ncm_log_smote = confusion_matrix(original_ytest, y_pred_log_reg_smote)\n\ncm_knears_smote = confusion_matrix(original_ytest, y_pred_knears_smote)\n\ncm_tree_smote = confusion_matrix(original_ytest, y_pred_tree_smote)\n\ncm_forest_smote = confusion_matrix(original_ytest, y_pred_forest_smote)\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n\n# Logistic Regresiion Confusion Matrix\n\nax1.imshow(cm_log_smote, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for Logistic Regression')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_log_smote[i][j]))\n\n# KNearest Neighbors Confusion Matrix  \n\nax2.imshow(cm_knears_smote, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for KNearest Neighbors Classifier')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_knears_smote[i][j]))\n        \n# Decision Tree Confusion Matrix\n\nax3.imshow(cm_tree_smote, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax3.set_title('Confusion Matrix for Decision Tree Classifier')\nax3.set_ylabel('True label')\nax3.set_xlabel('Predicted label')\nax3.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax3.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax3.text(j,i, str(s[i][j])+\" = \"+str(cm_tree_smote[i][j]))\n\n# Random Forest Confusion Matrix\n\nax4.imshow(cm_forest_smote, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax4.set_title('Confusion Matrix for Random Forest Classifier')\nax4.set_ylabel('True label')\nax4.set_xlabel('Predicted label')\nax4.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax4.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax4.text(j,i, str(s[i][j])+\" = \"+str(cm_forest_smote[i][j]))\n        \n\n","854abae6":"# Finding the F1 scores for all our classifiers:\n\nfrom sklearn.metrics import f1_score\n\nprint(\"Logisic Regression:\", f1_score(original_ytest, y_pred_log_reg_smote, average='weighted'))  \nprint(\"KNearsNeighbors:\", f1_score(original_ytest, y_pred_knears_smote, average='weighted')) \nprint(\"DecisionTree Classifier:\", f1_score(original_ytest, y_pred_tree_smote, average='weighted')) \nprint(\"RandomForest Classifier:\", f1_score(original_ytest, y_pred_forest_smote, average='weighted')) ","2ebb72f5":"# Importing the required libraries and functions:\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.layers import Dropout","9b935ef2":"# Building the first Neural Network model which trains on the randomly Under-Sampled data\n\ninput_size = X_train.shape[1]\n\nundersample_NN = Sequential()\nundersample_NN.add(Dense(input_size, input_shape = (input_size, ), activation = 'relu'))\nundersample_NN.add(Dense(32, activation = 'relu'))\nundersample_NN.add(Dropout(0.4))\nundersample_NN.add(Dense(64, activation = 'relu'))\n#undersample_NN.add(Dropout(0.4))\nundersample_NN.add(Dense(2, activation = 'softmax'))","5ce0e79c":"# Getting the summary of our under sample model architecture:\nundersample_NN.summary()","da407404":"# Compiling the undersample Neaural Network model\n\nundersample_NN.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","864ddb65":"# Fitting the model\n\nundersample_NN.fit(X_train, y_train, validation_split = 0.2, batch_size = 25, epochs = 10, shuffle = True, verbose = 2)","d9e06cbb":"# Predicitng the outcomes for the original test dataset\n\nundersample_predictions_original = undersample_NN.predict(original_Xtest, batch_size = 200, verbose = 0)","db27890f":"# Predicitng the classes of transactions for the original test dataset\n\nundersample_predictions = undersample_NN.predict_classes(original_Xtest, batch_size = 200, verbose = 0)","fe19d6da":"# Building the second Neural Network which trains on the Over-Sampled data\n\ninput_size = X_train_res.shape[1]\n\noversample_NN = Sequential()\noversample_NN.add(Dense(input_size, input_shape = (input_size, ), activation = 'relu'))\noversample_NN.add(Dense(32, activation = 'relu'))\noversample_NN.add(Dropout(0.4))\noversample_NN.add(Dense(64, activation = 'relu'))\noversample_NN.add(Dropout(0.4))\noversample_NN.add(Dense(2, activation = 'softmax'))","e2242c6c":"# Getting the summary of our over sampled model architecture:\noversample_NN.summary()","1336583d":"# Compiling the over-sampled Neural Network model:\n\noversample_NN.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","889825b5":"# Fitting the model\n\noversample_NN.fit(X_train_res, y_train_res, validation_split = 0.2, batch_size = 25, epochs = 5, shuffle = True, verbose = 2)","58477082":"# Predicitng the outcomes for the original test dataset\n\noversample_predictions_original = oversample_NN.predict(original_Xtest, batch_size = 200, verbose = 0)","ef792f79":"# Predicting the classes of transactions for the original test dataset\n\noversample_predictions = oversample_NN.predict_classes(original_Xtest, batch_size = 200, verbose = 0)","ab4cbcff":"# Visualising the performance of our Neural Networks by making the confusion matrix:\n\ncm_undersample_NN = confusion_matrix(original_ytest, undersample_predictions)\ncm_oversample_NN = confusion_matrix(original_ytest, oversample_predictions)\ncm_ideal = confusion_matrix(original_ytest, original_ytest)\nf, ((ax1, ax2)) = plt.subplots(1,2, figsize=(10,7), sharey=True)\n\n# Under-Sampled Data Neural Network Confusion Matrix\n\nax1.imshow(cm_undersample_NN, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for the Under-Sampled \\ndata Neural Network')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_undersample_NN[i][j]))\n        \n        \n# Over-Sampled Data Neural Network Confusion Matrix\n\nax2.imshow(cm_oversample_NN, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for the Over-Sampled \\ndata Neural Network')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_oversample_NN[i][j]))\n        \nf,(ax5) = plt.subplots(1,1, figsize = (6, 6), sharey = True)\nax5.imshow(cm_ideal, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax5.set_title('Confusion Matrix for the Ideal Scenario')\nax5.set_ylabel('True label')\nax5.set_xlabel('Predicted label')\nax5.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax5.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax5.text(j,i, str(s[i][j])+\" = \"+str(cm_ideal[i][j]))\n        \n\n","22f35aa6":"# Now let us also print the classification report for both our Neural Networks:\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Classification report for the Under-Sampled data Neural Network\n\nprint(\"Under-Sampled Data Neural Network:\")\nprint(\"Accuracy Score:\", accuracy_score(original_ytest, undersample_predictions))\nprint(classification_report(original_ytest, undersample_predictions))\n\n# Classification report for the Over-Sampled data Neural Network\nprint(\"Over-Sampled Data Neural Network:\")\nprint(\"Accuracy Score:\", accuracy_score(original_ytest, oversample_predictions))\nprint(classification_report(original_ytest, oversample_predictions))","1ccb2c34":"# Finding the F1 score of both the neural networks:\n\nfrom sklearn.metrics import f1_score\n\n# F1 score of the under-sampled Neural Network:\nprint(\"Undersampled Neural Network F1 score:\", f1_score(original_ytest, undersample_predictions, average='weighted'))  \n\n# F1 score of the over-sampled Neural Network:\nprint(\"Oversampled Neural Network F1 score:\", f1_score(original_ytest, y_pred_knears_smote, average='weighted')) ","c94e736c":"# Finding the outlier fraction and the number of fraud and non-fraud cases:\n\nFraud = df[df['Class']==1]\n\nValid = df[df['Class']==0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))\n\nprint(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))","31de6446":"# Creating the Isolated Forest algorithm object:\n\nfrom sklearn.ensemble import IsolationForest\n\nisolated_forest_model = IsolationForest(n_estimators = 100, max_samples = len(X), \n                                       contamination = outlier_fraction, random_state = 42, verbose = 0)","ee8b2eab":"# Fitting the Isolated Forest model:\n\nisolated_forest_model.fit(original_Xtrain, original_ytrain)","99e8a758":"# Making predictions for the original testing data\n\nscores_prediction_iso = isolated_forest_model.decision_function(original_Xtest)\n\ny_pred_iso = isolated_forest_model.predict(original_Xtest)","2d008afd":"# Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\ny_pred_iso[y_pred_iso == 1] = 0\ny_pred_iso[y_pred_iso == -1] = 1\nn_errors = (y_pred_iso != original_ytest).sum()","b51b0f6c":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Number of errors for Isolated Forest Algorithm:\", n_errors)\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest,y_pred_iso))\nprint(\"Classification Report for Isolation Forest:\")\nprint(classification_report(original_ytest,y_pred_iso))","b82b50cd":"# Now let us plot the confusion matrix for the Isolated Forest model:\n\nfrom sklearn.metrics import confusion_matrix\n \ncm_iso = confusion_matrix(original_ytest, y_pred_iso)\n\nf,(ax1) = plt.subplots(1,1, figsize = (6, 6), sharey = True)\nax1.imshow(cm_iso, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for Support Vector Classifier')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_iso[i][j]))\n","c3b44116":"# Building the XGBClassifier:\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nxgb = XGBClassifier()","bbbde608":"# Training the model\n\nxgb.fit(original_Xtrain, original_ytrain)","25e2d20f":"# Predicitng for the original testing dataset\n\nxgb_pred = xgb.predict(original_Xtest)","c15d84c6":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, xgb_pred))\nprint(\"Classification Report for XGBClassifier:\")\nprint(classification_report(original_ytest, xgb_pred))","6b760a1e":"# Building the GBM Classifier:\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier()","d5255bfa":"# Training the model\n\ngbm.fit(original_Xtrain, original_ytrain)","8d2d7e9b":"# Predicitng for the original testing dataset\n\ngbm_pred = gbm.predict(original_Xtest)","8a6d8f4f":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, gbm_pred))\nprint(\"Classification Report for GBMClassifier:\")\nprint(classification_report(original_ytest, gbm_pred))","b821e291":"# Building the AdaBoost Classifier:\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_model = AdaBoostClassifier()","327607e1":"# Training the model\n\nada_model.fit(original_Xtrain, original_ytrain)","a628aced":"# Predicitng for the original testing dataset\n\nada_pred = ada_model.predict(original_Xtest)","2848d7ab":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, ada_pred))\nprint(\"Classification Report for AdaBoost Classifier:\")\nprint(classification_report(original_ytest, ada_pred))","34443ed9":"# Printing the confusion matrix for our Boosting Algorithms\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_xgb = confusion_matrix(original_ytest, xgb_pred)\ncm_gbm = confusion_matrix(original_ytest, gbm_pred)\ncm_ada = confusion_matrix(original_ytest, ada_pred)\n\nf, ((ax1, ax2, ax3)) = plt.subplots(1,3, figsize=(15,10), sharey=True)\n\n# XGB Classifier\n\nax1.imshow(cm_xgb, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for XGB Classifier')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_xgb[i][j]))\n        \n        \n# Gradient Boosting Machine\n\nax2.imshow(cm_gbm, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for GBM Classifier')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_gbm[i][j]))\n        \n\n# AdaBoost Classifier\n\nax3.imshow(cm_ada, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax3.set_title('Confusion Matrix for AdaBoost Classifier')\nax3.set_ylabel('True label')\nax3.set_xlabel('Predicted label')\nax3.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax3.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax3.text(j,i, str(s[i][j])+\" = \"+str(cm_ada[i][j]))\n        ","33e1076e":"# Finding the F1 score of our boosting algorithms:\n\nfrom sklearn.metrics import f1_score\n\n# F1 score of the XGB Classifier:\nprint(\"XGB Classifier F1 score:\", f1_score(original_ytest, xgb_pred, average='weighted'))  \n\n# F1 score of the GBM Classifier:\nprint(\"GBM Classifier F1 score:\", f1_score(original_ytest, gbm_pred, average='weighted')) \n\n# F1 score of the AdaBoost Classifier:\nprint(\"AdaBoost Classifier F1 score:\", f1_score(original_ytest, ada_pred, average = 'weighted'))","b8b9d7c9":"# XGB Classifier on the under-Sampled Data:\n\nxgb_under = XGBClassifier()\nxgb_under.fit(X_train, y_train)","ea689f9f":"# Predicting using our XGB Classifier on the original dataset\n\nxgb_under_pred = xgb_under.predict(original_Xtest)","9f0baa00":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, xgb_under_pred))\nprint(\"Classification Report for XGB Classifier:\")\nprint(classification_report(original_ytest, xgb_under_pred))","d51b01ae":"# GBM Classifier on the randomly Under-Sampled Data:\n\ngbm_under = GradientBoostingClassifier()\ngbm_under.fit(X_train, y_train)","13f0aeb4":"# Predicitng using the GBM Classifier on the original dataset\n\ngbm_under_pred = gbm_under.predict(original_Xtest)","0b55fa8f":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, gbm_under_pred))\nprint(\"Classification Report for GBM Classifier:\")\nprint(classification_report(original_ytest, gbm_under_pred))","73c069a9":"# AdaBoost Classifier on the randomly Under-Sampled Data:\n\nada_under = AdaBoostClassifier()\nada_under.fit(X_train, y_train)","c8e6baf4":"# Predicitng using the AdaBoost Classifier on the original dataset\n\nada_under_pred = ada_under.predict(original_Xtest)","6f808691":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, ada_under_pred))\nprint(\"Classification Report for AdaBoost Classifier:\")\nprint(classification_report(original_ytest, ada_under_pred))","8455bb7e":"# Printing the confusion matrix for our Boosting Algorithms on our Randomly Under-Sampled Data:\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_xgb_under = confusion_matrix(original_ytest, xgb_under_pred)\ncm_gbm_under = confusion_matrix(original_ytest, gbm_under_pred)\ncm_ada_under = confusion_matrix(original_ytest, ada_under_pred)\n\nf, ((ax1, ax2, ax3)) = plt.subplots(1,3, figsize=(15,10), sharey=True)\n\n# XGB Classifier\n\nax1.imshow(cm_xgb_under, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for XGB Classifier')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_xgb_under[i][j]))\n        \n        \n# Gradient Boosting Machine\n\nax2.imshow(cm_gbm_under, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for GBM Classifier')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_gbm_under[i][j]))\n        \n\n# AdaBoost Classifier\n\nax3.imshow(cm_ada_under, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax3.set_title('Confusion Matrix for AdaBoost Classifier')\nax3.set_ylabel('True label')\nax3.set_xlabel('Predicted label')\nax3.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax3.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax3.text(j,i, str(s[i][j])+\" = \"+str(cm_ada_under[i][j]))\n        ","54331c64":"# Finding the F1 score of our boosting algorithms on the Randomly Under-Sampled Data:\n\nfrom sklearn.metrics import f1_score\n\n# F1 score of the under-sampled XGB Classifier:\nprint(\"XGB Classifier F1 score on the under-sampled data:\", f1_score(original_ytest, xgb_under_pred, average='weighted'))  \n\n# F1 score of the under-sampled GBM Classifier:\nprint(\"GBM Classifier F1 score on the under-sampled data:\", f1_score(original_ytest, gbm_under_pred, average='weighted')) \n\n# F1 score of the under-sampled AdaBoost classifier:\nprint(\"AdaBoost Classifier F1 score on the under-sampled data:\", f1_score(original_ytest, ada_under_pred, average = 'weighted'))","c4ba6cf0":"# XGB Classifier on the Over-Sampled Data:\n\nxgb_over = XGBClassifier()\nxgb_over.fit(X_train_res, y_train_res)","a63f67b2":"# Predicting using our XGB Classifier on the original dataset\n\nxgb_over_pred = xgb_over.predict(original_Xtest)","71eb23d4":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, xgb_over_pred))\nprint(\"Classification Report for XGB Classifier:\")\nprint(classification_report(original_ytest, xgb_over_pred))","b2cc17c2":"# GBM Classifier on the randomly Over-Sampled Data:\n\ngbm_over = GradientBoostingClassifier()\ngbm_over.fit(X_train_res, y_train_res)","294fb539":"# Predicitng using the GBM Classifier on the original dataset\n\ngbm_over_pred = gbm_over.predict(original_Xtest)","452f5cbf":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, gbm_over_pred))\nprint(\"Classification Report for GBM Classifier:\")\nprint(classification_report(original_ytest, gbm_over_pred))","631adddb":"# AdaBoost Classifier on the randomly Over-Sampled Data:\n\nada_over = AdaBoostClassifier()\nada_over.fit(X_train_res, y_train_res)","fd8003e5":"# Predicitng using the AdaBoost Classifier on the original dataset\n\nada_over_pred = ada_over.predict(original_Xtest)","c5794e28":"# Printing the accuracy score and the classification report\n\nfrom sklearn.metrics import classification_report,accuracy_score\n\nprint(\"Accuracy Score :\")\nprint(accuracy_score(original_ytest, ada_over_pred))\nprint(\"Classification Report for AdaBoost Classifier:\")\nprint(classification_report(original_ytest, ada_over_pred))","e70635b3":"# Printing the confusion matrix for our Boosting Algorithms on our Randomly Over-Sampled Data:\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_xgb_over = confusion_matrix(original_ytest, xgb_over_pred)\ncm_gbm_over = confusion_matrix(original_ytest, gbm_over_pred)\ncm_ada_over = confusion_matrix(original_ytest, ada_over_pred)\n\nf, ((ax1, ax2, ax3)) = plt.subplots(1,3, figsize=(15,10), sharey=True)\n\n# XGB Classifier\n\nax1.imshow(cm_xgb_over, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax1.set_title('Confusion Matrix for XGB Classifier')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\nax1.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax1.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax1.text(j,i, str(s[i][j])+\" = \"+str(cm_xgb_over[i][j]))\n        \n        \n# Gradient Boosting Machine\n\nax2.imshow(cm_gbm_over, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax2.set_title('Confusion Matrix for GBM Classifier')\nax2.set_ylabel('True label')\nax2.set_xlabel('Predicted label')\nax2.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax2.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax2.text(j,i, str(s[i][j])+\" = \"+str(cm_gbm_over[i][j]))\n        \n\n# AdaBoost Classifier\n\nax3.imshow(cm_ada_over, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nax3.set_title('Confusion Matrix for AdaBoost Classifier')\nax3.set_ylabel('True label')\nax3.set_xlabel('Predicted label')\nax3.xaxis.set_ticklabels(['','','No Fraud','','','','Fraud'])\nax3.yaxis.set_ticklabels(['','','No Fraud','','','', 'Fraud'])\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        ax3.text(j,i, str(s[i][j])+\" = \"+str(cm_ada_over[i][j]))\n        ","92269a63":"# Finding the F1 score of our boosting algorithms on the Over-Sampled Data:\n\nfrom sklearn.metrics import f1_score\n\n# F1 score of the over-sampled XGB Classifier:\nprint(\"XGB Classifier F1 score on the over-sampled data:\", f1_score(original_ytest, xgb_over_pred, average='weighted'))  \n\n# F1 score of the over-sampled GBM Classifier:\nprint(\"GBM Classifier F1 score on the over-sampled data:\", f1_score(original_ytest, gbm_over_pred, average='weighted')) \n\n# F1 score of the over-sampled AdaBoost classifier:\nprint(\"AdaBoost Classifier F1 score on the over-sampled data:\", f1_score(original_ytest, ada_over_pred, average = 'weighted'))","3b70e8da":"> # Confusion Matrix \n\n* A confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known.\n* It allows easy identification of confusion between classes e.g. one class is commonly mislabeled as the other. Most performance measures are computed from the confusion matrix.\n\n* **Positive\/Negative**: Type of Class (label)\n\n* **True\/False**: Correctly or Incorrectly classified by the model.\n\n* **True Negatives** (Top-Left Square): This is the number of correctly classifications of the \"No\" (No Fraud Detected) class. \n\n* **False Negatives** (Top-Right Square): This is the number of incorrectly classifications of the \"No\"(No Fraud Detected) class. \n\n* **False Positives** (Bottom-Left Square): This is the number of incorrectly classifications of the \"Yes\" (Fraud Detected) class \n\n* **True Positives** (Bottom-Right Square): This is the number of correctly classifications of the \"Yes\" (Fraud Detected) class.","e23e4796":"**NOTE:** We notice how highly skewed and biased our dataset is. We observe that 99.83% of our dataset is for non-fraud transactions and only about 0.17% are of fraud cases. This can be a problem for us because if we make a classifier which always predicts the transactions as Non-Frauds and we have 100 test cases, still we will be able to get 99.83% accuracy which can make us think that we actually are doing really well. Clearly, this is not the case and hence we will take care of this highly skewed data as well as use some other metrics to test how our classifier is performing.","063c15b0":"# What all do we aim to do ?\n\n*  Understanding the data\n\n*  Preprocess the data \n\n*  Correlating the data\n\n*  Detecting and Removing Outliers\n\n*  Implementing Random Under-Sampling\n\n*  Building classification models for our under-sampled data\n\n*  Implementing over-sampling using the SMOTE technique\n\n*  Building classification models for our over-sampled data\n\n*  Implementing a simple Neural Networks on our under-sampled and over-sampled dataset\n\n*  Implementing the Isolated Forest Algorithm to detect the fraud cases\n\n*  Implementing certain boosting algorithms to see how they do on our original dataset as well as on the under-sampled dataset","332727d2":"> # Summary for the Isolated Forest Algorithm:\n\n* This experiment serves a true reminder as to why we should not trust accuracy as a measure when we are dealing with skewed datasets. \n* Our Isolated Forest Algorithm has a high accuracy -99.76%. However, when we plot the confusion matrix, we are able to see the true performance of our classifier. It classified only 36 out of the 98 fraud transactions correctly. Although, it did good on the non-fraud part of the dataset but still it was unable to perform good for the main aim we had for this project i.e. fraud transaction detection.","43029285":"We have to scale the time and the amount column as well because all the other feautres are aleady scaled and thus by scaling time and amount, we can more effectively use them as a feature.","9036e3e7":"> # SMOTE Technique (Over-Sampling)\n\nSMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.","ad38f6e8":"> # ROC Curve\n\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\nTrue Positive Rate\nFalse Positive Rate\nTrue Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:\n\n \nFalse Positive Rate (FPR) is defined as follows:\n\n \nAn ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n\n# NOTE: We use ROC curves when we have a balanced datasets.","ef60d00e":"> Random Under-Sampling\n\nThis is a technique in which we randomly remove certain data points so as to maintain a 50\/50 ratio of fraud and non-fraud cases. This assures that our classification algorithm is not biased and we are able to better generalise on the given data.\n\n**Steps to implement Random under-sampling :**\n\n1) First, we find the number of fraud cases we have in our dataset. \n\n2) Then, after finding the number of fraud cases, we make the number of non-fraud cases equal to the number of fraud cases i.e 492 in our case. \n\n3) After implementing this technique, we have a sub-sample of our dataframe with a 50\/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n\n**NOTE:** Although this method does reduce the high biased nature of our dataframe but still it is its own set of problems, the major one being that we loose a lot of information. In our original dataset, we had 2,84,315 non-fraud cases which we have reduced to 492 after applying random under-sampling.","36465143":"# Now let us perform an experiment: We will try to train the boosting algorithms using our randomly Under-Sampled data.","383a5024":"**NOTE:** We see that there are no NULL values in our dataset which is a good thing for us as now we do not have to take care of them separately. ","cdf03e79":"# Introduction \n\nThis notebook demonstrates my approach to solving the problem of detecting a credit card transaction as fraud or not fraud based on the features that we are given. We use various classification algorithms and also try to hand engineer our data to make it more suitable in order to detect the frauds. \nBefore beginning, I would like to thank Janio Martinez Bachmann. Being a starter myself, his kernel was of great help to me and enabled me to explore the data in a much better way. I strongly recommend all of you to check out his kernel as well : https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets?scriptVersionId=16695845","7b4bfd32":"* The above block of code can be run to find the best parameters. I have commented out the execution part because it takes a lot of time to get compiled. \n\n* Thus, the best parameters found using GridSearchCv are:\n    * C = 100 \n    * Penalty = l2","df8d3183":"> # Neural Networks: Comparing under-sampling and over-sampling using Neaural Networks\n\nHere, we will implement a simple neural network to compare which of the two i.e. Under-Sampling and Over-Sampling technique that we have implemented is better.\n\n**Goal: **\nWe try to figure out which of the two techniques that we have implemented to take care of our skewed dataset is better. The main reason for doing is that in a real life situation, we want to predict both the fraud as well as the non-fraud cases. If our model predicts the non-fraud cases as fraud ones very often, then this can be a problem as well because this can upset a genuine user who made a transaction by classifying it as non-fraud and then blocking his\/her card.","5f479fed":"> # Summary for the Over-Sampled Data::\n\n* We observe that by creating synthetic data points to train our data using the SMOTE technique, we managed to solve the problem of the high biased and skewed nature of our original dataset. All of our classification models seem to be doing quite well on the original dataset. \n\n* The best classifier according to the F1 score is the Random Forest Classifier. We observe that:\n\n      * Logistic Regression seems to be doing the best job in identifying the fraud transactions. It only   leaves out 6 of the 98 that we have in our original dataset. However, Logistic Regression also      classifies many of the non-fraud transactions as fraud ones which is undesirable and thus it has a  comparitively small F1 score. \n      \n      * The Random Forest Classifier does a good job in idenifying the fraud transactions but what sets it  apart is its ability to more correctly identify the non-fraud transactions, thus giving the best    F1 score.      ","45ab5086":"> # Boosting Algorithms\n\n* The term \u2018Boosting\u2019 refers to a family of algorithms which converts weak learner to strong learners.\n\n* For choosing the right distribution, here are the following steps:\n\n* **Step 1**:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n\n* **Step 2**: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n\n* **Step 3**: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n\n* Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classi\ufb01ed or have higher errors by preceding weak rules.\n\n\nWe try three Boosting Algorithms:\n\n1. XGB Classifier\n2. GBM Classifier\n3. AdaBoost Classifier","46dbca1c":"> # Neural Network Architecture:\n\nWe use a simple neural network architecture with two hidden layers and also implement Dropout to prevent over-fitting. We use the cross-entropy loss for the optimizer and also use the AdamOptimizer and relu activation function at the two hidden layers and a softmax function at the output layer. We set the learning rate to be 0.001\n\n![image.png](attachment:image.png)","bc375da2":"# Now let us perform an experiment: We will try to train the boosting algorithms using our Over-Sampled data.","bb689c9b":"> # Summary for the Boosting Algorithms:\n\n* We observe that our boosting algorithms do a decent job given the time they take to get executed.\n* The Boosting algorithms that are trained on the original dataset do a great job in identifying the non-fraud transactions, especially the XGB Algorithm. However, they leave quite a fraud transactions as non-fraud ones which is a major issue.\n\n* The Boosting algorithms that are trained on the randomly under-sampled dataset are able to an exellent job in identifying the fraud transactions. However, they classify many of the non-fraud transactions as fraud ones.\n\n* The Boosting algorithms that are trained on the over-sampled dataset do a decent job in identifying the fraud transactions but classify many of the non-fraud transactions as fraud ones. Hence, they might not be the classifier to use for the given problem.","3655fdd5":"> # Applying t-SNE algorithm:\n\nWe appl the t-SNE algorithm to get a visual sense of the data that is given to us. It is a clustering algorithm that first reduces the dimensions of our data into a plottable form and then forms clusters.\n","0901f135":"> # Precision - Recall Curve\n\nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n\nPrecision (P) is defined as the number of true positives (Tp) over the number of true positives plus the number of false positives (Fp).\n\n**P = Tp\/Tp+Fp**\n\nRecall (R) is defined as the number of true positives (Tp) over the number of true positives plus the number of false negatives (Fn).\n\n**R = Tp\/Tp+Fn**\n\nThese quantities are also related to the (F1) score, which is defined as the harmonic mean of precision and recall.\n\n**F1 score = 2*P*R\/P+R**\n\n# NOTE: We use precision-recall curves for unbalanced datasets.","ea5091b4":"# Splitting the original dataset\n\n* Before we proceed to randomly under-sample or over-sample our data, we need to split the original dataset into training and testing dataset. This is because our ultimate goal is to test our classifier on the original dataframe and not on the randomly under-sampled or over-sampled data. Thus, we split our original dataframe into training and testing data.\n* We also make sure that both of our datasets have the same percentage of fraud and non-fraud cases, so as to maintain the bias we had in our original dataset.","dd05be64":"# Creating a sub-sample\n\n**Why do we create a sub-sample?**\n\n* We create a sub-sample to remove the problem of having a highly skewed dataset. We create a sub-sample having 492 randomly selected non-fraud cases and also scaling the time and amount columns for the same. This assures that we have a 50\/50 distribution of both the classes in our dataset and assures that our classification algorithm does not become biased. \n* If we do not create a sub-sample, then our model would overfit and assume that in most cases there will not be a fraud. This is particularly bad as we want our model to predict and be certain not to miss out on any fraud cases. Thus, creating a sub-sample helps us in better generalising over both the classes.\n","b996072b":"> # Isolation Forest Algorithm\n\n* One of the newest techniques to detect anomalies is called Isolation Forests. The algorithm is based on the fact that anomalies are data points that are few and different. As a result of these properties, anomalies are susceptible to a mechanism called isolation.\n\n* This method is highly useful and is fundamentally different from all existing methods. It introduces the use of isolation as a more effective and efficient means to detect anomalies than the commonly used basic distance and density measures. Moreover, this method is an algorithm with a low linear time complexity and a small memory requirement. It builds a good performing model with a small number of trees using small sub-samples of fixed size, regardless of the size of a data set.\n\n* Typical machine learning methods tend to work better when the patterns they try to learn are balanced, meaning the same amount of good and bad behaviors are present in the dataset.\n\n**How Isolation Forests Work**\n\n* The Isolation Forest algorithm isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. On the other hand, isolating normal observations require more conditions. Therefore, an anomaly score can be calculated as the number of conditions required to separate a given observation.\n\n* The way that the algorithm constructs the separation is by first creating isolation trees, or random decision trees. Then, the score is calculated as the path length to isolate the observation.","31a40887":"> # Summary for the Randomly Under-Sampled Data:\n\n* We observe that accuracy metrics can sometime be misleading and may not be the best measure of how good or bad the classifier is actually doing. This is especially the case when we have an imbalanced dataset with highly skewed or biased data. In practise, this often happens when we are dealing with outlier detection or fraud detection because the amount of data we have for fraud and outlier class is much less as compared to the genuine ones. \n* When we deal with highly biased or skewed data, using F1 score as a measure to see how our classifier is doing can be a good option.\n* As we observe from our F1 scores, all of our models seem to be doing quite well on our original highly biased dataset. Decision Tree Classifier seems to be getting the highest F1 score suggesting that this might be the best classifier to use for this problem. However, other models also have their benefits. \n\n      * Random Forest Classifier only leaves out 2 fraud transactions as non-fraud outof the 98 fraud       transactions we had in our testing set. This shows that the Random Forest Classifier does the       best job in figuring out the fraud transactions. However, it also classifies many non-fraud         transactions as fraud ones which can be a problem as any user who made a genuine transaction        might find it annoying to get his\/her card blocked because our classifier predicted it as a         fraud one. \n   \n      * Decision Tree Classifier identifies most of the fraud transactions and also does a better job of    classifying the non-fraud ones.","c77904ac":"># Correlating the data","9b7534ae":"> # Anamoly Detection and Outlier Detection\n\n* We try to remove the extreme outliers because having extreme outliers in our dataset can cause the classification to divert from the general trend because during the training process, it tries hard to classify the outliers correctly which does not help in the longer run.\n* We remove the outliers that are present in classes that have strong correlations with our class feature. We do so because these are the features which highly affect our output and hence having outliers in these features makes our algorithm less robust to generalise.","42b03d3d":"> # Summary of Neural Networks using the under-sampled and over-sampled data:\n\n* We can see that our over-sampled Neural Network has a higher F1 score as compared to our under-sampled Neural Network. This shows us that our over-sampled Neural Network is able to better gneralize over the data given to us.\n* However, on seeing the confusion matrix of both the Neural Netwoks, we see that the under-sampled Neural Network is able to better identify the fraud transactions but then it also classifies a large number of non-fraud transactions as fraud ones which is not desirable.","ca2ef254":"# Observed Correlations :\n\n* Negative Correlations : \n1. V14\n2. V12\n3. V10\n4. V16\n\n\n* Positive Correlations :\n1. V4\n2. V11","f5525a65":"# Understanding our data\n\nThe data we are given has 28 features which have undergone PCA. PCA is a dimension reduction algorithm. We assume that are features are scaled as it is a standard practise to scale the features before applying PCA. We cannot really make much sense of our data just by viewing it as after undergoing PCA, the features have been merged and thus we need to find other ways to understand the corelations between differnt features.","ac8ff2f1":"> # Classification models on our Randomly Under-Sampled Data:\n\nWe test out various classification models on our randomly under-sampled data. We build a model using each of the following algorithms:\n* Logistic Regression\n* KNearestNeighbors\n* Support Vetor Machines\n* Decision Tree Classifier\n* Random Forest Classifier\n* Naive Bayes Classifier\n","05bfc7df":"> # Interquartile Range Method\n\n**Interquartile Range (IQR)**: We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.\n\n* We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect. \n* **The Tradeoff**: The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.","0ccff4d0":"> # Correlation Matrcies\n\n* Correlation matrcies basically help us in understanding the various patterns between different features and the fraud cases. In our particular case, the correlation matrices are of great importance because we do not actually know what the 28 features in our input dataset represnt. Thus, by observing the correlations between different features and fraud class, we are able to identify the positive and negative correlations between them. \n* There are 2 types of correlations : \n1. Positive Correlaions: These are the features for which higher value tend to point towards a fraud transaction.\n2. Negative Correlations: These are the features for which lower values tend to point towards a fraud transaction.\n\n**NOTE**: We visualise the correlations on our balanced dataset because otherwise our correlations will be affected by the highly biased and skewed dataset that we have for a particular(non-fraud) class.","748fd057":"> # Development of Models:\n\n* First, we test out various classfication models on our randomly under-sampled data.\n* Second, we test out various classification models on our Over-Sampled data that we create using the SMOTE technique.\n* Third, we test out the Isolated Forest algorithm on our original dataset.\n* Fourth, we test out the various boosting algorithms on both our original dataset as well as on the under-sampled data","59c78f70":"> # Loading and Understanding our Data"}}