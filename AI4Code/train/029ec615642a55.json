{"cell_type":{"c0a956a6":"code","9ffd1fbc":"code","8f98f10b":"code","d32a33e4":"code","65591542":"code","90cfed4e":"code","32aeb453":"code","62c7bb38":"code","aeb144d9":"code","23fb96a6":"code","41300315":"code","d90231e2":"code","8fb1760b":"code","9bbe95d8":"code","c9e43253":"code","a979f71b":"code","c2ed9214":"code","371364c6":"code","b21788ff":"code","98c0e729":"markdown","c62b1ac3":"markdown","6c183d14":"markdown","3a49cb1c":"markdown","a85649f9":"markdown","6a0bc937":"markdown","039dd598":"markdown","565d4d11":"markdown","376460e4":"markdown","a554702a":"markdown","0007c25e":"markdown","76702661":"markdown"},"source":{"c0a956a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ffd1fbc":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv\", index_col=\"date_time\", parse_dates=True)\ndf.tail()","8f98f10b":"profile = ProfileReport(df, title=\"Pandas Profiling Report\")\nprofile","d32a33e4":"for column in df.columns:\n    if \"target\" in column:\n        fig = plt.figure()\n        _ = df[column].plot(figsize=(12, 8), title=column)","65591542":"from sklearn.preprocessing import MinMaxScaler #JUST TO NORMALIZE THE TARGET IN A SIGNLE GRAPH","90cfed4e":"df['dayoftheweek'] = df.index.dayofweek #Weekly Seasonality\ndf['hourofday'] = df.index.hour #Daily Seasonality\ndf['time'] = df.index.astype(np.int64) #Yearly Trend","32aeb453":"def view_seasonality(view):\n    \n    fig = plt.figure(figsize=(12, 8))\n    legends=[]\n    for column in df.columns:\n        if \"target\" in column:\n            plt.plot(1+view.index, MinMaxScaler().fit_transform(view[column].values.reshape(-1, 1)))\n            legends.append(column)\n    plt.xlabel(view.index.name)\n    plt.legend(legends)","62c7bb38":"view = df.groupby(by='hourofday').mean()\nview_seasonality(view)","aeb144d9":"view = df.groupby(by='dayoftheweek').mean()\nview_seasonality(view)","23fb96a6":"# Resample the entire dataset by daily average\nrollmean = df.resample(rule='D').mean()\nrollstd = df.resample(rule='D').std()\n# Plot time series for each sensor with its mean and standard deviation\nnames = ['sensor_4']\nfor name in names:\n    _ = plt.figure(figsize=(18,3))\n    _ = plt.plot(df[name], color='blue', label='Original')\n    _ = plt.plot(rollmean[name], color='red', label='Rolling Mean')\n    _ = plt.plot(rollstd[name], color='black', label='Rolling Std' )\n    _ = plt.legend(loc='best')\n    _ = plt.title(name)\n    plt.show()","41300315":"!pip install \"mxnet<2.0.0\"\n!pip install autogluon","d90231e2":"from autogluon.tabular import TabularDataset, TabularPredictor","8fb1760b":"TIME_LIMIT = 60 #Simulation time (IN SECONDS) SET IT TO AT LEAST 3600 IN A REAL SCENARIO\n#all target columns have target in the column name\ntarget = [column for column in df.columns if \"target\" in column] \nfeatures = df.columns.drop(target)","9bbe95d8":"def multi_target_prediction(i):\n    \n    '''\n    Simple function to iterate the AutoML process for the 3 targets\n    '''\n    \n    train_data = TabularDataset(df[features.tolist()+[target[i]]])\n    label = target[i]\n    \n    save_path = f'agModels-predictClass{i}'\n    \n    predictor = TabularPredictor(\n        label = label,\n        path = save_path).fit(train_data, \n                              presets='best_quality', \n                              num_stack_levels = 3,\n                              num_bag_folds = 5,\n                              num_bag_sets = 1,\n                              time_limit=TIME_LIMIT)\n\n    return predictor","c9e43253":"predictors = [multi_target_prediction(i) for i in range(3)]","a979f71b":"test = pd.read_csv(dirname + '\/test.csv', index_col='date_time', parse_dates=True)\ntest['dayoftheweek'] = test.index.dayofweek\ntest['hourofday'] = test.index.hour\ntest['time'] = test.index.astype(np.int64)\nt = TabularDataset(test)","c2ed9214":"predictions = [predictor.predict(t) for predictor in predictors]","371364c6":"submission = pd.DataFrame(test.index.values,  columns=['date_time'])\nsubmission[target] = np.vstack(predictions).T","b21788ff":"submission.to_csv('.\/sumbission.csv', index=False)","98c0e729":"All the rows in the dataframe seems to be space by time intervals of one hour. Lets continue our analysis using a pandas profiler, to check for missing values, outliers and correlations.","c62b1ac3":"# Time Series Forecasting with AutoGluon\n\nThis notebook is dedicated to solving the July kaggle playground challenge (https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021).\n\nWe will first start by analyzing the data, and perform some basic feature engineering. Then we will perform AutoML with AutoGluon","6c183d14":"As expected, rush hours have higer levels of pollution, also during the night the targets are much lower (few peoples drive at 4 am)","3a49cb1c":"## Model Selection\n\nWe can now begin the AutoML process.\nNote that in this kaggle notebook I severely reduced the time limit on the autoML process, so please increase it if you intend to follow it","a85649f9":"## Feature Engineering\n\nWe shall now try to perform some very basic feature engineering to improve the performance of our ML models","6a0bc937":"Again, in the weekends, the pollution level decrease as expected!\n\nOne very promising feature to add is then something realted to holidays (like a binary flag). Sadly, no information is available on the country of origin for this data, so for the time being I avoided including it.\n\nLets now take a look at sensor data","039dd598":"Perform the AutoML for the 3 different targets","565d4d11":"## Exploratory Data Analysis\n\nThe first step in our analysis is to take a look at the data. Since we are dealing with time series, seasonalities will be fundamental to improve the performance of our machine learning models. For the sake of concisiveness, we also pass the datetime as the index and we parse it as a date.","376460e4":"Lets take a closer look on the targets time series:","a554702a":"Each single one of the target has strong fluctuations. If we think about the problem statement, we are dealing with pollution level, that will most likely be influenced by the movement of peoples. \n\nThus, we should check the following assumptions:\n\n- In a single day we will have stronger pollution levels during rush hours\n- In a week working days will be characterized by higer pollution levels","0007c25e":"It is clear that is not impossible for the sensor to be broken for a long period of time. Another possible feature to insert is then a binary flag about the current status of the various sensors. This chould be achieved with SPRT or other anomalies detection strategies. If someone has a suggestion please put them in the comment ;)","76702661":"We can now cast our predictions on the test dataset using the optimized models!"}}