{"cell_type":{"4d50f800":"code","93421b3a":"code","fc41c164":"code","cc37e156":"markdown","1b73bf7d":"markdown","6f52b6fb":"markdown","56b7d782":"markdown","24814dfb":"markdown","70844e4a":"markdown","fb59d4ce":"markdown","b96a23bc":"markdown","518d7ec4":"markdown","ca84375f":"markdown"},"source":{"4d50f800":"import numpy as np\nimport math\n\ndef logloss(true_label, predicted, eps=1e-15):\n    p = np.clip(predicted, eps, 1 - eps)\n    if true_label == 1:\n        return -math.log(p)\n    else:\n        return -math.log(1 - p)","93421b3a":"print(\"The label to be predicted is {} and the model's prediction is {}. Hence Logloss is {}\".format(1,0.5,logloss(1,0.5)))\n\nprint(\"The label to be predicted is {} and the model's prediction is {}. Hence Logloss is {}\".format(1,0.1,logloss(1,0.9)))\n\nprint(\"The label to be predicted is {} and the model's prediction is {}. Hence Logloss is {}\".format(1,0.9,logloss(1,0.1)))","fc41c164":"import matplotlib.pyplot as plt \n# predicted probabilities\np = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95]\nlgloss = [logloss(1,i) for i in p]\nplt.plot(p,lgloss)\nplt.xlabel(\"probalities\")\nplt.ylabel(\"LogLoss\")\nplt.title(\"LogLoss for Different Probabilities\")\n","cc37e156":"The goal of machine learning algorithms is usually to learn labels for particular distributions or patterns and be able to predict or classify them when the they see it again. In order to get better at learn these distributions\/patterns, they have to minimize the error from misclassfying\/mispredictiong such labels. So it can be said that the goal of  machine learning models is to minimize this error. Logarithmic Loss is one of such error\/cost functions used in training machine learrning models\n\nLogarithmic loss measures the performance of a classification model where the prediction input is a probability value between 0 and 1. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high log loss.","1b73bf7d":"**Reference**\n\n* https:\/\/datawookie.netlify.app\/blog\/2015\/12\/making-sense-of-logarithmic-loss\/\n* http:\/\/wiki.fast.ai\/index.php\/Log_Loss\n*  ","6f52b6fb":"### Log Loss Function\nLet\u2019s consider a simple implementation of a Log Loss function:","56b7d782":"From the above, it can be said that Log Loss quantifies the accuracy of a classifier by penalising false classifications. Minimising the Log Loss is basically equivalent to maximising the accuracy of the classifier, but there is a subtle twist which we\u2019ll get to in a moment.\n\nIn order to calculate Log Loss the classifier must assign a probability to each class rather than simply yielding the most likely class. Mathematically Log Loss is defined as\n\n$$ - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^M y_{ij} \\log \\, p_{ij} $$\n\nwhere N is the number of samples or instances, M is the number of possible labels, \\(y_{ij}\\) is a binary indicator of whether or not label j is the correct classification for instance i, and \\(p_{ij}\\) is the model probability of assigning label j to instance i. A perfect classifier would have a Log Loss of precisely zero. Less ideal classifiers have progressively larger values of Log Loss. If there are only two classes then the expression above simplifies to\n\n$$ - \\frac{1}{N} \\sum_{i=1}^N [y_{i} \\log \\, p_{i} + (1 - y_{i}) \\log \\, (1 - p_{i})].$$\n\nNote that for each instance only the term for the correct class actually contributes to the sum.","24814dfb":"# Logarithmic Loss","70844e4a":"* In the first case the classification is neutral: it assigns equal probability to both classes, resulting in a Log Loss of 0.69315. \n* In the second case the classifier is relatively confident in the first class. Since this is the correct classification the Log Loss is reduced to 0.10536. \n* The third case is an equally confident classification, but this time for the wrong class. The resulting Log Loss escalates to 2.3026. Relative to the neutral classification, being confident in the wrong class resulted in a far greater change in Log Loss. Obviously the amount by which Log Loss can decrease is constrained, while increases are unbounded.","fb59d4ce":"## **Difference Between LogLoss and Accuracy**","b96a23bc":"* Accuracy is the count of predictions where your predicted value equals the actual value. Accuracy is not always a good indicator because of its yes or no nature.\n* Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view into the performance of our model.","518d7ec4":"### Graphing Log Loss","ca84375f":"Log Loss heavily penalises classifiers that are confident about an incorrect classification. For example, if for a particular observation, the classifier assigns a very small probability to the correct class then the corresponding contribution to the Log Loss will be very large indeed. Naturally this is going to have a significant impact on the overall Log Loss for the classifier. The bottom line is that it\u2019s better to be somewhat wrong than emphatically wrong. Of course it\u2019s always better to be completely right, but that is seldom achievable in practice! \n\nThere are at least two approaches to dealing with poor classifications:\n\n* Examine the problematic observations relative to the full data set. Are they simply outliers? In this case, remove them from the data and re-train the classifier.\n* Consider smoothing the predicted probabilities using, for example, Laplace Smoothing. This will result in a less \u201ccertain\u201d classifier and might improve the overall Log Loss."}}