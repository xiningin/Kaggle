{"cell_type":{"41cdcdf9":"code","15eab0d1":"code","61f8f3bf":"code","0a7b8049":"code","09b1d731":"code","759baba7":"code","66f89421":"code","3fa33365":"code","2fefee39":"code","6eb078ed":"code","21c8880b":"code","e9bec9fb":"code","584ba604":"code","89cdaceb":"code","40676737":"code","b6ee9474":"code","18106dbf":"code","a15dd210":"code","3a17254f":"code","058c84c9":"code","580f9e37":"code","3ec6edb7":"code","5fe874f4":"markdown","b00aabd5":"markdown","8f93f0a5":"markdown","e7b45ed3":"markdown","70931cfb":"markdown","1cadec28":"markdown","28f67aa2":"markdown","ecdcd838":"markdown","530d99d0":"markdown","f850b6d1":"markdown","2913b44e":"markdown","a493546d":"markdown","43ba7c6f":"markdown"},"source":{"41cdcdf9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ncolors = [\"#9DAFB0\", \"#75E6DA\", '#189AB4', '#05445E', '#4D707E', '#56B66B']\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","15eab0d1":"data = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndata.drop(['id'], axis=1, inplace=True)\ndata","61f8f3bf":"feature_num = ['age', 'avg_glucose_level', 'bmi']\nfeature_cat = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\nlabel = ['stroke']","0a7b8049":"data.isnull().sum()","09b1d731":"fig, ax = plt.subplots(figsize=(8, 6))\n\nsns.countplot(data=data, x='stroke', ax=ax, palette=colors[2:4])\n\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.set_axisbelow(True)\n\n\nplt.legend(['count'])\nplt.grid(axis='y', alpha=0.5)\nplt.suptitle(\"Count Plot\", ha='left', x=0.125, y=0.97, fontsize=18, fontweight='bold')\nplt.title(\"distribution of target feature (stroke)\", loc='left')\nplt.show()","759baba7":"data = data[data.gender != 'Other']","66f89421":"df = data[feature_num]\n\nfig, ax = plt.subplots(2, 3, figsize=(14, 12))\n\nfor i in range(3) : \n    median = df.median()[feature_num[i]]\n    sns.kdeplot(data=df, x=feature_num[i], ax=ax[0, i], shade=True, color=colors[2])\n    ax[0, i].axvline(x=median, color='#05445E')\n\n    ax[0, i].set_title(f'{feature_num[i]} distribution', loc='left', pad=10)\n    ax[0, i].grid(axis='y', alpha=0.3)\n    ax[0, i].annotate(\n        f'median: {median}', \n        xy=(median, 0.015), \n        xytext=(median+20, 0.015), \n        arrowprops=dict(arrowstyle='-|>',\n        fc='black', shrinkA=0, shrinkB=0,            \n        connectionstyle='angle,angleA=0,angleB=90,rad=10')\n    )\n\n    ax[0, i].set_axisbelow(True)\n\n\nfor i in range(3) : \n    median = df.median()[feature_num[i]]\n    sns.boxplot(data=df, y=feature_num[i], ax=ax[1, i], color=colors[2])\n    ax[1, i].set_title(f'{feature_num[i]} distribution', loc='left', pad=10)\n    ax[1, i].grid(axis='y', alpha=0.3)\n    ax[1, i].set_axisbelow(True)\n  \nplt.suptitle(\"Distribution Plot\", ha='left', x=0.125, y=0.93, fontsize=18, fontweight='bold')\nplt.show()","3fa33365":"df1 = data[data.stroke == 1]\ndf0 = data[data.stroke == 0]\n\n\nfig, ax = plt.subplots(1, 3, figsize=(14, 6))\nfor i in range(3) : \n    sns.kdeplot(data=df1, x=feature_num[i], ax=ax[i], shade=True, color=colors[2])\n    sns.kdeplot(data=df0, x=feature_num[i], ax=ax[i], shade=True, color=colors[5])\n\n    ax[i].set_title(f'{feature_num[i]} distribution', loc='left')\n    ax[i].legend(['1', '0'])\n    ax[i].grid(axis='y', alpha=0.3)\n    ax[i].set_axisbelow(True)\n\nplt.suptitle(\"Distribution Plot per Target\", ha='left', x=0.125, y=0.97, fontsize=18, fontweight='bold')\nplt.show()","2fefee39":"df = data\ncorr = df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(11, 9))\n\nmask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n\nsns.heatmap(corr, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, cmap=sns.color_palette(\"Blues\", as_cmap=True))\n\nax.set_axisbelow(True)\n\nplt.legend(['Pearson Correlation'])\nplt.suptitle(\"Count Plot\", ha='left', x=0.125, y=0.94, fontsize=18, fontweight='bold')\nplt.title(\"correlation for every feature\", loc='left')\nplt.show()","6eb078ed":"data.groupby('ever_married').mean()[['bmi']]","21c8880b":"data['bmi'] = data.groupby('ever_married').transform(lambda x: x.fillna(x.mean()))['bmi']\ndata","e9bec9fb":"X = data.drop(['stroke'], axis=1)\ny = data['stroke']","584ba604":"transformer_num = make_pipeline(MinMaxScaler())\ntransformer_cat = make_pipeline(OneHotEncoder())\n\npreprocessor = make_column_transformer(\n    (transformer_num, feature_num),\n    (transformer_cat, feature_cat),\n)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, train_size=0.8)\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[-1]]","89cdaceb":"smote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","40676737":"clf_rand = RandomForestClassifier(n_estimators=100)\n\nclf_rand.fit(X_train, y_train) \n\npred_rand= clf_rand.predict(X_valid)\n\nprint(classification_report(pred_rand, y_valid.to_numpy()))","b6ee9474":"clf_nb = GaussianNB()\n\nclf_nb.fit(X_train, y_train) \n\npred_nb = clf_nb.predict(X_valid)\n\nprint(classification_report(pred_nb, y_valid.to_numpy()))","18106dbf":"model = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    \n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    \n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    \n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    \n    layers.Dense(units=1, activation='sigmoid'),\n    \n])","a15dd210":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)","3a17254f":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    x=X_train, y=y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32,\n    epochs=200,\n    callbacks=[early_stopping]\n    \n)","058c84c9":"fig, ax = plt.subplots(1, 2,figsize=(16,6))\n\nax[0].plot(history.history['loss'], color=colors[2])\nax[0].plot(history.history['val_loss'], color=colors[5])\nax[0].spines['right'].set_visible(False)\nax[0].spines['top'].set_visible(False)\nax[0].set_axisbelow(True)\nax[0].legend([\"loss\", \"val loss\"])\nax[0].grid(axis='y', alpha=0.5)\nax[0].set_title(\"Loss graph\", loc='left', fontsize=14)\nax[0].set_ylabel(\"Loss\")\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(history.history['binary_accuracy'], color=colors[2])\nax[1].plot(history.history['val_binary_accuracy'], color=colors[5])\nax[1].spines['right'].set_visible(False)\nax[1].spines['top'].set_visible(False)\nax[1].set_axisbelow(True)\nax[1].legend([\"accuracy\", \"val accuracy\"])\nax[1].set_title(\"Accuracy graph\", loc='left', fontsize=14)\nax[1].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Epochs\")\n\n\n\nplt.grid(axis='y', alpha=0.5)\nplt.suptitle(\"Model Performance History\", ha='left', x=0.125, y=0.97, fontsize=18, fontweight='bold')\nplt.show()","580f9e37":"y_pred = model.predict(X_valid)\ny_pred = [1 if i > 0.5 else 0 for i in y_pred]","3ec6edb7":"print(classification_report(y_pred, y_valid.to_numpy()))","5fe874f4":"# EDA","b00aabd5":"# Loading Data","8f93f0a5":"# Pre-processing","e7b45ed3":"## Random Forest","70931cfb":"the dataset is highly imbalance, we will try to oversample the dataset using SMOTE later on","1cadec28":"# Introduction\n\n\nStroke is a condition that occurs when the blood supply to the brain decreases due to a blockage (ischemic stroke) or rupture of a blood vessel (hemorrhagic stroke). Without blood, the brain will not get oxygen and nutrients, so the cells in the affected brain area will soon die. \n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n\nStroke is a medical emergency because brain cells can die in just a matter of minutes. The death of brain cells causes the parts of the body that are controlled by the damaged areas of the brain to not function properly. Prompt treatment can minimize the level of damage to the brain and the possibility of complications. \n\n\nFrom the description above, we know that stroke is simply deadly. In this notebook im trying to found out can we predict the possibility of a patient having a stroke based on the given dataset.\n\n## Attribute Information\n\n* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n* age: age of the patient\n* hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* ever_married: \"No\" or \"Yes\"\n* work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* Residence_type: \"Rural\" or \"Urban\"\n* avg_glucose_level: average glucose level in blood\n* bmi: body mass index\n* smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n","28f67aa2":"## Naive Bayes","ecdcd838":"# Modelling","530d99d0":"# Handle Missing Data","f850b6d1":"# SMOTE\n\nI apply smote only on train set to avoid data leakage","2913b44e":"### Side Note\n\nif you have any feedback or criticism, please feel free to comments cheers!","a493546d":"## Deep Learning","43ba7c6f":"# Conclusion\n\nsince the data is highly imbalance and not enough feature that is given, our model aren't able to predict target 1 (person has stroke) in a high accuracy even though upsampling using SMOTE is already used on our dataset. \n\ngiven the right amount of dataset and feature, performance of the model might improve. "}}