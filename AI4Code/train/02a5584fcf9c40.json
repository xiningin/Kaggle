{"cell_type":{"447082da":"code","a3e338e6":"code","0079ff17":"code","3df28288":"code","3d2dd304":"code","da5099e8":"code","b9bb9b03":"code","543fa070":"code","6b1a3f1b":"code","c29f0e19":"code","d96ab470":"code","4d49b816":"code","9d7dc8ac":"code","af46189a":"code","c1eb5ca7":"code","323ab71a":"code","edda697e":"code","f7930a01":"code","f94c17bb":"code","2e84b3e1":"code","f333631f":"code","df5bb267":"code","ad695cd5":"code","8cd5afb6":"code","7b9907d1":"code","84d6432b":"code","1fe58f85":"code","663263b9":"code","b55ded13":"code","5a5f1d4c":"code","b12d765c":"code","630cf0f6":"code","5da5e5e3":"code","5e4c7623":"code","aec176a7":"code","c425d335":"code","1ff04036":"code","000d9aba":"code","aa3d45ce":"code","64743a5c":"code","105c4dff":"code","244a5ae1":"code","1cfeec28":"code","5c6102ea":"code","441fb1b6":"code","dd4d53cb":"code","19eba3de":"code","c72d99d8":"code","211413ed":"code","bc380874":"code","b085c523":"code","66ed966e":"code","d7c6b68c":"code","280703e9":"code","635668b5":"code","0a8e82aa":"code","074f1963":"code","aa4a3d63":"markdown","dbfac5e3":"markdown","90958078":"markdown","e2b6090f":"markdown","0fe88dd9":"markdown","f530860e":"markdown","6081884b":"markdown","0bffa90e":"markdown","93dcae7b":"markdown","892ddba6":"markdown","11e554a1":"markdown","5272cbc5":"markdown","1cf29173":"markdown","e9c35398":"markdown","b8565bd8":"markdown","68a2b7af":"markdown","90053ff2":"markdown","cd1a4f11":"markdown","0dc08eeb":"markdown","426b94c0":"markdown","f4c16a04":"markdown","235b1554":"markdown","615a1593":"markdown","4bdaf47c":"markdown","49e2b4c6":"markdown","75bb5bbc":"markdown"},"source":{"447082da":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Please write all the code with proper documentation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import tree\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","a3e338e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0079ff17":"#Training data\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint('Training data shape: ', train.shape)\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint('Testing data shape: ', test.shape)\n","3df28288":"train.head(5)","3d2dd304":"test.head()","da5099e8":"#Missing values in training set\nprint(\"Missing values in train data\")\ntrain.isnull().sum()\n","b9bb9b03":"#Missing values in testing set\nprint(\"Missing values in test data\")\ntest.isnull().sum()","543fa070":"train['target'].value_counts()","6b1a3f1b":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","c29f0e19":"train_positive=train[train['target']==1]['text']\ntrain_positive.values[120]","d96ab470":"train_positive.values[125]","4d49b816":"train_negative=train[train['target']==0]['text']\ntrain_negative.values[12]","9d7dc8ac":"train_negative.values[11]","af46189a":"train['keyword'].value_counts()","c1eb5ca7":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')\nplt.show()","323ab71a":"train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","edda697e":"train['location'].value_counts()","f7930a01":"sns.barplot(y=train['location'].value_counts()[:20].index,x=train['location'].value_counts()[:20],orient='h')\nplt.show()","f94c17bb":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                           \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],orient='h')\nplt.show()","2e84b3e1":"# https:\/\/stackoverflow.com\/questions\/16206380\/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n# Refer my Github for more similar examples\n# Below is a genric function which can be found at a lot of surces to clean text\nfrom bs4 import BeautifulSoup\n# https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro\nimport re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\nprint('Text after cleaning')\ntrain['text'][120]\n","f333631f":"eng_stopwords = set(stopwords.words(\"english\"))\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))","df5bb267":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'num_stopwords', data = train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target'] == 1.0]['num_stopwords'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['target'] == 0.0]['num_stopwords'][0:] , label = \"0\" , color = 'blue' )\nplt.legend()\nplt.show()","ad695cd5":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))","8cd5afb6":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'num_words', data = train[0:])\nplt.subplot(1,2,2)\nsns.distplot(train[train['target'] == 1.0]['num_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['target'] == 0.0]['num_words'][0:] , label = \"0\" , color = 'blue' )\nplt.legend()\nplt.show()","7b9907d1":"from wordcloud import WordCloud\n\nwordcloud = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(train_positive))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Disaster Tweets',fontsize=40);\n\n","84d6432b":"wordcloud = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(train_negative))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Non Disaster Tweets',fontsize=40);","1fe58f85":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","663263b9":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train['text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())\n","b55ded13":"X=preprocessed_reviews[:]\ny=train['target'][:]\nX_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=42)","5a5f1d4c":"# bow is bag of words\nbow = CountVectorizer()\ntrain_vectors = bow.fit_transform(train['text'])\ntest_vectors = bow.transform(test[\"text\"])","b12d765c":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n# Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","630cf0f6":"# bow is bag of words\nbow = CountVectorizer(ngram_range=(1, 2))\ntrain_vectors = bow.fit_transform(train['text'])\ntest_vectors = bow.transform(test[\"text\"])","5da5e5e3":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n# Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","5e4c7623":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","aec176a7":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# #Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n## Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# #Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","c425d335":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)","1ff04036":"X=train['text']\ny=train['target'][:]\nX_train=X[:6500]\nX_test=X[6500:]\ny_train=y[:6500]\ny_test=y[6500:]","000d9aba":"max(train['text'].apply(lambda x: len(x)))","aa3d45ce":"np.mean(train['text'].apply(lambda x: len(x)))","64743a5c":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","105c4dff":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\ntest=tokenizer.texts_to_sequences(test['text'])","244a5ae1":"## Zero Padding\nmax_review_length = 157\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","1cfeec28":"test = sequence.pad_sequences(test, maxlen=max_review_length)","5c6102ea":"print(X_train[45])","441fb1b6":"# create the model\ntop_words=10000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())","dd4d53cb":"def plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","19eba3de":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=1, verbose=2, validation_data=(X_test, y_test))\n\n\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])","c72d99d8":"# fig,ax = plt.subplots(1,1)\n# ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# # list of epoch numbers\n# x = list(range(1,16))\n# vy = history.history['val_loss']\n# ty = history.history['loss']\n# plt_dynamic(x, vy, ty, ax)\n","211413ed":"# create the model\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())\nepoch=15\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\n\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","bc380874":"# create the model\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","b085c523":"# create the model\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n\nmodel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# model.add(LSTM(128))\n# model.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","66ed966e":"# create the model\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n\nmodel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# model.add(LSTM(128))\n# model.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","d7c6b68c":"# clf_nb = MultinomialNB()\n# scores = model_selection.cross_val_score(clf_nb, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n# print('scores of Naive Bayes: ')\n# print(scores)","280703e9":"# clf_nb.fit(train_tfidf, train[\"target\"])","635668b5":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.evaluate(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","0a8e82aa":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\nsample_submission = pd.read_csv(submission_file_path)\nsample_submission[\"target\"] = model.predict_classes(test)\nsample_submission.to_csv(\"submission.csv\", index=False)","074f1963":"# submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\n# test_vectors=test\n# submission(submission_file_path,model,test_vectors)","aa4a3d63":"### Text in Positive and negative class","dbfac5e3":"Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or [not.](https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro)","90958078":"### Competition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n","e2b6090f":"> 1. ### **Importing Necessary Libraries**","0fe88dd9":"### Evaluation Metrics\nThe valid metric considered for this competition is[ F1](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html) score. \n\nThe F score is defined as the weighted harmonic mean of the test\u2019s precision and recall.\n![](https:\/\/imgur.com\/nC4QwrO.png)\n\nWhere, \nTP - True Positive\nFP - False Positive\nTN - True Negative\nFN - False Negative","f530860e":"## Submissions","6081884b":"> ### **3. Exploratory Data Analysis(EDAs)**","0bffa90e":"> * >  ### **2. Reading CSV Files**","93dcae7b":"### **Text Processing**","892ddba6":"**Replacing the ambigious locations name with Standard names**","11e554a1":"[Naive Bayes](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)","5272cbc5":"### Data Provided\nDataset contains a set of tweets which are divided into train and test tweets. \nThe aim of this competetition is to predict whether the given tweet is of a Disaster or not. \n\nOur job is to create a ML model to predict whether the test set tweets belong to a disaster or not, in the form of 1 or 0.This is a classic case of a Binary Classification problem.","1cf29173":"### Missing Values","e9c35398":"### LSTMs","b8565bd8":"## Modelling Using Various Models and Vectorization Techniques","68a2b7af":"### **Using TFIDF using bigram features**","90053ff2":"### Contents:\n1.  Importing Necessary Libraries\n1.  Reading different csv files\n1.  Exploratory Data Analysis(EDAs)\n1.  Text Cleaning \n1.  Basic Models\n1.  Long Short Term Memory(LSTMs) ","cd1a4f11":"### ** Using Bag of Words**","0dc08eeb":"### WordCloud","426b94c0":"Unigram Features from BOW","f4c16a04":"> ### **Number of Words and Stop Words**\n\nLet's just check Number of Stopwords and Number of Words for fun","235b1554":"### **Keyword Column**","615a1593":"**Location Column**","4bdaf47c":"> ### Target Column","49e2b4c6":"Bigram features from BOW","75bb5bbc":"> ### **4. Text Cleaning and Preprocessing**"}}