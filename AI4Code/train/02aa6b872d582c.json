{"cell_type":{"1c6a8ef4":"code","4635d705":"code","3a5b336b":"code","657a0e40":"code","f1322f92":"code","42034abd":"code","a1b227fb":"code","a4208522":"code","267f8afe":"code","6f429cb0":"code","3fd60c21":"code","526cea0a":"code","cb3ae9df":"code","2b08959e":"code","f4c45267":"code","9bd5fee0":"code","71b25842":"code","eb1572af":"code","c0ffed68":"code","f21878b3":"code","3fb4919f":"code","cd0d2e1d":"code","0053254e":"code","530fa126":"code","e6b5b84b":"code","906227b7":"code","3ba9f2c7":"code","28bf4420":"code","af1e997a":"code","49d1f407":"code","31fc01a6":"code","25fb51cf":"code","4a100511":"code","8fc3df7c":"code","3dd78a1f":"code","755848b2":"code","0eabafa9":"code","c80781c1":"code","e49c5544":"code","607ccad6":"code","3c907d8c":"code","b42f4578":"code","c3e1b767":"code","79b174b5":"code","0d670996":"code","768dd520":"code","a260842a":"code","960d8918":"code","14fe200a":"code","c23aee8b":"code","994e933c":"code","3107d887":"code","b99933c4":"code","c1b84c2c":"code","f7641559":"code","358b1dd3":"code","2e038b3b":"code","18d14941":"code","52fd6c14":"code","8edc1a5e":"code","63090cc5":"markdown","dd7ec184":"markdown","b8955085":"markdown","2559867e":"markdown","2d59ac0a":"markdown","94dda1ab":"markdown","9448e338":"markdown","99a04a09":"markdown","8a9b4a03":"markdown","ea5e0c23":"markdown","cb6e5402":"markdown","4dd7c4c3":"markdown","3554a620":"markdown","46aa2abd":"markdown","4e831f59":"markdown","465b94d4":"markdown","15b5c371":"markdown","19463a1a":"markdown","84b0ade1":"markdown","d8018503":"markdown","f955e6c4":"markdown","9123e67c":"markdown","e96f17fa":"markdown","48f0b2c4":"markdown","e2b6a904":"markdown","41040c9c":"markdown","813ee369":"markdown","32f0946f":"markdown","d0cc26a5":"markdown","0cea8ab2":"markdown","240a2b35":"markdown","8f0e104d":"markdown","79fb3d38":"markdown","1a7ef259":"markdown"},"source":{"1c6a8ef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4635d705":"import glob\nimport os.path as osp\nimport copy\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision import datasets, models\nfrom torchvision.utils import make_grid\n\nimport os\nimport time\nfrom PIL import Image\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')","3a5b336b":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n## Image Augmentation \n\n# skimage\nfrom skimage.io import imshow, imread, imsave\nfrom skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\nfrom skimage import color,data\nfrom skimage.exposure import adjust_gamma\nfrom skimage.util import random_noise\n\n\n# 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib import colors\n\n\n#OpenCV-Python\nimport cv2\n\n# imgaug\nimport imageio\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nSAMPLE_LEN=100","657a0e40":"class Config:\n    num_classes = 12\n    img_size = 224\n    batch_size = 32\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    min_lr = 10**-12\n    max_lr = 10\n    pretrained = False\n    criterion = nn.CrossEntropyLoss()\n    epochs = 20","f1322f92":"train_image_path = '..\/input\/plant-pathology-2021-fgvc8\/train_images'\ntest_image_path = '..\/input\/plant-pathology-2021-fgvc8\/test_images'\ntrain_df_path = '..\/input\/plant-pathology-2021-fgvc8\/train.csv'\ntest_df_path = '..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv'","42034abd":"df_train = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/train.csv\")\ndf_sub = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\")","a1b227fb":"# #The number of labels\n# len(df_train.labels.unique())","a4208522":"# #The no.values per label\n# df_train.labels.value_counts()","267f8afe":"# plt.figure(figsize=(15,12))\n# labels = sns.barplot(df_train.labels.value_counts().index,df_train.labels.value_counts())\n# for item in labels.get_xticklabels():\n#     item.set_rotation(45)","6f429cb0":"# source = df_train['labels'].value_counts()","3fd60c21":"# fig = go.Figure(data=[go.Pie(labels=source.index,values=source.values)])\n# fig.update_layout(title='Label distribution')\n# fig.show()","526cea0a":"# img_shapes = {}\n# for image_name in tqdm(os.listdir(train_image_path)[:300]):\n#     image = cv2.imread(os.path.join(train_image_path, image_name))\n#     img_shapes[image.shape] = img_shapes.get(image.shape, 0) + 1\n\n# print(img_shapes)","cb3ae9df":"# def visualize_batch(path,image_ids, labels):\n#     plt.figure(figsize=(16, 12))\n    \n#     for ind, (image_id, label) in enumerate(zip(image_ids, labels)):\n#         plt.subplot(3, 3, ind + 1)\n#         image = cv2.imread(os.path.join(path, image_id))\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#         plt.imshow(image)\n#         plt.title(f\"Class: {label}\", fontsize=12)\n#         plt.axis(\"off\")\n#     plt.show()","2b08959e":"# tmp_df = df_train.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n# visualize_batch(train_image_path,image_ids,labels)","f4c45267":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()","9bd5fee0":"def encode_label(df):\n    df['encoded_label'] = le.fit_transform(df.labels.values)\n    return df\n\nencode_label(df_train)\n    \n# L\u01b0u t\u1eeb \u0111i\u1ec3n m\u00e3 h\u00f3a\ndf_labels_idx = df_train.loc[df_train.duplicated([\"labels\", \"encoded_label\"])==False]\\\n                [[\"encoded_label\", \"labels\"]].set_index(\"encoded_label\").sort_index()\ndisplay(df_labels_idx)","71b25842":"# #label = complex\n# tmp_df = df_train[df_train[\"encoded_label\"] == 0]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","eb1572af":"# #label = frog_eye_leaf_spot\n# tmp_df = df_train[df_train[\"encoded_label\"] == 1]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","c0ffed68":"# #label = frog_eye_leaf_spot complex\n# tmp_df = df_train[df_train[\"encoded_label\"] == 2]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","f21878b3":"# #label = healthy\n# tmp_df = df_train[df_train[\"encoded_label\"] == 3]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","3fb4919f":"# #label = powdery_mildew\n# tmp_df = df_train[df_train[\"encoded_label\"] == 4]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","cd0d2e1d":"# #label = powdery_mildew complex\n# tmp_df = df_train[df_train[\"encoded_label\"] == 5]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","0053254e":"# #label = rust\n# tmp_df = df_train[df_train[\"encoded_label\"] == 6]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","530fa126":"# #label = rust complex\n# tmp_df = df_train[df_train[\"encoded_label\"] == 7]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","e6b5b84b":"# #label = rust frog_eye_leaf_spot\n# tmp_df = df_train[df_train[\"encoded_label\"] == 8]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","906227b7":"# #label = scab\n# tmp_df = df_train[df_train[\"encoded_label\"] == 9]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","3ba9f2c7":"# #label = scab frog_eye_leaf_spot\n# tmp_df = df_train[df_train[\"encoded_label\"] == 10]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","28bf4420":"# #label = scab frog_eye_leaf_spot complex\n# tmp_df = df_train[df_train[\"encoded_label\"] == 11]\n# print(f\"Total train images for class 0: {tmp_df.shape[0]}\")\n\n# tmp_df = tmp_df.sample(9)\n# image_ids = tmp_df[\"image\"].values\n# labels = tmp_df[\"labels\"].values\n\n# visualize_batch(train_image_path, image_ids, labels)","af1e997a":"# def load_image(image_id):\n#     file_path = image_id\n#     image = cv2.imread(train_image_path+'\/'+ file_path)\n#     return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# # L\u1ea5y 100 \u1ea3nh l\u00e0m sample v\u1edbi SAMPLE_LEN=100 cho RBG Channel Analysis\n\n# train_images = df_train[\"image\"][:SAMPLE_LEN].apply(load_image)","49d1f407":"# red_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\n# green_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\n# blue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\n# values = [np.mean(train_images[idx]) for idx in range(len(train_images))]","31fc01a6":"# fig = ff.create_distplot([red_values], group_labels=[\"R\"], colors=[\"red\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh \u0110\u1ecf\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","25fb51cf":"# fig = ff.create_distplot([green_values], group_labels=[\"G\"], colors=[\"green\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh Xanh L\u00e1\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","4a100511":"# fig = ff.create_distplot([blue_values], group_labels=[\"B\"], colors=[\"blue\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh Xanh Lam\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","8fc3df7c":"# fig = go.Figure()\n\n# for idx, values in enumerate([red_values, green_values, blue_values]):\n#     if idx == 0:\n#         color = \"Red\"\n#     if idx == 1:\n#         color = \"Green\"\n#     if idx == 2:\n#         color = \"Blue\"\n#     fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n    \n# fig.update_layout(yaxis_title=\"Mean value\", xaxis_title=\"Color channel\",\n#                   title=\"Mean value vs. Color channel\", template=\"plotly_white\")","3dd78a1f":"# fig = ff.create_distplot([red_values, green_values, blue_values],\n#                          group_labels=[\"R\", \"G\", \"B\"],\n#                          colors=[\"red\", \"green\", \"blue\"])\n# fig.update_layout(title_text=\"Distribution of red channel values\", template=\"simple_white\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[1].marker.line.width = 0.5\n# fig.data[2].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[2].marker.line.width = 0.5\n# fig","755848b2":"from sklearn.model_selection import train_test_split","0eabafa9":"def make_datapath_list(phase='train', val_size=0.3):\n    if phase in [\"train\", \"val\"]:\n        phase_path = \"train_images\"\n    elif phase in [\"test\"]:\n        phase_path = \"test_images\"\n    else:\n        print(f\"{phase} not in path\")    \n        \n    \"\"\"\n    Use resized training dataset for betting training speed\n    Resized datase from: https:\/\/www.kaggle.com\/ankursingh12\/resized-plant2021\n    \"\"\"\n    if phase == 'train' or phase == 'val': \n        rootpath = \"\/kaggle\/input\/resized-plant2021\/img_sz_512\/\"\n    else:\n        rootpath = \"\/kaggle\/input\/plant-pathology-2021-fgvc8\/test_images\/\"\n    \n    target_path = osp.join(rootpath+\"\/*.jpg\")\n    path_list = []\n    \n    for path in glob.glob(target_path):\n        path_list.append(path)\n        \n    if phase in [\"train\", \"val\"]:\n        train, val = train_test_split(path_list, test_size=val_size, random_state=47, shuffle=True)\n        if phase == \"train\":\n            path_list = train\n        else:\n            path_list = val\n    \n    return path_list","c80781c1":"train_list = make_datapath_list(phase='train')\nprint(f'The length of training set: {len(train_list)}')\nval_list = make_datapath_list(phase='val')\nprint(f'The length of valuation set: {len(val_list)}')\ntest_list = make_datapath_list(phase='test')\nprint(f'The length of testing set: {len(test_list)}')","e49c5544":"import albumentations as A\nfrom albumentations import Compose\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\ntransform = {\n    'train': Compose([\n        A.Rotate(p=0.1, limit=(-85, 80)),\n        A.RandomShadow(\n            num_shadows_lower=2, \n            num_shadows_upper=3, \n            shadow_dimension=3, \n            shadow_roi=(0, 0.7, 0.4, 0.8), \n            p=0.4\n        ),\n        A.ShiftScaleRotate(\n            shift_limit=0.055, \n            scale_limit=0.065, \n            rotate_limit=35, \n            p=0.6\n        ),\n        A.RandomFog(\n            fog_coef_lower=0.2, \n            fog_coef_upper=0.2, \n            alpha_coef=0.2, \n            p=0.3\n        ),\n        A.RGBShift(\n            r_shift_limit=25, \n            g_shift_limit=15, \n            b_shift_limit=15, \n            p=0.3\n        ),\n        A.RandomBrightnessContrast(p=0.3),\n        A.GaussNoise(\n            var_limit=(50, 70),  \n            always_apply=False, \n            p=0.3\n        ),\n        A.Resize(height=Config.img_size, width=Config.img_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n    'val': Compose([\n        A.Resize(Config.img_size, Config.img_size),\n        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),            \n        ToTensorV2()\n    ]),\n    'test': Compose([\n        A.Resize(Config.img_size, Config.img_size),\n        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n}","607ccad6":"class PlantDataset(Dataset):\n    \"\"\"\n    Class to create a Dataset\n    \n    Attributes\n    ----------\n    df_train : DataFrame\n        DataFrame containing the image labels.\n    file_list : list\n        A list containing the paths to the images\n    transform : object\n        Instance of the preprocessing class (ImageTransform)\n    phase : 'train' or 'val' or 'test'\n        Specify whether to use train, validation, or test\n    \"\"\"\n    def __init__(self, df_train, file_list, transform=None, phase='train'):\n        self.df_train = df_train\n        self.df_labels_idx = df_labels_idx\n        self.file_list = file_list\n        self.transform = transform[phase]\n        self.phase = phase\n        \n    def __len__(self):\n        \"\"\"\n        Returns the number of images.\n        \"\"\"\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Get data in Tensor format and labels of preprocessed images.\n        \"\"\"\n        \n        # Load the index number image.\n        img_path = self.file_list[index]\n        img = Image.open(img_path)\n        \n        # Preprocessing images\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_transformed = self.transform(image=img)\n        \n        # image name\n        image_name = img_path[-20:]\n        \n        # Extract the labels\n        if self.phase in [\"train\", \"val\"]:\n            label = df_train.loc[df_train[\"image\"]==image_name][\"encoded_label\"].values[0]\n        elif self.phase in [\"test\"]:\n            label = -1\n        \n        return img_transformed, label, image_name","3c907d8c":"train_dataset = PlantDataset(df_train, train_list, transform=transform, phase='train')\nval_dataset = PlantDataset(df_train, val_list, transform=transform, phase='val')\ntest_dataset = PlantDataset(df_train, test_list, transform=transform, phase='test')\n\nindex = 0\n\nprint(\"\u3010train dataset\u3011\")\nprint(f\"img num : {train_dataset.__len__()}\")\n# print(f\"img : {train_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {train_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {train_dataset.__getitem__(index)[2]}\")\n\nprint(\"\\n\u3010validation dataset\u3011\")\nprint(f\"img num : {val_dataset.__len__()}\")\n# print(f\"img : {val_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {val_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {val_dataset.__getitem__(index)[2]}\")\n\nprint(\"\\n\u3010test dataset\u3011\")\nprint(f\"img num : {test_dataset.__len__()}\")\n# print(f\"img : {test_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {test_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {test_dataset.__getitem__(index)[2]}\")","b42f4578":"train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, num_workers=2,shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, num_workers=2, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, num_workers=2, shuffle=False)\n\n# to Dictionary\ndataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}","c3e1b767":"for i, image_data in enumerate(train_dataloader):\n    break","79b174b5":"plt.figure(figsize=(20, 20))\n\nim = make_grid(image_data[0]['image'], nrow=8)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)))","0d670996":"# # Load the Model back from file\n# Pkl_Filename = '..\/input\/pickle-test\/Resmodel50_trained_1fc.pkl'\n# with open(Pkl_Filename, 'rb') as file:  \n#     model = pickle.load(file)","768dd520":"from sklearn.metrics import f1_score, accuracy_score","a260842a":"model_config = {\n    \"name\": \"2 FCs, 0.0001 Lr, 30 Epochs\",\n    \"classifier\": torch.nn.Sequential(\n                  torch.nn.Linear(2048, 1024),\n                  torch.nn.Linear(1024, 12)),\n    \"lr\": 0.001,\n    \"epoch\": 20\n}","960d8918":"use_pretrained = True\npretrained_model = models.resnet152(pretrained=use_pretrained)","14fe200a":"name, classifier, lr, epoch = model_config.values()\n\npretrained_model.fc = classifier\n\n# print(f'Model name: {name}')\n# print(pretrained_model)","c23aee8b":"def lr_finder(model, min_lr, max_lr, dataset_lenght=train_dataset.__len__(), \\\n              batch_size=Config.batch_size, criterion=Config.criterion):\n    iter_lrs = [min_lr]\n    iter_losses = []\n    \n    factor = np.exp(np.log(max_lr \/ min_lr) \/ (dataset_lenght \/ batch_size))\n    \n    # Train model with 1 epoch\n    model.to(Config.device)\n    for i, data in tqdm(enumerate(dataloaders_dict['train']), total=len(dataloaders_dict['train'])):\n        \n        optimizer = Adam(model.parameters(), lr=min_lr)\n        \n        # set inputs, labels based on dataloader's batch data\n        inputs = data[0]['image']\n        labels = data[1]\n        inputs = inputs.to(Config.device)\n        labels = labels.to(Config.device)\n\n        #zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # backward + optimize only if in training phase\n        loss.backward()\n        optimizer.step()\n                \n        # Update and append next iteration learning rate\n        iter_lrs.append(min_lr)\n        min_lr = min_lr * factor\n        \n        # Append this iteration loss\n        iter_losses.append(np.log(loss.cpu().data.numpy().tolist()))\n        \n    iter_lrs.pop()\n    \n    # Plot loss vs log-scaled learning rate\n    plt.figure(figsize=(10, 7))\n    plot = sns.lineplot(iter_lrs, iter_losses)\n    plot.set(xscale=\"log\", \n             xlabel=\"Learning Rate (log-scale)\", \n             ylabel=\"Training Loss\",\n             title=\"Optimal learning rate is slightly below minimum\")","994e933c":"# test_model = copy.deepcopy(pretrained_model)\n# lr_finder(test_model, Config.min_lr, Config.max_lr)","3107d887":"# Config.lr = 10**-3\n# optimizer = Adam(pretrained_model.parameters(), lr=Config.lr)\noptimizer = Adam(pretrained_model.parameters(), lr=1e-3)","b99933c4":"def plot_result(train_losses, train_accuracy, train_f1, val_losses, val_accuracy, val_f1):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 7))\n    ax1.plot(train_losses, label='Train')\n    ax1.plot(val_losses, label='Validation')\n    ax1.set_title('Loss')\n    ax1.legend()\n\n    ax2.plot(train_accuracy, label='Train')\n    ax2.plot(val_accuracy, label='Validation')\n    ax2.set_title('Accuracy')\n    ax2.legend()\n\n    ax3.plot(train_f1, label='Train')\n    ax3.plot(val_f1, label='Validation')\n    ax2.set_title('F1 Score')\n    ax3.legend()","c1b84c2c":"def append_list(list, appended):\n    for el in appended:\n        list.append(el)\n    return list","f7641559":"def train_model(model, criterion, optimizer, num_epochs=Config.epochs):\n    \n    train_losses = []\n    train_accuracy = []\n    train_f1 = []\n\n    val_losses = []\n    val_accuracy = []\n    val_f1 = []\n    \n    print(f\"Devices to be used : {Config.device}\")\n    model.to(Config.device)\n    torch.backends.cudnn.benchmark = True\n    \n    start_time = time.time()\n        \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            \n            epoch_targets = []\n            epoch_predictions = []\n\n            # Iterate over data.\n            for i, data in tqdm(enumerate(dataloaders_dict[phase]), total=len(dataloaders_dict[phase])):\n#                 inputs = np.transpose(data[0]['image'], (0, 3, 1, 2))\n                inputs = data[0]['image']\n                labels = data[1]\n                inputs = inputs.to(Config.device)\n                labels = labels.to(Config.device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                np_preds = preds.cpu().data.numpy()\n                np_labels = labels.cpu().data.numpy()\n                append_list(epoch_predictions, np_preds)\n                append_list(epoch_targets, np_labels)\n                \n                batch_f1 = f1_score(preds.cpu().data.numpy(), labels.cpu().data.numpy(), average='weighted')\n                \n#                 if i % 50 == 0 and i != 0:\n#                 print(f'Batch: {i}  |  Loss: {loss.item():.4f}   |   F1-score: {batch_f1:.4f}%')         \n\n            epoch_loss = running_loss \/ len(dataloaders_dict[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders_dict[phase].dataset)\n            \n            epoch_f1 = f1_score(epoch_predictions, epoch_targets, average='weighted')\n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accuracy.append(epoch_acc)\n                train_f1.append(epoch_f1)\n            else:\n                val_losses.append(epoch_loss)\n                val_accuracy.append(epoch_acc)\n                val_f1.append(epoch_f1)\n    \n            print('{} Loss: {:.4f} Acc: {:.4f} F1_score: {:.4f}'.format('----> ' + phase.capitalize(), epoch_loss, epoch_acc, epoch_f1))\n            \n    print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed\n    \n    plot_result(train_losses, train_accuracy, train_f1, val_losses, val_accuracy, val_f1)\n    \n    return model","358b1dd3":"trained_model = train_model(pretrained_model, Config.criterion, optimizer)","2e038b3b":"def save_model(model, filename):\n    Pkl_Filename = name + \".pkl\"\n\n    with open(Pkl_Filename, 'wb') as file:\n        pickle.dump(model, file)\n        \nsave_model(trained_model, name)","18d14941":"class PlantPredictor():\n    \"\"\"\n    Class for predicting labels from output results\n    \n    Attributes\n    ----------\n    df_labels_idx: DataFrame\n        DataFrame that associates INDEX with a label name\n    \"\"\"\n    \n    def __init__(self, model, df_labels_idx, dataloaders_dict):\n        self.model = model\n        self.df_labels_idx = df_labels_idx\n        self.dataloaders_dict = dataloaders_dict\n        self.df_submit = pd.DataFrame()\n        \n    \n    def __predict_max(self, out):\n        \"\"\"\n        Get the label name with the highest probability.\n        \n        Parameters\n        ----------\n        predicted_label_name: str\n            Name of the label with the highest prediction probability\n        \"\"\"\n        maxid = np.argmax(out.detach().numpy(), axis=1)\n        df_predicted_label_name = self.df_labels_idx.iloc[maxid]\n        \n        return df_predicted_label_name\n    \n    def inference(self):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        df_pred_list = []\n        for i, data in enumerate(self.dataloaders_dict['test']):\n            image_name = data[2]\n            self.model.to(device)\n            inputs = data[0]['image']\n            inputs = inputs.to(device)\n            out = self.model(inputs)\n            device = torch.device(\"cpu\")\n            out = out.to(device)\n            df_pred = self.__predict_max(out).reset_index(drop=True)\n            df_pred[\"image\"] = image_name\n            df_pred_list.append(df_pred)\n            \n        self.df_submit = pd.concat(df_pred_list, axis=0)\n        self.df_submit = self.df_submit[[\"image\", \"labels\"]].reset_index(drop=True)","52fd6c14":"predictor = PlantPredictor(trained_model, df_labels_idx, dataloaders_dict)\npredictor.inference()\ndf_submit = predictor.df_submit.copy()","8edc1a5e":"df_submit.to_csv('submission.csv', index=False)\ndf_submit","63090cc5":"## Learning rate finder\nhttps:\/\/towardsdatascience.com\/the-learning-rate-finder-6618dfcb2025","dd7ec184":"There are 12 different labels.","b8955085":"### Quan s\u00e1t:\nGi\u00e1 tr\u1ecb k\u00eanh m\u00e0u xanh l\u00e1 c\u00e2y c\u00f3 ph\u00e2n ph\u1ed1i \u0111\u1ed3ng \u0111\u1ec1u h\u01a1n gi\u00e1 tr\u1ecb k\u00eanh m\u00e0u \u0111\u1ecf nh\u01b0ng l\u1ec7ch ph\u1ea3i, v\u1edbi \u0111\u1ec9nh nh\u1ecf h\u01a1n. S\u1ef1 ph\u00e2n b\u1ed1 c\u0169ng c\u00f3 \u0111\u1ed9 l\u1ec7ch b\u00ean ph\u1ea3i (tr\u00e1i ng\u01b0\u1ee3c v\u1edbi m\u00e0u \u0111\u1ecf) v\u00e0 ch\u1ebf \u0111\u1ed9 l\u1edbn h\u01a1n kho\u1ea3ng 160. \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng m\u00e0u xanh l\u00e1 c\u00e2y r\u00f5 n\u00e9t h\u01a1n trong nh\u1eefng h\u00ecnh \u1ea3nh n\u00e0y so v\u1edbi m\u00e0u \u0111\u1ecf, \u0111i\u1ec1u n\u00e0y c\u00f3 \u00fd ngh\u0129a, b\u1edfi v\u00ec \u0111\u00e2y l\u00e0 h\u00ecnh \u1ea3nh c\u1ee7a nh\u1eefng chi\u1ebfc l\u00e1!","2559867e":"## Ph\u00e2n ph\u1ed1i K\u00eanh Xanh Lam","2d59ac0a":"# 4. Hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1 model","94dda1ab":"## 2.1 M\u00e3 h\u00f3a nh\u00e3n\nM\u00e3 h\u00f3a c\u00e1c nh\u00e3n c\u00f3 trong t\u1eadp d\u1eef li\u1ec7u v\u1ec1 d\u1ea1ng *integer* \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c.","9448e338":"## 2.2 T\u1ea1o \u0111\u01b0\u1eddng d\u1eabn cho \u1ea3nh v\u00e0 ph\u00e2n chia t\u1eadp hu\u1ea5n luy\u1ec7n, t\u1eadp th\u1ea9m \u0111\u1ecbnh","99a04a09":"## 2.5 Create Dataloader","8a9b4a03":"# 2. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u","ea5e0c23":"## Ph\u00e2n ph\u1ed1i K\u00eanh \u0110\u1ecf","cb6e5402":"# M\u1ed9t s\u1ed1 \u1ea3nh v\u00ed d\u1ee5 t\u1eeb t\u1eadp d\u1eef li\u1ec7u\nCh\u00fang t\u00f4i s\u1ebd ki\u1ec3m tra k\u00edch th\u01b0\u1edbc c\u1ee7a 300 h\u00ecnh \u1ea3nh \u0111\u1ea7u ti\u00ean\n\nNh\u01b0 c\u00f3 th\u1ec3 th\u1ea5y b\u00ean d\u01b0\u1edbi th\u00ec t\u1ea5t c\u1ea3 c\u00e1c h\u00ecnh \u1ea3nh c\u00f3 k\u00edch th\u01b0\u1edbc kh\u00e1c nhau.","4dd7c4c3":"# 3. Define model","3554a620":"D\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3, ch\u00fang ta c\u00f3 th\u1ec3 ch\u1ecdn learning rate c\u1ee7a Adam = 10^-3. ","46aa2abd":"### 5.3 Make submission","4e831f59":"## Ph\u00e2n ph\u1ed1i K\u00eanh Xanh L\u00e1","465b94d4":"## 2.3 Augumentation\nNh\u1eadn th\u1ea5y ch\u00fang ta c\u00f3 kh\u00e1 \u00edt d\u1eef li\u1ec7u, ...","15b5c371":"# RGB Analysis\nHistogram l\u00e0 m\u1ed9t bi\u1ec3u di\u1ec5n \u0111\u1ed3 h\u1ecda cho bi\u1ebft t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb m\u00e0u kh\u00e1c nhau trong h\u00ecnh \u1ea3nh. Trong kh\u00f4ng gian m\u00e0u RGB, c\u00e1c gi\u00e1 tr\u1ecb pixel n\u1eb1m trong kho\u1ea3ng t\u1eeb 0 \u0111\u1ebfn 255 trong \u0111\u00f3 0 l\u00e0 m\u00e0u \u0111en v\u00e0 255 l\u00e0 m\u00e0u tr\u1eafng. Ph\u00e2n t\u00edch bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 gi\u00fap ch\u00fang ta hi\u1ec3u \u0111\u01b0\u1ee3c ph\u00e2n b\u1ed1 \u0111\u1ed9 s\u00e1ng, \u0111\u1ed9 t\u01b0\u01a1ng ph\u1ea3n v\u00e0 c\u01b0\u1eddng \u0111\u1ed9 c\u1ee7a h\u00ecnh \u1ea3nh. B\u00e2y gi\u1edd ch\u00fang ta h\u00e3y xem bi\u1ec3u \u0111\u1ed3 c\u1ee7a m\u1ed9t m\u1eabu \u0111\u01b0\u1ee3c ch\u1ecdn ng\u1eabu nhi\u00ean t\u1eeb m\u1ed7i danh m\u1ee5c.","19463a1a":"## 2.4 Create custom Dataset","84b0ade1":"### 4.1 C\u00e1c bi\u1ec3u \u0111\u1ed3 c\u1ee7a m\u00f4 h\u00ecnh\n> 1. Loss\n> 2. Accuracy\n> 3. F1","d8018503":"## 3.2 Define new model","f955e6c4":"### 4.2 Train model","9123e67c":"## K\u1ebft Lu\u1eadn\nCh\u00fang t\u00f4i th\u1ea5y r\u1eb1ng \u1ea3nh c\u00f3 \u0111\u1ed9 ph\u00e2n gi\u1ea3i r\u1ea5t cao n\u00ean ch\u00fang t\u00f4i sau \u0111\u00f3 \u0111\u00e3 \u00e1p d\u1ee5ng resize l\u1ea1i \u1ea3nh \u0111\u1ec3 c\u1ea3i thi\u1ec7n th\u1eddi gian ch\u1ea1y.","e96f17fa":"### Quan s\u00e1t :\nC\u00e1c gi\u00e1 tr\u1ecb k\u00eanh m\u00e0u \u0111\u1ecf c\u00f3 v\u1ebb g\u1ea7n nh\u01b0 ph\u00e2n ph\u1ed1i chu\u1ea9n, nh\u01b0ng h\u01a1i l\u1ec7ch v\u1ec1 b\u00ean tr\u00e1i (\u0110\u1ed9 l\u1ec7ch \u00e2m). \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng k\u00eanh m\u00e0u \u0111\u1ecf c\u00f3 xu h\u01b0\u1edbng t\u1eadp trung nhi\u1ec1u h\u01a1n \u1edf c\u00e1c gi\u00e1 tr\u1ecb cao h\u01a1n, v\u00e0o kho\u1ea3ng 100. C\u00f3 s\u1ef1 thay \u0111\u1ed5i l\u1edbn v\u1ec1 gi\u00e1 tr\u1ecb m\u00e0u \u0111\u1ecf trung b\u00ecnh tr\u00ean c\u00e1c h\u00ecnh \u1ea3nh.","48f0b2c4":"### K\u1ebft lu\u1eadn\nPh\u00e2n t\u00edch k\u00eanh m\u00e0u th\u00ec th\u1ea5y k\u00eanh m\u00e0u xanh ch\u1ee7 y\u1ebfu c\u00f3 ph\u00e2n b\u1ed1 \u1edf v\u00f9ng c\u00f3 intensity m\u1ea1nh ( v\u00ec n\u00f3 t\u1eadp trung \u1edf ph\u1ea7n cu\u1ed1i \u0111\u1ed3 th\u1ecb) ==> D\u1ec5 hi\u1ec3u v\u00ec \u1ea3nh to\u00e0n l\u00e1. V\u00ed d\u1ee5 nh\u01b0 \u1ea3nh to\u00e0n l\u00e1 m\u00e0 th\u1ea5y k\u00eanh m\u00e0u \u0111\u1ecf l\u1ea1i m\u1ea1nh h\u01a1n th\u00ec s\u1ebd ph\u00e1t hi\u1ec7n b\u1ea5t th\u01b0\u1eddng. \u1ede \u0111\u00e2y k\u00eanh m\u00e0u \u0111\u1ecf l\u1ea1i ph\u00e2n b\u1ed1 \u1edf v\u00f9ng gi\u1eefa m\u1ea1nh h\u01a1n k\u00eanh m\u00e0u xanh d\u01b0\u01a1ng, ch\u1ee9ng t\u1ecf m\u00e0u \u0111\u1ecf n\u00f3 c\u0169ng c\u00f3 xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u00e0 do m\u1ed9t s\u1ed1 l\u00e1 c\u00e2y b\u1ecb s\u00e2u b\u00eanh hay c\u00f3 m\u00e0u \u0111\u1ecf","e2b6a904":"# N\u1ed9i dung\n\n* [<font size=4>EDA<\/font>](#1)\n    * [Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u, l\u1eadp bi\u1ec3u \u0111\u1ed3](#1.1)\n    * [M\u1ed9t s\u1ed1 \u1ea3nh v\u00ed d\u1ee5 t\u1eeb t\u1eadp d\u1eef li\u1ec7u](#1.2)\n    * [RGB Analysis](#1.3)\n    * [Parallel categories plot](#1.3)\n","41040c9c":"# 5. Make prediction","813ee369":"# 1.  Thi\u1ebft l\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c tham s\u1ed1 c\u1ed1 \u0111\u1ecbnh\nThi\u1ebft l\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c tham s\u1ed1 c\u1ed1 \u0111\u1ecbnh c\u00f3 trong b\u00e0i:\n1. *num_classes*: t\u1ed5ng s\u1ed1 l\u01b0\u1ee3ng nh\u00e3n.\n2. *img_size*: k\u00edch th\u01b0\u1edbc c\u1ee7a \u1ea3nh sau qu\u00e1 tr\u00ecnh resized b\u1edfi DataLoader.\n3. *batch_size*: k\u00edch th\u01b0\u1edbc m\u1ed7i batch.\n4. *device*: accelerator \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng.\n5. *criterion*: h\u00e0m m\u1ea5t m\u00e1t (loss function) \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng.","32f0946f":"Ch\u00fang t\u00f4i \u0111\u00e3 plot m\u1ed9t v\u00e0i h\u00ecnh \u1ea3nh trong training data \u1edf tr\u00ean (c\u00e1c gi\u00e1 tr\u1ecb RGB c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c nh\u00ecn th\u1ea5y b\u1eb1ng c\u00e1ch di chu\u1ed9t qua h\u00ecnh \u1ea3nh). C\u00e1c ph\u1ea7n m\u00e0u xanh l\u00e1 c\u00e2y c\u1ee7a h\u00ecnh \u1ea3nh c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam r\u1ea5t th\u1ea5p, nh\u01b0ng ng\u01b0\u1ee3c l\u1ea1i, c\u00e1c ph\u1ea7n m\u00e0u n\u00e2u c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam cao. \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng c\u00e1c ph\u1ea7n m\u00e0u xanh l\u00e1 c\u00e2y (healthy) c\u1ee7a h\u00ecnh \u1ea3nh c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam th\u1ea5p, trong khi c\u00e1c ph\u1ea7n unhealthy c\u00f3 nhi\u1ec1u kh\u1ea3 n\u0103ng c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam cao. **\u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 cho th\u1ea5y r\u1eb1ng k\u00eanh m\u00e0u xanh lam c\u00f3 th\u1ec3 l\u00e0 ch\u00eca kh\u00f3a \u0111\u1ec3 ph\u00e1t hi\u1ec7n b\u1ec7nh tr\u00ean c\u00e2y tr\u1ed3ng**","d0cc26a5":"### Quan s\u00e1t:\nK\u00eanh m\u00e0u xanh lam c\u00f3 s\u1ef1 ph\u00e2n b\u1ed1 \u0111\u1ed3ng \u0111\u1ec1u nh\u1ea5t trong s\u1ed1 ba k\u00eanh m\u00e0u, v\u1edbi \u0111\u1ed9 l\u1ec7ch t\u1ed1i thi\u1ec3u (l\u1ec7ch m\u1ed9t ch\u00fat sang tr\u00e1i). K\u00eanh m\u00e0u xanh lam cho th\u1ea5y s\u1ef1 thay \u0111\u1ed5i l\u1edbn gi\u1eefa c\u00e1c h\u00ecnh \u1ea3nh trong t\u1eadp d\u1eef li\u1ec7u.","0cea8ab2":"## T\u1ed5ng h\u1ee3p c\u00e1c k\u00eanh","240a2b35":"## 3.1 Load model if exists ","8f0e104d":"### 4.2 Save model","79fb3d38":"## 5.2 Create pridictor ","1a7ef259":"## K\u1ebft Lu\u1eadn\n\n- T\u1eadp d\u1eef li\u1ec7u kh\u00e1 kh\u00f4ng c\u00e2n b\u1eb1ng theo bi\u1ec3u \u0111\u1ed3 h\u00ecnh tr\u00f2n \u1edf tr\u00ean\n- Ch\u00fang t\u00f4i s\u1ebd ch\u1ecdn chi\u1ebfn l\u01b0\u1ee3c l\u1ea5y m\u1eabu th\u00edch h\u1ee3p \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y. Data augmentation \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 th\u00eam c\u00e1c m\u1eabu b\u1ed5 sung t\u1eeb c\u00e1c l\u1edbp thi\u1ec3u s\u1ed1. Trong  h\u00ecnh \u1ea3nh c\u1ee7a ch\u00fang t\u00f4i, \u0111i\u1ec1u n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c x\u1eed l\u00fd \u0111\u01b0\u1ee3c b\u1eb1ng c\u00e1ch th\u00eam \u0111\u1ed9 m\u00e9o v\u00e0o d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch th\u1ef1c hi\u1ec7n d\u1ecbch, xoay, thay \u0111\u1ed5i t\u1ef7 l\u1ec7 c\u0169ng nh\u01b0 b\u1eb1ng c\u00e1ch th\u00eam c\u00e1c lo\u1ea1i nhi\u1ec5u (\u00e1p d\u1ee5ng albumentation)"}}