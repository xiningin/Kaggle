{"cell_type":{"a8f3fbe0":"code","b74a01b9":"code","6b056185":"code","4da58f88":"code","7999156d":"code","0fb1b4b7":"code","2e114018":"code","8c2656d5":"code","d15d181f":"code","06c2490b":"code","203b8e1f":"code","88cd743b":"code","e001dd84":"code","37806dce":"code","ff2353fb":"code","da66ff02":"code","dd6d4402":"markdown","3c9cd588":"markdown","e1a451a4":"markdown","48fee5e6":"markdown","fc352e5f":"markdown","02e8ef4e":"markdown","ee5599c5":"markdown","fa0a3170":"markdown"},"source":{"a8f3fbe0":"import time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\n# seaborn and matplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# scipy (feature engineering)\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nimport lightgbm as lgb\nimport warnings\n# Configurations\nwarnings.simplefilter(action='ignore', category=UserWarning)\nRANDOM_SEED = 19\nnp.random.seed(RANDOM_SEED)\nsns.set()\n\ndef plot_multiclass(result):\n    # Plot multi_logloss and 1 - multi_error\n    num_rounds = len(result['multi_logloss-mean'])\n    fig, ax1 = plt.subplots(figsize=(10, 5))\n    fig.suptitle('logloss (blue) and accuracy (orange)', fontsize=14)\n    ax2 = ax1.twinx()\n    ax1.set_xlabel('boosting round')\n    ax1.set_ylabel('logloss')\n    ax2.set_ylabel('accuracy')\n    p1 = sns.lineplot(x=np.arange(num_rounds), y=result['multi_logloss-mean'],\n                      ax=ax1, color='blue')\n    multi_accuracy = [1 - v for v in result['multi_error-mean']]  # not sure if this is right\n    p2 = sns.lineplot(x=np.arange(num_rounds), y=multi_accuracy, ax=ax2, color='orange')","b74a01b9":"data_type = {'acoustic_data': np.int16, 'time_to_failure': np.float64}\ntrain = pd.read_csv('..\/input\/train.csv', dtype=data_type)","6b056185":"def extract_segment_features(frame, index, x):\n    frame.loc[index, 'std'] = x.values.std()\n    frame.loc[index, 'mean'] = x.values.mean()\n    frame.loc[index, 'max'] = x.values.max()\n    frame.loc[index, 'min'] = x.values.min()\n    frame.loc[index, 'std_abs'] = x.abs().std()\n    frame.loc[index, 'max_abs'] = x.abs().max()\n    frame.loc[index, 'mean_abs_change'] = np.mean(np.abs(x.diff()))\n    frame.loc[index, 'std_abs_change'] = np.std(np.abs(x.diff()))\n    \n    frame.loc[index, 'mad'] = x.mad()\n    frame.loc[index, 'iqr'] = stats.iqr(x.values)\n    frame.loc[index, 'kurt'] = x.kurtosis()\n    frame.loc[index, 'skew'] = x.skew()\n    frame.loc[index, 'q05'] = np.quantile(x, 0.05)\n    frame.loc[index, 'q95'] = np.quantile(x, 0.95)\n    \n    for windows in [16, 64, 512, 4096]:\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        x_roll_std = x.rolling(windows).std().dropna().values\n        frame.loc[index, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)   \n        frame.loc[index, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        frame.loc[index, 'mean_roll_std_' + str(windows)] = np.mean(x_roll_mean)\n        frame.loc[index, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n    return frame\n\n\ndef make_train(train_data, size=150000, skip=150000):\n    num_segments = int(np.floor((train_data.shape[0] - size) \/ skip)) + 1\n    # We will be removing segments that belongs to two quakes\n    num_segments -= 16\n\n    X_train = pd.DataFrame(index=range(num_segments), dtype=np.float64)\n    y_train = pd.DataFrame(index=range(num_segments), columns=['quake_number'])\n    quake_count = 0\n    \n    for index in tqdm_notebook(range(num_segments)):\n        seg = train_data.iloc[index*skip:index*skip + size]\n        \n        if any(seg.time_to_failure.diff() > 5):\n            quake_count += 1\n            continue\n        \n        y_train.loc[index, 'quake_number'] = quake_count\n        y_train.loc[index, 'time_to_failure'] = seg.time_to_failure.values[-1]\n        X_train = extract_segment_features(X_train, index, seg.acoustic_data)\n    return X_train, y_train","4da58f88":"X_tr, y_tr = make_train(train)\nX_tr.head()","7999156d":"plt.figure(figsize=(10, 5))\nplt.title(\"Correlation heatmap for features\")\nax = sns.heatmap(X_tr.corr(), annot=False, linewidths=.3, cmap=\"YlGnBu\")\nplt.figure(figsize=(10, 5))\nplt.title(\"Count of segments for each earthquake (y_tr)\")\nax = sns.countplot(x=\"quake_number\", data=y_tr, palette='GnBu_d')","0fb1b4b7":"keep_idx = y_tr[(y_tr.quake_number > 0) & (y_tr.quake_number < 16)].index\nX_tr, y_tr = X_tr.iloc[keep_idx], y_tr.iloc[keep_idx]\ny_tr.quake_number = y_tr.quake_number - 1  # start counting at 0\nplt.figure(figsize=(10, 5))\nplt.title(\"New count (y_tr)\")\nax = sns.countplot(x=\"quake_number\", data=y_tr, palette='GnBu_d')","2e114018":"params = {\n    'objective': 'multiclass',  # Softmax\n    'metric': ['multi_logloss', 'multi_error'],\n    'num_class': 15,\n    \"boosting\": \"gbdt\",\n    'num_leaves': 32,\n    'min_data_in_leaf': 10, \n    'max_depth': -1,\n    'learning_rate': 0.01,\n    \"feature_fraction\": 1,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.9,\n    \"bagging_seed\": 19,\n    \"lambda_l2\": 0.1,\n    \"num_boost_round\": 90000,\n    \"verbosity\": -1,\n    \"nthread\": -1,\n}\ndataset = lgb.Dataset(X_tr, label=y_tr.quake_number)\nresult = lgb.cv(params, dataset, nfold=10, early_stopping_rounds=100, stratified=False)\nplot_multiclass(result)","8c2656d5":"params['num_boost_round'] = len(result['multi_error-mean'])\nbst = lgb.train(params, dataset)\ns = pd.DataFrame({'feature': X_tr.columns,\n                  'gain': bst.feature_importance(importance_type='gain')})\ns.sort_values(by='gain', ascending=False).head()","d15d181f":"params['num_boost_round'] = 99999\nparams['num_class'] = 3\nidx = y_tr[(y_tr.quake_number >= 0) & (y_tr.quake_number < 3)].index\ndataset = lgb.Dataset(X_tr.iloc[idx], label=y_tr.loc[idx, 'quake_number'])\nresult = lgb.cv(params, dataset, nfold=10, early_stopping_rounds=100, stratified=False)\nplot_multiclass(result)","06c2490b":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    \"boosting\": \"gbdt\",\n    'num_leaves': 22,\n    'min_data_in_leaf': 10, \n    'max_depth': -1,\n    'learning_rate': 0.01,\n    \"feature_fraction\": 1,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.9,\n    \"bagging_seed\": 19,\n    \"lambda_l2\": 0.05,\n    \"num_boost_round\": 90000,\n    \"verbosity\": -1,\n    \"nthread\": -1,\n}\n# Predict if a segment came from group A or B\ndef binary_prediction(q1, q2):\n    assert(q1 < q2)\n    idx = y_tr[(y_tr.quake_number == q1) | (y_tr.quake_number == q2)].index\n    binary_target = y_tr.loc[idx, 'quake_number'] > q1\n    dataset = lgb.Dataset(X_tr.iloc[idx], label=binary_target)\n    result = lgb.cv(params, dataset, nfold=10, early_stopping_rounds=100, stratified=True)\n    num_rounds = len(result['auc-mean'])\n\n    plt.figure(figsize=(10, 5))\n    plt.title(\"AUC - earthquake {} vs {}\".format(q1, q2))\n    ax = sns.lineplot(x=np.arange(num_rounds), y=result['auc-mean'])","203b8e1f":"binary_prediction(2, 3)","88cd743b":"binary_prediction(2, 11)","e001dd84":"binary_prediction(1, 6)","37806dce":"binary_target = (y_tr.time_to_failure > 12).astype('int8')\nbinary_target.value_counts()","ff2353fb":"f = X_tr.columns\ndataset = lgb.Dataset(X_tr[f], label=binary_target)\nresult = lgb.cv(params, dataset, nfold=10, early_stopping_rounds=100,\n                stratified=False, shuffle=True)\nnum_rounds = len(result['auc-mean'])\n\nplt.figure(figsize=(10, 5))\nplt.title(\"AUC - predicting TTF > 12\")\nax = sns.lineplot(x=np.arange(num_rounds), y=result['auc-mean'])","da66ff02":"f = ['q05_roll_std_64', 'q95_roll_mean_64']\ndataset = lgb.Dataset(X_tr[f], label=binary_target)\nresult = lgb.cv(params, dataset, nfold=10, early_stopping_rounds=100,\n                stratified=False, shuffle=True)\nnum_rounds = len(result['auc-mean'])\n\nplt.figure(figsize=(10, 5))\nplt.title(\"AUC - predicting TTF > 12\")\nax = sns.lineplot(x=np.arange(num_rounds), y=result['auc-mean'])","dd6d4402":"<h2>Predict TTF > 12<\/h2>\n\nMost of the error is coming from long earthquake cycles, we can try a classifier to distinguish between long and short cycles.","3c9cd588":"Trying the first 3 earthquakes only:","e1a451a4":"<h2>Predict earthquake (multiclass model)<\/h2>\n\nUsing a classifier with 15 possible classes:","48fee5e6":"<h2>Features<\/h2>\n\nI'm using a feature set similar to lukyanenko's kernel. Segments that belongs to two quakes are removed, so we have 4194 - 16 = 4178 data points.","fc352e5f":"<h2>Overview<\/h2>\n\nThere are quite a few discussions about the best validation method in LANL competition. The main argument against KFold (shuffle) is that segments from the same earthquake in train and validation sets could leak information about the later. This doesn't happen in the test set, since earthquakes are totally different from the training data.\n\nTo check this argument, I am trying to predict which earthquake a segment came from. I am not sure if this is the correct approach, so let me know your ideas about this experiment.","02e8ef4e":"<h2>Binary prediction<\/h2>\n\nWe can also try a binary classification model with only two earthquakes. In this case, the results are very different depending on the quakes we are comparing:","ee5599c5":"Let's also remove the first and last group; so we have 15 groups.","fa0a3170":"With only two features:"}}