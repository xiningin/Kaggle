{"cell_type":{"4a3524e5":"code","2431d4e0":"code","7a34f794":"code","1e3ffc88":"code","be4752ae":"code","e6e60d44":"code","90f40908":"code","04827bdb":"code","9f6e751f":"code","cf3647f8":"code","6c7b2f38":"code","6e69cfc6":"code","5094462c":"code","207393e2":"code","16105218":"code","12f1e132":"code","47b057fc":"code","f20381bd":"code","eeaf756d":"code","e1c61fd0":"code","c5666f96":"code","722d2d1c":"code","113e0480":"code","05cac0cc":"code","0fe3279b":"code","770568d4":"code","a5a5da63":"code","98ab9c2c":"code","caf3cb86":"code","63bd1837":"code","49466104":"code","016fbda1":"code","1ca3505e":"code","5399f068":"code","325c4f87":"code","de7b7875":"code","51d88b86":"code","036d1a57":"code","63a8f64c":"code","232a772d":"code","1956d85a":"code","68f1438f":"code","b626ed04":"code","ae52886f":"code","09fae6a8":"code","21e1ddde":"code","59744150":"code","f2c130fb":"code","04987f68":"code","34113bde":"code","7787492d":"code","dfe3a3d3":"code","aedc8844":"code","7ef7382b":"code","024ca019":"markdown","c47b5a80":"markdown","a2b3fb1d":"markdown","5da03045":"markdown","b7ab5575":"markdown","e224b0e1":"markdown","b10de9da":"markdown","beb0a2fe":"markdown","fe7b2fec":"markdown","dddda681":"markdown","87231abe":"markdown"},"source":{"4a3524e5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","2431d4e0":"df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')","7a34f794":"df.head()","1e3ffc88":"df.describe()","be4752ae":"df.drop('id',axis = 1,inplace = True)","e6e60d44":"df.head()","90f40908":"df.isnull().sum()","04827bdb":"sns.set_style('whitegrid')\nsns.FacetGrid(df,height=12,hue='target').map(sns.scatterplot,'f0','f2')","9f6e751f":"len(df)","cf3647f8":"y_temp = df.target\nX_temp = df.iloc[:,:-1]","6c7b2f38":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_temp = sc.fit_transform(X_temp)","6e69cfc6":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()","5094462c":"model.fit(X_temp[:10000],y_temp[:10000])","207393e2":"importance = model.coef_[0]\nindex = []\nfor i,v in enumerate(importance):\n    if v < 0:\n        index.append(i)\n    print('Feature: %0d, Score: %.5f' % (i,v))","16105218":"plt.figure(figsize = (20,8))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","12f1e132":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_temp[:10000],y_temp[:10000])\n\nimportance = model.feature_importances_\nvalue = []\n\nfor i,v in enumerate(importance):\n    value.append(v)\n    print('Feature: %0d, Score: %.5f' % (i,v))\n    \nplt.figure(figsize = (20,8))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","47b057fc":"np.mean(value)","f20381bd":"main_f = []\nfor i,v in enumerate(importance):\n    if v > 0.010000000000000002:\n        main_f.append(i)","eeaf756d":"df.columns","e1c61fd0":"for i in range(len(index)):\n    index[i] = 'f'+str(index[i])","c5666f96":"for i in range(len(main_f)):\n    main_f[i] = 'f'+str(main_f[i])","722d2d1c":"main_f","113e0480":"df_new = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')","05cac0cc":"common = []\nfor i in range(len(main_f)):\n    if main_f[i] in index:\n        common.append(main_f[i])\nuncommon = []\nfor i in range(len(index)):\n    if not(index[i] in common):\n        uncommon.append(index[i])\n        ","0fe3279b":"len(uncommon + common)","770568d4":"uncommon + common","a5a5da63":"df_new.drop(uncommon,axis=1,inplace =True)","98ab9c2c":"df_new.drop(common,axis = 1,inplace  = True)\ndf_new.head()","caf3cb86":"df_new.drop('id',axis = 1,inplace = True)","63bd1837":"df_new.head(1)","49466104":"y = df_new.target\nX = df_new.iloc[:,:-1]","016fbda1":"X","1ca3505e":"X = sc.fit_transform(X)","5399f068":"x_train,x_val,y_train,y_val = train_test_split(X,y,test_size = 0.025,random_state = 0)","325c4f87":"x_train.shape,y_train.shape","de7b7875":"x_val.shape,y_val.shape","51d88b86":"x_input = tf.keras.layers.Input(shape=(44))\nx1 = tf.keras.layers.Dense(384, activation='swish')(x_input)\nx1 = tf.keras.layers.BatchNormalization()(x1)\nx2 = tf.keras.layers.Dropout(0.45)(x1)\n\nx2 = tf.keras.layers.Dense(192, activation='swish')(x2)\nx2 = tf.keras.layers.BatchNormalization()(x2)\nx3 = tf.keras.layers.Dropout(0.35)(x2)\n\nx3 = tf.keras.layers.Dense(96, activation='swish')(x3)\nx3 = tf.keras.layers.BatchNormalization()(x3)\nx3 = tf.keras.layers.Dropout(0.25)(x3)\n\nx4 = tf.keras.layers.Dense(192, activation='swish')(x3)\nx4 = tf.keras.layers.BatchNormalization()(x4)\nx4 = tf.keras.layers.Multiply()([x2, x4])\nx4 = tf.keras.layers.Dropout(0.35)(x4)\n\nx5 = tf.keras.layers.Dense(384, activation='swish')(x4)\nx5 = tf.keras.layers.BatchNormalization()(x5)\nx5 = tf.keras.layers.Multiply()([x1, x5])\nx5 = tf.keras.layers.Dropout(0.45)(x5)\n\nx = tf.keras.layers.Concatenate()([x3, x5])\nx = tf.keras.layers.Dense(128, activation='swish')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.25)(x)\n\nx_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\nmodel_nn = tf.keras.Model(inputs = x_input,outputs = x_output)","036d1a57":"model_nn.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-2), loss=\"binary_crossentropy\", metrics=['AUC'])\n\n\nlr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=4)\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15,mode=\"min\", restore_best_weights=True)\n   ","63a8f64c":"BS = 128\nSPE = len(df)\/\/BS","232a772d":"class CallKar(tf.keras.callbacks.Callback):\n    def on_epoch_end(self,epochs,logs = {}):\n        if logs.get('loss') < 0.3:\n            self.model.stop_training = True\ncl = CallKar()","1956d85a":"hist = model_nn.fit(x_train,y_train,\n                    batch_size=BS,epochs=50,\n                    callbacks=[cl,es,lr],\n                    validation_data=(x_val,y_val),\n                    steps_per_epoch=SPE,shuffle=True)","68f1438f":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.show()","b626ed04":"plt.plot(hist.history['auc'])\nplt.plot(hist.history['val_auc'])\nplt.show()","ae52886f":"df_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","09fae6a8":"df_test.shape\nid = df_test.id\ndf_test.drop('id',axis = 1,inplace = True)","21e1ddde":"df_test.head()","59744150":"df_test.drop(uncommon,axis=1,inplace =True)\ndf_test.drop(common,axis = 1,inplace  = True)","f2c130fb":"df_test.shape","04987f68":"X_test = sc.fit_transform(df_test)","34113bde":"X_test","7787492d":"predict = model_nn.predict(X_test)","dfe3a3d3":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","aedc8844":"submission['target'] = predict","7ef7382b":"submission.to_csv('submission.csv',index=False)","024ca019":"# These Features are with more effect on data.","c47b5a80":"# Check The Importance Of features.","a2b3fb1d":"> *the values with negative have less impact on the data*","5da03045":"# After taking major features our dataset of 100 features comes to 45","b7ab5575":"### 45 features have major effect on dataset","e224b0e1":"we use here swish activation function which is *f(x) = x \u00b7 sigmoid(x)*","b10de9da":"### some features are common so let's take the unique values","beb0a2fe":"# Feature Selection","fe7b2fec":"# Model Implementation","dddda681":"## These are the values greater than mean among dataset","87231abe":"# We take the features which have mean greater than mean from all values."}}