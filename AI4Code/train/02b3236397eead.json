{"cell_type":{"c48dc7f5":"code","ff63822b":"code","4836fef9":"code","cd74f065":"code","46a744a3":"code","68afea6b":"code","bc3c5ea0":"code","183d3b59":"code","dd0834e5":"code","024c17f2":"code","e5cc2b84":"code","c083bb71":"code","c0aa4468":"code","5cd58a8b":"code","312c7ca1":"code","2627353c":"markdown","4a47fd4c":"markdown","32bcc98b":"markdown","fe9dfb01":"markdown","f84ba9ae":"markdown","23c74c7b":"markdown","63db8c21":"markdown"},"source":{"c48dc7f5":"!pip install livelossplot #\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u53ef\u89c6\u5316\u5de5\u5177","ff63822b":"import os #\u8f93\u5165\u8f93\u51fa\u5de5\u5177\uff0c\u62fc\u63a5\u8def\u5f84\nimport cv2 #\u7ed8\u56fe\u5de5\u5177\uff08\u753b\u98de\u673a\u7684\u7ebf\uff09\nimport ast #\u8bfb\u6587\u4ef6\nimport numpy as np #\u6700\u57fa\u7840\u7684python\u5305\uff0c\u6570\u636e\u7ed3\u6784\uff08list\uff0carray...\uff09\nimport pandas as pd #\u6570\u636e\u5904\u7406\u5de5\u5177\uff08\u5904\u7406csv\u683c\u5f0f\u7684\u6587\u4ef6\uff09\nimport matplotlib.pyplot as plt #\u7ed8\u56fe\u5de5\u5177\uff08\u753b\u5f88\u591a\u98de\u673a\uff09\nfrom tqdm import tqdm_notebook\nfrom glob import glob\nfrom keras.utils import to_categorical #\u8bfb\u7c7b\u522blabel\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy #\u8bc4\u4ef7\u6307\u6807\nfrom keras.metrics import categorical_crossentropy #\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\nfrom keras.models import Sequential #\u5e8f\u5217\u6a21\u578b\uff08\u7f51\u7edc\u4e00\u5c42\u4e00\u5c42\uff09\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint #\u56de\u8c03\u51fd\u6570\uff08\u8c03\u6574\u6a21\u578b\u53c2\u6570\uff0c\u4fdd\u5b58\u6700\u4f18\u6a21\u578b\uff09\u3002reducelr\uff0c\u52a8\u6001\u51cf\u5c0f\u5b66\u4e60\u7387\n#from keras_tqdm import TQDMNotebookCallback\nfrom livelossplot import PlotLossesKeras #\u4e3akeras\u7ed8\u5236loss\u56fe\u50cf\u7684\u5305\nfrom keras.optimizers import Adam #\u4e00\u79cd\u5f15\u5165\u52a8\u91cf\u7684\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\uff0cAdam\uff0c\uff08SGD\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff09\u4e3a\u4e86\u8c03\u6574\u6a21\u578b\u53c2\u6570\u3002\n\n#\u4e0b\u4e00\u65f6\u523b\u7684\u53c2\u6570 = \u4e0a\u4e00\u65f6\u523b\u7684\u53c2\u6570 + \uff08\u5b66\u4e60\u7387 * \u68af\u5ea6\uff09 \u5b66\u4e60\u7387\uff1a\u8c03\u6574\u68af\u5ea6\u8de8\u5ea6\u7684\u5927\u5c0f\u3002\n\nfrom keras.preprocessing import image #\u9884\u5904\u7406\u56fe\u50cf\u7684\uff0cimg\u5b58\u50a8\u77e9\u9635\u6570\u636e\nfrom keras.models import Model #keras\u7684\u57fa\u7840\u6a21\u578b\u3002keras\u662f\u4e00\u4e2a\u9876\u5c42\u5305\u3002 \u6c47\u7f16\u8bed\u8a00-\uff08C\u8bed\u8a00\uff09-python-tensorflow-keras\nfrom keras import backend as K\n%matplotlib inline #\u80fd\u663e\u793a\u4f60\u753b\u7684\u56fe\n\nBASE_SIZE = 256 #\u56fe\u50cf\u77e9\u9635\nDP_DIR = '..\/input\/inputdata\/input\/shuffle-csvs\/'\nINPUT_DIR = '..\/input\/inputdata\/input\/quickdraw-doodle-recognition\/'\nNCSVS = 100 #\u9a8c\u8bc1\u96c6\u6837\u672c\u6570\nNCATS = 340 #\u603b\u7c7b\u522b\u6570\n\n# \u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u5c06\u7b14\u5212\u5750\u6807\u6570\u636e\u8f6c\u6362\u4e3a\u56fe\u50cf\u77e9\u9635\u6570\u636e\u3002\u6240\u6709\u56fe\u50cf\u4e00\u5b9a\u8981\u8f6c\u6362\u6210\u77e9\u9635\u6570\u636e\u3002\ndef draw_cv2(raw_strokes, size=256, lw=6):\n    #np.zeros((a,b)) \u751f\u6210\u4e00\u4e2aa\u884cb\u5217\u7684\u77e9\u9635\uff0cnp.uint8\u662fint\u7684\u4e00\u79cd\uff0c8\u4f4d\u7684\u65e0\u7b26\u53f7\u6574\u5f62\u3002\n    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n    for stroke in raw_strokes: #\u4ece3\u7ef4\u52302\u7ef4\u4e86\n        for i in range(len(stroke[0]) - 1): #\u4ece\u7b2c\u4e00\u7b14\u5f00\u59cb #len(a)\u6c42\u5217\u8868\u6216\u8005\u6570\u7ec4\u7684\u957f\u5ea6\n            #cv2.line(img, start, end, color, thickness) cv2\u662f\u4e00\u4e2a\u7ed8\u56fe\u5e93\uff0ccv2.line()\u662f\u753b\u7ebf\u51fd\u6570\n            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), (255,0,0), lw)\n    if size != BASE_SIZE: #\u907f\u514d\u51fa\u9519\n        return cv2.resize(img, (size, size))\n    else:\n        return img\n    \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4836fef9":"fig = plt.figure(figsize=(20, 11)) #\u5236\u4f5c\u4e00\u4e2a\u56fe\u50cf\uff0c\u957f20\u82f1\u5bf8\u5bbd11\u82f1\u5bf8\u3002#\u753b\u5e03\ncolumns = 6 #\u5b50\u56fe\u5217\u6570\nrows = 3 #\u5b50\u56fe\u884c\u6570\nsize = 256 #\u56fe\u50cf\u77e9\u9635\u7684\u884c\u5217\u6570 CNN\u5b9e\u8d28\u4e0a\u662f\u5728\u5904\u7406size\u6240\u5728\u7684\u6570\u636e \u56fe\u50cf\u7684\u672c\u8d28\u662f\u77e9\u9635\nfilename = os.path.join(INPUT_DIR, 'train_simplified\/airplane.csv')\n\n#pd.sample(n) \u968f\u673a\u9009\u62e9n\u4e2a\u6837\u672c\uff0cn\u884c\u6570\u636e\ndf = pd.read_csv(filename).sample(columns*rows)\n\n#dataframe\u5c06csv\u8bfb\u5165python\u7684\u4e00\u79cd\u6570\u636e\u683c\u5f0f\n#df.apply(func) \u5bf9dataframe\u4e2d\u7684\u6bcf\u4e00\u884c\u6267\u884c\u81ea\u5b9a\u4e49\u51fd\u6570func\uff0c\u8fd9\u91cc\u7684ast.literal_eval\u8d1f\u8d23\u8bfb\u5165dataframe\u4e2d\u7684\u6570\u636e\ndf['drawing'] = df['drawing'].apply(ast.literal_eval) #\u786e\u4fdd\u8fed\u4ee3\u6027\n\n#\u521b\u5efa\u4e09\u7ef4\u6570\u7ec4x\nx = np.zeros((len(df), size, size))\nfor i, raw_strokes in enumerate(df.drawing.values): \n    #enumerate(x)\uff1a\u7ed9\u6570\u636e\u52a0\u7d22\u5f15\uff0c\u4f7f\u5176\u53ef\u4ee5\u904d\u5386\u3002 \u4f8b\u5982enumerate(['dog','cat','human']) -> [(1,'dog'),(2,'cat'),(3,'human')]\n    x[i] = draw_cv2(raw_strokes, size=size, lw=6)\nx = x \/ 255. #0~255 \u5f52\u4e00\u5316\nx = x.reshape((len(df), size, size)).astype(np.float32) #\uff08\u7b2c\u4e00\u7ef4\uff0c\u7b2c\u4e8c\u7ef4\uff0c\u7b2c\u4e09\u7ef4\uff09\ny = np.array(df['word']) #\u5c06\u7c7b\u522b\u6570\u636e\u8bfb\u5165\uff0c\u5e76\u5b58\u5728\u6570\u7ec4y\u91cc\n\nfor i in range(columns*rows):\n    fig.add_subplot(rows,columns,i+1) #\u521b\u5efa3X6\u5b50\u56fe\u4e2d\u7684\u7b2ci+1\u4e2a\n    plt.imshow(x[i,]) #\u5c06\u7b2ci\u4e2a\u56fe\u50cf\u753b\u51fa\u6765\uff0c[i\uff0c]\u540e\u9762\u7684\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u753b\u51fa\u6765\n    plt.title(y[i]) #\u7b2ci\u4e2a\u56fe\u50cf\u6807\u9898\u662fdf['word']\nplt.show() #\u5c55\u793a\u6240\u6709\u56fe\u50cf","cd74f065":"# \u83b7\u53d6\u7c7b\u522b\u540d\u79f0\u5e76\u5bf9\u7a7a\u683c\u8fdb\u884c\u66ff\u6362\nclasses_path = os.listdir(INPUT_DIR + 'train_simplified\/')\nclasses_path = sorted(classes_path, key=lambda s: s.lower())\nclass_dict = {x[:-4].replace(' ', '_'):i for i, x in enumerate(classes_path)} #enumerate(x)\uff1a\u7ed9\u6570\u636e\u52a0\u7d22\u5f15\uff0c\u4f7f\u5176\u53ef\u4ee5\u904d\u5386\u3002\nlabels = {x[:-4].replace(' ', '_') for i, x in enumerate(classes_path)}\n\nn_labels = len(labels)\nprint(\"Number of classes: {}\".format(n_labels))   \n\nn_files = n_labels # number of csv files same as labels due to the structure.","46a744a3":"class Image_Generator: #\u751f\u6210\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e \u767d\u677f\u6f14\u793a\n    \n    def __init__(self, size, batchsize, ks, lw=6, ncsvs=NCSVS):\n        self.size = size #\u56fe\u50cf\u5c3a\u5bf8\n        self.bs = batchsize #Batch\u5c3a\u5bf8\n        self.ks = ks #\u8bad\u7ec3\u96c6\u6240\u7528\u7684100\u4e2a\u6570\u636e\u96c6\u7d22\u5f15 train_k{0-99}  ks ncsvs\n        self.lw = lw #\u753b\u7ebf\u7684\u7c97\u7ec6\n        self.ncsvs = ncsvs #\u9a8c\u8bc1\u96c6\u6240\u7528\u7684100\u4e2a\u6570\u636e\u96c6\u7d22\u5f15\n    \n    def create_train(self): #\u521b\u5efa\u8bad\u7ec3\u96c6\n        while True: #\u6ca1\u6709\u6761\u4ef6\u7684\u5faa\u73af\n            for k in np.random.permutation(self.ks): #\u968f\u673a\u4ece100\u4e2atrain_k.csv\u6587\u4ef6\u4e2d\u9009\u62e9\n                filename = os.path.join(DP_DIR, 'train_k{}.csv'.format(k))\n                #filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n                for df in pd.read_csv(filename, chunksize=self.bs): #\u4ececsv\u968f\u673a\u8bfb\u5165batch_size\u6570\u636e\n                    x, y = Image_Generator.df_to_image_array(self, df)\n                    yield x, y #\u7b49\u4e8ereturn\n    \n    def create_valid(self): #\u521b\u5efa\u9a8c\u8bc1\u96c6\n        val_mark = Image_Generator.valid_mark(self)\n        while True:\n             #filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(val_mark))\n            filename = os.path.join(DP_DIR, 'train_k{}.csv'.format(val_mark))\n            for df in pd.read_csv(filename, chunksize=self.bs):\n                x, y = Image_Generator.df_to_image_array(self, df)\n                yield x, y  #\u7b49\u4e8ereturn  \n    \n    def df_to_image_array(self, df):\n        df['drawing'] = df['drawing'].apply(ast.literal_eval) #\u4ecedataframe\u4e2d\u83b7\u53d6\u5750\u6807\u6570\u636e\n        x = np.zeros((len(df), self.size, self.size))\n        for i, raw_strokes in enumerate(df.drawing.values):\n            x[i] = draw_cv2(raw_strokes, size=self.size, lw=self.lw) #\u5c06\u7b14\u5212\u5750\u6807\u8f6c\u5316\u4e3a\u77e9\u9635\n        x = x \/ 255.\n        x = x.reshape((len(df), self.size, self.size, 1)).astype(np.float32)  #uint8              \n        y = to_categorical(df.y, num_classes=NCATS) #NCATS=340\n        return x, y #x\u77e9\u9635\uff08\u56fe\u50cf\uff09\u6570\u636e\uff0cy\u662f\u6807\u7b7e\u6216\u8005\u53eb\u7c7b\u522b\u540d\n    \n    def valid_mark(self): #\u9a8c\u8bc1\u96c6\u4e0d\u80fd\u548c\u8bad\u7ec3\u96c6\u91cd\u590d\n        for i in range(self.ncsvs):\n            if i not in self.ks: \n                return i","68afea6b":"size = 256\nbatchsize = 64\ntrain_datagen = Image_Generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1)).create_train()","bc3c5ea0":"x, y = next(train_datagen) #\u4ece\u4e00\u4e2abatch\u7684\u8bad\u7ec3\u6837\u672c\u4e2d\u968f\u673a\u9009\u51fa18\u4e2a\u3002\nfig = plt.figure(figsize=(20, 11))\nfor i in range(columns*rows):\n    fig.add_subplot(rows,columns,i+1)\n    plt.imshow(x[i,].reshape(size, size))\nplt.show()","183d3b59":"size = 80\nbatchsize = 512 \n#NCSVS \u9a8c\u8bc1\u96c6\u7684\u7d22\u5f15\u6570\n#\u751f\u6210\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\ntrain_datagen = Image_Generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1)).create_train() #create_train\u662f\u7c7bImage_Generator\u91cc\u9762\u7684\u65b9\u6cd5\nvalid_datagen = Image_Generator(size=size, batchsize=batchsize, ks=range(NCSVS - 1)).create_valid() #\n","dd0834e5":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D #\u5377\u79ef\u5c42\uff0c\u6700\u5927\u6c60\u5316\u5c42\uff0c\u5e73\u5747\u6c60\u5316\u5c42\nfrom keras.layers import Dense, Dropout, Flatten, Activation #\u5168\u8fde\u63a5\u5c42\uff0cDropout\u5c42\uff08\u968f\u673a\u4e22\u5f03\u4e00\u4e9b\u795e\u7ecf\u5143\uff0c\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\uff09\uff0c\u504f\u5e73\u5c42\uff08\u5c06\u6570\u636e\u53d8\u4e3a\u4e00\u7ef4\uff09\uff0c\u6fc0\u6d3b\u5c42\nfrom keras.applications import MobileNet #\u4e00\u79cdCNN\uff0cMobileNet\u90e8\u7f72\u5728\u624b\u673a\u4e0a\u7684\uff0c\u53c2\u6570\u89c4\u6a21\u5c0f400w\n\n#input_shape=(\u7b2c\u4e00\u7ef4\uff0c\u7b2c\u4e8c\u7ef4\uff0c\u7b2c\u4e09\u7ef4)\n#input_shape=(2,2,1) \u4f8b\u5b50\uff1a[[[a,b],[c,d]]]\n#input_shape=(3,2,1) \u4f8b\u5b50\uff1a[[[a,b,e],[c,d,f]]]\n#input_shape=(2,3,1) \u4f8b\u5b50\uff1a[[[a,b],[c,d],[e,f]]]\n#include_top\uff1a\u662f\u5426\u5305\u62ec\u9876\u90e83\u4e2aDense\u5c42\u3002 weights\uff1a\u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002\n#\u53ea\u8981size\u8bbe\u7f6e\u7684\u548c\u6b64\u9879\u76ee\u4e00\u6837\uff0c\u5c31\u53ef\u4ee5\u5957\u7528\nbase_model = MobileNet(input_shape=(size, size, 1), include_top=False, weights=None, classes=n_labels) #\u5c01\u88c5\u597d\u7684\u6a21\u578b\uff0c\u662f\u4e00\u4e2a\u9ad8\u7ea7\u7684CNN\uff0c\u7279\u70b9\u5c31\u662f\u53c2\u6570\u5c11\uff0c\u6027\u80fd\u597d\u3002\n#\u56e0\u4e3a\u6211\u4eec\u5b66\u6821\u7684\u8bbe\u5907\u6709\u9650\uff0c\u6ca1\u6709\u5f88\u597d\u7684GPU\uff0c\u53ea\u67091080Ti\uff0c\u6240\u4ee5\u6211\u7528\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u7684CNN\u7f51\u7edc\u3002\n\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x) #\u4ee5x\u4e3a\u6b64\u5c42\u7684\u8f93\u5165\nx = Dense(1024, activation='relu')(x) #Dense(node) 512 or 1024\npredictions = Dense(n_labels, activation='softmax')(x) #n_labels = 340\uff0c softmax\u5c31\u662f\u5f52\u4e00\u5316\u6982\u7387[0.2, 0.7, 0.3]. \n\nmodel = Model(inputs=base_model.input, outputs=predictions) #base_model\u5bf9\u8c61\u7ee7\u627f\u4e8eMobilNet\uff0c\u6240\u4ee5MobilNet\u7684\u5c5e\u6027\u4ed6\u90fd\u6709\uff0c\u5176\u4e2d\u5c31\u5305\u62ec.input\u548c.output\nmodel.summary()","024c17f2":"from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3) # top-3\n\nmodel.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy',\n              metrics=[categorical_accuracy, top_3_accuracy]) #\u6a21\u578b\u7f16\u8bd1\uff0c\u8bbe\u7f6e\u4f18\u5316\u5668\uff0c\u8bbe\u7f6e\u635f\u5bb3\u51fd\u6570\uff0c\u8bbe\u7f6e\u8bc4\u4ef7\u6307\u6807","e5cc2b84":"model_name = '\/kaggle\/working\/best_model.h5' #\u4fdd\u5b58\u6a21\u578b\u7684\u8def\u5f84\uff0c\u6a21\u578b\u7684\u5b58\u50a8\u90fd\u662f.h5\u6587\u4ef6\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=2),  #\u662f\u5728\u8fde\u7eed\u51e0\u4e2aepoch\u4e4b\u540e\uff0c\u6a21\u578b\u4ecd\u7136\u4e0d\u6536\u655b\u3002factor\u662f\u8870\u51cf\u7cfb\u6570\uff0cfactor*\u5b66\u4e60\u7387\uff0c\u6700\u5c0f\u5c0f\u52301e-6\n             EarlyStopping(monitor='val_loss', patience=9, verbose=2), #verbose\u662f\u53ef\u89c6\u5316\uff0c0\u662f\u4e0d\u663e\u793a\u4efb\u4f55\u4fe1\u606f\uff0c1\u662f\u6bcf\u4e2aiteration\u90fd\u663e\u793a\uff0c2\u662f\u6bcf\u4e2aepoch\u663e\u793a\u4e00\u6b21\n             ModelCheckpoint(model_name, save_best_only=True, save_weights_only=True), # save best model\n             PlotLossesKeras()] #callbacks\u91cc\u7684\u51fd\u6570\u662f\u81ea\u5e26\u7684\uff0c\u91cc\u9762\u7684\u6570\u636e\u53ef\u4ee5\u6539","c083bb71":"history = model.fit_generator(train_datagen, steps_per_epoch=1000, epochs=25, verbose=1, \n                              validation_data=valid_datagen, validation_steps=200, callbacks=callbacks) #fit\u662f\u62df\u5408\u6a21\u578b\uff0cepochs\uff1a\u628a\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\u591a\u5c11\u6b21\uff0c\u81ea\u5df1\u8c03\u3002","c0aa4468":"def gen_graph(history, title):#hi\n    plt.plot(history.history['categorical_accuracy'])\n    plt.plot(history.history['val_categorical_accuracy'])\n    plt.plot(history.history['top_3_accuracy'])\n    plt.plot(history.history['val_top_3_accuracy'])\n    plt.title('Accuracy ' + title)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation', 'Test top 3', 'Validation top 3'], loc='upper left')\n    plt.show()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss ' + title)\n    plt.ylabel('MLogLoss')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n\ngen_graph(history, \"Your Name\")","5cd58a8b":"def df_to_image_array(df, size, lw=6):\n    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n    x = np.zeros((len(df), size, size))\n    for i, raw_strokes in enumerate(df.drawing.values):\n        x[i] = draw_cv2(raw_strokes, size=size, lw=lw)\n    x = x \/ 255.\n    x = x.reshape((len(df), size, size, 1)).astype(np.float32)                        \n    return x\n\nmodel.load_weights('\/kaggle\/working\/best_model.h5')\npred_results = []\nchunksize = 10000\nTEST_DIR = '..\/input\/testdata\/'\nreader = pd.read_csv(TEST_DIR + 'test_simplified.csv', chunksize=chunksize)\n#reader = pd.read_csv(INPUT_DIR + 'test_simplified.csv', chunksize=chunksize)\nfor chunk in tqdm_notebook(reader):\n    imgs = df_to_image_array(chunk, size)\n    pred = model.predict(imgs, verbose=0)\n    top_3 =  np.argsort(-pred)[:, 0:3]  \n    pred_results.append(top_3)\nprint(\"Finished test predictions...\")","312c7ca1":"#prepare data for saving\nreverse_dict = {v: k for k, v in class_dict.items()}\npred_results = np.concatenate(pred_results)\nprint(\"Finished data prep...\")\n\npreds_df = pd.DataFrame({'first': pred_results[:,0], 'second': pred_results[:,1], 'third': pred_results[:,2]})\npreds_df = preds_df.replace(reverse_dict)\n\npreds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\nsub = pd.read_csv(TEST_DIR + 'sample_submission.csv', index_col=['key_id'])\n#sub = pd.read_csv(INPUT_DIR + 'sample_submission.csv', index_col=['key_id'])\nsub['word'] = preds_df.words.values\nsub.to_csv('submit.csv')\nsub.head()","2627353c":"# \u6df1\u5ea6\u5b66\u4e60\u57fa\u672c\u6982\u5ff5\n* \u73b0\u6709\u4e00\u8bad\u7ec3\u6570\u636e\u96c6traing_data\uff0c\u5171\u67091000\u4e2a\u6837\u672c\uff0c\u5171\u67093\u4e2aclass\uff1adog\uff0ccat\uff0chuman\u3002\u6bcf\u4e2a\u6837\u672c\u67093\u79cd\u53ef\u80fd\u7684label\uff08\u7c7b\u6807\u7b7e\uff09\uff1a[1,0,0]\u8868\u793adog\uff0c[0,1,0]\u8868\u793acat\uff0c[0,0,1]\u8868\u793ahuman\n* Epoch\uff1a\u4f7f\u7528\u8bad\u7ec3\u96c61000\u4e2a\u6837\u672c\u5bf9\u6a21\u578b\u8fdb\u884c\u4e00\u6b21\u5b8c\u6574\u8bad\u7ec3\n* Batch\uff1a\u4f7f\u7528\u8bad\u7ec3\u96c6\u4e2d\u4e00\u5c0f\u90e8\u5206\u6837\u672c\u5bf9\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u4e00\u6b21\u66f4\u65b0\uff0c\u8ba1\u7b97\u4e00\u6b21LOSS\uff0cBatchsize=100\u5c31\u662f\u7528100\u4e2a\u6837\u672c\u505a\u8fd9\u4ef6\u4e8b\u3002\uff08\u5f88\u591a\u60c5\u51b5\u4e0b\u662f\u6700\u91cd\u8981\u7684\uff0c512\u4e4b\u5185\uff09\n* Iteration\uff1a\u4f7f\u7528\u4e00\u4e2aBatch\u7684\u6570\u636e\u5b8c\u6210\u4e00\u6b21\u53c2\u6570\u66f4\u65b0\u7684\u8fc7\u7a0b\u3002\n* \u6a21\u578b\u53c2\u6570\u5982\u4f55\u66f4\u65b0\uff1f\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0cLoss\u8d8a\u5927\uff0c\u68af\u5ea6\u8d8a\u5927\uff0cLoss\u8d8a\u5c0f\uff0c\u68af\u5ea6\u8d8a\u5c0f\u3002","4a47fd4c":"# \u7528\u8bad\u7ec3\u96c6\u6570\u636e\u62df\u5408CNN\u6a21\u578b","32bcc98b":"# **\u6784\u5efa\u5377\u79ef\u795e\u7ecf\u7f51\u7edc**\n* \u767d\u677f\u8bb2\u89e3\u5377\u79ef\u539f\u7406","fe9dfb01":"# \u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\n* \u73b0\u6709\u4e00\u8bad\u7ec3\u6570\u636e\u96c6traing_data\uff0c\u5171\u67091000\u4e2a\u6837\u672c\uff0c\u5171\u67093\u4e2aclass\uff1adog\uff0ccat\uff0chuman\u3002\u6bcf\u4e2a\u6837\u672c\u67093\u79cd\u53ef\u80fd\u7684label\uff08\u7c7b\u6807\u7b7e\uff09\uff1a[1,0,0]\u8868\u793adog\uff0c[0,1,0]\u8868\u793acat\uff0c[0,0,1]\u8868\u793ahuman\n* \u5047\u8bbeCNN\u4ee5\u4e00\u5f20dog\u7684\u56fe\u7247\u4e3ainput\uff0c\u4ee5\u9884\u6d4b\u5411\u91cfa\u4e3aoutput\uff0c\u5176\u4e2da=[0.2, 0.7, 0.1]\uff0c\u5982\u4f55\u8ba1\u7b97Loss\uff1f\n* \uff081\uff09a=[0.2, 0.7, 0.1]\u8868\u793aCNN\u8ba4\u4e3a\u8fd9\u5f20\u56fe\u7247\u670920%\u7684\u6982\u7387\u662fdog\uff0c70%\u7684\u6982\u7387\u662fcat\uff0c10%\u7684\u6982\u7387\u662fhuman\uff0cCNN\u4f1a\u6839\u636e\u6700\u5927\u7684\u6982\u7387\u505a\u51fa\u5224\u65ad\uff0c\u4e5f\u5c31\u662f\u8ba4\u4e3a\u8fd9\u5f20\u56fe\u662fcat\u3002\n* \uff082\uff09\u591a\u5206\u7c7b\u7684\u5355\u6837\u672c\u635f\u5931\u51fd\u6570\uff1a \n\\begin{equation}\nLoss = \\sum_i(-y_i \\cdot \\log \\hat{y}_i)\n\\end{equation}\n\u5176\u4e2di\u4e3a\u7c7b\u522b\u3002\n\u6240\u4ee5\u5728\u4f8b\u5b50\u4e2d\uff0c$loss = -1\\cdot \\log0.2 -0 \\cdot \\log0.7 - 0 \\cdot \\log0.1 = -\\log0.2$.\n\u6211\u4eec\u5e0c\u671b\u6700\u5c0f\u5316Loss\uff0c\u8fd9\u6837\u5c31\u4f1a\u4f7f\u9884\u6d4b\u5411\u91cf\u4e0e\u771f\u5b9e\u7c7b\u522b\u66f4\u52a0\u63a5\u8fd1\uff01","f84ba9ae":"# \u606d\u559c\u4f60\uff01\u5df2\u7ecf\u5b8c\u6210\u4e86\u56fe\u50cf\u6570\u636e\u9884\u5904\u7406\u5de5\u4f5c\uff01\n* \u5177\u4f53\u5730\uff0c\u4f60\u8bfb\u5165\u4e86\u5b58\u50a8\u5728csv\u6587\u4ef6\u4e2d\u7684\u7528\u6237\u7b14\u5212\u5750\u6807\uff0c\u5e76\u4f7f\u7528\u51fd\u6570draw_cv2\u5c06\u5176\u8f6c\u5316\u4e3a\u8ba1\u7b97\u673a\u53ef\u4ee5\u5904\u7406\u7684\u56fe\u50cf\u6570\u636e\uff01\n\n* \u63a5\u4e0b\u6765\uff0c\u4f60\u5c06\u8981\u5229\u7528shuffle_doodle.py\u6587\u4ef6\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u8bad\u7ec3\u6570\u636e\u96c6\u653e\u5728..\/input\/inputdata\/input\/shuffle-csvs\u4e2d\uff01\n* \u539f\u6765\u5171\u6709340\u4e2acsv\u6587\u4ef6\uff0c\u6bcf\u4e2a\u6587\u4ef6\u4e2d\u5305\u542b\u7684\u90fd\u662f\u540c\u7c7b\u6570\u636e\u3002shuffle\u7684\u76ee\u7684\u662f\u5c06340\u4e2acsv\u6587\u4ef6\u968f\u673a\u6253\u4e71\u5e76\u7ec4\u6210100\u4e2acsv\u6587\u4ef6\uff0c\u6bcf\u4e2a\u6587\u4ef6\u4e2d\u5305\u542b\u4e0d\u540c\u7c7b\u6570\u636e\u3002","23c74c7b":"# \u56de\u8c03\u51fd\u6570\uff08\u5b58\u50a8\u6700\u4f18\u6a21\u578b\uff09","63db8c21":"# \u8bc4\u4ef7\u6307\u6807-\u7c7b\u522b\u51c6\u786e\u7387\n* \u767d\u677f\u8bb2\u51c6\u786e\u7387\u3001topk\u51c6\u786e\u7387"}}