{"cell_type":{"a5879700":"code","4417ae66":"code","8bb832e8":"code","ed312a3a":"code","b751c5b8":"code","7621c9f3":"code","f56abf56":"code","f799f7ff":"code","89afebd5":"code","5b8afc43":"code","8b61f1e5":"code","10910685":"code","7ba81302":"code","1d3120cc":"code","ae07dccb":"code","16e2a417":"code","e5ca624a":"code","5df9c325":"code","7a856839":"code","56d4fc19":"code","55189c81":"code","9cdf1258":"code","8b0a21a4":"code","ecfffc27":"code","aaf1e1d8":"code","91126874":"code","dfa317c5":"code","13faf8db":"code","c1f20f35":"code","bf3c8969":"code","2d165cb8":"code","e5fd84d1":"code","a5579108":"code","358f7aa5":"code","3c4e9d2c":"code","b11a32fb":"code","24450ec9":"markdown","afa04b0f":"markdown","d41bcfe3":"markdown","895941ae":"markdown","f873dcbe":"markdown","d2cd6142":"markdown","5f489361":"markdown","158150d0":"markdown","52ebf8f0":"markdown","f6b4b7ce":"markdown","d69d2f2e":"markdown","1c84dd3c":"markdown","911dc2f2":"markdown","d99c2016":"markdown","34e096c7":"markdown","519153bc":"markdown","db784695":"markdown","9b2fd35a":"markdown","c0258514":"markdown","54bd729e":"markdown","82398ee9":"markdown","5854f858":"markdown","f43fbd3b":"markdown","358000c6":"markdown","81ed606e":"markdown","5b318eff":"markdown","8f8141dd":"markdown","c1f5522f":"markdown","dfcacbeb":"markdown","9cf98b41":"markdown","6b8c3754":"markdown","a4f05dfd":"markdown"},"source":{"a5879700":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4417ae66":"!pip install spacy ","8bb832e8":"train = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ntrain.head()","ed312a3a":"train.iloc[9,1]","b751c5b8":"paragraphs = [\"\u0ba8\u0bc6\u0bb2\u0bcd\u0b9a\u0ba9\u0bcd \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe (Nelson Rolihlahla Mandela, 18 \u0b9a\u0bc2\u0bb2\u0bc8 1918 \u2013 5 \u0ba4\u0bbf\u0b9a\u0bae\u0bcd\u0baa\u0bb0\u0bcd 2013)\", \"\u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe\u0bb5\u0bbf\u0ba9\u0bcd \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbe\u0b9f\u0bcd\u0b9a\u0bbf \u0bae\u0bc1\u0bb1\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0ba4\u0bc7\u0bb0\u0bcd\u0ba8\u0bcd\u0ba4\u0bc6\u0b9f\u0bc1\u0b95\u0bcd\u0b95\u0baa\u0bcd\u0baa\u0b9f\u0bcd\u0b9f \u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1\u0ba4\u0bcd \u0ba4\u0bb2\u0bc8\u0bb5\u0bb0\u0bcd \u0b86\u0bb5\u0bbe\u0bb0\u0bcd.\", \"\u0b85\u0ba4\u0bb1\u0bcd\u0b95\u0bc1\",\"\u0bae\u0bc1\u0ba9\u0bcd\u0ba9\u0bb0\u0bcd \u0ba8\u0bbf\u0bb1\u0bb5\u0bc6\u0bb1\u0bbf\u0b95\u0bcd\u0b95\u0bc1 \u0b8e\u0ba4\u0bbf\u0bb0\u0bbe\u0b95\u0baa\u0bcd \u0baa\u0bcb\u0bb0\u0bbe\u0b9f\u0bbf\u0baf \u0bae\u0bc1\u0b95\u0bcd\u0b95\u0bbf\u0baf \u0ba4\u0bb2\u0bc8\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bc1\u0bb3\u0bcd \u0b92\u0bb0\u0bc1\u0bb5\u0bb0\u0bbe\u0b95 \u0b87\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd.\",\"MPWOLKE\"\"\u0ba4\u0bca\u0b9f\u0b95\u0bcd\u0b95\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd\",\"\u0b85\u0bb1\u0baa\u0bcd\u0baa\u0bcb\u0bb0\u0bcd (\u0bb5\u0ba9\u0bcd\u0bae\u0bc1\u0bb1\u0bc8\u0baf\u0bb1\u0bcd\u0bb1) \u0bb5\u0bb4\u0bbf\u0baf\u0bbf\u0bb2\u0bcd \u0ba8\u0bae\u0bcd\u0baa\u0bbf\u0b95\u0bcd\u0b95\u0bc8 \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bbf\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4 \u0b87\u0bb5\u0bb0\u0bcd, \u0baa\u0bbf\u0bb1\u0b95\u0bc1  \u0b86\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95 \u0ba4\u0bc7\u0b9a\u0bbf\u0baf \u0b95\u0bbe\u0b99\u0bcd\u0b95\u0bbf\u0bb0\u0bb8\u0bbf\u0ba9\u0bcd \u0b87\u0bb0\u0bbe\u0ba3\u0bc1\u0bb5\u0baa\u0bcd\",\"MPWOLKE\",\" \u0baa\u0bbf\u0bb0\u0bbf\u0bb5\u0bc1\u0b95\u0bcd\u0b95\u0bc1 \u0ba4\u0bb2\u0bc8\u0bae\u0bc8 \u0ba4\u0bbe\u0b99\u0bcd\u0b95\u0bbf\u0ba9\u0bbe\u0bb0\u0bcd.\",\"\u0b87\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bcd\", \"\u0bae\u0bb0\u0baa\u0bc1\u0b9a\u0bbe\u0bb0\u0bbe \u0b95\u0bca\u0bb0\u0bbf\u0bb2\u0bcd\u0bb2\u0bbe\u0baa\u0bcd \u0baa\u0bcb\u0bb0\u0bcd\u0bae\u0bc1\u0bb1\u0bc8\u0ba4\u0bcd \u0ba4\u0bbe\u0b95\u0bcd\u0b95\u0bc1\u0ba4\u0bb2\u0bc8 \u0ba8\u0bbf\u0bb1\u0bb5\u0bc6\u0bb1\u0bbf\",\"MPWOLKE\",\" \u0b85\u0bb0\u0b9a\u0bc1\u0b95\u0bcd\u0b95\u0bc1 \u0b8e\u0ba4\u0bbf\u0bb0\u0bbe\u0b95 \u0ba8\u0b9f\u0ba4\u0bcd\u0ba4\u0bbf\u0ba9\u0bb0\u0bcd.\", \"\u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe\u0bb5\u0bbf\u0ba9\u0bcd 27\", \"\u0b86\u0ba3\u0bcd\u0b9f\u0bc1 \u0b9a\u0bbf\u0bb1\u0bc8\u0bb5\u0bbe\u0b9a\u0bae\u0bcd, \u0ba8\u0bbf\u0bb1\u0bb5\u0bc6\u0bb1\u0bbf\u0b95\u0bcd \u0b95\u0bca\u0b9f\u0bc1\u0bae\u0bc8\u0baf\u0bbf\u0ba9\u0bcd \u0baa\u0bb0\u0bb5\u0bb2\u0bbe\u0b95 \u0b85\u0bb1\u0bbf\u0baf\u0baa\u0bcd\u0baa\u0b9f\u0bcd\u0b9f \u0b9a\u0bbe\u0b9f\u0bcd\u0b9a\u0bbf\u0baf\u0bae\u0bbe\u0b95 \u0bb5\u0bbf\u0bb3\u0b99\u0bcd\u0b95\u0bc1\u0b95\u0bbf\u0bb1\u0ba4\u0bc1.\", \"\u0b9a\u0bbf\u0bb1\u0bc8\u0baf\u0bbf\u0ba9\u0bcd\" , \"\u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bcd\u0baa\u0bbe\u0bb2\u0bbe\u0ba9 \u0b95\u0bbe\u0bb2\u0ba4\u0bcd\u0ba4\u0bc8 \u0b87\u0bb5\u0bb0\u0bcd \u0bb0\u0bbe\u0baa\u0ba9\u0bcd \u0ba4\u0bc0\u0bb5\u0bbf\u0bb2\u0bcd \u0b9a\u0bbf\u0bb1\u0bbf\u0baf \u0b9a\u0bbf\u0bb1\u0bc8 \u0b85\u0bb1\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0b95\u0bb4\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd. \",\"1990 \u0b87\u0bb2\u0bcd\", \"\u0b85\u0bb5\u0bb0\u0ba4\u0bc1 \u0bb5\u0bbf\u0b9f\u0bc1\u0ba4\u0bb2\u0bc8\u0b95\u0bcd\u0b95\u0bc1 \u0baa\u0bbf\u0bb1\u0b95\u0bc1 \u0b85\u0bae\u0bc8\u0ba4\u0bbf\u0baf\u0bbe\u0ba9 \u0bae\u0bc1\u0bb1\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0baa\u0bc1\u0ba4\u0bbf\u0baf \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0b95\u0bcd \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1 \u0bae\u0bb2\u0bb0\u0bcd\u0ba8\u0bcd\u0ba4\u0ba4\u0bc1.\", \"\u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe\", \"\u0b89\u0bb2\u0b95\u0bbf\u0bb2\u0bcd \u0b85\u0ba4\u0bbf\u0b95\u0bae\u0bcd \u0bae\u0ba4\u0bbf\u0b95\u0bcd\u0b95\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0ba4\u0bb2\u0bc8\u0bb5\u0bb0\u0bcd\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b92\u0bb0\u0bc1\u0bb5\u0bb0\u0bbe\u0b95 \u0bb5\u0bbf\u0bb3\u0b99\u0bcd\u0b95\u0bbf\u0ba9\u0bbe\u0bb0\u0bcd.\\n\u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe, \u0b87\u0ba9\u0bb5\u0bc6\u0bb1\u0bbf \u0b86\u0b9f\u0bcd\u0b9a\u0bbf\u0baf\u0bbf\u0bb2\u0bcd \u0b8a\u0bb1\u0bbf\u0b95\u0bcd\u0b95\u0bbf\u0b9f\u0ba8\u0bcd\u0ba4 \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe\u0bb5\u0bc8 \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbe\u0b9f\u0bcd\u0b9a\u0bbf\u0baf\u0bbf\u0ba9\u0bcd \u0bae\u0bbf\u0bb3\u0bbf\u0bb0\u0bcd\u0bb5\u0bc1\u0b95\u0bcd\u0b95\u0bc1 \u0b87\u0b9f\u0bcd\u0b9f\u0bc1\u0b9a\u0bcd \u0b9a\u0bc6\u0ba9\u0bcd\u0bb1\u0bb5\u0bb0\u0bcd.\", \"\u0b85\u0bae\u0bc8\u0ba4\u0bbf\u0bb5\u0bb4\u0bbf\u0baa\u0bcd\",\" \u0baa\u0bcb\u0bb0\u0bbe\u0bb3\u0bbf\u0baf\u0bbe\u0b95, \u0b86\u0baf\u0bc1\u0ba4\u0baa\u0bcd \u0baa\u0bcb\u0bb0\u0bbe\u0b9f\u0bcd\u0b9f\u0ba4\u0bcd \u0ba4\u0bb2\u0bc8\u0bb5\u0ba9\u0bbe\u0b95, \u0ba4\u0bc7\u0b9a\u0ba4\u0bcd\u0ba4\u0bc1\u0bb0\u0bcb\u0b95\u0b95\u0bcd \u0b95\u0bc1\u0bb1\u0bcd\u0bb1\u0bae\u0bcd \u0b9a\u0bc1\u0bae\u0ba4\u0bcd\u0ba4\u0baa\u0bcd\u0baa\u0b9f\u0bcd\u0b9f \u0b95\u0bc1\u0bb1\u0bcd\u0bb1\u0bb5\u0bbe\u0bb3\u0bbf\u0baf\u0bbe\u0b95, 27 \u0b86\u0ba3\u0bcd\u0b9f\u0bc1\u0b95\u0bb3\u0bcd \u0b9a\u0bbf\u0bb1\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0bb5\u0bbe\u0b9f\u0bbf \u0baa\u0bbf\u0ba9\u0bcd\u0ba9\u0bb0\u0bcd \u0bb5\u0bbf\u0b9f\u0bc1\u0ba4\u0bb2\u0bc8\u0baf\u0bbe\u0b95\u0bbf \u0b95\u0bc1\u0b9f\u0bbf\u0baf\u0bb0\u0b9a\u0bc1 \u0ba4\u0bb2\u0bc8\u0bb5\u0bb0\u0bbe\u0b95, \u0b85\u0bae\u0bc8\u0ba4\u0bbf\u0b95\u0bcd\u0b95\u0bbe\u0ba9 \u0ba8\u0bcb\u0baa\u0bb2\u0bcd \u0baa\u0bb0\u0bbf\u0b9a\u0bc1 \u0baa\u0bc6\u0bb1\u0bcd\u0bb1\u0bb5\u0bb0\u0bbe\u0b95 \u0b87\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0b85\u0bb0\u0b9a\u0bbf\u0baf\u0bb2\u0bcd \u0baa\u0baf\u0ba3\u0bae\u0bcd \u0ba4\u0bca\u0b9f\u0bb0\u0bcd\u0ba8\u0bcd\u0ba4\u0ba4\u0bc1.\", \"\u0b9a\u0bc2\u0ba9\u0bcd 2008\u0bb2\u0bcd\" \"\u0baa\u0bca\u0ba4\u0bc1 \u0bb5\u0bbe\u0bb4\u0bcd\u0b95\u0bcd\u0b95\u0bc8\u0baf\u0bbf\u0bb2\u0bbf\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bc1 \u0bb5\u0bbf\u0bb2\u0b95\u0bc1\u0bb5\u0ba4\u0bbe\u0b95 \u0b85\u0bb1\u0bbf\u0bb5\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd.\\n \u0b87\u0bb3\u0bae\u0bc8 \\n\u0ba8\u0bc6\u0bb2\u0bcd\u0b9a\u0ba9\u0bcd\" \"\u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe 1918\" \"\u0b86\u0bae\u0bcd \u0b86\u0ba3\u0bcd\u0b9f\u0bc1 \u0b9a\u0bc2\u0bb2\u0bc8 \u0bae\u0bbe\u0ba4\u0bae\u0bcd 18 \u0b86\u0bae\u0bcd\" \"\u0ba4\u0bbf\u0baf\u0ba4\u0bbf \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe\u0bb5\u0bbf\u0bb2\u0bcd \u0b89\u0bb3\u0bcd\u0bb3 \u0b95\u0bc1\u0bb2\u0bc1 \u0b95\u0bbf\u0bb0\u0bbe\u0bae\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0baa\u0bbf\u0bb1\u0ba8\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd.\" \"\u0b87\u0bb5\u0bb0\u0ba4\u0bc1 \u0ba4\u0ba8\u0bcd\u0ba4\u0bc8 \u0b9a\u0bcb\u0b9a\u0bbe \u0baa\u0bb4\u0b99\u0bcd\u0b95\u0bc1\u0b9f\u0bbf \u0b87\u0ba9 \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bcd \u0ba4\u0bb2\u0bc8\u0bb5\u0bb0\u0bcd \u0b86\u0bb5\u0bbe\u0bb0\u0bcd[1].\"\" \u0b87\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0ba4\u0ba8\u0bcd\u0ba4\u0bc8\u0b95\u0bcd\u0b95\u0bc1 \u0ba8\u0bbe\u0ba9\u0bcd\u0b95\u0bc1 \u0bae\u0ba9\u0bc8\u0bb5\u0bbf\u0b95\u0bb3\u0bcd.\"\" 4 \u0b86\u0ba3\u0bcd\u0b95\u0bb3\u0bc1\u0bae\u0bcd 9 \u0baa\u0bc6\u0ba3\u0bcd\u0b95\u0bb3\u0bc1\u0bae\u0bbe\u0b95 13 \u0baa\u0bbf\u0bb3\u0bcd\u0bb3\u0bc8\u0b95\u0bb3\u0bcd. \u0bae\u0bc2\u0ba9\u0bcd\u0bb1\u0bbe\u0bb5\u0ba4\u0bc1 \u0bae\u0ba9\u0bc8\u0bb5\u0bbf\u0b95\u0bcd\u0b95\u0bc1 \u0bae\u0b95\u0ba9\u0bbe\u0b95\u0baa\u0bcd \u0baa\u0bbf\u0bb1\u0ba8\u0bcd\u0ba4\u0bb5\u0bb0\u0bcd \u0ba4\u0bbe\u0ba9\u0bcd \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe. \u0b87\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0bb4\u0bc1\u0baa\u0bcd\u0baa\u0bc6\u0baf\u0bb0\u0bcd \\\u0ba8\u0bc6\u0bb2\u0bcd\u0b9a\u0ba9\u0bcd \u0bb0\u0bcb\u0baa\u0bbf\u0b9a\u0bb2\u0bbe \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe\\. \u0ba8\u0bc6\u0bb2\u0bcd\u0b9a\u0ba9\u0bcd \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe \u0b8e\u0ba9\u0bcd\u0bb1\u0bc7 \u0baa\u0bca\u0ba4\u0bc1\u0bb5\u0bbe\u0b95 \u0b85\u0bb4\u0bc8\u0baa\u0bcd\u0baa\u0bbe\u0bb0\u0bcd\u0b95\u0bb3\u0bcd. \u0b87\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0b9a\u0bbf\u0bb1\u0bc1\u0bb5\u0baf\u0ba4\u0bbf\u0bb2\u0bcd \u0b95\u0bc1\u0ba4\u0bcd\u0ba4\u0bc1\u0b9a\u0bcd \u0b9a\u0ba3\u0bcd\u0b9f\u0bc8 \u0bb5\u0bc0\u0bb0\u0bb0\u0bbe\u0b95\u0bb5\u0bc7 \u0b85\u0bb1\u0bbf\u0baf\u0baa\u0bcd \u0baa\u0bc6\u0bb1\u0bcd\u0bb1\u0bbe\u0bb0\u0bcd.\\n\\n\u0b85\u0ba8\u0bcd\u0ba4\u0b95\u0bcd \u0b95\u0bc1\u0b9f\u0bc1\u0bae\u0bcd\u0baa\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bbf\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bc1  \u0bae\u0bc1\u0ba4\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0bb2\u0bbf\u0bb2\u0bcd \u0baa\u0bb3\u0bcd\u0bb3\u0bbf \u0b9a\u0bc6\u0ba9\u0bcd\u0bb1 \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe, \u0b87\u0bb3\u0bae\u0bcd \u0bb5\u0baf\u0ba4\u0bbf\u0bb2\u0bcd \u0b86\u0b9f\u0bc1, \u0bae\u0bbe\u0b9f\u0bc1 \u0bae\u0bc7\u0baf\u0bcd\u0ba4\u0bcd\u0ba4\u0bc1\u0b95\u0bcd\u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bc7 \u0baa\u0bb3\u0bcd\u0bb3\u0bbf\u0b95\u0bcd\u0b95\u0bc2\u0b9f\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0baa\u0b9f\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd. \u0baa\u0bcb\u0bb0\u0bcd \u0baa\u0bc1\u0bb0\u0bbf\u0baf\u0bc1\u0bae\u0bcd \u0b95\u0bb2\u0bc8\u0b95\u0bb3\u0bc8\u0baf\u0bc1\u0bae\u0bcd \u0baa\u0baf\u0bbf\u0ba9\u0bcd\u0bb1\u0bbe\u0bb0\u0bcd. \u0b87\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0baa\u0bc6\u0baf\u0bb0\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba9\u0bcd\u0ba9\u0bbe\u0bb2\u0bcd \u0b89\u0bb3\u0bcd\u0bb3 \"\"\u0ba8\u0bc6\u0bb2\u0bcd\u0b9a\u0ba9\u0bcd\" \"\u0b87\u0bb5\u0bb0\u0bcd \u0b95\u0bb2\u0bcd\u0bb5\u0bbf \u0b95\u0bb1\u0bcd\u0bb1 \u0bae\u0bc1\u0ba4\u0bb2\u0bcd \u0baa\u0bb3\u0bcd\u0bb3\u0bbf\u0baf\u0bbf\u0ba9\u0bcd \u0b86\u0b9a\u0bbf\u0bb0\u0bbf\u0baf\u0bb0\u0bbf\u0ba9\u0bbe\u0bb2\u0bcd \u0b9a\u0bc2\u0b9f\u0bcd\u0b9f\u0baa\u0bcd\u0baa\u0b9f\u0bcd\u0b9f\u0ba4\u0bc1 \u0b8e\u0ba9\u0bcd\u0baa\u0ba4\u0bc1 \u0b95\u0bc1\u0bb1\u0bbf\u0baa\u0bcd\u0baa\u0bbf\u0b9f\u0ba4\u0bcd\u0ba4\u0b95\u0bcd\u0b95\u0ba4\u0bc1. \u0b95\u0bb2\u0bcd\u0bb5\u0bbf\u0baf\u0bb1\u0bbf\u0bb5\u0bc8\u0baa\u0bcd \u0baa\u0bc6\u0bb1\u0bc1\u0bb5\u0ba4\u0bbf\u0bb2\u0bcd \u0baa\u0bc6\u0bb0\u0bbf\u0ba4\u0bc1\u0bae\u0bcd \u0ba8\u0bbe\u0b9f\u0bcd\u0b9f\u0bae\u0bcd \u0b95\u0bca\u0ba3\u0bcd\u0b9f \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe, \u0bb2\u0ba3\u0bcd\u0b9f\u0ba9\u0bcd \u0bae\u0bb1\u0bcd\u0bb1\u0bc1\u0bae\u0bcd \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe \u0baa\u0bb2\u0bcd\u0b95\u0bb2\u0bc8\u0b95\u0bcd\u0b95\u0bb4\u0b95\u0b99\u0bcd\u0b95\u0bb3\u0bbf\u0bb2\u0bc1\u0bae\u0bcd \u0baa\u0b9f\u0bcd\u0b9f\u0baa\u0bcd\u0baa\u0b9f\u0bbf\u0baa\u0bcd\u0baa\u0bc8 \u0bae\u0bc7\u0bb1\u0bcd\u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bbe\u0bb0\u0bcd. 1941 \u0b86\u0bae\u0bcd \u0b86\u0ba3\u0bcd\u0b9f\u0bc1 \u0b9c\u0bcb\u0b95\u0bbe\u0ba9\u0bb8\u0bcd\u0baa\u0bc7\u0bb0\u0bcd\u0b95\u0bcd \u0b9a\u0bc6\u0ba9\u0bcd\u0bb1\u0bc1 \u0baa\u0b95\u0bc1\u0ba4\u0bbf \u0ba8\u0bc7\u0bb0\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b9a\u0b9f\u0bcd\u0b9f\u0b95\u0bcd\u0b95\u0bb2\u0bcd\u0bb5\u0bbf \u0baa\u0b9f\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd. \u0b92\u0bb0\u0bc1 \u0ba4\u0b99\u0bcd\u0b95\u0b9a\u0bcd \u0b9a\u0bc1\u0bb0\u0b99\u0bcd\u0b95 \u0baa\u0bbe\u0ba4\u0bc1\u0b95\u0bbe\u0baa\u0bcd\u0baa\u0bc1 \u0b85\u0ba4\u0bbf\u0b95\u0bbe\u0bb0\u0bbf\u0baf\u0bbe\u0b95\u0bb5\u0bc1\u0bae\u0bcd, \u0ba4\u0bcb\u0b9f\u0bcd\u0b9f \u0bae\u0bc1\u0b95\u0bb5\u0bb0\u0bbe\u0b95\u0bb5\u0bc1\u0bae\u0bcd \u0baa\u0ba3\u0bbf\u0baf\u0bbe\u0bb1\u0bcd\u0bb1\u0bbf \u0bb5\u0ba8\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd.\\n\u0b85\u0baa\u0bcd\u0baa\u0bcb\u0ba4\u0bc1 \\'\u0ba8\u0bcb\u0bae\u0ba4\u0bbe\u0bae\u0bcd \u0b9a\u0b99\u0bcd\u0b95\u0bb0\u0bcd\\' \u0b8e\u0ba9\u0bcd\u0bb1 \u0b9a\u0bc6\u0bb5\u0bbf\u0bb2\u0bbf\u0baf\u0bb0\u0bc8\u0ba4\u0bcd  \u0ba4\u0bbf\u0bb0\u0bc1\u0bae\u0ba3\u0bae\u0bcd \u0b9a\u0bc6\u0baf\u0bcd\u0ba4\u0bc1 \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bbe\u0bb0\u0bcd. \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe \u0b86\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95 \u0ba4\u0bc7\u0b9a\u0bbf\u0baf \u0b95\u0bbe\u0b99\u0bcd\u0b95\u0bbf\u0bb0\u0bb8\u0bcd \u0b87\u0baf\u0b95\u0bcd\u0b95\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0ba4\u0bc0\u0bb5\u0bbf\u0bb0\u0bae\u0bbe\u0b95 \u0b88\u0b9f\u0bc1\u0baa\u0b9f\u0bcd\u0b9f\u0bc1 \u0b87\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0ba4\u0bbe\u0bb2\u0bcd \u0bae\u0ba9\u0bc8\u0bb5\u0bbf\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0b85\u0bb5\u0bb0\u0bc1\u0b95\u0bcd\u0b95\u0bc1\u0bae\u0bcd \u0b87\u0b9f\u0bc8\u0baf\u0bc7 \u0b95\u0bb0\u0bc1\u0ba4\u0bcd\u0ba4\u0bc1 \u0bb5\u0bc7\u0bb1\u0bc1\u0baa\u0bbe\u0b9f\u0bc1 \u0b8f\u0bb1\u0bcd\u0baa\u0b9f\u0bcd\u0b9f\u0ba4\u0bc1. \u0baa\u0bbf\u0ba9\u0bcd\u0ba9\u0bb0\u0bcd \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95 \u0b85\u0bb0\u0b9a\u0bc1, \u0b86\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95 \u0ba4\u0bc7\u0b9a\u0bbf\u0baf \u0b95\u0bbe\u0b99\u0bcd\u0b95\u0bbf\u0bb0\u0bb8\u0bcd \u0b95\u0b9f\u0bcd\u0b9a\u0bbf\u0baf\u0bc8\u0ba4\u0bcd \u0ba4\u0b9f\u0bc8 \u0b9a\u0bc6\u0baf\u0bcd\u0ba4\u0ba4\u0bc1. \u0bae\u0ba3\u0bcd\u0b9f\u0bc7\u0bb2\u0bbe \u0bae\u0bc0\u0ba4\u0bc1 \u0bb5\u0bb4\u0b95\u0bcd\u0b95\u0bc1 \u0ba4\u0bca\u0b9f\u0bb0\u0baa\u0bcd\u0baa\u0b9f\u0bcd\u0b9f\u0ba4\u0bc1. \u0b90\u0ba8\u0bcd\u0ba4\u0bbe\u0ba3\u0bcd\u0b9f\u0bc1\u0b95\u0bb3\u0bbe\u0b95 \u0b85\u0ba8\u0bcd\u0ba4 \u0bb5\u0bb4\u0b95\u0bcd\u0b95\u0bc1 \u0bb5\u0bbf\u0b9a\u0bbe\u0bb0\u0ba3\u0bc8 \u0ba8\u0b9f\u0ba8\u0bcd\u0ba4\u0bc1 \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bc1 \u0b87\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0baa\u0bcb\u0ba4\u0bc1 1958 \u0b86\u0bae\u0bcd \u0b86\u0ba3\u0bcd\u0b9f\u0bc1 \u0bb5\u0bbf\u0ba9\u0bcd\u0ba9\u0bbf \u0bae\u0b9f\u0bbf\u0b95\u0bbf \u0bb2\u0bc7\u0ba9\u0bbe \u0b8e\u0ba9\u0bcd\u0baa\u0bb5\u0bb0\u0bc8 \u0bae\u0ba3\u0ba8\u0bcd\u0ba4\u0bbe\u0bb0\u0bcd. \u0bb5\u0bbf\u0ba9\u0bcd\u0ba9\u0bbf \u0ba4\u0ba9\u0ba4\u0bc1 \u0b95\u0ba3\u0bb5\u0bb0\u0bbf\u0ba9\u0bcd \u0b95\u0bca\u0bb3\u0bcd\u0b95\u0bc8\u0b95\u0bb3\u0bc1\u0b95\u0bcd\u0b95\u0bbe\u0b95\u0baa\u0bcd \u0baa\u0bcb\u0bb0\u0bbe\u0b9f\u0bbf \u0bb5 (Truth and Reconciliation Commission)\"]  ","7621c9f3":"text = \" \".join(paragraphs)\nwords = text.split(\" \")","f56abf56":"from collections import Counter \ncnt = Counter(words)\n\ncnt.most_common(10)\n# print ","f799f7ff":"from spacy.lang.ta import STOP_WORDS as STOP_WORDS_TA\n\n#from spacy.lang.hi import STOP_WORDS as hindi_stopwords\n#from spacy.lang.ta import STOP_WORDS as tamil_stopwords","89afebd5":"#https:\/\/colab.research.google.com\/github\/rahul1990gupta\/indic-nlp-datasets\/blob\/master\/examples\/Getting_started_with_processing_hindi_text.ipynb#scrollTo=ghnpMGzngcOn\n\n# Let's remove the stop words before printing most common words \n\nfrom spacy.lang.ta import Tamil\n\nnlp = Tamil()\n\ndoc = nlp(text)\n\nnot_stop_words = []\nfor token in doc:\n  if token.is_stop:\n    continue\n  if token.is_punct or token.text ==\"|\":\n    continue \n  not_stop_words.append(token.text)\n\n\nnot_stop_cnt = Counter(not_stop_words)\n\nnot_stop_cnt.most_common(10)","5b8afc43":"# Cancel Wordcloud Go to plan B \n# Stanza, iNLTK and Indic NLP now are plan A  \nfrom wordcloud import WordCloud\nfrom spacy.lang.ta import STOP_WORDS as STOP_WORS_TA\nimport matplotlib.pyplot as plt","8b61f1e5":"!pip install torch==1.3.1+cpu -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","10910685":"!pip install inltk","7ba81302":"from inltk.inltk import setup\nsetup('hi')","1d3120cc":"from inltk.inltk import tokenize\n\nhindi_text = \"\"\"\u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0915\u094d\u0930\u0928\u094d\u0924\u093f\u0915\u093e\u0930\u0940 \u092f\u0941\u0926\u094d\u0927 (1775\u20131783), \u091c\u093f\u0938\u0947 \u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u091c\u094d\u092f \u092e\u0947\u0902 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u094d\u0930\u0924\u093e \u092f\u0941\u0926\u094d\u0927 \u092f\u093e \u0915\u094d\u0930\u0928\u094d\u0924\u093f\u0915\u093e\u0930\u0940 \u092f\u0941\u0926\u094d\u0927 \u092d\u0940 \u0915\u0939\u093e \u091c\u093e\u0924\u093e \u0939\u0948, \u0917\u094d\u0930\u0947\u091f \u092c\u094d\u0930\u093f\u091f\u0947\u0928 \u0914\u0930 \u0909\u0938\u0915\u0947 \u0924\u0947\u0930\u0939 \u0909\u0924\u094d\u0924\u0930 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0909\u092a\u0928\u093f\u0935\u0947\u0936\u094b\u0902 \u0915\u0947 \u092c\u0940\u091a \u090f\u0915 \u0938\u0948\u0928\u094d\u092f \u0938\u0902\u0918\u0930\u094d\u0937 \u0925\u093e, \u091c\u093f\u0938\u0938\u0947 \u0935\u0947 \u0909\u092a\u0928\u093f\u0935\u0947\u0936 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u094d\u0930 \u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u091c\u094d\u092f \u0905\u092e\u0947\u0930\u093f\u0915\u093e \u092c\u0928\u0947\u0964 \u0936\u0941\u0930\u0942\u0906\u0924\u0940 \u0932\u0921\u093c\u093e\u0908 \u0909\u0924\u094d\u0924\u0930 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u092e\u0939\u093e\u0926\u094d\u0935\u0940\u092a \u092a\u0930 \u0939\u0941\u0908\u0964 \u0938\u092a\u094d\u0924\u0935\u0930\u094d\u0937\u0940\u092f \u092f\u0941\u0926\u094d\u0927 \u092e\u0947\u0902 \u092a\u0930\u093e\u091c\u092f \u0915\u0947 \u092c\u093e\u0926, \u092c\u0926\u0932\u0947 \u0915\u0947 \u0932\u093f\u090f \u0906\u0924\u0941\u0930 \u092b\u093c\u094d\u0930\u093e\u0928\u094d\u0938 \u0928\u0947 1778 \u092e\u0947\u0902 \u0907\u0938 \u0928\u090f\"\"\"\n\n# tokenize(input text, language code)\ntokenize(hindi_text, \"hi\") #Hi is Hindi language","ae07dccb":"from inltk.inltk import get_similar_sentences\n\n# get similar sentences to the one given in hindi\noutput = get_similar_sentences('\u092e\u0948\u0902 \u0906\u091c \u092c\u0939\u0941\u0924 \u0916\u0941\u0936 \u0939\u0942\u0902', 5, 'hi')\n\nprint(output)","16e2a417":"from inltk.inltk import get_embedding_vectors\n\n# get embedding for input words\nvectors = get_embedding_vectors(\"\u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0909\u092a\u0928\u093f\u0935\u0947\u0936\u094b\u0902 \u0915\u0947\u092f\u093e\", \"hi\") #hi is Hindi language\n\nprint(vectors)\n# print shape of the first word\nprint(\"shape:\", vectors[0].shape)","e5ca624a":"from inltk.inltk import get_sentence_similarity\n\n# similarity of encodings is calculated by using cmp function whose default is cosine similarity\nget_sentence_similarity('\u092e\u0941\u091d\u0947 \u092d\u094b\u091c\u0928 \u092a\u0938\u0902\u0926 \u0939\u0948\u0964', '\u092e\u0948\u0902 \u0910\u0938\u0947 \u092d\u094b\u091c\u0928 \u0915\u0940 \u0938\u0930\u093e\u0939\u0928\u093e \u0915\u0930\u0924\u093e \u0939\u0942\u0902 \u091c\u093f\u0938\u0915\u093e \u0938\u094d\u0935\u093e\u0926 \u0905\u091a\u094d\u091b\u093e \u0939\u094b\u0964', 'hi')","5df9c325":"get_sentence_similarity(\"\u0936\u0941\u0930\u0942\u0906\u0924\u0940 \u0932\u0921\u093c\u093e\u0908 \u0909\u0924\u094d\u0924\u0930 \u0905\u092e\u0947\u0930\u093f\u0915\u0940\", \"\u092e\u0939\u093e\u0926\u094d\u0935\u0940\u092a \u092a\u0930 \u0939\u0941\u0908\u0964 \u0938\u092a\u094d\u0924\u0935\u0930\u094d\u0937\u0940\u092f \u092f\u0941\u0926\u094d\", 'hi')","7a856839":"!pip install indic-nlp-library","56d4fc19":"# download the resource\n!git clone https:\/\/github.com\/anoopkunchukuttan\/indic_nlp_resources.git","55189c81":"# download the repo\n!git clone https:\/\/github.com\/anoopkunchukuttan\/indic_nlp_library.git","9cdf1258":"import sys\nfrom indicnlp import common\n\n# The path to the local git repo for Indic NLP library\nINDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n\n# The path to the local git repo for Indic NLP Resources\nINDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n\n# Add library to Python path\nsys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n\n# Set environment variable for resources folder\ncommon.set_resources_path(INDIC_NLP_RESOURCES)","8b0a21a4":"from indicnlp.tokenize import sentence_tokenize\n\nindic_string=\"\"\"\u0baa\u0bc6\u0ba3\u0bcd\u0b95\u0bb3\u0bcd \u0b87\u0bb1\u0baa\u0bcd\u0baa\u0ba4\u0bc1\u0bae\u0bcd, \u0baa\u0bbf\u0bb1\u0ba8\u0bcd\u0ba4\u0baa\u0bbf\u0ba9\u0bcd \u0b95\u0bc1\u0bb4\u0ba8\u0bcd\u0ba4\u0bc8\u0b95\u0bb3\u0bcd \u0b87\u0bb1\u0baa\u0bcd\u0baa\u0ba4\u0bc1\u0bae\u0bcd \u0b9a\u0bb0\u0bcd\u0bb5 \u0b9a\u0bbe\u0ba4\u0bbe\u0bb0\u0ba3\u0bae\u0bcd. \u0bb2\u0bc7\u0b9a\u0bbe\u0ba9 \u0b9a\u0bbf\u0bb0\u0bbe\u0baf\u0bcd\u0baa\u0bcd\u0baa\u0bc1\u0b95\u0bb3\u0bc1\u0bae\u0bcd \u0b95\u0bc0\u0bb1\u0bb2\u0bcd\u0b95\u0bb3\u0bc1\u0bae\u0bcd \u0b95\u0bc2\u0b9f \u0bae\u0bb0\u0ba3\u0ba4\u0bcd\u0ba4\u0bbf\u0bb1\u0bcd\u0b95\u0bc1 \u0b87\u0b9f\u0bcd\u0b9f\u0bc1\u0b9a\u0bcd \u0b9a\u0bc6\u0ba9\u0bcd\u0bb1\u0ba9. \u0b92\u0bb0\u0bc1 \u0ba8\u0bc1\u0ba3\u0bcd\u0ba3\u0bc1\u0baf\u0bbf\u0bb0\u0bc8 \u0bb5\u0bc8\u0ba4\u0bcd\u0ba4\u0bc1 \u0b87\u0ba9\u0bcd\u0ba9\u0bca\u0ba9\u0bcd\u0bb1\u0bc8\u0b95\u0bcd \u0b95\u0bca\u0bb2\u0bcd\u0bb2\u0bae\u0bc1\u0b9f\u0bbf\u0b95\u0bbf\u0bb1 \u0baa\u0bc6\u0ba9\u0bbf\u0bb8\u0bbf\u0bb2\u0bbf\u0ba9\u0bcd \u0baa\u0bcb\u0ba9\u0bcd\u0bb1 \u0ba8\u0b9a\u0bcd\u0b9a\u0bc1\u0bae\u0bc1\u0bb1\u0bbf \u0bae\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bc1\u0b95\u0bb3\u0bcd \"\"\"\n# Split the sentence, language code \"hi\" is passed for hingi\nsentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n\n# print the sentences\nfor t in sentences:\n    print(t)","ecfffc27":"from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n\n# Input text \"\ninput_text='\u0baa\u0bc6\u0ba3\u0bcd\u0b95\u0bb3\u0bcd \u0b87\u0bb1\u0baa\u0bcd\u0baa\u0ba4\u0bc1\u0bae\u0bcd, \u0baa\u0bbf\u0bb1\u0ba8\u0bcd\u0ba4\u0baa\u0bbf\u0ba9\u0bcd \u0b95\u0bc1\u0bb4\u0ba8\u0bcd\u0ba4\u0bc8\u0b95\u0bb3\u0bcd \u0b87\u0bb1\u0baa\u0bcd\u0baa\u0ba4\u0bc1\u0bae\u0bcd \u0b9a\u0bb0\u0bcd\u0bb5 \u0b9a\u0bbe\u0ba4\u0bbe\u0bb0\u0ba3\u0bae\u0bcd.'\n\n# Transliterate from Hindi to Tamil\nprint(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"ta\"))","aaf1e1d8":"from indicnlp.langinfo import *\n\n# Input character \nc='\u0909'\n# Language is Hindi or 'hi'\nlang='hi'\n\nprint('Is vowel?:  {}'.format(is_vowel(c,lang)))\nprint('Is consonant?:  {}'.format(is_consonant(c,lang)))\nprint('Is velar?:  {}'.format(is_velar(c,lang)))\nprint('Is palatal?:  {}'.format(is_palatal(c,lang)))\nprint('Is aspirated?:  {}'.format(is_aspirated(c,lang)))\nprint('Is unvoiced?:  {}'.format(is_unvoiced(c,lang)))\nprint('Is nasal?:  {}'.format(is_nasal(c,lang)))","91126874":"!pip install stanza","dfa317c5":"import stanza\nstanza.download('hi')","13faf8db":"nlp = stanza.Pipeline('hi')","c1f20f35":"doc = nlp(\"\u091c\u093f\u0938\u0947 \u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u091c\u094d\u092f \u092e\u0947\u0902 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u094d\u0930\u0924\u093e \u092f\u0941\u0926\u094d\u0927 \u092f\u093e \u0915\u094d\u0930\u0928\u094d\u0924\u093f\u0915\u093e\u0930\u0940 \u092f\u0941\u0926\u094d\u0927 \u092d\u0940 \u0915\u0939\u093e \u091c\u093e\u0924\u093e\")\nprint(doc)\nprint(doc.entities)","bf3c8969":"nlp = stanza.Pipeline('hi', processors='tokenize,pos')\ndoc = nlp('\u091c\u093f\u0938\u0947 \u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u091c\u094d\u092f \u092e\u0947\u0902 \u0905\u092e\u0947\u0930\u093f\u0915\u0940 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u094d\u0930\u0924\u093e \u092f\u0941\u0926\u094d\u0927 \u092f\u093e \u0915\u094d\u0930\u0928\u094d\u0924\u093f\u0915\u093e\u0930\u0940 \u092f\u0941\u0926\u094d\u0927 \u092d') # doc is class Document\ndicts = doc.to_dict() # dicts is List[List[Dict]], representing each token \/ word in each sentence in the document","2d165cb8":"from stanza.models.common.doc import Document\n\ndicts = [[{'id': 1, 'text': 'Test', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=0|end_char=4'}, {'id': 2, 'text': 'sentence', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Number=Sing', 'misc': 'start_char=5|end_char=13'}, {'id': 3, 'text': '.', 'upos': 'PUNCT', 'xpos': '.', 'misc': 'start_char=13|end_char=14'}]] # dicts is List[List[Dict]], representing each token \/ word in each sentence in the document\ndoc = Document(dicts) # doc is class Document","e5fd84d1":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize')\ndoc = nlp('\u0915\u0947\u0902\u0926\u094d\u0930 \u0915\u0940 \u092e\u094b\u0926\u0940 \u0938\u0930\u0915\u093e\u0930 \u0928\u0947 \u0936\u0941\u0915\u094d\u0930\u0935\u093e\u0930 \u0915\u094b \u0905\u092a\u0928\u093e \u0905\u0902\u0924\u0930\u093f\u092e \u092c\u091c\u091f \u092a\u0947\u0936 \u0915\u093f\u092f\u093e. \u0915\u093e\u0930\u094d\u092f\u0935\u093e\u0939\u0915 \u0935\u093f\u0924\u094d\u0924')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","a5579108":"print([sentence.text for sentence in doc.sentences])","358f7aa5":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_no_ssplit=True)\ndoc = nlp('\u0bb5\u0bb4\u0bbf\u0baf\u0bbf\u0bb2\u0bcd \u0ba8\u0bae\u0bcd\u0baa\u0bbf\u0b95\u0bcd\u0b95\u0bc8 \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bbf\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4 \u0b87\u0bb5\u0bb0\u0bcd,MPWOLKE, \u0baa\u0bbf\u0bb1\u0b95')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","3c4e9d2c":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_pretokenized=True)\ndoc = nlp('\u0915\u0947\u0902\u0926\u094d\u0930 \u0915\u0940 \u092e\u094b\u0926\u0940 \u0938\u0930\u0915\u093e\u0930 \u0928\u0947 \u0936\u0941\u0915\u094d\u0930\u0935\u093e\u0930 \u0915\u094b \u0905\u092a\u0928\u093e \u0905\u0902\u0924\u0930\u093f\u092e \u092c\u091c\u091f \u092a\u0947\u0936 \u0915\u093f\u092f\u093e. \u0915\u093e\u0930\u094d\u092f\u0935\u093e\u0939\u0915 \u0935\u093f\u0924\u094d\u0924')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","b11a32fb":"import stanza\n\nnlp = stanza.Pipeline(lang='hi', processors={'tokenize': 'spacy'}) # spaCy tokenizer is currently only allowed in English pipeline.\ndoc = nlp('\u0915\u0947\u0902\u0926\u094d\u0930 \u0915\u0940 \u092e\u094b\u0926\u0940 \u0938\u0930\u0915\u093e\u0930 \u0928\u0947 \u0936\u0941\u0915\u094d\u0930\u0935\u093e\u0930 \u0915\u094b \u0905\u092a\u0928\u093e \u0905\u0902\u0924\u0930\u093f\u092e \u092c\u091c\u091f \u092a\u0947\u0936 \u0915\u093f\u092f\u093e. \u0915\u093e\u0930\u094d\u092f\u0935\u093e\u0939\u0915 \u0935\u093f\u0924\u094d\u0924')\nfor i, sentence in enumerate(doc.sentences):\n    print(f'====== Sentence {i+1} tokens =======')\n    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')","24450ec9":"![](https:\/\/i.ytimg.com\/vi\/Mtkktl0kHV0\/mqdefault.jpg)youtube.com","afa04b0f":"#perform transliteration using the Indic NLP Library: Transliterating from Hindi to Tamil","d41bcfe3":"#Set the path so that Python knows where to find these on your computer:","895941ae":"#Tokenization with iNLTK\n\nThe first step we do to solve any NLP task is to break down the text into its smallest units or tokens. iNLTK supports tokenization of all the 12 languages I showed earlier:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","f873dcbe":"#Tokenization and Sentence Segmentation","d2cd6142":"#Document to Python Object","5f489361":"#Splitting input text into sentences\n\n\"Indic NLP Library supports many basic text processing tasks like normalization, tokenization at the word level, etc. But sentence level tokenization is what I find interesting because this is something that different Indian languages follow different rules for.\"\n\n\"Here is an example of how to use this sentence splitter:\"\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","158150d0":"#Three Important NLP Libraries for Indian Languages\n\nAuthor: MOHD SANAD ZAKI RIZVI, JANUARY 23, 2020 \n\nText Processing for Indian Languages using Python:\n\niNLTK\n\nIndic NLP Library\n\nStanza - https:\/\/stanfordnlp.github.io\/stanza\/index.html\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","52ebf8f0":"#Generate similar sentences from a given text input\n\nSince iNLTK is internally based on a Language Model for each of the languages it supports, we can do interesting stuff like generate similar sentences given a piece of text!\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","f6b4b7ce":"#Phonetics for Indian Sub-Continent languages Alphabet\n\n\"The Indian Sub-Continent languages have strong phonetics for their alphabet and that\u2019s why in the Indic NLP Library, each character has a phonetic vector associated with it that defines its properties.\"\n\n\"An example where we take the simple Hindi character \u2018\u0909\u2019 :\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","d69d2f2e":"#Install inltk","1c84dd3c":"#Python Object to Document","911dc2f2":"#Hi means hindi language. Not hello.","d99c2016":"#The model gives out a cosine similarity of 0.67 which means that the sentences are pretty close, and that\u2019s correct.","34e096c7":"#Journeys of a thousand miles begin with a single step!  BioBERT wait for me with Stanza!","519153bc":"#Start with Pretokenized Text\n\n\"In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\\n) separated sentences, where each sentence is space separated tokens. Just set tokenize_pretokenized as True to bypass the neural tokenizer.\"\n\n\"The code below shows an example of bypassing the neural tokenizer:\"\n\nhttps:\/\/stanfordnlp.github.io\/stanza\/tokenize.html","db784695":"#After the snippet above I couldn't make anything more with Stanza since Hindi is not supported yet.\n\nException: spaCy tokenizer is currently only allowed in English pipeline. ","9b2fd35a":"#Use the tokenizer just for sentence segmentation. To access segmented sentences, simply use","c0258514":"#Installing Stanza","54bd729e":"#Tokenization without Sentence Segmentation\n\n\"Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\\n\\n) in the raw text. Just set tokenize_no_ssplit as True to disable sentence segmentation. Here is an example\"\n\nhttps:\/\/stanfordnlp.github.io\/stanza\/tokenize.html","82398ee9":"Notice that each word is denoted by an embedding of 400 dimensions.","5854f858":"#Exception: spaCy tokenizer is currently only allowed in English pipeline.","f43fbd3b":"#I included my account name in the doc below:","358000c6":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSMcqi3PiHDWsNk5YrwALttBllQQdpvfgsOxg&usqp=CAU)amazon.in","81ed606e":"Sources:\n    \n Stanza Package: A Python NLP Package for Many Human Languages\n https:\/\/stanfordnlp.github.io\/stanza\/sentiment.html   \n \n \n Text Processing for Indian Languages using Python\n https:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","5b318eff":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Indic NLP Library<\/span><\/h1><br>","8f8141dd":"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.\n\nCitation: Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.\n\nhttps:\/\/stanfordnlp.github.io\/stanza\/","c1f5522f":"#Above the similarity is 0.09326018","dfcacbeb":"#Extract embedding vectors\n\n\"When we are training machine learning or deep learning-based models for NLP tasks, we usually represent the text data by an embedding like TF-IDF, Word2vec, GloVe, etc. These embedding vectors capture the semantic information of the text input and are easier to work with for the models (as they expect numerical input).\"\n\n\"iNLTK under the hood utilizes the ULMFiT method of training language models and hence it can generate vector embeddings for a given input text. Here\u2019s an example:\"\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","9cf98b41":"#Finding similarity between two sentences\n\niNLTK provides an API to find semantic similarities between two pieces of text. This is a really useful feature! We can use the similarity score for feature engineering and even building sentiment analysis systems. Here\u2019s how it works:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/3-important-nlp-libraries-indian-languages-python\/","6b8c3754":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">iNLTK Toolkit for Indic Languages<\/span><\/h1><br>\n\niNLTK (Natural Language Toolkit for Indic Languages)\n\niNLTK has a dependency on PyTorch 1.3.1, to use iNLTK is necessary to install that below:\n\nhttps:\/\/inltk.readthedocs.io\/en\/latest\/","a4f05dfd":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Stanza NLP Tookit<\/span><\/h1><br>"}}