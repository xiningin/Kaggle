{"cell_type":{"73cde501":"code","0c50fdd0":"code","7ff20750":"code","5862a516":"code","0e225aff":"code","3aaea281":"code","79a494a5":"code","ba6e5a2c":"code","eba22f0f":"code","2ec3443e":"code","fabd7438":"code","52c4b2da":"code","07ba69df":"code","62ea3f78":"code","2c774d2b":"code","df4486a9":"code","277add9b":"code","997f140e":"code","b2c4c96a":"code","1bfb38c0":"code","ae8e4cd7":"code","ecd84de6":"code","a027b193":"code","3c37fb93":"code","34034485":"code","71afc92e":"code","62c1e293":"code","96c2a939":"code","e1f631ca":"code","1a336196":"code","933c4d57":"code","ae4990e3":"code","2fc9d480":"code","ad1d4be8":"code","f255e15a":"code","5f43570b":"code","3c089e2f":"code","c29c02fc":"code","9928aff1":"code","e4d122fa":"code","a3593359":"code","f6b449fc":"code","91d26282":"code","83e340a7":"code","0b8a7fdd":"code","47ad82ee":"code","4facdc88":"code","1a37126e":"code","d382e608":"code","cda3b35d":"code","100d31af":"code","218e4a80":"code","fd08d105":"code","437110cb":"code","01e70eb5":"code","f5457b01":"code","8717bab4":"code","ffa33324":"code","a8ed955a":"code","f6f3246e":"code","e2ff0e7d":"code","d2d28943":"code","0b578885":"code","94ac2456":"code","3e41da07":"code","8236504b":"code","9421ead2":"code","cf9d7b60":"code","f9383546":"code","5cd86f3d":"code","d4f089bc":"code","cebfc721":"code","177f89e2":"code","9898d8cb":"code","fc5ea9ef":"code","43aac02c":"code","458691e9":"code","30a2aa2f":"code","14c523cc":"code","52dd04a2":"code","603a834a":"code","dceeee44":"code","11844561":"code","23d73db1":"code","bebf6031":"code","2c4b7619":"code","ce30243c":"code","3e13a4cc":"code","4fbd38f1":"code","0717fbd9":"code","daae4462":"code","5455fb3b":"code","e8d1c064":"code","680e8489":"code","65bd1475":"code","aa2296e4":"code","14a67f4c":"code","c36b4df0":"code","4dfd2e57":"code","0c40458d":"code","3409e641":"code","4f524faf":"code","a77650e4":"code","3c09e83a":"code","c2d41567":"code","9c78f705":"code","0a9750a1":"code","7dfe98b6":"code","10adca46":"code","593429d3":"code","0ea53a6b":"code","0ea089c5":"code","7f3f8fe2":"code","75405757":"code","8fc4d18b":"code","d0081bed":"code","4daab0e9":"code","421548f0":"code","c4bc819e":"code","c319286d":"code","3b589ad0":"code","30f22d7a":"code","e7474dcd":"code","5a5b8902":"code","ed5682c1":"code","42cc9bae":"code","b538b08b":"code","a04cd6c9":"code","c0b02736":"code","3ae61a99":"code","6731a099":"code","033157f7":"code","1530ef35":"code","4a6f8a35":"code","86c162f5":"code","f72ce5d3":"code","63ec7421":"code","7e4aca9a":"code","3cdd2ac9":"code","ebf00409":"code","233c4d66":"code","529f0cc5":"code","b0f4b967":"code","c5428834":"code","ccae9515":"code","840e4d5a":"code","6d31dae3":"code","98cbb87c":"code","4df2c2fc":"code","04d310b6":"code","b175cd94":"code","66efaf2b":"code","d9c18f58":"code","f5a4a75f":"code","0ec5eaf5":"code","0b302eb2":"code","7c630f08":"code","e3e5f818":"code","d5be3e12":"code","5b4c44f3":"code","8d8965e4":"code","51292e7c":"code","1dc26c2a":"code","55a99e9a":"code","3b3f87a2":"code","8e28a4d5":"code","dc438122":"code","dea96376":"code","b896a1d1":"code","384ded47":"code","8125c649":"code","073285d9":"code","3b64ab2f":"code","e7c7f738":"code","30241643":"code","69fed4bf":"code","f401d5e7":"code","cefbc726":"code","6078bb40":"code","76e5c09a":"code","b1680643":"code","6b92590d":"code","0ba2eb6a":"code","63912763":"code","67bda89c":"code","27bbe101":"code","8baef0cd":"code","3f424f4e":"code","6ffe417d":"code","fcd40440":"code","bdc7c530":"code","d0a2b891":"code","029e39d4":"code","656fd1c3":"code","2dc55be9":"code","4da67353":"code","84c39208":"code","aa515103":"code","44f4dfbb":"code","5337e147":"code","ba56ed7c":"code","2e2f973b":"code","999b454f":"code","fd3d181b":"code","0bc4c18d":"code","ac8c67a0":"code","10c5ec45":"code","905e2577":"code","4b1a803f":"code","936a6aa6":"code","986dfc24":"code","c5168963":"code","84ab9ab0":"code","55dc9eab":"code","45eab50b":"code","301e28b4":"code","a23f0b72":"code","3422dab2":"code","eec826c8":"code","ad739add":"code","3e76b2f1":"code","06b64be7":"code","e83142ce":"code","28805113":"code","305d01ee":"code","b33179fd":"code","c7471e67":"code","55d7f6d7":"code","365d1127":"code","b6bc53f2":"code","1147525d":"code","f27a98c4":"code","e1682306":"code","92f4207e":"code","3dd60d24":"code","d46df7c1":"code","0eaeddec":"code","cadd4c62":"code","158c60e2":"code","37d5e7dd":"code","5a7cc90c":"code","9416f01b":"code","6acfadde":"code","667f70f4":"code","b647d1c4":"code","90732301":"code","a069086f":"code","4cb52db5":"code","174dd18f":"code","dd2e12c1":"code","65b21963":"code","8c2fd6a6":"code","c424a207":"code","90878c8b":"code","f195fc62":"code","402382bc":"code","3f3f1e60":"code","9c2a8919":"code","71e0f373":"code","3ae39e54":"code","9101145e":"code","d8b09840":"code","3e79e047":"code","395a3c63":"code","d1bca951":"code","64ad6351":"code","b3c92ac1":"code","49804f46":"code","20e02545":"code","1fc65f2d":"code","a3bfc3dd":"code","3a49533f":"code","f93baebf":"code","7dc60711":"code","c4ae1c69":"code","78716b0e":"code","84f44e46":"code","9b1e1331":"code","2e1d378a":"code","a8fc6e7f":"code","0ecc666c":"code","441f7d25":"code","8b2f7c8c":"code","d689e8b7":"code","540c976e":"code","94c10d4a":"code","7de38562":"code","ddbde2dc":"code","021d1ebc":"code","65408d4a":"code","ab524863":"code","9de1e1c7":"code","acf60e5d":"code","72577ba3":"code","2b5b9b02":"code","dbfe5def":"code","b1d708a6":"code","d8243ddc":"code","efda397e":"code","df4fb148":"code","de2ba88a":"code","8a5195d0":"code","185b478b":"code","5f42268c":"code","eaf79061":"code","b60a3116":"code","d2291070":"code","8f84d003":"code","3f3ffa10":"markdown","8187c910":"markdown","0e4b2aaa":"markdown","b938a16c":"markdown","994da801":"markdown","29d0bd7f":"markdown","fd1c6a6c":"markdown","84b67e5e":"markdown","989af7b1":"markdown","c6fb5889":"markdown","c1e6fc2a":"markdown","03f27b4d":"markdown","797d0cbf":"markdown","8d8ec3be":"markdown","3b3c3a23":"markdown","f71989fa":"markdown","eda7accd":"markdown","dead9362":"markdown","bec433f1":"markdown","1b663b57":"markdown","7184a039":"markdown","067086c7":"markdown","739d1986":"markdown","229c2efd":"markdown","cea6abb0":"markdown","5c1a4afa":"markdown","5689f807":"markdown","d4b503b2":"markdown","7e3ce248":"markdown","217d9e32":"markdown","178ca541":"markdown","de26eac2":"markdown","b2f7048f":"markdown","9503cf93":"markdown","f83fed79":"markdown","f2402206":"markdown","b2cba0fc":"markdown","73c0d191":"markdown","328540f2":"markdown","d9d69435":"markdown","a28dcf76":"markdown","fe715574":"markdown","877b08f8":"markdown","d86b9198":"markdown","400032e3":"markdown","5bc03425":"markdown","a2d31ff1":"markdown","2ec24b37":"markdown","c3aac10c":"markdown","3f3f0f72":"markdown","fc2b3d72":"markdown","ab91f091":"markdown","949fa6de":"markdown","d98c8e9d":"markdown","54a2c1bd":"markdown","3c7f32f0":"markdown","56b08a9a":"markdown","f7dea0db":"markdown","2c66fcbb":"markdown","826eb872":"markdown","6cccdf7b":"markdown","6bf7b788":"markdown","838f106d":"markdown","6037ed91":"markdown","d3375caa":"markdown","cb503444":"markdown","f728a2dd":"markdown","46f6afa8":"markdown","ed681ae2":"markdown","9363f703":"markdown","5d291948":"markdown","fc5c6c3b":"markdown","81692d99":"markdown","2748eceb":"markdown","6b8b23b1":"markdown","46565e1f":"markdown","c87c1757":"markdown"},"source":{"73cde501":"import re\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.cluster import KMeans\n\nfrom random import randint\n\npd.set_option('display.max_colwidth', 100)\n\noption_rs = 1234  # Option random state\noption_cm = sns.light_palette('green', as_cmap=True)\noption_color = \"#5fba7d\"\noption_plot_width = 12","0c50fdd0":"def table_amount(df, col_name, is_norm, is_sort, is_all, is_empty, is_style):\n    year_key = \"year\"\n\n    if col_name not in df.columns:\n        return False\n\n    year_name = res_cols_dict.get(year_key)\n\n    if not year_name or year_name not in df.columns:\n        return False\n\n    if col_name == year_name:\n        return False\n    \n    table_index = df[col_name]\n    table_columns = df[year_name]\n\n    if is_empty == False:\n        table_index = table_index.copy().fillna('-Empty-')\n\n    if is_norm == True:\n        how_normalize = 'columns'\n    else:\n        how_normalize = False\n\n    result = pd.crosstab(table_index, table_columns,\n                         rownames=[col_name], colnames=[year_name],\n                         margins=is_all, normalize=how_normalize)\n\n    if how_normalize:\n        result = result.mul(100).round(2)\n\n    if is_sort == True and 'All' in result.columns:\n        result = result.sort_values('All', ascending=False)\n\n    if is_all == True and 'All' in result.index:\n        result = result.drop(['All'], axis=0)\n    \n    trim_to = 50\n    result.index = result.index.astype(str)\n    result.index = [x[:trim_to] + \" [...]\"\n                        if len(x) > trim_to else x\n                            for x in result.index]   \n\n    result.index.name = \"Answer\"\n    for key, value in res_cols_dict.items():\n        if col_name == value:\n            result.index.name = key\n        \n    if is_style == True and 'All' in result.columns:\n        result = result.style.bar(subset=['All'], color=option_color)\n\n    return result\n\n\ndef plot_amount(df, col_name, hue_name, is_empty, is_norm, is_sort):\n    plot_data = df.copy()\n\n    max_len_value = 30\n    plot_width = option_plot_width\n    min_height = 4\n    max_height = int(plot_width * 1.5)\n    plot_height = int(plot_data[col_name].nunique() * 0.8)\n\n    if plot_height < min_height:\n        plot_height = min_height\n    \n    if plot_height > max_height:\n        plot_height = max_height\n\n    if is_norm == True:\n        multiple = 'fill'\n    else:\n        multiple = 'stack'\n\n    if is_empty == False:\n        plot_data[col_name] = plot_data[col_name].fillna('-Empty-')\n\n    def _trim_value(x):        \n        if isinstance(x, str) and len(x) > max_len_value:\n            return x[:max_len_value] + \" [...]\"\n        else:\n            return x\n    \n    plot_data[col_name] = plot_data[col_name].apply(_trim_value)\n    \n    if is_sort == True:\n        sorter = plot_data[col_name].value_counts(ascending=False) \\\n                                    .index.to_list()\n\n        plot_data[col_name] = plot_data[col_name].astype(\"category\")\n        plot_data[col_name] = plot_data[col_name].cat.set_categories(sorter)\n        \n        plot_data = plot_data.sort_values([col_name])\n\n    hue_order = plot_data[hue_name].value_counts(ascending=False) \\\n                                   .index.to_list()\n    \n    plt.figure(figsize=(plot_width, plot_height))\n    sns.histplot(y=col_name, hue=hue_name, data=plot_data,\n                 hue_order=hue_order,\n                 multiple=multiple, shrink=.75)\n    plt.title(\"Distribution by {}\".format(hue_name))\n    plt.ylabel(\"\")\n    plt.xlabel(\"\")\n    plt.show()\n    \n\ndef plot_age(df, col_name, is_norm):\n    age_name = \"age\"\n    age_col = res_cols_dict.get(age_name)\n    \n    if not age_col:\n        return False\n    \n    if col_name not in df.columns or age_col not in df.columns:\n        return False\n    \n    plot_data = df[[age_col, col_name]].copy()\n\n    max_len_value = 50\n    plot_width = option_plot_width\n    min_height = 4\n    max_height = int(plot_width * 1.5)\n    plot_height = int(plot_data[col_name].nunique() * 0.8)\n\n    if plot_height < min_height:\n        plot_height = min_height\n    \n    if plot_height > max_height:\n        plot_height = max_height\n\n    if is_norm == True:\n        multiple = 'fill'\n    else:\n        multiple = 'stack'\n\n    def _trim_value(x):        \n        if isinstance(x, str) and len(x) > max_len_value:\n            return x[:max_len_value] + \" [...]\"\n        else:\n            return x\n    \n    plot_data[col_name] = plot_data[col_name].apply(_trim_value)\n\n    hue_order = plot_data[col_name].value_counts(ascending=False) \\\n                                   .index.to_list()\n\n    plt.figure(figsize=(plot_width, plot_height))\n    sns.histplot(x=age_col, hue=col_name, data=plot_data.sort_values(age_col),\n                 hue_order=hue_order,\n                 multiple=multiple, shrink=.75)\n    plt.title(\"Distribution by {}\".format(age_name))\n    plt.ylabel(\"\")\n    plt.xlabel(\"\")\n    plt.show()\n    \n    \ndef agender(df, saga_name, type_ge=2):\n    age_name = \"age\"\n    gender_name = \"gender\"\n    country_name = \"country\"\n    \n    age_col = res_cols_dict.get(age_name)    \n    gender_col = res_cols_dict.get(gender_name)\n    country_col = res_cols_dict.get(country_name)\n\n    if saga_name in [age_name, gender_name, age_col, gender_col]:\n        return False\n\n    mask_gender = df[gender_col].isin(['Man', 'Woman'])\n    data = df.loc[mask_gender, :].copy()\n\n    if saga_name == country_name:\n        indx_table = data[country_col]\n        indx_name = [country_name]\n\n    elif 'Clusters' in saga_name:\n        if saga_name not in data.columns:\n            return False\n        else:\n            indx_table = data[saga_name]\n            indx_name = [saga_name]\n\n    elif 'SA' in saga_name:\n        if saga_name not in data.columns:\n            return False\n        else:\n            indx_table = data[saga_name]\n            indx_name = [saga_name]\n    \n    elif 'GA' in saga_name:\n        group_cols = data.filter(like=(saga_name + \"_\")).columns.to_list()\n        \n        if not group_cols:\n            return False\n        \n        data = pd.melt(data, id_vars=[gender_col, age_col], value_vars=group_cols)\n        \n        data = data.drop('variable', axis=1).rename(columns={'value': saga_name})\n        \n        indx_table = data[saga_name]\n        indx_name = [saga_name]\n    \n    else:\n        return False\n\n    if type_ge == 1:\n        cols_table = [data[age_col], data[gender_col]]\n        cols_names = [age_name, gender_name]\n        \n    elif type_ge == 2:\n        cols_table = [data[gender_col], data[age_col]]\n        cols_names = [gender_name, age_name]\n        \n    else:\n        return False\n\n    \n    result = pd.crosstab(index=indx_table, columns=cols_table,\n                         rownames=indx_name, colnames=cols_names,\n                         margins=True)\n    \n    result.index = result.index.astype(str)\n    trim_to = 45\n    result.index = [x[:trim_to] + \" [...]\"\n                        if len(x) > trim_to else x\n                            for x in result.index]\n\n    result.index.name = \"Answer\"\n\n    all_col = \"All\"\n    if all_col in result.columns:\n        result = result.sort_values(all_col, ascending=False)\n\n        result = result.drop(columns=all_col, level=0)\n\n        if all_col in result.index:\n            result = result.drop(index=all_col)\n\n    result = result.style.background_gradient(cmap=option_cm, axis=None)\n    \n    return result\n\n\ndef edugender(df, col_name, type_ge):\n    edu_col = \"SA4\"\n    edu_name = \"Education\"\n    gender_name = \"gender\"    \n    gender_col = res_cols_dict.get(gender_name)\n\n    if col_name not in df.columns:\n        return False\n\n    if col_name  == edu_col or col_name == gender_col:\n        return False\n\n    if edu_col not in df.columns or gender_col not in df.columns:\n        return False\n    \n    degree_list = [\"Bachelor\u2019s degree\", \"Master\u2019s degree\", \"Doctoral degree\"]\n    mask_edu = df[edu_col].isin(degree_list)\n    mask_gender = df[gender_col].isin(['Man', 'Woman'])\n    \n    mask_all = (mask_gender & mask_edu)\n\n    cols_list = [edu_col, gender_col, col_name]\n\n    data = df.loc[mask_all, cols_list].copy()\n\n    data[edu_col] = data[edu_col].astype(\"category\")\n    data[edu_col] = data[edu_col].cat.set_categories(degree_list)\n    data = data.sort_values([edu_col])\n    \n    if type_ge == 1:\n        cols_table = [data[edu_col], data[gender_col]]\n        cols_names = [edu_name, gender_name]\n        \n    elif type_ge == 2:\n        cols_table = [data[gender_col], data[edu_col]]\n        cols_names = [gender_name, edu_name]\n        \n    else:\n        return False\n\n    indx_table = data[col_name]\n    indx_name = [col_name]\n    \n    result = pd.crosstab(index=indx_table, columns=cols_table,\n                         rownames=indx_name, colnames=cols_names,\n                         margins=True)\n\n    trim_to = 45\n    result.index = result.index.astype(str)\n    result.index = [x[:trim_to] + \" [...]\"\n                        if len(x) > trim_to else x\n                            for x in result.index]\n    result.index.name = \"Answer\"\n\n    all_col = \"All\"\n    if all_col in result.columns:\n        result = result.sort_values(all_col, ascending=False)\n\n        result = result.drop(columns=all_col, level=0)\n\n        if all_col in result.index:\n            result = result.drop(index=all_col)\n\n    result = result.style.background_gradient(cmap=option_cm, axis=None)\n    \n    return result\n\n\ndef transform_cols(df, method, is_drop_first=False):\n    if isinstance(df, pd.Series):\n        df = pd.DataFrame(df)\n    \n    cols_list = [col for col in df.columns\n                 if df[col].dtype == 'object']\n    \n    if not cols_list:\n        return None\n    \n    data = df[cols_list].copy()\n    \n    methods = ['onehot', 'bin', 'cols', 'rows']\n    \n    if not method or method not in methods:\n        return methods\n    \n    if method == 'onehot':\n        cols_list = data.columns.to_list()\n        \n        if is_drop_first != True:\n            is_drop_first = False\n        \n        data = pd.get_dummies(data, prefix=cols_list, drop_first=is_drop_first)\n    \n    if method == 'bin':\n        data = data.notna().astype(int)\n    \n    if method == 'cols':\n        for col in data.columns:\n            repl_dict = data[col].value_counts(normalize=True).to_dict()\n            data[col] = data[col].replace(repl_dict).fillna(0)\n\n    if method == 'rows':\n        data = data.notna().astype(int)\n        data = data.div(data.sum(axis=1), axis=0).fillna(0)\n        \n    return data","7ff20750":"def get_columns(df, select_by=None):\n    if not select_by:\n        multiple_cols = df.filter(like='_').columns.tolist()\n\n        multiple_groups = []\n        for x_col in multiple_cols:\n            group_split = re.findall('_Part|_OTHER', x_col)[0]\n            found_group = x_col.split(group_split)[0]\n\n            if found_group not in multiple_groups:\n                multiple_groups.append(found_group)\n\n        single_cols = [col for col in df.columns\n                          if col not in multiple_cols\n                           and \"Q\" in col]  # skip Duration column\n\n    if select_by == \"SAGA\":\n        single_cols = df.filter(like=\"SA\").columns.to_list()\n        multiple_cols = df.filter(like=\"GA\").columns.to_list()\n\n        multiple_groups = []\n        for multiple_col in multiple_cols:\n            found_group = multiple_col.split(\"_\")[0]\n            if found_group not in multiple_groups:\n                multiple_groups.append(found_group)\n    \n    return single_cols, multiple_cols, multiple_groups","5862a516":"# Dataset 2018-2021\npath_to_data = \"..\/input\/dataset-kaggle-survey-2018-2021\/kaggle_survey_2018-2021_data.csv\"\ndata = pd.read_csv(path_to_data, low_memory=False)\n\npath_to_data_header = \"..\/input\/dataset-kaggle-survey-2018-2021\/kaggle_survey_2018-2021_header.csv\"\ndata_header = pd.read_csv(path_to_data_header)\ndata_description = data_header.loc[0].to_dict()\n\nsingle_cols, multiple_cols, multiple_groups = get_columns(data, select_by='SAGA')\n\n# Dataset 2021\npath_to_data_21 = \"..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv\"\ndata_21 = pd.read_csv(path_to_data_21, low_memory=False)\n\nsingle_cols_21, multiple_cols_21, multiple_groups_21 = get_columns(data_21)","0e225aff":"def df_diff():\n    result = pd.DataFrame()\n\n    if single_cols_21 and multiple_cols_21 and multiple_groups_21:\n        _ = [len(single_cols_21), len(multiple_groups_21), len(multiple_cols_21)]\n        result['data_21'] = pd.DataFrame(_)\n\n    if single_cols and multiple_cols and multiple_groups:\n        _ = [len(single_cols), len(multiple_groups), len(multiple_cols)]\n        result['data'] = pd.DataFrame(_)\n       \n    result.index = ['single columns', 'multiple groups', 'multiple columns']\n\n    return result\n\n\ndef df_fullness(df, cols_list, is_stats=True):\n    if is_stats == True:\n        df_shape = df.shape\n        print(\"\\nRows: {}\".format(df_shape[0]))\n\n        year_stats = pd.DataFrame(df['Year'].value_counts(sort=False))\n        print(\"\\n{}\\n\".format(year_stats))\n    \n    fullness = pd.DataFrame()\n    \n    fullness['Fullness'] = df[cols_list].notna().mean() \\\n                        .mul(100).round(2).apply(\"{}%\".format)\n    \n    if data_description:\n        fullness['Questions'] = pd.Series(data_description)\n\n    return fullness","3aaea281":"df_diff()","79a494a5":"df_fullness(data, single_cols)","ba6e5a2c":"def get_notna(df, cols_list, less_than=None):\n    result = df[cols_list].notna().sum(axis=1)\n    \n    if less_than:\n        result = (result < less_than)  # mask\n    \n    return result\n\n\ndef df_notna_stats(df, cols_list, per_list=None, is_style=True):\n    if not per_list:\n        per_list = [.1, .25, .5, .75, .9]\n\n    result = df[cols_list].notna().sum(axis=1) \\\n                          .groupby(df['Year']) \\\n                          .describe(percentiles=per_list)\n\n    result = result.drop(['count', 'mean', 'std'], axis=1).astype(int)\n\n    if is_style == True:\n        result = result.style.background_gradient(subset=['min', 'max'], cmap=option_cm)\n    \n    return result\n\n\ndef df_notna_table(df, cols_list, is_norm=False, is_sort=False, is_all=False):\n    num_notna = get_notna(data, cols_list)\n    year_data = data['Year']\n    \n    if is_norm == True:\n        how_normalize = 'index'\n    else:\n        how_normalize = False\n        \n    result = pd.crosstab(year_data, num_notna, colnames=['Number notna'],\n                         margins=is_all, normalize=how_normalize)\n\n    if how_normalize:\n        result = result.mul(100).round(2)\n    \n    if is_sort == True and 'All' in result.columns:\n        result = result.sort_values('All', ascending=False)        \n    \n    return result","eba22f0f":"df_notna_stats(data, single_cols)","2ec3443e":"check_notna = df_notna_table(data, single_cols)\ncheck_notna","fabd7438":"plt.figure(figsize=(12, 4))\nsns.heatmap(check_notna, cmap=option_cm)\nplt.show()","52c4b2da":"plt.figure(figsize=(12, 4))\nsns.heatmap(check_notna, cmap=option_cm,\n            vmin=0, vmax=1, linewidths=.5)\nplt.show()","07ba69df":"number_notna = get_notna(data, single_cols)","62ea3f78":"data.loc[number_notna == 3,\n         single_cols].sample(3, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","2c774d2b":"data.loc[number_notna == 4,\n         single_cols].sample(3, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","df4486a9":"data.loc[number_notna == 6,\n         single_cols].sample(3, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","277add9b":"# df_notna_table(data, single_cols, is_all=True)","997f140e":"is_mute = get_notna(data, single_cols, less_than=7)\n\ndata.loc[is_mute,\n         [\"Year\"] + single_cols].sample(5, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","b2c4c96a":"data.loc[is_mute, \"Year\"].value_counts(sort=False)","1bfb38c0":"def df_duration_stats(df, per_list=None, is_style=True):\n    if not per_list:\n        per_list = [.1, .25, .5, .75, .9]\n        \n    result = df.groupby('Year')['Duration'].describe(percentiles=per_list)\n    result = result.drop(['count', 'mean', 'std'], axis=1)\n    \n    if is_style == True:\n        result = result.style.background_gradient(subset=['min'], cmap=option_cm)\n\n    return result","ae8e4cd7":"df_duration_stats(data)","ecd84de6":"check_duration = data['Duration'] < 40  # minutes\nplot_data = data.loc[check_duration]\n\nplt.figure(figsize=(12, 6))\nsns.histplot(data=plot_data, x=\"Duration\", hue=\"Year\", binwidth=.5)\nplt.ylabel(\"\")\nplt.xlabel(\"\")\nplt.show()","a027b193":"g = sns.FacetGrid(plot_data, col=\"Year\", col_wrap=2,\n                  height=3.5, aspect=1.5)\ng.map(sns.histplot, \"Duration\", binwidth=.5)\ng.set_axis_labels(\"\", \"\");","3c37fb93":"df_duration_stats(data[~is_mute], is_style=False)","34034485":"is_fast = (data['Duration'] < 2)  # less than X minutes\n\ndata.loc[is_fast,\n         single_cols].sample(3, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","71afc92e":"data.loc[is_fast, \"Year\"].value_counts(sort=False)","62c1e293":"is_normal = (~is_fast & ~is_mute)\n\n# Check not normal\ndata.loc[~is_normal,\n         ['Duration', 'Year'] + single_cols].sample(7, random_state=option_rs) \\\n        .style.highlight_null(null_color='grey')","96c2a939":"def df_segments():\n    concat_data = []\n    concat_cols = []\n    x_col = \"Year\"\n\n    concat_data.append(data[x_col].value_counts(sort=False))\n    concat_cols.append('Origin')\n\n    concat_data.append(data.loc[is_mute, x_col].value_counts(sort=False))\n    concat_cols.append('Mute')\n\n    concat_data.append(data.loc[is_fast, x_col].value_counts(sort=False))\n    concat_cols.append('Fast')\n\n    concat_data.append(data.loc[~is_normal, x_col].value_counts(sort=False))\n    concat_cols.append('Not normal')\n\n    concat_data.append(data.loc[is_normal, x_col].value_counts(sort=False))\n    concat_cols.append('Normal')\n    \n    result = pd.concat(concat_data, keys=concat_cols, axis=1).fillna(0).astype(int)\n    \n    if 'Not normal' in result.columns:\n        result = result.style.text_gradient(subset=['Not normal'],\n                                            cmap=\"brg\", low=0.5, high=1.0)\n    \n    return result","e1f631ca":"df_segments()","1a336196":"res_cols_dict = {'year': 'Year',\n                 'age': 'SA1',\n                 'gender': 'SA2',\n                 'country': 'SA3'}","933c4d57":"res_cols = list(res_cols_dict.values())\nres_data = data.loc[is_normal, res_cols].copy()\n\nprint(res_data.shape)","ae4990e3":"def res_amount(type_info=None, is_norm=False, is_sort=True, is_all=True, is_empty=True, is_style=True):\n    df = res_data\n    \n    if not type_info:\n        return list(res_cols_dict.keys())[1:]\n\n    col_name = res_cols_dict.get(type_info)\n    \n    if not col_name:\n        return False\n    \n    return table_amount(df, col_name, is_norm, is_sort, is_all, is_empty, is_style)\n\n\ndef res_trim_to(n_labels, type_info):\n    df = res_data\n\n    if type_info == 'year':\n        return False\n\n    col_name = res_cols_dict.get(type_info)\n    \n    if not col_name or col_name not in df.columns:\n        return False\n    \n    if n_labels >= df[col_name].nunique():\n        return None\n    \n    all_labels = pd.DataFrame(df[col_name].value_counts()).T\n    \n    other_name = \"Other\"\n    if other_name in all_labels.columns:\n        _ = all_labels.pop(other_name)\n        \n    popular_labels = all_labels.iloc[:, :n_labels -1].columns.to_list()\n    other_labels = [x_labels for x_labels in all_labels\n                        if x_labels not in popular_labels]\n    \n    df[col_name].replace(other_labels, other_name, inplace=True)\n    \n    return True\n\n\ndef res_trim_after(x_age, type_info):\n    df = res_data\n\n    if type_info == 'year':\n        return False\n    \n    col_name = res_cols_dict.get(type_info)\n    \n    if not col_name or col_name not in df.columns:\n        return False\n\n    replace_x_age = str(x_age) + \"+\"\n\n    if replace_x_age in df[col_name].unique():\n        return None\n    \n    mask_age = df[col_name].str.slice(stop=2).astype(int).ge(x_age)\n\n    df[col_name].mask(mask_age, replace_x_age, inplace=True)\n    \n    return True\n\n\ndef res_agender(type_ge=2):\n    data = res_data\n    saga_name = \"country\"\n    \n    return agender(data, saga_name, type_ge)","2fc9d480":"res_amount('gender')","ad1d4be8":"res_trim_to(3, 'gender')","f255e15a":"res_amount('gender')","5f43570b":"res_amount('country')","3c089e2f":"res_trim_to(10, 'country')","c29c02fc":"res_amount('country')","9928aff1":"res_amount('age', is_sort=False)","e4d122fa":"res_trim_after(60, 'age')","a3593359":"res_amount('age', is_sort=False)","f6b449fc":"res_agender()","91d26282":"def res_plot_amount(type_info, hue_info, is_empty=True, is_norm=True, is_sort=False):\n    if type_info == hue_info:\n        return False\n    \n    df = res_data\n\n    col_name = res_cols_dict.get(type_info)\n    hue_col = res_cols_dict.get(hue_info)\n    \n    if not col_name or not hue_col:\n        return False\n    \n    plot_data = df[[col_name, hue_col]].copy() \\\n                .rename(columns={col_name: type_info,\n                                 hue_col: hue_info})\n    \n    if type_info in ['age', 'year']:\n        plot_data = plot_data.sort_values(by=type_info)\n\n    if type_info == 'year':\n        plot_data[type_info] = plot_data[type_info].astype(str)\n\n    return plot_amount(plot_data, type_info, hue_info, is_empty, is_norm, is_sort)","83e340a7":"res_plot_amount('age', 'gender', is_norm=False)","0b8a7fdd":"res_plot_amount('age', 'gender')","47ad82ee":"res_plot_amount('age', 'country')","4facdc88":"res_plot_amount('country', 'gender', is_sort=True)","1a37126e":"res_plot_amount('country', 'year', is_sort=True)","d382e608":"def clean_money_cols(df, cols_list):\n    result = []\n    for col in cols_list:\n        if col not in df.columns:\n            continue\n        \n        money_abbr = [\"\\$\", \"\\(USD\\)\"]\n        for abbr in money_abbr:\n            if df[col].str.contains(abbr).any():\n                df[col] = df[col].replace(regex=abbr, value=\"\")\n                df[col] = df[col].str.strip()\n                \n                if col not in result:\n                    result.append(col)\n    \n    if result:\n        return result\n\n\ndef sa_info(df, cols_list, is_style=True):\n    sa_isna = df.loc[: , cols_list].isna()\n\n    sa_isna_sum = sa_isna.sum().sum()    \n    print(\"\\n{} empty cells\\n\".format(sa_isna_sum))\n    \n    result = pd.DataFrame()\n    result['Empty'] = sa_isna.mean().mul(100).round(2).apply(\"{}%\".format)\n\n    result['Unique'] = df.loc[: , cols_list].nunique()\n\n    if data_description:\n        result['Questions'] = pd.Series(data_description)\n\n    result = result.reset_index().rename(columns={'index': 'Column'})\n\n    if is_style == True and 'Unique' in result.columns:\n        \"\"\"\n        result = result.style.text_gradient(subset=['Unique'],\n                                            cmap=option_cm,\n                                            low=0.75, high=1.0)\n        \"\"\"\n        result = result.style.background_gradient(subset=['Unique'],\n                                                  cmap=option_cm)\n    \n    return result","cda3b35d":"def answers_corr(df, upd='cols', method=None, plot_width=12):\n    if not method:\n        method = 'pearson'\n\n    methods = ['pearson', 'kendall', 'spearman']\n    \n    if method not in methods:\n        return methods\n\n    data = transform_cols(df, upd)\n    \n    if isinstance(data, list):\n        return data\n    \n    corr_data = data.corr(method) \\\n                    .dropna(axis=0, how='all').dropna(axis=1, how='all')\n    \n    corr_len = len(corr_data.columns.to_list())\n    plot_height = int(corr_len * 0.8)\n\n    min_height = 6\n    max_height = plot_width\n\n    if plot_height < min_height:\n        plot_height = min_height\n    \n    if plot_height > max_height:\n        plot_height = max_height\n    \n    grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .2}\n\n    f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws,\n                                    figsize=(plot_width, plot_height))\n\n    ax = sns.heatmap(corr_data, annot=True, linewidths=.5, cmap=\"YlGnBu\",\n                     ax=ax, cbar_ax=cbar_ax,\n                     cbar_kws={\"orientation\": \"horizontal\"})","100d31af":"sa_cols = [col for col in single_cols\n               if col not in res_data.columns]\n\nsa_data = res_data.join(data.loc[res_data.index, sa_cols])","218e4a80":"clean_money_cols(sa_data, sa_cols)  # deleted text: '$' or '(USD)'","fd08d105":"sa_info(sa_data, sa_cols)","437110cb":"# answers_corr(sa_data[sa_cols])","01e70eb5":"def sa_amount(col_name, is_norm=False, is_sort=True, is_all=True, is_empty=True, is_style=True):    \n    df = sa_data\n    \n    if col_name not in df.columns:\n        return False\n\n    print(\"\\n>>> ({}) {}\\n\".format(col_name, data_description.get(col_name)))\n\n    return table_amount(df, col_name, is_norm, is_sort, is_all, is_empty, is_style)   \n\n\ndef sa_trim_to(n_labels, col_name):\n    df = sa_data\n\n    if col_name not in df.columns:\n        return False\n    \n    if n_labels >= df[col_name].nunique():\n        return None\n    \n    all_labels = pd.DataFrame(df[col_name].value_counts()).T\n    \n    other_name = \"Other\"\n    if other_name in all_labels.columns:\n        _ = all_labels.pop(other_name)\n\n    popular_labels = all_labels.iloc[:, :n_labels -1].columns.to_list()\n    other_labels = [x_labels for x_labels in all_labels\n                        if x_labels not in popular_labels]\n    \n    df[col_name].replace(other_labels, other_name, inplace=True)\n    \n    return True\n\n\ndef sa_plot_amount(col_name, type_info, is_empty=True, is_norm=True, is_sort=True):\n    df = sa_data\n\n    hue_col = res_cols_dict.get(type_info)\n\n    if col_name not in df.columns or not hue_col:\n        return False\n    \n    plot_data = df[[col_name, hue_col]].copy() \\\n                                       .rename(columns={hue_col: type_info})\n\n    return plot_amount(plot_data, col_name, type_info, is_empty, is_norm, is_sort)\n\n\ndef sa_plot_age(col_name, is_norm=True):\n    df = sa_data\n\n    if col_name not in df.columns:\n        return False\n    \n    return plot_age(df, col_name, is_norm)\n\n    \ndef sa_agender(saga_name, type_ge=2):\n    data = sa_data\n    \n    return agender(data, saga_name, type_ge)\n\n\ndef sa_edugender(col_name, type_ge=2):\n    df = sa_data\n\n    if col_name not in df.columns:\n        return False\n    \n    return edugender(df, col_name, type_ge)","f5457b01":"locals().update({x.lower(): x for x in sa_cols})\n\nprint(sa4, \"...\", sa20)","8717bab4":"sa_amount(sa4)","ffa33324":"sa_trim_to(4, sa4)","a8ed955a":"sa_amount(sa4)","f6f3246e":"sa_agender(sa4)","e2ff0e7d":"sa_plot_amount(sa4, 'gender', is_sort=False)","d2d28943":"sa_plot_amount(sa4, 'country', is_sort=False)","0b578885":"sa_plot_age(sa4)","94ac2456":"sa_amount(sa5)","3e41da07":"sa_trim_to(8, sa5)","8236504b":"sa_agender(sa5)","9421ead2":"sa_edugender(sa5)","cf9d7b60":"sa_plot_amount(sa5, 'year')","f9383546":"sa_plot_age(sa5)","5cd86f3d":"sa_amount(sa6)","d4f089bc":"sa_agender(sa6)","cebfc721":"sa_edugender(sa6)","177f89e2":"sa_plot_amount(sa6, 'gender')","9898d8cb":"sa_plot_age(sa6)","fc5ea9ef":"sa_amount(sa7, is_empty=False)","43aac02c":"sa_amount(sa7)","458691e9":"sa_trim_to(5, sa7)","30a2aa2f":"sa_agender(sa7)","14c523cc":"sa_edugender(sa7)","52dd04a2":"sa_plot_amount(sa7, 'country')","603a834a":"sa_plot_age(sa7)","dceeee44":"sa_amount(sa8, is_empty=False)","11844561":"sa_amount(sa8)","23d73db1":"sa_trim_to(4, sa8)","bebf6031":"sa_agender(sa8)","2c4b7619":"sa_edugender(sa8)","ce30243c":"sa_plot_amount(sa8, 'gender')","3e13a4cc":"sa_plot_age(sa8)","4fbd38f1":"sa_amount(sa9, is_empty=False)","0717fbd9":"sa_amount(sa9)","daae4462":"sa_agender(sa9)","5455fb3b":"sa_edugender(sa9)","e8d1c064":"sa_plot_amount(sa9, 'year')","680e8489":"sa_plot_age(sa9)","65bd1475":"sa_amount(sa10, is_empty=False)","aa2296e4":"sa_agender(sa10)","14a67f4c":"sa_edugender(sa10)","c36b4df0":"sa_plot_amount(sa10, 'gender', is_empty=False)","4dfd2e57":"sa_plot_age(sa10)","0c40458d":"sa_amount(sa11)","3409e641":"sa_trim_to(8, sa11)","4f524faf":"sa_agender(sa11)","a77650e4":"sa_edugender(sa11)","3c09e83a":"sa_plot_amount(sa11, 'year')","c2d41567":"sa_plot_age(sa11)","9c78f705":"sa_amount(sa12)","0a9750a1":"sa_agender(sa12)","7dfe98b6":"sa_edugender(sa12)","10adca46":"sa_plot_amount(sa12, 'country')","593429d3":"sa_plot_age(sa12)","0ea53a6b":"sa_amount(sa13, is_empty=False)","0ea089c5":"sa_agender(sa13)","7f3f8fe2":"sa_edugender(sa13)","75405757":"sa_plot_age(sa13)","8fc4d18b":"sa_amount(sa14, is_empty=False)","d0081bed":"sa_agender(sa14)","4daab0e9":"sa_edugender(sa14)","421548f0":"sa_plot_amount(sa14, 'year', is_empty=False)","c4bc819e":"sa_plot_amount(sa14, 'country', is_empty=False)","c319286d":"sa_plot_age(sa14)","3b589ad0":"sa_amount(sa15)","30f22d7a":"sa_agender(sa15)","e7474dcd":"sa_edugender(sa15)","5a5b8902":"sa_plot_amount(sa15, 'country')","ed5682c1":"sa_plot_age(sa15)","42cc9bae":"sa_amount(sa16)","b538b08b":"sa_agender(sa16)","a04cd6c9":"sa_edugender(sa16)","c0b02736":"sa_plot_amount(sa16, 'country')","3ae61a99":"sa_plot_age(sa16)","6731a099":"sa_amount(sa17, is_empty=False)","033157f7":"sa_amount(sa17)","1530ef35":"sa_trim_to(6, sa17)","4a6f8a35":"sa_agender(sa17)","86c162f5":"sa_edugender(sa17)","f72ce5d3":"sa_plot_amount(sa17, 'country')","63ec7421":"sa_plot_age(sa17)","7e4aca9a":"sa_amount(sa18)","3cdd2ac9":"sa_trim_to(8, sa18)","ebf00409":"sa_agender(sa18)","233c4d66":"sa_edugender(sa18)","529f0cc5":"sa_plot_amount(sa18, 'country')","b0f4b967":"sa_plot_age(sa18)","c5428834":"sa_amount(sa19)","ccae9515":"sa_trim_to(4, sa19)","840e4d5a":"sa_agender(sa19)","6d31dae3":"sa_edugender(sa19)","98cbb87c":"sa_plot_amount(sa19, 'country')","4df2c2fc":"sa_plot_amount(sa19, 'gender')","04d310b6":"sa_plot_age(sa19)","b175cd94":"sa_amount(sa20)","66efaf2b":"sa_agender(sa20)","d9c18f58":"sa_edugender(sa20)","f5a4a75f":"sa_plot_amount(sa20, 'year', is_empty=False)","0ec5eaf5":"sa_plot_age(sa20)","0b302eb2":"def get_ga_info(df):\n    df_cols = df.shape[1]\n    df_size = df.size\n    df_count = df.count().sum()\n    fullness = round((df_count * 100) \/ df_size, 2)\n    fullness = \"{}%\".format(fullness)\n    \n    emptiness = round(df.isna().mean().mean() * 100, 2)\n    emptiness = \"{}%\".format(emptiness)\n\n    df_info = {'Cols': [df_cols], 'Empty': [emptiness]}\n\n    result = pd.DataFrame(df_info)\n\n    df_stats = pd.DataFrame(df.notna().sum(axis=1) \\\n                            .describe()).T \\\n                            .loc[:, 'min':'max'].astype(int)\n    \n    result = result.join(df_stats)\n    \n    return result\n\n\ndef ga_info(df, groups_list, is_style=True):\n    \n    result = pd.DataFrame()\n    \n    for group_name in groups_list:\n        group_data = df.filter(like=(group_name + \"_\"))\n\n        # Group title\n        cols_list = group_data.columns.to_list()\n        first_question = data_description.get(cols_list[0])\n        group_select = '(Select all that apply)'\n        group_title = first_question.split(group_select)[0]\n\n        # Group info\n        group_result = get_ga_info(group_data)\n        group_result['Question'] = group_title\n        group_result.index = [group_name]\n        \n        result = result.append(group_result)\n\n    if is_style == True:\n        color_cols = result.loc[:, 'min':'max'].columns.to_list()\n        \"\"\"\n        result = result.style.text_gradient(subset=color_cols,\n                                            cmap=option_cm,\n                                            low=0.75, high=1.0)\n        \"\"\"\n        result = result.style.background_gradient(subset=color_cols,\n                                                  cmap=option_cm,\n                                                  low=0.3, high=1.0,\n                                                  axis=None)\n\n    return result","7c630f08":"ga_data = res_data.join(data.loc[res_data.index, multiple_cols])\n\nprint(ga_data.shape)","e3e5f818":"ga_info(ga_data, multiple_groups)","d5be3e12":"def ga_amount(group_name, is_norm=False, is_sort=True, is_all=True, is_empty=True, is_style=True):\n    df = ga_data\n    \n    if group_name not in multiple_groups:\n        return False\n\n    cols_list = df.filter(like=(group_name + \"_\")).columns.to_list()\n\n    first_question = data_description.get(cols_list[0])\n    group_select = '(Select all that apply)'\n    group_title = first_question.split(group_select)[0]\n\n    year_col = res_cols_dict.get('year')\n\n    group_data = pd.melt(df, id_vars=[year_col],\n                         value_vars=cols_list,\n                         value_name=group_name)\n\n    print(\"\\n>>> ({}) {}\\n\".format(group_name, group_title))\n\n    return table_amount(group_data, group_name, is_norm, is_sort, is_all, is_empty, is_style)\n\n\ndef ga_corr(group_name, upd='rows', method=None, plot_width=12):\n    df = ga_data\n\n    if group_name not in multiple_groups:\n        return False\n\n    group_data = df.filter(like=(group_name + \"_\"))\n    \n    return answers_corr(group_data, upd, method, plot_width)\n\n\ndef ga_describe(group_name):\n    df = ga_data\n\n    if group_name not in multiple_groups:\n        return False\n\n    group_data = df.filter(like=(group_name + \"_\"))\n    \n    return group_data.describe()\n\n\ndef ga_trim_to(n_labels, group_name):\n    df = ga_data\n\n    if group_name not in multiple_groups:\n        return False\n\n    group_data = df.filter(like=(group_name + \"_\"))\n    \n    if n_labels >= len(group_data.columns):\n        return None\n\n    group_stats = group_data.describe().loc[['top', 'count'], :].T \\\n                            .sort_values(by='count', ascending=False)\n    \n    other_name = \"Other\"\n    is_other = group_stats['top'].str.contains(other_name).values\n    \n    cols_list = group_stats.loc[~is_other, 'top'].index.to_list()\n    other_col = group_stats.loc[is_other, 'top'].index.to_list()  # list[0]\n    \n    trim_list = cols_list[ n_labels - 1 : ]\n    \n    if not other_col:\n        other_col = group_name + '_XXX'\n        df[other_col] = np.nan\n    else:\n        other_col = other_col[0]\n    \n    for x_col in trim_list:\n        mask_notna = df[x_col].notna()\n        df[x_col].mask(mask_notna, other_name, inplace=True)\n\n        df[other_col].fillna(df[x_col], inplace=True)\n        df.drop([x_col], axis=1, inplace=True)\n        \n    return True\n\n\ndef ga_plot_amount(group_name, type_info, is_empty=True, is_norm=True, is_sort=True):\n    df = ga_data\n\n    hue_col = res_cols_dict.get(type_info)\n\n    if group_name not in multiple_groups or not hue_col:\n        return False\n            \n    cols_list = df.filter(like=(group_name + \"_\")).columns.to_list()\n\n    plot_data = pd.melt(df, id_vars=[hue_col], value_vars=cols_list,\n                                               value_name=group_name) \\\n                            .rename(columns={hue_col: type_info})\n\n    return plot_amount(plot_data, group_name, type_info, is_empty, is_norm, is_sort)\n\n\ndef ga_plot_age(group_name, is_norm=True):\n    df = ga_data\n\n    if group_name not in multiple_groups:\n        return False\n\n    cols_list = df.filter(like=(group_name + \"_\")).columns.to_list()\n\n    age_name = \"age\"\n    age_col = res_cols_dict.get(age_name)\n\n    group_data = pd.melt(df, id_vars=[age_col], value_vars=cols_list,\n                                               value_name=group_name)\n\n    return plot_age(group_data, group_name, is_norm)\n\n\ndef ga_agender(saga_name, type_ge=2):\n    df = ga_data\n    \n    return agender(df, saga_name, type_ge)\n\n\ndef ga_edugender(group_name, type_ge=2):\n    if group_name not in multiple_groups:\n        return False\n\n    group_cols = ga_data.filter(like=(group_name + \"_\")).columns.to_list()\n\n    edu_col = \"SA4\"\n    gender_col = res_cols_dict.get('gender')\n    \n    info_cols = [gender_col, edu_col]\n\n    df = pd.concat([sa_data[info_cols], ga_data[group_cols]], axis=1)\n    \n    group_data = pd.melt(df, id_vars=[edu_col, gender_col],\n                         value_vars=group_cols, value_name=group_name)\n\n    return edugender(group_data, group_name, type_ge)","5b4c44f3":"locals().update({x.lower(): x for x in multiple_groups})\n\nprint(ga0, ga2, \"...\")\nprint(*multiple_groups)","8d8965e4":"ga_amount(ga0)","51292e7c":"ga_trim_to(9, ga0)","1dc26c2a":"ga_amount(ga0)","55a99e9a":"ga_agender(ga0)","3b3f87a2":"ga_edugender(ga0)","8e28a4d5":"ga_plot_amount(ga0, 'gender')","dc438122":"ga_plot_amount(ga0, 'country')","dea96376":"ga_plot_age(ga0)","b896a1d1":"ga_amount(ga1)","384ded47":"ga_trim_to(10, ga1)","8125c649":"ga_agender(ga1)","073285d9":"ga_edugender(ga1)","3b64ab2f":"ga_plot_amount(ga1, 'gender')","e7c7f738":"ga_plot_amount(ga1, 'year')","30241643":"ga_plot_age(ga1)","69fed4bf":"ga_amount(ga2)","f401d5e7":"ga_trim_to(6, ga2)","cefbc726":"ga_agender(ga2)","6078bb40":"ga_edugender(ga2)","76e5c09a":"ga_plot_amount(ga2, 'country')","b1680643":"ga_plot_amount(ga2, 'year')","6b92590d":"ga_plot_age(ga2)","0ba2eb6a":"ga_amount(ga3)","63912763":"ga_agender(ga3)","67bda89c":"ga_edugender(ga3)","27bbe101":"ga_plot_amount(ga3, 'gender')","8baef0cd":"ga_plot_amount(ga3, 'country')","3f424f4e":"ga_plot_age(ga3)","6ffe417d":"ga_amount(ga4)","fcd40440":"ga_trim_to(7, ga4)","bdc7c530":"ga_agender(ga4)","d0a2b891":"ga_edugender(ga4)","029e39d4":"ga_plot_amount(ga4, 'year')","656fd1c3":"ga_plot_age(ga4)","2dc55be9":"ga_amount(ga5)","4da67353":"ga_trim_to(8, ga5)","84c39208":"ga_agender(ga5)","aa515103":"ga_edugender(ga5)","44f4dfbb":"ga_plot_amount(ga5, 'gender')","5337e147":"ga_plot_age(ga5)","ba56ed7c":"ga_amount(ga6)","2e2f973b":"ga_trim_to(10, ga6)","999b454f":"ga_agender(ga6)","fd3d181b":"ga_edugender(ga6)","0bc4c18d":"ga_plot_amount(ga6, 'country')","ac8c67a0":"ga_plot_age(ga6)","10c5ec45":"ga_amount(ga7)","905e2577":"ga_agender(ga7)","4b1a803f":"ga_edugender(ga7)","936a6aa6":"ga_plot_amount(ga7, 'country')","986dfc24":"ga_plot_age(ga7)","c5168963":"ga_amount(ga8)","84ab9ab0":"ga_agender(ga8)","55dc9eab":"ga_edugender(ga8)","45eab50b":"ga_plot_amount(ga8, 'country')","301e28b4":"ga_plot_age(ga8)","a23f0b72":"ga_amount(ga9)","3422dab2":"ga_agender(ga9)","eec826c8":"ga_edugender(ga9)","ad739add":"ga_plot_amount(ga9, 'country')","3e76b2f1":"ga_plot_age(ga9)","06b64be7":"ga_amount(ga10)","e83142ce":"ga_trim_to(5, ga10)","28805113":"ga_agender(ga10)","305d01ee":"ga_edugender(ga10)","b33179fd":"ga_plot_amount(ga10, 'country')","c7471e67":"ga_plot_age(ga10)","55d7f6d7":"ga_amount(ga11)","365d1127":"ga_trim_to(7, ga11)","b6bc53f2":"ga_agender(ga11)","1147525d":"ga_edugender(ga11)","f27a98c4":"ga_plot_amount(ga11, 'gender')","e1682306":"ga_plot_age(ga11)","92f4207e":"ga_amount(ga14)","3dd60d24":"ga_agender(ga14)","d46df7c1":"ga_edugender(ga14)","0eaeddec":"# ga_plot_amount(ga14, 'country')","cadd4c62":"ga_plot_age(ga14)","158c60e2":"ga_amount(ga15)","37d5e7dd":"ga_agender(ga15)","5a7cc90c":"ga_edugender(ga15)","9416f01b":"# ga_plot_amount(ga15, 'country')","6acfadde":"ga_plot_age(ga15)","667f70f4":"ga_amount(ga16)","b647d1c4":"ga_trim_to(5, ga16)","90732301":"ga_agender(ga16)","a069086f":"ga_edugender(ga16)","4cb52db5":"ga_plot_amount(ga16, 'country')","174dd18f":"ga_plot_age(ga16)","dd2e12c1":"ga_amount(ga17)","65b21963":"ga_trim_to(5, ga17)","8c2fd6a6":"ga_agender(ga17)","c424a207":"ga_edugender(ga17)","90878c8b":"ga_plot_amount(ga17, 'country')","f195fc62":"ga_plot_age(ga17)","402382bc":"ga_amount(ga18)","3f3f1e60":"ga_agender(ga18)","9c2a8919":"ga_edugender(ga18)","71e0f373":"ga_plot_amount(ga18, 'country')","3ae39e54":"ga_plot_age(ga18)","9101145e":"ga_amount(ga19)","d8b09840":"ga_agender(ga19)","3e79e047":"ga_edugender(ga19)","395a3c63":"ga_plot_amount(ga19, 'country')","d1bca951":"ga_plot_age(ga19)","64ad6351":"ga_amount(ga20)","b3c92ac1":"ga_agender(ga20)","49804f46":"ga_edugender(ga20)","20e02545":"ga_plot_amount(ga20, 'country')","1fc65f2d":"ga_plot_age(ga20)","a3bfc3dd":"ga_amount(ga21)","3a49533f":"ga_agender(ga21)","f93baebf":"ga_edugender(ga21)","7dc60711":"ga_plot_amount(ga21, 'country')","c4ae1c69":"ga_plot_age(ga21)","78716b0e":"ga_amount(ga24)","84f44e46":"ga_trim_to(6, ga24)","9b1e1331":"ga_agender(ga24)","2e1d378a":"ga_edugender(ga24)","a8fc6e7f":"ga_plot_amount(ga24, 'country')","0ecc666c":"ga_plot_age(ga24)","441f7d25":"ga_amount(ga25)","8b2f7c8c":"ga_agender(ga25)","d689e8b7":"ga_edugender(ga25)","540c976e":"ga_plot_amount(ga25, 'gender')","94c10d4a":"ga_amount(ga26)","7de38562":"ga_agender(ga26)","ddbde2dc":"ga_edugender(ga26)","021d1ebc":"ga_plot_amount(ga26, 'year')","65408d4a":"ga_plot_age(ga26)","ab524863":"def get_empty_sa(more_than=None):\n    df = sa_data\n    \n    cols_list = [col for col in sa_data.columns\n                 if col not in res_data.columns]\n\n    empty_cols_list = []\n\n    if more_than == 0:\n        return empty_cols_list\n\n    if not more_than:\n        return sa_info(df, cols_list)\n    \n    def check_col(df, col_name):\n        data = df[col_name]\n        \n        emptiness = data.isna().mean() * 100\n        \n        if emptiness > more_than:\n            return col_name\n\n    for col in cols_list:\n        result = check_col(df, col)\n        \n        if result:\n            empty_cols_list.append(result)\n            \n    return empty_cols_list\n\n\ndef get_empty_ga(more_than=None):\n    df = ga_data    \n    groups_list = multiple_groups\n\n    empty_group_list = []\n\n    if more_than == 0:\n        return empty_group_list\n\n    if not more_than:\n        return ga_info(df, groups_list)\n    \n    def check_group(df, group_name):\n        group_data = df.filter(like=(group_name + \"_\"))\n        \n        emptiness = group_data.isna().mean().mean() * 100\n        \n        if emptiness > more_than:\n            return group_name\n    \n    for group in groups_list:\n        result = check_group(df, group)\n        \n        if result:\n            empty_group_list.append(result)\n            \n    return empty_group_list\n\n\ndef transform_res(method, skip_cols=None, is_drop_first=False):\n    df = res_data\n    \n    cols_list = res_data.columns.to_list()\n\n    if skip_cols:\n        skip_cols = [res_cols_dict.get(type_info)\n                     for type_info in skip_cols]\n\n        cols_list = [col for col in cols_list\n                     if col not in skip_cols]\n\n    data = df[cols_list]\n    \n    result = transform_cols(data, method, is_drop_first)\n\n    if not isinstance(result, pd.DataFrame):\n        result = pd.DataFrame(index=df.index)\n    \n    return result\n\n\ndef transform_sa(method, skip_cols=None):\n    df = sa_data\n    \n    cols_list = [col for col in sa_data.columns\n                 if col not in res_data.columns]\n    \n    if skip_cols:\n        cols_list = [col for col in cols_list\n                     if col not in skip_cols]\n    \n    data = df[cols_list]\n    \n    return transform_cols(data, method)\n\n\ndef transform_ga(method, skip_groups=None):\n    df = ga_data\n    groups_list = multiple_groups\n    \n    if len(skip_groups) == len(groups_list):\n        return pd.DataFrame(index=ga_data.index)\n    \n    concat_data = []\n    \n    for group_name in groups_list:\n        \n        if skip_groups and group_name in skip_groups:\n            continue\n            \n        group_data = df.filter(like=(group_name + \"_\"))\n        concat_data.append(transform_cols(group_data, method))\n    \n    return pd.concat(concat_data, axis=1)","9de1e1c7":"# Check empty\n# get_empty_sa()\n# get_empty_ga()","acf60e5d":"skip_res = ['country', 'gender', 'age']  # 'country', 'gender', 'age'\nskip_sa = get_empty_sa(more_than=50)  # 50% empty\nskip_ga = get_empty_ga(more_than=80)  # 80% empty\n\nprint(skip_sa)\nprint(skip_ga)","72577ba3":"%%time\nX = transform_res('onehot', skip_res, is_drop_first=False) \\\n    .join(transform_sa('cols', skip_sa)) \\\n    .join(transform_ga('rows', skip_ga))","2b5b9b02":"X.shape","dbfe5def":"X.info(verbose=False, memory_usage='deep')","b1d708a6":"%%time\nn_clusters = 6  # randint(2, 22)\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=option_rs)\nX_clusters = kmeans.fit_predict(X)","d8243ddc":"%%time\npca = PCA(n_components=3, random_state=option_rs)\nX_pca = pca.fit_transform(X)","efda397e":"total_var = pca.explained_variance_ratio_.sum() * 100\n\nfig = px.scatter_3d(\n    X_pca, x=0, y=1, z=2, color=X_clusters,\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=800, height=600\n)\n\nfig.show()","df4fb148":"# === KernelPCA ===\n# Insufficient memory to process all data (to allocate more memory ...)\n\ntest_kernel_pca = False\n\nif test_kernel_pca == True:\n\n    x_data_trim_to = 0.3139  # 2021 year\n    x_data_length = int(X.shape[0] * x_data_trim_to)\n\n    X_short = X.iloc[:x_data_length]\n    print(X_short.shape)\n\n    X_short_clusters = kmeans.fit_predict(X_short)\n\n    kernels = ['linear', 'rbf', 'poly', 'sigmoid', 'cosine']\n    check_kernel = kernels[1]\n\n    %%time\n\n    kpca = KernelPCA(n_components=3, kernel=check_kernel, random_state=option_rs)\n    X_short_kpca = kpca.fit_transform(X_short)\n\n    fig = px.scatter_3d(\n        X_short_kpca, x=0, y=1, z=2, color=X_short_clusters,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n        width=800, height=600\n    )\n\n    fig.show()","de2ba88a":"def get_saga_info(df, saga_info):\n    col_clusters = \"Clusters\"\n    \n    if saga_info in res_cols_dict.keys():\n        col_name = res_cols_dict.get(saga_info)\n        cols_list = [col_name, col_clusters]\n        data = df[cols_list].copy()\n        data = data.rename(columns={col_name: saga_info})\n        \n        if saga_info == 'age':\n            data = data.sort_values(by=saga_info)\n\n        if saga_info == 'year':\n            data[saga_info] = data[saga_info].astype(str)\n        \n        return data\n        \n    if 'SA' in saga_info:\n        if saga_info not in single_cols:\n            return False\n        else:\n            col_name = saga_info\n            cols_list = [col_name, col_clusters]\n\n            return df[cols_list]\n    \n    if 'GA' in saga_info:\n        if saga_info not in multiple_groups:\n            return False\n        else:\n            group_name = saga_info\n            cols_group = df.filter(like=(group_name + \"_\")).columns.to_list()\n            \n            data = pd.melt(df, id_vars=[col_clusters],\n                           value_vars=cols_group, value_name=group_name)\n            \n            cols_list = [group_name, col_clusters]\n            \n            return data[cols_list]\n\n\ndef clusters_amount(saga_info, is_norm=True, is_sort=True, is_all=True, is_empty=True):\n    df = clusters_data\n    col_clusters = \"Clusters\"\n    \n    result = get_saga_info(df, saga_info)\n    \n    if is_empty == False:\n        result = result.fillna('-Empty-')\n\n    if is_norm == True:\n        how_normalize = 'index'\n    else:\n        how_normalize = False\n    \n    result = pd.crosstab(result[saga_info], result[col_clusters], margins=is_all,\n                         rownames=['Answer'], normalize=how_normalize)\n\n    if how_normalize:\n        result = result.mul(100).round(2)\n    \n    if is_sort == True and 'All' in result.columns:\n        result = result.sort_values('All', ascending=False)    \n    \n    return result\n        \n        \ndef clusters_plot(saga_info, is_empty=True, is_norm=False, is_sort=True):\n    df = clusters_data\n    col_clusters = \"Clusters\"\n    \n    data = get_saga_info(df, saga_info)\n    \n    plot_data = data[[saga_info, col_clusters]].copy().astype(str)\n\n    return plot_amount(plot_data, col_clusters, saga_info, is_empty, is_norm, is_sort)\n    \n\ndef clusters_agender(type_ge=2):\n    data = clusters_data\n    \n    return agender(data, 'Clusters', type_ge)","8a5195d0":"clusters = pd.Series(data=X_clusters, index=res_data.index, name='Clusters')\nclusters_data = res_data \\\n                .join(sa_data.drop(res_data.columns, axis=1)) \\\n                .join(ga_data.drop(res_data.columns, axis=1)) \\\n                .join(clusters)\n\nprint(clusters_data.shape)","185b478b":"clusters_amount('age', is_norm=False)","5f42268c":"clusters_agender()","eaf79061":"clusters_plot('age')","b60a3116":"clusters_amount('gender')","d2291070":"clusters_plot('gender')","8f84d003":"# ...","3f3ffa10":"> By age and country, it can be seen that the youngest respondents is in India, and the oldest is in the USA.\n> \n> It can also be seen that two countries - China and Japan - vary greatly in audience composition with age.","8187c910":"### GA 2: ... hosted notebook ...","0e4b2aaa":"### SA 14: ... employer incorporate machine learning methods ...\n\n> This question has the largest increase in empty responses. If we look at the countries, we can see that India has the highest percentage of such responses.","b938a16c":"### SA 20: ... primary tool ...","994da801":"### SA 18: ... big data products ...","29d0bd7f":"### GA 17: ... business intelligence tools ... familiar ...","fd1c6a6c":"### SA 4: ... formal education ...\n\n> This question has 6 different answer types, 3 of which are the most popular.\n> \n> We will cut this list down to 4 by including the remaining answers in \"Other\".\n> \n> The graph shows that the higher the education, the greater the percentage of women.\n> \n> If we look at the countries, India has a decreasing percentage of those with higher education, while the USA, Germany and Britain have an increasing percentage.","84b67e5e":"### SA 13: ... individuals are responsible for data science workloads ...","989af7b1":"### GA 4: ... visualization libraries ...","c6fb5889":"### SA 19: ... business intelligence tools ...","c1e6fc2a":"### GA 5: ... machine learning frameworks ...","03f27b4d":"> Let's analyze the data on the graph, cut off the long tail after 40 minutes per questionnaire.\n>\n> It can be seen that every year the number of anomalies decreases - the graph is leveled.","797d0cbf":"### GA 25: ... begun or completed data science courses ...","8d8ec3be":"> The filling of the cells (survey) by year also differs, in 2018-2019 most of the answers were for 11-12 questions from the respondent, in 2021 - 17 answers.","3b3c3a23":"### GA 15: ... big data products ... familiar ...","f71989fa":"## Age\n\n> Age is divided into small subgroups. It can be seen from the data that the number of respondents increases from 18 to 25, after 30 it begins to decline.\n> \n> For the convenience of further work, we will reduce the range of ages by recording the subgroups after a certain age in one last.","eda7accd":"## 2.2 Check and modify","dead9362":"### SA 9: ... used a TPU ...\n\n> This question has answers only for 2020-2021, if we look at those who have used TPU one or more times - their number is growing, but not much.","bec433f1":"## 1.1 Introduction\n\n> This notebook uses already downloaded and old cleaned data, because the answers are stored in different columns and have different structures.\n> \n> Let's see what we managed to collect together, how many single and group answers.","1b663b57":"### SA 6: ... writing code ...\n\n> The most popular part of the answers to this question is \"1-3 years\", while the figure practically does not change between the ages of 18 and 29, either for men or women, that is, a decrease.\n> \n> For the answer option \"3-5 years\" the number increases to the age group 25-29 years - the same as the previous answer - and then also decreases.\n> \n> If you look at the graph, you can see that the percentage of women is the highest for the answer \"I never wrote code\" and gradually decreases with increasing to the minimum value for the answer \"20+ years\".","7184a039":"## 5.3 Analysis","067086c7":"### GA 21: ... automated machine learning tools ... familiar ...","739d1986":"### GA 19: ... automated machine learning tools ... familiar ...","229c2efd":"### GA 26: ... media sources ...","cea6abb0":"# 2. About Respondents\n\n> After we have decided on the characteristics of the dataset, let's look at those who answered the questions. We are interested in information about the respondent - age, gender and country.\n> \n> For the convenience of working with this data, we will create a separate dataframe and add meaningful names for the required columns.","5c1a4afa":"### SA 15: ... yearly compensation ...\n\n> The number of respondents to this question depends on the answer option; the higher the amount, the fewer respondents.\n> \n> The graph shows that the larger the amount, the more respondents from the USA.","5689f807":"## 1.4 Results\n\n> Let's check what we got. The data that we have excluded and the data that we have left for analysis.","d4b503b2":"### GA 9: ... role at work ...","7e3ce248":"### SA 16: ... spent on machine learning ...","217d9e32":"## Check and Visualisation\n\n> Let's start analyzing the answers, for this we will create the required number of variables for each available question.\n> \n> In those groups where there are many empty columns (with answers), we will enlarge these answers - move them to Other.","178ca541":"> Examples of those responses where there were only 3, only 4, or only 6 answers.","de26eac2":"### GA 3: ... specialized hardware ...","b2f7048f":"> Let's check the presence of empty values with answers, as well as unique values - how many answer options there are for each question.\n> \n> It can be seen that the longer the respondent answered the survey, the more gaps there were in the answers.","9503cf93":"> As a result, we have 20 single columns (one column for different answers) and 23 group columns (where there is a separate column for each answer).\n> \n> In order to determine how to work with data, we will choose only single answers for evaluation. Let's look at these columns, what their names are, how complete they are, and what questions they have.","f83fed79":"## Check and Visualisation\n\n> Let's start analyzing the answers, for this we will create the required number of variables for each available question.\n> \n> Columns 1-3 in the available dataset refer to respondents.\n> \n> We will enlarge the answer options with a small number - transfer them to Other.","f2402206":"### SA 10: ... used machine learning ...\n\n> Most of those who answered this question indicated less than 2 years of using machine learning methods. In second place were those who either did not answer or indicated the answer \"I do not use ML\".\n> \n> At the same time, in 2019 there is no such possibility, but if we look at the number of empty ones, we can see that this was an alternative to the answer \"I do not use ML\".\n> \n> If we analyze how the answers are distributed by gender, we can see that the percentage of women decreases from option \"I do not use ML\" to option \"2-3\" and after that it does not decrease much.","b2cba0fc":"> After all this, we can create a segment of respondents, those who answered normally - by the number of responses and the speed of filling out the survey.\n>\n> For this we use the already created segments.","73c0d191":"# 1. About Data\n\n> In 2021, the survey was conducted from 09.01.2021 to 04.10.2021 and after clearing the data, it contains 25973 answers, in other years (2018-2020) the number of answers is different.\n> \n> To compare correctly the data for the period of interest, we need to understand what they have in common and what is different.","328540f2":"## Gender\n\n> The answers include 5 options, of which two are basic - man and woman, which did not change much during the analyzed period.\n> \n> For the convenience of visualizing and analyzing data, we will collect the rest of the answers in one group. As a result, we will have three options with which we will work.","d9d69435":"### GA 6: ... ML algorithms ...","a28dcf76":"> Based on this, it is possible to single out a segment of respondents, let's call them mute, and, if necessary, exclude them from further analysis.\n> \n> The **threshold is less than 7 answers** from the respondent.","fe715574":"## 5.2 PCA","877b08f8":"# 5. Clastering\n\n> The resulting three dataframes are merged and we are trying to do clustering.\n> \n> At the stage of data merged, we choose what we do exclude - the individual characteristics of the respondents and\/or the threshold values of the voids in the answers.","d86b9198":"## 1.2 Check notna\n\n> It can be seen that the fullness of the columns with the answers is different, it depends on the fact that in different years some answers could be absent in the survey.\n> \n> If we look at the general statistics, we can see that in 2018-2020 the minimum number of cells (answers to questions) was 3 - they include age, gender and country, in 2021 the minimum number of cells (answers) is 6.","400032e3":"### GA 14: ... big data products ... use ...","5bc03425":"### SA 7: ... programming language ...\n\n> The absolute leader in the answer to this question is Python. The number of empty responses has been declining over the past two years.\n> \n> The list of answers can be shortened for clarity of further analysis.\n> \n> The popularity of Python's answer grows from 18 to 39, then gradually declines. If we look at the countries, we can see that the distribution is different for the \"C\/C++\", India has the largest.","a2d31ff1":"### GA 16: ... business intelligence tools ... use ...","2ec24b37":"> Let's look at this data without the mute respondents segment. It can be seen that the minimum response time has increased.","c3aac10c":"# 4. About Answers (group)\n\n> In this section, we analyze group question-answers (there is a column for each answer option).\n> \n> We will also analyze in terms of age, gender, country of the respondent and the year of the survey.\n> \n> For this task, we will also create a separate dataframe, which includes group questions and data about respondents.","3f3f0f72":"# 3. About Answers (single)\n\n> After we have decided on the respondents, let's see how they answered the questions. In this section, we analyze single question-answers (one column). We are interested in quantitative and qualitative - age, gender, country, year - the analysis of these answers.\n> \n> For the convenience of working with this data, we will create a separate dataframe and add the previously processed data about the respondents.","fc2b3d72":"## 1.3 Check duration\n\n> The duration of filling out the questionnaire is different for each respondent, let's see the statistical data by year.\n> \n> It can be seen that in 2018-2020 the minimum time was about 30 seconds. Considering that there are profiles with only 3 responses, this is not surprising.","ab91f091":"### GA 24: ... publicly share ...","949fa6de":"> We can see the difference between these countries and by gender. USA has the most diverse audience.","d98c8e9d":"### GA 0: ... programming languages ...","54a2c1bd":"### GA 10: ... cloud computing platforms ... use ...","3c7f32f0":"## Country & Age & Gender (Man \/ Woman)","56b08a9a":"### GA 7: ... computer vision methods ...","f7dea0db":"## What is this notebook about?\n\n> It will tell a data story about a subset of the data science community represented in 2021-2018 industry surveys.","2c66fcbb":"### GA 1: ... IDE's ...","826eb872":"## 2.3 Visualization","6cccdf7b":"> Let's look at these respondents by year. It can be seen that this segment did not affect the data of the 2021 survey.","6bf7b788":"> If we analyze the data by country and year, we can see that Nigeria has the largest growth in 2021. The largest decline (2021 to 2018) is in China.","838f106d":"## 2.1 Introduction\n\n> We will work with the res_data dataframe, which contains a segment of normal answers.","6037ed91":"### GA 18: ... automated machine learning tools ... use ...","d3375caa":"### SA 8: ... computing platform ...\n\n> This question was absent in 2018-2019, the number of empty answers in 2020-2021 is minimal.\n> \n> The most popular answer is \"A personal computer\".\n> \n> If we analyze how the answers are distributed by gender, we can see that the percentage of women decreases from the option \"Other\" to the option \"A deep learning workstation\".","cb503444":"### SA 5: ... current role ...\n\n> This question can also be shortened by the number of answers by moving the part to \"Other\".\n> \n> The most popular answer is \"Student\", second as \"Data Scientist \/ ML Engineer\".\n> \n> If we look at the data by year, we can see that the largest increase in 2021, compared to previous years, was in the answer - \"Not employed\".","f728a2dd":"> To cut off those who answered quickly, we will highlight the segment of those who answered fast, less than 2 minutes per questionnaire.\n> \n> Examples of their answers, as well as data by year.","46f6afa8":"### SA 17: ... cloud platforms ...","ed681ae2":"## Country\n\n> There are a large number of answer options, among which there are two leaders - India and the USA, and there is also a group called Others.\n> \n> For convenience, we will reduce the number of answer options by moving some of them to the Others.","9363f703":"### GA 11: ... cloud computing platforms ... more familiar ...","5d291948":"### GA 20: ... automated machine learning tools ... use ...","fc5c6c3b":"## 5.1 Extract & Predict","81692d99":"> Let's check the statistics, where we are interested in the average number of responses, as well as how many voids there were that will need to be analyzed separately.\n> \n> It can be seen that the majority (75%) of the respondents chose 2 or more answer options in only a few questions.","2748eceb":"### GA 8: ... natural language processing ...","6b8b23b1":"### SA 12: ... size of the company ...\n\n> The most popular answer to this question is \"0-49 employees\". In second place is the \"10,000+ employees\" option.\n> \n> If we look at the countries, we can see that in the USA and India this response has the highest percentage.","46565e1f":"> Answer options in some cases are distributed unevenly, it is better to visualize them with normalization.\n> \n> If we look at the data for age and gender, we can clearly see that the older the age group, the more man.","c87c1757":"### SA 11: ... industry ...\n\n> This question was in 2018 and 2021, it has the largest list of answer options, so we will shorten it.\n> \n> The two most popular options are \"Computers\/Technology\" and \"Academics\/Education\".\n> \n> The \"Manufacturing\/Fabrication\" answer had an upward change."}}