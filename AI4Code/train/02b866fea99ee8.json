{"cell_type":{"af236444":"code","c8f8f82a":"code","7e9cea72":"code","38026998":"code","5658b728":"code","a116dd60":"code","5d24ea69":"code","03ec6c27":"code","6de730c2":"code","73e5d300":"code","347dfa2b":"code","7cb449b8":"code","bad454eb":"code","3a0bf192":"code","29e283cb":"code","b2b9d990":"code","c00846cc":"code","dedad708":"code","96b617d9":"code","82a03427":"code","98929d2e":"code","d0ff0703":"code","dca6f9cc":"code","00737c13":"code","51594acb":"code","e013cffb":"code","29c2800a":"code","fd66b6de":"code","1e7e0916":"code","b197780c":"code","9d2f12d6":"code","0eb640b5":"code","5bd31629":"code","b8588913":"code","f017bc1c":"code","4c2d7075":"code","189568d2":"code","3c4d0616":"code","f33c5f60":"code","977ffdf5":"code","99ab3aae":"code","f37480fc":"code","3feaac59":"code","a7ba645b":"code","a224c369":"code","fc730768":"code","7340ad28":"code","354ca7ef":"code","e30145bb":"code","bf84f4a3":"code","7c098ea9":"code","4bc3e83d":"code","93b1f89d":"markdown","5b7baaec":"markdown","87ffd5bb":"markdown","0216a7cd":"markdown","711e408b":"markdown","167fc848":"markdown","c1c47d28":"markdown","30492e46":"markdown","e6a9d5e0":"markdown","a4c62363":"markdown","1645dcd0":"markdown","d0aed39f":"markdown","9c317048":"markdown","3959308d":"markdown"},"source":{"af236444":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8f8f82a":"data = pd.read_csv(\"\/kaggle\/input\/automobile-dataset\/Automobile_data.csv\")\ndata.head()","7e9cea72":"data.isnull().sum()","38026998":"# We can see that there is no null values in the data from above information.\n# But however we can aslo see from head() that there are few values as '?' in normalized-losses which can indicate as NaN.","5658b728":"print('Number of ? in columns are')\nfor col in data.columns:\n    if len(data[data[col]=='?']) > 0:\n        print(col, 'has ->', len(data[data[col]=='?']))","a116dd60":"# We can see that normalized-losses has the highest numbers of '?' of 41, so we can not delete the rows as it could affect the data. So instead we will replace it with mean values. We will replace the remaining columns with mean values and median as required. \n# We can delete rows where price is '?' as it is low in number and price is the target variable.  ","5d24ea69":"# Repalcing '?' with NaN\ndata['price'] = data['price'].replace('?', np.NaN)\ndata['normalized-losses'] = data['normalized-losses'].replace('?', np.NaN)\ndata['num-of-doors'] = data['num-of-doors'].replace('?', np.NaN)\ndata['stroke'] = data['stroke'].replace('?', np.NaN)\ndata['horsepower'] = data['horsepower'].replace('?', np.NaN)\ndata['peak-rpm'] = data['peak-rpm'].replace('?', np.NaN)\ndata['bore'] = data['bore'].replace('?', np.NaN)\ndata['num-of-doors'] = data['num-of-doors'].replace('?', np.NaN)\n\n#Removing rows where price are NaN\ndata = data[data['price'].notna()]","03ec6c27":"data.info()","6de730c2":"# We can see that the data type of some numerical columns are put in as object\n# We will convert it to numerical data type","73e5d300":"# Check to see the column names and identify the columns that needs to be changed\ndata.select_dtypes(['object']).columns","347dfa2b":"# From above cell we can see that 'normalized-losses','bore', stroke','horsepower','peak-rpm' and 'price' are object type instead of numeric","7cb449b8":"# converting Object type to float type\ndata['normalized-losses'] = pd.to_numeric(data['normalized-losses'])\ndata['bore'] = pd.to_numeric(data['bore'])\ndata['stroke'] = pd.to_numeric(data['stroke'])\ndata['horsepower'] = pd.to_numeric(data['horsepower'])\ndata['peak-rpm'] = pd.to_numeric(data['peak-rpm'])\ndata['price'] = pd.to_numeric(data['price'])\n","bad454eb":"# Checking Num-of-doors unique values to replace for NaN\ndata['num-of-doors'].value_counts()","3a0bf192":"# Since it has only 2 unique values and 'four' has the most count, we will replace NaN with 'four'\n\ndata['num-of-doors'] = data['num-of-doors'].fillna('four')","29e283cb":"# We will use mean as the data in these columns are continuous\ndata['normalized-losses'] = data['normalized-losses'].fillna(data['normalized-losses'].mean())\ndata['horsepower'] = data['horsepower'].fillna(data['horsepower'].mean())\n\n# We will use median since bore, stroke, peak-rpm have few unique values \ndata['bore'] = data['bore'].fillna(data['bore'].median())\ndata['stroke'] = data['stroke'].fillna(data['stroke'].median())\ndata['peak-rpm'] = data['peak-rpm'].fillna(data['peak-rpm'].median())\n","b2b9d990":"numerical_cols = data.select_dtypes(['int32','int64','float']).columns\nnumerical_cols","c00846cc":"# Checking Correlaion between numerical columns and target variable\ncorr_matrix = data[numerical_cols].corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_matrix['price'].sort_values(ascending=False).to_frame()[1:],square=True ,annot=True)\nplt.show()","dedad708":"# Engine-size, curb-weight, horsepower are the top 3 highly correlated features to our price. That means these 3 features are very important for our price prediction.\n# Whereas symboling, compression_ratio and stroke are least correlated features. We will probably drop these columns later if necessary","96b617d9":"# Plotting our top 3 highly correlated features against target variable\nplt.figure(figsize=(8,5))\nsns.scatterplot(data=data, x=data['engine-size'], y=data['price'])\nplt.xlabel('Engine Size', fontsize=12)\nplt.ylabel('Price', fontsize=12)\nplt.title('Engine size vs Price', weight='bold',fontsize=12)\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.scatterplot(data=data, x='curb-weight', y='price')\nplt.xlabel('Curb weight', fontsize=12)\nplt.ylabel('Price', fontsize=12)\nplt.title('Curb weight vs Price', weight='bold', fontsize=12)\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.scatterplot(data=data, x='horsepower', y='price')\nplt.xlabel('Horsepower', fontsize=12)\nplt.ylabel('Price', fontsize=12)\nplt.title('HorsePower vs Price', weight='bold', fontsize=12)\nplt.show()","82a03427":"plt.scatter(data['symboling'], data['price'])\nplt.xlabel('Symboling')\nplt.ylabel('Price')\nplt.title('Symboling vs Price',weight='bold', size=12)\nplt.show()","98929d2e":"# Symboling rating is risk factor therefore it can not be considered as Numerical \n# It is considered as Ordinal Category\n# We will create different columns describing each rating further in the analysis","d0ff0703":"categorical_cols = data.select_dtypes(['object']).columns\ncategorical_cols","dca6f9cc":"print(f'Unqiue value counts of fuel-type are: ','\\n', data['fuel-type'].value_counts())\nprint()\nprint(f'Unqiue value counts of aspiration are', '\\n',data['aspiration'].value_counts())\nprint()\nprint(f'Unqiue value counts of num-of-doors are', '\\n',data['num-of-doors'].value_counts())\nprint()\nprint(f'Unqiue value counts of engine-location are', '\\n',data['engine-location'].value_counts())","00737c13":"# Storing the above columns into separate variables and transforming them into binary values\ncols = ['fuel-type', 'aspiration', 'num-of-doors', 'engine-location']\n\nle = LabelEncoder()\n\ndata[cols] = data[cols].apply(le.fit_transform)\n","51594acb":"plt.figure(figsize=(2.5,5))\nplt.scatter(data['fuel-type'], data['price'])\nplt.xlabel('Fuel type', size=10)\nplt.ylabel('Price', size=10)\nplt.title('Fuel Type vs Price', weight='bold', size=12)\nplt.show()","e013cffb":"plt.figure(figsize=(2.5,5))\nplt.scatter(data['aspiration'], data['price'])\nplt.xlabel('Aspiration', size=10)\nplt.ylabel('Price', size=10)\nplt.title('Aspiration vs Price', weight='bold', size=12)\nplt.show()","29c2800a":"plt.figure(figsize=(2.5,5))\nplt.scatter(data['engine-location'], data['price'])\nplt.xlabel('engine-location', size=10)\nplt.ylabel('Price', size=10)\nplt.title('engine-location vs Price', weight='bold', size=12)\nplt.show()","fd66b6de":"plt.figure(figsize=(2.5,5))\nplt.scatter(data['num-of-doors'], data['price'])\nplt.xlabel('Num-of-doors', size=10)\nplt.ylabel('Price', size=10)\nplt.title('Number of doors vs price', weight='bold', size=12)\nplt.show()","1e7e0916":"plt.scatter(data=data, x='num-of-cylinders', y='price', c=data['num-of-doors'])\nplt.xlabel('num-of-cylinders', size=10)\nplt.ylabel('price', size=10)\nplt.title('Number of cylinders vs price', weight='bold', size=12)\nplt.show()","b197780c":"# Plotting bar graph for various categorical features\nf, ax = plt.subplots(figsize=(12,8))\n\nax.barh(data['make'].value_counts().index, data['make'].value_counts().sort_values(ascending=True))\nplt.xticks(size=10)\nplt.yticks(size=12)\nax.set_xlabel('Counts', fontsize=12)\nax.set_title('Counts of Car Manufacturers', weight='bold', fontsize=12)\nplt.show()","9d2f12d6":"f, ax = plt.subplots(figsize=(8,4))\n\nax.barh(data['body-style'].value_counts().index, data['body-style'].value_counts().sort_values(ascending=True))\nplt.xticks(size=10)\nplt.yticks( size=12)\nax.set_xlabel('Counts', fontsize=12)\n#ax.set_ylabel('Body Style', fontsize=15)\nax.set_title('Different types of body style', fontsize=12, weight='bold')\nplt.show()","0eb640b5":"f, ax = plt.subplots(figsize=(8,3))\n\nax.barh(data['drive-wheels'].value_counts().index, data['drive-wheels'].value_counts().sort_values(ascending=True))\nplt.xticks(size=10)\nplt.yticks( size=12)\nax.set_xlabel('Counts', fontsize=12)\n#ax.set_ylabel('Body Style', fontsize=15)\nax.set_title('Different drive wheels', fontsize=12, weight='bold')\nplt.show()","5bd31629":"data.describe()","b8588913":"# normalized-losses has maximum higher than expected from 75 percentile as seen from above description\nplt.boxplot(data['normalized-losses'])\nplt.xlabel('Normalized-losses', weight='bold', size=12)\nplt.show()\nprint(f'Number of possible outliers in normalized-losses are',len(data[numerical_cols][data['normalized-losses']>200]))","f017bc1c":"# compression-ratio has maximum higher than expected from 75 percentile as seen from above description\nplt.boxplot(data['compression-ratio'])\nplt.xlabel('Compression Ratio', weight='bold', size=12)\nplt.show()\nprint(f'Number of possible outliers in compression ratio are',len(data[numerical_cols][data['compression-ratio']>12]))","4c2d7075":"plt.scatter(data=data, x='compression-ratio', y='stroke')\nplt.scatter(data=data, x='compression-ratio', y='bore')\nplt.xlabel('Compression Ratio', size=10)\nplt.title('Compression ratio vs stroke and bore', weight='bold', size=12)\nplt.legend(['stroke', 'bore'])\nplt.show()","189568d2":"# Horsepower has max higher than expected from the 75 percentile as seen above\nplt.boxplot(data['horsepower'])\nplt.xlabel('Horsepower', weight='bold', size=12)\nplt.show()","3c4d0616":"# Checking further with engine size as they both are related\nsns.scatterplot(data=data, x='horsepower', y='engine-size')\nplt.title('Horsepower vs Engine', weight='bold', size=12)\nplt.show()","f33c5f60":"# Plotting Engine-size vs Price for further Analysis\nsns.scatterplot(data=data, x='engine-size', y='price')\nplt.title('Engine-size vs Price', weight = 'bold', size=12)\nplt.show()","977ffdf5":"# peak-rpm has max higher than expected from the 75 percentile as seen above\nplt.boxplot(data['peak-rpm'])\nplt.xlabel('Peak-rpm', weight='bold', size=12)\nplt.show()\nprint(f'Total number of possible outliers in peak-rpm are:',len(data['peak-rpm'][data['peak-rpm']>6000]))","99ab3aae":"# Replacing Outliers with 99 percentile\n\n# Finding 99 percentile in normalized losses to replace the outliers\nreplaced_val = np.percentile(data['normalized-losses'], 99)\nprint('99th percentile of Normalized-losses is',replaced_val)\ndata[numerical_cols][data['normalized-losses']>replaced_val]","f37480fc":"# There are 2 values in normalized-losses which can be replaced\ndata['normalized-losses'] = data['normalized-losses'].replace(231, replaced_val)\ndata['normalized-losses'] = data['normalized-losses'].replace(256, replaced_val)","3feaac59":"# Finding 99 percentile in peak-rpm to replace the outliers\nval = np.percentile(data['peak-rpm'], 99)\nprint('99th percentile of peak-rpm is',val)\ndata[numerical_cols][data['peak-rpm']>val] ","a7ba645b":"# Both the entries looks repeated so we will drop both rows as both are outliers and number of outliers will remain 1\n\ndata.drop(data[data['peak-rpm']==6600].index, axis=0, inplace=True)","a224c369":"categorical_cols = data.select_dtypes(['object']).columns\ncategorical_cols","fc730768":"# Creating new feature columns for each categorical data\ndata = pd.get_dummies(data , columns=['symboling','make', 'body-style', 'drive-wheels', 'num-of-cylinders', 'fuel-system', 'engine-type'], prefix=['symboling','make', 'body-style', 'drive-wheels', 'num-of-cylinders', 'fuel-system', 'engine-type'])\ndata.columns","7340ad28":"# We will drop the following columns as it has very low corrrelation to our target variable\ndata.drop(['compression-ratio', 'stroke'],axis=1, inplace=True)","354ca7ef":"# Creating Features data and Target data\nX = data.drop(['price'], axis=1)\ny = data['price']\nprint(X.shape, y.shape)","e30145bb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.12, random_state= 0)\nprint(f'Training data size is',X_train.shape, y_train.shape)\nprint(f'Testing data size is',X_test.shape, y_test.shape)","bf84f4a3":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n\npred = lr.predict(X_test)\n\nsns.regplot(x = y_test, y=pred)\nplt.show()\n\nprint('r2 score using Linear Regression model is :',r2_score(y_test, pred))\n","7c098ea9":"from sklearn.ensemble import RandomForestRegressor","4bc3e83d":"rfg = RandomForestRegressor()\n\nrfg.fit(X_train, y_train)\n\ny_pred = rfg.predict(X_test)\n\nsns.regplot(x=y_test, y=y_pred)\nplt.show()\n\nprint('r2 score using RandomForrestRegressor is:', r2_score(y_test, y_pred))","93b1f89d":"### We can conclude that there are outliers in 2 columns: Normalized-losses and peak-rpm","5b7baaec":"# Imputation","87ffd5bb":"### Visualization of Outliers","0216a7cd":"# We can Conclude that RandomForrestRegressor gives the best r2 score of 0.95 followed by Linear Regression with r2 score of 0.94","711e408b":"## Visualization on Numerical Data","167fc848":"## Splitting the data","c1c47d28":"# EDA","30492e46":"## Checking for Outliers","e6a9d5e0":"Since the number of compression-ratio points is high and it also fits well with stroke and bore hence it can not be an outlier","a4c62363":"#### Replacing NaN values ","1645dcd0":"# Training our ML model","d0aed39f":"## Visualization of Categorical Data","9c317048":"#### Now there are no null values in our data and our data is ready for EDA","3959308d":"Since horsepower and engine are connected and engine size shows no outliers, we can say that there is no outliers in horespower too"}}