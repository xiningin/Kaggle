{"cell_type":{"ecf21008":"code","3776aee2":"code","5c796e85":"code","ee69ae77":"code","1943ba29":"code","12bb4a48":"code","677ae6f2":"code","6a6eb622":"code","fb5bd99d":"code","f03f9a36":"code","cde4e3bc":"code","f03e8b9b":"code","d5fcae48":"code","2fe77086":"code","6fdfc4f5":"code","897d6091":"code","700eaa69":"code","c9f6efd7":"code","3be43d31":"code","0e4b1af4":"code","52a10d71":"code","8ddadfc9":"code","56e75d68":"code","a0089c8b":"code","2d122c4e":"code","7265147f":"code","b3852585":"code","e0a53138":"code","893b139e":"code","d58a1c8f":"code","47fd0b61":"markdown","1a249d12":"markdown","a5539c76":"markdown","10be33f9":"markdown","92208af1":"markdown","1e6c5da4":"markdown","5fcd9f73":"markdown","b584080a":"markdown","40b0d11a":"markdown","b7001d58":"markdown","566aa19c":"markdown","310a0b59":"markdown","62c816e2":"markdown","c553725f":"markdown","69250a6c":"markdown","193f9988":"markdown","5a5025ab":"markdown","cc70a21b":"markdown","77d838de":"markdown","38375d47":"markdown","a210a678":"markdown","b54e4346":"markdown","d3307ca4":"markdown","6ace87d6":"markdown","7fee2807":"markdown","49a55e22":"markdown","deddcbab":"markdown","cd9b13e9":"markdown","50513c82":"markdown","32e07096":"markdown","312e9856":"markdown","e4acc241":"markdown","2cde20ce":"markdown","86ce979a":"markdown","adde07b7":"markdown","c47672f9":"markdown","b16c2cb9":"markdown","68841a0a":"markdown","6a228a58":"markdown"},"source":{"ecf21008":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","3776aee2":"import warnings\n\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","5c796e85":"from sklearn.datasets import load_diabetes\ndf = load_diabetes()\ndf.keys()","ee69ae77":"unscaled_X = df.data\ny = df.target","1943ba29":"scaler = StandardScaler()\nX = scaler.fit_transform(unscaled_X)","12bb4a48":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","677ae6f2":"lr = LinearRegression()\n\n# Train the model\nmodel_lr = lr.fit(X_train, y_train)\n\n# Prediction\ny_pred_train_lr = lr.predict(X_train)\ny_pred_test_lr = lr.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_pred_train_lr).round(5)))\nprint('Testing accuracy : {}\\n'.format(r2_score(y_test, y_pred_test_lr).round(5)))","6a6eb622":"mse=cross_val_score(lr, X, y, scoring='neg_mean_squared_error',cv=5)\nmean_mse=np.mean(mse)\nprint(-(mean_mse).round(5))","fb5bd99d":"coefs = pd.Series(lr.coef_, index = df.feature_names)\ncoefs.plot(kind = 'barh', cmap = 'autumn')\nplt.show()","f03f9a36":"ridge1 = Ridge()\n\n# Fit the model\nmodel_ridge1 = ridge1.fit(X_train, y_train)\n\n# Prediction\ny_pred_train_ridge1 = ridge1.predict(X_train)\ny_pred_test_ridge1 = ridge1.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_pred_train_ridge1).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_pred_test_ridge1).round(5)))","cde4e3bc":"mse1 = cross_val_score(ridge1, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse1 = np.mean(mse1)\nprint(-(mean_mse1).round(5))","f03e8b9b":"coefs_ridge1 = pd.Series(ridge1.coef_, index = df.feature_names)\ncoefs_ridge1.plot(kind = 'barh', cmap = 'gist_heat')\nplt.show()","d5fcae48":"ridge2 = Ridge()\n\nparameters = {'alpha' : [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 0.9, 0.8, 0.7, 0.5, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}\n\nridge_regressor = GridSearchCV(ridge2, parameters, scoring='neg_mean_squared_error', cv= 10)\n\nridge_regressor.fit(X_train, y_train)","2fe77086":"print(ridge_regressor.best_params_)\nprint(-(ridge_regressor.best_score_))","6fdfc4f5":"# Prediction\ny_train_pred_ridg2 = ridge_regressor.predict(X_train)\ny_test_pred_ridg2 = ridge_regressor.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_train_pred_ridg2).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_test_pred_ridg2).round(5)))","897d6091":"mse2 = cross_val_score(ridge_regressor, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse2 = np.mean(mse2)\nprint(-(mean_mse2).round(5))","700eaa69":"lasso1 = Lasso()\n\n# Fit the model\nmodel_lasso1 = lasso1.fit(X_train, y_train)\n\n# Prediction\ny_pred_train_lasso1 = lasso1.predict(X_train)\ny_pred_test_lasso1 = lasso1.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_pred_train_lasso1).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_pred_test_lasso1).round(5)))","c9f6efd7":"mse3 = cross_val_score(lasso1, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse3 = np.mean(mse3)\nprint(-(mean_mse3).round(5))","3be43d31":"coefs_lasso1 = pd.Series(lasso1.coef_, index = df.feature_names)\ncoefs_lasso1.plot(kind = 'barh', cmap = 'autumn')\nplt.show()","0e4b1af4":"lasso2 = Lasso()\n\nparameters = {'alpha' : [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 0.9, 0.8, 0.7, 0.5, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}\n\nlasso_regressor = GridSearchCV(lasso2, parameters, scoring='neg_mean_squared_error', cv= 10)\n\nlasso_regressor.fit(X_train, y_train)","52a10d71":"print(lasso_regressor.best_params_)\nprint(-(lasso_regressor.best_score_))","8ddadfc9":"mse4 = cross_val_score(lasso_regressor, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse4 = np.mean(mse4)\nprint(-(mean_mse4))","56e75d68":"# Prediction\ny_train_pred_lasso2 = lasso_regressor.predict(X_train)\ny_test_pred_lasso2 = lasso_regressor.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_train_pred_lasso2).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_test_pred_lasso2).round(5)))","a0089c8b":"enet1 = ElasticNet()\n\n# Fit the model\nmodel_enet1 = enet1.fit(X_train, y_train)\n\n# Prediction\ny_pred_train_enet1 = enet1.predict(X_train)\ny_pred_test_enet1 = enet1.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_pred_train_enet1).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_pred_test_enet1).round(5)))","2d122c4e":"mse5 = cross_val_score(enet1, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse5 = np.mean(mse5)\nprint(-(mean_mse5).round(5))","7265147f":"coefs_enet1 = pd.Series(enet1.coef_, index = df.feature_names)\ncoefs_enet1.plot(kind = 'barh', cmap = 'autumn')\nplt.show()","b3852585":"enet2 = ElasticNet()\n\nparameters = {'alpha' : [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 0.9, 0.8, 0.7, 0.5, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}\n\nenet_regressor = GridSearchCV(enet2, parameters, scoring='neg_mean_squared_error', cv= 10)\n\nenet_regressor.fit(X_train, y_train)","e0a53138":"print(enet_regressor.best_params_)\nprint(-(enet_regressor.best_score_))","893b139e":"mse5 = cross_val_score(enet_regressor, X, y, scoring = 'neg_mean_squared_error',cv=5)\nmean_mse5 = np.mean(mse5)\nprint(-(mean_mse5))","d58a1c8f":"# Prediction\ny_train_pred_enet2 = enet_regressor.predict(X_train)\ny_test_pred_enet2 = enet_regressor.predict(X_test)\n\n# Accuracy Score\nprint('Training accuracy : {}\\n'.format(r2_score(y_train, y_train_pred_enet2).round(5)))\nprint('Testing accuracy : {}'.format(r2_score(y_test, y_test_pred_enet2).round(5)))","47fd0b61":"<br\/>\n\n## 1.5 Key differences\n\nRidge and Lasso Regression are almost same. But the only key difference they have is Ridge Regression doesn't remove any features while the features with less importance is removed by Lasso Regression.","1a249d12":"**Linear Regression**  \nTraining accuracy : 0.55393  \nTesting accuracy : 0.33222  \n<br\/>\n\n**Ridge Regression**  \nTraining accuracy : 0.55379   \nTesting accuracy : 0.33105  \n<br\/>\n\n**Ridge Regression with GridSearchCV**  \nTraining accuracy : 0.55126  \nTesting accuracy : 0.33448  \n<br\/>\n\n**Lasso Regression**  \nTraining accuracy : 0.5505  \nTesting accuracy : 0.33435  \n<br\/>\n\n**Lasso Regression with GridSearchCV**  \nTraining accuracy : 0.5505  \nTesting accuracy : 0.33435  \n<br\/>\n\n**Elastic Net Regression**  \nTraining accuracy : 0.51923  \nTesting accuracy : 0.35694  \n<br\/>\n\n**Elastic Net Regression with GridSearchCV**  \nTraining accuracy : 0.55358  \nTesting accuracy : 0.33047  \n\n<br\/>\n\nFrom the scores mentined above, we are seeing that **Elastic Net Regression** performs well. It reduces overfitting along with increases the testing accuracy.","a5539c76":"##### Mean Squared Error","10be33f9":"<br\/>\n<br\/>\n\n## 2.9 Elastic Net Regression with GridSearchCV","92208af1":"##### Horizontal Barplot for observing the significance of each features","1e6c5da4":"<br\/>\n<br\/>\n\n## 1.1 What is Regularization\n\nGetting a good accuracy score isn't always pleasant to a data scientist specially when he gets a good training accuracy along with a poor test accuracy. Though this type of model can predict the training set very well, but it can't perform well for the test as well as new dataset. This type of situation or model is called **Overfitting**. So how can we solved this problem?\n<br\/>\n\nHere comes up **Regularizaton** to solve this problem. So in short, we can say that Regularizaton is the process to prevent a model to be overfitted.\n<br\/>\n\nBut how? Regularization can be carried out by **Ridge Regression**, **Lasso Regression** and **Elastic Net Regression**. We will try to learn how these regression work and reduce the risk of overfitting throughout this kernel.\n\n<br\/>\n","5fcd9f73":"##### Modules","b584080a":"<br\/>\n<br\/>\n\n# Conclusion","40b0d11a":"##### Mean Squared Error","b7001d58":"<br\/>\n\n## 1.4 Elastic Net Regression\n\nElastic Net is the combination of Ridge and Lasso Regression. It has both Ridge Regression penalty and Lasso Regression penalty in it's own penalty.\n<br\/>\n\n**Elastic Net Regression penalty = lambda1 x square of the magnitude of the coefficients + lambda2 x |the magnitude of the coefficients|**\n<br\/>\n\nWhen,  \nlambda1 = lambda2 = 0       -     Elastic Net becomes least square parameter estimates  \nlambda1 > 0 and lambda2 = 0 -     Ridge Regression  \nlambda1 = 0 and lambda2 > 0 -     Lasso Regression  \nlambda1  >0 and lambda2 > 0 -     Elastic Net Regression  ","566aa19c":"<br\/>\n<br\/>\n\n## 2.3 Linear Regression Model","310a0b59":"##### Mean Squared Error","62c816e2":"##### Splitting into X and y","c553725f":"##### Horizontal Barplot for observing the significance of each features","69250a6c":"<br\/>\n<br\/>\n<br\/>\n\n## 2.1 Importing necessary modules along with dataset","193f9988":"## **Table of contents:**  \n**1. Intuition**  \n<br\/>\n     1.1 What is Regularization  \n     1.2 Ridge Regression  \n     1.3 Lasso Regression  \n     1.4 Elastic Net Regression  \n     1.5 Key differences  \n     1.6 Criteria to choose regularization method\n     <br\/>\n\n**2. Example with Diabetes Dataset**  \n2.1 Importing necessary modules along with dataset  \n2.2 Data processing  \n2.3 Linear Regression Model  \n2.4 Ridge Regression  \n2.5 Ridge Regression with GridSearchCV  \n2.6 Lasso Regression\n2.7 Elastic Net Regression\n2.8 Elastic Net Regression with GridSearchCV  \n2.9 Conclusion","5a5025ab":"##### Horizontal Barplot for observing the significance of each features","cc70a21b":"<br\/>\n<be\/>\n\n# 2.5 Ridge Regression with GridSearchCV","77d838de":"##### Best parameter and best score","38375d47":"##### Best parameter and best score","a210a678":"<br\/>\n<br\/>\n\n## 2.4 Ridge Regression","b54e4346":"##### Scaling","d3307ca4":"##### Best parameter and best score","6ace87d6":"##### Mean Squared Error","7fee2807":"<br\/>\n\n## 1.2 Ridge Regression\n\nIt's also called L2 regularization. It shrinks the coefficients by adding a penalty to the least squares.\n<br\/>\n\n**Ridge Regression penalty =  lambda1  x  square of the magnitude of the coefficients**\n<br\/>\n\nBy shrinking the coefficients Ridge Regession reduces the model complexity and multi-collinearity. But it keeps all the variables.","49a55e22":"##### Horizontal Barplot for observing the significance of each features","deddcbab":"<br\/>\n<br\/>\n\n## 2.6 Lasso Regression","cd9b13e9":"<br\/>\n<br\/>\n\n## 2.2 Data processing","50513c82":"##### Mean Squared Error","32e07096":"##### Mean Squared Error","312e9856":"##### To avoid unwanted warnings","e4acc241":"##### Spliting into train and test set","2cde20ce":"<br\/>\n\n## 1.3 Lasso Regression\n\nIt's also called L1 regularization. It shrinks the coefficients by adding a penalty to the least squares.\n<br\/>\n\n**Lasso Regression penalty =  lambda2  x  |the magnitude of the coefficients|**\n<br\/>\n\nBy shrinking the coefficients Lasso Regession also reduces the model complexity and multi-collinearity like Ridge Regression. But in Lasso Regression, the coefficients of less important variables get multiplied by zero and removed.","86ce979a":"<br\/>\n<br\/>\n\n## 2.7 Lasso Regression with GridSearchCV","adde07b7":"<br\/>\n\n## 1.6 Criteria to choose regularization method\n<br\/>\n\n**Ridge Regression**\n1. When all the features you have are important to your model  \n2. When you don't want to do feature selection as well as feature removing  \n<br\/>\n\n**Lasso Regression**\n1. When you have too many features  \n2. And you know some of them don't have any significance to your model  \n3. When you want to remove the features with less importance  \n<br\/>\n\n**Elastic Net Regression**\n1. When you don't know whether all the features have significance or not  \n2. when there are strong correlations between features","c47672f9":"##### Mean Squared Error","b16c2cb9":"<br\/>\n<br\/>\n\n## 2.8 Elastic Net Regression","68841a0a":"![okol.png](attachment:okol.png)","6a228a58":"##### Load the dataset"}}