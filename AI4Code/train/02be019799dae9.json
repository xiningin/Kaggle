{"cell_type":{"6d84aca6":"code","27604da0":"code","5325a584":"code","5dc4076f":"code","e9eddad8":"code","04dc8ed4":"code","5f44886f":"code","c862431b":"code","db0826c7":"code","dd580731":"code","a2db4c0d":"code","a16b6ef9":"code","c7d00b63":"code","049247bc":"code","c3306130":"code","bdaa1ac7":"code","47d05f5f":"code","3b7c1656":"code","6b3d144f":"code","e9173c38":"code","d3b70fff":"code","6c1d1970":"code","b25f49c3":"code","611d97ea":"code","15464135":"code","5de6afcf":"markdown","11cfb77b":"markdown"},"source":{"6d84aca6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","27604da0":"dataframe=pd.read_csv('..\/input\/heart.csv')\ndataframe.tail()\n","5325a584":"dataframe.info()","5dc4076f":"#%% x ve y axis\ny=dataframe.target.values   # values => np array\nx_data=dataframe.drop([\"target\"],axis=1)","e9eddad8":"y","04dc8ed4":"#%% normalization   feature scaling\nx=(x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","5f44886f":"x.head()","c862431b":"#%% train test splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)","db0826c7":"# holding results in a list\nscores_accuracy=[]","dd580731":"# Logistic regression classication\n# LR with sklearn\n\nfrom sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()\nlr.fit(x_train,y_train)\n\nlr_score = lr.score(x_test,y_test)\nscores_accuracy.append([\"LR\",lr_score])\n\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))","a2db4c0d":"# LR confusion matrix\ny_predict = lr.predict(x_test)\ny_true = y_test\ncm = confusion_matrix (y_true,y_predict)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict\")\nplt.ylabel(\"y_true\")\nplt.show()","a16b6ef9":"# KNN  classification\n# Knn with sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 9) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\nknn_score = knn.score(x_test,y_test)\nscores_accuracy.append([\"KNN\",knn_score])\nprint(\" {} nn score: {} \".format(9,knn.score(x_test,y_test)))\n","c7d00b63":"# KNN confusion matrix\ny_predict_knn = knn.predict(x_test)\ny_true_knn = y_test\ncm_knn = confusion_matrix (y_true_knn,y_predict_knn)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict_knn\")\nplt.ylabel(\"y_true_knn\")\nplt.show()","049247bc":"# Findind k values in range(1,15)\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\n# 3 is the best k value in range(1,15) i used 9 above.","c3306130":"# SVM Classification\n# SVM with sklearn\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nsvm_score = svm.score(x_test,y_test)\nscores_accuracy.append([\"SVM\",svm_score])\n\nprint(\"accuracy of svm algo: \", svm.score(x_test,y_test))\n","bdaa1ac7":"# SVM confusion matrix\ny_predict_svm = svm.predict(x_test)\ny_true_svm = y_test\ncm_svm = confusion_matrix (y_true_svm,y_predict_svm)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict_svm\")\nplt.ylabel(\"y_true_svm\")\nplt.show()","47d05f5f":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nnb_score = nb.score(x_test,y_test)\nscores_accuracy.append([\"NB\",nb_score])\n\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","3b7c1656":"# NB confusion matrix\ny_predict_nb = nb.predict(x_test)\ny_true_nb = y_test\ncm_nb = confusion_matrix (y_true_nb,y_predict_nb)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm_nb,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict_nb\")\nplt.ylabel(\"y_true_nb\")\nplt.show()","6b3d144f":"# check score regularly\n# del scores_accuracy[index] if you needed\n#del scores_accuracy[1]\nscores_accuracy","e9173c38":"# Decision Tree Classification\n# Decision Tree With Sklearn\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\ndt_score = dt.score(x_test,y_test)\nscores_accuracy.append([\"DT\",dt_score])\n\nprint(\"print accuracy of decision tree algo: \",dt.score(x_test,y_test))","d3b70fff":"# DT confusion matrix\ny_predict_dt = dt.predict(x_test)\ny_true_dt = y_test\ncm_dt = confusion_matrix (y_true_dt,y_predict_dt)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dt,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict_dt\")\nplt.ylabel(\"y_true_dt\")\nplt.show()","6c1d1970":"# Randon Forest Classification\n#Random Forest With Sklearn\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(n_estimators=100,random_state=1) # n_estimators= number of trees\nrf.fit(x_train,y_train)\n\nrf_score = rf.score(x_test,y_test)\nscores_accuracy.append([\"RF\",rf_score])\n\nprint(\"random forest result: \", rf.score(x_test,y_test))","b25f49c3":"# Rf confusion matrix\ny_predict_rf = rf.predict(x_test)\ny_true_rf = y_test\ncm_rf = confusion_matrix (y_true_rf,y_predict_rf)\nf, ax =plt.subplots(figsize=(5,5))\nsns.heatmap(cm_rf,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_predict_rf\")\nplt.ylabel(\"y_true_rf\")\nplt.show()","611d97ea":"scores_accuracy","15464135":"algorithms=(\"LR\",\"DT\",\"RF\",\"KNN\",\"NB\",\"SVM\")\nscores = (lr_score,dt_score,rf_score,knn_score,nb_score,svm_score)\ny_pos = np.arange(1,7)\ncolors = (\"red\",\"gray\",\"purple\",\"green\",\"orange\",\"blue\")\nplt.figure(figsize=(18,10))\nplt.bar(y_pos,scores,color=colors)\nplt.xticks(y_pos,algorithms,fontsize=18)\nplt.yticks(np.arange(0.00, 1.01, step=0.05))\nplt.grid()\nplt.suptitle(\"Bar Chart Comparison of Models\",fontsize=15)\nplt.show()\n","5de6afcf":"# Comparison Of Classification Models Accuracy \n* In this kernal i am going to study each models below and find accuracy of it then compare\n* Logistic Regression Classification\n* KNN   Classification\n* SVM Classification\n* Naive Byes Classification\n* Decision Tree Classification\n* Random Forest Classification\n","11cfb77b":"# Conclusion\n* According to test size=0.3(which means 70 % of data train 30% of daha test)\n* Best accuracy comes vith SVM  and result is = 0.8461538461538461"}}