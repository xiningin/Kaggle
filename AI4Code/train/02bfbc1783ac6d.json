{"cell_type":{"5bf62f94":"code","2679aabf":"code","03e273c6":"code","a12dca27":"code","fe30e7f2":"code","a56d773f":"code","d1434c32":"code","adef90e6":"code","0457568c":"code","40a323f4":"code","54d30d0f":"code","6557a23b":"code","a0d6c9fd":"code","982da4e1":"markdown"},"source":{"5bf62f94":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndata = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndata.info() # It appears that we have missing data for Ph Values, Sulfate and Trihalomethanes\nsns.set_style('whitegrid')\nsns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='coolwarm') # This is a nice visual way to see what is missing","2679aabf":"sns.distplot(data['ph'].dropna(),bins = 30,color = 'blue' )","03e273c6":"sns.distplot(data['Sulfate'].dropna(),bins = 30, color = 'green')","a12dca27":"sns.distplot(data['Trihalomethanes'].dropna(),bins = 30, color = 'red')","fe30e7f2":"#These graphs all appear to show a normal (or Gaussian) Distribution, let's investigate this so we can accurately \n#Fill in missing values.\ndata[['ph','Sulfate','Trihalomethanes']].describe()\nnd_mean = data[['ph','Sulfate','Trihalomethanes']].mean()\nnd_std = data[['ph','Sulfate','Trihalomethanes']].std()\n\nnd_lb = nd_mean - 2*nd_std\nnd_ub = nd_mean + 2*nd_std #Here we are trying to see if 95% of data lies in a 2sigma range of the mean\n\n(data[(data['ph'] >= nd_lb['ph']) & (data['ph'] <= nd_ub['ph'])].count())\/data.count() # The ph metric does adhear to this\n(data[(data['Sulfate'] >= nd_lb['Sulfate']) & (data['Sulfate'] <= nd_ub['Sulfate'])].count())\/data.count() #92%\n(data[(data['Trihalomethanes'] >= nd_lb['Trihalomethanes']) & (data['Trihalomethanes'] <= nd_ub['Trihalomethanes'])].count())\/data.count()#95%\n#These metrics adhear to the 2Sigma rule and so we will fill in missing values using a random sampling from a normal distrbution\n#We will now try to impute the missing values with values from the normal distribution.\ndata.fillna({'ph': np.random.normal( loc = nd_mean['ph'], scale = nd_std['ph']), 'Sulfate': np.random.normal( loc = nd_mean['Sulfate'], scale = nd_std['Sulfate']), 'Trihalomethanes': np.random.normal( loc = nd_mean['Trihalomethanes'], scale = nd_std['Trihalomethanes'])},inplace = True)\ndata.isnull().sum()\n\n\n#We have imputed the missing values with a normally distributed number for each of the columns. This does neglect correlations between variables.","a56d773f":"data['Potability'].value_counts()\nplt.figure(figsize = (5,5))\ndata.Potability.value_counts().plot(kind ='pie')","d1434c32":"plt.figure(figsize = (10,8))\nsns.heatmap(data.drop('Potability', axis= 1).corr(), annot = True)\n# There isn't any immediately obvious correlations.","adef90e6":"#We now have a full dataset we can apply machine learning algorithms to.\n# we'll investigate which model is most accurate in predicting the potability of water.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nX = data.drop(['Potability'], axis = 1)\ny = data['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n","0457568c":"#We start with a Logistic Regression\nLR = LogisticRegression()\nLR.fit(X_train,y_train)\nResults=LR.predict(X_test)\nclassification_report(y_test,Results)","40a323f4":"#Decision Tree\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\npred = dtree.predict(X_test)\nclassification_report(y_test,pred)","54d30d0f":"#Random Forest\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\nprediction= rfc.predict(X_test)\nclassification_report(y_test,prediction)\nimportance = rfc.feature_importances_\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()\n#There doesn't appear to be a specific feature which is more important than any other","6557a23b":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\nPred= knn.predict(X_test)\nerror_rate=[]\nfor i in range (1,40):\n    knn= KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    Pred_i=knn.predict(X_test)\n    error_rate.append(np.mean(Pred_i != y_test))\n    plt.figure(figsize=(10,6))\n\nplt.plot(range(1,40),error_rate)\n         \nerror_rate.index(min(error_rate))\nknn = KNeighborsClassifier(n_neighbors=13)\nknn.fit(X_train,y_train)\nPred= knn.predict(X_test)\nclassification_report(y_test,Pred)\n# Here we used the K nearest neighbours algorithim. We used a for loop to find the no. neighbours which has the least error\n#we then used this value for n when we look at the classification report","a0d6c9fd":"# TO CONCLUDE:\n#Random Forest was the best in this case with 65% accuracy , K nearest neighbours was very close with 63%\n# The decision tree was 59% accurate and the logistic regression was by far least effective here with a measly 35%\n\n\n# Imported data and relevant diaries.\n#Checked for missing values\n# Checked if the variables which had missing values were appropriate to be modeled with a normal distrubtion (2 sigma rating)\n# Filled missing values with the a random number from the appropriate normal distribution\n# checked for correlations between variables, none were obvious.\n# ","982da4e1":"We can see that there are missing values for Ph Values, Sulfate and Trihalomethanes. I then used a heatmap to visualise this."}}