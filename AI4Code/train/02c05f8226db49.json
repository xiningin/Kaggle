{"cell_type":{"2dd5b1fc":"code","5c4bf0de":"code","91072de2":"code","d9f346dd":"code","16c498ac":"code","c9f905f0":"code","dac4c23a":"code","454c21b8":"code","6de895b3":"code","fbb5b7d9":"code","7dd99a3d":"code","58ff4d54":"code","5a4cfe81":"code","2d928da7":"code","606b5bb6":"code","5b0dfc3d":"code","931ee5e3":"code","f751b733":"code","971a626a":"code","c2811353":"code","0c6d58ae":"code","1172b895":"code","df79e893":"code","c68438a6":"code","3d643dce":"code","6b6c9ab1":"code","56a39592":"code","a873c517":"code","4a665286":"code","bff43b66":"code","4271bc38":"code","5f669b05":"code","dd2f9f24":"code","0dee4b6e":"code","569461b4":"code","a80e6a18":"code","68fadae9":"code","74fe2d2b":"code","54b8f9fd":"code","cbadd355":"code","b68daff3":"code","f1a48761":"code","5a348fc7":"code","6bc8ceb8":"code","722c130f":"code","795e9c99":"code","fac3a414":"code","b809ea1f":"code","5b709962":"code","59ffeece":"code","10d6754b":"code","f79cb8d3":"code","e66dc9c0":"code","dd07135a":"code","9465e7da":"code","b4792cbf":"code","7c08d9c2":"code","5f7bc985":"code","3367312c":"code","4808d7af":"code","ecbeae3e":"code","96591497":"code","d64867fd":"code","a45ba9b6":"code","697839e5":"code","02a1e75c":"code","8b57f7cc":"code","44cdbd7d":"code","70fcd591":"code","45b9d814":"code","a3b9960e":"code","47625674":"code","235fddb8":"code","24a3e8a6":"code","684fdd08":"code","7165fdc0":"code","c2840014":"code","fdca1119":"code","e9081b63":"code","e45abd84":"code","610aa56a":"code","4805a318":"code","d3deaa64":"code","ea8c198a":"code","fb257ba8":"code","7df32642":"code","1b5baf94":"code","a29c08aa":"code","8a1f7697":"code","c0d503a0":"code","8f7ee3cc":"code","29c7d053":"code","3f744c10":"code","8a412a7f":"code","c2a20fd5":"code","51c6b8d1":"code","9186c10c":"code","37b9a0ac":"code","e57bfcf4":"code","69121964":"code","9f429092":"code","565be3cb":"code","fdfd757b":"code","302c1339":"code","bcb3fcbc":"code","e6c30aa0":"code","244cc106":"code","df267d07":"code","16092df0":"code","534211a2":"code","fa42ca59":"code","8fd450c9":"code","28382dee":"code","1a801192":"code","f89b0976":"code","9aefe722":"code","543179c0":"code","328f4aba":"code","cb0aacfe":"code","8eb39834":"code","ac9af1f6":"code","b743ccb0":"code","845cf90a":"code","a94a1a62":"code","30bd8c63":"code","a9fbde42":"code","ce1f4977":"code","e2f9746c":"markdown","9ea62c80":"markdown","7119cf67":"markdown","8e371594":"markdown","f10ffeb2":"markdown","fd1cb75f":"markdown","0c3ca54f":"markdown","df7edcbd":"markdown","3ec88705":"markdown","f0275f1a":"markdown","451267ed":"markdown","fe289b7d":"markdown","5e470e54":"markdown","35a447a6":"markdown","b3a15312":"markdown","ec9f472a":"markdown","755e0aca":"markdown","6e08c875":"markdown","af6c4947":"markdown","2103c15e":"markdown","46ba5ad6":"markdown","536f70dc":"markdown","d9bb6496":"markdown","15c767f1":"markdown","52b1d2c4":"markdown","87209954":"markdown","a33fddb1":"markdown","385d635b":"markdown","8b4e42f7":"markdown","20974f8a":"markdown","333c8298":"markdown","67c9007e":"markdown","0c2a1886":"markdown","6a7929b9":"markdown","89905f0c":"markdown","94ac3bf9":"markdown","3f42815f":"markdown","2de2b8e2":"markdown","8062af9e":"markdown","3e7d17f1":"markdown","167bc642":"markdown","d779079d":"markdown","cb58a715":"markdown","407bc491":"markdown","92ac2d8d":"markdown","0c89c7bf":"markdown","c90431dc":"markdown","34dfa58b":"markdown","f0d525a6":"markdown","1eab9ddf":"markdown","fb6adfc1":"markdown","2ea53015":"markdown","6b467636":"markdown","0f43ba91":"markdown","b42c8ba4":"markdown","809e2b82":"markdown","de2e0486":"markdown","ec601724":"markdown","520749b2":"markdown","034bad34":"markdown","c7254101":"markdown","5d9a7e7e":"markdown","38712043":"markdown","d98a9f76":"markdown"},"source":{"2dd5b1fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport emoji\n\n#Count vectorizer for N grams\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n# Nltk for tekenize and stopwords\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\n# Visuvalization Libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\n\n#Loading data\ndf=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndf.head()","5c4bf0de":"def missing_value_of_data(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percentage=round(total\/data.shape[0]*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n","91072de2":"missing_value_of_data(df)\n#  Reason for 0 percentage value = Round of 1 divided by 27481 will be 0","d9f346dd":"df=df.dropna()","16c498ac":"def count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","c9f905f0":"count_values_in_column(df,'sentiment')","dac4c23a":"def unique_values_in_column(data,feature):\n    unique_val=pd.Series(data.loc[:,feature].unique())\n    return pd.concat([unique_val],axis=1,keys=['Unique Values'])\n","454c21b8":"unique_values_in_column(df,'sentiment')","6de895b3":"    \ndef duplicated_values_data(data):\n    dup=[]\n    columns=data.columns\n    for i in data.columns:\n        dup.append(sum(data[i].duplicated()))\n    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])","fbb5b7d9":"duplicated_values_data(df)","7dd99a3d":"df.describe()","58ff4d54":"def find_url(string): \n    text = re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n    return \"\".join(text) # converting return value from list to string","5a4cfe81":"sentence=\"I love spending time at https:\/\/www.kaggle.com\/\"\nfind_url(sentence)","2d928da7":"df['url']=df['text'].apply(lambda x:find_url(x))","606b5bb6":"def find_emoji(text):\n    emo_text=emoji.demojize(text)\n    line=re.findall(r'\\:(.*?)\\:',emo_text)\n    return line","5b0dfc3d":"sentence=\"I love \u26bd very much \ud83d\ude01\"\nfind_emoji(sentence)\n\n# Emoji cheat sheet - https:\/\/www.webfx.com\/tools\/emoji-cheat-sheet\/\n# Uniceode for all emoji : https:\/\/unicode.org\/emoji\/charts\/full-emoji-list.html","931ee5e3":"df['emoji']=df['text'].apply(lambda x: find_emoji(x))","f751b733":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","971a626a":"sentence=\"Its all about \\U0001F600 face\"\nprint(sentence)\nremove_emoji(sentence)","c2811353":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","0c6d58ae":"def find_email(text):\n    line = re.findall(r'[\\w\\.-]+@[\\w\\.-]+',str(text))\n    return \",\".join(line)","1172b895":"sentence=\"My gmail is abc99@gmail.com\"\nfind_email(sentence)","df79e893":"df['email']=df['text'].apply(lambda x: find_email(x))","c68438a6":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)","3d643dce":"sentence=\"#Corona is trending now in the world\" \nfind_hash(sentence)","6b6c9ab1":"df['hash']=df['text'].apply(lambda x: find_hash(x))","56a39592":"def find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)","a873c517":"sentence=\"@David,can you help me out\"\nfind_at(sentence)","4a665286":"df['at_mention']=df['text'].apply(lambda x: find_at(x))","bff43b66":"def find_number(text):\n    line=re.findall(r'[0-9]+',text)\n    return \" \".join(line)","4271bc38":"sentence=\"2833047 people are affected by corona now\"\nfind_number(sentence)","5f669b05":"df['number']=df['text'].apply(lambda x: find_number(x))","dd2f9f24":"def find_phone_number(text):\n    line=re.findall(r\"\\b\\d{10}\\b\",text)\n    return \"\".join(line)","0dee4b6e":"find_phone_number(\"9998887776 is a phone number of Mark from 210,North Avenue\")","569461b4":"df['phone_number']=df['text'].apply(lambda x: find_phone_number(x))","a80e6a18":"def find_year(text):\n    line=re.findall(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",text)\n    return line","68fadae9":"sentence=\"India got independence on 1947.\"\nfind_year(sentence)","74fe2d2b":"df['year']=df['text'].apply(lambda x: find_year(x))","54b8f9fd":"def find_nonalp(text):\n    line = re.findall(\"[^A-Za-z0-9 ]\",text)\n    return line","cbadd355":"sentence=\"Twitter has lots of @ and # in posts.(general tweet)\"\nfind_nonalp(sentence)","b68daff3":"df['non_alp']=df['text'].apply(lambda x: find_nonalp(x))","f1a48761":"def find_punct(text):\n    line = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', text)\n    string=\"\".join(line)\n    return list(string)","5a348fc7":"example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\"\nprint(find_punct(example))","6bc8ceb8":"df['punctuation']=df['text'].apply(lambda x : find_punct(x))","722c130f":"def stop_word_fn(text):\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(text) \n    non_stop_words = [w for w in word_tokens if not w in stop_words] \n    stop_words= [w for w in word_tokens if w in stop_words] \n    return stop_words","795e9c99":"example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\nstop_word_fn(example_sent)","fac3a414":"df['stop_words']=df['text'].apply(lambda x : stop_word_fn(x))","b809ea1f":"def ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df","5b709962":"ngrams_top(df['text'],(1,1),n=10)","59ffeece":"ngrams_top(df['text'],(2,2),n=10)","10d6754b":"ngrams_top(df['text'],(3,3),n=10)","f79cb8d3":"def rep(text):\n    grp = text.group(0)\n    if len(grp) > 1:\n        return grp[0:1] # can change the value here on repetition\ndef unique_char(rep,sentence):\n    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n    return convert","e66dc9c0":"sentence=\"heyyy this is loong textttt sooon\"\nunique_char(rep,sentence)","dd07135a":"df['unique_char']=df['text'].apply(lambda x : unique_char(rep,x))","9465e7da":"def find_dollar(text):\n    line=re.findall(r'\\$\\d+(?:\\.\\d+)?',text)\n    return \" \".join(line)\n\n# \\$ - dollar sign followed by\n# \\d+ one or more digits\n# (?:\\.\\d+)? - decimal which is optional","b4792cbf":"sentence=\"this shirt costs $20.56\"\nfind_dollar(sentence)","7c08d9c2":"df['dollar']=df['text'].apply(lambda x : find_dollar(x))","5f7bc985":"#Number greater than 930\ndef num_great(text): \n    line=re.findall(r'9[3-9][0-9]|[1-9]\\d{3,}',text)\n    return \" \".join(line)","3367312c":"sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\"\nnum_great(sentence)","4808d7af":"#Number greater than 930 (Just part of example)\ndf['num_great']=df['text'].apply(lambda x : num_great(x))","ecbeae3e":"# Number less than 930\ndef num_less(text):\n    only_num=[]\n    for i in text.split():\n        line=re.findall(r'^(9[0-2][0-0]|[1-8][0-9][0-9]|[1-9][0-9]|[0-9])$',i) # 5 500\n        only_num.append(line)\n        all_num=[\",\".join(x) for x in only_num if x != []]\n    return \" \".join(all_num)","96591497":"sentence=\"There are some countries where less than 920 cases exist with 1100 observations\"\nnum_less(sentence)","d64867fd":"#Number greater than 930 (Just part of example)\ndf['num_less']=df['text'].apply(lambda x : num_less(x))","a45ba9b6":"def or_cond(text,key1,key2):\n    line=re.findall(r\"{}|{}\".format(key1,key2), text) \n    return \" \".join(line)","697839e5":"sentence=\"sad and sorrow displays emotions\"\nor_cond(sentence,'sad','sorrow')","02a1e75c":"# Looks for sorrow or sad word\ndf['sad_or_sorrow']=df['text'].apply(lambda x : or_cond(x,'sad','sorrow'))","8b57f7cc":"def and_cond(text):\n    line=re.findall(r'(?=.*do)(?=.*die).*', text) \n    return \" \".join(line)","44cdbd7d":"print(\"Both string present:\",and_cond(\"do or die is a motivating phrase\"))\nprint(\"Only one string present :\",and_cond('die word is other side of emotion'))","70fcd591":"# Looks for do and die both else empty\ndf['do_and_die']=df['text'].apply(lambda x : and_cond(x))","45b9d814":"# mm-dd-yyyy format \ndef find_dates(text):\n    line=re.findall(r'\\b(1[0-2]|0[1-9])\/(3[01]|[12][0-9]|0[1-9])\/([0-9]{4})\\b',text)\n    return line\n","a3b9960e":"sentence=\"Todays date is 04\/28\/2020 for format mm\/dd\/yyyy, not 28\/04\/2020\"\nfind_dates(sentence)","47625674":"df['dates']=df['text'].apply(lambda x : find_dates(x))","235fddb8":"def only_words(text):\n    line=re.findall(r'\\b[^\\d\\W]+\\b', text)\n    return \" \".join(line)\n","24a3e8a6":"sentence=\"the world population has grown from 1650 million to 6000 million\"\nonly_words(sentence)","684fdd08":"df['only_words']=df['text'].apply(lambda x : only_words(x))","7165fdc0":"def only_numbers(text):\n    line=re.findall(r'\\b\\d+\\b', text)\n    return \" \".join(line)","c2840014":"sentence=\"the world population has grown from 1650 million to 6000 million\"\nonly_numbers(sentence)","fdca1119":"df['only_num']=df['text'].apply(lambda x : only_numbers(x))","e9081b63":"# Extracting word with boundary\ndef boundary(text):\n    line=re.findall(r'\\bneutral\\b', text)\n    return \" \".join(line)","e45abd84":"sentence=\"Most tweets are neutral in twitter\"\nboundary(sentence)","610aa56a":"df['bound']=df['text'].apply(lambda x : boundary(x))","4805a318":"def search_string(text,key):\n    return bool(re.search(r''+key+'', text))","d3deaa64":"sentence=\"Happy Mothers day to all Moms\"\nsearch_string(sentence,'day')","ea8c198a":"df['search_day']=df['text'].apply(lambda x : search_string(x,'day'))","fb257ba8":"def pick_only_key_sentence(text,keyword):\n    line=re.findall(r'([^.]*'+keyword+'[^.]*)', text)\n    return line","7df32642":"sentence=\"People are fighting with covid these days.Economy has fallen down.How will we survice covid\"\npick_only_key_sentence(sentence,'covid')","1b5baf94":"df['pick_senence']=df['text'].apply(lambda x : pick_only_key_sentence(x,'covid'))","a29c08aa":"def pick_unique_sentence(text):\n    line=re.findall(r'(?sm)(^[^\\r\\n]+$)(?!.*^\\1$)', text)\n    return line","8a1f7697":"sentence=\"I thank doctors\\nDoctors are working very hard in this pandemic situation\\nI thank doctors\"\npick_unique_sentence(sentence)","c0d503a0":"df['pick_unique']=df['text'].apply(lambda x : pick_unique_sentence(x))","8f7ee3cc":"def find_capital(text):\n    line=re.findall(r'\\b[A-Z]\\w+', text)\n    return line","29c7d053":"sentence=\"World is affected by corona crisis.No one other than God can save us from it\"\nfind_capital(sentence)","3f744c10":"df['caps_word']=df['text'].apply(lambda x : find_capital(x))","8a412a7f":"df['text_length']=df['text'].str.split().map(lambda x: len(x))\ndf[['text','text_length']].sample(3)","c2a20fd5":"df['char_length']=df['text'].str.len()\ndf[['text','char_length']].sample(3)","51c6b8d1":"def find_id(text):\n    line=re.findall(r'\\bIND(\\d+)', text)\n    return line","9186c10c":"sentence=\"My company id is IND50120.And I work under Asia region\"\nfind_id(sentence)","37b9a0ac":"df['get_id']=df['text'].apply(lambda x : find_id(x))","e57bfcf4":"my_string_rows = df[df['text'].str.contains(\"good\")]\nmy_string_rows[['text']].sample(3)","69121964":"df.head()","9f429092":"rcParams[\"figure.figsize\"] = 15,10\ndf[\"sentiment\"].value_counts().plot(kind=\"pie\")","565be3cb":"rcParams[\"figure.figsize\"] = 15,10\nsns.countplot(x=df[\"sentiment\"],hue=df[\"sentiment\"])","fdfd757b":"from collections import Counter\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nimport string\nfrom wordcloud import WordCloud","302c1339":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","bcb3fcbc":"df['text'] = df['text'].apply(lambda x:clean_text(x))","e6c30aa0":"df['temp_list'] = df['text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","244cc106":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', width=700, height=700,color='Common_words')\nfig.show()","df267d07":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","16092df0":"def generate_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black').generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()","534211a2":"total_text = df.text.values[:100]\ngenerate_word_cloud(total_text)","fa42ca59":"def remove_stopword(x):\n    return [w for w in x if not w in stop]","8fd450c9":"df['temp_list'] = df['temp_list'].apply(lambda x:remove_stopword(x))","28382dee":"top = Counter([item for sublist in df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","1a801192":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', width=700, height=700,color='Common_words')\nfig.show()","f89b0976":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","9aefe722":"positive = df[df[\"sentiment\"]==\"positive\"]\nnegative = df[df[\"sentiment\"]==\"negative\"]\nneutral = df[df[\"sentiment\"]==\"neutral\"]","543179c0":"positive['temp_list'] = positive['text'].apply(lambda x:str(x).split())\npotive_top = Counter([item for sublist in df['temp_list'] for item in sublist])\npositive_temp = pd.DataFrame(potive_top.most_common(20))\npositive_temp.columns = ['Common_words','count']\npositive_temp.style.background_gradient(cmap='Blues')","328f4aba":"fig = px.bar(positive_temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', width=700, height=700,color='Common_words')\nfig.show()","cb0aacfe":"fig = px.treemap(positive_temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","8eb39834":"positive_total_text = positive.text.values[:100]\ngenerate_word_cloud(total_text)","ac9af1f6":"negative_total_text = negative.text.values[:100]\ngenerate_word_cloud(negative_total_text)","b743ccb0":"neutral_total_text = neutral.text.values[:100]\ngenerate_word_cloud(neutral_total_text)","845cf90a":"!pip install webcolors\nimport webcolors","a94a1a62":"def find_color(string): \n    text = re.findall('\\#(?:[0-9a-fA-F]{3}){1,2}',string)\n    conv_name=[]\n    for i in text:\n        conv_name.append(webcolors.hex_to_name(i))\n    return conv_name","30bd8c63":"sentence=\"Find the color of #00FF00 and #FF4500\"\nfind_color(sentence)","a9fbde42":"df.sample(10)\n# We will see empty values too as most of text may not have related feature.You can filter and check.","ce1f4977":"# **Version 16** : <font size=\"+3\" color=\"red\"><b><i>Loading...<\/i><\/b><\/font><br><br>\n# <a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","e2f9746c":"<font size=\"+3\" color=\"blue\"><b>2. Data<\/b><\/font><br><a id=\"2\"><\/a>\n\nI will be using data from [Twitter Sentiment Analysis](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/data) competiton","9ea62c80":"<font size=\"+3\" color=\"blue\"><b>5. End Notes<\/b><\/font><br> <a id=\"5\"><\/a>","7119cf67":"<font size=\"+2\" color=\"indigo\"><b>4.17 OR<\/b><\/font><br><a id=\"4.17\"><\/a>","8e371594":"<a id=\"4.24\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.24 Pick Sentence<\/b><\/font><br><br>\nIf we want to get all sentence with particular keyword.We can use below function","f10ffeb2":"If you want to change match repetitive characters to n numbers,**chage the return line in the *rep function* to grp[0:n]**.","fd1cb75f":"# Basic Visuvalizations to understand the data","0c3ca54f":"### Find and convert emoji to text","df7edcbd":"<font size=\"+3\" color=\"blue\"><b>3. Basic Data Explorers<\/b><\/font><br> <a id=\"3\"><\/a>","3ec88705":"<font size=\"+2\" color=\"indigo\"><b>4.9 Non Alphanumeric<\/b><\/font><br><a id=\"4.9\"><\/a>","f0275f1a":">Target Distribution","451267ed":"Try more hex codes:https:\/\/www.rapidtables.com\/web\/css\/css-color.html","fe289b7d":"<font size=\"+2\" color=\"indigo\"><b>4.18 AND<\/b><\/font><br><a id=\"4.18\"><\/a>","5e470e54":"# WordCloud","35a447a6":"<font size=\"+2\" color=\"indigo\"><b>3.5 Stat<\/b><\/font><br><a id=\"3.5\"><\/a>","b3a15312":"<a id='top'><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><center>Table of content<\/center><\/h2>\n* [1. Objective](#1)   \n* [2. Data](#2)\n* [3. Basic Data Explorers](#3)\n    - [3.1 Missing values](#3.1)\n    - [3.2 Count values](#3.2)\n    - [3.3 Unique values](#3.3)\n    - [3.4 Duplicate values](#3.4)\n    - [3.5 Stat](#3.5)\n* [4. Regex Helpers](#4)\n    - [4.1 URL](#4.1)\n    - [4.2 Emoticons](#4.2)\n    - [4.3 Email](#4.3)\n    - [4.4 Hash](#4.4)\n    - [4.5 Mention](#4.5)\n    - [4.6 Number](#4.6)\n    - [4.7 Phone Number](#4.7)\n    - [4.8 Year](#4.8)\n    - [4.9 Non Alphanumeric](#4.9)\n    - [4.10 Punctuations](#4.10)\n    - [4.11 Stopwords](#4.11)\n    - [4.12 N-grams](#4.12)\n    - [4.13 Repetitive Character](#4.13)\n    - [4.14 Dollar](#4.14)\n    - [4.15 Number-Greater](#4.15)\n    - [4.16 Number- Lesser](#4.16)\n    - [4.17 OR](#4.17)\n    - [4.18 AND](#4.18)\n    - [4.19 Dates](#4.19)\n    - [4.20 Only Words](#4.20)\n    - [4.21 Only Numbers](#4.21)\n    - [4.22 Boundaries](#4.22)\n    - [4.23 Search](#4.23)\n    - [4.24 Pick Sentence](#4.24)\n    - [4.25 Duplicate Sentence](#4.25)\n    - [4.26 Caps Words](#4.26)\n    - [4.27 Length of Words](#4.27)\n    - [4.28 Length of Characters](#4.28)\n    - [4.29 Get ID](#4.29)\n    - [4.30 Specific String Rows](#4.30)\n    - [4.31 Hex code to Color](#4.31)\n    \n* [5. End Notes](#5)\n    ","ec9f472a":"<font size=\"+2\" color=\"indigo\"><b>4.12 N-grams<\/b><\/font><br><a id=\"4.12\"><\/a>","755e0aca":"## Bar-chart","6e08c875":"<font size=\"+2\" color=\"chocolate\"><b>Reference<\/b><\/font><br>\n* https:\/\/www.guru99.com\/python-regular-expressions-complete-tutorial.html\n* https:\/\/docs.python.org\/3.4\/library\/re.html\n* https:\/\/www3.ntu.edu.sg\/home\/ehchua\/programming\/howto\/Regexe.html#zz-1.9\n* https:\/\/www.debuggex.com\/cheatsheet\/regex\/python\n* https:\/\/blog.finxter.com\/python-regex-and-operator-tutorial-video\/\n* https:\/\/www.oreilly.com\/library\/view\/regular-expressions-cookbook\/9781449327453\/ch04s04.html\n* https:\/\/www.webfx.com\/tools\/emoji-cheat-sheet\/","af6c4947":"<font size=\"+2\" color=\"indigo\"><b>4.11 Stopwords<\/b><\/font><br><a id=\"4.11\"><\/a>","2103c15e":"<font size=\"+3\" color=purple ><b> <center><u>Text Helper Functions<\/u><\/center><\/b><\/font>","46ba5ad6":"<a id=\"4.28\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.28 Length of characters<\/b><\/font><br><br>\nNo regex but added one liner to identify length of characters in a sentence including space","536f70dc":"<font size=\"+2\" color=\"indigo\"><b>3.1 Missing values<\/b><\/font><br><a id=\"3.1\"><\/a>","d9bb6496":"<font size=\"+2\" color=\"indigo\"><b>4.6 Number<\/b><\/font><br><a id=\"4.6\"><\/a>\n\nPick only number from sentence","15c767f1":"<font size=\"+2\" color=\"indigo\"><b>4.19 Dates<\/b><\/font><br><a id=\"4.19\"><\/a>","52b1d2c4":"<font size=\"+2\" color=\"indigo\"><b>4.5 Mention<\/b><\/font><br><a id=\"4.5\"><\/a>\n\n@ - Used to mention someone in tweets","87209954":"<font size=\"+2\" color=\"indigo\"><b>4.14 Dollar<\/b><\/font><br><a id=\"4.14\"><\/a>","a33fddb1":"### Remove Emoji from text","385d635b":"<a id=\"4.22\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.22 Boundaries<\/b><\/font><br><br>\nPicking up the words with boundaries","8b4e42f7":"<font size=\"+2\" color=\"indigo\"><b>4.10 Punctuations<\/b><\/font><br><a id=\"4.10\"><\/a>\n\nRetrieve punctuations from sentence.","20974f8a":"<font size=\"+2\" color=\"indigo\"><b>4.16 Number Lesser<\/b><\/font><br><a id=\"4.16\"><\/a>","333c8298":"## *Data with new Features*","67c9007e":"Since these kind of basic python functions are pretty much well known to all python users.We dont need much focus here.Still I will elaborate this section in upcoming versions.","0c2a1886":"<font size=\"+3\" color=\"blue\"><b>4. Regex Helpers<\/b><\/font><br> \n<a id=\"4\"><\/a>","6a7929b9":"Major RE functions\n\n* **re.findall**   - Module is used to search for \u201call\u201d occurrences that match a given pattern.\n* **re.sub**       - Substitute the matched RE patter with given text\n* **re.match**     - The match function is used to match the RE pattern to string with optional flags\n* **re.search**    - This method takes a regular expression pattern and a string and searches for that pattern with the string.\n\n\nWe will be mostly using re.findall to detect patterns.","89905f0c":"<a id=\"4.25\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.25 Duplicate Sentence<\/b><\/font><br><br>\nMost webscrapped data contains duplicated sentence.This function could retrieve unique ones.","94ac3bf9":"<a id=\"4.30\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.30 Specific String Rows<\/b><\/font><br><br>\nQuering for specific string can also be done by directly applying *\"str.contains(\"XXXX\")\"* to a series\/column of a dataframe","3f42815f":"<font size=\"+2\" color=\"indigo\"><b>4.15 Number-Greater<\/b><\/font><br><a id=\"4.15\"><\/a>","2de2b8e2":"<font size=\"+2\" color=\"indigo\"><b>4.1 URL<\/b><\/font><br><a id=\"4.1\"><\/a>","8062af9e":"## After Pre-Processing","3e7d17f1":"We will drop NA values","167bc642":"<a id=\"4.31\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.31 Hex code to Color<\/b><\/font><br><br>\nConverting hex color codes to color names.We will install and import webcolors. (only for CSS3 colors)","d779079d":"## Pie chart","cb58a715":"And yes,if you above kernels,please give an appreciation to me via an <font size=\"+1\" color=\"red\"><b>Upvote<\/b><\/font>.","407bc491":"<font size=\"+2\" color=\"indigo\"><b>3.3 Unique Values<\/b><\/font><br><a id=\"3.3\"><\/a>","92ac2d8d":"[](http:\/\/)<font size=\"+2\" color=\"indigo\"><b>3.2 Count Values<\/b><\/font><br><a id=\"3.2\"><\/a>","0c89c7bf":"<font size=\"+3\" color=\"blue\"><b>1. Objective<\/b><\/font><br><a id=\"1\"><\/a>","c90431dc":"<font size=\"+2\" color=\"indigo\"><b>4.3 Email<\/b><\/font><br><a id=\"4.3\"><\/a>\n\nExtract email from text","34dfa58b":"<font size=\"+2\" color=\"indigo\"><b>4.2 Emoticons<\/b><\/font><br><a id=\"4.2\"><\/a>","f0d525a6":"<a id=\"4.21\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.21 Only Numbers<\/b><\/font>","1eab9ddf":"<font size=\"+2\" color=\"indigo\"><b>4.13 Repetitive Character<\/b><\/font><br><a id=\"4.13\"><\/a>","fb6adfc1":"<a id=\"4.29\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.29 Get ID<\/b><\/font><br><br>\nMost data has IDs in it with some prefix.So if we want to pick only numbers in ID leaving the prefix out,we can apply below function.","2ea53015":"The aim of this kernel is to provide helper function for basic text processing.This functions can aid you to understand the data much better and perform EDA.\nMy major focus will be on apply regex on text but still i have mentioned few basic starter codes on the 3rd part of kernel.Rest all sections will deal with text preprocessing.The whole kernel can be useful for everyone especially **beginners**.\n\n<font size=\"+1\"><i>Readers,I am making you lazy to code but at same time I am helping you out.Do utilize this kernel for any of your text oriented competitions.<\/i><\/font><br><br>\n    \n<font size=\"+1\" color=chocolate ><b>Please appreciate me through your Upvote.<\/b><\/font>","6b467636":"<font size=\"+2\" color=\"brown\"><i><b><center>\"With hard work, you can get fire out of a stone.\"<\/center><\/b><\/i><\/font>","0f43ba91":"<a id=\"4.27\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.27 Length of words<\/b><\/font><br><br>\nNo regex but added one liner to identify length of words in a sentence","b42c8ba4":"<a id=\"4.26\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.26 Caps Words<\/b><\/font><br><br>\nExtract words starting with capital letter.Some words like names,place or universal object are usually mentioned in a text starting with CAPS.","809e2b82":"No worries.This is not the end of kernel.**I have updated this kernel with few more functions (Now version 14 is live)**.Going forward i will add more helper functions in upcoming versions.I would like to get appreciation from you with an \ud83d\udc4d .Please <font size=\"+1\" color=\"red\"><b>Upvote<\/b><\/font> and keep it in your favourite list.\n\nThanks for your patience.\n\n\n*Happy Kaggling!!!*.\n","de2e0486":"<!-- **Version 16** : <font size=\"+3\" color=\"red\"><b><i>Loading...<\/i><\/b><\/font><br><br>\n<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a> -->","ec601724":"<font size=\"+2\" color=\"indigo\"><b>3.4 Duplicate Values<\/b><\/font><br><a id=\"3.4\"><\/a>","520749b2":"<font size=\"+2\" color=\"indigo\"><b>4.20 Only Words<\/b><\/font><br><a id=\"4.20\"><\/a>","034bad34":"**Version 4**  :  First Run <br>\n**Version 6**  :  Added 9 new regex function required for text processing.<br>\n**Version 7**  :  Added few functions and modified scripts.<br>\n**Version 8**  :  Updated kernel with few more functions and modified scripts.<br>\n**Version 9**  :  Modified script <br>\n**Version 10** :  Added description and modified script in 4.13 <br>\n**Version 11** :  Added few more helpers<br>\n**Version 12** :  Few modifications and added few more helpers<br>\n**Version 13** :  Added two more functions.<br>\n**Version 14** :  Small modification<br>\n**Version 15** :  Added more function<br>\n**Version 16** :  *Loading...*","c7254101":"<font size=\"+2\" color=\"indigo\"><b>4.8 Year<\/b><\/font><br><a id=\"4.8\"><\/a>\n\nExtract year from 1940 till 2020","5d9a7e7e":"<a id=\"4.23\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4.23 Search<\/b><\/font><br><br>\nIs the key word present in the sentence?","38712043":"<font size=\"+2\" color=\"indigo\"><b>4.4 Hash<\/b><\/font><br><a id=\"4.4\"><\/a>\n\nThis value is especially to denote trends in twitter.","d98a9f76":"<font size=\"+2\" color=\"indigo\"><b>4.7 Phone Number<\/b><\/font><br><a id=\"4.7\"><\/a>\n\nIndian Mobile numbers have ten digit.I will write that pattern below"}}