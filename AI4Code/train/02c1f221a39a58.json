{"cell_type":{"14301922":"code","4df983e0":"code","855e4faa":"code","eac214af":"code","b1a4a149":"code","78298ad0":"code","f50d2791":"code","ec8527e3":"code","096bbd7b":"code","18d041a9":"code","de1e0efe":"code","7a64e159":"code","144b82c6":"code","84bf3eb0":"code","5c4ce6a5":"code","79476f2e":"code","09702a81":"code","d2bbe514":"code","b52daa00":"code","257ba23f":"code","d27fa789":"code","96929815":"code","c599ef41":"code","04d369a3":"code","dec34b7c":"code","f937ab62":"code","ff31c8e2":"code","4c839d48":"code","f5c945b5":"code","90e3eb3e":"code","f1f73a53":"code","21d45609":"code","4f74a7cb":"code","ced5ff23":"code","ddbdd082":"code","ba14394b":"code","da66faf5":"code","445e8f8a":"code","6b4bdd1e":"code","d6a77078":"code","b8a6ed22":"code","d24516df":"code","3626fd6f":"code","dbf0c2aa":"code","8454821d":"code","bd4224bf":"code","f953f8fc":"code","402b02cb":"code","ea35bb33":"code","66724470":"code","1373ecfd":"markdown","f0520757":"markdown","06ddce2e":"markdown","5f0d31da":"markdown","aacd0128":"markdown","100f26d8":"markdown","0c9ec035":"markdown","2910daed":"markdown","f9076cb3":"markdown","38ea6ee3":"markdown","3a77d5e8":"markdown","a394c924":"markdown","6a118ccd":"markdown","1f29c322":"markdown","0e7a2efc":"markdown","81087fbd":"markdown","d3b28710":"markdown","abe18d1b":"markdown","1b144a69":"markdown"},"source":{"14301922":"''' Library Import'''\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4df983e0":"''' SK-Learn Library Import'''\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RandomizedLasso,LassoLarsCV\nfrom sklearn.exceptions import ConvergenceWarning \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\nimport sklearn.datasets ","855e4faa":"'''Scipy, Stats Library'''\nfrom scipy.stats import skew","eac214af":"''' To Ignore Warning'''\nimport warnings\nwarnings.filterwarnings('ignore')","b1a4a149":"''' To Do : Inline Priting of Visualizations '''\nsns.set()\n%matplotlib inline","78298ad0":"''' Importing Data : from the Archive Directly'''\ndf = pd.read_csv(r'..\/input\/abalone.csv')","f50d2791":"'''Display The head --> To Check if Data is Properly Imported'''\ndf.head()","ec8527e3":"''' Feature Information of the DataSet'''\ndf.info()","096bbd7b":"'''Feature Distirbution of data for Float and Int Data Type'''\ndf.describe()","18d041a9":"'''Numerical Features and Categorical Features'''\nnf = df.select_dtypes(include=[np.number]).columns\ncf = df.select_dtypes(include=[np.object]).columns","de1e0efe":"'''List of Numerical Features'''\nnf","7a64e159":"''' List of Categorical Features'''\ncf","144b82c6":"'''Histogram : to see the numeric data distribution'''\ndf.hist(figsize=(20,20), grid = True, layout = (2,4), bins = 30)","84bf3eb0":"'''After Seeing Above Graph of Data Distribution, I feel the Data is skewed, So checking for Skewness '''\nskew_list = skew(df[nf],nan_policy='omit') #sending all numericalfeatures and omitting nan values\nskew_list_df = pd.concat([pd.DataFrame(nf,columns=['Features']),pd.DataFrame(skew_list,columns=['Skewness'])],axis = 1)","5c4ce6a5":"skew_list_df.sort_values(by='Skewness', ascending = False)","79476f2e":"'''Missing Values '''\nmv_df = df.isnull().sum().sort_values(ascending = False)\npmv_df = (mv_df\/len(df)) * 100\nmissing_df = pd.concat([mv_df,pmv_df], axis = 1, keys = ['Missing Values','% Missing'])","09702a81":"missing_df","d2bbe514":"'''Target Column Analysis'''\nprint(\"Value Count of Rings Column\")\nprint(df.Rings.value_counts())\nprint(\"\\nPercentage of Rings Column\")\nprint(df.Rings.value_counts(normalize = True))","b52daa00":"print(len(df.Rings.unique()))","257ba23f":"'''Sex Count of Abalone, M - Male, F - Female, I - Infant'''\nsns.countplot(x='Sex', data = df)","d27fa789":"'''Sex Ratio in Abalone'''\nprint(\"\\nSex Count in Percentage\")\nprint(df.Sex.value_counts(normalize = True))\nprint(\"\\nSex Count in Numbers\")\nprint(df.Sex.value_counts())","96929815":"'''Small Feature Engineering, Deriving Age from Rings Column, Age = Rings + 1.5'''\ndf['Age'] = df['Rings'] + 1.5\ndf['Age'].head(5)","c599ef41":"'''Sex and Age Visulization'''\nplt.figure(figsize = (20,7))\nsns.swarmplot(x = 'Sex', y = 'Age', data = df, hue = 'Sex')\nsns.violinplot(x = 'Sex', y = 'Age', data = df)","04d369a3":"df.groupby('Sex')[['Length', 'Diameter', 'Height', 'Whole weight', \n                   'Shucked weight','Viscera weight', 'Shell weight', 'Age']].mean().sort_values(by = 'Age',ascending = False)","dec34b7c":"'''LabelEnconding the Categorical Data'''\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'].tolist())","f937ab62":"'''One Hot Encoding for Sex Feature '''\ntransformed_sex_feature = OneHotEncoder().fit_transform(df['Sex'].values.reshape(-1,1)).toarray()\ndf_sex_encoded = pd.DataFrame(transformed_sex_feature, columns = [\"Sex_\"+str(int(i)) for i in range(transformed_sex_feature.shape[1])])\ndf = pd.concat([df, df_sex_encoded], axis=1)","ff31c8e2":"df.head()","4c839d48":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['Rings','Age','Sex'], axis = 1)\nYtrain = df['Rings']","f5c945b5":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","90e3eb3e":"'''Creating Object of LogisticRegression'''\nlogreg = LogisticRegression()\n'''Learning from Training Set'''\nlogreg.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = logreg.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","f1f73a53":"'''For Both, LabelEncoding and OneHotEncoding -> The accuracy is 25 %'''\nresult_acc","21d45609":"'''Creating New Target Variable '''\ndf['newRings'] = np.where(df['Rings'] > 10,1,0)","4f74a7cb":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['newRings','Rings','Age','Sex'], axis = 1)\nYtrain = df['newRings']","ced5ff23":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","ddbdd082":"'''Creating Object of LogisticRegression'''\nlogreg = LogisticRegression()\n'''Learning from Training Set'''\nlogreg.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = logreg.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","ba14394b":"result_acc","da66faf5":"'''Importing SVM from SK-Learn'''\nfrom sklearn import svm","445e8f8a":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['Rings','Age','Sex'], axis = 1)\nYtrain = df['Rings']","6b4bdd1e":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","d6a77078":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='linear', C=1, gamma=1) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","b8a6ed22":"result_acc","d24516df":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='rbf', C=1, gamma=100) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","3626fd6f":"result_acc","dbf0c2aa":"'''Making a Copy of the primary DataSet'''\nnew_df = df.copy()","8454821d":"'''Feature Engineering , class 1 - 1-8, class 2 - 9-8, class 3 - 11 >'''\nnew_df['newRings_1'] = np.where(df['Rings'] <= 8,1,0)\nnew_df['newRings_2'] = np.where(((df['Rings'] > 8) & (df['Rings'] <= 10)), 2,0)\nnew_df['newRings_3'] = np.where(df['Rings'] > 10,3,0)","bd4224bf":"new_df['newRings'] = new_df['newRings_1'] + new_df['newRings_2'] + new_df['newRings_3']","f953f8fc":"'''Learning Features and Predicting Features'''\nXtrain = new_df.drop(['Rings','Age','Sex','newRings_1','newRings_2','newRings_3'], axis = 1)\nYtrain = new_df['newRings']","402b02cb":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","ea35bb33":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='rbf', C=1, gamma=100) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","66724470":"result_acc","1373ecfd":"###### Final Conclusion : we have not removed Outliers ( as we ad to capture all the type of different shapes and weights of abalone ), But with Less number of classes, SVM is giving an accuracy of 98% ( not Fully Tested ). ","f0520757":"###### Simple Logistic Regression Model\nNo of Classes : 28","06ddce2e":"###### No of Classes In Target","5f0d31da":"###### According to the rules\n- For a normally Distributed Data, Skewness should be greater than 0\n- Skewness > 0 , More weight is on the right tail of the distribution\n","aacd0128":"###### We can see, the Model Accuracy has increased with Tweaking SVM parameters, it is now 38 percent.\n- Lets Try to reduce the number of classes and see how the model is performing","100f26d8":"###### According to Described Information: \n\n- 1)No Feature has Minimum Value = 0, except *Height*\n- 2)All Features are not Normally Distributed, ( Theortically if feature is normally distributed, Mean = Median = Mode ).\n- 3)But Features are close to Normality\n- 4)All numerical, Except Sex\n- 5)Each Feature has Different Scale","0c9ec035":"###### Multi-Class Classification : When you have one target Column with 3 or more discreet values to predict, you state the problem as multi-class classification.","2910daed":"###### According to The above Graph\n- Male : Majority Between 7.5 to 19\n- Female : Majority Between 8 to 19\n- Infant : Majority Between 6 to < 10","f9076cb3":"##### So to Handle Multi-Class Classification, We can Try SVM Model, as it works well for multi-class and multi-label Classification","38ea6ee3":"###### We can see, the Model Accuracy has increased with SVM, it is now 37 percent.\n- Lets Try to tweak the model Learning Process and see if the accuracy is increases or not.","3a77d5e8":"######  Abalone Data-set - > Extension of EDA Kernel by [Rageeni Sah](https:\/\/www.kaggle.com\/ragnisah\/eda-abalone-age-prediction)\n\n- Model Insights \n- Different Classification Algorithms used,\n- Work Done by [Sriram Arvind Lakshmanakumar](https:\/\/www.kaggle.com\/sriram1204), [Nikhita Agarwal](https:\/\/www.kaggle.com\/nikhitaagr)","a394c924":"##### According to the Infomation: \n\n- 1)No-Null data\n- 2)1 - Object Type\n- 3)7 - Float Type\n- 4)1 - Int Type","6a118ccd":"###### References Used:\n\n- SVM -> https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/\n- Abalone DataSet -> https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/abalone\/\n- Logistic Regression -> Andrews Ng\n\n###### Things To DO: \n- Outlier Data Handling ( to be kept or Removed)\n- More Visulization for Outlier Data\n- Model Output VIsualization\n- More Tweaks on C and Gamma Parameter","1f29c322":"###### Simple Logistic Regression Model\n\n- No of Classes : 2\n- 1 - Rings > 10\n- 0 - Rings <= 10 ","0e7a2efc":"   ###### Preprocessing Data for the Model","81087fbd":"### Visualization","d3b28710":"###### We WIll first try with all the 28 classes in the target column, using linear kernel , Regularization parameter value as 1, and gamma 1","abe18d1b":"###### Data Splitting for Model\n- Learning Features\n- Predicting Feature\n- Train & Test Split","1b144a69":"##### Note : If you have Binary Classification, Logistic Regression is able to Boost to Higher Accuracy\n"}}