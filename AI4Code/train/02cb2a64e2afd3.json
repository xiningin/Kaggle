{"cell_type":{"a39a55b8":"code","12b2babb":"code","fff46d32":"code","11687f58":"code","db9ae1c7":"code","1e9477c1":"code","a9d2204e":"code","ae5678a2":"code","70a6105e":"code","b5eec24d":"code","16cb0f61":"code","861d658b":"code","5e7e713d":"code","2f8d4a5e":"code","29e6685e":"code","e09ca710":"code","2cfd183a":"code","f6e57083":"code","1bd402e8":"code","c1f54137":"code","bdae345a":"code","c0327e62":"code","d50978fc":"code","92aca1b5":"code","5c286867":"code","04c67bba":"code","14beacbe":"code","df7e198e":"code","3e5de383":"code","b2c1dccb":"code","0c352731":"code","1c349e4f":"code","f443c620":"code","ad7614dc":"code","0608e2df":"code","435da1a5":"code","1d03a6e9":"code","a3dae9ac":"code","acafd53d":"code","6d4a989b":"code","1ed8827a":"markdown","dfbf5e9f":"markdown","4803da3c":"markdown","95a5d7a9":"markdown","c008ee06":"markdown","53aeea1f":"markdown","27b62dda":"markdown","2237a8fa":"markdown","f0773b63":"markdown","bcde258c":"markdown","fa067a7a":"markdown","354b9b09":"markdown","728adcb9":"markdown","43e9eb11":"markdown","fca9a696":"markdown","170d71ad":"markdown","51ae9d8b":"markdown","eb7d3f49":"markdown","1a09c4b6":"markdown","9301f5a7":"markdown","61ef5e70":"markdown","1acf5ace":"markdown","240327e3":"markdown","b0375695":"markdown","b34a1aa5":"markdown","f2ef6411":"markdown","bee2144b":"markdown","d256d9f8":"markdown","e1a9a155":"markdown","c667c322":"markdown","56dbf7db":"markdown","ab2baf25":"markdown","509f0470":"markdown","277733e4":"markdown","540cdcc6":"markdown","94312fb6":"markdown","91c5ab43":"markdown","beed89e6":"markdown","41d12c89":"markdown","547e3d8d":"markdown","ae85ae82":"markdown","8a66f917":"markdown","a31f149c":"markdown","79441f35":"markdown","6532ab70":"markdown","44ca5fad":"markdown","4177c14f":"markdown","a67840aa":"markdown","3cbea8d6":"markdown","100ffe77":"markdown","fc5301f5":"markdown","6e4685a3":"markdown","4f99e203":"markdown","1c062ec1":"markdown","46a57108":"markdown","e0de8d2d":"markdown","7b26ba97":"markdown"},"source":{"a39a55b8":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","12b2babb":"# Load Iris dataset from Scikit-learn\nfrom sklearn.datasets import load_iris\n\n# Create input and output features\nfeature_names = load_iris().feature_names\nX_data = pd.DataFrame(load_iris().data, columns=feature_names)\ny_data = load_iris().target\n\n# Show the first five rows of the dataset\nX_data.head()","fff46d32":"# Import f_classif from Scikit-learn\nfrom sklearn.feature_selection import f_classif","11687f58":"# Create f_classif object to calculate F-value\nf_value = f_classif(X_data, y_data)\n\n# Print the name and F-value of each feature\nfor feature in zip(feature_names, f_value[0]):\n    print(feature)","db9ae1c7":"# Create a bar chart for visualizing the F-values\nplt.figure(figsize=(4,4))\nplt.bar(x=feature_names, height=f_value[0], color='tomato')\nplt.xticks(rotation='vertical')\nplt.ylabel('F-value')\nplt.title('F-value Comparison')\nplt.show()","1e9477c1":"# Import VarianceThreshold from Scikit-learn\nfrom sklearn.feature_selection import VarianceThreshold","a9d2204e":"# Create VarianceThreshold object to perform variance thresholding\nselector = VarianceThreshold()\n\n# Perform variance thresholding\nselector.fit_transform(X_data)\n\n# Print the name and variance of each feature\nfor feature in zip(feature_names, selector.variances_):\n    print(feature)","ae5678a2":"# Create a bar chart for visualizing the variances\nplt.figure(figsize=(4,4))\nplt.bar(x=feature_names, height=selector.variances_, color='tomato')\nplt.xticks(rotation='vertical')\nplt.ylabel('Variance')\nplt.title('Variance Comparison')\n\nplt.show()","70a6105e":"# Create VarianceThreshold object to perform variance thresholding\nselector = VarianceThreshold(threshold=0.2)\n\n# Transform the dataset according to variance thresholding\nX_data_new = selector.fit_transform(X_data)\n\n# Print the results\nprint('Number of features before variance thresholding: {}'.format(X_data.shape[1]))\nprint('Number of features after variance thresholding: {}'.format(X_data_new.shape[1]))","b5eec24d":"# Import mutual_info_classif from Scikit-learn\nfrom sklearn.feature_selection import mutual_info_classif","16cb0f61":"# Create mutual_info_classif object to calculate mutual information\nMI_score = mutual_info_classif(X_data, y_data, random_state=0)\n\n# Print the name and mutual information score of each feature\nfor feature in zip(feature_names, MI_score):\n    print(feature)","861d658b":"# Create a bar chart for visualizing the mutual information scores\nplt.figure(figsize=(4,4))\nplt.bar(x=feature_names, height=MI_score, color='tomato')\nplt.xticks(rotation='vertical')\nplt.ylabel('Mutual Information Score')\nplt.title('Mutual Information Score Comparison')\n\nplt.show()","5e7e713d":"# Import SelectKBest from Scikit-learn\nfrom sklearn.feature_selection import SelectKBest","2f8d4a5e":"# Create a SelectKBest object\nskb = SelectKBest(score_func=f_classif, # Set f_classif as our criteria to select features\n                  k=2)                  # Select top two features based on the criteria\n\n# Train and transform the dataset according to the SelectKBest\nX_data_new = skb.fit_transform(X_data, y_data)\n\n# Print the results\nprint('Number of features before feature selection: {}'.format(X_data.shape[1]))\nprint('Number of features after feature selection: {}'.format(X_data_new.shape[1]))","29e6685e":"# Print the name of the selected features\nfor feature_list_index in skb.get_support(indices=True):\n    print('- ' + feature_names[feature_list_index])","e09ca710":"# Import ExhaustiveFeatureSelector from Mlxtend\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS","2cfd183a":"# Import logistic regression from Scikit-learn\nfrom sklearn.linear_model import LogisticRegression","f6e57083":"# Create a logistic regression classifier\nlr = LogisticRegression()\n\n# Create an EFS object\nefs = EFS(estimator=lr,        # Use logistic regression as the classifier\/estimator\n          min_features=1,      # The minimum number of features to consider is 1\n          max_features=4,      # The maximum number of features to consider is 4\n          scoring='accuracy',  # The metric to use to evaluate the classifier is accuracy \n          cv=5)                # The number of cross-validations to perform is 5\n\n# Train EFS with our dataset\nefs = efs.fit(X_data, y_data)\n\n# Print the results\nprint('Best accuracy score: %.2f' % efs.best_score_) # best_score_ shows the best score \nprint('Best subset (indices):', efs.best_idx_)       # best_idx_ shows the index of features that yield the best score \nprint('Best subset (corresponding names):', efs.best_feature_names_) # best_feature_names_ shows the feature names \n                                                                     # that yield the best score","1bd402e8":"# Transform the dataset\nX_data_new = efs.transform(X_data)\n\n# Print the results\nprint('Number of features before transformation: {}'.format(X_data.shape[1]))\nprint('Number of features after transformation: {}'.format(X_data_new.shape[1]))","c1f54137":"# Show the performance of each subset of features\nefs_results = pd.DataFrame.from_dict(efs.get_metric_dict()).T\nefs_results.sort_values(by='avg_score', ascending=True, inplace=True)\nefs_results","bdae345a":"# Create a horizontal bar chart for visualizing \n# the performance of each subset of features\nfig, ax = plt.subplots(figsize=(12,9))\ny_pos = np.arange(len(efs_results))\nax.barh(y_pos, \n        efs_results['avg_score'],\n        xerr=efs_results['std_dev'],\n        color='tomato')\nax.set_yticks(y_pos)\nax.set_yticklabels(efs_results['feature_names'])\nax.set_xlabel('Accuracy')\nplt.show()","c0327e62":"# Import SequentialFeatureSelector from Mlxtend\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","d50978fc":"# Create a logistic regression classifier\nlr = LogisticRegression()\n\n# Create an SFS object\nsfs = SFS(estimator=lr,       # Use logistic regression as our classifier\n          k_features=(1, 4),  # Consider any feature combination between 1 and 4\n          forward=True,       # Set forward to True when we want to perform SFS\n          scoring='accuracy', # The metric to use to evaluate the classifier is accuracy \n          cv=5)               # The number of cross-validations to perform is 5\n\n# Train SFS with our dataset\nsfs = sfs.fit(X_data, y_data)\n\n# Print the results\nprint('Best accuracy score: %.2f' % sfs.k_score_)   # k_score_ shows the best score \nprint('Best subset (indices):', sfs.k_feature_idx_) # k_feature_idx_ shows the index of features \n                                                    # that yield the best score\nprint('Best subset (corresponding names):', sfs.k_feature_names_) # k_feature_names_ shows the feature names \n                                                                  # that yield the best score","92aca1b5":"# Transform the dataset\nX_data_new = sfs.transform(X_data)\n\n# Print the results\nprint('Number of features before transformation: {}'.format(X_data.shape[1]))\nprint('Number of features after transformation: {}'.format(X_data_new.shape[1]))","5c286867":"# Show the performance of each subset of features considered by SFS\nsfs_results = pd.DataFrame.from_dict(sfs.subsets_).T \nsfs_results","04c67bba":"# Create a horizontal bar chart for visualizing \n# the performance of each subset of features\nfig, ax = plt.subplots(figsize=(6,2))\ny_pos = np.arange(len(sfs_results))\nax.barh(y_pos, \n        sfs_results['avg_score'], \n        color='tomato')\nax.set_yticks(y_pos)\nax.set_yticklabels(sfs_results['feature_names'])\nax.set_xlabel('Accuracy')\nplt.show()","14beacbe":"# Create a logistic regression classifier\nlr = LogisticRegression()\n\n# Create an SBS object\nsbs = SFS(estimator=lr,       # Use logistic regression as our classifier\n          k_features=(1, 4),  # Consider any feature combination between 1 and 4\n          forward=False,      # Set forward to False when we want to perform SBS\n          scoring='accuracy', # The metric to use to evaluate the classifier is accuracy \n          cv=5)               # The number of cross-validations to perform is 5\n\n# Train SBS with our dataset\nsbs = sbs.fit(X_data.values, y_data, custom_feature_names=feature_names)\n\n# Print the results\nprint('Best accuracy score: %.2f' % sbs.k_score_)   # k_score_ shows the best score \nprint('Best subset (indices):', sbs.k_feature_idx_) # k_feature_idx_ shows the index of features \n                                                    # that yield the best score\nprint('Best subset (corresponding names):', sbs.k_feature_names_) # k_feature_names_ shows the feature names \n                                                                  # that yield the best score","df7e198e":"# Transform the dataset\nX_data_new = sbs.transform(X_data)\n\n# Print the results\nprint('Number of features before transformation: {}'.format(X_data.shape[1]))\nprint('Number of features after transformation: {}'.format(X_data_new.shape[1]))","3e5de383":"# Show the performance of each subset of features considered by SBS\nsbs_results = pd.DataFrame.from_dict(sbs.subsets_).T\nsbs_results","b2c1dccb":"# Create a horizontal bar chart for visualizing \n# the performance of each subset of features\nfig, ax = plt.subplots(figsize=(6,2))\ny_pos = np.arange(len(sbs_results))\nax.barh(y_pos, \n        sbs_results['avg_score'], \n        color='tomato')\nax.set_yticks(y_pos)\nax.set_yticklabels(sbs_results['feature_names'])\nax.set_xlabel('Accuracy')\nplt.show()","0c352731":"# Compare the selection generated by EFS, SFS, and SBS\nprint('Best subset by EFS:', efs.best_feature_names_)\nprint('Best subset by SFS:', sfs.k_feature_names_)\nprint('Best subset by SBS:', sbs.k_feature_names_)","1c349e4f":"# Import RandomForestClassifier from Scikit-learn\nfrom sklearn.ensemble import RandomForestClassifier","f443c620":"# Import train_test_split from Scikit-learn\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into 30% test and 70% training\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=0)","ad7614dc":"# Create a random forest classifier\nrfc = RandomForestClassifier(random_state=0, \n                             criterion='gini') # Use gini criterion to define feature importance\n\n# Train the classifier\nrfc.fit(X_train, y_train)\n\n# Print the name and gini importance of each feature\nfor feature in zip(feature_names, rfc.feature_importances_): \n    print(feature)","0608e2df":"from sklearn.feature_selection import SelectFromModel","435da1a5":"# Create a random forest classifier\nrfc = RandomForestClassifier(random_state=0, \n                             criterion='gini') # Use gini criterion to define feature importance\n\n# Create a SelectFromModel object \nsfm = SelectFromModel(estimator=rfc, # Use random forest classifier to identify features\n                      threshold=0.2) # that have an importance of more than 0.2\n\n# Train the selector\nsfm = sfm.fit(X_train, y_train)\n\n# Print the names of the most important features\nprint('The most important features based on random forest classifier:')\nfor feature_list_index in sfm.get_support(indices=True):\n    print('- ' + feature_names[feature_list_index])","1d03a6e9":"# Transform the dataset\nX_important_train = sfm.transform(X_train)\nX_important_test = sfm.transform(X_test)\n\n# Print the results\nprint('Number of features before transformation: {}'.format(X_train.shape[1]))\nprint('Number of features after transformation: {}'.format(X_important_train.shape[1]))","a3dae9ac":"# Import accuracy_score from Scikit-learn\nfrom sklearn.metrics import accuracy_score","acafd53d":"# Create a random forest classifier\nrfc_full = RandomForestClassifier(random_state=0, criterion='gini')\n\n# Train the classifier using dataset with full features\nrfc_full.fit(X_train, y_train)\n\n# Make predictions\npred_full = rfc_full.predict(X_test)\n\n# Generate accuracy score\nprint('The accuracy of classifier with full features: {:.2f}'.format(accuracy_score(y_test, pred_full)))","6d4a989b":"# Create a random forest classifier\nrfc_lim = RandomForestClassifier(random_state=0, criterion='gini')\n\n# Train the classifier with limited features\nrfc_lim.fit(X_important_train, y_train)\n\n# Make predictions\npred_lim = rfc_lim.predict(X_important_test)\n\n# Generate accuracy score\nprint('The accuracy of classifier with limited features: {:.2f}'.format(accuracy_score(y_test, pred_lim)))","1ed8827a":"EFS has five important parameters:\n- *estimator*: the classifier that we intend to train\n- *min_features*: the minimum number of features to select\n- *max_features*: the maximum number of features to select\n- *scoring*: the metric to use to evaluate the classifier\n- *cv*: the number of cross-validations to perform\n\nIn this example, we use logistic regression as our classifier\/estimator. ","dfbf5e9f":"We will do some demonstrations on how to implement each feature selection method in Python. To do that, we use Iris flower dataset. We import the Iris flower dataset from Scikit-learn by calling the following:","4803da3c":"# 2.0. Wrapper Methods\nWrapper methods try to find a subset of features that yield the best performance for a model by training, evaluating, and comparing the model with different combinations of features. Wrapper methods enable the detection of relationships among features. However, they can be computationally expensive, especially if the number of features is high. The risk of overfitting is also high if the number of instances in the dataset is insufficient. \n\nThere are some diferrences between filter and wrapper methods:\n- Filter methods do not incorporate a machine learning model in order to determine if a feature is good or bad whereas wrapper methods use a machine learning model and train it the feature to decide if it is essential or not.\n- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally costly, and in the case of massive datasets, wrapper methods are not the most effective feature selection method to consider.\n- Filter methods may fail to find the best subset of features in situations when there is not enough data to model the statistical correlation of the features, but wrapper methods can always provide the best subset of features because of their exhaustive nature.\n- Using features from wrapper methods in your final machine learning model can lead to overfitting as wrapper methods already train machine learning models with the features and it affects the true power of learning. But the features from filter methods will not lead to overfitting in most of the cases.\n\nWe will discuss three wrapper methods:\n1. [Exhaustive feature selection (EFS)](#section 2.1.)\n2. [Sequential forward selection (SFS)](#section 2.2.)\n3. [Sequential backward selection (SBS)](#section 2.3.)\n\nEach will be discussed in the following.","95a5d7a9":"Let's visualize the results by creating a bar chart:","c008ee06":"# References\nThis notebook cannot be created without the following references:","53aeea1f":"# 1.0. Filter Methods\nIn filter methods, features are selected independently from any machine algorithms. Filter methods generally use a specific criteria, such as scores in statistical test and variances, to rank the importance of individual features. Filter methods have some advantages:\n- Because of their independency to the selection of machine learning algorithms, they can be used as the input of any machine learning models. \n- They are generally effective in computation time.\n\nThe main weakness of filter methods is that they do not consider the relationships among features. That's why they are mainly used as the pre-processing step of any feature selection pipeline. We will discuss three types of filter selection methods:\n1. [ANOVA F-value](#section 1.1.)\n2. [Variance Threshold](#section 1.2.)\n3. [Mutual Information](#section 1.3.)","27b62dda":"We can transform the dataset into a new dataset containing only the subset of features that generates the best score by using **transform** method.","2237a8fa":"Let's visualize the results by creating a bar chart:","f0773b63":"<a id='section 3.0.'><\/a>","bcde258c":"## 1.3. Mutual Information\nMutual information (MI) measures the dependence of one variable to another by quantifying the amount of information obtained about one feature, through the other feature. MI is symmetric and non-negative, and is zero if and only if the input and output feature are independent. Unlike ANOVA F-value, mutual information can capture non-linear relationships between input and output feature.\n\nWe can use Scikit-learn to calculate MI. Scikit-learn has two functions to calculate MI:\n- **mutual_info_classif**, which calculate MI for classification task\n- **mutual_info_regression**, which calculate MI for regression task \n\nWe will use **mutual_info_classif** because the Iris dataset entails a classification task.","fa067a7a":"There is no difference between the dataset after and before the transformation because the subset that yields the best score include all of the features.\n\nWe can see the performance of each subset of features by calling **get_metric_dict**.","354b9b09":"## 3.3. Comparing the Accuracy of Classifier with Full Features and with Limited Features\nLet's compare the accuracy of a classifier with full features and a classifier with limited features (i.e., the top two important features based on random forest classifier).","728adcb9":"In this simple scenario, selecting the best combination of features out of the 4 available features in the Iris set, we end up with similar results regardless of which selection algorithms we used. In other cases with larger dataset and higher number of features, the selection is highly likely to be different for each selection algorithm.","43e9eb11":"## 1.4. Using Selector Object for Selecting Features\nWe can use **SelectKBest** from Scikit-learn to select features according to the k highest scores, determined by a filter method. First, we need to import **SelectKBest**. ","fca9a696":"We can transform the dataset into a new dataset containing only the most important features by using **transform** method.","170d71ad":"<a id='section 2.0.'><\/a>","51ae9d8b":"<a id='section 1.1.'><\/a>","eb7d3f49":"Then, we perform variance thresholding by calling the following:","1a09c4b6":"## 2.2. Sequential Forward Selection (SFS)\nSFS finds the best subset of feature by adding a feature that best improves the model at each iteration. \n\nMlxtend provides **SequentialFeatureSelector** function to perform SFS.","9301f5a7":"Let's visualize the performance of each subset of features by creating a horizontal bar chart:","61ef5e70":"### 2.3. Sequential Backward Selection (SBS)\nSBS is the opposite of SFS. SBS starts with all features and removes the feature that has the least importance to the model at each iteration. \n\nTo perform SBS use, we can use **SequentialFeatureSelector** by Mlxtend. This is the same function that we use to perform SFS. The different is we have to set the *k_features* parameter to False. ","1acf5ace":"There is no difference between the dataset after and before the transformation because the subset that yields the best score include all of the features.\n\nWe can see the performance of each subset of features considered by SFS by calling **subsets_**.","240327e3":"Let's visualize the performance of each subset of features by creating a horizontal bar chart:","b0375695":"<a id='section 1.3.'><\/a>","b34a1aa5":"## 2.1. Exhaustive Feature Selection (EFS)\nEFS finds the best subset of features by evaluating all feature combinations. Suppose we have a dataset with three features. EFS will evaluate the following feature combinations:\n- *feature_1*\n- *feature_2*\n- *feature_3*\n- *feature_1* and *feature_2*\n- *feature_1* and *feature_3*\n- *feature_2* and *feature_3*\n- *feature_1*, *feature_2*, and *feature_3*\n\nEFS selects a subset that generates the best performance (e.g., accuracy, precision, recall, etc.) of the model being considered.\n\nMlxtend provides **ExhaustiveFeatureSelector** function to perform EFS.","f2ef6411":"We can transform the dataset into a new dataset containing only the subset of features that generates the best score by using **transform** method.","bee2144b":"If we add up all the importance scores, the result is 100%. As we can see, **petal length** and **petal width** correspond to 83% of the total importance score. They are clearly the most important features!","d256d9f8":"<a id='section 1.2.'><\/a>","e1a9a155":"**SelectFromModel** has two important parameters:\n- *estimator*: the machine learning algorithm used to select features\n- *threshold*: the threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded.\n\nLet's demonstrate **SelectFromModel** by a random forest classifier with gini importance. We will select features that have importance above 0.2. ","c667c322":"**SelectKBest** has two important parameters:\n- *score_func*: the filter function that is used for feature selection \n- *k*: the number of top features to select \n\nLet's demonstrate **SelectKBest** by using ANOVA F-value as our filter method. We will select the top two features based on the ANOVA F-value.","56dbf7db":"As we can see, **VarianceThreshold** automatically eliminate features that have variance below 0.2. In this case, it removes **sepal width**, which has 0.188 variance. ","ab2baf25":"## 3.2. Using Selector Object for Selecting Features\nWe can use **SelectFromModel** from Scikit-learn to select features according to a threshold of feature importance. First, we need to import **SelectKBest**.","509f0470":"## 3.1. Feature Selection Using Random Forest\nRandom forest is one of the most popular learning algorithms used for feature selection in a data science workflow. As explained by Chris Albon:\n>*\"... the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features.\"*\n\nTo perform feature selection using random forest classifier, let's first import **RandomForestClassifier** from Scikit-learn.","277733e4":"As we can see, **SelectKBest** automatically eliminate two features that have the lowest F-value. \n\nWe can identify the name of selected features by calling **get_support** method. ","540cdcc6":"<a id='section 2.1.'><\/a>","94312fb6":"# Basics of Feature Selection with Python\nIn machine learning, feature selection is the process of choosing a subset of input features that contribute the most to the output feature for use in model construction. Feature selection is substantially important if we have datasets with high dimensionality (i.e., large number of features). High-dimensional datasets are not preferred because they have lengthy training time and have high risk of overfitting. Feature selection helps to mitigate these problems by selecting features that have high importance to the model, such that the data dimensionality can be reduced without much loss of the total information. Some benefits of feature selection are:\n- Reduce training time\n- Reduce the risk of overfitting\n- Potentially increase model's performance \n- Reduce model's complexity such that interpretation becomes easier\n\nThe objective of this notebook is to introduce the fundamental of feature selection in Python. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.  If you think this notebook gives something valuable to your journey to data science, PLEASE UPVOTE. It will keep me motivated.\n\nWe will discuss three key methods to perform feature selection together with their implementation in Python:\n1. [Filter methods](#section 1.0.)\n2. [Wrapper methods](#section 2.0.)\n3. [Embedded methods](#section 3.0.)\n\nBefore we get started, let's import the necessary Python libraries.","91c5ab43":"Please note that we use gini criterion to define feature importance. There are other criteria of importance, but we only limit our discussion to gini criterion.","beed89e6":"By default, **VarianceThreshold** removes only zero-variance features. Zero-variance feature means that the feature has the same value in all instances. Suppose we want to eliminate features that have variance score below 0.2, we can specify *threshold* parameter.","41d12c89":"EFS has five important parameters:\n- *estimator*: the classifier that we intend to train\n- *k_features*: the number of features to select. A tuple containing a min and max value can be provided, and the SFS will consider return any feature combination between min and max that scored highest in cross-validtion.\n- *forward*: use SFS if True and use SBS if False \n- *scoring*: the metric to use to evaluate the classifier\n- *cv*: the number of cross-validations to perform\n\nIn this example, we use logistic regression as our classifier\/estimator. ","547e3d8d":"<a id='section 2.3.'><\/a>","ae85ae82":"## 1.1. ANOVA F-value\nANOVA F-value method estimates the degree of linearity between the input feature (i.e., predictor) and the output feature. A high F-value indicates high degree of linearity and a low F-value indicates low degree of linearity. The main disadvantage of using ANOVA F-value is it only captures linear relationships between input and output feature. In other words, any non-linear relationships cannot be detected by F-value. \n\nWe can use Scikit-learn to calculate ANOVA F-value. First, we need to load the library. Scikit-learn has two functions to calculate F-value:\n- **f_classif**, which calculate F-value between input and output feature for classification task\n- **f_regression**, which calculate F-value between input and output feature for classification task\n\nWe will use **f_classif** because the Iris dataset entails classification task.","8a66f917":"It can be seen that we can reduce the number of features without significantly reduce the performance of the model.","a31f149c":"Let's compare the selection generated by EFS, SFS, and SBS.","79441f35":"## 1.2. Variance Threshold\nVariance threshold method removes features whose variance below a pre-defined cutoff value. It is based on the notion that features that do not vary much within themselves have low predictive power. The main weakness of variance threshold is that it does not consider  the relationship of input features with the output feature.\n\nIt should be noted that, before performing variance thresholding, all features should be standardized so they will have the same scale. \n\nScikit-learn provides **VarianceThreshold** function to perform variance threshold method.","6532ab70":"Then, we calculate F-value for each input feature in the Iris dataset by calling the following:","44ca5fad":"<a id='section 2.2.'><\/a>","4177c14f":"There is no difference between the dataset after and before the transformation because the subset that yields the best score include all of the features.\n\nWe can see the performance of each subset of features considered by SFS by calling **subsets_**.","a67840aa":"We need to split our dataset into train and test split because the feature selection is a part of the training process. ","3cbea8d6":"An alternative of **SelectKBest** is to use **SelectPercentile**, which select features according to a percentile of the highest scores.","100ffe77":"Let's visualize the results by creating a bar chart:","fc5301f5":"We can transform the dataset into a new dataset containing only the subset of features that generates the best score by using **transform** method.","6e4685a3":"Let's visualize the performance of each subset of features by creating a horizontal bar chart:","4f99e203":"Then, we calculate MI by calling the following:","1c062ec1":"Then, we perform EFS by calling the following:","46a57108":"<a id='section 1.0.'><\/a>","e0de8d2d":"# 3.0. Embedded Methods\nEmbedded methods combine the strong points of filter and wrapper methods by taking advantage of machine algorithms that have their own built-in feature selection process. They integrate a feature selection step as a part of the training process (i.e., feature selection and training process are performed simultaneously). Embedded methods generally have a more efficient process than wrapper methods because they eliminate the need to retrain every single subset of features being examined. Some of machine algorithms that can be used for feature selection are:\n- LASSO regression\n- Ridge regression\n- Decision tree\n- Random forest\n- Support vector machine\n\nIn the next section, we will focus on feature selection using random forest.","7b26ba97":"- [Feature Selection Using Random Forest by Chris Albon](https:\/\/chrisalbon.com\/machine_learning\/trees_and_forests\/feature_selection_using_random_forest\/)\n- [Why, How and When to apply Feature Selection](https:\/\/towardsdatascience.com\/why-how-and-when-to-apply-feature-selection-e9c69adfabf2)\n- [Introduction to Feature Selection methods with an example](https:\/\/www.analyticsvidhya.com\/blog\/2016\/12\/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables\/)\n- [An Introduction to Feature Selection](https:\/\/machinelearningmastery.com\/an-introduction-to-feature-selection\/)\n- [Beginner's Guide to Feature Selection in Python](https:\/\/www.datacamp.com\/community\/tutorials\/feature-selection-python)\n- [Sequential Feature Selector](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/)"}}