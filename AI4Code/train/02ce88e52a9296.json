{"cell_type":{"e75bfb5f":"code","cb176b2b":"code","098311b2":"code","5c0c0281":"code","dc1ce9a2":"code","e1bafcb0":"code","1f045000":"code","0e3e17e3":"code","17c38b22":"code","3107a916":"code","4f23aace":"code","7b132436":"code","cb0068d3":"code","fd2f439c":"code","695b59f7":"code","c14ff153":"code","53733633":"code","b9aa5e05":"code","cbc2a7f5":"code","a6b3099e":"code","b9af80ba":"code","34dbac83":"code","cf4797ad":"code","753099d6":"code","14203471":"code","18f694e7":"code","a0f880dd":"code","8c29e23e":"code","8c506e43":"code","017f1b72":"code","2b6d9cdb":"code","0e25ea1d":"code","95e00c88":"code","073b7456":"code","f03f040a":"code","5d5cfa51":"markdown","76631dd7":"markdown","6a32da70":"markdown","387903f7":"markdown","29ee38f9":"markdown","ee343877":"markdown","17314f47":"markdown","13bad32a":"markdown","643cc187":"markdown","337f3cbb":"markdown","6a2c9429":"markdown"},"source":{"e75bfb5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","cb176b2b":"#Import Dataset from Seaborn library\niris = pd.read_csv(\"..\/input\/IRIS.csv\")","098311b2":"#checking the data\niris.head(10)","5c0c0281":"#Visualizing data based on the different features\nsns.pairplot(data=iris, hue='species', palette='Set2')","dc1ce9a2":"#converting target variable classes into numerical\niris['species'] = iris['species'].replace('Iris-setosa',0)\niris['species'] = iris['species'].replace('Iris-versicolor',1)\niris['species'] = iris['species'].replace('Iris-virginica',2)","e1bafcb0":"iris['species'].head(10)","1f045000":"#Splitting the dataset into train and test for model building\nfrom sklearn.model_selection import train_test_split\n\nx = iris.iloc[:,:-1]\ny = iris.iloc[:,4]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.30)","0e3e17e3":"y.head()","17c38b22":"#importing libraries for model \nfrom sklearn.svm import SVC","3107a916":"#Building model on train data\n#model = SVC(kernel='rbf', gamma=0.7, C=1.0)\nmodel =  SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n    probability=False, random_state=None, shrinking=True, tol=0.001,\n    verbose=False)\nmodel.fit(x_train, y_train)","4f23aace":"#Predicting for the test data\npredictions = model.predict(x_test)","7b132436":"#importing libraries for model evaluation\nfrom sklearn.metrics import classification_report, accuracy_score","cb0068d3":"#Confusion matrix\nprint(pd.crosstab(y_test, predictions))","fd2f439c":"print(classification_report(y_test, predictions))","695b59f7":"#Checking Accuracy using accuracy_score\naccuracy_score(y_test, predictions)*100","c14ff153":"model2 = SVC(kernel='linear', C=1.0).fit(x_train, y_train)\npredictions2 = model2.predict(x_test)\nprint(pd.crosstab(y_test, predictions2))\nprint(classification_report(y_test, predictions2))\n","53733633":"model3 = SVC(kernel='poly', degree=3, C=1.0).fit(x_train, y_train)\npredictions3 = model3.predict(x_test)\nprint(pd.crosstab(y_test, predictions3))\nprint(classification_report(y_test, predictions3))","b9aa5e05":"model3.get_params","cbc2a7f5":"from sklearn.model_selection import GridSearchCV\n# Grid Search\n# Parameter Grid\nparam_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 0.00001, 10]}\n \n# Make grid search classifier\nclf_grid = GridSearchCV(SVC(), param_grid, verbose=1)\n \n# Train the classifier\nclf_grid.fit(x_train, y_train)\n \n# clf = grid.best_estimator_()\nprint(\"Best Parameters:\\n\", clf_grid.best_params_)\nprint(\"Best Estimators:\\n\", clf_grid.best_estimator_)","a6b3099e":"from mlxtend.plotting import plot_decision_regions\n#from sklearn import datasets\n#iris = datasets.load_iris()\nX = np.array(iris.iloc[:, [0, 2]])\ny = np.array(iris.iloc[:,4])\n\n# Training a classifier\nmodel_plot = SVC(C=1.0, kernel='linear')\nmodel_plot.fit(X, y)\n\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=model_plot, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM on Iris')\nplt.show()","b9af80ba":"y_train.shape","34dbac83":"from xgboost import XGBClassifier\nclf = XGBClassifier()\nclf\nclf.fit(x_train, y_train)\npred = clf.predict(x_test)\nprint(pd.crosstab(y_test, pred))\nprint(classification_report(y_test, pred))","cf4797ad":"#importing library and segregation of data as train and test using DMatrix Data structure\nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndtest = xgb.DMatrix(x_test, label=y_test)","753099d6":"#paramaters \nparam = {\n    'max_depth': 3,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 3}  # the number of classes that exist in this datset\nnum_round = 5  # the number of training iterations\n\n\n#model builing using training data\nbst = xgb.train(param, dtrain, num_round)\npred = bst.predict(dtest)\n\n","14203471":"#Check the Accuracy\nbst.dump_model('dump.raw.txt')\n#Calculating prediction accuracy\nimport numpy as np\nbest_preds = np.asarray([np.argmax(line) for line in pred])\nfrom sklearn.metrics import precision_score\nprint(precision_score(y_test, best_preds, average='macro'))\n# >> 1.0","18f694e7":"# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nxgb.plot_tree(bst, num_trees=1)\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nfig.savefig('treeIris.png') ","a0f880dd":"#Feature importance\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nplot_importance(bst)\npyplot.show()","8c29e23e":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(x_train,y_train)\npred = clf.predict(x_test)\nprint(pd.crosstab(y_test, pred))\nprint(classification_report(y_test, pred))","8c506e43":"from sklearn.neighbors import KNeighborsClassifier\n# Instantiate learning model (k = 3)\nclassifier = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nclassifier.fit(x_train, y_train)\n\npred = classifier.predict(x_test)\nprint(pd.crosstab(y_test, pred))\nprint(classification_report(y_test, pred))","017f1b72":"x = iris.iloc[:, [1, 2, 3, 4]].values","2b6d9cdb":"x.shape","0e25ea1d":"\n#Finding the optimum number of clusters for k-means classification\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()\n","95e00c88":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","073b7456":"#Visualising the clusters\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Iris-versicolour')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')\n\nplt.legend()","f03f040a":"\nprint(pd.crosstab(iris['species'], kmeans.labels_))\nprint(classification_report(iris['species'], kmeans.labels_))","5d5cfa51":"**Plotting kernel plot on iris_data**","76631dd7":"Parameter Tuning using GridSearchCV","6a32da70":"Training and Fitting the Model","387903f7":"Modelling Using XGBoost","29ee38f9":"Modelling Using Naive Bayes Classifier","ee343877":"Modelling Using XGB Library","17314f47":"Modelling Using K-Means Clusetr","13bad32a":"Model Evaluation","643cc187":"Modelling Using Poly kernel","337f3cbb":"Modelling Using Linear kernel","6a2c9429":"Modelling Using KNN"}}