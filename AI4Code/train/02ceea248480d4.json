{"cell_type":{"7d8c23bf":"code","826a320a":"code","dce9b008":"code","5c0e4643":"code","b0b819f9":"code","29b1aad0":"code","58fac465":"code","18d52a22":"code","648f8cb0":"code","41efbc13":"code","d22105e7":"code","f3434db2":"code","54cedbe8":"code","eef7aecb":"code","685cdcf5":"code","2cca2b29":"code","b4de8dac":"code","a48ed0f3":"code","fe9e2e0f":"code","c659bd07":"code","1beab7a2":"code","88a581a9":"code","b1d08876":"code","b2cb7cae":"code","e172ca0d":"code","8a33d3a1":"code","24ff027c":"code","3cc24e06":"code","c931a032":"code","efc7fe6d":"code","4a721ab6":"code","569c3de2":"markdown","b3632676":"markdown","2889fea5":"markdown","2997a667":"markdown","b5121beb":"markdown","579a55cb":"markdown","a86d11e4":"markdown","02e7fdb3":"markdown","0542dd43":"markdown","3d6a139b":"markdown","cd5a6f40":"markdown"},"source":{"7d8c23bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', -1)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport plotly_express as px\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","826a320a":"train=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")","dce9b008":"print(\"Shape of Training Data \",train.shape)\nprint(\"Shape of Testing Data \",test.shape)","5c0e4643":"train.head(10)","b0b819f9":"lang_count=train.groupby(['language']).size().reset_index().rename(columns={0:'count'})\nprint(\"Number of Languages in the Data \",train['language'].nunique())","29b1aad0":"fig = px.pie(lang_count, values='count', names='language', title='Distribution of Text Across Languages')\nfig.show()","58fac465":"label_mapping={0:'entailment',1:'neutral',2:'contradiction'}","18d52a22":"train['Relationship_Type']=train['label'].apply(lambda x:label_mapping[x])\ntrain.head()","648f8cb0":"type_count=train.groupby(['Relationship_Type']).size().reset_index().rename(columns={0:'count'})\n","41efbc13":"fig = px.pie(type_count, values='count', names='Relationship_Type', title='Distribution of Type of Relationship Between Premise and Hypothesis')\nfig.show()","d22105e7":"from transformers import BertTokenizer,TFBertModel\nimport tensorflow as tf","f3434db2":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"In TPU STRATERGY\")\n    print('Number of replicas:', strategy.num_replicas_in_sync)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","54cedbe8":"MODEL_NAME=\"bert-base-multilingual-cased\"\nMAX_LEN=64\nBATCH_SIZE=32","eef7aecb":"tokeniser=BertTokenizer.from_pretrained(MODEL_NAME)\n","685cdcf5":"print(\"Vocab Size of the Bert Multilingual Tokeniser \",tokeniser.vocab_size)","2cca2b29":"from keras.preprocessing.sequence import pad_sequences\n","b4de8dac":"def convertToTokens(sentence,tokeniser):\n    tokens=tokeniser.tokenize(sentence)\n    tokens.append('[SEP]')\n    return tokeniser.convert_tokens_to_ids(tokens)","a48ed0f3":"def encode(hypothesis_list,premise_list,tokeniser,max_seq_len=128):\n    num_examples=len(hypothesis_list)\n    hypothesis=tf.ragged.constant([convertToTokens(s,tokeniser) for s in np.array(hypothesis_list)])\n    premise=tf.ragged.constant([convertToTokens(s,tokeniser) for s in np.array(premise_list)])\n    \n    ### Add CLS Token to the beginning of the Hypothesis. \n    cls=[tokeniser.convert_tokens_to_ids(['[CLS]'])]*hypothesis.shape[0]\n    \n    \n    input_ids=tf.concat([cls,hypothesis,premise],axis=-1) \n    print(\"Input IDS Type \",type(input_ids))\n    \n    ### mask should be zero for all \n    mask=tf.zeros_like(input_ids).to_tensor() ## Same shape with all elements filled with 0 as input_ids\n    \n    ### Token_Type_IDS => 0 for CLS and the Hypothesis and 1 for the premise\n    cls_token_type=tf.zeros_like(cls)\n    hypothesis_token_type=tf.zeros_like(hypothesis)\n    premise_token_type=tf.ones_like(premise)\n    \n    input_type_ids=tf.concat([cls_token_type,hypothesis_token_type,premise_token_type],axis=-1).to_tensor()\n    \n    return {'input_word_ids': input_ids.to_tensor(),'input_mask': mask,'input_type_ids': input_type_ids}","fe9e2e0f":"X=train[['hypothesis','premise']]\ny=train['label']","c659bd07":"from sklearn.model_selection import train_test_split","1beab7a2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"Shape of Train Data \",X_train.shape)\nprint(\"Shape of Test Data \",X_test.shape)\n","88a581a9":"from transformers import TFAutoModel ## Will create Model Architecture, based on the path of the pretrained model","b1d08876":"from tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam","b2cb7cae":"def createModel():\n    with strategy.scope():\n        model=TFAutoModel.from_pretrained(MODEL_NAME)\n        input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_token', dtype='int32')\n        input_mask=tf.keras.layers.Input(shape=(MAX_LEN,), name='input_mask', dtype='int32')\n        input_token_ids=tf.keras.layers.Input(shape=(MAX_LEN,), name='input_token_ids', dtype='int32')\n        ### From the Model, we need to extract the Last Hidden Layer - this is the first element of the model output\n        embedding=model([input_ids,input_mask,input_token_ids])[0]\n        ### Extract the CLS Token from the Embedding Layer. CLS Token is aggregate of the entire sequence representation. It is the first token\n        cls_token=embedding[:,0,:] ## embedding is of the size batch_size*MAX_LEN*768\n    \n        ### Add a Dense Layer, with three outputs \n        output_layer = Dense(3, activation='softmax')(cls_token)\n    \n        classification_model= Model(inputs=[input_ids, input_mask, input_token_ids], outputs = output_layer)\n    \n        classification_model.compile(Adam(lr=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    \n        #classification_model.summary()\n    \n    return classification_model\n    \n    \n    \n    ","e172ca0d":"with strategy.scope():\n    bert_model=createModel()\n    bert_model.summary()","8a33d3a1":"train_inputs=encode(X_train.hypothesis.values,X_train.premise.values,tokeniser)\nval_inputs=encode(X_test.hypothesis.values,X_test.premise.values,tokeniser)\nhistory = bert_model.fit([train_inputs['input_word_ids'],train_inputs['input_mask'],train_inputs['input_type_ids']],y_train,validation_data=([val_inputs['input_word_ids'],val_inputs['input_mask'],val_inputs['input_type_ids']],y_test),epochs = 15, batch_size = 16,shuffle = True)","24ff027c":"test_inputs = encode(test.hypothesis.values, test.premise.values, tokeniser)","3cc24e06":"predictions = [np.argmax(i) for i in bert_model.predict([test_inputs['input_word_ids'],test_inputs['input_mask'],test_inputs['input_type_ids']])]\n","c931a032":"submission = pd.DataFrame()\nsubmission['id']=test['id'].tolist()\nsubmission['prediction'] = predictions","efc7fe6d":"submission.head()","4a721ab6":"submission.to_csv(\"submission.csv\", index = False)","569c3de2":"The dataset is highly balanced and no Class Imbalance Problem is there.\n\n\n### Loading MultiLingual BERTTokeniser From HuggingFace","b3632676":"### Distribution across Languages","2889fea5":"We can see that there is data from other language too in the dataset. Let us look at the language distribution","2997a667":"57% of the data is in English. We can either use multilingual BERT or can translate text from other languages to English","b5121beb":"### Check for TPU Availablity and start it","579a55cb":"### Split the Data into Train and Validation Data","a86d11e4":"### Predict on Test Data","02e7fdb3":"### Loading the Data","0542dd43":"### Distribution of Type of Relationship between Premise and Hypothesis","3d6a139b":"### Introduction\n\nNLP has been used to solve problems like Question Answering, Sentiment Analysis, Recognising Entities etc, but can it also be used to understand relationship between Sentences? \nGiven two sentences, can we say whether they make logical sense or no? Applications such as these can help in fact checking, detecting fake news etc.\n\nLets look at the data and get an understanding of what data is about","cd5a6f40":"### Encode the Hypothesis and Premise Using the MultiLingual BERT Tokeniser.\n\nWhile Encoding the Hypothesis and the Premise, there must be a [SEP] token between them and also since our problem is a Classification Problem, we have to add the [CLS] Token at the beginning of the input\n\nThe encode function must also return three features \n\n- input_ids => This is the encoded token id assocuated with each token in the input sequence\n- token_type_ids => This is all 0 for the first sentence and 1 for the second sentence\n- attention_mask => This is all 1 for the complete sequence. If there is any padding done then for the padding token, the value is 0\n"}}