{"cell_type":{"655af99b":"code","e8bfa671":"code","2cc34489":"code","c97730e4":"code","1d80eccf":"code","6e3b1322":"code","e4bd4b21":"code","16bff159":"code","ebdbb01a":"code","62fc3ff7":"code","7529e87c":"code","8f73af31":"code","8c97777d":"markdown","c50e6de5":"markdown","b6ab05f9":"markdown","db87c718":"markdown","5a7f2745":"markdown","efdc8036":"markdown","f52bb112":"markdown"},"source":{"655af99b":"#Import required Libaries and functions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8bfa671":"# reading in Data and setting Constants\nSOURCE = \"\/kaggle\/input\/dry-bean-dataset\/Dry_Bean.csv\"\nCLASSES = np.array([\"SEKER\",\"BARBUNYA\",\"BOMBAY\",\"CALI\",\"DERMASON\",\"HOROZ\",\"SIRA\"])\ndata = pd.read_csv(SOURCE)","2cc34489":"#Display first 5 lines of the Dataset\ndata.head()","c97730e4":"#Visualisation for 6 variable Scatterplot including Class labels\ndata6 = data[[\"Area\",\"Perimeter\",\"AspectRation\",\"Eccentricity\",\"roundness\",\"Compactness\",\"Class\"]]\ndata6.head()\nsns.set_theme(style=\"whitegrid\")\nsns.pairplot(data6, hue=\"Class\")","1d80eccf":"#Using the correlation matrix to aid my descriptions in accordance with the scatterplot matrix\ndata6.corr()","6e3b1322":"# Finding instances of each class\ndef findInstances(beans,classes):\n    instanceCount = dict()\n    for beanClass in CLASSES:\n        numinstances = beans.Class.value_counts()[beanClass]\n        instanceCount[beanClass] = numinstances\n        print(f\"There are {numinstances} instances for class: {beanClass}\")\n\n    minInstance = min(instanceCount, key=instanceCount.get)\n\n    print(f\"\\nThe class of beans with the least amount of instances is: {minInstance}\")\n    \n    \nfindInstances(data,CLASSES)","e4bd4b21":"# Using train_test_split \ndataAttributes = data.drop(\"Class\",axis=1)\ndataClasses = data[[\"Class\"]]\n\n# setting random_state to 0 for reproducable results\nx_train, x_test, y_train, y_test = train_test_split(dataAttributes,dataClasses, test_size = 0.2, train_size=0.8, random_state=0)\n\nprint(f\"The number of instances in the training set: {len(x_train)}\")\nprint(f\"The number of instances in the testing set: {len(x_test)}\")","16bff159":"# Implementing StandardScaler feature scaling\nx_train = StandardScaler().fit_transform(x_train)\nx_test = StandardScaler().fit_transform(x_test)","ebdbb01a":"# Support Vector Machine Classifier \nsvmClf = svm.SVC(gamma=0.14, C=1, random_state = 0)\nsvmClf.fit(x_train,np.ravel(y_train))\n\n# Displaying Results in Confusion Matrix\nplot_confusion_matrix(svmClf, x_test, y_test)  \nplt.xticks(rotation=90)\nplt.grid(False)\nplt.title('Dry Bean SVM Confusion Matrix')\nplt.show()  ","62fc3ff7":"# Building classification report for the results from the SVM classifier\npredictedSVM = svmClf.predict(x_test)\nreportSVM = classification_report(y_test, predictedSVM, output_dict=True)\nreportdfSVM = pd.DataFrame(reportSVM).transpose()\nreportdfSVM","7529e87c":"# Stochastic Gradient Descent Classifier\nsgdClf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha =0.004, learning_rate = \"optimal\", random_state=0)\nsgdClf.fit(x_train,np.ravel(y_train))\n\n# SGD Classifier Confusion Matrix\nplot_confusion_matrix(sgdClf, x_test, y_test)  \nplt.xticks(rotation=90)\nplt.grid(False)\nplt.title('Dry Bean SGD Confusion Matrix')\nplt.show()  ","8f73af31":"# Building classification report for the results from the SGD classifier\npredictedSGD = sgdClf.predict(x_test)\nreportSGD = classification_report(y_test, predictedSGD, output_dict=True)\nreportdfSGD = pd.DataFrame(reportSGD).transpose()\nreportdfSGD","8c97777d":"## Part 1: Data Exploration Aims\n- Display first 5 lines of Data\n- Display scatterplot and their correlation between 6 variables taken from the dataset\n- Display number of instances for each Bean class","c50e6de5":"#### Choice of Hyperparameters for SVC\nTwo hyperparameters worth examining are gamma and C. The gamma hyperparameter defines the \"learning step\" of one training example. The C or regularisation hyperparameter defines the penalty for misclassified points and will controll the size of the margin or \"street\" in the SVM Classifier. Here, I choose to use gamma = 0.14 and C = 13. \nThis moderate C value allows the SVM Classifier to tolerate some misclassifications and this relatively small gamma value allows the model to generate a large similarity radius, allowing more area for classification of a class of bean. ","b6ab05f9":"#### Describing the Scatterplot Matrix\nLooking at these scatterplots, we can assert that there are clear clusters forming within some scatterplots, namely between the area and perimeter attributes with all other attributes (but not with themselves). We can assume that these clusters are filled with beans with similar attributes and may be more likely to be classified within a particular class. \nIt is also clear that there is a clear relationship between area and perimeter, suggesting they are proportionate with a high correlation coefficient, being 0.967. \n\nAssessing variables \"Compactness\" with \"Aspect Ration\" and \"Eccentricity\", each relationship shows a clear inversely proportionate relationship. The \"Compactness\" attribute has a -0.988 correlation coefficient with \"Aspect Ration\", showing a slight concave out curve. Similarly, the \"Compactness\" attribute has a -0.970 correlation coefficient with \"Eccentricity\", showing a slight concave in curve in the scatter matrix. \n\nIt is also worth noting that for the relationship between the \"Roundness\" attribute and the \"Aspect Ration, \"Ecccentricity\" and \"Compactness\" is unique in the sense that there seems to be an upperbound between these scatter plots. They have a defined upperbound where there are no outliers which go beyond the upper bound (at least in this study). This suggests that for a bean of a given \"Aspect Ration\", \"Eccentricity\" or \"Compactness\" value, there will be a maximum roundess.  Outliers for low values of roundess do exist. There seems to be some moderate correlation between roundness and these attributes:\n- Aspect Ration: -0.767\n- Eccentricity: -0.722\n- Compactness: 0.768","db87c718":"## Results of Analysis\n#### SVM Classifier Confusion Matrix Results \nFrom inspection of both confusion matrices, the diagonals had far larger results than the other sections. This suggests we have a large proportion of correctly classified classes for the beans. Some notable observations include how there are many more instances of the DERMASON class of beans and thus we should not be surprised that there are far more incorrect classifications of the DERMASON class. In particular, there were some DERMASON beans which have been incorrectly classified as SIRA beans with 35 incorrect classifications and likewise, 51 true SIRA beans have been incorrectly classified as DERMASON beans. From this errors, we may assume that these two beans may be more similar to each other compared to the other beans. \n\n#### SVM Classifier Classification Report Results\nThis combination of hyperparameters produced strong results for me with accuracy reaching 93.64%. Inspecting the f1-score (weighted result from both precision and recall) from the BOMBAY class (as well as inspecting the confusion matrix), you can see how this class has been perfectly classified from the SVM classifier with no misclassifications. On the other hand, the SIRA class was the most poorly classified class of beans with f1-score of 0.8844.\n\n\n## Comparison between SVM Classifier Model vs SGD Classfier Model\n\n#### SGD Classifier Confusion Matrix Results and Comparison\nFrom inspection of the confusion matrix, overall, the results were very similar to the confusion matrix generated from the SVM classifier but comparitively, there seemed to be more misclassifications scattered around the confusion matrix. Again, the diagonals held the majority of correct classifications, meaning that the majority of predictions were the true class. Noteable observations in the SGD classifier observed numerous true DERMASON beans which have been incorrectly classified as SIRA beans with 37 incorrect classifications and likewise, 54 true SIRA beans have been incorrectly classified as DERMASON beans. These results were analogous to the results found in the SVM classifier.\n\n#### SGD Classifier Classification Report Results and Comparison\nUsing the specified set of hyperametres, I achived an accuracy of 91.92%, which is slightly lower than the accuracy from the SVM classifier. Although close, the SVM classifer was slightly superior in this regard. Inspecting the f1-score, the BOMBAY class still achived 100% correct classification rate. This suggests this class of bean was unique enough from the other classes to be consistently and accurately classified correctly by both classifiers in this dataset. \n\n### Final Thoughts\nWith the exception of the BOMBAY class achiving the same \/ optimal results between both classifiers, the SGD classifier consistently produced slightly infurior results by comparing the f1-score between all corresponding classes. ","5a7f2745":"## Part 2: Preprocessing Data for Training \n- Applying Train-Test-Split to form 80%-20% split of data\n- Applying StandardScaler feature scaling to standardise data","efdc8036":"## Training Models\n- Train a model using Support Vector Machine Classifier\n- Train a model using Stochastic Gradient Descent Classifier","f52bb112":"## Dry Bean Analysis\n**Author: Nathan Zhou**\n\n![Capture-min.PNG](attachment:a823ec64-82af-43d2-a53e-50a9d087c698.PNG)\n\nWho doesn't love beans? I sure do!  \nThe goal for this small project is to tinker about with different classification models using the Dry Bean Dataset, specifically to compare the classification performance between a Support Vector Machine Classifier vs Stochastic Gradient Descent Classifer, whilst also trying to achive a high accuracy rate for classification using both models.\n\nDataset Abstract: Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains."}}