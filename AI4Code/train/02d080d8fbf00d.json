{"cell_type":{"714fa6df":"code","589d19ed":"code","e09eddc3":"code","62806e5f":"code","463c0ea1":"code","72011515":"code","37140dd3":"code","685b644c":"code","7bf10399":"code","88eb2e9c":"code","daf92989":"code","e227bbed":"code","82c4e48d":"code","a3857666":"code","52573982":"code","e4f662c1":"code","74f28c56":"code","d4f6e4ce":"code","21c7d2da":"code","6369a8e1":"code","39d0d351":"code","088299da":"code","bc4d1e0b":"code","6ad7ed61":"code","48643b40":"code","9b341be8":"code","5335af8f":"code","6c997055":"code","5c77f6e7":"markdown","fd5e38b0":"markdown","74106042":"markdown","07b85810":"markdown","671de21c":"markdown","6578caa9":"markdown","8068b566":"markdown","3056d652":"markdown","85ca8f7f":"markdown","a69da70d":"markdown","c72b977b":"markdown","dbfa02ba":"markdown","ae2379ef":"markdown","d1fb5724":"markdown","85965614":"markdown","022ae868":"markdown","36a554ae":"markdown","6f903353":"markdown","46207585":"markdown","3dc29712":"markdown","2fd1c6d6":"markdown"},"source":{"714fa6df":"### Importing utilities\nfrom __future__ import unicode_literals, print_function, division\nimport pandas as pd\nimport numpy as np\nimport time\nimport math\nimport pickle\n\nimport matplotlib.pyplot as plt\n#plt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nfrom prettytable import PrettyTable\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport  nltk.translate.bleu_score as bleu\nfrom matplotlib.font_manager import FontProperties\n\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","589d19ed":"### Some parameters (Details in coming sections)\nSOS_token = 0\nEOS_token = 1\nUNK_token = 2\n\nMAX_LENGTH = 20\nteacher_forcing_ratio = 0.5\nhidden_size = 1000","e09eddc3":"data = open('..\/input\/englishhindi-pairs-data\/eng-hin.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\npairs = [[s for s in l.split('\\t')[:2]] for l in data]\npairs[100:110]","62806e5f":"class Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"UNK\": 2}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n        self.n_words = 3  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","463c0ea1":"# Turn a Unicode string to plain ASCII, thanks to\n# https:\/\/stackoverflow.com\/a\/518232\/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\n\n\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s","72011515":"def readLangs(lang1, lang2, reverse=False):\n    print(\"Started Loading Data...\")\n    print(\"Reading lines...\")\n\n    \n   # Read the file and split into lines\n    lines = open('..\/input\/englishhindi-pairs-data\/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n        read().strip().split('\\n')\n    \n    #print(lines)\n\n    # Split every line into pairs and normalize\n    pairs = [[s for s in l.split('\\t')[:2]] for l in lines]\n    \n    #print(pairs)\n    \n   \n    print(\"len\", len(pairs))\n    # pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs","37140dd3":"def filterPair(p):\n    #print(p[1])\n    try:\n        return len(p[0].split(' ')) < MAX_LENGTH and \\\n            len(p[1].split(' ')) < MAX_LENGTH\n    except:\n        print(\"Skipped this pair : \", p)\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","685b644c":"def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs","7bf10399":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","88eb2e9c":"input_lang, output_lang, pairs = prepareData('eng', 'hin', False)\nprint(\"Data Loaded....See Example below - \")\nprint(random.choice(pairs))","daf92989":"# print(\"Saving Dictionary\")\n# pickle.dump(input_lang, open('save\/input.pkl', 'wb'))\n# pickle.dump(output_lang, open('save\/output.pkl', 'wb'))","e227bbed":"def indexesFromSentence(lang, sentence):\n    words = sentence.split(' ')\n    indexes = []\n    for word in words:\n        if word not in lang.word2index.keys():\n            indexes.append(lang.word2index[\"UNK\"])\n        else:\n            indexes.append(lang.word2index[word])\n\n    return indexes\n    #return [lang.word2index[word] for word in words]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)","82c4e48d":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, layers=1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.layers = layers\n        # -----------------------------------------------------------------\n        # self.weight_matrix = create_weight_matrix()\n        # self.embedding, num_embeddings, embedding_dim = create_emb_layer(self.weight_matrix, True)\n        self.embedding = nn.Embedding(input_size, hidden_size, layers)\n\n        # -------------------------------------------------------------------\n        # self.gru = nn.GRU(embedding_dim, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, layers)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.lstm(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(self.layers, 1, self.hidden_size, device=device), torch.zeros(self.layers, 1, self.hidden_size, device=device)","a3857666":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","52573982":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH, layers=1):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n        self.layers = layers\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.layers)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        #print(\"d0\", embedded.shape)\n        # print(\"d1\", embedded[0].shape)\n        # print(\"d2----\", hidden[0][0].shape)\n        #\n        # m = torch.cat((embedded[0], hidden[0][0]), 1)\n        # print(\"d3\", m.shape)\n        # r = self.attn(m)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n\n        # print(\"d4\", attn_weights.shape)\n        # print(\"d5\", encoder_outputs.unsqueeze(0).shape)\n\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n\n        output, hidden = self.lstm(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(self.layers, 1, self.hidden_size, device=device), torch.zeros(self.layers, 1, self.hidden_size, device=device)","e4f662c1":"def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() \/ target_length","74f28c56":"def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total \/ print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, iter \/ n_iters),\n                                         iter, iter \/ n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total \/ plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    #print(plot_losses)\n    showPlot(plot_losses)\n    #pickle.dump(plot_losses, open('save\/gru1n.pkl', 'wb'))","d4f6e4ce":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]\n\n\ndef evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        # Bleu\n        ref = [pair[1].split()]\n        bl = bleu.sentence_bleu(ref, output_sentence.split())\n        print(\"BLEU Score\", bl)\n        print('')","21c7d2da":"def showPlot(points):\n    #print(\"function called\")\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)\n    plt.xlabel(\"steps\")\n    plt.ylabel(\"loss\")\n    plt.show()\n    fig.savefig('loss.png')\n    \ndef showAttention(input_sentence, output_words, attentions, attr):\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    \n    i = attentions.shape[0]\n    attn = [a[:i+2].tolist() for a in attentions]\n\n    \n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['<EOS>'], rotation=90)\n    \n    hindi_font = FontProperties(fname = '..\/input\/nirmala-font\/Nirmala.ttf')\n    ax.set_yticklabels([''] + output_words, fontproperties=hindi_font)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    fig.savefig(\"attention_{}.png\".format(attr))\n    plt.show()\n\n\ndef evaluateAndShowAttention(encoder1, attn_decoder1, input_sentence, attr):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions, attr)","6369a8e1":"def count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","39d0d351":"def run_training():\n    print(\"========== Building Encoder... ==========\\n\")\n    encoder1 = EncoderRNN(input_lang.n_words, hidden_size, layers=1).to(device)\n    print(encoder1)\n    print('\\n')\n    eparam = count_parameters(encoder1)\n    print('\\n')\n    print(\"========== Building Decoder... ==========\\n\")\n    attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1, layers=1).to(device)\n    print(attn_decoder1)\n    print('\\n')\n    dparam = count_parameters(attn_decoder1)\n    print(\"\\nTotal parameters in encoder + decoder : \", eparam+dparam)\n    print('\\n')\n    print(\"========== Starting Training... ==========\\n\")\n    trainIters(encoder1, attn_decoder1, 50000, print_every=1000)\n    print(\"========== Finished Training... ==========\\n\")\n    # Evaluation and Visualization\n    \n    return encoder1, attn_decoder1","088299da":"encoder1, attn_decoder1 = run_training()","bc4d1e0b":"evaluateRandomly(encoder1, attn_decoder1)","6ad7ed61":"output_words, attentions = evaluate(\n        encoder1, attn_decoder1, \"He is going home\")\nprint(\" \".join(output_words))","48643b40":"evaluateAndShowAttention(encoder1, attn_decoder1, \"I can drive a car\", \"sent-1\")","9b341be8":"evaluateAndShowAttention(encoder1, attn_decoder1, \"She is very beautiful\", \"sent-2\")","5335af8f":"evaluateAndShowAttention(encoder1, attn_decoder1, \"Where are you going\", \"sent-2\")","6c997055":"evaluateAndShowAttention(encoder1, attn_decoder1, \"Do you know him?\", \"sent-2\")","5c77f6e7":"# 3. Model Architecture (The Seq2Seq Model)\n\nA Sequence to Sequence network (also known as seq2seq network, or Encoder Decoder network) is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence.\n\n![model](https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2017\/10\/Encoder-Decoder-Architecture-for-Neural-Machine-Translation.png)\n\nUnlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n\nWith a seq2seq model the encoder creates a single vector which, in the ideal case, encodes the \u201cmeaning\u201d of the input sequence into a single vector \u2014 a single point in some N dimensional space of sentences. Let's look into the Encoder -","fd5e38b0":"We know have both Encoder and Decoder with us. We can go ahead and train the model. But before that Let's look into the concepts of Attention.\n\n### Attention \n\nIf only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n\nAttention allows the decoder network to \u201cfocus\u201d on a different part of the encoder\u2019s outputs for every step of the decoder\u2019s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n\n![attn](https:\/\/github.com\/pashupati98\/kaggle-archives\/blob\/main\/img\/attn.PNG?raw=true)\n\nLet's implement a decoder with attention. We'll use this decoder for our final model.","74106042":"Conclusion - This is a very simple architecture trained a small dataset. Yet, the model has learned pretty good for small sentences. It can be futher improved by training it on large corpus.\n\n### If you found this project interesting consider UPVOTING it. Thanks!\n\nReferences \n- Ilya Sutskever, Oriol Vinyals, Quoc V. Le Sequence to sequence learning with neural networks\n- Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio\u2217 (2016) Neural machine translation by jointly learning to align and translate\n\nHappy Learning!","07b85810":"## The Encoder\n\nThe encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n\n![](https:\/\/github.com\/pashupati98\/kaggle-archives\/blob\/main\/img\/gru_encoder.PNG?raw=true)\n\n### Note - Refer this [Link](https:\/\/towardsdatascience.com\/recurrent-neural-networks-d4642c9bc7ce) if you aren't familiar with RNN (Recurrent Neural Network), GRU, LSTM, Hidden State etc.\n\nLet's implement an Encoder. We'll use LSTM units.","671de21c":"Now, we need the data in a format that is easy to process and feed. We\u2019ll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called Lang which has word \u2192 index (word2index) and index \u2192 word (index2word) dictionaries, as well as a count of each word word2count to use to later replace rare words.","6578caa9":"### Training utility\n\nTo train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.","8068b566":"A function to turn Unicode characters to ASCII if needed. Another function to make everything lowercase, and trim most punctuation.","3056d652":"### Visualization Utility\n\nPlotting is done with matplotlib, using the array of loss values plot_losses saved while training.\n\nWe can also plot the attention because it has highly interpretable outputs. It is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.","85ca8f7f":"# 2. Data Preparation \n\nThe data for this task is a set of many English to hindi translation pairs. The Hindi-English Corpora available on Kaggle can also be used here.","a69da70d":"The whole training process will go like this:\n\n- Start a timer\n- Initialize optimizers and criterion\n- Create set of training pairs\n- Start empty losses array for plotting\n\nThen we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.","c72b977b":"Utility functions to create tensors from sentences\n\nTensor - A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.","dbfa02ba":"### Finally, a wrapper function to run the experiment.","ae2379ef":"A function to read the data and to create objects of the Lang Class","d1fb5724":"A function to filter data based on sequence lenght if needed","85965614":"Some helper functions to print time elapsed and estimated time remaining given the current time and progress %.","022ae868":"### Evaluation Utility\n\nEvaluation is mostly the same as training, but there are no targets so we simply feed the decoder\u2019s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder\u2019s attention outputs for display later.","36a554ae":"The above example were from the data itself. so it perfromed really well. Let's check on some out of the data examples.","6f903353":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">1. Introduction<\/p>\n\nNeural Machine Translation is a state-of-the-art machine translation approach that utilizes deep learning techniques to predict the likelihood of a set of words in sequence. In simple words it translates from one language to another. In the below image we see google translating from English to Hindi pretty efficiently.\n\n<img width=700, height=700, src=\"https:\/\/github.com\/pashupati98\/kaggle-archives\/blob\/main\/img\/google%20translate.PNG?raw=true\">\n\n\n> This notebook is pytorch based implementation of a seq2seq machine translation model. This implementaton is based on some papers and blogs (reference at the bottom). It covers the code in as much detail as possible rather than using high level frameworks.","46207585":"## The Decoder\n\nThe decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation. In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n\nAt every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder\u2019s last hidden state).\n    \n![decoder](https:\/\/github.com\/pashupati98\/kaggle-archives\/blob\/main\/img\/decoder.PNG?raw=true)\n    \nLet's implement the decoder using LSTM units.","3dc29712":"A wrapper function to do the data preparation steps","2fd1c6d6":"Let's check our machine translation system on some examples"}}