{"cell_type":{"b5b99992":"code","43257eec":"code","2effa663":"code","3ac4b06c":"code","97be1295":"code","49f2dbe4":"code","a675ef84":"code","814aa2b5":"code","4291c349":"code","bf5a9cee":"code","d944a129":"code","d4a94101":"code","baa1b322":"code","08f5672f":"code","0a06e1af":"code","1ca6498b":"code","53ece679":"code","f2a247a8":"code","c690cfa2":"code","656a4812":"code","d2768153":"code","0b4f90c2":"code","36830433":"code","de1533ba":"code","03710109":"markdown","221b302c":"markdown","44142718":"markdown","9f22d27b":"markdown","bbd2aa8f":"markdown","e1c23fbd":"markdown","77934852":"markdown","dd47bdd8":"markdown","6a138c53":"markdown","5b9ef50f":"markdown","b9736f0f":"markdown","4d985462":"markdown","11c056fc":"markdown","a4877fbc":"markdown"},"source":{"b5b99992":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43257eec":"df=pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","2effa663":"df.head()","3ac4b06c":"df.describe()","97be1295":"df.isnull()","49f2dbe4":"df.fillna(0,inplace=True)","a675ef84":"df['DEATH_EVENT'].value_counts()","814aa2b5":"X=df.values\nX[0:2]","4291c349":"xt=X[:,0:12]\nxt[0:1]","bf5a9cee":"y=X[:,12]\ny[0:5]","d944a129":"from sklearn import preprocessing\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n%matplotlib inline","d4a94101":"xt=preprocessing.StandardScaler().fit(xt).transform(xt)","baa1b322":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(xt,y,test_size=0.1,random_state=4)","08f5672f":"from sklearn.metrics import classification_report,confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Greys):\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0a06e1af":"from sklearn.neighbors import KNeighborsClassifier\nkmax=10\nmean_acc=np.zeros((kmax-1))\n\nfor i in range(1,kmax):\n    kn=KNeighborsClassifier(n_neighbors=i).fit(xtrain,ytrain)\n    yhat=kn.predict(xtest)\n    mean_acc[i-1]=metrics.accuracy_score(ytest,yhat)\n    \nplt.plot(range(1,kmax),mean_acc,'r')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors')\nplt.tight_layout()\nplt.show()\n    \n    \n","1ca6498b":"print(\"The best accuracy of KNN was\", mean_acc.max(),\"with k=\",mean_acc.argmax()+1)\nkn=KNeighborsClassifier(n_neighbors=7).fit(xtrain,ytrain)\nyhat=kn.predict(xtest)\ncnf_matrix=confusion_matrix(ytest,yhat,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='LOGISTIC REGRESSION LIBLINEAR',cmap=plt.cm.Blues)\n\nprint(classification_report(ytest,yhat))\n","53ece679":"from sklearn.linear_model import LogisticRegression\n","f2a247a8":"LR1=LogisticRegression(C=0.01,solver=\"liblinear\").fit(xtrain,ytrain)\nyhat1=LR1.predict(xtest)\n\ncnf_matrix=confusion_matrix(ytest,yhat1,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='LOGISTIC REGRESSION LIBLINEAR',cmap=plt.cm.Reds)\n\nprint(classification_report(ytest,yhat1))\n","c690cfa2":"LR2=LogisticRegression(C=0.01,solver=\"newton-cg\").fit(xtrain,ytrain)\nyhat2=LR2.predict(xtest)\n\ncnf_matrix=confusion_matrix(ytest,yhat2,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='LOGISTIC LINEAR NEWTON-CG')\n\nprint(classification_report(ytest,yhat2))","656a4812":"from sklearn import svm","d2768153":"svmM1=svm.SVC(C=0.7,kernel='rbf')\nsvmM1.fit(xtrain,ytrain)\nyhatsvm1=svmM1.predict(xtest)\n\ncnf_matrix=confusion_matrix(ytest,yhatsvm1,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='SVM rbf',cmap=plt.cm.Greens)\n\nprint(classification_report(ytest,yhatsvm1))","0b4f90c2":"svmM2=svm.SVC(kernel='linear')\nsvmM2.fit(xtrain,ytrain)\nyhatsvm2=svmM2.predict(xtest)\n\ncnf_matrix=confusion_matrix(ytest,yhatsvm2,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='SVM linear',cmap=plt.cm.Oranges)\n\nprint(classification_report(ytest,yhatsvm2))","36830433":"svmM3=svm.SVC(kernel='poly')\nsvmM3.fit(xtrain,ytrain)\nyhatsvm3=svmM3.predict(xtest)\n\ncnf_matrix=confusion_matrix(ytest,yhatsvm3,labels=[1,0])\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['DeathEvent=1','DeathEvent=0'],normalize='False',title='SVM poly',cmap=plt.cm.Blues)\n\nprint(classification_report(ytest,yhatsvm3))","de1533ba":"from sklearn import metrics\nprint(\"KNN Accuracy                            : \",metrics.accuracy_score(ytest,yhat))\nprint(\"Logistic Regression(liblinear) Accuracy : \",metrics.accuracy_score(ytest,yhat1))\nprint(\"Logistic Regression(newton-cg) Accuracy : \",metrics.accuracy_score(ytest,yhat2))\nprint(\"SVM(rbf Kernel) Accuracy                : \",metrics.accuracy_score(ytest,yhatsvm1))\nprint(\"SVM(linear Kernel) Accuracy             : \",metrics.accuracy_score(ytest,yhatsvm2))\nprint(\"SVM(polynomial Kernel)                  : \",metrics.accuracy_score(ytest,yhatsvm3))","03710109":"Using the Polynomial Kerbel:-","221b302c":"USeing Liblinear as the solver :-","44142718":"Applying KNN for the data :-","9f22d27b":"The Logistic Regresiion(with linear solver) works best for the data with the highest accuracy, followed by SVM(with the linear Kernel) and then K Nearest Neighbors, taking 7 neighbors.\n\nHope this kernel and the insight helped.\nCheers","bbd2aa8f":"Checking for any null vales and filling it with zeros :-","e1c23fbd":"We see that accuracy is highest when we consider 7 Neighbors","77934852":"Now, using Logistic Regression :-","dd47bdd8":"Using Newton-cg as the solver :-","6a138c53":"Using the RBF Kerbel:-","5b9ef50f":"Standardizing all the features' values :-","b9736f0f":"Splitting the data for training and testing :-","4d985462":"Using the Linear Kerbel:-","11c056fc":"Now, using SVM to classify the data :-","a4877fbc":"Seeing a small portion of the data :-"}}