{"cell_type":{"281125ef":"code","68cded08":"code","fc498b91":"code","3209cb43":"code","11fcba72":"code","d7724825":"code","5a5d548b":"code","1203c3d9":"code","ad9656c0":"code","8c0e5bed":"code","d08667ac":"code","3d63d3bd":"code","ff651809":"code","1b58c0e8":"code","ba7d63d2":"code","ce6e0586":"code","0de26628":"code","fb429faf":"code","6bdd8dce":"code","968fc305":"code","4ce1a911":"code","2c0c7960":"code","a3fea1c3":"code","9930a2f0":"code","3368d415":"code","94d3315e":"code","1d13edb9":"markdown","15720d9a":"markdown","715043f0":"markdown","46913e34":"markdown","58acc732":"markdown","3fff4750":"markdown","80e72f25":"markdown","21e35223":"markdown"},"source":{"281125ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","68cded08":"import tensorflow as tf\nimport seaborn as sns # seabonrn for visualization\nimport matplotlib.pyplot as plt # plots for visualization\nimport matplotlib.image as mpimg # to load image data\nfrom sklearn.model_selection import train_test_split # to split train.csv data for cross validation\nfrom sklearn.metrics import confusion_matrix # to construct confusion matrix\nimport itertools # has iterator functions for efficient looping\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential # to construct in CNN\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D # Operations on CNN\nfrom keras.optimizers import RMSprop # RMSProp optimizer\nfrom keras.preprocessing.image import ImageDataGenerator # Image generator class\nfrom keras.callbacks import ReduceLROnPlateau # Reduce learning rate callback function","fc498b91":"%matplotlib inline\nsns.set()","3209cb43":"# Load the data (images stored as pixel values)\ntrain = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\ntest.columns","11fcba72":"Y = train[\"label\"] # target of train.csv data\nX = train.drop(labels = [\"label\"],axis = 1) # Drop 'label' column from train.csv data\ntestX = test.drop(labels = [\"id\"],axis = 1) # Drop 'id' column from test.csv data\ng = sns.countplot(Y) # display the distribution of digits in train.csv data\nY.value_counts().sort_values() # count the distribution of digits in train.csv data","d7724825":"del train # delete train dataframe to free some space \n#del test # delete test dataframe to free some space","5a5d548b":"#Check for null and missing values in train.csv data\nX.isnull().any().describe() # the result shows there is no null values","1203c3d9":"#Check for null and missing values in test.csv data\ntestX.isnull().any().describe() # the result shows there is no null values","ad9656c0":"X = X \/ 255.0 # Normalize train.csv data\ntestX = testX \/ 255.0 # Normalize test.csv the data","8c0e5bed":"# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\nX = X.values.reshape(-1,28,28,1)\ntestX = testX.values.reshape(-1,28,28,1)","d08667ac":"X.shape #reshaped from \"42000 X 784\" to \"60000 X 28 X 28 X 1\"","3d63d3bd":"testX.shape","ff651809":"Y = to_categorical(Y, num_classes = 10) # one hot encoding (example: 2 -> [0,0,1,0,0,0,0,0,0,0])","1b58c0e8":"Y # display the encoded target column","ba7d63d2":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, shuffle=True) # Split the train and the validation set for cross validation","ce6e0586":"# dimensions of the cross validation data\nprint(X_train.shape)\nprint(X_val.shape)\nprint(Y_train.shape)\nprint(Y_val.shape)","0de26628":"g = plt.imshow(X_train[0][:,:,0]) # first image","fb429faf":"g = plt.imshow(X_train[1][:,:,0]) # second image","6bdd8dce":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu',\n                 input_shape = (28,28,1))) # input 2D convolutional layer\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2))) # 2D max pooling\nmodel.add(Dropout(0.25)) # applies dropout to prevent overfitting\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten()) # Flattens without affecting the size\nmodel.add(Dense(256, activation = \"relu\")) # dense layer of size 256 units\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\")) # dense layer of size 10 to output the digit value","968fc305":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # RMSprop optimizer divides the gradient by root mean square\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"]) # Compile the model\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            factor=0.3, \n                                            verbose=1, \n                                            min_lr=0.00001) # reduces learning rate if the learning is stagnant","4ce1a911":"# Generate batches of tensor image data with real-time data augmentation\n# The data will be looped over (in batches).\n# With data augmentation to prevent overfitting (accuracy 0.99286)\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # zoom image\n        width_shift_range=0.1,  # shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # shift images vertically (fraction of total height)\n        horizontal_flip=False,  # flip images horizontally\n        vertical_flip=False)  # flip images vertically\ndatagen.fit(X_train) # apply datagen augmentation to train.csv data","2c0c7960":"# Fit the model\nepochs = 30\nbatch_size = 96\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","a3fea1c3":"# list all data in history\nprint(history.history.keys())\n#history.history['loss']","9930a2f0":"fig, ax = plt.subplots(2,1) # aligning two plots horizontally\nax[0].plot(history.history['loss'], label=\"Training loss\") \nax[0].plot(history.history['val_loss'],label=\"validation loss\")\nlegend = ax[0].legend(loc='best')\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best')","3368d415":"results = model.predict(testX) # predicting test.csv data\nresults = np.argmax(results,axis = 1) # select the index with maximum probability\nresults = pd.Series(results,name=\"label\")","94d3315e":"# creating submission file\n#submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1) \nsubmission = pd.concat([test['id'],results],axis = 1) \nsubmission.to_csv(\"submission.csv\",index=False)\nprint('csv file ready for submission')","1d13edb9":"###Visualizing the model **loss** and **accuracy**","15720d9a":"###Display sample images from numpy array","715043f0":"###Preprocessing","46913e34":"###Loading the data set","58acc732":"###Making predictions on test.csv data","3fff4750":"###Importing libraries","80e72f25":"###Setting environment for visulaization","21e35223":"#Constructing the Convolutional Neural Network (CNN)\n#### [Conv2D(relu) -> Conv2D(relu) -> MaxPool2D -> Dropout] -> [Conv2D(relu) -> Conv2D(relu) -> MaxPool2D -> Dropout] ->\n####-> Flatten -> Dense -> Dropout -> Out"}}