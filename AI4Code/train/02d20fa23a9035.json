{"cell_type":{"5a255b8a":"code","af23dac1":"code","05697bfd":"code","fdc5a8c6":"code","0472cea1":"code","fb4dc9a4":"code","7a149d40":"code","edeb8d76":"code","a5339a02":"code","425ed99e":"code","1dae90be":"code","7b016ec9":"code","84107924":"code","70d36a84":"code","91f5cf4e":"code","a9c01fc7":"code","65d8981d":"code","8901b8cd":"code","a784caea":"code","b3ca6499":"code","5fcd8a0d":"code","d659d902":"code","f30eb055":"code","0aa57247":"code","13255b35":"code","dda20da5":"code","ae62b639":"code","0b556aa5":"code","916737c0":"code","8d156ffb":"code","97138bb5":"code","905c7799":"code","a1f85e00":"code","948be66d":"code","cb97fe5a":"code","67483fd9":"markdown","73e376f8":"markdown","f880fdb1":"markdown","ce3a0dcc":"markdown","ec8e9d62":"markdown"},"source":{"5a255b8a":"import os, sys\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom ast import literal_eval\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm as print_progress","af23dac1":"dataset_dir = '..\/input\/zaloai2021-legal-text-retrieval\/'","05697bfd":"with open(dataset_dir + 'stopwords.txt', 'r') as f_reader:\n    stopwords = []\n    for w in f_reader.readlines():\n        if len(w.split()) == 1:\n            stopwords.append(w[:-1])\n    \nstopwords[:10]","fdc5a8c6":"train_qa_df = pd.read_csv(dataset_dir+'train_qna.csv')\ntrain_qa_df['relevant_titles'] = train_qa_df['relevant_titles'].apply(literal_eval)\ntrain_qa_df['relevant_articles'] = train_qa_df['relevant_articles'].apply(literal_eval)\ntrain_qa_df","0472cea1":"legal_corpus_df = pd.read_csv(dataset_dir+'legal_corpus_merged_u369.csv')\nlegal_corpus_df","fb4dc9a4":"legal_corpus_df.isna().sum()","7a149d40":"legal_corpus_df.dropna(subset=['content'], inplace=True)","edeb8d76":"print(legal_corpus_df.content_length.describe())\n\nsns.distplot(legal_corpus_df.content_length)","a5339a02":"legal_corpus_df[legal_corpus_df.content_length>512].sort_values(by=['content_length'])","425ed99e":"while True:\n    duplicated_indices = legal_corpus_df[legal_corpus_df.duplicated(subset=['law_title', 'article_id', 'clause_id', 'point_id', 'sentence_id'], \n                                                                    keep='last')].index.values\n    if len(duplicated_indices) == 0:\n        break\n        \n    legal_corpus_df.loc[duplicated_indices, 'sentence_id'] = legal_corpus_df.loc[duplicated_indices-1, 'sentence_id'].values + 1","1dae90be":"legal_corpus_df[legal_corpus_df.duplicated(subset=['law_title', 'article_id', 'clause_id', 'point_id', 'sentence_id'], \n                                           keep=False)]","7b016ec9":"separator = '|'\nlegal_corpus_df['law_code'] = legal_corpus_df[  'law_title'].astype(str) + separator + \\\n                              legal_corpus_df[ 'article_id'].astype(str) + separator + \\\n                              legal_corpus_df[  'clause_id'].astype(str) + separator + \\\n                              legal_corpus_df[   'point_id'].astype(str) + separator + \\\n                              legal_corpus_df['sentence_id'].astype(str)\nlegal_corpus_df['law_code'] = legal_corpus_df['law_code'].apply(lambda code: code.encode(\"utf-8\").hex())\n\ncolumns = ['law_title', 'article_id', 'clause_id', 'point_id', 'sentence_id', 'law_code']\n# legal_corpus_df = legal_corpus_df.merge(\n#     pd.read_csv(dataset_dir+'legal_corpus_hashmap.csv'), on=columns[:-1])\nlegal_corpus_df[columns]","84107924":"if not os.path.isdir('.\/embeddings'):\n    os.makedirs('.\/embeddings')\n    \nlegal_corpus_df[columns].to_csv('.\/embeddings\/legal_corpus_hashmap.csv', index=False)","70d36a84":"legal_corpus_df[legal_corpus_df['law_code'].duplicated()]","91f5cf4e":"legal_corpus_legend_df = pd.read_csv(dataset_dir+'legal_corpus_legend.csv')\nlegal_corpus_legend_df['dept'] = legal_corpus_legend_df['dept'].apply(literal_eval)\nlegal_corpus_legend_df","a9c01fc7":"def extract_departments_from_law_ids(law_ids: list or tuple):\n    depts_all = []\n    for law_id in law_ids:\n        depts = legal_corpus_legend_df[legal_corpus_legend_df.title==law_id].dept.values.tolist()[0]\n        if len(depts) == 0:\n            raise ValueError(f'Cannot find department for law title {law_id}')\n        depts_all.extend(depts)\n\n    return depts_all\n\ntrain_qa_df['all_depts'] = train_qa_df['relevant_titles'].apply(extract_departments_from_law_ids)\ntrain_qa_df[['relevant_titles', 'all_depts']]","65d8981d":"from collections import Counter\n\ndef find_most_common_dept(depts: list or tuple):\n    return Counter(depts).most_common(1)[0][0]\n\ntrain_qa_df['best_dept'] = train_qa_df['all_depts'].apply(find_most_common_dept)\ntrain_qa_df[['all_depts', 'best_dept']]","8901b8cd":"legal_corpus_df = legal_corpus_df.merge(legal_corpus_legend_df[['title', 'dept']], \n                                        how='inner', left_on='law_title',\n                                                    right_on='title')\nlegal_corpus_df","a784caea":"pip install sentence-transformers==1.2.1","b3ca6499":"from tqdm.autonotebook import trange\nfrom typing import List, Union\n\nimport logging\nimport numpy as np\nimport torch\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import batch_to_device\n\n\nlogger = logging.getLogger(__name__)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass Encoder(SentenceTransformer):\n\n    \"\"\"\n    Overwrite class SentenceTransformer for numpy+tensorflow processing\n        (The original code is more suitable for pytorch)\n        \n    Attention: v1.2.1\n    \"\"\"\n\n    def encode(self, sentences: Union[str, List[str], List[int]],\n               batch_size: int = 32,\n               show_progress_bar: bool = None,\n               output_value: str = 'sentence',\n               device: str = None,\n               normalize_embeddings: bool = False) -> Union[List[torch.Tensor], np.ndarray, torch.Tensor]:\n        \"\"\"\n        Computes sentence embeddings\n        :param\n            sentences: sentences to be encoded\n            batch_size: batch size used for the computation\n            show_progress_bar: Output a progress bar when encode sentences\n            output_value:  Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings.\n            device: Which torch.device to use for the computation\n            normalize_embeddings: If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.\n\n        :return:\n            By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\n        \"\"\"\n        self.eval()\n        if show_progress_bar is None:\n            show_progress_bar = (logger.getEffectiveLevel()==logging.INFO or logger.getEffectiveLevel()==logging.DEBUG)\n\n        output_value = 'token_embeddings' if 'token' in output_value.lower() else 'sentence_embedding'\n            \n        input_was_string = False\n        if isinstance(sentences, str) or not hasattr(sentences, '__len__'):\n            # Cast an individual sentence to a list with length 1\n            sentences = [sentences]\n            input_was_string = True\n\n        if device is None:\n            device = self._target_device\n\n        self.to(device)\n\n        all_embeddings = []\n        length_sorted_idx = np.argsort([-self._text_length(sen) for sen in sentences])\n        sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n\n        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n            sentences_batch = sentences_sorted[start_index:start_index+batch_size]\n            features = self.tokenize(sentences_batch)\n            features = batch_to_device(features, device)\n\n            with torch.no_grad():\n                out_features = self.forward(features)\n\n                if output_value == 'token_embeddings':\n                    embeddings = []\n                    for token_emb, attention in zip(out_features[output_value], \n                                                    out_features['attention_mask']):\n                        last_mask_id = len(attention) - 1\n                        while last_mask_id > 0 and attention[last_mask_id].item() == 0:\n                            last_mask_id -= 1\n                        \n                        token_emb = token_emb[0:last_mask_id+1]\n                        embeddings.append(token_emb.cpu() if torch.cuda.is_available() else token_emb)\n                else:   # Sentence embeddings\n                    embeddings = out_features[output_value]\n                    embeddings = embeddings.detach()\n                    if normalize_embeddings:\n                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n                    embeddings = embeddings.cpu() if torch.cuda.is_available() else embeddings\n\n                all_embeddings.extend(embeddings)\n\n        all_embeddings = [all_embeddings[idx] for idx in np.argsort(length_sorted_idx)]\n        all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n\n        if input_was_string:\n            all_embeddings = all_embeddings[0]\n\n        return all_embeddings","5fcd8a0d":"sbert_version = 'distilUSE'\nsbert_dir = os.path.join('..\/input\/sentence-transformers', sbert_version)\nencoder = Encoder(sbert_dir)\ntokenizer = encoder.tokenizer","d659d902":"def tokenize_to_id(sentence: str):\n    tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n    return tokens\n\ntrain_qa_df['question'] = train_qa_df['question'].apply(lambda sentence: sentence.lower())\ntrain_qa_df['tokens'] = train_qa_df['question'].apply(lambda sentence: tokenize_to_id(sentence))\ntrain_qa_df['num_tokens'] = train_qa_df['tokens'].apply(len)\n\ntrain_qa_df[train_qa_df.num_tokens>512]","f30eb055":"embeddings = encoder.encode(train_qa_df['question'].values.tolist(),\n                             batch_size=16,\n                             output_value='token',\n                             show_progress_bar=True)\nembeddings.shape ","0aa57247":"if not os.path.isdir('.\/embeddings\/question'):\n    os.makedirs('.\/embeddings\/question')   \n    \nfolder_to_zip = '.\/embeddings\/question\/'\n    \nfor e_id, emb in print_progress(zip(train_qa_df.question_id.values, embeddings), \n                          total=len(train_qa_df)):\n    fn = f'{e_id}.npz'\n    fpath = folder_to_zip + fn\n    np.savez_compressed(file=fpath, emb=emb)","13255b35":"legal_corpus_df.loc[3334, 'content'] = legal_corpus_df.loc[3334, 'content'].replace('< m\u1ee9c ph\u1ea1t ti\u1ec1n >', 'ti\u1ec1n')","dda20da5":"import re\n\n\ntext = legal_corpus_df.loc[227428, 'content']\nprint(text)\n    \nprint('-'*19)\npattern_date = re.compile(\"(\\d+\/\\d+\/\\d+)\")\nfor m in pattern_date.finditer(text):\n    print(m.group())\n    text = text.replace(m.group(), 'date')\n\nprint('-'*19)\npattern_title = re.compile(\"(\\d+\/\\d+\/\\w+(?:-\\w+|\\w+)+)\")\nfor m in pattern_title.finditer(text):\n    print(m.group())\n    text = text.replace(m.group(), 'th\u00f4ng t\u01b0')\n    \nprint('-'*19)\nprint(text)","ae62b639":"%%time\n\nimport re\n\npattern_date = re.compile(\"(\\d+\/\\d+\/\\d+)\")\npattern_title = re.compile(\"(\\d+\/\\d+\/\\w+(?:-\\w+|\\w+)+)\")\n\n\ndef tokenize_and_clean(sentence: str) -> list:\n    for m in pattern_date.finditer(sentence):\n        sentence = sentence.replace(m.group(), 'date')\n    for m in pattern_title.finditer(sentence):\n        sentence = sentence.replace(m.group(), 'circular')\n        \n    for punct in list(\".!\uff01?\uff1f.\uff61:\uff1a;\uff1b+-,\uff0c()[]{}<>\"):\n        sentence = sentence.replace(punct, '')\n        \n    tokens = tokenizer.tokenize(sentence.lower())\n    return pd.Series([tokens, sentence])\n\n\nlegal_corpus_df[['tokens', 'content']] = legal_corpus_df['content'].apply(lambda sentence: tokenize_and_clean(sentence))\nlegal_corpus_df[['tokens', 'content']]","0b556aa5":"legal_corpus_df['num_tokens'] = legal_corpus_df['tokens'].apply(len)\nlegal_corpus_df[legal_corpus_df.num_tokens>512].sort_values(by=['num_tokens'])","916737c0":"len(legal_corpus_df[legal_corpus_df.num_tokens>512])","8d156ffb":"%%time\n\nfolder_to_zip = f'.\/embeddings\/corpus\/'\nif not os.path.isdir(folder_to_zip):\n    os.makedirs(folder_to_zip)\n\nbatch_size = 64\npart_size = 1_000 * batch_size\ncorpus_size = len(legal_corpus_df)\nnum_corpus_parts = int(np.ceil(corpus_size\/part_size))\nfor p_id in range(num_corpus_parts):\n    # avoid Kaggle limited storage\n    corpus_df = legal_corpus_df.loc[p_id*part_size:(p_id+1)*part_size-1]\n    corpus_indices = corpus_df.index.values\n    print(f\"\\n\\nProcessing part {p_id+1} \/ {num_corpus_parts} for sample from {corpus_indices[0]} to {corpus_indices[-1]}\")\n\n    del embeddings\n    embeddings = encoder.encode(corpus_df['content'].values.tolist(),\n                                batch_size=batch_size,\n                                output_value='sentence',\n                                show_progress_bar=True)\n    \n    for e_id, emb in print_progress(zip(corpus_df.law_code.values, embeddings), \n                              total=len(corpus_df)):\n        fn = f'{e_id}.npz'\n        fpath = folder_to_zip + fn\n        np.savez_compressed(file=fpath, emb=emb)","97138bb5":"import shutil\n\ndef zip_folder(zipped_folder, target_dir):\n    if not zipped_folder.endswith('.zip'):\n        zipped_folder += '.zip'\n    zip_obj = zipfile.ZipFile(zipped_folder, 'w', zipfile.ZIP_DEFLATED)\n    rootlen = len(target_dir) + 1\n    for base, dirs, files in os.walk(target_dir):\n        for file in files:\n            fn = os.path.join(base, file)\n            zip_obj.write(fn, fn[rootlen:])\n            \nzip_folder('.\/embeddings.zip', '.\/embeddings')\nshutil.rmtree('.\/embeddings')","905c7799":"pip install umap-learn[plot]","a1f85e00":"from umap import UMAP\n\ndim_reducer = UMAP(\n    n_neighbors=15, # number of neighboring points used in local approximations of manifold structure --> larger = more global structure being preserved\n    min_dist=0.3, # to control how tightly the embedding is allowed compress points together -> larger = more evenly distributed, smaller = optimise more accurately\n    metric='correlation' # https:\/\/umap-learn.readthedocs.io\/en\/latest\/parameters.html?highlight=metric#metric\n)\n\nembeddings_reduced = dim_reducer.fit_transform(embeddings)\nembeddings_reduced.shape","948be66d":"from umap import plot as uplt\n\nuplt.points(dim_reducer, labels=corpus_df['dept'].apply(find_most_common_dept))","cb97fe5a":"# embeddings_reduced = dim_reducer.fit_transform(embeddings)\n# embeddings_reduced.shape\n\n# uplt.points(dim_reducer, labels=train_qa_df.best_dept)","67483fd9":"# Import Libraries","73e376f8":"## Visualize vocabulary embeddings\n\nReference: https:\/\/umap-learn.readthedocs.io\/en\/latest\/\n\nSource code: https:\/\/github.com\/lmcinnes\/umap","f880fdb1":"# Build sentence embeddings","ce3a0dcc":"# Load Data","ec8e9d62":"## Load Encoder"}}