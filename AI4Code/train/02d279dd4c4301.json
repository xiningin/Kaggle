{"cell_type":{"ec0260ee":"code","77668e51":"code","2e191cc3":"code","f24b7159":"code","3fdc2d0b":"code","a0440319":"code","e1adb14d":"code","d8d73b74":"code","ae8c0564":"code","94e9c66e":"code","26952b5e":"code","3964702f":"code","335400b5":"code","05107ce2":"code","b0773d6b":"code","bcdc46ad":"code","d22055d9":"code","b4531f23":"markdown","3d7eec21":"markdown","8fe8df05":"markdown","dca2c02c":"markdown","a3ed13c1":"markdown","baef9e2e":"markdown","d0b4b500":"markdown","4e2dc261":"markdown","f039714a":"markdown"},"source":{"ec0260ee":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import PowerTransformer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\n# Importing librarys to use on interactive graphs\nimport plotly.offline as plty\nfrom plotly import tools\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \n\n# to set a style to all graphs\nplt.style.use('fivethirtyeight')\ninit_notebook_mode(connected=True)\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\n%matplotlib inline","77668e51":"FILE_PATH = '..\/input\/mammography-breast-cancer\/mammography.csv'\n\ndata_df = pd.read_csv(FILE_PATH, header=None)\n\ndata_df.head(5)","2e191cc3":"  \nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111)\n_ = sns.countplot(data_df[6], ax=ax)\n_ = ax.set_title('Cancer vs Non-Cancer', fontsize=20)\n_ = ax.set_ylabel('Count', fontsize=14)\n_ = ax.set_xlabel('')\n_ = ax.set_xticklabels(['Non-microcalcifications','Microcalcifications'], fontsize=13)","f24b7159":"color_dict = {\"'-1'\":'blue', \"'1'\":'red'}\n# map each row to a color based on the class value \ncolors = [color_dict[str(x)] for x in data_df.values[:, -1]] \n    # pairwise scatter plots of all numerical variables \n_ = scatter_matrix(data_df, diagonal='kde', color=colors,figsize=(12,10)) \n_ = plt.show()","3fdc2d0b":"X = data_df.values[:,:-1]\ny = data_df.values[:,-1]\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","a0440319":"def f2_measure(y_true, y_pred):\n    return fbeta_score(y_true, y_pred, beta=2)\n\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    #metric = make_scorer(f2_measure)\n    \n    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    return scores","e1adb14d":"model = DummyClassifier(strategy='stratified')\n\n\nscores = evaluate_model(X, y, model)\n\nprint('Mean F2: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","d8d73b74":"def get_models():\n    models, names = list(), list()\n    \n    # LR\n    models.append(LogisticRegression(solver='lbfgs')) \n    names.append('LR')\n    \n    # SVM\n    models.append(SVC(gamma='scale'))\n    names.append('SVM')\n    \n    # Bagging \n    models.append(BaggingClassifier(n_estimators=1000)) \n    names.append('BAG')\n    \n    # RF \n    models.append(RandomForestClassifier(n_estimators=1000)) \n    names.append('RF')\n    \n    # GBM \n    models.append(GradientBoostingClassifier(n_estimators=1000)) \n    names.append('GBM')\n    return models, names\n\nmodels, names = get_models()\nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))","ae8c0564":"plt.boxplot(results, labels=names, showmeans=True)\nplt.show()","94e9c66e":"def get_models():\n    models, names = list(), list()\n    \n    # LR\n    models.append(LogisticRegression(solver='lbfgs', class_weight='balanced')) \n    names.append('LR')\n    \n    # SVM\n    models.append(SVC(gamma='scale', class_weight='balanced')) \n    names.append('SVM')\n    \n    # RF\n    models.append(RandomForestClassifier(n_estimators=1000)) \n    names.append('RF')\n    \n    return models, names","26952b5e":"models, names = get_models()\nresults = list()\n\nfor i in range(len(models)):\n    \n    steps = [('p',PowerTransformer()),('m',models[i])]\n    pipeline = Pipeline(steps=steps)\n    \n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n    print('>%s %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))","3964702f":"plt.figure(figsize=(10,5))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","335400b5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=123)","05107ce2":"from sklearn.model_selection import RandomizedSearchCV\n\n\nmodel = SVC(gamma='scale', class_weight='balanced')\n\ncs = [0.1, 1, 10, 100, 1000]\n\nsteps = [('p',PowerTransformer()),('m',model)]\npipeline = Pipeline(steps=steps)\n\nparams = {\"m__C\": cs}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nsvc_rscv = RandomizedSearchCV(estimator=pipeline,\n                          param_distributions=params,\n                           n_iter=100,\n                          cv=cv,\n                          scoring='roc_auc',\n                          n_jobs=-1,\n                           verbose=1\n                          )","b0773d6b":"svc_rscv.fit(X_train, y_train)","bcdc46ad":"print(\"Best: %f using %s\" % (svc_rscv.best_score_, svc_rscv.best_params_))\nmeans = svc_rscv.cv_results_['mean_test_score']\nstds = svc_rscv.cv_results_['std_test_score']\nparams = svc_rscv.cv_results_['params']","d22055d9":"from sklearn.metrics import confusion_matrix, classification_report\n\nbest_model = svc_rscv.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(classification_report(y_test, y_pred))","b4531f23":"<h3><center>4. Evaluate Models<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nROC Curves and area under ROC Curves were chosen with the intent to minimize the false- positive rate (complement of the specificity) and maximize the true-positive rate (sensitivity), the two axes of the ROC Curve. The use of the ROC Curves also suggests the desire for a probabilistic model from which an operator can select a probability threshold as the cut-off between the acceptable false positive and true positive rates.\n    <\/div>","3d7eec21":"<h3><center>Mammography - Breast Cancer Classification<\/center><\/h3>\n\n![image.png](attachment:image.png)\n\n<h3>About the Dataset :<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nCancer detection is a popular example of an imbalanced classification problem because there are often significantly more cases of non-cancer than actual cancer. A standard imbalanced classification dataset is the mammography dataset that involves detecting breast cancer from radiological scans, specifically the presence of clusters of microcalcifications that appear bright on a mammogram. This dataset was constructed by scanning the images, segmenting them into candidate objects, and using computer vision techniques to describe each candidate object.<br><br>\nIt is a popular dataset for imbalanced classification because of the severe class imbalance, specifically where 98 percent of candidate microcalcifications are not cancer and only 2 percent were labeled as cancer by an experienced radiographer. In this notebook, we will discover how to develop and evaluate models for the imbalanced mammography cancer classification dataset\n<\/div>","8fe8df05":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\nWe can see that the distributions for many variables do differ for the two-class labels, suggesting that some reasonable discrimination between the cancer and no cancer cases will be feasible.\n    <\/div>","dca2c02c":"<h3><center>5. Cost Sensitive Algorithms<\/center><\/h3>","a3ed13c1":"<h3><center>3. Model Test & Baseline Result<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nA model that predicts a random class label in proportion to the base rate of each class will result in a ROC AUC of 0.5, the baseline in performance on this dataset. This is a so-called no skill classifier. This can be achieved using the DummyClassifier class from the scikit-learn library and setting the strategy argument to \u2018stratified\u2019.<\/div>","baef9e2e":"<h3><center>2. Exploring Data<\/center><\/h3>","d0b4b500":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe dataset involved first started with 24 mammograms with a known cancer diagnosis that were scanned. The images were then pre-processed using image segmentation computer vision algorithms to extract candidate objects from the mammogram images. Once segmented, the objects were then manually labeled by an experienced radiologist. A total of 29 features were extracted from the segmented objects thought to be most relevant to pattern recognition, which was reduced to 18, then finally to six, as follows (taken directly from the paper):<ul>\n    <li>Area of object (in pixels).\n    <li>Average gray level of the object.\n    <li>Gradient strength of the object\u2019s perimeter pixels.\n    <li>Root mean square noise fluctuation in the object.\n    <li>Contrast, average gray level of the object minus the average of a two-pixel wide border surrounding the object.\n    <li>A low order moment based on shape descriptor.\n    <\/ul>\nThere are two classes and the goal is to distinguish between microcalcifications and non-\nmicrocalcifications using the features for a given segmented object.\n<\/div>\n\n            Non-microcalcifications: negative case, or majority class.\n            Microcalcifications: positive case, or minority class.\n    \n","4e2dc261":"<h3><center>6. Fitting Final model<\/center><\/h3>","f039714a":"<h3><center>1. Reading Data<\/center><\/h3>"}}