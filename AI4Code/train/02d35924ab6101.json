{"cell_type":{"4f518ca4":"code","bbda54a9":"code","27f5122c":"code","7b438721":"code","ebd8daf8":"code","faef6584":"code","0c50e3e3":"code","fe11968e":"code","89d27a73":"code","0495f347":"code","79848268":"code","67011433":"code","57b472f8":"code","7957de2b":"code","4e9b8e6d":"code","af4fa0df":"code","7afa4804":"code","3043afad":"code","977652c7":"code","10ac184e":"code","96cf8564":"code","dd8e4c77":"code","bd9e8e43":"code","82f21ef4":"code","5ac12aa0":"markdown","7574aa3a":"markdown","f5f8abf6":"markdown","39a674d1":"markdown","e19bec68":"markdown","32b5a1ac":"markdown","aa395477":"markdown","7ca45ac4":"markdown","0c924c82":"markdown","28051107":"markdown"},"source":{"4f518ca4":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","bbda54a9":"!pip install \/kaggle\/input\/transformers\/transformers-2.2.1-py3-none-any.whl","27f5122c":"!pip install transformers","7b438721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ebd8daf8":"df_submit = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_train.text = df_train.text.astype(str)\ndf_test.text = df_test.text.astype(str)","faef6584":"df_train","0c50e3e3":"import os\nimport sys\nimport glob\nimport torch\nimport time\nfrom tqdm import tqdm_notebook as tqdm\nimport transformers\n# DEVICE = torch.device(\"cuda\")\ntokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n# tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n# model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n# model.to(DEVICE)","fe11968e":"### SINCE WHOLE USEFUL VECTORS ARE 125, we can just use 129...\n### [CLS] + text[82] + [SEP] + location[30] + [SEP] + keyword[7] + [SEP]\n\ndef to_ids(tokenizer, text):\n    ids = []\n    tokens = tokenizer.tokenize(text)\n    for token in tokens:\n        ids.append(tokenizer._convert_token_to_id(token))\n    return ids\ndef pad_to_length(ids, length):\n    ids = ids[:length]\n    if len(ids) < length:\n        ids += ([0] * (length - len(ids)))\n    return ids\ndef join_segments(tokenizer, *segments):\n    vector = [tokenizer._convert_token_to_id('[CLS]')]\n    sep_id = tokenizer._convert_token_to_id('[SEP]')\n    for segment in segments:\n        vector += segment\n        vector.append(sep_id)\n    return vector\n","89d27a73":"train_vectors = []\nfor _, (text, location, keyword) in tqdm(df_train[['text', 'location', 'keyword']].fillna(\"-\").iterrows(), total=df_train.shape[0]):\n    text_ids = pad_to_length(to_ids(tokenizer, text), 85)\n    location_ids = pad_to_length(to_ids(tokenizer, location), 31)\n    keyword_ids = pad_to_length(to_ids(tokenizer, keyword), 8)\n    train_vectors.append(join_segments(tokenizer, text_ids, location_ids, keyword_ids))\nX_train = np.array(train_vectors)","0495f347":"test_vectors = []\nfor _, (text, location, keyword) in tqdm(df_test[['text', 'location', 'keyword']].fillna(\"-\").iterrows(), total=df_test.shape[0]):\n    text_ids = pad_to_length(to_ids(tokenizer, text), 85)\n    location_ids = pad_to_length(to_ids(tokenizer, location), 31)\n    keyword_ids = pad_to_length(to_ids(tokenizer, keyword), 8)\n    test_vectors.append(join_segments(tokenizer, text_ids, location_ids, keyword_ids))\nX_test = np.array(test_vectors)","79848268":"y_train = df_train.target.values","67011433":"df_train.target.plot.hist()","57b472f8":"from transformers import DistilBertForSequenceClassification, DistilBertModel","7957de2b":"from torch.nn import BCEWithLogitsLoss\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom tqdm import trange\nfrom transformers import AdamW\nfrom torch.optim import Adam\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport torch.nn.functional as F\ndef train(model, num_epochs, train_dataloader, valid_dataloader, crit_function=nn.CrossEntropyLoss(), device='cpu'):\n#     optimizer = AdamW(model.parameters(), lr=3.5e-5, weight_decay=0.01, correct_bias=False)\n#     model.to(device)\n    \"\"\"\n    Train the model and save the model with the lowest validation loss\n    \"\"\"\n#     crit_function = nn.BCEWithLogitsLoss()\n    model.to(device)\n    start_epoch = 0\n#     optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01, correct_bias=False)\n    optimizer = Adam(model.parameters(), lr=1.5e-5, weight_decay=0.00)\n\n#     optimizer = torch.optim.Adamax(model.parameters(), lr=3e-5)\n    # trange is a tqdm wrapper around the normal python range\n    for i in trange(num_epochs, desc=\"Epoch\"):\n        # if continue training from saved model\n        actual_epoch = start_epoch + i\n        # Set our model to training mode (as opposed to evaluation mode)\n        model.train()\n\n        # Tracking variables\n        tr_loss = 0\n        num_train_samples = 0\n\n        t = tqdm(total=len(train_data), desc=\"Training: \", position=0)\n        # Train the data for one epoch\n        for step, batch in enumerate(train_dataloader):\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_labels = batch\n            # Clear out the gradients (by default they accumulate)\n            optimizer.zero_grad()\n            # Forward pass\n            loss = model(b_input_ids, labels=b_labels)\n            # store train loss\n#             print(loss)\n            loss = loss[0]\n#             input()\n            tr_loss += loss.item()\n            num_train_samples += b_labels.size(0)\n            # Backward pass\n            loss.backward()\n            # Update parameters and take a step using the computed gradient\n            optimizer.step()\n            #scheduler.step()\n            t.update(n=b_input_ids.shape[0])\n        t.close()\n        # Update tracking variables\n        epoch_train_loss = tr_loss\/num_train_samples*batch_size\n#         train_loss_set.append(epoch_train_loss)\n\n        print(\"Train loss: {}\".format(epoch_train_loss))\n\n        # Validation\n\n        # Put model in evaluation mode to evaluate loss on the validation set\n        model.eval()\n\n        # Tracking variables \n        eval_loss = 0\n        num_eval_samples = 0\n\n        v_preds = []\n        v_labels = []\n\n        # Evaluate data for one epoch\n        t = tqdm(total=len(validation_data), desc=\"Validating: \", position=0)\n        eval_losses = []\n        for batch in valid_dataloader:\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            b_input_ids,b_labels = batch\n            # Telling the model not to compute or store gradients,\n            # saving memory and speeding up validation\n            with torch.no_grad():\n                # Forward pass, calculate validation loss\n                logits = model(b_input_ids)[0]\n#                 print(b_labels)\n#                 print(logits)\n                loss = crit_function(logits.view(-1, 2),b_labels.view(-1))\n#                 print(loss)\n                preds = torch.argmax(F.softmax(logits.view(-1, 2), dim=-1), dim=-1)\n#                 print(preds)\n#                 input()\n                v_labels.append(b_labels.cpu().numpy())\n                v_preds.append(preds.cpu().numpy())\n                # store valid loss\n                eval_losses.append(loss.item())\n                num_eval_samples += b_labels.size(0)\n            t.update(n=b_labels.shape[0])\n        t.close()\n\n        v_labels = np.hstack(v_labels)\n        v_preds = np.hstack(v_preds)\n        print(v_labels.shape)\n        epoch_eval_loss = sum(eval_losses)\/len(eval_losses)\n#         valid_loss_set.append(epoch_eval_loss)\n        print('Epoch #{} Validation Results:'.format(i + 1))\n        print('\\tvalidation BCE loss: ~{}'.format(epoch_eval_loss))\n        print('\\tF1: {}'.format(f1_score(v_labels, v_preds)))\n        print('\\tPrecision: {}'.format(precision_score(v_labels, v_preds)))\n        print('\\tRecall: {}'.format(recall_score(v_labels, v_preds)))\n        print(\"\\n\")\n\n    return model","4e9b8e6d":"def get_pred_probs(model, test_dataloader, test_length ,device):\n    model.eval()\n    model.to(device)\n\n    # Tracking variables \n    eval_loss = 0\n    num_eval_samples = 0\n\n    v_preds = []\n    v_probs = []\n\n    # Evaluate data for one epoch\n    t = tqdm(total=test_length, desc=\"Inferencing test data: \", position=0)\n    eval_losses = []\n    for batch in test_dataloader:\n        # Add batch to GPU\n#         print(batch)\n#         input('PAUSED')\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids= batch[0]\n        with torch.no_grad():\n            # Forward pass, calculate validation loss\n            logits = model(b_input_ids)[0]\n            probs = F.softmax(logits.view(-1, 2), dim=-1)\n            preds = torch.argmax(probs, dim=-1)\n            v_probs.append(probs.cpu().numpy())\n            v_preds.append(preds.cpu().numpy())\n        t.update(n=probs.shape[0])\n    t.close()\n    probs = np.vstack(v_probs)\n    classifications = np.hstack(v_preds)\n    \n    return probs, classifications","af4fa0df":"import logging\nimport math\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn import functional as F\n\nfrom transformers.modeling_distilbert import DistilBertPreTrainedModel\nfrom transformers.modeling_bert import BertPreTrainedModel\nfrom transformers.file_utils import add_start_docstrings\nfrom transformers.modeling_utils import PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits, PreTrainedModel, SequenceSummary\n\n","7afa4804":"class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    \"\"\"\n\n    def __init__(self, config):\n        super(DistilBertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.distilbert = DistilBertModel(config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, config.num_labels)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n        distilbert_output = self.distilbert(\n            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n        )\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n#         pooled_output = torch.mean(hidden_state, 1)\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        pooled_output = nn.SELU()(pooled_output)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        outputs = (logits,) + distilbert_output[1:]\n        if labels is not None:\n            if self.num_labels == 1:\n                loss_fct = nn.MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = nn.CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)","3043afad":"import gc\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DataLoader\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nbatch_size = 16\nkf = StratifiedKFold(n_splits=7, random_state=8980, shuffle=True)\nkf.get_n_splits(X_train, y_train)\nprobs_from_folds = []\nfor ind, (tr, val) in enumerate(kf.split(X_train, y_train)):\n    # new model per split\n    print(\"FOLD #{} STARTING!\".format(ind + 1))\n    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n    \n#     model = DistilBertForSequenceClassification.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    \n    # split train datatset\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_val = X_train[val]\n    y_val = y_train[val]\n    \n    # convert train dataset\n    X_tr = torch.tensor(X_tr)\n    X_val = torch.tensor(X_val)\n    y_tr = torch.tensor(y_tr, dtype=torch.long)\n    y_val = torch.tensor(y_val, dtype=torch.long)\n    \n    # prepare train dataloader\n    train_data = TensorDataset(X_tr, y_tr)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data,\\\n                                  sampler=train_sampler,\\\n                                  batch_size=batch_size)\n\n    validation_data = TensorDataset(X_val, y_val)\n    validation_sampler = SequentialSampler(validation_data)\n    validation_dataloader = DataLoader(validation_data,\\\n                                       sampler=validation_sampler,\\\n                                       batch_size=batch_size)\n    \n    #start training!\n    model = train(model=model, num_epochs=2,\\\n                  train_dataloader=train_dataloader, valid_dataloader=validation_dataloader,\n                  device='cuda')\n    \n    \n    # prepare test data\n    test_data = torch.tensor(X_test)\n    test_data = TensorDataset(test_data)\n    test_sampler = SequentialSampler(test_data)\n    test_dataloader = DataLoader(test_data,\\\n                                       sampler=test_sampler,\\\n                                       batch_size=batch_size)\n    \n    probs, _ = get_pred_probs(model, test_dataloader,\\\n                   test_length=df_test.shape[0] ,device='cuda')\n    probs_from_folds.append(probs)\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    \n    ","977652c7":"pff = np.array(probs_from_folds)\npff_mean = np.mean(pff, axis=0)","10ac184e":"pff_mean","96cf8564":"preds = np.argmax(pff_mean, axis=1)","dd8e4c77":"df_submit['target'] = preds","bd9e8e43":"df_submit","82f21ef4":"df_submit.to_csv('submission.csv', index=False)","5ac12aa0":"# 6. Convert softmaxed probs to class","7574aa3a":"# 2. load data","f5f8abf6":"### Credit: HuggingFace @ Github","39a674d1":"# 4. Train with customized DistilBert classifiers per each fold\n - After training and validating with each fold, use each classifier to calculate probabilities","e19bec68":"[SUBMISSION FILE](.\/submission.csv)","32b5a1ac":"# 3. Preprocess the data for training","aa395477":"# 7. SUBMIT!","7ca45ac4":"# 1. Install needed libraries","0c924c82":"# 5. Calculate mean probabilities from accumulated predictions","28051107":"## See if there is a severe class imbalance..."}}