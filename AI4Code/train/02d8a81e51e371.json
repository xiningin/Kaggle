{"cell_type":{"c8c7622e":"code","eb4618da":"code","8d3c9052":"code","43799433":"code","d3f4f840":"code","ebd3b8bd":"code","e557610c":"code","706e3188":"code","7114a162":"code","b91d1516":"code","4b894e3f":"code","1028d0f3":"code","4fd1ff06":"code","18caf786":"code","a3fc7874":"code","5de3749f":"code","1743e2b6":"code","c8a1d671":"code","1167e700":"code","51c5936b":"code","9bd63ad5":"code","cc3b9161":"code","2e3e5d8e":"code","83dd0a50":"code","f1ee136a":"code","dba7be22":"code","93519f5e":"code","4d9f8103":"code","9a1cd596":"code","2e572798":"code","6e098fd4":"code","62bbdef3":"code","babadbd1":"code","f0cc9517":"code","910d14cf":"code","0dfde598":"code","00c58639":"code","2ce90b55":"code","80bc323d":"code","d64261bb":"code","4f413488":"code","60fbeeb1":"code","a698526e":"code","d98a9527":"code","22cf3088":"code","79c4f8e9":"code","b6d05f83":"code","c9decb35":"code","d1410036":"code","2bae7a51":"code","3dbf3aed":"code","32c9f886":"code","4ba50eb4":"code","bd0631fb":"code","c67ec1bf":"code","f816d37b":"code","64f0a752":"code","57618def":"code","9d22ac84":"code","ad0bc372":"code","10f1fef9":"code","77a0a0c7":"code","520332f3":"code","d8ac8775":"code","1acdc342":"code","2063bf26":"code","37737262":"code","9f081e54":"code","724c0b50":"code","5232a87d":"code","3425ab57":"code","f13830b5":"code","4de48b65":"code","fb90321f":"code","c42c243f":"code","b267a288":"code","be70d324":"code","8f67530b":"code","9785ad46":"code","8f377b06":"code","c38387b5":"code","b7ed3bba":"code","421abfb0":"code","729f5fe9":"code","df597086":"code","d2cd6123":"code","cd6aca69":"code","94b49e8d":"code","4b1d9daf":"code","8567c383":"code","532ab96f":"code","0834785a":"code","6f0f6ad5":"code","f282f5f4":"code","62ebfc35":"code","8d0a9aa1":"code","38a83976":"code","1ed17c30":"code","d7b6e4f0":"code","fbc9a39c":"code","6bb9873a":"code","f2435fcd":"code","bffab2f8":"code","6cb9436d":"code","40340ef6":"code","79ed80e5":"code","779efaad":"code","65eefd35":"code","0df2835b":"code","559fb985":"code","6c8a9b05":"code","3db8b441":"code","9103e298":"code","e776e537":"code","e4389a46":"code","930322dd":"code","b1f6aa95":"code","be892ac1":"code","b6515906":"code","82773688":"code","2f26e533":"code","211dccea":"code","d222af40":"code","b7938798":"markdown","611149ea":"markdown","5ff76104":"markdown","90f3d5fe":"markdown","6368d578":"markdown","8cf73b7e":"markdown","3598e84f":"markdown","41d4938f":"markdown","671722ac":"markdown","767beebc":"markdown","a6289b3d":"markdown","013972ad":"markdown","0522a8ab":"markdown","e3b9b867":"markdown","c9a11e1a":"markdown","76c9852d":"markdown","26a3168a":"markdown","0a550a98":"markdown","b5ad955a":"markdown","62ca7600":"markdown","1e6a7a86":"markdown","91a186e2":"markdown","b9907a6d":"markdown","2b1e0475":"markdown","a519a5f8":"markdown","1815f191":"markdown","9df49f4a":"markdown","2a453509":"markdown","93616062":"markdown","8b836ea1":"markdown","17ff384a":"markdown","fb0ea37f":"markdown","a3765636":"markdown","69c1cb14":"markdown","684e3f36":"markdown","21519d62":"markdown","4ae35d84":"markdown","93a9b8a9":"markdown","5896497d":"markdown","a2412c2c":"markdown","f475e507":"markdown","0772b3c5":"markdown","ac67d1db":"markdown","8ce972d7":"markdown","21ac080a":"markdown","c1d08098":"markdown"},"source":{"c8c7622e":"import pandas as pd","eb4618da":"# Meta data :\nmeta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")","8d3c9052":"meta.head(3)","43799433":"meta.shape","d3f4f840":"meta.columns","ebd3b8bd":"meta.dtypes","e557610c":"# Charge all .Json files:\nimport glob\npapers = glob.glob(f'\/kaggle\/input\/CORD-19-research-challenge\/**\/*.json', recursive=True)","706e3188":"len(papers)","7114a162":"# import json\n# papers_data = pd.DataFrame(columns=['PaperID','Title','Section','Text','Affilations'], index=range(len(papers)*50))\n\n# # Remove duplicates in a list:\n# def my_function(x):\n#     return list(dict.fromkeys(x))\n\n# i=0\n# for j in range(len(papers)):\n#     with open(papers[j]) as file:\n#         content = json.load(file)\n        \n#         # ID and Title:\n#         pap_id = content['paper_id']\n#         title =  content['metadata']['title']\n        \n#         # Affiations:\n#         affiliation = []\n#         for sec in content['metadata']['authors']:\n#             try:\n#                 affiliation.append(sec['affiliation']['institution'])\n#             except:\n#                 pass\n#         affiliation = my_function(affiliation)\n        \n#         # Abstract\n#         for sec in content['abstract']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n            \n#         # Body text\n#         for sec in content['body_text']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n\n# papers_data.dropna(inplace=True)\n# papers_data = papers_data.astype(str).drop_duplicates() \n\n# # Text processing:\n# import nltk\n# nltk.download('punkt')\n# # Lowercase:\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 1] = papers_data.iloc[i, 1].lower()\n#         papers_data.iloc[i, 2] = papers_data.iloc[i, 2].lower()\n#         papers_data.iloc[i, 3] = papers_data.iloc[i, 3].lower()\n#         papers_data.iloc[i, 4] = papers_data.iloc[i, 4].lower()\n#     except:\n#         pass\n    \n# # Tokenization:\n\n# from nltk.tokenize import word_tokenize, sent_tokenize , RegexpTokenizer\n\n# tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation\n# papers_data[\"Title_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n# papers_data[\"Text_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = tokenizer.tokenize(papers_data.iloc[i, 1])\n#         papers_data.iloc[i, 6] = tokenizer.tokenize(papers_data.iloc[i, 3])\n#     except:\n#         pass\n    \n# # Remove stopwords:\n# nltk.download('stopwords')\n# from nltk.corpus import stopwords\n# stop_words = set(stopwords.words('english')) \n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = [w for w in papers_data.iloc[i, 5] if not w in stop_words] \n#         papers_data.iloc[i, 6] = [w for w in papers_data.iloc[i, 6] if not w in stop_words]\n#     except:\n#         pass\n    \n# # Words count:  \n# papers_data[\"Words_count\"] = 0\n\n# # for i in range(len(papers_data)):\n# #     try:\n# #         papers_data.iloc[i, 7] = len(papers_data.iloc[i, 6])\n# #     except:\n# #         pass\n    \n# # Lemmatization :\n# nltk.download('wordnet')\n\n# from nltk.stem import WordNetLemmatizer\n\n# wordnet_lemmatizer = WordNetLemmatizer()\n\n# papers_data[\"Text_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 6])):\n#         papers_data.iloc[i, 8].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 6][j]))\n        \n# papers_data[\"Title_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 5])):\n#         papers_data.iloc[i, 9].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 5][j]))\n        \n# papers_data.to_csv(\"\/kaggle\/input\/processed-researches-data\/papers_data_final.csv\")\nprint(\"Preprocessing done!\")","b91d1516":"papers_data = pd.read_csv(\"\/kaggle\/input\/processed-researches-data\/papers_data_final.csv\")\ndel papers_data['Unnamed: 0']\npapers_data.head()","4b894e3f":"import ast\npapers_data['Affilations'] = papers_data['Affilations'].apply(lambda x: ast.literal_eval(x))\npapers_data['Text_Lem_words'] = papers_data['Text_Lem_words'].apply(lambda x: ast.literal_eval(x))","1028d0f3":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef my_function(x):\n    return list(dict.fromkeys(x))\n\nkeywords =['incubation','disease', 'age', 'health','contagious','recovery','periods']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","4fd1ff06":"incubation =[]\nfor i in range(len(papers_data)):\n    if \"incubation\" in papers_data.iloc[i, 8]:\n        incubation.append(i)","18caf786":"len(incubation)","a3fc7874":"disease =[]\nfor i in range(len(papers_data)):\n    if \"disease\" in papers_data.iloc[i, 8]:\n        disease.append(i)","5de3749f":"len(disease)","1743e2b6":"age =[]\nfor i in range(len(papers_data)):\n    if \"age\" in papers_data.iloc[i, 8]:\n        age.append(i)","c8a1d671":"len(age)","1167e700":"health =[]\nfor i in range(len(papers_data)):\n    if \"health\" in papers_data.iloc[i, 8]:\n        health.append(i)","51c5936b":"len(health)","9bd63ad5":"contagious =[]\nfor i in range(len(papers_data)):\n    if \"contagious\" in papers_data.iloc[i, 8]:\n        contagious.append(i)","cc3b9161":"len(contagious)","2e3e5d8e":"recovery =[]\nfor i in range(len(papers_data)):\n    if \"recovery\" in papers_data.iloc[i, 8]:\n        recovery.append(i)","83dd0a50":"len(recovery)","f1ee136a":"period =[]\nfor i in range(len(papers_data)):\n    if \"period\" in papers_data.iloc[i, 8]:\n        period.append(i)","dba7be22":"len(period)","93519f5e":"# At least 6 of words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 5: \n        return(True)  \n    return(False)    \n  \ntask1_1 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_1.append(i)\n    \nlen(task1_1)","4d9f8103":"## Results for task 1.1 :\nfor i in task1_1:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","9a1cd596":"# Task 1.1:\nprint(task1_1)","2e572798":"task1_1_rank = papers_data.iloc[task1_1, :]\ntask1_1_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask1_1_rank['Title'] = task1_1_rank['Title'].astype(str) \n\ntask1_1_rank = pd.merge(task1_1_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_1_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_1_rank['publish_time'] = task1_1_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_1_rank['publish_time'] = task1_1_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_1_rank['publish_time'] = pd.to_numeric(task1_1_rank['publish_time'])\ntask1_1_rank = task1_1_rank.sort_values(by='publish_time', ascending=False)\ntask1_1_rank.reset_index(inplace=True,drop=True)","6e098fd4":"rank = pd.read_csv(\"\/kaggle\/input\/shanghai-ranking\/rank-univ.csv\")\ndel rank['Unnamed: 0']\nrank.head(5)","62bbdef3":"# Extract the affiliations score to the task's results:\ntask1_1_rank['Aff_Score'] = 0\nfor i in range(len(task1_1_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_1_rank.iloc[i, 4]:\n            task1_1_rank.iloc[i, 11] = rank.iloc[j, 3]","babadbd1":"task1_1_rank","f0cc9517":"task1_1_rank[\"Ranking_Score\"] = task1_1_rank[\"publish_time\"]*0.8 + task1_1_rank[\"Aff_Score\"]*0.2","910d14cf":"task1_1_rank.head(5)","0dfde598":"task1_1_rank = task1_1_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_1_rank.reset_index(inplace=True,drop=True)\ntask1_1_rank","00c58639":"## 20 - Ranked Results for task 1.1 :\n\nfor i in range(len(task1_1_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_1_rank.iloc[i, 0])\n    print(\"Title: \", task1_1_rank.iloc[i, 1])\n    print(\"Section: \", task1_1_rank.iloc[i, 2])\n    print(\"Text: \", task1_1_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","2ce90b55":"keywords =['asymptomatic','prevalence', 'shedding', 'transmission','children','elderly','age']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","80bc323d":"# At least 8 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 7: \n        return(True)  \n    return(False)    \n  \ntask1_2 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_2.append(i)\n    \nlen(task1_2)","d64261bb":"## Results for task 1.2 :\nfor i in task1_2:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","4f413488":"# Task 1.2:\nprint(task1_2)","60fbeeb1":"task1_2_rank = papers_data.iloc[task1_2, :]\ntask1_2_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask1_2_rank['Title'] = task1_2_rank['Title'].astype(str) \n\ntask1_2_rank = pd.merge(task1_2_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_2_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_2_rank['publish_time'] = task1_2_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_2_rank['publish_time'] = task1_2_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_2_rank['publish_time'] = pd.to_numeric(task1_2_rank['publish_time'])\ntask1_2_rank = task1_2_rank.sort_values(by='publish_time', ascending=False)\ntask1_2_rank.reset_index(inplace=True,drop=True)","a698526e":"# Extract the affiliations score to the task's results:\ntask1_2_rank['Aff_Score'] = 0\nfor i in range(len(task1_2_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_2_rank.iloc[i, 4]:\n            task1_2_rank.iloc[i, 11] = rank.iloc[j, 3]","d98a9527":"task1_2_rank[\"Ranking_Score\"] = task1_2_rank[\"publish_time\"]*0.8 + task1_2_rank[\"Aff_Score\"]*0.2\ntask1_2_rank = task1_2_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_2_rank.reset_index(inplace=True,drop=True)\ntask1_2_rank","22cf3088":"## 20 - Ranked Results for task 1.2 :\n\nfor i in range(len(task1_2_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_2_rank.iloc[i, 0])\n    print(\"Title: \", task1_2_rank.iloc[i, 1])\n    print(\"Section: \", task1_2_rank.iloc[i, 2])\n    print(\"Text: \", task1_2_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","79c4f8e9":"keywords =['prevalence', 'seasonality','transmission']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","b6d05f83":"# At least 4 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 3: \n        return(True)  \n    return(False)    \n  \ntask1_3 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_3.append(i)\n    \nlen(task1_3)","c9decb35":"## Results for task 1.3 :\nfor i in task1_3:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","d1410036":"# Task 1.3:\nprint(task1_3)","2bae7a51":"task1_3_rank = papers_data.iloc[task1_3, :]\ntask1_3_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask1_3_rank['Title'] = task1_3_rank['Title'].astype(str) \n\ntask1_3_rank = pd.merge(task1_3_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_3_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_3_rank['publish_time'] = task1_3_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_3_rank['publish_time'] = task1_3_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_3_rank['publish_time'] = pd.to_numeric(task1_3_rank['publish_time'])\ntask1_3_rank = task1_3_rank.sort_values(by='publish_time', ascending=False)\ntask1_3_rank.reset_index(inplace=True,drop=True)","3dbf3aed":"# Extract the affiliations score to the task's results:\ntask1_3_rank['Aff_Score'] = 0\nfor i in range(len(task1_3_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_3_rank.iloc[i, 4]:\n            task1_3_rank.iloc[i, 11] = rank.iloc[j, 3]","32c9f886":"task1_3_rank[\"Ranking_Score\"] = task1_3_rank[\"publish_time\"]*0.8 + task1_3_rank[\"Aff_Score\"]*0.2\ntask1_3_rank = task1_3_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_3_rank.reset_index(inplace=True,drop=True)\ntask1_3_rank","4ba50eb4":"## 20 - Ranked Results for task 1.3 :\n\nfor i in range(len(task1_3_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_3_rank.iloc[i, 0])\n    print(\"Title: \", task1_3_rank.iloc[i, 1])\n    print(\"Section: \", task1_3_rank.iloc[i, 2])\n    print(\"Text: \", task1_3_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","bd0631fb":"keywords =['physical', 'coronavirus','charge','distribution','adhesion','hydrophilic','phobic','surfaces','environmental','survival','decontamination','affected','area','viral','shedding']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","c67ec1bf":"# At least 10 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 9: \n        return(True)  \n    return(False)    \n  \ntask1_4 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_4.append(i)\n    \nlen(task1_4)","f816d37b":"## Results for task 1.4 :\nfor i in task1_4:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","64f0a752":"# Task 1.4:\nprint(task1_4)","57618def":"task1_4_rank = papers_data.iloc[task1_4, :]\ntask1_4_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask1_4_rank['Title'] = task1_4_rank['Title'].astype(str) \n\ntask1_4_rank = pd.merge(task1_4_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_4_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_4_rank['publish_time'] = task1_4_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_4_rank['publish_time'] = task1_4_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_4_rank['publish_time'] = pd.to_numeric(task1_4_rank['publish_time'])\ntask1_4_rank = task1_4_rank.sort_values(by='publish_time', ascending=False)\ntask1_4_rank.reset_index(inplace=True,drop=True)","9d22ac84":"# Extract the affiliations score to the task's results:\ntask1_4_rank['Aff_Score'] = 0\nfor i in range(len(task1_4_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_4_rank.iloc[i, 4]:\n            task1_4_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_4_rank[\"Ranking_Score\"] = task1_4_rank[\"publish_time\"]*0.8 + task1_4_rank[\"Aff_Score\"]*0.2\ntask1_4_rank = task1_4_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_4_rank.reset_index(inplace=True,drop=True)\ntask1_4_rank","ad0bc372":"## 20 - Ranked Results for task 1.4 :\n\nfor i in range(len(task1_4_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_4_rank.iloc[i, 0])\n    print(\"Title: \", task1_4_rank.iloc[i, 1])\n    print(\"Section: \", task1_4_rank.iloc[i, 2])\n    print(\"Text: \", task1_4_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","10f1fef9":"keywords =['persistence', 'stability','multitude','substrates','sources','nasal','discharge','sputum','urine','fecal','blood']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","77a0a0c7":"# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 6: \n        return(True)  \n    return(False)    \n  \ntask1_5 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_5.append(i)\n    \nlen(task1_5)","520332f3":"## Results for task 1.5 :\nfor i in task1_5:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","d8ac8775":"# Task 1.5 \nprint(task1_5)","1acdc342":"task1_5_rank = papers_data.iloc[task1_5, :]\ntask1_5_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask1_5_rank['Title'] = task1_5_rank['Title'].astype(str) \n\ntask1_5_rank = pd.merge(task1_5_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_5_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_5_rank['publish_time'] = task1_5_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_5_rank['publish_time'] = task1_5_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_5_rank['publish_time'] = pd.to_numeric(task1_5_rank['publish_time'])\ntask1_5_rank = task1_5_rank.sort_values(by='publish_time', ascending=False)\ntask1_5_rank.reset_index(inplace=True,drop=True)","2063bf26":"# Extract the affiliations score to the task's results:\ntask1_5_rank['Aff_Score'] = 0\nfor i in range(len(task1_5_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_5_rank.iloc[i, 4]:\n            task1_5_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_5_rank[\"Ranking_Score\"] = task1_5_rank[\"publish_time\"]*0.8 + task1_5_rank[\"Aff_Score\"]*0.2\ntask1_5_rank = task1_5_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_5_rank.reset_index(inplace=True,drop=True)\ntask1_5_rank","37737262":"## 20 - Ranked Results for task 1.5 :\n\nfor i in range(len(task1_5_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_5_rank.iloc[i, 0])\n    print(\"Title: \", task1_5_rank.iloc[i, 1])\n    print(\"Section: \", task1_5_rank.iloc[i, 2])\n    print(\"Text: \", task1_5_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","9f081e54":"keywords =['persistence', 'virus','surface','material','copper','steel','stainlesss','plastic']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","724c0b50":"# At least 6 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 5: \n        return(True)  \n    return(False)    \n  \ntask1_6 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_6.append(i)\n    \nlen(task1_6)","5232a87d":"## Results for task 1.6 :\nfor i in task1_6:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","3425ab57":"# Task 1.6:\nprint(task1_6)","f13830b5":"task1_6_rank = papers_data.iloc[task1_6, :]\ntask1_6_rank.reset_index(inplace=True,drop=True)\n\ntask1_6_rank['Title'] = task1_6_rank['Title'].astype(str) \n\ntask1_6_rank = pd.merge(task1_6_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_6_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_6_rank['publish_time'] = task1_6_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_6_rank['publish_time'] = task1_6_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_6_rank['publish_time'] = pd.to_numeric(task1_6_rank['publish_time'])\ntask1_6_rank = task1_6_rank.sort_values(by='publish_time', ascending=False)\ntask1_6_rank.reset_index(inplace=True,drop=True)","4de48b65":"# Extract the affiliations score to the task's results:\ntask1_6_rank['Aff_Score'] = 0\nfor i in range(len(task1_6_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_6_rank.iloc[i, 4]:\n            task1_6_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_6_rank[\"Ranking_Score\"] = task1_6_rank[\"publish_time\"]*0.8 + task1_6_rank[\"Aff_Score\"]*0.2\ntask1_6_rank = task1_6_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_6_rank.reset_index(inplace=True,drop=True)\ntask1_6_rank","fb90321f":"## 20 - Ranked Results for task 1.6 :\n\nfor i in range(len(task1_6_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_6_rank.iloc[i, 0])\n    print(\"Title: \", task1_6_rank.iloc[i, 1])\n    print(\"Section: \", task1_6_rank.iloc[i, 2])\n    print(\"Text: \", task1_6_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","c42c243f":"keywords =['natural', 'history','virus','shedding','infected','person']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","b267a288":"# At least 6 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 5: \n        return(True)  \n    return(False)    \n  \ntask1_7 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_7.append(i)\n    \nlen(task1_7)","be70d324":"## Results for task 1.7 :\nfor i in task1_7:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","8f67530b":"task1_7_rank = papers_data.iloc[task1_7, :]\ntask1_7_rank.reset_index(inplace=True,drop=True)\n\ntask1_7_rank['Title'] = task1_7_rank['Title'].astype(str) \n\ntask1_7_rank = pd.merge(task1_7_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_7_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_7_rank['publish_time'] = task1_7_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_7_rank['publish_time'] = task1_7_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_7_rank['publish_time'] = pd.to_numeric(task1_7_rank['publish_time'])\ntask1_7_rank = task1_7_rank.sort_values(by='publish_time', ascending=False)\ntask1_7_rank.reset_index(inplace=True,drop=True)","9785ad46":"# Extract the affiliations score to the task's results:\ntask1_7_rank['Aff_Score'] = 0\nfor i in range(len(task1_7_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_7_rank.iloc[i, 4]:\n            task1_7_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_7_rank[\"Ranking_Score\"] = task1_7_rank[\"publish_time\"]*0.8 + task1_7_rank[\"Aff_Score\"]*0.2\ntask1_7_rank = task1_7_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_7_rank.reset_index(inplace=True,drop=True)\ntask1_7_rank","8f377b06":"## 20 - Ranked Results for task 1.7 :\n\nfor i in range(len(task1_7_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_7_rank.iloc[i, 0])\n    print(\"Title: \", task1_7_rank.iloc[i, 1])\n    print(\"Section: \", task1_7_rank.iloc[i, 2])\n    print(\"Text: \", task1_7_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","c38387b5":"keywords =['diagnostic', 'product','clinical','process']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","b7ed3bba":"# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask1_8 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_8.append(i)\n    \nlen(task1_8)","421abfb0":"## Results for task 1.8 :\nfor i in task1_8:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","729f5fe9":"task1_8_rank = papers_data.iloc[task1_8, :]\ntask1_8_rank.reset_index(inplace=True,drop=True)\n\ntask1_8_rank['Title'] = task1_8_rank['Title'].astype(str) \n\ntask1_8_rank = pd.merge(task1_8_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_8_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_8_rank['publish_time'] = task1_8_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_8_rank['publish_time'] = task1_8_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_8_rank['publish_time'] = pd.to_numeric(task1_8_rank['publish_time'])\ntask1_8_rank = task1_8_rank.sort_values(by='publish_time', ascending=False)\ntask1_8_rank.reset_index(inplace=True,drop=True)","df597086":"# Extract the affiliations score to the task's results:\ntask1_8_rank['Aff_Score'] = 0\nfor i in range(len(task1_8_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_8_rank.iloc[i, 4]:\n            task1_8_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_8_rank[\"Ranking_Score\"] = task1_8_rank[\"publish_time\"]*0.8 + task1_8_rank[\"Aff_Score\"]*0.2\ntask1_8_rank = task1_8_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_8_rank.reset_index(inplace=True,drop=True)\ntask1_8_rank","d2cd6123":"## 20 - Ranked Results for task 1.8 :\n\nfor i in range(len(task1_8_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_8_rank.iloc[i, 0])\n    print(\"Title: \", task1_8_rank.iloc[i, 1])\n    print(\"Section: \", task1_8_rank.iloc[i, 2])\n    print(\"Text: \", task1_8_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","cd6aca69":"keywords =['disease', 'model','animal','infection','transmission']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","94b49e8d":"# At least 8 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 7: \n        return(True)  \n    return(False)    \n  \ntask1_9 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_9.append(i)\n    \nlen(task1_9)","4b1d9daf":"## Results for task 1.9 :\nfor i in task1_9:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","8567c383":"task1_9_rank = papers_data.iloc[task1_9, :]\ntask1_9_rank.reset_index(inplace=True,drop=True)\n\ntask1_9_rank['Title'] = task1_9_rank['Title'].astype(str) \n\ntask1_9_rank = pd.merge(task1_9_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_9_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_9_rank['publish_time'] = task1_9_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_9_rank['publish_time'] = task1_9_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_9_rank['publish_time'] = pd.to_numeric(task1_9_rank['publish_time'])\ntask1_9_rank = task1_9_rank.sort_values(by='publish_time', ascending=False)\ntask1_9_rank.reset_index(inplace=True,drop=True)","532ab96f":"# Extract the affiliations score to the task's results:\ntask1_9_rank['Aff_Score'] = 0\nfor i in range(len(task1_9_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_9_rank.iloc[i, 4]:\n            task1_9_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_9_rank[\"Ranking_Score\"] = task1_9_rank[\"publish_time\"]*0.8 + task1_9_rank[\"Aff_Score\"]*0.2\ntask1_9_rank = task1_9_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_9_rank.reset_index(inplace=True,drop=True)\ntask1_9_rank","0834785a":"## 20 - Ranked Results for task 1.9 :\n\nfor i in range(len(task1_9_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_9_rank.iloc[i, 0])\n    print(\"Title: \", task1_9_rank.iloc[i, 1])\n    print(\"Section: \", task1_9_rank.iloc[i, 2])\n    print(\"Text: \", task1_9_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","6f0f6ad5":"keywords =['tool', 'studies','monitor','phenotypic','potential','adaptation','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","f282f5f4":"# At least 8 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 7: \n        return(True)  \n    return(False)    \n  \ntask1_10 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_10.append(i)\n    \nlen(task1_10)","62ebfc35":"## Results for task 1.10 :\nfor i in task1_10:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","8d0a9aa1":"task1_10_rank = papers_data.iloc[task1_10, :]\ntask1_10_rank.reset_index(inplace=True,drop=True)\n\ntask1_10_rank['Title'] = task1_10_rank['Title'].astype(str) \n\ntask1_10_rank = pd.merge(task1_10_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_10_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_10_rank['publish_time'] = task1_10_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask1_10_rank['publish_time'] = task1_10_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_10_rank['publish_time'] = pd.to_numeric(task1_10_rank['publish_time'])\ntask1_10_rank = task1_10_rank.sort_values(by='publish_time', ascending=False)\ntask1_10_rank.reset_index(inplace=True,drop=True)","38a83976":"# Extract the affiliations score to the task's results:\ntask1_10_rank['Aff_Score'] = 0\nfor i in range(len(task1_10_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_10_rank.iloc[i, 4]:\n            task1_10_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_10_rank[\"Ranking_Score\"] = task1_10_rank[\"publish_time\"]*0.8 + task1_10_rank[\"Aff_Score\"]*0.2\ntask1_10_rank = task1_10_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_10_rank.reset_index(inplace=True,drop=True)\ntask1_10_rank","1ed17c30":"## 20 - Ranked Results for task 1.10 :\n\nfor i in range(len(task1_10_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_10_rank.iloc[i, 0])\n    print(\"Title: \", task1_10_rank.iloc[i, 1])\n    print(\"Section: \", task1_10_rank.iloc[i, 2])\n    print(\"Text: \", task1_10_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","d7b6e4f0":"keywords =['Immune','immunity','coronavirus','corona','covid']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","fbc9a39c":"# At least 4 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 3: \n        return(True)  \n    return(False)    \n  \ntask1_11 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_11.append(i)\n    \nlen(task1_11)","6bb9873a":"## Results for task 1.11 :\nj = 0\nfor i in task1_11:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    j = j + 1\n    if j==10:\n        break","f2435fcd":"task1_11_rank = papers_data.iloc[task1_11, :]\ntask1_11_rank.reset_index(inplace=True,drop=True)\n\ntask1_11_rank['Title'] = task1_11_rank['Title'].astype(str) \n\ntask1_11_rank = pd.merge(task1_11_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_11_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_11_rank['publish_time'] = task1_11_rank['publish_time'].apply(lambda x:  str(x).replace('Oct 28 Mar-Apr',''))\ntask1_11_rank['publish_time'] = task1_11_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_11_rank['publish_time'] = pd.to_numeric(task1_11_rank['publish_time'])\ntask1_11_rank = task1_11_rank.sort_values(by='publish_time', ascending=False)\ntask1_11_rank.reset_index(inplace=True,drop=True)","bffab2f8":"# Extract the affiliations score to the task's results:\ntask1_11_rank['Aff_Score'] = 0\nfor i in range(len(task1_11_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_11_rank.iloc[i, 4]:\n            task1_11_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_11_rank[\"Ranking_Score\"] = task1_11_rank[\"publish_time\"]*0.8 + task1_11_rank[\"Aff_Score\"]*0.2\ntask1_11_rank = task1_11_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_11_rank.reset_index(inplace=True,drop=True)\ntask1_11_rank","6cb9436d":"## 20 - Ranked Results for task 1.11 :\n\nfor i in range(len(task1_11_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_11_rank.iloc[i, 0])\n    print(\"Title: \", task1_11_rank.iloc[i, 1])\n    print(\"Section: \", task1_11_rank.iloc[i, 2])\n    print(\"Text: \", task1_11_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","40340ef6":"keywords =['Effectiveness', 'movement','control','strategies','prevent','secondary','transmission','health','care','community','settings']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","79ed80e5":"# At least 13 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 12: \n        return(True)  \n    return(False)    \n  \ntask1_12 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_12.append(i)\n    \nlen(task1_12)","779efaad":"## Results for task 1.12 :\nfor i in task1_12:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","65eefd35":"task1_12_rank = papers_data.iloc[task1_12, :]\ntask1_12_rank.reset_index(inplace=True,drop=True)\n\ntask1_12_rank['Title'] = task1_12_rank['Title'].astype(str) \n\ntask1_12_rank = pd.merge(task1_12_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_12_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_12_rank['publish_time'] = task1_12_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask1_12_rank['publish_time'] = task1_12_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_12_rank['publish_time'] = pd.to_numeric(task1_12_rank['publish_time'])\ntask1_12_rank = task1_12_rank.sort_values(by='publish_time', ascending=False)\ntask1_12_rank.reset_index(inplace=True,drop=True)","0df2835b":"# Extract the affiliations score to the task's results:\ntask1_12_rank['Aff_Score'] = 0\nfor i in range(len(task1_12_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_12_rank.iloc[i, 4]:\n            task1_12_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_12_rank[\"Ranking_Score\"] = task1_12_rank[\"publish_time\"]*0.8 + task1_12_rank[\"Aff_Score\"]*0.2\ntask1_12_rank = task1_12_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_12_rank.reset_index(inplace=True,drop=True)\ntask1_12_rank","559fb985":"## 20 - Ranked Results for task 1.12 :\n\nfor i in range(len(task1_12_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_12_rank.iloc[i, 0])\n    print(\"Title: \", task1_12_rank.iloc[i, 1])\n    print(\"Section: \", task1_12_rank.iloc[i, 2])\n    print(\"Text: \", task1_12_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","6c8a9b05":"keywords =['Effectiveness', 'protective','ppe','equipment','usefulness','risk','reduce','health','care','community','settings','transmission']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","3db8b441":"# At least 12 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask1_13 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_13.append(i)\n    \nlen(task1_13)","9103e298":"## Results for task 1.13 :\nfor i in task1_13:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","e776e537":"task1_13_rank = papers_data.iloc[task1_13, :]\ntask1_13_rank.reset_index(inplace=True,drop=True)\n\ntask1_13_rank['Title'] = task1_13_rank['Title'].astype(str) \n\ntask1_13_rank = pd.merge(task1_13_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_13_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_13_rank['publish_time'] = task1_13_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask1_13_rank['publish_time'] = task1_13_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_13_rank['publish_time'] = pd.to_numeric(task1_13_rank['publish_time'])\ntask1_13_rank = task1_13_rank.sort_values(by='publish_time', ascending=False)\ntask1_13_rank.reset_index(inplace=True,drop=True)","e4389a46":"# Extract the affiliations score to the task's results:\ntask1_13_rank['Aff_Score'] = 0\nfor i in range(len(task1_13_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_13_rank.iloc[i, 4]:\n            task1_13_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_13_rank[\"Ranking_Score\"] = task1_13_rank[\"publish_time\"]*0.8 + task1_13_rank[\"Aff_Score\"]*0.2\ntask1_13_rank = task1_13_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_13_rank.reset_index(inplace=True,drop=True)\ntask1_13_rank","930322dd":"## 20 - Ranked Results for task 1.13 :\n\nfor i in range(len(task1_13_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_13_rank.iloc[i, 0])\n    print(\"Title: \", task1_13_rank.iloc[i, 1])\n    print(\"Section: \", task1_13_rank.iloc[i, 2])\n    print(\"Text: \", task1_13_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","b1f6aa95":"keywords =['transmission','role','environment']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","be892ac1":"# At least 6 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 5: \n        return(True)  \n    return(False)    \n  \ntask1_14 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task1_14.append(i)\n    \nlen(task1_14)","b6515906":"## Results for task 1.14 :\nfor i in task1_14:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    ","82773688":"task1_14_rank = papers_data.iloc[task1_13, :]\ntask1_14_rank.reset_index(inplace=True,drop=True)\n\ntask1_14_rank['Title'] = task1_14_rank['Title'].astype(str) \n\ntask1_14_rank = pd.merge(task1_14_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask1_14_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask1_14_rank['publish_time'] = task1_14_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask1_14_rank['publish_time'] = task1_14_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask1_14_rank['publish_time'] = pd.to_numeric(task1_14_rank['publish_time'])\ntask1_14_rank = task1_14_rank.sort_values(by='publish_time', ascending=False)\ntask1_14_rank.reset_index(inplace=True,drop=True)","2f26e533":"# Extract the affiliations score to the task's results:\ntask1_14_rank['Aff_Score'] = 0\nfor i in range(len(task1_14_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task1_14_rank.iloc[i, 4]:\n            task1_14_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask1_14_rank[\"Ranking_Score\"] = task1_14_rank[\"publish_time\"]*0.8 + task1_14_rank[\"Aff_Score\"]*0.2\ntask1_14_rank = task1_14_rank.sort_values(by='Ranking_Score', ascending=False)\ntask1_14_rank.reset_index(inplace=True,drop=True)\ntask1_14_rank","211dccea":"## 20 - Ranked Results for task 1.14 :\n\nfor i in range(len(task1_14_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task1_14_rank.iloc[i, 0])\n    print(\"Title: \", task1_14_rank.iloc[i, 1])\n    print(\"Section: \", task1_14_rank.iloc[i, 2])\n    print(\"Text: \", task1_14_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","d222af40":"task1_1_rank.to_csv(\"task1_1_rank.csv\")\ntask1_2_rank.to_csv(\"task1_2_rank.csv\")\ntask1_3_rank.to_csv(\"task1_3_rank.csv\")\ntask1_4_rank.to_csv(\"task1_4_rank.csv\")\ntask1_5_rank.to_csv(\"task1_5_rank.csv\")\ntask1_6_rank.to_csv(\"task1_6_rank.csv\")\ntask1_7_rank.to_csv(\"task1_7_rank.csv\")\ntask1_8_rank.to_csv(\"task1_8_rank.csv\")\ntask1_9_rank.to_csv(\"task1_9_rank.csv\")\ntask1_10_rank.to_csv(\"task1_10_rank.csv\")\ntask1_11_rank.to_csv(\"task1_11_rank.csv\")\ntask1_12_rank.to_csv(\"task1_12_rank.csv\")\ntask1_13_rank.to_csv(\"task1_13_rank.csv\")\ntask1_14_rank.to_csv(\"task1_14_rank.csv\")","b7938798":"### Task 1.14: ***Role of the environment in transmission***","611149ea":"### Task 1.4: ***Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).***","5ff76104":"### By affiliations Scores:\n","90f3d5fe":"### Ranking by the most recent:","6368d578":"### By the effiliations scores:","8cf73b7e":"### By affiliations Scores:","3598e84f":"### Task 1.7: ***Natural history of the virus and shedding of it from an infected person***","41d4938f":"### Ranking by the most recent:","671722ac":"### Task 1.1: ***Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.***","767beebc":"### Ranking by the most recent:","a6289b3d":"### Task 1.6: ***Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).***","013972ad":"### Ranking by most Recent:","0522a8ab":"# COVID-19: Open Research Dataset Challenge (CORD-19)\n# Task 01: What is known about transmission, incubation, and environmental stability?\n\n\n\n### Purpose:\n\n- Our purpose is to extract relevant text sections related to the proposed questions from all the research dataset. \n\n### Description of the methodology steps:\n\n- Upload and structure all researches paper's sections separately.\n- P[](http:\/\/)rocess the sections' text using the nltk package (lowercase, remove punctuations, tokenization, Lemmatization).\n- To search for the relevant results about each task:   \n    - Select the keywords and their synonyms.   \n    - Search in the dataset those keywords (or most of them) on the lemmatized tokens of the sections.\n- While the obtained results are very large, a ranking of the resulted papers' sections is recommended using both: the freshness degree and the rank score of the papers' institutions, according to the following Formula: \n    \n    ***Score = Affiliation Score . (0.2) + Publication Year . (0.8)***\n    \n    \n- The obtained results now are ranked, and ready to get used!\n\n","e3b9b867":"### By affiliations Score:","c9a11e1a":"### By affiliations Scores:","76c9852d":"### Ranking by the most recent:","26a3168a":"### Ranking by the most recent:","0a550a98":"### By affiliations scores:","b5ad955a":"### By affiliation scores:","62ca7600":"### Save all sub-tasks results:","1e6a7a86":"### By affiliations Scores:\n\n- Ranking using the : http:\/\/www.shanghairanking.com\/arwu2019.html","91a186e2":"### By affiliations scores:","b9907a6d":"### Task 1.10: ***Tools and studies to monitor phenotypic change and potential adaptation of the virus***","2b1e0475":"### By affiliations score:","a519a5f8":"### By affiliations scores:","1815f191":"### Ranking by the most recent:","9df49f4a":"### Task 1.8: ***Implementation of diagnostics and products to improve clinical processes***","2a453509":"# Search for papers sections containing task's keywords:","93616062":"### Task 01 : What is known about transmission, incubation, and environmental stability?\n\n#### Task Details\n\n- What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n\n- Specifically, we want to know what the literature reports about:\n    \n    - Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n    \n    - Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n    - Seasonality of transmission.\n    - Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n    - Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n    - Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n    - Natural history of the virus and shedding of it from an infected person\n    - Implementation of diagnostics and products to improve clinical processes\n    - Disease models, including animal models for infection, disease and transmission \n    - Tools and studies to monitor phenotypic change and potential adaptation of the virus \n    - Immune response and immunity\n    - Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n    - Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n    - Role of the environment in transmission","8b836ea1":"### Ranking by teh most recent:","17ff384a":"### Task 1.13: ***Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings***","fb0ea37f":"### By affilitions Scores:","a3765636":"### Task 1.12: ***Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings***","69c1cb14":"### Ranking by most Recent:","684e3f36":"### Ranking by the most recent:","21519d62":"### Ranking by the most recent:","4ae35d84":"### Task1.2: ***Prevalence of asymptomatic shedding and transmission (e.g., particularly children)***","93a9b8a9":"### Task 1.9: ***Disease models, including animal models for infection, disease and transmission***","5896497d":"### Task 1.5: ***Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).***","a2412c2c":"### Ranking by the most recent:","f475e507":"### Ranking by the most recent:","0772b3c5":"### By affiliation scores:","ac67d1db":"### Ranking by the most recent:","8ce972d7":"### Task 1.11: ***Immune response and immunity***","21ac080a":"### By afffiliation scores:","c1d08098":"### Task 1.3: ***Seasonality of transmission.***"}}