{"cell_type":{"66f06c69":"code","772b7e16":"code","cdc3a0b2":"code","ee8a4c42":"code","01cc0217":"code","33683659":"code","9af15257":"code","470295d0":"code","c53b48a3":"code","f08473a0":"code","8fa4bf18":"code","bda55674":"code","cafeabba":"code","a823c45d":"code","fe038258":"code","9febb229":"code","8928cc2d":"code","e9556b24":"code","3f341920":"code","d5c10cdf":"code","431f0e36":"code","6c810a12":"code","4052cb4f":"code","04e5fc7f":"code","1e611a09":"code","df6795d4":"code","f02031e3":"code","44173ac2":"code","fe947e1d":"code","d354e839":"code","e4bf1e37":"code","337ecc3b":"code","c86abdb6":"code","5c4aa7e3":"code","752d639e":"code","c9337b6c":"code","67acc936":"code","2e8609f8":"code","273a5f84":"code","6f485a0e":"code","268d4971":"code","f4146569":"code","a1424416":"code","f0515f03":"code","bad3ea3a":"code","6a0dd057":"code","2a28752b":"code","a8a22706":"code","fd570f1a":"code","9b8660d6":"code","1f977053":"code","22d2ff59":"code","9b377f6e":"code","9a9ea947":"code","fa41b0dd":"code","b0eb3355":"markdown","e7faa6bc":"markdown","d052ca16":"markdown","58c02e58":"markdown","8796efa0":"markdown","5dc9140c":"markdown","9eceb5af":"markdown","ba7da71f":"markdown","03d7d243":"markdown","25d6de9d":"markdown","fd711bb8":"markdown","127042d7":"markdown","a75e998f":"markdown","97575c47":"markdown","6c055ed7":"markdown","3b704b0e":"markdown","4396608d":"markdown","c93f5956":"markdown","7c12725a":"markdown","be2e77a7":"markdown","4cc93859":"markdown","c40c00d1":"markdown","f9225249":"markdown","9bbc0baa":"markdown","749366ed":"markdown","c402d939":"markdown","49486b4d":"markdown","312aebe1":"markdown","ce5833b0":"markdown","1531e828":"markdown","6869473d":"markdown","090cddfb":"markdown","8a19ca16":"markdown","9cdfbd8b":"markdown","69771fc0":"markdown","99520069":"markdown"},"source":{"66f06c69":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\nimport nltk\nnltk.download('punkt')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nimport seaborn as sns \nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\n\nimport time\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","772b7e16":"true_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","cdc3a0b2":"true_news['target'] = 1\nfake_news['target'] = 0\ndf = pd.concat([true_news, fake_news]).reset_index(drop = True)\ndf['complete'] = df['title'] + ' ' + df['text']\ndf.head()","ee8a4c42":"df.isnull().sum()","01cc0217":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use','says'])\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:\n            result.append(token)\n            \n    return result","33683659":"# Transforming the unmatching subjects to the same notation\ndf.subject=df.subject.replace({'politics':'PoliticsNews','politicsNews':'PoliticsNews'})","9af15257":"sub_tf_df=df.groupby('target').apply(lambda x:x['title'].count()).reset_index(name='Counts')\nsub_tf_df.target.replace({0:'False',1:'True'},inplace=True)\nsub_tf_df","470295d0":"fig = px.bar(sub_tf_df, x=\"target\", y=\"Counts\",\n             color='Counts', barmode='group',\n             height=400)\nfig.show()","c53b48a3":"sub_check=df.groupby('subject').apply(lambda x:x['title'].count()).reset_index(name='Counts')\nfig=px.bar(sub_check,x='subject',y='Counts',color='Counts',title='Count of News Articles by Subject')\nfig.show()","f08473a0":"df['clean_title'] = df['title'].apply(preprocess)\ndf['clean_title'][0]","8fa4bf18":"df['clean_joined_title']=df['clean_title'].apply(lambda x:\" \".join(x))\ndf.head()","bda55674":"#wordcloud for true news \nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.target == 1].clean_joined_title))\nplt.imshow(wc, interpolation = 'bilinear')","cafeabba":"#wordcloud for fake news \nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.target == 0].clean_joined_title))\nplt.imshow(wc, interpolation = 'bilinear')","a823c45d":"maxlen = -1\nfor doc in df.clean_joined_title:\n    tokens = nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen = len(tokens)\nprint(\"The maximum number of words in a title is =\", maxlen)\nfig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined_title], nbins = 50)\nfig.show()","fe038258":"X_train, X_test, y_train, y_test = train_test_split(df.clean_joined_title, df.target, test_size = 0.2,random_state=2, stratify = df.target)","9febb229":"print(X_test.head())\nprint(y_test.head())","8928cc2d":"lengths = [len(x) for x in df.clean_joined_title]\nmax_length = max(lengths)\nmax_length\ntrunc_type = 'post'\npadding_type = 'post'\n\n","e9556b24":"embedding_dim = 100\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nvocab_size=len(word_index)\n","3f341920":"sequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n","d5c10cdf":"test_sequences = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","431f0e36":"print(X_train[0], y_train[0])\n","6c810a12":"from keras.callbacks import EarlyStopping\noverfitCallback = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=5,\n                              verbose=0, mode='auto')\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, 15, input_length=max_length),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\nhistory = model.fit(padded, y_train, epochs=num_epochs, validation_data=(test_sequences, y_test), verbose=2, callbacks=[overfitCallback])\n\nprint(\"Training Complete\")","4052cb4f":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()","04e5fc7f":"df.head()","1e611a09":"df_whole = df.loc[:,[\"complete\",\"target\"]]\ndf_whole.head()","df6795d4":"df_whole['clean_text'] = df['complete'].apply(preprocess)\ndf_whole['clean_text'][0]","f02031e3":"\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(df_whole['complete'])\nx_tokenized = tokenizer.texts_to_sequences(df_whole['complete'])\n","44173ac2":"x = df_whole[\"complete\"]\ny = df_whole[\"target\"]","fe947e1d":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=18)","d354e839":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?:\/\/\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","e4bf1e37":"#Convert text to vectors, our classifier only takes numerical data. \nmax_vocab = 10000\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)","337ecc3b":"# tokenize the text into vectors \nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","c86abdb6":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)","5c4aa7e3":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_vocab, 32),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","752d639e":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])","c9337b6c":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()","67acc936":"model.evaluate(X_test, y_test)","2e8609f8":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","273a5f84":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","6f485a0e":"matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(16, 10))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15)","268d4971":"length_array = [len(s) for s in X_train]\nSEQUENCE_LENGTH = int(np.quantile(length_array,0.75))\nprint(SEQUENCE_LENGTH)","f4146569":"# We've added 1 because or word index has numbers from 1 to end but we've added 0 tokens in padding so our vocab now has \n#len(tokenizer.word_index) + 1\nVOCAB_LENGTH = len(tokenizer.word_index) + 1\nVECTOR_SIZE = 100\n\ndef getModel():\n    \"\"\"\n    Returns a trainable Sigmoid Convolutional Neural Network\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Embedding(input_dim= VOCAB_LENGTH, output_dim=VECTOR_SIZE, input_length=SEQUENCE_LENGTH))\n    \n    model.add(layers.Conv1D(128,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(256,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(512,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1,activation=\"sigmoid\"))\n    \n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n    \n    return model\n","a1424416":"model = getModel()\nmodel.summary()","f0515f03":"history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=1)","bad3ea3a":"model.save_weights(\"trained_model.h5\")","6a0dd057":"import pickle\nwith open(\"tokenizer.pickle\",mode=\"wb\") as F:\n    pickle.dump(tokenizer,F)","2a28752b":"import json\nlabel_map = {0:\"Fake\",\n             1:\"Real\"\n            }\n\njson.dump(label_map,open(\"label_map.json\",mode=\"w\"))","a8a22706":"def cleanText(text):\n    cleaned = re.sub(\"[^'a-zA-Z0-9]\",\" \",text)\n    lowered = cleaned.lower().strip()\n    return lowered","fd570f1a":"x_cleaned = [cleanText(t) for t in x]","9b8660d6":"class DeployModel():\n    \n    def __init__(self,weights_path,tokenizer_path,seq_length,label_map_path\n                ):\n        \n        self.model = getModel()\n        self.model.load_weights(weights_path)\n        self.tokenizer = pickle.load(open(tokenizer_path,mode=\"rb\"))\n        self.seq_len = seq_length\n        self.label_map = json.load(open(label_map_path))\n    \n    def _prepare_data(self,text):\n        \n        cleaned = cleanText(text)\n        tokenized = self.tokenizer.texts_to_sequences([cleaned])\n        padded = pad_sequences(tokenized,maxlen=self.seq_len)\n        return padded\n    \n    def _predict(self,text):\n        \n        text = self._prepare_data(text)\n        pred = int(self.model.predict_classes(text)[0])\n        return str(pred)\n    \n    def result(self,text):\n        \n        pred = self._predict(text)\n        return self.label_map[pred]","1f977053":"deploy_model = DeployModel(weights_path=\".\/trained_model.h5\",\n                           tokenizer_path=\".\/tokenizer.pickle\",\n                           seq_length=SEQUENCE_LENGTH,\n                           label_map_path=\".\/label_map.json\"\n                          )","22d2ff59":"test_text_real = x_cleaned[1000]","9b377f6e":"print(test_text_real)\nprint(\"\\n\\n===========================\")\nprint(\"Results: \",deploy_model.result(test_text_real))","9a9ea947":"test_text_fake = x_cleaned[30000]","fa41b0dd":"print(test_text_fake)\nprint(\"\\n\\n===========================\")\nprint(\"Results: \",deploy_model.result(test_text_fake))","b0eb3355":"**Tokenizing and the padding the titles so that all have the same length, which is the length of the longest title**","e7faa6bc":"## Data Cleaning","d052ca16":"**Saving our label map using json library.**","58c02e58":"**Video, Obama, trump, hillary are some of the most evident words present in Fake news dataset.**","8796efa0":"\n## Model 2: Text Classification with RNN on whole text","5dc9140c":"**Making sure our text data is clean.**","9eceb5af":"## Model 3:  Fake news classification on whole text Using CNN","ba7da71f":"## Detection of the topics that are emerging most in the analysis of Fake news.","03d7d243":"**Saving weights of our model and pickle our tokenizer.**","25d6de9d":"**Observation: The maximum number of titles ranges from 7-8 words on average. It will be difficult to determine whether the news is real or false based on these few words alone. But we're hoping we won't get a lot of accuracy just by looking at the title. Let us continue with our forecast.**","fd711bb8":"## Reading the Dataset","127042d7":"**Distribution of Subjects between the True and Fake News**","a75e998f":"**Creating a target variable and merging the datasets for true and false news**","97575c47":"**Apply padding so we have the same length for each article**","6c055ed7":"**We will be using CNN + LSTM, which are genrally used for the task of generating textual descriptions of images. \nIn our model CNN will be used as feature extracter on the textual input and pass the it to LSTM through hidden layer for classification.**","3b704b0e":"## Analysis to check how efficient News Headlines are to predict if the news are fake or not.","4396608d":"**To avoid too much overfitting, an early stopping callback was described. The model is made up of 5 layers that are stacked in a specific order. After 4 epochs, the model stopped early and has a 98\npercent validation accuracy.**","c93f5956":"**Official, White House, trump, China, North Korea are some of the most evident words present in Real news dataset.**","7c12725a":"## Importing Important Libraries","be2e77a7":"### Subjects receiving the most News Coverage","4cc93859":"### Building the RNN.","c40c00d1":"**RNNs are a form of Neural Network in which the output from the previous step is used as input in the current step.**\n\n**Here we built a Sequential model that processes sequences of texts, embeds each texts into a 32-dimensional vector, then processes the sequence of vectors using 2 Bidirectional LSTM layers of 64 units and 16 units respectively because when working with text its important to take into account the context of the text.**","f9225249":"**To avoid too much overfitting, an early stopping callback was described. The model is made up of six layers that are stacked in a specific order. After 7 epochs, the model stopped early and has a 95 percent validation accuracy.**","9bbc0baa":"**Observations Political News and World News hold the most domination counts in the data set that we have considered.**","749366ed":"## Lets Look at the Count of Words Distribution in the Title","c402d939":"## Creating a deployable model","49486b4d":"**Building and training our convolutional neural network using keras' sequential api.**","312aebe1":"**Normalizing our data: changing it to lower case, getting rid of extra spaces, and url links.**","ce5833b0":"**Visualizing our training over time**","1531e828":"**Now we'll tokenize our data using Tensorflow's tokenizer**","6869473d":"**Checking for the null values in the data**","090cddfb":"## News\n\nThe term \"news\" refers to details about current events. This can be done in a variety of ways, including word of mouth, writing, postal services, broadcasting, electronic communication, and the testimony of incident observers and witnesses. War, government, politics, education, health, the environment, economy, industry, fashion, and entertainment, as well as sporting events and quirky or unusual events, are all common topics for news coverage. Technological and social advancements, also motivated by government communication and espionage networks, have accelerated the spread of news and influenced its content.\n\n## Fake News\n\nFake news is content that is inaccurate or misleading and is perceived as news. It is sometimes used to damage a person's or entity's image or to profit from advertising revenue. Fake news, which was once popular in print, has become more prevalent with the rise of social media, especially the Facebook News Feed.The dissemination of fake news has been linked to political divide, post-truth politics, confirmation bias, and social media algorithms. It is sometimes created and spread by hostile foreign actors, particularly during elections. The use of anonymously hosted fake news websites has made prosecuting sources of fake news for libel more difficult.\n\nBy contrasting with real news, fake news can lessen the influence of real news; a Buzzfeed study showed that top fake news reports about the 2016 US presidential election generated more engagement on Facebook than top stories from major media outlets. It also has the ability to erode public confidence in serious news coverage. Thus making the classification of fake news at an early stage a very important need of the hour.\n\nWe worked on fake-and-real-news-dataset as provided by Cl\u00e9ment Bisaillon to devise some Deep learning algorithms to make classification of Fake news easier.\n\nFollowing are the models we worked on:\nFinals Deep Learning models designed :\n\n**1.      Detection of the topics that are emerging most in the analysis of Fake news.**\n\n**2.      Fake news detection using only news titles with CNN+LSTM.**\n\n**3.      Fake news Classification using RNN(LSTM) on whole text.**\n\n**4.      Fake news classification using CNN.**\n\n\n\n","8a19ca16":"**Evaluation on test data gave loss: 0.0512 - accuracy: 0.9860**","9cdfbd8b":"**Convolutiona Neural network with**:\n* 3 Conv1D layers with 128, 256 and 512 filters with kernel size set to 4 meaning each output is calculated based on previous 4 time steps and relu activation.\n* 3 MaxPooling1D layers to downsample the input representation by taking the maximum value over the window of size 2.\n* 1 Flatten layer to flatten the output of the convolutional layers to create a single long feature vector.\n* 1 Dense layers with 1 neuron and sigmoid activation.\n\n**Gives 97% validation accuracy with 1 epoch.**","69771fc0":"**Observation The dataset looks really balanced and hence working on this is pretty easy. Thus we need not work on to make this dataset more balanced, and can safely assume this is a balanced dataset**","99520069":"## Model 1: Fake news detection using only news titles with CNN + LSTM"}}