{"cell_type":{"e3387b8c":"code","ea5292b6":"code","90ce265f":"code","d9c09b6a":"code","f8ba21df":"code","96b7a18f":"code","b1ccfc4b":"code","6ae50fd9":"code","71c75fd7":"code","8be8c2f9":"code","fdf89a6e":"code","a629174b":"code","cb3e7c88":"code","4afe6896":"code","18783d53":"code","8717a4f8":"code","fb652980":"code","0ab060fe":"code","94bbcebd":"code","9bdc8abb":"code","9f7208dd":"code","8dac2cf6":"code","61427fcb":"code","07cb32cd":"code","c3a6f18b":"markdown","f1519a23":"markdown","abf544b4":"markdown","7d70a737":"markdown","1b74c4df":"markdown","3f1c9bfb":"markdown","3edaa049":"markdown","36ea1e61":"markdown","4d91b57c":"markdown","4532d9a4":"markdown","339aa5e3":"markdown"},"source":{"e3387b8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea5292b6":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample","90ce265f":"DATA_FILEPATH = \"\/kaggle\/input\/creditcardfraud\/creditcard.csv\"\ndf_credit_card = pd.read_csv(DATA_FILEPATH)","d9c09b6a":"df_credit_card","f8ba21df":"df_credit_card.info()","96b7a18f":"df_credit_card.describe(include='all')","b1ccfc4b":"print(df_credit_card.isnull().sum())\nprint(np.isinf(df_credit_card).sum())","6ae50fd9":"print(df_credit_card['Class'].value_counts())\nprint(df_credit_card['Class'].value_counts()\/len(df_credit_card)*100)","71c75fd7":"df_credit_card.corr().replace(1, 0).max().astype(np.float16)","8be8c2f9":"df_credit_card.hist(bins = 50, figsize=(20, 20))\nplt.show()","fdf89a6e":"from fitter import Fitter, get_common_distributions\n\n\nfor col in df_credit_card.columns:\n    f = Fitter(df_credit_card[col].values, distributions=get_common_distributions())\n    f.fit()\n    # may take some time since by default, all distributions are tried\n    # but you call manually provide a smaller set of distributions\n    print(f.summary())\n    plt.show()","a629174b":"alpha = 0.05\nfor col in df_credit_card.columns:\n    stat, p = stats.normaltest(df_credit_card[col])\n    print(col, \"\\t\", 'Statistics=%.3f, p=%.10f' % (stat, p), end = \" \")\n    if p < alpha:\n        print('Sample looks Gaussian (reject H0)')\n    else:\n        print('Sample does not look Gaussian (fail to reject H0)')","cb3e7c88":"df_correct = df_credit_card[df_credit_card['Class'] == 0]\ndf_fraud = df_credit_card[df_credit_card['Class'] == 1]\n\nX_train_correct, X_test_correct, y_train_correct, y_test_correct = train_test_split(df_correct.drop(columns=['Class']), df_correct['Class'], test_size=0.33, random_state=42)\nX_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(df_fraud.drop(columns=['Class']), df_fraud['Class'], test_size=0.33, random_state=42)\n\nX_train = pd.concat([X_train_correct, X_train_fraud], ignore_index=True)\nX_test = pd.concat([X_test_correct, X_test_fraud], ignore_index=True)\ny_train = pd.concat([y_train_correct, y_train_fraud], ignore_index=True)\ny_test = pd.concat([y_test_correct, y_test_fraud], ignore_index=True)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nprint(y_train.value_counts())\nprint(y_test.value_counts())","4afe6896":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix, classification_report","18783d53":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)","8717a4f8":"print(\"Training set: \")\nroc_score = roc_auc_score(y_train, rfc.predict(X_train))\nprint('ROC score = {0:0.2f}'.format(roc_score))\n\nprint(\"Testing set: \")\nroc_score = roc_auc_score(y_test, rfc.predict(X_test))\nprint('ROC score = {0:0.2f}'.format(roc_score))","fb652980":"print(\"Training set: \")\nmcc = matthews_corrcoef(y_train, rfc.predict(X_train))\nprint('MCC score = {0:0.2f}'.format(mcc))\n\nprint(\"Testing set: \")\nmcc = matthews_corrcoef(y_test, rfc.predict(X_test))\nprint('MCC score = {0:0.2f}'.format(mcc))","0ab060fe":"print(confusion_matrix(y_train, rfc.predict(X_train)))\nprint(confusion_matrix(y_test, rfc.predict(X_test)))","94bbcebd":"print(classification_report(y_train, rfc.predict(X_train)))\nprint(classification_report(y_test, rfc.predict(X_test)))","9bdc8abb":"print(df_correct)\nprint(df_fraud)","9f7208dd":"from sklearn.utils import resample","8dac2cf6":"new_df_correct = df_correct.copy().reset_index(drop=True)\nnew_df_fraud = resample(df_fraud.copy(), replace=True, n_samples=len(new_df_correct)).reset_index(drop=True)\n\nprint(new_df_correct)\nprint(new_df_fraud)","61427fcb":"new_df = pd.concat([new_df_correct, new_df_fraud], ignore_index=True)\nnew_X = new_df.drop(columns = 'Class')\nnew_y = new_df['Class']","07cb32cd":"from sklearn.model_selection import cross_val_score\nclf = RandomForestClassifier()\nscores = cross_val_score(clf, new_X, new_y, cv=5)\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","c3a6f18b":"### Check for null and inf values","f1519a23":"The data is imbalanced.","abf544b4":"### Check the distributions of the data","7d70a737":"### Check for correct datatypes.","1b74c4df":"## Split the dataset","3f1c9bfb":"### Test for normal distribution","3edaa049":"### Visualize the distribution of data","36ea1e61":"### Check for imbalanced dataset","4d91b57c":"### Check for highly correlated inputs","4532d9a4":"## Load Data","339aa5e3":"## Analyze the dataset"}}