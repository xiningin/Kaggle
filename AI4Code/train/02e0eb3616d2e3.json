{"cell_type":{"71fff356":"code","a766a84e":"code","0cb13f59":"code","e582fbaa":"code","91d5d198":"code","718b14f7":"code","8e5990bb":"code","7fd42a9f":"code","076c32ab":"code","7c2e390d":"code","d2619a76":"code","da266c01":"code","02c6d289":"code","1ab25353":"code","06f5fc7a":"code","5e4d501e":"code","86bce7f4":"code","d1e09a1b":"code","69bdf782":"code","b2b711a7":"code","a7730b13":"code","de2647f2":"code","ccd7a45b":"code","f8278a96":"code","ec7fe14c":"code","8a3918fa":"code","8523c159":"code","ccee729b":"code","9b9969ab":"code","932991ee":"code","040f7209":"code","443229ad":"code","e336c53d":"code","86af4369":"code","22b460a0":"code","84aa2ecf":"code","c2e499eb":"code","cc7bc418":"code","427be63e":"code","b73f57a8":"code","6453f9b9":"code","90800582":"code","7afb0ca1":"code","6a2f3d8f":"code","c40a19be":"code","a574f2d2":"code","735de70a":"code","0318527f":"code","5478a39d":"code","2d7d578f":"code","43db0f54":"code","9c1dfcc7":"code","80cd78dc":"code","0c82fa24":"code","4437e3bf":"code","60a7163a":"code","f8919ae6":"code","6e53fd7b":"code","c7242c39":"code","7f1f8665":"code","e1cde3a4":"code","84300d27":"code","ec6f80a9":"code","5ad8cc91":"code","500ae73c":"code","8b074bf7":"code","0d56bbbc":"code","81752267":"markdown","bb366add":"markdown","62e52672":"markdown","677d1ea5":"markdown","305a9fd2":"markdown","85849a49":"markdown","8bba6ad4":"markdown","60207ab5":"markdown","8f787fe5":"markdown","b730199f":"markdown","534485e9":"markdown","9df50ccf":"markdown"},"source":{"71fff356":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm,skew\nfrom matplotlib.pyplot import figure\nfrom sklearn.linear_model import LinearRegression","a766a84e":"train_df=pd.read_csv('..\/input\/mercbenz\/train.csv')\ntrain_df.head()\ntest_df=pd.read_csv('..\/input\/mercbenz\/test.csv')\ntest_df.head()","0cb13f59":"train_df.drop(\"ID\",axis=1,inplace =True)\ntest_df.drop(\"ID\",axis=1, inplace=True)","e582fbaa":"train_df.describe()\ntest_df.describe()","91d5d198":"plt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","718b14f7":"train_df.loc[train_df['y'] > 250]","8e5990bb":"outliers = [883]\ntrain_df= train_df.drop(train_df.index[outliers])","7fd42a9f":"train_df.describe()","076c32ab":"# replot the graph inorder to find out if there are any abnormlay left\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()\n","7c2e390d":"train_df['y'].describe()\nsns.distplot(train_df['y']);\n\n#plot Skewness and Kurtosis\nprint(\"Skewness: %f\" % train_df['y'].skew())\nprint(\"Kurtosis: %f\" % train_df['y'].kurt())","d2619a76":"fig =plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train_df['y'], fit=norm);\n(mu,sigma)= norm.fit(train_df['y'])\nprint('\\n mu= {:.2f}\\n'.format(mu,sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=${:.2f})'.format(mu,sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('y distribution')\nplt.subplot(1,2,2)\nres=stats.probplot(train_df['y'],plot=plt)\nplt.suptitle('Before Transformation')\n\ntrain_df.y=np.log1p(train_df.y)\ny_train=train_df.y.values\ny_train_orig=train_df.y\n\nfig=plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train_df['y'], fit=norm);\n(mu,sigma)=norm.fit(train_df['y'])\nprint(' \\n mu={:.2f} and sigma = {:.2f}\\n'.format(mu,sigma))\nplt.legend(['Normal dist. ($\\mu=${:.2f} and $\\sigma=$ {:.2f})'.format(mu,sigma)], loc='best')\nplt.title('y Distribution')\nplt.subplot(1,2,2)\nres=stats.probplot(train_df['y'], plot=plt)\nplt.suptitle('After Transformation')\n\n#judging from the box cos tranformation we managed to bring down the spread slightly.","da266c01":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","02c6d289":"train_df.isnull().sum()\n# there is no null values\ntest_df.isnull().sum()","1ab25353":"unique_values_dict = {}\nfor col in train_df.columns:\n    if col not in [\"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_df[col].unique()).tolist())\n        tlist = unique_values_dict.get(unique_value, [])\n        tlist.append(col)\n        unique_values_dict[unique_value] = tlist[:]\nfor unique_val, columns in unique_values_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"--------------------------------------------------\")","06f5fc7a":"train_df.X1.describe()","5e4d501e":"train_df = train_df.drop(columns=['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347'])\ntest_df = test_df.drop(columns=['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347'])","86bce7f4":"train_df.head()","d1e09a1b":"test_df.head()","69bdf782":"ranks = train_df.groupby(\"X0\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X0\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","b2b711a7":"ranks = train_df.groupby(\"X1\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X1\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","a7730b13":"ranks = train_df.groupby(\"X2\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X2\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","de2647f2":"ranks = train_df.groupby(\"X3\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X3\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","ccd7a45b":"ranks = train_df.groupby(\"X4\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X4\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","f8278a96":"ranks = train_df.groupby(\"X5\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X5\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","ec7fe14c":"ranks = train_df.groupby(\"X6\")[\"y\"].median().sort_values(ascending=False)[::-1].index\n\nvar_name = \"X6\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","8a3918fa":"ranks = train_df.groupby(\"X8\")[\"y\"].median().sort_values(ascending=False)[::-1].index\nvar_name = \"X8\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order=ranks)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","8523c159":"var_name = \"X8\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nranks_1 = train_df.groupby(\"X0\")[\"y\"].median().sort_values(ascending=False)[::-1].index\nranks_2 = train_df.groupby(\"X2\")[\"y\"].median().sort_values(ascending=False)[::-1].index\nplt.yticks(range(len(ranks_2)), ranks_2) # rearranging according to median y value\nplt.xticks(range(len(ranks_1)),ranks_1)\nsns.scatterplot(x='X0', y='X2', data=train_df, sizes=(40,400))\nplt.xlabel('x0', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title(\"Distribution of  variable with \"+var_name, fontsize=15)\nplt.show()\n## insert weights for the nodes to tell how X0 and X2 look with overall build up time","ccee729b":"column_list=['X10', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X189', 'X190', 'X191', 'X192', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X234', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X291', 'X292', 'X294', 'X295', 'X296', 'X298', 'X299', 'X300', 'X301', 'X302', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X382', 'X383', 'X384', 'X385']\nlabel_list=[\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\nplot_df=[]\nplot_df=train_df[column_list].sum(axis=1)\ny=train_df.y\nplot_df=pd.DataFrame(plot_df)\nplot_df.insert(1,\"y\",train_df.y)\nplot_df.columns=['Sumofflags','y'] #rename axis\ntotal_bins=10\nbins = np.linspace(plot_df.Sumofflags.min(),plot_df.Sumofflags.max(),total_bins)\ndelta=bins[1]-bins[0]\nidx=np.digitize(plot_df.Sumofflags,bins)\nrunning_median=[np.median(plot_df.y[idx==k]) for k in range(total_bins)]\nsns.scatterplot(x='Sumofflags', y='y',data=plot_df)\nplt.plot(bins-delta\/2,running_median,'r--',lw=2.5,alpha=0.8)\nplt.axis('tight')\nplt.show()\nprint(plot_df.Sumofflags.describe())","9b9969ab":"train_df[column_list]","932991ee":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nbin_df=train_df[column_list]\nbin_df_2=test_df[column_list]\nbin_df_std=StandardScaler().fit_transform(bin_df)\nbin_df_std2=StandardScaler().fit_transform(bin_df_2)\ncov_mat=np.cov(bin_df_std.T)\neigen_vals, eigen_vecs=np.linalg.eig(cov_mat)","040f7209":"import matplotlib.pyplot as plt\n\ntot= sum(eigen_vals)\nvar_exp=[(i\/tot) for i in sorted(eigen_vals,reverse=True)]\ncum_var_exp = np.cumsum(var_exp)","443229ad":"plt.bar(range(0,356), var_exp, alpha=0.5,\n        align='center', label='individual explained variance')\nplt.step(range(0,356), cum_var_exp, where='mid',\n         label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.show()","e336c53d":"pca = PCA(n_components=155)\nprincipalComponents = pca.fit_transform(bin_df_std)\nprincipalDf= pd.DataFrame(data = principalComponents)\nprincipalComponents_test = pca.fit_transform(bin_df_std2)\nprincipalDf_2= pd.DataFrame(data=principalComponents_test)","86af4369":"principalDf_2.head()\ntrain_df.head()","22b460a0":"feature_list= train_df.columns[1:]\n\nprint(\"{} duplicate entries in training, out of {}, a {:.2f} %\".format(\n    len(train_df[train_df.duplicated(subset=feature_list, keep=False)]),\n    len(train_df),\n    100 * len(train_df[train_df.duplicated(subset=feature_list, keep=False)]) \/ len(train_df)\n    ))\ndup_df=train_df[train_df.duplicated(subset=feature_list, keep=False)].sort_values(by='X0')\ndup_df","84aa2ecf":"duplicate_std = train_df[train_df.duplicated(subset=feature_list,\n                             keep=False)].groupby(list(feature_list.values))['y'].aggregate(['std', 'size']).reset_index(drop=False)\n\ndup_df=duplicate_std.sort_values(by='std', ascending=False)\ndup_df\n\ntbd=dup_df.iloc[0:10]\ntbd=tbd.drop(['std','size'], axis = 1)\ntbd","c2e499eb":"plt.scatter(x=\"size\",y=\"std\", data=dup_df)\nplt.ylabel('std deviation')\nplt.xlabel('count')","cc7bc418":"def dataframe_difference(df1, df2, which='both'):\n    \"\"\"Find rows which are similar between two DataFrames.\"\"\"\n    comparison_df = df1.merge(df2,\n                              indicator=True,\n                              how='outer')\n    if which is None:\n        diff_df = comparison_df[comparison_df['_merge'] != 'both']\n    else:\n        diff_df = comparison_df[comparison_df['_merge'] == which]\n    return diff_df","427be63e":"diff_df=dataframe_difference(train_df,tbd)\ndiff_df","b73f57a8":"dup = [635,636,693,694,1168,1169,1170,1617,1618,1619,2019,2020,2351,2352,2574,2575,3012,3013,3077,3078,3923,3924]\ntrain_df = train_df.drop(columns=['X4'])\ntrain_df=train_df.drop(columns=column_list)","6453f9b9":"test_df= test_df.drop(columns=['X4'])\ntest_df= test_df.drop(columns=column_list)","90800582":"test_df.head()","7afb0ca1":"train_df.head()","6a2f3d8f":"ID = [i for i in range(4208)]\nID2= [i for i in range(4209)]\nprincipalDf.insert(0, 'ID', ID)\ntrain_df.insert(0,'ID',ID)","c40a19be":"principalDf_2.insert(0,'ID2',ID2,True)\ntest_df.insert(0,'ID2',ID2,True)","a574f2d2":"train_df_final=pd.merge(train_df, principalDf, how='outer', on=['ID'])\ntrain_df_final\n","735de70a":"test_df_final=pd.merge(test_df, principalDf_2, how='outer', on=['ID2'])\ntest_df_final","0318527f":"from sklearn.model_selection import train_test_split\ny=train_df_final.y.values","5478a39d":"train_df_final=train_df_final.drop(columns='y')\nfinal_features=pd.get_dummies(train_df_final)\nprint(final_features.shape)\nX= final_features.iloc[:len(y),:]\nX_test=pd.get_dummies(test_df_final)\nprint(X.shape, X_test.shape,y.shape)","2d7d578f":"overfit = []\n\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros\/len(X) * 100 >99.95:\n        overfit.append(i)\n\n\nX=X.drop(overfit,axis=1).copy()\nprint(X.shape)","43db0f54":"overfit = []\n\nfor i in X_test.columns:\n    counts = X_test[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros\/len(X) * 100 >99.95:\n        overfit.append(i)\n\nX_test=X_test.drop(overfit,axis=1).copy()\nprint(X_test.shape)","9c1dfcc7":"outlier=set(X_test.columns).symmetric_difference(set(X.columns))\nX= X.drop(columns=['X2_an', 'X2_ah', 'X1_q', 'X2_av', 'ID', 'X1_d', 'X2_y', 'X2_at', 'X2_x'])\n","80cd78dc":"X_test=X_test.drop(columns=['ID2', 'X2_ab', 'X2_ad', 'X2_af', 'X5_g'])\n","0c82fa24":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost.sklearn import XGBRegressor\nfrom lightgbm import LGBMRegressor","4437e3bf":"k_folds= KFold(n_splits=18, shuffle=True, random_state=42)\n\n#model scoring and validation function\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=k_folds))\n    return (rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","60a7163a":"## lightGBM\n\nlightgbm=LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=9000, max_bin=200, bagging_fraction=0.75, bagging_freq=5,bagging_seed=7,feature_fraction=0.2, feature_fraction_seed=7, verbose=-1)\n","f8919ae6":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt,cv=k_folds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7,alphas=alphas2, random_state=42, cv=k_folds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                                                        alphas=e_alphas, cv=k_folds,l1_ratio=e_l1ratio))\nstack_gen = StackingCVRegressor(regressors=(ridge,lasso,elasticnet,lightgbm), meta_regressor=elasticnet,use_features_in_secondary=True)\n\nsvr= make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))\n","6e53fd7b":"# store models, scores and prediction values \nmodels = {'Ridge': ridge,\n          'Lasso': lasso, \n          'ElasticNet': elasticnet,\n          'lightgbm': lightgbm,\n          'Svd': svr}\npredictions = {}\nscores = {}","c7242c39":"for name, model in models.items():\n    \n    model.fit(X,y)\n    predictions[name]= np.expm1(model.predict(X))\n    \n    score= cv_rmse(model,X=X)\n    scores[name] = (score.mean(), score.std())","7f1f8665":"\nprint('---- Score with CV_RMSLE-----')\nscore = cv_rmse(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","e1cde3a4":"print('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso')\nlasso_model = lasso.fit(X, y)\nprint('Ridge')\nridge_model = ridge.fit(X, y)\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\nprint('Stack_gen_model')\nstack_gen_model=stack_gen.fit(np.array(X), np.array(y))","84300d27":"def blend_models_predict(X):\n    return ((0.16  * elastic_model.predict(X)) + \\\n            (0.16 * lasso_model.predict(X)) + \\\n            (0.11 * ridge_model.predict(X)) + \\\n            (0.2 * lgb_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.27 * stack_gen_model.predict(np.array(X))))","ec6f80a9":"print('RMSLE score on the train data: ')\nprint(rmsle(y, blend_models_predict(X)))","5ad8cc91":"print('Predict Submission')\nsubmission = pd.read_csv('..\/input\/mercbenz\/train.csv')\nsubmission.iloc[:,1] = (np.expm1(blend_models_predict(X_test)))","500ae73c":"X.head()","8b074bf7":"X_test.head()","0d56bbbc":"submission.to_csv(\"submission.csv\", index=False)","81752267":"# Principal component analysis","bb366add":"1. Sum of flags is from 94 to 31.\n2. There is no noticeable trend, increasing 1's in binary features does not contribute to longer manufacturing time\n3. Mean = 58.02","62e52672":"since it is impossible for us to determine each features, we can sum up all the binary features to get a good feel of how the data changes with total binary features","677d1ea5":"# Duplicates","305a9fd2":"for columns containing only 0 it can be removed.","85849a49":"We can see that most of our dataset seems to be in line with others, except for one abnormaly, which we will remove.","8bba6ad4":"# Exploratory Data Analysis (EDA)\nin this portion we will look closely at eahc of the features to determine if we can derive more insight into our data. Since the data sets are masked, we are only about to look at it quantitatively.","60207ab5":"from what we are seeing, most of the duplicates have very low std deviation from each other. the data points that are too different despite being having the same parameters should be deleted","8f787fe5":"Since the majority of X4 only provides a small variance of y, we can look at removing it during the modelling phase","b730199f":"PCA is used for feature extraction when you have alot of variable to consider, and want to know which few features contirbutes to most of the variance","534485e9":"PCA shows that it will need a little bit more than a few features to explain majority of the variance of the figures. \n\nLooking through other notebooks, there has been duplicated features combination that doesn not give the same output(manufacturing time) so lets look a those","9df50ccf":"# Modelling"}}