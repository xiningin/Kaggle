{"cell_type":{"6919f403":"code","85363132":"code","420a7f3e":"code","e15f608b":"code","b93a7ba7":"code","7b5f3dad":"code","5b979497":"code","3fc3f88f":"code","f2a47b4e":"code","bdecd06b":"code","69564d44":"code","4b2a2bfe":"code","d0c18e9e":"code","7b9ce896":"code","c21c4ad0":"code","1aa7598b":"code","cc18c914":"code","ac84b54b":"code","c8bb5927":"code","571839dd":"code","d9a478f2":"code","068a3c7c":"code","437388b5":"code","43bc61d0":"code","98e9c5d8":"code","4df58b97":"code","955e5314":"code","0e791ad1":"code","d1ff8e0c":"code","aca3adaa":"code","84aa2cbd":"code","14c0c79f":"code","604c241c":"code","c84f3f3f":"code","88cf9f04":"code","606b9682":"code","d405f663":"code","9580179c":"code","11fcded5":"code","8b007ae9":"code","33d411ba":"code","2ec27a79":"code","9dc57d37":"markdown","b58e2a32":"markdown","82da123f":"markdown","01eeb3cc":"markdown","7ef8a178":"markdown","19ff9b12":"markdown","48dcb590":"markdown","77c55b31":"markdown","1de50248":"markdown","44608941":"markdown","aed714c4":"markdown"},"source":{"6919f403":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85363132":"df=pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf.head()","420a7f3e":"df.describe()","e15f608b":"df.info()","b93a7ba7":"#","7b5f3dad":"a=df.isnull().sum()\nb=(df.isnull().sum()\/len(df))*100\npd.concat([a,b],axis=1)","5b979497":"df.hist(figsize=(20,15),grid=False)","3fc3f88f":"df.info()\nnum_col=[]\ncat_col=[]\nfor col in df.select_dtypes(exclude='object'):\n    num_col.append(col)\nfor col in df.select_dtypes(include='object'):\n    cat_col.append(col)","f2a47b4e":"#clean cat_col\nfor col in cat_col[2:]:\n    print(f'{col} ----->{df[col].isnull().sum()}------->{df[col].nunique()}')\n    print(df[col].unique())\n#convert date columns\n#Convert to 0;s and 1's\n","bdecd06b":"df['Date']=pd.to_datetime(df['Date'])\ndf['month']=df['Date'].dt.month\ndf['day']=df['Date'].dt.day\ndf['year']=df['Date'].dt.year\n\n\nrain={'No':0,'Yes':1}\ndf['RainToday']=df['RainToday'].map(rain)\ndf['RainTomorrow']=df['RainTomorrow'].map(rain)\n","69564d44":"wind=['WindGustDir','WindDir9am','WindDir3pm']\nfig, ax = plt.subplots(1,3, figsize = (20,5), constrained_layout = True)\n\n\nfor i, col in enumerate(wind): \n    sns.countplot(df[col], ax = ax[i],hue=df['RainTomorrow'])\n    ax[i].set_ylabel(col)\n    ax[i].set_xlabel(None)\n    \n    ","4b2a2bfe":"sns.countplot(df['RainToday'],hue=df['RainTomorrow'])","d0c18e9e":"\n#lets fill the categorical data\nfor col in cat_col[1:]:\n    df[col].fillna(df[col].mode()[0],axis=0,inplace=True)\nfor col in num_col:\n    df[col].fillna(df[col].median(),axis=0,inplace=True)    ","7b9ce896":"plt.figure(figsize=(25,10))\nsns.heatmap(df.corr(),annot=True,cmap=\"coolwarm\",)\n\n\n#MInTemp,MaxTemp\n#Pressure9\n#Temp9","c21c4ad0":"from warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\nfig = plt.figure(figsize=(14,10))\nc=1\nfor i in num_col:\n    plt.subplot(4, 4, c)\n   # plt.title('{}, subplot: {}{}{}'.format(i, a, b, c))\n    #plt.xlabel(i)\n    sns.boxplot(df[i],\n    linewidth=1)\n    c = c + 1\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nplt.show()\n","1aa7598b":"#outliers removal\n\ndf_train=df.copy()\nfor col in ['MinTemp','MaxTemp','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm']:\n    \n    # Lower and upper threshold\n    q1 = df[col].quantile(.25)\n    q3 = df[col].quantile(.75)\n    print(f'{col}----->{q1}----->{q3}')\n    iqr=q3-q1\n    lower_threshold=q1-1.5*iqr\n    upper_threshold=q3+1.5*iqr\n    print(f'{col}----->{lower_threshold}----->{upper_threshold}')\n    df_train=df_train[(df_train[col]>=lower_threshold) & (df_train[col]<=upper_threshold)]\n    print(df_train.shape)\n    ","cc18c914":"df_train.drop('Date',axis=1,inplace=True)","ac84b54b":"df_train[cat_col[1:]]\nimport scipy.stats as stats\ndef chi2(data,col):\n    contigency_data = pd.crosstab(data[col],data['RainTomorrow'])\n\n    stat,pvalue,dof,exp =stats.chi2_contingency(contigency_data)\n\n    print('stat=%.3f, p=%.3f' % (stat, pvalue))\n    if pvalue > 0.05:\n        print(f'No effect of {col} on target')\n    else:\n        print(f'There is a effect of {col} on target')\n        \nfor col in cat_col[1:]:\n       chi2(df_train,col)     ","c8bb5927":"dummies=pd.get_dummies(df[['WindGustDir','WindDir9am','WindDir3pm']],drop_first=True)\ndf_train=df_train.join(dummies)\ndf_train.drop(['Location','WindGustDir','WindDir9am','WindDir3pm'],axis=1,inplace=True)","571839dd":"from keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nfrom keras import callbacks\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler","d9a478f2":"X = df_train.drop([\"RainTomorrow\"], axis=1)\ny = df_train[\"RainTomorrow\"]\n\n# Splitting test and training sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX.shape\nscaler = StandardScaler()\n_ = scaler.fit(X_train)\nX_trn = scaler.transform(X_train)\nX_val = scaler.transform(X_valid)\n","068a3c7c":"early_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=20, \n    restore_best_weights=True,\n)\n\n\n\nmodel=Sequential()\nmodel.add(Dense(units=32,kernel_initializer='he_uniform',activation = 'relu', input_dim = 65))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=16,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=8,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=4,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1,kernel_initializer='he_uniform',activation = 'sigmoid'))\n\nmodel.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(X_trn, y_train, epochs=100,validation_split=0.3,callbacks=[early_stopping],batch_size=30)","437388b5":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")","43bc61d0":"y_pred = model.predict(X_val)\ny_pred = (y_pred > 0.5)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_valid, y_pred)\nsns.heatmap(cf_matrix, annot = True, annot_kws = {'size':20})\n","98e9c5d8":"print(classification_report(y_valid, y_pred))\n\n#we can see that  recall rate is poor.\nprint(df_train['RainTomorrow'].value_counts())#80:20","4df58b97":"X.columns","955e5314":"from sklearn.feature_selection import f_classif,SelectKBest,chi2,mutual_info_classif \n\nfs = SelectKBest(score_func=mutual_info_classif, k='all')\n# learn relationship from training data\nfs.fit(df_train[num_col],df_train['RainTomorrow'])\n# transform train input data\nX_train_fs = fs.transform(df_train[num_col])\n# transform test input data\n\n","0e791ad1":"fs.scores_\n","d1ff8e0c":"for i in range(len(fs.scores_)):\n\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n","aca3adaa":"feat_imp=pd.DataFrame(index=df_train[num_col].columns,data=fs.scores_)\nfeat_imp.sort_values(0,ascending=False).plot(kind='barh')","84aa2cbd":"feat_imp[feat_imp>0.015].index","14c0c79f":"X_new=df_train[['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm',\n       'Temp9am', 'Temp3pm']]\nX_new","604c241c":"dummies=pd.get_dummies(df[['WindGustDir','WindDir9am','WindDir3pm']],drop_first=True)\nX_new=X_new.join(dummies)\nX_new","c84f3f3f":"y_new=df_train['RainTomorrow']\n# Splitting test and training sets\nX_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new, test_size = 0.3, random_state = 42)\n\n\nscaler = MinMaxScaler()\n_ = scaler.fit(X_train)\nX_trn = scaler.transform(X_train)\nX_val = scaler.transform(X_valid)\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=20, \n    restore_best_weights=True,\n)\n\n\n\nmodel=Sequential()\nmodel.add(Dense(units=32,kernel_initializer='he_uniform',activation = 'relu', input_dim = 61))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=16,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=8,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=4,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1,kernel_initializer='he_uniform',activation = 'sigmoid'))\n\nmodel.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(X_trn, y_train, epochs=100,validation_split=0.3,callbacks=[early_stopping],batch_size=30)\n\nhistory_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")","88cf9f04":"y_pred = model.predict(X_val)\ny_pred = (y_pred > 0.5)\n#plt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_valid, y_pred)\ncf_matrix","606b9682":"'''\nmapping=df.groupby(['Location'])['RainTomorrow'].sum().to_dict()\ndf_train['Location']=df['Location'].copy()\ndf_train['Loc_train']=df_train['Location'].map(mapping)\nX_new['Loc_freq']=df_train['Loc_train']\n'''","d405f663":"X=df_train.drop(df_train[['Location','RainTomorrow']],axis=1)\ny=df_train['RainTomorrow']","9580179c":"from imblearn.under_sampling import NearMiss\nfrom collections import Counter\nundersample=NearMiss(0.7)\nX_nm,y_nm=undersample.fit_resample(X,y)\nprint(f'before sampling y_shape {Counter(y_new)}')\nprint(f'after sampling y_shape {Counter(y_nm)}')","11fcded5":"#X_nm.drop(X_nm['Loc_freq'],axis=0,inplace=True)\nX_nm=X_nm.iloc[:,:-1]\n","8b007ae9":"# Splitting test and training sets\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_nm, y_nm, test_size = 0.3, random_state = 42)\n\n\nscaler = MinMaxScaler()\n_ = scaler.fit(X_train)\nX_trn = scaler.transform(X_train)\nX_val = scaler.transform(X_valid)\n\n\n\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=10, \n    restore_best_weights=True,\n)\n\n\n\nmodel=Sequential()\nmodel.add(Dense(units=64,kernel_initializer='he_uniform',activation = 'relu', input_dim = 65))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=32,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=16,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=4,kernel_initializer='he_uniform',activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=1,kernel_initializer='he_uniform',activation = 'sigmoid'))\n\nmodel.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(X_trn, y_train, epochs=50,validation_split=0.3,callbacks=[early_stopping],batch_size=30)\n\n\nhistory_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")","33d411ba":"y_pred = model.predict(X_val)\ny_pred = (y_pred > 0.5)\n#plt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_valid, y_pred)\ncf_matrix\nprint(classification_report(y_valid, y_pred))\n\n#we can see that  recall rate is poor.","2ec27a79":"#Eventhough accuracy is less comapred to actual data set,but recall has imporved significantly.\nThankyou","9dc57d37":"# Preprocessing Cat COls","b58e2a32":"# getting dummies","82da123f":"# ****Checking for Features","01eeb3cc":"# Grouping Num and Cat Cols","7ef8a178":"# Outliers Removal","19ff9b12":"# Chi2 to check for dependence","48dcb590":"# NULL VALUES","77c55b31":"# Null Values Impute","1de50248":"# since it is imbalance class using undersampling","44608941":"# Data Splitting","aed714c4":"# Upvote it if you find helpful."}}