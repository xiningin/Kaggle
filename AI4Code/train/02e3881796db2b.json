{"cell_type":{"3de5360c":"code","589e10bb":"code","0045e67d":"code","560c10f5":"code","4cd5a8dd":"code","805e0255":"code","70db765d":"code","60910d87":"code","28d40b09":"code","f84481ab":"code","51c86490":"code","ea5143f2":"code","e03fe082":"code","978fc68f":"code","8559b2ca":"code","4f2226f9":"code","aabaec85":"code","e194eed9":"code","4fea10bf":"code","44306b60":"code","376b35bf":"code","425a4fa1":"code","582530aa":"code","2fea7eae":"code","35dfacee":"code","d0a3d534":"code","a7688061":"markdown","9fec0b13":"markdown","49693d8e":"markdown","b3ff9e49":"markdown","680f304d":"markdown","4dd7e55b":"markdown","63d741a7":"markdown","1b18b7ff":"markdown","fb1c8709":"markdown","f4f28335":"markdown"},"source":{"3de5360c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot as plt\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom imblearn.pipeline import Pipeline\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split","589e10bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0045e67d":"stroke_data = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","560c10f5":"stroke_data.describe()","4cd5a8dd":"stroke_data.set_index('id', inplace = True)","805e0255":"#bi-variate analysis\nplt.style.use('seaborn-dark')\nsns.pairplot(stroke_data,hue='stroke',palette='Dark2');\nplt.tight_layout()","70db765d":"Stroke_plot = stroke_data['stroke'].value_counts().reset_index()\nStroke_plot.columns = ['stroke','count']\n\npx.pie(Stroke_plot,values='count',names='stroke',template='plotly',title='Stroke')","60910d87":"plt.figure(figsize=(10,6))\nsns.set_context(context='notebook',font_scale=1.2)\nsns.heatmap(stroke_data[['age','avg_glucose_level','bmi']].corr(method='pearson'),cmap='Blues',annot=True);\nplt.tight_layout()","28d40b09":"#Checking the null values\nstroke_data[stroke_data.isnull().any(axis=1)]","f84481ab":"#dropping null values\nstroke_data.dropna(inplace= True)","51c86490":"#Checking minimum values of age\nstroke_data[stroke_data.age < 1]\n#All ages less than 1 belong to children, which seems correct.","ea5143f2":"#separating the target column\ny = stroke_data.iloc[:,-1]\nx = stroke_data.iloc[:,:-1]","e03fe082":"#applying one hot encoding to convert categorical data into numerical data\ncat_data = x.select_dtypes(include=['object']).copy()\ncolumns = cat_data.columns\nx = pd.get_dummies(x, columns=columns)","978fc68f":"#Balancing the classes of the data by first applying oversampling and then undersampling method \nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\n\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\nx_ros, y_ros = pipeline.fit_resample(x, y)\n#The resultant classes are now in 2:1 ratio, from earlier 95:5 ratio","8559b2ca":"y_ros.value_counts()","4f2226f9":"#splitting the dataset \nx_ros_train, x_ros_test, y_ros_train, y_ros_test = train_test_split(x_ros, y_ros, test_size = 0.6, random_state = 42)","aabaec85":"#Normalizing the data\nscaler = StandardScaler()\nx_ros_train_scaled = scaler.fit_transform(x_ros_train) #dataset 2\nx_ros_test_scaled = scaler.fit_transform(x_ros_test)   #dataset 2","e194eed9":"from sklearn import metrics\nLR = LogisticRegression().fit(x_ros_train_scaled, y_ros_train)\n\npredict_train_LR = LR.predict(x_ros_train_scaled)\npredict_test_LR = LR.predict(x_ros_test_scaled)\n\n# accuracy score\nLR_train_score = LR.score(x_ros_train_scaled,y_ros_train)\nLR_test_score = LR.score(x_ros_test_scaled,y_ros_test)\n\n# f1-score\nLR_f1_score = metrics.f1_score(y_ros_test, predict_test_LR)\nLR_recall = metrics.recall_score(y_ros_test, predict_test_LR)\n\n\nprint('Accuracy on Train set',LR_train_score)\nprint('Accuracy on Test set',LR_test_score)\nprint('F1-score on Test set:',LR_f1_score)\nprint('\\n')\nprint(metrics.classification_report(y_ros_test, predict_test_LR))","4fea10bf":"SVM = svm.SVC().fit(x_ros_train_scaled, y_ros_train)\n\npredict_train_SVM = SVM.predict(x_ros_train_scaled)\npredict_test_SVM = SVM.predict(x_ros_test_scaled)\n\n# accuracy score\nSVM_train_score = SVM.score(x_ros_train_scaled,y_ros_train)\nSVM_test_score = SVM.score(x_ros_test_scaled,y_ros_test)\n\n# f1-score\nSVM_f1_score = metrics.f1_score(y_ros_test, predict_test_SVM)\nSVM_recall = metrics.recall_score(y_ros_test, predict_test_SVM)\n\n\nprint('Accuracy on Train set',SVM_train_score)\nprint('Accuracy on Test set',SVM_test_score)\nprint('F1-score on Test set:',SVM_f1_score)\nprint('\\n')\nprint(metrics.classification_report(y_ros_test, predict_test_SVM))","44306b60":"RF = RandomForestClassifier().fit(x_ros_train_scaled, y_ros_train)\n\npredict_train_RF = RF.predict(x_ros_train_scaled)\npredict_test_RF = RF.predict(x_ros_test_scaled)\n\n# accuracy score\nRF_train_score = RF.score(x_ros_train_scaled,y_ros_train)\nRF_test_score = RF.score(x_ros_test_scaled,y_ros_test)\n\n# f1-score\nRF_f1_score = metrics.f1_score(y_ros_test, predict_test_RF)\nRF_recall = metrics.recall_score(y_ros_test, predict_test_RF)\n\n\nprint(RF.get_params())\nprint('Accuracy on Train set',RF_train_score)\nprint('Accuracy on Test set',RF_test_score)\nprint('F1-score on Test set:',RF_f1_score)\nprint('\\n')\nprint(metrics.classification_report(y_ros_test, predict_test_RF))","376b35bf":"from sklearn.model_selection import RandomizedSearchCV\n\n# to get best parameters\n\n# fine Tune the model using RandomizedSearchCV\n\n#\"\"\"\nparameters= {'n_estimators':[8, 32, 64, 100, 200],\n            'max_depth':[10, 12],\n            'max_features':[5, 8, 10],\n            'min_samples_split' : [2,4],\n            'min_samples_leaf' : [1,2]}\n\n\nrf = RandomForestClassifier()\n\nrf_model_tune = RandomizedSearchCV(rf, param_distributions = parameters, cv=3,n_iter = 20, verbose=2, random_state=42)\n\nrf_model_tune.fit(x_ros_train,y_ros_train)","425a4fa1":"rf_model_tune.best_params_","582530aa":"\nRF_model = RandomForestClassifier(n_estimators= 200,min_samples_split= 2,min_samples_leaf=1,max_features= 5,max_depth=10,bootstrap= False)\n\n# fit the model\nRF_model.fit(x_ros_train,y_ros_train)\n\n# model score\npredict_train_RF = RF_model.predict(x_ros_train)\npredict_test_RF = RF_model.predict(x_ros_test)\n\n# accuracy score\nRF_train_score = RF_model.score(x_ros_train,y_ros_train)\nRF_test_score = RF_model.score(x_ros_test,y_ros_test)\n\n# f1-score\nRF_f1_score = metrics.f1_score(y_ros_test,predict_test_RF)\nRF_recall = metrics.recall_score(y_ros_test,predict_test_RF)\nprint('Accuracy on Train set',RF_train_score)\nprint('Accuracy on Test set',RF_test_score)\nprint('F1-score on Test set:',RF_f1_score)\nprint(metrics.classification_report(y_ros_test,predict_test_RF))\n","2fea7eae":"RF = RandomForestRegressor()\nRF.fit(x_ros_train_scaled, y_ros_train)\nimportance = RF.feature_importances_\nplt.barh(x_ros.columns, importance)","35dfacee":"model_compare = pd.DataFrame({\n\n\n'Models':['LogisticRegression','Support Vector Machine','RandomForestClassifier'],\n'f1_score':[LR_f1_score, SVM_f1_score, RF_f1_score],\n'recall':[LR_recall, SVM_recall, RF_recall],\n'Accuracy on train set':[LR_train_score,SVM_train_score,RF_train_score],\n'Accuracy on test set':[LR_test_score, SVM_test_score,RF_test_score]\n\n})\n\nmodel_compare = model_compare.sort_values('recall',ascending=False)\n","d0a3d534":"model_compare.style.background_gradient(cmap='Greens')","a7688061":"# Fiting the models and comparing the results","9fec0b13":"## Correlation","49693d8e":"# Exploratory Analysis","b3ff9e49":"# Models comparison","680f304d":"# Data split and normalization","4dd7e55b":"Evidence of interrelationship among age, avg_glucose level and bmi. At a given bmi, higher the age, more cases of strokes. At higher age, there are more cases of strokes in people with higher avg glucose level. ","63d741a7":"We will be training 3 models on the data set obtained after balancing the classes","1b18b7ff":"Minimum age as 0.08. Should be further checked during preprocessing. ","fb1c8709":"# Preprocessing","f4f28335":"# Model tuning"}}