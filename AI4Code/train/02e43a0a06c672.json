{"cell_type":{"76945fe4":"code","54b2c0dd":"code","2a4b88e1":"code","c967de08":"code","902f81a3":"code","9a1e0109":"code","4c6fd4d9":"code","acb69fdf":"code","1485caca":"code","f2969304":"code","2f0d9884":"code","680c6e4a":"code","01aac782":"code","9c25af6d":"code","a7e634ae":"code","6b2d93c2":"code","06d59853":"code","fe8ef646":"code","cc053e21":"code","ced506c7":"code","24acd932":"code","0be431a5":"code","6a3635b0":"code","bde6d9fb":"code","9b6983fe":"code","6984f7fd":"code","1bb38b8a":"code","a8519a13":"code","d76097e1":"code","df581c5f":"code","587009c7":"code","58d8ece2":"code","94c633b0":"code","8573dc52":"code","304599f2":"markdown","03918564":"markdown","08e297b9":"markdown","2f827fef":"markdown","921b159a":"markdown","6bf736c1":"markdown","ccbc3c4b":"markdown","d4207fb7":"markdown","4fc226e6":"markdown","a9ac56c9":"markdown","b0e4e5ef":"markdown","32172633":"markdown","e49121ea":"markdown","d92f48de":"markdown","d50ee0b6":"markdown","eae3b684":"markdown","43bd40fe":"markdown","ebe8415f":"markdown","dea79d09":"markdown","acbc2203":"markdown","9a4ff59c":"markdown","bf95ca63":"markdown","c1aca64d":"markdown","2d5cb6bc":"markdown","732d8599":"markdown","37b358af":"markdown","75daf232":"markdown","a9929a24":"markdown","8f467006":"markdown","577c8aa4":"markdown","71310454":"markdown","46a5d57f":"markdown"},"source":{"76945fe4":"import numpy as np\n\nimport matplotlib\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Establecemos la aleatoridad de los numeros mediante un \"seed\".\nnp.random.seed(0)","54b2c0dd":"# Creamos un par de metodos auxiliares que nos permitir\u00e1n crear los datos las redes neuronales.\n\ndef create_data(func, sample_size, std, domain=[0, 1]):\n    x = np.linspace(domain[0], domain[1], sample_size)\n    np.random.shuffle(x)\n    t = func(x) + np.random.normal(scale=std, size=x.shape)\n    return x, t\n\ndef sinusoidal(x):\n    return np.sin(2 * np.pi * x)\n\ndef create_sinusoidal_data():\n    x_train, y_train = create_data(sinusoidal, 10, 0.2)\n    x_test = np.linspace(0., 1.0, 50)\n    y_test = sinusoidal(x_test)\n    return x_train, y_train, x_test, y_test\n\ndef polynomial_features(x, M):\n    if np.isscalar(x):\n        return x**np.arange(1, M+1)\n    elif (isinstance(x, (list, tuple)) or\n          (isinstance(x, (np.ndarray,)) and x.ndim == 1)):\n        return np.vstack([x_**np.arange(1, M+1) for x_ in x])\n    else:\n        raise TypeError(\"wrong type\")","2a4b88e1":"x_train, y_train, x_test, y_test = create_sinusoidal_data()\n\nplt.figure(figsize=(20,5))\nplt.suptitle('Mini dataset de Prueba', fontsize = 20)\nplt.plot(x_test, y_test)\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.grid()\nplt.show()","c967de08":"# Importamos las librer\u00edas de Torch, para ser utilizadas.\nimport torch\n\n# Numero de par\u00e1metros\nP = 3\n\n# Si es posible, se enviar\u00e1 el c\u00f3digo a alguna GPU, y de esta forma acelerar el proceso de ejecuci\u00f3n.\n# De no estar disponible, se utilizar\u00e1 el CPU.\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Generamos los datos de train y test para la ejecuci\u00f3n de los ejercicios.\nx_train_vec = polynomial_features(x_train, P)\ny_train_vec = y_train.reshape(-1, 1)\n\nx_test_vec = polynomial_features(x_test, P)\ny_test_vec = y_test.reshape(-1, 1)\n\n# Trasladamos estos vectores generados previamente a \"tensores\", utilizados en Pytorch.\nx_train_tensor = torch.from_numpy(np.float32(x_train_vec)).to(device)\ny_train_tensor = torch.from_numpy(np.float32(y_train_vec)).to(device)\nn_train = len(x_train_tensor)\n\nx_test_tensor = torch.from_numpy(np.float32(x_test_vec)).to(device)\ny_test_tensor = torch.from_numpy(np.float32(y_test_vec)).to(device)\nn_test = len(x_test_tensor)","902f81a3":"# Importamos \"torch.nn\" para poder crear las capas de la red.\nfrom torch import nn\n\nclass LinearRegression(nn.Module):\n    def __init__(self, num_features):\n        super().__init__() \n        \n        # La capa que aplica una transformaci\u00f3n lineal a los datos entrantes.\n        self.fc = nn.Linear(in_features=num_features, out_features=1, bias=True)\n        \n    def forward(self, x):\n        out = self.fc(x)\n        return out","9a1e0109":"from prettytable import PrettyTable\nfrom torch.optim import SGD\n\n# Creamos la tabla que nos permitir\u00e1 mostrar los resultados obtenidos.\nresultados = PrettyTable()\nresultados.field_names = ['Regularizaci\u00f3n', 'Learning Rate', 'Weight Decay', 'Epochs', 'Min Loss']\n\n# Inicializamos las variables.\n# Generamos un diccionario donde guardar los mejores par\u00e1metros obtenidos del algoritmo.\nparametros_L1 = {\"lr\": 0, \"wd\": 0, \"epochs\": 0}\n\n# Variable creada para guardar el modelo optimo, es decir, aquel en el que se obtenga menor \"Loss\".\nmin_loss = 999999999\n\n# Establecemos ejemplos de par\u00e1metros para realizar la b\u00fasqueda en grilla.\nLEARNING_RATE = [0.001, 0.01, 0.1]\nWEIGHT_DECAY = [0.001, 0.01, 0.1]\nEPOCHS = [500, 1000, 5000, 10000]\n\n# Creamos un \"criterio\" que mide el error absoluto medio (MAE).\ncriterion = nn.L1Loss(reduction='sum')\n\nfor lr in LEARNING_RATE:\n    for wd in WEIGHT_DECAY:        \n        for epochs in EPOCHS:\n            \n            # Instanciamos el modelo\n            model = LinearRegression(num_features=P)\n            \n            # Lo enviamos al dispoitivo disponible (GPU\/CPU).\n            model.to(device)\n            \n            # Implementamos \"descenso de gradiente estoc\u00e1stico\".\n            optimizer = SGD(model.parameters(), lr=lr, weight_decay=wd)\n            \n            # Seteamos el modelo para entrenar.\n            model.train()\n            \n            train_loss = []\n            test_loss = []\n\n            for epoch in range(epochs):\n                \n                # Limpiamos todos los gradientes.\n                optimizer.zero_grad()\n                \n                # Realizamos la predicci\u00f3n con el modelo (forward pass).\n                y_pred = model(x_train_tensor)\n                \n                # Calculamos la p\u00e9rdida (loss) y la dividmos por el numero de obs.\n                loss = criterion(y_pred, y_train_tensor) \/ n_train\n                \n                # Calculamos los gradientes (backward pass).\n                loss.backward()\n                \n                # Realizamos el paso de optimizaci\u00f3n, actualizando los pesos.\n                optimizer.step()                                    \n                train_loss.append(loss.detach().item())\n\n                with torch.no_grad():\n                    y_pred = model(x_test_tensor)\n                    loss = criterion(y_pred, y_test_tensor) \/ n_test\n                    test_loss.append(loss.item())\n            \n            # Seteamos el modelo para predecir, sin variar sus pesos.\n            model.eval()\n            y_pred = model(x_test_tensor)\n            after_train = criterion(y_pred, y_test_tensor) \/ n_test\n\n            if after_train.item() < min_loss:\n                \n                # Actualizamos los valores del diccionario.\n                parametros_L1.update({\"lr\": lr,\"wd\": wd,\"epochs\": epochs})\n                min_loss = after_train.item()\n                \n                # Obtenemos una instancia del modelo, con sus respectivos pesos y caracter\u00edsticas.\n                best_model_L1 = type(model)(num_features=P)\n                best_model_L1.load_state_dict(model.state_dict())\n                \n                best_train_loss_L1 = train_loss\n                best_test_loss_L1 = test_loss\n\n# Guardamos los mejores resultados e imprimimos los mejores parametros.\nresultados.add_row(['L1', str(parametros_L1.get('lr')), str(parametros_L1.get('wd')), str(parametros_L1.get('epochs')), str(round(min_loss, 3))])            \nprint(\"Mejores par\u00e1metros:\",parametros_L1)","4c6fd4d9":"# Inicializamos las variables.\n# Generamos un diccionario donde guardar los mejores par\u00e1metros obtenidos del algoritmo.\nparametros = {\"lr\": 0, \"wd\": 0, \"epochs\": 0}\n\n# Variable creada para guardar el modelo optimo, es decir, aquel en el que se obtenga menor \"Loss\".\nmin_loss = 99999\n\n# Establecemos ejemplos de par\u00e1metros para realizar la b\u00fasqueda en grilla.\nLEARNING_RATE = [0.5, 0.3, 0.2, 0.1]\nWEIGHT_DECAY = [0.1, 0.01, 0.001, 0.0001]\nEPOCHS = [500, 1000, 5000, 10000]\n\n# Creamos un criterio que mide el error cuadr\u00e1tico medio (MSE).\ncriterion = nn.MSELoss(reduction='sum')\n\nfor lr in LEARNING_RATE:\n    for wd in WEIGHT_DECAY:\n        for epochs in EPOCHS:\n            \n            # Instanciamos el modelo\n            model = LinearRegression(num_features=P)\n            # Lo enviamos al dispoitivo disponible (GPU\/CPU).\n            model.to(device)\n            \n            # Implementamos \"descenso de gradiente estoc\u00e1stico\".\n            optimizer = SGD(model.parameters(), lr=lr, weight_decay=wd)\n            \n            # Seteamos el modelo para entrenar.\n            model.train()\n            \n            train_loss = []\n            test_loss = []\n\n            for epoch in range(epochs):\n                \n                # Limpiamos todos los gradientes.\n                optimizer.zero_grad()\n                \n                # Realizamos la predicci\u00f3n con el modelo (forward pass).\n                y_pred = model(x_train_tensor)\n                \n                # Calculamos la p\u00e9rdida (loss) y la dividmos por el numero de obs.\n                loss = criterion(y_pred, y_train_tensor) \/ n_train\n                \n                # Calculamos los gradientes (backward pass).\n                loss.backward()\n                \n                # Realizamos el paso de optimizaci\u00f3n, actualizando los pesos.\n                optimizer.step()\n\n                train_loss.append(loss.detach().item())\n\n                with torch.no_grad():\n                    y_pred = model(x_test_tensor)\n                    loss = criterion(y_pred, y_test_tensor) \/ n_test\n                    test_loss.append(loss.item())\n            \n            # Seteamos el modelo para predecir, sin variar sus pesos.\n            model.eval()\n            y_pred = model(x_test_tensor)\n            after_train = criterion(y_pred, y_test_tensor) \/ n_test\n\n            if after_train.item() < min_loss:\n                \n                # Actualizamos los valores del diccionario.\n                parametros.update({\"lr\": lr,\"wd\": wd,\"epochs\": epochs})\n                min_loss = after_train.item()\n                \n                # Obtenemos una instancia del modelo, con sus respectivos pesos y caracter\u00edsticas.\n                best_model = type(model)(num_features=P)\n                best_model.load_state_dict(model.state_dict())\n                \n                best_train_loss = train_loss\n                best_test_loss = test_loss\n\n# Guardamos los mejores resultados e imprimimos los mejores par\u00e1metros.\nresultados.add_row(['L2', str(parametros.get('lr')), str(parametros.get('wd')), str(parametros.get('epochs')), str(round(min_loss, 3))])  \nprint(\"Mejores par\u00e1metros: \",parametros)","acb69fdf":"# Imprimimos los valores optimos previamente obtenidos de ambos modelos.\nprint(resultados)\n\n# Graficamos los resultados obtenidos previamente.\nplt.figure(figsize=(20,10))\nplt.suptitle('L1 vs L2', fontsize = 20)\n\nplt.subplot(221)\nplt.title('Valores de Loss (L1)', fontsize= 15)\nplt.plot(np.arange(parametros_L1[\"epochs\"]), best_train_loss_L1)\nplt.plot(np.arange(parametros_L1[\"epochs\"]), best_test_loss_L1)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\n# Obtenemos los valores predichos por el modelo.\nbest_model_L1.to(device)\nbest_model_L1.eval()  \ny_pred = best_model_L1(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(222)\nplt.title('Real vs Predicci\u00f3n (L1)', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.subplot(223)\nplt.title('Valores de Loss (L2)', fontsize= 15)\nplt.plot(np.arange(parametros[\"epochs\"]), best_train_loss)\nplt.plot(np.arange(parametros[\"epochs\"]), best_test_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\n# Obtenemos los valores predichos por el modelo.\nbest_model.to(device)\nbest_model.eval()  \ny_pred = best_model(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(224)\nplt.title('Real vs Predicci\u00f3n (L2)', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.show()","1485caca":"class FeedforwardNet(nn.Module):\n    \n        def __init__(self, input_size, hidden_size):\n            super(FeedforwardNet, self).__init__()\n            self.input_size = input_size\n            self.hidden_size  = hidden_size\n            \n            self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n            self.fc2 = nn.Linear(self.hidden_size, 1)\n            \n        def forward(self, x):\n            x = self.fc1(x)\n            output = self.fc2(x)\n            return output","f2969304":"# Generamos la tabla para guardar los resultados.\nresultados = PrettyTable()\nresultados.field_names = ['Learning Rate', 'N\u00ba de Neuronas', 'Test Loss']\n\nparametros = {\"lr\": 0, \"hs\": 0}\nmin_loss = 99999\n\n# \"HIDEEN_SIZE\" representa el N\u00ba de neuronas.\nHIDEEN_SIZE = [2, 5, 10, 20]\nLEARNING_RATE = [0.001, 0.01, 0.1]\nepochs = 10000\n\ncriterion = nn.MSELoss(reduction='sum')\n\nfor h in HIDEEN_SIZE:    \n    for lr in LEARNING_RATE:\n        model = FeedforwardNet(input_size = P, hidden_size = h)\n        model.to(device)\n        model.train()\n        \n        optimizer = SGD(model.parameters(), lr=lr)\n\n        train_loss = []\n        test_loss = []\n\n        for epoch in range(epochs):\n            \n            # Limpiamos todos los gradientes.\n            optimizer.zero_grad()\n                \n            # Realizamos la predicci\u00f3n con el modelo (forward pass).\n            y_pred = model(x_train_tensor)\n                \n            # Calculamos la p\u00e9rdida (loss) y la dividmos por el numero de obs.\n            loss = criterion(y_pred, y_train_tensor) \/ n_train\n                \n            # Calculamos los gradientes (backward pass).\n            loss.backward()\n                \n            # Realizamos el paso de optimizaci\u00f3n, actualizando los pesos.\n            optimizer.step()\n\n            train_loss.append(loss.detach().item())\n\n            with torch.no_grad():\n                y_pred = model(x_test_tensor)\n                loss = criterion(y_pred, y_test_tensor) \/ n_test\n                test_loss.append(loss.item())\n                    \n        model.eval()\n        y_pred = model(x_test_tensor)\n        after_train = criterion(y_pred, y_test_tensor) \/ n_test\n        \n        if after_train.item() < min_loss:\n            parametros.update({\"lr\": lr,\"hs\": h})\n            min_loss = after_train.item()\n            \n            best_model = type(model)(input_size=P, hidden_size=h)\n            best_model.load_state_dict(model.state_dict())\n            \n            best_train_loss = train_loss\n            best_test_loss = test_loss\n            \n        resultados.add_row([str(lr), str(h), str(round(after_train.item(), 3))])\n        \nprint(\"Mejores par\u00e1metros: \",parametros)","2f0d9884":"# Ordenamos la tabla por la columna \"Test Loss\"\nresultados.sortby = \"Test Loss\"\nprint(resultados)","680c6e4a":"plt.figure(figsize=(20,5))\nplt.suptitle('FeedForwardNet', fontsize = 20)\n\nplt.subplot(121)\nplt.title('Valores de Loss', fontsize= 15)\nplt.plot(np.arange(epochs), best_train_loss)\nplt.plot(np.arange(epochs), best_test_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\nbest_model.to(device)\nbest_model.eval()  \ny_pred = best_model(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(122)\nplt.title('Real vs Predicci\u00f3n', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.show()","01aac782":"class FeedforwardNet_Relu_Tahn(nn.Module):\n        def __init__(self, input_size, hidden_size):\n            super(FeedforwardNet_Relu_Tahn, self).__init__()\n            self.input_size = input_size\n            self.hidden_size = hidden_size\n\n            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n            self.relu = torch.nn.ReLU()\n            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n            self.tanh = torch.nn.Tanh()  \n            \n        def forward(self, x):\n            x = self.fc1(x)\n            x = self.relu(x)\n            x = self.fc2(x)\n            x = self.tanh(x)\n            return x","9c25af6d":"resultados = PrettyTable()\nresultados.field_names = ['Learning Rate', 'N\u00ba de neuronas', 'Test Loss']\n\nparametros = {\"lr\": 0, \"hs\": 0}\nmin_loss = 99999\n\nHIDEEN_SIZE = [2, 5, 10, 20]\nLEARNING_RATE = [0.001, 0.01, 0.1]\nepochs = 10000\n\ncriterion = nn.MSELoss(reduction='sum')\n\nfor h in HIDEEN_SIZE:  \n    for lr in LEARNING_RATE:\n        \n        model = FeedforwardNet_Relu_Tahn(input_size=P, hidden_size=h)\n        model.to(device)\n        model.train()\n        \n        optimizer = SGD(model.parameters(), lr=lr)\n\n        train_loss = []\n        test_loss = []\n\n        for epoch in range(epochs):\n            \n            # Limpiamos todos los gradientes.\n            optimizer.zero_grad()\n                \n            # Realizamos la predicci\u00f3n con el modelo (forward pass).\n            y_pred = model(x_train_tensor)\n                \n            # Calculamos la p\u00e9rdida (loss) y la dividmos por el numero de obs.\n            loss = criterion(y_pred, y_train_tensor) \/ n_train\n                \n            # Calculamos los gradientes (backward pass).\n            loss.backward()\n                \n            # Realizamos el paso de optimizaci\u00f3n, actualizando los pesos.\n            optimizer.step()\n\n            train_loss.append(loss.detach().item())\n\n            with torch.no_grad():\n                y_pred = model(x_test_tensor)\n                loss = criterion(y_pred, y_test_tensor) \/ n_test\n                test_loss.append(loss.item())\n                    \n        model.eval()\n        y_pred = model(x_test_tensor)\n        after_train = criterion(y_pred, y_test_tensor) \/ n_test\n\n        if after_train.item() < min_loss:\n            parametros.update({\"lr\": lr,\"hs\": h})\n            min_loss = after_train.item()\n            \n            best_model = type(model)(input_size=P, hidden_size=h)\n            best_model.load_state_dict(model.state_dict())\n            \n            best_train_loss = train_loss\n            best_test_loss = test_loss\n        \n        resultados.add_row([str(lr), str(h), str(round(after_train.item(), 3))])\n        \nprint(\"Mejores par\u00e1metros: \",parametros)","a7e634ae":"# Ordenamos la tabla por la columna \"Test Loss\"\nresultados.sortby = \"Test Loss\"\nprint(resultados)","6b2d93c2":"plt.figure(figsize=(20,5))\nplt.suptitle('Net con funciones de activaci\u00f3n', fontsize = 20)\n\nplt.subplot(121)\nplt.title('Valores de Loss', fontsize= 15)\nplt.plot(np.arange(epochs), best_train_loss)\nplt.plot(np.arange(epochs), best_test_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\nbest_model.to(device)\nbest_model.eval()  \ny_pred = best_model(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(122)\nplt.title('Real vs Predicci\u00f3n', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.show()","06d59853":"# Importamos la libreria \"data\" que nos permitir\u00e1 generar \"DataLoaders\" y asi iterar sobre los datos.\nfrom torch.utils import data\n\n#Inicializamos las variables.\n# Utilizamos las neuronas optimas del ejercicio anterior.\nh = parametros.get('hs')\n\nresultados = PrettyTable()\nresultados.field_names = ['Learning Rate', 'Batch Size', 'Test Loss']\n\nparametros = {\"lr\": 0, \"bs\": 0}\nmin_loss = 99999\nepochs = 10000\nBATCH_SIZE = [2, 10, 50, 100]\nLEARNING_RATE = [0.001, 0.01, 0.1]\n\n# Generamos los dataset de trein y test a partir de los tensores.\ntrain_data = data.TensorDataset(x_train_tensor, y_train_tensor)\ntest_data = data.TensorDataset(x_test_tensor, y_test_tensor)\n\nfor batch in BATCH_SIZE:\n    \n    # Generamos los iterables sobre de los dos datasets previamente creados. \n    train_loader = data.DataLoader(dataset=train_data, batch_size=batch, shuffle=True, num_workers=0,) \n    test_loader = data.DataLoader(dataset=test_data, batch_size=batch, shuffle=False, num_workers=0,)\n    \n    for lr in LEARNING_RATE:\n        train_loss, test_loss = [], []\n        model = FeedforwardNet_Relu_Tahn(input_size=P, hidden_size=h)\n        model.to(device)\n        model.train()\n        \n        optimizer = SGD(model.parameters(), lr=lr)\n        \n        for epoch in range(epochs):\n            train_loss_acc = 0.\n            \n            # Aqu\u00ed es donde realizamos la iteraci\u00f3n sobre los elementos del dataset, segun el \"batch_size\" elegido.\n            for x, y in train_loader:\n                x.to(device, non_blocking=True)\n                y.to(device, non_blocking=True)\n\n                optimizer.zero_grad()\n                y_pred = model(x)\n                loss = criterion(y_pred, y)\n                loss.backward()\n                optimizer.step()\n\n                train_loss_acc += loss.detach().item()\n            train_loss.append(train_loss_acc \/ len(train_data))\n            \n            with torch.no_grad():\n                test_loss_acc = 0.\n                for x, y in test_loader:        \n                    x.to(device, non_blocking=True)\n                    y.to(device, non_blocking=True)        \n\n                    y_pred = model(x)      \n                    loss = criterion(y_pred, y)\n\n                    test_loss_acc += loss.item()    \n                test_loss.append(test_loss_acc \/ len(test_data))\n\n        model.eval()\n        y_pred = model(x_test_tensor)\n        after_train = criterion(y_pred, y_test_tensor) \/ len(y_test_tensor)\n\n        if after_train.item() < min_loss:\n            parametros.update({\"lr\": lr,\"bs\": batch})\n            min_loss = after_train.item()\n            \n            best_model = type(model)(input_size=P, hidden_size=h)\n            best_model.load_state_dict(model.state_dict())\n            \n            best_train_loss = train_loss\n            best_test_loss = test_loss\n        \n        resultados.add_row([str(lr), str(batch), str(round(after_train.item(), 3))])\n\nprint(\"Mejores par\u00e1metros: \",parametros)","fe8ef646":"print(\"Utilizando \", h, \" N\u00ba de neuronas\" )\n# Ordenamos la tabla por la columna \"Test Loss\"\nresultados.sortby = \"Test Loss\"\nprint(resultados)","cc053e21":"plt.figure(figsize=(20,5))\n\nplt.subplot(121)\nplt.title('Valores de Loss', fontsize= 15)\nplt.plot(np.arange(epochs), best_train_loss)\nplt.plot(np.arange(epochs), best_test_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\nbest_model.to(device)\nbest_model.eval()  \ny_pred = best_model(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(122)\nplt.title('Real vs Predicci\u00f3n', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.show()","ced506c7":"# Inicializamos las variables.\nparametros = {\"lr\": 0, \"bs\": 0}\n\n# Utilizamos las neuronas optimas del ejercicio anterior.\nmin_loss = 99999\nepochs = 10000\nBATCH_SIZE = [2, 4, 6, 8]\nLEARNING_RATE = [0.001, 0.01, 0.1]\n\ncriterion = nn.MSELoss(reduction='sum')\n\nfor batch in BATCH_SIZE:\n    \n    # Generamos los iterables sobre de los dos datasets previamente creados.\n    train_loader = data.DataLoader(dataset=train_data, batch_size=batch, shuffle=True, num_workers=0,) \n    test_loader = data.DataLoader(dataset=test_data, batch_size=batch, shuffle=False, num_workers=0,)\n    \n    for lr in LEARNING_RATE:\n        train_loss, test_loss = [], []\n        model = FeedforwardNet_Relu_Tahn(input_size=P, hidden_size=h)\n        model.to(device)\n        model.train()\n        \n        optimizer = SGD(model.parameters(), lr=lr)\n        \n        for epoch in range(epochs):\n            train_loss_acc = 0.\n            \n            # Limpiamos los gradientes.\n            optimizer.zero_grad()\n            \n            # Aqu\u00ed es donde realizamos la iteraci\u00f3n sobre los elementos del dataset, segun el \"batch_size\" elegido.\n            for x, y in train_loader:\n                x.to(device, non_blocking=True)\n                y.to(device, non_blocking=True)\n\n                y_pred = model(x)\n                loss = criterion(y_pred, y) \/ batch\n                loss.backward()\n                \n                train_loss_acc += loss.detach().item()\n            train_loss.append(train_loss_acc \/ len(train_data))\n            \n            # Actualizamos utilizando el gradiente sobre la funci\u00f3n de costo acumulada, tras recorrer todo el dataset.\n            optimizer.step()\n            \n            with torch.no_grad():\n                test_loss_acc = 0.\n                for x, y in test_loader:        \n                    x.to(device, non_blocking=True)\n                    y.to(device, non_blocking=True)        \n\n                    y_pred = model(x)      \n                    loss = criterion(y_pred, y) \/ batch\n\n                    test_loss_acc += loss.item()    \n                test_loss.append(test_loss_acc \/ len(test_data))\n\n        model.eval()\n        y_pred = model(x_test_tensor)\n        after_train = criterion(y_pred, y_test_tensor) \/ len(y_test_tensor)\n\n        if after_train.item() < min_loss:\n            parametros.update({\"lr\": lr,\"bs\": batch})\n            min_loss = after_train.item()\n            \n            best_model = type(model)(input_size=P, hidden_size=h)\n            best_model.load_state_dict(model.state_dict())\n            \n            best_train_loss = train_loss\n            best_test_loss = test_loss\n        \n        resultados.add_row([str(lr), str(batch), str(round(after_train.item(), 3))])\n        \nprint(\"Mejores par\u00e1metros: \", parametros)","24acd932":"print(\"Utilizando un N\u00ba de \", h, \" neuronas\" )\n# Ordenamos la tabla por la columna \"Test Loss\"\nresultados.sortby = \"Test Loss\"\nprint(resultados)","0be431a5":"plt.figure(figsize=(20,5))\n\nplt.subplot(121)\nplt.title('Valores de Loss', fontsize= 15)\nplt.plot(np.arange(epochs), best_train_loss)\nplt.plot(np.arange(epochs), best_test_loss)\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.grid()\n\nbest_model.to(device)\nbest_model.eval()  \ny_pred = best_model(x_test_tensor)\ny_pred = y_pred.cpu().detach().numpy().squeeze()\n\nplt.subplot(122)\nplt.title('Real vs Predicci\u00f3n', fontsize= 15)\nplt.plot(x_test, y_test)\nplt.plot(x_test, y_pred, color='orange')\nplt.scatter(x_train, y_train, color='red')\nplt.ylabel('$\\sin(2 \\pi x)$')\nplt.xlabel('$x$')\nplt.legend(['real', 'predict', 'train'])\nplt.grid()\n\nplt.show()","6a3635b0":"import torch\nfrom torch.autograd import Variable\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam\nfrom sklearn.metrics import accuracy_score\nimport imageio\nimport torch.nn.functional as F\nimport os\n\nimport torchvision\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Generamos algunas trasnformaciones que van a ser aplicadas al train set.\ntransform_train = transforms.Compose([\n    \n    # Por ejemplo algunos recortes aleatorios.\n    transforms.RandomCrop(32, padding=4),\n    \n    # Y tambien algunos giros de ejes al azar.\n    transforms.RandomHorizontalFlip(),\n    \n    # Para finalmente enviarlo al tensor y normalizarlo.\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# Normalizamos el conjunto de test igual que el conjunto de train sin augmentation.\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])","bde6d9fb":"# Descargando el dataset y aplicando las transformaciones.\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True, download=True, transform = transform_train)\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False, download=True, transform = transform_test)\n\n# Trasladamos los sets a su respectivo dataloader.\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size= 128, shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","9b6983fe":"# Funci\u00f3n auxiliar que nos permitir\u00e1 mostrar las imagenes.\ndef imshow(img):\n    img = img \/ 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","6984f7fd":"fig = plt.figure(figsize=(20,12)) \nfig.suptitle('Ejemplos del dataset:',fontsize=20)\n\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\ni = 1\n\nfor im, lab in zip(images, labels):\n    sub = plt.subplot(4, 8, i)\n    img = im \/ 2 + 0.5\n    npimg = img.numpy()\n    \n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.xlabel(classes[lab])\n    plt.xticks([])\n    plt.yticks([])\n    \n    if i == 32:\n        break\n    i += 1","1bb38b8a":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n    \n        self.conv_layer = nn.Sequential(\n\n            # Bloque 1\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Bloque 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout2d(p=0.05),\n\n            # Bloque 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        self.fc_layer = nn.Sequential(\n            nn.Dropout(p=0.1),\n            nn.Linear(4096, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.1),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layer(x)\n        return x","a8519a13":"net = Net()\nnet.to(device)\n\nbatch = 32\nstep = len(trainset) \/\/ batch\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(net.parameters(), lr = 0.000075, weight_decay=0.0001)\n\nnet.train()\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size= batch, shuffle=True, num_workers=2)\n\nfor epoch in range(40):\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\nprint(\"Entrenamiento Finalizado\")","d76097e1":"net.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.cuda()).sum().item()\n\nprint('Exactitud de la red para las 10000 imagenes del test: %d %%' % (100 * correct \/ total))","df581c5f":"fig = plt.figure(figsize=(20,15)) \nfig.suptitle('Ejemplos de predicciones:',fontsize=15)\n\ndataiter = iter(testloader)\nwith torch.no_grad():\n    for i in range(1,33):\n        images, _ = dataiter.next()\n        _, predicted = torch.max(net(images.cuda()), 1)\n        sub = plt.subplot(4, 8, i)\n        img = images[0] \/ 2 + 0.5\n        npimg = img.numpy()\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        plt.xlabel(classes[predicted])\n        i += 1","587009c7":"import torchvision.models as models\nfrom torch.optim import lr_scheduler\nfrom collections import OrderedDict\n\nmodel = models.vgg11(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfc = nn.Sequential(OrderedDict([\n    ('0', nn.Linear(4096,1024)),\n    ('1', nn.ReLU(inplace=True)),\n    ('2', nn.Dropout(p=0.25)),\n    ('3', nn.Linear(1024,500)),\n    ('4', nn.ReLU(inplace=True)),\n    ('5', nn.Dropout(p=0.2)),\n    ('6', nn.Linear(500,10)),\n    ('7', nn.LogSoftmax())\n]))\n\nmodel.classifier[6] = fc\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)","58d8ece2":"batch = 32\nstep = len(trainset) \/\/ batch\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch, shuffle=True, num_workers=4)\ntestloader = torch.utils.data.DataLoader(testset, shuffle=False, num_workers=4)\n\nmodel.train()\nrunning_loss = 0.0\n\nfor epoch in range(40):\n    loss = 0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n          \n    scheduler.step()\n    \nprint(\"Entrenamiento Finalizado\")","94c633b0":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Exactitud de la red para las 10000 imagenes del test: %d %%' % (100 * correct \/ total))","8573dc52":"fig = plt.figure(figsize=(20,15)) \nfig.suptitle('Ejemplos de predicciones:',fontsize=20)\n\ndataiter = iter(testloader)\nwith torch.no_grad():\n    for i in range(1,33):\n        images, _ = dataiter.next()\n        _, predicted = torch.max(model(images.cuda()), 1)\n        sub = plt.subplot(4, 8, i)\n        img = images[0] \/ 2 + 0.5\n        npimg = img.numpy()\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        plt.xlabel(classes[predicted])\n        i += 1","304599f2":"Para comenzar, crearemos un mini dataset con el que las redes trabajaran:","03918564":"## Gr\u00e1ficos","08e297b9":"Luego trasladamos los elementos generados a los **\"tensores\"** para ser utilizados por las redes:","2f827fef":"<a id=\"bibliografia\"><\/a>\n# Bibliograf\u00eda\n\n## Conceptos y uso de Herramientas\n\n* [Regularizaci\u00f3n](https:\/\/iartificial.net\/regularizacion-lasso-l1-ridge-l2-y-elasticnet\/)\n* [CIFAR-10 en Pytorch](https:\/\/www.stefanfiott.com\/machine-learning\/cifar-10-classifier-using-cnn-in-pytorch\/)\n* [Construcci\u00f3n de Redes Neuronales con Pytorch](https:\/\/blog.paperspace.com\/pytorch-101-building-neural-networks\/)\n* [M\u00e9todos de Activaci\u00f3n](https:\/\/www.geeksforgeeks.org\/python-pytorch-tanh-method\/)\n* [Image Augmentation](https:\/\/www.analyticsvidhya.com\/blog\/2019\/12\/image-augmentation-deep-learning-pytorch\/)\n* [Documentaci\u00f3n de Pytorch](https:\/\/pytorch.org\/tutorials\/index.html)","921b159a":"<a id=\"resultados-redes-profundas\"><\/a>\n### Resultados","6bf736c1":"Finalmente, terminaremos obteniendo una estructura como se muestra en la siguiente imagen:\n\n![](https:\/\/i.imgur.com\/XSPzvR4.png)\n\n### Entrenamiento","ccbc3c4b":"<a id=\"resultados-batch-size\"><\/a>\n## Resultados","d4207fb7":"<a id=\"mini-batches\"><\/a>\n# Mini-Batches SGD","4fc226e6":"<a id=\"arquitectura-red\"><\/a>\n## Arquitectura de la Red","a9ac56c9":"<a id=\"red-resultados\"><\/a>\n## Resultados del Test","b0e4e5ef":"<a id=\"resultados-mini-batches\"><\/a>\n## Resultados","32172633":"<a id=\"conclusion-redes-profundas\"><\/a>\n### Conclusi\u00f3n\n\nLuego de la realizaci\u00f3n del ejercicio y de probar diferentes combinaciones, podemos decir que:\n* El entrenamiento se hace mas lento entre mas capas ocultas se usen.\n* Si se llegase a utilizar un gran numero de neuronas en la capa oculta sin poder solucionar el problema, se deberia a\u00f1adir una segunda capa reduciendo el numero de neuronas en cada una.\n* Si se utilizan demasiadas neuronas en la capa oculta, puede causar **\"Overfitting\"**, es decir, el modelo particulariza, no generaliza.\n* Un numero muy peque\u00f1o de neuronas podria generar **\"Underfitting\"**, es decir, no se resuelve el problema.\n\n<a id=\"funciones-activacion\"><\/a>\n# Funciones de Activaci\u00f3n\n\nLa funci\u00f3n de activaci\u00f3n se encarga de devolver una salida a partir de un valor de entrada, normalmente el conjunto de valores de salida en un rango determinado. Se buscan funciones que las derivadas sean simples, para minimizar con ello el coste computacional. Algunos ejemplos de estas funciones, son:\n\n* **\"Sigmoid\":** transforma los valores introducidos a una escala (0,1), donde los valores altos tienen de manera asint\u00f3tica a 1 y los valores muy bajos tienden de manera asint\u00f3tica a 0.\n* **\"ReLU\" (Rectified Lineal Unit):** transforma los valores introducidos anulando los valores negativos y dejando los positivos tal y como entran.\n* **\"Tanh\" (Tangent Hyperbolic):** transforma los valores introducidos a una escala (-1,1), donde los valores altos tienen de manera asint\u00f3tica a 1 y los valores muy bajos tienden de manera asint\u00f3tica a -1.\n\nPara este caso estaremos utilizando las dos ultimas funciones de activaci\u00f3n, a la vez que seguimos probando distintos hiperparametros.\n\n## Definici\u00f3n de la Red","e49121ea":"## Entrenamiento","d92f48de":"<a id=\"redes-neuronales\"><\/a>\n# Redes Neuronales Artificiales\n\nUna **Red Neuronal Artificial** es un sistema de nodos interconectados de forma ordenada, distribuido en capas, a trav\u00e9s del cual una se\u00f1al de entrada se propaga para producir una salida. Constan de una capa de entrada, una o varias capas ocultas y una capa de salida y se las puede entrenar para que \u201caprendan\u201d a reconocer ciertos patrones.\n\n## Sin capa oculta (Regresi\u00f3n Lineal)\n\nEn primera instancia, probaremos el rendimiento de una red **sin capa oculta**, en donde podemos destacar algunos componentes como son:\n* ``__init__`` : donde creamos las diferentes capas que tendr\u00e1 la red, una en este caso.\n* ``forward`` : donde determinamos el recorrido que hace el algoritmo en la red, para calcular el valor de activaci\u00f3n de las neuronas desde las entradas hasta obtener los valores de salida.","d50ee0b6":"<a id=\"conclusi\u00f3n-batch-size\"><\/a>\n## Conclusi\u00f3n\n\nVentajas de utilizar un **\"batch size\"** < al tama\u00f1o del dataset:\n\n* Requiere menos memoria. Como entrena la red con menos muestras, el procedimiento de entrenamiento general requiere menos memoria. Eso es especialmente importante si no puede ajustar todo el conjunto de datos en la memoria de su m\u00e1quina.\n\n* Por lo general, las redes entrenan m\u00e1s r\u00e1pido con mini lotes. Eso es porque actualizamos los pesos despu\u00e9s de cada propagaci\u00f3n.\n\nDesventajas de utilizar un **\"batch size\"** < al tama\u00f1o del dataset:\n\n* Cuanto m\u00e1s peque\u00f1o sea el lote, menos precisa ser\u00e1 la estimaci\u00f3n del gradiente.","eae3b684":"## Gr\u00e1ficos","43bd40fe":"<a id=\"cifar-10\"><\/a>\n# Trabajando sobre \"CIFAR-10\"\n\nEl dataset **CIFAR-10** consta de 60.000 im\u00e1genes en color de 32x32 en 10 clases, con 6000 im\u00e1genes por clase. Hay 50.000 im\u00e1genes de entrenamiento y 10.000 im\u00e1genes de prueba. Para mejor los resultados obtenidos por nuestra red, comenzaremos aplicando **Data Augmentation** a nuestros datos.\n\n<a id=\"data-augmentation\"><\/a>\n## Data Augmentation\n\nCon esta tecnica podemos aumentar la cantidad y modificar la forma de los de datos que nuestra red va a utilizar e interpretar:\n\n* Introduciendo perturbaciones en los datos originales. Como, por ejemplo, tomando una imagen original centrada, la replicarla  descentrada, invirtiendo ejes, cortando ciertas porciones, etc.\n* Utilizando distintas distribuciones. Por ejemplo, a\u00f1adiendo ruido o diferentes efectos a las imagenes.","ebe8415f":"<a id=\"preentrenado-resultados\"><\/a>\n## Resultados del Test","dea79d09":"### Ejemplos de predicciones","acbc2203":"<a id=\"resultados-funciones-activacion\"><\/a>\n## Resultados","9a4ff59c":"## Entrenamiento","bf95ca63":"Finalmente, terminaremos obteniendo una estructura como se muestra en la siguiente imagen:\n\n![](https:\/\/i.imgur.com\/V2zFTIz.png)\n\n<a id=\"componentes-redes\"><\/a>\n## Componentes de las redes\n\nDentro de las diferentes redes y ejercicios que se desarrollar\u00e1n en el presente trabajo practico, encontramos diferentes elementos, como son:\n\n* **Peso:** los pesos de las neuronas son **coeficientes** que pueden adaptarse dentro de la red que determinan la intensidad de la se\u00f1al de entrada registrada por la neurona artificial.\n* **Learning Rate:** tambien denominado **\u00abtasa de aprendizaje\u00bb**, determina en gran medida la velocidad y calidad del aprendizaje del algoritmo indicando cuanto pueden cambiar los pesos de las neuronas. Si es una tasa muy grande, el algoritmo aprende m\u00e1s r\u00e1pido pero tendremos mayor imprecisi\u00f3n en el resultado. Si es demasiado peque\u00f1o, tardar\u00e1 mucho y podr\u00eda no finalizar nunca.\n* **Weight Decay:** es un t\u00e9rmino adicional utilizado despu\u00e9s de cada actualizaci\u00f3n que multiplican los pesos por un factor ligeramente inferior a 1, evitando que crezcan demasiado.\n* **Epoch:** en una \u00e9poca, todo el dataset de entrenamiento es pasado (tomado como entrada, determinada la salida y calculados los gradientes) por la red neuronal (NN).\n* **Optimizer:** recibe los par\u00e1metros del modelo y la tasa de aprendizaje (learning rate) y se encarga de actualizar dichos par\u00e1metros en funci\u00f3n del gradiente de la funci\u00f3n de p\u00e9rdida de forma iterativa durante el entrenamiento de la red.\n\n<a id=\"regularizacion\"><\/a>\n## Regularizaci\u00f3n\n\nLa **regularizaci\u00f3n** consiste en a\u00f1adir un termino de penalizaci\u00f3n a la funci\u00f3n de coste. Esta penalizaci\u00f3n produce modelos m\u00e1s simples que generalizan mejor ayudando a reducir el sobreajuste. Los modelos que son excesivamente complejos tienden a sobreajustar. Es decir, a encontrar una soluci\u00f3n que funciona muy bien para los datos de entrenamiento pero muy mal para datos nuevos.  Nos interesan los modelos que adem\u00e1s de aprender bien, tambi\u00e9n funcionen tengan un buen rendimiento con datos nuevos.\n\n<a id=\"l1\"><\/a>\n### Regularizaci\u00f3n Lasso (L1)\n\nLa **regularizaci\u00f3n Lasso**, tambi\u00e9n llamada L1, se mide como la media del valor absoluto de los coeficientes del modelo. Esta regularizaci\u00f3n nos va a servir de ayuda cuando sospechemos que varios de los atributos de entrada (features) sean irrelevantes. Al usar la regularizaci\u00f3n Lasso, estamos fomentando que la soluci\u00f3n sea poco densa. Es decir, favorecemos que algunos de los coeficientes acaben valiendo 0.","c1aca64d":"### Gr\u00e1ficos","2d5cb6bc":"## Gr\u00e1ficos","732d8599":"## Entrenamiento","37b358af":"Una vez importamos todas las librerias necesarias, procedemos a descargar el contenido del dataset, aplicarles la transformaci\u00f3n propuesta previamente y a cargar cada set en un **dataloader** que, como habiamos dicho mas arriba, nos permitir\u00e1 iterar cada conjunto.","75daf232":"<a id=\"red-preentrenada\"><\/a>\n## Utilizando una red pre-entrenada\n\nDurante el entrenamiento de una red neuronal se calculan los pesos \u00f3ptimos para la clasificaci\u00f3n, asociados a las conexiones entre neuronas. Esos pesos determinan el funcionamiento de la red. Despu\u00e9s de entrenar la red podemos guardar los pesos, as\u00ed como la arquitectura de la red. De esta manera, al usar la red en el futuro, no tenemos que volver a entrenarla.\n\nEl entrenamiento de una red requiere bastante tiempo, y una gran cantidad de recursos a nivel computacional y una forma de reducir el consumo de estos recursos y el tiempo es utilizar una red pre-entrenada. As\u00ed, a parte de  poder trabajar directamente con una red capaz de clasificar con precisi\u00f3n, tambi\u00e9n podemos usarla como base para nuestros modelos. El uso de una red pre-entrenada y su adaptaci\u00f3n a unas necesidades concretas se conoce como **fine tuning**.\n\nEn este caso utilizaremos el modelo **\"VGG-11\"**, una rede convolucional profunda para el reconocimiento de im\u00e1genes a gran escala.","a9929a24":"<a id=\"batch-size\"><\/a>\n# Batch Size\n\nPytorch provee distintas clases para el accesso a datos, como los *TensorDataset* y *DataLoader*:\n\n* *DataLoader* nos permite realizar iteraciones sobre un dataset y proporcionar los datos en grupos. El tama\u00f1o del grupo est\u00e1 determinado por el **\"Batch Size\"**. Es especialmente \u00fatil para iterar sobre datasets grandes, como por ejemplo los datasets de im\u00e1genes, permitiendo abstraernos de tener que implementar c\u00f3digo espec\u00edfico para manejar aspectos como las rutas de los archivos o la aleatoriedad en su iteraci\u00f3n.\n\n* *TensorDataset*, por otra parte, nos permite generar un dataset de tensores para ser iterado a partir del DataLoader.\n\nEn este caso, vamos a analizar diferentes valores de **\"Batch Size\"**:","8f467006":"![](https:\/\/www.quintagroup.com\/blog\/blog-images\/machine-learning-libraries\/pytorch.png\/@@images\/image.png)\n\n# \u00cdndice\n\n* [Introducci\u00f3n](#introduccion)\n* [Redes Neuronales Artificiales](#redes-neuronales)\n    * [Componentes de las redes](#componentes-redes)\n    * [Regularizaci\u00f3n](#regularizacion)\n        * [Regularizaci\u00f3n Lasso (L1)](#l1)\n        * [Regularizaci\u00f3n Ridge (L2)](#l2)\n        * [Resultados](#resultados-regularizacion)\n    * [Redes con una o varias capas ocultas](#redes-profundas)\n        * [Resultados](#resultados-redes-profundas)\n        * [Conclusiones](#conclusion-redes-profundas)\n* [Funciones de Activaci\u00f3n](#funciones-activacion)\n    * [Resultados](#resultados-funciones-activacion)\n* [Batch Size](#batch-size)\n    * [Resultados](#resultados-batch-size)\n    * [Conclusiones](#conclusi\u00f3n-batch-size)\n* [Mini-Batches SGD](#mini-batches)\n    * [Resultados](#resultados-mini-batches)\n* [Trabajando con CIFAR-10](#cifar-10)\n    * [Data Augmentation](#data-augmentation)\n    * [Arquitectura de la Red](#arquitectura-red)\n        * [Resultados](#red-resultados)\n    * [Utilizando una red pre-entrenada](#red-preentrenada)\n        * [Resultados](#preentrenado-resultados)\n* [Bibliograf\u00eda](#bibliografia)\n\n<a id=\"introduccion\"><\/a>\n# Introducci\u00f3n\n\n## Pytorch\nPytorch es un framework open source para Machine Learning que se destaca gracias a su facilidad de uso y su capacidad nativa para ejecutar en la GPU (tarjeta gr\u00e1fica), lo que permite acelerar procesos usualmente lentos como, por ejemplo, el entrenamiento de modelos. \n\n### Tensores\nUsamos Pytorch para la construcci\u00f3n de **tensores**, estructuras de datos b\u00e1sicas en la creaci\u00f3n de deep learning, que luego son enviados al GPU para ser manipulamos con diferentes propositos.","577c8aa4":"<a id=\"resultados-regularizacion\"><\/a>\n### Resultados","71310454":"<a id=\"l2\"><\/a>\n### Regularizaci\u00f3n Ridge (L2)\n\nLa **regularizaci\u00f3n Ridge**, tambi\u00e9n llamada L2, se mide como la media del cuadrado de los coeficientes del modelo. Esta regularizaci\u00f3n nos va a servir de ayuda cuando sospechemos que varios de los atributos de entrada (features) est\u00e9n correlados entre ellos. Ridge hace que los coeficientes acaben siendo m\u00e1s peque\u00f1os. Esta disminuci\u00f3n de los coeficientes minimiza el efecto de la correlaci\u00f3n entre los atributos de entrada y hace que el modelo generalice mejor. ","46a5d57f":"<a id=\"redes-profundas\"><\/a>\n## Redes con una o varias capas ocultas\n\nGeneralmente, una red neuronal se compone de una capa de entrada, una o varias capas intermedias u ocultas y una capa de salida. El n\u00famero de capas ocultas, as\u00ed como la cantidad de neuronas en ellas, depende de la complejidad y el tipo de problema a resolver."}}