{"cell_type":{"350e88b6":"code","f998deb7":"code","99b993bb":"code","73c90b10":"code","da41a007":"code","7bd0064c":"code","f5debc57":"code","0c2e2def":"code","7d42d749":"code","35f8eed7":"code","a01929b0":"code","e7c5fb9a":"code","59e36d61":"code","3090f114":"code","79a4699a":"code","fd5e7f08":"code","f2671a6a":"code","5beec323":"code","005214ae":"code","abb066c8":"code","2ecf0708":"code","8078356f":"code","11e30cc8":"code","38d9dca5":"code","cc46dcae":"code","effae958":"markdown","2d9427ce":"markdown","f49e0c49":"markdown","f5e130c6":"markdown","bf1d010a":"markdown","10d914fd":"markdown"},"source":{"350e88b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several \n#helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f998deb7":"from matplotlib import pyplot as plt\nfrom math import floor\nimport seaborn as sns\nimport random\nfrom scipy import ndarray\nimport skimage as sk\nfrom skimage import transform\nfrom skimage import util\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\nfrom keras.layers import Input,Conv2D,Dense, Dropout, BatchNormalization, MaxPooling2D, Activation, Flatten, AvgPool2D\nfrom keras.layers import  BatchNormalization as btn\nfrom keras.models import Model, Sequential\nfrom keras.applications.resnet50 import ResNet50\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nfrom keras.callbacks import LearningRateScheduler\nfrom IPython.display import HTML\nimport base64\nfrom scipy.ndimage.interpolation import shift\nfrom keras.optimizers import Adam\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n%matplotlib inline","99b993bb":"train_csv = pd.read_csv('..\/input\/train.csv')\ntest_csv = pd.read_csv('..\/input\/test.csv')","73c90b10":"train_csv.head()","da41a007":"train_csv.shape,test_csv.shape","7bd0064c":"### CHECKING DISTRIBUTION OF DATASET\nsns.set(color_codes=True)\nsns.distplot(train_csv.iloc[:,0],label = 'LABELS',kde=False\n             ,color='red',norm_hist=False,rug=False);","f5debc57":"size_of_img = (int(np.sqrt(train_csv.shape[1])),int(np.sqrt(train_csv.shape[1])))\nfig=plt.figure(figsize=(72,72))\nfor i in range(60):\n    ax=fig.add_subplot(12,12,i+1)\n    plot_image = np.array(train_csv.iloc[i,1:]).reshape(size_of_img)\n    ax.imshow(plot_image,cmap='Greys')\nplt.show()","0c2e2def":"def dataset_distribution(train,distribution = [60,20,20]):\n    # dividing dataset in TRAIN, DEV, TEST\n    # distribution is an array which tell divide percentage\n    train = np.array(train)\n    np.random.shuffle(train)\n    perc_train = floor(distribution[0] * 0.01*train.shape[0])\n    perc_dev = perc_train + floor(distribution[1] * 0.01*train.shape[0])\n    perc_test = perc_dev + floor(distribution[2] * 0.01*train.shape[0])\n    train_feature = train[0:perc_train,1:]\n    train_label =  train[0:perc_train,0]\n    \n    dev_feature = train[perc_train:perc_dev,1:]\n    dev_label =  train[perc_train:perc_dev,0]\n    \n    test_feature = train[perc_dev:perc_test,1:]\n    test_label =  train[perc_dev:perc_test,0]\n    \n    return train_feature\/255, train_label, dev_feature\/255, dev_label, test_feature\/255, test_label","7d42d749":"def one_hot_encoding(label):\n    ### ONE HOT ENCODING OF LABEL\n    no_of_class = np.unique(label).shape[0]\n    enc_labels = np.zeros((label.shape[0],no_of_class))\n    for index in range(label.shape[0]):\n        enc_labels[index,label[index]] = 1\n    return enc_labels","35f8eed7":"def de_encoding(prediction):\n    ### DE CODING LABEL\n    predict = np.zeros((prediction.shape[0]))\n    for index in range(prediction.shape[0]):\n        predict[index] = np.argmax(prediction[index])\n    return predict","a01929b0":"def change_to_image(in_feature):\n    ### CHANGING ARRAY TO IMAGE\n    ### RESHAPING ARRAY FROM (,784) TO (1,28,28,1)\n    feature = np.zeros(shape = (in_feature.shape[0],size_of_img[0],size_of_img[1], 1))\n    for image_index in range(in_feature.shape[0]):\n        feature[image_index] = (in_feature[image_index]).reshape(size_of_img[0],size_of_img[1], 1)\n    return feature","e7c5fb9a":"\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"): \n    ### CREATING CSV FILE\n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","59e36d61":"def acc(y_test,prediction):\n\n    ### PRINTING ACCURACY OF PREDICTION\n    ### RECALL\n    ### PRECISION\n    ### CLASIFICATION REPORT\n    ### CONFUSION MATRIX\n    cm = confusion_matrix(y_test, prediction)\n    recall = np.diag(cm) \/ np.sum(cm, axis = 1)\n    precision = np.diag(cm) \/ np.sum(cm, axis = 0)\n    \n    print ('Recall:', recall)\n    print ('Precision:', precision)\n    print ('\\n clasification report:\\n', classification_report(y_test,prediction))\n    print ('\\n confussion matrix:\\n',confusion_matrix(y_test, prediction))\n    \n    ax = sns.heatmap(confusion_matrix(y_test, prediction),linewidths= 0.5,cmap=\"YlGnBu\")","3090f114":"train_feature, train_labels, dev_feature, dev_labels,_,__ = dataset_distribution(train_csv,[80,20,0])\ntrain_feature.shape, train_labels.shape, dev_feature.shape, dev_labels.shape","79a4699a":"train_image = change_to_image(train_feature)\ndev_image     = change_to_image(dev_feature)","fd5e7f08":"train_label = one_hot_encoding(train_labels)\ndev_label = one_hot_encoding(dev_labels)","f2671a6a":"no_of_class  = 10\nno_of_class,train_image.shape,dev_image.shape","5beec323":"perc = 85\n#no_of_image_in_train = floor(train_csv.shape[0]*perc*0.01)\nno_of_class = 10\nprint(\"no_of_class : \",no_of_class)\nprint(\"train_image.shape : \",train_image.shape)\nprint(\"dev_image.shape : \",dev_image.shape)\nprint(\"train_label.shape : \",train_label.shape)\nprint(\"dev_label.shape: \",dev_label.shape)\nprint(\"train_feature: \",train_feature.shape)\nprint(\"train_labels.shape : \",train_labels.shape)\n","005214ae":"gen = ImageDataGenerator(\n        rotation_range=3,  \n        zoom_range = 0.070,  \n        width_shift_range=0.01, \n        height_shift_range=0.01)\n","abb066c8":"\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size = 3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size = 3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size = 3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.44))\n\nmodel.add(Conv2D(256, kernel_size = 4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(0.65))\nmodel.add(Dense(10, activation='softmax'))\n\n\n# COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","2ecf0708":"model.summary()","8078356f":"# DECREASE LEARNING RATE EACH EPOCH\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n# TRAIN NETWORKS\nepochs = 13\nX_train2, X_val2, Y_train2, Y_val2 = train_image,dev_image, train_label, dev_label\nhistory = model.fit_generator(gen.flow(X_train2,Y_train2, batch_size=100),\n    epochs = epochs, steps_per_epoch = X_train2.shape[0]\/\/100,  \n    validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=2)\n","11e30cc8":"name_title = ['Loss','Accuracy']\nfor i in range(0,2):\n    ax=fig.add_subplot(8,8,i+1)\n    plt.plot(history.history[list(history.history.keys())[i]], label = list(history.history.keys())[i] )\n    plt.plot(history.history[list(history.history.keys())[i+2]],label = list(history.history.keys())[i+2] )\n    plt.xlabel('Epochs', fontsize=18)\n    plt.ylabel(name_title[i], fontsize=18)\n    plt.legend()\n    plt.show()","38d9dca5":"X_test = dev_image\ncnn_pred = model.predict_proba(change_to_image(np.array(test_csv)\/255))\nresults = model.predict(X_test)\nresults = np.argmax(results,axis = 1)\nacc(dev_labels,results)","cc46dcae":"create_download_link(pd.DataFrame(cnn_pred))","effae958":"### UNDERSTANDING DATA","2d9427ce":"### MODEL","f49e0c49":"### DATA AUGMENTATION","f5e130c6":"### PLOTING IMAGES","bf1d010a":"### DATA PREPROCESSING","10d914fd":"### GETTING DATA"}}