{"cell_type":{"f893cd52":"code","75287fe9":"code","3710937e":"code","b24b8398":"code","76080f9e":"code","030eb9b6":"code","36df86fc":"code","9e8aa1e0":"code","e373b28f":"code","fe5a6522":"code","0d3283e1":"code","fe69c444":"code","d2f2baa9":"code","c0a1508b":"code","dd1aa850":"code","15fc6f71":"code","0257ccc0":"code","6928a16a":"code","0f8091fd":"code","11d01cfb":"code","82c3bf07":"code","a9dc42f9":"code","b2df34de":"code","b2d56dec":"code","71fbe986":"code","f80ff0db":"code","4c15e6cf":"code","b0496c60":"code","602ca96e":"code","8fdfd161":"code","cfdd76e3":"code","07ba9dd6":"code","a31cc7d3":"code","bf7366cf":"code","33f6fcab":"code","bcfa7af4":"code","125da5a9":"code","3a52773b":"code","f0454973":"code","a08c7999":"code","dea8bd68":"code","504e207e":"code","7ddd6591":"code","923971b3":"code","b8f46478":"code","b79701b1":"code","adaee031":"code","abf2ce4a":"code","0434b389":"code","f21b96e6":"code","96587e2c":"code","2856cf55":"code","4ad1ec02":"code","71c9982c":"code","4b6da78e":"code","8c1b0ccf":"code","8bb3b072":"code","e6430d91":"code","36b0c6c5":"code","55b72104":"code","51acd213":"code","6b9c11e9":"code","8d7c49aa":"code","f626490d":"markdown","e443c82c":"markdown","bf247417":"markdown","44ea6c85":"markdown","1198473f":"markdown","0a6920e7":"markdown","980ce29d":"markdown","e0df8603":"markdown","30fc1678":"markdown","21eda5b5":"markdown","e3cae035":"markdown","19b1a760":"markdown","608583e6":"markdown","a2cc1018":"markdown","2077347a":"markdown","7025e76c":"markdown","9f2a4ea0":"markdown","223f6dd9":"markdown","1d1107e3":"markdown","9b859cc1":"markdown","e80f8f72":"markdown","3f88b830":"markdown","fdb00473":"markdown","e64db505":"markdown","f862d7bd":"markdown","1ee3a4f1":"markdown","d1eb7c50":"markdown","c2bc4008":"markdown","b5dcf3bf":"markdown","c6478d92":"markdown","ffca44ee":"markdown","18953056":"markdown","99b02f36":"markdown","2641a1c8":"markdown","84acb9b0":"markdown","36280617":"markdown","dd0a59bc":"markdown","d022ae5b":"markdown","1d48fda1":"markdown","19f07714":"markdown","43970e58":"markdown","d9d7fc5a":"markdown","7f121654":"markdown","c5e81153":"markdown","f997c5b6":"markdown","c9a5140e":"markdown","3afd3c8a":"markdown","114cc868":"markdown","8510fe59":"markdown","f5329443":"markdown","b6f656a9":"markdown","f5101428":"markdown","dd9f4c79":"markdown","ba4c3a88":"markdown","ccc17040":"markdown","bcf5cdb7":"markdown","cb71df1d":"markdown","e03cc567":"markdown","5756bb44":"markdown","069ae6a2":"markdown","f4bcc852":"markdown","921ad1a8":"markdown","af8cc42c":"markdown","faa889da":"markdown","18a2abc8":"markdown","68868002":"markdown"},"source":{"f893cd52":"import os\nimport warnings\nwarnings.simplefilter(action = 'ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\ndef ignore_warn(*args, **kwargs):\n    pass\n\nwarnings.warn = ignore_warn #getting rid of warning from sklearn and seaborn\n\n#Packages\nfrom scipy.stats import skew, norm, probplot, boxcox, f_oneway\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nfrom collections import Counter\nfrom sklearn.base import TransformerMixin \n%matplotlib inline","75287fe9":"red_df = pd.read_csv('..\/input\/winequality-red.csv', sep = ';')\nwhite_df = pd.read_csv('..\/input\/winequality-white.csv', sep = ';')\n#creating a wine_color column to distinguish between type\nred_df['wine_color'] = 'red' \nwhite_df['wine_color'] = 'white' ","3710937e":"red_df.head()","b24b8398":"white_df.head()","76080f9e":"white_df['label'] = white_df['quality'].apply(lambda x: 1 if x <= 5 else 2 if x <= 7 else 3)\nred_df['label'] = red_df['quality'].apply(lambda x: 1 if x <= 5 else 2 if x <= 7 else 3)\n\nwine = pd.concat([red_df, white_df], axis = 0) #Combing\n\n#shuffle data for randomization of data points\nwine = wine.sample(frac = 1, random_state = 77).reset_index(drop = True)","030eb9b6":"wine.isnull().sum()","36df86fc":"class null_cleaner(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        fills missing values:\n        -If the column is dtype object they are imputed with the most frequent\n         value within the column\n        -The other columns with data types are imputed with the mean\n         of the corresponding column\n        \"\"\"\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\nwine = null_cleaner().fit_transform(wine)","9e8aa1e0":"wine.describe().transpose()","e373b28f":"wine.info()","fe5a6522":"Counter(wine['quality'])","0d3283e1":"Counter(wine['label'])","fe69c444":"sns.countplot(wine['wine_color'], palette = ['red', 'white'], edgecolor = 'black')\nplt.show()","d2f2baa9":"fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize= (10,10), sharey = True)\nplt.rc('font', size = 12)\ntitle = fig.suptitle('Wine Type vs Quality', fontsize = 15, y = 1.05)\n\naxes[0, 0].set_title('Red Wine')\naxes[0, 0].set_xlabel('Quality')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].bar(list(red_df['quality'].value_counts().index), \n               list(red_df['quality'].value_counts().values),\n               color = 'red', edgecolor = 'black')\n\naxes[0, 1].set_title('White Wine')\naxes[0, 1].set_xlabel('Quality')\naxes[0, 1].bar(list(white_df['quality'].value_counts().index), \n               list(white_df['quality'].value_counts().values),\n               color = 'white', edgecolor = 'black')\n\naxes[1, 0].set_xlabel('Quality Label')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].bar(list(red_df['label'].value_counts().index), \n               list(red_df['label'].value_counts().values),\n               color = 'red', edgecolor = 'black')\n\naxes[1, 1].set_xlabel('Quality Label')\naxes[1, 1].bar(list(white_df['label'].value_counts().index), \n               list(white_df['label'].value_counts().values),\n               color = 'white', edgecolor = 'black')\n\nplt.tight_layout()","c0a1508b":"fig, axes = plt.subplots(10, 2, figsize = (12,17), sharex = True)\n\nax = sns.boxplot(x=\"quality\", y=\"fixed acidity\", data=white_df, orient='v', \n    ax=axes[0, 0])\nax = sns.boxplot(x=\"quality\", y=\"fixed acidity\", data=red_df, orient='v', \n    ax=axes[0, 1])\nax = sns.boxplot(x=\"quality\", y=\"volatile acidity\", data=white_df, orient='v', \n    ax=axes[1, 0])\nax = sns.boxplot(x=\"quality\", y=\"volatile acidity\", data=red_df, orient='v', \n    ax=axes[1, 1])\nax = sns.boxplot(x=\"quality\", y=\"citric acid\", data=white_df, orient='v', \n    ax=axes[2, 0])\nax = sns.boxplot(x=\"quality\", y=\"citric acid\", data=red_df, orient='v', \n    ax=axes[2, 1])\nax = sns.boxplot(x=\"quality\", y=\"residual sugar\", data=white_df, orient='v', \n    ax=axes[3, 0])\nax = sns.boxplot(x=\"quality\", y=\"residual sugar\", data=red_df, orient='v', \n    ax=axes[3, 1])\nax = sns.boxplot(x=\"quality\", y=\"chlorides\", data=white_df, orient='v', \n    ax=axes[4, 0])\nax = sns.boxplot(x=\"quality\", y=\"chlorides\", data=red_df, orient='v', \n    ax=axes[4, 1])\nax = sns.boxplot(x=\"quality\", y=\"free sulfur dioxide\", data=white_df, orient='v', \n    ax=axes[5, 0])\nax = sns.boxplot(x=\"quality\", y=\"free sulfur dioxide\", data=red_df, orient='v', \n    ax=axes[5, 1])\nax = sns.boxplot(x=\"quality\", y=\"density\", data=white_df, orient='v', \n    ax=axes[6, 0])\nax = sns.boxplot(x=\"quality\", y=\"density\", data=red_df, orient='v', \n    ax=axes[6, 1])\nax = sns.boxplot(x=\"quality\", y=\"pH\", data=white_df, orient='v', \n    ax=axes[7, 0])\nax = sns.boxplot(x=\"quality\", y=\"pH\", data=red_df, orient='v', \n    ax=axes[7, 1])\nax = sns.boxplot(x=\"quality\", y=\"sulphates\", data=white_df, orient='v', \n    ax=axes[8, 0])\nax = sns.boxplot(x=\"quality\", y=\"sulphates\", data=red_df, orient='v', \n    ax=axes[8, 1])\nax = sns.boxplot(x=\"quality\", y=\"alcohol\", data=white_df, orient='v', \n    ax=axes[9, 0])\nax = sns.boxplot(x=\"quality\", y=\"alcohol\", data=red_df, orient='v', \n    ax=axes[9, 1])\n\naxes[0,0].title.set_text('White')\naxes[0,1].title.set_text('Red')\nplt.tight_layout()","dd1aa850":"pd.concat([red_df.describe().T, white_df.describe().T], axis = 1, keys = ['Red Wine Statistical Description','White Wine Statistical Description'])","15fc6f71":"bad_q = wine[wine['label'] == 1].describe()\navg_q = wine[wine['label'] == 2].describe()\nhigh_q = wine[wine['label'] == 3].describe()\npd.concat([bad_q, avg_q, high_q], axis = 0, keys = ['Bad Quality Wine', 'Average Quality Wine', 'High Quality Wine'])","0257ccc0":"def type_h_testing(feature):\n    F, p = f_oneway(red_df[feature],\n                    white_df[feature])\n    if p <= 0.05: # Standard Measure \n        result = 'Reject'\n    else:\n        result = 'Accept'\n    print('ANOVA test for {}:'.format(feature))\n    print('F Statistic: {:.2f} \\tp-value: {:.3f} \\tNull Hypothese: {}'.format(F, p, result))\n    \ndef quality_h_testing(feature):\n    F, p = f_oneway(wine[wine['label'] == 1][feature],\n                    wine[wine['label'] == 2][feature],\n                    wine[wine['label'] == 3][feature])\n    if p <= 0.05:\n        result = 'Reject'\n    else:\n        result = 'Accept'\n    print('ANOVA test for {}:'.format(feature))\n    print('F Statistic: {:.2f} \\tp-value: {:.3f} \\tNull Hypothesis: {}'.format(F, p, result))","6928a16a":"wine.head()","0f8091fd":"print('Anova Test for Types of Wine \\n')\nfor column in wine.drop(['wine_color', 'label', 'quality'], axis = 1).columns:\n    type_h_testing(column)","11d01cfb":"print('Anova Test for Types of Wine \\n')\nfor column in wine.drop(['wine_color', 'label', 'quality'], axis = 1).columns:\n    quality_h_testing(column)","82c3bf07":"wine.head()","a9dc42f9":"# re-shuffle data to randomize data points\nwine = wine.sample(frac = 1, random_state = 77).reset_index(drop = True)\ntype_encoder = LabelEncoder()\nwine['wine_color'] = type_encoder.fit_transform(wine['wine_color'].values)\n# 'white': 1, 'red': 0","b2df34de":"sns.set(font_scale=1.5)\ncorr = wine.corr().drop('label', axis = 1)\ntype_sorted = corr.sort_values('wine_color', ascending = False).keys()\ncorr_matrix = corr.loc[type_sorted, type_sorted] #sorts columns\nmask = np.zeros_like(corr_matrix, dtype = np.bool) \nmask[np.triu_indices_from(mask)] = True #mask for heatmap\nplt.figure(figsize = (16, 9))\nsns.heatmap(corr_matrix, cmap = sns.diverging_palette(h_neg = 220, h_pos = 10, s = 80, l = 60, as_cmap = True),\n            annot = True, mask = mask)\n\nplt.show()","b2d56dec":"g = sns.pairplot(data = wine, hue = 'wine_color', \n                 palette = 'Reds')\n\nfor i, j in zip(*np.triu_indices_from(g.axes, 1)):\n    g.axes[i, j].set_visible(False)\n\nfig = g.fig\nfig.suptitle('Wine Attributes Pairwise Plots by Types', fontsize = 40, y = 1.05)\n\nplt.show()","71fbe986":"corr = wine.corr().drop('quality', axis = 1)\nsort_ql = corr.sort_values('label', ascending = False).keys()\ncorr_matrix = corr.loc[sort_ql, sort_ql]\nmask = np.zeros_like(corr_matrix)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize = (16, 9))\nsns.heatmap(corr_matrix, cmap = sns.diverging_palette(h_neg = 220, h_pos = 10, s = 80, l =60, as_cmap = True),\n            annot = True, mask = mask)\n\nplt.show()","f80ff0db":"sns.set(font_scale = 1)\nfig, axes = plt.subplots(1, 2, figsize = (15,5))\nfig.suptitle('Wine Type, Quality, and Alcohol Content', y = 1.02)\n\nsns.boxplot(x = 'label', y = 'alcohol', data = wine, \n            hue = 'wine_color', palette = 'Reds', ax = axes[0])\nh, l = axes[0].get_legend_handles_labels()\naxes[0].set_xlabel(\"Wine Quality Label\")\naxes[0].set_ylabel(\"Wine Alcohol %\")\naxes[0].legend(h, ['White', 'Red'], title = 'Wine Color')\n\n\nsns.boxplot(x = 'quality', y = 'alcohol', data = wine, \n            hue = 'wine_color', palette = 'Reds', ax = axes[1])\nh2, l2 = axes[1].get_legend_handles_labels()\naxes[1].legend(h2, ['White', 'Red'], title = 'Wine Color')\naxes[1].set_xlabel(\"Wine Quality\")\naxes[1].set_ylabel(\"Wine Alcohol %\")\n\nplt.show()","4c15e6cf":"f, axes = plt.subplots(1, 2, figsize = (15, 5))\nf.suptitle('Wine Type, Quality, and Acidity', y = 1.02)\n\nsns.violinplot(x = 'label', y = 'volatile acidity', data = wine, \n               hue = 'wine_color', palette = 'Reds', ax = axes[0],\n               split = True,  inner = 'quart')\nh, l = axes[0].get_legend_handles_labels()\naxes[0].set_xlabel('Wine Quality Label')\naxes[0].set_ylabel('Wine Fixed Acidity')\naxes[0].legend(h, ['White', 'Red'], title = 'Wine Color')\n\nsns.violinplot(x = 'quality', y = 'volatile acidity', data = wine, \n               hue = 'wine_color', palette = 'Reds', ax = axes[1], \n               split = True, inner = 'quart')\nh2, l2 = axes[1].get_legend_handles_labels()\naxes[1].set_xlabel('Wine Quality')\naxes[1].set_ylabel('Wine Fixed Acidity')\naxes[1].legend(h, ['White', 'Red'], title = 'Wine Color')\n\nplt.show()","b0496c60":"g = sns.FacetGrid(wine, col = 'wine_color', hue = 'label', aspect = 1.5)\ng = g.map(plt.scatter, 'volatile acidity', 'alcohol' ,alpha = 0.8,\n          edgecolor = 'white')\naxes = g.axes.flatten()\naxes[0].set_title('Red Wine')\naxes[1].set_title('White Wine')\nfig = g.fig\nfig.suptitle('Wine Color, Quality, Alcohol, and volatile Acidity', y = 1.08)\nl = g.add_legend(title = 'Wine Quality Class')\n\n\ng = sns.FacetGrid(wine, col = 'wine_color', hue = 'label', aspect = 1.5)\ng = g.map(plt.scatter, 'volatile acidity', 'total sulfur dioxide',\n          edgecolor = 'white', alpha = 0.8)\naxes = g.axes.flatten()\naxes[0].set_title('Red Wine')\naxes[1].set_title('White Wine')\nl = g.add_legend(title = 'Wine Quality Class')\n","602ca96e":"fig, axes = plt.subplots(6, 2, figsize = (12,17))\n\nax = sns.distplot(wine['volatile acidity'], ax=axes[0, 0])\nax = sns.distplot(wine['fixed acidity'], ax=axes[1, 0])\nax = sns.distplot(wine['citric acid'], ax=axes[2, 0])\nax = sns.distplot(wine['residual sugar'], ax=axes[3, 0])\nax = sns.distplot(wine['chlorides'], ax=axes[4, 0])\nax = sns.distplot(wine['free sulfur dioxide'], ax=axes[5, 0])\nax = sns.distplot(wine['total sulfur dioxide'], ax=axes[0, 1])\nax = sns.distplot(wine['density'], ax=axes[1, 1])\nax = sns.distplot(wine['pH'], ax=axes[2, 1])\nax = sns.distplot(wine['sulphates'], ax=axes[3, 1])\nax = sns.distplot(wine['alcohol'], ax=axes[4, 1])\nax = sns.distplot(wine['quality'], ax=axes[5, 1], kde = False)\n\nplt.tight_layout()","8fdfd161":"numeric_features = list(wine.dtypes[(wine.dtypes != \"str\") & (wine.dtypes !='object')].index)\nnumeric_features.remove('wine_color')\n\n#using scipy.stats.skew to measure skew value\nskewed_features = wine[numeric_features].apply(lambda x: skew(x))\nskew_values = pd.DataFrame({'skew' :skewed_features})\n# Using a skew criteria of 0.7 or -0.7\nskew_values = skew_values[np.absolute(skew_values) > 0.7].dropna()\nprint('There are {} skewed parameters, here are the largest:'.format(len(skew_values.index)))\nprint(skew_values.sort_values(by = 'skew', ascending = False))","cfdd76e3":"maxlog = {}\n\n#no constant since dealing with right skewed parmaters\nfor feature in skew_values.index:\n    wine[feature], maxlog[feature] = boxcox(wine[feature])\n\n#skewness check after data transform\nskew_check = wine[skew_values.index].apply(lambda x: skew(x))\nskew_check = pd.DataFrame({'new skew': skew_check})\n\ndisplay(pd.concat([skew_values, skew_check], axis = 1).sort_values(by = 'skew', ascending = False))","07ba9dd6":"def QQ_plot(data, measure):\n    fig = plt.figure(figsize = (12, 4))\n    \n    #grabbing mu and sigma after fitting the data to a normal distribution\n    (mu, sigma) = norm.fit(data) \n    \n    fig1 = fig.add_subplot(1 ,2, 1)\n    sns.distplot(data, fit = norm)\n    fig1.set_title(measure + ' Distribution (mu = {:.2f}, sigma = {:.2f})'.format(mu, sigma), loc = 'center')\n    fig1.set_xlabel(measure)\n    fig1.set_ylabel('Frequency')\n    \n    #qq plot\n    fig2 = fig.add_subplot(1, 2, 2)\n    res = probplot(data, plot = fig2)\n    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f}, kurtosis: {:.6f} )'.format(data.skew(), data.kurt()), loc='center')\n    \n    plt.tight_layout()\n    \n\nfor feature in skew_values.index:\n    QQ_plot(wine[feature], str(feature))\n    ","a31cc7d3":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\n\ncols = wine.columns.str.replace(' ', '_')\ndf = wine.copy()\ndf.columns = cols\ncols = list(cols.drop(['wine_color', 'label', 'quality']))\n\ndef VRF(predict, data, y):\n\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(data), columns = cols)\n    features = '+'.join(cols)\n    df['label'] = y.values\n    \n    #grabbing y and X based off regression\n    y, X = dmatrices(predict + ' ~' + features, data = df, return_type = 'dataframe')\n    \n    #Vif factor calculations for each X\n    vif = pd.DataFrame()\n    vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['features'] = X.columns\n    \n    #VIF inspection\n    display(vif.sort_values('VIF Factor', ascending = False))\n    return vif\n\nvif = VRF('label', df.loc[:, cols], wine['label'])","bf7366cf":"cols = wine.columns.str.replace(' ', '_')\ndf = wine.copy()\ndf.columns = cols\n\n# Remove the higest correlations and run a multiple regression\ncols = list(cols.drop(['wine_color', 'label', 'quality', 'density']))\n\nvif = VRF('label', df.loc[:, cols], wine['label'])\n\ndel df, vif","33f6fcab":"class select_features(object):\n    def __init__(self, select_cols):\n        self.select_cols = select_cols\n        \n    def fit(self, X, y):\n        pass\n    \n    def transform(self, X):\n        return X.loc[:, self.select_cols]\n    \n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        df = self.transform(X)\n        return df\n    \n    def __getitem__(self, x):\n        return self.X[x], self.Y[x]","bcfa7af4":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\nfrom sklearn.decomposition import PCA\n\ndef pca_analysis(df, y_train, feat):\n    scale = StandardScaler()\n    df = pd.DataFrame(scale.fit_transform(df), index=df.index)\n    pca_all = PCA(random_state=101, whiten=True).fit(df)\n\n    my_color=y_train\n\n    # Store results of PCA in a data frame\n    result=pd.DataFrame(pca_all.transform(df), columns=['PCA%i' % i for i in range(df.shape[1])], index=df.index)\n\n    # Plot initialisation\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result['PCA0'], result['PCA1'], result['PCA2'], c=my_color, cmap=\"Set2_r\", s=60,\n               edgecolor = 'white')\n\n    # make simple, bare axis lines through space:\n    xAxisLine = ((min(result['PCA0']), max(result['PCA0'])), (0, 0), (0,0))\n    ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n    yAxisLine = ((0, 0), (min(result['PCA1']), max(result['PCA1'])), (0,0))\n    ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n    zAxisLine = ((0, 0), (0,0), (min(result['PCA2']), max(result['PCA2'])))\n    ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n\n    # label the axes\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n    ax.set_zlabel(\"PC3\")\n    ax.set_title(\"PCA on the Wines dataset for \" + (feat))\n    plt.show()\n\n    X_train , X_test, y, y_test = train_test_split(df , y_train, test_size=0.3, random_state=0)\n\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNC = KNC.fit(X_train, y)\n    print('KNeighbors Classifier Training Accuracy: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train))))\n    y_pred = KNC.predict(X_test)\n    print('KNeighbors Classifier Test Accuracy: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n\n    print('_' * 40)\n    print('\\nAccurance on', feat, ' Prediction By Number of PCA COmponents:\\n')\n    AccPca = pd.DataFrame(columns=['Components', 'Var_ratio', 'Train_Acc', 'Test_Acc'])\n\n    for componets in np.arange(1, df.shape[1]):\n        variance_ratio = sum(pca_all.explained_variance_ratio_[:componets])*100\n        pca = PCA(n_components=componets, random_state=101, whiten=True)\n        X_train_pca = pca.fit_transform(X_train)\n        Components = X_train_pca.shape[1]\n        KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n        KNC = KNC.fit(X_train_pca, y)\n        Training_Accuracy = accuracy_score(y, KNC.predict(X_train_pca))\n        X_test_pca = pca.transform(X_test)\n        y_pred = KNC.predict(X_test_pca)\n        Test_Accuracy = accuracy_score(y_test, y_pred)\n        AccPca = AccPca.append(pd.DataFrame([(Components, variance_ratio, Training_Accuracy, Test_Accuracy)],\n                                            columns=['Components', 'Var_ratio', 'Train_Acc', 'Test_Acc']))#], axis=0)\n\n    AccPca.set_index('Components', inplace=True)\n    display(AccPca.sort_values(by='Test_Acc', ascending=False))\n\n#running wine color pca analysis\ncols = wine.columns\ncols = list(cols.drop(['wine_color', 'label', 'quality']))\npca_analysis(wine.loc[:, cols], wine['wine_color'], 'wine_color')\n\n#running wine quality pca analysis\ncols = wine.columns\ncols = list(cols.drop(['wine_color', 'label', 'quality']))\npca_analysis(wine.loc[:, cols], wine['label'], 'Quality')","125da5a9":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\ndef LDA_analysis(df, y_train, feat):\n    X_train , X_test, y, y_test = train_test_split(df , y_train, test_size=0.3, random_state=0)\n\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNC = KNC.fit(X_train, y)\n    print('KNC Training Accuracy: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train))))\n    y_pred = KNC.predict(X_test)\n    print('KNC Test Accuracy: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n    print('_' * 40)\n    print('\\nApply LDA:\\n')\n    lda = LDA(n_components=2, store_covariance=True)\n    X_train_lda = lda.fit_transform(X_train, y)\n    #X_train_lda = pd.DataFrame(X_train_lda)\n\n    print('Number of features after LDA:',X_train_lda.shape[1])\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNCr = KNC.fit(X_train_lda, y)\n    print('LR Training Accuracy With LDA: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train_lda))))\n    X_test_lda = lda.transform(X_test)\n    y_pred = KNC.predict(X_test_lda)\n    print('LR Test Accuracy With LDA: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n\n    if X_train_lda.shape[1]==1:\n        fig = plt.figure(figsize=(20,5))\n        fig.add_subplot(121)\n        plt.scatter(X_train_lda[y==0, 0], np.zeros((len(X_train_lda[y==0, 0]),1)), color='red', alpha=0.1)\n        plt.scatter(X_train_lda[y==1, 0], np.zeros((len(X_train_lda[y==1, 0]),1)), color='blue', alpha=0.1)\n        plt.title('LDA on Training Data Set')\n        plt.xlabel('LDA')\n        fig.add_subplot(122)\n        plt.scatter(X_test_lda[y_test==0, 0], np.zeros((len(X_test_lda[y_test==0, 0]),1)), color='red', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==1, 0], np.zeros((len(X_test_lda[y_test==1, 0]),1)), color='blue', alpha=0.1)\n        plt.title('LDA on Test Data Set')\n        plt.xlabel('LDA')\n    else:\n        fig = plt.figure(figsize=(20,5))\n        fig.add_subplot(121)\n        plt.scatter(X_train_lda[y==0, 0], X_train_lda[y==0, 1], color='black', alpha=0.1)\n        plt.scatter(X_train_lda[y==1, 0], X_train_lda[y==1, 1], color='yellow', alpha=0.1)\n        plt.scatter(X_train_lda[y==2, 0], X_train_lda[y==2, 1], color='red', alpha=0.1)\n        plt.title('LDA on Training Data Set')\n        plt.xlabel('LDA')\n        fig.add_subplot(122)\n        plt.scatter(X_test_lda[y_test==0, 0], X_test_lda[y_test==0, 1], color='black', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==1, 0], X_test_lda[y_test==1, 1], color='yellow', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==2, 0], X_test_lda[y_test==2, 1], color='red', alpha=0.1)\n        plt.title('LDA on Test Data Set')\n        plt.xlabel('LDA')\n\n    plt.show()\n    \ncols = wine.columns\ncols = list(cols.drop(['wine_color', 'label', 'quality']))\nLDA_analysis(wine.loc[:, cols], wine['wine_color'], 'Type')\n\ncols = wine.columns\ncols = list(cols.drop(['wine_color', 'label', 'quality']))\nLDA_analysis(wine.loc[:, cols], wine['label'], 'Quality')","3a52773b":"cols = wine.columns\ncols = list(cols.drop(['wine_color','label']))\ny = wine['wine_color']\nX_train, X_test, y_train, y_test = train_test_split(wine.loc[:, cols], y, test_size=0.3, random_state = 77)","f0454973":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve ","a08c7999":"clf = Pipeline([\n    ('pca', PCA(random_state = 77)),\n    ('clf', LogisticRegression(random_state = 77))])\n\n\n#dictionary of parameters to tune\nn_components = [10, 12]\nwhiten = [True]\nC = [0.003, 0.009, 0.01, 0.1]\ntol = [0.001, 0.0001, 0.01]\n\nparam_grid =\\\n    [{'clf__C': C\n     ,'clf__solver': ['liblinear', 'saga'] \n     ,'clf__penalty': ['l1', 'l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': [None, 'balanced']\n     ,'pca__n_components' : n_components\n     ,'pca__whiten' : whiten\n},\n    {'clf__C': C\n     ,'clf__max_iter': [3, 9, 2, 7, 4]\n     ,'clf__solver': ['newton-cg', 'sag', 'lbfgs']\n     ,'clf__penalty': ['l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': [None, 'balanced'] \n     ,'pca__n_components' : n_components\n     ,'pca__whiten' : whiten\n}]\n\n#Grid search allows us to figure out the best model given some varying parameters\ngs = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'accuracy', cv = 5, verbose = 1, n_jobs = -1)\nLR = Pipeline([\n        #('sel', select_fetaures(select_cols=list(shadow))),\n        ('scl', StandardScaler()),\n        #('lda', LDA(store_covariance=True)),\n        ('gs', gs)]) \n\nLR.fit(X_train,y_train)\npredictions = LR.predict(X_test)","dea8bd68":"confusion = confusion_matrix(y_test, predictions)\nticklabels = ['False', 'True']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\nprint(classification_report(y_test, predictions))\n\nauc_score = auc(roc_curve(y_test, predictions)[0],roc_curve(y_test, predictions)[1])\nprint('AUC score: {}'.format(auc_score))","504e207e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)\n\nwtp_dnn_model = Sequential()\nwtp_dnn_model.add(Dense(64, activation = 'relu', input_shape = (12,)))\nwtp_dnn_model.add(Dropout(rate = 0.3))\nwtp_dnn_model.add(Dense(32, activation = 'relu'))\nwtp_dnn_model.add(Dense(16, activation = 'relu'))\nwtp_dnn_model.add(Dense(1, activation = 'sigmoid'))\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n\nwtp_dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = wtp_dnn_model.fit(X_train, y_train, epochs=55, batch_size=50, \n                            shuffle=True, validation_split=0.2, verbose=1, \n                            callbacks = [early_stop])\n\nwtp_dnn_ypred = wtp_dnn_model.predict_classes(X_test)","7ddd6591":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\nt = f.suptitle('Deep Neural Net Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepochs = np.arange(1, len(history.history['accuracy']) + 1)\nax1.plot(epochs, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(epochs)\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epochs, history.history['loss'], label='Train Loss')\nax2.plot(epochs, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(epochs)\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")\n\nplt.tight_layout()","923971b3":"confusion = confusion_matrix(y_test, wtp_dnn_ypred)\nticklabels = ['False', 'True']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\nprint(classification_report(y_test, wtp_dnn_ypred))\n\nauc_score = auc(roc_curve(y_test, wtp_dnn_ypred)[0], roc_curve(y_test, wtp_dnn_ypred)[1])\nprint('AUC score: {}'.format(auc_score))","b8f46478":"X_train, X_test, y_train, y_test = train_test_split(wine.drop(['label', 'quality'], axis = 1), wine['label'], test_size=0.30, random_state=77)","b79701b1":"from sklearn.tree import DecisionTreeClassifier\n\n#For model evaluation\nmodel = {}\n\nclf = Pipeline([('clf', DecisionTreeClassifier(random_state = 77))])\n\n# parameter dictionary for tuning\ncriterion = ['gini', 'entropy']\nsplitter = ['best']\nmax_depth = [8, 9 ,10, 11, 15]\nmin_samples_leaf = [2, 3, 5]\nclass_weight = ['balanced', None]\n\nparam_grid =\\\n    [{ 'clf__class_weight': class_weight\n      ,'clf__criterion': criterion\n      ,'clf__splitter': splitter\n      ,'clf__max_depth': max_depth\n      ,'clf__min_samples_leaf': min_samples_leaf\n}]\n\ngs = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'accuracy',\n                  cv = 5, verbose = 1, n_jobs = -1)\n\nDT = Pipeline([\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nDT.fit(X_train, y_train)\nDT_predictions = DT.predict(X_test)\n\n#For model evaulation later\nmodel['DT'] = DT_predictions","adaee031":"print(classification_report(y_test, DT_predictions))\nconfusion = confusion_matrix(y_test, DT_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, DT.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))","abf2ce4a":"from graphviz import Source\nfrom sklearn import tree\nfrom IPython.display import Image\n\ndt = gs.best_estimator_.get_params()['clf']\ndt.fit(X_train, y_train)\ntarget_names = ['Bad', 'Average', 'High']\n\nmax_depth = 10\ngraph = Source(tree.export_graphviz(dt, out_file = None, class_names = target_names, filled = True,\n                                 rounded= True, special_characters = False, feature_names = cols, \n                                 max_depth = max_depth))\npng_data = graph.pipe(format = 'png')\nwith open('dtree_structure.png','wb') as f:\n    f.write(png_data)\n    \nImage(png_data)","0434b389":"from sklearn.tree import DecisionTreeClassifier\n\n#For model evaluation\nmodel = {}\n\nclf = Pipeline([('clf', DecisionTreeClassifier(random_state = 77))])\n\n# parameter dictionary for tuning\ncriterion = ['gini', 'entropy']\nsplitter = ['best']\nmax_depth = [8, 9 ,10, 11, 15]\nmin_samples_leaf = [2, 3, 5]\nclass_weight = ['balanced', None]\n\nparam_grid =\\\n    [{ 'clf__class_weight': class_weight\n      ,'clf__criterion': criterion\n      ,'clf__splitter': splitter\n      ,'clf__max_depth': max_depth\n      ,'clf__min_samples_leaf': min_samples_leaf\n}]\n\ngs = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'accuracy',\n                  cv = 5, verbose = 1, n_jobs = -1)\n\nDT = Pipeline([\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nDT.fit(X_train, y_train)\nDT_predictions = DT.predict(X_test)\n\n#For model evaulation later\nmodel['DT'] = DT_predictions","f21b96e6":"print(classification_report(y_test, RF_predictions))\nconfusion = confusion_matrix(y_test, RF_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, RF.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))\nprint('Accuracy difference in RFC vs DTC: {}'.format(accuracy_score(y_test, RF_predictions) - accuracy_score(y_test, DT_predictions)))\nprint('AUC difference in RFC vs DTC: {}'.format(auc_score - roc_auc_score(y_test, DT.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')))","96587e2c":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', KNeighborsClassifier())])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nn_components= [n_comp_base - 5, n_comp_base - 4, n_comp_base - 3, n_comp_base - 2] \nwhiten = [True, False]\n\nparam_grid =\\\n    [{'clf__n_neighbors': [10, 11, 12, 13] \n     ,'clf__weights': ['distance'] \n     ,'clf__algorithm' : ['ball_tree'] #, 'brute', 'auto',  'kd_tree', 'brute']\n     ,'clf__leaf_size': [12, 11, 13]\n     ,'clf__p': [1] \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nKNNC = Pipeline([\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nKNNC.fit(X_train,y_train)\n\nKNNC_predictions = KNNC.predict(X_test)\n\n#For model evaulation later\nmodel['KNNC'] = KNNC_predictions","2856cf55":"print(classification_report(y_test, KNNC_predictions))\nconfusion = confusion_matrix(y_test, KNNC_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, KNNC.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))\naccuracy_score(y_test, KNNC_predictions)","4ad1ec02":"from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n\nclf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', GradientBoostingClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\n#cv=None, dual=False,  scoring=None, refit=True,  multi_class='ovr'\nn_components= [n_comp_base - 5, n_comp_base - 4, n_comp_base - 3, n_comp_base - 2] \nwhiten = [True, False]\nlearning_rate =  [1e-02] #, 5e-03, 2e-02]\nn_estimators= [400]\nmax_depth = [10]\nn_comp = [2, 3, 4, 5]\n\nparam_grid =\\\n    [{'clf__learning_rate': learning_rate\n     ,'clf__max_depth': max_depth\n     ,'clf__n_estimators' : n_estimators \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nGBC = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ])  \n\nGBC.fit(X_train,y_train)\n\nGBC_predictions = GBC.predict(X_test)\n\n\n#For model evaulation later\nmodel['GBC'] = GBC_predictions","71c9982c":"print(classification_report(y_test, GBC_predictions))\nconfusion = confusion_matrix(y_test, GBC_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, GBC.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))","4b6da78e":"from sklearn.ensemble import AdaBoostClassifier\n\nclf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', AdaBoostClassifier(random_state=101))])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nn_components= [n_comp_base - 5, n_comp_base - 4, n_comp_base - 3, n_comp_base - 2] \nwhiten = [True, False]\nn_comp = [2, 3, 4, 5]\n\nparam_grid =\\\n    [{'clf__learning_rate': [2e-01, 15e-02]\n     ,'clf__n_estimators': [500, 600, 700] \n     ,'clf__algorithm' : ['SAMME.R'] # 'SAMME'\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     #,'lda__n_components': n_comp\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nADAB = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        #('lda', LDA(store_covariance=True)),\n        ('gs', gs)\n ])  \n\nADAB.fit(X_train,y_train)\n\nADAB_predictions = ADAB.predict(X_test)\n\n#For model evaulation later\nmodel['ADAB'] = ADAB_predictions","8c1b0ccf":"print(classification_report(y_test, ADAB_predictions))\nconfusion = confusion_matrix(y_test, ADAB_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, ADAB.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))","8bb3b072":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', LogisticRegression(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\n\nn_components= [n_comp_base - 5, n_comp_base - 4, n_comp_base - 3, n_comp_base - 2] \nwhiten = [True, False]\nC =  [1.0] #, 1e-06, 5e-07, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 10.0, 100.0, 1000.0]\ntol = [1e-06] #, 5e-07, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n\nparam_grid =\\\n    [{'clf__C': C\n     ,'clf__solver': ['liblinear', 'saga'] \n     ,'clf__penalty': ['l1', 'l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': ['balanced']\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n},\n    {'clf__C': C\n     ,'clf__max_iter': [3, 9, 2, 7, 4]\n     ,'clf__solver': ['newton-cg', 'sag', 'lbfgs']\n     ,'clf__penalty': ['l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': ['balanced'] \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nLRC = Pipeline([\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ])  \n\nLRC.fit(X_train,y_train)\n\nLRC_predictions = LR.predict(X_test)\n\n#For model evaulation later\nmodel['LRC'] = LRC_predictions","e6430d91":"print(classification_report(y_test, LRC_predictions))\nconfusion = confusion_matrix(y_test, LRC_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')\n\n#'ovo' multiclass for the average of all possible pairwise combinations, insensitive to class imbalance\nauc_score = roc_auc_score(y_test, LRC.predict_proba(X_test), multi_class = 'ovo', average = 'weighted')\nprint('AUC score: {}'.format(auc_score))","36b0c6c5":"from sklearn.svm import LinearSVC\n\nclf = Pipeline([\n        ('pca', PCA(random_state = 101)),\n        ('clf', LinearSVC(random_state=101, multi_class='ovr', class_weight='balanced'))])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nn_components= [n_comp_base - 5, n_comp_base - 4, n_comp_base - 3, n_comp_base - 2] \nwhiten = [True, False]\nC =  [0.06, 0.08, 0.07] #, 1.0, 10.0, 100.0, 1000.0]\ntol = [1e-06]\nmax_iter = [10, 15, 9]\n\nparam_grid =\\\n    [{'clf__loss': ['hinge']\n     ,'clf__tol': tol\n     ,'clf__C': C\n     ,'clf__penalty': ['l2']\n     ,'clf__max_iter' : max_iter\n     ,'clf__dual' : [True]\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }\n    ,{'clf__loss': ['squared_hinge']\n     ,'clf__tol': tol\n     ,'clf__C': C\n     ,'clf__penalty': ['l2', 'l1']\n     ,'clf__max_iter' : max_iter\n     ,'clf__dual' : [False]\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nLSVC = Pipeline([\n        ('scl', StandardScaler()),\n        #('lda', LDA(n_components = 2, store_covariance=True)),\n        ('gs', gs)\n ])  \n\nLSVC.fit(X_train,y_train)\n\nLSVC_predictions = LSVC.predict(X_test)\n\n\n#For model evaulation later\nmodel['LSVC'] = LSVC_predictions","55b72104":"print(classification_report(y_test, LSVC_predictions))\nconfusion = confusion_matrix(y_test, LSVC_predictions)\nticklabels = ['1: Bad', '   2: Average', '   3: High']\nax = sns.heatmap(confusion, annot = True, cbar = False, fmt = 'g', \n                 yticklabels = ticklabels, xticklabels = ticklabels,\n                 cmap = 'OrRd', linecolor = 'black', linewidth = 1)\nax.set_xlabel('True\/Actual')\nax.set_ylabel('Predicted')","51acd213":"from sklearn.metrics import precision_score, recall_score, f1_score\n#DONT FORGET XG\n\nmodel_df = pd.DataFrame()\n\nfor key in model:\n    model_df['score'] = ['accuracy', 'recall', 'precision', 'f1']\n    \n    accuracy = accuracy_score(y_test, model[key])\n    recall = recall_score(y_test, model[key], average = 'weighted')\n    precision = precision_score(y_test, model[key], average = 'weighted')\n    f1 = f1_score(y_test, model[key], average = 'weighted')\n    \n    model_df[key] = [accuracy, recall, precision, f1]\n    \n    print('{}: \\\n        \\n Accuracy score: {} \\\n        \\n Recall score: {} \\\n        \\n Precision score: {} \\\n        \\n f1 score: {} \\\n        '.format(key, accuracy, recall, precision, f1))\n\nmodel_df = model_df.set_index('score')","6b9c11e9":"model_df.head()","8d7c49aa":"sclf = StackingClassifier(classifiers=[RF, GBC],\n                          use_probas=False,\n                          average_probas=False,\n                          use_features_in_secondary=False,\n                          meta_classifier= RF)\n\nsclf.fit(X_train,y_train)\n\nsclf_predictions = sclf.predict(X_test)","f626490d":"Lets compare the model metrics and then trying a staked model. ","e443c82c":"### Inferential Statistics\n\nThe general notion of inferential statistics is to draw inferences and propositions of a population using a data sample. The idea is to use statistical methods and models to draw statistical inferences from a given hypotheses. Each hypothesis consists of a null hypothesis and an alternative hypothesis. Based on statistical test results, if the result is statistically significant based on pre-set significance levels (e.g., if obtained p-value is less than 5% significance level), we reject the null hypothesis in favor of the alternative hypothesis. Otherwise, if the results is not statistically significant, we conclude that our null hypothesis was correct.\n\nA great statistical model to prove or disprove the difference in mean among subsets of data is to use the one-way ANOVA test. ANOVA stands for \u201canalysis of variance,\u201d which is a nifty statistical model and can be used to analyze statistically significant differences among means or averages of various groups. This is basically achieved using a statistical test that helps us determine whether or not the means of several groups are equal. \n\nThe alternative hypotheses, H<sub>A<\/sub>, tells us that there exists at least two group means that are statistically significantly different from each other. Usually the F-statistic and the associated p-value from it is used to determine the statistical significance. Typically a p-value less than 0.05 is taken to be a statistically significant result where we reject the null hypothesis in favor of the original. \n\nSo let's evaluate the hypotheses of each of our previous highlights through the statistical inference test","bf247417":"## Statistical Descriptions\n\nLet's view the statistical components and correlations of our data set for red and white wines.  ","44ea6c85":"#### Logistic Regression\n\nThis class implements regularized [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) using the 'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle both dense and sparse input. \n\n**Additional Parameters**\n - class_weight : dict or 'balanced', default: None\n   The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples \/ (n_classes * np.bincount(y))``.\n\n   For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. For example, our class 0 is 1.24 times more frequent than class 1. So you should increase the class_weight of class 1 relative to class 0, say {1: 0.6, 0: 0.4}. If the class_weight doesn't sum to 1, it will basically change the regularization parameter.\n\n   \"balanced\" basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\n   \n\n - warm_start : bool, default: False. Useless for liblinear solver.\n - ``'clf__multi_class' : ['ovr', 'multinomial']`` for ``'clf__solver': ['newton-cg', 'sag', 'lbfgs']``\n\n**Attributes:**\n - coef_ : array, shape (1, n_features) or (n_classes, n_features)\n - intercept_ : array, shape (1,) or (n_classes,)\n - n_iter_ : array, shape (n_classes,) or (1, )\n\n**See also:**\n - SGDClassifier : incrementally trained logistic regression (when given the parameter ``loss=\"log\"``).\n - sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n \n See the best results below, wheres get with PCA 21 but take more time then LDA.","1198473f":"From the accuracy and loss plot, you can note that the early stopped the learning process at the 43<sup>rd<\/sup> epoch, this seems to me an accurate stop and if continued may cause overfitting.\n\nLet's check the report.","0a6920e7":"## Uploading Data and Preparing Environment\n\nThe data set for can be accessed through the [UCI site](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality).\n\n### Importing Preprocessing and Data Visualization Tools\n","980ce29d":"### Hyper Parametrization\nHyperparameters are also known as meta-parameters and are usually set before we start the model training process. These hyperparameters do not have any dependency on being derived from the underlying data on which the model is trained. Usually these hyperparameters represent some high level concepts or knobs, which can be used to tweak and tune the model during training to improve its performance. \n\nSo, let's start with the wine type classifier models.\n\n### Wine Type Classifier Models:\nWe will start with trying to classify the wines by red or white. \n\nLet's prepare our data with focus on type of wines and make the split of training and test datasets to use in the following tasks.","e0df8603":"A good portion of the correlations seem to be weak, but there are some important observations:\n\n* Total and free sulfur dioxide has the highest correlation with white wines and each other. In fact, the second is a parcel of the first and represents a colinearity that can be a problem for linear classifiers, especially if the target will predict the color, in that case we need to drop the free sulfur dioxide.\n* The residual sugar has a 0.50 relation to the total sulfur dioxide and 0.40 with fre sulfur dioxide, it is a god indication that wich more residual sugar more sulfur dioxide is added by the winemaker. The 0.50 indicates that white wine tends to have higher residual sugar than red wine.\n* The density also has a relatively high positive correlation to residual sugar (0.55) and relatively high negative correlation to alcohol (-0.69).\n* The chlorides and volatile acidity has -0.51 and -0,65 correlations between color, indicating a correlation that can assist in wine color classification.\n\nLet's explore a graphical representation of the relationship between the features using pair plots. ","30fc1678":"#### Decision Tree Classifier\n\n[Decision Trees](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n\n___Some advantages of decision trees are:___\n\n- Simple to understand and to interpret. Trees can be visualised.\n- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n- Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n- Able to handle multi-output problems.\n- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\n___The disadvantages of decision trees include:___\n\n- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.","21eda5b5":"This did wonderful in all metrics performance.\n\nThese results are really sufficient and obtained through a very simple model, but out of curiosity, we will submit the same data to a Deep Neural Network Classifier.","e3cae035":"[](http:\/\/)This model performed similarly to the KNN model but still isn't perfoming as well as our random forrest classifier. \n\n#### AdaBoost classifier\n\nIs a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n\nThis class implements the algorithm known as [AdaBoost-SAMME](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier).\n\n**Parameters:**\n\n - ***n_estimators***: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n\n - ***learning_rate***: Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n\n - ***algorithm***: {\u2018SAMME\u2019, \u2018SAMME.R\u2019}. If \u2018SAMME.R\u2019 then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If \u2018SAMME\u2019 then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.","19b1a760":"#### Deep Neural Network\n\nThe most simple neural network is the \"perceptron\", which, in its simplest form, consists of a single neuron. The perceptrons only work with numerical data, so, you should convert any nominal data into a numerical format.\n\nThe the perceptron has a important limitation, it could only represent linear separations between classes. To overcome this we can use the multi-layer perceptron overcomes that can be represent more complex decision boundaries.\n\nMulti-layer perceptrons are also known as \"feed-forward neural networks\". These are more complex networks as they consist of multiple neurons that are organized in layers. The number of layers is usually limited to two or three, but theoretically, there is no limit!\n\nAmong the layers, you can distinguish an input layer, hidden layers and an output layer. Multi-layer perceptrons are often fully connected. This means that there's a connection from each perceptron in a certain layer to each perceptron in the next layer. Even though the connectedness is no requirement, this is typically the case.\n\nOne of the most powerful and easy-to-use Python libraries for developing and evaluating deep learning models is [Keras](https:\/\/keras.io\/models\/sequential\/). It wraps the efficient numerical computation libraries Theano and TensorFlow. The advantage of this is mainly that you can get started with neural networks in an easy and fun way. A quick way to get started is to use the Keras Sequential model: it's a linear stack of layers. You can easily create the model by passing a list of layer instances to the constructor, which you set up by running `model = Sequential()`.\n\nSo, let's go and create our DNN classifier of type wines in  Keras with TensorFlow:","608583e6":"#### Ensemble models\nMoving forward with our mission of improving our wine quality predictive model, let\u2019s look at some ensemble modeling methods. Ensemble models are typically Machine Learning models that combine or take a weighted (average\\majority) vote of the predictions of each of the individual base model estimators that have been built using supervised methods of their own. The ensemble is expected to generalize better over underlying data, be more robust, and make superior predictions as compared to each individual base model. Ensemble models can be categorized under three major families.\n- ***Bagging methods***: \n    In this approach, a bootstrap samples, i.e. independent samples with replacement, are taken and several base models are built on these sampled datasets. At any instance, an average of all predictions from the individual estimators is taken for the ensemble model to make its final prediction. Random sampling tries to reduce model variance, reduce overfitting, and boost prediction accuracy. \n- ***Boosting methods***: \n    In this method the idea is to combine several weak base learners to form a powerful ensemble. Weak learners are trained sequentially over multiple iterations of the training data with weight modifications inserted at each retrain phase. At each re-training of a weak base learner, higher weights are assigned to those training instances which were misclassified previously. Thus, these methods try to focus on training instances which it wrongly predicted in the previous training sequence. Boosted models are prone to over-fitting so one should be very careful. Examples include Gradient Boosting, AdaBoost, and the very popular XGBoost.\n- ***Stacking methods***:\n    In stacking based methods, we first build multiple base models over the training data. Then the final ensemble model is built by taking the output predictions from these models as its additional inputs for training to make the final prediction.\n \n#### Random Forest Classifier\nLet's now try building a model using a bagging methods by run a random forests\n\nA [random forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n\nIn the random forest model, each base learner is a decision tree model trained on a bootstrap sample of the training data. Besides this, when we want to split a decision node in the tree, the split is chosen from a random subset of all the features instead of taking the best split from all the features. Due to the introduction of this randomness, bias increases and when we average the result from all the trees in the forest, the overall variance decreases, giving us a robust ensemble model which generalizes well. We will be using the RandomForestClassifier from scikit-learn, which averages the probabilistic prediction from all the trees in the forest for the final prediction instead of taking the actual prediction votes and then averaging it","a2cc1018":"This is in alignment with the insights we gained form the ANOVA test and correlations we looked at earlier. \n\n* Once again the higher quality wines seem to have lower sulphate levels. However, the highest sulphate values are found in the medium quality wines. \n* Lower levels of volatile acid seem to correspond with higher quality labels.\n* The highest correlation is 0.72, which is between free sulfur dioxide and total sulfur dioxide. This is the same correlation we picked on in the previous pair plot.\n\nIt might be beneficial to take a closer look at the box plots for the alcohol content depending upon quality.","2077347a":"### Nulls Cleaning","7025e76c":"It is interesting how the alcohol content doesn't give much variation to see if the wine is either red or white, yet it makes a large difference in quality. It seems to be that the higher the average alcohol concentration the better the quality. However, lower quality wines have the lowest standard deviation. \n\nThere is less chlorides and volatile acidity with a smaller standard deviation in higher quality of wines.\n\nSulfur dioxide increase with quality as well, but the standard deviation decreases with an increase in quality.\n\nHigher quality wines also has less of a fixed acidity, but the standard deviation is slighlty higher in mean quality. ","9f2a4ea0":"We have achieved our goal by getting good models we were able to create classification models for both the type and the quality of the wine going through all phases in a standard Machine Learning and data mining workflow models.\n\nWe obtained good insights in the EDA phase, making use even of tests of validation of our hypotheses, as well as we identify and treat problems of collinearity and multicollinearity.\n\nIn the data engineering and treatment, we identify, transform and validate the transformation of skewed features, handle nulls, provide a function to select features into the pipeline and, have success to use dimensionality reduction to construct visualizations capable of showing us how much we could separate the data and how complex it would be with overlaps.\n\nWe had success in evaluating our models and running grid search cross validation. We also applied more modern methods to interpret complex models.\n\nIn the first classification challenge, a simpler model was able to obtain an excellent result and with a low cost of processing, with use only those treatments done in the previous stages of modeling.\n\nIn the classification of quality in low, medium and high, we saw that simple models would not be able to capture the nuances of variation in the data, especially of high quality wines with so few records in comparison to the other classes. In the other hand, we got good results in the three major families of the ensemble. The ensemble was more robust, as we could confirm by their ROC curves, and it could also make superior predictions as compared to each individual base model. \n\nWe conclude that it is possible to predict the quality of a wine and its type from the physicalchemicals attributes. \nAlthough the prediction of the type has only educational foundation, the prediction of quality presents some practical applicabilities, like:\n- A wine store or or large distributor can qualify new wines, even before acquiring them, and thus better evaluate their purchase cost and their opportunity to sell.\n- The result of the model, its interpretation and all the evaluations of EDA provide methods and rules of decision that can help winemakers to look for wines of better qualities.\n\nHowever, our model has a gap of about 13%. Part of this can be explained by the fact that the quality notes are in fact notes singled out by an expert who take into account other attributes, which are not the physicalchemical ones, and therefore are not present in this data set. The statement of the note is a sensory evaluation, based on the tasting process, just to name a few we would have:\n- Others relevant physicals features not present like opacity and viscosity \n- As we have seen, some of the characteristics that we have can influence the smell and taste of the wine, but certainly we do not have all the necessary data, for example the grape variety or type.\n- We don't have the tannin, one of the basic wine characteristics. In wine it is the presence of phenolic compounds that add bitterness to a wine.\n- The wines traditionally produced in the Verde Region of Portugal are young wines that have high acidity and marked freshness. So the guard time is not so relevant for the classification of this wine, but the harvest is, as in other wines too.\n\nFinally, wine is also an emotional drink, so the notes are also affected by this, even though the average of the last three professionals.\n\nSo, cheers!\n![image](https:\/\/i.gifer.com\/ERng.gif)\n\n## Next steps\nFor quality we can:\n- Make some different engineering features and polynomials transformations.\n- Try a regression model to predict the quality as a number.\n- Try a quality classification in the all range of quality.\n- Try a tensorflow, but remember, in general, because you don't have a ton of data, the worse overfitting can and will be. That's why you should use a small network.","223f6dd9":"As you can see, our results aren't nearly as nice as our wine type classification results. This is most likely a result of data imbalances since high quality wines seem to be ocntinuously misclassified as either medium or low quality. This is expected since we lack training samples for high quality wine. If we run the model and set class_weight to balanced, we will able to raise the recall and f1-score of the high class, but we will most likely have significant loss in other performance metrics. \n\nThe main advantage of decision tree based models is model interpretability, since it is quite easy to understand and interpret the decision rules which led to a specific model prediction. Trees can be even visualized to understand and interpret decision rules better. For better understand, I rerun the decision tree with the same parameters, but without scaling the features and include the following code helps us visualize decision trees:","1d1107e3":"It seems that our nearest neighbors model for quality didn't perform nearly as well as the wine color classifier. The second graph seems to indicate that principal components are a bit more tangled than wine color that has a clear clustering. ","9b859cc1":"We were able to bring the largest skewed values closer to a more normal distribution. Lets take a look at the distributions again and analyze the quantile-quantile (qq) plots. The qq plot is a probabiltiy plot that compares two probability distributions through a comparison of their quantiles. ","e80f8f72":"## Exploratory Data Analysis\n\nAfter the null values have been properly dealt with we can evaluate the distribution and spread of the data. We will confirm the inbalances that exist within the classes and figure out possible correlations. ","3f88b830":"#### Salty\nSalt content is not typicall used as wine descriptor. If you're like me, when you think of wine characteristics salt is one of the last things that comes to mind. Wine producing countries commonly have legal maximums for sodium chloride content. The salinity of a wine is usually determined by the soil the grapes grow in. Dryer regions often suffer from saltier wines because of irrigation systems that increase soil salinity. Most characteristics of soil don't translate to grape composition but salt is the exception. \n\n***Salty Characteristics***\n* Salt often isn't found in wines because it is not a sought after but when it is imagine pouring some wine on a saltine and you'll have a similar experience.\n\n***Salty Types and Measures***\n* **Chlorides**: The chloride concentration in wine is a result of the terriot and comes from regions with high irrigation. This quality is expressed in units of $\\frac{g(sodiumchloride)}{dm^3}$.","fdb00473":"#### Body\nThis is often a misunderstood identifier outside of the wine world. The body of a wine is commonly described as light, medium, or full bodied and is a result of many factors. It depends on wine variety, origin, or the alcohol level of the wine. The body is used as a general impression of the wine. \n\n***Body Characteristics***\n* Red wines are typically more full bodied than whites. \n* A wine that is said to be full bodied often has a variety of flavors and evolves as it rest on your palette. \n\n***Body Types and Measures***\n* **Density**: This is measured as relative density. Relative density usually refers to the weight of wine relative to an equivalent volume of water. It can also be used a measure of the conversion of sugar to alcohol. In this data set the units for density is $\\frac{g}{cm^3}$.","e64db505":"In general white wines tend to have a higher sugar content then red wines, which is apparent by the mean of the residual sugar for red and white wines. The total sulfur dioxide and citric acids seems to have a larger presence in white wines compared to reds. Reds on the other hand have a higher sulphate, fixed acidity, and volatile acidity content then white wines. \n\nLets now examine the differences in general statistics for quality.","f862d7bd":"Looking at each side of the violin plot it is very evident that red wines tend to have a lower fixed acidity than white wines. There also seems to be an overall decrease in acidity as quality increases, especially for white wines. \n\nLet's examine some other parameters in closer detail.","1ee3a4f1":"## Modeling\nWe start to looking at different approaches to implement classifiers models, and use hyper parametrization, cross validation and compare the results between different errors measures.\n\nFirst we will make some support functions help us evaluate ours models through a standard. Next, we proceed with the classifications of wines types and next of wines quality labels. We will finalize our quality classification model with a staking approach.\n\n### Evaluating Results\nWe will be using a confusion matrix, classification report, F1 score, recall, precision and ROC AUC to examine our models results. Here is a description of our evaluation metrics: \n- The model main performance metrics Accuracy, Precision, Recall and F1 Score\n- The prediction confusion matrix\n- The ROC AUC Score\n- Features importance plot\n\nThe reciever operating curve (ROC curve) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. This is created by plotting the true positive rate against the false positive rate\n\n","d1eb7c50":"The boxplots show outliers for quite a few columns. Fixed acidity for red wine has a first quartile value of 7.1 with its second quartile value being 7.9. Not much of a variance which explains the huge number of outliers. White wines fixed acidity has a first quartile value of 6.3 and second quartile value of 6.8, again not much variance. Similar logic applies for volatile acidity for both types of wine. The Citric acid column seems to be uniformly distributed. The other columns red (white) are described below. Residual sugar has a minimum of 0.9 (0.6) but a maximum of 15.5 (65.8), far too much difference. Chlorides also have a huge difference with their minimum value being 0.012 (0.009) and the maximum being 0.61 (0.35). This is the case for free sulfur dioxide as well, minimum is 1 (2.0) and maximum is 72 (289).","c2bc4008":"### Dimensionality Reduction\n\n#### PCA (Principal Component Analysis)\n\nPCA is statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principle components. If there are n observations wit p variables, then the number of distinct principle components is `min(n-1,p)`. This transformation is defined in such a way that the first principal component has the largest possible variance, and each suceeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set.\n\nPCA basically reduces the number of variables (dimensionality) of your data trying to minimize the loss of information as you perform this transformation. ","b5dcf3bf":"### Wine Quality Classifier Models:\nNow, let's move on to modeling a wine quality classifier. For this, let's prepare our dataset to focus on our wine quality label feature.","c6478d92":"There seems to be a pattern depicting slightly higher sulphate levels for higher quality rated wine samples. It seems to be clear that sulphate levels for red wines are much higher as compared to the white wines. \n\nFrom the first pair of graphs we see that higher quality wine samples tend to have a lower level of volatile acidity but higher levels of alcohol content when compared to lower rated wines. In general the volatile acidity is marginally lower in white wines compared to red wines. \n\nThe volatile acid and total sulfur dioxide are considerably lower in high quality wine samples. Volatile acidity levels are slightly lower in white wine samples opposed to red wine samples. ","ffca44ee":"from the pair plot we notice several patterns and correlations. These observations are in alignment with the insights we obtained from the ANOVA test and our presumptions regarding correlations. \n\n* The residual sugar, total sulfur dioxide, and citric acid in white wine seems to be much higher than in red wines. \n* Sulphates, fixed acidity, and volatile acidity seem to be higher in red wine than in white wine. \n* There seems to be a negative correlation between density and alcohol. This is confirmed by the linear trend of the points from left to right in a sharply decreasing direction. \n* Density also seems to have a positive correlation with residual sugar, which is slightly reinforced by the outliers. \n\nLet's examine the correlation depending upon wine quality.","18953056":"As evident by the graphs it seems a large portion of our attributed are right skewed, we will try to address this issue with a Box-Cox transformation. ","99b02f36":"Getting rid of density seemed to decrease our multicolinearity. \n\n### Feature Selection into the Pipeline\nSince we have a few features it may be enough to only remove collinear and multicollinear. However, although collinearity and multicollinearity levels are low and there are models to handle this. It may still be interesting to proceed with the selection of variables within the pipeline, allowing us to decide if there will be selection of variables and by which method we should proceed with. We can still improve the results through hyper parameterization and cross-validation.","2641a1c8":"#### Classifications\nThese are the values we will be attempting to predict through our models as well as examine the correlations between them and the other parameters. \n* **Wine Color\/Type**: We concatenated two data sets into one called 'wine'. These two data sets were composed of red and white wines. Upon the concatenation we created a new column called 'color' that labeled the wine as either or red or wine so that information wouldn't be lost. A wine can either be a 'red' or 'white' wine.\n* **Quality**: A variety of wine experts graded each wine from 0 (horrible) to 10 (incredible). The quality score is the median of at least three evaluations.\n* **Quality Label**: We then manually organized the data into three labels of 1, 2, and 3. The bins were created in order to try and offset some of the data inbalance. The quality label of 1 is associated with a quality score of 1, 2, 3, 4, or 5. A quality label of 2 is associated with a quality score of 6 or 7. A quality label of 3 is associated with a quality score of 8, 9, or 10.","84acb9b0":"This model performed well and didn't missclassify between high quality and bad quality wines. However, it did not do as well as the random forest model. \n\n#### Gradient Boosting for Classification\n\n[GB](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\n- loss: loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm.","36280617":"This model is terrible, especially when classifying high quality wines, but it is interesting like LR. This model does not classify wines of medium and low quality as high and may be an option to be evaluated in a staking model. KNNC and GBC did better by hitting more predictions of high quality wines. \n\nAgain, although the accuracy seems to be okay it fails in the precision and recall for the classification of high quality wines. ","dd0a59bc":"### Identify  and Treat Multicollinearity:\n**Multicollinearity** is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model, leading to unreliable and unstable estimates of regression coefficients. To make matters worst multicollinearity can emerge even when isolated pairs of variables are not collinear.\n\nTo identify, we need start with the coefficient of determination, r<sup>2<\/sup>, is the square of the Pearson correlation coefficient r. The coefficient of determination, with respect to correlation, is the proportion of the variance that is shared by both variables. It gives a measure of the amount of variation that can be explained by the model (the correlation is the model). It is sometimes expressed as a percentage (e.g., 36% instead of 0.36) when we discuss the proportion of variance explained by the correlation. However, you should not write r<sup>2<\/sup> = 36%, or any other percentage. You should write it as a proportion (e.g., r<sup>2<\/sup> = 0.36).\n![image](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQTS_TVxaBpLmAGthSUAS9w7SVKsmLOtocz7ts-MXioJwa-Se0U)\n\nAlready the **Variance Inflation Factor** (**VIF**) is a measure of collinearity among predictor variables within a multiple regression.  It may be calculated for each predictor by doing a linear regression of that predictor on all the other predictors, and then obtaining the R<sup>2<\/sup> from that regression.  It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variance of a single beta if it were fit alone [1\/(1-R<sup>2<\/sup>)]. Thus, a VIF of 1.8 tells us that the variance (the square of the standard error) of a particular coefficient is 80% larger than it would be if that predictor was completely uncorrelated with all the other predictors. The VIF has a lower bound of 1 but no upper bound. Authorities differ on how high the VIF has to be to constitute a problem (e.g.: 2.50 (R<sup>2<\/sup> equal to 0.6), sometimes 5 (R<sup>2<\/sup> equal to .8), or greater than 10 (R<sup>2<\/sup> equal to 0.9) and so on). \n\nBut there are several situations in which multicollinearity can be safely ignored:\n\n - ***Interaction terms*** and ***higher-order terms*** (e.g., ***squared*** and ***cubed predictors***) ***are correlated*** with main effect terms because they include the main effects terms. **Ops!** Sometimes we use ***polynomials*** to solve problems, **indeed!** But keep calm, in these cases,  **standardizing** the predictors can **removed the multicollinearity**. \n - ***Indicator***, like ***dummy*** or ***one-hot-encode***, that represent a ***categorical variable with three or more categories***. If the proportion of cases in the reference category is small, the indicator will necessarily have high VIF's, even if the categorical is not associated with other variables in the regression model. But, you need check if some dummy is collinear or has multicollinearity with other features outside of their dummies.\n - ***Control feature** if the ***feature of interest*** **do not have high VIF's**. Here's the thing about multicollinearity: it's only a problem for the features that are **collinear**. It increases the standard errors of their coefficients, and it may make those coefficients unstable in several ways. But so long as the collinear feature are only used as control feature, and they are not collinear with your feature of interest, there's no problem. The coefficients of the features of interest are not affected, and the performance of the control feature as controls is not impaired.\n\nSo, generally, we could run the same model twice, once with severe multicollinearity and once with moderate multicollinearity. This provides a great head-to-head comparison and it reveals the classic effects of multicollinearity. However, when standardizing your predictors doesn't work, you can try other solutions such as:\n- removing highly correlated predictors\n- linearly combining predictors, such as adding them together\n- running entirely different analyses, such as partial least squares regression or principal components analysis\n\nWhen considering a solution, keep in mind that all remedies have potential drawbacks. If you can live with less precise coefficient estimates, or a model that has a high R-squared but few significant predictors, doing nothing can be the correct decision because it won't impact the fit.\n\nGiven the potential for correlation among the predictors, we'll display the variance inflation factors (VIF), which indicate the extent to which multicollinearity is present in a regression analysis. Hence such variables need to be removed from the model. Deleting one variable at a time and then again checking the VIF for the model is the best way to do this.\n\nSo, I start the analysis already having removed the features with the highest collinearities and run VIF.","d022ae5b":"We have confirmed that there is a large imbalance in data. This is especially apparent between the high quality wines (3.0) and the average quality wines (2.0). ","1d48fda1":"As you can see we improved the results with our stacked model, although small if compared to what we had already obtained in other ensemble models, it has proved the value of staked methods.\n\nIn addition, as you can see we did not lose the ability to evaluate the ROC curve, importance of the characteristics or even to apply the methods of interpretation of the model.","19f07714":"#### KNeighbors Classifier\n\nNeighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n\nscikit-learn implements two different nearest neighbors classifiers: [KNeighborsClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user. [RadiusNeighborsClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) implements learning based on the number of neighbors within a fixed radius of each training point, where is a floating-point value specified by the user.\n\nWe will create a KNeighborsClassifier model with following parameters:\n- n_neighbors: Number of neighbors to use by default for kneighbors queries.\n- weights: weight function used in prediction. \n    - 'uniform': uniform weights. All points in each neighborhood are weighted equally.\n    - 'distance': weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n    - [callable]: a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n- algorithm: Algorithm used to compute the nearest neighbors:\n    - 'ball_tree' will use BallTree\n    = 'kd_tree' will use KDTree\n    = 'brute' will use a brute-force search.\n    = 'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n    \n    Note: fitting on sparse input will override the setting of this parameter, using brute force.\n\n- leaf_size: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n- p: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.","43970e58":"## Data Engineering\n\nWe need to perform a bit more cleaning, transformation, selection, and reduction in order for our data to be ready to sample. \n\n#### Box Cox Transformation\n\nA box cox transformation turn data that is not normally distributed and puts into a normal shape. This is done for two reasons:\n\n- **Model bias and spurious interactions**: If you are performing a regression or any statistical modeling, this asymmetrical behavior may lead to a bias in the model. If a factor has a significant effect on the average, because the variability is much larger, many factors will seem to have a stronger effect when the mean is larger. This is not due, however, to a true factor effect but rather to an increased amount of variability that affects all factor effect estimates when the mean gets larger. This will probably generate spurious interactions due to a non-constant variation, resulting in a **very complex model** with many **spurious** and **unrealistic** interactions.\n- **Normality is an important assumption for many statistical techniques**: such as individuals control charts, Cp\/Cpk analysis, t-tests and analysis of variance (ANOVA). A substantial departure from normality will bias your capability estimates.\n\nOne solution to this is to transform your data into normality using a [Box-Cox transformation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html) means that you are able to run a broader number of tests.\n\nAt the core of the Box Cox transformation is an exponent, lambda (\u03bb), which varies from -5 to 5. All values of \u03bb are considered and the optimal value for your data is selected; The 'optimal value' is the one which results in the best approximation of a normal distribution curve. The transformation of Y has the form:\n<a href=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcox.png\"><img src=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcox.png\" alt=\"\" width=\"222\" height=\"70\" class=\"aligncenter size-full wp-image-13940\" \/><\/a>\n\nThe scipy implementation proceeded with this formula, you need to take care of negatives values before this transformation if you have some. A common technique for handling negative values is to add a constant value to the data prior to applying the log transform. The transformation is therefore log(Y+a) where a is the constant. Some people like to choose a so that min(Y+a) is a very small positive number (like 0.001). Others choose a so that min(Y+a) = 1. For the latter choice, you can show that a = b \u2013 min(Y), where b is either a small number or is 1.\nThis test only works for positive data. However, Box and Cox did propose a second formula that can be used for negative y-values, not implemented in scipy:\n<a href=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcoxNeg.png\"><img src=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcoxNeg.png\" alt=\"\" width=\"272\" height=\"76\" class=\"aligncenter size-full wp-image-13941\" \/><\/a>\nThe formula are deceptively simple. Testing all possible values by hand is unnecessarily labor intensive.\n\n<p align='center'> Common Box-Cox Transformations \n<\/p>\n\n\n| Lambda value (\u03bb) | Transformed data (Y') |\n|------------------|-----------------------|\n|        -3\t       | Y\\*\\*-3 = 1\/Y\\*\\*3    |\n|        -2        | Y\\*\\*-2 = 1\/Y\\*\\*2    |\n|        -1        | Y\\*\\*-1 = 1\/Y         |\n|       -0.5       | Y\\*\\*-0.5 = 1\/(\u221a(Y))  |\n|         0        | log(Y)(\\*)            |\n|        0.5       | Y0.5 = \u221a(Y)           |\n|         1        | Y\\*\\*1 = Y            |\n|         2        | Y\\*\\*2                |\n|         3        | Y\\*\\*3                |\n\n\n\n(\\*)Note: the transformation for zero is log(0), otherwise all data would transform to Y\\*\\*0 = 1.\nThe transformation doesn't always work well, so make sure you check your data after the transformation with a normal probability plot or if the skew are reduced, tending to zero.\n\nLets examine the distributions to see the non-normalities within our data set. ","d9d7fc5a":"#### Sulfites\n\nSulfites in wine refer to the naturally occuring chemical compounds. Sulfur dioxide is naturally found in wines and is a byproduct of fermentation, but most winemakers choose to add a little extra to prevent growth of undesirable yeast and microbes, as well as to protect against oxidation. \n\n***Sulfite Characteristics***\n\n* Wines with a lower acidity will need more sulfites than higher acidity wines. A wine with a pH of 3.6 and above are much less stable and in order to preserve shelf-life more sulfites are needed.\n* White wines typically need more sulfites because of the shorter length of their fermentation. A typical dry white wine may have around 100 mg\/L whereas a typical dry red wine will have around half of that. \n* A higher sugar content could lead to further fermentation so more sulfites will be required to prevent this.\n* Wines that need less temperature to be preserved tend to release free sulfur compounds. This can be fixed through decantation or chilling the wines. \n\n***Sulfite Types and Measures***\n\n* **Sulphates**:These are mineral salts that contain sulfur. Sulphates have a relationship with wine that is simmilar to gluten in food. They are a regular part of the winemaking procedure and are considered essential. These sulphates are connected to the fermentation process and drastically effect the wines aroma and flavor. The data expresses sulphates in units of $\\frac{g(potassiumsulphate)}{dm^3}$. \n* **Free Sulfur Dioxide**: This is the free sulfur dioxide that when added to wine after the remaining part binds. Winemakers continuously try to get this free sulfur to bind since too much of it is considered undesirable. An abundance of these free sulphites results in a rotten egg kind of odor. This column is measured in units of $\\frac{mg}{dm^3}$.\n* **Total Sulfur Dioxide**: As indicated by the name, this is the total amount of bound and free sulfite. This is still expressed in units of $\\frac{mg}{dm^3}$. More sulfur is typically added to kill additional bacteria and preserve freshness. There are usually legal limits for the amount of sulfur that can be within wine. ","7f121654":"Let's begin by creating a quality label column based on the score. \n\n* Bad quality for 5 and below\n* Avergae quality for 6 or 7\n* high quality for 7 and above","c5e81153":"With this results, it is interesting try a staked model with RF and GBC, for meta-classier we can use RF or GBC. Other option can include the KNeighbors and use of LR, LinearSVC or KNeighbors as meta-classier. Let's see one of then to can realize how to make that.","f997c5b6":"KNNC and GBC seemed to fo a good job hitting more predictions of high quality wines. The best model in terms of general metrics seems to be GBC and KNNC as well.\n\nIt will be interesting to try a staked model with RF and GBC. For meta-classier we can use RF or GBC. ","c9a5140e":"#### Acidity\n\nAcids are an essential attribute that contribute the taste of a wine. Acidity in food and drinks tastes tart and zesty. The taste acidity is also sometimes confused with alcohol. Wines with higher acidity will feel lighter-bodied to the palette and come across as 'spritzy'. A large reduction in acid will cause the wines to taste flat. Less acidity is associated with a more round and rich taste. \n\n***Acidity Characteristics***\n* Tingling sensation that focuses on the front and sides of your tongue.\n* A gravelly filament on the side and roof of your mouth.\n* A Wet or cold sensation similar to eating a green apple.\n* Your mouth has a sensation to produce more saliva.\n\n***Acid Types and Measures***\n\n* **Fixed Acidity**: The fixed acids include malic, citric, succinic, and tartaric acids. These are all found in the grapes of the wine, except for succinic acid. The data set includes this measure in units of $\\frac{g(tartaricacid)}{dm^3}$.\n* **Citric Acid**: This is one of the fixed acids. It is associated with wines. Most of this acid is consumed during the fermentation process and is occasionally added afterwards to make the wine feel 'new'. The data set measures citric acid in units of $\\frac{g}{dm^3}$.\n* **Volatile Acidity**: These acids are distilled from wine before the production process. Volatile acids include lactic, formic, and butyric acids. An excess of volatile acids will lead to undesirable flavors that usually resemble an upleasant bitterness. The legal limit of volatile acids in the United States is 1.2 g\/L for red wine and 1.1 g\/L for white wine. In this data set volatile acidity is expressed in units of $\\frac{g(aceticacid)}{dm^3}$. \n* **pH**: pH stands for the potential of hydrogen. It is a scale commonly used to express a compounds acidity or basicity. The fixed acidity contributed the most towards pH of wines. Solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. Pure water is neutral on this scale at 7. Most wines are between a pH of 2.9 and 3.9. This classifies wine as acidic.","3afd3c8a":"There seems to be a large discrepancy in data counts. We have far more information on white wines than red wines. This imbalance can be dealt with through sorting an inputted wine as either red or white and then assessing its quality label. Let's explore the quality distributions for red and white separately now.","114cc868":"The kurtosis is a measure of whether the data are heavily-tailed or light_tailed relative to a normal distribution. Data sets with high kurtosis tend to have heavy tails or outliers. Data with low kurtosis tend to have light tails, or a lack of outliers.\n\nIt seems that the box cox transformation has presented significant improvements. the distributions pass the QQ plot test since most points in the right graphs seem to be on the line. They have comparable quantiles.","8510fe59":"#### Sweetness\n\nHow sweet is the wine? When it hits are palette do we find it familiar to a piece of candy or is it more sour? The human perception of sweet begins at the tip of the tongue. Wine is usually the most direct impression when first tasting a wine. Wine connoisseurs focus on the tip of their tongue when measuring sweetness. The sweetness can tingle your taste buds and cause salivation. Dry wines often still have some level of sweetness in order to make it more full-bodies (as a fancy wine connoisseur might say). \n\n***Sweetness Characteristics***\n* Tingling sensation towards the tip of your tongue\n* The wine is thicker with a higher level of viscosity; the wine tears fall slower.\n* An oily filament on the middle of your tongue that lingers.\n* Dry reds often contain higher residual sugar than expected.\n\n***Sweetness Types and Measures***\n* **Residual Sugar**: This refers to the sugar found within grapes after fermentation, which is why its refered to as residual. This data was measured in $\\frac{g}{dm^3}$.\n","f5329443":"### Correlation\n\nTo examine the linear relationship that exist between features we will check the correlations and create a heat map for better visualization. \n\nA correlation matric is identical to a covariance matrix that is computed from standardized data. The correlation matrix is a square matrix that contrains the Pearson product-moment correlation coefficients (often abbreviated as [Pearson's r](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient)), which measure the linear dependence between pairs of features:\n\n![image](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/602e9087d7a3c4de443b86c734d7434ae12890bc)\n\nPearson's correlation coefficient can simply be calculated as the covariance between two features x and y (numerator) divided by the product of their standard deviations (denominator):\n\n![image](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\n\nThe covariance between standardized features is in fact equal to their linear correlation coefficient.\nUse NumPy's corrcoef and seaborn's heatmap functions to plot the correlation matrix array as a heat map.\n\nTo fit a linear regression model, we are interested in those features that have a high correlation with our target variable. So, let's prepare the dataset and the type and quality label targets variables. ","b6f656a9":"This model didn't gave a good performance and shows to much loss in the high quality classification. Let's move on.","f5101428":"Importing the two data frames we will be working with.","dd9f4c79":"#### Logistic Regression\nSince it is a binary classification task, we try first with a simple logistic regression.\n\nThis class implements regularized [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) using the 'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle both dense and sparse input. \n\n**Main Parameters**\n - class_weight : dict or 'balanced', default: None\n   The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples \/ (n_classes * np.bincount(y))``.\n\n   For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. For example, our class 0 is 1.24 times more frequent than class 1. So you should increase the class_weight of class 1 relative to class 0, say {1: 0.6, 0: 0.4}. If the class_weight doesn't sum to 1, it will basically change the regularization parameter.\n\n   \"balanced\" basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\n   \n\n - warm_start : bool, default: False. Useless for liblinear solver.\n - ``'clf__multi_class' : ['ovr', 'multinomial']`` for ``'clf__solver': ['newton-cg', 'sag', 'lbfgs']``\n\n**Attributes:**\n - coef_ : array, shape (1, n_features) or (n_classes, n_features)\n - intercept_ : array, shape (1,) or (n_classes,)\n - n_iter_ : array, shape (n_classes,) or (1, )\n\n**See also:**\n - SGDClassifier : incrementally trained logistic regression (when given the parameter ``loss=\"log\"``).\n - sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n \n See our the best results below with first model from a pipeline with grid search CV.","ba4c3a88":"Each box plot depicts the distribution of alcohol level for a particular wine quality rating that is seperated by wine types. We can see that the wine alcohol by volume distribution has an increasing trend based on higher quality rated wine samples. There are some outliers in each quality level, which is often depicted by individual points. \n\nNow lets take a closer look at the box plots for wine types, quality, and acidity.","ccc17040":"This model did not present a good result but performed better than AdaBooster Classifier. However, if we observe its ROC curve, it appears to be more stable and might be a good candidate for meta classifier model.\n\n#### Linear Support Vector Classification\n\n[LSVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) is similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n\nThe combination of penalty='l1' and loss='hinge' is not supported, and penalty='l2' and loss='hinge' needs dual=True.","bcf5cdb7":"## General Data Description","cb71df1d":"We see a similar issue when examining the accuracy between the wine color model and the quality label model. This is just testing accuracy so it may be beneficial checking F1, precision, and recall. However, the graphs look good for LDA and PCA analysis. ","e03cc567":"As you can see, we got incredible results from each evaluation, the dense neural network seems to be performing a bit better. The results confirms that DNN is marginally better.","5756bb44":"# Wine Quality and Type Classification\n\n\n### Introduction\nWine is made through the fermentation of grapes. This fermentation is a result of yeast consuming the sugar within the grapes and then converting it to ethanol, at which point it gets its alcoholic properties. Uderstanding the physiochemical attributes of wine could provide insights into the quality and region that wine originates from. In order to figure out this relationship that exist, we will utilize standard machine learning and data processing techniques. The various models will predict whether the wine sample is red or white. Once its type is categorized its quality will then be determined. \n\nThe dataset is from [UCI](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality) and is described below. \n\nThe two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the Vinho Verde website or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\nThe classes are ordered and not balanced: there are many more normal wines than excellent or poor ones. Outlier detection algorithms will need to be used to detect the few excellent or poor wines. All input variables may not be relevant, feature engineering will be essential to test various selection methods. \n","069ae6a2":"All the VIF values seem to be at reasonable values so we will leave most of them except for density which seems to be abnormally high. Let's see what that does to out VIF values.","f4bcc852":"#### Alcohol\n\nI am sure we are all well acquainted with alcohol. Alcohol is formed as a result of yeast converting sugar during the fermentation process. The percentage of alcohol can vary from wine to wine. There are a variety of different taste receptors that sense alcohol. This is white alcohol can taste bitter, sweet, or even spicy. Genetics tend to play a huge role in the number of taste receptors are activated. The warming sensation in the back of the throat is attributed to the amount of alcohol within the wine. \n\n***Alcohol Characteristics***\n* A wine with higher alcohol content can taste oily and bolder while wines with less alcohol tend to taste lighter. \n\n***Alcohol Types and Measures***\n* Alcohol: There is usually one metric to measure alcohol. It is typically measured as percent volume or alcohol by volume.","921ad1a8":" ## Wine Attributes and Properties\n\nIt is important to understand the attributes and properties when it comes to running models and performing proper data diagnostics. ","af8cc42c":"As you may notice we have zero records with null characteristics, so we won't need to evaluate the option to simply eliminate rows with nulls instead of entering in medians and modes. If the data was missing values we could use the following function to impute the null values depending upon data type.","faa889da":"#### Linear Discriminant Analysis (LDA)\n\nThis form of data science is a very common technique used for supervised classification problems. A Linear Discriminant Analysis is a dimensionality reduction technique that reduces the dimensions by trying to remove redunant and dependent features. LDA can be used as a technique for feature extraction to increase the computational efficiency and decrease over fitting.\n\n**Some Important Parameters:**\nsolver : string, optional\n    Solver to use, possible values:\n      - svd: Singular value decomposition (default).\n        Does not compute the covariance matrix, therefore this solver is\n        recommended for data with a large number of features.\n      - eigen: Eigenvalue decomposition, can be combined with shrinkage.\n\nshrinkage : string or float, optional\n    Shrinkage parameter, possible values:\n      - None: no shrinkage (default).\n      - auto: automatic shrinkage using the Ledoit-Wolf lemma.\n      - float between 0 and 1: fixed shrinkage parameter.\n","18a2abc8":"Now we can check for correlations based on wine types.","68868002":"### Make Staked Classifier\nCreate an ensemble model by staking models with [StackingClassifier](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/#example-2-using-probabilities-as-meta-features) of mlxtend.classifier.\n\nStacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n![image](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png)"}}