{"cell_type":{"586761cf":"code","e157d519":"code","59344ba0":"code","cfd34e11":"code","63844b8a":"code","54ad6b7b":"code","d8729fee":"code","22f598ab":"code","b866a64d":"code","6d7b7436":"code","fd42dc53":"markdown","3921264a":"markdown","c0206a5b":"markdown","7716cf16":"markdown","cd3e54b6":"markdown","3373ea69":"markdown","5fabc585":"markdown","b9caaecf":"markdown","b38e6f93":"markdown","a5f52c4d":"markdown","5f44b2a0":"markdown"},"source":{"586761cf":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()","e157d519":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport seaborn as sn\n\nfrom sklearn.metrics import confusion_matrix\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import matthews_corrcoef\n\ndef get_acc_and_F1(ratio):\n    # create the synthetic dataset\n    X, y = make_classification(n_samples=25000, \n                            n_features=20, \n                            n_informative=2, \n                            n_redundant=2, \n                            n_repeated=0, \n                            n_classes=2, \n                            n_clusters_per_class=2, \n                            weights=[ratio, 1-ratio], \n                            flip_y=0.01, \n                            class_sep=1.0, \n                            hypercube=True, \n                            shift=0.0, \n                            scale=1.0, \n                            shuffle=True, \n                            random_state=42)\n\n    # test train split with shuffling\n    sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n    \n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    \n    # calculate the metrics\n    accuracy     = accuracy_score(y_test, predictions)\n    acc_bal      = balanced_accuracy_score(y_test, predictions)\n    av_precision = average_precision_score(y_test, predictions)\n    recall       = recall_score(y_test, predictions)\n    f1           = f1_score(y_test, predictions)\n    roc          = roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])\n    mcc          = matthews_corrcoef(y_test, predictions)\n\n    return(accuracy, acc_bal, av_precision, recall, f1, roc, mcc)\n\nratios_list        = [0.5, 0.7, 0.8, 0.85, 0.9, 0.92, 0.95, 0.96, 0.99, 0.999]\naccuracy_list      = []\nbal_accuracy_list  = []\nav_precision_list  = []\nrecall_list        = []\nf1_values_list     = []\nroc_list           = []\nmcc_list           = []\n\nfor ratio in ratios_list:\n    accuracy, acc_bal, av_precision, recall, f1, roc, mcc = get_acc_and_F1(ratio)\n    accuracy_list.append(accuracy)\n    bal_accuracy_list.append(acc_bal)\n    av_precision_list.append(av_precision)\n    recall_list.append(recall)\n    f1_values_list.append(f1)\n    roc_list.append(roc)\n    mcc_list.append(mcc)\n    \n# now plot\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.plot(ratios_list,accuracy_list,     lw=3, label='accuracy score',          c='black')\nax.plot(ratios_list,roc_list,          lw=3, label='ROC AUC score',           c='brown')\nax.plot(ratios_list,bal_accuracy_list, lw=3, label='balanced accuracy score', c='darkslateblue')\nax.plot(ratios_list,f1_values_list,    lw=3, label='F1 score',                c='pink')\nax.plot(ratios_list,recall_list,       lw=3, label='recall',                  c='olive')\nax.plot(ratios_list,av_precision_list, lw=3, label='average precision',       c='orangered')\nax.plot(ratios_list,mcc_list,          lw=3, label='Matthews correlation',    c='orchid')\n\nax.set_title(\"\", fontsize=18)\nax.set_xlabel (\"imbalance ratio\", fontsize=18)\nax.set_ylabel (\"score\", fontsize=18)\nplt.legend(loc=\"lower left\",fontsize=16)\n\ngradient = np.linspace(0.5, 1, 100).reshape(1, -1)\nplt.imshow(gradient , extent=[0.5, 1, 0, 1], aspect='auto', cmap='RdYlGn_r', alpha=0.80)\n\nplt.show();","59344ba0":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data['Survived'].value_counts().to_frame().T","cfd34e11":"train_data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ntrain_data['target'].value_counts().to_frame().T","63844b8a":"train_data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nX = train_data.drop(['Class'], axis = 1)\ny = train_data['Class']\ny.value_counts().to_frame().T","54ad6b7b":"from imblearn.over_sampling import SMOTE\nX_resampled, y_resampled = SMOTE().fit_resample(X, y)","d8729fee":"y_resampled.value_counts().to_frame().T","22f598ab":"X, y = make_classification(n_samples=500, \n                            n_classes=2,\n                            n_features=2,\n                            n_informative=2, \n                            n_redundant=0, \n                            n_repeated=0,\n                            weights=[0.99, 1-0.99],\n                            class_sep=0.3,\n                            random_state=2)\n\n# define the colours to use\nmy_cmap = matplotlib.colors.ListedColormap(['orange', 'darkorchid'])\n# now make a scatter plot\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap, alpha=0.8)\n# hide the labels\nplt.xticks([])\nplt.yticks([])\nplt.show();","b866a64d":"X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n# now plot again\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled, cmap=my_cmap, alpha=0.8)\nplt.xticks([])\nplt.yticks([])\nplt.show();","6d7b7436":"undersample = pd.concat([train_data[train_data['Class']==1], \n                         train_data[train_data['Class']==0].sample(frac=0.05, random_state=42)])\n\n# now randomly shuffle the dataframe rows in order to mix up the classes again\nundersample = undersample.sample(frac=1, random_state=42)","fd42dc53":"From this plot we can see two things; the first is that up to a ratio of 85%\/15% then our classifier is not too bad, whilst beyond 95%:5% things get really bad. And secondly, that the accuracy score is a truly awful metric for gauging how well our classifier is doing, indeed it can even be seen to be going up as things get worse! This has led to some practitioners to call this the '*accuracy paradox*'. At the very least one should use the *balanced* accuracy score.\n## kaggle Datasets\nThese are quite a few [datasets](https:\/\/www.kaggle.com\/datasets) for binary classification that can be found on kaggle, here are three examples:\n\n## [Titanic](https:\/\/www.kaggle.com\/c\/titanic):","3921264a":"### What does SMOTE do?\nHere is a scatter plot of an imbalanced dataset ","c0206a5b":"we can see that the Titanic data has a roughly 60:40 split, so we can fairly safely treat that dataset as being balanced, and get away with using the accuracy score as a metric.\n\n## [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\/):","7716cf16":"we can see that there are six points in the dataset from the minority class. \n\nWe now apply the SMOTE resampling","cd3e54b6":"# Classification: Just how imbalanced is '*imbalanced*'?\n\nIt is often stated that when performing classification it is preferable to have 'balanced' data, and if the data is imbalanced then special techniques should be used. However, just how imbalanced is '*imbalanced*'? In this short notebook we shall create a synthetic dataset consisting of 25000 samples using the Scikit-learn [make_classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_classification.html) routine having two classes (*i.e. binary classification*). We shall then vary the ratio of the classes and for each case calculate the \n[accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html),\nthe [balanced_accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html), which is given by\nthe arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate):\n\n$$ {\\mathtt{balanced-accuracy}} = \\frac{1}{2} \\left(\\frac{\\mathrm{TP}}\n{\\mathrm{TP}+\\mathrm{FN}} + \\frac{\\mathrm{TN}}\n{\\mathrm{TN}+\\mathrm{FP}}\\right)$$\n\nIt is worth mentioning that in the limit of a balanced dataset the `balanced_accuracy_score` becomes equivalent to the `accuracy_score`.\nWe shall also use the [average_precision_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.average_precision_score.html),\nthe [recall_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html), \nthe [f1_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html), which is given by: \n\n$$ F_1 = 2\\frac{precision . recall}{precision + recall}$$\n\nand the [area under the ROC curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html), where a score of 0.5 indicates that the classifier has been 'uninformative'.\n\nNote that we use the [StratifiedShuffleSplit](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html) when performing out test\/train split, thus preserving the percentage of samples for each class.\nFor our classification we shall use the Scikit-learn [RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html), with nothing other than the default settings:","3373ea69":"Now in this dataset we have a 99.8%:0.2% split, so we are most certainly dealing with imbalanced data. Performing classification on this dataset without first performing resampling would be a disaster; here one has to use techniques such as [SMOTE](https:\/\/imbalanced-learn.org\/dev\/references\/generated\/imblearn.over_sampling.SMOTE.html), which can be found in the [imbalanced-learn](https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn) package.\nIf you are unfamiliar with dealing with imbalanced data may I highly recommend the excellent notebook [\"*Credit Fraud || Dealing with Imbalanced Datasets*\"](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets) written by [Janio Martinez Bachmann](https:\/\/www.kaggle.com\/janiobachmann).\nIt is also worth mentioning that the baseline `accuracy_score` for this dataset, obtained by setting all of the predictions simply to be the majority class (this is known as the [Zero Rule](https:\/\/machinelearningcatalogue.com\/algorithm\/alg_zero-rule.html) benchmark) would be `0.998`, clearly not a useful metric to compare against when measuring any improvements.\n\n## Over-sampling the minority class: Very simple SMOTE example\nSMOTE is an over-sampling approach in which the minority class is over-sampled by creating \"synthetic\" examples, rather than say by over-sampling with replacement. \nThis is very easy to implement using the [imbalanced-learn](https:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn) package, for [example](https:\/\/imbalanced-learn.org\/stable\/over_sampling.html), using the Credit Card data from above:","5fabc585":"We can see that what SMOTE does is create new synthetic data points which are situated randomly along line segments that connect existing minority class points.\n\n# Under-sampling the majority class\nUnder-sampling the majority class has the advantage of speeding up training time, at has been mentioned in a number of publications that doing this has been found to have little negative impact on the model score. Indeed, under-sampling was a technique used by the winners of the kaggle [TalkingData AdTracking Fraud Detection Challenge](https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/56475).\n\nA na\u00efve way to perform under sampling using pandas (here keeping only 5% of the majority class `0`) is simply","b9caaecf":"resulting now in a balanced dataset","b38e6f93":"In general the [random forest classifier](https:\/\/doi.org\/10.1023\/A:1010933404324) makes for an excellent choice when it comes to creating a baseline score:\n * It effectively has only one adjustable parameter, `max_depth`, where one can control the pruning. If one is careful about the tree depth then:\n * [Random forests does not overfit. You can run as many trees as you want](https:\/\/www.stat.berkeley.edu\/~breiman\/RandomForests\/cc_home.htm). So you do not need to worry about the `n_estimators` parameter.\n * They are a non-parametric technique, so there is no need to [rescale the features](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling) beforehand, *i.e.* so that they have [a mean value of 0 and unit variance](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)\n * Also, there is no need [transform the features](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PowerTransformer.html) so as to have a Gaussian distribution\n * No need to perform [categorical feature encoding](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features) beforehand (\\*)\n * The Random Forest algorithm is resilient to missing data (\\*)\n \n(\\* Note: The scikit-learn implementation of the Random Forest requires [one hot encoding](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) for object types, and does not natively handle missing values)\n\nLater one can then compare the random forest baseline result against more advanced techniques, such as [gradient boosted trees](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting).\n \n## Results","a5f52c4d":"which can be useful for testing purposes. However, see  [imbalanced-learn](https:\/\/imbalanced-learn.org\/stable\/) for more sophisticated methods.\n# Conclusion\nEach dataset has to be taken on its own merits, and just how imbalanced is \"imbalanced\" will also depend on the technique and package that you are using to perform the classification. For example, XGBoost has `scale_pos_weight`,  CatBoost has `class_weights` and LightGBM has `pos_bagging_fraction` (see the corresponding documentation for more details).\nHowever, as a rough guide, if the data is imbalanced by more than 85:15 then resampling should definitely be used. Furthermore, I would advise against ever using the (unbalanced) accuracy score as a classification metric.\n# Related reading\n* [\"Balanced accuracy score\"](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#balanced-accuracy-score)\n* [\"F1 score\"](https:\/\/en.wikipedia.org\/wiki\/F-score) on Wikipedia\n* [\"ROC curve\"](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) on Wikipedia\n* Other Scikit-learn [classification metrics](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics)\n* [\"Oversampling and undersampling in data analysis\"](https:\/\/en.wikipedia.org\/wiki\/Oversampling_and_undersampling_in_data_analysis) on Wikipedia\n* [imbalanced-learn documentation](https:\/\/imbalanced-learn.org\/stable\/)\n\n### **See also:**\n* [Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, W. Philip Kegelmeyer \"*SMOTE: Synthetic Minority Over-sampling Technique*\", Journal of Artificial Intelligence Research vol **16** pp. 321-357 (2002)](https:\/\/www.jair.org\/index.php\/jair\/article\/view\/10302\/24590)\n* [Chen Wang, Chengyuan Deng, Suzhen Wang \"*Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost*\", Pattern Recognition Letters vol. **136** pp. 190-197 (2020)](https:\/\/doi.org\/10.1016\/j.patrec.2020.05.035)\n* [Qiuming Zhu \"*On the performance of Matthews correlation coefficient (MCC) for imbalanced dataset*\", Pattern Recognition Letters vol. **136** pp. 71-80 (2020)](https:\/\/doi.org\/10.1016\/j.patrec.2020.03.030)","5f44b2a0":"we have a 55:45 split, so again, a balanced dataset.\n\n## [Credit Card Fraud Detection](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\/):"}}