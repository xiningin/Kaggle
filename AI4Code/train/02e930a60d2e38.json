{"cell_type":{"93648f14":"code","e99b79e1":"code","713474d5":"code","f5a44900":"code","4c173816":"code","b3095bfa":"code","aa4e62dd":"code","e53d8188":"code","a1cddbc7":"code","0024d772":"code","3965644b":"code","99966767":"code","8e14fc21":"code","82ae9135":"code","764095c1":"code","cfe19ebc":"code","8b7bac9b":"code","074e19a6":"code","ad33f4ed":"code","42af30a3":"code","26e8373d":"code","0a2d697a":"code","86e83b8e":"code","df915d3f":"code","589f63ea":"code","f49b258a":"code","d3650fea":"code","22496c4b":"code","1164c221":"code","74b3d065":"code","765e0929":"code","9597e08a":"code","a9e5f851":"code","6d53c630":"code","dcdf6607":"code","974e0e98":"code","9cfb658a":"code","c239c6a3":"code","013cfbf8":"code","02036217":"code","5af6c688":"code","fc0e204e":"code","1bb4d204":"code","1fc6ea99":"code","41726703":"code","4f96b340":"code","b25da35a":"code","db1c91e6":"code","c74e3289":"code","8e63dc14":"code","58f35ea8":"code","cfabdac1":"code","466c495d":"code","59ca5a80":"code","1150718e":"code","543f8c20":"code","f2a27700":"code","122293a9":"code","0353f6ad":"code","8f780260":"code","faae43c8":"code","c4421581":"code","ca2130b9":"code","15e5be1b":"code","fd6a0247":"code","f0c6b32e":"code","4e42e87a":"markdown","e086fce0":"markdown","b92df7aa":"markdown","411c3e4b":"markdown","76716503":"markdown","6ea75d12":"markdown","5f7d57fc":"markdown","fd0e9484":"markdown","c4d90527":"markdown","7e3b1237":"markdown","1918a82b":"markdown","023b467c":"markdown","27829c1f":"markdown","02c9137c":"markdown","3e25087e":"markdown","f8616dea":"markdown","e6c60da0":"markdown","c8cf69bf":"markdown","a1f91d62":"markdown","5d857f72":"markdown","06002b4b":"markdown","1eeb40a0":"markdown","349cda0d":"markdown","81a7a640":"markdown","9a4e2f8f":"markdown","d779cad1":"markdown","ec5438d1":"markdown","0d36c8a1":"markdown","98655982":"markdown","99342295":"markdown","b5986020":"markdown","9a45d9f4":"markdown","23159ddc":"markdown","58835f2d":"markdown","2d1e2d59":"markdown","688e978d":"markdown","20806a35":"markdown","0c24523c":"markdown","3b7a0858":"markdown","cfa89d44":"markdown","c5d68a6d":"markdown","cad3e4f7":"markdown","f48d5b41":"markdown","078fec48":"markdown","1ef45cae":"markdown","df30ac95":"markdown","499823ef":"markdown","e17f7763":"markdown","b9a9b601":"markdown","f7a546f9":"markdown","0af1e3b6":"markdown","70f23543":"markdown","b5eedace":"markdown","e66df711":"markdown","94919d11":"markdown","8799692e":"markdown","61e96f9f":"markdown","26f59148":"markdown","9ea9ccc9":"markdown","2ff6ae7a":"markdown","f50ad9f0":"markdown","486c14f4":"markdown"},"source":{"93648f14":"%%capture\n!pip install -U transformers\n!pip install datasets\n!pip install fsspec==2021.5.0\n!pip install jiwer==2.2.0","e99b79e1":"import os\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset, load_metric,Dataset,concatenate_datasets,set_caching_enabled, ClassLabel\nimport pandas as pd\n\nimport random\nfrom IPython.display import display, HTML\n\nimport json\nfrom transformers import Wav2Vec2CTCTokenizer,Wav2Vec2ForCTC,Wav2Vec2Processor,Trainer,TrainingArguments,Wav2Vec2FeatureExtractor\n\nimport re\nset_caching_enabled(False)\n\nimport soundfile as sf\nimport torchaudio\n\n\nimport IPython.display as ipd\n\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom tqdm import tqdm\nimport torch\n\n# Set environment variables\nos.environ['WANDB_DISABLED '] = 'True'\n\nimport warnings\nwarnings.filterwarnings('ignore')","713474d5":"import logging\nimport transformers\ntransformers.logging.get_verbosity = lambda: logging.NOTSET","f5a44900":"transformers.logging.get_verbosity()\n","4c173816":"import datasets\ndatasets.logging.get_verbosity = lambda: logging.NOTSET","b3095bfa":"df =pd.read_csv(\"..\/input\/wolof-asr\/Train.csv\")\ndf[\"votes\"] = df[\"up_votes\"]-df[\"down_votes\"]\n\n#Add audio path and renaming columns\ndf[\"path\"] = \"..\/input\/wolof-asr\/Noise Removed\/tmp\/WOLOF_ASR_dataset\/noise_remove\/\"+df[\"ID\"]+\".wav\"\ndf.rename(columns = {'transcription':'sentence'}, inplace = True)\n\ntrain,test = train_test_split(df, test_size=0.005, random_state=42)\ncommon_voice_train_1 = Dataset.from_pandas(train[3000:])\ncommon_voice_train_2 = Dataset.from_pandas(train[0:3000])\n\ncommon_voice_test = Dataset.from_pandas(test)\n","aa4e62dd":"sum(df.sentence.value_counts()>1)","e53d8188":"train.shape,test.shape","a1cddbc7":"common_voice_train_1 = common_voice_train_1.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])\ncommon_voice_train_2 = common_voice_train_2.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])\n\ncommon_voice_test = common_voice_test.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])","0024d772":"common_voice_train_1.shape","3965644b":"def show_random_elements(dataset, num_examples=10):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    display(HTML(df.to_html()))","99966767":"show_random_elements(common_voice_train_1.remove_columns([\"path\"]), num_examples=20)","8e14fc21":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd]'\n\ndef remove_special_characters(batch):\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n    return batch","82ae9135":"common_voice_train_1 = common_voice_train_1.map(remove_special_characters)\ncommon_voice_train_2 = common_voice_train_2.map(remove_special_characters)\n\ncommon_voice_test = common_voice_test.map(remove_special_characters)","764095c1":"show_random_elements(common_voice_train_1.remove_columns([\"path\"]))","cfe19ebc":"def extract_all_chars(batch):\n  all_text = \" \".join(batch[\"sentence\"])\n  vocab = list(set(all_text))\n  return {\"vocab\": [vocab], \"all_text\": [all_text]}","8b7bac9b":"vocab_train_2 = common_voice_train_2.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train_2.column_names)\n\nvocab_train_1 = common_voice_train_1.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train_1.column_names)\nvocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)","074e19a6":"vocab_list = list(set(vocab_train_2[\"vocab\"][0]) |set(vocab_train_1[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))","ad33f4ed":"vocab_dict = {v: k for k, v in enumerate(vocab_list)}\nvocab_dict","42af30a3":"vocab_dict[\"|\"] = vocab_dict[\" \"]\ndel vocab_dict[\" \"]","26e8373d":"vocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\nlen(vocab_dict)","0a2d697a":"with open('vocab.json', 'w') as vocab_file:\n    json.dump(vocab_dict, vocab_file)","86e83b8e":"tokenizer = Wav2Vec2CTCTokenizer(\".\/vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")","df915d3f":"feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)","589f63ea":"processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)","f49b258a":"processor.save_pretrained(\".\/ASR WOLOF Data\/wav2vec2-large-xlsr-WOLOF\")","d3650fea":"common_voice_train_1[0]","22496c4b":"def speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = sf.read(batch[\"path\"])\n    batch[\"speech\"] = speech_array\n    batch[\"sampling_rate\"] = sampling_rate\n    batch[\"target_text\"] = batch[\"sentence\"]\n    return batch\n    \ndef speech_file_to_array_fn_test(batch):\n    speech_array, sampling_rate = sf.read(batch[\"path\"])\n    batch[\"speech\"] = speech_array\n    batch[\"sampling_rate\"] = sampling_rate\n    return batch","1164c221":"common_voice_test.column_names","74b3d065":"common_voice_train_1 = common_voice_train_1.map(speech_file_to_array_fn, remove_columns=common_voice_train_1.column_names,num_proc=4)\ncommon_voice_train_2 = common_voice_train_2.map(speech_file_to_array_fn, remove_columns=common_voice_train_2.column_names,num_proc=4)\n\ncommon_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names,num_proc=4)","765e0929":"import IPython.display as ipd\nimport numpy as np\nimport random\n\nrand_int = random.randint(0, len(common_voice_train_1)-1)\n\nipd.Audio(data=np.asarray(common_voice_train_1[rand_int][\"speech\"]), autoplay=True, rate=16000)","9597e08a":"rand_int = random.randint(0, len(common_voice_train_1)-1)\n\nprint(\"Target text:\", common_voice_train_1[rand_int][\"target_text\"])\nprint(\"Input array shape:\", np.asarray(common_voice_train_1[rand_int][\"speech\"]).shape)\nprint(\"Sampling rate:\", common_voice_train_1[rand_int][\"sampling_rate\"])","a9e5f851":"def prepare_dataset(batch):\n    # check that all files have the correct sampling rate\n    assert (\n        len(set(batch[\"sampling_rate\"])) == 1\n    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n\n    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n    \n    with processor.as_target_processor():\n        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n    return batch","6d53c630":"def prepare_dataset_test(batch):\n    # check that all files have the correct sampling rate\n    assert (\n        len(set(batch[\"sampling_rate\"])) == 1\n    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n\n    batch[\"input_values\"] = processor(batch[\"speech\"], padding=True,sampling_rate=batch[\"sampling_rate\"][0]).input_values\n    \n    return batch","dcdf6607":"common_voice_train_1.shape,common_voice_test.shape","974e0e98":"common_voice_train_1 = common_voice_train_1.map(prepare_dataset, remove_columns=common_voice_train_1.column_names, batch_size=8, num_proc=4, batched=True)\ncommon_voice_train_2 = common_voice_train_2.map(prepare_dataset, remove_columns=common_voice_train_2.column_names, batch_size=8, num_proc=4, batched=True)","9cfb658a":"common_voice_train = concatenate_datasets([common_voice_train_1, common_voice_train_2])","c239c6a3":"import torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lenghts and need\n        # different padding methods\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=self.padding,\n                max_length=self.max_length_labels,\n                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n                return_tensors=\"pt\",\n            )\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch","013cfbf8":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","02036217":"wer_metric = load_metric(\"wer\")","5af6c688":"def compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids)\n    # we do not want to group tokens when computing the metrics\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","fc0e204e":"len(processor.tokenizer)","1bb4d204":"model = Wav2Vec2ForCTC.from_pretrained(\n    \"facebook\/wav2vec2-large-xlsr-53\", \n    attention_dropout=0.1,\n    hidden_dropout=0.1,\n    feat_proj_dropout=0.0,\n    mask_time_prob=0.05,\n    layerdrop=0.1,\n    gradient_checkpointing=True,\n    ctc_loss_reduction=\"mean\",\n    pad_token_id=processor.tokenizer.pad_token_id,\n    vocab_size=len(processor.tokenizer)\n)","1fc6ea99":"model.freeze_feature_extractor()","41726703":"training_args = TrainingArguments(\n  output_dir=\".\/wav2vec2-large-xlsr-WOLOF\",\n  group_by_length=True,\n  per_device_train_batch_size=16,\n  gradient_accumulation_steps=2,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=40,\n  fp16=True,\n  save_steps=500,\n  eval_steps=500,\n  logging_steps=500,\n  learning_rate=3e-4,\n  warmup_steps=1000,\n  save_total_limit=2,\n)","4f96b340":"trainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=common_voice_train,\n    eval_dataset=common_voice_test,\n    tokenizer=processor.feature_extractor,\n)","b25da35a":"os.environ['WANDB_MODE'] = 'dryrun'\nos.environ['WANDB_DISABLED '] = 'True'","db1c91e6":"common_voice_train.shape","c74e3289":"trainer.train()# this take 6 hours,so i will let you train on your own. Train version -> Version 1","8e63dc14":"model.save_pretrained(\"wav2vec2-large-xlsr-WOLOF\")\nprocessor.save_pretrained(\"wav2vec2-large-xlsr-WOLOF\")","58f35ea8":"model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2-large-xlsr-WOLOF\").to(\"cuda\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-large-xlsr-WOLOF\")","cfabdac1":"val =pd.read_csv(\"..\/input\/wolof-asr\/Test.csv\")\nval[\"path\"] = \"..\/input\/wolof-asr\/Noise Removed\/tmp\/WOLOF_ASR_dataset\/noise_remove\/\"+val[\"ID\"]+\".wav\"\nval.rename(columns = {'transcription':'sentence'}, inplace = True)\ncommon_voice_val = Dataset.from_pandas(val)","466c495d":"common_voice_val = common_voice_val.remove_columns([ \"ID\",\"age\",  \"down_votes\", \"gender\",  \"up_votes\"])","59ca5a80":"val","1150718e":"common_voice_val.column_names","543f8c20":"common_voice_val = common_voice_val.map(speech_file_to_array_fn_test, remove_columns=common_voice_val.column_names)","f2a27700":"common_voice_val = common_voice_val.map(prepare_dataset_test, remove_columns=common_voice_val.column_names, batch_size=8, num_proc=4, batched=True)","122293a9":"common_voice_val.shape[0]","0353f6ad":"# common_voice_val.save_to_disk(\"\/content\/drive\/MyDrive\/ASR WOLOF Data\/Pre_Data\/Submission\")","8f780260":"# common_voice_val = Dataset.load_from_disk(\"\/content\/drive\/MyDrive\/ASR WOLOF Data\/Pre_Data\/Submission\")","faae43c8":"final_pred = []\nfor i in tqdm(range(common_voice_val.shape[0])):    \n    input_dict = processor(common_voice_val[i][\"input_values\"], return_tensors=\"pt\", padding=True)\n\n    logits = model(input_dict.input_values.to(\"cuda\")).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)[0]\n    prediction = processor.decode(pred_ids)\n    final_pred.append(prediction)","c4421581":"val.columns","ca2130b9":"val","15e5be1b":"val[\"transcription\"] = final_pred\nval[\"transcription\"] = val[\"transcription\"].str.capitalize()\nval.iloc[1390,6] = \"ah\"","fd6a0247":"val[[\"ID\",\"transcription\"]].to_csv(\"submission24.csv\",index=False)","f0c6b32e":"val[\"transcription\"] ","4e42e87a":"### Preprocess Data\n\nSo far, we have not looked at the actual values of the speech signal but just kept the path to its file in the dataset. `XLSR-Wav2Vec2` expects the audio file in the format of a 1-dimensional array, so in the first step, let's load all audio files into the dataset object.\n\nLet's first check the serialization format of the downloaded audio files by looking at the first training sample.","e086fce0":"In a final step, we use the json file to instantiate an object of the `Wav2Vec2CTCTokenizer` class.","b92df7aa":"The pretrained Wav2Vec2 checkpoint maps the speech signal to a sequence of context representations as illustrated in the figure above. A fine-tuned XLSR-Wav2Vec2 checkpoint needs to map this sequence of context representations to its corresponding transcription so that a linear layer has to be added on top of the transformer block (shown in yellow). This linear layer is used to classifies each context representation to a token class analogous how, *e.g.*, after pretraining a linear layer is added on top of BERT's embeddings for further classification - *cf.* with *\"BERT\"* section of this [blog post](https:\/\/huggingface.co\/blog\/warm-starting-encoder-decoder).\n\nThe output size of this layer corresponds to the number of tokens in the vocabulary, which does **not** depend on XLSR-Wav2Vec2's pretraining task, but only on the labeled dataset used for fine-tuning. So in the first step, we will take a look at Common Voice and define a vocabulary based on the dataset's transcriptions.","411c3e4b":"# Loading Preprocessed Data\n**\ud83d\udea8\ud83d\udea8Before running this code please run\ud83d\udea8\ud83d\udea8**:\n- 1) asr-mp3-to-wav-dataset.ipynb, which have joblib code to convert all the audio files from mp3 to wav using 8 parallel processs. \n- 2) wolof-audio-noise-removing.ipynb, which has used joblib and noise cancelation to remove silence and noise from the audio file and save it in wav format.","76716503":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https:\/\/deepnote.com?utm_source=created-in-deepnote-cell&projectId=74570277-c535-49fd-8c53-ed6cab466887' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image\/svg+xml;base64,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' > <\/img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote<\/span><\/a>","6ea75d12":"Good! This looks better. We have removed most special characters from transcriptions and normalized them to lower-case only.\n\nIn CTC, it is common to classify speech chunks into letters, so we will do the same here. \nLet's extract all distinct letters of the training and test data and build our vocabulary from this set of letters.\n\nWe write a mapping function that concatenates all transcriptions into one long transcription and then transforms the string into a set of chars. \nIt is important to pass the argument `batched=True` to the `map(...)` function so that the mapping function has access to all transcriptions at once.","5f7d57fc":"In a final step, we define all parameters related to training. \nTo give more explanation on some of the parameters:\n- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n- `learning_rate` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Common Voice dataset and might be suboptimal for other speech datasets.\n\nFor more explanations on other parameters, one can take a look at the [docs](https:\/\/huggingface.co\/transformers\/master\/main_classes\/trainer.html?highlight=trainer#trainingarguments).\n\n**Note**: If one wants to save the trained models in his\/her google drive the commented-out `output_dir` can be used instead.","fd0e9484":"### Create XLSR-Wav2Vec2 Feature Extractor","c4d90527":"A XLSR-Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n\n- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n- `sampling_rate`: The sampling rate at which the model is trained on.\n- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, XLSR-Wav2Vec2 models should **always** make use of the `attention_mask`.","7e3b1237":"Now, we can load the pretrained `XLSR-Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https:\/\/pytorch.org\/docs\/stable\/checkpoint.html) and also set the loss reduction to \"*mean*\".\n\nBecause the dataset is quite small and because WOLO audio files are quite noisy, fine-tuning Facebook's [wav2vec2-large-xlsr-53 checkpoint](https:\/\/huggingface.co\/facebook\/wav2vec2-large-xlsr-53) seems to require some hyper-parameter tuning. Therefore, I had to play around a bit with different values for dropout, [SpecAugment](https:\/\/arxiv.org\/abs\/1904.08779)'s masking dropout rate, layer dropout, and the learning rate until training seemed to be stable enough. \n","1918a82b":"we are now ready to load the original train file and add path to Noise removed wav folder.","023b467c":"### Training","27829c1f":"loading saved model","02c9137c":"### Create Wav2Vec2CTCTokenizer","3e25087e":"# Submission and Inference","f8616dea":"We can see that the transcriptions contain some special characters, such as `,.?!;:`. Without a language model, it is much harder to classify speech chunks to such special characters because they don't really correspond to a characteristic sound unit. *E.g.*, the letter `\"s\"` has a more or less clear sound, whereas the special character `\".\"` does not.\nAlso in order to understand the meaning of a speech signal, it is usually not necessary to include special characters in the transcription.\n\nIn addition, we normalize the text to only have lower case letters and append a word separator token at the end.","e6c60da0":"Next, the evaluation metric is defined. As mentioned earlier, the \npredominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well.","c8cf69bf":"Applying speech to array fucntion especially for test data set","a1f91d62":"# Initialization","5d857f72":"Next, we will create the feature extractor.","06002b4b":"Alright! The transcription can definitely be recognized from our prediction, but it is far from being perfect. Training the model a bit longer, spending more time on the data preprocessing, and especially using a language model for decoding would certainly improve the model's overall performance. \n\nFor a demonstration model on a low-resource language, the results are acceptable, however \ud83e\udd17.","1eeb40a0":"This pard is prediction part which process inputvalues and then use model to prideict logits which can later be decode.","349cda0d":"Training will take between 180 and 240 minutes depending on the GPU allocated to this notebook. While the trained model yields somewhat satisfying results on *Common Voice*'s test data of Turkish, it is by no means an optimally fine-tuned model. The purpose of this notebook is to demonstrate how XLSR-Wav2Vec2's [checkpoint](https:\/\/huggingface.co\/facebook\/wav2vec2-large-xlsr-53) can be fine-tuned on a low-resource ASR dataset.\n\nIn case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (*right mouse click -> inspect -> Console tab and insert code*).","81a7a640":"Finally, we can process the dataset to the format expected by the model for training. We will again make use of the `map(...)` function.\n\nFirst, we check that the data samples have the same sampling rate of 16kHz.\nSecond, we extract the `input_values` from the loaded audio file. In our case, this includes only normalization, but for other speech models, this step could correspond to extracting, *e.g.* [Log-Mel features](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum). \nThird, we encode the transcriptions to label ids.\n\n**Note**: This mapping function is a good example of how the `Wav2Vec2Processor` class should be used. In \"normal\" context, calling `processor(...)` is redirected to `Wav2Vec2FeatureExtractor`'s call method. When wrapping the processor into the `as_target_processor` context, however, the same method is redirected to `Wav2Vec2CTCTokenizer`'s call method.\nFor more information please check the [docs](https:\/\/huggingface.co\/transformers\/master\/model_doc\/wav2vec2.html#transformers.Wav2Vec2Processor.__call__).","9a4e2f8f":"The first component of XLSR-Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and as stated in the [paper](https:\/\/arxiv.org\/pdf\/2006.13979.pdf) does not need to be fine-tuned anymore. \nThus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part.","d779cad1":"The model will return a sequence of logit vectors:\n$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n\nA logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$.","ec5438d1":"for test fucntion we have removed `processor()` part as we dont have transcription.","0d36c8a1":"\n\n\n---\n\n${}^1$ In the [paper](https:\/\/arxiv.org\/pdf\/2006.13979.pdf), the model was evaluated using the phoneme error rate (PER), but by far the most common metric in ASR is the word error rate (WER). To keep this notebook as general as possible we decided to evaluate the model using WER.","98655982":"removing unnecessary columns ","99342295":"applying prepare data set for test which is difernt from train as I have removed transcript part.","b5986020":"# Model Available \n\n![huggingface](https:\/\/raw.githubusercontent.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/8fa97ca55964662fb882bd840cc660853695e79e\/img\/HF.svg)\n\n[![github](https:\/\/img.shields.io\/badge\/huggingface-wav2vec2_large_xlsr_53_wolof-ffbf00?logo=huggingface&style=for-the-badge)](https:\/\/huggingface.co\/kingabzpro\/wav2vec2-large-xlsr-53-wolof)\n\nYou can download my model from hugging face and use it directly to produce similar results. \n\n[![image-20210605165906300](https:\/\/github.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/blob\/main\/img\/image-20210605165906300.png?raw=true)](https:\/\/huggingface.co\/kingabzpro\/wav2vec2-large-xlsr-53-wolof)\n\nRepo Image Credit: [miro](https:\/\/miro.medium.com\/max\/700\/0*yiRqJ9RcZ9suWOFK.jpg)\n\n\n# Results\n\n![score](https:\/\/github.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/blob\/main\/img\/score.png?raw=true)\n\nYou can check my result on [Zindi](https:\/\/zindi.africa\/competitions\/ai4d-baamtu-datamation-automatic-speech-recognition-in-wolof\/leaderboard), I got 8th rank in AI4D Baamtu Datamation - Automatic Speech Recognition in WOLOF\n\n**Result**: 7.88 %","9a45d9f4":"ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. \n\nIn \ud83e\udd17 Transformers, the XLSR-Wav2Vec2 model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https:\/\/huggingface.co\/transformers\/master\/model_doc\/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https:\/\/huggingface.co\/transformers\/master\/model_doc\/wav2vec2.html#wav2vec2featureextractor).\n\nLet's start by creating the tokenizer responsible for decoding the model's predictions.","23159ddc":"**Most of the code was inspired by [fine-tune-xlsr-wav2vec2](https:\/\/huggingface.co\/blog\/fine-tune-xlsr-wav2vec2)**","58835f2d":"This seemed to have worked! Let's listen to a couple of audio files to better understand the dataset and verify that the audio was correctly loaded. \n\n**Note**: *You can click the following cell a couple of times to listen to different speech samples.*","2d1e2d59":" we can save our processed dataset and use it for faster inference.","688e978d":"Cool, now our vocabulary is complete and consists of 39 tokens, which means that the linear layer that we will add on top of the pretrained XLSR-Wav2Vec2 checkpoint will have an output dimension of 39.","20806a35":"[<ProgressiveImage src=\"https:\/\/github.com\/norvig\/pytudes\/blob\/master\/ipynb\/Advent-2020.ipynb\">](\/static\/buttons\/view-in-deepnote-white.svg)","0c24523c":"Cool, we see that all letters of the alphabet occur in the dataset (which is not really surprising) and we also extracted the special characters `\" \"` and `'`. Note that we did not exclude those special characters because: \n\n- The model has to learn to predict when a word is finished or else the model prediction would always be a sequence of chars which would make it impossible to separate words from each other.\n- From the transcriptions above it seems that words that include an apostrophe, so I decided to keep the apostrophe in the dataset. This might be a wrong assumption though.\n\nOne should always keep in mind that the data-preprocessing is a very important step before training your model. E.g., we don't want our model to differentiate between `a` and `A` just because we forgot to normalize the data. The difference between `a` and `A` does not depend on the \"sound\" of the letter at all, but more on grammatical rules - *e.g.* use a capitalized letter at the beginning of the sentence. So it is sensible to remove the difference between capitalized and non-capitalized letters so that the model has an easier time learning to transcribe speech. \n\nIt is always advantageous to get help from a native speaker of the language you would like to transcribe to verify whether the assumptions you made are sensible. ","3b7a0858":"Good! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corresponds to 16kHz, and the target text is normalized.","cfa89d44":"Removing all the unecessary columns. ","c5d68a6d":"Now, we create the union of all distinct letters in the training dataset and test dataset and convert the resulting list into an enumerated dictionary.","cad3e4f7":"## Training\n\nThe data is processed so that we are ready to start setting up the training pipeline. We will make use of \ud83e\udd17's [Trainer](https:\/\/huggingface.co\/transformers\/master\/main_classes\/trainer.html?highlight=trainer) for which we essentially need to do the following:\n\n- Define a data collator. In contrast to most NLP models, XLSR-Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning XLSR-Wav2Vec2 requires a special padding data collator, which we will define below\n\n- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n\n- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n\n- Define the training configuration.\n\nAfter having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech.","f48d5b41":"### Set-up Trainer\n\nLet's start by defining the data collator. The code for the data collator was copied from [this example](https:\/\/github.com\/huggingface\/transformers\/blob\/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3\/examples\/research_projects\/wav2vec2\/run_asr.py#L81).\n\nWithout going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\nAnalogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss.","078fec48":"It can be heard, that the speakers change along with their speaking rate, accent, and background environment, etc. Overall, the recordings sound acceptably clear though, which is to be expected from a crowd-sourced read speech corpus.\n\nLet's do a final check that the data is correctly prepared, by printing the shape of the speech input, its transcription, and the corresponding sampling rate.\n\n**Note**: *You can click the following cell a couple of times to verify multiple samples.*","1ef45cae":"XLSR-Wav2Vec2 is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems and mainly in Automatic Speech Recognition and handwriting recognition. \n\nI highly recommend reading the blog post [Sequence Modeling with CTC (2017)](https:\/\/distill.pub\/2017\/ctc\/) very well-written blog post by Awni Hannun.","df30ac95":"Now, all instances can be passed to Trainer and we are ready to start training!","499823ef":"# Fine-tuning XLSR-Wav2Vec2 for WOLOF ASR with \ud83e\udd17 Transformers\nAudio preprocessing and finetuning using wav2vec2-large-xlsr model on AI4D Baamtu Datamation - automatic speech recognition in WOLOF data.\n\n[![github](https:\/\/img.shields.io\/badge\/github-ffbf00?logo=github&color=black&style=for-the-badge)](https:\/\/github.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2)\n\n > The properly trained model can be found in **Version 1**\n\n## Zindi ASR Competition \n\n[![image-20210605165339167](https:\/\/github.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/blob\/main\/img\/image-20210605165339167.png?raw=true)](https:\/\/zindi.africa\/competitions\/ai4d-baamtu-datamation-automatic-speech-recognition-in-wolof)\n\nThe challenge will focus on a public transport use case for two reasons. First, many users of public transport can\u2019t read or speak French, so they can\u2019t interact with existing apps that help passengers to find a bus for a given destination. And second, there is already an existing app in Senegal, [WeeGo](https:\/\/www.weegolines.com\/), which helps passengers to get transport information.\n\nThe goal of this competition is to build an ASR model that will help illiterate people use existing apps to find which bus they can take to reach their destination, without having to know how to read or write.\n\n**About Baamtu Datamation (**[**baamtu.com**](https:\/\/baamtu.com\/)**)**\n\n![img](https:\/\/zindpublic.blob.core.windows.net\/public\/uploads\/image_attachment\/image\/668\/b20d1f94-eefe-4cc0-81a8-063454bb8609.png)\n\n\n\nBaamtu Datamation is a Senegalese company focused on helping companies to leverage AI and Big Data.\n\n**About AI4D-Africa; Artificial Intelligence for Development-Africa Network (**[**ai4d.ai**](https:\/\/ai4d.ai\/)**)**\n\n![img](https:\/\/zindpublic.blob.core.windows.net\/public\/uploads\/image_attachment\/image\/667\/270177ae-6e2f-4ef3-aa8d-5d8521cc5df0.png)\n\n## WOLOF Speech Data\n\n<a href=\"https:\/\/zindi.africa\/competitions\/ai4d-baamtu-datamation-automatic-speech-recognition-in-wolof\/data\">\n<img src=\"https:\/\/raw.githubusercontent.com\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/8fa97ca55964662fb882bd840cc660853695e79e\/img\/Zindi.svg\"\/><\/a>\n\nThe Zindi data is easily accessible if you have Zindia account. **I don't have permission to publish the data in repos due to their updates in terms and conditions.**\n\nThere are 6683 audio files in the train set and 1590 in the test set. You will use these files to train your model and submit your translations.\n\nThe goal of this competition is to build an ASR model that will help illiterate people use existing apps to find which bus they can take to reach their destination, without having to know how to read or write.\n\n**Files available for download:**\n\n- **clips.zip** - contains all the audio files\n- **Train.csv** - contains the audio IDs and the transcription. This is the dataset that you will use to train your model.\n- **Test.csv** - contains the audio IDs and some information about the audio file. It does not include the transcription. You will use the model you trained on Train.csv to make your translation predictions for the test files.\n- **SampleSubmission.csv** - shows the submission format for this competition, with the ID column indicating the test audio files. The \u201ctranscription\u201d column contains your predictions. The order of the rows does not matter, but the names of the ID must be correct.","e17f7763":"Let's write a short function to display some random samples of the dataset and run it a couple of times to get a feeling for the transcriptions.","b9a9b601":"Great, XLSR-Wav2Vec2's feature extraction pipeline is thereby fully defined!\n\nTo make the usage of XLSR-Wav2Vec2 as user-friendly as possible, the feature extractor and tokenizer are *wrapped* into a single `Wav2Vec2Processor` class so that one only needs a `model` and `processor` object.","f7a546f9":"## Preprocessing `.mp3` to`.wav` file format.\n\n[![View in Deepnote](https:\/\/deepnote.com\/static\/buttons\/view-in-deepnote-white.svg)](https:\/\/deepnote.com\/viewer\/github\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/blob\/main\/1-asr-mp3-to-wav-dataset.ipynb)\n\nThe audio data that was given to us was in`.mp3` format and we have to downs ample and convert it into `.wav` format in order to finetune `facebook\/wav2vec2-large-xlsr-53` model. \n\n_I have used **Joblib** Parallel to reduce the conversion time._\n\n### Reference\n\nIdea is from this notebook :[Kaggle]( https:\/\/www.kaggle.com\/raghaw\/panda-medium-resolution-dataset-25x256x256)\n\nJoblib Parallel form : [YouTube](https:\/\/www.youtube.com\/watch?v=Ny3O4VpACkc&list=PL98nY_tJQXZnoCDfHLo58tRHUyNvrRVzn&index=4)\n\n\n\n## Removing silence and noise from Audio files.\n\n[![View in Deepnote](https:\/\/deepnote.com\/static\/buttons\/view-in-deepnote-white.svg)](https:\/\/deepnote.com\/viewer\/github\/kingabzpro\/WOLOF-ASR-Wav2Vec2\/blob\/main\/2-wolof-audio-noise-removing.ipynb)\n\nThe main function was inspired by [Remove Background\/Dead Noise](https:\/\/www.kaggle.com\/jainarindam\/imp-remove-background-dead-noise), where he have used simple audio library to remove both noise and silence. Then I used `Joblib` to increase the processing power. All the files were then zipped and ready to be used in both **Kaggle** and **Colab** for performance enhancement. ","0af1e3b6":"Before we start, let's install both `datasets` and `transformers` from master. Also, we need the `torchaudio`, `soundfile` and `librosa` package to load audio files and the `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https:\/\/huggingface.co\/metrics\/wer) metric ${}^1$.","70f23543":"Alright! The transcriptions look fairly clean. ","b5eedace":"Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n\nA pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in data points being twice as long. Thus, \nbefore fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n\nXLSR-Wav2Vec2 was pretrained on the audio data of [Babel](https:\/\/huggingface.co\/datasets\/librispeech_asr), \n[Multilingual LibriSpeech (MLS)](https:\/\/ai.facebook.com\/blog\/a-new-open-data-set-for-multilingual-speech-research\/), and [Common Voice](https:\/\/huggingface.co\/datasets\/common_voice). Most of those datasets were sampled at 16kHz, so that WOLOF audio is sampled at 48kHz, has to be downsampled to 16kHz for training. Therefore, we will have to downsample our fine-tuning data to 16kHz in the following. I have already downsampled and converted my audio files in mp3-wav notebook.\n\n","e66df711":"To make it clearer that `\" \"` has its own token class, we give it a more visible character `|`. In addition, we also add an \"unknown\" token so that the model can later deal with characters not encountered in Common Voice's training set. \n\nFinally, we also add a padding token that corresponds to CTC's \"*blank token*\". The \"blank token\" is a core component of the CTC algorithm. For more information, please take a look at the \"Alignment\" section [here](https:\/\/distill.pub\/2017\/ctc\/).","94919d11":"Importing All Libraries.","8799692e":"Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https:\/\/ai.facebook.com\/blog\/wav2vec-20-learning-the-structure-of-speech-from-raw-audio\/) by Alexei Baevski, Michael Auli, and Alex Conneau.  Soon after the superior performance of Wav2Vec2 was demonstrated on the English ASR dataset LibriSpeech, *Facebook AI* presented XLSR-Wav2Vec2 (click [here](https:\/\/arxiv.org\/abs\/2006.13979)). XLSR stands for *cross-lingual  speech representations* and refers to XLSR-Wav2Vec2`s ability to learn speech representations that are useful across multiple languages.\n\nSimilar to Wav2Vec2, XLSR-Wav2Vec2 learns powerful speech representations from hundreds of thousands of hours of speech in more than 50 languages of unlabeled speech. Similar, to [BERT's masked language modeling](http:\/\/jalammar.github.io\/illustrated-bert\/), the model learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network.\n\n\n\nThe authors show for the first time that massively pretraining an ASR model on cross-lingual unlabeled speech data, followed by language-specific fine-tuning on very little labeled data achieves state-of-the-art results. See Table 1-5 of the official [paper](https:\/\/arxiv.org\/pdf\/2006.13979.pdf).","61e96f9f":"\n\n---\n\n${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n\n${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`.","26f59148":"## Prepare Data, Tokenizer, Feature Extractor","9ea9ccc9":"loading our dataset, please put orginal `Test.csv` and cleaned audio file path in `val[\"path\"]`","2ff6ae7a":"one values is missing to we have added dummy transcription so that I can make submission directly. Using capitalize dose improve score but not that much.","f50ad9f0":"Next, we can prepare the dataset.","486c14f4":"Let's now save the vocabulary as a json file."}}