{"cell_type":{"fdd20af9":"code","a1b63b87":"code","256a93a6":"code","ce9a98c0":"code","fd375687":"code","4c4ec931":"code","55fd0ef4":"code","2228c937":"code","0bf72419":"code","cedd0405":"markdown","2d875d56":"markdown","f333c740":"markdown","6974cedd":"markdown","a027fc01":"markdown","94567e63":"markdown","460e7a7e":"markdown","3127de68":"markdown","3371e0dd":"markdown"},"source":{"fdd20af9":"# Import necessary modules for data analysis and data visualization. \n# Data analysis modules\n# Pandas is probably the most popular and important modules for any work related to data management. \nimport pandas as pd\n\n# numpy is a great library for doing mathmetical operations. \nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n## Importing the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\nimport os ## imporing os\nprint(os.listdir(\"..\/input\/\")) ","a1b63b87":"## Take a look at the overview of the dataset. \ntrain.sample(5)","256a93a6":"test.sample(5)","ce9a98c0":"## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n\n## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)\n\n## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\n\nall_data.Cabin = [i[0] for i in all_data.Cabin]\n\nwith_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n\ndef cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    \n\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers\n\nmissing_value = test[(test.Pclass == 3) & (test.Embarked == \"S\") & (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)\n\n## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n\n# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\n# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list\/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)\n\n## Title\n## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]\n\n## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1\n\ndef family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n\ntrain['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\n\ntrain.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)\n\n## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare\/train.family_size\ntest['calculated_fare'] = test.Fare\/test.family_size\n\ndef fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)\n\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\n\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)\n\n## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)\n\n## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);\n\n## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n\"\"\"train.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)\"\"\"\n\n# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n#age_filled_data_nor = NuclearNormMinimization().complete(df1)\n#Data_1 = pd.DataFrame(age_filled_data, columns = df1.columns)\n#pd.DataFrame(zip(Data[\"Age\"],Data_1[\"Age\"],df[\"Age\"]))\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X,y,test_size = .33, random_state = 0)\n\n# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\ntrain_x = sc.fit_transform(train_x)\n## transforming \"test_x\"\ntest_x = sc.transform(test_x)\n\n## transforming \"The testset\"\ntest = sc.transform(test)\n\n## changing calculated_fare type\ntrain.calculated_fare = train.calculated_fare.astype(float)\n\n## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)","fd375687":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, StratifiedKFold\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) ","4c4ec931":"print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)","55fd0ef4":"dectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)","2228c937":"from sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image\ndot_data = StringIO()  \nexport_graphviz(dectree_grid, out_file=dot_data,  \n                feature_names=column_names,  class_names = ([\"Survived\" if int(i) is 1 else \"Not_survived\" for i in y.unique()]),\n                filled=True, rounded=True,\n                proportion=True,\n                special_characters=True)  \n(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\n\n## alternative tree\n#import graphviz\n#from sklearn import tree\n#dot_data = tree.export_graphviz(decision_tree=dectree_grid, out_file=None, feature_names=column_names, )\n#graph = graphviz.Source(dot_data)\n#graph.render(\"house\")\n#graph\n\nImage(graph.create_png())","0bf72419":"## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)","cedd0405":"**> Sample test dataset**","2d875d56":"**> Sample train dataset**","f333c740":"Let's get familier with some of the hyperparameters of Decision Tree before we jump right in to the good stuff. \n\n><b>criterion : string, optional (default=\u201dgini\u201d)<\/b>\nThe function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain.\n\n><b>splitter : string, optional (default=\u201dbest\u201d)<\/b>\n    The strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n    \n><b>max_depth : int or None, optional (default=None)<\/b>\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n><b>min_samples_split : int, float, optional (default=2)<\/b>\n    The minimum number of samples required to split an internal node:\nIf int, then consider `min_samples_split` as the minimum number.\nIf float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.\nChanged in version 0.18: Added float values for fractions.\n\n><b>min_samples_leaf : int, float, optional (default=1)<\/b>\n    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n* If int, then consider `min_samples_leaf` as the minimum number.\n* If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.\n\n><b>min_weight_fraction_leaf : float, optional (default=0.)<\/b>\nThe minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n\n><b>max_features : int, float, string or None, optional (default=None)<\/b>\nThe number of features to consider when looking for the best split:\n* If int, then consider `max_features` features at each split.\n* If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n* If \u201cauto\u201d, then `max_features=sqrt(n_features).`\n* If \u201csqrt\u201d, then `max_features=sqrt(n_features).`\n* If \u201clog2\u201d, then `max_features=log2(n_features).`\n* If None, then `max_features=n_features.`\nNote: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n\n><b>random_state : int, RandomState instance or None, optional (default=None)<\/b>\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random.`\n\n><b>max_leaf_nodes : int or None, optional (default=None)<\/b>\nGrow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n\n><b>min_impurity_decrease : float, optional (default=0.)<\/b>\nA node will be split if this split induces a decrease of the impurity greater than or equal to this value.\nThe weighted impurity decrease equation is the following:\n`N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity`\n                    `- N_t_L \/ N_t * left_impurity)`\nwhere `N` is the total number of samples, `N_t` is the number of samples at the current node, `N_t_L` is the number of samples in the left child, and `N_t_R` is the number of samples in the right child.\n`N`, `N_t`, `N_t_R` and `N_t_L` all refer to the weighted sum, if `sample_weight` is passed.\nNew in version 0.19.\n\n><b>min_impurity_split : float, (default=1e-7)<\/b>\nThreshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.`Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.`\n\n><b>class_weight : dict, list of dicts, \u201cbalanced\u201d or None, default=None<\/b>\n* Weights associated with classes in the form `{class_label: weight}`. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n* Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].\n* The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y))\n* For multi-output, the weights of each column of y will be multiplied.\n* Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n\n><b>presort : bool, optional (default=False)<\/b>\nWhether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.\n\n### Grid search on Decision Tree","6974cedd":"These are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night.","a027fc01":"<h3>Some advantages of decision trees are:<\/h3>\n* Simple to understand and to interpret. Trees can be visualised.\n* Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n* Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n* Able to handle multi-output problems.\n* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\n<h3>The disadvantages of decision trees include:<\/h3>\n* Decision-tree learners can create over-complex trees that do not generalise the data well. This is called **overfitting**. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem. \n* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n<h4 align=\"right\">Source:Sklearn<\/h4>\n<h4>Resources: <\/h4> \n* [Let\u2019s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8](https:\/\/www.youtube.com\/watch?v=LDRbO9a6XPU)\n* [Super data science](https:\/\/www.udemy.com\/machinelearning\/learn\/v4\/t\/lecture\/5732730?start=8)\n* [General Assembly](https:\/\/generalassemb.ly) \n* [Decision tree playlist](https:\/\/www.youtube.com\/watch?v=AmCV4g7_-QM&list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO&index=3) \n* [How to calculate Gini Index](https:\/\/www.youtube.com\/watch?v=7VeUPuFGJHk)\n* [Sklearn](https:\/\/scikit-learn.org\/stable\/modules\/tree.html)\n\n","94567e63":"Above is a full-grown decision tree. I think having a tree shown like that can help a lot in understanding how the decision tree works.","460e7a7e":"I am going to do all sorts of preparation ( including data munging, preparation, replacing NULL values, standard scaling...dummy variables) on the Titanic dataset to make it ready for the machine learning algorithm. If you would like to find out how I did it step-by-step. Please click <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\"> here.<\/a>","3127de68":"<h1>Decision Tree Classifier<\/h1>\n\nThe idea behind the decision tree is pretty simple. To build a tree, we use a decision tree algorithm called CART. CART stands for Classification and Regression Trees. This means that, the decision tree can be used for both regression and classifications problems. To perform classification or regression, decision trees make sequential, hierarchical decisions about the outcome variable based on the predictor data.\n\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. We call this the leaf node. Here is an excellent example of the decision tree. \n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*GgUEazXbr2CYLUsPgdYzDw.png\" width=\"600\">\n\nIf you think about it, this is sort of like the \"20 Questions\" game where an individual or group has the opportunity to identify an unknown object by asking a series of up to 20 \"yes\" or \"no\" questions. For example, if we want to tie it with this dataset, let's say we are asking to find out whether a certain person in the test dataset survived or not. We may ask a question like, is the person \"male\" or \"female.\" Let's say the answer is \"female.\" Then the algorithm might ask about the person's Pclass. Let's say the Pclass is 1.\n\nAs you can see by inquiring these details about this person we can give a certain percentage with confidence about a person's survival status. If you want to know more how the decision tree does all these mathematically and technically,  you can keep on reading. Otherwise, you can skip to the next section. \n\nAt first, we will add a root node for the tree. All node receives a list of rows from the dataset.  The root node receives the whole dataset. This node then asks true\/false questions about one of the features. Based on the answer, we split the dataset into smaller subsets. The number of subsets depends on the unique values of that feature. These subsets then become the input of each child nodes. Each child node then asks another question about a different feature leading to break down further into more subgroups. This process goes on. As we keep reading, one questions beg to be answered. How do we know which questions to ask and when? \n\n<b>The trick to building an effective tree is to know which questions to ask and when.<\/b> To find the best possible questions, we need to quantify how much uncertainty is there in a single node and how much a question help to unmix the labels. We can find out how much uncertainty\/impurity is there in a single node using a concept called <i>Gini Impurity<\/i>. We can find out how much a question reduces that uncertainty using a matrix called <i>Information gain.<\/i> These two combined helps us decide which question to ask in each node. Let's dive into how these are calculated.\n\n<h3>Gini Impurity:<\/h3>\nGini Impurity ranges between 0 and 1, where a lower value indicates less uncertainty and higher value indicates higher uncertainty. In other words, when we look at the Gini index, we want to look the at the lower value of Gini Index as those are the once produced the most unmixed subsets. [here](https:\/\/www.youtube.com\/watch?v=7VeUPuFGJHk) is an excellent video about calculating the Gini index.\n\nGini impurity quantifies our chances of being incorrect if we randomly assign a label to an example in the same set. For example, Let's say there are 5 different mixes with five various names, Our chance of being right is 1\/5. So, our chances of being wrong is (1-1\/5) = 0.8. So, 0.8 is our Gini Impurity score. \n\nHere is the equation for Gini score. \n\n### $$ \\text{Gini} = \\sum_{i=1}^{classes} p(i\\;|\\;t)(1 -p(i\\;|\\;t)) = 1 - \\sum_{i=1}^{classes} p(i\\;|\\;t)^2 $$\n\n\n#### This part of the kernel is a working progress. Please check back again for future updates.#### \n\n\n<h3>Information Gain<\/h3>\nInformation gain is basically difference between the Gini Index of the parent note and the <b>weighted<\/b> average of the child nodes.","3371e0dd":"\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/cdn-uploads\/Decision_Tree-2.png\" class=\"center\" width=\"600\" >\n<h4 align=\"right\">Source: GeeksforGeeks<\/h4>\n## <div style=\"text-align: center\" >Decision Tree with Titanic Dataset<\/div>\n<div style=\"text-align: center\"> Being a part of Kaggle gives me unlimited access to learn, share and grow as a Data Scientist. In this kernel, I want to solve <font color=\"red\"><b>Titanic competition<\/b><\/font>, a popular machine learning dataset using <font color=\"red\"><b>Decision Tree Classifier<\/b><\/font>. This kernel is a part of my machine learning series articles. If you would like to find out more about other machine learning models, please checkout this <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\/edit\/run\/13339359\">this<\/a> kernel. <\/b> I will also describe how best to evaluate model results along with many other tips. So let's get started.<\/div>\n\n\n***\n<div style=\"text-align:center\"> If there are any recommendations\/changes you would like to see in this notebook, please <b>leave a comment<\/b>. Any feedback\/constructive criticism would be genuinely appreciated. <b>This notebook is always a work in progress. So, please stay tuned for more to come.<\/b><\/div>\n\n\n<div style=\"text-align:center\">If you like this notebook or find this notebook helpful, Please feel free to <font color=\"red\"><b>UPVOTE<\/b><\/font> and\/or <font color=\"Blue\"><b>leave a comment.<\/b><\/font><\/div><br>\n\n<div style=\"text-align: center\"><b>You can also Fork and Run this kernel from <a href=\"https:\/\/github.com\/masumrumi\">Github<\/b><\/a>\n    <\/div>\n\n### <div style=\"text-align: center\">Stay Tuned for More to Come!!<\/div>"}}