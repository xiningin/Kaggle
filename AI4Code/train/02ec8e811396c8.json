{"cell_type":{"d59886a5":"code","f8577b6e":"code","eb0ce1ea":"code","d4d9c4ac":"code","d996364e":"code","6ad75a49":"code","44520026":"code","1fef5ee2":"code","6d095179":"code","a8561f13":"code","356f21dd":"code","ee08fa6a":"code","05dbd058":"code","8a04d7fa":"code","fb7bf69e":"markdown","d69676ea":"markdown","e84b214d":"markdown","194bad93":"markdown","41fbacd5":"markdown","84e31e4e":"markdown","bd402494":"markdown","d6af0fb9":"markdown","dcb9ff11":"markdown","bc4420e0":"markdown","e950f3cd":"markdown","3a136400":"markdown","83596185":"markdown","ff612084":"markdown","1aad74fb":"markdown"},"source":{"d59886a5":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n\ndata = pd.read_csv('..\/input\/weatherAUS.csv')\ndata.sample(5)","f8577b6e":"\ndata.drop(['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RISK_MM'], axis=1, inplace=True)\ndata.head(5)\ndata.fillna(data.mean(), inplace=True)\ndata.head(5)","eb0ce1ea":"# Now we can change that day and next days'predictions (yes and no) to 1 and 0:\ndata.RainToday = [1 if each == 'Yes' else 0 for each in data.RainToday]\ndata.RainTomorrow = [1 if each == 'Yes' else 0 for each in data.RainTomorrow]\ndata.sample(3)","d4d9c4ac":"y = data.RainTomorrow.values\nx_data = data.drop('RainTomorrow', axis=1)\nx_data.head()","d996364e":"# In order to scale all the features between 0 and 1:\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head(5)","6ad75a49":"# importing sklearn's library for splitting our dataset:\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=75)\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\nprint('x_train shape is: ', x_train.shape)\nprint('y_train shape is: ', y_train.shape)\nprint('x_test shape is: ', x_test.shape)\nprint('y_test shape is: ', y_test.shape)","44520026":"def initialize_weight_bias(dimension):\n    w = np.full((dimension,1), 0.01)    # Create a matrix by the size of (dimension,1) and fill it with the values of 0.01\n    b = 0.0\n    return w,b","1fef5ee2":"def sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","6d095179":"def forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation:\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    \n    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)     # loss function formula\n    cost = (np.sum(loss)) \/ x_train.shape[1]                               # cost function formula\n    \n    # backward propagation:\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}\n    \n    return cost, gradients","a8561f13":"def update(w, b, x_train, y_train, learning_rate, nu_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Initialize for-back propagation for the number of iteration times. Then updating w and b values and writing the cost values to a list:  \n    for i in range(nu_of_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n    \n        # Update weight and bias values:\n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        # Show every 20th value of cost:\n        if i % 20 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print('Cost after iteration %i: %f' %(i,cost))\n    \n    parameters = {'weight': w, 'bias':b}\n    \n    # Visulization of cost values:\n    plt.plot(index, cost_list2)\n    plt.xlabel('Nu of Iteration')\n    plt.ylabel('Cost Function Value')\n    plt.show()\n    \n    return parameters, gradients, cost_list","356f21dd":"def prediction(w, b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    return y_prediction","ee08fa6a":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, nu_of_iteration):\n    dimension = x_train.shape[0]\n    w, b = initialize_weight_bias(dimension)    \n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, nu_of_iteration)\n    \n    y_test_predictions = prediction(parameters['weight'], parameters['bias'], x_test) \n    \n    print('Test accuracy: {}%'.format(100 - np.mean(np.abs(y_test_predictions - y_test))*100))","05dbd058":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, nu_of_iteration=400)","8a04d7fa":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint('Test accuracy of sklearn logistic regression library: {}'.format(lr.score(x_test.T, y_test.T)))","fb7bf69e":"**2.6. Dividing Dataset for Training and Testing the Model**","d69676ea":"**3.6. Implementing Logistic Regression Using Test Data**","e84b214d":"**3.1. Creating the Initial Parameters (Weight and Bias)**","194bad93":"**4. LOGISTIC REGRESSION WITH SKLEARN LIBRARY**","41fbacd5":"**3.2. Defining the Sigmoid Function**","84e31e4e":"**3.4. Defining Update Parameters Method**","bd402494":"**3.5. Defining Prediction Method**","d6af0fb9":"Importing Data and Cleaning Data","dcb9ff11":"**1. IMPORTING LIBRARIES**","bc4420e0":"**2. DATA PREPARATION**DATA PREPARATION","e950f3cd":"**3. LOGISTIC REGRESSION**","3a136400":"**3.3. Defining Forward and Backward Propagation**","83596185":"**2.4. Excluding Tomorrow's Prediction from the Dataset**","ff612084":"**2.5. Normalization Progress**","1aad74fb":"**2.3. Converting Predictions to Binary for Logistic Regression**"}}