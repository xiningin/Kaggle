{"cell_type":{"afd51c22":"code","090c484e":"code","2e8817a6":"code","49c48b36":"code","20f7a138":"code","95fe0a0f":"code","996f0298":"code","3b233101":"code","1344f24f":"code","3ab7c423":"code","2783729a":"code","a16534e5":"code","efa7ceb5":"code","06dec728":"code","a52aa15e":"code","8149058c":"code","0ab1c1a0":"code","fa3d16b9":"code","393fa58d":"code","80290abf":"code","1c90be15":"code","56a84248":"code","be24f555":"code","22c0b751":"code","67dc2eab":"code","a64d0a47":"code","ba4eacbc":"code","f689acf7":"code","f6057bcd":"code","fc1d16ff":"code","68c7210b":"code","d3d721d1":"code","69a7f051":"code","efb30bb9":"code","18b1826a":"code","a7b8587d":"code","646fd2c0":"code","37b60861":"code","1ff17128":"code","7d8e5ada":"code","290f1ed9":"code","2cc3a1a8":"code","e4facf38":"code","f89d74b6":"code","4c45886f":"code","55a85cd0":"code","2fc00f30":"code","de359d3f":"code","8db769c4":"code","cc798de6":"code","ea06eff2":"code","c992fd89":"code","4671e056":"code","2705ae97":"code","cdf89a09":"code","c508c277":"code","666ade7c":"code","d057c866":"code","36a151a5":"code","4d322bec":"code","4f792cdc":"code","a268714e":"code","6748da53":"code","1475efbd":"code","80acf296":"code","8739fbb5":"code","c1dcb1d7":"code","434644d9":"code","63a3b923":"code","bab74922":"code","e8dd352f":"code","5cbfbe65":"code","e3fce2bc":"code","ad534a3f":"code","e74dde1e":"code","5284f37e":"code","31b994da":"code","e9bc2ba3":"code","f85f837c":"code","260d0fb9":"code","f778c4c2":"code","f47e4aaf":"code","79bf87db":"code","4f1afff9":"code","cd262081":"code","0f871ef8":"code","82887d04":"code","914cb133":"markdown","69b96aed":"markdown","eb156fb6":"markdown","bc7f8069":"markdown","78d8fbe7":"markdown","68e4c864":"markdown","c463971d":"markdown","be2dd10a":"markdown","6f4e89f7":"markdown","0f2373ff":"markdown","4a015423":"markdown","269e22c5":"markdown","731fbace":"markdown","e961554a":"markdown","b3b558c5":"markdown"},"source":{"afd51c22":"## Install the necessary Libraries (LANGUAGE DETECTION)\n!pip install fasttext\n!pip install pycountry","090c484e":"import numpy as np\nimport pandas as pd\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport tqdm as tqdm\nimport itertools\n#from google.colab import drive\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"darkgrid\")\n\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nimport fasttext\nfrom pycountry import languages\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.cm as cm\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","2e8817a6":"## Read the data from EXCEL\nincidents = pd.read_excel(\"..\/input\/automatic-ticket-assignment-using-nlp\/Input Data Synthetic (created but not used in our project).xlsx\")\n#model_data = pd.read_excel(\"modelInput.xlsx\")\n## Quick View \nincidents.head(3)","49c48b36":"## basic info\nincidents.info()","20f7a138":"## Shape\nincidents.shape","95fe0a0f":"incidents.columns","996f0298":"incidents[\"Assignment group\"].unique()","3b233101":"incidents[\"Assignment group\"].nunique()","1344f24f":"## find nulls\nincidents[incidents.isnull().any(axis=1)]","3ab7c423":"# drop nulls\nincidents.dropna(inplace=True)\nincidents.shape","2783729a":"## Duplicates \nsub_incidents = incidents[['Short description', 'Description', 'Caller','Assignment group']].copy()\nduplicateRowsDF = sub_incidents[sub_incidents.duplicated()]\nduplicateRowsDF","a16534e5":"# Remove Duplicates\nincidents_upd = incidents.drop_duplicates(['Short description', 'Description', 'Caller', 'Assignment group'])","efa7ceb5":"## Group by Categories\ndf_grp = incidents_upd.groupby(['Assignment group']).size().reset_index(name='counts')\ndf_grp","06dec728":"df_grp.describe()","a52aa15e":"sns.set_style(\"darkgrid\")\n\n## distibution based on Percentage\ndf_grp[\"count_perc\"] = round((df_grp[\"counts\"]\/incidents.shape[0])*100,2)\ndf_grp.sort_values([\"count_perc\"], axis=0, \n                 ascending=False, inplace=True) ","8149058c":"## View the Distribution of all Records\nplt.subplots(figsize = (20,5))\n \nplt.plot(df_grp[\"Assignment group\"], df_grp[\"count_perc\"]) \nplt.xlabel('Assignment Group') \nplt.ylabel('Percentage') \nplt.xticks(rotation=90)\nplt.title('Incident Distribution') \n  \n \nplt.show() ","0ab1c1a0":"## View the Distribution of only records that DONOT belong to GRP_0\ndf_test = df_grp[df_grp[\"Assignment group\"] != 'GRP_0']\nplt.subplots(figsize = (20,5))\n\nplt.plot(df_test[\"Assignment group\"], df_test[\"counts\"]) \nplt.xlabel('Assignment Group') \nplt.ylabel('counts') \nplt.xticks(rotation=90)\n\nplt.title('Incident Distribution For non GRP_0') \n  \n\nplt.show() ","fa3d16b9":"## merging the Short Description and Description Columns\nincidents_sum = pd.DataFrame({\"Description\": incidents_upd[\"Short description\"] + \" \" + incidents_upd[\"Description\"],\n                             \"AssignmentGroup\": incidents_upd[\"Assignment group\"]}, \n                                                       columns=[\"Description\",\"AssignmentGroup\"])","393fa58d":"## NLTK Downloads\n\nnltk.download('stopwords')\nstop = set(stopwords.words('english')) \nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')","80290abf":"import nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)\n\n\ntemp =[]\nfor sentence in incidents_sum[\"Description\"]:\n    sentence = sentence.lower()\n    #sentence = sentence.str.replace('\\d+', '')\n    cleanr = re.compile('<.*?>')\n    sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n    sentence = re.sub(r'\\S+@\\S+', 'EmailId', sentence)\n    sentence = re.sub(r'\\'', '', sentence, re.I|re.A)\n    sentence = re.sub(r'[0-9]', '', sentence, re.I|re.A)\n    #print (\"Sentence1.5 = \",sentence)\n    sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n    #print (\"Sentence2 = \",sentence)\n    sentence = sentence.lower()\n    sentence = re.sub(r'com ', ' ', sentence, re.I|re.A)\n    sentence = re.sub(r'hello ', ' ', sentence, re.I|re.A)\n    l_sentence = lemmatize_sentence(sentence)\n\n    words = [word for word in l_sentence.split() if word not in stopwords.words('english')]\n    temp.append(words)","1c90be15":"incidents_sum[\"Lemmatized_clean\"] = temp","56a84248":"import fasttext\nfrom pycountry import languages\nPRETRAINED_MODEL_PATH = '\/tmp\/lid.176.bin'\nmodel = fasttext.load_model(PRETRAINED_MODEL_PATH)\n\ntemp1 =[]\ntemp2 = []\nfor sentence in incidents_sum[\"Lemmatized_clean\"]:\n    acc = 0\n    try:\n      predictions = model.predict(sentence)\n      prediction_lang = re.sub('__label__', '',str(predictions[0][0][0]))\n      acc = round(predictions[1][0][0],2) * 100\n      language_name = languages.get(alpha_2=prediction_lang).name\n    except:\n      language_name = \"NOT DETERMINED\"\n    temp1.append(language_name)\n    temp2.append(acc)","be24f555":"incidents_sum[\"Language\"] = temp1\nincidents_sum[\"Accuracy\"] = temp2","22c0b751":"## we load the data into a EXcel to check the output manually (faster)\nincidents_sum.to_excel(\"TempOutput.xlsx\")","67dc2eab":"## Additional Text Cleaning\n## we noticed that there are several rows that have all junk characters (tried decoding UTF-8 but did not work)\n## this indicated that the characters were junk and we planned to remove the 36 rows that had the junk characters\n## We initially tried regular expression but not sure why this did not remove all of the junk characters\n## We observed that lemmatization of the junk characters returned blank array and hence we use that condition\n## to remove the rows with Junk Characters\n\nincidents_sum1 = incidents_sum[incidents_sum['Lemmatized_clean'].map(lambda d: len(d)) > 0]","a64d0a47":"## GRP_0 Visualization:\n\n## create a column to mark records with GRP_0 and non GRP_0=>GRP_X\nincidents_sum1['GRP_MOD'] = incidents_sum1['AssignmentGroup'].apply(lambda x: 'GRP_X' if x != 'GRP_0' else x)","ba4eacbc":"stopwords = set(STOPWORDS)\n## function to create Word Cloud\ndef show_wordcloud(data, title):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","f689acf7":"## view word cloud for GRP_0\ntext_Str = incidents_sum1['Lemmatized_clean'][incidents_sum1['GRP_MOD'].isin([\"GRP_0\"])].tolist()\nshow_wordcloud(text_Str, \"GRP_0 WORD CLOUD\")","f6057bcd":"## GRP_X Visualization:\ntext_Str = incidents_sum1['Lemmatized_clean'][incidents_sum1['GRP_MOD'].isin([\"GRP_X\"])].tolist()\nshow_wordcloud(text_Str, \"GRP_X WORD CLOUD\")","fc1d16ff":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_1\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_9 WORD CLOUD\" )","68c7210b":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_39\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_9 WORD CLOUD\" )","d3d721d1":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_1\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_9 WORD CLOUD\" )","69a7f051":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_8\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_8 WORD CLOUD\" )","efb30bb9":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_24\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_24 WORD CLOUD\" )","18b1826a":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_12\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_12 WORD CLOUD\" )","a7b8587d":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_9\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_9 WORD CLOUD\" )","646fd2c0":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_2\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_2 WORD CLOUD\" )","37b60861":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_16\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_16 WORD CLOUD\" )","1ff17128":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_13\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_13 WORD CLOUD\" )","7d8e5ada":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_25\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_25 WORD CLOUD\" )","290f1ed9":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_29\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_29 WORD CLOUD\" )","2cc3a1a8":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_33\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_33 WORD CLOUD\" )","e4facf38":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_38\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_38 WORD CLOUD\" )","f89d74b6":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_45\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_45 WORD CLOUD\" )","4c45886f":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_40\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","55a85cd0":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_41\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_41 WORD CLOUD\" )","2fc00f30":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_42\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","de359d3f":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_43\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","8db769c4":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_44\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","cc798de6":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_45\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","ea06eff2":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_46\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","c992fd89":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_47\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_47 WORD CLOUD\" )","4671e056":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_48\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_40 WORD CLOUD\" )","2705ae97":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_49\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_49 WORD CLOUD\" )","cdf89a09":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_37\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_30 WORD CLOUD\" )","c508c277":"text_Str1 = incidents_sum['Lemmatized_clean'][incidents_sum['AssignmentGroup'].isin([\"GRP_69\",\"GRP_70\",\"GRP_71\",\"GRP_72\",\"GRP_73\"])].tolist()\nshow_wordcloud(text_Str1,\"GRP_8 WORD CLOUD\" )","666ade7c":"def find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    print (\"Iters\", iters)\n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=200, batch_size=300, random_state=20).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1,figsize=(15,5))\n   \n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\n    #set_size(50,5)\n    plt.show()\n\n","d057c866":"  ## Function to PLot the Clusters using Tsne and PCA\n  def plot_tsne_pca(data, labels, size_d, component_count):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=size_d, replace=False)\n    \n    pca = PCA(n_components=component_count).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=component_count).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=size_d, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i\/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot')\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot')","36a151a5":"# create teh vectorizer\ntfidf = TfidfVectorizer(min_df=5 ,use_idf=True )\n\n## join the list members of the column\nincidents_sum1['Lemmatized_clean_upd']=[\" \".join(description) for description in incidents_sum1['Lemmatized_clean'].values]\n\n## From the word clouds it was noticed that the earlier Regular Expression replacement of numbers did not work\nincidents_sum1['Lemmatized_clean_upd'] = incidents_sum1['Lemmatized_clean_upd'].str.replace('\\d+', '')\n\n\n## fitting the vectorizer\ntfidf.fit(incidents_sum1.Lemmatized_clean_upd)\ntext = tfidf.transform(incidents_sum1.Lemmatized_clean_upd)\n\n## Get Feature Names and Store the values in a Dataframe\ntf_matrix = text.toarray()\nvocab = tfidf.get_feature_names()\ntf_df = pd.DataFrame(np.round(tf_matrix, 2), columns=vocab)\n\n## View the nunmber of Features\ntf_df.columns","4d322bec":"tf_df.shape","4f792cdc":"find_optimal_clusters(text, 73)","a268714e":"clusters = MiniBatchKMeans(n_clusters=34, init_size=100, batch_size=200, random_state=20).fit_predict(text)\nclusters.shape","6748da53":"num_features = 8372\npca_comp_count = 100\nplot_tsne_pca(text, clusters,num_features,pca_comp_count )","1475efbd":"df = incidents_sum1.copy()\ndf.drop(columns=[\"Description\",\"Lemmatized_clean\",\"Language\",\"Accuracy\"], inplace=True)\ndf1 = df[df[\"GRP_MOD\"] == \"GRP_X\"]\n\ntfidf = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\ntfidf.fit(df1.Lemmatized_clean_upd)\ntext1 = tfidf.transform(df1.Lemmatized_clean_upd)\n\ntf_matrix = text1.toarray()\nvocab = tfidf.get_feature_names()\ntf_df = pd.DataFrame(np.round(tf_matrix, 2), columns=vocab)\nfind_optimal_clusters(text1, 73)","80acf296":"clusters = MiniBatchKMeans(n_clusters=32, init_size=100, batch_size=200, random_state=20).fit_predict(text1)\nclusters.shape","8739fbb5":"num_features = 4447\npca_comp_count = 100\nplot_tsne_pca(text1, clusters,num_features,pca_comp_count )","c1dcb1d7":"## copy all the data into a new dataframe. We will use this to select top features \n## for segregrating between Group 0 and Group X. This is because using all the features\n## generated by TFIDF will take huge time for any models.\n\nfrom sklearn.feature_selection import SelectKBest\n\ndf = model_data.copy()\n#df.drop(columns=[\"Description\",\"Lemmatized_clean\",\"Language\",\"Accuracy\"], inplace=True)\n","434644d9":"df.head()","63a3b923":"df.head()\n!pwd\n\ndf.to_excel(\"modelInput.xlsx\")","bab74922":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport  numpy, textblob, string\n#from keras.preprocessing import text, sequence\n#from keras import layers, models, optimizers\n\n## split the data into Train and Text (0.75)\n\ntrain_x, test_x, train_y, test_y = model_selection.train_test_split(df['Lemmatized_clean_upd'], df['GRP_MOD'], test_size=0.25, random_state=42)","e8dd352f":"## encode the GRP_X and GRP_0 variable\n\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)","5cbfbe65":"tfidf_vect = TfidfVectorizer(min_df=5 ,use_idf=True,analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df['Lemmatized_clean_upd'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(test_x)","e3fce2bc":"## common function for Classifiers\ndef train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    #return metrics.accuracy_score(predictions, test_y)\n    return predictions\n\n\ndef cal_accuracy(model_name, y_test, y_pred): \n\n    print (\"############  Model Used: \",model_name, \" ####################\")\n    print(\"Confusion Matrix:\\n \", \n        metrics.confusion_matrix(y_test, y_pred)) \n      \n    print (\"Accuracy : \", \n    metrics.accuracy_score(y_test,y_pred)*100) \n    \n    print(\"Recall: {:.2f}\".format(metrics.recall_score(y_test, y_pred)))\n    print(\"Precision: {:.2f}\".format(metrics.precision_score(y_test, y_pred)))\n      \n    print(\"Report : \", \n    metrics.classification_report(y_test, y_pred))","ad534a3f":"## naive Bayes\n# Naive Bayes on Word Level TF IDF Vectors\nmodel_name = \"Naive Bayes\"\npred_result = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\ncal_accuracy (model_name,test_y, pred_result)\n#print(\"NB, WordLevel TF-IDF: \", accuracy)","e74dde1e":"model_name = \"Logistic Regression\"\npred_result = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\ncal_accuracy (model_name,test_y, pred_result)\n\n#accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n#print (\"LR, WordLevel TF-IDF: \", accuracy)","5284f37e":"model_name = \"SVM\"\npred_result = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\nsvm.SVC()\ncal_accuracy (model_name,test_y, pred_result)\n\n#accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n#print (\"SVM, N-Gram Vectors: \", accuracy)","31b994da":"model_name = \"RandomForest\"\npred_result = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\ncal_accuracy (model_name,test_y, pred_result)\n\n#accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n#print (\"RF, Count Vectors: \", accuracy)","e9bc2ba3":"# Grid Search\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import GridSearchCV\n# Parameter Grid\n\nparam_grid = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n                   ] \n# Make grid search classifier\nclf_grid = GridSearchCV(svm.SVC(), param_grid, verbose=1)\n \n# Train the classifier\nclf_grid.fit(xtrain_tfidf, train_y)\n \n# clf = grid.best_estimator_()\nprint(\"Best Parameters:\\n\", clf_grid.best_params_)\nprint(\"Best Estimators:\\n\", clf_grid.best_estimator_)","f85f837c":"model_name = \"SVM\"\npred_result = train_model(svm.SVC(C=3, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False), xtrain_tfidf, train_y, xvalid_tfidf)\ncal_accuracy (model_name,test_y, pred_result)","260d0fb9":"df_X = df[df[\"GRP_MOD\"] == \"GRP_X\"]","f778c4c2":"df_X.drop(\"Unnamed: 0\", axis = 1, inplace=True)","f47e4aaf":"df_X","79bf87db":"tfidf_vect = TfidfVectorizer(min_df=5 ,use_idf=True,analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df_X['Lemmatized_clean_upd'])\ntrain_x = df_X['Lemmatized_clean_upd']\n#test_x = df_X['AssignmentGroup']\nxtrain_tfidf =  tfidf_vect.transform(train_x)\n\n\n\n## Get Feature Names and Store the values in a Dataframe\ntf_matrix = xtrain_tfidf.toarray()\nvocab = tfidf_vect.get_feature_names()\ntf_df = pd.DataFrame(np.round(tf_matrix, 2), columns=vocab)\n\n## View the nunmber of Features\ntf_df.columns\n#xvalid_tfidf =  tfidf_vect.transform(test_x)","4f1afff9":"tf_df.head()","cd262081":"import scipy.cluster.hierarchy as shc\n#plt.figure(figsize=(20, 7))\nplt.subplots(figsize = (40,8))  \nplt.title(\"Dendrograms\")\nplt.xlabel('Clusters')\nplt.xticks(rotation=90)\nplt.ylabel('counts')\n\ndend = shc.dendrogram(shc.linkage(tf_df, method='ward'))\n\n\n \n  \n\nplt.show() ","0f871ef8":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\ncluster.fit_predict(tf_df)","82887d04":"plt.figure(figsize=(10, 7))\nplt.scatter(tf_df[:,0], c=cluster.labels_, cmap='rainbow')","914cb133":"**TEXT Processing - CLEANING and Associated activies**","69b96aed":"**Word Clouds for some random Groups**","eb156fb6":"**Cleaning Data and Lemmatization**","bc7f8069":"drive.mount('\/content\/drive\/', force_remount=True)\nimport os\nproject_path =  '\/content\/drive\/My Drive\/CapstoneProject\/'\nos.chdir(project_path)\nos.getcwd()\n## copy the language model data to a Tmp folder. We will need this later to get\n## language\n\n!wget -O \/tmp\/lid.176.bin https:\/\/dl.fbaipublicfiles.com\/fasttext\/supervised-models\/lid.176.bin","78d8fbe7":"**Find Optimal Clusters - Visualize Elbow Method**","68e4c864":"**Hierarchical Cluster for Group X**\n\n* ","c463971d":"**GROUP_X analysis**","be2dd10a":"**Word Clouds for Groups that have higher incidents other than GRP_0**","6f4e89f7":"**Visualize the Dsitribution of Records across Groups**","0f2373ff":"**Word Cloud Visualization - We will try to visualize word clouds for GRP_0, Non GRP_0 and some of the Groups apart from GRP_0 that had higher number of Incidents**","4a015423":"!ls","269e22c5":"**Use Fast Text to detect languages and store it into two additional columns language and accuracy**","731fbace":"**LOAD THE DATA FROM THE EXCEL**","e961554a":"**TFIDF Vectorization**\n\nWe now Use TFIDF Vectorization method to convert the Words in the description to number vectors.\nOnce done, we will use clustering mechanisms to see if there are clusters that can be visualized. We will use KMeans find out the Optimal number of Clusters and then visualize the same.\n\n","b3b558c5":"Now clean up the Description column to address the following\n* Convert each character in a sentence to lowercase character\n* Remove HTML Tags\n* Remove punctuations\n* Remove stopwords\n* Remove common words like com, hello\n* Remove NUmbers - (r'[0-9] , is used for replace, but there are some numbers which remain in the data.. that needs to be tested further\n* Stemming was causing invalid words, hence used a lemmatizer"}}