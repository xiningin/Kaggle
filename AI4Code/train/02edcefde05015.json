{"cell_type":{"9e23d817":"code","d4bf94de":"code","2f0ee7d5":"code","0ff2dd8e":"code","4d98708e":"code","a3498178":"code","bc5e03c3":"code","2b5fbaea":"code","6c31b9d7":"code","3a88bc20":"code","342fb0f9":"code","412beac7":"code","c482bca7":"code","2238a9e4":"code","1afb5a98":"code","172d7ce8":"code","366f36ce":"code","d8ac9f65":"code","e9c998b7":"code","3a84413f":"code","a893e29f":"code","77efcf09":"code","a0b6ee6f":"code","c0f5b918":"code","375542f0":"code","211da2ad":"code","9299964b":"code","ec72fa74":"code","2134d82e":"code","76050428":"code","331377af":"code","5cfe8351":"code","f54a6674":"markdown","370fe4ef":"markdown","e2ddf4e4":"markdown","ebe20141":"markdown","d4184bc5":"markdown","3fe71a0a":"markdown","70eff1c9":"markdown","d405a23b":"markdown","8acc1413":"markdown","31e44cfe":"markdown","2abf78f3":"markdown"},"source":{"9e23d817":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt","d4bf94de":"# Loading data\ndf = pd.read_csv('..\/input\/creditcard.csv')\n\nprint(\"Number of samples: {}\".format(len(df)))\nprint(\"Number of attributes: {}\".format(len(df.columns)))","2f0ee7d5":"df.describe()","0ff2dd8e":"df.sample(5)","4d98708e":"df.groupby(\"Class\").count()[\"Time\"]","a3498178":"y = df['Class'].copy()\nX = df.copy()\ndel X['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nc_param = 0.1 #should use cross valid to find this\nlr = LogisticRegression(C = c_param, penalty = 'l2', class_weight ='balanced', max_iter =100)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\nmat_conf = confusion_matrix(y_test, y_pred)\nmat_conf","bc5e03c3":"print(\"We have {0} well detected fraud (True positives)\".format(mat_conf[1,1]))\nprint(\"We have {0} undetected fraud (False negatives)\".format(mat_conf[1,0]))\nprint(\"We have {0} normal behavior classified as fraud (Flase positives)\".format(mat_conf[0,1]))\n","2b5fbaea":"lr_weight = LogisticRegression(C = c_param, penalty = 'l2', class_weight ={0:1,1:400}, max_iter =100)\nlr_weight.fit(X_train, y_train)\n\ny_pred_weight = lr_weight.predict(X_test)\nmat_conf_weight = confusion_matrix(y_test, y_pred_weight)\nmat_conf_weight","6c31b9d7":"import lightgbm as lgb\n","3a88bc20":"train_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test)\n","342fb0f9":"#param = {'num_leaves': 31, 'objective': 'binary', \"verbosity\" : 1}\n#param = {'objective': 'binary', \"verbosity\" : 1}\n\nparam = {'objective': 'binary', \n         \"verbosity\" : 1,\n         #\"is_unbalance\" : True,\n         #\"max_bin\" : 40,\n         'learning_rate' : 0.001,\n        } #even using this params yields the same results\n\nparam = {'objective': 'binary', \"verbosity\" : 1,\"learning_rate\" : 0.01 }\n\n\nparam['metric'] = ['auc', 'binary_logloss','cross_entropy']\nevals_result={}","412beac7":"nb_rounds = 1000\nverbose_eval = int(nb_rounds\/10)\ngbm = lgb.train(param,\n                train_data,\n                num_boost_round=nb_rounds,\n                valid_sets=[train_data, test_data],\n                evals_result=evals_result,\n                verbose_eval=verbose_eval,\n                #early_stopping_rounds=50\n               )","c482bca7":"ax = lgb.plot_metric(evals_result, metric='auc')\nax = lgb.plot_metric(evals_result, metric='binary_logloss')\nax = lgb.plot_metric(evals_result, metric='xentropy')\n\nplt.show()","2238a9e4":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(gbm, height=0.8, ax=ax)\nax.grid(False)\nplt.ylabel('Feature', size=12)\nplt.xlabel('Importance', size=12)\nplt.title(\"Importance of the Features of our LightGBM Model\", fontsize=15)\nplt.show()","1afb5a98":"ypred_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\nthreshold = 0.75\nmat_conf_lgb = confusion_matrix(y_test, ypred_lgb>threshold)\nmat_conf_lgb","172d7ce8":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom inspect import signature\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, ypred_lgb)\n\nprecision, recall, _ = precision_recall_curve(y_test, ypred_lgb)\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n          average_precision))","366f36ce":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_test, ypred_lgb)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","d8ac9f65":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nimport matplotlib.pyplot as plt \nfrom sklearn.utils import shuffle\n\n","e9c998b7":"def show_train_history(train_history,train,validation):\n    plt.plot(train_history.history[train])\n    plt.plot(train_history.history[validation])\n    plt.title('Train History')\n    plt.ylabel(train)\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc='best')\n    plt.show()\n\ndef precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall","3a84413f":"df.Class.value_counts()","a893e29f":"df.sort_values(by='Class', ascending=False, inplace=True) #easier for stratified sampling\n#df_full.drop('Time', axis=1,  inplace = True)\ndf_sample = df.iloc[:3000,:]\nshuffle_df = shuffle(df_sample, random_state=42)\ndf_sample.Class.value_counts()\n","77efcf09":"y = shuffle_df['Class'].copy()\nX = shuffle_df.copy()\ndel X['Class']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(x_train)\nx_train_trans = scaler.transform(x_train)\nx_test_trans = scaler.transform(x_test)","a0b6ee6f":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\nmodel = Sequential()\nmodel.add(Dense(200, input_dim=30, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nsgd_opti = SGD(lr=10, momentum=0.01, decay=0.0, nesterov=False)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                                   patience=5, verbose=1, mode='min',\n                                   min_delta=0.0001, cooldown=0, min_lr=1e-8)\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2,\n                      patience=30) # probably needs to be more patient, but kaggle time is limited\n\ncallbacks_list = [early, reduceLROnPlat]\n\n\nmodel.compile( loss =\"binary_crossentropy\",  #loss='binary_crossentropy',\n              optimizer = 'adam', #optimizer = sgd_opti, #optimizer='rmsprop',\n              metrics=['accuracy', precision, recall])\n\nratio = df.groupby(\"Class\").count()[\"Time\"][0] \/ df.groupby(\"Class\").count()[\"Time\"][1]\n\nclass_weight = {0: 5,\n                1: 1}\n\ntrain_history = model.fit(x_train_trans,y_train,\n          epochs=200,\n          validation_split=0.2,\n          batch_size=512,\n          class_weight=class_weight)","c0f5b918":"show_train_history(train_history,'acc','val_acc')\nshow_train_history(train_history,'loss','val_loss')\nshow_train_history(train_history,'precision','val_precision')\nshow_train_history(train_history,'recall','val_recall')\n\nscore = model.evaluate(x_test_trans, y_test, batch_size=128)","375542f0":"print(score)","211da2ad":"y_pred = model.predict_classes(x_test_trans)\nmat_conf = confusion_matrix(y_test, y_pred.astype(int))\nmat_conf","9299964b":"y = df['Class'].copy()\nX = df.copy()\ndel X['Class']\nX_TEST = scaler.transform(X)\n\nscore = model.evaluate(X_TEST, y, batch_size=128)\nprint(score)\nY_PRED = model.predict_classes(X_TEST)\nmat_conf_all_dataset = confusion_matrix(y,Y_PRED.astype(int))\nmat_conf_all_dataset\n","ec72fa74":"def custom_loss(y_true,y_pred):\n    bce = K.mean(K.binary_crossentropy(y_true,y_pred),axis=-1)\n    return bce","2134d82e":"model.compile( loss =\"binary_crossentropy\",  #loss='binary_crossentropy',\n              optimizer = 'adam', #optimizer = sgd_opti, #optimizer='rmsprop',\n              metrics=['accuracy', precision, recall])\n\n\ntrain_history = model.fit(x_train_trans,y_train,\n          epochs=200,\n          validation_split=0.5,\n          batch_size=512,\n          class_weight=class_weight,\n        callbacks=callbacks_list,)","76050428":"show_train_history(train_history,'acc','val_acc')\nshow_train_history(train_history,'loss','val_loss')\nshow_train_history(train_history,'precision','val_precision')\nshow_train_history(train_history,'recall','val_recall')\n\nscore = model.evaluate(x_test_trans, y_test, batch_size=128)","331377af":"y_pred = model.predict_classes(x_test_trans)\nmat_conf = confusion_matrix(y_test, y_pred.astype(int))\nmat_conf","5cfe8351":"y = df['Class'].copy()\nX = df.copy()\ndel X['Class']\nX_TEST = scaler.transform(X)\n\nscore = model.evaluate(X_TEST, y, batch_size=128)\nprint(score)\nY_PRED = model.predict_classes(X_TEST)\nmat_conf_all_dataset = confusion_matrix(y,Y_PRED.astype(int))\nmat_conf_all_dataset","f54a6674":"# Input Data Analysis","370fe4ef":"## Custom Loss","e2ddf4e4":"What we observe is that the simple logistic regression model is not so bad at capturing the fraud detection pattern","ebe20141":"Plotting the Precision recall curve","d4184bc5":"Reducing the numbre of false positive by introducing the class weight","3fe71a0a":"### Evaluate on all dataset","70eff1c9":"In this section, we will try to use the LGBM framework to train a tree-based model that could achieve good results on unbalanced data.","d405a23b":"## LGBM ","8acc1413":"# Model Training using supervised learning","31e44cfe":"## Logistic Regression","2abf78f3":"## Multi Layer Perceptron "}}