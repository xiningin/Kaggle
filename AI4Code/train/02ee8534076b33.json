{"cell_type":{"495e173d":"code","2f433549":"code","41d55d8c":"code","4ddbee26":"code","d0d2b8fc":"code","3616be5e":"code","2b1ff18d":"code","d1a016b4":"code","42e700c4":"code","89c95ca7":"code","35210809":"code","f4fbd4fc":"code","3a4f0260":"code","b1f42d95":"code","b51bbfe4":"code","1346be85":"code","1a18ece2":"code","e1462ee1":"code","30e7616b":"code","f7b4df25":"code","e550da49":"code","1d28b021":"code","32f6784c":"code","fb03542b":"code","446016eb":"code","6c94044d":"code","e85e6fe2":"code","a113ebd3":"code","93c2881e":"code","bea37c7a":"code","57709125":"code","10a2eecc":"code","add95a73":"code","4adfec95":"code","75dace7d":"code","769f2639":"code","c94fe4a7":"code","06fb1cd9":"code","66f088cc":"code","a16e223b":"code","895b4cac":"code","99ab402c":"code","a0abed42":"markdown","eb93c98c":"markdown"},"source":{"495e173d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","2f433549":"\nann_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Annotations.csv\")\nposts_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Posts.csv\")\ncats_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Categories.csv\")\nNews_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Newspaper_Staff.csv\")\narticles_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Articles.csv\")\ncvsplit_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/CrossValSplit.csv\")\nconsol_df=pd.read_csv(\"\/kaggle\/input\/10k-german-news-articles\/Annotations_consolidated.csv\")\n","41d55d8c":"ann_df.head()","4ddbee26":"posts_df.head()","d0d2b8fc":"cats_df.head()","3616be5e":"News_df.head()","2b1ff18d":"articles_df.head()","d1a016b4":"cvsplit_df.head()","42e700c4":"consol_df.head()","89c95ca7":"ann_df['Category'].unique()","35210809":"ann_df.info()","f4fbd4fc":"articles_df.info()","3a4f0260":"articles_df[\"Body\"][0]","b1f42d95":"posts_df[\"Body\"][2]","b51bbfe4":"posts_df.info()","1346be85":"consol_df.info()","1a18ece2":"posts_df[posts_df[\"ID_Post\"]==3326]","e1462ee1":"result_Df = pd.merge(posts_df, consol_df, on=\"ID_Post\")","30e7616b":"print(result_Df.info())\n","f7b4df25":"result_Df.head()","e550da49":"tr_Df = pd.merge(result_Df, articles_df, on=\"ID_Article\")","1d28b021":"tr_Df.info()","32f6784c":"tr_Df.head(2)","fb03542b":"tr_Df['Body_y'][0]","446016eb":"tr_Df[\"Category\"].unique()","6c94044d":"tr_Df[\"Value\"].unique()","e85e6fe2":"import re\ndef clean_text(sent):\n  \n  sentence = sent.lower()\n  S=re.sub('\"','',sentence)\n  S = re.sub(r'[^\\w]', ' ', S)\n  S = re.sub(\"\\s\\s+\", \" \", S)\n\n  return S\ntr_Df['Body_y']=tr_Df['Body_y'].apply(clean_text)","a113ebd3":"tr_Df['Body_y'][0:3]","93c2881e":" from sklearn.model_selection import train_test_split\n X_train, X_test, y_train, y_test = train_test_split( tr_Df['Body_y'], tr_Df['Value'], test_size=0.20, random_state=42,stratify=tr_Df['Category'])","bea37c7a":"train=pd.DataFrame(pd.concat([X_train,y_train],axis=1))\ntest=pd.DataFrame(pd.concat([X_test,y_test],axis=1))","57709125":"train.head()","10a2eecc":"!pip install transformers","add95a73":"from tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch","4adfec95":"def gen_id_mask(data):\n  sms = data.Body_y.values\n\n  # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n  sms = [\"[CLS] \" + sm + \" [SEP]\" for sm in sms]\n  labels = data.Value.values\n\n  tokenizer = BertTokenizer.from_pretrained('dbmdz\/bert-base-german-cased')\n\n  tokenized_texts = [tokenizer.tokenize(sm) for sm in sms]\n  \n\n  MAX_LEN = 256\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n  attention_masks = []\n\n  # Create a mask of 1s for each token followed by 0s for padding\n  for seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n\n  print('success')\n  return (input_ids,labels,attention_masks)","75dace7d":"train_input_ids,train_labels,train_input_masks=gen_id_mask(train)\ntest_input_ids,test_labels,test_input_masks=gen_id_mask(test)","769f2639":"train_input_ids[0]","c94fe4a7":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif torch.cuda.is_available():    \n\n    # use the GPU.    \n    device = torch.device(\"cuda\")\n    print('GPU Device name is :', torch.cuda.get_device_name(0))\nmodel_bert = BertForSequenceClassification.from_pretrained(\n    \"dbmdz\/bert-base-german-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel_bert.cuda()","06fb1cd9":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","66f088cc":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#Converting the inputs for model into torch tensors\ntrain_inputs = torch.tensor(train_input_ids)\nvalidation_inputs = torch.tensor(test_input_ids)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(test_labels)\ntrain_masks = torch.tensor(train_input_masks)\nvalidation_masks = torch.tensor(test_input_masks)\n\n\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 16\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","a16e223b":"from transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom tqdm import tqdm, trange\n\nimport io\n","895b4cac":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","99ab402c":"import random\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\n#Code is adapted from following path (run_glue.py)\n#https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n\n# Prepare optimizer and schedule (linear warmup and decay)\n# no_decay = ['bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n#         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n#         ]\n\n# optimizer = AdamW(optimizer_grouped_parameters, lr=2e-3)\nparam_optimizer = list(model_bert.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\n\noptimizer = AdamW(optimizer_grouped_parameters,lr=1e-5)\n# Number of training epochs (authors recommend between 2 and 4)\n\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * 10 #The value 10 is no of epochs\n\n# Create the learning rate scheduler.\n# scheduler = get_linear_schedule_with_warmup(optimizer, \n#                                             num_warmup_steps = 0, # Default value in run_glue.py\n#                                             num_training_steps = total_steps)\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss and accuracies after each epoch so we can plot them.\nloss_values = []\nTrain_Master_accuracy=[]\nValidation_Master_accuracy=[]\nepochs=2\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model_bert.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model_bert.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        outputs = model_bert(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        #scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ====================================================\n    #               Validation and Training Accuracies\n    # ====================================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model_bert.eval()\n\n    # Tracking variables \n    train_loss, train_accuracy = 0, 0\n    nb_train_steps, nb_train_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in train_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            outputs = model_bert(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        train_accuracy += tmp_train_accuracy\n\n        # Track the number of batches\n        nb_train_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Train Accuracy: {0:.2f}\".format(train_accuracy\/nb_train_steps))\n    Train_Master_accuracy.append(train_accuracy\/nb_train_steps)\n    print(\"  Accuracy evaluation on training took: {:}\".format(format_time(time.time() - t0)))\n\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            outputs = model_bert(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    Validation_Master_accuracy.append(eval_accuracy\/nb_eval_steps)\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))","a0abed42":"#The Bert model fine tuning code is adapted from the huggingface and beautiful article and video shared by \nhttps:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/\n","eb93c98c":"I'm assuming the x values are \"Body_y\" column and The predictors as \"Value\" column"}}