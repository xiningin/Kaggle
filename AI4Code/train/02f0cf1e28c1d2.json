{"cell_type":{"004a4be4":"code","32dfebec":"code","084c0373":"code","d0aa5dd9":"code","e71f0142":"code","faaa2e1b":"code","4c003573":"code","41def1c6":"code","c759288b":"code","750b9ed0":"code","8fdf8f02":"code","b2127d50":"code","d67cd03e":"code","e1b83e30":"code","c331a7f7":"code","0b191cf6":"code","01be196a":"code","c270a915":"code","6c8efa78":"code","4b54b067":"code","39f2b281":"code","6aee2c2a":"code","34248a38":"code","b15b7100":"markdown","39d639d1":"markdown","8f410876":"markdown","635deb6b":"markdown","dd473f13":"markdown","21f84602":"markdown","c9e42b2c":"markdown","e80dada0":"markdown","4aff44a1":"markdown","a528767f":"markdown","fb668168":"markdown","a9135554":"markdown","5c053f21":"markdown","e7af375a":"markdown","c5c7785a":"markdown","9e817dda":"markdown","b11ec5f5":"markdown","e20b2d5b":"markdown"},"source":{"004a4be4":"!pip install git+https:\/\/github.com\/tensorflow\/examples.git\n!pip install -U tfds-nightly","32dfebec":"import tensorflow as tf\nfrom tensorflow_examples.models.pix2pix import pix2pix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Conv2D,Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom IPython.display import clear_output\nimport random, re, math\nimport pandas as pd \nimport keras.backend as K\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nimport zipfile","084c0373":"\ntrain_img_zip = '\/kaggle\/input\/carvana-image-masking-challenge\/train.zip'\ntrain_mask_zip='\/kaggle\/input\/carvana-image-masking-challenge\/train_masks.zip'\n\ntrain_img = zipfile.ZipFile(train_img_zip, 'r')\ntrain_img.extractall('\/kaggle\/working')\ntrain_img.close()\n\ntrain_mask = zipfile.ZipFile(train_mask_zip, 'r')\ntrain_mask.extractall('\/kaggle\/working')\ntrain_mask.close()","d0aa5dd9":"\ntrain_dir = os.path.join('\/kaggle\/working\/train')\ntrain_mask_dir = os.path.join('\/kaggle\/working\/train_masks')\n","e71f0142":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nplt.subplots(2, 5, figsize=(60, 20))\npic_index = 0\n\n\npic_index += 4\nnext_car_pix = [os.path.join(train_dir, fname) \n                for fname in sorted(os.listdir(train_dir))[pic_index-4:pic_index]]\nnext_mask_pix = [os.path.join(train_mask_dir, fname) \n                for fname in sorted(os.listdir(train_mask_dir))[pic_index-4:pic_index]]\n\nfor i, img_path in enumerate(next_car_pix+next_mask_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(2, 4, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","faaa2e1b":"img = mpimg.imread(train_dir+'\/28d9a149cb02_12.jpg')\n\nimg.shape","4c003573":"def creat_dataframe(img_path,mask_path):\n\n    car_ids = []\n    car_paths = []\n    mask_ids=[]\n    mask_paths=[]\n    for p in (img_path,mask_path):\n        for dirname, _, filenames in os.walk(p):\n            for filename in filenames:\n                path = os.path.join(dirname, filename)  \n                if p==img_path:\n                    car_paths.append(path)\n                    car_id = filename.split(\".\")[0]\n                    car_ids.append(car_id)\n                    df=pd.DataFrame(data = {\"id\": car_ids, \"img_path\": car_paths}).set_index('id')\n                else:\n                    mask_paths.append(path)\n                    mask_id = filename.split(\".\")[0]\n                    mask_id = mask_id.split(\"_mask\")[0]\n                    mask_ids.append(mask_id)\n                    df_mask=pd.DataFrame(data = {\"id\": mask_ids, \"mask_path\": mask_paths}).set_index('id')\n                    \n    df[\"mask_path\"] = df_mask[\"mask_path\"]\n         \n    return df","41def1c6":"df=creat_dataframe('\/kaggle\/working\/train','\/kaggle\/working\/train_masks')\ndf.head()","c759288b":"df.info()","750b9ed0":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nIMAGE_SIZE = [256, 256]\nEPOCHS = 40\nSEED = 777\nBATCH_SIZE = 16 ","8fdf8f02":"def flip(image,mask):\n    \n    image = tf.image.flip_left_right(image)\n    mask = tf.image.flip_left_right(mask)\n    \n    return image,mask\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n        \n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n  \n    \ndef transform(image,mask):\n    # is borrowed from https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 \n    \n    rot = 10. * tf.random.normal([1],dtype='float32')\n    shr = 2. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 10. * tf.random.normal([1],dtype='float32') \n    w_shift = 10. * tf.random.normal([1],dtype='float32') \n  \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    m = tf.gather_nd(mask,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),tf.reshape(m,[DIM,DIM,1])\n\n\n\n\ndef read_image_and_mask(image_path, mask_path=None,resize=IMAGE_SIZE):\n    \n    image=tf.io.read_file(image_path)\n    image=tf.image.decode_jpeg(image, channels=3)\n    image=tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, dtype=tf.float32)\/255.\n    if not mask_path is None:\n        mask=tf.io.read_file(mask_path)\n        mask=tf.image.decode_jpeg(mask, channels=3)\n        mask=mask[:,:,:1]\n        mask=tf.image.resize(mask, IMAGE_SIZE)\n        mask = tf.cast(mask, dtype=tf.float32)\/255.\n        return image, mask\n    return image\n\n\n\n\ndef get_training_dataset(df):\n    \n    training_dataset = tf.data.Dataset.from_tensor_slices((df[\"img_path\"].values, df[\"mask_path\"].values))\n    training_dataset = training_dataset.map(read_image_and_mask,num_parallel_calls=AUTO)\n    training_dataset = training_dataset.map(flip,num_parallel_calls=AUTO)\n    training_dataset = training_dataset.map(transform,num_parallel_calls=AUTO)\n    training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n    training_dataset = training_dataset.batch(BATCH_SIZE)\n    training_dataset = training_dataset.repeat()\n    training_dataset = training_dataset.prefetch(AUTO)\n\n    return training_dataset\n\n\ndef get_validation_dataset(df):\n  \n  validation_dataset = tf.data.Dataset.from_tensor_slices((df[\"img_path\"].values, df[\"mask_path\"].values))\n  validation_dataset = validation_dataset.map(read_image_and_mask)\n  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n  \n\n  return validation_dataset\n\n\ndef get_test_dataset(images):\n  \n  test_dataset = tf.data.Dataset.from_tensor_slices((images))\n  test_dataset = test_dataset.map(read_image_and_mask)\n  test_dataset = test_dataset.batch(10, drop_remainder=True)\n\n  return test_dataset","b2127d50":"tr_df, val_df = train_test_split(creat_dataframe('\/kaggle\/working\/train','\/kaggle\/working\/train_masks'), random_state=SEED, test_size=.25)\ntr_dataset = get_training_dataset(tr_df)\nval_dataset = get_validation_dataset(val_df)","d67cd03e":"row = 2; col = 4;\nall_elements = tr_dataset.unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,mask) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n        plt.imshow(mask[j,:,:,0],alpha=.5,cmap='Reds')\n    plt.show()\n    break","e1b83e30":"def display(display_list):\n    plt.figure(figsize=(15, 15))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()","c331a7f7":"def batch_predict(image,model):\n    preds=model.predict(image)  \n    threshold = 0.5\n    preds[preds > threshold] = 1.0\n    preds[preds <= threshold] = 0.0 \n    return preds\n\ndef vis_compare(dataset=val_dataset,num_case=1):\n       \n    for sample in dataset.take(1):\n        image, label = sample[0].numpy(), sample[1].numpy()\n    preds=batch_predict(image,model)\n    if num_case>1:\n        cases=[j for j in np.random.choice(image.shape[0],size=num_case,replace=False)]    \n        for i in cases:\n            truth=(image[i],label[i])\n            pred=(image[i],preds[i])\n            print(f\"case_number_{i}\")\n            display([image,label,preds])\n            print('\\n')\n            print(464*'*')\n            print('\\n')\n    else:\n        truth=(image[0],label[0])\n        pred=(image[0],preds[0])\n        display([image[0],label[0],preds[0]])\n            \n    \n    \n    plt.show() ","0b191cf6":"class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    vis_compare()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n","01be196a":"class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):\n    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n      number of no improvement, training stops.\n  \"\"\"\n\n    def __init__(self, patience=3):\n        super(EarlyStoppingAtMinLoss, self).__init__()\n        self.patience = patience\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None\n\n    def on_train_begin(self, logs=None):\n        # The number of epoch it has waited when loss is no longer minimum.\n        self.wait = 0\n        # The epoch the training stops at.\n        self.stopped_epoch = 0\n        # Initialize the best as infinity.\n        self.best = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(\"val_loss\")\n        if np.less(current, self.best):\n            self.best = current\n            self.wait = 0\n            # Record the best weights if current results is better (less).\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n            \n","c270a915":"def dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\n\ndef dice_loss(in_gt, in_pred):\n    return 1-dice_coef(in_gt, in_pred)","6c8efa78":"OUTPUT_CHANNELS = 1\nbase_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3], include_top=False)\n\n\n\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\n\n\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\ndown_stack.trainable = False\n\nup_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]\n\ndef unet_model(output_channels):\n    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n\n    skips = down_stack(inputs)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip])\n\n    last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2, activation='sigmoid',\n      padding='same')  \n\n    x = last(x)\n    model = tf.keras.Model(inputs=inputs, outputs=x)\n    model.compile(optimizer='adam',loss = dice_loss,\n              metrics=[dice_coef,'binary_accuracy'])\n\n    return model\n\n\n\n\n\nmodel = unet_model(1)\n\n\n\n\n\n\n","4b54b067":"steps_per_epoch = len(tr_df)\/\/BATCH_SIZE\n\n\nhistory = model.fit(tr_dataset,\n                          steps_per_epoch=steps_per_epoch, validation_data=val_dataset, epochs=40,\n                          callbacks=[EarlyStoppingAtMinLoss()]) #,DisplayCallback()","39f2b281":"model.evaluate(val_dataset)","6aee2c2a":"def vis_compare(dataset=val_dataset,num_case=1):\n       \n    for sample in dataset.take(1):\n        image, label = sample[0].numpy(), sample[1].numpy()\n    preds=model.predict(image)\n    if num_case>1:\n        cases=[j for j in np.random.choice(image.shape[0],size=num_case,replace=False)]    \n        for i in cases:\n            truth=(image[i],label[i])\n            pred=(image[i],preds[i])\n            print(f\"case_number_{i}\")\n            display([image[i],label[i],preds[i]])\n            print('\\n')\n            print(464*'*')\n            print('\\n')\n    else:\n        truth=(image[0],label[0])\n        pred=(image[0],preds[0])\n        display([image[0],label[0],preds[0]])\n            \n    \n    \n    plt.show()","34248a38":"vis_compare(dataset=val_dataset,num_case=10)","b15b7100":"\nThe model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the Pix2pix tutorial.","39d639d1":"## Evaluation","8f410876":"#### We should make the image_path and mask_path have the same id:","635deb6b":"## Let's take a look at some image examples and their correponding mask from the dataset.","dd473f13":"## useful callback class","21f84602":"#### Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. \n","c9e42b2c":"## Making a dataframe to use in tensorflow dataset","e80dada0":"## the shape of one random image","4aff44a1":"## Dataset  functions","a528767f":"## Display Example Augmentation\u00b6with mask\n","fb668168":"## Extracting the files","a9135554":"## Train the model","5c053f21":"## Visualization function ","e7af375a":"## Importing Library","c5c7785a":"## Define the model","9e817dda":"## Loss function","b11ec5f5":"## Configuration","e20b2d5b":"# Image segmentation in Tensorflow"}}