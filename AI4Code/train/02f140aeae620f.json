{"cell_type":{"a82ac0d2":"code","91910fbb":"code","c4fc6f61":"code","f7a4a7d0":"code","af5941d6":"code","02edb7f2":"code","1a4075ed":"code","07c7f721":"code","6398490c":"code","e7b6eb44":"code","565f63b3":"code","c873bbe6":"code","ba4df5fe":"code","1377e132":"code","019696a5":"code","d2a0f091":"code","2b9e6d1c":"code","d27f2780":"code","adb56461":"code","c40de722":"code","d03f8b66":"code","af71ee78":"markdown","eb2b4feb":"markdown","2a89d111":"markdown","99b0fffb":"markdown","6c542731":"markdown","5566b507":"markdown","6e7c8442":"markdown","726b4f4d":"markdown","5985ea3d":"markdown","7d6383bb":"markdown"},"source":{"a82ac0d2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","91910fbb":"df=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding=('ISO-8859-1'))","c4fc6f61":"df.head()","f7a4a7d0":"df=df.drop('Unnamed: 2',axis=1)\ndf=df.drop('Unnamed: 3',axis=1)\ndf=df.drop('Unnamed: 4',axis=1)","af5941d6":"df.head()","02edb7f2":"df=df.rename(columns={'v1':'label','v2':'message'})\ndf.head()","1a4075ed":"import re              #importing necessary NLP libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","07c7f721":"lemmatizer=WordNetLemmatizer()              #object creation for lemmatzation on corpus of data","6398490c":"corpus=[]","e7b6eb44":"for i in range(0,len(df)):\n    review=re.sub('[^a-zA-Z]','',df['message'][i])\n    review=review.lower()\n    review=review.split()\n    review=[lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review=' '.join(review)\n    corpus.append(review)","565f63b3":"df.head()","c873bbe6":"from sklearn.feature_extraction.text import CountVectorizer ","ba4df5fe":"cv=CountVectorizer(max_features=1000)         #countvectorizer is the library which helps in creating BOW. cv is the object","1377e132":"X=cv.fit_transform(corpus).toarray()            #independent variable","019696a5":"y=pd.get_dummies(df['label'])               #creating one hot(dummy variables) vectors for target variable\ny=y.iloc[:,1].values","d2a0f091":"from sklearn.model_selection import train_test_split                      #Do the train test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","2b9e6d1c":"from sklearn.naive_bayes import MultinomialNB          #for this we will use Multinomial Bayes algorithm to determine the label","d27f2780":"spam_detect_model=MultinomialNB().fit(X_train,y_train)","adb56461":"y_pred=spam_detect_model.predict(X_test)","c40de722":"from sklearn.metrics import accuracy_score","d03f8b66":"print(accuracy_score(y_test,y_pred))","af71ee78":"# Introduction to NLP","eb2b4feb":"Next step is to remove the punctuations, stopwords like 'a', 'the', 'is' etc as these words do not contribute to the model.\nAlso, convert into lower cases as similar words with different cases will be treated differently.\nThen apply Lemmatization on those texts(each row) to remove the suffixes and reduce it to its dictionary root form.\nWords like studies, studying will get converted to study. This is done to bring uniformity.","2a89d111":"This was all about the basic flow of a NLP project through spam classifier project. If you happen to like it, do leave an upvote! Thanks for giving it a read!!","99b0fffb":"**Importing Dataset**","6c542731":"Now that we have cleaned the data, its time to convert those words into vectors using Bag of Words technique.\n[https:\/\/medium.com\/analytics-vidhya\/spam-classification-using-nlp-f360bdad5723](http:\/\/)\n\nIn this technique I have explained the Bag of words in detail. You can refer the logic of technique from there.\nIn short Bag of words just creates a set of vectors containing count of word occurences in the document and it creates such vectors for each row of message column.","5566b507":"**Importing Libraries**","6e7c8442":"Natural Language Processing(NLP) is a subset of Artifical Intelligence(AI) that helps the computers to understand, interpret and utilize the human language. NLP allows the applications to communicate with people using human language.\nWhenever our data contains large chunks of text, we use NLP techniques to first clean that data and then feed it to the model. We will understand each and every step in detail one by one.","726b4f4d":"First let's drop the last three columns as they are of no use. v1 column is our target variable(predict whether spam or ham) and v2 column is independent variable. Also, we will rename v1 and v2 columns to label and message respectively.","5985ea3d":"1. Tokenization (Breaking down of large texts into smaller tokens i.e. paragraphs to sentences and sentences to words)\n2. Text Data Cleaning (Removing punctuations, stopwords, converting to lower cases etc)\n3. Stemming or Lemmatization (Removing the suffixes of similar words to their root word to get uniformity)\n4. Converting the remaining words to vectors by pre-processing techniques like Bag of Words.\n5. Feeding those vectors to the model\n\nIn this kernel, let's go through all these points by implementing a basic Spam classifier using NLP techniques.","7d6383bb":"**Steps in any basic NLP project**"}}