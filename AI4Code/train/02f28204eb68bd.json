{"cell_type":{"2f4fd924":"code","5c0b745d":"code","7127748d":"code","825a3afc":"code","7255cf0d":"code","6298dff7":"code","6909e346":"code","821012b8":"code","87261d81":"code","7e679558":"code","b8c1549c":"code","b867d056":"code","da503d7c":"code","f235acad":"code","33f971ee":"code","054ce200":"code","aae22e94":"code","a57ec73d":"code","a4586d5c":"code","dccdb113":"code","59aaa217":"code","7eccd29b":"code","2c50dd39":"code","c620a6f5":"code","189e0d76":"code","b286898b":"code","f3c25d1a":"code","227aa93b":"code","6e86dd09":"code","3deb7b7b":"code","add6330d":"code","b2b2aea5":"code","66fb8d68":"code","ce9da734":"code","37a2d343":"code","96c36c65":"code","414c9a37":"code","a6424f8d":"code","a0a80b91":"markdown","8a0ac7aa":"markdown","ed1bb5ef":"markdown","8fdd0de4":"markdown","67f222d6":"markdown","3b939447":"markdown","3685e5ff":"markdown","cc7be61d":"markdown"},"source":{"2f4fd924":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","5c0b745d":"df = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_for_use.pkl\")\ndf_fe = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_fe.pkl\")\n","7127748d":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')","825a3afc":"thresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","7255cf0d":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n","6298dff7":"# !pip install --upgrade git+https:\/\/github.com\/stanfordmlgroup\/ngboost.git","6909e346":"# from ngboost import NGBRegressor, NGBClassifier\n# from ngboost.ngboost import NGBoost\n# from ngboost.learners import default_tree_learner\n# from ngboost.scores import CRPS, MLE , LogScore\n# from ngboost.distns import LogNormal, Normal\n# from ngboost.distns import k_categorical, Bernoulli","821012b8":"# start = time.time()\n\n# ngb_clf = NGBClassifier(Dist=k_categorical(2), verbose=10 ,Score=LogScore, random_state = 2020 )\n# ngb_clf.fit(X_train, y_train , early_stopping_rounds=100)\n\n\n# ngb_runtime = time.time() - start\n\n\n\n# # test roc_auc_score\n# get_eval_by_threshold(y_test, ngb_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\n# NGb_roc_score = roc_auc_score(y_test, ngb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\n\n\n# print( 'NGboost_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(NGb_roc_score ,ngb_runtime ))\n\n","87261d81":"##RandomForest with stratified 5 Fold","7e679558":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)","b8c1549c":"##RandomForest with stratified 5 Fold\n\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\n\nclf = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, n_jobs=-1, random_state = 2020, oob_score=True)\ncv = StratifiedKFold(n_splits=5,random_state = 2020)\ny_preds_rf = np.zeros(X_test.shape[0])\nn_iter = 0 \nfor train_index,test_index in cv.split(X_train,y_train):\n    trx , tsx = X_train.iloc[train_index] , X_train.iloc[test_index]\n    vly , vlt = y_train.iloc[train_index] , y_train.iloc[test_index]\n    RFC = RFC.fit(trx,vly)   \n    loglossTraining = log_loss(vly, \\\n                                RFC.predict_proba(trx)[:,1])\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[tsx.index,:] = \\\n        RFC.predict_proba(tsx)[:,1]  \n    loglossCV = log_loss(vlt, \\\n        predictionsBasedOnKFolds.loc[tsx.index,1])\n    cvScores.append(loglossCV)\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \n    n_iter += 1\n    cv_roc_score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1], average = 'macro')\n    cv_precision, cv_recall, _ = precision_recall_curve(y_test,RFC.predict_proba(X_test)[:,1])\n    cv_pr_auc = auc(cv_recall, cv_precision)\n    print( '\\n#{0}, CV_ROC_AUC : {1} , RF_CV_PR_AUC : {2} '.format(n_iter ,cv_roc_score, cv_pr_auc))\n    y_preds_rf += RFC.predict_proba(X_test)[:,1]\/ cv.n_splits\n\nrf_cv_roc_score = roc_auc_score(y_test, y_preds_rf, average = 'macro')\nrf_cv_precision, rf_cv_recall, _ = precision_recall_curve(y_test,y_preds_rf)\nrf_cv_pr_auc = auc(rf_cv_recall, rf_cv_precision)    \nloglossRandomForestsClassifier = log_loss(y_train, \n                                          predictionsBasedOnKFolds.loc[:,1])\nprint('Random Forests Log Loss: ', loglossRandomForestsClassifier)\n    \nprint( 'RF_cv_ROC_AUC : {0:.4f} , RF_cv_PR_AUC : {1:.4f} '.format(rf_cv_roc_score ,rf_cv_pr_auc ))","b867d056":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n          Area under the curve = {0:0.2f}'.format(\n          areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","da503d7c":"params_xGB = {\n    'nthread':16, # \ucf54\uc5b4 \uc218\n    'gamma': 0, # \uac10\ub9c8 : \ubc94\uc704 (0 ~ \ubb34\ud55c\ub300) , \ub514\ud3f4\ud2b8 0\n        # \uc774 \uac12\uc774 \ub192\uc73c\uba74 \ubcf5\uc7a1\uc131\uc774 \uac10\uc18c(\ud3b8\ud5a5 \uc99d\uac00, \ubcc0\ub3d9 \uac10\uc18c) \n    'max_depth': 6, # max_depth : \ubc94\uc704 (1 ~ \ubb34\ud55c\ub300) , \ub514\ud3f4\ud2b8 6 ## \ud2b8\ub9ac\uc758 \ucd5c\ub300 \uae4a\uc774\n    'min_child_weight': 1, # min_child_weight : \ubc94\uc704 (0 ~ \ubb34\ud55c\ub300) , \ub514\ud3f4\ud2b8 1 ## \uc790\uc2dd\ub178\ub4dc\uc5d0 \ud544\uc694\ud55c \uac00\uc911\uce58\uc758 \ucd5c\uc18c \ud569\uacc4\n    'max_delta_step': 0, # max_delta_step : \ubc94\uc704 (0 ~ \ubb34\ud55c\ub300) ,  \ub514\ud3f4\ud2b8 0 ## \uac01 \ud2b8\ub9ac\uc758 \uac00\uc911\uce58 \ucd94\uc815\uc744 \uc704\ud55c \ucd5c\ub300 \ub378\ud0c0 \ub2e8\uacc4\n    'subsample': 1.0, # subsample : \ubc94\uc704 (0 ~ 1) , \ub514\ud3f4\ud2b8 1\n        # \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc0d8\ud50c\ub9c1 \ube44\uc728\n    'colsample_bytree': 1.0, # colsample_bytree : \ubc94\uc704 (0 ~ 1) , \ub514\ud3f4\ud2b8 1\n        # \ud6c8\ub828 \ud53c\uccd0\uc758 \uc0d8\ud50c\ub9c1 \ube44\uc728\n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2020,\n    'tree_method' : 'gpu_hist',\n}","f235acad":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2020)\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n    \n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000, \n                 nfold=5, early_stopping_rounds=200, verbose_eval=50)\n    \n    # \uc218\uc815 \uc0ac\ud56d : np.arrary \ub85c \uc7ac\uc815\uc758 \ud558\uba74\uc11c \uacbd\uace0 \uba54\uc138\uc9c0\ub97c \uc9c0\uc6b8 \uc218 \uc788\uc74c\n    best_rounds = np.argmin(np.array(bst['test-logloss-mean']))\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n    \n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('XGBoost Gradient Boosting Log Loss: ', loglossXGBoostGradientBoosting)","33f971ee":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","054ce200":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n#     'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2020,\n    'verbose': 50,\n    'num_threads':16,\n    'random_state ' : 2020\n}\n\n","aae22e94":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 , verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', loglossLightGBMGradientBoosting)","a57ec73d":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","a4586d5c":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","dccdb113":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","59aaa217":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","7eccd29b":"\nprint(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\nprint(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n          logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","2c50dd39":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","c620a6f5":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","189e0d76":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","b286898b":"# \uc559\uc0c1\ube14","f3c25d1a":"predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)\npredictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(predictionsBasedOnKFoldsRandomForests['prediction'] \\\n    .astype(float),how='left').join( \\\n    predictionsBasedOnKFoldsXGBoostGradientBoosting['prediction'].astype(float), \\\n    how='left',rsuffix=\"2\").join( \\\n    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \\\n    how='left',rsuffix=\"3\")\npredictionsBasedOnKFoldsFourModels.columns = \\\n    ['predsRF','predsXGB','predsLightGBM']","227aa93b":"X_trainWithPredictions = \\\n    X_train.merge(predictionsBasedOnKFoldsFourModels,\n                  left_index=True,right_index=True)","6e86dd09":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}","3deb7b7b":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFoldsEnsemble = \\\n    pd.DataFrame(data=[],index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = \\\n        X_trainWithPredictions.iloc[train_index,:], \\\n        X_trainWithPredictions.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 ,verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n        gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossEnsemble = log_loss(y_train, \\\n        predictionsBasedOnKFoldsEnsemble.loc[:,'prediction'])\nprint('Ensemble Log Loss: ', loglossEnsemble)","add6330d":"print('Feature importances:', list(gbm.feature_importance()))","b2b2aea5":"preds = pd.concat([y_train,predictionsBasedOnKFoldsEnsemble.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","66fb8d68":"scatterData = predictionsTestSetLightGBMGradientBoosting.join(y_test,how='left')\nscatterData.columns = ['Predicted Probability','True Label']\nax = sns.regplot(x=\"True Label\", y=\"Predicted Probability\", color='k', \n                 fit_reg=False, scatter_kws={'alpha':0.1},\n                 data=scatterData).set_title( \\\n                'Plot of Prediction Probabilities and the True Label')","ce9da734":"scatterDataMelted = pd.melt(scatterData, \"True Label\", \\\n                            var_name=\"Predicted Probability\")\nscatterDataMelted.head()\nax = sns.stripplot(x=\"value\", y=\"Predicted Probability\", \\\n                   hue='True Label', jitter=0.4, \\\n                   data=scatterDataMelted).set_title( \\\n                   'Plot of Prediction Probabilities and the True Label')","37a2d343":"## XGboost with Stratified 5 Fold\n\nval = np.zeros(X_train.shape[0])\npred = np.zeros(X_test.shape[0])\nfolds = StratifiedKFold(n_splits=5, random_state=2020)\n\nmodel_xgb =  xgb.XGBClassifier(\n                              n_estimators=10000,\n                              objective='binary:logistic', \n\n                              verbosity =1,\n                              eval_metric  = 'aucpr',\n                              tree_method='gpu_hist',\n                              random_state = 2020, \n                              n_jobs=-1)\n\nfor fold_index, (train_index,val_index) in enumerate(folds.split(X_train,y_train)):\n    print('Batch {} started...'.format(fold_index))\n    gc.collect()\n    bst = model_xgb.fit(X_train.iloc[train_index],y_train.iloc[train_index],\n              eval_set = [(X_train.iloc[val_index],y_train.iloc[val_index])],\n              early_stopping_rounds=200,\n              verbose= 100, \n              eval_metric ='aucpr'\n              )\n\n    pred += bst.predict_proba(X_test)[:,1]\/folds.n_splits\n\nxgb_cv_roc_score = roc_auc_score(y_test, pred, average = 'macro')\nxgb_cv_precision, xgb_cv_recall, _ = precision_recall_curve(y_test,pred)\nxgb_cv_pr_auc = auc(xgb_cv_recall, xgb_cv_precision)\n\n\n\nprint( 'XGboost_cv_ROC_AUC : {0:.4f} , XGboost_cv_PR_AUC : {1:.4f} '.format(xgb_cv_roc_score ,xgb_cv_pr_auc ))\n","96c36c65":"###LightGBM with Stratified 5 Fold\n\nX = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n\nfrom lightgbm import LGBMClassifier\n\nfrom time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'binary_logloss',\n            'metric_freq' : 50,\n            'max_depth' :4, \n            'num_leaves' : 31,\n            'learning_rate' : 0.01,\n            'feature_fraction' : 1.0,\n            'bagging_fraction' : 1.0,\n            'bagging_freq' : 0,\n            'bagging_seed' : 2020,\n            'num_threads' : 16\n           }\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\ny_preds = np.zeros(X_test.shape[0])\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X_train,y_train)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    y_preds += clf.predict(X_test) \/ 5\n    \n    \n    \nprint('-' * 30)\nprint('Training is completed!.')\n# print(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n\n\nlgbm_cv_roc_score = roc_auc_score(y_test, y_preds, average = 'macro')\nlgbm_cv_precision, lgbm_cv_recall, _ = precision_recall_curve(y_test,y_preds)\nlgbm_cv_pr_auc = auc(lgbm_cv_recall, lgbm_cv_precision)\n\n\n\nprint( 'LightGBM_cv_ROC_AUC : {0:.4f} , LightGBM_cv_PR_AUC : {1:.4f} '.format(lgbm_cv_roc_score ,lgbm_cv_pr_auc ))\n\n","414c9a37":"## Results\n\nprint( 'RandomForest_ROC_AUC : {0:.4f} , RandomForest_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(rf_roc_score ,rf_pr_auc, RF_runtime ))\nprint( 'XGboost_gpu_ROC_AUC : {0:.4f} , XGboost_gpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_gpu_roc_score ,xgb_gpu_pr_auc, xgb_gpu_runtime ))\nprint( 'XGboost_cpu_ROC_AUC : {0:.4f} , XGboost_cpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_cpu_roc_score ,xgb_cpu_pr_auc, xgb_cpu_runtime ))\nprint( 'LightGBM_ROC_AUC : {0:.4f} , LightGBM_PR_AUC : {1:.4f} ,Runtime : {2:.4f}'.format(lgbm_roc_score ,lgbm_pr_auc, lgbm_cpu_runtime))\nprint( 'RF_cv_ROC_AUC : {0:.4f} , RF_cv_PR_AUC : {1:.4f} '.format(rf_cv_roc_score ,rf_cv_pr_auc ))\nprint( 'XGboost_cv_ROC_AUC : {0:.4f} , XGboost_cv_PR_AUC : {1:.4f} '.format(xgb_cv_roc_score ,xgb_cv_pr_auc ))\nprint( 'LightGBM_cv_ROC_AUC : {0:.4f} , LightGBM_cv_PR_AUC : {1:.4f} '.format(lgbm_cv_roc_score ,lgbm_cv_pr_auc ))\n","a6424f8d":"########## Export\n\n\n#save model\njoblib.dump(rf_clf , 'rf_clf.pkl')\njoblib.dump(lgbm_clf , 'lgbm_clf.pkl')\njoblib.dump(xgb_clf , 'xgb_clf.pkl')\n# joblib.dump(ngb_clf , 'ngb_clf.pkl')\n\n\n\n","a0a80b91":"### Data Description \n\n* emp_length_int : Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n* home_ownership : The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\n* income_category : Categorized Income (Low, Medium, High) \n* annual_inc : The self-reported annual income provided by the borrower during registration.\n\n* loan_amount : The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n* term : The number of payments on the loan. Values are in months and can be either 36 or 60.\n\n* application_type : Indicates whether the loan is an individual application or a joint application with two co-borrowers\n* purpose : A category provided by the borrower for the loan request.\n* interest_payments : ;;\n* loan_condition : Condition of the Loan [TARGET] (Good Loan = 0 , Bad Loan = 1)\n* interest_rate :  Interest Rate on the loan\n* grade : LC assigned loan grade\n* dti : A ratio calculated using the borrower\u2019s total monthly debt payments on the total debt obligations, - - - excluding mortgage and the requested LC loan, divided by the borrower\u2019s self-reported monthly income.\n* total_pymnt : Payments received to date for total amount funded\n* total_rec_prncp : Principal received to date\n* recoveries : post charge off gross recovery\n* installment : The monthly payment owed by the borrower if the loan originates.\n* region : region of Loan being executed","8a0ac7aa":"fork from previous EDA kernel https:\/\/www.kaggle.com\/possiblemanjr\/handling-imbalanced-data-eda-small-fe","ed1bb5ef":"### train_test_split (Stratify)","8fdd0de4":"### Libraries","67f222d6":"### Import Data","3b939447":"### Utilities","3685e5ff":"### Data Description (Korean)\n* year : \ub300\ucd9c \ubc1c\uc0dd \uc5f0\ub3c4\n* issue_d : \ub300\ucd9c \ubc1c\uc0dd \uc77c\uc790\n* final_d : \ub9c8\uc9c0\ub9c9 \uac70\ub798\uc77c\uc790\n* emp_length_int : \"\uadfc\uc18d\ub144\uc218. 0\uc740 1\ub144 \ubbf8\ub9cc, 10\uc740 10\ub144 \uc774\uc0c1\"\n* home_ownership : \"\ub4f1\ub85d \uc2dc \ub300\ucd9c\uc790\uc5d0\uac8c\uc11c \uc81c\uacf5\ub41c \uc9d1 \ubcf4\uc720 \uc0c1\ud0dc. RENT(\ub300\uc5ec) = 1, OWN(\uc18c\uc720) = 2, MORTAGE(\ub2f4\ubcf4\ub300\ucd9c) = 3 \"\n* home_ownership_cat : ;;\n* income_category : \"\uc218\uc775 Low = 1, Medium = 2, High = 3 \uc73c\ub85c \ubd84\ub958\"\n* annual_inc : \ub4f1\ub85d\uc2dc \ub300\ucd9c\uc790\uc5d0\uac8c\uc11c \uc81c\uacf5\ub41c \uc5f0\uac04 \uc18c\ub4dd\n* income_cat : ;;\n* loan_amount : \ub300\ucd9c\uae08\uc561(\ub2ec\ub7ec)\n* term : \"\ub300\ucd9c\uae30\uac04(36\uac1c\uc6d4 = 1, 60\uac1c\uc6d4 = 2)\"\n* term_cat : ;;\n* application_type : \"\uac1c\uc778 \ub300\ucd9c \uc2e0\uccad(=1) \uc778\uc9c0, 2\uba85\uc758 \ub300\ucd9c\uc790\uc5d0 \uc758\ud574 \uacf5\ub3d9\uc73c\ub85c \uc2e0\uccad\ub41c \ub300\ucd9c \uc2e0\uccad (=2)\uc778\uc9c0 \uc5ec\ubd80\"\n* application_type_cat : ;;\n* purpose : \ub300\ucd9c\uc774\uc720\n* purpose_cat : \"\ub300\ucd9c\uc6a9\ub3c4(\ube5b \uccad\uc0b0, \uce74\ub4dc \ub300\uae08 \uacb0\uc81c, \uc9d1 \uac1c\ubc1c \ub4f1\ub4f1) \n[credit_card = 1, car = 2, small_business = 3, other = 4, wedding = 5, debt_consolidation =6, \nhome_improvement = 7, major_purchase = 8, medical = 9, moving = 10, vacation = 11, house =12,\nrenewable_energy = 13,  educational = 14]\"\n* interest_payments : \"\uc774\uc790 \uc9c0\ubd88? (Low = 1, High =2 \ub85c \ubd84\ub958)\"\n* interest_payments_cat : ;;\n* loan_condition : \ub300\ucd9c\uc758 \uc0c1\ud0dc(TARGET) (Good Loan = 0 , Bad Loan = 1)\n* loan_condition_cat : ;;\n* interest_rate : \ub300\ucd9c\uc758 \uc774\uc790\uc728\n* grade : \ub300\ucd9c \ub4f1\uae09 ( A ~ G , 1~7)\n* grade_cat : ;;\n* dti : \uae08\uc735\ubd80\ucc44 \uc0c1\ud658\ub2a5\ub825\uc744 \uc18c\ub4dd\uc73c\ub85c \ub530\uc838\uc11c \ub300\ucd9c\ud55c\ub3c4\ub97c \uc815\ud558\ub294 \uacc4\uc0b0\ube44\uc728\n* total_pymnt : \ucd1d \uc0c1\ud658\uae08\uc561\n* total_rec_prncp : ???\n* recoveries : \ud68c\uc218\n* installment : \ubd84\ud560 \ubd88\uc785\uae08\n* region : \uac70\ub798\uc9c0\uc5ed","cc7be61d":"NGboost"}}