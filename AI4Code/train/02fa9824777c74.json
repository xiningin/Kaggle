{"cell_type":{"01dc54ee":"code","216cb97d":"code","94cbfe8a":"code","a68cf9bc":"code","650759b9":"code","bfadfc5c":"code","5f1a2242":"code","7c6593b7":"code","4d9fdd5b":"code","4a4f53b1":"code","cb0ab0b8":"code","d68ff4e4":"code","a2124a02":"code","e8167bfd":"code","4656cf22":"code","63a8f158":"code","7b00397b":"code","35b1e2bf":"code","049a0f05":"code","c324073c":"code","751a246b":"code","70b78048":"code","d3fa5cb2":"code","b37decc7":"code","b08bd5e6":"code","99694f76":"code","17e76dcc":"code","969af6ed":"code","add2b016":"code","5de349f8":"code","0d36f58c":"code","b3e0e70f":"code","05693c6d":"code","b3489b23":"code","62496eae":"markdown","8af6a51f":"markdown","0eeba706":"markdown","fb2ed6e2":"markdown","7fc427d9":"markdown","842aeecf":"markdown","3ed9c5b7":"markdown","653c2c00":"markdown","143823f8":"markdown","3097af04":"markdown","621dd4d1":"markdown","3dac6326":"markdown","a46fad59":"markdown","d10fc01f":"markdown","67665211":"markdown"},"source":{"01dc54ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","216cb97d":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","94cbfe8a":"train_transaction.head()","a68cf9bc":"test_transaction.head()","650759b9":"train_identity.head()","bfadfc5c":"test_identity.head()","5f1a2242":"train_transaction.shape , test_transaction.shape","7c6593b7":"train_identity.shape , test_identity.shape","4d9fdd5b":"l1 = train_transaction.columns\nl2= train_identity.columns\nlist(set(l1) & set(l2)) ","4a4f53b1":"train = train_transaction.merge(train_identity , how = 'left' , on = 'TransactionID')\ntest = test_transaction.merge(test_identity , how = 'left' , on = 'TransactionID')\nprint(train.shape)\nprint(test.shape)","cb0ab0b8":"import gc\n\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()","d68ff4e4":"train.isnull().sum()","a2124a02":"train.dtypes","e8167bfd":"cat_cols = [c for c in train.columns if train[c].dtype == object ]\ncat_cols","4656cf22":"for c in cat_cols:\n    print('number of unique entries for column' , c , '=' , train[c].nunique())","63a8f158":"train.isFraud.value_counts()","7b00397b":"train.isFraud.value_counts().plot('bar')\nprint('target ratio is', round(20663\/len(train)*100,2) , 'percent')","35b1e2bf":"pd.options.display.float_format = '{:.2f}'.format\ntrain.TransactionDT.describe()","049a0f05":"test.TransactionDT.describe()","c324073c":"train.TransactionDT.max() < test.TransactionDT.min()","751a246b":"import matplotlib.pyplot as plt\n\nplt.hist(train['TransactionAmt'] , bins = 100)\nplt.title('transaction amount for train set')\nplt.show()\n\nplt.hist(test['TransactionAmt'] , bins = 100)\nplt.title('transaction amount for test set')\nplt.show()","70b78048":"train.TransactionAmt.describe()","d3fa5cb2":"test.TransactionAmt.describe()","b37decc7":"plt.hist(np.log(train['TransactionAmt']) , bins = 100)\nplt.title('Log scale transaction amount for train set')\nplt.show()\n\nplt.hist(np.log(test['TransactionAmt']), bins = 100)\nplt.title('Log scale transaction amount for test set')\nplt.show()","b08bd5e6":"## Let's subset the numerical columns in train data ##\n\nnum_cols = [c for c in train.columns if train[c].dtype != object ]\ntrain_num = train[num_cols]\n#print(train_num.shape)\ntrain_num.head()\nmissing_cols = [c for c in train_num.columns if train_num[c].isnull().sum()\/len(train_num) >0.80 ]\nlen(missing_cols)","99694f76":"for c in train_num.columns:\n    print('number of unique entries for column' , c , '=' , train_num[c].nunique())","17e76dcc":"num_cols = [c for c in train_num.columns if train_num[c].nunique()>5000 ]\nlen(num_cols) ## 40 columns\n#num_cols\ntrain_num = train_num[num_cols]\ntrain_num = train_num.fillna(train_num.mean())\ntrain_num['target'] = train['isFraud']","969af6ed":"import random\n\ndata1 = train_num.sample(frac= 0.1 , random_state=10)\ndata1.head()","add2b016":"data1.target.value_counts()","5de349f8":"print('target ratio in the sample data is' , round(2060\/len(data1)*100,2) , 'which seems okay')","0d36f58c":"target = data1['target']\ndel data1['target'], data1['TransactionDT'], data1['TransactionID']","b3e0e70f":"## Let's try PCA on this dataset ##\n\nfrom sklearn.preprocessing import StandardScaler\ndata_pca = StandardScaler().fit_transform(data1)\n\n#data_pca = pd.DataFrame(data_pca)\n#data_pca.head()\n#data_pca.describe()\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\ncomps = pca.fit_transform(data_pca)\n\nfinal_pca_data = pd.DataFrame(data = comps , columns=['pc1' , 'pc2' , 'pc3'])\n\nfinal_pca_data.head()","05693c6d":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import FigureWidget\n\ntraces = go.Scatter3d(\n    x=final_pca_data['pc1'],\n    y=final_pca_data['pc2'],\n    z=final_pca_data['pc3'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        color=target,\n        colorscale='Viridis',\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","b3489b23":"%%time\n\n## same with T-SNE ##\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3 , random_state=0)\ndata_tsne = tsne.fit_transform(data1)\n\ndata_tsne\n\ndata_tsne = pd.DataFrame(data_tsne , columns=['tsne1' , 'tsne2' , 'tsne3'])\ndata_tsne.head()\n\n## 3D plot with TSNE components ##\n\ntraces = go.Scatter3d(\n    x=data_tsne['tsne1'],\n    y=data_tsne['tsne2'],\n    z=data_tsne['tsne3'],\n    mode='markers',\n    marker=dict(\n        size=4,\n        opacity=0.2,\n        color=target,\n        colorscale='Viridis',\n     )\n)\n\nlayout = go.Layout(\n    autosize=True,\n    showlegend=True,\n    width=800,\n    height=1000,\n)\n\nFigureWidget(data=[traces], layout=layout)","62496eae":"Let's check if we preserve the target ratio in our sample data or not","8af6a51f":"Let's see missing values in the train data variable wise ","0eeba706":"It seems that we have highly imbalanced binary target distribution.\nNow, from [Bojan's public kernel](https:\/\/www.kaggle.com\/tunguz\/adversarial-ieee) we know that TransactionDT is the variable which has different distribution in both train and test data, let's re-confirm that quickly. ","fb2ed6e2":"Let's see the number of different entries for those categorical columns ","7fc427d9":"What are the categorical columns do we have in this data?, let's see","842aeecf":"Wow, as we can see that, there are 69 columns in the train data which have more than 80% missing entries.\nBut, are all of them really numeric?","3ed9c5b7":"Let's quickly check the distribution of the target variable here","653c2c00":"We can see that the transaction amount in the train data is ranging till 32K where as the same ranges till 10K in test, let's see their distribution in the log scale.","143823f8":"So, we see that the data was splot by this variable and we can not use this variable readily, probably creating other variables from this variable like - time or day or weekend or not kind of variable can be useful from this one (If possible). Also, this variable can help us in creating different effective validation strategy.\n\nAnyway, let's move on and see the distribution of transaction amount.","3097af04":"From the above plot it seems that the yellow points (which are target = 1) are quite mixed up with the purple (target = 0) ones, which may indicate not to do oversampling of target = 1s blindly. So, we may need to be careful incase we try oversampling. A better perspective is possible if we plot t-sne components, which I have commented below. However, please note that the results are based on a very small subset so results colud be very much inconclusive. ","621dd4d1":"Now a 3D plot using these PCA data. I learned this 3D plotting from [this kernel](https:\/\/www.kaggle.com\/chechir\/molecular-eda-3d-plots)","3dac6326":"There are lot's of columns which have very less number of unique values altogether, probably treating them as categorical can help.\nLet's subset our train data with columns which have more unique numbers, so probably the numerical columns. Also, let's impute the missing values by their column means","a46fad59":"We have lots of variables with missing values, now let's see the data types of each column","d10fc01f":"What about the numeric columns? Let's look at them quickly.","67665211":"Let's for the sake of simplicity and time we randomly select the 10% of the data and run a PCA on that, after that, we will take 3 PCA components to plot."}}