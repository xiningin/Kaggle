{"cell_type":{"a281d379":"code","58a5b2c1":"code","8c5e3a05":"code","f1e44460":"code","882a3875":"code","4242bbf2":"code","94af5dd1":"code","6ab4934c":"code","b1a085c1":"code","a613f390":"code","b18bf21f":"code","208c92a9":"code","7d9e71ba":"code","5e294abd":"code","9c4b13b5":"code","d0794368":"code","23766dd1":"code","a2ede476":"code","868f3d02":"code","87213752":"code","680c9b7b":"code","c7b520f7":"code","a6e8ed26":"code","cd7ec117":"code","3c3d59a0":"code","db363267":"code","228c6ef1":"code","83261191":"code","e736d632":"code","84e5a7b2":"code","7c0539f6":"code","05effb17":"code","83a30840":"code","baa28b47":"code","cc68106b":"code","1e74f36a":"code","8226f645":"code","3784f0ee":"code","60050e6c":"code","c3608906":"code","d079745e":"code","ec7f5b9c":"markdown","4d17f9f4":"markdown","e33ff9eb":"markdown","ef79ae9b":"markdown","03d7097c":"markdown","bc7b8df5":"markdown","6bba6f24":"markdown"},"source":{"a281d379":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58a5b2c1":"# Import all required libraries. Few I have added down wherever it was required.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","8c5e3a05":"# First fetch train and test dataset in your notebook\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","f1e44460":"train.head()","882a3875":"train.columns","4242bbf2":"train.shape","94af5dd1":"# Lets us divide all column into NUMERICAL and CATEGORICAL type for our future data engineering.\nnum_columns  = train.select_dtypes(include=['int64','float64']).columns\ncat_features = train.select_dtypes(include='object').columns","6ab4934c":"num_columns","b1a085c1":"num_columns.size","a613f390":"cat_features","b18bf21f":"cat_features.size","208c92a9":"# To find out number of Null values in train dataset\ntrain.isnull().sum()","7d9e71ba":"# To find out number of Null values in train dataset. For better display\ntrain.isnull().sum().sort_values(ascending=False)[:38]","5e294abd":"for i in num_columns:\n    print(i)\n    \n    train[i].fillna(train[i].median(), inplace=True)\n    if(i != \"SalePrice\"): # \"SalePrice\" is Target column and not present in test dataset\n        test[i].fillna(test[i].median(), inplace=True)","9c4b13b5":"#train[\"GarageYrBlt\"].fillna(train[\"GarageYrBlt\"].median(), inplace=True)\n#test[\"GarageYrBlt\"].fillna(test[\"GarageYrBlt\"].median(), inplace=True)\n#train[\"MasVnrArea\"].fillna(train[\"MasVnrArea\"].median(), inplace=True)\n#test[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(), inplace=True)\n#train[\"LotFrontage\"].fillna(train[\"LotFrontage\"].median(), inplace=True)\n#test[\"LotFrontage\"].fillna(test[\"LotFrontage\"].median(), inplace=True)\n#train[\"MasVnrType\"].fillna(\"None\", inplace=True)\n#test[\"MasVnrType\"].fillna(\"None\", inplace=True)","d0794368":"cat_column_has_null = [\"Alley\", \"Electrical\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n","23766dd1":"for i in cat_features:\n    train[i].fillna(\"None\", inplace=True)\n    test[i].fillna(\"None\", inplace=True)","a2ede476":"train.head()","868f3d02":"train.isnull().sum().sort_values(ascending=False)[:38]","87213752":"test.isnull().sum().sort_values(ascending=False)[:38]","680c9b7b":"for i in cat_features:\n    print(i)\n    train[i] = LabelEncoder().fit_transform(train[i])  \n    test[i] = LabelEncoder().fit_transform(test[i])  ","c7b520f7":"train[\"MSZoning\"].head()","a6e8ed26":"test[\"MSZoning\"].head()","cd7ec117":"train[\"Electrical\"].head()","3c3d59a0":"# ID column should be removed as it is record identifier.\n# LotFrontage has too many Null values and it is a numerical value so it may be better to just drop it.\n# GarageYrBlt, MasVnrArea, and MasVnrType all have a fairly decent amount of missing values. MasVnrType \n# is categorical so we can replace the missing values with \"None\", as we did before. \n# We can fill the others with median.\n\n\ntrain = train.drop(columns=['Id','Street','PoolQC','Utilities'],axis=1)\ntest = test.drop(columns=['Id','Street','PoolQC','Utilities'],axis=1)\n\n#training.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values","db363267":"train.head()","228c6ef1":"# Train dataset divide into X and y\n\n# 'SalePrice' is the target column here in our dataset\n\ntrain_X = train.drop('SalePrice', axis = 1) # train_WithoutLabel\ntrain_y = train['SalePrice'] # train_onlyLabel\ntrain_X.shape, train_y.shape","83261191":"# Splitting the training dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y,test_size = 0.2, random_state = 1)\n\n","e736d632":"# Model development\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef Predictive_Model(estimator):\n    estimator.fit(X_train, y_train)\n    prediction = estimator.predict(X_test)\n    accuracy_score = estimator.score(X_train, y_train)\n    print( \"Prediction Score\", (accuracy_score * 100).round(2))\n    print('R_squared:', metrics.r2_score(y_test, prediction))\n    print('Square Root of MSE:',np.sqrt(metrics.mean_squared_error(y_test, prediction)))\n    plt.figure(figsize=(10,5))\n    sns.distplot(y_test, hist=True, kde=False)\n    sns.distplot(prediction, hist=True, kde=False)\n    plt.legend(labels=['Actual Values of Price', 'Predicted Values of Price'])\n    plt.xlim(0,)\ndef FeatureBar(model_Features, Title, yLabel):\n    #plt.figure(figsize=(10,5))\n    plt.bar(df.columns[df.columns!='SalePrice'].values, model_Features)\n    plt.xticks(rotation=45)\n    plt.title(Title)\n    plt.ylabel(yLabel)","84e5a7b2":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nPredictive_Model(lr)","7c0539f6":"from sklearn.linear_model import Ridge\nrr = Ridge(alpha=100)\nPredictive_Model(rr)","05effb17":"from sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=5)\nPredictive_Model(knn)","83a30840":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_depth=15, random_state=0)\nPredictive_Model(dt)","baa28b47":"regressor = ['Linear Regression', 'Ridge Regression', 'KNN', 'Decision Tree']\nmodels = [LinearRegression(), Ridge(alpha=100), KNeighborsRegressor(n_neighbors=5), DecisionTreeRegressor(max_depth=15, random_state=0)]\nR_squared = []\nRMSE = []\nPrediction_score = []\nfor m in models:\n    m.fit(X_train, y_train)\n    prediction_m = m.predict(X_test)\n    accuracy_score = m.score(X_train, y_train)\n    prediction_score = (accuracy_score * 100).round(2)\n    r2 = metrics.r2_score(y_test, prediction_m)\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction_m))\n    Prediction_score.append(prediction_score)\n    R_squared.append(r2)\n    RMSE.append(rmse)\nbasic_result = pd.DataFrame({'R squared':R_squared,'RMSE':RMSE,'Prediction Score':Prediction_score}, index=regressor)\nbasic_result\n\n\n\n","cc68106b":"dt = DecisionTreeRegressor(max_depth=15, random_state=0)\ndt.fit(X_train, y_train)\ndt_prediction = dt.predict(test)","1e74f36a":"# We had drop \"ID\" column from test dataset, hence again loading test data for getting ID\ntest_dataset = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.DataFrame({\n        \"Id\": test_dataset[\"Id\"],\n        \"SalePrice\": dt_prediction\n    })\n\nsubmission.to_csv(\"house_price_prediction_decision_tree.csv\", index=False)\nprint(submission.shape)","8226f645":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_prediction = lr.predict(test)","3784f0ee":"# We had drop \"ID\" column from test dataset, hence again loading test data for getting ID\ntest_dataset = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.DataFrame({\n        \"Id\": test_dataset[\"Id\"],\n        \"SalePrice\": lr_prediction\n    })\n\nsubmission.to_csv(\"house_price_prediction_Linear_Regression.csv\", index=False)\nprint(submission.shape)","60050e6c":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nrf = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_train, y_train)\nprediction = grid_rf.predict(X_test)\naccuracy_score = grid_rf.score(X_train, y_train)\nprint( \"Prediction Score\", (accuracy_score * 100).round(2))\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","c3608906":"grid_rf_predictions = grid_rf.predict(test)","d079745e":"test_dataset = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.DataFrame({\n        \"Id\": test_dataset[\"Id\"],\n        \"SalePrice\": grid_rf_predictions\n    })\n\nsubmission.to_csv(\"house_price_prediction_RandomForest.csv\", index=False)\nprint(submission.shape)","ec7f5b9c":"**K-Nearest Neighbors(KNN)**","4d17f9f4":"**RandomForest Regression**","e33ff9eb":"**Linear Regression**","ef79ae9b":"## Our Strategy to handle missing data:\n* 1. For Numerical: Fill the missing values with a measure like median, mean, or mode.\n* 2. For Categorical: Fill the missing values with the most common term that appeared from the entire column    ","03d7097c":"**Performance Summary**","bc7b8df5":"**Ridge Regression**","6bba6f24":"**Decision Tree**"}}