{"cell_type":{"0c55da27":"code","61d4eb12":"code","086084fc":"code","03a9152c":"code","37e70928":"code","eb50492e":"code","88fbc623":"code","5e1ead6c":"code","5caff103":"code","3f9a841f":"code","398f7343":"code","c8738a96":"code","174dc790":"code","d179a0a3":"code","f4923e5b":"code","ea1c3520":"code","ef360a8b":"code","59c0af2b":"code","61b1a546":"code","7a2f7f6c":"code","3094c537":"code","e4dc0bd1":"code","8659f43b":"code","24f2778e":"code","dea247de":"markdown"},"source":{"0c55da27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61d4eb12":"pd.set_option('max_columns', None)\ndata_chunk = pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv', parse_dates=['date_time', 'srch_ci', 'srch_co'], nrows=10**6)\n\n\ndata_chunk.head()","086084fc":"data_chunk.info()","03a9152c":"data_chunk.dtypes.values","37e70928":"def compress_dataset(data_chunk):\n    for feature, data_type in zip(data_chunk.dtypes.index, data_chunk.dtypes.values):\n    #     print(feature, data_type)\n        if data_type == 'int64':\n            data_chunk[feature] = data_chunk[feature].astype('int8')\n        if data_type=='float64':\n            data_chunk[feature] = data_chunk[feature].astype('float16')\n    return data_chunk\n        ","eb50492e":"compressed_data = pd.DataFrame()\nwith pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv', iterator=True) as reader:\n    data = reader.get_chunk(10**6)\n    \n    while data is not None:\n        \n    #     print(data.info(verbose=False))\n        data = compress_dataset(data)\n    #     print(data.info(verbose=False))\n        compressed_data=compressed_data.append(data)\n        \n        try: \n            data = reader.get_chunk(10**6)\n        except:\n            print('Done reading hwole dataset!!')\n            break\n            ","88fbc623":"compressed_data.info()","5e1ead6c":"import matplotlib.pyplot as plt\nimport seaborn as sns","5caff103":"sns.countplot(x='posa_continent', data=compressed_data)             ","3f9a841f":"compressed_data['site_name'].value_counts(ascending=False)[:10]","398f7343":"compressed_data['hotel_cluster'].value_counts(ascending=False)[:10]","c8738a96":"destinations = pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/destinations.csv')\n\ndestinations.head()","174dc790":"with pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv', iterator=True) as reader:\n    data = reader.get_chunk(10**6)\n        ","d179a0a3":"data.shape","f4923e5b":"pd.set_option('display.max_columns', None)\ndata.head()","ea1c3520":"data.columns","ef360a8b":"destinations.head()","59c0af2b":"train = pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv')","61b1a546":"t1 = pd.DataFrame()\nwith pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv', iterator=True) as reader:\n    data = reader.get_chunk(10**4)\n    t1 = data[data.is_booking != 0]\n    \nt1.groupby('user_id').size().unstack()","7a2f7f6c":"t1 = data[data.is_booking != 0]\ngroupby","3094c537":"import pandas as pda\nprocessed_data = pd.DataFrame()\nwith pd.read_csv('\/kaggle\/input\/expedia-hotel-recommendations\/train.csv', iterator=True) as reader:\n    data = reader.get_chunk(10**6)\n    chunks=1\n    while data is not None:\n        t1 = data[data.is_booking != 0]\n        unique_users = t1.user_id.unique()\n        \n        t1.groupby('user_id').count\n        \n        for user in unique_users:\n            bookings = len(t1.loc[t1['user_id'] == user])\n            if bookings>=20:\n                t1 = t1[t1.user_id != user]\n        print(t1.shape)\n        print('Proccessed chunk number : {}'.format(chunks))\n        processed_data = processed_data.append(t1)\n        break\n        try:\n            data = reader.get_chunk(10**6)\n        except :\n            print('Read all data!!')\n            break\n            \n        chunks+=1\n    print(chunks)\n    \n    \n                \n    ","e4dc0bd1":"processed_data.shape","8659f43b":"plt.figure(figsize=(20,8))a\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.countplot(data = data,x='site_name',order = data['Class'].value_counts().index,palette=\"flare\")\ns","24f2778e":"class MyNumbers:\n  def __iter__(self):\n    self.a = 1\n    return self\n\n  def __next__(self):\n    if self.a <= 20:\n      x = self.a\n      self.a += 1\n      return x\n    else:\n      raise StopIteration\n\nmyclass = MyNumbers()\nmyiter = iter(myclass)\n\nfor x in myiter:\n  print(x)\n","dea247de":" The original data consists of 37 million rows which represent 1.2 million users, so its better to downsample it as far as possible"}}