{"cell_type":{"c1e2c569":"code","d5aca26a":"code","87169918":"code","0efc655a":"code","6eb646b2":"code","e6870613":"code","4a81b87c":"code","2f80f656":"code","881edfcc":"code","022e0890":"code","a4e98522":"code","77f3df6d":"code","7603e467":"code","3dadb90f":"code","220ab4df":"code","4ddf1bfb":"code","6c9b6d2e":"code","f9e0b649":"code","ce2e5088":"code","3509f1b8":"code","26958684":"code","a039f9ee":"code","a518aab2":"code","bb0c23fe":"code","659092d1":"code","e67b7afa":"code","f81a6334":"code","c449d74c":"code","e5c5e1e1":"code","6e708c16":"code","77643598":"code","f56109a8":"code","e6cafda5":"code","aa380e32":"code","8c602f28":"code","6af55d9b":"code","90912401":"code","31db0149":"markdown"},"source":{"c1e2c569":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d5aca26a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet","87169918":"df = pd.read_csv(\"..\/input\/onion-or-not\/OnionOrNot.csv\")","0efc655a":"df.head()","6eb646b2":"df.isna().sum()","e6870613":"df.label.value_counts()","4a81b87c":"x = df[:20000]\ntrue_text = x[x.label == 1].text\nfalse_text = x[x.label == 0].text","2f80f656":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(true_text))\nplt.imshow(wc , interpolation = 'bilinear')","881edfcc":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(false_text))\nplt.imshow(wc , interpolation = 'bilinear')","022e0890":"final_truetext = wc.process_text(\" \".join(true_text))\nfinal_falsetext = wc.process_text(\" \".join(false_text))","a4e98522":"final_truetext = sorted(final_truetext.items(),key = \n             lambda kv:(kv[1], kv[0]))\nfinal_falsetext = sorted(final_falsetext.items(),key = \n             lambda kv:(kv[1], kv[0]))","77f3df6d":"len(final_truetext)","7603e467":"len(final_falsetext)","3dadb90f":"final_truetext = final_truetext[-3000:]\nfinal_falsetext = final_falsetext[-3000:]","220ab4df":"text_true = []\ntext_false = []\nfor i in range(3000):\n    text_true.append(final_truetext[i][0])\n    text_false.append(final_falsetext[i][0])\ntext_true[:5],text_false[:5]    ","4ddf1bfb":"predictions = []\nfor i in test_text:\n    x = i.split()\n    for j in x:\n        if j in text_true and j not in text_false:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\nlen(predictions)","6c9b6d2e":"len(test_category)","f9e0b649":"count = 0\nfor i in range(len(predictions)):\n    test_category = list(test_category)\n    if(predictions[i] == test_category[i]):\n        count += 1\nprint(count)","ce2e5088":"accuracy = (count\/len(predictions))*100\naccuracy","3509f1b8":"print(\"Accuracy using WordCloud is : \", accuracy , \"%\")","26958684":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","a039f9ee":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","a518aab2":"lemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text","bb0c23fe":"df.text = df.text.apply(lemmatize_words)","659092d1":"def join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string    ","e67b7afa":"df.text = df.text.apply(join_text)","f81a6334":"train_text = df.text[:20000]\ntrain_category = df.label[:20000]\ntest_text = df.text[20000:]\ntest_category = df.label[20000:]","c449d74c":"cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(train_text)\n#transformed test reviews\ncv_test_reviews=cv.transform(test_text)\n\nprint('BOW_cv_train:',cv_train_reviews.shape)\nprint('BOW_cv_test:',cv_test_reviews.shape)","e5c5e1e1":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(train_text)\n#transformed test reviews\ntv_test_reviews=tv.transform(test_text)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","6e708c16":"lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n#Fitting the model for Bag of words\nlr_bow=lr.fit(cv_train_reviews,train_category)\nprint(lr_bow)\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(tv_train_reviews,train_category)\nprint(lr_tfidf)","77643598":"#Predicting the model for bag of words\nlr_bow_predict=lr.predict(cv_test_reviews)\n##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(tv_test_reviews)","f56109a8":"#Accuracy score for bag of words\nlr_bow_score=accuracy_score(test_category,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_bow_score)\n#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(test_category,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","e6cafda5":"#Classification report for bag of words\nlr_bow_report=classification_report(test_category,lr_bow_predict,target_names=['0','1'])\nprint(lr_bow_report)\n\n#Classification report for tfidf features\nlr_tfidf_report=classification_report(test_category,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","aa380e32":"#training the model\nmnb=MultinomialNB()\n#fitting the nb for bag of words\nmnb_bow=mnb.fit(cv_train_reviews,train_category)\nprint(mnb_bow)\n#fitting the nb for tfidf features\nmnb_tfidf=mnb.fit(tv_train_reviews,train_category)\nprint(mnb_tfidf)","8c602f28":"#Predicting the model for bag of words\nmnb_bow_predict=mnb.predict(cv_test_reviews)\n#Predicting the model for tfidf features\nmnb_tfidf_predict=mnb.predict(tv_test_reviews)","6af55d9b":"#Accuracy score for bag of words\nmnb_bow_score=accuracy_score(test_category,mnb_bow_predict)\nprint(\"mnb_bow_score :\",mnb_bow_score)\n#Accuracy score for tfidf features\nmnb_tfidf_score=accuracy_score(test_category,mnb_tfidf_predict)\nprint(\"mnb_tfidf_score :\",mnb_tfidf_score)","90912401":"mnb_bow_report = classification_report(test_category,mnb_bow_predict,target_names = ['0','1'])\nprint(mnb_bow_report)\nmnb_tfidf_report = classification_report(test_category,mnb_tfidf_predict,target_names = ['0','1'])\nprint(mnb_tfidf_report)","31db0149":"# Analysis by using just WordCloud"}}