{"cell_type":{"065395ef":"code","77079aaa":"code","466fa65f":"code","a9a97c55":"code","9c44c792":"code","dc54afd2":"code","bbbebbc6":"code","b8f8bdc7":"code","aa4a81cf":"code","d8221e68":"code","c1a74d63":"code","4f6a78b9":"code","c30251d9":"code","671008af":"code","a334dc0e":"code","4131db91":"code","cce8e284":"code","f80ac756":"code","d4c91243":"code","d51b1958":"code","7d31e44d":"code","0e02c0a3":"code","8e855b66":"code","065eeea0":"code","a1a17d13":"code","77487cb4":"code","4885e247":"code","fb5b22a5":"code","e94230c9":"code","ce3e7db0":"code","0fbf282c":"code","982d936d":"code","94bb3adc":"code","a0fd2d23":"code","f581d991":"code","904a4014":"code","504c7d67":"code","fc7b5141":"code","405b1b97":"code","dd454a92":"code","d3c68aaa":"code","e911cc9b":"code","e7783e4d":"code","b215e784":"code","93e6889a":"code","0ebfa43e":"code","a99540d8":"code","6fa9247b":"code","2e886d28":"code","ea383936":"code","46378684":"code","d5f80f3c":"code","2cddea80":"code","ec666932":"code","729eca95":"code","24193666":"code","cc44cbe2":"code","f9c614b8":"code","b9c2a241":"code","b6fbe789":"code","0a1f6bd9":"code","389a891d":"code","afb4fd70":"code","6da5b98b":"code","cdb01c88":"code","5edd9a31":"code","6d7809b4":"code","c8bcc7c3":"code","151c2ff3":"code","a9a553c1":"code","c4c5b01d":"code","1ceca578":"code","7e4e8f7b":"code","7143fe27":"code","ded227c3":"code","7b547894":"code","3f923ab8":"markdown","b73d4b78":"markdown","8c338fbf":"markdown","ac9c75a4":"markdown","04cf24b4":"markdown","5544c9d0":"markdown","0723edcb":"markdown","12604809":"markdown","e689a9a5":"markdown","04bafbf3":"markdown","de268350":"markdown","f0ca6afc":"markdown","da488ee0":"markdown","bfc39da4":"markdown","4fcaabcd":"markdown","30fb0cb7":"markdown","eee3aec2":"markdown","1d270154":"markdown","4d7c938a":"markdown","4c7eb316":"markdown","6b7b0ade":"markdown","133f96e5":"markdown","7335709c":"markdown","378416e9":"markdown","bc3d95f7":"markdown","9e2f1c5f":"markdown","f3cab95b":"markdown","0dfe933f":"markdown","eae16c0d":"markdown","99829321":"markdown","3d93da83":"markdown","21138958":"markdown","7565aacd":"markdown","ec5cb9eb":"markdown","dbdf31d2":"markdown","b019b7ef":"markdown","d4d06ae8":"markdown","c6f98280":"markdown","02c63ecc":"markdown","a71f8551":"markdown","2454617a":"markdown","e08cddf5":"markdown","9d85f407":"markdown","76ed363b":"markdown","776ee114":"markdown","d9a2a062":"markdown","21f08d68":"markdown","c9b2a40e":"markdown"},"source":{"065395ef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","77079aaa":"df = pd.read_csv('..\/input\/clinvar_conflicting.csv')\ndf.head()","466fa65f":"df.info()","a9a97c55":"df = df.drop(['CLNDISDBINCL', 'CLNDNINCL', 'CLNSIGINCL', 'SSR', 'DISTANCE', 'MOTIF_NAME', 'MOTIF_POS', 'HIGH_INF_POS', 'MOTIF_SCORE_CHANGE'], axis = 1)","9c44c792":"for var in ['CLNVI', 'INTRON', 'BAM_EDIT', 'SIFT', 'PolyPhen', 'BLOSUM62']:\n    df[var] = df[var].apply(lambda x: 1 if x == x else 0).astype('category')\n    print(df[var].value_counts())","dc54afd2":"df = df.rename({'CLASS': 'target'}, axis = 1)\ndf['target'] = df['target'].astype('category')\ndf['target'].value_counts()","bbbebbc6":"df['CHROM'].value_counts()","b8f8bdc7":"df['CHROM'] = df['CHROM'].astype('str').apply(lambda x: x.strip())\ndf['CHROM'] = df['CHROM'].astype('category')","aa4a81cf":"df['POS'].describe()","d8221e68":"for var in ['REF', 'ALT', 'Allele']:\n    print(df[var].value_counts()[0:10])","c1a74d63":"for var in ['REF', 'ALT', 'Allele']:\n    df[var] = df[var].apply(lambda x: 'O' if x not in ['A', 'C', 'G', 'T'] else x).astype('category')","4f6a78b9":"df[['AF_ESP', 'AF_EXAC', 'AF_TGP']].describe()","c30251d9":"df[['AF_ESP', 'AF_EXAC', 'AF_TGP']].hist()","671008af":"df['AF_ESP'] = df['AF_ESP'].apply(lambda x: 1 if x > 0 else 0).astype('category')\ndf['AF_EXAC'] = df['AF_EXAC'].apply(lambda x: 1 if x > 0 else 0).astype('category')\ndf['AF_TGP'] = df['AF_TGP'].apply(lambda x: 1 if x > 0 else 0).astype('category')","a334dc0e":"print(len(df['CLNDISDB'].unique()))\ndf['CLNDISDB'].value_counts()[0:10]","4131db91":"df = df.drop('CLNDISDB', axis = 1)","cce8e284":"print(len(df['CLNDN'].unique()))\ndf['CLNDN'].value_counts()[0:20]","f80ac756":"name_df = df['CLNDN'].str.split(pat = '|', expand = True)\nname_df.head()\ntop_100_dn = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)[0:100]\nprint(top_100_dn[0:10])\n\ntop_100_dn_list = list(top_100_dn.index)\nprint(top_100_dn_list[0:10])","d4c91243":"for dn in top_100_dn_list:\n    df[dn] = df['CLNDN'].apply(lambda x: 1 if dn in x else 0).astype('category')\ndf = df.drop('CLNDN', axis = 1)","d51b1958":"print(df.columns)","7d31e44d":"print(len(df['CLNHGVS'].unique()))\ndf = df.drop('CLNHGVS', axis = 1)","0e02c0a3":"print(df['CLNVC'].value_counts())","8e855b66":"clnvc_types = ['single_nucleotide_variant', 'Deletion', 'Duplication']\ndf['CLNVC'] = df['CLNVC'].apply(lambda x: x if x in clnvc_types else 'Other').astype('category')","065eeea0":"df['MC'].value_counts()[0:10]","a1a17d13":"name_df = df['MC'].str.split(pat = '[|,]', expand = True)\nname_df.head()\ntop_mc = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)[0:20]\nprint(top_mc)\n\ntop_mc_list = [x for x in list(top_mc.index) if 'SO:' not in x]\nprint(top_mc_list)","77487cb4":"df['MC'] = df['MC'].fillna('unknown')\nfor mc in top_mc_list:\n    df[mc] = df['MC'].apply(lambda x: 1 if mc in x else 0).astype('category')\n    print(df[mc].value_counts())\ndf = df.drop('MC', axis = 1)","4885e247":"df['ORIGIN'] = df['ORIGIN'].fillna(0).apply(lambda x: 1 if x == 1.0 else 0).astype('category')","fb5b22a5":"name_df = df['Consequence'].str.split(pat = '&', expand = True)\nname_df.head()\ntop_mc = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)\nprint(top_mc[0:20])","e94230c9":"for mc in top_mc_list:\n    mc2 = mc + '2'\n    df[mc2] = df['Consequence'].apply(lambda x: 1 if mc in x else 0).astype('category')\n    df[mc] = df[[mc, mc2]].apply(lambda x: max(x[mc], x[mc2]), axis = 1).astype('category')\n    print(df[mc].value_counts())\n    df=df.drop(mc2, axis = 1)\ndf = df.drop('Consequence', axis = 1)","ce3e7db0":"df['IMPACT'].value_counts()","0fbf282c":"df['IMPACT'] = df['IMPACT'].astype('category')","982d936d":"len(df['SYMBOL'].unique())","94bb3adc":"df['SYMBOL'].value_counts()[0:10]","a0fd2d23":"top_100_symb = df['SYMBOL'].value_counts()[0:100].index\ndf['SYMBOL'] = df['SYMBOL'].apply(lambda x: x if x in top_100_symb else 'Other').astype('category')","f581d991":"df['SYMBOL'].value_counts()[0:100]","904a4014":"df = df.drop('Feature', axis = 1)","504c7d67":"for var in ['Feature_type', 'BIOTYPE']:\n    print(df[var].value_counts())\n    df = df.drop(var, axis = 1)","fc7b5141":"len(df['EXON'].unique())","405b1b97":"df = df.drop('EXON', axis = 1)","dd454a92":"df = df.drop(['cDNA_position', 'CDS_position', 'Protein_position'], axis = 1)","d3c68aaa":"df = df.drop(['Amino_acids', 'Codons'], axis = 1)","e911cc9b":"df['STRAND'].value_counts()","e7783e4d":"df['STRAND'] = df['STRAND'].fillna(df['STRAND'].mode())\ndf['STRAND'] = df['STRAND'].astype('category')","b215e784":"df['LoFtool'] = df['LoFtool'].fillna(df['LoFtool'].median())","93e6889a":"df['CADD_PHRED'] = df['CADD_PHRED'].fillna(df['CADD_PHRED'].median())","0ebfa43e":"df['CADD_RAW'] = df['CADD_RAW'].fillna(df['CADD_RAW'].median())","a99540d8":"from sklearn.preprocessing import StandardScaler\n\nnum_var_list = ['POS', 'LoFtool', 'CADD_PHRED', 'CADD_RAW']\nscl = StandardScaler()\ndf[num_var_list] = scl.fit_transform(df[num_var_list])","6fa9247b":"target = df['target']\nfeatures = df.drop('target', axis = 1)","2e886d28":"#Original columns\nlist(df.columns[0:23])","ea383936":"df.iloc[:, 0:23].info()","46378684":"#Original feature set\norig_feat = list(features.columns[0:22])\norig_feat_cat = [x for x in orig_feat if x not in num_var_list]","d5f80f3c":"features[num_var_list].describe()","2cddea80":"features[num_var_list].hist()","ec666932":"plt.figure(figsize=(8,8))\nsns.heatmap(features[num_var_list].corr(),\n            vmin=0,\n            vmax=1,\n            cmap='YlGnBu',\n            annot=np.round(features[num_var_list].corr(), 2))","729eca95":"features = features.drop('CADD_RAW', axis = 1)","24193666":"import scipy.stats as ss\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","cc44cbe2":"num_feat = len(orig_feat_cat)\ncat_corr_arr = np.empty((num_feat, num_feat))\nfor i, row in enumerate(orig_feat_cat):\n    for j, col in enumerate(orig_feat_cat):\n        #print((i, j))\n        cat_corr_arr[i, j] = cramers_v(features[row], features[col])\nprint(cat_corr_arr[0:5, 0:5])","f9c614b8":"plt.figure(figsize=(16, 14))\nsns.heatmap(cat_corr_arr,\n            vmin=0,\n            vmax=1,\n            cmap='YlGnBu',\n            xticklabels = orig_feat_cat,\n            yticklabels = orig_feat_cat,\n            annot=np.round(cat_corr_arr, 2))","b9c2a241":"features = features.drop(['Allele', 'IMPACT', 'SYMBOL', 'PolyPhen'], axis = 1)","b6fbe789":"from sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, make_scorer","0a1f6bd9":"features = pd.get_dummies(features, drop_first = True)\nprint(features.columns)","389a891d":"f1_scorer = make_scorer(f1_score)","afb4fd70":"dm_clf = DummyClassifier(random_state = 42)\nmean_dm_cv_score = cross_val_score(dm_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Dummy Classifier: {:.3}\".format(mean_dm_cv_score))","6da5b98b":"gnb_clf = GaussianNB()\nmean_gnb_cv_score = cross_val_score(gnb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Gaussian Naive Bayes Classifier: {:.3}\".format(mean_gnb_cv_score))","cdb01c88":"bnb_clf = BernoulliNB()\nmean_bnb_cv_score = cross_val_score(bnb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Bernoulli Naive Bayes Classifier: {:.3}\".format(mean_bnb_cv_score))","5edd9a31":"adb_clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(), random_state = 42)\nmean_adb_cv_score = cross_val_score(adb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for AdaBoost Decision Tree Classifier: {:.3}\".format(mean_adb_cv_score))","6d7809b4":"adb_clf = AdaBoostClassifier(base_estimator = LogisticRegression(solver = 'lbfgs'), random_state = 42)\nmean_adb_cv_score = cross_val_score(adb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for AdaBoost Logistic Regression Classifier: {:.3}\".format(mean_adb_cv_score))","c8bcc7c3":"import xgboost as xgb\nxgb_clf = xgb.XGBClassifier(seed = 123)\nmean_xgb_cv_score = cross_val_score(xgb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for XGBoost Classifier: {:.3}\".format(mean_xgb_cv_score))","151c2ff3":"rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42)\nmean_rf_cv_score = cross_val_score(rf_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Random Forest Classifier: {:.3}\".format(mean_rf_cv_score))","a9a553c1":"bnb_param_grid = {\n'alpha': [0.1, 0.5, 1, 2, 5],\n'fit_prior': [True, False]\n}","c4c5b01d":"import time\nstart = time.time()\nbnb_grid_search = GridSearchCV(bnb_clf, bnb_param_grid, scoring = f1_scorer, cv = 3)\nbnb_grid_search.fit(features, target)\n\nprint(\"Cross Validation F1 Score: {:.3}\".format(bnb_grid_search.best_score_))\nprint(\"Total Runtime for Grid Search on Bernoulli Naive Bayes: {:.4} seconds\".format(time.time() - start))\nprint(\"\")\nprint(\"Optimal Parameters: {}\".format(bnb_grid_search.best_params_))","1ceca578":"best_bnb = bnb_grid_search.best_estimator_","7e4e8f7b":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.33, random_state = 42)\nbest_bnb.fit(X_train, y_train)\ny_hat_test = best_bnb.predict(X_test) \nbnb_confusion_matrix = confusion_matrix(y_test, y_hat_test)\nprint(bnb_confusion_matrix)\nbnb_classification_report = classification_report(y_test, y_hat_test)\nprint(bnb_classification_report)","7143fe27":"feat_df = pd.DataFrame()\nfeat_df['prob_0'] = np.exp(best_bnb.feature_log_prob_[0])\nfeat_df['prob_1'] = np.exp(best_bnb.feature_log_prob_[1])\nfeat_df.index = features.columns\nfeat_df.head()","ded227c3":"feat_df['ave_prob'] = feat_df.apply(lambda x: (x[0] + x[1])\/2, axis = 1)\nfeat_df['prob_diff'] = feat_df.apply(lambda x: np.abs(x[0] - x[1]), axis = 1)\nfeat_df.head()","7b547894":"feat_df.sort_values('prob_diff', ascending=False).head(10)","3f923ab8":"Each value is a list of diseases. It seems like I could clean this by creating dummy variables for specific common diseases in each list. I will create dummies for the top 100 diseases.","b73d4b78":"## Feature Probabilities\n\nWhile Naive Bayes doesn't yeild feature importances, I am able to look at which features have the largest difference in predicted probability of being present between the two target classes. The allele frequency variables jump out as having the largest differences, as do genes associated with unknown disease variants.","8c338fbf":"## Very Low Incidence Features\n\nHere I drop features with under 600 entries (1% of dataset).","ac9c75a4":"## LoFtool\n\nNumeric variable: Loss of Function tolerance score for loss of function variants. Will fill missing values with median.","04cf24b4":"# Modeling\n\nFor this problem, I choose to use multiple classifiers to see how they compare. I start with a dummy classifier as a baseline for comparison. Then I proceed to Random Forest Classifier, Naive Bayes, and AdaBoost. I will test the effects of various parameter spefications on model performance.","5544c9d0":"## XGBoost\n\nI next decide to use XGBoost, a popular boosting algorithm. This does not seem to improve performance.","0723edcb":"## REF, ALT, Allele\n\nThese variables are for capture variant alleles - should be categorical.","12604809":"## Dummy\n\nThe F1 score for the Dummy classifier is 0.253, providing a point of comparison for other models.","e689a9a5":"# Conclusion\n\nIn this analysis, I find that I am able to predict when experts will disagree about gene severity moderately well, with an F1 score of 0.437 for my final Bernoulli Naive Bayes model. This is a notable improvement over the dummy model, with F1 score of 0.253. This model can be used to prioritize research on gene variants with debatable severity. However, there is still a fair amount of misclassification, specifically with concurrences being classified as disagreements more often than warranted. Future analysis could look for ways to better balance the overall accuracy with the recall of the model.","04bafbf3":"## POS\n\nThis variable captures position of the gene on the chromosome. Will need to treat this with care in analysis, since it depends on CHROM.","de268350":"# Obtaining the Data\n\nFor this project, I downloaded the dataset from the Kaggle page as a csv.","f0ca6afc":"## Performance\n\nLooking at the performance, I see that the unbalanced nature of the classes seem to yeild a fair amount of misclassification. Specifically, a number of cases where experts agreed were classified as being cases of disagreement (the upper right of the confusion matrix).","da488ee0":"## Low Incidence Features\n\nHere I dichotomize features that are present for less than half the dataset, 1 indicating that data are present, 0 otherwise.","bfc39da4":"# Scrubbing the Data\n\nThere seem to be a number of feilds with missing data and incorrect types. In this section, I scrub the dataset squeaky-clean.","4fcaabcd":"## Consequence\n\nThis variable is similar to MC, but with slightly different values. I'm not sure why. I will use it to update the MC dummy variables from before.","30fb0cb7":"## Separate target and features","eee3aec2":"## IMPACT\n\nCategorical variable capturing variant impact","1d270154":"## AF_ESP, AF_EXAC, and AF_TGP\n\nThese variables capture the allele frequency as found in other datasets. They are almost all zero, so I dichotomize them into zero vs non-zero.","4d7c938a":"## CLNVC\n\nThis variant type variable is almost all one value - I will turn it into a categorical variable by consolidating low-incidence types.","4c7eb316":"## Target: CLASS\n\nThe CLASS vartible is the target variable, which indicates whether there were conflicting submissions.","6b7b0ade":"## Amino_acids, Codons\n\nThese have a large number of unique values, so I drop them.","133f96e5":"## ORIGIN\n\nHere is the description: \"Allele origin. One or more of the following values may be added: 0 - unknown; 1 - germline; 2 - somatic; 4 - inherited; 8 - paternal; 16 - maternal; 32 - de-novo; 64 - biparental; 128 - uniparental; 256 - not-tested; 512 - tested-inconclusive; 1073741824 - other\" Since almost all have origin 1 (germline), I will recode this to have 0 for all other values to make it a dummy variable.","7335709c":"## MC\n\nMolecular consequence is a categorical variable, need to clean up rare values. Since values are lists of consequences, I will do this similarly to how I did it for the names, splitting up the series and coding dummies.","378416e9":"## cDNA_position, CDS_position, Protein_position\n\nThese represent relative positions of the base pair in various ways. These are all distance measures, which I think are irrelevant to the problem at hand, and difficult to clean so I drop them.","bc3d95f7":"## Feature_type and BIOTYPE\n\nThese features have little information (almost all records have same value), so I drop them.","9e2f1c5f":"## Random Forest\n\nLastly I fit a Random Forest model, which has an F1 score of 0.212.","f3cab95b":"There are a lot of low-frequency categories - I will lump them together into an \"other\" category.","0dfe933f":"## Bernoulli Naive Bayes Tuning\n\nHere I use grid search and random search to tune the highest performing model: Bernoulli Naive Bayes. The best hyperparameters yeild an F1 score of 0.437. This is not great, but substantially better than the dummy model.","eae16c0d":"## Scaling numeric variables","99829321":"## EXON\n\nThis captures the relative exon number. Given the very large numbers of unique values, I choose to drop it.","3d93da83":"## CLNDISDB\n\nThis variable contains IDs for diseases in other databases. This variable has a large number of values, so it will be difficult to use it. I see that different values for this variable often contain the same identifiedrs, making the values arguable not unique (e.g. 'MedGen:CN169374' appears in multiple values). I choose to drop it.","21138958":"## STRAND\n\nCategorical: defined as + (forward) or - (reverse)","7565aacd":"## SYMBOL\n\nThis variable is the Gene symbol\/ID. It has many values - I will make it categorical, but only keep the top 100 values, recoding the rest as \"Other\".","ec5cb9eb":"I choose to drop the Allele, IMPACT, SYMBOL and PolyPhen variables due to high correlations.","dbdf31d2":"## Feature\n\nThis is an ID associated with gene name - deleting due to redundancy","b019b7ef":"## Initialization","d4d06ae8":"# Interpreting the Model\n\nBelow I explore the final model to better understand the important properties of the model.","c6f98280":"# Exploring the Data","02c63ecc":"# Overview\n\nI completed this project as part of my Data Science course at the Flatiron School. Check it out! https:\/\/flatironschool.com\/\n\nThis project uses a Kaggle dataset to predict gene classifications. In this dataset, we are given multiple genetic variants and various properties of each. Expert raters at different laboratories rated these variants based on their perceived clinical classifications, with ratings ranging from Benign to Pathogenic. The target variable is whether the raters have clinical classifications that are concordant, meaning that they are in the same clinical category.\n\nI approach this with the OMESN framework. Data cleaning turns out to be the most substantial part of this project. In the end, I test a few different modeling approaches and present the results of the highest scoring model.","a71f8551":"## Evaluation Metric\n\nFor this problem I choose the F1 score, which balances precision and recall. Since there are fewer positive cases than negative one, I want to include recall as part of my metric, but I want to include precision as well to avoid over-classification.","2454617a":"## Numeric Variables\n\nIn this section I explore the numeric variables in the dataset. I find that there is a high correlation between CADD_PHRED and CADD_RAW, so I choose to drop one of them. I drop CADD_RAW due to the long right tail.","e08cddf5":"## Categorical Variables\n\nAssociations between categorical variables can be difficult to visualize. I use Cramer's V to understand the associations between each pair of categorical features, adapting this code: https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9","9d85f407":"## CLNHGVS\n\nThis variable is all unique values that I don't understand related to HGVS expression. I choose to drop it.","76ed363b":"## Naive Bayes\n\nGaussian Naive Bayes doesn't seem like a natural fit, given that there are many one-hot encoded variables in this dataset, but I am curious whether its performance is better than the Dummy classifier - at 0.341 it does seem to be better. Bernoulli Naive Bayes does even better still.","776ee114":"## AdaBoost\n\nI decide to fit AdaBooost next using decision tree and logistic regression classifiers. These models provide no improvement over Naive Bayes.","d9a2a062":"## CLNDN\n\nThis captures the preferred disease name using the identifiers from CLNDISDB. This may be cleaner than the other variable, and is probably important for prediction, so I will attempt to clean it.","21f08d68":"## CADD_PHRED, CADD_RAW\n\nDifferent scores of deleteriousness - I keep them and fill missing values with medians.","c9b2a40e":"## CHROM\n\nThis variable captures the chromosome on which the variant is located. This should be a categorical variable. Strangely, there are two \"16\"s in this list, which should be combined. I do this by converting it to a strip and striping the spaces before making it into a category."}}