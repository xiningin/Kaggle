{"cell_type":{"503f871e":"code","730c1b5c":"code","b207a2dc":"code","b2ccba3d":"code","bed3a138":"code","19195a19":"code","35784d72":"code","4861975d":"code","d8822cca":"code","e7b11cf4":"code","abfd5bde":"code","661e8c4d":"code","380b6f36":"code","eae504ed":"code","7f6ff0a0":"code","582863d9":"code","86ba254c":"code","5e6493ed":"code","51382869":"code","cd4096c7":"code","6e8e7940":"code","003f1cef":"code","7dbb55fa":"code","aa253cba":"code","afa34cde":"code","e7dbf1a7":"code","62c6f531":"code","d1085f8e":"code","dc478ef6":"code","3f75f946":"code","abe8e2a7":"code","563e5131":"markdown","ee97ebba":"markdown","537fd35a":"markdown","89b484c1":"markdown","3e284847":"markdown","a8caee36":"markdown","1ef94214":"markdown"},"source":{"503f871e":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","730c1b5c":"# Make scorer: accuracy\naccuracy = make_scorer(accuracy_score)","b207a2dc":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntestSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\nsubmitSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/sample_submission.csv')\n\ntrainSet.head()","b2ccba3d":"# Drop columns with lacking data\ntrain = trainSet.drop(columns=['Id','idhogar','rez_esc', 'v18q1', 'v2a1', 'dependency', 'edjefe', 'edjefa'])\n\n# Drop rows with missing values\ntrain = train.dropna(axis=0)\n\nprint(train.shape)\ntrain.head()","bed3a138":"# Select features with high importance\nselected = ['tipovivi5', 'hogar_mayor', 'abastaguano', 'epared2', 'area1',\n       'tipovivi1', 'elimbasu2', 'refrig', 'mobilephone', 'energcocinar2',\n       'pisocemento', 'pareddes', 'elimbasu1', 'etecho2', 'lugar4',\n       'paredmad', 'paredzinc', 'lugar1', 'tamviv', 'lugar3',\n       'television', 'rooms', 'epared1', 'tipovivi3', 'etecho1',\n       'SQBedjefe', 'epared3', 'parentesco9', 'bedrooms', 'r4m2',\n       'overcrowding', 'sanitario5', 'paredzocalo', 'eviv1', 'paredpreb',\n       'etecho3', 'abastaguadentro', 'r4m3', 'techozinc', 'pisomadera',\n       'sanitario3', 'eviv2', 'tamhog', 'v14a', 'r4t2', 'public',\n       'lugar5', 'elimbasu3', 'r4m1', 'hacdor', 'r4t3', 'energcocinar4',\n       'r4h1', 'sanitario2', 'hogar_adul', 'r4h2', 'cielorazo',\n       'qmobilephone', 'tipovivi4', 'pisomoscer', 'meaneduc', 'tipovivi2',\n       'paredblolad', 'computer', 'r4t1', 'pisonotiene', 'SQBdependency',\n       'eviv3', 'hacapo', 'hogar_nin', 'v18q']","19195a19":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train[selected], train['Target'],\n                                                  test_size=0.2, random_state=123,\n                                                  stratify=train['Target'])","35784d72":"!pip install pycaret","4861975d":"from pycaret.classification import *","d8822cca":"# Generate\nval_index = np.random.choice(range(trainSet.shape[0]), round(trainSet.shape[0]*0.2), replace=False)\n\n# Split trainSet\ntrainSet1 = trainSet.drop(val_index)\ntrainSet2 = trainSet.iloc[val_index,:]","e7b11cf4":"# Create the model\ncaret = setup(data = trainSet1, target='Target', session_id=123,\n              numeric_imputation='mean',  categorical_imputation='constant',\n              normalize = True, combine_rare_levels = True, rare_level_threshold = 0.05,\n              remove_multicollinearity = True, multicollinearity_threshold = 0.95)","abfd5bde":"# Show the models\ncaret_models = compare_models(fold=5)","661e8c4d":"# Create the top 5 models\net = create_model('et', fold=5)\nrf = create_model('rf', fold=5)\ndt = create_model('dt', fold=5)\nxgboost = create_model('xgboost', fold=5)\nlightgbm = create_model('lightgbm', fold=5)\n# If each algorithm is created in 1 cell, each output will show each cross-validation result.\n# Below is the cross_validation report of lightGBM as the it is the last line","380b6f36":"# Tune the models, for example for LightGBM\nlightgbm_tune = tune_model(lightgbm, fold=5)","eae504ed":"# Show the tuned hyperparameters, for example for LightGBM\nplot_model(lightgbm_tune, plot='parameter')","7f6ff0a0":"# Bagging LightGBM\nlightgbm_bagging = ensemble_model(lightgbm_tune, fold=5)","582863d9":"# Boosting LightGBM\nlightgbm_boost = ensemble_model(lightgbm_tune, method='Boosting', fold=5)","86ba254c":"# Return top 5 models\ncaret_models_5 = compare_models(n_select=5)","5e6493ed":"# Stacking with GBM as the meta-model\nstack = stack_models(caret_models_5, meta_model=et, fold=5)","51382869":"# Blending top models\ncaret_blend = blend_models(estimator_list=[lightgbm_tune,rf,dt])","cd4096c7":"# Predict the validation data\npred_caret = predict_model(caret_blend, data = trainSet2.drop(columns=['Target']))\npred_caret = pred_caret['Label']\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(trainSet2['Target'], pred_caret)))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(trainSet2['Target'], pred_caret), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(trainSet2['Target'], pred_caret))","6e8e7940":"!pip install autoviml\n!pip install shap","003f1cef":"from autoviml.Auto_ViML import Auto_ViML","7dbb55fa":"# Create the model\nviml, features, train_v, test_v = Auto_ViML(trainSet1, 'Target', trainSet2.drop(columns=['Target']),\n                                            scoring_parameter='balanced_accuracy', hyper_param='RS',\n                                            feature_reduction=True, Boosting_Flag=True,\n                                            Binning_Flag=False,Add_Poly=0, Stacking_Flag=False, \n                                            Imbalanced_Flag=True, verbose=1)","aa253cba":"viml\n# The model picks XGBClassifier","afa34cde":"# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(trainSet2['Target'], test_v['Target_Ensembled_predictions'])))\nprint('')\n\n# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(trainSet2['Target'], test_v['Target_Ensembled_predictions']), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(trainSet2['Target'], test_v['Target_Ensembled_predictions']))","e7dbf1a7":"!pip install -U https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/fix\/logging\/LightAutoML-0.2.16.2-py3-none-any.whl\n!pip install openpyxl","62c6f531":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task","d1085f8e":"train_data = pd.concat([X_train, y_train], axis=1)\ntrain_data.head()","dc478ef6":"# Create the model\nlight = TabularAutoML(task=Task('multiclass',), timeout=60*3, cpu_limit=4)\n\n# Fit the training data\ntrain_light = light.fit_predict(train_data, roles = {'target': 'Target'})\n\n# Predict the validation data\npred_light = light.predict(X_val)","3f75f946":"# Convert the prediction result into dataframe\npred_light2 = pred_light.data\npred_light2 = pd.DataFrame(pred_light2, columns=['4','2','3','1'])\npred_light2 = pred_light2[['1','2','3','4']]\npred_light2['Pred'] = pred_light2.idxmax(axis=1)\npred_light2['Pred'] = pred_light2['Pred'].astype(int)\npred_light2.head()","abe8e2a7":"# Prediction results\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_light2['Pred']), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_light2['Pred']))","563e5131":" # \ud83e\udd16 Automated Machine Learning-Classification\n  \nThis notebook provides Automated Machine Learning (AutoML) algorithms for a multi-class classification task. Data preparation is just simply performed as the pre-processing will be automatically done, followed by building Machine Learning algorithms and tuning the hyperparameters. The objective of this notebook is to serve as a cheat sheet.","ee97ebba":"To find the process of feature selection, please visit this notebook https:\/\/www.kaggle.com\/rendyk\/multi-classclassification-accuracy-povertylevel\n\nThat notebook demonstrates regression using conventional Machine Learning algorithms for learning the same dataset.","537fd35a":"The task is to predict which poverty class each household is in. There are 4 classes of poverty level: 1 = extreme poverty, 2 = moderate poverty, 3 = vulnerable households, and 4 = non vulnerable households.","89b484c1":"# 10. LightAutoML","3e284847":"# 9. AutoViML","a8caee36":"# 8. PyCaret","1ef94214":"Please find the part 1 here https:\/\/www.kaggle.com\/rendyk\/automl-for-classification"}}