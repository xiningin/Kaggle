{"cell_type":{"60fbf3ec":"code","70bcbe9a":"code","8bc03adb":"code","ee67fb2a":"code","4c690a96":"code","05923d94":"code","1d5ffcea":"code","607c025b":"code","7165db33":"code","c28a6b5a":"code","0f3d1c4e":"code","9daa3caf":"code","aaa3de30":"code","1c6ff0df":"code","308d4c5e":"code","3cf8a0b0":"code","93026fce":"code","de5c8a60":"code","046b8ccc":"code","8454ab6a":"code","11f0fb58":"markdown","d133c4c8":"markdown","c4764cd1":"markdown","fcf51b5c":"markdown","8cde1bfa":"markdown"},"source":{"60fbf3ec":"import pandas as pd\nimport numpy as np\n\nimport re \nimport scipy\nfrom scipy import sparse\nimport gc \n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100","70bcbe9a":"n_folds = 7\n\nfrac_1 = 0.7\nfrac_1_factor = 1.3","8bc03adb":"def create_folds(train_path_name):\n    for fld in range(n_folds):\n        print(f'Fold: {fld}')\n        \n        if train_path_name == 'dfr_fld':\n            tmp_df = df.sample(frac=frac_1, random_state = 10*(fld+1))\n        \n        else:\n            tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                                df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                                    random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n\n        tmp_df.to_csv(f'\/kaggle\/working\/{train_path_name}{fld}.csv', index=False)\n        print(tmp_df.shape)\n        print(tmp_df['y'].value_counts())","ee67fb2a":"def clean(data, col):\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    # Remove ip address\n    data[col] = data[col].str.replace(r'(([0-9]+\\.){2,}[0-9]+)',' ')\n    \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')\n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    \n    return data","4c690a96":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(df.shape)\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","05923d94":"# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']\/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","1d5ffcea":"df['y'].value_counts()","607c025b":"create_folds('df_fld')","7165db33":"df = clean(df,'text')","c28a6b5a":"create_folds('df_clean_fld')","0f3d1c4e":"df = pd.read_csv('..\/input\/toxic-tweets-dataset\/FinalBalancedDataset.csv')\nprint(df.shape)","9daa3caf":"df = df[['Toxicity', 'tweet']].rename(columns={'Toxicity': 'y', 'tweet': 'text'})\nprint(df['y'].value_counts())\ndf.sample(5)","aaa3de30":"df = clean(df,'text')","1c6ff0df":"create_folds('df_tweets_fld')","308d4c5e":"df = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\nprint(df.shape)\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(f'****** {col} *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","3cf8a0b0":"# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']\/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.y.value_counts()","93026fce":"df = clean(df,'text')","de5c8a60":"create_folds('dfm_fld')","046b8ccc":"df = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nprint(df.shape)\n\ndf = df[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})\n\ndf['y'] = (df['y'] - df.y.min()) \/ (df.y.max() - df.y.min()) \ndf.y.hist()","8454ab6a":"create_folds('dfr_fld')","11f0fb58":"# Read Jigsaw multilingual data CLEANED","d133c4c8":"# Ruddit data","c4764cd1":"# Toxic tweet","fcf51b5c":"# Toxic comment classification","8cde1bfa":"# Toxic comment classification clean"}}