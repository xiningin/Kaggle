{"cell_type":{"f722cf20":"code","839ef391":"code","0bdea8d7":"code","a8b6529b":"code","61e1f77f":"code","e87e1ec5":"code","938a72b5":"code","6aeafc97":"code","86efcfc0":"code","ccfc7281":"code","6d601f9c":"code","51f127d1":"code","dd0d974e":"code","7df48ddd":"code","25b0759a":"code","7b12d88b":"code","b17b163a":"code","7c91adad":"code","568740ae":"code","2e334fe9":"code","8b450350":"code","ca08556d":"code","5e6f9afd":"code","7a99e71e":"code","adbefa90":"code","b1c65559":"code","9940dc20":"code","ba7aa72b":"code","5e680255":"code","382aa875":"code","1220a0a5":"code","61b36855":"code","c907cbc1":"code","306de428":"code","3aef27c9":"code","0d0e61ea":"code","4d56e76b":"code","d3e48e5d":"code","94ccf913":"code","c5afd403":"code","81248030":"code","0f51ca49":"code","3605a2ec":"code","a60e703b":"code","a59a331a":"code","89c398a0":"markdown","49fbbf14":"markdown","a037d402":"markdown","e0fafe3f":"markdown","3c3bbb51":"markdown","1499489f":"markdown","5adbf9b5":"markdown","3ea474f6":"markdown","a6b008c9":"markdown","b326f3f8":"markdown","1b86d478":"markdown","5ad4304c":"markdown","31b2f955":"markdown","808d6b64":"markdown","0f030d4e":"markdown","730a58e2":"markdown","89febbfa":"markdown","7293564a":"markdown","7e2e2e5f":"markdown","ebd939a4":"markdown","e2d8646f":"markdown","54b0fdef":"markdown","9c4a5824":"markdown","2aab29fd":"markdown","2cd167a2":"markdown","0f309d4b":"markdown","2297f6d0":"markdown","30c29dab":"markdown","82804e18":"markdown","346fa62b":"markdown","991b411e":"markdown","61685df0":"markdown","485546ce":"markdown","ed88fd9e":"markdown","f778049c":"markdown","26b8fbbb":"markdown","b423f2ac":"markdown","db98539e":"markdown","002c4317":"markdown","eba26a7c":"markdown","451d1d6b":"markdown","1b03d1a2":"markdown","be08401f":"markdown"},"source":{"f722cf20":"import pandas as pd\nimport pyarrow.parquet as pq\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt","839ef391":"train_meta = pd.read_csv(\"..\/input\/metadata_train.csv\")\n#train_meta.head(6)","0bdea8d7":"test_meta = pd.read_csv(\"..\/input\/metadata_test.csv\")\n#test_meta.head(6)","a8b6529b":"#I use bkt as short for bucket, rather than bin for bin since I tend to read bin as binary\nbkt_count = 160\ndata_size = 800000\nbkt_size = int(data_size\/bkt_count)\n\ndef summarize_df_np(meta_df, data_type, p_id):\n    count = 0\n    measure_rows = []\n\n    for measurement_id in meta_df[\"id_measurement\"].unique():\n        count += 1\n        idx1 = measurement_id * 3\n        input_col_names = [str(idx1), str(idx1+1), str(idx1+2)]\n        df_sig = pq.read_pandas('..\/input\/'+data_type+'.parquet', columns=input_col_names).to_pandas()\n        df_sig = df_sig.clip(upper=127, lower=-127)\n\n        df_diff = pd.DataFrame()\n        for col in input_col_names:\n            df_diff[col] = df_sig[col].diff().abs()\n        \n        data_measure = df_sig.values\n        data_diffs = df_diff.values\n        sig_rows = []\n        sig_ts_rows = []\n        for sig in range(0, 3):\n            #take the data for each 3 signals in a measure separately\n            data_sig = data_measure[:,sig]\n            data_diff = data_diffs[:,sig]\n            bkt_rows = []\n            diff_avg = np.nanmean(data_diff)\n            for i in range(0, data_size, bkt_size):\n                # cut data to bkt_size (bucket size)\n                bkt_data_raw = data_sig[i:i + bkt_size]\n                bkt_avg_raw = bkt_data_raw.mean() #1\n                bkt_sum_raw = bkt_data_raw.sum() #1\n                bkt_std_raw = bkt_data_raw.std() #1\n                bkt_std_top = bkt_avg_raw + bkt_std_raw #1\n                bkt_std_bot = bkt_avg_raw - bkt_std_raw #1\n\n                bkt_percentiles = np.percentile(bkt_data_raw, [0, 1, 25, 50, 75, 99, 100]) #7\n                bkt_range = bkt_percentiles[-1] - bkt_percentiles[0] #1\n                bkt_rel_perc = bkt_percentiles - bkt_avg_raw #7\n\n                bkt_data_diff = data_diff[i:i + bkt_size]\n                bkt_avg_diff = np.nanmean(bkt_data_diff) #1\n                bkt_sum_diff = np.nansum(bkt_data_diff) #1\n                bkt_std_diff = np.nanstd(bkt_data_diff) #1\n                bkt_min_diff = np.nanmin(bkt_data_diff) #1\n                bkt_max_diff = np.nanmax(bkt_data_diff) #1\n\n                raw_features = np.asarray([bkt_avg_raw, bkt_std_raw, bkt_std_top, bkt_std_bot, bkt_range])\n                diff_features = np.asarray([bkt_avg_diff, bkt_std_diff, bkt_sum_diff])\n                bkt_row = np.concatenate([raw_features, diff_features, bkt_percentiles, bkt_rel_perc])\n                bkt_rows.append(bkt_row)\n            sig_rows.extend(bkt_rows)\n        measure_rows.extend(sig_rows)\n    df_sum = pd.DataFrame(measure_rows)\n    #df_sum = df_sum.astype(\"float32\")\n    return df_sum\n","61e1f77f":"def process_subtrain(arg_tuple):\n    meta, idx = arg_tuple\n    df_sum = summarize_df_np(meta, \"train\", idx)\n    return idx, df_sum","e87e1ec5":"from sklearn.preprocessing import MinMaxScaler\n\nminmax = MinMaxScaler(feature_range=(-1,1))","938a72b5":"def create_chunk_indices(meta_df, chunk_idx, chunk_size):\n    start_idx = chunk_idx * chunk_size\n    end_idx = start_idx + chunk_size\n    meta_chunk = meta_df[start_idx:end_idx]\n    print(\"start\/end \"+str(chunk_idx+1)+\":\" + str(start_idx) + \",\" + str(end_idx))\n    print(len(meta_chunk))\n    #chunk_idx in return value is used to sort the processed chunks back into original order,\n    return (meta_chunk, chunk_idx)","6aeafc97":"from multiprocessing import Pool\n\nnum_cores = 4\n\ndef process_train():\n    #splitting here by measurement id's to get all signals for a measurement into single chunk\n    measurement_ids = train_meta[\"id_measurement\"].unique()\n    df_split = np.array_split(measurement_ids, num_cores)\n    chunk_size = len(df_split[0]) * 3\n    \n    chunk1 = create_chunk_indices(train_meta, 0, chunk_size)\n    chunk2 = create_chunk_indices(train_meta, 1, chunk_size)\n    chunk3 = create_chunk_indices(train_meta, 2, chunk_size)\n    chunk4 = create_chunk_indices(train_meta, 3, chunk_size)\n\n    #list of items for multiprocessing, 4 since using 4 cores\n    all_chunks = [chunk1, chunk2, chunk3, chunk4]\n    \n    pool = Pool(num_cores)\n    #this starts the (four) parallel processes and collects their results\n    #-> process_subtrain() is called concurrently with each item in all_chunks \n    result = pool.map(process_subtrain, all_chunks)\n    #parallel processing can be non-deterministic in timing, so here I sort results by their chunk id\n    #to maintain results in same order as in original files (to match metadata from other file)\n    print(\"sorting\")\n    result = sorted(result, key=lambda tup: tup[0])\n    print(\"sorted\")\n    sums = [item[1] for item in result]\n    \n    df_train = pd.concat(sums)\n    df_train = df_train.reset_index(drop=True)\n    #np.save() would be another option but this works for now\n    df_train.to_csv(\"my_train.csv.gz\", compression=\"gzip\")\n\n    df_train_scaled = pd.DataFrame(minmax.fit_transform(df_train))\n    df_train_scaled.to_csv(\"my_train_scaled.csv.gz\", compression=\"gzip\")\n    return df_train, df_train_scaled","86efcfc0":"ps = process_train()","ccfc7281":"#first 10 rows of raw feature data\nps[0].head(10)","6d601f9c":"#same first 10 rows in scaled format\nps[1].head(10)","51f127d1":"ps[1].values.shape","dd0d974e":"bkt_count*len(train_meta)","7df48ddd":"ps[1][0:160].plot(figsize=(8,5))","25b0759a":"ps[1].iloc[:,0:5][0:160].plot()","7b12d88b":"ps[1].iloc[:,5:8][0:160].plot()","b17b163a":"ps[1].iloc[:,8:15][0:160].plot()","7c91adad":"ps[1].iloc[:,15:22][0:160].plot()","568740ae":"measurement_ids = train_meta[\"id_measurement\"].unique()\nrows = []\nfor mid in measurement_ids:\n    idx1 = mid*3\n    idx2 = idx1 + 1\n    idx3 = idx2 + 1\n    sig1_idx = idx1 * bkt_count\n    sig2_idx = idx2 * bkt_count\n    sig3_idx = idx3 * bkt_count\n    sig1_data = ps[1][sig1_idx:sig1_idx+bkt_count]\n    sig2_data = ps[1][sig2_idx:sig2_idx+bkt_count]\n    sig3_data = ps[1][sig3_idx:sig3_idx+bkt_count]\n    #this combines the above read 3*160 rows for 3 signals into 1 combined set with with 160 rows\n    #and from 22 features on 3*160 to 66 (=22*3) features on 160 rows.\n    row = np.concatenate([sig1_data, sig2_data, sig3_data], axis=1).flatten().reshape(bkt_count, sig1_data.shape[1]*3)\n    rows.append(row)\ndf_train_combined = pd.DataFrame(np.vstack(rows))\ndf_train_combined.to_csv(\"my_train_combined_scaled.csv.gz\", compression=\"gzip\")\n","2e334fe9":"#slot 1 (measurement 1, signal 1, rows 0-159) for single signal version\nps[1].iloc[:,15:22][0:160].plot()","8b450350":"#slot 2 (measurement 1, signal 2, rows 160-319) for single signal version\nps[1].iloc[:,15:22][160:320].plot()","ca08556d":"#slot 1 (measurement 1, signal 1) for combined signal version\ndf_train_combined.iloc[:,15:22][0:160].plot()","5e6f9afd":"#slot 2 (measurement 1, signal 2) for combined signal version\ndf_train_combined.iloc[:,37:44][0:160].plot()","7a99e71e":"#signal 1, single signal version\nps[1].iloc[0:4]","adbefa90":"#signal 2, single signal version\nps[1].iloc[160:164]","b1c65559":"#signal 3, single signal version\nps[1].iloc[320:324]","9940dc20":"#signal 4 (or signal 1 for measurement id 2))\nps[1].iloc[480:484]","ba7aa72b":"df_train_combined.iloc[0:4]","5e680255":"df_train_combined.iloc[160:164]","382aa875":"del ps\ndel df_train_combined","1220a0a5":"def process_subtest(arg_tuple):\n    meta, idx = arg_tuple\n    df_sum = summarize_df_np(meta, \"test\", idx)\n    return idx, df_sum","61b36855":"from multiprocessing import Pool\n\nnum_cores = 4\n\ndef process_test():\n    measurement_ids = test_meta[\"id_measurement\"].unique()\n    df_split = np.array_split(measurement_ids, num_cores)\n    chunk_size = len(df_split[0]) * 3\n    \n    chunk1 = create_chunk_indices(test_meta, 0, chunk_size)\n    chunk2 = create_chunk_indices(test_meta, 1, chunk_size)\n    chunk3 = create_chunk_indices(test_meta, 2, chunk_size)\n    chunk4 = create_chunk_indices(test_meta, 3, chunk_size)\n\n    all_chunks = [chunk1, chunk2, chunk3, chunk4]\n    \n    pool = Pool(num_cores)\n    result = pool.map(process_subtest, all_chunks)\n    result = sorted(result, key=lambda tup: tup[0])\n\n    sums = [item[1] for item in result]\n\n    df_test = pd.concat(sums)\n    df_test = df_test.reset_index(drop=True)\n    df_test.to_csv(\"my_test.csv.gz\", compression=\"gzip\")\n\n    df_test_scaled = pd.DataFrame(minmax.transform(df_test))\n    df_test_scaled.to_csv(\"my_test_scaled.csv.gz\", compression=\"gzip\")\n    return df_test, df_test_scaled","c907cbc1":"pst = process_test()","306de428":"pst[0].head(10)","3aef27c9":"pst[1].head(10)","0d0e61ea":"pst[1].values.shape","4d56e76b":"pst[1][0:160].plot()","d3e48e5d":"pst[1].iloc[:,0:5][0:160].plot()","94ccf913":"pst[1].iloc[:,5:8][0:160].plot()","c5afd403":"pst[1].iloc[:,8:15][0:160].plot()","81248030":"pst[1].iloc[:,15:22][0:160].plot()","0f51ca49":"measurement_ids = test_meta[\"id_measurement\"].unique()\nstart = measurement_ids[0]\nrows = []\nfor mid in measurement_ids:\n    #test measurement id's start from 2904 and indices at 0, so need to align\n    mid = mid - start\n    idx1 = mid*3\n    idx2 = idx1 + 1\n    idx3 = idx2 + 1\n    sig1_idx = idx1 * bkt_count\n    sig2_idx = idx2 * bkt_count\n    sig3_idx = idx3 * bkt_count\n    sig1_data = pst[1][sig1_idx:sig1_idx+bkt_count]\n    sig2_data = pst[1][sig2_idx:sig2_idx+bkt_count]\n    sig3_data = pst[1][sig3_idx:sig3_idx+bkt_count]\n    row = np.concatenate([sig1_data, sig2_data, sig3_data], axis=1).flatten().reshape(bkt_count, sig1_data.shape[1]*3)\n    rows.append(row)\ndf_test_combined = pd.DataFrame(np.vstack(rows))\ndf_test_combined.to_csv(\"my_test_combined_scaled.csv.gz\", compression=\"gzip\")\n","3605a2ec":"pst[1].iloc[0:4]","a60e703b":"df_test_combined.head(4)","a59a331a":"df_test_combined.shape","89c398a0":"And the second bucket at 160-320 rows should have signal 4 (or signal 1 for measurement id 2) from above, in columns 0-21:","49fbbf14":"Now, the same multiprocessing elements for the test set as above for the training set:","a037d402":"Similarly, combine 3 signals into one row as features for the combined version for test data:","e0fafe3f":"Now the same 2 signals in the combined dataset:","3c3bbb51":"Actual code to call multiprocessing for the training data:","1499489f":"### Verification","5adbf9b5":"The above shows the shape of the generated data. In this case we have 22 features, so 22 columns. \n\n160 rows per signal (one \"bucket\" per row) so overall size matches:","3ea474f6":"Brief look at the combined test set signals to see it makes sense:","a6b008c9":"The scaler to produce the scaled version on of the data once all four parallel chunks have finished processing:","b326f3f8":"The above showed an example for one signal with all the 22 features total.\n\nAs another dataset, I combine for each measurement id, the 3 signals into one row as features.\n\nThis allows running models where all 3 signals for a measirement id are treated as unified features, such as in the kernel I linked at the beginning.\n\n## Training-dataset combining:\n\nThe combine 3 signals code for the training dataset:","1b86d478":"And some brief look at the data itself, to see it is valid:","5ad4304c":"## Training dataset processing","31b2f955":"## A look at the processed test data","808d6b64":"All-in-one figure for the mess:","0f030d4e":"All that is bit of a mess, so just the first bullet on its own -\n* 5 for general bucket\/bin statistics: bkt_mean, bkt_std, bkt_std_top, bkt_std_bottom, bkt_range","730a58e2":"* The 3 for diff\/lag in bucket\/bin:","89febbfa":"Check shape of combined signals to match number of unique measurements in test data:","7293564a":"Next the diff features:\n* 3 for diff\/lag in bucket\/bin: bkt_diff_mean, bkt_diff_std, bkt_diff_sum","7e2e2e5f":"## Feature Generation","ebd939a4":"## Multiprocessing","e2d8646f":"For verification, a look at the results to check the signal combination has worked:\n\nFirst the 2 first signals in the single signal dataset:","54b0fdef":"## Test-dataset Processing and Features","9c4a5824":"To use the kernel output as a dataset in another (e.g., GPU) kernel, create a kernel and select \"+ Add Data\" on the right side panel.","2aab29fd":"### A look at combined test-dataset","2cd167a2":"## Test-dataset combinations","0f309d4b":"All the above checks match, so I judge this as working as intended.\n\nSince this should now all be saved to disk, clean up some memory:","2297f6d0":"And the relative percentiles:\n* 7 relative percentiles: 0, 1, 25, 50, 75, 99, 100","30c29dab":"## Importing the Results to Other Kernels","82804e18":"vs.","346fa62b":"For comparison, the signals 1-3 for measurement id 1 in combined set:\n\nWith 22 features, signal 1 from above should be in columns 0-21, signal 2 in columns 22-43, and signal 3 in columns 44-65.","991b411e":"* The 5 general bucket\/bin statistics:","61685df0":"One more, a look at the data contents to check also the values:","485546ce":"Sum and average are overlapping, which is why only 2 lines show. So scaled average is the same as a scaled sum in this case. Should probably look at the othe features more closely as well, but that would be another story.","ed88fd9e":"# Preprocessing with Multiprocessing\n\nThis kernel uses the Python multiprocessing module to make use of all 4 cores you get in a Kaggle CPU kernel. The input data is split into four chunks, and each is processed using the given feature extraction function in parallel. The results are stored on disk in both scaled and non-scaled format. Output compression is used the avoid kernel output size limits.\n\nKaggle CPU kernels have 4 CPU cores, while GPU kernels have only 2 CPU cores. Running pre-processing in a separate kernel like this helps use both kernel types more optimally. CPU kernel with more cores to pre-process, GPU kernel to build and experiment with models using the CPU kernel output as data source.\n\nThe features in this version base on the ones in https:\/\/www.kaggle.com\/braquino\/5-fold-lstm-attention-fully-commented-0-694, and a few lag\/diff ones I played with. Should be simple to tune for any other features\/processing.\n\nAll the features are created in function \"summarize_df_np\". Change that to produce different features. \n\nThis kernel produces a set of output files as follows:\n\n- my_train.csv.gz: The raw data as processed (features, buckets, whatever you call them). Each signal separately in 160 rows per signal. \"raw\" as in not scaled.\n- my_train_scaled.csv.gz: The same data as my_train.csv.gz, scaled using min-max-scaler with -1 to 1 scale.\n- my_train_combined_scaled.csv.gz: The scaled data but all 3 signals per measurement on a single row.\n- my_test.csv.gz: Same as above but for test data.\n- my_test_scaled.csv.gz: Same as above but for test data.\n- my_test_combined_scaled.csv.gz: Same as above but for test data.\n","f778049c":"The function to process all the chunks and generate features in the parallel running processes:","26b8fbbb":"Next the percentiles:\n* 7 percentiles: 0, 1, 25, 50, 75, 99, 100","b423f2ac":"## Combined dataset from above single-signal data","db98539e":"Function process_subtrain() is passed to the Python multiprocessing for all four cores\/chunks. \n\nIt calls the feature processing function for the training data:","002c4317":"* The 7 percentiles:","eba26a7c":"* The 7 relative percentiles:","451d1d6b":"## Training dataset processing validation","1b03d1a2":"The feature processing function in this kernel generates these features:\n* 5 for general bucket\/bin statistics: bkt_mean, bkt_std, bkt_std_top, bkt_std_bottom, bkt_range\n* 3 for diff\/lag in bucket\/bin: bkt_diff_mean, bkt_diff_std, bkt_diff_sum\n* 7 percentiles: 0, 1, 25, 50, 75, 99, 100\n* 7 relative percentiles: 0, 1, 25, 50, 75, 99, 100\n-> total of 22 \"features\"\n\nTo show a bit how they all look together after scaled to range -1 to 1, a look at the first signal as processed into 160 buckets from 800k:","be08401f":"Function to create the chunks sizes\/indices to split the data into chunks. Used for both train and test data:"}}