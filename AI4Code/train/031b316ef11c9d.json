{"cell_type":{"4bd8472c":"code","f16df9dc":"code","f90d80b5":"code","7f046c4e":"code","b0337b59":"code","e2f5035a":"code","e376a417":"code","90da6dba":"code","eb052997":"code","3db1b50b":"code","3a768fe1":"code","e3e49158":"code","7d1afadd":"code","a74b1255":"code","85626b67":"code","aa398925":"code","f77a5740":"code","01bcf0c8":"code","a858c75d":"code","d8e2e872":"code","a3edf2c7":"code","c0a23bd7":"code","81fb770e":"code","2f60663c":"code","5c91d53b":"code","56023ee8":"code","25615d34":"code","e8b00e2f":"code","0b4538ae":"code","1a88281d":"code","1490722d":"code","b313414c":"code","f442e214":"code","ec338415":"code","38b36e0c":"code","cca1cbe8":"code","0357a2bf":"code","0883675e":"code","de835ed6":"code","a41fae11":"code","b61cdb76":"code","a9f073e2":"code","e0efe347":"code","670549bf":"code","98899ed4":"code","3636d27f":"code","3b731a79":"code","c1ca49bc":"code","f3cd533f":"code","cd5223a5":"code","602dd908":"code","ba7579a4":"code","05fc7454":"code","baf41cbf":"code","0171ee14":"code","0ab286c0":"code","b0c313c4":"code","5faf23fa":"code","5aa66cb6":"code","a81698f3":"code","f755169e":"code","4308cab2":"code","a3c61a32":"code","dbe008a7":"code","93c9530a":"code","d50b56b7":"code","3e50fdaf":"code","884bc83d":"code","8b2c7215":"code","467da855":"code","c2f4ec1c":"code","21049ac5":"code","1b5f183c":"code","b087e7da":"code","18de1f37":"code","58544b1d":"code","02115393":"code","697ccc1e":"code","02bc1eef":"code","e505e6c9":"code","726d6245":"code","7fe92e0b":"code","d0dfbdc4":"code","0d517d7d":"code","0e93fd9d":"code","478ecc9f":"code","0b7613f8":"code","ef518690":"code","82a2b35e":"code","d83c1dce":"code","dc0c7cbc":"code","a8604776":"code","c48ea1d9":"code","28858fd2":"code","d7a79282":"code","62a13fa0":"code","dac5b1cc":"code","5eaa1507":"code","55d96edd":"code","d0b76b9c":"code","1882fd12":"code","1ea5d3f0":"markdown","212ac28c":"markdown","ac1b50fe":"markdown","5cb80897":"markdown","031589fa":"markdown","a4fdd4a8":"markdown","d2a270b2":"markdown","12bb51fd":"markdown","2c24bb21":"markdown","8630c8f1":"markdown","19648097":"markdown","f0607d50":"markdown","a4df5158":"markdown","08c06bca":"markdown","f6c9c003":"markdown","afdaf795":"markdown","9c531bf7":"markdown","0f0659bb":"markdown","f09cb05c":"markdown","cefb505c":"markdown","8f851e03":"markdown","dbbc75a3":"markdown","e7256942":"markdown","36407747":"markdown","3d589578":"markdown","ae55bc08":"markdown","c9a77a9a":"markdown","9e1f21c0":"markdown","b2ba76db":"markdown","f6985f3b":"markdown","d1a689dd":"markdown","319ad851":"markdown","fd59adc7":"markdown","89c0ad36":"markdown","ca0028c4":"markdown","a4c771df":"markdown","f94a7b83":"markdown","252f11a9":"markdown","75a1f20b":"markdown","b681cebc":"markdown","7f019adf":"markdown","639aff2a":"markdown","6b754786":"markdown","fc9ac41f":"markdown","6ecc7fcd":"markdown","870ba6b1":"markdown","9d7b8095":"markdown","7fc735bc":"markdown","d5b35bf5":"markdown","fbd4c42f":"markdown","d4629063":"markdown","312faaad":"markdown","82595532":"markdown","08fef148":"markdown","c9f4fc9b":"markdown","8775f95e":"markdown","c911dfe8":"markdown","b6514ee2":"markdown"},"source":{"4bd8472c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f16df9dc":"train_data =  '..\/input\/titanic\/train.csv'\ntest_data = '..\/input\/titanic\/test.csv'\n","f90d80b5":"train_set = pd.read_csv(train_data)\ntest_set = pd.read_csv(test_data)","7f046c4e":"print(f'Train dataset has {train_set.shape[0]} rows and {train_set.shape[1]} columns.')\nprint(f'Test dataset has {test_set.shape[0]} rows and {test_set.shape[1]} columns.')","b0337b59":"train_set.head()","e2f5035a":"train_set.describe()","e376a417":"train_set.info()","90da6dba":"train_set.columns","eb052997":"#Checking all the  null values present in every column in the  Dataset\ntrain_set.isnull().sum(axis=0)","3db1b50b":"train_set.describe()","3a768fe1":"#data visualaisation\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')","e3e49158":"#Heatmap of the missing values present in the all feature of the training data.\nsns.heatmap(train_set.isnull(), cmap='plasma')","7d1afadd":"#Correlation heatmap between the predictor variables. \nsns.heatmap(train_set.corr(),annot=True, cmap='BrBG')","a74b1255":"sns.countplot(data=train_set, x='Survived', palette='Set1')\nprint(train_set.Survived.value_counts())","85626b67":"#Passenger survival based on the Sex and Age feature \nsns.countplot(data=train_set, x='Survived', hue='Sex', palette='Set2')","aa398925":"#Number of values in the embarked feature\nprint(train_set.Embarked.value_counts())\nsns.countplot(data=train_set,x='Embarked',palette='Set1')","f77a5740":"sns.pairplot(data=train_set)","01bcf0c8":"train_set.columns","a858c75d":"train_set.isnull().sum()","d8e2e872":"#missing values in the Cabin\ntrain_set['Cabin'].isnull().sum()","a3edf2c7":"train_set['Cabin'].value_counts().head()","c0a23bd7":"#inorder to apply same analysis to our test and train data we need to merge them togather\nmerged = pd.concat([train_set,test_set], sort = False)\nmerged.head(3)","81fb770e":"merged['Cabin'].value_counts().head(3)","2f60663c":"#filling the na values in the Cabin column\nmerged['Cabin'].fillna('X', inplace=True)","5c91d53b":"merged.head(3)","56023ee8":"# Keeping 1st charater from the Cabin\nmerged['Cabin'] = merged['Cabin'].apply(lambda x: x[0])\nmerged['Cabin'].value_counts()\nsns.countplot(data = merged, x='Cabin')","25615d34":"merged['Name'].head(5)","e8b00e2f":"#Extracting the Title from the names, it will give us the sex of the passengers\nmerged['Title'] = merged['Name'].str.extract('([A-Za-z]+)\\.')\nmerged['Title'].head()","0b4538ae":"#There can be also differnt titles present in the dataset apart for the common titles such as Mr, Mrs etc\nmerged['Title'].value_counts()","1a88281d":"# Replacing  Dr, Rev, Col, Major, Capt with 'Officer'\nmerged['Title'].replace(to_replace = ['Dr', 'Rev', 'Col', 'Major', 'Capt'], value = 'Officer', inplace=True)\n\n# Replacing Dona, Jonkheer, Countess, Sir, Lady with 'Aristocrate'\nmerged['Title'].replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True)\n\n#  Replace Mlle and Ms with Miss. And Mme with Mrs.\nmerged['Title'].replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'}, inplace = True)","1490722d":"merged['Title'].value_counts()\nsns.countplot(data=merged, x='Title')","b313414c":"merged['SibSp'].value_counts()\n","f442e214":"merged['Parch'].value_counts()","ec338415":"# Merging Sibsp and Parch and creating new variable called 'Family_size'\nmerged['Family_size'] = merged.SibSp + merged.Parch + 1  # Adding 1 for single person\nmerged['Family_size'].value_counts()","38b36e0c":"# Create buckets of single, small, medium, and large and then put respective values into them.\nmerged['Family_size'].replace(to_replace = [1], value = 'single', inplace = True)\nmerged['Family_size'].replace(to_replace = [2,3], value = 'small', inplace = True)\nmerged['Family_size'].replace(to_replace = [4,5], value = 'medium', inplace = True)\nmerged['Family_size'].replace(to_replace = [6, 7, 8, 11], value = 'large', inplace = True)","cca1cbe8":"merged['Family_size'].value_counts()","0357a2bf":"sns.countplot(data=merged,x='Family_size')","0883675e":"# let's preview the Ticket variable.\nmerged['Ticket'].head(10)","de835ed6":"# Assign N if there is only number and no character. If there is a character, extract the character only.\nticket = []\nfor x in list(merged['Ticket']):\n    if x.isdigit():\n        ticket.append('N')\n    else:\n         ticket.append(x.replace('.','').replace('\/','').strip().split(' ')[0])\n# Swap values\nmerged['Ticket'] = ticket","a41fae11":"merged['Ticket'].value_counts()","b61cdb76":"# Keeping 1st charater from the Ticket\nmerged['Ticket'] = merged['Ticket'].apply(lambda x: x[0])\nmerged['Ticket'].value_counts()","a9f073e2":"sns.countplot(data=merged, x='Ticket')","e0efe347":"#We define a function which counts number of outliers present in the variable\ndef outliers(variable):\n    global filtered # Global keyword is used inside a function only when we want to do assignments or when we want to change a variable.\n    \n    # Calculate 1st, 3rd quartiles and iqr.\n    q1, q3 = variable.quantile(0.25), variable.quantile(0.75)\n    iqr = q3 - q1\n    \n    # Calculate lower fence and upper fence for outliers\n    l_fence, u_fence = q1 - 1.5*iqr , q3 + 1.5*iqr   # Any values less than l_fence and greater than u_fence are outliers.\n    \n    # Observations that are outliers\n    outliers = variable[(variable<l_fence) | (variable>u_fence)]\n    print('Total Outliers of', variable.name,':', outliers.count())\n    \n    # Drop obsevations that are outliers\n    filtered = variable.drop(outliers.index, axis = 0)","670549bf":"#Total number of outliers present in the fare\noutliers(merged['Fare'])","98899ed4":"#Visualization of the outliers in the Fare distribution\nplt.figure(figsize=(13,2))\nsns.boxplot(x=merged['Fare'], palette='magma')\nplt.title('Fare distribution with Outliers', fontsize=15)","3636d27f":"#Visualization of Fare districution without Outliers\nplt.figure(figsize=(13,2))\nsns.boxplot(x=filtered, palette='Blues')\nplt.title('Fare distribution with Outliers', fontsize=15)","3b731a79":"#Number of outliers in the Age distribution \nplt.figure(figsize=(13,2))\nsns.boxplot(x=merged['Age'], palette='Blues')\nplt.title('Age distribution with outliers', fontsize=15)","c1ca49bc":"#Number of outliers in the Age distribution\nplt.figure(figsize=(13,2))\nsns.boxplot(x=filtered, palette='Blues')\nplt.title('Age distribution without outliers', fontsize=15)","f3cd533f":"#Listing all the missing values in the dataset\nmerged.isnull().sum()","cd5223a5":"merged.sample(5)\n","602dd908":"merged['Embarked'].value_counts()","ba7579a4":"#Here S is the most frequent so we will be putting S in the empty places\nmerged['Embarked'].fillna(value = 'S', inplace=True)","05fc7454":"#Fare is the discriptive value so it will be filled with the median values \nmerged['Fare'].fillna(value= merged['Fare'].median(), inplace=True)","baf41cbf":"#We will put the median group value in the missing values of the NA\n#We need to draw heatmap of the all the correlated value, we need to convert categorical data into numerical data\ndf = merged.loc[:, ['Sex', 'Pclass', 'Embarked', 'Title', 'Family_size', 'Parch', 'SibSp', 'Cabin', 'Ticket']]\nfrom sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\ndf = df.apply(LE.fit_transform)\ndf.head(5)\n","0171ee14":"#Moving Age variable to the Labeled dataframe\ndf['Age'] = merged['Age']\ndf.head(2)","0ab286c0":"#Moving Age variable to the index 0\ndf = df.set_index('Age').reset_index()\ndf.head(2)","b0c313c4":"#Drawing heatmap of the attribute which is correlated to the Age variable.\nplt.figure(figsize=(10,6))\nsns.heatmap(df.corr(), cmap='BrBG', annot=True)\nplt.title('Correlation map of Age', fontsize=15)\nplt.show()","5faf23fa":"#correlation between PCLass and Age\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=merged)","5aa66cb6":"#correlated values of Age with Ticket\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"Title\", y=\"Age\",data= merged)","a81698f3":"#Imputing the missing valuse of Age variable with median values of Pclass and Title\n## Impute Age with median of respective columns (i.e., Title and Pclass)\nmerged['Age'] = merged.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))","f755169e":"merged.sample(5)","4308cab2":"#Age Distribution\nplt.figure(figsize=(10,6))\nsns.distplot(merged['Age'], color='Red')","a3c61a32":"#binning the Age variable\nlabel_names = ['infant', 'child', 'teenager','young_adult', 'adult', 'aged']\n\n#range for each bin categrories of age\ncut_points = [0,5,12,18,35,60,81]\n\n#view categorized Age with original Age.\nmerged['Age_binned'] = pd.cut(merged['Age'], cut_points, labels = label_names)\n\n#Age with Categorized Age.\nmerged[['Age', 'Age_binned']].head(3)","dbe008a7":"#visualisation of the Fare variable\nplt.figure(figsize=(10,6))\nsns.distplot(merged['Fare'], color='Blue')","93c9530a":"#binning the Fare variable \ngroups = ['low','medium','high','very high']\n\n# Create range for each bin categories of Fare\ncut_points = [-1, 130, 260, 390, 520]\n\n#Create and view categorized Fare with original Fare\nmerged['Fare_binned'] = pd.cut(merged.Fare, cut_points, labels = groups)\n\n# Fare with Categorized Fare\nmerged[['Fare', 'Fare_binned']].head(2)","d50b56b7":"merged.sample(5)","3e50fdaf":"merged.dtypes","884bc83d":"# Correcting data types, converting into categorical variables.\nmerged.loc[:, ['Pclass', 'Sex', 'Embarked', 'Cabin', 'Title', 'Family_size', 'Ticket']] = merged.loc[:, ['Pclass', 'Sex', 'Embarked', 'Cabin', 'Title', 'Family_size', 'Ticket']].astype('category')\n\n# Due to merging there are NaN values in Survived for test set observations.\nmerged['Survived'] = merged['Survived'].dropna().astype('int') #Converting without dropping NaN throws an error","8b2c7215":"#verify converted data types\nmerged.dtypes","467da855":"merged.head(3)","c2f4ec1c":"#Dropping the features which will not be helpfull for us.\n# droping the feature that would not be useful anymore\nmerged.drop(columns = ['Name', 'Age','SibSp', 'Parch','Fare'], inplace = True, axis = 1)\nmerged.columns","21049ac5":"merged.head(2)","1b5f183c":"# convert categotical data into dummies variables\nmerged = pd.get_dummies(merged, drop_first=True)\nmerged.head(2)","b087e7da":"#Splitting our test and train data from the merged dataframe.\n#Let's split the train and test set to feed machine learning algorithm.\n#train will be the first 891 values and test will be the \ntrain = merged.iloc[:891, :]\ntest  = merged.iloc[891:, :]","18de1f37":"train.head(3)","58544b1d":"test.head(3)","02115393":"#Drop passengerid from train set and Survived from test set.'''\ntrain = train.drop(columns = ['PassengerId'], axis = 1)\ntest = test.drop(columns = ['Survived'], axis = 1)","697ccc1e":"# setting the data as input and output for machine learning models\nX_train = train.drop(columns = ['Survived'], axis = 1) \ny_train = train['Survived']\n\n# Extract test set\nX_test  = test.drop(\"PassengerId\", axis = 1).copy()","02bc1eef":"X_train.head(3)\n","e505e6c9":"y_train.head(3)","726d6245":"X_test.head(3)","7fe92e0b":"# See the dimensions of input and output data set.'''\nprint('Input Matrix Dimension:  ', X_train.shape)\nprint('Output Vector Dimension: ', y_train.shape)\nprint('Test Data Dimension:     ', X_test.shape)","d0dfbdc4":"#Initialising the instances of all 5 classifiers\n\n#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\n\n#K-nearest neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()\n\n#Desicion Tree\nfrom sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier()\n\n#Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\n\n#Support vector machine\nfrom sklearn.svm import SVC\nSVM = SVC(gamma='auto')\n\n#XG Boost classifier\nfrom xgboost import XGBClassifier\nXGB = XGBClassifier(n_jobs=-1, random_state=42)","0d517d7d":"#Creating a function which will give us the training score of the differnt types of classifier\ndef train_accuracy(model):\n    model.fit(X_train, y_train)\n    train_accuracy = model.score(X_train, y_train)\n    train_accuracy = np.round(train_accuracy*100,2)\n    return train_accuracy","0e93fd9d":"#creating a summary table of train_accuracy\ntrain_accuracy = pd.DataFrame({'Training accuracy %':[train_accuracy(LR),train_accuracy(KNN),train_accuracy(DT),train_accuracy(RF),train_accuracy(SVM), train_accuracy(XGB)]})\ntrain_accuracy.index = ['Logistic Regression','KNN','Decision Tree','Random Forest','SVM','XGB']\nsorted_train_accuracy = train_accuracy.sort_values(by='Training accuracy %', ascending = False)\n\nsorted_train_accuracy","478ecc9f":"# Create a function that returns mean cross validation score for different models.\ndef val_score(model):\n    from sklearn.model_selection import cross_val_score\n    val_score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n    val_score = np.round(val_score*100, 2)\n    return val_score\n\n# making the summary table of cross validation accuracy.\nval_score = pd.DataFrame({'val_score(%)':[val_score(LR), val_score(KNN), val_score(DT), val_score(RF), val_score(SVM), val_score(XGB)]})\nval_score.index = ['Logistic Regression', 'KNN','Decision Tree', 'Random Forest', 'SVC','XGB']\nsorted_val_score = val_score.sort_values(by = 'val_score(%)', ascending = False)\n\n#cross validation accuracy of the Classifiers\nsorted_val_score\n","0b7613f8":"#Hypertuning the parameters for grid search CV\n# define all the model hyperparameters one by one first\n\n# 1. For logistic regression\nlr_params = {'penalty':['l1', 'l2'],\n             'C': np.logspace(0, 2, 4, 8 ,10)}\n\n# 2. For KNN\nknn_params = {'n_neighbors':[4,5,6,7,8,9,10],\n              'weights':['uniform', 'distance'],\n              'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n              'p':[1,2]}\n\n# 3. For DT\ndt_params = {'max_features': ['auto', 'sqrt', 'log2'],\n             'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n             'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n             'random_state':[46]}\n# 4. For RF\nrf_params = {'criterion':['gini','entropy'],\n             'n_estimators':[ 10, 30, 200, 400],\n             'min_samples_leaf':[1, 2, 3],\n             'min_samples_split':[3, 4, 6, 7], \n             'max_features':['sqrt', 'auto', 'log2'],\n             'random_state':[46]}\n# 5. For SVC\nsvc_params = {'C': [0.1, 1, 10,100], \n              'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n              'gamma': [ 1, 0.1, 0.001, 0.0001]}\n\n#6. For XGB\nxgb_params = xgb_params_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n                         \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n                         \"min_child_weight\" : [ 1, 3, 5, 7 ],\n                         \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n                         \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }","ef518690":"#function to find the tuned hyperprametrs of the model\ndef tune_hyperparameters(model, param_grid):\n    from sklearn.model_selection import GridSearchCV\n    global best_params, best_scores\n    \n    #grid search object with 10 cross folds\n    grid = GridSearchCV(model,param_grid, verbose=0, cv=10, scoring='accuracy', n_jobs=-1)\n    \n    #fitting with the grid object\n    grid.fit(X_train, y_train)\n    best_params, best_scores = grid.best_params_, np.round(grid.best_score_*100,2)\n    return best_params, best_scores\n    ","82a2b35e":"#Apply tune hyperparameters in the created function\n#Tuning hyperparametes for Logistic Regression\ntune_hyperparameters(LR, param_grid=lr_params)\nlr_best_params, lr_best_score = best_params,best_scores \nprint('Logistic Regression Best Score:', lr_best_score)\nprint('And Best Parameters:', lr_best_params)","d83c1dce":"#Tuning hyperparametes for KNN\ntune_hyperparameters(KNN, param_grid=knn_params)\nknn_best_params, knn_best_score = best_params,best_scores \nprint('KNN Best Score:', knn_best_score)\nprint('And Best Parameters:', knn_best_params)","dc0c7cbc":"#Tuning hyperparametes for DT\ntune_hyperparameters(DT, param_grid=dt_params)\ndt_best_params, dt_best_score = best_params,best_scores \nprint('DT Best Score:', dt_best_score)\nprint('And Best Parameters:', dt_best_params)","a8604776":"#Tuning hyperparametes for RF\ntune_hyperparameters(RF, param_grid=rf_params)\nrf_best_params, rf_best_score = best_params,best_scores \nprint('RF Best Score:', rf_best_score)\nprint('And Best Parameters:', rf_best_params)","c48ea1d9":"#Tuning hyperparametes for XGB\ntune_hyperparameters(XGB, param_grid=xgb_params)\nxgb_best_params, xgb_best_score = best_params,best_scores \nprint('SVC Best Score:', xgb_best_score)\nprint('And Best Parameters:', xgb_best_params)","28858fd2":"#Tuning hyperparametes for SVC\ntune_hyperparameters(SVM, param_grid=svc_params)\nsvc_best_params, svc_best_score = best_params,best_scores \nprint('SVC Best Score:', svc_best_score)\nprint('And Best Parameters:', svc_best_params)","d7a79282":"#create a summary table of best scores after byperparameter tuning\ntuned_score = pd.DataFrame({'tuned_score(%)':[lr_best_score, knn_best_score, dt_best_score, rf_best_score, svc_best_score,xgb_best_score]})\ntuned_score.index = ['Logistic Regression', 'KNN','Decision Tree', 'Random Forest', 'SVC','XGB']\nsorted_tuned_score = tuned_score.sort_values(by = 'tuned_score(%)', ascending = False)\n\n#cross validation accuracy of the Classifiers\nsorted_tuned_score","62a13fa0":"# Instantiate the models with optimized hyperparameters.\nlr  = LogisticRegression(**lr_best_params)\nknn = KNeighborsClassifier(**knn_best_params)\ndt  = DecisionTreeClassifier(**dt_best_params)\nrf  = RandomForestClassifier(**rf_best_params)\nsvc = SVC(**svc_best_params)\nxgb = XGBClassifier(**xgb_best_params)","dac5b1cc":"#train all the model with optimized hyperparameters\nmodels = {'LR':lr,'KNN':knn,'DT':dt,'RF':rf,'SVC':svc, 'XGB':xgb}\n\n#10 folds cross validation after optimized hyperparametrs\nscore = []\nfor x,(keys, items) in enumerate(models.items()):\n    # Train the models with optimized parameters using cross validation.\n    # No need to fit the data. cross_val_score does that for us.\n    # But we need to fit train data for prediction in the follow session.\n    from sklearn.model_selection import cross_val_score\n    items.fit(X_train,y_train)\n    scores = cross_val_score(items, X_train, y_train, cv = 10, scoring='accuracy')*100\n    score.append(scores.mean())\n    print('Mean Accuracy: %0.4f (+\/- %0.4f) [%s]'  % (scores.mean(), scores.std(), keys))\n    ","5eaa1507":"# Make prediction using all the trained models\nmodel_prediction = pd.DataFrame({'LR':lr.predict(X_test), 'KNN':knn.predict(X_test), 'DT':dt.predict(X_test),'RF':rf.predict(X_test), 'SVC':svc.predict(X_test), 'XGB':xgb.predict(X_test)})\n\n#All the Models Prediction \nmodel_prediction.head()","55d96edd":"#Submission with Most accurate random forest classifier\nsubmisson = pd.DataFrame({\"PassengerID\":test[\"PassengerId\"],\"Survived\":rf.predict(X_test)})\nsubmisson.to_csv('submisson_rf.csv',index=False)","d0b76b9c":"#Submission with Most accurate SVC classifier\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':svc.predict(X_test)})\nsubmission.to_csv(\"submission_svc.csv\", index = False)","1882fd12":"#submission with the most accurate XGBClassifier\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':xgb.predict(X_test)})\nsubmission.to_csv(\"submission_xgb.csv\", index = False)","1ea5d3f0":"**Model builing and Evaluation**","212ac28c":"**K-Fold Cross Validation** <br\/> \n<br\/>\nLet's say we will use 10-fold cross validation. So k = 10 and we have total 891 observations. Each fold would have 891\/10 = 89.1 observations. So basically k-fold cross validation uses fold-1 (89.1 samples) as the testing set and k-1 (9 folds) as the training sets and calculates test accuracy.This procedure is repeated k times (if k = 10, then 10 times); each time, a different group of observations is treated as a validation or test set. This process results in k estimates of the test accuracy which are then averaged out.","ac1b50fe":"**Merging both Train and Test data** <br\/>\n<br\/>\nOne important reason to combine sets is to maintain consistency between the sets. For example for categorical encoding, If all categories aren't present in both sets (and even if they are) they might be labelled differently if done in two separate operations. Or in unsupervised processing, like word2vec, you want to use an overall corpus of text, rather than two separate, smaller corpuses.\n\n<br\/> We also merge the datas so that we do not have to apply same analysis to both test and train data","5cb80897":"**Feature discription in the training data** <br\/>\n* PassengerId - Id number of the passengers\n* Survived - If the passenger survived. <br\/>\n   * **1 = Survived**\n   * **0 = Not Survived**\n   \n* Pclass - (Passenger Class) feature is the socio-economic status of the passenger. It is a categorical ordinal feature which has 3 unique values (1, 2 or 3); <br\/>\n   * **1 = Survived**\n   * **0 = Not Survived**\n* Name - Name of the passenger\n* Sex - Sex of the passenger\n* Age - Age of the passenger\n* SibSp - Total number of passengers's siblings \n* Parch - Total number of passengers's parent or child\n* Ticket - Ticket number of the passenger\n* Fare - Fare price of the ticket\n* Cabin - Cabin details\n* Embarked - Embarked is port of embarkation. It is a categorical feature and it has 3 unique values (C, Q or S); <br\/>\n\n    * **C = Cherbourg**\n    * **Q = Queenstown**\n    * **S = Southampt **","031589fa":"**Taking care of missing values in the name feature** ","a4fdd4a8":"There is 177 missing values present in Age feature and 687 values present in Cabin feature.","d2a270b2":"* We can observe that there is weak correlation between the predictors as it is less than +0.5\n* The highly correlated features are Parch and SibSp but they weakly correlated because it is less than +0.5","12bb51fd":"* Ticket variable contains alphanumeric, only numbers and character type variables.\n* We will create two groups-one will contain just number and other will only contain character extracted from string.\n* And assign 'N' to the number type variable.\n* If a row contains both character and number, we will keep only character.","2c24bb21":"* In the heatmap the black flags repersent the missing values in the 'Age', 'Cabin' and 'Embarked' features. We can observe that the missing value is very high in Cabin compared to other features.","8630c8f1":"* For the better information of the passenger's family we would join SibSp and Parch in one feature","19648097":"**Correlation between the predictors** <br\/>\n\n* Features are highly correlated with each other and dependent to each other.\n","f0607d50":"* We would only keep the first character of the cabin.","a4df5158":"In the training data we have total of 2 float, 5 integer and 5 object data types. Also notice that out of 891 rows, Age and Cabin have null values. We'll focus on missing values later.","08c06bca":"**In the above tabel, LR and XGB  have the highest cross validation accuracy among the remaining models.**","f6c9c003":"**Binning the Data** <br\/>\n<br\/>\nData binning (also called Discrete binning or bucketing) is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall in a given small interval, a bin, are replaced by a value representative of that interval, often the central value.","afdaf795":"**Dropping Features** <br\/>\n<br\/>\nNow we have both transformed and the original variables transformation have been made from. So we should safely drop the variables that we think would not be useful anymore for our survival analysis since they are very unlikely to be analyzed in their raw forms.","9c531bf7":"* We can see that Embarked is a categorical value, therefore it will be imputed first","0f0659bb":"**Pairplots** <br\/>\n<br\/>* A pairs plot allows us to see both distribution of single variables and relationships between two variables. ","f09cb05c":"** Missing values in the Cabin feature**","cefb505c":"**What is Exploratory Data Analysis?**<br\/>\n<br\/>Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.<br\/>\n<br\/>It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand, before getting them dirty with it. <br\/>\n<br\/>To starts with, I imported necessary libraries (for this example pandas, numpy, matplotlib and seaborn) and loaded the data set.<br\/>\nNote : Whatever inferences I could extract, I\u2019ve mentioned with bullet points.\n","8f851e03":"* This column contains string that furth contains titles such as Mr, Mrs, Master etc.\n* These title give us useful information about sex and age for example Mr=Male, Mrs=Female and married, miss= Female and young.\n* Now we want to extract these titles from Name to check if there is any association between these titles and Survived.","dbbc75a3":"**Processing Sibling and Parent** <br\/>\n<br\/>Our goal is to seperate the family into type of family such as 'Small', 'Large' etc","e7256942":"**Submission**","36407747":"**Imputing the Fare variable**<br\/>\n* We will use median of the variable to fill the missing values","3d589578":"**Retrain and Predict Using Optimized Hyperparameters**","ae55bc08":"**Ticket feature**\n<br\/>\n* Ticket variable contains alphanumeric, only numbers and character type variables.\n* We will create two groups-one will contain just number and other will only contain character extracted from string.\n* And assign 'N' to the number type variable.\n* If a row contains both character and number, we will keep only character.","c9a77a9a":"**General information about the dataset** <br\/>\n<br\/>In this kernal I work on the Titanic passenger prediction competition. <br\/>\n<br\/> RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in 1912 after the ship struck an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making it one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.<br\/>\n<br\/> We have a binary classification problem for surivor prediction. At first I'll explore the data and try to find valuable insights, then I'll do some feature engineering and then it wil be time to build models. <br\/>\n\n<br\/> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fd\/RMS_Titanic_3.jpg)","9e1f21c0":"* We can observe that the no of female survivors are more than the male survivors.","b2ba76db":"**Survival distribution** <br\/>\n* 38.38% (342\/891) of the training set is Class 1.\n* 61.62% (549\/891) of the training set is Class 0.\n* The no of survived pessengers are very less than non surviving passengers.","f6985f3b":"**Important observations noticed from the data** <br\/>\n*  Only 38% of the passengers surived.\n*  The average age of the passengers in the ship was 29.69.\n*  The average fare was 49.69.\n*  The maximum age was 80 years and minimum was 0.42.\n*  The maximum fare was 512.32 and minimum fare was 0.","d1a689dd":"* when we copares the cross validation scores with tunned scores we can see that all the classifier are improved. Among the classifiers, RF, XGB and SVC have the highest accuracy after tunning hyperparameters.","319ad851":"**Outlier detection in the Age**","fd59adc7":"* Create a bucket to store the nominal value into categorical based on the size of the family","89c0ad36":"**Dealing with the missing values** <br\/>\n<br\/>\nMany real-world datasets may contain missing values for various reasons. They are often encoded as NaNs, blanks or any other placeholders. Training a model with a dataset that has a lot of missing values can drastically impact the machine learning model\u2019s quality. Some algorithms such as scikit-learn estimators assume that all values are numerical and have and hold meaningful value.","ca0028c4":"**Model Training** <br\/>\nWe will be using 5 different classifiers models for this binary classification technique","a4c771df":"**If you like my kernal, please consider upvoating** <br\/>\n<br\/>\n    Thanks","f94a7b83":"**Cross-validation: Evaluating estimator performance**\n\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting.\n\nOne thing we can do is to split the train set in two groups, usually in 80:20 ratio. That means we would train our model on 80% of the training data and we reserve the rest 20% for evaluating the model since we know the ground truth for this 20% data. This is the first model evaluation technique. In sklearn we have a train_test_split method for that.\n\nBut Train_test split has its drawbacks. Because this approach introduces bias as we are not using all of our observations for testing and also we're reducing the train data size. To overcome this we can use a technique called cross validation where all the data is used for training and testing periodically.\n\nHowever, as the train set gets larger, train_test_split has its advantage over k-fold cross validation. Train_test_split is k-times faster than k-fold cross validation. If the training set is very large, both train_test_split and k-fold cross validation perform identically. So for a large training data, train_test_split is prefered over k-fold cross validation to accelerate the training process.","252f11a9":"**Imputing the Embarked feature**","75a1f20b":"**In the above table, DT, RF, XGB models have highest train accuracy. But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not. we can't use training accuracy for our model evaluation rather we must know how our model will perform on the data our model is yet to see. **","b681cebc":"**Data Visualization** <br\/>\n<br\/>\nData visualization is an important aspect of EDA. It allows us to find hidden pattern and the valuable key insight which might not be intrepreted by the data only.","7f019adf":"**No of survived person** ","639aff2a":"**Data loading and overview** <br\/>\n<br\/>We have given two dataset, one is training data which we will be using for training our Machine learning model and other one is test data which will be used for testing our dataset.","6b754786":"* We can observe that the Cabin numbers are in alphanumeric format.\n* NaN's doesn't have any cabins\n* We will flag NaN's as the X and we will only keep the first character of the cabin number\n","fc9ac41f":"**Impuitng the Age variable** <br\/>\nTo impute Age with grouped median, we need to know which features are heavily correlated with Age. Let's find out the variables correlated with Age.","6ecc7fcd":"**Tuning the hyperparametrs** <br\/>\n<br\/>\nA hyperparameter is a parameter whose value is set before the learning process begins.\n<br\/>\n\n\nSome examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.\nIn sklearn, hyperparameters are passed in as arguments to the constructor of the model classes.","870ba6b1":"**Seperating the title from the name feature** <br\/>\n<br\/> The name of the passenger will not be very helpful for us in the modelling so we bin them in differnet category\n","9d7b8095":"* Here Embarked is the cateogorical value, so we can repalce it with the mode of the variable (i.e. The most frequent value)","7fc735bc":"So we have two relatively small-sized datasets with a less no of columns. This is the main advantage of this dataset. It could help us understand data points on beginners level.","d5b35bf5":"**Encoding Categorical Variables** <br\/>\n<br\/>We would like to use one hot encoding instead of label encoding. a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.","fbd4c42f":"* Using one hot encoding for the labels","d4629063":"* We can also use boxplot ot detect the number of outliers present in the Variable","312faaad":"3.5 **Outliers Detection\u00b6** <br\/>\n\nIn statistics, an outlier is a data point that significantly differs from the other data points in a sample. Often, outliers in a data set can alert statisticians to experimental abnormalities or errors in the measurements taken, which may cause them to omit the outliers from the data set. If they do omit outliers from their data set, significant changes in the conclusions drawn from the study may result. <br\/>\n\n\nSee the data Description table above for min, 1st quartile, 2nd quartile(median), 3rd quartile, and max values of a variable.\n\nWe will use IQR method to detect the outliers for variable Age and Fare though we won't remove them.","82595532":"* We can see that there is very few passengers by differnt titles\n* We can seperate them in other Title.","08fef148":"**Distribution curve of the data to know the spread of the data** <br\/>\n<br\/>The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.","c9f4fc9b":"* Emabarked, Sex and Ticket are having weak correlation with Age.\n* PClass and Title have highest correlation between Age variable, so will fill the missing values of Age with Median Values of the PClass and Title.","8775f95e":"* The most number of passengers were on the cabin were headed to Southmpt.","c911dfe8":"**Imputing Missing Variables** <br\/>\n<br\/>There is various techniques to imputing missing variable in the datasets for exmaple linear regession, K-NN, Mean\/mode\/median, Deep learning etc. The simpliest way to impute missing values of a variable is to impute its missing values with its mean, median or mode depending on its distribution and variable type(categorical or numerical).\n\nHowever, one clear disadvantage of using mean, median or mode to impute missing values is the addition of bias if the amount of missing values is significant (like Age). So simply replacing them with the mean or the median age might not be the best solution since the age may differ by groups and categories of passengers.\n\nTo solve this, we can group our data by some variables that have no missing values and for each subset compute the median age to impute the missing values.","b6514ee2":"**Data Tranformation** <br\/>\n<br\/>\nIn this section, we will transform our continuous variables. After that, redundant and useless features will be deleted. And finally categorical variables will be encoded into numerical to feed our machine learning models."}}