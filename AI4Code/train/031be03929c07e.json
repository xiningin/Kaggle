{"cell_type":{"cf3eb7f8":"code","b1a2f860":"code","9a51d663":"code","79ffec45":"code","182a2ca1":"code","14e7156f":"code","a3a430c8":"code","ce57d483":"code","06427ad4":"code","e927694b":"code","bf979a65":"code","7b857546":"code","8c069b83":"code","5d038b63":"code","de3f8515":"code","0c7d655f":"code","d3e760b9":"code","cc1c3a7f":"code","ee4b941b":"code","f3840d60":"code","ff8e74b2":"code","b8d82f4d":"code","084a2ba1":"code","15717410":"code","ff94ceab":"code","615ffa1c":"code","d7b60c8e":"code","b7d0f4e7":"code","b6e23109":"code","86f2ac25":"code","521812bd":"code","4e530b7a":"code","e383d7b2":"code","6471fe41":"code","074dede9":"code","0de09049":"code","cb03a48c":"code","b58d81fc":"code","e2120cd5":"code","a502df4d":"markdown","85de38e3":"markdown","ba5c7eea":"markdown","583c6d35":"markdown","4253c3ca":"markdown","a53f3ccc":"markdown","6b7246ca":"markdown","dd58adf4":"markdown","61f8a63e":"markdown","33feca4a":"markdown","cc0b1dae":"markdown","c8e5d1a6":"markdown","af1c23fa":"markdown","43c0aade":"markdown","6cfb7d1d":"markdown","7c5bd924":"markdown","1edac30a":"markdown","03419114":"markdown","49ff6311":"markdown","497c0de3":"markdown","0c8a790d":"markdown","17e34d8a":"markdown"},"source":{"cf3eb7f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1a2f860":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","9a51d663":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndf.shape","79ffec45":"df.head()","182a2ca1":"df.info()","14e7156f":"df['waterfront'] = df['waterfront'].astype('object')\ndf['view'] = df['view'].astype('object')\ndf['condition'] = df['condition'].astype('object')\ndf['grade'] = df['grade'].astype('object')\ndf['zipcode'] = df['zipcode'].astype('object')","a3a430c8":"# date column should be dropped \ndf = df.drop('date',axis=1)","ce57d483":"df_cat = df.select_dtypes('object')\ndf_num = df.select_dtypes(np.number)","06427ad4":"# Checking for null values\ndf.isnull().sum()","e927694b":"df[df=='?'].count()","bf979a65":"pd.options.display.float_format = '{:.2f}'.format","7b857546":"df.describe()","8c069b83":"df['price'].nunique()","5d038b63":"plt.rcParams['figure.figsize'] = 18,5\nfig,ax= plt.subplots(1,3)\nsns.distplot(df['price'],ax=ax[0])\nsns.boxplot(df['price'],ax=ax[1])\nsns.violinplot(df['price'],ax=ax[2])\nplt.show()","de3f8515":"# Numerical columns\nfor i in df_num:\n    fig,ax= plt.subplots(1,3)\n    print(i)\n    sns.distplot(df_num[i],ax=ax[0],color='Green')\n    sns.boxplot(df_num[i],ax=ax[1],palette='Greens')\n    sns.violinplot(df_num[i],ax=ax[2],palette='Greens')\n    plt.show()","0c7d655f":"# Categorical columns\nplt.rcParams['figure.figsize'] = 9,4\nfor i in df_cat:\n    fig,ax= plt.subplots(1,2)\n    print(i)\n    df_cat[i].value_counts().plot(kind='bar',rot=0,ax=ax[0],cmap='Spectral')\n    df_cat[i].value_counts().plot(kind='pie',autopct='%.1f%%',ax=ax[1],cmap='Spectral')\n    plt.show()","d3e760b9":"# Numerical with Target variable\ni=1\nfor col in df_num:\n    print(col,'Vs price')\n    sns.scatterplot(df_num['price'],df_num[col])\n    plt.show()","cc1c3a7f":"# categorical with target variable\nplt.rcParams['figure.figsize']= 10,4\nfor col in df_cat:\n    fig,ax= plt.subplots(1,2)\n    print(col,'Vs price')\n    sns.boxplot(df_cat[col],df['price'],ax=ax[0],palette='coolwarm')\n    sns.violinplot(df_cat[col],df['price'],ax=ax[1],palette='coolwarm')\n    plt.show()","ee4b941b":"import scipy.stats as st\nst.shapiro(df['price'])","f3840d60":"plt.rcParams['figure.figsize'] = 15,8\nsns.heatmap(df.corr(method='spearman'),annot=True,cmap='cubehelix')\nplt.show()","ff8e74b2":"df = df.drop(['id','sqft_lot','sqft_lot15','yr_renovated','long'],axis=1)","b8d82f4d":"for col in df_cat:\n    print(col,'Vs price')\n    print(st.kruskal(df['price'],df_cat[col]))\n    print('\\n')","084a2ba1":"X = df.drop('price',axis=1)\ny = df['price']","15717410":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nfor i in X.select_dtypes(np.number):\n    X[i] = pt.fit_transform(X[[i]])","ff94ceab":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in X.select_dtypes('object'):\n    X[i] = le.fit_transform(X[[i]])","615ffa1c":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(X,y,train_size=0.7,random_state=42)","d7b60c8e":"xtrain.shape , xtest.shape , ytrain.shape , ytest.shape","b7d0f4e7":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfor i in xtrain.columns:\n    xtrain[i] = sc.fit_transform(xtrain[[i]])\nfor i in xtest.columns:\n    xtest[i] = sc.fit_transform(xtest[[i]])","b6e23109":"from sklearn.metrics import adjusted_rand_score, r2_score, mean_squared_error, mean_absolute_error","86f2ac25":"import statsmodels.api as sm\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nmodel.summary()","521812bd":"xtrain = xtrain.drop(['sqft_above','sqft_basement'],axis=1)\nxtest = xtest.drop(['sqft_above','sqft_basement'],axis=1)","4e530b7a":"# Linear regression\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr = lr.fit(xtrain,ytrain)\nypred = lr.predict(xtest)\nprint('Training r2_Score',lr.score(xtrain,ytrain))\nprint('Testing r2_Score ',lr.score(xtest,ytest))","e383d7b2":"# Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_depth=10,min_samples_split=15,max_leaf_nodes=50, random_state=42)\ndt = dt.fit(xtrain,ytrain)\nypred_dt = lr.predict(xtest)\nprint('Training r2_score', dt.score(xtrain,ytrain))\nprint('Testing r2_score', dt.score(xtest,ytest))","6471fe41":"# Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=150,max_depth=7)\nrf = rf.fit(xtrain,ytrain)\nypred_rf = rf.predict(xtest)\nprint('Training r2_score',rf.score(xtrain,ytrain))\nprint('Testing r2_score',rf.score(xtest,ytest))","074dede9":"# AdaBoost Regressor\nfrom sklearn.ensemble import AdaBoostRegressor\nadb = AdaBoostRegressor(n_estimators=10,random_state=42)\nadb = adb.fit(xtrain,ytrain)\nypred_adb = adb.predict(xtest)\nprint('Training r2_score', adb.score(xtrain,ytrain))\nprint('Testing r2_score',adb.score(xtest,ytest))","0de09049":"# Gradient Boost Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=42)\ngb = gb.fit(xtrain,ytrain)\nypred_gb = gb.predict(xtest)\nprint('Training r2_score',gb.score(xtrain,ytrain))\nprint('Testing r2_score',gb.score(xtest,ytest))","cb03a48c":"# XGBoost Regressor\nimport xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=8,random_state=42)\nxgb = xgb.fit(xtrain,ytrain)\nypred_xgb = xgb.predict(xtest)\nprint('Training r2_score',xgb.score(xtrain,ytrain))\nprint('Testing r2_score',xgb.score(xtest,ytest))","b58d81fc":"important_features = pd.DataFrame({'Features': xtrain.columns, \n                                   'Importance': gb.feature_importances_})\n\n# print the dataframe\nimportant_features.sort_values(by='Importance', ascending=False, inplace=True)\nimportant_features","e2120cd5":"plt.rcParams['figure.figsize'] = 8,5\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\nplt.show()","a502df4d":"#### There are no null values in the form of '?' too.","85de38e3":"##### So, all the categorical features seem to have an effect on Price column.","ba5c7eea":"#### Inferences from describe:\n\n* The target variable varies between 75000 and 77 lakh. The mean is greater than median, so it is right skewed. We can also observe that there are few luxury houses which have price more than 7 lakh, as we can see by comparing the 75th percentile and the maximum\n* The number of bedrooms vary between 0 and 33. The mean and median are almost same. We can see that, the maximum and 75th percentile are very much different, so most of the houses have 3-4 bedrooms and luxury flats alone have in two digits.\n* The number of bathrooms vary between 0 and 8. The mean and median are slightly  different, so the distribution should be skewed.\n* sqft_living varies between 290 and 13540, so there are small and luxury houses in our dataset And, here too the mean and median are very much different, so the data is skewed.\n* sqft_lot varies between 520 to 1 crore, it definitely confirms luxury houses and mansions in our dataset, the mean and median are different, so it is also skewed.\n* Number of floors varies between 1 to 3.5 and the mean and median are almost equal\n* sqft_above varies between 290 to 9410, and the mean and median are very much different So, the distribution is skewed.\n* sqft_basement varies between 0 and 4820. So, very few houses have underground levels and few of them are small and few are big.\n* yr_built varies between 1900 to 2015, so there are very old to new houses. But, most of them seem to be little  old. The mean and median are different here too.\n* yr_renovated varies between 0 to 2015. So, many of the houses are not renovated and only a few of them have been renovated in recent times.\n* sqft_living15 varies between 399 to 6210. The mean and median are highly different, so the distribution is skewed\n* sqft_lot15 varies between 651 to 871200, which is clearly right skewed.","583c6d35":"#### Gradient Boost Regressor is giving the highest rsquare value,so it is the best model.","4253c3ca":"#### Conclusion (Things to be considered mainly for knowing the house price):\n* The sqft_living is influencing the price feature mostly, so the price of a house is highly determined by the sqft of the living room\n* It is followed by grade, as we saw from bivariate analysis, it is having a direct and strong relationship with price\n* Lat is also influencing price, so it depends upon the area to determine the price\n* yr_built is having good effect on price, even though it is not having a linear relationship with price, it is influencing price too\n* zipcode, same as latitude is having some influence\n* waterfront and view are also having effect on price, which is observed from bivariate too\n* The number of bathrooms is having little effect on price as we saw from bivariate, but number of bedrooms is not having that much effect.","a53f3ccc":"#### Few of the features are not identified with the right datatype, so changing them","6b7246ca":"Our target variable is highly skewed.","dd58adf4":"### Hypothesis testing","61f8a63e":"### Model building","33feca4a":"### Univariate Analysis","cc0b1dae":"#### Inferences from the univariate analysis of Numerical columns:\n* Bedrooms are mostly between 0 to 10, and only one of them has above 30, which is a outlier\n* Number of bathrooms is mainly between 0 to 4, and it varies till 8\n* sqft_living is right skewed too, with few observations having higher value of sqft_living\n* sqft_lot is highly right skewed, with most of the data in the outliers\n* Number of floors is mostly between 1 to 2, with a maximum of 3.4\n* sqft_above has most of the values between 0 to 4000 and the values lead till 10000\n* sqft_basement is right skewed too, with most of the houses with less than 1500 and few houses with more than that\n* yr_built as saw from the describe function, is varying from 1900 to 2020\n* yr_renovated has only two values, which is 0 and 2000, which means that most of the houses are not renovated and few were renovated in 2000\n* lat is varying between 47.1 to 47.8, so we can say that these houses are in a particular region\n* long is varying between -122.6 to -121.2, this to confirms that these are houses from closer regions\n* sqft_living15 is varying between 0 to 6000 and mainly in the range till 4000, it is also right skewed\n* sqft_lot15 is highly right skewed.\n","c8e5d1a6":"#### So, there are no null values in our dataset","af1c23fa":"#### For categorical features with a numerical feature which is skewed, we should use kruskal test","43c0aade":"#### Inferences from the bivariate analysis of the numerical features:\n* The number of bedrooms doesn't seem to have a linear relationship with the price feature\n* The number of bathrooms is having almost linear relationship with the price, with increase in number of bathrooms, the price is increasing too\n* The sqft_living feature is having almost perfect linear relationship with price\n* The sqft_lot feature doesn't seem to have a relation with price\n* The number of floors too is not contributing much to the price feature\n* sqft_above is having a linear relationship with price\n* sqft_basement is not having much effect on price feature\n* yr_built has very little relation with the price feature\n* yr_renovated with only two values in it, does not have an effect on price feature\n* lat seems to have very little effect on price\n* long doesn't seem to have much effect on price\n* sqft_living15 is having slightly lineary relationship with price\n* sqft_lot15 is not having any relation with price","6cfb7d1d":"### Target Variable","7c5bd924":"#### The target variable is skewed, so spearman correlation should be used between numerical features.","1edac30a":"### Bivariate Analysis","03419114":"#### Normality test for the target variable","49ff6311":"#### Inferences from the categorical features:\n* In waterfront column, most of the values is 0 and only 0.8% of people's apartment was overlooking the waterfront, so it is a good factor\n* In view column too, most of the values are 0 and 2 is second highest, so the houses are not having that good of a view\n* In condition column, most of the values are 3, followed by 4, so almost all the houses are in good condition, only around 10% of houses are not in good condition\n* In grade column, most of the values, around 40% have given grade of 7 followed by 8, and 9. So, these are good houses\n* zipcode has around 70 unique values and most of them contribute almost equally.","497c0de3":"### Transformation and Encoding","0c8a790d":"#### Inferences from the bivariate analysis of categorical features:\n* For both the values of waterfront the price follows same distribution, but the highest price is where the waterfront value is 0\n* For views having values 2 or more is having very good price compared to 0 and 1\n* Condition also is having direct relationship with price, houses with condition of 3 or 4 are having very good price and with values 1 and 2 are having very less price\n* Grade is having exact direct relationship with price, when the grade is less price is less and when grade is high, price is also very high, for houses with grade 11 and above are having very high price\n* zipcode is not distributed uniformly, but few zipcodes are having very high prices, so based on region price is also varying","17e34d8a":"* The features, id, sqft_lot, sqft_lot15,long have very less correlation with the target variable.\n* Similarly yr_renovated also does not have any relation with price as observed from the bivariate analysis too, since it has only 2 values. These 5 variables should be dropped."}}