{"cell_type":{"d7d1ea2c":"code","1bc7706b":"code","624980bd":"code","2e198041":"code","d3ac0ce0":"code","99caa05a":"code","b40e4e73":"code","71f24872":"code","c9fe804c":"code","3a56fb21":"code","7046da02":"code","4b30b95a":"code","19d5f9e3":"code","e91215fa":"code","ff34b08d":"code","9eda32ad":"code","0c2fac04":"code","a0355e42":"code","b870136a":"code","2f6173cd":"code","7997cf8d":"code","30cf5196":"code","d7f0d60a":"code","5e12a00d":"code","6b9873ea":"code","abf0c64e":"code","5dd94ac3":"code","c04db7d8":"code","f82c61a7":"code","c001d1cb":"code","cad0a992":"code","7dd2ae6b":"code","af9060a4":"code","9137efca":"code","6dec9a00":"code","b05f517f":"code","33f6c86f":"code","249ce578":"code","888d0f0b":"code","deed5c9d":"code","b26eb012":"code","2f1dea4f":"code","11c6da49":"code","a1f0c8b5":"code","d1383054":"code","4da446e9":"code","4a3d5bd6":"code","51809017":"code","84a9c4c4":"code","81a4d389":"code","0d81428a":"code","638327e7":"code","ac93b7bb":"code","1f40d087":"code","b657dd2e":"code","df66e756":"code","b1f328b0":"code","ad5014a2":"code","b1e5f093":"code","adbb00f4":"code","e500c512":"code","277e8d58":"code","8cfd1ca7":"code","17cae04e":"code","751320ae":"code","ec347ee1":"code","a4e284bd":"code","50176951":"code","59af6332":"code","cdb864e3":"code","9d55f36d":"code","54f03d0f":"code","526d7590":"code","721374d5":"code","e1de7f7b":"code","03cf7ef7":"code","bd2ff6a5":"code","ed21aa42":"code","85cd6272":"code","9bd46298":"code","99c54ac3":"code","57231adf":"markdown","b310a670":"markdown","7cd04e78":"markdown","d8d5d3ea":"markdown","dc8db46f":"markdown","1becaad5":"markdown","130c3869":"markdown","e2986c74":"markdown","7672dc6b":"markdown","7bae0f27":"markdown","2b9b298b":"markdown","4e12f3e4":"markdown","20c368f2":"markdown","f9a6220e":"markdown","5788e995":"markdown","13ce9502":"markdown","59f8aec2":"markdown","1fe5dcb2":"markdown","dec4ccd9":"markdown","2434a656":"markdown","3a375638":"markdown","5a1f892f":"markdown","06d87505":"markdown","e0b18af9":"markdown","6753e8fd":"markdown","a99f1798":"markdown","32df4bd4":"markdown","21a1b14b":"markdown","2042385f":"markdown","3e172441":"markdown","8ecc797d":"markdown","e08d9dbd":"markdown","c127de54":"markdown","0797225b":"markdown","74f6c296":"markdown","6c9d1e8d":"markdown","2f5b9b2c":"markdown","026ca1b9":"markdown","6ce3824f":"markdown","3e9f320d":"markdown","5e383df3":"markdown","dc1a9bba":"markdown","18093fd0":"markdown","f19b4926":"markdown","e5be9628":"markdown","005984b2":"markdown","9226cf8d":"markdown","3d734cbb":"markdown","81bac365":"markdown","c72b676d":"markdown","9d132df8":"markdown","bcc5cb8a":"markdown"},"source":{"d7d1ea2c":"#importing the libraries that we use\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nsns.set(color_codes=True) # adds a nice background to the graphs\n%matplotlib inline","1bc7706b":"pks_df = pd.read_csv(\"..\/input\/parkinson-classifications\/Data - Parkinsons\")","624980bd":"pks_df.head(10).style.background_gradient(cmap=\"RdYlBu\")","2e198041":"pks_df.shape","d3ac0ce0":"pks_df.dtypes","99caa05a":"pks_df.info()","b40e4e73":"pks_df.describe().T","71f24872":"pks_df.skew()","c9fe804c":"pks_df[pks_df.duplicated()]","3a56fb21":"Target = pks_df[\"status\"]","7046da02":"#Plots to see the distribution of the features individually\ndef distributionPlot(pks_df):\n    plt.figure(figsize= (20,15))\n    plt.subplot(5,5,1)\n    sns.distplot(pks_df[\"MDVP:Fo(Hz)\"],hist=False,kde=True, color='lightblue')\n    plt.xlabel('MDVP:Fo(Hz)')\n\n    plt.subplot(5,5,2)\n    sns.distplot(pks_df[\"MDVP:Fhi(Hz)\"],hist=False,kde=True, color='lightgreen')\n    plt.xlabel('MDVP:Fhi(Hz)')\n\n    plt.subplot(5,5,3)\n    sns.distplot(pks_df[\"MDVP:Flo(Hz)\"],hist=False,kde=True, color='pink')\n    plt.xlabel('MDVP:Flo(Hz)')\n\n    plt.subplot(5,5,4)\n    sns.distplot(pks_df[\"MDVP:Jitter(%)\"],hist=False,kde=True, color='gray')\n    plt.xlabel('MDVP:Jitter(%)')\n\n    plt.subplot(5,5,5)\n    sns.distplot(pks_df[\"MDVP:Jitter(Abs)\"],hist=False,kde=True, color='cyan')\n    plt.xlabel('MDVP:Jitter(Abs)')\n\n    plt.subplot(5,5,6)\n    sns.distplot(pks_df[\"MDVP:RAP\"],hist=False,kde=True, color='Aquamarine')\n    plt.xlabel('MDVP:RAP')\n\n    plt.subplot(5,5,7)\n    sns.distplot(pks_df[\"MDVP:PPQ\"],hist=False,kde=True, color='lightblue')\n    plt.xlabel('MDVP:PPQ')\n\n    plt.subplot(5,5,8)\n    sns.distplot(pks_df[\"Jitter:DDP\"],hist=False,kde=True, color='lightgreen')\n    plt.xlabel('Jitter:DDP')\n\n    plt.subplot(5,5,9)\n    sns.distplot(pks_df[\"MDVP:Shimmer\"],hist=False,kde=True, color='pink')\n    plt.xlabel('MDVP:Shimmer')\n\n    plt.subplot(5,5,10)\n    sns.distplot(pks_df[\"MDVP:Shimmer(dB)\"],hist=False,kde=True, color='gray')\n    plt.xlabel('MDVP:Shimmer(dB)')\n\n    plt.subplot(5,5,11)\n    sns.distplot(pks_df[\"Shimmer:APQ3\"],hist=False,kde=True, color='cyan')\n    plt.xlabel('Shimmer:APQ3')\n\n    plt.subplot(5,5,12)\n    sns.distplot(pks_df[\"Shimmer:APQ5\"],hist=False,kde=True, color='Aquamarine')\n    plt.xlabel('Shimmer:APQ5')\n\n    plt.subplot(5,5,13)\n    sns.distplot(pks_df[\"MDVP:APQ\"],hist=False,kde=True, color='lightblue')\n    plt.xlabel('MDVP:APQ')\n\n    plt.subplot(5,5,14)\n    sns.distplot(pks_df[\"Shimmer:DDA\"],hist=False,kde=True, color='lightgreen')\n    plt.xlabel('Shimmer:DDA')\n\n    plt.subplot(5,5,15)\n    sns.distplot(pks_df[\"NHR\"],hist=False,kde=True, color='pink')\n    plt.xlabel('NHR')\n\n    plt.subplot(5,5,16)\n    sns.distplot(pks_df[\"HNR\"],hist=False,kde=True, color='gray')\n    plt.xlabel('HNR')\n\n    plt.subplot(5,5,17)\n    sns.distplot(pks_df[\"RPDE\"],hist=False,kde=True, color='cyan')\n    plt.xlabel('RPDE')\n\n    plt.subplot(5,5,18)\n    sns.distplot(pks_df[\"DFA\"],hist=False,kde=True, color='Aquamarine')\n    plt.xlabel('DFA')\n\n    plt.subplot(5,5,19)\n    sns.distplot(pks_df[\"spread1\"],hist=False,kde=True, color='lightblue')\n    plt.xlabel('spread1')\n\n    plt.subplot(5,5,20)\n    sns.distplot(pks_df[\"spread2\"],hist=False,kde=True, color='lightgreen')\n    plt.xlabel('spread2')\n\n    plt.subplot(5,5,21)\n    sns.distplot(pks_df[\"D2\"],hist=False,kde=True, color='pink')\n    plt.xlabel('D2')\n\n    plt.subplot(5,5,22)\n    sns.distplot(pks_df[\"PPE\"],hist=False,kde=True, color='gray')\n    plt.xlabel('PPE')\n\n\n    plt.subplot(5,5,23)\n    sns.countplot(pks_df[\"status\"], color='Green')\n    plt.xlabel('status')\n\n\n    plt.show()\ndistributionPlot(pks_df)","4b30b95a":"#Plots to see the distribution of the features individually\ndef outlierPlot(pks_df):\n    plt.figure(figsize= (20,15))\n    plt.subplot(5,5,1)\n    sns.boxplot(pks_df[\"MDVP:Fo(Hz)\"],orient=\"v\", color='lightblue')\n    plt.xlabel('MDVP:Fo(Hz)')\n\n    plt.subplot(5,5,2)\n    sns.boxplot(pks_df[\"MDVP:Fhi(Hz)\"],orient=\"v\", color='lightgreen')\n    plt.xlabel('MDVP:Fhi(Hz)')\n\n    plt.subplot(5,5,3)\n    sns.boxplot(pks_df[\"MDVP:Flo(Hz)\"], orient=\"v\",color='pink')\n    plt.xlabel('MDVP:Flo(Hz)')\n\n    plt.subplot(5,5,4)\n    sns.boxplot(pks_df[\"MDVP:Jitter(%)\"],orient=\"v\", color='gray')\n    plt.xlabel('MDVP:Jitter(%)')\n\n    plt.subplot(5,5,5)\n    sns.boxplot(pks_df[\"MDVP:Jitter(Abs)\"],orient=\"v\", color='cyan')\n    plt.xlabel('MDVP:Jitter(Abs)')\n\n    plt.subplot(5,5,6)\n    sns.boxplot(pks_df[\"MDVP:RAP\"],orient=\"v\", color='Aquamarine')\n    plt.xlabel('MDVP:RAP')\n\n    plt.subplot(5,5,7)\n    sns.boxplot(pks_df[\"MDVP:PPQ\"],orient=\"v\", color='lightblue')\n    plt.xlabel('MDVP:PPQ')\n\n    plt.subplot(5,5,8)\n    sns.boxplot(pks_df[\"Jitter:DDP\"],orient=\"v\", color='lightgreen')\n    plt.xlabel('Jitter:DDP')\n\n    plt.subplot(5,5,9)\n    sns.boxplot(pks_df[\"MDVP:Shimmer\"],orient=\"v\", color='pink')\n    plt.xlabel('MDVP:Shimmer')\n\n    plt.subplot(5,5,10)\n    sns.boxplot(pks_df[\"MDVP:Shimmer(dB)\"],orient=\"v\", color='gray')\n    plt.xlabel('MDVP:Shimmer(dB)')\n\n    plt.subplot(5,5,11)\n    sns.boxplot(pks_df[\"Shimmer:APQ3\"],orient=\"v\", color='cyan')\n    plt.xlabel('Shimmer:APQ3')\n\n    plt.subplot(5,5,12)\n    sns.boxplot(pks_df[\"Shimmer:APQ5\"],orient=\"v\", color='Aquamarine')\n    plt.xlabel('Shimmer:APQ5')\n\n    plt.subplot(5,5,13)\n    sns.boxplot(pks_df[\"MDVP:APQ\"],orient=\"v\", color='lightblue')\n    plt.xlabel('MDVP:APQ')\n\n    plt.subplot(5,5,14)\n    sns.boxplot(pks_df[\"Shimmer:DDA\"],orient=\"v\", color='lightgreen')\n    plt.xlabel('Shimmer:DDA')\n\n    plt.subplot(5,5,15)\n    sns.boxplot(pks_df[\"NHR\"],orient=\"v\", color='pink')\n    plt.xlabel('NHR')\n\n    plt.subplot(5,5,16)\n    sns.boxplot(pks_df[\"HNR\"],orient=\"v\", color='gray')\n    plt.xlabel('HNR')\n\n    plt.subplot(5,5,17)\n    sns.boxplot(pks_df[\"RPDE\"],orient=\"v\", color='cyan')\n    plt.xlabel('RPDE')\n\n    plt.subplot(5,5,18)\n    sns.boxplot(pks_df[\"DFA\"],orient=\"v\", color='Aquamarine')\n    plt.xlabel('DFA')\n\n    plt.subplot(5,5,19)\n    sns.boxplot(pks_df[\"spread1\"],orient=\"v\", color='lightblue')\n    plt.xlabel('spread1')\n\n    plt.subplot(5,5,20)\n    sns.boxplot(pks_df[\"spread2\"],orient=\"v\", color='lightgreen')\n    plt.xlabel('spread2')\n\n    plt.subplot(5,5,21)\n    sns.boxplot(pks_df[\"D2\"],orient=\"v\", color='pink')\n    plt.xlabel('D2')\n\n    plt.subplot(5,5,22)\n    sns.boxplot(pks_df[\"PPE\"],orient=\"v\", color='gray')\n    plt.xlabel('PPE')\n\n\n\n    plt.show()\n\noutlierPlot(pks_df)","19d5f9e3":"#Checking the pair plot between each feature\nsns.pairplot(pks_df)  #pairplot\nplt.show()","e91215fa":"#person coefficient \npks_df.corr()","ff34b08d":"#Correlation\ncorr = pks_df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr,mask=mask,square=True,linewidths=2.5,cmap=\"viridis\",annot=True)","9eda32ad":"sns.countplot(Target)\nplt.show()","0c2fac04":"print(\"number of parkinson people in the dataset \",len(pks_df.loc[pks_df[\"status\"]==1]))\nprint(\"number of Healthy people in the dataset \",len(pks_df.loc[pks_df[\"status\"]==0]))","a0355e42":"pks_df.info()","b870136a":"updated_cols = list(pks_df.columns)\nupdated_cols.remove('name')\nupdated_cols.remove('status')\nfor column in updated_cols:\n    print(column,\" : \", len(pks_df.loc[pks_df[column]<0]))","2f6173cd":"pks_df[\"spread1\"]","7997cf8d":"def outliearTreat(df):\n    '''\n    This function is to treat outliers in the dataframe\n    input : dataframe that need to be treated\n    output: dataframe that treated with outlier capping treatment\n    '''\n    cols = list(df.columns)\n    cols.remove('name')\n    cols.remove('status')\n    for columnName in cols:\n        Q1 = df[columnName].quantile(0.25)\n        Q3 = df[columnName].quantile(0.75)\n        IQR = Q3 - Q1\n        whisker = Q1 + 1.5 * IQR\n        LowerBound = Q1- 1.5 * IQR\n        df[columnName] = df[columnName].apply(lambda x : whisker if x>whisker else x)\n        df[columnName] = df[columnName].apply(lambda x : LowerBound if x<LowerBound else x)\n    return df","30cf5196":"outliearTreat(pks_df)","d7f0d60a":"pks_df.skew()","5e12a00d":"outlierPlot(pks_df)\ndistributionPlot(pks_df)","6b9873ea":"#Standardise the numerical columns\ndef standardScalar(df):\n    '''\n    This function is to treat outliers in the dataframe\n    input : dataframe that need to be treated\n    output: dataframe that treated with outlier capping treatment\n    '''\n    cols = list(df.columns)\n    cols.remove('name')\n    cols.remove('status')\n    # the scaler object (model)\n    scalar = StandardScaler()\n    for columnName in cols:\n        # fit and transform the data\n        df[columnName] = scalar.fit_transform(df[[columnName]])\n    return df","abf0c64e":"standardScalar(pks_df)","5dd94ac3":"X = pks_df.drop(['status','name'],axis=1)     # Predictor feature columns \nY = Target   # Predicted class (1, 0) \n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n# 1 is just any random seed number\n\nx_train.head()","c04db7d8":"print(\"{0:0.2f}% data is in training set\".format((len(x_train)\/len(pks_df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(x_test)\/len(Target.index)) * 100))","f82c61a7":"#Dropping above mentioned columns\nx_test = x_test[X.columns.difference(['MDVP:Jitter(Abs)','MDVP:RAP','Jitter:DDP','spread1','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ'])]","c001d1cb":"model_stats = {'model_name':[],'train_accuracy':[],'test_accuracy':[],'f1_score':[],'roc_score':[]}","cad0a992":"def logistReg(x_train,y_train,solver=\"liblinear\"):\n    # Fit the model on train\n    model = LogisticRegression(solver=solver)\n    model.fit(x_train, y_train)\n    #predict on test\n    y_predict = model.predict(x_test)\n    y_predictprob = model.predict_proba(x_test)\n\n    coef_df = pd.DataFrame(model.coef_,columns=list(x_train.columns))\n    model_stats['model_name'].append(\"LogisticRegression\")\n    coef_df['intercept'] = model.intercept_\n    model_score = model.score(x_train, y_train)\n    print(f\"Accuracy of Training Data: {model_score}\")\n    model_stats['train_accuracy'].append(model_score)\n    model_score = model.score(x_test, y_test)\n    print(f\"Accuracy of Test Data: {model_score}\")\n    model_stats['test_accuracy'].append(model_score)\n    print(coef_df)\n    print(metrics.classification_report(y_test,y_predict))\n    cm=metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])\n\n    df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                      columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\n    plt.figure(figsize = (8,5))\n    sns.heatmap(df_cm, annot=True)\n    plt.show()\n    print(\"f1 score\", metrics.f1_score(y_test,y_predict))\n    print(\"Auc Roc Score: \",metrics.roc_auc_score(y_test,y_predict))\n    model_stats['f1_score'].append(metrics.f1_score(y_test,y_predict))\n    model_stats['roc_score'].append(metrics.roc_auc_score(y_test,y_predict))\n    return y_predictprob,y_predict","7dd2ae6b":"#OverSampling the minority to get the better results\nxtrain_resampled, ytrain_resampled = SMOTE(sampling_strategy=1,random_state=46).fit_resample(x_train[X.columns.difference(['MDVP:Jitter(Abs)','MDVP:RAP','Jitter:DDP','spread1','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ'])],y_train)\n","af9060a4":"y_predProb,y_predict = logistReg(xtrain_resampled,ytrain_resampled)","9137efca":"fprLR, tprLR, threshLR = metrics.roc_curve(y_test, y_predProb[:,1], pos_label=1)","6dec9a00":"scores =[]\nfor k in range(1,30):\n    NNH = KNeighborsClassifier(n_neighbors = k, weights = 'distance', metric='euclidean' )\n    NNH.fit(xtrain_resampled, ytrain_resampled)\n    scores.append(NNH.score(x_test, y_test))","b05f517f":"plt.plot(range(1,30),scores)\nplt.show()","33f6c86f":"NNH = KNeighborsClassifier(n_neighbors= 3 , weights = 'distance', metric='euclidean' )\nNNH.fit(xtrain_resampled, ytrain_resampled)\ny_predKnn = NNH.predict(x_test)","249ce578":"print(metrics.classification_report(y_test,y_predKnn))\ncm=metrics.confusion_matrix(y_test, y_predKnn, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nNNH_TrainAcc = NNH.score(xtrain_resampled,ytrain_resampled)\nNNH_TestAcc = NNH.score(x_test,y_test)\nNNH_Roc = metrics.roc_auc_score(y_test,y_predKnn)\nNNH_F1 = metrics.f1_score(y_test,y_predKnn)\nprint(f\"Score of Knn Test Data : {NNH_TestAcc}\")\nprint(f'Score of Knn Train Data : {NNH_TrainAcc}')\nprint(f\"Roc AUC score of KNN : {NNH_Roc}\")\nprint(f\"f1 score of KNN : {NNH_F1}\\n\")\nmodel_stats['model_name'].append(\"KNN\")\nmodel_stats['train_accuracy'].append(NNH_TrainAcc)\nmodel_stats['test_accuracy'].append(NNH_TestAcc)\nmodel_stats['f1_score'].append(NNH_F1)\nmodel_stats['roc_score'].append(NNH_Roc)","888d0f0b":"pred_prob_NNH = NNH.predict_proba(x_test)\nfprNNH, tprNNH, threshNNH = metrics.roc_curve(y_test, pred_prob_NNH[:,1], pos_label=1)","deed5c9d":"NBmodel = GaussianNB()\nNBmodel.fit(xtrain_resampled,ytrain_resampled)\ny_NBPred = NBmodel.predict(x_test)","b26eb012":"print(metrics.classification_report(y_test,y_NBPred))\ncm=metrics.confusion_matrix(y_test, y_NBPred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nNB_TrainAcc = NBmodel.score(xtrain_resampled,ytrain_resampled)\nNB_TestAcc = NBmodel.score(x_test,y_test)\nNB_Roc = metrics.roc_auc_score(y_test,y_NBPred)\nNB_F1 = metrics.f1_score(y_test,y_NBPred)\nprint(f\"Score of NB Test Data : {NB_TestAcc}\")\nprint(f'Score of NB Train Data : {NB_TrainAcc}')\nprint(f\"Roc AUC score of NB : {NB_Roc}\")\nprint(f\"f1 score of NB : {NB_F1}\\n\")\nmodel_stats['model_name'].append(\"Naive Bayes\")\nmodel_stats['train_accuracy'].append(NB_TrainAcc)\nmodel_stats['test_accuracy'].append(NB_TestAcc)\nmodel_stats['f1_score'].append(NB_F1)\nmodel_stats['roc_score'].append(NB_Roc)","2f1dea4f":"pred_prob_NB = NBmodel.predict_proba(x_test)\nfprNB, tprNB, threshNB = metrics.roc_curve(y_test, pred_prob_NB[:,1], pos_label=1)","11c6da49":"svc_model = SVC(kernel='poly',probability=True)\nsvc_model.fit(xtrain_resampled,ytrain_resampled)","a1f0c8b5":"y_svmPred = svc_model.predict(x_test)","d1383054":"print(metrics.classification_report(y_test,y_svmPred))\ncm=metrics.confusion_matrix(y_test, y_svmPred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nSVM_TrainAcc = svc_model.score(xtrain_resampled,ytrain_resampled)\nSVM_TestAcc = svc_model.score(x_test,y_test)\nSVM_F1 = metrics.f1_score(y_test,y_svmPred)\nSVM_Roc = metrics.roc_auc_score(y_test,y_svmPred)\nprint(f\"Score of svm Test Data : {SVM_TestAcc}\")\nprint(f'Score of svm Train Data : {SVM_TrainAcc}')\nprint(f\"Roc AUC score of svm : {SVM_Roc}\")\nprint(f\"f1 score of svm : {SVM_F1}\\n\")\nmodel_stats['model_name'].append(\"SVM\")\nmodel_stats['train_accuracy'].append(SVM_TrainAcc)\nmodel_stats['test_accuracy'].append(SVM_TestAcc)\nmodel_stats['f1_score'].append(SVM_F1)\nmodel_stats['roc_score'].append(SVM_Roc)","4da446e9":"pd.DataFrame(model_stats)","4a3d5bd6":"pred_prob_svm = svc_model.predict_proba(x_test)\nfprsvm, tprsvm, threshsvm = metrics.roc_curve(y_test, pred_prob_svm[:,1], pos_label=1)","51809017":"cols = list(pks_df.columns)\ncols.remove('name')\ncols.remove('status')","84a9c4c4":"clf1 = KNeighborsClassifier(n_neighbors=3)\nclf2 = SVC(kernel='poly',probability=True)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3],\n                          use_probas=True,\n                          average_probas=False,\n                          meta_classifier=lr)","81a4d389":"stck_model = sclf.fit(xtrain_resampled,ytrain_resampled)","0d81428a":"y_stckPred = sclf.predict(x_test)","638327e7":"print(metrics.classification_report(y_test,y_stckPred))\ncm=metrics.confusion_matrix(y_test, y_stckPred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nStack_TrainAcc = stck_model.score(xtrain_resampled,ytrain_resampled)\nStack_TestAcc = stck_model.score(x_test,y_test)\nStack_Roc = metrics.roc_auc_score(y_test,y_stckPred)\nStack_F1 = metrics.f1_score(y_test,y_stckPred)\nprint(f\"Score of stacking classifier Test Data : {Stack_TestAcc}\")\nprint(f'Score of stacking classifier Train Data : {Stack_TrainAcc}')\nprint(f\"Roc AUC score of stacking classifier : {Stack_Roc}\")\nprint(f\"f1 score of stacking classifier : {Stack_F1}\\n\")\nmodel_stats['model_name'].append(\"StackingClassifier\")\nmodel_stats['train_accuracy'].append(Stack_TrainAcc)\nmodel_stats['test_accuracy'].append(Stack_TestAcc)\nmodel_stats['f1_score'].append(Stack_F1)\nmodel_stats['roc_score'].append(Stack_Roc)","ac93b7bb":"dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\ndTree.fit(xtrain_resampled, ytrain_resampled)","1f40d087":"print(dTree.score(xtrain_resampled, ytrain_resampled))\nprint(dTree.score(x_test, y_test))","b657dd2e":"## Reducing over fitting (Regularization)\ndTreeR = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3, max_leaf_nodes=4, random_state=1)\ndTreeR.fit(xtrain_resampled, ytrain_resampled)\nprint(dTreeR.score(xtrain_resampled, ytrain_resampled))\nprint(dTreeR.score(x_test, y_test))","df66e756":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dTreeR.feature_importances_, columns = [\"Imp\"], index = xtrain_resampled.columns))","b1f328b0":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(base_estimator=dTree, n_estimators=50,random_state=1)\n#bgcl = BaggingClassifier(n_estimators=50,random_state=1)\n\nbgcl = bgcl.fit(xtrain_resampled, ytrain_resampled)","ad5014a2":"y_bgclpred = bgcl.predict(x_test)","b1e5f093":"print(metrics.classification_report(y_test,y_bgclpred))\ncm=metrics.confusion_matrix(y_test, y_bgclpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nBag_TrainAcc = bgcl.score(xtrain_resampled,ytrain_resampled)\nBag_TestAcc = bgcl.score(x_test,y_test)\nBag_F1 = metrics.f1_score(y_test,y_bgclpred)\nBag_Roc = metrics.roc_auc_score(y_test,y_bgclpred)\nprint(f\"Score of Bagging classifier Test Data : {Bag_TestAcc}\")\nprint(f'Score of Bagging classifier Train Data : {Bag_TrainAcc}')\nprint(f\"Roc AUC score of Bagging classifier : {Bag_Roc}\")\nprint(f\"f1 score of Bagging classifier : {Bag_F1}\\n\")\nmodel_stats['model_name'].append(\"Bagging\")\nmodel_stats['train_accuracy'].append(Bag_TrainAcc)\nmodel_stats['test_accuracy'].append(Bag_TestAcc)\nmodel_stats['f1_score'].append(Bag_F1)\nmodel_stats['roc_score'].append(Bag_Roc)","adbb00f4":"# param_grid = {'n_estimators': [50, 60, 70, 80, 90, 100],\n#                 'max_features': [5,6,7,8,9,10,11,12,15,16,18,20]}\n\n# gbcl = GridSearchCV(estimator=GradientBoostingClassifier(), \n#                         param_grid=param_grid, \n#                         cv=10,\n#                         verbose=True, n_jobs=-1)\n\n\n# gbcl = gbcl.fit(xtrain_resampled, ytrain_resampled)\n# y_gbclpred = gbcl.predict(x_test)\n# gbcl.best_params_","e500c512":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 100,max_features = 7,random_state=1)\ngbcl = gbcl.fit(xtrain_resampled, ytrain_resampled)\ny_gbclpred = gbcl.predict(x_test)","277e8d58":"print(metrics.classification_report(y_test,y_gbclpred))\ncm=metrics.confusion_matrix(y_test, y_gbclpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nGB_TrainAcc = gbcl.score(xtrain_resampled,ytrain_resampled)\nGB_TestAcc = gbcl.score(x_test,y_test)\nGB_F1 = metrics.f1_score(y_test,y_gbclpred)\nGB_Roc = metrics.roc_auc_score(y_test,y_gbclpred)\nprint(f\"Score of GradientBoost classifier Test Data : {GB_TestAcc}\")\nprint(f'Score of GradientBoost classifier Train Data : {GB_TrainAcc}')\nprint(f\"Roc AUC score of GradientBoost classifier : {GB_Roc}\")\nprint(f\"f1 score of GradientBoost classifier : {GB_F1}\\n\")\nmodel_stats['model_name'].append(\"GradientBoosting\")\nmodel_stats['train_accuracy'].append(GB_TrainAcc)\nmodel_stats['test_accuracy'].append(GB_TestAcc)\nmodel_stats['f1_score'].append(GB_F1)\nmodel_stats['roc_score'].append(GB_Roc)","8cfd1ca7":"# param_grid = {'n_estimators': [50, 60, 70, 80, 90, 100],\n#                 'max_features': [5,6,7,8,9,10,11,12,15,16,18,20]}\n\n# rfcl = GridSearchCV(estimator=RandomForestClassifier(), \n#                         param_grid=param_grid, \n#                         cv=10,\n#                         verbose=True, n_jobs=-1)\n\n# rfcl = rfcl.fit(xtrain_resampled, ytrain_resampled)\n# y_rfclpred = rfcl.predict(x_test)","17cae04e":"#rfcl.best_params_","751320ae":"rfcl = RandomForestClassifier(n_estimators = 100, random_state=1,max_features=8)\nrfcl = rfcl.fit(xtrain_resampled, ytrain_resampled)\ny_rfclpred = rfcl.predict(x_test)","ec347ee1":"print(metrics.classification_report(y_test,y_rfclpred))\ncm=metrics.confusion_matrix(y_test, y_rfclpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nRF_TrainAcc = rfcl.score(xtrain_resampled,ytrain_resampled)\nRF_TestAcc = rfcl.score(x_test,y_test)\nRF_F1 = metrics.f1_score(y_test,y_rfclpred)\nRF_Roc = metrics.roc_auc_score(y_test,y_rfclpred)\nprint(f\"Score of RandomForest classifier Test Data : {RF_TestAcc}\")\nprint(f'Score of RandomForest classifier Train Data : {RF_TrainAcc}')\nprint(f\"Roc AUC score of RandomForest classifier : {RF_Roc}\")\nprint(f\"f1 score of RandomForest classifier : {RF_F1}\\n\")\nmodel_stats['model_name'].append(\"RandomForest\")\nmodel_stats['train_accuracy'].append(RF_TrainAcc)\nmodel_stats['test_accuracy'].append(RF_TestAcc)\nmodel_stats['f1_score'].append(RF_F1)\nmodel_stats['roc_score'].append(RF_Roc)","a4e284bd":"# param_grid = {'n_estimators': [50, 60, 70, 80, 90, 100,150,200],\n#               'max_features': [5,6,7,8,9,10,11,12,15,16,18,20],\n#               'learning_rate':[10 ** x for x in range(-3,2)],\n#               'max_depth': [x for x in range(1,10)],\n#               'gamma': [0.0001, 0.0005, 0.001, 0.005]\n#              }\n\n# xgb_estimator = GridSearchCV(estimator=XGBClassifier(), \n#                         param_grid=param_grid, \n#                         cv=10,\n#                         verbose=True, n_jobs=-1)\n\n\n# xgb_estimator = xgb_estimator.fit(xtrain_resampled, ytrain_resampled)\n# #y_gbclpred = xgb_estimator.predict(x_test)\n# xgb_estimator.best_params_","50176951":"xgb_estimator = XGBClassifier( learning_rate=1,\n                               n_estimators=50,\n                               max_depth=2,\n                               #min_child_weight=1,\n                               gamma=0.0001,\n                               #subsample=0.8,\n                               #colsample_bytree=0.8,\n                               #n_jobs=-1,\n                               #reg_alpa=1,\n                              max_features= 5,\n                               #scale_pos_weight=1,\n                               random_state=42,\n                               verbose=1)\nxgb_estimator.fit(xtrain_resampled,ytrain_resampled)","59af6332":"y_xgbclpred = xgb_estimator.predict(x_test)","cdb864e3":"print(metrics.classification_report(y_test,y_xgbclpred))\ncm=metrics.confusion_matrix(y_test, y_xgbclpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nxgb_TrainAcc = xgb_estimator.score(xtrain_resampled,ytrain_resampled)\nxgb_TestAcc = xgb_estimator.score(x_test,y_test)\nxgb_F1 = metrics.f1_score(y_test,y_xgbclpred)\nxgb_Roc = metrics.roc_auc_score(y_test,y_xgbclpred)\nprint(f\"Score of XGBoost classifier Test Data : {xgb_TestAcc}\")\nprint(f'Score of XGBoost classifier Train Data : {xgb_TrainAcc}')\nprint(f\"Roc AUC score of XGBoost classifier : {xgb_Roc}\")\nprint(f\"f1 score of XGBoost classifier : {xgb_F1}\\n\")\nmodel_stats['model_name'].append(\"XGBoost\")\nmodel_stats['train_accuracy'].append(xgb_TrainAcc)\nmodel_stats['test_accuracy'].append(xgb_TestAcc)\nmodel_stats['f1_score'].append(xgb_F1)\nmodel_stats['roc_score'].append(xgb_Roc)","9d55f36d":"pargrid_adb = {'n_estimators':[50,100,200,400,600,800],\n              'learning_rate':[10 ** x for x in range(-3,2)],\n              'base_estimator':[svc_model]}\nadbgscv = GridSearchCV(estimator=AdaBoostClassifier(),param_grid = pargrid_adb,cv=5,verbose=True,n_jobs=-1)","54f03d0f":"adbgscv.fit(xtrain_resampled,ytrain_resampled)\ny_adbpred = adbgscv.predict(x_test)","526d7590":"adbgscv.best_estimator_","721374d5":"adbgscv = AdaBoostClassifier(base_estimator=SVC(kernel='poly', probability=True),\n                   learning_rate=0.1, n_estimators=600,random_state=2)\nadbgscv.fit(xtrain_resampled,ytrain_resampled)\ny_adbpred = adbgscv.predict(x_test)","e1de7f7b":"print(metrics.classification_report(y_test,y_adbpred))\ncm=metrics.confusion_matrix(y_test, y_adbpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nadb_TrainAcc = adbgscv.score(xtrain_resampled,ytrain_resampled)\nadb_TestAcc = adbgscv.score(x_test,y_test)\nadb_F1 = metrics.f1_score(y_test,y_adbpred)\nadb_Roc = metrics.roc_auc_score(y_test,y_adbpred)\nprint(f\"Score of AdaBoost classifier Test Data : {adb_TestAcc}\")\nprint(f'Score of AdaBoost classifier Train Data : {adb_TrainAcc}')\nprint(f\"Roc AUC score of AdaBoost classifier : {adb_Roc}\")\nprint(f\"f1 score of AdaBoost classifier : {adb_F1}\\n\")\nmodel_stats['model_name'].append(\"AdaBoost\")\nmodel_stats['train_accuracy'].append(adb_TrainAcc)\nmodel_stats['test_accuracy'].append(adb_TestAcc)\nmodel_stats['f1_score'].append(adb_F1)\nmodel_stats['roc_score'].append(adb_Roc)","03cf7ef7":"clf1 = KNeighborsClassifier(n_neighbors= 3 , weights = 'distance', metric='euclidean' )\nclf2 = GradientBoostingClassifier(n_estimators = 80,max_features = 8,random_state=1)\nclf3 = BaggingClassifier(base_estimator=dTree, n_estimators=50,random_state=1)\nclf4 = AdaBoostClassifier(base_estimator=SVC(kernel='poly', probability=True),\n                   learning_rate=0.1, n_estimators=600,random_state=2)\n\nvotingclf = VotingClassifier(estimators=[ ('knn',clf1),('grb', clf2),('bgg', clf3),('adb', clf4)], voting='hard')\nvotingclf = votingclf.fit(xtrain_resampled,ytrain_resampled)","bd2ff6a5":"y_votclpred = votingclf.predict(x_test)\nprint(metrics.classification_report(y_test,y_votclpred))\ncm=metrics.confusion_matrix(y_test, y_votclpred, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (8,5))\nsns.heatmap(df_cm, annot=True)\nplt.show()\nvtc_TrainAcc = votingclf.score(xtrain_resampled,ytrain_resampled)\nvtc_TestAcc = votingclf.score(x_test,y_test)\nvtc_F1 = metrics.f1_score(y_test,y_votclpred)\nvtc_Roc = metrics.roc_auc_score(y_test,y_votclpred)\nprint(f\"Score of stacking classifier Test Data : {vtc_TestAcc}\")\nprint(f'Score of stacking classifier Train Data : {vtc_TrainAcc}')\nprint(f\"Roc AUC score of stacking classifier : {vtc_Roc}\")\nprint(f\"f1 score of stacking classifier : {vtc_F1}\\n\")\nmodel_stats['model_name'].append(\"VotingClf\")\nmodel_stats['train_accuracy'].append(vtc_TrainAcc)\nmodel_stats['test_accuracy'].append(vtc_TestAcc)\nmodel_stats['f1_score'].append(vtc_F1)\nmodel_stats['roc_score'].append(vtc_Roc)","ed21aa42":"results_df = pd.DataFrame(model_stats)\nresults_df","85cd6272":"plt.figure(figsize = (18,10))\nsns.barplot(x =\"test_accuracy\",y = \"model_name\",data=results_df)\nplt.title(\"Model vs TestAccuracy\")\nplt.show()","9bd46298":"plt.figure(figsize = (18,10))\nsns.barplot(x =\"f1_score\",y = \"model_name\",data=results_df)\nplt.title(\"Model vs F1Score\")\nplt.show()","99c54ac3":"plt.figure(figsize = (18,10))\nsns.barplot(x =\"roc_score\",y = \"model_name\",data=results_df)\nplt.title(\"Model vs RocScore\")\nplt.show()","57231adf":"From the above it shows there are no null values in the dataset\n\nTotal number of Attributes 24","b310a670":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#23AD94\">Bivariate Analysis","7cd04e78":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#AE30E5\">Univariate Analysis","d8d5d3ea":"Logistic regression is overfitting\n\nKnn is close to over fit\n\nNaive bayes is also overfitting\n\nSVM is behaving properly and giving accurate results\n\nFrom the above models Accuracy score\/F1 score of KNN is best but since the Problem is more intended to detect the infected paitients.\n\nMostly I prefer TypeII error(False Negative) to be minimal so that infected persons can be identified accurately and treated.\nHere in this case it is also ok even if the False positives are more at the cost of false negatives.\n\n\n##### Best Model choosen from the above models is SVC\n#### Lets improve further by using Stacking technique","dc8db46f":"There are 195 rows(records) of data and 24 features\/columns","1becaad5":"### Logistic Regression","130c3869":"#### Accuracies of the Basic models for classification is as below.","e2986c74":"END","7672dc6b":"#### Checking for outliers in each variable","7bae0f27":"### Ensemble Learning - AdaBoosting","2b9b298b":"#### Lets Study each column distribution","4e12f3e4":"<center>\n<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#AE30E5\"> <h3> Parkinson Disease Differentiation\n<\/center>\n\n###### Objective:\nGoal is to classify the patients into the respective labels(either Parkinson or Heatly) using the attributes from their voice recordings\n\n###### Exploratory Data Analysis\n    - Univariate Analysis - Outlier and Frequency Analysis\n    - Bivariate Analysis - Visualization\n    - Variable Reduction - Multicollinearity\n\n###### Data Pre-Processing - \n    - Missing Values Treatment - Numerical (Mean\/Median imputation) and Categorical (Separate Missing Category or Merging)\n    - Outlier Treatment\n    - Skewness reduction of variable\n    - Standardising the Numerical columns\n    \n###### Model Build and Model Diagnostics\n    - Train and Test split\n    - Significance of each Variable\n    - Gini and ROC \/ Concordance analysis - Rank Ordering\n    - Classification Table Analysis - Accuracy\n\n###### Model Validation\n    - Accuracies\n    - ROC Curve - p-value and sign testing for the model coefficients\n    - Diagnostics check to remain similar to Training Model build\n    - BootStrapping, if necessary\n###### Choose the best model\n    - There are lot of measures to choose the best model such as accuracy,Roc score and f1 score\n    - Preferred best model with lower TypeII error","20c368f2":"### Ensemble Techniques ","f9a6220e":"#### Load the Data","5788e995":"### GradientBoost","13ce9502":"### STACKING","59f8aec2":"#### Checking the negative values in each feature","1fe5dcb2":"#### Skewness Reduction","dec4ccd9":"<span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#23AD94\">Model Buidling","2434a656":"##### Decision Tree","3a375638":"### Naive Bayes","5a1f892f":"**If finds this Notebook helpful. Your upvote is appreciated**","06d87505":"### Spliting the Data","e0b18af9":"Feature:  name is an Object data type and \n\nothers are float64 datatypes \n\nstatus variable is int64 datatype","6753e8fd":"### SVM","a99f1798":"#### Import Necessary Packages","32df4bd4":"<span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#AE30E5\"> Exploratory Data Analysis","21a1b14b":"Could observe there is a significant decrease in skewness of most of the variables\n\nOutliers have influenced the skewness in the data.\n\nWe can avoid Skewness treatment of the data","2042385f":"### XGBOOST","3e172441":"#### Attribute Information:\n- name - ASCII subject name and recording number\n- MDVP:Fo(Hz) - Average vocal fundamental frequency\n- MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n- MDVP:Flo(Hz) - Minimum vocal fundamental frequency \n- MDVP:Jitter(%)\n- MDVP:Jitter(Abs)\n- MDVP:RAP\n- MDVP:PPQ\n- Jitter:DDP\n- Several measures of variation in fundamental frequency - \n- MDVP:Shimmer\n- MDVP:Shimmer(dB)\n- Shimmer:APQ3\n- Shimmer:APQ5\n- MDVP:APQ\n- Shimmer:DDA - Several measures of variation in amplitude\n- NHR,HNR - Two measures of ratio of noise to tonal components in the voice \n- status - Health status of the subject (one) - Parkinson's, (zero) - healthy \n- RPDE,D2 - Two nonlinear dynamical complexity measures\n- DFA - Signal fractal scaling exponent\n- spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation \n- car name: string (unique for each instance)","8ecc797d":"We will use 70% data for training and 30% of the data for testing.","e08d9dbd":"### Standardising numerical features","c127de54":"### K-NN Classification Model","0797225b":"### Random Forest Classifier","74f6c296":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#23AD94\">Data Pre Processing","6c9d1e8d":"##### Above plot gives us the best k value to choose based on the score","2f5b9b2c":".","026ca1b9":"#### Lets oversample the target column 0 values and make the target data uniform.","6ce3824f":"<span style=\"font-family: Arial; font-weight:bold;font-size:1em;color:#306CE5\"> There are no duplicate enteries in the dataset","3e9f320d":"#### Outliers Treatment","5e383df3":"<span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#5921E3\">Conclusion\n\n\n\n<span style=\"font-family: Arial; font-weight:bold;font-size:1.2em;color:#167FE8\">Best Model with respect to Accuracy\/F1 Score\/ Roc Score is Voting classifier which is the combination of four different classifier.\n\n\n\n\n<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#E83519\">But \n    \n<span style=\"font-family: Arial; font-weight:bold;font-size:1em;color:#1098CB\">Since the Objective is mostly biased <span style=\"font-family: Arial; font-weight:bold;font-size:1em;color:#1098CB\">on health we do not want to take any chances in having Type II error.\n<span style=\"font-family: Arial; font-weight:bold;font-size:1em;color:#1098CB\">(False negative) which is predicting a disease infected person as healthy.\n\n<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#15B031\">Based on the above points we choose Stacking Classifier\/AdaBoost(with base estimator as SVC) as the best Model for prediction of parkinson disease. ","dc1a9bba":"### Based on the multicollinearity we selectively dropped few variables as below.\n['MDVP:Jitter(Abs)','MDVP:RAP','Jitter:DDP','spread1','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ']","18093fd0":".","f19b4926":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.8em;color:#EB7528\"> Inferences from Univariate Analysis :\n\n- Total number of records in the dataset 195\n- Total number of features in the dataset 24\n- There are no null values in the dataset\n- There are no duplicates in the dataset\n- Skewness value between -0.5 and +0.5 is allowed but the skewness above this are known as highly skewed\n- These below columns have higher skewness\n\n- MDVP:Fhi(Hz)        2.542146\n- MDVP:Flo(Hz)        1.217350\n- MDVP:Jitter(%)      3.084946\n- MDVP:Jitter(Abs)    2.649071\n- MDVP:RAP            3.360708\n- MDVP:PPQ            3.073892\n- Jitter:DDP          3.362058\n- MDVP:Shimmer        1.666480\n- MDVP:Shimmer(dB)    1.999389\n- Shimmer:APQ3        1.580576\n- Shimmer:APQ5        1.798697\n- MDVP:APQ            2.618047\n- Shimmer:DDA         1.580618\n- NHR                 4.220709\n\n- Distribution of Parkinson disease people and healthy people are not uniformly distributed\n- Outliers are present in most of the columns from the above box plot.\n","e5be9628":"#### we already know that the target variable status is not uniformly distributed.\n\n","005984b2":"- We could see there are many column right skewed as well as left skewes we can apply different transformations to reduce the skewness in the data\n\n\nThe logarithm, x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation and can be used to reduce right skewness.\n\nThe square, x to x\u00b2, has a moderate effect on distribution shape and it could be used to reduce left skewness.","9226cf8d":"Seems the above model is quit overfitting lets regularise or make the model more generalised","3d734cbb":"<span style=\"font-family: Arial; font-weight:bold;font-size:2.5em;color:#23AD94\">Best Model Analysis","81bac365":"There are many independent variables which are correlated among themselves (multicollinear) and pearson coefficient is greater than 0.8\n\n- MDVP:Jitter(Abs) - MDVP:Jitter(%) correlation coeff - 0.94\n- MDVP:RAP - MDVP:Jitter(%) correlation coeff - 0.99\n- MDVP:RAP - MDVP:Jitter(Abs) cc - 0.92\n- MDVP:PPQ - MDVP:Jitter(%) cc - 0.97\n- MDVP:PPQ - MDVP:Jitter(Abs) cc - 0.9\n- MDVP:PPQ - MDVP:RAP cc - 0.96\n- Jitter:DDP - MDVP:Jitter(%) cc - 0.99\n- Jitter:DDP - MDVP:Jitter(Abs) cc - 0.92\n- Jitter:DDP - MDVP:RAP cc - 1\n- Jitter:DDP - MDVP:PPQ cc - 0.96\n- PPE - spread1 cc - 0.96\n- MDVP:Shimmer - MDVP:PPQ cc - 0.8\n- MDVP:Shimmer(dB) - MDVP:Jitter(%) cc - 0.8\n- MDVP:Shimmer(dB) - MDVP:PPQ cc - 0.84\n- MDVP:Shimmer(dB) - MDVP:Shimmer cc - 0.99\n- Shimmer:APQ3 - MDVP:Shimmer cc - 0.99\n- Shimmer:APQ3 - MDVP:Shimmer(dB) cc - 0.96\n- Shimmer:APQ5 - MDVP:Shimmer cc - 0.98\n- Shimmer:APQ5 - MDVP:Shimmer(dB) cc - 0.97\n- Shimmer:APQ5 - Shimmer:APQ3 cc - 0.96\n- MDVP:APQ - MDVP:PPQ cc - 0.8\n- MDVP:APQ - MDVP:Shimmer cc - 0.95\n- MDVP:APQ - MDVP:Shimmer(dB) cc - 0.96\n- MDVP:APQ - Shimmer:APQ3' cc - 0.9\n- MDVP:APQ - 'Shimmer:APQ5' cc - 0.95\n- Shimmer:DDA - MDVP:Shimmer' cc - 0.99\n- Shimmer:DDA - 'MDVP:Shimmer(dB)' cc- 0.96\n- Shimmer:DDA - 'Shimmer:APQ3' cc - 1\n- Shimmer:DDA - 'Shimmer:APQ5' cc - 0.96\n- Shimmer:DDA - 'MDVP:APQ cc - 0.9\n- NHR - MDVP:Jitter(%) cc - 0.91\n- NHR - MDVP:Jitter(Abs) cc - 0.83\n- NHR - MDVP:RAP cc - 0.92\n- NHR - MDVP:PPQ cc - 0.84\n- NHR - Jitter:DDP cc - 0.92\n\nAmong these variables we can select either of those variables.","c72b676d":"### Bagging","9d132df8":"#### Lets check for duplicates in the dataset","bcc5cb8a":"### Voting Classifier"}}