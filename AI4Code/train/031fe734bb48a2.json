{"cell_type":{"46c9b5d3":"code","b5848049":"code","2752c3ac":"code","2d348620":"code","9351d557":"code","6203b1f7":"code","f7c2f3c9":"code","40b87719":"code","fa0c2a71":"code","ce3472b4":"code","5800f190":"code","4128d5e9":"code","8db1f209":"code","f671950a":"code","c6452e7a":"code","595e90d0":"code","32cd151c":"code","efc5d247":"code","401e70b9":"code","8f878466":"code","d3a40aaf":"code","7403644d":"code","dba55026":"code","fac11717":"code","d8780444":"markdown","d2145a77":"markdown","fa7f7789":"markdown","08323977":"markdown","09fc45fd":"markdown"},"source":{"46c9b5d3":"# import data processing and visualisation libraries\nimport numpy as np\nimport pandas as pd\nimport keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import tensorflow and keras\nimport tensorflow as tf\n#from tensorflow import keras\nimport os\n\nprint(\"Packages imported...\")","b5848049":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nwith tf.device('\/GPU:0'):\n    print('Yes, there is GPU')\n    \ntf.debugging.set_log_device_placement(True)","2752c3ac":"# MAKE THE SEED FIXED FOR PRODUCTIVITY\n# Lets set all random seeds\nimport random\nimport time, datetime\n\ndef get_current_time() -> str:\n    \"\"\"returns the current time in (str)\"\"\"\n    time_string = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y_%m_%d_%H_%M\")\n    return str(time_string)\ndef seed_everything(seed=0):\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 21\nseed_everything(seed)\n#warnings.filterwarnings('ignore')","2d348620":"## Exploring files in folder\nfolder_path = '..\/input\/petfinder-pawpularity-score\/'\n#print(list(os.walk(folder_path)))\nfor path, directories, files in os.walk(folder_path):\n    print(path,'--> number of files : ', len(files))\n# I SEE TEST DATA IS MEANT TO BE TEST IN WILD. but it's kinda usless honestly it's taken from same dataset, same human hand","9351d557":"#Preparing data in dataframe for easier data handling\ntrain_file = '..\/input\/petfinder-pawpularity-score\/train.csv'\ndata_df = pd.read_csv(train_file)\ndata_df['path'] = data_df['Id'].map(lambda x: str(folder_path+'\/train\/'+x)+'.jpg')\n#train_df = train_df.drop(columns=['Id'])\n#train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ndata_df.head()","6203b1f7":"# Distrubtion of the target\ntarget_col = 'Pawpularity'\nfig, ax = plt.subplots(figsize =(20, 10))\nax.hist(data_df[target_col], bins=100)\nax.set_title(f'Targets Histogram ')\nplt.show()\n# honeslty I didn't need the visuals as I saw the count frequency of the folder, but hey it won't bite","f7c2f3c9":"#SHOWING SOME RANDOM IMAGES\nimport random\nimport matplotlib.image as mpimg\n\nsigns = data_df[target_col].unique().tolist()\nimages = []\nprint(f'total number of unique traget : {len(signs)}')\nno_of_samples = 5\nrandom_signs = random.choices(signs, k=no_of_samples)\nfor sign in random_signs:\n    rows = data_df[data_df[target_col]==sign]['path']\n    #print(rows)\n    filepath = random.choice(list(rows))\n    #print(filepath)\n    img = mpimg.imread(filepath)\n    plt.figure()\n    plt.title(sign)\n    plt.imshow(img)","40b87719":"from sklearn.model_selection import train_test_split\n#ENCODING LABEL to 0,1,2,3,4,5,6, etc..\n# parameters\nx_col = 'path'\ny_col = 'Pawpularity'\ntest_size = 0.2\n# NO NEED TO DO SPLITTIN< JUST EVALUATE ON THE TEST SAMPLE PROVIDED. LAST YEAR SHOULD BE MAPPED FROM 0 to 100.\n\n#splitting data ..................\ntrain_df, test_df = train_test_split(data_df, test_size= test_size, random_state=seed, stratify=data_df[[y_col]])\nprint(f'train size : {len(train_df)}')\nprint(f'test size : {len(test_df)}')","fa0c2a71":"# CREATING DATA GENERATORS\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# for efficentNetB0 input is 224\n# for B2 260\n\nimg_width, img_height = 300, 300\nbatch_size = 32\nno_of_classes = 1\n\n# NO AUGMENTAION, JUST NRORMALIZING THE DATA\n# TRAINING GENERATOR\n# WITH AUGMENTAIONS\ntrain_datagen = ImageDataGenerator(preprocessing_function = tf.keras.applications.efficientnet.preprocess_input,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True,\n                                   vertical_flip = True,\n                                   fill_mode = 'nearest'\n                                  )\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,x_col=x_col, y_col=y_col,\n    target_size=(img_width, img_height),\n    class_mode='raw',\n    batch_size=batch_size,\n    seed=seed,\n    shuffle=True,\n)\n\n# TESTING GENERATOR\nvalidation_datagen = ImageDataGenerator(preprocessing_function = tf.keras.applications.efficientnet.preprocess_input)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    dataframe=test_df, x_col=x_col, y_col=y_col,\n    target_size=(img_width, img_height),\n    class_mode='raw',\n    batch_size=batch_size,\n    seed=seed,\n    shuffle=True,\n)","ce3472b4":"from keras.models import Sequential\nfrom keras.layers import GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n#from keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB3\n\n# I FEEL LIKE THE DROPOUT IS A BIT LARGE\ndef create_model():\n    model = Sequential()\n    # initialize the model with input shape\n    model.add(\n        EfficientNetB3(\n            input_shape = (img_width, img_height, 3), \n            include_top = False,\n            weights='imagenet',\n            drop_connect_rate=0.6,\n        )\n    )\n    model.add(GlobalAveragePooling2D())\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu', bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(no_of_classes))\n\n    return model","5800f190":"# #import keras\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dense, Flatten, BatchNormalization, Dropout, Input\n\n# # SIMPLE MODEL \n# model = Sequential()\n\n# # model.add(Conv2D(32, (5, 5), input_shape=(img_width, img_height, 3)))\n\n# # model.add(BatchNormalization())\n# # model.add(Activation('relu'))\n# # model.add(MaxPooling2D((2, 2)))\n# # model.add(Dropout(0.4))\n\n# # model.add(Conv2D(64, (3, 3)))\n# # model.add(BatchNormalization())\n# # model.add(Activation('relu'))\n# # model.add(MaxPooling2D((2, 2)))\n# # model.add(Dropout(0.4))\n\n# # model.add(Conv2D(64, (3, 3)))\n# # model.add(BatchNormalization())\n# # model.add(Activation('relu'))\n# # model.add(MaxPooling2D((2, 2)))\n# # model.add(Dropout(0.4))\n\n# # model.add(Flatten())\n\n# # model.add(Dense(512, activation='relu'))\n\n# # model.add(Dense(no_of_classes))\n\n# # EFFICENT NET SOLUTION\n# # Importing EfficientNets pretrained model\n# # MODEL ONE \n# img_mod = \"\/kaggle\/input\/keras-applications-models\/EfficientNetB0.h5\"\n# efnet_model = tf.keras.models.load_model(img_mod)\n# efnet_model.trainable = False\n\n# model = Sequential()\n\n# model.add(Input(shape=(img_width, img_height, 3)))\n# model.add(efnet_model)\n# # OUTPUT OF EFNET is 1280 \n# model.add(BatchNormalization())\n# model.add(Dropout(0.2))          \n# model.add(Dense(128, activation='relu'))\n# model.add(Dense(no_of_classes))\n# ##############################\n\n# model.summary()","4128d5e9":"create_model().summary()","8db1f209":"# from tensorflow.keras.utils import plot_model\n# plot_model(model, show_shapes=True)","f671950a":"# OERFORMING EARLY STOPS \n\ndef compile_model(model):\n    # put model trackers\n    model.compile(optimizer='adam',\n                  loss='mse',\n                  metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"])\n    return model","c6452e7a":"sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n\nfrom tensorflow.compat.v1.keras import backend as K\nK.set_session(sess)","595e90d0":"## K-FOLDING\n\nfrom sklearn.model_selection import StratifiedKFold\n#### STRAIFY LABEL CUT TO 20 PIECES\nQ = 20\ndata_df['stratify_label'] = pd.qcut(data_df['Pawpularity'], q = Q, labels = range(Q))\n########################################################################################\n# NO OF FOLDS\n# NOF O EPCOSH\nk_folds = 5\nepochs = 50\nkfold = StratifiedKFold(n_splits = k_folds, shuffle = True, random_state = seed)\nkfold_splits = kfold.split(data_df.index, data_df['stratify_label'])\n\nhistory_objs = []\ncurrent_time = get_current_time()\nfor fold, (train_index, val_index) in enumerate(kfold_splits):\n    # CREATE MODEL, MAYBE IT RESET WEIGHTS?\n    # CREATE THE MODEL FROM SCRATCH AND COMPILE IT EACH FOLD\n    model = create_model()\n    model = compile_model(model)\n    train_df = data_df.loc[train_index].reset_index()\n    test_df = data_df.loc[val_index].reset_index()\n    # GENERATE THE DATA\n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe=train_df,x_col=x_col, y_col=y_col,\n        target_size=(img_width, img_height),class_mode='raw', batch_size=batch_size,\n        shuffle=False,\n    )\n    validation_generator = validation_datagen.flow_from_dataframe(\n        dataframe=test_df, x_col=x_col, y_col=y_col,\n        target_size=(img_width, img_height), class_mode='raw', batch_size=batch_size,\n        shuffle=False\n    )\n    \n    # MAKE THE CHECKPOINTS\n    early_stop = EarlyStopping(patience=10, monitor='val_mae', restore_best_weights=True)\n    ckpt = ModelCheckpoint(f'feature_model_{fold}_{current_time}.h5',\n                                          verbose = 1, \n                                          monitor = 'val_mae',\n                                          mode = 'min', \n                                          save_weights_only = True,\n                                          save_best_only = True)\n    with tf.device('\/GPU:0'):\n        history = model.fit(train_generator,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=validation_generator,\n                            callbacks = [early_stop, ckpt]\n                           )\n        history_objs.append(history)\n","32cd151c":"# # TRAINNING\n\n# epochs = 50\n# history = model.fit(train_generator,\n#                     epochs=epochs,\n#                     verbose=1,\n#                     validation_data=validation_generator,\n#                     callbacks = [early_stop]\n#                    )","efc5d247":"print(\"The model metrics are\")\nfor idx, history in enumerate(history_objs):\n    print(idx)\n    metrics = pd.DataFrame(history.history)\n    display(metrics)\n    print('==========================================================')","401e70b9":"for idx, history in enumerate(history_objs):\n    print(idx)\n    acc=history.history['rmse']\n    val_acc=history.history['val_rmse']\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n\n    epochs=range(len(acc))\n\n    fig = plt.figure(figsize=(14,7))\n    plt.plot(epochs, acc, 'r', label=\"Training RMSE\")\n    plt.plot(epochs, val_acc, 'b', label=\"Validation RMSE\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and validation RMSE')\n    plt.legend(loc='lower right')\n    plt.show()\n    fig = plt.figure(figsize=(14,7))\n    plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n    plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n    plt.legend(loc='upper right')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and validation loss')\n    plt.show()\n    print('=====================================================================================================')","8f878466":"# #Preparing data in dataframe for easier data handling\n# test_file = '..\/input\/petfinder-pawpularity-score\/test.csv'\n# test_df = pd.read_csv(test_file)\n# test_df['path'] = test_df['Id'].map(lambda x: str(folder_path+'\/test\/'+x)+'.jpg')\n# #train_df = train_df.drop(columns=['Id'])\n# #train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n# test_df.head()","d3a40aaf":"# test_datagen = ImageDataGenerator(rescale=1.\/255)\n# train_generator = test_datagen.flow_from_dataframe(\n#     dataframe=test_df, x_col=x_col,\n#     target_size=(img_width, img_height),class_mode=None,\n#     #validate_filenames=False,\n#     shuffle=False) ","7403644d":"#predictions = model.predict(train_generator)","dba55026":"#predictions","fac11717":"# result_df = pd.DataFrame()\n# result_df['Id'] = test_df['Id']\n# result_df['Pawpularity'] = predictions\n# result_df.to_csv('submission.csv', index=False)","d8780444":"## 2. Exploring data","d2145a77":"## 6. Predictions","fa7f7789":"## 3. Data Prepocessing\n","08323977":"## 1. Importing Packages","09fc45fd":"## 4. Modeling"}}