{"cell_type":{"b83fabd1":"code","b93dc524":"code","ba9cbbc1":"code","edd7cd4d":"code","d90d3a74":"code","14864d1f":"code","6c6002d2":"code","afd25de2":"code","3acd8553":"code","7cef3cbf":"code","3518f66c":"code","d2d753a3":"code","7e1c7663":"code","10df0bf1":"code","5e9fb83a":"code","6da64d11":"code","4071829a":"code","4f6348c6":"code","f07e0b35":"code","3be3b3aa":"code","0ab4ea31":"code","309b02e5":"code","743eef54":"code","7f3ede6c":"code","1042735a":"code","aa263809":"code","d53b3af0":"code","d84cdf95":"markdown","d6283390":"markdown","f07c594f":"markdown","c51156be":"markdown","dab59937":"markdown","745fd8e2":"markdown","9c6ede07":"markdown","71b0f508":"markdown","343c9d0b":"markdown","0c89dc24":"markdown","31e98aae":"markdown","c44731f1":"markdown","a91df7be":"markdown","a78a39fe":"markdown","b27301b5":"markdown","27fda677":"markdown","5969d8a1":"markdown","243ede8f":"markdown","7068f4ec":"markdown","5626a93e":"markdown","1457ae10":"markdown","e3320102":"markdown","297bd542":"markdown"},"source":{"b83fabd1":"import math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap as shap\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nimport xgboost\nimport catboost\nimport lightgbm as lightgbm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n## Load the data \nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain = pd.read_csv(\"..\/input\/widsdatathon2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2022\/test.csv\")\n\nprint(\"Number of train samples are\",train.shape)\nprint(\"Number of test samples are\",test.shape)\ncategorical_features = ['State_Factor', 'building_class', 'facility_type']\n\nnumerical_features=train.select_dtypes('number').columns\ntrain.info()\n","b93dc524":"train.head(10)\n        \n","ba9cbbc1":"#code copied from https:\/\/www.kaggle.com\/usharengaraju\/wids2022-lgbm-starter-w-b\nnames = train.columns\nplt.figure(figsize = (25,11))\nsns.heatmap(train.isna().values, xticklabels=train.columns)\nplt.title(\"Missing values in training Data\", size=20)","edd7cd4d":"\n# drop columns with > 60% missing values\nlst = train.isna().sum() \/ len(train)\np = pd.DataFrame(lst)\np.reset_index(inplace=True)\np.columns = ['a', 'b']\nlow_count = p[p['b'] > 0.6]\ntodelete = low_count['a'].values\nprint(todelete)\ntrain.drop(todelete, axis=1, inplace=True)\ntest.drop(todelete, axis=1, inplace=True)\nprint(\"Number of train samples are\",train.shape)\nprint(\"Number of test samples are\",test.shape)\n\n\n","d90d3a74":"missing_columns = [col for col in train.columns if train[col].isnull().any()]\nmissingvalues_count =train.isna().sum()\nmissingValues_df = pd.DataFrame(missingvalues_count.rename('Null Values Count')).loc[missingvalues_count.ne(0)]\n\n \nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.impute import KNNImputer\n\n\n# if you want to use an imputer with more options: SimpleImputer   \n\n#strategystring, default=\u2019mean\u2019\n#The imputation strategy.\n\n#If \u201cmean\u201d, then replace missing values using the mean along each column. Can only be used with\n#numeric data.\n\n#If \u201cmedian\u201d, then replace missing values using the median along each column. Can only be used\n#with numeric data.\n\n#If \u201cmost_frequent\u201d, then replace missing using the most frequent value along each column.\n#Can be used with strings or numeric data. If there is more than one such value, only the \n#smallest is returned.\n\n#If \u201cconstant\u201d, then replace missing values with fill_value. Can be used with strings or numeric data.\n\n#cat_imputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = \"missing\")\n#train_test[categorical_cols] = cat_imputer.fit_transform(train_test[categorical_cols])\n\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ncat_miss = intersection(missing_columns, categorical_features)\nnum_miss = intersection(missing_columns, numerical_features)\n\n\nif len(cat_miss)>0:\n    cat_imputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = \"most_frequent\")\n    train[cat_miss] = cat_imputer.fit_transform(train[cat_miss])\n    test[cat_miss] = cat_imputer.fit_transform(test[cat_miss])\n\nnum_imputer = SimpleImputer(missing_values = np.nan,  strategy = \"median\")\ntrain[num_miss] = num_imputer.fit_transform(train[num_miss])\ntest[num_miss] = num_imputer.fit_transform(test[num_miss])\n\n\nknn_imputer = KNNImputer(n_neighbors=10, weights=\"uniform\")\n#train[num_miss] = knn_imputer.fit_transform(train[num_miss])\n#test[num_miss] = knn_imputer.fit_transform(test[num_miss])","14864d1f":"#Correlation map to see how features are correlated with each other and with target\ndf = train[['Year_Factor', 'State_Factor', 'floor_area', 'year_built', \n                 'energy_star_rating', 'ELEVATION','cooling_degree_days', \n                 'heating_degree_days', 'precipitation_inches', 'snowfall_inches',\n                 'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',\n                 'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',\n                 'days_above_100F', 'days_above_110F', 'direction_max_wind_speed',\n                 'direction_peak_wind_speed', 'max_wind_speed',\n                 'site_eui']]\n\nplt.figure(figsize = (15,15))\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)","6c6002d2":"relevant_vis = ['State_Factor', 'building_class', 'Year_Factor'] \nfig, ax = plt.subplots(1, 3, figsize=(30, 6))\nfor variable, subplot in zip(relevant_vis, ax.flatten()):\n    sns.countplot(train[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","afd25de2":"    \nfig, ax = plt.subplots(1, 3, figsize=(30, 6))\nfor variable, subplot in zip(relevant_vis, ax.flatten()):\n    sns.countplot(test[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","3acd8553":"plt.figure(figsize=(25, 4))\nplt.subplot(141)\nsns.boxplot(train.floor_area , color = \"#ffd514\")\nplt.subplot(142)\nsns.boxplot(train.year_built , color = \"#ffd514\")\nplt.subplot(143)\nsns.boxplot(train.ELEVATION , color = \"#ffd514\")\nplt.subplot(144)\nsns.boxplot(train.site_eui , color = \"#ffd514\")","7cef3cbf":"plt.figure(figsize=(25, 4))\nplt.subplot(141)\nsns.boxplot(test.floor_area , color = \"#ffd514\")\nplt.subplot(142)\nsns.boxplot(test.year_built , color = \"#ffd514\")\nplt.subplot(143)\nsns.boxplot(test.ELEVATION , color = \"#ffd514\")\n","3518f66c":"\n#numerical_features = train.select_dtypes('number').columns\n#train[numerical_features].hist(bins=15, figsize=(30, 20), layout=(8, 8))","d2d753a3":"      \nvis_df = train[train['site_eui'] <= 200]\n\nplt.figure(figsize=(16, 4))\nplt.subplot(121)\nsns.boxplot(x=vis_df['Year_Factor'], y=vis_df['site_eui'])\n \n","7e1c7663":"\n    \nplt.figure(figsize=(16, 4))\nplt.subplot(121)\nsns.boxplot(x=vis_df['State_Factor'], y=vis_df['site_eui'])\nplt.subplot(122)\nsns.boxplot(x=vis_df['State_Factor'], y=vis_df['energy_star_rating'])","10df0bf1":"\nsns.jointplot(x=train['site_eui'], y=train['floor_area']);\n\n#sns.jointplot(x=train['floor_area'], y=train['ELEVATION']);","5e9fb83a":"## visualizations from https:\/\/www.kaggle.com\/farazrahman\/bee-building-energy-efficiency-eda\nimport plotly.express as px\nimport plotly.graph_objects as go\n\ntemp_year_group = (train.query('year_built >=1900')).groupby(\"year_built\")['site_eui'].agg('median').reset_index()#.sort_values(by ='site_eui', ascending = False)\ntemp_year_group['year_built'] = temp_year_group['year_built'].astype(str)\nfig = px.line(temp_year_group, x=\"year_built\", y=\"site_eui\")\nfig.update_layout(title_text='Median site eui of buildings', title_x=0.5, title_y=1, margin=dict(t=30, l=30, r=30, b=30))\nfig.show()","6da64d11":"temp = train.groupby(\"energy_star_rating\")[\"site_eui\"].agg('median').reset_index()\nfig = px.scatter(temp, x=\"energy_star_rating\", y=\"site_eui\")\nfig.update_layout(title_text='Median site eui of building when compared with energy star rating', title_x=0.5, title_y=1, margin=dict(t=30, l=30, r=30, b=30))\nfig.show()","4071829a":"\n\n#encoding\n\nlabel_encoder = LabelEncoder()\nfor col in categorical_features:\n    train[col] = label_encoder.fit_transform(train[col])\n    test[col] = label_encoder.fit_transform(test[col])\n\n\n\n#remove columns with low variation\n\ndef remove_feature_with_low_var(df, threshold=0.1):\n    for col in df.columns:\n        if df[col].std() < threshold:\n            df = df.drop([col], axis=1)\n    return df\n\n\n#remove correlations\n\nnumerical_features=train.select_dtypes('number').columns\nnum_feature = [col for col in numerical_features]\ndrop_columns = []\n# Create correlation matrix\ncorr_matrix = train[num_feature].corr().abs()\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.98\nto_drop = [column for column in upper.columns if any(upper[column] > 0.98)]\ntrain.drop(to_drop, inplace=True, axis=1)\ntest.drop(to_drop, inplace=True, axis=1)\n\n\n\n# drop uninformative columns\n\ntarget = train[\"site_eui\"]\ntrain = train.drop([\"site_eui\",\"id\"],axis =1)\ntest = test.drop([\"id\"],axis =1)\ntemp_test = test\ncolumn_names = train.columns\n    \n#scaling\n\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\n\n","4f6348c6":"tr = pd.DataFrame(train, columns = column_names.values)\nte = pd.DataFrame(test, columns = column_names.values)\ntr[\"energy_facility\"] = tr[\"floor_area\"].div(tr[\"energy_star_rating\"])\nte[\"energy_facility\"] = te[\"floor_area\"].div(te[\"energy_star_rating\"])\ncolumn_names = tr.columns\ntrain = tr.values\ntest = te.values","f07e0b35":"#outlier detection\n\n#### If you want to inspect outliers or use some type of flag features if the sample is an outlier\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\n#iso = LocalOutlierFactor(n_neighbors=35, contamination=0.01)\niso = IsolationForest(contamination=0.01)\n\noutliers = iso.fit_predict(train)\n# select all rows that are not outliers\n\ntrain = train[outliers!=-1]\ntarget = target[outliers!=-1]","3be3b3aa":"\nfrom sklearn.linear_model import LassoCV\nlasso = LassoCV().fit(train, target)\nimportance = np.abs(lasso.coef_)\nfeature_names = np.array(column_names)\n\nfeat_ind = np.argsort(importance)[::-1]\ntotlasso = 20\nfeature_imp = pd.DataFrame(columns=['Value','Feature'])\nfeature_imp.loc[:,'Value'] = importance\nfeature_imp.loc[:,'Feature'] = column_names\n\nfeature_imp = pd.DataFrame(sorted(zip(importance, column_names)), columns=['Value','Feature'])\ndata=feature_imp.sort_values(by=\"Value\", ascending=False)\ndata = data.iloc[0:30, :]\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=data)\nplt.title('Lasso Features')\nplt.tight_layout()\nplt.show()\n\n#if we want to use only the selected fetures by LASSO to train the classifiers\n#X_train = pd.DataFrame(train,columns = column_names)\n#train = X_train.iloc[:, feat_ind]\n#X_test = pd.DataFrame(test, columns = column_names)\n#test = X_test.iloc[:, feat_ind]\n#column_names = train.columns\n\n\n## a different feature selection method, forward sequential feature selector\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n# Sequential Forward Selection(sfs)\nsfs = SFS(LinearRegression(),\n          k_features=30,\n          forward=True,\n          floating=False,\n          scoring = 'r2',\n          cv = 0)\n\n#sfs.fit(train, target)\n#sfs.k_feature_names_\n\n#if we want to use only the selected fetures by SFS to train the classifiers\n#train = pd.DataFrame(train, columns = column_names)\n#test = pd.DataFrame(test, columns = column_names)\n#train = train[list(sfs.k_feature_names_)]\n#test = test[list(sfs.k_feature_names_)]\n\n\n","0ab4ea31":"#splitting into training and validation sets\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.2, random_state = 50)\n\n","309b02e5":"#model training and testing\n\nxgboost_model = xgboost.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10) #gamma=0, subsample=0.75, colsample_bytree=0.4,\nxgboost_model.fit(X_train,y_train)\ny_pred = xgboost_model.predict(X_test)\nRMSE1 = math.sqrt(np.square(np.subtract(y_pred,y_test)).mean())\nprint(RMSE1)\n\n#xgboost_model.fit(train, target)\n#res = pd.DataFrame(xgboost_model.predict(test))\n#res.to_csv(\"cl1.csv\", index = False)\n\n\n#lgbm_model = lightgbm.LGBMRegressor(n_estimators=1000,learning_rate=0.05, max_depth=10)\n#lgbm_model.fit(X_train,y_train)\n#y_pred = lgbm_model.predict(X_test)\n#RMSE2 = math.sqrt(np.square(np.subtract(y_pred,y_test)).mean())\n#print(RMSE2)\n\n#lgbm_model.fit(train, target)\n#res = pd.DataFrame(lgbm_model.predict(test))\n#res.to_csv(\"cl2.csv\", index = False)\n\n\n#Catboost classifier\n#cboost_model = catboost.CatBoostRegressor(learning_rate=0.05, depth=10, loss_function = 'RMSE', eval_metric = 'RMSE', iterations = 5000, verbose=False)\n#cboost_model.fit(X_train, y_train)\n#y_pred = cboost_model.predict(X_test)\n#RMSE3 = math.sqrt(np.square(np.subtract(y_pred,y_test)).mean())\n#print(RMSE3)\n\n#cboost_model.fit(train, target)\n#res = pd.DataFrame(cboost_model.predict(test))\n#res.to_csv(\"cl3.csv\", index = False)\n\n#xgboost_ = pd.read_csv('cl1.csv')\n#lgbm_ = pd.read_csv('cl2.csv')\n#cboost = pd.read_csv('cl3.csv')\n\n#res = 0.5*xgboost_.values + 0.4*lgbm_.values + 0.3*cboost.values\n#sub = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/sample_solution.csv\")\n#sub[\"site_eui\"] = res\n#sub.to_csv(\"submission_voting_simple.csv\", index = False)\n\n","743eef54":"# Grid search of the best parameters\nfrom sklearn.model_selection import GridSearchCV\n\n\n#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n#xgboost_model = xgboost.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10) #gamma=0, subsample=0.75, colsample_bytree=0.4,\n#clf = GridSearchCV(xgboost_model, parameters)\n#clf.fit(train, target)","7f3ede6c":"\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.pipeline import Pipeline\n#from functools import partial\n#from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n\n\nmodels = list()\n# create the pipeline\n#pipe = Pipeline([('ContinuousFeturesSelector',ce), ('m', clf3)])\n# add as a tuple to the list of models for voting\nmodels.append(('xgboost', xgboost_model))\n#models.append(('lgbm', lgbm_model))\n#models.append(('cgboost', cboost_model))\n#models.append(('pipe_NN', clf3))\n\n# define the voting ensemble\n\n#model = VotingRegressor(estimators=models,weights=[6,4,3]) \n#model.fit(X_train,y_train)\n#y_pred = model.predict(X_test)\n#RMSE_V = math.sqrt(np.square(np.subtract(y_pred,y_test)).mean())\n#print(\"combined voting RMSE is: \",RMSE_V)\n","1042735a":"\n\n#model.fit(train, target)\n\n#res = model.predict(test)\n#sub = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/sample_solution.csv\")\n#sub[\"site_eui\"] = res\n#sub.to_csv(\"submission.csv\", index = False)\n","aa263809":"#explanations code from https:\/\/www.kaggle.com\/shreyasajal\/wids-datathon-2022-explainable-ai-walkthrough\n\nX_test_ft=pd.DataFrame(X_test, columns=column_names)\n\nexplainer = shap.Explainer(xgboost_model)\nshap_values = explainer(X_test_ft)\n\nshap.summary_plot(shap_values, X_test_ft,plot_type=\"bar\")\n\n","d53b3af0":"\nshap.summary_plot(shap_values, X_test_ft)\n\n","d84cdf95":"### Plotting the correlation of features will show us those features that are more correlated between them but we can also see the correlation with the target variable","d6283390":"### Let's first import all the libraries we will use and load the train and test sets from Kaggle. After loading the data, we show the number of samples for each set and the first ten rows of our training data.","f07c594f":"### Feature importances and influences on the outcomes using SHAP values","c51156be":"**Considerations** \n\n- Training data given for year factor 1,2,3,4,5,6 and test data for year 7\n- There are outlier values for some of the features, which can be seen from the box plots, year built, ELEVATION\n- Distribution of features for the training set is different than distribution of features for the test set e.g. bulding class, facility type\n- Most of the samples in the training set have state factor 6, while in the test set this state is missing\n- Some of the features are correlated\n- Eui target variable seems to slightly decrease per year\n- State 6 seems to be the state with highest mean site eui","dab59937":"**Considerations** \n\n- Buildings with recent year built have a lower site energy usage intensity\n- site eui is has an inverse relationship with energy star rating","745fd8e2":"### Showing the histogram distribution for each of the numerical features","9c6ede07":"### Outlier detection and deletion, we used an ML method but outliers can also be detected and removed considering any criteria of interests e.g. year built < 200","71b0f508":"# WiDS Maastricht Datathon 2022 Pre-training Day-2 Notebook #\n","343c9d0b":"### Feature engineering, creating new features that make sense from the ones given","0c89dc24":"### Next step is missing value imputation, we use a simple imputer that uses the median to avoid the infuence of outliers","31e98aae":"### Feature selection","c44731f1":"### Numerical vs numerical features plot with distributions on top of the plots","a91df7be":"### Splitting the training set to obtain a validation set that help us to understand the performance of regressors. Note that the prediction is expected for year 7 while training data is for years from 1-6\n![cross_validation.png](attachment:26929250-c158-4b7f-a15d-9627a7cc01e4.png)\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html ","a78a39fe":"### Boxplots for some of the numerical features for training and test set can show us outliers and distribution mismatch ","b27301b5":"### Dropping columns that have more than 60% of missing values ","27fda677":"### Energy star rating vs state factor, numerical vs categorical features","5969d8a1":"### We explore the missing values with heatmaps","243ede8f":"Climate change is one of the most urgent threats humanity is facing. WiDS datathon brings a prediction challenge this year with a focus on mitigating the effects of climate change by means of energy efficiency. This notebook is created to provide the second day training for the datathon organized by Maastricht University in the framework of WIDS Datathon 2022. The aim is to present and explain the different steps in the machine learning pipeline so others can implement them based on their preferences.\nThe data consists of roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. \nFeatures include building characteristics and weather data for the location of the building, and as target variable the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). The task is to predict the Site EUI for each row in the dataset.\n\nThe notebook is brought to you by [Yenisel Plasencia Cala\u00f1a](https:\/\/www.linkedin.com\/in\/yenisel-plasencia-cala%C3%B1a-phd-10144190\/), [Chang Sun](https:\/\/www.linkedin.com\/in\/chang-sun-maastricht\/), [Carlos Utrilla Guerrero](https:\/\/nl.linkedin.com\/in\/carlos-utrilla-guerrero-97ba7b31), [Parveen Kumar](https:\/\/nl.linkedin.com\/in\/parveensenza).\n\n**Main steps presented**\n\n- Loading the data\n- Exploring missing values\n- Handling missing values\n- Different types of data exploration\n- More data cleaning, removing correlated features \n- Feature encoding and scaling\n- Feature engineering\n- Outlier detection\n- Feature selection\n- Splitting the data into training and validation\n- Training individual regression models\n- Training a combined model\n- Creating submission\n- Model explainability\n\n","7068f4ec":"### Training individual models, as this is a problem where the outputs are continuous values it is better framed as a regression problem and not a classification one.","5626a93e":"### Plotting the categorical features for the training and test set to find distribution mismatch","1457ae10":"### Training using the whole training set for higher accuracy and creating the submision file with the results for the test set","e3320102":"### Training and testing a voting regressor model","297bd542":"### Encoding the categorical features so they can be used as input of classifiers, removing correlations, and scaling numerical features"}}