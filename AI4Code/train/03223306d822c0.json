{"cell_type":{"4e15c6e4":"code","5bfa5f5b":"code","3de4dcb7":"code","4dd01de2":"code","c06fb3e8":"code","405251ef":"code","916624ca":"code","ad24d2f4":"code","0b964305":"code","3d7d9024":"code","bbb14f58":"code","1a898f4d":"code","6acc297f":"code","b3aaf987":"code","879d802c":"code","45134024":"code","438f07d5":"code","2fb63893":"code","75797bb8":"code","bbd8b7ee":"code","0be33f5b":"code","c678747d":"code","dfc85900":"code","1c7b2970":"code","27c95a2e":"code","6dcfc265":"code","603e805d":"code","3d160501":"code","472105c9":"code","c441d9aa":"code","a1127969":"code","e244a87d":"code","ad12c16f":"code","6d1f07e4":"code","dd63ae51":"code","fae18ac1":"code","e58ac5f8":"code","0c3c5f7d":"code","42d23044":"code","7bfb3361":"code","e4f3b0e6":"code","ab971581":"code","896b384b":"code","2b42a211":"code","704a1201":"code","59bef7f9":"code","c7f87db4":"code","c264a559":"code","81a47e45":"code","dbf07b0a":"code","66fc4bb9":"code","b9cd3edc":"code","75af5f25":"code","cf79a14a":"code","39aab922":"code","7d02b95a":"code","2bdf5880":"code","5db42dc5":"code","db71cc64":"code","a2907db9":"code","4df0fa88":"code","fb1248fa":"code","c2ba5ff7":"code","18cbe149":"code","27e62129":"code","bcef9703":"code","6e6aff12":"code","c962803e":"code","5c02776b":"code","a35a0704":"code","2b59c48a":"code","7b689975":"code","1697a633":"code","354e2cbb":"code","78c6b980":"code","99e9a423":"code","f8763e0d":"code","c1c9730f":"code","be7e5547":"code","7d0b5645":"code","6d5f1268":"code","48902b1d":"code","26564a96":"code","b83e5033":"code","322030a9":"code","a4c9ae7f":"code","f6d8e292":"code","5387bdf6":"code","bdbd2e37":"code","cdf920b5":"code","56c29d99":"code","6a689410":"code","0a70d126":"code","bf31b072":"code","7a4f13fc":"code","2c212715":"code","5cddc36c":"code","0a2cf82c":"code","b5572076":"code","1e272b18":"code","7591ce95":"code","25e01768":"code","ac049892":"code","ccbd6a5d":"code","e0feae99":"code","00ec6344":"code","9a67a2f9":"code","690e1204":"code","2f1874c8":"code","0c50f3c7":"code","d60cab17":"code","eedbb66f":"code","8e27f1f4":"code","427d4c1f":"code","9b1ea718":"code","a897cd8e":"code","97d52b0b":"code","e93b1b22":"code","3521068e":"code","9298cf56":"code","1311c03c":"code","5281e385":"code","f062f330":"code","a7d8cfde":"code","a3f6d74d":"code","179e6ce3":"code","e4b0136f":"code","f8e719cc":"code","145d2fe8":"code","5327d821":"code","b0697480":"code","419cd675":"code","3d9edc09":"code","5210aa62":"code","26080365":"code","0cf0490b":"code","545b3120":"code","41ac5dbc":"code","c17349fe":"code","21eeaf32":"code","e4d6e488":"code","0c0cd7dd":"code","6db9ca36":"code","e422d350":"code","16e38873":"code","f608338e":"code","d189cc38":"code","58b743ed":"code","fd9e17b9":"code","61a061ea":"code","5f6fa037":"markdown","67ca1422":"markdown","ab32fa89":"markdown","c4e7fe93":"markdown","1257fa27":"markdown","798f7f5a":"markdown","776e3a30":"markdown","4a23d659":"markdown","151562bf":"markdown","7c268b5e":"markdown","8e308f58":"markdown","5a124c48":"markdown","82e0d876":"markdown","fed566e4":"markdown","2b80d45b":"markdown","2698936f":"markdown","b1e43aae":"markdown","3364ca5d":"markdown","af097b0d":"markdown","ec6a0a87":"markdown"},"source":{"4e15c6e4":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n    \nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","5bfa5f5b":"pd.set_option('display.max_rows',100)\npd.set_option('display.max_columns',100)","3de4dcb7":"#Read The Data\n\ndf = pd.read_csv('..\/input\/capstone-projectibm-employee-attrition-prediction\/IBM HR Data new.csv')\n\ndf.head(5)","4dd01de2":"df.shape","c06fb3e8":"#check for ay null values\n\ndf.isnull().sum()[df.isnull().sum()!=0]","405251ef":"Null_values_percentage=(df.isnull().sum().sum()\/len(df))*100\nprint('Total',round(Null_values_percentage,3),'Of Null  Values are present in dataset.')","916624ca":"df.describe().T","ad24d2f4":"df.loc[(df.EnvironmentSatisfaction>df.EnvironmentSatisfaction.quantile(0.9))]","0b964305":"df.loc[(df.NumCompaniesWorked>10)] \n#Even if some of Lives Entire life Working No one can work at this much companies at One Life \n#We can Replace them Max 10 companies ","3d7d9024":"df.loc[(df.EnvironmentSatisfaction>df.NumCompaniesWorked.quantile(0.9))]","bbb14f58":"df.loc[(df.PerformanceRating>4),'PerformanceRating']\n","1a898f4d":"global y\ny = df.NumCompaniesWorked.quantile(0.9)\ndef capping(x):\n    \n    if x > y:\n        x=y\n    return x\ndf.NumCompaniesWorked=df.NumCompaniesWorked.apply(capping)\ndf.NumCompaniesWorked.value_counts()","6acc297f":"global y\ny = df.EnvironmentSatisfaction.quantile(0.9)\ndef capping(x):\n    \n    if x > y:\n        x=y\n    return x\ndf.EnvironmentSatisfaction=df.EnvironmentSatisfaction.apply(capping)\ndf.EnvironmentSatisfaction.value_counts()","b3aaf987":"global y\ny = df.PerformanceRating.quantile(0.9)\ndef capping(x):\n    \n    if x > y:\n        x=y\n    return x\ndf.PerformanceRating=df.PerformanceRating.apply(capping)\ndf.PerformanceRating.value_counts()","879d802c":"global y\ny = df.JobInvolvement.quantile(0.9)\ndef capping(x):\n    \n    if x > y:\n        x=y\n    return x\ndf.JobInvolvement=df.JobInvolvement.apply(capping)\ndf.JobInvolvement.value_counts()","45134024":"df.describe().T","438f07d5":"import seaborn as sns\nsns.boxplot(x=df['NumCompaniesWorked'])","2fb63893":"sns.boxplot(x=df['EnvironmentSatisfaction'])","75797bb8":"sns.boxplot(x=df['PerformanceRating'])","bbd8b7ee":"sns.boxplot(x=df['JobInvolvement'])","0be33f5b":"#Copied Original Dataset before EDA","c678747d":"df2=df.copy(deep=True) \ndf2","dfc85900":"df=df.dropna() #Total 1.5% Null values are available In dataset.","1c7b2970":"df.isnull().sum().shape","27c95a2e":"df.isnull().sum()","6dcfc265":"df.Attrition=df.Attrition.apply(lambda x: 1 if x=='Voluntary Resignation' else 0)\ndf.Attrition.value_counts()","603e805d":"df.to_csv('HR_Analyst_new.csv') \n#Saved Final File after Cleaning of Data. ","3d160501":"#by looking at this we can that our models base accuracy is 0.8118%\nround((df.Attrition.value_counts()[0]-df.Attrition.value_counts()[1])\/df.Attrition.value_counts()[0],2)","472105c9":"numeric_ = df.select_dtypes(exclude=['object']).copy()\ncategor_ = df.select_dtypes(['object']).copy()\nprint(color.BOLD+'\\033[91M CATEGORICAL COLUMNS- :'+color.END,categor_.columns,color.BOLD+'\\nShape of Categorical Data-:'+color.END,categor_.shape)\nprint(color.BOLD+'NUMERICAL COLUMNS-: '+color.END,numeric_.columns,color.BOLD+'\\nShape of Categorical Data-:'+color.END,numeric_.shape)","c441d9aa":"#DATA VISUALIZATION\nsns.countplot(df['Attrition'])\nfig = plt.gcf()\nfig.set_size_inches(7,7)\nplt.title('Attrition')","a1127969":"corr= numeric_.corr()\nplt.subplots(figsize=[12,7])\nsns.heatmap(corr,annot=True,mask=numeric_.corr()<0.3)","e244a87d":"plt.figure(figsize=[10,8])\nsns.distplot(df['Age'],hist=True,kde=True,color='k',bins=10)","ad12c16f":"# Majority of employees lie between the age range of 30 to 40","6d1f07e4":"df['Age'].value_counts()","dd63ae51":"df.Attrition.value_counts()\n#converting object to Numric","fae18ac1":"sns.catplot(x='Age',hue='Attrition',data=df,kind='count',height=10)","e58ac5f8":"df.EducationField.value_counts()","0c3c5f7d":"sns.catplot(x='EducationField',hue='Attrition',kind='count',data=df,height=7)\nplt.xticks(rotation=90)","42d23044":"df['Education'].value_counts()\nsns.countplot(df['Education'])","7bfb3361":"plt.xticks(rotation='vertical')\nsns.countplot(df['EducationField'])","e4f3b0e6":"# Around 30% of employees have education level of 3 and \n# Around 70% of employees are having 'Life Sciences' and 'Medical' education field.\n# For both male and female,attrition rate is higher for education level 1,2 and 3.","ab971581":"corr=df.corr()\nimport  seaborn as sns \nplt.figure(figsize=[20,15])\nsns.heatmap(corr,annot=True,cmap='YlGnBu',fmt='.0%')","896b384b":"#Dropped Unnecessary Columns from Dataset.\ndf=df.drop(['Over18','EmployeeNumber','StandardHours','EmployeeCount'],axis=1)","2b42a211":"df['Age_emp']=df.Age","704a1201":"df.drop('Age',axis=1,inplace=True)","59bef7f9":"df.head()","c7f87db4":"df.BusinessTravel.value_counts()","c264a559":"plt.xticks(rotation='vertical')\nsns.countplot(df['BusinessTravel'])","81a47e45":"# BusinessTravel We can see that Employee who Travel rarely there count is Higher.","dbf07b0a":"sns.barplot(df.BusinessTravel,df.Attrition,data=df)","66fc4bb9":"#People Who travel More there attrition Rate is Higher ","b9cd3edc":"df.Department.value_counts()","75af5f25":"sns.barplot(df.Department,df.Attrition,data=df)","cf79a14a":"plt.xticks(rotation='vertical')\nsns.countplot(df['Department'])","39aab922":"#R&D DepaDepartments has Maximum NUmber of Employees.","7d02b95a":"df.DistanceFromHome.value_counts()","2bdf5880":"df.DistanceFromHome = pd.to_numeric(df.DistanceFromHome,errors='coerce')","5db42dc5":"plt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\nsns.countplot(df['DistanceFromHome'])","db71cc64":"categor_.columns","a2907db9":"df.EducationField.value_counts()","4df0fa88":"def edufield(x):\n    if  x=='Test':\n        x='Other'\n    return x\ndf.EducationField=df.EducationField.apply(edufield)\ndf.EducationField.value_counts()","fb1248fa":"plt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\nsns.countplot(df['EducationField'])","c2ba5ff7":"df.Gender.value_counts()","18cbe149":"def gender(x):\n    if x=='Male':\n        x=1\n    elif x=='Female':\n        x=0\n    return x","27e62129":"df.Gender=df.Gender.apply(gender)\ndf.Gender = pd.to_numeric(df.Gender,errors='coerce')\n","bcef9703":"plt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\nsns.countplot(df['Gender'])","6e6aff12":"plt.figure(figsize=[7.,7])\nsns.barplot(df.Gender,df.Attrition,data=df,hue_order='Attrition')","c962803e":"#Attritio of Male is Higehr Compared With Female.","5c02776b":"df.HourlyRate.value_counts()\ndf.HourlyRate = pd.to_numeric(df.HourlyRate,errors='coerce')","a35a0704":"df.HourlyRate.value_counts()\nplt.figure(figsize=[15,10])\nplt.xticks(rotation='vertical')\nsns.countplot(sorted(df['HourlyRate']))","2b59c48a":"df.JobRole.value_counts()","7b689975":"plt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\nsns.countplot(sorted(df['JobRole']))","1697a633":"plt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\nsns.barplot(df.JobRole,df.Attrition,data= df,ci=80,hue_order='attrition')","354e2cbb":"#Here above we can see that Attrion of SalesRepresntative Is Much Higher.","78c6b980":"df.JobSatisfaction.value_counts()","99e9a423":"plt.figure(figsize=[10,10])\nplt.xticks(rotation=0)\nsns.barplot(df.JobSatisfaction,df.Attrition,data= df)","f8763e0d":"df.JobSatisfaction = pd.to_numeric(df.JobSatisfaction,errors='coerce')\n","c1c9730f":"df.MaritalStatus.value_counts()","be7e5547":"plt.figure(figsize=[10,10])\nplt.xticks(rotation=0)\nsns.barplot(df.MaritalStatus,df.Attrition,data= df)","7d0b5645":"#From Above Bar chart we can see that Employees those are single Whose Attrition Rate is Higher.","6d5f1268":"df.MonthlyIncome.value_counts()","48902b1d":"df.MonthlyIncome = pd.to_numeric(df.MonthlyIncome,errors='coerce')\n","26564a96":"df.OverTime.value_counts()","b83e5033":"def overtime(x):\n    if x=='Yes':\n        x=1\n    elif x=='No':\n        x=0\n    return x","322030a9":"df.OverTime=df.OverTime.apply(overtime)\ndf.OverTime = pd.to_numeric(df.OverTime,errors='coerce')\ndf.info()","a4c9ae7f":"sns.barplot(df.OverTime,df.Attrition,data=df)","f6d8e292":"df.PercentSalaryHike.value_counts()\ndf.PercentSalaryHike = pd.to_numeric(df.PercentSalaryHike,errors='coerce')\ndf.info()","5387bdf6":"def empsou(x):\n    if x=='Test':\n        x='Referral'\n    return x\ndf['Employee Source']= df['Employee Source'].apply(empsou)","bdbd2e37":"plt.figure(figsize=[10,10])\nplt.xticks(rotation=90)\nsns.barplot(df['Employee Source'],df.Attrition)","cdf920b5":"df['Employee Source'].value_counts()","56c29d99":"#From refereal and jora Emp source is employees Attrition Is Higher. ","6a689410":"df=df.drop(['Application ID'],axis=1)\ndf.head()","0a70d126":"df.Attrition.value_counts()\n","bf31b072":"df.Attrition.shape","7a4f13fc":"dfn=df.copy()\ny=df.Attrition","2c212715":"df3=pd.get_dummies(df)\ndf3","5cddc36c":"df3.to_csv('HR_Analyst_File.csv', index=False)","0a2cf82c":"df.head()","b5572076":"df.isnull().sum()","1e272b18":"# Print all the data types and their unique values\nfor col in dfn.columns:\n    if dfn[col].dtype=='object':\n        print(color.BOLD+str(col)+color.END+ ' : '+str(dfn[col].unique()))\n        print(dfn[col].value_counts())\n        print('-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x')","7591ce95":"#P rint all the data types and their unique values\nfor col in dfn.columns:\n    if dfn[col].dtype=='int64' or dfn[col].dtype=='float64':\n        print(color.BOLD+str(col)+color.END+ ' : '+str(dfn[col].unique()))\n        print(dfn[col].value_counts())\n        print('-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x')","25e01768":"dfn.drop('Attrition',axis=1,inplace=True)","ac049892":"dfnew=pd.get_dummies(dfn)\ndfnew.shape","ccbd6a5d":"dfn.to_csv('HR_Analyst_2.csv', index=False)","e0feae99":"dfnew.info()","00ec6344":"X=dfnew\nX.head()","9a67a2f9":"y.head()","690e1204":"X.shape,y.shape","2f1874c8":"from sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics","0c50f3c7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0)","d60cab17":"from sklearn.ensemble  import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=0,max_depth=5,min_samples_split=10000)\nrf.fit(X_train,y_train)","eedbb66f":"#Accuracy\nround(rf.score(X_train,y_train),2)","8e27f1f4":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,rf.predict(X_test) )\ncm","427d4c1f":"TN=cm[0][0]\nTP=cm[1][1]\nFN=cm[1][0]\nFP=cm[0][1]\nprint('Model Testing Accuracy={}'.format(round((TP+TN)\/(TP+TN+FP+FN)),3))","9b1ea718":"#StandardScaler","a897cd8e":"logmodel = LogisticRegression()\nsmote=SMOTE(sampling_strategy='minority',random_state=3)\nX_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)\npd.Series(y_train_sm).value_counts()","97d52b0b":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\ndef model_eval(algo,xtrain,xtest,ytrain,ytest):\n    algo.fit(xtrain,ytrain)\n    y_train_pred=algo.predict(xtrain)\n    y_train_prob=algo.predict_proba(xtrain)[:,1]\n\n    y_test_pred=algo.predict(xtest)\n    y_test_prob=algo.predict_proba(xtest)[:,1]\n    print(color.BOLD+\"MODEL USED FOR CLASSIFICATION :\"+color.END,algo)\n    print(color.BOLD+'Confusion Matrix-Train:\\n'+color.END,confusion_matrix(ytrain,y_train_pred))\n    print(color.BOLD+'Accuracy Score-Train:\\n'+color.END,accuracy_score(ytrain,y_train_pred))\n    print(color.BOLD+'Classification Report-Train:\\n'+color.END,classification_report(ytrain,y_train_pred))\n    print(color.BOLD+'AUC Score-Train:\\n'+color.END,roc_auc_score(ytrain,y_train_prob))\n    print('\\n')\n    print(color.BOLD+'Confusion Matrix-Test:\\n'+color.END,confusion_matrix(ytest,y_test_pred))\n    print(color.BOLD+'Accuracy Score-Test:\\n'+color.END,accuracy_score(ytest,y_test_pred))\n    print(color.BOLD+'Classification Report-Test:\\n'+color.END,classification_report(ytest,y_test_pred))\n    print(color.BOLD+'AUC Score-Test:\\n'+color.END,roc_auc_score(ytest,y_test_prob))\n    print('\\n')\n    print(color.BOLD+'Plot'+color.END)\n    fpr,tpr,thresholds= roc_curve(ytest,y_test_prob)\n    fig,ax1 = plt.subplots()\n    ax1.plot(fpr,tpr)\n    ax1.plot(fpr,fpr)\n    ax1.set_xlabel('False Positive Rate')\n    ax1.set_ylabel('True Positive Rate')\n    ax2=ax1.twinx()\n    ax2.plot(fpr,thresholds,'-g')\n    ax2.set_ylabel('TRESHOLDS')\n    plt.show()\n    print('-x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--')","e93b1b22":"lr=LogisticRegression()\nknn=KNN()\nrf=RandomForestClassifier()\nsvc=SVC()\nrfc=RandomForestClassifier()\ndt=DecisionTreeClassifier()\nknn=KNN()\nxgb=XGBClassifier()\nadb=AdaBoostClassifier()\nsgd=SGDClassifier()\ngnb=GaussianNB()\net=ExtraTreesClassifier()\nmodels=[]\nmodels.append(('MVLC',lr))\nmodels.append(('XGB',xgb))\nmodels.append(('RFC',rf))\nmodels.append(('DT',dt))\nmodels.append(('ExtraTreesClassifier',et))\nmodels.append(('KNNC',knn))\nmodels.append(('AdaBoostClassifier',adb))\nresults=[]\nnames=[]\nypred=[]\nfor name,model in models:\n    model.fit(X_train,y_train)\n    ypred= model.predict(X_test)\n    print(color.BOLD+name+color.END,'\\n:')\n    print(classification_report(y_test,ypred))\n    kfold=KFold(shuffle=True,n_splits=10,random_state=0)\n    cv_results=cross_val_score(model,X_train,y_train,cv=kfold,scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print(color.BOLD+\"%s: %f (%f)\"%(name,np.mean(cv_results)*100,np.var(cv_results,ddof=1))+color.END)\n    print('-x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x-')\n    ","3521068e":"#plotting+feature importance in model\nfrom sklearn.ensemble import RandomForestClassifier\nmodelRF = RandomForestClassifier()\nmodelRF.fit(X,y)\nmodelRF.feature_importances_\nprint(modelRF.feature_importances_)\ncolumn_name = pd.Series(modelRF.feature_importances_,index=X.columns)\nplt.figure(figsize =(10,10))\ncolumn_name.nlargest(8).sort_values(ascending=True).plot(kind='barh',color=my_cmap(my_norm(data)))\nplt.show()","9298cf56":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\ndef model_eval_2(algo,xtrain,xtest,ytrain,ytest):\n    algo.fit(xtrain,ytrain)\n    y_train_pred=algo.predict(xtrain)\n    y_train_prob=algo.predict_proba(xtrain)[:,1]\n\n    y_test_pred=algo.predict(xtest)\n    y_test_prob=algo.predict_proba(xtest)[:,1]\n    print(color.BOLD+\"MODEL USED FOR CLASSIFICATION :\"+color.END,algo)\n    #print(color.BOLD+'Confusion Matrix-Train:\\n'+color.END,confusion_matrix(ytrain,y_train_pred))\n    print(color.BOLD+'Accuracy Score-Train:\\n'+color.END,accuracy_score(ytrain,y_train_pred))\n    \n    print(color.BOLD+'AUC Score-Train:\\n'+color.END,roc_auc_score(ytrain,y_train_prob))\n    #print('\\n')\n    #print(color.BOLD+'Confusion Matrix-Test:\\n'+color.END,confusion_matrix(ytest,y_test_pred))\n    print(color.BOLD+'Accuracy Score-Test:\\n'+color.END,accuracy_score(ytest,y_test_pred))\n    #print(color.BOLD+'Classification Report-Test:\\n'+color.END,classification_report(ytest,y_test_pred))\n    print(color.BOLD+'AUC Score-Test:\\n'+color.END,roc_auc_score(ytest,y_test_prob))\n    #print('\\n')\n    #print(color.BOLD+'Plot'+color.END)\n    print('-x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x-')\n   ","1311c03c":"list1=[xgb,rf,dt,et,knn,adb]","5281e385":"ws=[]\nwos=[]\nfor i in list1:\n    print(color.BLUE+'WITH SMOTE'+color.END)\n    [ws.append(model_eval_2(i,X_train_sm,X_test,y_train_sm,y_test))]\n    print(color.BLUE+'WITHOUT SMOTE'+color.END)\n    [wos.append(model_eval_2(i,X_train,X_test,y_train,y_test))]\n","f062f330":"#Model Comparision With Box plots.\n\nfig = plt.figure(figsize=[10,7])\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","a7d8cfde":"#There Is not Much Difference WHile doing SMOTE and Without SMOTE which is Oversampling Techinique.","a3f6d74d":"models=[dt,adb,et,rf,xgb]\nfor i in models:\n    i.fit(X,y)\n    i.feature_importances_\n    print(i)\n    #Plot the data:\n    #my_colors = 'rgbkymc'  #red, green, blue, black, etc.\n    feature_ranks = pd.Series(i.feature_importances_,index=X.columns)\n    plt.figure(figsize =(10,10))\n    feature_ranks.nlargest(8).sort_values(ascending=True).plot(kind='barh')\n\n    plt.show()","179e6ce3":"a=[]\nfor i in models:\n    i.fit(X,y)\n    i.feature_importances_\n    imp_features = pd.Series(i.feature_importances_,index=X.columns)\n    x = pd.DataFrame(imp_features.nlargest(8).sort_values(ascending=False))\n    #print(i,'\\n',x.index.values,'\\n')\n    a.append(x.index.values)\n    b=pd.DataFrame(a)\n    c=b.T\n\nd=pd.DataFrame()\nfor i in c.columns:\n    d=pd.concat([d,c[i]],ignore_index=True)\nprint(d)\nd = d.rename(columns={0: 'Imp_Features'})\nd['Imp_Features'].value_counts()","e4b0136f":"new_X=dfnew[['DailyRate', 'Age_emp', 'DistanceFromHome', 'MonthlyIncome',\n       'TrainingTimesLastYear', 'TotalWorkingYears', 'MonthlyRate',\n       'HourlyRate', 'PercentSalaryHike','BusinessTravel_Travel_Frequently', 'OverTime', 'StockOptionLevel']]\nnew_X","f8e719cc":"new_y=df['Attrition']\nnew_y","145d2fe8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( new_X, new_y, test_size=0.4, random_state=0)","5327d821":"from sklearn.ensemble  import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=1000,criterion='gini',random_state=0,max_depth=5,min_samples_split=10000)\nrf.fit(X_train,y_train)\nrf.score(X_train,y_train)","b0697480":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,rf.predict(X_test) )\ncm","419cd675":"TN=cm[0][0]\nTP=cm[1][1]\nFN=cm[1][0]\nFP=cm[0][1]\nprint('Model Testing Accuracy={}'.format((TP+TN)\/(TP+TN+FP+FN)))","3d9edc09":"logmodel = LogisticRegression()\nsmote=SMOTE(sampling_strategy='minority',random_state=3)\nX_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)\npd.Series(y_train_sm).value_counts()","5210aa62":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\ndef model_eval(algo,xtrain,xtest,ytrain,ytest):\n    algo.fit(xtrain,ytrain)\n    y_train_pred=algo.predict(xtrain)\n    y_train_prob=algo.predict_proba(xtrain)[:,1]\n\n    y_test_pred=algo.predict(xtest)\n    y_test_prob=algo.predict_proba(xtest)[:,1]\n    print(color.BOLD+\"MODEL USED FOR CLASSIFICATION :\"+color.END,algo)\n    print(color.BOLD+'Confusion Matrix-Train:\\n'+color.END,confusion_matrix(ytrain,y_train_pred))\n    print(color.BOLD+'Accuracy Score-Train:\\n'+color.END,accuracy_score(ytrain,y_train_pred))\n    print(color.BOLD+'Classification Report-Train:\\n'+color.END,classification_report(ytrain,y_train_pred))\n    print(color.BOLD+'AUC Score-Train:\\n'+color.END,roc_auc_score(ytrain,y_train_prob))\n    print('\\n')\n    print(color.BOLD+'Confusion Matrix-Test:\\n'+color.END,confusion_matrix(ytest,y_test_pred))\n    print(color.BOLD+'Accuracy Score-Test:\\n'+color.END,accuracy_score(ytest,y_test_pred))\n    print(color.BOLD+'Classification Report-Test:\\n'+color.END,classification_report(ytest,y_test_pred))\n    print(color.BOLD+'AUC Score-Test:\\n'+color.END,roc_auc_score(ytest,y_test_prob))\n    print('\\n')\n    print(color.BOLD+'Plot'+color.END)\n    fpr,tpr,thresholds= roc_curve(ytest,y_test_prob)\n    fig,ax1 = plt.subplots()\n    ax1.plot(fpr,tpr)\n    ax1.plot(fpr,fpr)\n    ax1.set_xlabel('False Positive Rate')\n    ax1.set_ylabel('True Positive Rate')\n    ax2=ax1.twinx()\n    ax2.plot(fpr,thresholds,'-g')\n    ax2.set_ylabel('TRESHOLDS')\n    plt.show()\n    print('-x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x-')","26080365":"lr=LogisticRegression()\nknn=KNN()\nrf=RandomForestClassifier()\nsvc=SVC()\nrfc=RandomForestClassifier()\ndt=DecisionTreeClassifier()\nxgb=XGBClassifier()\net=ExtraTreesClassifier()\nmodels=[]\nmodels.append(('MVLC',lr))\nmodels.append(('XGB',xgb))\nmodels.append(('KNNC',knn))\nmodels.append(('RFC',rf))\nmodels.append(('ExtraTreesClassifier',et))\nmodels.append(('DT',dt))\nresults=[]\nnames=[]\nypred=[]\nfor name,model in models:\n    model.fit(X_train,y_train)\n    ypred= model.predict(X_test)\n    print(color.BOLD+name+color.END,'\\n')\n    print(classification_report(y_test,ypred))\n    kfold=KFold(shuffle=True,n_splits=10,random_state=0)\n    cv_results=cross_val_score(model,X_train,y_train,cv=kfold,scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print(color.BOLD+\"%s: %f (%f)\"%(name,np.mean(cv_results)*100,np.var(cv_results,ddof=1))+color.END)\n    print('\\n')\n    print(color.BOLD+'Plot'+color.END)\n    print('-x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x--x-x-x-x-x-')\n    ","0cf0490b":"\nfig = plt.figure(figsize=[12,12])\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.xticks(rotation=90)\nplt.show()","545b3120":"#HyperTuning Random Forest","41ac5dbc":"from sklearn.model_selection import GridSearchCV\nrfgridcv=GridSearchCV(estimator=RandomForestClassifier(),\nparam_grid=[{'n_estimators': [5,10,50],\n                               'max_depth':[5,10,15,20],\n                               'min_samples_leaf':[10,50,100],\n                               'min_samples_split': [20,100,200]}])\nrfgridcv.fit(X_train,y_train)\ny_train_pred=rfgridcv.predict(X_train)\ny_train_prob=rfgridcv.predict_proba(X_train)[:,1]\n\ny_test_pred=rfgridcv.predict(X_test)\ny_test_prob=rfgridcv.predict_proba(X_test)[:,1]\n\nprint('Confusion Matrix-Train\\n',confusion_matrix(y_train,y_train_pred))\nprint('Accuracy Score-Train\\n',accuracy_score(y_train,y_train_pred))\nprint('Classification Report-Train\\n',classification_report(y_train,y_train_pred))\nprint('AUC Score-Train\\n',roc_auc_score(y_train,y_train_prob))\nprint('\\n'*2)\nprint('Confusion Matrix-Test\\n',confusion_matrix(y_test,y_test_pred))\nprint('Accuracy Score-Test\\n',accuracy_score(y_test,y_test_pred))\nprint('Classification Report-Test\\n',classification_report(y_test,y_test_pred))\nprint('AUC Score-Test\\n',roc_auc_score(y_test,y_test_prob))\nprint('\\n'*3)\nprint('Plot : AUC-ROC Curve')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nthresholds[0] = thresholds[0]-1\nfig,ax1 = plt.subplots()\nax1.plot(fpr,tpr,label='ROC CURVE')\nax1.plot(fpr,fpr,label='AUC CURVE')\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nplt.legend(loc='best')\nax2=ax1.twinx()\nplt.show()","c17349fe":"rfgrid=GridSearchCV(estimator=RandomForestClassifier(),\n                   param_grid=[{'n_estimators': [5,10,50],\n                               'max_depth':[5,10,15,20],\n                               'min_samples_leaf':[10,50,100],\n                               'min_samples_split': [20,100,200]}])","21eeaf32":"rfgrid_fit=rfgrid.fit(X_train,y_train)\nrfgrid_fit","e4d6e488":"print(rfgrid_fit.best_estimator_)","0c0cd7dd":"rfgrid_score=rfgrid_fit.score(X_train,y_train)\nrfgrid_score","6db9ca36":"\nfrom sklearn.model_selection import RandomizedSearchCV\nrfrs_cv=RandomizedSearchCV(estimator=RandomForestClassifier(),\n                   param_distributions=[{'n_estimators': [5,10,50],\n                               'max_depth':[5,10,15,20],\n                               'min_samples_leaf':[10,50,100],\n                               'min_samples_split': [20,100,200]}])\nrfrs_cv.fit(X_train,y_train)\ny_train_pred=rfrs_cv.predict(X_train)\ny_train_prob=rfrs_cv.predict_proba(X_train)[:,1]\n\ny_test_pred=rfrs_cv.predict(X_test)\ny_test_prob=rfrs_cv.predict_proba(X_test)[:,1]\n\nprint('Confusion Matrix-Train\\n',confusion_matrix(y_train,y_train_pred))\nprint('Accuracy Score-Train\\n',accuracy_score(y_train,y_train_pred))\nprint('Classification Report-Train\\n',classification_report(y_train,y_train_pred))\nprint('AUC Score-Train\\n',roc_auc_score(y_train,y_train_prob))\nprint('\\n'*2)\nprint('Confusion Matrix-Test\\n',confusion_matrix(y_test,y_test_pred))\nprint('Accuracy Score-Test\\n',accuracy_score(y_test,y_test_pred))\nprint('Classification Report-Test\\n',classification_report(y_test,y_test_pred))\nprint('AUC Score-Test\\n',roc_auc_score(y_test,y_test_prob))\nprint('\\n'*3)\nprint('Plot : AUC-ROC Curve')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nthresholds[0] = thresholds[0]-1\nfig,ax1 = plt.subplots()\nax1.plot(fpr,tpr,label='ROC CURVE')\nax1.plot(fpr,fpr,label='AUC CURVE')\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nplt.legend(loc='best')\nax2=ax1.twinx()\nplt.show()","e422d350":"rfrandomized=RandomizedSearchCV(estimator=RandomForestClassifier(),\n                   param_distributions=[{'n_estimators': [5,10,50],\n                               'max_depth':[5,10,15,20],\n                               'min_samples_leaf':[10,50,100],\n                               'min_samples_split': [20,100,200]}])","16e38873":"rfrand_fit=rfrandomized.fit(X_train,y_train)\nrfrand_fit","f608338e":"print(rfrand_fit.best_estimator_)","d189cc38":"rfrand_score=rfrand_fit.score(X_train,y_train)\nrfrand_score","58b743ed":"#After Finding Best Estimaters from Both CVs\n#Both Hypertunning Models gives the same Estimaters for RandomForestClassifier","fd9e17b9":"#using GridSearchCV Best estimetors\nrfrs_cv=RandomForestClassifier(max_depth=20, min_samples_leaf=10, min_samples_split=20,\n                       n_estimators=50)\nrfrs_cv.fit(X_train,y_train)\ny_train_pred=rfrs_cv.predict(X_train)\ny_train_prob=rfrs_cv.predict_proba(X_train)[:,1]\n\ny_test_pred=rfrs_cv.predict(X_test)\ny_test_prob=rfrs_cv.predict_proba(X_test)[:,1]\n\nprint('Confusion Matrix-Train\\n',confusion_matrix(y_train,y_train_pred))\nprint('Accuracy Score-Train\\n',accuracy_score(y_train,y_train_pred))\nprint('Classification Report-Train\\n',classification_report(y_train,y_train_pred))\nprint('AUC Score-Train\\n',roc_auc_score(y_train,y_train_prob))\nprint('\\n'*2)\nprint('Confusion Matrix-Test\\n',confusion_matrix(y_test,y_test_pred))\nprint('Accuracy Score-Test\\n',accuracy_score(y_test,y_test_pred))\nprint('Classification Report-Test\\n',classification_report(y_test,y_test_pred))\nprint('AUC Score-Test\\n',roc_auc_score(y_test,y_test_prob))\nprint('\\n'*3)\nprint('Plot : AUC-ROC Curve')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nthresholds[0] = thresholds[0]-1\nfig,ax1 = plt.subplots()\nax1.plot(fpr,tpr,label='ROC CURVE')\nax1.plot(fpr,fpr,label='AUC CURVE')\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nplt.legend(loc='best')\nax2=ax1.twinx()\nplt.show()","61a061ea":"\nrf_randcv=RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=100,\n                       n_estimators=5)\n\nrf_randcv.fit(X_train,y_train)\ny_train_pred=rf_randcv.predict(X_train)\ny_train_prob=rf_randcv.predict_proba(X_train)[:,1]\n\ny_test_pred=rf_randcv.predict(X_test)\ny_test_prob=rf_randcv.predict_proba(X_test)[:,1]\n\nprint('Confusion Matrix-Train\\n',confusion_matrix(y_train,y_train_pred))\nprint('Accuracy Score-Train\\n',accuracy_score(y_train,y_train_pred))\n#print('Classification Report-Train\\n',classification_report(y_train,y_train_pred))\nprint('AUC Score-Train\\n',roc_auc_score(y_train,y_train_prob))\nprint('\\n'*2)\nprint('Confusion Matrix-Test\\n',confusion_matrix(y_test,y_test_pred))\nprint('Accuracy Score-Test\\n',accuracy_score(y_test,y_test_pred))\n#print('Classification Report-Test\\n',classification_report(y_test,y_test_pred))\nprint('AUC Score-Test\\n',roc_auc_score(y_test,y_test_prob))\nprint('\\n'*3)\nprint('Plot : AUC-ROC Curve')\nfpr,tpr,thresholds= roc_curve(y_test,y_test_prob)\nthresholds[0] = thresholds[0]-1\nfig,ax1 = plt.subplots()\nax1.plot(fpr,tpr,label='ROC CURVE')\nax1.plot(fpr,fpr,label='AUC CURVE')\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nplt.legend(loc='best')\nax2=ax1.twinx()\nplt.show()","5f6fa037":"> **MODEL BUILDING**","67ca1422":"* **So After applying Hypertunning with best Estimators for GridSearchCV**\n* I have got Train and Test Accuracy of **Train = 0.9589 & Test = 0.932**\n* **This Model Accuracy is Neither Overfit nor Underfit.**\n* **Its Balanced Accuracy So this is The Best Generalized Model for Model Builduing.**","ab32fa89":"**BUSINESS PROBLEM**\n\u201cAttrition in human resources refers to the gradual loss of employees over time. In general, relatively high attrition is problematic for companies. HR professionals often assume a leadership role in designing company compensation programs, work culture and motivation systems that help the organization retain top employees.\u201d\n\nOur role is to uncover the factors that lead to employee attrition through Exploratory Data Analysis, and explore them by using various classification models to predict if an employee is likely to quit. This could greatly increase the HR\u2019s ability to intervene on time and remedy the situation to prevent attrition.\n\nWhile this model can be routinely run to identify employees, who are most likely to quit, the key driver of success would be the human element of reaching out the employee, understanding the current situation of the employee and taking action to remedy controllable factors that can prevent attrition of the employee.\n\n**HR ANALYTICS**\nHuman resource analytics (HR analytics) is an area in the field of analytics that refers to applying analytic processes to the human resource department of an organization in the hope of improving employee performance and therefore getting a better return on investment. HR analytics does not just deal with gathering data on employee efficiency. Instead, it aims to provide insight into each process by gathering data and then using it to make relevant decisions about how to improve these processes.\n\n**DATASET**\nThis is a hypothetical dataset created by IBM data scientists. The dataset has (23436R X 37C) that contains numeric and categorical data types describing each employee\u2019s background and characteristics; and labelled (supervised learning) with whether they are still in the company or whether they have gone to work somewhere else. Machine Learning models can help to understand and determine how these factors relate to workforce attrition.","c4e7fe93":"* **Treating of Outliers from EnvironmentSatisfaction column**","1257fa27":"* **Removal of Outliers**\n","798f7f5a":"* **So After applying Hypertunning with best Estimators for RandomisedSearchCV**\n* I have got Train and Test Accuracy of **Train = 0.877 & Test = 0.868**\n* **This Model Accuracy is Neither Overfit nor Underfit.**\n","776e3a30":"**#Here performance Rating Cant be greater than 5 hence we will also cap them with max value.**","4a23d659":"**So in Our Data there are 1.5% of Null Values**\n* Insted of computing them we can directly DROP them bcz Dropped % of Null values is very less It wont affect our Momdel","151562bf":"* **Dropping Null Values.**","7c268b5e":"**#Mapped Categorial values from Target column to NUMERICAL VALUES for Model UnderStanding.**","8e308f58":"* ***GridSearchCV Hypertunning Model Generalized Model from Our Model Building.***","5a124c48":"* **There are some Outliers In Dataset Those are countable Hence We Replaced only Those values from Dataset with folloing function.Those Ourliers can be seen clearly in df.Describe() function In Max column**\n* **Those values also checked before only those many values are exists or are there many.**\n* **But still After completion of Model we can see there is not much difference by removing outlers from dataset after model Building we got similar results as with Outliers**","82e0d876":"* From Above both Hypertunning Methodes We found \n* GridSeachCV Model Train Accuracy > RandomisedSearchCV Train Accuracy\n* GridSeachCV Model Test Accuracy > RandomisedSearchCV Test Accuracy.\n* So We can see that GridSearchCV Model Accuracy is Higher Compared To RandmisedSearchCV hypertunning Model.\n","fed566e4":"* **Till Here We have droped Null Values and Saved the file for Visualisation.**\n\n* **Here We have checked For Numerical and categorical Columns Presnet In Dataset**","2b80d45b":"* **227 Are Null values Present in our Dataset**\n* Our Dataset Is Large so even If We Drop Null value rows from Our Dataset it wont affect our Model Accuracy","2698936f":"> ***If we look at the Indexs of all the rows which are giving many outliers Those are same Alternatively We can drop this Two Rows or Cap there outliers values from Data So here I have capped those values with the Respective row maximum Values***","b1e43aae":"* **Treating of Outliers from JobInvolvement column**","3364ca5d":"* **Treating of Outliers from PerformanceRating column**","af097b0d":"* **Treating of Outliers from NumCompaniesWorked column**","ec6a0a87":"*  **We can See some Outliers are present in the data We can remove them if Model isnot performing well.**"}}