{"cell_type":{"2e589cd3":"code","8331d859":"code","13a1d9b4":"code","f3fa3e22":"code","cfacff08":"code","ff5f809e":"code","48d44df0":"code","4ed21632":"code","eca13222":"code","eb20aa93":"code","11725067":"code","7b7a8262":"code","08814d8e":"code","77d12f06":"code","941a794d":"code","d3364730":"code","a5ad2ea0":"code","f3820795":"code","e3958d94":"code","590f10c0":"code","98295ca2":"code","7fe4b2f1":"code","b5a611aa":"code","07ff18aa":"code","4a485ff6":"code","4319b968":"code","53494612":"code","61d71f42":"code","b3f3308c":"code","37d0fc61":"code","1c111cb2":"code","f6f40ea2":"code","33ab16f4":"code","6cb4f7d0":"code","9b6e8ca2":"code","3f72d39b":"code","c17b8959":"code","a35c2f3b":"code","b974d568":"code","be033917":"code","42ce3230":"code","e7afa9d8":"code","b2e2b05f":"code","f7454812":"code","ff95bdd5":"code","fa023575":"code","52280c19":"code","140685b7":"code","36fb3000":"code","3dd6cb18":"code","48f1bcff":"code","994d9bde":"code","9b02f1ac":"code","f1428282":"code","9d4b5acd":"code","7a50970a":"code","244bf4e8":"code","71f509c0":"code","8f53ab2b":"code","91f12ee5":"code","27c89380":"code","a13681ac":"code","41567f6b":"code","0d1a5033":"code","df70ff5b":"code","848f4051":"code","7808b8a1":"code","6e6b6f32":"code","7b27b998":"code","0e84e508":"markdown","32770a09":"markdown","2e373a9d":"markdown","8e44bd38":"markdown","e4fa7be8":"markdown","980777be":"markdown","97620ec5":"markdown","38ad9b7c":"markdown","6bd4fbc6":"markdown","950f93c4":"markdown","bde3a8e0":"markdown","629eabb2":"markdown","4451305e":"markdown"},"source":{"2e589cd3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport glob\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom  collections import OrderedDict\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","8331d859":"meta=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nmeta.head()","13a1d9b4":"meta.shape","f3fa3e22":"meta=meta[((meta['has_pdf_parse']==True) |(meta['has_pmc_xml_parse']==True))]\nmeta_sm=meta[['cord_uid','sha','pmcid','title','abstract','publish_time','url']]\nmeta_sm.drop_duplicates(subset =\"title\", keep = False, inplace = True)\nmeta_sm.loc[meta_sm.publish_time=='2020-12-31'] = \"2020-03-31\"\nmeta_sm.head()","cfacff08":"meta_sm.shape","ff5f809e":"sys.path.insert(0, \"..\/\")\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n#inspired by this kernel. Thanks to the developer ref. https:\/\/www.kaggle.com\/fmitchell259\/create-corona-csv-file\n# Just set up a quick blank dataframe to hold all these medical papers. \n\ndf = {\"paper_id\": [], \"text_body\": []}\ndf = pd.DataFrame.from_dict(df)\ndf","48d44df0":"collect_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n\nfor i,file_name in enumerate (collect_json):\n    row = {\"paper_id\": None, \"text_body\": None}\n    if i%2000==0:\n        print (\"====processed \" + str(i)+ ' json files=====')\n        print()\n\n    with open(file_name) as json_data:\n            \n        data = json.load(json_data,object_pairs_hook=OrderedDict)\n        \n        row['paper_id']=data['paper_id']\n        \n        body_list = []\n       \n        for _ in range(len(data['body_text'])):\n            try:\n                body_list.append(data['body_text'][_]['text'])\n            except:\n                pass\n\n        body = \"\\n \".join(body_list)\n        \n        row['text_body']=body \n        df = df.append(row, ignore_index=True)\n  ","4ed21632":"df.shape","eca13222":"#merge metadata df with parsed json file based on sha_id\nmerge1=pd.merge(meta_sm, df, left_on='sha', right_on=['paper_id'])\nmerge1.head()","eb20aa93":"len(merge1)","11725067":"#merge metadata set with parsed json file based on pcmid\nmerge2=pd.merge(meta_sm, df, left_on='pmcid', right_on=['paper_id'])\nmerge2.head()","7b7a8262":"len(merge2)","08814d8e":"#combine merged sha_id and pcmid dataset, remove the duplicate values based on file name\nmerge_final= merge2.append(merge1, ignore_index=True)\nmerge_final.drop_duplicates(subset =\"title\", keep = False, inplace = True)\nlen(merge_final)","77d12f06":"merge_final.head()","941a794d":"#remove articles that are not related to COVID-19 based on publish time\ncorona=merge_final[(merge_final['publish_time']>'2019-11-01') & (merge_final['text_body'].str.contains('nCoV|Cov|COVID|covid|SARS-CoV-2|sars-cov-2'))]\ncorona.shape","d3364730":"import re \ndef clean_dataset(text):\n    text=re.sub('[\\[].*?[\\]]', '', str(text))  #remove in-text citation\n    text=re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '',text, flags=re.MULTILINE)#remove hyperlink\n    text=re.sub(r'\\\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\\\b', '', text)#remove email\n    text=re.sub(r'^a1111111111 a1111111111 a1111111111 a1111111111 a1111111111.*[\\r\\n]*',' ',text)#have no idea what is a11111.. is, but I remove it now\n    text=re.sub(r'  +', ' ',text ) #remove extra space\n    text=re.sub('[,\\.!?]', '', text)\n    text=re.sub(r's\/ ( *)\/\\1\/g','',text) \n    text=re.sub(r'[^\\w\\s]','',text) #strip punctions (recheck)\n    return text","a5ad2ea0":"import warnings\nwarnings.filterwarnings('ignore')\ncorona['text_body'] =corona['text_body'].apply(clean_dataset)\ncorona['title'] =corona['title'].apply(clean_dataset)\ncorona['abstract'] =corona['abstract'].apply(clean_dataset)\ncorona['text_body'] = corona['text_body'].map(lambda x: x.lower())\ncoro=corona.reset_index(drop=True)\ncoro.head()","f3820795":"coro['count_abstract'] = coro['abstract'].str.split().map(len)\ncoro['count_abstract'].sort_values(ascending=True)","e3958d94":"#check word count\ny = np.array(coro['count_abstract'])\n\nsns.distplot(y);","590f10c0":"coro['count_text'] = coro['text_body'].str.split().map(len)\ncoro['count_text'].sort_values(ascending=True)","98295ca2":"#check word count\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny = np.array(coro['count_abstract'])\n\nsns.distplot(y);","7fe4b2f1":"coro['count_text'] = coro['text_body'].str.split().map(len)\ncoro['count_text'].sort_values(ascending=True)","b5a611aa":"coro['count_text'].describe()","07ff18aa":"y = np.array(coro['count_text'])\n\nsns.distplot(y);","4a485ff6":"coro2=coro[((coro['count_text']>500)&(coro['count_text']<4000))]\ncoro2.shape","4319b968":"coro2.to_csv(\"corona.csv\",index=False)","53494612":"#split articles w\/o abstarct as the test dataset\n\ntest=coro2[coro2['count_abstract']<5]\ntest.head()","61d71f42":"test.shape","b3f3308c":"train= coro2.drop(test.index)\n\ntrain.head()","37d0fc61":"train.shape","1c111cb2":"train=train.reset_index(drop=True)\ntest=test.reset_index(drop=True)","f6f40ea2":"!pip install bert-extractive-summarizer","33ab16f4":"!pip install spacy\n!pip install transformers==2.6.0\n!pip install neuralcoref","6cb4f7d0":"# It seems there is something wrong with Bert Summarizer at the moment, if you want to see how it works, you can check out my last version","9b6e8ca2":"\nfrom summarizer import Summarizer\ntrain['summary']=\" \"\n\nfor i in range(2):\n    body=\" \"\n    result=\" \" \n    full=\" \" \n    model = Summarizer()\n    body=train['text_body'][i]\n    result = model(body, min_length=200)\n    full = ''.join(result)\n    train['summary'][i]=full\n     # print(i, train['summary'][i])\n     # print(\"===next====\")","3f72d39b":"#Bert does not work\n# It seems there is something wrong with the environment at the moment, if you want to see how it works, you can check out my last version\ntrain['summary'][0]","c17b8959":"body=train['text_body'][0]","a35c2f3b":"#GPT2\nfrom summarizer import Summarizer,TransformerSummarizer\nGPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\nfull = ''.join(GPT2_model(body, min_length=200))\nprint(full)","b974d568":"model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\nfull = ''.join(model(body, min_length=60))\nprint(full)","be033917":"!pip install spacy\n!pip install transformers","42ce3230":"import transformers\nimport torch","e7afa9d8":"from transformers import BartTokenizer, BartForConditionalGeneration\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'","b2e2b05f":"model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('bart-large-cnn')","f7454812":"from transformers import pipeline\n\n# load BART summarizer\nsummarizer = pipeline(task=\"summarization\")","ff95bdd5":"print(train['text_body'][0])","fa023575":"#I will redo this part\n\n#remove stop words\nimport gensim\nfrom gensim.parsing.preprocessing import remove_stopwords\n\nmy_extra_stop_words = ['preprint','paper','copyright','case','also','moreover','use','from', 'subject', 're', 'edu', 'use','and','et','al','medrxiv','peerreviewed','peerreview','httpsdoiorg','license','authorfunder','grant','ccbyncnd','permission','grant','httpsdoiorg101101202002']\n\ntrain['text_body']=train['text_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (my_extra_stop_words) and word not in gensim.parsing.preprocessing.STOPWORDS and len(word)>3]))\n\ncoronaRe=train.reset_index(drop=True)","52280c19":"import spacy\nnlp=spacy.load(\"en_core_web_sm\",disable=['parser', 'ner'])\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    text_out=[]\n    for word in texts:\n      data=nlp(word)\n      data=[word.lemma_ for word in data]\n      text_out.append(data)\n    return text_out\ncoronaRe['new_lem'] = lemmatization(coronaRe['text_body'],allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","140685b7":"from gensim.corpora import Dictionary\ndocs = coronaRe['new_lem']\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 10 documents, or more than 50% of the documents\ndictionary.filter_extremes(no_below=10, no_above=0.5)\n\n# Create Bag-of-words representation of the documents\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","36fb3000":"coronaRe.head()","3dd6cb18":"import gensim.corpora as corpora\n# Create Dictionary\ndictionary = gensim.corpora.Dictionary(coronaRe['new_lem'])\ncount = 0\nfor k, v in dictionary.iteritems():\n    #print(k, v)\n    count += 1\n#less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number).after the above two steps, keep only the first 4500 most frequent tokens.\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=4500)\n# Create Corpus\nbow_corpus = [dictionary.doc2bow(doc) for doc in coronaRe\n              ['new_lem']]\nbow_corpus_id=[ id for id in coronaRe['cord_uid']]\n# View\nprint(bow_corpus[:1])\n","48f1bcff":"# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                       id2word=dictionary,\n                                       num_topics=10, \n                                       random_state=100,\n                                       chunksize=100,\n                                       passes=10,\n                                       per_word_topics=True)","994d9bde":"from pprint import pprint\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","9b02f1ac":"lda_df = lda_model.get_document_topics(bow_corpus,minimum_probability=0)\nlda_df = pd.DataFrame(list(lda_df))\n\nnum_topics = lda_model.num_topics\n\nlda_df.columns = ['Topic'+str(i) for i in range(num_topics)]\nfor i in range(len(lda_df.columns)):\n    lda_df.iloc[:,i]=lda_df.iloc[:,i].apply(lambda x: x[1])\nlda_df['Automated_topic_id'] =lda_df.apply(lambda x: np.argmax(x),axis=1)\nlda_df.head()","f1428282":"#coherence score https:\/\/stackoverflow.com\/questions\/54762690\/coherence-score-0-4-is-good-or-bad\nfrom gensim.models import CoherenceModel\n# Compute Coherence Score\nfrom tqdm import tqdm\ncoherenceList_cv=[]\nnum_topics_list = np.arange(5,26)\nfor num_topics in tqdm(num_topics_list):\n  lda_model = gensim.models.LdaModel(corpus=bow_corpus,\n                                         id2word=dictionary,\n                                         num_topics=num_topics,\n                                         random_state=100,\n                                         chunksize=100,\n                                         passes=10,\n                                         alpha='auto',\n                                         per_word_topics=True)\n  coherence_model_lda = CoherenceModel(model=lda_model, texts=coronaRe['new_lem'], coherence='c_v')\n  coherence_lda = coherence_model_lda.get_coherence()\n  coherenceList_cv.append(coherence_lda)\nprint('\\nCoherence Score: ', coherence_lda)","9d4b5acd":"#re-do (not correct)\nplotData = pd.DataFrame({'Number of topics':num_topics_list,\n                         'CoherenceScore_cv':coherenceList_cv})\nf,ax = plt.subplots(figsize=(10,6))\nsns.set_style(\"darkgrid\")\nsns.pointplot(x='Number of topics',y= 'CoherenceScore_cv',data=plotData)\n\nplt.title('Topic coherence')","7a50970a":"#final model\n\nLda = gensim.models.LdaMulticore\nlda_final= Lda(corpus=bow_corpus, num_topics=17,id2word = dictionary, passes=10,chunksize=100,random_state=100)","244bf4e8":"from pprint import pprint\n# Print the Keyword in the 11 topics\npprint(lda_final.print_topics())\ndoc_lda = lda_final[corpus]","71f509c0":"lda_df = lda_final.get_document_topics(bow_corpus,minimum_probability=0)\nlda_df = pd.DataFrame(list(lda_df))\nlda_id=pd.DataFrame(list(bow_corpus_id))\nnum_topics = lda_final.num_topics\n\nlda_df.columns = ['Topic'+str(i) for i in range(num_topics)]\n\nfor i in range(len(lda_df.columns)):\n    lda_df.iloc[:,i]=lda_df.iloc[:,i].apply(lambda x: x[1])\n\nlda_df['Automated_topic_id'] =lda_df.apply(lambda x: np.argmax(x),axis=1)\n\nlda_df['cord_uid']= lda_id\nlda_df[39:40]","8f53ab2b":"topic=lda_df[['Automated_topic_id','cord_uid']]","91f12ee5":"plot_topics=lda_df.Automated_topic_id.value_counts().reset_index()\nplot_topics.columns=[\"topic_id\",\"quantity\"]\nplot_topics","27c89380":"ax = sns.barplot(x=\"topic_id\", y=\"quantity\",  data=plot_topics)","a13681ac":"coronaRe['topic_id']= topic['Automated_topic_id']\ncoronaRe.head()","41567f6b":"#https:\/\/medium.com\/@manivannan_data\/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz","0d1a5033":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\n\nimport en_ner_bionlp13cg_md\nnlp = en_ner_bionlp13cg_md.load()\ntext = train['abstract'][2]\ndoc = nlp(text)\nprint(list(doc.sents))","df70ff5b":"print(doc.ents)","848f4051":"from spacy import displacy\ndisplacy.render(next(doc.sents), style='dep', jupyter=True,options = {'distance': 110})","7808b8a1":"displacy.render(doc, style='ent')","6e6b6f32":"#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz\n#!pip install en_core_web_sm\n#!pip install git+https:\/\/github.com\/NLPatVCU\/medaCy.git@development","7b27b998":"!pip install git+https:\/\/github.com\/NLPatVCU\/medaCy.git@development\n!pip install git+https:\/\/github.com\/NLPatVCU\/medaCy_model_clinical_notes.git","0e84e508":"Before we can take benefits from those massive amounts of data, we often have to face a challenge of how to grab the essential knowledge quickly. Fortunately, text summarization could be one solution to solve this problem. From its name, you may guess that text summarization is an approach that shortens long pieces of information into a shorter version. Generally speaking, there are two types of text summarization techniques. i.e., extractive and abstractive text summarization. Here I am not going to make a long discussion about text summarization itself. This kernel is still under the exploration stage. I may use a different approach to finish those tasks given by Kaggle. However, I will provide a quick demo of how to implement extractive text summarization to generate summaries for those articles that do not have an abstract. For simplification purposes, I will use the [bert-extractive-summarizer](https:\/\/pypi.org\/project\/bert-extractive-summarizer\/) module to summarize the text. \n\nIf you are interested in text summarization technique, please refer to this [blog](https:\/\/towardsdatascience.com\/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f).","32770a09":"**Bart Summarization**\n\nsource: Haggingface [Bart](https:\/\/huggingface.co\/transformers\/model_doc\/bart.html)\n\nMain features:\n* Bart uses a standard seq2seq\/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n\n* The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n\n* BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.","2e373a9d":"**LDA model**","8e44bd38":"**Bert Text Summarization**\nsource:[HuggingFace](https:\/\/pypi.org\/project\/bert-extractive-summarizer\/#description)","e4fa7be8":"The kernel is under updating. The exploration will contain three parts:\n\n1. Text summarization (Bart, Bert, GPT2, XLNet)-I did not run all the code. It seems some bugs existed at the moment. If you want to see how to run with Bert summarization, please check out my last version. This is the first time that I put all those summarizations in one kernel, not sure if there is any conflict among those modules.  \n2. LDA\n3. Spark NLP ?","980777be":"## Introduction\n\nSince the end of 2019, Coronavirus has been a hot topic that attracted huge attention all around the world. According to the [Economist](https:\/\/www.economist.com\/graphic-detail\/2020\/03\/20\/coronavirus-research-is-being-published-at-a-furious-pace), in the first quarter of 2020 alone, there are more than 1000 scientific papers published contained the word \"coronavirus.\"\n\n\n![Coronavirus research is being published at a furious pace](https:\/\/www.economist.com\/img\/b\/1280\/757\/85\/sites\/default\/files\/20200321_WOC751.png)","97620ec5":"**parse json data**","38ad9b7c":"* Module B: [medacy](https:\/\/github.com\/NLPatVCU\/medaCy)","6bd4fbc6":"**Named Entity Recognition (NER)**\n\nI have also tested three different modules for name entity recognitions. Each of them has its pros and cons. ","950f93c4":"## Data preprocessing-to be updated\n\nI will add one flow chart when having more time, but here are the main steps:\n\n1. Importing metadata and converting it to a dataframe meta(45774)\n2. Selecting four variables from meta dataset: \"paper_id2, \"title2, \"abstract\" and \"publish_time\", combining these four vairables\n   into a new dataset called meta_sm(45774)\n2. Removing data w\/o \"publish_id\" from metadata(left 31753)\n3. Removing duplicates from metadata (left 31272)\n4. Parsing data from json file (\"paper_id\", \"text_body\") and converting it to a dataframe df. (33375)\n5. Merging meta_sm and df by \"paper_id\" (inner join left 29636)\n6. Selecting papers based on two criteria: (left 1198)\n\n    * Contains \"COVID\",etc. key words in the text_body \n    * Published after \"2019-11-01\" \n    \n7. Removing papers that have less than 500 or more than 8000 words. (left 1082). Reason: Some papers have very short\n    \"text_body\", and those texts may contain only citations. \n8. Split dataset into train (has abstract  863 articles) and test (missing abstract 219 articles)","bde3a8e0":"## Text summarization","629eabb2":"Here is one example that I generated using the following code: \n\n[{'summary_text': ' coronavirus disease 2019 (covid-19) began in december 2019 in china leading to a public health emergency of international concern (pheic) clinical, laboratory, and imaging features have been partially characterized in some observational studies. We performed a systematic literature review with meta-analysis, using three databases to assess clinical, lab, imaging features, and outcomes of confirmed cases.'}] ref.[colab](https:\/\/gist.github.com\/dizzySummer\/0377bb6db284d3df45fdf75fe5394647#file-bart-summarization-ipynb)\n","4451305e":"* Module A: Spacy"}}