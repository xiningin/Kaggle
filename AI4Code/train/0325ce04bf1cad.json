{"cell_type":{"cb9a1002":"code","0ce51ace":"code","7ab67208":"code","602c5f38":"code","1b263c73":"code","8a42aab2":"code","aadec86b":"code","28ee9485":"code","ee158dc6":"code","aa77d447":"code","256ec25e":"code","327dbdf6":"code","2c95a82b":"code","9e599d70":"code","4e1cb09d":"code","339e357d":"code","7141591f":"code","9f68c1d2":"code","88329a89":"markdown","b9999c3d":"markdown","6014e040":"markdown","f7938eda":"markdown","5f7e2a8b":"markdown","897aa009":"markdown","08c6752c":"markdown","4b5cd5ab":"markdown","64cf0c47":"markdown","b0d43206":"markdown","69e27512":"markdown","3ee88bbe":"markdown"},"source":{"cb9a1002":"import numpy as np\nfrom keras.datasets import imdb\nfrom keras import preprocessing\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 10000)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=300)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=300)","0ce51ace":"from keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 8, input_length = 300)) \n\nmodel.add(Flatten())\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\nmodel.summary()","7ab67208":"history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)","602c5f38":"model.evaluate(x_train,y_train)","1b263c73":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"] # Training accuracy\nval_acc = history.history[\"val_acc\"] # Validation accuracy\nloss = history.history[\"loss\"] # Training loss\nval_loss = history.history[\"val_loss\"] # Validation loss\n\nepochs = range(1, len(acc) + 1) #plots every epoch, here 10\n\nplt.plot(epochs, acc, \"bo\", label = \"Training acc\") # \"bo\" gives dot plot\nplt.plot(epochs, val_acc, \"b\", label = \"Validation acc\") # \"b\" gives line plot\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, \"bo\", label = \"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\n\nplt.show()","8a42aab2":"import os\n\nimdb_dir = \"aclImdb\"\ntrain_dir = os.path.join(imdb_dir, \"train\")\n\nlabels = []\ntexts = []\n\nfor label_type in [\"neg\", \"pos\"]:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname [-4:] == \".txt\":\n            f = open(os.path.join(dir_name, fname))\n            f = open(os.path.join(dir_name, fname), encoding='utf-8')\n            texts.append(f.read())\n            f.close()\n            if label_type == \"neg\":\n                labels.append(0)\n            else:\n                labels.append(1)             ","aadec86b":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing. sequence import pad_sequences\n\ntraining_samples = 200 # Trains on 200 samples\nvalidation_samples = 10000 # Validates o 10000 samples\nmax_words = 10000 # Considers only the top 10000 words in the dataset\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index             \nprint(\"Found %s unique tokens.\" % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=300)\n\nlabels = np.asarray(labels)\nprint(\"Shape of data tensor:\", data.shape)\nprint(\"Shape of label tensor:\", labels.shape)\n\nindices = np.arange(data.shape[0]) # Splits data into training and validation set, but shuffles is, since samples are ordered: \n# all negatives first, then all positive\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_train = data[:training_samples] \ny_train = labels[:training_samples] \nx_val = data[training_samples:training_samples+validation_samples]\ny_val = labels[training_samples:training_samples+validation_samples]\n","28ee9485":"glove_dir = \"..\/input\"\n\nembeddings_index = {}\n\nf = open(os.path.join(glove_dir, \"glove.6B.200d.txt\"), encoding='utf-8') #added , encoding='utf-8'\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype=\"float32\")\n    embeddings_index[word] = coefs\nf.close()\n\nprint(\"found %s word vectors.\" % len (embeddings_index))","ee158dc6":"embedding_dim = 200 # GloVe contains 200-dimensional embedding vectors for 400.000 words\n\nembedding_matrix = np.zeros((max_words, embedding_dim)) # embedding_matrix.shape (10000, 200)\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word) # embedding_vector.shape (200,)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector # Words not found in the embedding index will all be zeros","aa77d447":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length = 300)) \nmodel.add(Flatten()) \nmodel.add(Dense(32, activation = \"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\")) \nmodel.summary()","256ec25e":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False ","327dbdf6":"model.compile(optimizer = \"rmsprop\", \n              loss = \"binary_crossentropy\", # in a multiclass problem categorical_crossentropy would be used\n              metrics = [\"acc\"]) \nhistory = model.fit(x_train, y_train,\n                   epochs = 10,\n                   batch_size = 32,\n                   validation_data = (x_val, y_val))\nmodel.save_weights(\"pre_trained_glove_model.h5\")        ","2c95a82b":"model.evaluate(x_train,y_train)","9e599d70":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"] # Training accuracy\nval_acc = history.history[\"val_acc\"] # Validation accuracy\nloss = history.history[\"loss\"] # Training loss\nval_loss = history.history[\"val_loss\"] # Validation loss\n\nepochs = range(1, len(acc) + 1) #plots every epoch, here 10\n\nplt.plot(epochs, acc, \"bo\", label = \"Training acc\") # \"bo\" gives dot plot\nplt.plot(epochs, val_acc, \"b\", label = \"Validation acc\") # \"b\" gives line plot\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, \"bo\", label = \"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\n\nplt.show()","4e1cb09d":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n\nbatch_size = 32\n\nprint (\"loading data ...\")\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words = 10000)\n\ninput_train = input_train[:5000]\ny_train = y_train[:5000]\ninput_test = input_test[:5000]\ny_test = y_test[:5000]\n\nprint(len(input_train), \"train sequences\")\nprint(len(input_test), \"test sequences\")\n\n\nprint(\"Pad sequences (samples x time)\")\ninput_train = sequence.pad_sequences(input_train, maxlen=500)\nprint(\"input_train shape:\", input_train.shape)\nprint(\"input_test shape:\", input_test.shape)","339e357d":"from keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer = \"rmsprop\",\n             loss = \"binary_crossentropy\",\n             metrics = [\"acc\"])\nhistory = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)","7141591f":"model.evaluate(input_train,y_train)","9f68c1d2":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"] # Training accuracy\nval_acc = history.history[\"val_acc\"] # Validation accuracy\nloss = history.history[\"loss\"] # Training loss\nval_loss = history.history[\"val_loss\"] # Validation loss\n\nepochs = range(1, len(acc) + 1) #plots every epoch, here 10\n\nplt.plot(epochs, acc, \"bo\", label = \"Training acc\") # \"bo\" gives dot plot\nplt.plot(epochs, val_acc, \"b\", label = \"Validation acc\") # \"b\" gives line plot\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, \"bo\", label = \"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\n\nplt.show()","88329a89":"#### Plotting the Results","b9999c3d":"### Using Pretrained Word Embeddings","6014e040":"### Loading the IMDB data.","f7938eda":"### LSTM example\n#### Preparing the IMDB data","5f7e2a8b":"#### Model Definition","897aa009":"#### Tokenizing the data\nBecause pretrained word embeddings are meant to be particularily useful on problems where little data is available, I will restrict the traing data to the first 200 samples only. Let's see how well it works! ","08c6752c":"### Number of Parameters\n\n**Embedding** = vocabulary size i.e. input x output = 10.000 x 8 = 80.000\n\n**Dense** = 300 i.e. input_length x Output + 1 = 300 x 8 + 1 =2.401\n\n*** Total params = 82.401***","4b5cd5ab":"Next, an embedding matrix is needed that can be loaded into an Embedding layer. The matrix' shape must be (max_words, embedding_dim), which is has a shape of a 10000 x 200 matrix. GloVe is 200 x 400000.\n#### Preparing the GloVe word embeddings matrix","64cf0c47":"### Using an Embedding layer and classifier on the IMDB data.\n\n\n**input_dim** Size of the vocabulary, i.e. maximum integer index + 1.\n\n**output_dim** Dimension of the dense embedding.--> Every word will be mapped onto a vector with, in this case, 8 elements. This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word.\n\n**input_length** Length of input sequences, when it is constant. This argument is required if you are going to connect \n <Flatten> then <Dense> layers upstream (without it, the shape of the dense outputs cannot be computed).\n\n --> After the Embedding layer, the activations have shape (samples, 300, 8)","b0d43206":"The Embedding layer now has a single weight matrix: a 2D float matrix where each entry is the word vector meant to be associated with index.\n#### Loading pretrained word embedding into the Embeddig layer\nSetting this to False makes sure the Embedding layer is non-trainable when calling it. \nIf you were to set trainable = True, then it will allow the optimization algorithm to modify the values of the word embeddings.\nPretrained parts shouldn't be updated during training, to avoid them forgetting what they already \"know\".","69e27512":"#### Training and Evaluating the Model","3ee88bbe":"### Downloading and Preprocessing the GloVe word embedding\nfile **glove.6B.zip** found in https:\/\/nlp.stanford.edu\/projects\/glove\/ <br>\nThe entire GloVe word embedding file is loaded into memory as a dictionary of word to embedding array; embeddings_index = {}"}}