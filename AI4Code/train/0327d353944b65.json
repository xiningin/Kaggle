{"cell_type":{"e7e54654":"code","624521f8":"code","d96945b6":"code","9605e9b9":"code","bb8b6839":"code","9bf4305d":"code","5e695070":"code","abf3e77b":"code","839bff75":"code","3532cf72":"code","669271b3":"code","867e16cb":"code","52dcb92e":"code","9ac1f4bf":"code","eed7946e":"code","4ff31c62":"code","42c6eeb7":"code","1636f754":"code","be015abb":"code","78951982":"code","34caddb0":"code","69531590":"code","5f470ea2":"code","4e51c618":"code","ff74a698":"code","f5cfbe9b":"code","a6b2db25":"code","ec256632":"code","af3fb6d5":"code","28b6485d":"code","64c17017":"code","ded821cc":"code","10ef65c4":"code","4cebb143":"code","ba5709b8":"code","f62937e1":"code","b39e918b":"code","ec21aece":"code","293cc526":"code","9a3403d0":"code","102361cb":"code","8cd9c206":"code","0c1fc754":"code","7364c4d7":"code","2f6966dd":"code","2982aac8":"code","b36991bb":"code","97fa33d4":"code","deed950c":"markdown","2b885b89":"markdown","6a86c861":"markdown","868cb4b2":"markdown","146ec1b5":"markdown","62a5456e":"markdown"},"source":{"e7e54654":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","624521f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","d96945b6":"df.info()","9605e9b9":"df.describe()","bb8b6839":"df.shape","9bf4305d":"df.columns","5e695070":"sns.countplot(x=\"Class\",data=df)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","abf3e77b":"import plotly.express as px\n","839bff75":"df1 = df.groupby(by=[\"Class\"]).size().reset_index(name=\"counts\")","3532cf72":"fig = px.bar(data_frame=df1, x=\"Class\", y=\"counts\", color=\"Class\", barmode=\"group\",title=\"Class Distributions \\n (0: No Fraud || 1: Fraud)\")\n\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"blue\",\n    title_font_family=\"Times New Roman\",\n    title_font_color=\"red\",\n    legend_title_font_color=\"green\"\n)\nfig.update_xaxes(title_font_family=\"Arial\")\nfig.show()\n","669271b3":"from matplotlib.pyplot import figure\ndef distribution_plot(df,feature):\n    \n    feature_value = df[feature].values\n    fig, ax = plt.subplots(1, 1, figsize=(18,4))\n    sns.distplot(df[feature], ax=ax, color=\"r\")\n    ax.set_title(f'Distribution of {feature}', fontsize=14)\n    ax.set_xlim([min(feature_value), max(feature_value)])","867e16cb":"columns = df.columns.tolist()\ncolumns.remove(\"Time\")\n","52dcb92e":"for name in columns:\n    distribution_plot(df,name)","9ac1f4bf":"from fitter import Fitter, get_common_distributions, get_distributions","eed7946e":"def distribution_info(df,features):\n    f = Fitter(df[features].values,\n           distributions=['gamma',\n                          'lognorm',\n                          \"beta\",\n                          \"burr\",\n                          \"norm\",\n                         'logistic',\n                         \"skewnorm\",\n                         \"lognorm\",\n                         \"loggamma\"])\n    f.fit()\n    print(f\"{features} distribution info\")\n    print(f.summary())\n    f.get_best(method = 'sumsquare_error')","4ff31c62":"for name in columns:\n    distribution_info(df,name)","42c6eeb7":"def plot_corelation(df):\n    corr = df.corr()\n    figure(figsize=(8, 6), dpi=80)\n    ax = sns.heatmap(\n        corr, \n        vmin=-1, vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True\n    )\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=45,\n        horizontalalignment='right'\n    );","1636f754":"df1 = df[[\"V1\",\"V5\"]]\nplot_corelation(df1)","be015abb":"df1 = df.copy()\ndf1.drop(\"Class\",inplace=True,axis=1)\ndf1.drop(\"Time\",inplace=True,axis=1)","78951982":"plot_corelation(df1)","34caddb0":"!pip install git+git:\/\/github.com\/AutoViML\/AutoViz.git\n!pip install xlrd","69531590":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Class', \n    dfte=df, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","5f470ea2":"df_ =df.sample(n=5000)","4e51c618":"from pandas_profiling import ProfileReport\nreport = ProfileReport(df_)\nreport","ff74a698":"!pip install sweetviz\n","f5cfbe9b":"import sweetviz as sv\nadvert_report = sv.analyze([df, 'Data'])\nadvert_report.show_html()","a6b2db25":"from IPython.display import HTML\nHTML(filename='.\/SWEETVIZ_REPORT.html')","ec256632":"from sklearn.model_selection import train_test_split","af3fb6d5":"y = df.pop(\"Class\")","28b6485d":"df.drop(\"Time\",inplace=True,axis=1)","64c17017":"X = df","ded821cc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","10ef65c4":"from sklearn.svm import SVC","4cebb143":"X_train.shape","ba5709b8":"model = SVC()","f62937e1":"model.fit(X_train,y_train)","b39e918b":"prediction = model.predict(X_test)","ec21aece":"from sklearn.metrics import classification_report,confusion_matrix","293cc526":"print(confusion_matrix(y_test,prediction))","9a3403d0":"print(classification_report(y_test,prediction))","102361cb":"#from sklearn.model_selection import GridSearchCV","8cd9c206":"#grid_param = {\"C\":[0.1,1,10,100,1000],\"gamma\":[1,0.1,0.01,0.001,0.0001]}","0c1fc754":"#grid = GridSearchCV(SVC(),grid_param,verbose=3)","7364c4d7":"#grid.fit(X_train,y_train)","2f6966dd":"#grid.best_params_","2982aac8":"#grid.best_estimator_","b36991bb":"#grid_predictions = grid.predict(X_test)","97fa33d4":"#print(confusion_matrix(y_test,prediction))\n#print(classification_report(y_test,prediction))","deed950c":"# Function For Visualization of Distribution of Features","2b885b89":"# let us run SVM in this Unlabeled data","6a86c861":"positive case 492.","868cb4b2":"# Using AutoViz EDA tool","146ec1b5":"# Find Best Distribution of Each Features","62a5456e":"---\n# This Notebook is just to Do EDA of the dataset\nand use the refined Data in different Algorithems\n---"}}