{"cell_type":{"a7496148":"code","5b245a28":"code","d78947bb":"code","ff0087f3":"code","2fb50bc5":"code","6c62694e":"code","03ab92c3":"code","cac2d21e":"code","24771663":"code","1aaa690f":"code","39468e48":"code","2308be9f":"code","d16ded19":"code","a36887ea":"code","c1c7dad1":"code","30ae956e":"code","27873e42":"code","c8acb92b":"code","cc3a743c":"markdown","262c4bc9":"markdown","f0d5ff75":"markdown","c0747eab":"markdown","1fe003de":"markdown","6685b082":"markdown","1519e52f":"markdown","f6dbc8fc":"markdown","27ea32b6":"markdown","d8401eb6":"markdown","d7d3e424":"markdown","2dccd59c":"markdown"},"source":{"a7496148":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b245a28":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","d78947bb":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_null_data(df,Title):\n    miss = df.isnull().sum()\/len(df)\n    miss = miss[miss> 0]*100\n    miss.sort_values(inplace=True)\n    print(miss)\n\n    #visualising missing values\n    miss = miss.to_frame()\n    miss.columns = ['count']\n    miss.index.names = ['Name']\n    miss['Name'] = miss.index\n\n    #plot the missing value count\n    sns.set(style=\"whitegrid\", color_codes=True)\n    sns.barplot(x = 'Name', y = 'count', data=miss)\n    plt.xticks(rotation = 90)\n    plt.title(Title)\n    plt.show()","ff0087f3":"visualize_null_data(train_data,\"Titanic Training Data - Null Value %\")","2fb50bc5":"train_data.loc[(train_data.Sex=='male') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='male']['Age'].median()\ntrain_data.loc[(train_data.Sex=='female') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='female']['Age'].median()\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)","6c62694e":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","03ab92c3":"visualize_null_data(test_data,\"Titanic Test Data Null Values %\")","cac2d21e":"test_data.loc[(test_data.Sex=='male') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='male']['Age'].median()\ntest_data.loc[(test_data.Sex=='female') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='female']['Age'].median()\ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())","24771663":"def build_model(df,y):\n    X = pd.get_dummies(df)\n\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n    model.fit(X, y)\n    \n    return X, model","1aaa690f":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures_4 = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX_features_4, model_features_4 = build_model(train_data[features_4],y)","39468e48":"features_7=[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\"]\n#Build the model with new features now\nX_features_7, model_features_7 = build_model(train_data[features_7],y)","2308be9f":"#Exploring explainable\nimport lime\nfrom lime import lime_tabular\n\ninterpretor_features_4=lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_features_4),\n    feature_names=X_features_4.columns,\n    mode='classification'\n)\n\nForExplainableData_features_4=X_features_4.copy()\nForExplainableData_features_4.insert(0,'Survived',y)","d16ded19":"#Exploring explainable\nimport lime\nfrom lime import lime_tabular\n\ninterpretor_features_7=lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_features_7),\n    feature_names=X_features_7.columns,\n    mode='classification'\n)\n\nForExplainableData_features_7=X_features_7.copy()\nForExplainableData_features_7.insert(0,'Survived',y)","a36887ea":"AnalysisRow1=132\nAnalysisRow2=183\nForExplainableData_features_7.iloc[[AnalysisRow1,AnalysisRow2],:]","c1c7dad1":"exp=interpretor_features_4.explain_instance(\n    data_row=ForExplainableData_features_4.iloc[AnalysisRow1][1:],\n    predict_fn=model_features_4.predict_proba\n)\nexp.show_in_notebook(show_table=True)","30ae956e":"exp=interpretor_features_7.explain_instance(\n    data_row=ForExplainableData_features_7.iloc[AnalysisRow1][1:],\n    predict_fn=model_features_7.predict_proba\n)\nexp.show_in_notebook(show_table=True)","27873e42":"exp=interpretor_features_4.explain_instance(\n    data_row=ForExplainableData_features_4.iloc[AnalysisRow2][1:],\n    predict_fn=model_features_4.predict_proba\n)\nexp.show_in_notebook(show_table=True)","c8acb92b":"exp=interpretor_features_7.explain_instance(\n    data_row=ForExplainableData_features_7.iloc[AnalysisRow2][1:],\n    predict_fn=model_features_7.predict_proba\n)\nexp.show_in_notebook(show_table=True) ","cc3a743c":"# We have reached Core Part now\n\nLIME stands for Local Interpretable Model-agnostic Explanations. It was developed by Marco Ribeiro in 2016. It helps in explaining predictions of Machine Learning models.\n\nLet us pass our input data (X) and the feature names to LimeTabularExplainer method.  We will pass them for both set of features as given below:\n* **4 Features:** \"Pclass\", \"Sex\", \"SibSp\", \"Parch\"\n* **7 Features:** \"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\"","262c4bc9":"Based on above explainable with 7 factors:\n\n**Factors contributing for Survived are:**\n* PClass <=2 (i.e. 2)\n* Fare is > 31 (Fare is 39 here)\n* Age is <= 22 (Age is 1 here)\n* Embarked location is NOT Queenstown\n* Number of parents \/ children aboard the Titanic (Parch) > zero (i.e. 1 here)\n\n**Factors contributing for Not Survived are:**\n* Passenger is Male\n* Passenger is Not Female\n* Number of siblings \/ spouses aboard the Titanic (SibSb) > one (i.e. 2 here)\n* Embarked location is Not Cherbourg (C)\n* Embarked location is Southampton (S)\n\n**As we clearly see now it has predicted correctly to Survived.**\n\n**Age, Fare and Embarked Location factors helped for the correct prediction.**","f0d5ff75":"If you submit the output file for this you will get a score of 0.77511.  \n\n**Kindly refer this code (please click [here](https:\/\/www.kaggle.com\/rajamykaggle\/top-2-score-through-simple-feature-engineering)), in this, I have explained how simple feature engineering can help us to reach Top 2% of Submission.  As per this code, for the above features, we will get a score of 0.77511**","c0747eab":"* Let us read the training & test data\n* Identify the missing data in both datasets\n* Impute them\n* Run the Random Forest Classifier (RFC) in the above data","1fe003de":"As we see in the above additional factors that we have added like Age, Fare and Embarked fields helped us to predict the correct Survival status.\n\nHope you liked the above analysis.\n\n## If you have liked please upvote!!!\n\n# Conclusion:\n\nThese types of explainable helps us to clearly explain to customer, how adding additional features helps on improving the accuracy of the prediction.  What do you feel?  Kindly let me know your comments!!!!","6685b082":"If you submit the output file for this you will get a score of 0.78229.\n\n**Kindly refer this code (please click [here](https:\/\/www.kaggle.com\/rajamykaggle\/top-2-score-through-simple-feature-engineering)), in this, I have explained how simple feature engineering can help us to reach Top 2% of Submission.** As per this code, for the above features, we will get a score of 0.78229.  **0.7% increase in our score.**","1519e52f":"Let us identify two records where RFC has not done the prediction correctly with 4 features but correctly identified with 7 features.  After analyzing the records, identified these two rows 132 & 183 to explain further.\n\nLets look into that:","f6dbc8fc":"Based on above explainable with four factors, it is predicted as Not Survived, which is not correct:\n\n**Factors contributing for Survived are:**\n* PClass <= 2\n* Number of parents \/ children aboard the Titanic (Parch) > zero\n\n**Factors contributing for Not Survived are:**\n* Passenger is Male\n* Passenger is Not Female\n* Number of siblings \/ spouses aboard the Titanic (SibSb) > 1","27ea32b6":"Based on above explainable with four factors, it is predicted as Survived, which is not correct:\n\n**Factors contributing for Survived are:**\n* Passenger is Female\n* Passenger is Not Male\n\n**Factors contributing for Not Survived are:**\n* PClass is 3\n* Number of parents \/ children aboard the Titanic (Parch) <= zero\n* Number of siblings \/ spouses aboard the Titanic (SibSb) <= one and > 0\n\n","d8401eb6":"Based on above explainable with 7 factors:\n\n**Factors contributing for Survived are:**\n* Passenger is Female\n* Passenger is Not Male\n* Embarked location is NOT Queenstown (Q)\n* Fare is between 14.45 and 31 (Fare is 14.5 here)\n\n**Factors contributing for Not Survived are:**\n* PClass is 3\n* Age is > 35\n* Embarked location is Southampton (S)\n* Embarked location is NOT Cherbourg (C)\n* Number of parents \/ children aboard the Titanic (Parch) <= zero\n* Number of siblings \/ spouses aboard the Titanic (SibSb) is one\n\n\n**As we clearly see now it has predicted correctly to Not Survived.**\n\n**Age, Fare and Embarked Location factors helped for the correct prediction.**","d7d3e424":"Passenger 132 (As we don't have passenger name, representing 132th row as Passenger 132) not survived & Passenger 183 survived.  Now let us try to interpret for both of the rows using LIME.\n\nAs we notice, differences between both records are:\n1. For passenger 132, PClass is 3, # of siblings \/ spouses aboard the Titanic (SibSb) is one, # of parents \/ children aboard the Titanic (Parch) is zero, Age is 47, Fare is 14.5 & Gender is Female.\n2. For passenger 183, PClass is 2,  # of siblings \/ spouses aboard the Titanic (SibSb) is two, # of parents \/ children aboard the Titanic (Parch) is one, Age is 1, Fare is 39 & Gender is Male.","2dccd59c":"### **Explainable Machine Learning**\n\nDear All\n\nThis is Part 3 of my Explainable Machine Learning Series.  If you have not gone through Part 1, kindly go through the same by clicking [Explainable ML Part1](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part1), for Part 2, please click [here](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part2).\n\nIf you like this, ***kindly upvote that motivates*** me to continue writing and sharing.\n\nOne of the **key challenges in the Machine Learning** ***is the explainability of the ML model***. When we show the outcome of the model, how customer will understand what are the factors that are influencing the final outcome (i.e. prediction done by ML Model).\n\nCustomer could see this as a black box.  For example, let us say our ML model is predicting the end customer who will subscribe for a particular banking product.  When we show our ML Model result to our customer, their question could be, out of the given factors (dependent variables), which are the influencing factors that will help us to decide whether a particular end customer will subscribe to a product or not.  \n\nAs a data scientist how do we answer to this question?\n\n**Explainable AI helps to answer above question.**\n\nWe will continue where we have left in Part-2."}}