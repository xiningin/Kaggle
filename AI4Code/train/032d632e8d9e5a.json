{"cell_type":{"c349bd99":"code","04fcf3c3":"code","c4030ae1":"code","eb4124ab":"code","5663842b":"code","10880efb":"code","f824f155":"code","0eb27c8b":"code","6f98c553":"code","5f57c792":"code","53a79065":"markdown","1d6103ed":"markdown","20c0e98b":"markdown","3c0a8555":"markdown","852dcc8a":"markdown","deceb4b6":"markdown"},"source":{"c349bd99":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","04fcf3c3":"plt.style.use('bmh')\nplt.rcParams['figure.figsize'] = (20, 10)\ntitle_config = {'fontsize': 20, 'y': 1.05}","c4030ae1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","eb4124ab":"X_train = train.iloc[:, 2:].values.astype('float64')\ny_train = train['target'].values\nX_test = test.iloc[:, 1:].values.astype('float64')","5663842b":"from sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\n\nfor i in range(2):\n    fig, axes = plt.subplots(3, 6)\n    axes = axes.ravel()\n    for j in range(len(axes)):\n        feature = StandardScaler().fit_transform(X_train[y_train == i, j:j + 1])\n        hist = axes[j].hist(feature, bins='auto', histtype='step',\n                            linewidth=2, density=True)\n        grid = np.linspace(feature.min(), feature.max(), num=1000)\n        log_density = (GaussianMixture(n_components=10, reg_covar=0.03)\n                       .fit(feature).score_samples(grid[:, None]))\n        gmm = axes[j].plot(grid, np.exp(log_density))\n        axes[j].set_title(f'var_{j}', **title_config)\n        axes[j].set_ylim([0, 1])\n    fig.suptitle(f'Histogram vs Gaussian Mixture Model for Class {i}',\n                 **title_config)\n    fig.legend((hist[2][0], gmm[0]), ('Histogram', 'Gaussian mixture model'),\n               loc='upper center', bbox_to_anchor=(0.5, 1), ncol=2, fontsize=14)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.88)","10880efb":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom scipy.special import logsumexp\n\nclass GaussianMixtureNB(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_components=1, reg_covar=1e-06):\n        self.n_components = n_components\n        self.reg_covar = reg_covar\n    def fit(self, X, y):\n        self.log_prior_ = np.log(np.bincount(y) \/ len(y))\n        # shape of self.log_pdf_\n        shape = (len(self.log_prior_), X.shape[1])\n        self.log_pdf_ = [[GaussianMixture(n_components=self.n_components,\n                                          reg_covar=self.reg_covar)\n                          .fit(X[y == i, j:j + 1])\n                          .score_samples for j in range(shape[1])]\n                         for i in range(shape[0])]\n    def predict_proba(self, X):\n        # shape of log_likelihood before summing\n        shape = (len(self.log_prior_), X.shape[1], X.shape[0])\n        log_likelihood = np.sum([[self.log_pdf_[i][j](X[:, j:j + 1])\n                                  for j in range(shape[1])]\n                                 for i in range(shape[0])], axis=1).T\n        log_joint = self.log_prior_ + log_likelihood\n        return np.exp(log_joint - logsumexp(log_joint, axis=1, keepdims=True))\n    def predict(self, X):\n        return self.predict_proba(X).argmax(axis=1)","f824f155":"from sklearn.model_selection import StratifiedShuffleSplit\n\ni_train, i_valid = next(StratifiedShuffleSplit(n_splits=1).split(X_train, y_train))","0eb27c8b":"from sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score\n\npipeline = make_pipeline(StandardScaler(),\n                         GaussianMixtureNB(n_components=10, reg_covar=0.03))\npipeline.fit(X_train[i_train], y_train[i_train])\nprint('Training AUC is {}.'\n      .format(roc_auc_score(y_train[i_train],\n                            pipeline.predict_proba(X_train[i_train])[:, 1])))\nprint('Validation AUC is {}.'\n      .format(roc_auc_score(y_train[i_valid],\n                            pipeline.predict_proba(X_train[i_valid])[:, 1])))","6f98c553":"pipeline.fit(X_train, y_train)","5f57c792":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = pipeline.predict_proba(X_test)[:, 1]\nsubmission.to_csv('submission.csv', index=False)","53a79065":"# Submitting the Test Predictions\n\nWe retrain using all the data and submit the test predictions for the competition.","1d6103ed":"# Introduction\n\nIn this kernel, we attempt to improve the [Gaussian naive Bayes classifier](https:\/\/www.kaggle.com\/blackblitz\/gaussian-naive-bayes) by replacing the Gaussian model with the more flexible Gaussian mixture model.\n\nWe implement the Gaussian mixture naive Bayes model to predict Santander Customer Transaction Prediction data. The problem has a binary target and 200 continuous features, and we assume that these features are conditionally independent given the class. We model the target $Y$ as Bernoulli, taking values $0$ (negative) and $1$ (positive). The features $X_0,X_1,\\ldots,X_{199}$ are modelled as continuous random variables. Recall the Bayes rule:\n\n$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$\n\nThe prior $p_Y(y)$ will be taken as the proportion of the two classes, and the likelihood $f_{X_i|Y}(x_i|y)$ will be obtained by fitting the data with the Gaussian mixture model.","20c0e98b":"# Training and Evaluating the Model\n\nWe train and evaluate the model by using the training AUC and validation AUC. The process can take time because we train the Gaussian mixture model 400 times, and the training time will increase with higher `n_components`. In order to speed up the hyperparameter search, we use validation, which is k times faster than k-fold cross-validation. Feel free to use cross-validation if you have the time (and tell me if you find better hyperparameters).","3c0a8555":"# Getting Acquainted with the Gaussian Mixture Model\nThe Gaussian mixture model gives a mixture of normal distrubutions. We can use [sklearn.mixture.GaussianMixture](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.mixture.GaussianMixture.html) to fit the data and compare it with the histogram to get a feel of its behaviour. We also need to standardize the features because too narrow data can impair the fitting ability of the Gaussian mixture model. There are two important hyperparameters: `n_components` is the number of normal distributions to mix in and `reg_covar` is a regularization parameter that controls the spread of the bumps. Note that `score_samples` method gives the log density, so we need to exponentiate to get the density.","852dcc8a":"# Conclusion\n\nThe Gaussian mixture naive Bayes performs very well and is an improvement over the Gaussian naive Bayes, although it takes a little more time to train. It has the advantage that it is more flexible and does not require that the data come from a normal distribution. The only assumption is that the features are conditionally independent given the class. An alternative approach is to use kernel density estimation. Whichever method we use, the goal is to have a model that is simple (easily understood), tractable (easily computed) and accurate (represents reality very well).","deceb4b6":"# Implementing the Model\n\nWe are going to use the Gaussian mixture model to estimate the likelihood probability density functions $f_{X_i|Y}(x_i|y)$. Since multiplying a lot of small numbers will lead to underflow, we take the logarithm and turn products into sums:\n$$\\ln p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\underbrace{\\overbrace{\\ln p_Y(y)}^\\text{log prior}+\\overbrace{\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y)}^\\text{log likelihood}}_\\text{log joint}-\\overbrace{\\ln\\sum_{y'=0}^1e^{\\ln p_Y(y')+\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y')}}^\\text{log marginal}$$\n\nKey points in the implementation are:\n* The log prior is set as the logarithm of the proportion of different classes.\n* The log likelihood is computed by using [sklearn.mixture.GaussianMixture](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.mixture.GaussianMixture.html)'s `score_samples` method.\n* Computing the log marginal is prone to overflow\/underflow, so we use [scipy.special.logsumexp](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.logsumexp.html) to avoid that.\n* In the end, we convert back to probability by exponentiation.\n\nAll the heavy lifting is done by the Gaussian mixture model. The rest of the computation is very simple and fast."}}