{"cell_type":{"d05db24f":"code","52bfc152":"code","572bcee5":"code","0db3c033":"code","87ab4c8d":"code","a3b395c2":"code","1cd6a128":"code","ba3da6fa":"code","28d40ad3":"code","261dba71":"code","f3b2a615":"code","864625cc":"code","64872016":"code","8a7322dc":"code","2bac5fad":"code","b71fb4cf":"code","9b565fad":"code","803e1a23":"code","8441ebcb":"code","d29ec8eb":"code","d0c2445e":"code","3b881fff":"code","8446facb":"code","75324d7d":"code","6f296532":"code","807fa4fb":"code","c7fcf099":"code","989a87fe":"code","b24ed093":"code","a8202b94":"code","9780ad49":"code","906b3029":"code","7fe7c026":"code","bf055cbe":"code","85d08cce":"code","6ab99a6b":"code","7610c32b":"code","dd615f03":"code","3ef21277":"code","240223de":"code","1df2f6e6":"code","b38cd383":"code","9d96cae2":"code","2c55e55d":"code","629bd869":"code","468b74cb":"code","1cc53776":"code","a227b170":"code","f668c4d3":"code","ee5bfa38":"code","ac6a6236":"code","fe9afdcd":"code","fc6af5b5":"code","cc3013a8":"code","39262b92":"code","684409c4":"code","bac4c503":"code","6a820a8c":"code","b2748c3e":"code","ec84012c":"code","f8045b5c":"code","677fd1c9":"code","a45e44c3":"code","9a6ef1b3":"code","a21e915b":"code","cf3c210d":"code","77645da5":"code","62d56243":"code","edc2d961":"code","df3566ac":"code","b2874fbe":"code","81384324":"markdown","288672f3":"markdown","bcbcbf33":"markdown","40196ce5":"markdown","0bbc7f5b":"markdown","ff34853d":"markdown","a308983d":"markdown","b6434de9":"markdown","001a774e":"markdown","3f4a7d2b":"markdown","398d6377":"markdown","86cffda1":"markdown","5aedd54f":"markdown","e13a82f1":"markdown","9d9cc58a":"markdown","55c1fc8a":"markdown","9f351ca2":"markdown","ed009250":"markdown","a843cd27":"markdown","637bc34e":"markdown","ec733ab8":"markdown","50f1022a":"markdown"},"source":{"d05db24f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics,preprocessing\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport scipy.stats as stats\nfrom scipy.cluster.hierarchy import dendrogram,linkage\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.decomposition import PCA","52bfc152":"pro = pd.read_csv('..\/input\/Bank_Personal_Loan_Modelling-1.xlsx')\npro.head()","572bcee5":"## Checking the shape of the dataset\n\npro.shape","0db3c033":"pro.columns","87ab4c8d":"pro.rename(columns = {'Personal Loan':'Personal_Loan','ZIP Code':'ZIP_Code',\n                      'Securities Account':'Securities_Account','CD Account':'CD_Account'},inplace=True)","a3b395c2":"pro.isnull().sum()","1cd6a128":"## Checking for outliers using describe\n\npro.describe().transpose()","ba3da6fa":"sns.boxplot(pro['Age'])   # No outliers in Age Feature","28d40ad3":"sns.boxplot(pro['Experience'])    # No outliers in this feature","261dba71":"sns.boxplot(pro['Income'])   # There are few outliers present in the Income feature. We will treat them later","f3b2a615":"sns.boxplot(pro['Family'])   # No outliers","864625cc":"sns.boxplot(pro['CCAvg'])    # Outliers are present","64872016":"sns.boxplot(pro['Mortgage'])   # Many outliers are present","8a7322dc":"up_whisk=pro[\"Income\"].quantile(0.75)+1.5*(pro[\"Income\"].quantile(0.75) - pro[\"Income\"].quantile(0.25))\n\nfor i in pro['Income']:\n    if(i > up_whisk):\n        pro['Income']=pro['Income'].replace(i,up_whisk)","2bac5fad":"sns.boxplot(pro['Income'])  # From the box plot we can now see that the outliers from Income feature has been treated.","b71fb4cf":"# Now we will drop the ID feature and ZIP Code feature as they are unrelated to our dataset\n\npro.drop(['ID','ZIP_Code'],axis = 1, inplace=True)","9b565fad":"sns.countplot(x = pro['Family'])  # From the count plot, all families have almost equal members in it.","803e1a23":"sns.countplot(x = pro['Education'])  # Most of the persons in the dataset are undergrads","8441ebcb":"sns.countplot(x = pro['Securities_Account'])  # More number of people do not have securities account at the bank","d29ec8eb":"sns.countplot(x = pro['CD_Account']) # more customers have no CD account","d0c2445e":"sns.countplot(x = pro['Online'])  # More number of customers use online transaction compared to offline transactions","3b881fff":"sns.countplot(x = pro['CreditCard']) # Not many people use credit card issued by the bank","8446facb":"sns.countplot(x = pro['Personal_Loan'])  # From the graph, it is very clear that only very few customers get personal loan from the bank.\nplt.title('Personal Loan Details')","75324d7d":"sns.boxplot(y='Income',x='Personal_Loan',data = pro)","6f296532":"sns.boxplot(y='CCAvg',x='Personal_Loan',data = pro)","807fa4fb":"# We will now plot a pair plot for the remaining features in the dataset\n\nsns.pairplot(pro)","c7fcf099":"# Since the pairplot shows the correlation among all the features in a graphical way, it becomes difficult to interpret using it.\n# So, we will check on the correlation using heatmap and correlation matrix","989a87fe":"plt.figure(figsize=(10,10))\nsns.heatmap(pro.corr(),annot = True)","b24ed093":"# From the perspective of Personal Loan accpeted by the customer, we can filter the corrmap with only those who accepted the\n# personal loan\n\npro.corr().loc['Personal_Loan']","a8202b94":"# Dropping the dependant variable from dataset for unsupervised learning\n\nx = pro.drop('Personal_Loan',axis=1)\ny_original = pro['Personal_Loan']","9780ad49":"# First we will proceed with unsupervised learning using K-means clustering method\n# For that we need to determine the number of clusters using the elbow graph\n\ncluster_range = range(1,11)\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters)\n    clusters.fit(x)\n    cluster_errors.append(clusters.inertia_)","906b3029":"clusters_df = pd.DataFrame({'num_clusters':cluster_range,'cluster_errors':cluster_errors})\nclusters_df[0:11]","7fe7c026":"sns.pointplot(x=clusters_df.num_clusters,y=clusters_df.cluster_errors)","bf055cbe":"# Now we will fir the kmean with the newly found number of clusters.\n\nkmean = KMeans(n_clusters=2)\nkmean.fit(x)\n\ncenters = kmean.cluster_centers_","85d08cce":"y_kmean = list(kmean.labels_)","6ab99a6b":"# Now we will check the unsupervised learning for the same dataset with Hierarchial clustering method.\n# We will find the no of clusters using dendrogram\n\nz = linkage(x,'ward')\ndendrogram(z)\nplt.show()","7610c32b":"plt.figure(figsize=(25,10))\nplt.title('Hierarchial Clustering Dendrogram')\ndendrogram(z, leaf_rotation=90,leaf_font_size=8)\nplt.show()","dd615f03":"# We will truncate the dendrogram for easier visibility.\n\nplt.figure(figsize=(25,10))\nplt.title('Hierarchial Clustering Dendrogram (Truncated)')\ndendrogram(z,truncate_mode='lastp',p = 16,show_leaf_counts=True, leaf_rotation=90,leaf_font_size=12,show_contracted=True)\nplt.show()","3ef21277":"cluster = AgglomerativeClustering(n_clusters = 2, linkage = 'ward')\ny_hier = cluster.fit_predict(x)","240223de":"print(metrics.accuracy_score(y_original,y_kmean))","1df2f6e6":"print(metrics.accuracy_score(y_original,y_hier))","b38cd383":"# First we will perform PCA using all the available features in the dataset. We will find the required number of components using\n# elbow graph and we will fit it using the found number of clusters","9d96cae2":"pca = PCA(n_components=11)\npca.fit_transform(x)","2c55e55d":"plt.figure(figsize=(10,10))\nplt.plot(pca.explained_variance_ratio_,marker='o')\nplt.xlabel('number_of_components')\nplt.ylabel('cumulative explained variance')\nplt.xticks(range(12))\nplt.show()","629bd869":"pca_new = PCA(n_components=2)","468b74cb":"# We will split the train test data with x as unscaled x, y as y obtained from k_mean, and we will fit x data with pca\n\nx_train1,x_test1,y_train1,y_test1 = train_test_split(x,y_kmean,test_size = 0.3, random_state = 0)\nx_train1 = pca_new.fit_transform(x_train1)\nx_test1 = pca_new.transform(x_test1)","1cc53776":"rf = RandomForestClassifier()\nrf.fit(x_train1,y_train1)\ny_pred1_rf = rf.predict(x_test1)\nrandom_forest_unscaled = metrics.accuracy_score(y_test1,y_pred1_rf)\nprint(metrics.accuracy_score(y_test1,y_pred1_rf))","a227b170":"log = LogisticRegression()\nlog.fit(x_train1,y_train1)\ny_pred1_log = log.predict(x_test1)\nlog_unscaled = metrics.accuracy_score(y_test1,y_pred1_log)\nprint(metrics.accuracy_score(y_test1,y_pred1_log))","f668c4d3":"tree = DecisionTreeClassifier(max_depth = 4)\ntree.fit(x_train1,y_train1)\ny_pred1_tree = tree.predict(x_test1)\ntree_unscaled = metrics.accuracy_score(y_test1,y_pred1_tree)\nprint(metrics.accuracy_score(y_test1,y_pred1_tree))","ee5bfa38":"knn = KNeighborsClassifier()\nknn.fit(x_train1,y_train1)\ny_pred1_knn = knn.predict(x_test1)\nknn_unscaled = metrics.accuracy_score(y_test1,y_pred1_knn)\nprint(metrics.accuracy_score(y_test1,y_pred1_knn))","ac6a6236":"nb = GaussianNB()\nnb.fit(x_train1,y_train1)\ny_pred1_nb = nb.predict(x_test1)\nnb_unscaled = metrics.accuracy_score(y_test1,y_pred1_nb)\nprint(metrics.accuracy_score(y_test1,y_pred1_nb))","fe9afdcd":"svm = SVC()\nsvm.fit(x_train1,y_train1)\ny_pred1_svm = svm.predict(x_test1)\nsvm_unscaled = metrics.accuracy_score(y_test1,y_pred1_svm)\nprint(metrics.accuracy_score(y_test1,y_pred1_svm))","fc6af5b5":"x_std = preprocessing.StandardScaler().fit_transform(x)","cc3013a8":"pca = PCA(n_components=11)\npca.fit_transform(x_std)","39262b92":"plt.figure(figsize=(10,10))\nplt.plot(pca.explained_variance_ratio_,marker='o')\nplt.xlabel('number_of_components')\nplt.ylabel('cumulative explained variance')\nplt.xticks(range(12))\nplt.show()","684409c4":"pca_new_scaled = PCA(n_components=3)","bac4c503":"# We will split the train test data with x as scaled x, y as y obtained from k_mean, and we will fit x data with pca\n\nx_train2,x_test2,y_train2,y_test2 = train_test_split(x_std,y_kmean,test_size = 0.3, random_state = 0)\nx_train2 = pca_new.fit_transform(x_train2)\nx_test2 = pca_new.transform(x_test2)","6a820a8c":"rf2 = RandomForestClassifier()\nrf2.fit(x_train2,y_train2)\ny_pred2_rf = rf2.predict(x_test2)\nrandom_forest_scaled = metrics.accuracy_score(y_test2,y_pred2_rf)\nprint(metrics.accuracy_score(y_test2,y_pred2_rf))","b2748c3e":"log2 = LogisticRegression()\nlog2.fit(x_train2,y_train2)\ny_pred2_log = log2.predict(x_test2)\nlog_scaled = metrics.accuracy_score(y_test2,y_pred2_log)\nprint(metrics.accuracy_score(y_test2,y_pred2_log))","ec84012c":"tree2 = DecisionTreeClassifier(max_depth = 4)\ntree2.fit(x_train2,y_train2)\ny_pred2_tree = tree2.predict(x_test2)\ntree_scaled = metrics.accuracy_score(y_test2,y_pred2_tree)\nprint(metrics.accuracy_score(y_test2,y_pred2_tree))","f8045b5c":"knn2 = KNeighborsClassifier()\nknn2.fit(x_train2,y_train2)\ny_pred2_knn = knn2.predict(x_test2)\nknn_scaled = metrics.accuracy_score(y_test2,y_pred2_knn)\nprint(metrics.accuracy_score(y_test2,y_pred2_knn))","677fd1c9":"nb2 = GaussianNB()\nnb2.fit(x_train2,y_train2)\ny_pred2_nb = nb2.predict(x_test2)\nnb_scaled = metrics.accuracy_score(y_test2,y_pred2_nb)\nprint(metrics.accuracy_score(y_test2,y_pred2_nb))","a45e44c3":"svm2 = SVC()\nsvm2.fit(x_train2,y_train2)\ny_pred2_svm = svm2.predict(x_test2)\nsvm_scaled = metrics.accuracy_score(y_test2,y_pred2_svm)\nprint(metrics.accuracy_score(y_test2,y_pred2_svm))","9a6ef1b3":"# We will start with train test split of x and y with y as y_original from the original dataset\n\nx_train3,x_test3,y_train3,y_test3 = train_test_split(x,y_original,test_size = 0.3, random_state = 0)","a21e915b":"rf3 = RandomForestClassifier()\nrf3.fit(x_train3,y_train3)\ny_pred3_rf = rf3.predict(x_test3)\nrandom_forest_original = metrics.accuracy_score(y_test3,y_pred3_rf)\nprint(metrics.accuracy_score(y_test3,y_pred3_rf))","cf3c210d":"log3 = LogisticRegression()\nlog3.fit(x_train3,y_train3)\ny_pred3_log = log3.predict(x_test3)\nlog_original = metrics.accuracy_score(y_test3,y_pred3_log)\nprint(metrics.accuracy_score(y_test3,y_pred3_log))","77645da5":"tree3 = DecisionTreeClassifier(max_depth = 4)\ntree3.fit(x_train3,y_train3)\ny_pred3_tree = tree3.predict(x_test3)\ntree_original = metrics.accuracy_score(y_test3,y_pred3_tree)\nprint(metrics.accuracy_score(y_test3,y_pred3_tree))","62d56243":"knn3 = KNeighborsClassifier()\nknn3.fit(x_train3,y_train3)\ny_pred3_knn = knn3.predict(x_test3)\nknn_original = metrics.accuracy_score(y_test3,y_pred3_knn)\nprint(metrics.accuracy_score(y_test3,y_pred3_knn))","edc2d961":"nb3 = GaussianNB()\nnb3.fit(x_train3,y_train3)\ny_pred3_nb = nb3.predict(x_test3)\nnb_original = metrics.accuracy_score(y_test3,y_pred3_nb)\nprint(metrics.accuracy_score(y_test3,y_pred3_nb))","df3566ac":"svm3 = SVC()\nsvm3.fit(x_train3,y_train3)\ny_pred3_svm = svm3.predict(x_test3)\nsvm_original = metrics.accuracy_score(y_test3,y_pred3_svm)\nprint(metrics.accuracy_score(y_test3,y_pred3_svm))","b2874fbe":"accuracy = pd.DataFrame([[random_forest_unscaled,random_forest_scaled,random_forest_original],\n                         [log_unscaled,log_scaled,log_original],\n                         [tree_unscaled,tree_scaled,tree_original],\n                         [knn_unscaled,knn_scaled,knn_original],\n                         [nb_unscaled,nb_scaled,nb_original],\n                         [svm_unscaled,svm_scaled,svm_original]],\n                       columns = ['Unscaled Data with PCA','Scaled Data with PCA','Supervised Learning'],\n                       index = ['Random Forest Model','Logistic Regression Model','Decision Tree Model','KNN Model',\n                               'Naive Bayes Model','Support Vector Machine Model'])\naccuracy","81384324":"#### We will now perform Agglomerative Clustering using the number of clusters obtained above","288672f3":"#### Now we will again repeat the steps above starting from fitting the data in PCA","bcbcbf33":"### From the description provided in the dataset, we can consider 'Personal Loan' as the target variable","40196ce5":"#### From the dendrogram it is clear that the number of clusters in the given dataset is 2.","0bbc7f5b":"#### Thus the accuracy scores for all 6 models using Unscaled data is shown above","ff34853d":"###### There are no null values present in the given dataset","a308983d":"## Now we will scale the data using Standard Scalar","b6434de9":"### From the elbow graph it is clear that the number of components required for PCA is 2","001a774e":"# Unsupervised  Learning - Without Scaling Data","3f4a7d2b":"## Now we will verify the above accuracy scores of unscaled data and scaled data using target as kmean with using original y as target as done in supervised learning.","398d6377":"## From the above comparisons of the accuracy scores, we can conclude that the accuracy of Random Forest Model and Decision Tree Model using unscaled data with PCA is very much higher than the rest of the models and also higher than scaled data and using original target variable.","86cffda1":"## By comparing the accuracy scores obtained from K-mean and Hierarical with original target, we can see that using K-Means provide better accuracy. Hence we will use K-means Clustering from hereon.","5aedd54f":"#### From the correlation map and pairplot graph, we can infere that Income, CCAvg and CD Account are more corelated to Personal Loan and hence they are the most important features for this dataset","e13a82f1":"###### From the above table, we can infere that income and Mortgage features have outliers in them.","9d9cc58a":"### Descriptive Statistics","55c1fc8a":"We will now treat the outliers present in income feature. Since Mortgage and CCAvg are important variables related to our dataset, we are not going to remove the outliers in those two features. We will treat the outliers in Income feature by capping them with upper whisker.","9f351ca2":"### Loading the dataset","ed009250":"### Importing the required packages","a843cd27":"#### From the above elbow graph, the number of components in scaled data is 3","637bc34e":"##### Thus the accuracy of all 6 models using scaled data is shown above","ec733ab8":"###### From the above graphs, we can how many outliers are present in each feature of the dataset. Most of the outliers are in Income, Mortgage, and CCAvg","50f1022a":"#### From the Elbow graph we can see that the number of clusters for unscaled data is 2."}}