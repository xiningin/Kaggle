{"cell_type":{"0c0c7696":"code","b900c928":"code","a6142e0e":"code","dc8bb19d":"code","a2ea3320":"code","65a5abac":"code","c906e98e":"code","10d964bc":"code","5246369f":"code","c9c61c09":"code","0dd51953":"code","5cd99ed9":"code","04687de2":"code","310d3bef":"code","2f301a68":"code","58473ff7":"code","0bdd7d71":"code","e144871b":"code","599f1168":"code","5dc6e04e":"code","9782caf4":"code","d4da25df":"code","23f84fb9":"code","8feda2e5":"code","35c3c644":"code","68dccec5":"code","acb1c16b":"code","992cd76c":"code","494c15db":"code","26be8b2d":"code","6c301d46":"code","97fe5057":"code","3811fd6a":"code","1a6fc32b":"code","000f8fbd":"code","533ddf5c":"code","47d85d60":"code","d2a81169":"code","cba2f151":"code","4c841a98":"markdown","418f485c":"markdown","3f3f6ab3":"markdown","cd8da9d0":"markdown","6e2b8d93":"markdown","8b7ae1e2":"markdown","195878d7":"markdown","69a12299":"markdown","ac7e21ee":"markdown","f4223283":"markdown","1aa17ecc":"markdown","5a9a5e84":"markdown","52bc6ada":"markdown","1a541a18":"markdown","73527fe4":"markdown","35b73207":"markdown","6c58db96":"markdown","c49fe1b3":"markdown","c0c6c559":"markdown","60a75da3":"markdown","c76471bd":"markdown","5aaa2adb":"markdown","7f34b8bc":"markdown","51230978":"markdown","9bab8ef1":"markdown","dd009417":"markdown","f38c1ae4":"markdown","1c73b6cb":"markdown","474bbb56":"markdown","4d38e767":"markdown","5e6c8eb5":"markdown","5f0c5c07":"markdown","c25f240e":"markdown","2b10ddf8":"markdown","20f3fd41":"markdown","44f2b309":"markdown","50eae685":"markdown","4acd1ed6":"markdown","42221e50":"markdown","73e79ba8":"markdown","64bd7a1d":"markdown","9ea74985":"markdown","c054583f":"markdown","95046c1d":"markdown","33507ff8":"markdown","d36b8925":"markdown","f5d8cc85":"markdown","f6503f20":"markdown","3dd158ed":"markdown","33cd248c":"markdown","d45c3023":"markdown","b26c855f":"markdown"},"source":{"0c0c7696":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b900c928":"import string \nimport re\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","a6142e0e":"train_df = pd.read_csv('\/kaggle\/input\/penyisihan-datavidia-7-0\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/penyisihan-datavidia-7-0\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/penyisihan-datavidia-7-0\/sample_submission.csv')","dc8bb19d":"train_df.info()","a2ea3320":"test_df.info()","65a5abac":"train_df.head()","c906e98e":"test_df.head()","10d964bc":"train_df = train_df.drop(['review_id'], axis=1)\ntest_df = test_df.drop(['review_id'], axis=1)","5246369f":"train_df['review_text'] = train_df['review_text'].str.lower()\ntest_df['review_text'] = test_df['review_text'].str.lower()","c9c61c09":"train_df['review_text'] = train_df['review_text'].replace('\\n',' ', regex=True)\ntest_df['review_text'] = test_df['review_text'].replace('\\n',' ', regex=True)","0dd51953":"train_df['review_text'] = train_df['review_text'].str.replace('\\d+', ' ')\ntest_df['review_text'] = test_df['review_text'].str.replace('\\d+', ' ')","5cd99ed9":"#remove punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_punctuation)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_punctuation)\n\n#remove whitespace leading & trailing\ndef remove_whitespace_LT(text):\n    return text.strip()\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_whitespace_LT)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_whitespace_LT)\n\n#remove multiple whitespace into single whitespace\ndef remove_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_whitespace_multiple)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_whitespace_multiple)\n\n# remove single char\ndef remove_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n\ntest_df['review_text'] = test_df['review_text'].apply(remove_singl_char)\ntrain_df['review_text'] = train_df['review_text'].apply(remove_singl_char)","04687de2":"# NLTK word rokenize \ndef word_tokenize_wrapper(text):\n    return word_tokenize(text)\n\ntest_df['review_text_token'] = test_df['review_text'].apply(word_tokenize_wrapper)\ntrain_df['review_text_token'] = train_df['review_text'].apply(word_tokenize_wrapper)","310d3bef":"def normalisasi(token):\n  kbba=[kamus.strip('\\n').strip('\\r') for kamus in open('\/kaggle\/input\/normalisasitxt\/normalisasi.txt')]\n  dic={}\n  for i in kbba:\n    (key,val)=i.split('\\t')\n    dic[str(key)]=val\n  final_string = ' '.join(str(dic.get(word, word)) for word in token).split()\n  return final_string\ntrain_df['normalisasi'] = train_df['review_text_token'].apply(normalisasi)\ntest_df['normalisasi'] = test_df['review_text_token'].apply(normalisasi)","2f301a68":"list_stopwords = (['yg', 'dg', 'dgn', 'dengan', 'ny', 'd', 'klo',\n                   'kalo', 'amp', 'biar', 'bikin', 'bilang', 'jadi',\n                   'krn', 'nya', 'nih', 'sih', 'untuk', 'juga', 'the',\n                   'si', 'tau', 'tuh', 'utk', 'ya', 'dari', 'and',\n                   'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'karena',\n                   'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n                   '&amp', 'yah', 'yng', 'yang', 'di', 'ada', 'dan',\n                   'saya', 'ke', 'dll', 'gw', 'gwe', 'gua', 'itu', 'saja'])\n\nlist_stopwords = set(list_stopwords)\n\n\n#remove stopword pada list token\ndef stopwords_removal(words):\n    return [word for word in words if word not in list_stopwords]\n\ntrain_df['normalisasi'] = train_df['normalisasi'].apply(stopwords_removal)\ntest_df['normalisasi'] = test_df['normalisasi'].apply(stopwords_removal)","58473ff7":"cat = train_df['category'].value_counts()","0bdd7d71":"print('Jumlah review negatif', cat[0])\nprint('Jumlah review positif', cat[1])","e144871b":"flavors = ('Negative', 'Positive')\nexplode = (0, 0.1)\n\nplt.title('Persentase Review Positif dan Negatif')\nplt.pie(\n    cat,\n    labels=flavors,\n    autopct='%1.1f%%',\n    explode=explode,\n    shadow=True\n    )\nplt.show()","599f1168":"lens = train_df['normalisasi'].apply(lambda x: len(x))\nprint(lens.describe())\nlens.hist()","5dc6e04e":"train_df.drop(train_df[lens < 1].index, inplace=True)","9782caf4":"lens_positive = train_df.loc[(train_df.category == 1), 'normalisasi'].apply(lambda x: len(x))\nprint(lens_positive.describe())\nlens_positive.hist()","d4da25df":"negative = train_df.loc[(train_df.category == 0), 'normalisasi'].apply(lambda x: len(x))\nprint(negative.describe())\nnegative.hist()","23f84fb9":"words = train_df['normalisasi']\nallwords = []\nfor wordlist in words:\n  allwords += wordlist","8feda2e5":"mostcommon = FreqDist(allwords).most_common(100)\n\nwordcloud = WordCloud(width=1600, \n                      height=800,\n                      background_color='white').generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10),\n                 facecolor='white')\nplt.imshow(wordcloud,\n           interpolation='bilinear')\nplt.axis('off')\nplt.title('Top Most 100 Common Words', fontsize=100)\n\nplt.tight_layout(pad=0)\nplt.show()","35c3c644":"mostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\n\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60,\n           fontsize=40)\nplt.title('Frequency of 25 Most Common Words',\n          fontsize=60)\nplt.show()","68dccec5":"def gabung(sentence):\n  return ' '.join(sentence)\ntrain_df['normalisasi_text'] = train_df['normalisasi'].apply(gabung)\n\ngroup_by = train_df.groupby('category')['normalisasi_text'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\ngroup_by_0 = group_by.iloc[0]\nwords0 = list(zip(*group_by_0))[0]\nfreq0 = list(zip(*group_by_0))[1]\nplt.figure(figsize=(50,30))\nplt.bar(words0, freq0)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Frequency of 25 Most Common Words for Negative Review', fontsize=60)\nplt.show()","acb1c16b":"group_by = train_df.groupby('category')['normalisasi_text'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\ngroup_by_1 = group_by.iloc[1]\nwords1 = list(zip(*group_by_1))[0]\nfreq1 = list(zip(*group_by_1))[1]\nplt.figure(figsize=(50,30))\nplt.bar(words1, freq1)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.title('Frequency of 25 Most Common Words for Positive Review', fontsize=60)\nplt.show()","992cd76c":"# Memisahkan Review dengan Labelnya\nreview = train_df['normalisasi']\nlabel = train_df['category']","494c15db":"# Memecah dataset menjadi training set dan validation set\nreview_latih, review_test, label_latih, label_test = train_test_split(review, label, test_size=0.2, random_state=42)","26be8b2d":"pad_type = 'pre'\ntrunc_type = 'pre'\n\n# Tokenize our training data\ntokenizer = Tokenizer(num_words=1000, oov_token='x')\ntokenizer.fit_on_texts(review_latih)\ntokenizer.fit_on_texts(review_test)\n\n# Encode training data sentences into sequences\nsekuens_latih = tokenizer.texts_to_sequences(review_latih)\nsekuens_test = tokenizer.texts_to_sequences(review_test)\n\n# Get max training sequence length\nmaxlen = max([len(x) for x in sekuens_latih])\n\n# Pad the training sequences\npadded_latih = pad_sequences(sekuens_latih, padding=pad_type, truncating=trunc_type, maxlen=maxlen) \npadded_test = pad_sequences(sekuens_test, padding=pad_type, truncating=trunc_type, maxlen=maxlen)","6c301d46":"model = tf.keras.Sequential([\n  tf.keras.layers.Embedding(input_dim=1000, output_dim=16),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.LSTM(32),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dropout(0.8),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n  ])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', tfa.metrics.F1Score(num_classes=1, average='macro', threshold=0.5)])","97fe5057":"model.summary()","3811fd6a":"num_epochs = 30\nhist = model.fit(padded_latih, label_latih,\n                 epochs=num_epochs,\n                 validation_data=(padded_test, label_test),\n                 verbose=2)","1a6fc32b":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()","000f8fbd":"# summarize history for F1 Score\nplt.plot(hist.history['f1_score'])\nplt.plot(hist.history['val_f1_score'])\nplt.title('Training F1 Score vs Validation F1 Score')\nplt.ylabel('F1 Score')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()","533ddf5c":"# summarize history for Loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","47d85d60":"sekuens_submit = tokenizer.texts_to_sequences(test_df['normalisasi'])\npadded_submit = pad_sequences(sekuens_submit, padding=pad_type, truncating=trunc_type, maxlen=maxlen)","d2a81169":"s = model.predict_classes(padded_submit)","cba2f151":"submission['category'] = s\nsubmission.to_csv('submission.csv', index=False)","4c841a98":"## Normalisasi","418f485c":"## Distribusi Kategori Review","3f3f6ab3":"### Info Pada Dataset","cd8da9d0":"Tahap modeling bertujuan untuk melatih model menggunakan dataset yang tersedia yang kemudian setelah melalui proses training, model tersebut dapat digunakan untuk memprediksi label dari data baru yang belum pernah dijumpai. Pada tahap ini kami menggunakan arsitektur model *Recurrent Neural Networks (RNNs)*. Dalam arsitektur *RNNs* yang kami buat, kami menggunakan 4 layer utama dan menggunakan layer 'dropout' pada masing-masing layer yang bertujuan untuk mengurangi 'overfiting' pada data training. 4 Layer yang kami gunakan adalah sebagai berikut:\n1. Embedding <br>\nLayer Embedding diinisialisasi dengan bobot acak dan akan mempelajari embedding untuk semua kata dalam set data pelatihan. \n2. LSTM (Long Short-Term Memory) <br>\nLSTM adalah jenis arsitektur RNN yang sangat bagus dalam menangani urutan informasi yang panjang. Layer ini mengambil input dimensi (batch size, maxlen, dimensi embedding) dan mengembalikan output dimensi (batch size, 32). Ukuran output yang lebih besar berarti model yang lebih kompleks; kami telah memilih 32 setelah melakukan *tuning* berdasarkan kinerja model.\n3. Dense <br>\nSebuah hidden layer dengan fungsi aktifasi 'relu\"\n4. Dense <br>\nLayer terakhir untuk mengembalikan prediksi review positif atau negatif.","6e2b8d93":"## A. Understand the Business","8b7ae1e2":"Tahap normalisasi bertujuan untuk menyeragamkan kata yang memiliki makna sama namun dengan penulisan yang berbeda. Hal ini bisa diakibatkan karena kesalahan penulisan, penyingkatan kata, ataupun penggunaan \"bahasa gaul\".\n\nUntuk itu kami harus menyiapkan dataset untuk mapping kata yang ingin diseragamkan. Kami sudah menyiapkan dataset tersebut yang tersedia pada link github dibawah. Selanjutnya kami telah membuat fungsi yang dapat digunakan untuk proses normalisai.","195878d7":"*Exploratory Data Analysis (EDA)* adalah proses memeriksa dataset untuk menemukan fakta tentang data dan mengkomunikasikan fakta tersebut, seringkali melalui visualisasi.","69a12299":"Stopwords merupakan kata-kata yang sering muncul dan dianggap tidak memiliki makna. Dalam bahasa Indonesia, beberapa contoh stopwords yang sering muncul seperti \"yang\", \"dan\", \"di\", \"dari\", dan lain lain. Menghapus stopwords bertujuan untuk menghapus kata yang memiliki makna rendah, sehingga kita dapat lebih fokus terhadap kata yang memiliki makna lebih tinggi dalam sebuah kalimat.\n\nUntuk melakukan filtering stopwords, kami sudah membuat beberapa list kata yang termasuk stopwords. Selanjutnya kami membuat sebuah fungsi `stopwords_removal` yang akan mengecek apakah sebuah token ada di dalam list stopwords, jika 'Ya' maka kate tersebut akan dihapus dari token.","ac7e21ee":"Hasil dari model yang kami buat memiliki skor Macro F1 0.89835 pada public leaderboard. skor dari model masih dapat ditingkatkan dengan teknik preprocessing data yang tepat","f4223283":"### Menghapus New Line","1aa17ecc":"# 2. Data Preprocessing","5a9a5e84":"Setelah menyusun arsitektur model RNN, kami melakukan fitting dataset pada model dengan *epochs* 30.","52bc6ada":"Untuk menghapus angka dalam data, kami menggunakan *Regular Expression (Regex)*. Kami juga akan menggunakan fungsi `replace()` untuk proses ini.","1a541a18":"Pada kasus pemrosesan bahasa alami, data yang akan di proses merupakan data yang tidak terstruktur dimana data-data tersebut berupa teks atau kalimat. Oleh karena itu diperlukan pengolahan terhadap data tersebut sehingga menjadi data yang terstruktur agar dapat digunakan pada tahap berikutnya.\n\nPada tahap ini, kami manggunakan *library* `nltk` atau *Natural Language Tolkit*. Library `nltk` lazim digunakan untuk pemodelan dan preprosesing teks. `nltk` menyediakan metode-metode yang baik untuk mempersiapkan teks sebelum digunakan pada tahap selanjutnya.\n\nTahapan preprosesing data yang akan kami lakukan sebagai berikut:\n1. Case Folding\n2. Tokenizing\n3. Normalisasi\n4. Filtering","73527fe4":"### Import Files","35b73207":"Berikut *summary* dari model yang kami buat","6c58db96":"### Distribusi Jumlah Kata Review Positif","c49fe1b3":"Dari hasil diatas kita dapat ketahui bahwa dalam dataset yang kita miliki terdapat 86.4% review negatif dan 13.6% review positif. Total dari keseluruhan sampel yang kita miliki sebanyak 14856. Kemudian dari hasil tersebut dapat kita simpulkan bahwa data yang kita miliki adalah data yang *imbalance*.","c0c6c559":"### Mengubah Text Menjadi Lowercase","60a75da3":"# 6. Validasi","c76471bd":"## 3. Exploratory Data Analysis (EDA)","5aaa2adb":"# 7. Kesimpulan","7f34b8bc":"### Distribusi Jumlah Kata Review Negatif","51230978":"# 4. Feature Enginering","9bab8ef1":"## Tokenizing","dd009417":"# Datavidia 7.0\n## By: Tim VWXYZ - Universitas Trunojoyo\n### 1. Mohamad Zaelani (Leader) - mohamadzaelani09@gmail.com\n### 2. Yusuf Sugiono - ysf.sugiono@gmail.com\n### 3. Viki Wahyudi - vikiwahyudi1982@gmail.com","f38c1ae4":"Pada tahap ini kami berusaha memahami permasalahan yang sedang dihadapi. Pada kompetisi ini kami diminta untuk membuat model yang dapat memprediksi apakah sebuah review terhadap hotel adalah review buruk atau review positif.\n\nProblem Statement : Prediksi apakah suatu review termasuk review positif atau negatif.\n\nDengan membuat model yang dapat memprediksi sebuah review apakah termasuk review positif atau negatif maka perusahaan dapat menangani permasalahan dengan segera.","1c73b6cb":"## Text length","474bbb56":"Huruf besar dan kecil akan dianggap berbeda sehingga perlu ditangani lebih lanjut. Proses *case folding* pada data yang kami miliki akan menggunakan fungsi `lower()` pada class `Pandas.Series.str`. ","4d38e767":"# Referensi","5e6c8eb5":"### Distribusi Jumlah Kata Seluruh Review","5f0c5c07":"- https:\/\/medium.com\/@ksnugroho\/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n- https:\/\/yunusmuhammad007.medium.com\/text-preprocessing-menggunakan-pandas-nltk-dan-sastrawi-untuk-large-dataset-5fb3c0a88571\n- https:\/\/dair.ai\/Exploratory_Data_Analysis_for_Text_Data\/\n- https:\/\/keras.io\/api\/preprocessing\/text\/\n- https:\/\/github.com\/google\/applied-machine-learning-intensive","c25f240e":"Untuk menghapus karekter *new line* kami menggunakan fungsi `replace()` dan regular expression\/regex.","2b10ddf8":"### Import Library","20f3fd41":"## B. Understand the Data","44f2b309":"Hasil diatas menunjukkan bahwa panjang kata rata-rata yang terdapat pada dataset adalah 14 kata. Kemudian terdapat review dengan panjang 0 kata, maka kami akan menghapus data tersebut.","50eae685":"### Cek 5 Data Teratas","4acd1ed6":"*Tokenizing* merupakan proses untuk memisahkan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian dilakukan analisa. Dalam NLP, token merupakan \"kata\" yang diekstrak pada sebuah kalimat. Untuk melakukan *tokenizing* kami menggunakan fungsi `word_tokenize()` yang tersedia pada class `nltk.tokenize` pada modul `nltk`.","42221e50":"# 5. Modeling","73e79ba8":"### Menghapus Karakter Angka","64bd7a1d":"Dari hasil grafik diatas, skor validasi lebih tinggi daripada skor training disebabkan karena penggunaan layer dropout pada arsitektur model yang kami buat.","9ea74985":"Kami juga akan menghapus tanda baca dan whitespace yang berlebihan. Untuk melakukan itu, kami menggunakan modul `nltk` untuk menghapus tanda baca *(punctuation)* dan menggunakan modul `re` atau *regular expression*.","c054583f":"Pada tahap ini kami akan berusaha memahami dataset yang diberikan. Tahapan ini sangat penting karena pada kasus dilapangan terkadang memiliki kualitas yang buruk sehingga perlu penanganan lebih lanjut.\n\nBerikut data-data yang akan digunakan:\n- train.csv - Data training yang berisi review beserta kategori sentimen review tersebut\n- test.csv - Data uji \/ test yang berisi review yang ingin diketahui kategori sentimennya\n- sample_submission.csv - File yang berisi contoh format submisi pada platform kaggle","95046c1d":"## Term Frequency Analysis","33507ff8":"Pada tahap bussines understanding dan data undersanding, terdapat beberapa hal yang dapat kami simpulkan:\n\n- Target Variable: `category`\n- Problem Type:  Klasifikasi Binary dengan class '1' (review positif) dan '0' (review negatif)\n- Metric:  Macro F1-Score\n- Tidak ada kolom yang memiliki missing value untuk setiap dataset\n- Kolom `review_id` tidak akan digunakan dalam proses training model sehingga akan di hapus\n- Kolom `review_text` berisi kalimat-kalimat review dari pelanggan hotel sehingga kami harus melakukan Pemrosesan Bahasa Alami (Natural Language Processing).","d36b8925":"## Filtering (Menghapus Stopwords)","f5d8cc85":"## Case Folding","f6503f20":"Sebelum melakukan teknik preprosesing terhadap data yang diberikan, kami akan menghapus kolom `review_id` karena kolom tersebut tidak akan digunakan dalam proses training.","3dd158ed":"*Case folding* merupakan teknik preprosesing yang paling sederhana. Tujuan dari case folding adalah untuk mengubah semua huruf dalam dataset menjadi huruf kecil. Selain itu, karakter-karakter selain huruf juga akan dihapus. Pada tahapan ini kami akan menggunakan modul yang sudah tersedia.\n","33cd248c":"# 1. Understand the Problem (Business and Data)","d45c3023":"Machine learning hanya bisa memproses data dalam bentuk angka, oleh karena itu diperlukan proses rekayasa fitur *(Feature Enginering)* untuk mengubah data dalam bentuk teks menjadi data dalam bentuk angka. Pada tahap ini kami menggunakan *class* `tf.keras.preprocessing.text.Tokenizer`. \n\n*Class* ini memungkinkan untuk membuat vektor korpus teks, dengan mengubah setiap teks menjadi urutan bilangan bulat `sequence` (setiap bilangan bulat menjadi indeks token dalam `dictionary`) atau menjadi vektor di mana koefisien untuk setiap token dapat berupa biner, berdasarkan jumlah kata dan\/atau berdasarkan tf-idf. Argumen yang digunakan dalam *class* ini adalah sebagai berikut:\n- **num_words** : jumlah kata maksimum yang harus disimpan, berdasarkan frekuensi kata. Hanya kata-kata `num_words-1` paling umum yang akan disimpan.\n- **filter** : string di mana setiap elemen adalah karakter yang akan difilter dari teks. Default dari argumen ini adalah semua tanda baca, ditambah tab dan jeda baris, tanpa karakter `'`.\n- **lower** : boolean. Apakah akan mengonversi teks menjadi huruf kecil. Default `True`\n- **split**: str. Pemisah untuk memisahkan kata.\n- **char_level** : jika `True`, setiap karakter akan diperlakukan sebagai token.\n- **oov_token** : jika diberikan, maka akan ditambahkan ke `word_index` dan digunakan untuk mengganti kata-kata yang tidak ada selama pemanggilan fungsi `text_to_sequence`\n\nSelanjutnya kami akan menambahkan *padding* yang bertujuan untuk menyeragamkan panjang dari `sequence`.\n","b26c855f":"### Menghapus Tanda Baca dan Whitespace"}}