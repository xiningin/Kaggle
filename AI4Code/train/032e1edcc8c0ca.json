{"cell_type":{"f9d66adc":"code","0527d0f9":"code","c38e3e0a":"code","a6e4fd00":"code","f42a0fb7":"code","57eb4b36":"code","a0357664":"code","56d01534":"code","3468dca1":"code","4e207367":"code","6dbd44d6":"code","42d3e9e1":"code","cadebcd8":"code","0f578dd8":"code","8b150928":"code","9a1f7c93":"code","6362a413":"code","e8c96b07":"code","75ea7a12":"code","e4c45090":"code","953ecf83":"code","f4064e41":"code","f8a4d04a":"code","62da91f4":"code","ef474a4e":"code","97c3f6a9":"code","ee563049":"code","511bf257":"code","bbeb3b7a":"code","b2877518":"code","56264ba2":"code","72b83090":"code","4b037cfb":"code","c192dcb3":"code","3f901d6b":"code","6a20fcfc":"code","b070c984":"code","57afdcb8":"code","718692a2":"code","f9074bff":"code","ce237fe6":"code","fccc8059":"code","54d6975a":"code","b6d190b3":"code","a9fc57f2":"code","deada32d":"code","def089ba":"markdown","d98228ab":"markdown","29f0a89d":"markdown","70a5316d":"markdown","0d4085fc":"markdown","a8e7cbc7":"markdown","f31d4d09":"markdown","6d1ee3bb":"markdown","1a83d977":"markdown","6d2a0e60":"markdown","c8b0f4df":"markdown","63cd8773":"markdown","3446146a":"markdown","438b73c7":"markdown","bae1dbdc":"markdown","0385ba17":"markdown","d5e604c1":"markdown","49f88369":"markdown","d5cfe755":"markdown","22ec0061":"markdown","34359df9":"markdown","e3f711fe":"markdown","249c2776":"markdown","4371b592":"markdown","49f2e914":"markdown","70eee2c8":"markdown","031515e2":"markdown","7c19a4b3":"markdown","ba19caab":"markdown","da8e4c7f":"markdown","8779fd44":"markdown","dc39cda3":"markdown","f0685e90":"markdown","bc6348ee":"markdown","ae8d74cc":"markdown","8242cee5":"markdown","ec4edef0":"markdown","8ec7c422":"markdown","6337b442":"markdown","ca10a65f":"markdown","a37a9b2b":"markdown","9316bb29":"markdown","84a9a69e":"markdown","30e69bb8":"markdown","e6a6b5f2":"markdown","1c26fbd9":"markdown","46fe1d5f":"markdown","0c8166b7":"markdown","b596bc34":"markdown","0faf6af1":"markdown","405298dc":"markdown","6c0bf815":"markdown","39cba92c":"markdown"},"source":{"f9d66adc":"# pip install pycaret","0527d0f9":"# Libraries data handling\nimport numpy as np\nimport pandas as pd\n\n# Librarires for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(font = 'Serif', style = 'white', rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\n\n# Libraries for ML\nfrom pycaret.classification import *\n\n# Importing h2o library\nimport h2o\nfrom h2o.automl import H2OAutoML","c38e3e0a":"# Reading the data file\ndf = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","a6e4fd00":"# Checking the data types\ndf.info()","f42a0fb7":"# Changing the data type for the categorical variables\ncategorical_var = ['sex','cp','fbs','restecg','exng','slp','caa','thall']\ndf[categorical_var] = df[categorical_var].astype('category')","57eb4b36":"# Checking the mean, median, max \ndf.describe()","a0357664":"# Checking for the distribution of the numeric features\nnumeric_var = [i for i in df.columns if i not in categorical_var][:-1]\n\nfig, ax = plt.subplots(1,5, figsize = (15,5))\nfor axis, num_var in zip(ax, numeric_var):\n    sns.boxplot(y = num_var,data = df, ax = axis, color = 'skyblue')\n    axis.set_xlabel(f\"{num_var}\", fontsize = 12)\n    axis.set_ylabel(None)\n\nfig.suptitle('Box Plot for Identifying Outliers', fontsize = 20, weight = 'bold')\nfig.text(0.5, -0.05, 'Numeric Features', ha = 'center', fontsize = 14, weight = 'bold')\nplt.tight_layout()","56d01534":"# Let's check the output\n\ncolors = ['#06344d', '#00b2ff']\nsns.set(palette = colors, font = 'Serif', style = 'white', \n        rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})\n\nfig = plt.figure(figsize = (10, 6))\nax = sns.countplot(x = 'output', data = df)\n\nfor i in ax.patches:\n    ax.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()\/7, \n            s = f\"{np.round(i.get_height()\/len(df)*100, 0)}%\", \n            ha = 'center', size = 50, weight = 'bold', rotation = 90, color = 'white')\n\nplt.title(\"Heart Attack Count\", size = 20, weight = 'bold')\n\nplt.annotate(text = \"People with no \\n Hearth Attack\", xytext = (-0.4, 140), xy = (0.1, 100),\n             arrowprops = dict(arrowstyle = \"->\", color = 'black', connectionstyle = \"angle3, angleA = 0, angleB = 90\"), \n             color = 'green', weight = 'bold', size = 14)\nplt.annotate(text = \"People with \\n Hearth Attack\", xytext = (0.15, 150), xy = (1, 110), \n             arrowprops = dict(arrowstyle = \"->\", color = 'black', connectionstyle = \"angle3, angleA = 0, angleB = 90\"), \n             color = 'red', weight = 'bold', size = 14)\n\nplt.xlabel('Hearth Attack Count', weight = 'bold')\nplt.ylabel('Number of People', weight = 'bold');","3468dca1":"# Variation of Heart Attack rate with each numeric variable\n\nfig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = (15, 7), constrained_layout = True) # axis.patches can't be used\n# plt.suptitle('Variation of Heart Attack rate', size = 16, weight = 'bold')\n\nfor axis, num_var in zip(ax.ravel(), numeric_var): \n    sns.kdeplot(data = df, x = num_var, hue = 'output', ax = axis,\n                fill = True, multiple = 'stack', alpha = 0.6, linewidth = 1.5)\n    axis.set_ylabel(None)\n    axis.set_xlabel(None)\n\nfor i, num_var in zip(range(0, 5), numeric_var): \n    sns.histplot(data = df, x = num_var, hue = 'output', ax = ax[1][i])\n    ax[1][i].set_ylabel(None)\n    ax[1][i].set_xlabel(f'{num_var}', fontsize = 14)\n    \nfig.text(0.5, -0.05, 'Numeric Features', ha = 'center', fontsize = 14, weight = 'bold');\nfig.text(0.5, 1.05, 'Effect of Numeric Features on Heart Attack', ha = 'center', fontsize = 20, weight = 'bold');\n\nfig.show()","4e207367":"# Variation of Heart Attack rate with each categorical variable\nfig, ax=plt.subplots(2,4, figsize=(20,14), sharey=True)\n\nfor col, axis in zip(categorical_var, ax.ravel()):\n    sns.countplot(x=col, data=df, hue='output', ax=axis, order = np.sort(df[col].unique()))\n    for i in axis.patches:    \n        axis.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()+6,\n                s = f\"{i.get_height()}\", \n                ha = 'center', size = 14, weight = 'bold', rotation = 0, color = 'black',\n                bbox=dict(boxstyle=\"circle,pad=0.5\", fc='pink', ec=\"pink\", lw=2))\n\n    axis.set_title(f'Effect of {col} on Heart Attack', fontsize=16, weight='bold', y=1.05);\n    axis.set_xticklabels(df[col].unique(), fontsize=12)\n    axis.set_ylabel('Number of People', fontsize=14);\n    axis.set_xlabel(None);\n\nfig.text(0.5, 1.05, 'Effect of Categorical Features on Heart Attack', ha = 'center', fontsize = 20, weight = 'bold')\nplt.tight_layout()","6dbd44d6":"# Correlation between numeric variables\nfig=plt.figure(figsize=(10,7))\naxis=sns.heatmap(df[numeric_var].corr(), annot=True, linewidths=3, square=True, cmap='Blues', fmt=\".0%\")\n\naxis.set_title('Correlation between the features', fontsize=16, weight='bold', y=1.05);\naxis.set_xticklabels(numeric_var, fontsize=12)\naxis.set_yticklabels(numeric_var, fontsize=12, rotation=0);\n\nfig.text(0.5, 0.0, 'Numeric Features', ha = 'center', fontsize = 14, weight = 'bold');","42d3e9e1":"# Age distribution along gender and cholestrol levels\nfrom matplotlib.patches import Rectangle\n\ncolors = ['#06344d', '#00b2ff']\nlables=['<45', '45-60', '60+']\nsex=['0','1']\ndf['age_bins']=pd.cut(x=df['age'],bins=[25,45,60,100], labels=lables, include_lowest=True)\n\nfig, axis1 = plt.subplots(figsize=(10,7))\naxis2 = axis1.twinx()\naxis1=sns.countplot(x='age_bins', data=df, hue='sex')\n\n# for i in axis1.patches:\n#         i.text(x = i.get_x() + i.get_width()\/2, y = i.get_height()+6,\n#                s = f\"{i.get_height()}\", \n#                 ha = 'center', size = 12, weight = 'bold', rotation = 0, color = 'white',\n#                 bbox=dict(boxstyle=\"circle\", pad=0.5, fc='pink', ec=\"pink\", lw=2))\n\naxis2 = sns.boxplot(x='age_bins', y='chol', hue='sex', data=df)\n# axis2.grid(b=False)\naxis2.legend('')\n\naxis1.text(x = 0.8, y = 500,\n        s = 'Female', \n        ha = 'center', size = 12, weight = 'bold', rotation = 0, color = 'white',\n        bbox=dict(boxstyle=\"circle,pad=0.1\", fc= colors[0], ec=colors[0], lw=2))\n\naxis1.text(x = 1.15, y = 500,\n        s = 'Male', \n        ha = 'center', size = 12, weight = 'bold', rotation = 0, color = 'white',\n        bbox=dict(boxstyle=\"circle,pad=0.8\", fc= colors[1], ec=colors[1], lw=2))\n\naxis1.set_title('Age and Cholestrol levels', fontsize=16, weight='bold', y=1.05);\n\naxis1.text(x = 1, y = -60,\n        s = 'Age', \n        ha = 'center', size = 12, weight = 'bold', rotation = 0, color = 'black')\naxis1.set_ylabel('Cholestrol levels', size = 12, weight = 'bold');\n\nrect1 = Rectangle( (-0.4,150), 2.8, 200, linestyle = 'dashed', facecolor = 'None', clip_on=False, edgecolor = 'black')\naxis1.add_patch(rect1)\n\nrect2 = Rectangle( (-0.4,0), 2.8, 120, linestyle = 'dashed', facecolor = 'None', clip_on=False, edgecolor = 'black')\naxis1.add_patch(rect2)\n\naxis1.annotate(text = \"Cholestrol levels \\n Distribution\", xytext = (-0.2, 420), xy = (0.5, 350), \n             arrowprops = dict(arrowstyle = \"->\", color = 'black', connectionstyle = \"angle3, angleA = 0, angleB = 90\"), \n             color = 'black', weight = 'bold', size = 12)\n\naxis1.text(x = 0, y = 80,\n        s = 'Age Count', \n        ha = 'center', size = 12, weight = 'bold', rotation = 0, color = 'black');","cadebcd8":"categorical_var = ['sex','cp','fbs','restecg','exng','slp','caa','thall', 'age_bins']","0f578dd8":"# Using 'setup' from pycaret.classification\nclf = setup(df, target = 'output', categorical_features = categorical_var,\n            ordinal_features = {'restecg':['0','1','2'],\n                               'age_bins':['<45','45-60','60+']}, # 'restecg' is an ordinal feature like low, medium and high\n            remove_outliers = True, outliers_threshold = 0.05, # Removing outliers with threshold of 5 percentile\n            normalize = True,\n            normalize_method = 'zscore', # Mean => 0 and std. deviation => 1\n            train_size = 0.8,\n            fold = 10,\n            use_gpu = True)","8b150928":"# Comaparing the performance of various models\nbest_model = compare_models()","9a1f7c93":"# Creating the ML model\nlr = create_model('lr')","6362a413":"# Results for Test set\nresult = predict_model(lr)","e8c96b07":"# tune hyperparameters with custom_grid\nparams = {\"tol\": [0.0001, 0.0005, 0.001],\n          \"C\": [0.1, 0.5, 1, 1.5, 2]}\n\ntuned_lr = tune_model(lr, custom_grid = params)","75ea7a12":"# Results for Test set\nresult1 = predict_model(tuned_lr)","e4c45090":"# Creating the models\nknn = create_model('knn')\nlda = create_model('lda')","953ecf83":"# Stacking top 3 models\nstacked_model = stack_models(estimator_list=[knn, lda, tuned_lr])","f4064e41":"# Initializing h2o\nh2o.init()","f8a4d04a":"# Getting training and test dataframes from pycaret setup\nX_train, X_test, y_train, y_test = get_config('X_train'), get_config('X_test'), get_config('y_train'), get_config('y_test')\n\n# Combining the X_train with y_train and X_test with y_test \ndf_train = pd.concat([X_train, y_train], axis=1)\ndf_test = pd.concat([X_test, y_test], axis=1)\n\n# Creating validation data (optional step)\nfrom sklearn.model_selection import train_test_split\ndf_train, df_val = train_test_split(df_train, train_size = 0.7)\n\n# Creating train and test .csv files for h2o\ndf_train.to_csv('train.csv', index=False)\ndf_test.to_csv('test.csv', index=False)\ndf_val.to_csv('val.csv', index=False)","62da91f4":"# Reding the data using h2o frames\ntrain = h2o.import_file('.\/train.csv')\ntest = h2o.import_file('.\/test.csv')\nval = h2o.import_file('.\/val.csv')","ef474a4e":"# Identifing predictors and response\nx = train.columns\ny = \"output\"\nx.remove(y)\n\n# For binary classification, response should be a factor\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()","97c3f6a9":"# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1)\naml.train(x=x, y=y, training_frame=train, validation_frame = val)","ee563049":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=10)  # Print all rows instead of default (10 rows)","511bf257":"# Chosing th best model\nbest_h2o = aml.leader","bbeb3b7a":"plot_model(tuned_lr, plot = 'auc')","b2877518":"# build the roc curve:\nperf_test = best_h2o.model_performance(test)\nperf_test.plot(type = \"roc\")","56264ba2":"plot_model(tuned_lr, plot = 'confusion_matrix')","72b83090":"perf_test.confusion_matrix()","4b037cfb":"# Predicting using the best_h2o model\npredictions = best_h2o.predict(test)\n# Converting to pandas dataframe\nprediction = predictions.as_data_frame()","c192dcb3":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nmatrix = confusion_matrix(y_test, prediction['predict'])\n\nplt.style.use('seaborn-dark')\nfig, axis1 = plt.subplots(1,1, figsize=(10,6), constrained_layout = True)\nsns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                        linewidths=3, square=True, ax = axis1, annot_kws={\"fontsize\":16})\naxis1.set_title(f\"Confusion Matrics | Threshold : 0.5\", fontsize=16, y=1.05);\naxis1.set_xlabel('Predicted', fontsize=12)\naxis1.set_ylabel('Actual', fontsize=12)\naxis1.set_xticklabels([0,1], fontsize=12 )\naxis1.set_yticklabels([0,1], fontsize=12, rotation=0);","3f901d6b":"plot_model(tuned_lr, plot = 'class_report')","6a20fcfc":"perf_test","b070c984":"plot_model(tuned_lr, plot = 'boundary')","57afdcb8":"plot_model(tuned_lr, plot = 'error')","718692a2":"plot_model(tuned_lr, plot = 'learning')","f9074bff":"best_h2o.learning_curve_plot()","ce237fe6":"# Creating catboost model\ncatboost = create_model('catboost')","fccc8059":"plot_model(catboost, 'feature')","54d6975a":"interpret_model(catboost)","b6d190b3":"best_h2o.explain(test)","a9fc57f2":"interpret_model(catboost, plot = 'reason', observation = 12)","deada32d":"best_h2o.explain_row(test, row_index=12)","def089ba":"> We get the accuracy, AUC of ROC curve, Recall, Precision, etc. of 10+ models in single line of code! Now, we have to deciside which model to choose and this depends upon our evaluation criterion.","d98228ab":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Local Interpreting Model using ANN<\/center><\/h4><\/div>","29f0a89d":"> The `model_performance()` changes the threshold besed on the best f1 score. This can give false output so, let's first predict using the best_h2o model and then plot the confusion matrix again.","70a5316d":"> The ANN (AUC=0.948) is performing slightly better than the Logistic Regression (AUC=0.93)","0d4085fc":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Learning Curve for ANN<\/center><\/h4><\/div>","a8e7cbc7":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Confusion Matrix for ANN<\/center><\/h4><\/div>","f31d4d09":"#### Preprocessing and H2O\n> For training using h2o, we have to create a h2o frame, which is like a pandas dataframe. We have two options for doing this:\n> - read and preprocess the data using h2o frame\n> - read and preprocess the data using pandas and convert it to h2o frame\n\n> As I am more confirtable in handling the data with pandas so I am choosing the second method but if you feel confirtable in handling data with h2o frame, you can very well do that. We have already preprocessed the data using pycaret library so we will continue from that. From pycaret `setup` we get, X_train, y_train, X_test and y_test as pandas dataframe.\n\n> I didn't find any function to feed pandas dataframe to h2o, so first I convert these df to .csv and then read the .csv using `h2o.import_file`, to convert them into h2o frame.","6d1ee3bb":"<h3><center><div style=\"background-color:pink;border-radius:10px; padding: 10px;\"> Learning Curve \ud83d\udcc8 <\/div><\/center><\/h3>","1a83d977":"<div> <h3><center style=\"background-color:pink;border-radius:10px; padding: 10px;\">Confusion Matrix \ud83d\ude35<\/center><\/h3>\n \n> A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. is confused when it makes predictions","6d2a0e60":"<h1><center><div style = \"background-color:skyblue;border-radius:10px; padding: 10px;\"> Comparing the Models \ud83d\udd0d<\/div><\/center><\/h1>\n\n> `compare_models` trains varies models like Logistic Regression, Decision Tree, SVM, Random Forest, XGBoost, etc. and compares them based on varies parameters like Accuracy, AUC ROC score, Recall, Precision, etc. So, it becomes easy for us to choose from them.","c8b0f4df":"![image](https:\/\/miro.medium.com\/max\/961\/0*q2_0X7rTtdNFT6Xi.jpg)\n\n> The evaluation metric for this competition is F1-Score or F-Score.\nThe columns `trtbps`, `chol`, `thalachh`, and `oldpeak` contains some values which fall beyond the IQR (Inter Quantile Range), but we have to decide whether to call them outliers or not. For this case, I am considering 0.05 threshold for the outlier identification. Any data point which lies beyond 95 percentile or under 5 percentile is an outlier. You can also choose other value of the threshold. As I am not a medical expert, I am just keeping it on a safer side of *0.05*. \n\n<h5><div class=\"alert alert-block alert-info\"> \ud83d\udccc The outlier removal part will be done during setup of the data for PyCart.<\/div><\/h5>","63cd8773":" <h3><center><div style=\"background-color:pink;border-radius:10px; padding: 10px;\"> Decision Boundary <\/div><\/center><\/h3>","3446146a":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">AUC ROC Curve for Logistic Regression<\/center><\/h4><\/div>","438b73c7":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Learning Curve for Logistic Regression<\/center><\/h4><\/div>","bae1dbdc":"#### Run `pip install pycaret` before proceeding","0385ba17":"> This plot is based on the SHAP values. This plot is made of all the dots in the train data. It demonstrates the following information:\n\n> **Feature importance**: Variables are ranked in descending order.\n\n> **Impact**: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n\n> **Original value**: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n\n>**Correlation**: A high level of the `caa_0` has a high and positive impact on the Heart Attack. The \u201chigh\u201d comes from the red color, and the \u201cpositive\u201d impact is shown on the X-axis. Similarly, we will say the `thall_3` is negatively correlated with the target variable.\n\n> \ud83d\udcd8 If you want to learn about them in detail, you can read this [blog](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d).","d5e604c1":"<h1><div style=\"background-color:skyblue; border-radius:10px; padding: 10px;\"> <center>Looking into the Data \ud83d\udd2c<\/center><\/div><\/h1>","49f88369":"> The features `sex`,`cp`,`fbs`,`restecg`,`exng`,`slp`,`caa`,`thall` are categorical, but their data type is `int64`, so first let's change the data type.","d5cfe755":"<div> <h3><center style=\"background-color:pink;border-radius:10px; padding: 10px;\">AUC ROC Curve \ud83d\udcc8<\/center><\/h3><\/div>\n\n#### Area Under Curve (AUC) Receiver Operating Characteristic (ROC) Curve\n\n> The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.","22ec0061":"<div> <h3><center style=\"background-color:pink;border-radius:10px; padding: 10px;\">Classification Report \ud83d\udccb<\/center><\/h3>","34359df9":"<h1><center><div style = \"background-color:skyblue;border-radius:10px; padding: 10px;\"> Analysing the model \ud83e\uddd0<\/div><\/center><\/h1>\n\n> Analyzing performance of trained machine learning model is an integral step in any machine learning workflow. Analyzing model performance in PyCaret is as simple as writing `plot_model`. The function takes trained model object and type of plot as string within `plot_model` function.\n\n> The available plots are Area Under the Curve, Discrimination Threshold, Precision Recall Curve, Confusion Matrix, etc. for more info, click [here](https:\/\/pycaret.org\/plot-model\/).\n\n> We shall compare the performance of the models created by PyCaret and H2O, side by side.","e3f711fe":"<h3><center><div style=\"background-color:pink;border-radius:10px; padding: 10px;\"> Globle Interpretation \ud83c\udf0e <\/div><\/center><\/h3>","249c2776":"<h1><center><div style = \"background-color:skyblue;border-radius:10px; padding: 10px;\"> Data Setup aka Data Cleaning \ud83d\udebf<\/div><\/center><\/h1>\n\n> `setup` from pycaret.classification will do all the preprocessing necessary before building a model, like:\n>- Missing values : Imputing or removing for both categorical and numeric features\n>- Handling Outliers\n>- Class Imbalance using SMOTE\n>- One hot encoding : For categorical features \n>- Scaling : For numeric features\n>- Train and Test split\n>and many more, for more details, you can [check this](https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html).","4371b592":"> The results look similar, this means that models with exact confusion matrix can have different AUC score. Many people get confused in confusion matrix and AUC ROC, for better understanding of them, you can go through [this](https:\/\/www.kaggle.com\/sonukiller99\/confusion-in-consfusion-matrix-not-anymore) \ud83d\udcd9","49f2e914":"<h1><center><div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\"> Interpreting Model \ud83d\udcd6 <\/div><\/center><\/h1>\n\n> Regardless of the end goal of your data science solutions, an end-user will always prefer solutions that are interpretable and understandable. Moreover, as a data scientist you will always benefit from the interpretability of your model to validate and improve your work.","70eee2c8":"<h1><center><div style = \"background-color:skyblue;border-radius:10px; padding: 10px;\"> H2O.ai \ud83e\udd5b<\/div><\/center><\/h1>","031515e2":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Globle Interpreting Model using Logistic Regression<\/center><\/h4><\/div>\n\n<div class=\"alert alert-block alert-info\"> \ud83d\udccc For binary classification, only tree based models can only be used like Desicion Tree, Random Forest, XGBoost, CatBoost, lightgbm, etc. I will use Catboost but you can choose any other tree based algrithm also. So, for imterpreting using PyCaret, I will create a catboost model, you can also choose Random Forest, XGBoost or any other tree based model.<\/div>","7c19a4b3":"<h1><center> <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\"> Exploratory Data Analysis \ud83d\udcca<\/div><\/center><\/h1>","ba19caab":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Globle Interpreting Model using ANN<\/center><\/h4><\/div>","da8e4c7f":"> Our Recall as well as Accuracy \ud83d\udcc8 increased by 2%! You can see the hyperparamets of the model just by `model_name.get_params`. Let's check the results for the test data.","8779fd44":"<h5><div class=\"alert alert-block alert-info\">  The advantages I found in h2o over pycaret is that, h2o can create deep learning models and the models created have already tuned on some arbitary hyperparameters (which can be changed) ! <\/div><\/h5>","dc39cda3":"<h3><center><div style=\"background-color:pink;border-radius:10px; padding: 10px;\"> \u274c Misscalssification in each class \u274c <\/div><\/center><\/h3>","f0685e90":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Confusion Matrix for Logistic Regression<\/center><\/h4><\/div>","bc6348ee":"#### Now, we will try to further improve the results by stacking different models.\n\n<h3><center><div style = \"background-color:pink;border-radius:10px; padding: 10px;\"> Model Stacking \ud83d\udcda<\/div><\/center><\/h3>\n\n> Model stacking is an efficient ensemble method in which the predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions.\n\n<center><img src=\"http:\/\/i.imgur.com\/QBuDOjs.jpg\" ><\/center>\n\n***\n\n> We will stack top 3 models, but for that, we have to first create the remaining two. I am not considering SVM, as its AUC RUC score was 0.","ae8d74cc":"> The recall is 88%, this means that out 100 heart disease patients, we have correctly identified 88 and missclassifying 12 as no heart disease.\n\n> Now let's see the performance of our model on the test data...","8242cee5":"> The stacked model is not performing better than the tuned logistic regression model, can anyone tell? \ud83e\udd14 \n\n> I will use the tuned logistic regression model as the final model! \ud83d\udd12","ec4edef0":"<h2><center>  <div style=\"background-color:cyan;border-radius:10px; padding: 10px;\">Hope this increased your blood flow by excitment and if you like this, don't forget to upvote!<\/div><\/center><\/h2>","8ec7c422":"<h3><center><div style = \"background-color:pink;border-radius:10px; padding: 10px;\"> Creating and Evaluating ML models using H2O<\/div><\/center><\/h3>","6337b442":"<h3><center><div style = \"background-color:pink;border-radius:10px; padding: 10px;\"> Hyperparameter Tuning \ud83d\udd27\ud83d\udd28<\/div><\/center><\/h3>\n\n> Hyperparameter tuning is a time consuming task, as there are lots of hyperparameter associated with a algorithm and setting them up to the right value to get best results can take significant amount of \u23f3 time \u231b. \n\n> With PyCaret, tuning hyperparameters of a ML model in any module is as simple as writing `tune_model`. It tunes the hyperparameter of the model passed as an estimator using Random grid search with pre-defined grids that are fully customizable.","ca10a65f":"<h5><div class=\"alert alert-block alert-info\"> \ud83d\udccc H2O doesn't provide Decision Boundary and Missiclassification per class plots, so I will be plotting them for PyCaret alone.<\/div><\/h5>\n","a37a9b2b":"<h3><center><div style = \"background-color:pink;border-radius:10px; padding: 10px;\"> Creating and Evaluating ML models using PyCaret<\/div><\/center><\/h3>","9316bb29":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">AUC ROC Curve for ANN<\/center><\/h4><\/div>","84a9a69e":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Classification Report of Logistic Regression<\/center><\/h4><\/div>","30e69bb8":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Local Interpreting Model using Logistic Regression<\/center><\/h4><\/div>","e6a6b5f2":">The count of both the classes are comparable, so there is no class imbalance. If it is there, you can set `fix_imbalance` to True (by defalt it is False) and also can choose the method to remove it using `fix_imbalance_method` (by defalt it is SMOTE)","1c26fbd9":"![image](https:\/\/www.deccanherald.com\/sites\/dh\/files\/article_images\/2019\/11\/20\/heart-attack-1574189524.jpg)\n\n<h1><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Introduction<\/div><\/center><\/h1>\n\n#### \ud83d\udc99 Motivation\n> The heart is one of the most important parts as it pumps blood around your body, delivering oxygen and nutrients to your cells and removing waste products. A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. Fatty deposits build up over time, forming plaques in your heart's arteries. If a plaque ruptures, a blood clot can form and block your arteries, causing a heart attack. I is quite exciting to know, which parameters affect our heart and how! We know some of these like, cholestrol, age, blood pressure, etc. but I wanted to use data science skills to know more about this, and so I decided make this notebook!\n\n#### \ud83c\udfaf Goal\n> The goal of this exercise is to identify the parameters that influences the heart attack and build a ML model for the prediction of heart attack.\n\n#### \ud83e\udd16 AutoML\n> Also, I was thinking that, is there any library for ML which is super easy to use and will perform all the ML steps under one hood. You might think `sklearn` is there so why to go else where, yes, but what I want is to do all the preprocessing steps, creating and training the model , evaluating and interpreting it, in few lines of codes. That's more of expectations but to my surprice \ud83d\ude2e, I found few.\n\n> I am using Pycaret in this notebook. Pycaret is an open source library for ML. It performs all the ML steps and also it is very easy to use. For more details, you can [check this](https:\/\/pycaret.org\/).\n\n<center> <img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/05\/Screenshot-from-2020-05-13-18-30-22.png\"> <\/center>\n\n> There is one more similar library which is quite facinating and easy to use, H2O. It is also an open source library and I will be this library also for comparison purposes. So, let's get started !! \ud83d\ude97\n\n<center> <img src= \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTi48tkmw7TT50iGq2uJRuvRxg6Cz_aUdkkIpGZ5bVWpsugEobka8wWUFdfhqKKKupgR1Q&usqp=CAU\" ><\/center>","46fe1d5f":">The maximum correlation is 42% (negetive) between `thalachh` and `age`, it means that 42% variance in `thalachh` can be explained by `age` and vice versa. It seems that there is no multicollinearity, as for it to be present, the correlation should be higher than 80-85% (positive or negetive). If it is present, we can set `remove_multicollinearity` to True, during setup of PyCaret.","0c8166b7":">You can also save the model with [save_model](https:\/\/pycaret.org\/save-model\/), deploy your model with [deploy_model](https:\/\/pycaret.org\/deploy-model\/).","b596bc34":"<div> <h4><center style=\"background-color:orange;border-radius:10px; padding: 10px;\">Classification Report of ANN<\/center><\/h4><\/div>","0faf6af1":"<h1><center><div style = \"background-color:skyblue;border-radius:10px; padding: 10px;\"> Evaluation Criterion \ud83e\uddea<\/div><\/center><\/h1>\n\n#### I am selecting `recall` as the evaluation matric\n<center> <img src=\"https:\/\/miro.medium.com\/max\/1044\/1*I0Yd-o2yQsHBRKFbf0rjpQ.png\"><\/center>\n<center> <img src=\"https:\/\/skappal7.files.wordpress.com\/2018\/08\/confusion-matrix.jpg?w=748\"><\/center>\n\n> The reason that I am \ud83d\udd2c focusing on recall and not precision is that, if the precision is less it means we have classified a patient with no heart disease with a heart disease which is not bad as the patient will take more care of himself. But if we classify a patient as no heart disease who actually has it, then it can be very dagerous.\n***\n> From the \ud83d\udccb results, **Logistic Regression** has highest Accuracy, AUC, Recall, Precision, and F1. So we will use it to create the ML model.","405298dc":">When you run the cell, you have to press `enter` for proceeding further, otherwise you can type `quit` for not proceeding further.\n>Once you preceed further, you will see all the parameters and their corresponding values. If you are not happy with this, you can change the values and run the `setup` again.\n>For this example, the parameters look fine so I will proceed further.\n\n> \ud83d\udccc If you want to see the train and test data, you can use `get_config('X_train')` and `get_config('X_test')`","6c0bf815":"> The recall for the test data is 90% which is 2% more than the train data, it means that our model is underfitted. We have to tune the hyperparamets to arrive at the optimul solution.","39cba92c":"<h3><center><div style=\"background-color:pink;border-radius:10px; padding: 10px;\"> Local Interpretation <\/div><\/center><\/h3>"}}