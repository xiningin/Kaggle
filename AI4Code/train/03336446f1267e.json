{"cell_type":{"59bf15a9":"code","665b586e":"code","3e13e97e":"code","169e21f7":"code","0d90c06d":"code","f30e9f3b":"code","57087243":"code","1ec3edf4":"code","61ee73d4":"code","311bfbdf":"code","4dc0fbbf":"code","838b64ad":"code","3091036f":"code","2575c1e4":"code","b502f965":"code","ce2716d0":"code","cedca71c":"code","ba1f6e01":"code","d18b6ce6":"code","6b148334":"code","a25791b2":"code","ae901e69":"code","66013783":"code","3df79ddc":"code","aae0aa54":"code","e3d49c72":"code","efb347bc":"code","259fc898":"code","6fdae6ab":"code","c97bcc5b":"code","c08dc0d9":"code","9af2d509":"code","2f1c3006":"code","4f093313":"code","b286ad23":"code","c5cbebcd":"code","07f9dc7f":"code","9c9c566b":"code","7b0ce9a2":"code","2ec419fb":"code","70b957d4":"code","82759dfd":"code","1bec6332":"code","bffa0774":"code","c5447587":"code","ef43793c":"markdown","e125a8a2":"markdown","0e1b5a63":"markdown","7d35d532":"markdown","52eef7e3":"markdown","e2b17090":"markdown","cb08c472":"markdown","4a3e0307":"markdown","be8647db":"markdown","b545d98c":"markdown","627ad991":"markdown","aee38a5a":"markdown","ba38aadd":"markdown","bce6f811":"markdown","a8020a54":"markdown","9c245396":"markdown","754f420e":"markdown","ea062972":"markdown","cea13ed5":"markdown","26e3d151":"markdown","0ab7e7c9":"markdown","f3367ee3":"markdown","6bc04e7b":"markdown","dbc0c235":"markdown","ce301484":"markdown"},"source":{"59bf15a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","665b586e":"# We begin by importing all the necessary libraries needed\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#regression packages\nimport sklearn.linear_model as lm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import explained_variance_score\n\n#lasso regression\nfrom sklearn import linear_model\n\n#f_regression (feature selection)\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import SelectKBest\n\n# recursive feature selection (feature selection)\nfrom sklearn.feature_selection import RFE\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3e13e97e":"df = pd.read_csv(\"..\/input\/walmart-dataset\/Walmart.csv\")\ndf.head()","169e21f7":"# Finding information about the dataset\ndf.info()","0d90c06d":"# converting date object to datetime\ndf['Date'] = pd.to_datetime(df.Date)\ndf.head()","f30e9f3b":"# Reframing the columns by breaking the date into weeks, month and year for analysis\n\ndf['weekday'] = df.Date.dt.weekday\ndf['month'] = df.Date.dt.month\ndf['year'] = df.Date.dt.year\n\ndf.drop(['Date'], axis=1, inplace=True)#,'month'\n\ntarget = 'Weekly_Sales'\nfeatures = [i for i in df.columns if i not in [target]]\noriginal_df = df.copy(deep=True)\n\ndf.head()","57087243":"# Checking for data types\ndf.info()","1ec3edf4":"# checking for unique values\ndf.nunique()","61ee73d4":"# Checking for missing values in each column\ndf.isnull().sum()","311bfbdf":"# Finding the total sales\ndf.Weekly_Sales.sum()","4dc0fbbf":"# The stores with the highest weekly sales\ndf.groupby(['Store'])['Weekly_Sales'].sum().sort_values(ascending=False).head(5)","838b64ad":"# The stores with the highest weekly sales visualization\ndf.groupby(['Store'])['Weekly_Sales'].sum().sort_values(ascending=False).head(5).plot(kind='bar')","3091036f":"# The stores with the lowest weekly sales\ndf.groupby(['Store'])['Weekly_Sales'].sum().sort_values(ascending=False).tail(5)","2575c1e4":"# The stores with the lowest sales\ndf.groupby(['Store'])['Weekly_Sales'].sum().sort_values(ascending=False).tail(5).plot(kind='bar')","b502f965":"# The year with the highest sales\ndf.groupby(['year'])['Weekly_Sales'].sum().sort_values(ascending=False).head(5).plot(kind='bar')","ce2716d0":"# The year with the lowest sales\ndf.groupby(['year'])['Weekly_Sales'].sum().sort_values(ascending=False).tail(5).plot(kind='bar')","cedca71c":"# Summary Statistics\ndf.describe().T","ba1f6e01":"# Histogram for all columns\nimport matplotlib.pyplot as plt\ndf.hist(bins=50, figsize=(20,15))\nplt.show()","d18b6ce6":"#Let us analyze the distribution of the target variable\n\nplt.figure(figsize=[8,4])\nsns.distplot(df[target], color='g',hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=30)\nplt.title('Target Variable Distribution')\nplt.show()","6b148334":"# Correlation Analysis\ndf.corr()","a25791b2":"# Finding the the predictor with the highest relatioship with sales\ncorr = pd.DataFrame(df.corr()['Weekly_Sales'].drop('Weekly_Sales'))\ncorr.sort_values(['Weekly_Sales'], ascending = False)","ae901e69":"# correlation heatmap\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), annot=True)","66013783":"# Regression Analysis between sales and store\nsns.regplot('Store', 'Weekly_Sales', df)","3df79ddc":"# Regression Analysis between sales and fuel prices\nsns.regplot('Fuel_Price', 'Weekly_Sales', df)","aae0aa54":"# Regression Analysis between sales and unemployment\nsns.regplot('Unemployment', 'Weekly_Sales', df)","e3d49c72":"# data visualisation for \nsns.violinplot(x=\"Holiday_Flag\", y=\"Weekly_Sales\", data=df)","efb347bc":"# Categorical to Dummy Variables\ndf =  pd.get_dummies(df, columns=[\"Store\", \"Holiday_Flag\", \"weekday\", \"month\", \"year\"],\n                         prefix=[\"Store\", \"Holiday_Flag\", \"weekday\", \"month\", \"year\"],\n                         drop_first=True)\ndf.head(2)","259fc898":"### c. checking for p-value (TESTING FOR STATISTICAL SIGNIFICANCE OF INDEPENDENT VARIABLES for variable selection)\nimport scipy.stats as stats\ndf_corr = pd.DataFrame() # Correlation matrix\ndf_p = pd.DataFrame() # Matrix of p-values\nfor x in df.columns:   # assuming df as your dataframe name\n   for y in df.columns:\n      corr = stats.pearsonr(df[x], df[y])\n      df_corr.loc[x,y] = corr[0]\n      df_p.loc[x,y] = corr[1]\n\ndf_p['Weekly_Sales']","6fdae6ab":"# Separating target variable and predictors\ny = df ['Weekly_Sales']\nx = df.drop(['Weekly_Sales'], axis =1)","c97bcc5b":"# Normalization data to bring all values to common scale\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(x).transform(x)\nX[0:1]","c08dc0d9":"# splitting data into training and test data at 80% and 20% respectively\nfrom sklearn.model_selection import train_test_split\nxm_train, xm_test, ym_train, ym_test = train_test_split(X, y, train_size = 0.8, random_state = 100)","9af2d509":"lr = lm.LinearRegression()\nlr.fit(xm_train, ym_train)\ny_pred = lr.predict(xm_test)\nprint(\"mean square error: \", mean_squared_error(ym_test, y_pred))\nprint(\"variance or r-squared: \", explained_variance_score(ym_test, y_pred))","2f1c3006":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\n\nregr = RandomForestRegressor(n_estimators=13, random_state=0)\nregr.fit(xm_train, ym_train)\ny_predicted = regr.predict(xm_test)\nprint(\"mean square error: \", mean_squared_error(ym_test, y_predicted))\nprint(\"variance or r-squared: \", explained_variance_score(ym_test, y_predicted))","4f093313":"from sklearn import tree\nclf = tree.DecisionTreeRegressor()\nclf = clf.fit(xm_train, ym_train)\nntree = clf.predict(xm_test)\nprint(\"mean square error: \", mean_squared_error(ym_test, ntree))\nprint(\"variance or r-squared: \", explained_variance_score(ym_test, ntree))","b286ad23":"# Linear Regression \nDep_pred = lr.predict(X)\n# Evaluation\nprint(\"mean square error: \", mean_squared_error(y, Dep_pred))\nprint(\"variance or r-squared: \", explained_variance_score(y, Dep_pred))","c5cbebcd":"# Actual vs Predicted\nc = [i for i in range(1, 6436, 1)]\nfig = plt.figure()\nplt.plot(c, y, color = \"blue\", linewidth = 2.5, linestyle = \"-\" )\nplt.plot(c, Dep_pred, color = \"red\", linewidth = 2.5, linestyle = \"-\" )\nfig.suptitle('Actual and Predicted', fontsize = 20)\nplt.xlabel('Index', fontsize = 18)\nplt.ylabel('medv', fontsize = 16)","07f9dc7f":"# Random Forest Regressor\nDep = regr.predict(X)\nprint(\"mean square error: \", mean_squared_error(y, Dep))\nprint(\"variance or r-squared: \", explained_variance_score(y, Dep))","9c9c566b":"# Actual vs Predicted\nc = [i for i in range(1, 6436, 1)]\nfig = plt.figure()\nplt.plot(c, y, color = \"blue\", linewidth = 2.5, linestyle = \"-\" )\nplt.plot(c, Dep, color = \"red\", linewidth = 2.5, linestyle = \"-\" )\nfig.suptitle('Actual and Predicted', fontsize = 20)\nplt.xlabel('Index', fontsize = 18)\nplt.ylabel('medv', fontsize = 16)","7b0ce9a2":"# Top five most influencial variables\nfeature_importances = pd.DataFrame(regr.feature_importances_, index = x.columns, columns=['importance']).sort_values('importance', ascending=False).head()\n\nfeature_importances","2ec419fb":"# Using cross validation to test the best model\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(regr, X, y,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","70b957d4":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(tree_rmse_scores)","82759dfd":"# Tuning our model\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(X, y)\nprint(grid_search)","1bec6332":"# obtaining the best parameters\ngrid_search.best_params_","bffa0774":"# obtaining the best estimators\ngrid_search.best_estimator_","c5447587":"# printing all MSE for each parameter combinations\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n     print(np.sqrt(-mean_score), params)","ef43793c":"- We have a positive skewed distribution for sales since the tail is on the right side of the histogram. As we can see, most of the values are clustered to the left side of the x-axis","e125a8a2":"### Tuning our model before deployment","0e1b5a63":"# Model Building and Selection","7d35d532":"# Data Exploration and Visualization","52eef7e3":"### Building model with Random Forest Regressor","e2b17090":"### First of all we need to explain the positives and negatives associated with the numbers obtained.\n- The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the sales tends to go up when there the is a holiday. When the coefficient is close to \u20131, it means that there is a strong negative correlation; you can see a small positive correlation between the fuel price and sales(i.e., sales have a slight tendency to go up when fuel prices go up). Finally, coefficients close to 0 mean that there is no linear correlation.","cb08c472":"# Understanding the dataset and stating our hypothesis\n- The main task is to be build a predictive model for the Walmart sales using the 8 variables or predictors given. Now the question is, which of the independent variables or predictors have a huge influence on the dependent variable weekly sales. It is obvious that holidays contributes to the sales that is, during holidays customers buy lot of items thereby increasing sales therefore stores must be stocked up in order not to run out.\n- Our first hypothesis is, does these independent variables or predictors have influence on sales that is the dependent or target variable which will enable us to make better decisions so to prevent running out of stock.\n- The second hypothesis states that the predictors or independent variables have a high chance of giving us a good predictive model in predicting sales. The independent variables are all economic indicators that affects sales. For instance, the location of a store plays a major role in sales.","4a3e0307":"## Model Evaluation and Deployment\n- Deploying the whole dataset without the target label which we defined as X before splitting.\n- Before deploying, the dataset needs to cleaned especially when we ar using a different dataset.\n- Multiple Linear Regression and Random Forest Regressor are our best model so we will do further testing before choosing the best model for deployment.","be8647db":"- From the visualization above, we can see that on holidays Walmart made lot of sales due to its promotional packages ","b545d98c":"# Description:\n- One of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc.\n- Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.\n# Acknowledgements\n- The dataset is taken from Kaggle.\n### Objective:\n-\tUnderstand the Dataset & cleanup (if required).\n-\tBuild Regression models to predict the sales w.r.t single & multiple features.\n-\tAlso evaluate the models & compare their respective scores like R2, RMSE, etc.","627ad991":"### Building model with multiple linear regression","aee38a5a":"- There is a negative relationship between unemployment and sales. Here, as unemployment increases, sales in stores goes down. ","ba38aadd":"### Further evaluation using cross validation","bce6f811":"### Statistical Inference for variable or feature selection","a8020a54":"# Business Analytics and Visualization","9c245396":"- From the above figures, we have parameters 6 and n_estimator 30","754f420e":"### Building model with Decision Tree Regressor","ea062972":"# Loading Data","cea13ed5":"- Null hypothesis(Ho) states that slopes of the independent variables are equal to zero.\n- Alternate hypothesis(H1) states that slopes of the independent variables are not equal to zero.\n- At the confidence level of 95%, we have some predictors not statistically significant and in appropriation, we are to drop these variables. But in reality or the real world, some predictors may not be statistically significant but a high influence on the predicted variable.\n- For instance, in the reality, fuel prices do have negative relationship with sales in general with an exception to necessities. But with our dataset, the two variables have a small positive relationship due to the fact that, Walmart as a store deal with groceries which are necessities. No matter how high fuel prices goes up, customers has to eat. Therefore, even though we have a p-value greater than the 0.05 which is statistically not significant, we will still add it to the variables to build our model.","26e3d151":"- The relationship between sales and fuel price is a positive one. The increase in fuel prices causes a marginal increase in sales.","0ab7e7c9":"- Random Forest Regressor is chosen as the best model. When it comes to the most influential predictors in the predictive model, we have store number 20, leading the race. The top 5 are all coming from the stores which is consider it to be the location. This means that, location of stores is very important.","f3367ee3":"## We start our data cleaning from here","6bc04e7b":"- There is a negative relationship between the number of stores and sales. Thus, as the number of stores increases, sales decreases. ","dbc0c235":"# Findings and Recommendations\n- (i) Our first hypothesis is, does these independent variables or predictors have influence on sales that is the dependent or target variable which will enable us to make better decisions. From the regression analysis, our model built showed influence or relationship between the predictors and the response(sales). Let us first talk about store or location and sales. There exist a negative relationship between locations or stores and sales. Meaning, as the number of locations or stores increases, sales decreases and vice versa. In reality, we expect that the increase in the locations or stores should bring in more sales. But perhaps, these stores are not located at the right places. For instance, a store in the middle of a desert will not make sales more than a store in New York city. Other predictors with negative relationships with sales are Unemployment, consumer price index(CPI), Temperature etc. We can also talk about the relationship between fuel prices and sales. Our analysis showed a positive relationship between fuel price and sales. In general, an increase fuel price causes prices of items to go up. In view of this phenomenon, we would expect customers or consumers decrease purchases. But this is not the case, consumers are purchasing at a marginal rate. It is not surprising since Walmart is an groceries store which deals with necessities. We need groceries to survive, therefore an increase in fuel prices will not stop consumers from buy from Walmart. \n\n- The second hypothesis states that the predictors or independent variables have a high chance of giving us a good predictive model in predicting sales. Our predictive modeling results showed a higher coefficient determination(R2). Meaning, the predictors are good in predicting sales. The top five independent variables that are important in predicting sales are all from locations or stores. This tells us that, location is very important in predicting sales. \n\n### Recommendation\n- Our finding highlighted an increase in stores causing a decrease in sales. In view of this, many factors can account for this phenomenon. If stores are located in isolated areas, then these stores needs to be looked at. Mismanagement can also be a factor causing stores not to perform well. It was not surprising to see some stores at the bottom of the most influential predictors in predicting sales. Such stores include, 34, 26, 12 and 40.\n- Another findings is holidays promotions worked perfectly well and there is a positive relationship existing between sales and holidays. Therefore, on holidays stores must be stocked in order not to run out.\n- With unemployment, the negative relationship with sales should give a signal that anytime consumers or customers are employed, their purchasing power will go up thereby increasing the demand for items. Stores must stock up with items anytime unemployment index goes down.\n- The increase in fuel price should not scare store managers in stocking up since Walmart sells necessities. Increase in fuel prices does not affect groceries since consumer needs groceries to survive.\n\n### Future work on the dataset\n- We will be using deep learning in predicting sales.","ce301484":"- The evaluation metrics for choosing the best model are the coefficient determination(r-square or R2) and the mean square error(MSE). The \n- From the three regression models that we built, Multiple Linear regression model comes on top with 0.94 R2 and the least MSE. Followed by Random Forest Regressor with 0.93"}}