{"cell_type":{"d6fa02f7":"code","15427ecc":"code","daae39b7":"code","007b2643":"code","8149477a":"code","d8a46506":"code","d2e9cb44":"code","7c928ef0":"code","a98e9760":"code","2634ab81":"code","64f9b745":"code","ddc85756":"code","76860d49":"code","a0a6aadf":"code","a922926a":"code","b5e838f4":"code","dcafd758":"code","f8c131c9":"code","51904488":"code","19a55b60":"code","dd3c4844":"code","7edd7c14":"code","c4ba41b3":"code","21db7960":"code","2fcbaa3e":"code","219dd1ed":"code","61dadd52":"code","14450064":"code","287b55a0":"markdown","7d9292c6":"markdown","cf044263":"markdown","67e44e62":"markdown","433b99c4":"markdown","fd2d7178":"markdown","1da58a9e":"markdown"},"source":{"d6fa02f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15427ecc":"import tensorflow\nprint(tensorflow.__version__)","daae39b7":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt', halt_on_error=False)","007b2643":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')","8149477a":"train_data.info()","d8a46506":"train_data.head()","d2e9cb44":"train_data['target'].value_counts()","7c928ef0":"plt.figure(figsize=(12, 5))\n\nplt.subplot(121)\nplt.bar(train_data['target'].value_counts().index, train_data['target'].value_counts())\nplt.xlabel('Real and not real tweets groups')\nplt.ylabel('Number of targets')\n\nplt.subplot(122)\nplt.bar(train_data['target'].value_counts().index, train_data['target'].value_counts(normalize=True))\nplt.xlabel('Normalized values count')\nplt.ylabel('Real and not real tweets groups')\n\nplt.suptitle('Distribution of target')\nplt.show()","a98e9760":"all_words = []\nfor sent in train_data['text']:\n    tokenize_word = word_tokenize(sent)\n    for word in tokenize_word:\n        all_words.append(word)","2634ab81":"unique_words = set(all_words)\nprint(len(unique_words))","64f9b745":"vocab_length = 28000","ddc85756":"embedded_sentences = [one_hot(sent, vocab_length) for sent in train_data['text']]\nprint(embedded_sentences[:3])","76860d49":"# Let's count max sent vector size\nword_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(train_data['text'], key=word_count)\nlength_long_sentence = len(word_tokenize(longest_sentence))","a0a6aadf":"padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\nprint(padded_sentences[:3])","a922926a":"model = Sequential()\nmodel.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","b5e838f4":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","dcafd758":"early_stopping_callback = EarlyStopping(monitor='val_acc', patience=2)","f8c131c9":"history = model.fit(padded_sentences, train_data['target'], batch_size=100, epochs=20, verbose=1, validation_split=0.2, callbacks=[early_stopping_callback])\n\nprint(\"\\nStop on epoch: \", early_stopping_callback.stopped_epoch)","51904488":"loss, accuracy = model.evaluate(padded_sentences, train_data['target'], verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","19a55b60":"plt.plot(history.history['acc'], label='Test')\nplt.plot(history.history['val_acc'], label='Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Accurancy')\nplt.legend()\nplt.show()","dd3c4844":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","7edd7c14":"test_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","c4ba41b3":"embedded_sentences_test = [one_hot(sent, vocab_length) for sent in test_data['text']]\nprint(embedded_sentences_test[:3])","21db7960":"padded_sentences_test = pad_sequences(embedded_sentences_test, length_long_sentence, padding='post')\nprint(padded_sentences_test[:3])","2fcbaa3e":"submission['target'] = model.predict(padded_sentences_test)","219dd1ed":"submission['target'] = submission['target'].round().astype(int)","61dadd52":"submission['target']","14450064":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","287b55a0":"# EDA","7d9292c6":"# Submission","cf044263":"# Data processing","67e44e62":"# Load libs","433b99c4":"# Model","fd2d7178":"Just another try to predict tweets using embeddings and early stopping.\nFeel free to guide me with my errors.","1da58a9e":"# Train"}}