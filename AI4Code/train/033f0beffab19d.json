{"cell_type":{"259188c1":"code","4e904ba3":"code","1ed665d6":"code","13e3b877":"code","5e578aef":"code","03da1f5e":"code","f37c354c":"code","2a76ca33":"code","2efa11cb":"markdown","386dbdc1":"markdown","9cbf5594":"markdown"},"source":{"259188c1":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold, GridSearchCV\n\n!pip install featexp\nfrom featexp import get_univariate_plots, get_trend_stats","4e904ba3":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')","1ed665d6":"df_train['price'] = np.log1p(df_train['price'])\n\ndf_train = df_train.loc[df_train['id']!=8990]\ndf_train = df_train.loc[df_train['id']!=456]\ndf_train = df_train.loc[df_train['id']!=7259]\ndf_train = df_train.loc[df_train['id']!=2777]\ndf_train = df_train.loc[df_train['bedrooms']<9]\n\nskew_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n\nfor c in skew_columns:\n    df_train[c] = np.log1p(df_train[c].values)\n    df_test[c] = np.log1p(df_test[c].values)\n    \nfor df in [df_train,df_test]:\n    df['date'] = df['date'].apply(lambda x: x[0:8])\n    df['yr_renovated'] = df['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)\n    df['yr_renovated'] = df['yr_renovated'].fillna(df['yr_built'])\n\nfor df in [df_train,df_test]:\n    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n    #df['grade_condition'] = df['grade'] * df['condition']\n    df['sqft_ratio'] = df['sqft_living'] \/ df['sqft_lot']\n    df['sqft_total_size'] = df['sqft_living'] + df['sqft_lot'] + df['sqft_above'] + df['sqft_basement']\n    df['sqft_total15'] = df['sqft_living15'] + df['sqft_lot15'] \n    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n    df['date'] = df['date'].astype('int')\n    \ndf_train['per_price'] = df_train['price']\/df_train['sqft_total_size']\nzipcode_price = df_train.groupby(['zipcode'])['per_price'].agg({'mean','var'}).reset_index()\ndf_train = pd.merge(df_train,zipcode_price,how='left',on='zipcode')\ndf_test = pd.merge(df_test,zipcode_price,how='left',on='zipcode')\ndel df_train['per_price']\n\ny_reg = df_train['price']\ndel df_train['price']\ndel df_train['id']\ntest_id = df_test['id']\ndel df_test['id']\n\ntrain_columns = [c for c in df_train.columns if c not in ['id']]\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nparam = {'num_leaves': 32,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.015,\n         \"min_child_samples\": 15,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": 1,\n         \"nthread\": 8,\n         \"random_state\": 1}\n         \n#prepare fit model with cross-validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=777)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train)):\n    print(str(fold_)+'-th fold')\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][train_columns], label=y_reg.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][train_columns], label=y_reg.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1500)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n    predictions += clf.predict(df_test[train_columns], num_iteration=clf.best_iteration) \/ folds.n_splits\n    print()\ncv = np.sqrt(mean_squared_error(oof, y_reg))\nprint(cv)","13e3b877":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')\ndf_predicted = pd.read_csv('..\/input\/sample_submission.csv')\ndf_predicted['price'] = np.expm1(predictions)\n\n#\uc800\uc758 submission \uacb0\uacfc\uc778\ub370, \ub354 \uc88b\uc740 \uc131\ub2a5\uc758 submission \uacb0\uacfc\ub97c \uac00\uc9c0\uace0 \ud558\uba74 \ub354 \uc88b\uc740 \ubd84\uc11d \uacb0\uacfc\uac00 \ub098\uc62c \uac83 \uac19\uc2b5\ub2c8\ub2e4.\ndf_sub_test = pd.merge(df_test, df_predicted)","5e578aef":"df_train.keys()","03da1f5e":"stats = get_trend_stats(data=df_train, target_col='price', data_test=df_sub_test)","f37c354c":"stats","2a76ca33":"for feature in df_train.keys():\n    if feature == 'id':\n        continue\n    get_univariate_plots(data=df_train, target_col='price', data_test=df_sub_test, features_list=[feature])","2efa11cb":"> \uc704\uc5d0 \uc228\uae34 \ucf54\ub4dc\ub294 light gbm\uc73c\ub85c \uac04\ub2e8\ud788 \uc5f0\uc0b0\ud558\uc5ec \uc5bb\uc740 \uacb0\uacfc\uc785\ub2c8\ub2e4. <br\/>\n> [](https:\/\/www.kaggle.com\/seriousran\/google-reverse-geocoder)\uc5d0\uc11c \uacf5\uac1c\ud55c \ub0b4\uc6a9\uacfc \uac19\uc740 \uacb0\uacfc\uc785\ub2c8\ub2e4. <br\/>\n> \ub354 \uc88b\uc740 \uc131\ub2a5\uc758 predcition \uacb0\uacfc\ub97c \uac00\uc9c0\uace0 \ud558\uba74 \ub354 \uc88b\uc740 \ubd84\uc11d \uacb0\uacfc\uac00 \ub098\uc62c \uac83 \uac19\uc2b5\ub2c8\ub2e4.","386dbdc1":"\uc774 \ucee4\ub110\uc758 \ud575\uc2ec\uc740 \uc608\uce21\ud55c \uacb0\uacfc\uc640 train data\ub97c \ube44\uad50\ud558\uc5ec,\n__noisy\ud55c feature \ucc3e\uae30__,\n__trend correlation \ud655\uc778__\n\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\n\uc81c kernel\uc5d0 vote \ud574\uc8fc\uc2dc\uba74 \uacf5\uc720\uc5d0 \ud06c\ub098\ud070 \ud798\uc774 \ub429\ub2c8\ub2e4.","9cbf5594":"# [featexp](https:\/\/github.com\/abhayspawar\/featexp)\n- featexp\ub294 \uc9c0\ub3c4 \ud559\uc2b5(Supervised Learning)\uc744 \uc704\ud55c Feature \ud0d0\uc0c9\uae30\uc785\ub2c8\ub2e4.\n- feature \uc774\ud574, noisy feature \ucc3e\uae30, feature \ub514\ubc84\uae45, leakage \ucc3e\uae30, model \ubaa8\ub2c8\ud130\ub9c1\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.\n- featexp\ub294 feature\ub97c \ub3d9\uc77c\ud55c \ubaa8\uc9d1\ub2e8 bin\uc5d0 \uc800\uc7a5\ud558\uace0 \uac01 \uc800\uc7a5\uc18c\uc758 \uc885\uc18d \ubcc0\uc218\uc758 \ud3c9\uade0\uac12\uc744 \ud45c\uc2dc\ud569\ub2c8\ub2e4.\n- \uc774 \uadf8\ub9bc\uc744 \uc77d\ub294 \ubc29\ubc95\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n    1. Trend \uadf8\ub798\ud504\ub294 target (\uc774 \ub300\ud68c\uc5d0\uc11c\ub294 price\uac00 \ub418\uaca0\uc8e0)\uacfc feature\uac04\uc758 \uad00\uacc4\ub97c \uc774\ud574\ud558\ub294\ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.\n    2. population distribution\uc740 feature\uc774 \uc62c\ubc14\ub978\uc9c0 \ud655\uc778\ud558\ub294\ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.\n    3. Trend Direction \ubcc0\uacbd \ud69f\uc218 \ubc0f train & test trend \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\uc5b4 noisy feature\ub97c \uc2dd\ubcc4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        - Trend \ubcc0\uacbd \ud69f\uc218\uac00 \ub9ce\uac70\ub098, trend correlation\uc774 \ub0ae\uc740 \uac83\uc740 noise\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4."}}