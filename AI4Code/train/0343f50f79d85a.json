{"cell_type":{"92d822b7":"code","22dfbbcc":"code","b1d1b7ee":"code","2a5aa795":"code","f0f166f5":"code","b1e4a22c":"code","7c8610b8":"code","9985dddd":"code","5b372f41":"code","7f5a89de":"code","821f3b77":"code","f4b37bc1":"code","51dd5c0d":"code","8aacd0f0":"code","9f7ebdec":"code","09deaf37":"code","861be41f":"code","d46472a0":"code","e27d7189":"code","71dd6f19":"code","fd72bf66":"code","7b36420d":"code","f47ac6bc":"code","20214ff0":"code","2b9d4801":"code","77399f35":"code","ff88a3d0":"code","f95f054c":"code","8f7ade21":"code","0ade0468":"code","ca66cf0e":"code","215d7514":"code","e5429e24":"code","3cd0941c":"code","64be0c60":"code","181cb93d":"code","1e4b5b68":"code","8bf24d98":"code","5e48e847":"code","983b0ab8":"code","6f9cd600":"code","03c76bdb":"code","00a5aa9c":"code","44233ed7":"code","da878dc0":"code","3643febd":"code","0dd47321":"code","cea1f1c2":"code","f161fd87":"code","77fb8ad9":"code","54aa299d":"code","6dc09180":"code","50a5ee2a":"code","d2864d9a":"code","de1ea5dc":"code","82365457":"code","8d7911f0":"code","6492f3ad":"code","0454b972":"code","d648e63e":"code","3a53d950":"code","428935f4":"code","3d6672b3":"code","8ec217ca":"code","7cd6f992":"code","7a381155":"code","0426fb2b":"code","e0a6b0ec":"code","c1b27f7d":"code","f09ba63a":"code","62e8fc27":"code","69dd7b5c":"code","ae9c5a03":"code","91e705f9":"code","64fca7f8":"code","0003c4ee":"code","ae17b1cb":"code","35914113":"code","eb7232e3":"code","a9cece43":"code","e727964a":"code","503e357b":"code","f4ad17d7":"code","baedc657":"code","ed9d41fe":"code","2afb41c9":"code","6bf57b5d":"code","25e78b7e":"code","108a2ed6":"code","aa8f283f":"code","02c72b2a":"code","80584368":"code","6e5de80b":"code","5368cc41":"code","8ed2e9d6":"code","f3915cf7":"code","cd0d2a1b":"code","b1e23a97":"code","389b15f9":"code","05f77941":"code","b9f61be2":"code","7d22cc06":"code","04d2117f":"code","e6910de0":"code","23be257f":"code","12be317d":"code","dd21c8bf":"code","eee8b218":"code","115dfdb1":"code","5827fc7c":"markdown","a4c5c6fc":"markdown","aaaf6bde":"markdown","9544bec5":"markdown","7c0afad9":"markdown","eb315991":"markdown","97add988":"markdown","8d664947":"markdown","dbf408d5":"markdown","f56f28cf":"markdown","1c2335c5":"markdown","8f95808c":"markdown","85b94db7":"markdown","661aeecc":"markdown","4089fbc8":"markdown","a4bb27e8":"markdown","21952c89":"markdown","0796af0a":"markdown","ae59ec49":"markdown","108bcc2c":"markdown","9e383fb0":"markdown","86fc35e9":"markdown","fa567272":"markdown","1c43a195":"markdown","aae40651":"markdown","08bfe197":"markdown","9b783904":"markdown","17aa79c8":"markdown","e62347b5":"markdown","e218e159":"markdown","80a03878":"markdown","17089552":"markdown","9d450745":"markdown","79ba2b51":"markdown","c3daf630":"markdown","8aa7d6b8":"markdown","8534f6b6":"markdown","9cffc602":"markdown","07eccd7c":"markdown","8a67bc16":"markdown","535812cf":"markdown","e0ecd7f3":"markdown","095eb50d":"markdown","d5cdf11a":"markdown","9e22371f":"markdown","5d781eb1":"markdown","4cdcf896":"markdown","b230ca33":"markdown","b2d61234":"markdown","416cde4a":"markdown","a2a1eede":"markdown","61c12f17":"markdown","9f12a497":"markdown","78c8c9a0":"markdown","4064fdf8":"markdown","d9d43584":"markdown","4da99bce":"markdown","732e65d3":"markdown","0ac63df0":"markdown","11c80ce9":"markdown","288a4b5d":"markdown","e8fd92e3":"markdown","14fa1070":"markdown","06e939f0":"markdown","2e214075":"markdown","83313465":"markdown","88bc26ce":"markdown","52683f5a":"markdown","55322cb8":"markdown","e6ac9f4f":"markdown","7177654b":"markdown","20909b5f":"markdown","402c2e23":"markdown","9e4f5757":"markdown","1fe1954b":"markdown"},"source":{"92d822b7":"import sys\nprint('Python version: {}'.format(sys.version))\n\nimport pandas as pd\nprint('pandas version: {}'.format(pd.__version__))\n\nimport numpy as np\n\nprint('numpy version: {}'.format(np.__version__))\n\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint('matplotlib version: {}'.format(mlp.__version__))\n\nimport seaborn as sns\nprint('seaborn version: {}'.format(sns.__version__))\n\nimport os\nprint('\\nFile list:',os.listdir('..\/input'))\n\nimport time\nstart_time = time.time()\nimport warnings\nwarnings.filterwarnings('ignore')","22dfbbcc":"df = pd.read_csv('..\/input\/train.csv')\ndf.head()","b1d1b7ee":"df.info()","2a5aa795":"df.columns.to_series().groupby(df.dtypes).groups","f0f166f5":"#show few first rows\ndf.head()","b1e4a22c":"#Overview\ndf.describe()","7c8610b8":"df.isna().sum()","9985dddd":"sns.heatmap(data=df.isna(),yticklabels=False,cmap='coolwarm',cbar=False)","5b372f41":"#Group them by age and find the mean of each Pclass\nplt.figure(figsize=(12, 5))\nax = sns.boxplot(data=df,x=df['Pclass'],y=df['Age'],palette='coolwarm') # create plot object.\nmedians = df.groupby(['Pclass'])['Age'].median().values #get median values\nmedian_labels = [str(np.round(s, 2)) for s in medians] #create label from median values\npos = range(len(medians)) # get range of median values\n#Loop to put value label\nfor tick,label in zip(pos,ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick] + 0.5, median_labels[tick], \n            horizontalalignment='center', size=13, color='r', weight='semibold')","7f5a89de":"#create function to fill age\ndef fill_age_na(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","821f3b77":"df['Age'] = df[['Age','Pclass']].apply(fill_age_na,axis=1)","f4b37bc1":"df['Age'].isna().sum() # no more missing value","51dd5c0d":"col_to_drop = ['Cabin']\ndf.drop(columns=col_to_drop,axis=1,inplace=True)","8aacd0f0":"df.columns # 'Cabin' is now removed.","9f7ebdec":"sns.countplot(x=df['Embarked'])","09deaf37":"#or use mode\ndf['Embarked'].mode()[0]","861be41f":"#Let's fill it\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)","d46472a0":"#check if all are filled\ndf['Embarked'].isna().sum()","e27d7189":"# create name length feature, since I think longer name may harder to call by staff and lead to death\n# you may improve this by removing those initial first(remove Mr. Mrs, Ms, Dr. etc)\ndf['NameLength'] = df['Name'].apply(len)","71dd6f19":"df['NameLength'].hist(bins=30) #most of passenger has name length around 20-30 character","fd72bf66":"# create family size since bigger family may help each other and all survive\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1 # plus 1 for passenger itself","7b36420d":"df['FamilySize'].hist(bins=20) #most of passenger travel alone","f47ac6bc":"# create feature IsAlone to see if the passenger travel alone\ndef IsTravelAlone(col):\n    if col == 1:\n        return 1\n    else:\n        return 0","20214ff0":"df['IsAlone'] = df['FamilySize'].apply(IsTravelAlone)","2b9d4801":"sns.countplot(data=df,x=df['IsAlone']) # most of passenger travel alone","77399f35":"cols_drop = ['PassengerId','Name','Ticket']\ndf.drop(cols_drop, axis=1, inplace = True)","ff88a3d0":"#let's see how each feature interact to each other\nsns.pairplot(data=df,hue='Sex',size=1.2)","f95f054c":"print(df.groupby(['Sex'])['Survived'].mean())\nsns.countplot(x=df['Sex'],hue=df['Survived']) # total number of survived female is higher and survived mean is also higher than male","8f7ade21":"fig = plt.figure(figsize=(10,8))\nsns.violinplot(x='Sex',y='Age',hue='Survived',data=df,split=True)","0ade0468":"print(df.groupby(['Pclass'])['Survived'].mean()) # highest class has 62% survaival rate while lowest class has only 24% survival rate\nsns.catplot(x='Sex',y='Fare',hue='Survived',data=df,col='Pclass',kind='swarm')","ca66cf0e":"grid = sns.FacetGrid(data=df,col='Survived',size=8)\ngrid.map(plt.hist,'Age',bins=50)","215d7514":"#check family size\nsns.countplot(x=df['FamilySize'])","e5429e24":"# is there any relationship between age, fare and class\nsns.jointplot(x='Age',y='Fare',data=df)","3cd0941c":"#set overall size\nfig = plt.figure(figsize=(15,10))\n#set total number of rows and columns\nrow = 5\ncol = 2\n#set title\nfig.suptitle('Various plot',fontsize=20)\n\n#box 1\nfig.add_subplot()\nax = fig.add_subplot(2,2,1)\nsns.countplot(x='Sex',data=df,hue='IsAlone')\n#box 2\nax = fig.add_subplot(2,2,2)\ndf.groupby('Pclass')['Age'].plot(kind='hist',alpha=0.5,legend=True,title='Pclass vs Age')\n#box 3\nax = fig.add_subplot(2,2,3)\ndf.groupby('Pclass')['Fare'].plot(kind='hist',alpha=0.5,legend=True,title='Pclass vs Fare')\n#box 4\nax = fig.add_subplot(2,2,4)\nsns.violinplot(x='Sex',y='Age',data=df,hue='Survived',split=True)\n\n#some more setting\nplt.tight_layout(pad=4,w_pad=1,h_pad=1.5)\nplt.show()","64be0c60":"df.head() #which feature are still categorical","181cb93d":"categorical_feature = []\n#loop each column\nfor i in range(df.shape[1]):\n    #if column datatype is object\/categorical\n    if df[df.columns[i]].dtype == 'object':\n        categorical_feature.append(df.columns[i])\n        \n#show\ncategorical_feature","1e4b5b68":"#convert categorical feature to numerical\n#drop_first=True, will help avoid variable dummy trap\ndf = pd.get_dummies(data=df,columns=categorical_feature,drop_first=True) \ndf.head()","8bf24d98":"fig = plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidths=0.2)\nplt.show()","5e48e847":"from sklearn.model_selection import train_test_split","983b0ab8":"dfX = df.drop('Survived',axis=1)\ndfY = df['Survived']","6f9cd600":"X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.20, \n                                                    random_state=0)\n#I saw some kernel split into train, test and validation. Should I do that to improve the model ?","03c76bdb":"#check size of data\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","00a5aa9c":"from sklearn.preprocessing import StandardScaler","44233ed7":"sc = StandardScaler()","da878dc0":"X_train = sc.fit_transform(X_train) #fit scaler with training data\nX_test = sc.transform(X_test) #apply scaler to test data","3643febd":"X_train[0,:]","0dd47321":"X_test[0,:]","cea1f1c2":"df.corr().loc['Survived']","f161fd87":"# import library to evaluate model\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.model_selection import cross_val_score","77fb8ad9":"# this part just to show why should not use LinearRegression for binary outcome\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nmodel_lm = LinearRegression()\nmodel_lm.fit(X_train,y_train)\npred_lm = model_lm.predict(X_test)\n\n# find bad output\nbad_output = []\nfor i in pred_lm:\n    if i < 0 or i > 1:\n        bad_output.append(i)\n\nbad_output # so let's use LogisticRegression","54aa299d":"model_lg = LogisticRegression(solver='lbfgs')","6dc09180":"model_lg.fit(X_train,y_train)","50a5ee2a":"pred_lg = model_lg.predict(X_test)","d2864d9a":"pred_lg","de1ea5dc":"print(classification_report(y_test,pred_lg)) #classification report\n\n#confusion matrix\ncm = confusion_matrix(y_test, pred_lg)\nplt.figure(figsize=(4,4))\nsns.heatmap(cm, annot=True,cmap='RdYlGn')\nplt.title('Model: LogisticRegression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_lg)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","82365457":"acc_score = [] # create list to store accuracy score","8d7911f0":"def build_train_predict(clf,X_train,y_train,X_test,strAlg,acc_score):\n    '''\n    1. Create model\n    2. Train model\n    3. Prediction\n    4. Evaluate\n    5. Keep score\n    '''\n    model = clf\n    model.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    plot_score(y_test,pred,strAlg,acc_score)\n    return clf,pred","6492f3ad":"# create function to plot score for later use\ndef plot_score(y_test,y_pred,strAlg,lstScore):\n    '''\n    1. Compare prediction versus real result and plot confusion matrix\n    2. Store model accuracy score to list\n    '''\n    lstScore.append([strAlg,accuracy_score(y_test, y_pred)])\n    #print(classification_report(y_test,y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(4,4))\n    sns.heatmap(cm, annot=True,cmap='RdYlGn')\n    plt.title('Model: {0} \\nAccuracy:{1:.3f}'.format(strAlg,accuracy_score(y_test, y_pred)))\n    plt.ylabel('True')\n    plt.xlabel('Predicted')\n    plt.show()","0454b972":"model_lg,pred_lg = build_train_predict(LogisticRegression(),\n                                       X_train,y_train,X_test,\n                                       'LogisticRegression',acc_score)","d648e63e":"from sklearn.neighbors import KNeighborsClassifier","3a53d950":"model_knn,pred_knn = build_train_predict(KNeighborsClassifier(),\n                                       X_train,y_train,X_test,\n                                       'KNN',acc_score)","428935f4":"from sklearn.svm import SVC","3d6672b3":"model_svm,pred_svm = build_train_predict(SVC(),\n                                       X_train,y_train,X_test,\n                                       'SVM',acc_score)","8ec217ca":"from sklearn.naive_bayes import GaussianNB\nmodel_gnb,pred_gnb = build_train_predict(GaussianNB(),\n                                       X_train,y_train,X_test,\n                                       'GaussianNB',acc_score)","7cd6f992":"from sklearn.naive_bayes import BernoulliNB\nmodel_bnb,pred_bnb = build_train_predict(BernoulliNB(),\n                                       X_train,y_train,X_test,\n                                       'BernoulliNB',acc_score)","7a381155":"from sklearn.tree import DecisionTreeClassifier","0426fb2b":"model_dt,pred_dt = build_train_predict(DecisionTreeClassifier(),\n                                       X_train,y_train,X_test,\n                                       'DecisionTreeClassifier',acc_score)","e0a6b0ec":"from sklearn.ensemble import RandomForestClassifier","c1b27f7d":"model_rfc,pred_rfc = build_train_predict(RandomForestClassifier(),\n                                       X_train,y_train,X_test,\n                                       'RandomForestClassifier',acc_score)","f09ba63a":"from sklearn.ensemble import GradientBoostingClassifier","62e8fc27":"model_gbc,pred_gbc = build_train_predict(GradientBoostingClassifier(),\n                                       X_train,y_train,X_test,\n                                       'GradientBoostingClassifier',acc_score)","69dd7b5c":"from sklearn.ensemble import ExtraTreesClassifier","ae9c5a03":"model_et,pred_et = build_train_predict(ExtraTreesClassifier(),\n                                       X_train,y_train,X_test,\n                                       'ExtraTreesClassifier',acc_score)","91e705f9":"from sklearn.ensemble import AdaBoostClassifier","64fca7f8":"model_adb,pred_adb = build_train_predict(AdaBoostClassifier(),\n                                       X_train,y_train,X_test,\n                                       'AdaBoostClassifier',acc_score)","0003c4ee":"from xgboost import XGBClassifier","ae17b1cb":"model_xgb,pred_xgb = build_train_predict(XGBClassifier(),\n                                       X_train,y_train,X_test,\n                                       'XGBClassifier',acc_score)","35914113":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n#get number of input node and number of neuron in hidden layer\ndims = X_train.shape[1]\nh_dims = int((dims+1)\/2)\ndims,h_dims\n\n#create model\nmodel_ann = Sequential() #initialize\n#input\nmodel_ann.add(Dense(units=h_dims,kernel_initializer='uniform',activation='relu',input_dim=dims))\n#hidden\nmodel_ann.add(Dense(units=h_dims,kernel_initializer='uniform',activation='relu'))\n#output\nmodel_ann.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n#compile\nmodel_ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n#train\nmodel_ann.fit(X_train,y_train,batch_size=32,epochs=100,verbose=0)\n\n#evaluate\npred_ann = model_ann.predict(X_test)\npred_ann = pred_ann > 0.5\nplot_score(y_test,pred_ann,'ANN',acc_score)","eb7232e3":"# See the summary, which model is leading\ndf_acc = pd.DataFrame(acc_score,columns=['Name','TestScore']).sort_values(by=['TestScore','Name'],ascending=False)\ndf_acc","a9cece43":"from sklearn.model_selection import cross_val_score\n\n#create function to store\ndef cross_val_MinMaxMean(clf,X_train,y_train,fold):\n    scores = cross_val_score(clf,X_train,y_train,cv=fold)\n    print('Min: {} \\nMax: {} \\nMean: {}'.format(scores.min(),scores.max(),scores.mean()))","e727964a":"cross_val_MinMaxMean(LogisticRegression(),X_train,y_train,10)","503e357b":"cross_val_MinMaxMean(KNeighborsClassifier(),X_train,y_train,10)","f4ad17d7":"cross_val_MinMaxMean(SVC(),X_train,y_train,10)","baedc657":"cross_val_MinMaxMean(GaussianNB(),X_train,y_train,10)","ed9d41fe":"cross_val_MinMaxMean(DecisionTreeClassifier(),X_train,y_train,10)","2afb41c9":"cross_val_MinMaxMean(RandomForestClassifier(),X_train,y_train,10)","6bf57b5d":"cross_val_MinMaxMean(GradientBoostingClassifier(),X_train,y_train,10)","25e78b7e":"cross_val_MinMaxMean(ExtraTreesClassifier(),X_train,y_train,10)","108a2ed6":"cross_val_MinMaxMean(AdaBoostClassifier(),X_train,y_train,10)","aa8f283f":"cross_val_MinMaxMean(XGBClassifier(),X_train,y_train,10)","02c72b2a":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(h_dims,input_dim=dims,activation='relu'))\n    model.add(Dense(h_dims,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model,epochs=100,batch_size=10,verbose=0)\nkfold = StratifiedKFold(n_splits=10,shuffle=True)\ncross_val_MinMaxMean(model,X_train,y_train,kfold)","80584368":"#import library for model improvement\nfrom sklearn.model_selection import GridSearchCV\n\n# function to reduce coding\ndef wrap_gridsearchCV(clf,X_train,y_train,X_test,param_grid,strAlg,acc_score):\n    '''\n    1. Create GridSearch model\n    2. Train model\n    3. Predict\n    4. Evaluate\n    5. Keep score\n    '''\n    model = GridSearchCV(estimator=clf,param_grid=param_grid,cv=10,\n                         refit=True,verbose=0,n_jobs=-1)\n    model.fit(X_train,y_train)\n    print('\\nBest hyper-parameter: {} \\n'.format(model.best_params_))\n    pred = model.predict(X_test)\n    plot_score(y_test,pred,strAlg,acc_score)\n    return model,pred","6e5de80b":"param_grid = {\n    'C': [0.1,1, 10, 100, 1000],\n    'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],\n}\nmodel_grid_lg,pred_grid_lg = wrap_gridsearchCV(LogisticRegression(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'LogisticRegression GCV',acc_score)","5368cc41":"param_grid = {\n    'n_neighbors': [i for i in range(1,51)]\n}\nmodel_grid_knn,pred_grid_knn = wrap_gridsearchCV(KNeighborsClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'KNN GCV',acc_score)","8ed2e9d6":"param_grid = {\n    'C': [0.1,1, 10, 100, 1000],\n    'gamma': [1,0.1,0.01,0.001,0.0001]\n}\nmodel_grid_svm,pred_grid_svm = wrap_gridsearchCV(SVC(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'SVM GCV',acc_score)","f3915cf7":"# there is no hyper-parameter to play with","cd0d2a1b":"param_grid = {\n    'max_depth': [None,1,2,3,4,5,7,8,9,10],\n    'criterion': ['gini', 'entropy']\n}\nmodel_grid_dt,pred_grid_dt = wrap_gridsearchCV(DecisionTreeClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'DecisionTreeClassifier GCV',acc_score)","b1e23a97":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)],\n    'min_samples_leaf': [2,3,4,5]\n}\nmodel_grid_rfc,pred_grid_rfc = wrap_gridsearchCV(RandomForestClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'RandomForestClassifier GCV',acc_score)","389b15f9":"param_grid = {\n    'loss': ['deviance', 'exponential'],\n    'n_estimators': [i for i in range(100,1000,100)],\n    'min_samples_leaf': [1,2,3,4,5]\n}\nmodel_grid_gbc,pred_grid_gbc = wrap_gridsearchCV(GradientBoostingClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'GradientBoostingClassifier GCV',acc_score)","05f77941":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)],\n    'min_samples_leaf':[2,3,4,5]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(ExtraTreesClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'ExtraTreesClassifier GCV',acc_score)","b9f61be2":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'learning_rate' : [0.25, 0.75, 1.00]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(AdaBoostClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'AdaBoostClassifier GCV',acc_score)","7d22cc06":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(XGBClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'XGBClassifier GCV',acc_score)","04d2117f":"## later","e6910de0":"#Take feature importance to select feature for next training\ndt_fi = model_dt.feature_importances_\nrfc_fi = model_rfc.feature_importances_\ngbc_fi = model_gbc.feature_importances_\net_fi = model_et.feature_importances_\nada_fi = model_adb.feature_importances_\nxgb_fi = model_xgb.feature_importances_\n\nfi = [dt_fi,rfc_fi,gbc_fi,et_fi,ada_fi,xgb_fi]\nmodel_name = ['DecisionTree','RandomForrest','GradientBoost',\n        'ExtraTree','AdaBoost','XGBoost']\nmodel_name = pd.Series(model_name)\ndf_fi = pd.DataFrame(fi,columns=dfX.columns)\ndf_fi.index = model_name\ndf_fi","23be257f":"#set overall size\nfig = plt.figure(figsize=(20,10))\n#set total number of rows and columns\nrow = 2\ncol = 3\n#set title\nfig.suptitle('Feature importance',fontsize=20)\n\n# boxes\nfor index,i in enumerate(df_fi.index):\n    fig.add_subplot()\n    ax = fig.add_subplot(2,3,index+1)\n    sns.barplot(df_fi.loc[i],df_fi.columns)\n    \n\n#some more setting\nplt.tight_layout(pad=4,w_pad=1,h_pad=1.5)\nplt.show()","12be317d":"# Final score table","dd21c8bf":"pd.DataFrame(acc_score,columns=['model','score']).sort_values(by=['score','model'],\n                                                              ascending=False)","eee8b218":"#display tree graph\nimport graphviz\nfrom sklearn import tree\ntree_dot = tree.export_graphviz(model_dt,out_file=None, \n                                feature_names = dfX.columns, class_names = True,\n                                filled = True, rounded = True)\ntree_img = graphviz.Source(tree_dot) \ntree_img","115dfdb1":"print(\"--- %s seconds ---\" % (time.time() - start_time))","5827fc7c":"### 7.2.2 K-Nearest-Neigbors","a4c5c6fc":"It's also interesting to see what is age range of those male\/female who survived.\nYou may see that most of female who survived has lower age than female as well.","aaaf6bde":"# 1. What is this dataset all about\nThe Titanic was the largest ship afloat at the time of her maiden voyage and carried 2,224 people on that maiden voyage from Southampton on 10th April 1912. Her destination was New York City, America but her last port of call was at Cobh (Queenstown), Ireland on 11th April 1912.\n\nThis dataset contain all passengers record with their surviability.","9544bec5":"## 7.2 Cross validation\nAccuracy obtain from train dataset can be bias since some extreme observation may not exist in train dataset and lead to bad prediction when facing test dataset.\ncross-validation with Kfold would help on this issue. The score from cross validated model would be more reliable \n\n**This part will not be added to accuracy summmary table**","7c0afad9":"## 5.4 Normalization\/Standarization\nThere are few popular method to perform this task, it's depend on character of your data.\nFor this study I will use StandardScaler as my tool","eb315991":"**At the end of EDA, we may talk to friend or some domain expert to see if our insight are align with the history\/domain knowledge or not**","97add988":"### 7.3.10 XGBoost","8d664947":"### 7.2.6 Random forrest","dbf408d5":"## 3.4 Feature engineering\/Create new usful feature","f56f28cf":"# 4. Exploratory Data Analysis\nPersonally, I love this part the most. I want to see what data is telling me.","1c2335c5":"### From `.info()` we know that \n- Numerical feature are ['PassengerId', 'Survived', 'Pclass', 'SibSp', 'Parch','Age', 'Fare'] \n- Categorical feature are ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']","8f95808c":"### Additional exploration","85b94db7":"# About me and this notebook\n_About me_ : I am a person who has no degree in math or computer science. I work in data management area and learn Python by myself in last 2 years, but very small chance to use it in my work. I preferred to use Excel + VBA to complete all data related taks. I hope that keep practicing would make me become a data scientist someday.\n\n_About notebook_ : This notebook is to conclude what I've learned from various source, I am so new to this area please feel free to comment & suggest. I will learn from you guys.\n\nThanks,<br>\n**Nopp**\n\n-----\n**Credit\/Reference**:\n- https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\n- https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n- https:\/\/www.kaggle.com\/sshadylov\/titanic-solution-using-random-forest-classifier","661aeecc":"### 7.2.10 XGBoost","4089fbc8":"### 7.3.8 Extra Trees","a4bb27e8":"**Create function to reduce coding**","21952c89":"### 7.1.9 Adaboost","0796af0a":"### 7.3.7 Gradient Boost","ae59ec49":"**Let's fill them up one by one**\n- Age: I think passengers in different classes may have different income rate and age, older may be the one who earn more salary and can spend more. So I will group by `Pclass` then fill missing value with mean.\n- Cabin: There are too many missing data, so I will remove this feature\n- Embarked: Since only 2 of them are missing, and they are not numerical, So I can not replace missing value with mean value. I will fill it by most appear category","108bcc2c":"### 7.2.7 Gradient Boost","9e383fb0":"### 2. Passenger who in higher class will has higher since staff may help them first.\nplot below show that half or more than half of Pclass=1 are survived, and most of surviver are female that has fare at","86fc35e9":"### 7.2.9 Adaboost","fa567272":"### 7.3.6 Random forrest","1c43a195":"### 7.1.3 Suport Vector Machine","aae40651":"# 5. Preprocessing\nSince machine learning accept only numerical value, we have to convert all text to number.\n\n## 5.1 Convert all categorical feature to nummerical","08bfe197":"### 7.3.3 Suport Vector Machine","9b783904":"### 1. Male or Female has higher survive rate ?","17aa79c8":"### 7.1.10 XGBoost","e62347b5":"## 3.2 Take care missing data","e218e159":"## 5.2 See how each feature are correlated","80a03878":"### 7.2.4 Naive Bayes","17089552":"### 7.3.2 K-Nearest-Neigbors","9d450745":"# 3. Clean up process","79ba2b51":"### 3. Age 20-30 should have highest survive rate since they are in the healthiest age range\n20-40 is age range that has highest survived rate, my hypothesis is partially true","c3daf630":"## 3.3 Correct data\nI will skip this part since I don't see any value to correct","8aa7d6b8":"**Embarked** I will fill missing value by most frequent appear value","8534f6b6":"### 7.1.4 Naive Bayes","9cffc602":"### 7.1.7 Gradient Boost","07eccd7c":"# 0. Import some required dependencies","8a67bc16":"### 7.2.8 Extra Trees","535812cf":"### Now we know that\n- Age, Cabin and Embarked have some missing data.\n- Suvived rate is 0.3(30%)\n- Age has quite high variance compare to other features, 14.52 and 49.69\n- Oldest passenger is 80 years old\n- Highest fare is 512.32 USD\n- Sib, Parch is maximum at 8 and 6 accordingly.","e0ecd7f3":"### 7.1.8 Extra Trees","095eb50d":"### Which model should I use ? Please feel free to suggest\n\nThere are some more tasks to do on this kernel, I will continue soon.\nThanks to anyone who's reading this ;)","d5cdf11a":"## 3.5 Cut unwanted feature\nBase on my understanding to the data, I want to remove following feature before analysis\n- PassengerID: It's just id of the passenger\n- Name: I think name length can represent this feature in more usable way, name itself not usful.\n- Ticket: I don't think ticket number will relate to surviability at all, however if its telling where the passenger sit, may be we can keep it with new formatting.","9e22371f":"## 7.1 Try various machine learning algorithm\nSince other machine learning algorithm may perform better on this problem, I will try following model to see if they can give better performance.\n\nTo perform this part I've created a list to store accuracy score of each model.","5d781eb1":"### 7.2.1 Logistic Regression","4cdcf896":"## 6.1 Create baseline model","b230ca33":"### 7.3.5 Decision Tree","b2d61234":"## 7.3 Find better hyper parameters\n\nThis part will try to find better hyper-parameters for each model using `GridSearchCV`","416cde4a":"## 7.4 Feature selection\nAll input features can be `Signal` or `Noise`, we may try to keep only feature that seems to give better signal of our model and remove some noise feature.\nThis would give better model accuracy, we will test this below.\n\nSelecting feature is crucial, I may select a list of feature that well correlated with target feature(`survived`) using correlation matrix but it's may lead to overfitting as well.\nI would be better if model can tell us which feature is more important than other.\n\nTree based classifier has this attribute after the model is trained.","a2a1eede":"# 7. Model improvement\nSince the baseline model may not best fit to this problem, we should try different model, hyper-parameter or data shuffle to improve the model performance.\n\n** To improve the model, I plan to perform following task **\n1. Try various machine learning algorithm. Since each algorithm was developed to solve certain problem, so it other model may fit to this problem better than logistic regression. Other machine learning algorithm that I want to try are below.\n    - K-Nearest-Neigbors\n    - Support Vector Machine\n    - Naive bayes\n    - Decision Tree\n    - Random forrest\n    - GradientBoosting\n    - XGBoost\n    - Artificial Neural Network\n2. Apply cross validation for better generalization\n3. Find better hyper-parameters for each machine learning algorithm\n4. Feature selection\n5. Use differrerent normalization method\n","61c12f17":"### 7.1.5 Decision Tree","9f12a497":"Now all feature are scaled and ready to use as model input.","78c8c9a0":"**Evaluate the model**","4064fdf8":"### 7.3.1 Logistic Regression","d9d43584":"## 3.1 Overview\nBefore make any change to the input data, we should quickly see through input data and gain some basic understanding first","4da99bce":"### 7.1.6 Random forrest","732e65d3":"### 7.1.11 Artificial Neural Network","0ac63df0":"## Let's answer the questions","11c80ce9":"### 7.3.9 Adaboost","288a4b5d":"# 2. Ask some intesting questions\/Create hypothesis \n- Male or Female has higher survival rate ?\n- Passenger who in higher class would have higher rate of survive since the staff will help them first.\n- Age 20-30 should have highest survive rate since they are in the healthiest condition.","e8fd92e3":"**So my baseline model will be LogisticRegression**","14fa1070":"**Cabin** since there is too many missing value, I will remove it from dataframe\nHowever, I saw some kernel use this data as representative of passenger location before disaster","06e939f0":"# 6. Model building\nSince I saw that some feature are correlated(has linear relationship) with `Survived`.\nSo I wanted to use `LinearRegression`, but it would not be possible since the output is not limited to 0 and 1. It's can be higher than 1 or lower than 0.","2e214075":"### 7.2.3 Suport Vector Machine","83313465":"## 5.3 Split train and test dataset\nSince I have no label for test dataset, So I can not evaluate the model performance. So I will split the train dataset into train and test dataset\n","88bc26ce":"### 7.1.1 Logistic Regression","52683f5a":"### 7.2.11 Artificial Neural Network","55322cb8":"**Since the steps of each model would be similar to each other, I will use function to wrap those processes into 1 line**","e6ac9f4f":"### 7.2.5 Decision Tree","7177654b":"-------------------------------\n\n# Misc","20909b5f":"### Data dictionary\n- **survival** : Survival\t: 0 = No, 1 = Yes\n- **pclass**\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n- **sex**: gender of the passenger\n- **Age**: age in year unit\n- **sibsp**: number of siblings \/ spouses aboard the Titanic\t\n- **parch**: number of parents \/ children aboard the Titanic\t\n- **ticket**: Ticket number\t\n- **fare**: Passenger fare\t\n- **cabin**: Cabin number\t\n- **embarked**:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n-----\n### Variable note\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\n\n**Sibling** = brother, sister, stepbrother, stepsister\n\n**Spouse** = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**: The dataset defines family relations in this way...\n\n**Parent** = mother, father\n\n**Child** = daughter, son, stepdaughter, stepson, some children travelled only with a nanny, therefore parch=0 for them.","402c2e23":"### 7.3.11 Artificial Neural Network","9e4f5757":"### 7.1.2 K-Nearest-Neigbors\n","1fe1954b":"### 7.3.4 Naive Bayes"}}