{"cell_type":{"7a50145d":"code","a1c5911d":"code","11a4f572":"code","5d1bbef5":"code","b7c91cac":"code","c703527b":"code","72e90ffb":"code","a49ef8c5":"code","35235a97":"code","9a878716":"code","ab190dcb":"code","8becc8e2":"code","2210f16e":"code","2040160f":"code","5444c81d":"code","b39b5a11":"code","2bae315a":"code","1f4138d2":"code","ca9458ee":"code","32c04439":"code","ae6b3844":"code","c97ec128":"markdown","98f7019e":"markdown","21c1dfba":"markdown","30e64436":"markdown","e0182389":"markdown","b6cf6c2a":"markdown","101b249f":"markdown","6c27a90a":"markdown","1ef02d9b":"markdown"},"source":{"7a50145d":"# !conda install -c conda-forge gdcm -y","a1c5911d":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport pathlib\nimport pickle\nimport time\nimport copy\nimport pydicom\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\n\n# import tensorflow.keras.backend\n# from tensorflow.keras import layers as L\nfrom skimage.transform import resize\nfrom scipy.ndimage import zoom\nimport scipy.ndimage as ndimage\nfrom skimage import measure, morphology, segmentation\n\nfrom math import ceil","11a4f572":"# raw_train = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/data-preparation-for-osic\/train.csv')\n\n# size_image = pd.read_csv('\/kaggle\/input\/prep-data\/size_image.csv')\n# list_files = pd.read_csv('\/kaggle\/input\/prep-data\/list_files.csv', converters={\"files\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})","5d1bbef5":"#Constant\nTRAIN_PATH = '\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train'\nTEST_PATH = '\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/test'\n\nPINBALL_QUANTILE = np.array([0.2, 0.50, 0.8])\nLAMBDA_LOSS = 0.75\nID='Patient_Week'\n\nDESIRED_SIZE = (50,512,512)\nBATCH_SIZE = 1\nTEST_BATCH_SIZE = 1\nMASK_ITERATION = 4\n\nclip_bounds = (-1000, 200)\npre_calculated_mean = 0.02865046213070556\n\nPROB_DROPOUT = 0.2","b7c91cac":"# start_time = time.perf_counter()\n# for i in train.loc['Patient'].unique():\n#     file = h5py.File('\/kaggle\/input\/creating-dataset-2\/'+ i +'.h5', 'r')\n#     np.array(file.get('data'))[0,0,:,:,0]\n#     tf.print(\"End epoch:\", time.perf_counter() - start_time)\n#     file.close","c703527b":"test['Min_week'] = test.groupby('Patient')['Weeks'].transform('min')\nbase = test.loc[test.Weeks == test.Min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','Base_FVC']\ntest = test.merge(base, on='Patient', how='left')\ntest['Base_week'] = test['Weeks'] - test['Min_week']","72e90ffb":"from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n\n#Override OneHotEncoder to have the column names created automatically (Ex-smoker, Never Smoked...) \nclass OneHotEncoder(SklearnOneHotEncoder):\n    def __init__(self, **kwargs):\n        super(OneHotEncoder, self).__init__(**kwargs)\n        self.fit_flag = False\n\n    def fit(self, X, **kwargs):\n        out = super().fit(X)\n        self.fit_flag = True\n        return out\n\n    def transform(self, X, categories, index='', name='', **kwargs):\n        sparse_matrix = super(OneHotEncoder, self).transform(X)\n        new_columns = self.get_new_columns(X=X, name=name, categories=categories)\n        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=index)\n        return d_out\n\n    def fit_transform(self, X, categories, index, name, **kwargs):\n        self.fit(X)\n        return self.transform(X, categories=categories, index=index, name=name)\n\n    def get_new_columns(self, X, name, categories):\n        new_columns = []\n        for j in range(len(categories)):\n            new_columns.append('{}_{}'.format(name, categories[j]))\n        return new_columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.exceptions import NotFittedError\n\ndef standardisation(x, u, s):\n    return (x-u)\/s\n\ndef normalization(x, ma, mi):\n    return (x-mi)\/(ma-mi)\n\nclass data_preparation():\n    def __init__(self, bool_normalization=True,bool_standard=False):\n        self.enc_sex = LabelEncoder()\n        self.enc_smok = LabelEncoder()\n        self.onehotenc_smok = OneHotEncoder()\n        self.standardisation = bool_standard\n        self.normalization = bool_normalization\n        \n        \n    def __call__(self, data_untransformed):\n        data = data_untransformed.copy(deep=True)\n        \n        #For the test set\/Already fitted\n        try:\n            data['Sex'] = self.enc_sex.transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n            \n            #Normalization\n            if self.normalization:\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n        #For the train set\/Not yet fitted    \n        except NotFittedError:\n            data['Sex'] = self.enc_sex.fit_transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.fit_transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.fit_transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                self.base_week_mean = data['Base_week'].mean()\n                self.base_week_std = data['Base_week'].std()\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n\n                self.base_fvc_mean = data['Base_FVC'].mean()\n                self.base_fvc_std = data['Base_FVC'].std()\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n\n                self.base_percent_mean = data['Base_percent'].mean()\n                self.base_percent_std = data['Base_percent'].std()\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n\n                self.age_mean = data['Age'].mean()\n                self.age_std = data['Age'].std()\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n\n                self.weeks_mean = data['Weeks'].mean()\n                self.weeks_std = data['Weeks'].std()\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n\n                \n            #Normalization\n            if self.normalization:\n                self.base_week_min = data['Base_week'].min()\n                self.base_week_max = data['Base_week'].max()\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n\n                self.base_fvc_min = data['Base_FVC'].min()\n                self.base_fvc_max = data['Base_FVC'].max()\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n\n                self.base_percent_min = data['Percent'].min()\n                self.base_percent_max = data['Percent'].max()\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n\n                self.age_min = data['Age'].min()\n                self.age_max = data['Age'].max()\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n\n                self.weeks_min = data['Weeks'].min()\n                self.weeks_max = data['Weeks'].max()\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                \n                self.base_week_min = data['Min_week'].min()\n                self.base_week_max = data['Min_week'].max()\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n            \n        return data","a49ef8c5":"pickefile = open('\/kaggle\/input\/data-preparation-for-osic\/data_prep', 'rb')\ndata_prep = pickle.load(pickefile)\npickefile.close()\ntest = data_prep(test)","35235a97":"class DataGenerator(tf.keras.utils.Sequence):\n    \n    def on_epoch_end(self):#Indices=np.arange(size of dataset)\n        self.indices = np.arange(len(self.patients))\n    \n    def __len__(self):\n        return int(ceil(len(self.indices) \/ self.batch_size))\n    \n    def __init__(self, train, patients, batch_size=1, desired_size=(10,512,512), img_path=TRAIN_PATH, *args, **kwargs):\n        self.train = train\n        self.patients = patients\n        self.batch_size = batch_size\n        self.desired_size = desired_size\n        self.img_path = img_path\n        self.on_epoch_end()\n    \n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        patients = [self.patients[k] for k in indices]\n#         imgs = _read(self.img_path, patients=patients, desired_size=self.desired_size)\n        if 0<=index<=47:\n            notebook = '1'\n        elif 48<=index<=95:\n            notebook = '2'\n        elif 96<=index<=142:\n            notebook = '3'\n        elif 143<=index<=177:\n            notebook = '4'\n        file = h5py.File('\/kaggle\/input\/creating-dataset-'+ notebook +'\/'+ patients[0] +'.h5', 'r')\n        imgs = np.array(file.get('data'))\n        file.close()\n        \n        \n#         Data Augmentation\n#         Rotation\n        rand = np.random.randint(1,20)\n        if rand == 1:\n            print('Rotate1')\n            imgs = scipy.ndimage.rotate(imgs, np.random.randint(10,180), axes=(2,3), reshape=False)\n        elif rand == 2:\n            print('Rotate2')\n            imgs = scipy.ndimage.rotate(imgs, np.random.randint(-180,-10), axes=(2,3), reshape=False)\n        \n        #Shift\n        if np.random.randint(1,10) == 1:\n            print('Shift')\n            imgs = scipy.ndimage.shift(imgs, [0, np.random.randint(-8,8), np.random.randint(-64,64), np.random.randint(-64,64), 0], order=0)\n        \n        #Flip\n        if  np.random.randint(1,20) == 1:\n            print('Flip')\n            imgs = np.flip(imgs, axis = np.unique(np.random.randint(2,4, size=np.random.randint(1,3))))\n        \n#         if  np.random.randint(1,3) == 1:  \n#             imgs = zoom(imgs, [1,np.random.random()+1,np.random.random()+1,np.random.random()+1,1], mode='nearest')\n#         #Contrast\n#         factor=np.random.randint(-3, 3)\/1000\n#         if factor != 0:\n#             imgs = 128\/255 + factor * imgs - factor * 128\/255\n        \n        #Zoom\n#         if  np.random.randint(1,20) == 1:\n#             print('Zoom1')\n#             factor = min(round(np.random.random()+1, 2), 1.5)\n#             shape = imgs.shape[3]\n#             if factor>1:\n#                 imgs = zoom(imgs, [1, 1, 1, factor,1], mode='nearest', order=1)\n#                 imgs = imgs[:,:,:,round((imgs.shape[3]-shape-1)\/2):-round((imgs.shape[3]-shape-1)\/2),:][:,:,:,:DESIRED_SIZE[2],:]\n                \n#         if  np.random.randint(1,20) == 1:\n#             print('Zoom2')\n#             factor = min(round(np.random.random()+1, 2), 1.5)\n#             shape = imgs.shape[2]\n#             if factor>1:\n#                 imgs = zoom(imgs, [1, 1, factor, 1,1], mode='nearest', order=1)\n#                 imgs = imgs[:,:,round((imgs.shape[2]-shape-1)\/2):-round((imgs.shape[2]-shape-1)\/2),:,:][:,:,:DESIRED_SIZE[1],:,:]\n                \n#         if  np.random.randint(1,20) == 1:\n#             print('Zoom3')\n#             factor = min(round(np.random.random()+1, 2), 1.5)\n#             shape = imgs.shape[1]\n#             if factor>1:\n#                 imgs = zoom(imgs, [1, factor, 1, 1,1], mode='nearest', order=1)\n#                 imgs = imgs[:,round((imgs.shape[1]-shape-1)\/2):-round((imgs.shape[1]-shape-1)\/2),:,:,:][:,:DESIRED_SIZE[0],:,:,:]\n                \n        return [self.train[self.train['Patient'].isin(patients)].reset_index(drop=True), imgs], np.asarray(self.train[self.train['Patient'].isin(patients)]['FVC'])   ","9a878716":"train_generator = DataGenerator(train, train.Patient.unique(), batch_size=BATCH_SIZE, desired_size=DESIRED_SIZE, img_path=TRAIN_PATH)\ntest_generator = DataGenerator(test, test.Patient.unique(), batch_size=TEST_BATCH_SIZE, desired_size=DESIRED_SIZE, img_path=TEST_PATH)","ab190dcb":"# for [X1, X2], Y in train_generator:\n#     if X2.shape != (1,50,512,512,1):\n#         print(X2.shape)","8becc8e2":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return tf.keras.backend.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = PINBALL_QUANTILE\n    q = tf.constant(PINBALL_QUANTILE, dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return tf.keras.backend.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss","2210f16e":"# MLP = tf.keras.models.load_model('\/kaggle\/input\/simple-mlp-training\/model', compile=False, custom_objects={'score':score})","2040160f":"image_input = tf.keras.Input(shape=(DESIRED_SIZE[0],DESIRED_SIZE[1],DESIRED_SIZE[2],1) ,name=\"img_input\")\nx = tf.keras.layers.Conv3D(filters=8, kernel_size=(1,3,3), padding='valid', activation='relu')(image_input)\nx = tf.keras.layers.Conv3D(filters=8, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.MaxPool3D(pool_size=(1,2,2),strides=(1,2,2))(x)\nx = tf.keras.layers.Conv3D(filters=32, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.Conv3D(filters=32, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.MaxPool3D(pool_size=(1,2,2),strides=(1,2,2))(x)\nx = tf.keras.layers.Conv3D(filters=64, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.Conv3D(filters=64, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.MaxPool3D(pool_size=(2,2,2),strides=(2,2,2))(x)\nx = tf.keras.layers.Conv3D(filters=256, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.Conv3D(filters=256, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.MaxPool3D(pool_size=(2,2,2),strides=(2,2,2))(x)\nx = tf.keras.layers.Conv3D(filters=512, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.Conv3D(filters=512, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.Conv3D(filters=512, kernel_size=(1,3,3), padding='valid', activation='relu')(x)\nx = tf.keras.layers.MaxPool3D(pool_size=(2,2,2),strides=(2,2,2))(x)\nx = tf.keras.layers.GlobalMaxPooling3D()(x)\ncnn_output= tf.keras.layers.Dense(100, activation='relu')(x)\n\n\nnumpy_input = tf.keras.Input(shape=(10,), name=\"numpy_input\")\n# x = MLP.get_layer('dense_to_freeze1')(numpy_input)\n# x = MLP.get_layer('dense_to_freeze2')(x)\nx = tf.keras.layers.Dense(500, activation='relu', name='dense_to_freeze1')(numpy_input)\nx = tf.keras.layers.Dense(100, activation='relu', name='dense_to_freeze2')(x)\nx = tf.keras.layers.GaussianNoise(0.2, name='GaussianNoise')(x)\nx = tf.keras.layers.concatenate([cnn_output, x], axis=1, name='concatenate')\nx = tf.keras.layers.Dropout(PROB_DROPOUT, name='Dropout')(x)\nx = tf.keras.layers.Dense(100, activation='relu')(x)\nx = tf.keras.layers.Dense(20, activation='relu')(x)\noutput= tf.keras.layers.Dense(3)(x)","5444c81d":"CNN = tf.keras.Model(inputs=image_input, outputs=cnn_output)\nmodel = tf.keras.Model(inputs=[numpy_input, CNN.input], outputs=output)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999), loss=mloss(LAMBDA_LOSS))\n    \n# model.get_layer('dense_to_freeze1').trainable=False\n# model.get_layer('dense_to_freeze2').trainable=False\ntf.keras.utils.plot_model(model, show_shapes=True)","b39b5a11":"def eval_score(y_true, y_pred):\n    y_true = tf.dtypes.cast(y_true, tf.float32)\n    y_pred = tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return -tf.keras.backend.mean(metric)","2bae315a":"model = tf.keras.models.load_model('\/kaggle\/input\/3d-cnn-mlp\/model_4', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score})\nCNN = tf.keras.models.load_model('\/kaggle\/input\/3d-cnn-mlp\/CNN_4', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score})","1f4138d2":"# pre_trained_model = tf.keras.models.load_model('\/kaggle\/input\/3d-cnn-mlp\/model_9', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score})\n# pre_trained_CNN = tf.keras.models.load_model('\/kaggle\/input\/3d-cnn-mlp\/CNN_9', custom_objects={'loss':mloss(LAMBDA_LOSS), 'score':score})\n# CNN.set_weights(pre_trained_CNN.get_weights())\n# model.set_weights(pre_trained_model.get_weights())","ca9458ee":"# for i in range(len(CNN.layers)):\n#     CNN.layers[i].set_weights(pre_trained_CNN.layers[i].get_weights())\n#     CNN.layers[i].trainable = False","32c04439":"def train_step(X1, X2, Y, j, list_loss):\n    with tf.GradientTape() as tape:\n        X2 = tf.convert_to_tensor(X2)\n        tape.watch(X2)\n        out_imgs = CNN(X2)\n        X_imgs = tf.concat([tf.repeat(tf.reshape(out_imgs[i], (1,-1)), X1['Patient'].value_counts()[i], axis=0) for i in range(len(X1['Patient'].value_counts()))], axis=0)\n        X1 = tf.convert_to_tensor(np.asarray(X1[SELECTED_COLUMNS]))\n        tape.watch(X1)\n        X_tabular = model.get_layer('GaussianNoise')(model.get_layer('dense_to_freeze2')(model.get_layer('dense_to_freeze1')(X1)), training=True)\n        inp_mlp = model.get_layer('Dropout')(model.get_layer('concatenate')([X_imgs, X_tabular]), training=True)\n        inp_mlp = model.layers[-1](model.layers[-2](model.layers[-3](inp_mlp)))\n        loss = model.compiled_loss(tf.constant(Y), inp_mlp, regularization_losses=model.losses)\n    print('loss calculated')\n    grads = tape.gradient(loss, model.trainable_weights)\n    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n#     if loss > 30:\n#         model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#   if loss > 40:\n#       model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#       print('Optimized 2 times')\n#   if loss > 50:\n#       model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#       print('Optimized 3 times')\n#     if loss > 60:\n#         model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#         model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#         model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#         model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n#         print('Optimized 5 times')\n    list_loss.append(loss)\n            \n    print(\"Training loss (for one batch) at step %d: %.4f\" % (j, float(loss)))\n    return list_loss\n    \n\ndef eval_step():\n    y_eval_pred = np.array([[0,0,0]])\n    y_eval_true = np.array([0])\n    for [X1_eval, X2_eval], Y_eval in test_generator:\n        X2_eval = tf.convert_to_tensor(X2_eval)\n        out_imgs = CNN(X2_eval)\n        X_imgs = tf.concat([tf.repeat(tf.reshape(out_imgs[i], (1,-1)), X1_eval['Patient'].value_counts()[i], axis=0) for i in range(len(X1_eval['Patient'].value_counts()))], axis=0)\n        X1_eval = tf.convert_to_tensor(np.asarray(X1_eval[SELECTED_COLUMNS]))\n        X_tabular = model.get_layer('dense_to_freeze2')(model.get_layer('dense_to_freeze1')(X1_eval))\n        inp_mlp = model.get_layer('concatenate')([X_imgs, X_tabular])\n        inp_mlp = model.layers[-1](model.layers[-2](model.layers[-3](inp_mlp)))\n        y_eval_pred = np.append(y_eval_pred, inp_mlp.numpy(), axis=0)\n        y_eval_true = np.append(y_eval_true, Y_eval)\n    y_eval_pred = y_eval_pred[1:]\n    y_eval_true = y_eval_true[1:]\n    sc = eval_score(y_eval_true, y_eval_pred)\n    tf.print('Eval Score:%f ' % sc)","ae6b3844":"SELECTED_COLUMNS = ['Weeks', 'Percent', 'Age', 'Sex', 'Min_week', 'Base_FVC','Base_week', '_Currently smokes', '_Ex-smoker', '_Never smoked']\nepochs = 5\nloss = []\nfor epoch in range(epochs):\n    list_loss = []\n    start_time = time.perf_counter()\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    j=0\n    for [X1, X2], Y in train_generator:\n        list_loss = train_step(X1, X2, Y, j, list_loss)\n        j+=1\n    loss.append(np.mean(list_loss))\n    print(np.mean(list_loss))\n    if epoch%1==0:\n        CNN.save('CNN_' + str(epoch))\n        model.save('model_' + str(epoch))\n    tf.print(\"End epoch:\", time.perf_counter() - start_time)\n#     eval_step()","c97ec128":"# Data preparation for test","98f7019e":"# Data Generator","21c1dfba":"## Train","30e64436":"This notebook is still in development.","e0182389":"## Loss","b6cf6c2a":"# Model","101b249f":"# Librairies","6c27a90a":"## Create CNN + MLP and train","1ef02d9b":"## Load MLP"}}