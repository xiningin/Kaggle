{"cell_type":{"6d63288c":"code","96bca691":"code","ed95f375":"code","eb60745a":"code","0fc1fa22":"code","02af3281":"code","138b1689":"code","16b33bbd":"code","4be9361d":"code","9c86d98f":"code","b0ed3402":"code","dcef32c4":"code","2953682c":"code","068b3689":"code","0bc1757f":"code","16c1133d":"code","716cc68a":"code","a9ff195d":"code","a89bb0ca":"code","752175f2":"code","10ce16d2":"code","003b9044":"code","dfff7132":"code","6e316cda":"code","c35ee155":"code","4df09a04":"code","6d4e61dc":"code","d3572f06":"code","c0c5cee5":"code","a277b3c8":"code","61fb0411":"code","5dbdfcb9":"code","76e98474":"code","7a8af579":"code","2fafaf2c":"code","8511b097":"code","5f870882":"code","b85a5464":"code","e7372e78":"code","2bdb9789":"code","88af4aac":"code","396d0b66":"code","31fef502":"code","2638317b":"code","1455f7ce":"code","853baddd":"code","1b0407a9":"code","dcf515e4":"code","ab3b2e9c":"code","eb6b6919":"code","a76ca593":"code","47dc7eb7":"code","7a3e8569":"code","70541dbf":"code","483059ac":"code","439f9cec":"code","6f67a3b2":"code","251c3f82":"code","06925700":"code","2cc5d6ae":"code","646781e6":"code","d186005a":"code","873a4e1f":"code","cfff7743":"code","2a77271e":"code","8ade8939":"code","6c99310f":"markdown","3abe87e7":"markdown"},"source":{"6d63288c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96bca691":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ed95f375":"\nimport pandas_datareader as web\nimport datetime as dt","eb60745a":"import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\nimport time","0fc1fa22":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\nfrom statsmodels.tsa.seasonal import seasonal_decompose","02af3281":"df1=pd.read_csv(\"\/kaggle\/input\/audusd-csv-dataset\/AUDUSD_M1_201501020900_202108202354.csv\",sep='\\t')","138b1689":"df1.head()","16b33bbd":"df1.shape","4be9361d":"df1.info()\n","9c86d98f":"sns.heatmap(df1.isnull(),yticklabels=False)","b0ed3402":"%matplotlib inline \nplt.gcf().set_size_inches(20, 10, forward=True)\nplt.plot(df1['<CLOSE>'], color=\"black\")\n\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\n\nplt.show()","dcef32c4":"df1[\"DATE\"] = df1[\"<DATE>\"] +\" \" +df1[\"<TIME>\"]","2953682c":"df1.head()","068b3689":"df1['DATE'] = df1['DATE'].astype('datetime64[ns]')","0bc1757f":"df1.info()","16c1133d":"#df1[\"DATE\"]=pd.to_datetime(df1[\"<DATE>\"])\n#df1[\"TIME\"]=pd.to_datetime(df1[\"<TIME>\"])","716cc68a":"df1.set_index(\"DATE\", inplace=True)","a9ff195d":"%matplotlib inline \nplt.gcf().set_size_inches(20, 10, forward=True)\nplt.plot(df1['<CLOSE>'], color=\"black\")\n\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\n\nplt.show()","a89bb0ca":"#df1['<TIME>'] = df1['<TIME>'].astype('datetime64[ns]')","752175f2":"#df1.info()","10ce16d2":"df1.head()","003b9044":"df1[\"DATE1\"] = df1[\"<DATE>\"] +\" \" +df1[\"<TIME>\"]\ndf1['DATE1'] = df1['DATE1'].astype('datetime64[ns]')\ndf2=df1[df1[\"DATE1\"].dt.year == 2015]","dfff7132":"df2.shape","6e316cda":"df3=df1[df1['DATE1'].dt.year == 2016]","c35ee155":"df3.shape","4df09a04":"df4=df1[df1['DATE1'].dt.year == 2017]","6d4e61dc":"k=int(df4.shape[0]\/4)\ndf5=df4[:k]\ndf5.shape","d3572f06":"df5.head()","c0c5cee5":"vertical_stack = pd.concat([df2, df3,df5], axis=0)\n","a277b3c8":"def Rsi_finder(series, period):\n    delta = series.diff().dropna()\n    u = delta * 0\n    d = u.copy()\n    u[delta > 0] = delta[delta > 0]\n    d[delta < 0] = -delta[delta < 0]\n    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n    u = u.drop(u.index[:(period-1)])\n    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n    d = d.drop(d.index[:(period-1)])\n    rs = pd.DataFrame.ewm(u, com=period-1, adjust=False).mean() \/ \\\n         pd.DataFrame.ewm(d, com=period-1, adjust=False).mean()\n    return 100 - 100 \/ (1 + rs)","61fb0411":"vertical_stack['RSI'] = Rsi_finder(vertical_stack[\"<CLOSE>\"], 14)","5dbdfcb9":"for i in range(14):\n  vertical_stack[\"RSI\"][i]=vertical_stack[\"RSI\"][15]\nvertical_stack.head()","76e98474":"vertical_stack.drop(['<TIME>',\"<DATE>\",\"<VOL>\",\"DATE1\"], axis = 1,inplace=True)","7a8af579":"def calculate_sma(data, n):\n    sma = data.rolling(window = n).mean()\n    return pd.DataFrame(sma)","2fafaf2c":"vertical_stack[\"sma_30\"] = calculate_sma(vertical_stack['<CLOSE>'],30)\nfor i in range(29):\n  vertical_stack[\"sma_30\"][i]=vertical_stack['sma_30'][30]","8511b097":"def calculate_ema(prices, days, smoothing=2):\n    ema = [sum(prices[:days]) \/ days]\n    for price in prices[days:]:\n        ema.append((price * (smoothing \/ (1 + days))) + ema[-1] * (1 - (smoothing \/ (1 + days))))\n    return pd.DataFrame(ema)","5f870882":"vertical_stack[\"ema_10\"] = calculate_ema(vertical_stack['<CLOSE>'],10)\nfor i in range(9):\n    vertical_stack[\"ema_10\"][i]=vertical_stack['ema_10'][10]","b85a5464":"vertical_stack.tail()\nvertical_stack['1ema_10'] =vertical_stack['<CLOSE>'].ewm(span=10, adjust=False).mean()","e7372e78":"vertical_stack.tail()\nvertical_stack.drop([\"ema_10\"], axis = 1,inplace=True)","2bdb9789":"plt.gcf().set_size_inches(22, 10, forward=True)\n\nstart = 0\nend = 10000\n#actual=plt.plot(actual[start+60:end], label= \"actual\")\nreal = plt.plot(vertical_stack['sma_30'][start:end], label='sma', color=\"red\")\npred = plt.plot(vertical_stack['1ema_10'][start:end], label='ema', color=\"black\")\n\nplt.legend(['sma_30', 'ema_10'])\nplt.title(\" Share Price\")\nplt.xlabel(\"minutes\")\nplt.ylabel(\"Share Price\")\nplt.show()","88af4aac":"vertical_stack.info()","396d0b66":"#vertical_stack.set_index('DATE', inplace=True)\n#vertical_stack.info()","31fef502":"from sklearn import preprocessing\ndata_normaliser = preprocessing.StandardScaler()\ndf1_normalised = data_normaliser.fit_transform(vertical_stack)","2638317b":"df1_normalised[0,:]","1455f7ce":"history_points=60","853baddd":"ohlcv_histories_normalised = np.array([df1_normalised[i  : i + history_points].copy() for i in range(len(df1_normalised) - history_points)])","1b0407a9":"weighted_avg_values_normalised = np.array([df1_normalised[:,3][i + history_points].copy() for i in range(len(df1_normalised) - history_points)])","dcf515e4":"print(weighted_avg_values_normalised.shape)\nweighted_avg_values_normalised = np.expand_dims(weighted_avg_values_normalised, -1)\nweighted_avg_values_normalised.shape","ab3b2e9c":"weighted_avg_values = np.array([vertical_stack[\"<CLOSE>\"][i + history_points].copy() for i in range(len(vertical_stack) - history_points)])","eb6b6919":"y_normaliser = preprocessing.StandardScaler()\ny_normaliser.fit(np.expand_dims( weighted_avg_values,-1 ))","a76ca593":"print(weighted_avg_values.shape)","47dc7eb7":"test_split = 0.89 # the percent of data to be used for testing\nn = int(ohlcv_histories_normalised.shape[0] * test_split)\nn","7a3e8569":"x_train = ohlcv_histories_normalised[:n]\nprint(x_train.shape)","70541dbf":"y_train = weighted_avg_values_normalised[:n]\ny_train.reshape(n,1)\nprint(y_train.shape)","483059ac":"x_test = ohlcv_histories_normalised[n:]\ny_test = weighted_avg_values_normalised[n:]","439f9cec":"corrMatx = vertical_stack.corr()\ncorrMatx","6f67a3b2":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","251c3f82":"with tpu_strategy.scope():\n    start = time.process_time()\n    model21=Sequential()\n    model21.add(LSTM(units=100, return_sequences=True, input_shape=(history_points,9)))\n    model21.add(Dropout(0.20))\n    model21.add(LSTM(units=100, return_sequences=True))\n    model21.add(Dropout(0.20)) \n    model21.add(LSTM(units=100, return_sequences=True))\n    model21.add(Dropout(0.20)) \n    model21.add(LSTM(units=100))\n    model21.add(Dropout(0.20)) \n    model21.add(Dense(units=1))\n    model21.compile(optimizer=\"adam\", loss=\"mean_squared_error\")","06925700":"model21.fit(x_train,y_train,batch_size=128, epochs=100) \nprint(time.process_time() - start)","2cc5d6ae":"y_test_predicted21 = model21.predict(x_test)","646781e6":"y_test= y_normaliser.inverse_transform(y_test)\ny_test","d186005a":"y_test_predicted21 = y_normaliser.inverse_transform(y_test_predicted21)\ny_test_predicted21","873a4e1f":"plt.gcf().set_size_inches(22, 10, forward=True)\n\nstart = 0\nend = -1\n#actual=plt.plot(actual[start+60:end], label= \"actual\")\nreal = plt.plot(y_test[start:end], label='real', color=\"red\")\npred = plt.plot(y_test_predicted21[start:end], label='predicted', color=\"black\")\n\nplt.legend(['Real', 'Predicted'])\nplt.title(\" Share Price\")\nplt.xlabel(\"minutes\")\nplt.ylabel(\"Share Price\")\nplt.show()","cfff7743":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_test_predicted21)","2a77271e":"def percentage_error(actual, predicted):\n    res = np.empty(actual.shape)\n    for j in range(actual.shape[0]):\n        if actual[j] != 0:\n            res[j] = (actual[j] - predicted[j]) \/ actual[j]\n        else:\n            res[j] = predicted[j] \/ np.mean(actual)\n    return res\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100","8ade8939":"mean_absolute_percentage_error(y_test, y_test_predicted21)","6c99310f":"To find out the null values","3abe87e7":"No NaN values"}}