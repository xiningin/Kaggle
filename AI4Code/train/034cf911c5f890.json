{"cell_type":{"2ad4f926":"code","ce50e089":"code","4b0bcacb":"code","ea764f90":"code","28a6add1":"code","b475f459":"code","08f69f32":"code","b132b48d":"code","00e9abd1":"code","8cfcc823":"code","b3d6ddd9":"code","a3eecdf4":"code","da2f108d":"code","20568bd6":"markdown","f2381f7b":"markdown","128989cf":"markdown","a0902432":"markdown","23ebbb04":"markdown","9737ae80":"markdown","baf55730":"markdown","321aa48f":"markdown","573b7320":"markdown"},"source":{"2ad4f926":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.tree import DecisionTreeClassifier\nimport gc\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n# import pickle\nimport matplotlib.cm as cm\nimport seaborn as sn\nfrom collections import Counter\nimport lightgbm as lgb\n# from kmodes.kprototypes import KPrototypes\nimport gc\n# %reload_ext autotime","ce50e089":"dataset = pd.read_csv('..\/input\/breast-cancer-data-set\/breast_cancer.csv')\nX = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values","4b0bcacb":"dataset.head()","ea764f90":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","28a6add1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b475f459":"print(X_train)","08f69f32":"print(X_test)","b132b48d":"print(y_train)","00e9abd1":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","8cfcc823":"y_pred = classifier.predict(X_test)","b3d6ddd9":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.summer):\n    plt.clf\n    plt.imshow(cm, interpolation='nearest')\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(target_names))\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    plt.tight_layout()\n \n    width, height = cm.shape\n \n    for x in range(width):\n        for y in range(height):\n            plt.annotate(str(cm[x][y]), xy=(y, x), \n                        horizontalalignment='center',\n                        verticalalignment='center',color='black',fontsize=22)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a3eecdf4":"print(classification_report(y_test, y_pred))\ncnf_matrix = confusion_matrix(y_test, y_pred)\n#print(cnf_matrix)\naccuracy_score(y_test, y_pred)","da2f108d":"plot_confusion_matrix(cnf_matrix, np.unique(y_pred))","20568bd6":"Splitting the dataset into the Training set and Test set","f2381f7b":"**Feature scaling** will allow to put all our features on the same scale. Why we do this?. Well that's because for some of the Machine Learning models that's in order to avoid some features to be dominated by other features in such a way that the dominated features are not even considered by some Machine Learning models.\nNow you also need to be aware that we won't have to apply feature scaling for all the Machine Learning Models but just for some of them.","128989cf":"Predicting the Test set results","a0902432":"Training the Logistic Regression model on the Training set","23ebbb04":"Making the Confusion Matrix","9737ae80":"\nLet's start by importing the libraries.","baf55730":"We just made some predictions on the test set you know X test and we have these predictions in the y pred vector and \nnow we're going to make a confusion matrix which will tell us exactly how many correct predictions model did and \nincorrect predictions it did.","321aa48f":"And here is the confusion matrix so let's look at the cells one by one.The first one here is the number of correct predictions that a tumor is benign. So here 84 means that we had 84 correct predictions that a tumor is benign and 47 correct predictions that the tumor is malignant.\n\nNow 3 here is the number of incorrect predictions that the tumor is benign. We actually call it a false positive and 4 is the number of incorrect predictions that the tumor is malignant. We call it a false negative. We have actually 84 + 47 correct predictions meaning 131 correct predictions out of 137 right there in total and we have\n3 + 3 equals six incorrect predictions on the test set out of one hundred and thirty seven so that's amazing.\n\nThat's a very good first sign right?.","573b7320":"Importing the dataset"}}