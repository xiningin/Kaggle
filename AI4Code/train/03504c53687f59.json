{"cell_type":{"1cff2c68":"code","cd8d03f7":"code","8b162735":"code","7c0bf164":"code","2661deae":"code","bdec31f6":"code","24c5b334":"code","a878b5be":"code","c85ba3d1":"code","fe57f3cc":"code","18a1abf5":"code","2a8013ea":"code","014eb1a6":"code","f0226e20":"code","043aaf96":"code","9384e20e":"code","334a69f4":"code","0ec1384b":"code","47ce6c0a":"markdown","8889cd87":"markdown","2bbe3829":"markdown","3f4d9479":"markdown","65b8087d":"markdown","f62a33c5":"markdown","a8cc9bae":"markdown","f1288957":"markdown","7cdcd55a":"markdown","27f7644c":"markdown","f93773fb":"markdown","93627721":"markdown","2309fc71":"markdown","51f6c736":"markdown","82446897":"markdown","b4707de6":"markdown","2a6e7b4c":"markdown","5f6ed982":"markdown","cd604906":"markdown","7d10668d":"markdown","079df71d":"markdown","a90b7c64":"markdown"},"source":{"1cff2c68":"# import pytse_client as tse\n# data = tse.download(symbols=\"\u0641\u0645\u0644\u06cc\", write_to_csv=True)","cd8d03f7":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\ndef preprocess(csv_path, history_points):\n    data = pd.read_csv(csv_path)\n    data = data.drop(columns=['date', 'adjClose', 'value', 'count'])\n    data = data.drop(0, axis=0)\n    data_np = data.to_numpy()\n    # Train-Test-Validation Split\n    # 70% for train data\n    # 15% for test data\n    # 15% for validation data\n    n1 = int(data_np.shape[0] * 0.7)\n    n2 = int((data_np.shape[0] - n1) \/ 2)\n    x_train = data_np[:n1]\n    x_val = data_np[n1: n1 + n2]\n    x_test = data_np[n1 + n2:]\n    y_train_real = slicing(x_train, history_points)[1]\n    y_train_real = np.expand_dims(y_train_real, -1)\n    scale_back = preprocessing.MinMaxScaler()\n    scale_back.fit(y_train_real)\n    y_test_real = slicing(x_test, history_points)[1]\n\n    minmax_scale = preprocessing.MinMaxScaler().fit(x_train)\n    x_train_n = minmax_scale.transform(x_train)\n    x_val_n = minmax_scale.transform(x_val)\n    x_test_n = minmax_scale.transform(x_test)\n\n    ohlvc_train, y_train = slicing(x_train_n, history_points)\n    x_val_n, y_val = slicing(x_val_n, history_points)\n    ohlvc_test, y_test = slicing(x_test_n, history_points)\n    y_train = np.expand_dims(y_train, -1)\n\n    assert ohlvc_train.shape[0] == y_train.shape[0] \n    return ohlvc_train, y_train, ohlvc_test, y_test, x_val_n, y_val, y_test_real, scale_back\n\ndef slicing(data, history_points):\n    # using the last {history_points} open high low close volume data points, predict the next open value\n    ohlvc_histories = np.array(\n        [data[i: i + history_points].copy() for i in range(len(data) - history_points)])\n    next_day_open_values = np.array(\n        [data[:, 0][i + history_points].copy() for i in range(len(data) - history_points)])\n\n    return ohlvc_histories, next_day_open_values","8b162735":"csv_path = '..\/input\/national-iranian-copper-industries-co-stocks\/femeli-daily.csv'\nhistory_points = 50\nx_train, y_train, x_test, y_test, x_val, y_val, y_test_real, scale_back = preprocess(csv_path, history_points)","7c0bf164":"import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\n\nnp.random.seed(4)\ntf.random.set_seed(4)\nlstm_input = Input(shape=(history_points,5), name='lstm_input')\nx = LSTM(50, name='lstm_0')(lstm_input)\nx = Dropout(0.2, name='lstm_dropout_0')(x)\nx = Dense(64, name='dense_0')(x)\nx = Activation('sigmoid', name='sigmoid_0')(x)\nx = Dense(1, name='dense_1')(x)\noutput = Activation('linear', name='linear_output')(x)\nmodel = Model(inputs=lstm_input, outputs=output)\nadam = optimizers.Adam(lr=0.0005)\nmodel.compile(optimizer=adam, loss='mse')\n\n# Fitting model\nmcp_save = ModelCheckpoint('.\/stocks_price.h5', save_best_only=True, monitor='val_loss', mode='min')\nmodel.fit(x=x_train, y=y_train, batch_size=32, epochs=50, shuffle=True,validation_data=(x_val, y_val), callbacks=[mcp_save], verbose=0)\nmodel.load_weights('.\/stocks_price.h5')\n\n# Evaluate model (scaled data)\nevaluation = model.evaluate(x_test, y_test)\nprint(\"Prediction Mean Squared Error for normalized data : {}\".format(evaluation))\n\n# Evaluate model (unscaled data)\ny_test_predicted = model.predict(x_test)\ny_test_predicted = scale_back.inverse_transform(y_test_predicted)\nrmse = np.square(np.mean(y_test_real - y_test_predicted))\nscaled_rmse = rmse \/ (np.max(y_test_real) - np.min(y_test_real)) * 100\n\nprint(\"Adjucted Prediction Root Mean Squared Error for real data : {} % \".format(scaled_rmse))","2661deae":"from keras.utils import plot_model\nplot_model(model, to_file='model.v1.png')","bdec31f6":"import matplotlib.pyplot as plt\nplt.gcf().set_size_inches(22, 15, forward=True)\n\nstart = 0\nend = -1\n\nreal = plt.plot(y_test_real[start:end], label='real')\npred = plt.plot(y_test_predicted[start:end], label='predicted')\n\nplt.legend(['Real', 'Predicted'])\n\nplt.show()","24c5b334":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n\ndef preprocess(csv_path, history_points):\n    data = pd.read_csv(csv_path)\n    data = data.drop(columns=['date', 'adjClose', 'value', 'count'])\n    data = data.drop(0, axis=0)\n    data_np = data.to_numpy()\n    \"\"\"\n    Train-Test-Validation Split\n    \"\"\"\n    n1 = int(data_np.shape[0] * 0.7)\n    n2 = int((data_np.shape[0] - n1) \/ 2)\n    x_train = data_np[:n1]\n    x_val = data_np[n1: n1 + n2]\n    x_test = data_np[n1 + n2:]\n    y_train_real = slicing(x_train, history_points)[1]\n    y_train_real = np.expand_dims(y_train_real, -1)\n    scale_back = preprocessing.MinMaxScaler()\n    scale_back.fit(y_train_real)\n    y_test_real = slicing(x_test, history_points)[1]\n\n    minmax_scale = preprocessing.MinMaxScaler().fit(x_train)\n    x_train_n = minmax_scale.transform(x_train)\n    x_val_n = minmax_scale.transform(x_val)\n    x_test_n = minmax_scale.transform(x_test)\n\n    ohlvc_train, y_train = slicing(x_train_n, history_points)\n    x_val_n, y_val = slicing(x_val_n, history_points)\n    ohlvc_test, y_test = slicing(x_test_n, history_points)\n    y_train = np.expand_dims(y_train, -1)\n\n    \"\"\"\n    Technical Indicators\n    \"\"\"\n    x_train_ind = slicing(x_train, history_points)[0]\n    tech_ind_train = []\n    for his in x_train_ind:\n        # since I'm using his[1,2,4],I'm taking the SMA of the high, low ,closing price\n        sma = np.mean(his[:, [1, 2, 4]])\n        tech_ind_train.append(np.array([sma]))\n\n    x_test_ind = slicing(x_test, history_points)[0]\n    tech_ind_test = []\n    for his in x_test_ind:\n        sma = np.mean(his[:, [1, 2, 4]])\n        tech_ind_test.append(np.array([sma]))\n\n    x_val_ind = slicing(x_val, history_points)[0]\n    tech_ind_val = []\n    for his in x_val_ind:\n        sma = np.mean(his[:, [1, 2, 4]])\n        tech_ind_val.append(np.array([sma]))\n\n    tech_ind_scaler = preprocessing.MinMaxScaler().fit(tech_ind_train)\n    tech_ind_train = tech_ind_scaler.transform(tech_ind_train)\n    tech_ind_test = tech_ind_scaler.transform(tech_ind_test)\n    tech_ind_val = tech_ind_scaler.transform(tech_ind_val)\n\n    tech_ind_train = np.array(tech_ind_train)\n    tech_ind_test = np.array(tech_ind_test)\n    tech_ind_val = np.array(tech_ind_val)\n\n    assert ohlvc_train.shape[0] == y_train.shape[0] == tech_ind_train.shape[0]\n    return ohlvc_train, y_train, ohlvc_test, y_test, x_val_n, y_val, y_test_real, scale_back, tech_ind_train, tech_ind_test, tech_ind_val\n\n\ndef slicing(data, history_points):\n    # using the last {history_points} open high low close volume data points, predict the next open value\n    ohlvc_histories = np.array(\n        [data[i: i + history_points].copy() for i in range(len(data) - history_points)])\n    next_day_open_values = np.array(\n        [data[:, 0][i + history_points].copy() for i in range(len(data) - history_points)])\n\n    return ohlvc_histories, next_day_open_values","a878b5be":"csv_path = '..\/input\/national-iranian-copper-industries-co-stocks\/femeli-daily.csv'\nhistory_points = 50\n\nx_train, y_train, x_test, y_test, x_val, y_val, y_test_real, scale_back, tech_ind_train, tech_ind_test, tech_ind_val = preprocess(\n        csv_path, history_points)","c85ba3d1":"import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\nimport numpy as np\nfrom keras.callbacks import ModelCheckpoint\n\nnp.random.seed(4)\n\ntf.random.set_seed(4)\n\n# define two sets of inputs\nlstm_input = Input(shape=(history_points, 5), name='lstm_input')\ndense_input = Input(shape=(tech_ind_train.shape[1],), name='tech_input')\n\n# the first branch operates on the first input\nx = LSTM(50, name='lstm_0')(lstm_input)\nx = Dropout(0.2, name='lstm_dropout_0')(x)\nlstm_branch = Model(inputs=lstm_input, outputs=x)\n\n# the second branch operates on the second input\ny = Dense(20, name='tech_dense_0')(dense_input)\ny = Activation(\"relu\", name='tech_relu_0')(y)\ny = Dropout(0.2, name='tech_dropout_0')(y)\ntechnical_indicators_branch = Model(inputs=dense_input, outputs=y)\n\n# combine the output of the two branches\ncombined = concatenate([lstm_branch.output, technical_indicators_branch.output], name='concatenate')\n\nz = Dense(64, activation=\"sigmoid\", name='dense_pooling')(combined)\nz = Dense(1, activation=\"linear\", name='dense_out')(z)\n\n# our model will accept the inputs of the two branches and then output a single value\nmodel = Model(inputs=[lstm_branch.input, technical_indicators_branch.input], outputs=z)\n\nadam = optimizers.Adam(lr=0.0005)\n\nmodel.compile(optimizer=adam, loss='mse')\n\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.v2.png', show_shapes=True)\n\n# Fitting model\nmcp_save = ModelCheckpoint('.\/stocks_price.h5', save_best_only=True, monitor='val_loss', mode='min')\n\nmodel.fit(x=[x_train,tech_ind_train], y=y_train, batch_size=32, epochs=50, shuffle=True,validation_data=([x_val, tech_ind_val], y_val), callbacks=[mcp_save], verbose=0)\nmodel.load_weights('.\/stocks_price.h5')\n\n# Evaluate model (scaled data)\nevaluation = model.evaluate([x_test, tech_ind_test], y_test)\nprint(\"Prediction Error for normalized data : {}\".format(evaluation))\n\n# Evaluate model (unscaled data)\ny_test_predicted = model.predict([x_test, tech_ind_test])\ny_test_predicted = scale_back.inverse_transform(y_test_predicted)\nrmse = np.square(np.mean(y_test_real - y_test_predicted))\nscaled_rmse = rmse \/ (np.max(y_test_real) - np.min(y_test_real)) * 100\n\nprint(\"Adjucted Prediction Root Mean Squared Error for real data : {} % \".format(scaled_rmse))","fe57f3cc":"import matplotlib.pyplot as plt\nplt.gcf().set_size_inches(22, 15, forward=True)\n\nstart = 0\nend = -1\n\nreal = plt.plot(y_test_real[start:end], label='real')\npred = plt.plot(y_test_predicted[start:end], label='predicted')\n\nplt.legend(['Real', 'Predicted'])\n\nplt.show()","18a1abf5":"def calc_ema(values, time_period):\n    # https:\/\/www.investopedia.com\/ask\/answers\/122314\/what-exponential-moving-average-ema-formula-and-how-ema-calculated.asp\n    sma = np.mean(values[:, [1, 2, 4]])\n    ema_values = [sma]\n    k = 2 \/ (1 + time_period)\n    for i in range(len(values) - time_period, len(values)):\n        close = values[i][4]\n        ema_values.append(close * k + ema_values[-1] * (1 - k))\n    return ema_values[-1]","2a8013ea":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n\ndef preprocess(csv_path, history_points):\n    data = pd.read_csv(csv_path)\n    data = data.drop(columns=['date', 'adjClose', 'value', 'count'])\n    data = data.drop(0, axis=0)\n    data_np = data.to_numpy()\n    \"\"\"\n    Train-Test-Validation Split\n    \"\"\"\n    n1 = int(data_np.shape[0] * 0.7)\n    n2 = int((data_np.shape[0] - n1) \/ 2)\n    x_train = data_np[:n1]\n    x_val = data_np[n1: n1 + n2]\n    x_test = data_np[n1 + n2:]\n    y_train_real = slicing(x_train, history_points)[1]\n    y_train_real = np.expand_dims(y_train_real, -1)\n    scale_back = preprocessing.MinMaxScaler()\n    scale_back.fit(y_train_real)\n    y_test_real = slicing(x_test, history_points)[1]\n\n    minmax_scale = preprocessing.MinMaxScaler().fit(x_train)\n    x_train_n = minmax_scale.transform(x_train)\n    x_val_n = minmax_scale.transform(x_val)\n    x_test_n = minmax_scale.transform(x_test)\n\n    ohlvc_train, y_train = slicing(x_train_n, history_points)\n    x_val_n, y_val = slicing(x_val_n, history_points)\n    ohlvc_test, y_test = slicing(x_test_n, history_points)\n    y_train = np.expand_dims(y_train, -1)\n\n    \"\"\"\n    Technical Indicators\n    \"\"\"\n    x_train_ind = slicing(x_train, history_points)[0]\n    tech_ind_train = []\n    for his in x_train_ind:\n        # since I'm using his[1,2,4], I'm taking the SMA of the high, low ,closing price\n        sma = np.mean(his[:, [1, 2, 4]])\n        # 12 and 26 is the default values for ema in MACD indicator\n        macd = calc_ema(his, 12) - calc_ema(his, 26)\n        tech_ind_train.append(np.array([sma, macd]))\n\n    x_test_ind = slicing(x_test, history_points)[0]\n    tech_ind_test = []\n    for his in x_test_ind:\n        sma = np.mean(his[:, [1, 2, 4]])\n        # 12 and 26 is the default values for ema in MACD indicator\n        macd = calc_ema(his, 12) - calc_ema(his, 26)\n        tech_ind_test.append(np.array([sma, macd]))\n\n    x_val_ind = slicing(x_val, history_points)[0]\n    tech_ind_val = []\n    for his in x_val_ind:\n        sma = np.mean(his[:, [1, 2, 4]])\n        # 12 and 26 is the default values for ema in MACD indicator\n        macd = calc_ema(his, 12) - calc_ema(his, 26)\n        tech_ind_val.append(np.array([sma, macd]))\n\n    tech_ind_scaler = preprocessing.MinMaxScaler().fit(tech_ind_train)\n    tech_ind_train = tech_ind_scaler.transform(tech_ind_train)\n    tech_ind_test = tech_ind_scaler.transform(tech_ind_test)\n    tech_ind_val = tech_ind_scaler.transform(tech_ind_val)\n\n    tech_ind_train = np.array(tech_ind_train)\n    tech_ind_test = np.array(tech_ind_test)\n    tech_ind_val = np.array(tech_ind_val)\n\n    assert ohlvc_train.shape[0] == y_train.shape[0] == tech_ind_train.shape[0]\n    return ohlvc_train, y_train, ohlvc_test, y_test, x_val_n, y_val, y_test_real, scale_back, tech_ind_train, tech_ind_test, tech_ind_val\n\n\ndef slicing(data, history_points):\n    # using the last {history_points} open high low close volume data points, predict the next open value\n    ohlvc_histories = np.array(\n        [data[i: i + history_points].copy() for i in range(len(data) - history_points)])\n    next_day_open_values = np.array(\n        [data[:, 0][i + history_points].copy() for i in range(len(data) - history_points)])\n\n    return ohlvc_histories, next_day_open_values","014eb1a6":"csv_path = '..\/input\/national-iranian-copper-industries-co-stocks\/femeli-daily.csv'\nhistory_points = 50\n\nx_train, y_train, x_test, y_test, x_val, y_val, y_test_real, scale_back, tech_ind_train, tech_ind_test, tech_ind_val = preprocess(csv_path, history_points)","f0226e20":"import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\nimport numpy as np\nfrom keras.callbacks import ModelCheckpoint\n\nnp.random.seed(4)\n\ntf.random.set_seed(4)\n\n# define two sets of inputs\nlstm_input = Input(shape=(history_points, 5), name='lstm_input')\ndense_input = Input(shape=(tech_ind_train.shape[1],), name='tech_input')\n\n# the first branch operates on the first input\nx = LSTM(50, name='lstm_0')(lstm_input)\nx = Dropout(0.2, name='lstm_dropout_0')(x)\nlstm_branch = Model(inputs=lstm_input, outputs=x)\n\n# the second branch operates on the second input\ny = Dense(20, name='tech_dense_0')(dense_input)\ny = Activation(\"relu\", name='tech_relu_0')(y)\ny = Dropout(0.2, name='tech_dropout_0')(y)\ntechnical_indicators_branch = Model(inputs=dense_input, outputs=y)\n\n# combine the output of the two branches\ncombined = concatenate([lstm_branch.output, technical_indicators_branch.output], name='concatenate')\n\nz = Dense(64, activation=\"sigmoid\", name='dense_pooling')(combined)\nz = Dense(1, activation=\"linear\", name='dense_out')(z)\n\n# our model will accept the inputs of the two branches and then output a single value\nmodel = Model(inputs=[lstm_branch.input, technical_indicators_branch.input], outputs=z)\n\nadam = optimizers.Adam(lr=0.0005)\n\nmodel.compile(optimizer=adam, loss='mse')\n\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.v2.png', show_shapes=True)\n\n# Fitting model\nmcp_save = ModelCheckpoint('.\/stocks_price.h5', save_best_only=True, monitor='val_loss', mode='min')\n\nmodel.fit(x=[x_train,tech_ind_train], y=y_train, batch_size=32, epochs=50, shuffle=True,validation_data=([x_val, tech_ind_val], y_val), callbacks=[mcp_save], verbose=0)\nmodel.load_weights('.\/stocks_price.h5')\n\n# Evaluate model (scaled data)\nevaluation = model.evaluate([x_test, tech_ind_test], y_test)\nprint(\"Prediction Error for normalized data : {}\".format(evaluation))\n\n# Evaluate model (unscaled data)\ny_test_predicted = model.predict([x_test, tech_ind_test])\ny_test_predicted = scale_back.inverse_transform(y_test_predicted)\nrmse = np.square(np.mean(y_test_real - y_test_predicted))\nscaled_rmse = rmse \/ (np.max(y_test_real) - np.min(y_test_real)) * 100\n\nprint(\"Adjucted Prediction Root Mean Squared Error for real data : {} % \".format(scaled_rmse))","043aaf96":"import matplotlib.pyplot as plt\nplt.gcf().set_size_inches(22, 15, forward=True)\n\nstart = 0\nend = -1\n\nreal = plt.plot(y_test_real[start:end], label='real')\npred = plt.plot(y_test_predicted[start:end], label='predicted')\n\nplt.legend(['Real', 'Predicted'])\n\nplt.show()","9384e20e":"buys = []\nsells = []\nthresh = 1.005\n\nx = 0\nfor ohlcv, ind in zip(x_test, tech_ind_test):\n    normalised_price_today = ohlcv[-1][0]\n    normalised_price_today = np.array([[normalised_price_today]])\n    price_today = scale_back.inverse_transform(normalised_price_today)\n    predicted = np.squeeze(scale_back.inverse_transform(model.predict([np.array([ohlcv]), np.array([ind])])))\n    delta = predicted \/ price_today\n    if delta > thresh:\n        buys.append((x, price_today[0][0]))\n    elif delta < thresh:\n        sells.append((x, price_today[0][0]))\n    x += 1","334a69f4":"import matplotlib.pyplot as plt\n\nplt.gcf().set_size_inches(22, 15, forward=True)\n\nstart = 0\nend = -1\n\nreal = plt.plot(y_test_real[start:end], label='real')\npred = plt.plot(y_test_predicted[start:end], label='predicted')\n\nplt.scatter(list(list(zip(*buys))[0]), list(list(zip(*buys))[1]), c='#00ff00',label='Buy')\nplt.scatter(list(list(zip(*sells))[0]), list(list(zip(*sells))[1]), c='#ff0000', label='Sell')\n\n\n\nplt.legend(['Real', 'Predicted','Buy','Sell'])\n\nplt.show()","0ec1384b":"def compute_earnings(buys, sells):\n    purchase_amt = 10000000 # 1 million toman\n    stock = 0\n    balance = 0\n    while len(buys) > 0 and len(sells) > 0:\n        if buys[0][0] < sells[0][0]:\n            # time to buy $10 worth of stock\n            balance -= purchase_amt\n            stock += purchase_amt \/ buys[0][1]\n            buys.pop(0)\n        else:\n            # time to sell all of our stock\n            balance += stock * sells[0][1]\n            stock = 0\n            sells.pop(0)\n    print(balance)\n\nprint(compute_earnings(buys,sells))","47ce6c0a":"# Predicting stock prices using deep learning\n\n**If a human investor can be successful, why can\u2019t a machine?\n**\n![image.png](attachment:image.png)\nI would just like to add a disclaimer \u2014 this project is entirely intended for research purposes! I\u2019m just a student learning about deep learning, and the project is a work in progress, please don\u2019t put any money into it!\n\nAlgorithmic trading has revolutionised the stock market and its surrounding industry. Over 70% of all trades happening in the US right now are being handled by bots[1]. Gone are the days of the packed stock exchange with suited people waving sheets of paper shouting into telephones.\n\nThis got me thinking of how I could develop my own algorithm for trading stocks, or at least try to accurately predict them.","8889cd87":"Long Short Term Memory cells are like mini neural networks designed to allow for memory in a larger neural network. This is achieved through the use of a recurrent node inside the LSTM cell. This node has an edge looping back on itself with a weight of one, meaning at every feedfoward iteration the cell can hold onto information from the previous step, as well as all previous steps. Since the looping connection\u2019s weight is one, old memories wont fade over time like they would in traditional RNNs.\n\nLTSMs and recurrent neural networks are as a result good at working with time series data thanks to their ability to remember the past. By storing some of the old state in these recurrent nodes, RNNs and LSTMs can reason about current information as well as information the network had seen one, ten or a thousand steps ago. Even better, I don\u2019t have to write my own implementation of an LSTM cell; [they\u2019re a default layer in Tensorflow\u2019s Keras.](https:\/\/keras.io\/layers\/recurrent\/#lstm)\n\nSo I had my plan; to use LSTMs and Keras to predict the stock market, and perhaps even make some money.","2bbe3829":"But not bad! I\u2019m not sure why the predicted value is consistently lower than the actual value, maybe it\u2019s something to do with the way the test and train sets are split.","3f4d9479":"To update our technical indicators loop to include the MACD indicator:\n","65b8087d":"# References\n\n[1] : https:\/\/www.experfy.com\/blog\/the-future-of-algorithmic-trading\n\n[2] : https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\n[3] : https:\/\/jovianlin.io\/why-is-normalization-important-in-neural-networks\/\n\n[4] : https:\/\/www.investopedia.com\/terms\/t\/technicalindicator.asp\n\n[5] : https:\/\/www.investopedia.com\/terms\/s\/sma.asp\n\n[6] : https:\/\/www.investopedia.com\/terms\/m\/macd.asp\n\n[7] : https:\/\/www.investopedia.com\/ask\/answers\/122314\/what-exponential-moving-average-ema-formula-and-how-ema-calculated.asp","f62a33c5":"Then plot the trades.","a8cc9bae":"# The Conclusion\nI think there is still some room for improvement for the prediction algorithm. Namely, the technical indicators used, history_points hyperparameter, buy\/sell algorithm\/hyperparameters and model architecture are all things that I would like to optimise in the future.\n\nThe full code for this project is available on my [GitHub](https:\/\/github.com\/swmnnmt\/Algorithmic-trading-with-artificial-intelligence.git). Feel free to leave any feedback\/improvements over on the issues page!\n\nI do plan to expand on this project some more, to really push the limits of what can be achieved using just numerical data to predict stocks. Keep up to date with what I\u2019m doing on my GitHub!","f1288957":"# The Dataset\n\nThe good thing about stock price history is that it\u2019s basically a well labelled pre formed dataset. After some googling I found a service called [pytse client](https:\/\/pypi.org\/project\/pytse-client\/). They offered the daily price history of Tehran Stock Exchange for the past 19 years. This included the open, high, low, close, volume,etc of trades for each day, from today all the way back up to 1380. ","7cdcd55a":"Given these buys and sells, if we say that at each \u2018buy\u2019 we buy up 10 million Rials worth of the stock, and at each \u2018sell\u2019 we sell all of the stock, the algorithm would have earned about 95 million Rials. But bear in mind that is across 500 days. The code to calculate the earnings of the algorithm is here;","27f7644c":"Which gives us a model that looks like:\n","f93773fb":"# The Model\nI started this project only knowing how to write sequential Keras code, but I ended up learning it\u2019s functional API since I wanted a more complex network structure, eventually featuring two inputs with different layer types in each branch.\n\nI\u2019ll go over the most basic model that I came up with first.\n\nThe input layer has shape **(history_points, 5)**, since each input data point is an array shaped like [history_points \u00d7 OHLVC]. The model has 50 LSTM cells in the first layer, a dropout layer to prevent overfitting and then some dense layers to bring all of the LSTM data together.\nAn important feature of this network is the linear output activation, allowing the model to tune its penultimate weights accurately.\n\n# The Training\nWell now I are training the network with training data. To do this, consider the batch size to be 32 and the number of iterations to be 50. I also shuffle the data order. With the help of call back, I save and use the model that has the least amount of loss for validation data.\n\n# The Evaluation\nTo more accurately evaluate the model, let\u2019s see how it predicts the test set in comparison with the real values. First I scale the predicted values up, then I compute the root mean squared error, but then to make the error relative to the dataset I divide it by the \u2018spread\u2019 of the test data \u2014 the max test data point minus the min.","93627721":"\n> This model appears to not suffer the previous problem of being continuously off by a fixed amount, but does seem to suffer from not catching sudden jumps as well. Like at around x-coordinate 120, a large jump and dip in the real price occurs but the model fails to capture this effectively. But it is getting better! And it seems that technical indicators could be the way forward.\nLet\u2019s try including a more advanced technical indicator: the Moving Average Convergence Divergence. The MACD is calculated by subtracting the 26-period Exponential Moving Average from the 12-period EMA[6]. The EMA is calculated[7] using the formula:\n![image.png](attachment:image.png)","2309fc71":"Well, so far I have prepared training and test data. The next step is to define the model. So let's go to model definition.","51f6c736":"I got a final evaluation score of 0.006, which seems super low but remember that this is the mean squared error of the normalised data. After scaling this value will go up significantly, so it\u2019s not a great metric for loss.\n\nThis gives us a root mean squared error of 718. Is that good? It\u2019s not amazing, it means on average the predicted line deviates over 718% from the actual. Let\u2019s see how it looks on a graph.","82446897":"# Preprocessing\n\n## Data cleaning\n\nFor the stocks that had their IPO listing within the past 19 years, the first day of trading that stock often looked anomalous due to the massively high volume. This inflated max volume value also affected how other volume values in the dataset were scaled when normalising the data, so I opted to drop the oldest data points out of every set. I also drop the date since the model doesn\u2019t need to know anything about when trades happened, all it needs is well ordered time series data.\n\n\n## Train,validate, test data spliting\n\nWell now I need to separate the training and test data. I dedicate 70% of the data to training, 15% to validation and the remaining 15% to testing.\n\n## Data Standardization\n\nLooking at the data, I will see that the data needs to be standardized. Standardization means that the range of values is the same for all attributes. There are several ways to standardize. One of these methods is to use the minimum and maximum data values.[3]\n\nFor our data, price range and trading volume are very different. Therefore, the data must be standardized. I will standardize using the minimum and maximum amount of data. To do this, I use the scikit-learn library and the preprocessing module.\n\nFor this purpose, first obtain the minimum and maximum training data using the command preprocessing.MinMaxScaler().fit. Then I standardize the test and training data with the help of Min and Max training data.\n\nNote that I can not standardize validation and test data with their own Min and Max. If I do, I are actually cheating the network. Therefore, I standardize all data with Min and Max training data.\n\nx_train_n,x_test_n and x_val_n now contain the normalised stock prices.\n\n## Get ready for model consumption\n\nI want to estimate the stock price when the market opens. In this project, I provide data for 50 days to the network and I want to estimate the opening price of the stock the next day. For this reason, I first separate the data into 50 slices. Here I also separate the openning price on the 51st day as a label. To do this, I write function slicing:\n\nThen, using the function I wrote, I separate the training, validation, and test data by 50 to 50, and I also separate their labels.\n\n\nThe ohlvc_train list will be our x parameter when training the neural network. Each value in the list is a numpy array containing 50 open, high, low, volume and close going from oldest to newest. This is controlled by the history_points parameter, as seen inside the slice operation.\n\nSo for each x value I are getting the [i : i + history_points] stock prices (remember that numpy slicing is [inclusive:exclusive]), then the y value must be the singular [i + history_points] stock price; the stock price for the very next day.\n\nHere I also have to choose what value I are intending on predicting. I decided to predict the open value for the next day, so I need to get the 1st (0 index) element of every ohlvc value in the data, hence **data_scaled[:,0]**.\n\nThere\u2019s also a variable called scale_back to hold on to. This is used at the end of a prediction, where the model will spit out a normalised number between 0 and 1, I want to apply the reverse of the dataset normalisation to scale it back up to real world values. In the future I will also use this to compute the real world (un-normalised) error of our model.\n\nThen to get the data working with Keras I make the y array 2-dimensional by way of np.expand_dims(). And finally I keep hold of the unscaled next day open values for plotting results later.\n\nJust before returning the data I check that the number of x\u2019s == the number of y\u2019s.\n","b4707de6":"# The Algorithm\nArmed with an okay-ish stock prediction algorithm I thought of a na\u00efve way of creating a bot to decide to buy\/sell a stock today given the stock\u2019s history. In essence you just predict the opening value of the stock for the next day, and if it is beyond a threshold amount you buy the stock. If it is below another threshold amount, sell the stock. This dead simple algorithm actually seemed to work quite well \u2014 visually at least.","2a6e7b4c":"I got a final evaluation score of 0.004, which seems super low but remember that this is the mean squared error of the normalised data. After scaling this value will go up significantly, so it\u2019s not a great metric for loss.\n\nAnd I get an adjusted mean squared error of 298%! Much lower, and the prediction appears to fit significantly closer to the test set when plotted:","5f6ed982":"I\u2019ve learned a lot about neural networks and machine learning over the summer and one of the most recent and applicable ML technologies I learnt about is the LSTM cell [2].\n![image.png](attachment:image.png)\n> An LSTM cell. Credit: https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n","cd604906":"# The Improvements\nI could try to make our model more complex, and also increase the size of the dataset. Let\u2019s start with trying to create a more complex model.\nA common metric used by stock market analysts are technical indicators[4]. Technical indicators are math operations done on stock price history, and are traditionally used as visual aids to help identify the direction the market is going to change in. I can augment our model to accept these technical indicators through a secondary input branch.\nFor now let\u2019s use only the simple moving average SMA indicator as an extra input into our network.\nTo calculate the simple moving average of a stock, simply take the average of the stock\u2019s opening, high and low price over the past n time steps[5]. This works great for us, since I are already dealing with fixed timestep blocks of price history. To include the SMA in our model I will have to change our dataset handling code.","7d10668d":"This happens just after I have defined the ohlvc_histories and next_day_open_values arrays. I loop across every 50-price block of data and calculate the mean of the high, low, close column, and add that value to a technical_indicators list. The list then goes through the same transformations as the rest of the data, being scaled to fit within the values 0 to 1. I then change the return statement to return the technical indicators, as well as the other stuff I returned from before.\nNow to augment the model to match this new dataset. I want to be able to use our old LSTM structure, but incorporate the SMA technical indicator somewhere in the mix. Since the SMA is not a time-series piece of data I shouldn\u2019t pass it through the LSTM. Instead I should mix it in before the final prediction is made; I should input it into the penultimate 64-node dense layer. So I will need a model with two inputs, a concatenation layer and one output.\n\nNote how I used tech_ind_train.shape[1] as the input shape for the tech_input layer. This means that any new technical indicators I add will fit in just fine when I recompile the model.\nThe evaluation code has to be changed to match this dataset change as well.\n\nI pass in a list of [ohlvc, technical_indicators] as the input to our model. This order matches the way I defined our model\u2019s input.","079df71d":"And I get an adjusted mean squared error of 413%! Much lower, and the prediction appears to fit significantly closer to the test set when plotted:","a90b7c64":"![image.png](attachment:image.png)\n> Machines are great with numbers!\n"}}