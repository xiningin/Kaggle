{"cell_type":{"8b882aca":"code","dae7e137":"code","703dba02":"code","00401f24":"code","8559c93e":"code","39e90f6b":"code","c02fd01a":"code","6a3a1f9b":"code","a48cc055":"code","443482aa":"code","a9044ed9":"code","6b3ee93a":"code","8498aef0":"code","88ff4bf7":"code","78ff5854":"code","4854cf1d":"code","b5186531":"code","7abe76d1":"code","eab5c0f2":"code","d63aee2a":"code","5f833977":"code","0865d30c":"code","c5fd164d":"code","ddb7d1df":"code","5fc34ec5":"code","4bfc28fd":"code","3ecc0ab8":"code","165d57a9":"code","d8b38965":"code","8df29c74":"code","5014b53c":"code","ce5824ae":"code","9e58123f":"code","77142348":"code","ca19c16b":"markdown","70ec3310":"markdown","b968fe96":"markdown","f4671fa6":"markdown","731517e5":"markdown","d68afbd6":"markdown","81bc7153":"markdown","7f1735de":"markdown","fb0ae3e8":"markdown","80118f59":"markdown","34d4ba30":"markdown","a37142cc":"markdown","7497c1f5":"markdown","d45602d7":"markdown","80ce917a":"markdown","3c619f4b":"markdown","e289606e":"markdown","036ae76e":"markdown","164b8d2f":"markdown","0226862d":"markdown","d1365648":"markdown","7a0ebf45":"markdown","f565cd6b":"markdown","a8793a05":"markdown","a51d302e":"markdown","30464f71":"markdown","0e0fec89":"markdown","4a3a193a":"markdown","0011bf3a":"markdown","e6e72d27":"markdown","c267af43":"markdown","db1303ab":"markdown","237d131f":"markdown","28266884":"markdown","2de36344":"markdown","f610bf31":"markdown","ca7b062d":"markdown"},"source":{"8b882aca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dae7e137":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\n\n","703dba02":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","00401f24":"train_data.info()","8559c93e":"train_data.describe()","39e90f6b":"train_data.head()","c02fd01a":"train_data['Sex'] = train_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntest_data['Sex'] = test_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","6a3a1f9b":"for i in range(0, 2):\n    for j in range(1, 4):\n        group = train_data[(train_data['Sex'] == i) & \\\n                              (train_data['Pclass'] == j)]['Age'].dropna()\n        age_of_group = group.mean()\n        train_data.loc[ (train_data.Age.isnull()) & (train_data.Sex == i) & (train_data.Pclass == j),'Age'] = age_of_group\n        \nfor i in range(0, 2):\n    for j in range(1, 4):\n        group = test_data[(test_data['Sex'] == i) & \\\n                              (test_data['Pclass'] == j)]['Age'].dropna()\n        age_of_group = group.mean()\n        test_data.loc[ (test_data.Age.isnull()) & (test_data.Sex == i) & (test_data.Pclass == j),'Age'] = age_of_group\n\n","a48cc055":"train_data[\"Embarked\"].value_counts()","443482aa":"train_data.loc[(train_data.Embarked.isnull()),'Embarked']='S'\ntest_data.loc[(test_data.Embarked.isnull()),'Embarked']='S'","a9044ed9":"train_data.info()","6b3ee93a":"print (\"Mean age of male passengers of the Titanic is : \" + str(train_data[train_data[\"Sex\"]==0][\"Age\"].mean())+\"\\n\")\nprint (\"Mean age of female passengers of the Titanic is : \" + str(train_data[train_data[\"Sex\"]==1][\"Age\"].mean())+\"\\n\") ","8498aef0":"number_men=len(train_data[train_data[\"Sex\"]==0])\nnumber_men_s=sum(train_data[train_data[\"Sex\"]==0][\"Survived\"])\n\nprint(\"Percentage of survivors in men:\", 100*number_men_s\/number_men, \"%\")\n\nnumber_women=len(train_data[train_data[\"Sex\"]==1])\nnumber_women_s=sum(train_data[train_data[\"Sex\"]==1][\"Survived\"])\n\nprint(\"Percentage of survivors in female:\", 100*number_women_s\/number_women, \"%\")\n","88ff4bf7":"number_class1=len(train_data[train_data[\"Pclass\"]==1])\nnumber_class1_s=sum(train_data[train_data[\"Pclass\"]==1][\"Survived\"])\n\nprint(\"Percentage of survivors in Class1:\", 100*number_class1_s\/number_class1, \"%\")\n\nnumber_class2=len(train_data[train_data[\"Pclass\"]==2])\nnumber_class2_s=sum(train_data[train_data[\"Pclass\"]==2][\"Survived\"])\n\nprint(\"Percentage of survivors in Class2:\", 100*number_class2_s\/number_class2, \"%\")\n\nnumber_class3=len(train_data[train_data[\"Pclass\"]==3])\nnumber_class3_s=sum(train_data[train_data[\"Pclass\"]==3][\"Survived\"])\n\nprint(\"Percentage of survivors in Class3:\", 100*number_class3_s\/number_class3, \"%\")","78ff5854":"for element in list(set(train_data[\"SibSp\"])):\n    number=len(train_data[train_data[\"SibSp\"]==element])\n    number_s=sum(train_data[train_data[\"SibSp\"]==element][\"Survived\"])\n    print(\"Percentage of survivors for passenger with \", element, \"siblings\/spouse is \" , 100*number_s\/number, \"%\")\n    \n    ","4854cf1d":"for element in list(set(train_data[\"Parch\"])):\n    number=len(train_data[train_data[\"Parch\"]==element])\n    number_s=sum(train_data[train_data[\"Parch\"]==element][\"Survived\"])\n    print(\"Percentage of survivors for passenger with \", element, \"parent\/child is \" , 100*number_s\/number, \"%\")","b5186531":"train_data[\"fam_Member\"]=train_data[\"Parch\"]+train_data[\"SibSp\"]\nfor element in list(set(train_data[\"fam_Member\"])):\n    number=len(train_data[train_data[\"fam_Member\"]==element])\n    number_s=sum(train_data[train_data[\"fam_Member\"]==element][\"Survived\"])\n    print(\"Percentage of survivors for passenger with \", element, \"Family members is \" , 100*number_s\/number, \"%\")","7abe76d1":"import math\nfor element in list(set(train_data[\"Embarked\"])):\n    number=len(train_data[train_data[\"Embarked\"]==element])\n    number_s=sum(train_data[train_data[\"Embarked\"]==element][\"Survived\"])\n    if number!=0:\n        print(\"Percentage of survivors for passenger who embarked from \", element, \"is \" , 100*number_s\/number, \"%\")","eab5c0f2":"train_data=pd.get_dummies(train_data, columns=[\"Embarked\"])\n\ntrain_data.head()","d63aee2a":"\nY_train=train_data[\"Survived\"]\nX_train=train_data.drop([\"Survived\",\"Name\",\"Fare\",\"SibSp\",\"Parch\",\"Ticket\",\"Cabin\",\"PassengerId\"], axis=1)\nY_train.head()\n","5f833977":"X_train.head()","0865d30c":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0,max_iter=250)\nclassifier.fit(X_train, Y_train)","c5fd164d":"test_data[\"fam_Member\"]=test_data[\"Parch\"]+test_data[\"SibSp\"]\n","ddb7d1df":"test_data=pd.get_dummies(test_data, columns=[\"Embarked\"])","5fc34ec5":"X_test=test_data.drop([\"Name\",\"Fare\",\"SibSp\",\"Parch\",\"Ticket\",\"Cabin\",\"PassengerId\"], axis=1)","4bfc28fd":"Y_pred = classifier.predict(X_test)\nprecision = round(classifier.score(X_train, Y_train) * 100, 2)\nprecision","3ecc0ab8":"Y_pred","165d57a9":"from sklearn.neighbors import KNeighborsClassifier\n\n\nknn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(X_train, Y_train)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\n\n","d8b38965":"Y_pred = knn.predict(X_test)\nY_pred","8df29c74":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","5014b53c":"Y_pred = decision_tree.predict(X_test)\nY_pred","ce5824ae":"X_test","9e58123f":"test_data","77142348":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('\/submission.csv', index=False)","ca19c16b":"Logistic regression is known as the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.","70ec3310":"Before diving into our models, it is essential we encode our categorival variables. We will be using the dummy variables to encode the variable \"Embarked\" instead of a label encoding (S=1,C=2,Q=3). The reason is we don't want the algorithm to consider that there is an order between the classes.","b968fe96":"Looking at the result, we can assume that being a woman gives a better probability of survival (the survival rate among women is very high (74%))\n","f4671fa6":"Looking at the results, the survival rate decreases as we \"decrease\" in our classes.\nNow, let's look at how the survival rate changes taking into account the number of siblings\/spouses or parents\/children aboard the Titanic.","731517e5":"We choose to replace the missing values with the value that is most frequent, since there are only two missing values.","d68afbd6":"## <H3>Combining SibSp and Parch","81bc7153":"For our model to perform well, we have to impute the missing values. Since the variable cabin has too many missing values, we will focus only on the variables \"age\" and \"Embarked\"\n","7f1735de":"<H3>Age Variable","fb0ae3e8":"<H3> Percentage of survivors per class , SibSp , Parch:","80118f59":"<H3> Percentage of Men and Women who survived","34d4ba30":"# Conclusion and insights ","a37142cc":"<H1> Getting started with Titanic","7497c1f5":"Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches.","d45602d7":"With a score of 93.27, it appears that decision tree fits perfectly our data. Yet, it doesn't mean that it is the best one we can use to predict. The three build models were based on multiple assumptions, different methods of imputing missing variables, and droping multiple variables.\nIn an other scenario, one can transform the variable \"Age\" to classes of ages and choose another method of imputation. Also,we can work on the variable \"Name\" and try to extract different categories from it, as it contains multiple titles such as (Mr,Miss,Mrs...).\n\n","80ce917a":"Next, we prepare our independent variable vector and our features vectors. We will drop the variables : Cabin, Ticket, Name, Passenger ID. The variables SibSp and Parch were replaced by the variable \"fam_Member\" which stands for the number of relatives that were aboard for a given passenger.","3c619f4b":" Further definition and more on the challenge is [here](https:\/\/www.kaggle.com\/c\/titanic) at Kaggle.","e289606e":"<H3>Embarked Variable","036ae76e":"<H1>Loading the test data set","164b8d2f":"<H1> Looking for patterns","0226862d":"Next we are trying to fit our model using the KNN Algorithm;\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. The algorithm captures the idea of similarity, and will try to find similar groups based on their characteristics.","d1365648":"## Decision tree","7a0ebf45":"<H1> Building our model\n   ","f565cd6b":"<H1>Exploring Our Data","a8793a05":"Our training sample has 891 observations:\n* The mean age of the sample is 29.70\n* Around 38% of the passengers of our sample have survived.\n* More than 75% of the passengers traveled without a parent\/child.\n* More than 50% of the passengers were in class 3.\n* There are 687 missing values in the variable \"Cabin\"\n* There are 177 missing values in the variable \"Age\".\n* There are two missing values in the variable \"Embarked\".\n","a51d302e":"<H2>Variable Encoding\n   ","30464f71":"<H3>This notebook aims to build a predictive model that determins if a passenger, taking into account his different charasteristcs and variables, is likely to survive.","0e0fec89":"<H1>Loading the train data set","4a3a193a":"First things first, we are going to get our two datasets: \n* train_data : the data that will be use to fit the model\n* test_data : the data used to submit our predictions","0011bf3a":"## Logistic regression","e6e72d27":"<H2>Getting the data","c267af43":"One way to impute the missing values in the variable \"age\" is to use the mean we observed in our sample. Another way would be to take the mean or median values of Age across a specified class and gender.\nThe second option is the one we choose.","db1303ab":"Our logisitc regression has a precision score of 80.58","237d131f":"## KNN ","28266884":"Our problem is a classification and regression problem. We are trying to identify relationship between the survival and other variables (Gender, Age, Port...). Plenty of supervised learning are available, but we choose to train our model using three algortihms:\n* Logistic Regression\n* Decision Tree\n* KNN (k-Nearest Neighbor)\n","2de36344":"Before we impute our missing values, let's recode the variable Sex","f610bf31":"<H3>Mean age by sex\n","ca7b062d":"<H1> Imputing missing values:"}}