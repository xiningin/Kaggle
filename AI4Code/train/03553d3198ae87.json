{"cell_type":{"62a78ecc":"code","e990f696":"code","93fc82a2":"code","1c10252b":"code","497373ca":"code","a950ea9c":"code","3a6c0ad9":"code","d7b38348":"code","a144904e":"code","b197d714":"code","52606bce":"code","944b1a97":"code","1c240cf2":"code","6d6e40f3":"code","c4873ba7":"code","16470e40":"code","33ece125":"code","67f5e49f":"code","aee13086":"code","85ce5440":"code","ccbd0dd8":"code","4fece5a4":"code","04040b0d":"code","c955ebbe":"code","900fb985":"code","51d3c838":"code","a11dbf6d":"code","dc684a88":"markdown","5cf8e352":"markdown","48a9b4f1":"markdown"},"source":{"62a78ecc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e990f696":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","93fc82a2":"train = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/train.csv\", parse_dates=['datetime'])\ntest = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/test.csv\", parse_dates=['datetime'])\nsub = pd.read_csv(\"\/kaggle\/input\/bike-sharing-demand\/sampleSubmission.csv\")","1c10252b":"new_tr = train.copy()\nnew_test = test.copy()","497373ca":"## \ub354\ubbf8\ubcc0\uc218, \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131\nnew_tr['year'] = new_tr['datetime'].dt.year\nnew_tr['month'] = new_tr['datetime'].dt.month\nnew_tr['day'] = new_tr['datetime'].dt.day\nnew_tr['hour'] = new_tr['datetime'].dt.hour\nnew_tr['minute'] = new_tr['datetime'].dt.minute\nnew_tr['second'] = new_tr['datetime'].dt.second\nnew_tr['dayofweek'] = new_tr['datetime'].dt.dayofweek","a950ea9c":"new_test['year'] = new_test['datetime'].dt.year\nnew_test['month'] = new_test['datetime'].dt.month\nnew_test['day'] = new_test['datetime'].dt.day\nnew_test['hour'] = new_test['datetime'].dt.hour\nnew_test['minute'] = new_test['datetime'].dt.minute\nnew_test['second'] = new_test['datetime'].dt.second\nnew_test['dayofweek'] = new_test['datetime'].dt.dayofweek","3a6c0ad9":"col_names = ['year','month','day','hour','dayofweek']\ni = 0\nplt.figure(figsize=(12,35))  ##\uc804\uccb4 \uadf8\ub798\ud504 \ud06c\uae30 \uc9c0\uc815\n\nfor name in col_names: ## \uceec\ub7fc\uba85\uc73c\ub85c \ubc18\ubcf5\n  i = i+1\n  plt.subplot(6,2,i)  ##2\ud5892\uc5f4, i = 1,2,3,4 (\uc67c\ucabd \uc0c1\ub2e8\ubd80\ud130 \uc2dc\uacc4\ubc29\ud5a5\uc73c\ub85c \uc21c\ubc88 \uc9c0\uc815)\n  sns.countplot(x = name, data = new_tr)\n  plt.title(\"train feature\")\n    \n  i = i+1\n  plt.subplot(6,2,i)  ##2\ud5892\uc5f4, i = 1,2,3,4 (\uc67c\ucabd \uc0c1\ub2e8\ubd80\ud130 \uc2dc\uacc4\ubc29\ud5a5\uc73c\ub85c \uc21c\ubc88 \uc9c0\uc815)\n  sns.countplot(x = name, data = new_test)\n  plt.title(\"test feature\")\nplt.show()","d7b38348":"plt.figure(figsize=(15,10))\ng = sns.heatmap(new_tr.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=False)","a144904e":"feature_names = [ 'season', 'holiday', 'workingday', 'weather', \n                  'temp', 'atemp', 'humidity', 'windspeed', \n                  \"year\", \"hour\", \"dayofweek\"]  # \uacf5\ud1b5 \ubcc0\uc218","b197d714":"### \ud53c\ucc98 \uc120\ud0dd X, y, test \ub370\uc774\ud130 \uc14b\uc5d0 \ub300\ud574 \ub370\uc774\ud130 \ub098\ub204\uae30,\n# y - log_count\ub97c \uc0dd\uc131\ud558\uace0 \uc9c0\uc815\ud558\uae30","52606bce":"new_tr['log_count'] = np.log1p(new_tr['count'])","944b1a97":"from sklearn.model_selection import train_test_split","1c240cf2":"# new_tr, new_test\nfeature_names = [ 'season', 'holiday', 'workingday', 'weather', \n                  'temp', 'atemp', 'humidity', 'windspeed', \n                  \"year\", \"hour\", \"dayofweek\"]  # \uacf5\ud1b5 \ubcc0\uc218\nX = new_tr[feature_names]\ny = new_tr['log_count']\n\nX_test_last = new_test[feature_names]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                   test_size=0.1,\n                                                   random_state=77)\n\n","6d6e40f3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time","c4873ba7":"model = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\nscores = cross_val_score(model, X_test, y_test, cv=5,\n                        scoring='neg_mean_squared_error')\nprint(\"MSE \ud3c9\uade0 : \",np.abs( scores.mean() ))","16470e40":"def model_val(model_name, model_obj):\n    now_time = time.time()\n    \n    model.fit(X_train, y_train)\n    \n    scores = cross_val_score(model, X_test, y_test, cv=5,\n                            scoring = 'neg_mean_squared_error')\n    avg_score = np.abs( scores.mean() )\n    print(\"MSE \ud3c9\uade0 : \", avg_score)\n    \n    pro_time = time.time() - now_time\n    print(\"\uc218\ud589 \uc2dc\uac04 : {0:3f}\".format(pro_time))\n    print(\"{} Score : {}\".format(model_name, avg_score))\n    return avg_score, pro_time","33ece125":"model_list = [\"RandomForestRegressor\",  \"xgb_basic\", \n              \"lightgbm-model1\", \"lightgbm-model2\"]\n\nexe_model = []\nmodel_score = []\nmodel_time = []","67f5e49f":"m_name = model_list[0]\n\nif m_name not in exe_model:\n    model = RandomForestRegressor(random_state=30)\n    mse_score, p_time = model_val(m_name, model)\n    \n    exe_model.append(m_name) #\uc2e4\ud589 \uc644\ub8cc\n    model_score.append(mse_score)\n    model_time.append(p_time)\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")\n","aee13086":"# 2\ubc88\uc9f8 \nm_name = model_list[1]\n\nif m_name not in exe_model:\n    model = RandomForestRegressor(random_state=30)\n    mse_score, p_time = model_val(m_name, xg_reg)\n    \n    exe_model.append(m_name) #\uc2e4\ud589 \uc644\ub8cc\n    model_score.append(mse_score)\n    model_time.append(p_time)\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', \n                colsample_bytree = 0.3, # \uac01\ub098\ubb34\ub9c8\ub2e4 \uc0ac\uc6a9\ud558\ub294 feature \ube44\uc728\n                learning_rate = 0.1,\n                max_depth = 3, \n                alpha = 0.1, \n                n_estimators = 1000)\n","85ce5440":"# 3\ubc88\uc9f8 \n# m_lgbm1 = lgb.LGBMRegressor()\nm_name = model_list[2]\n\nif m_name not in exe_model:\n    m_lgbm1 = RandomForestRegressor(random_state=30)\n    mse_score, p_time = model_val(m_name, m_lgbm1)\n    \n    exe_model.append(m_name) #\uc2e4\ud589 \uc644\ub8cc\n    model_score.append(mse_score)\n    model_time.append(p_time)\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")\n\n","ccbd0dd8":"# 4\ubc88\uc9f8\nhyperparameters = {'boosting_type': 'gbdt', \n                   'colsample_bytree': 0.7250136792694301, \n                   'is_unbalance': False, \n                   'learning_rate': 0.013227664889528229,\n                   'min_child_samples': 20, \n                   'num_leaves': 56, \n                   'reg_alpha': 0.7543896477745794, \n                   'reg_lambda': 0.07152751159655985, \n                   'subsample_for_bin': 240000, \n                   'subsample': 0.5233384321711397, \n                   'n_estimators': 2000}\n\nm_name = model_list[2]\n\nif m_name not in exe_model:\n    m_lgbm2 = lgb.LGBMRegressor(**hyperparameters)\n    mse_score, p_time = model_val(m_name, m_lgbm2)\n    \n    exe_model.append(m_name) #\uc2e4\ud589 \uc644\ub8cc\n    model_score.append(mse_score)\n    model_time.append(p_time)\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")\n","4fece5a4":"m_name = model_list[1]\n\n# \uc810\uc218 \ubc0f \uac78\ub9b0 \uc2dc\uac04\nif m_name not in exe_model:\n    xg_reg = xgb.XGBRegressor(objective ='reg:linear', \n                colsample_bytree = 0.3, # \uac01\ub098\ubb34\ub9c8\ub2e4 \uc0ac\uc6a9\ud558\ub294 feature \ube44\uc728\n                learning_rate = 0.1,\n                max_depth = 3, \n                alpha = 0.1, \n                n_estimators = 1000)  # n_estimators=100\n\n    mse_score, p_time = model_val(m_name, xg_reg)\n    \n    exe_model.append(m_name)        # \uc2e4\ud589\ub41c \ubaa8\ub378\n    model_score.append(mse_score)   # \uc2e4\ud589\ub41c \ubaa8\ub378 \uc810\uc218\n    model_time.append(p_time)       # \uc2e4\ud589\ub41c \ubaa8\ub378 \uac78\ub9b0 \uc2dc\uac04\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")","04040b0d":"import lightgbm as lgb \n\nm_name = model_list[2]\n\n# \uc810\uc218 \ubc0f \uac78\ub9b0 \uc2dc\uac04\nif m_name not in exe_model:\n    m_lgbm1 = lgb.LGBMRegressor()\n\n    mse_score, p_time = model_val(m_name, m_lgbm1)\n    \n    exe_model.append(m_name)        # \uc2e4\ud589\ub41c \ubaa8\ub378\n    model_score.append(mse_score)   # \uc2e4\ud589\ub41c \ubaa8\ub378 \uc810\uc218\n    model_time.append(p_time)       # \uc2e4\ud589\ub41c \ubaa8\ub378 \uac78\ub9b0 \uc2dc\uac04\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")","c955ebbe":"hyperparameters = {'boosting_type': 'gbdt', \n                   'colsample_bytree': 0.7250136792694301, \n                   'is_unbalance': False, \n                   'learning_rate': 0.013227664889528229,\n                   'min_child_samples': 20, \n                   'num_leaves': 56, \n                   'reg_alpha': 0.7543896477745794, \n                   'reg_lambda': 0.07152751159655985, \n                   'subsample_for_bin': 240000, \n                   'subsample': 0.5233384321711397, \n                   'n_estimators': 2000}\n\nm_name = model_list[3]\n\n# \uc810\uc218 \ubc0f \uac78\ub9b0 \uc2dc\uac04\nif m_name not in exe_model:\n    m_lgbm2 = lgb.LGBMRegressor(**hyperparameters)\n\n    mse_score, p_time = model_val(m_name, m_lgbm2)\n    \n    exe_model.append(m_name)        # \uc2e4\ud589\ub41c \ubaa8\ub378\n    model_score.append(mse_score)   # \uc2e4\ud589\ub41c \ubaa8\ub378 \uc810\uc218\n    model_time.append(p_time)       # \uc2e4\ud589\ub41c \ubaa8\ub378 \uac78\ub9b0 \uc2dc\uac04\nelse:\n    print(f\"{m_name} \uc774\ubbf8 \uc2e4\ud589 \uc644\ub8cc\")","900fb985":"import pandas as pd\n\ndict_dat = {\"model_name\":exe_model, \"score\":model_score, \"time\":model_time}\ndat = pd.DataFrame(dict_dat)\ndat","51d3c838":"hyperparameters = {'boosting_type': 'gbdt', \n                   'colsample_bytree': 0.7250136792694301, \n                   'is_unbalance': False, \n                   'learning_rate': 0.013227664889528229,\n                   'min_child_samples': 20, \n                   'num_leaves': 56, \n                   'reg_alpha': 0.7543896477745794, \n                   'reg_lambda': 0.07152751159655985, \n                   'subsample_for_bin': 240000, \n                   'subsample': 0.5233384321711397, \n                   'n_estimators': 2000}\n\n# \ucd5c\uc885 \ubaa8\ub378 \uc120\ud0dd \ubc0f \uc81c\ucd9c\nm_lgbm2 = lgb.LGBMRegressor(**hyperparameters)\nm_lgbm2.fit(X_train, y_train)\n\npred = m_lgbm2.predict(X_test_last)\nsub['count'] = np.expm1(pred)\nsub.to_csv(\"submission.csv\", index=False)","a11dbf6d":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.barplot(x='score', y= 'model_name', data=dat)\n\nplt.subplot(1,2,2)\nsns.barplot(x='time', y='model_name', data=dat)","dc684a88":"## xgboost \ubaa8\ub378 \uc0dd\uc131 \ubc0f \ud3c9\uac00\ud574 \ubcf4\uae30","5cf8e352":"## lightgbm \ubaa8\ub378 \uc0dd\uc131 \ubc0f \ud3c9\uac00\ud574\ubcf4\uae30","48a9b4f1":"## lightgbm \ubaa8\ub378 \uc0dd\uc131 \ubc0f \ud3c9\uac00\ud574 \ubcf4\uae30(\ud558\uc774\ud37c \ud30c\ub9ac\ubbf8\ud130 \ud29c\ub2dd)"}}