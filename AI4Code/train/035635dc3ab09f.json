{"cell_type":{"ac4e04f4":"code","3f7c2b51":"code","c2ebcc38":"code","4654ea70":"code","2c5dfa7e":"code","6743f924":"code","1dad5480":"code","492035b6":"code","b6464faf":"code","459fd5dc":"code","21207fca":"code","5c6318a1":"code","8e2b1c63":"code","2f2e1ff9":"code","8b7cc572":"code","df74f4a2":"code","4cd57313":"code","ed211998":"code","16fdffde":"code","2a982deb":"code","2c30e862":"code","e0b67439":"code","ae5552b7":"code","b6bd51d4":"code","d68b5a0f":"code","b5579f5e":"code","02ee0dad":"code","5da6b659":"code","1bcb85a9":"code","a9bb264e":"code","c8fa2994":"code","09e6e18a":"code","c42619ba":"code","777c97be":"code","89bdcf6b":"code","6f82f73d":"code","1cdc93d9":"code","4aab7aa2":"code","98852894":"code","c255cac0":"code","cfc9fc6f":"code","f8743aff":"code","4ddda3c6":"code","475b236d":"code","1d8d0d04":"code","cd69b330":"code","29578fe3":"code","4297fdab":"code","33214f65":"code","f4d75489":"code","71cc3502":"code","7548c411":"code","696e3359":"code","35865df9":"code","a3562ba3":"code","6c8b66b4":"markdown","0b8d5524":"markdown","384ef5dc":"markdown","0875fd01":"markdown","3b505b9b":"markdown","9701ed31":"markdown","b9a3f0b6":"markdown","d1854547":"markdown","9af08354":"markdown","6b9ea8b1":"markdown","34fa6d4d":"markdown","c76d8ea0":"markdown","f34deb70":"markdown","493f1c32":"markdown","df88008e":"markdown","32168076":"markdown","ca614760":"markdown","2a3e3fb8":"markdown","19afd16d":"markdown","f4887ec8":"markdown","9b4f5172":"markdown","5bd0be37":"markdown","96dcb852":"markdown","9de30d6c":"markdown","2b9bb4f8":"markdown","9f978a34":"markdown","41ebbf36":"markdown","f6fb8178":"markdown","28aac554":"markdown","dde47513":"markdown","c442d5c8":"markdown","48ef2157":"markdown","95b536f0":"markdown","1d10713b":"markdown","81b91ac2":"markdown","c410dd5c":"markdown","01d629e4":"markdown","16a2e694":"markdown","c24bf335":"markdown","61f5de2d":"markdown","de3264ec":"markdown","a55feeab":"markdown"},"source":{"ac4e04f4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import confusion_matrix,precision_score,classification_report,f1_score,roc_curve,roc_auc_score,auc,accuracy_score,recall_score\nfrom sklearn import metrics\n%matplotlib inline","3f7c2b51":"df = pd.read_excel(\"\/kaggle\/input\/bank-loan-modelling\/\"\n                     \"Bank_Personal_Loan_Modelling.xlsx\",\n                     sheet_name = \"Data\")","c2ebcc38":"df['Personal Loan'].value_counts()","4654ea70":"df.head()","2c5dfa7e":"df.info()","6743f924":"df.shape","1dad5480":"df.describe()","492035b6":"df.drop('ID',inplace=True,axis=1)","b6464faf":"# No columns have null data in the file\ndf.isnull().sum()","459fd5dc":"#finding unique data\ndf.apply(lambda x: len(x.unique()))","21207fca":"# There are 52 records with negative experience. Before proceeding any further we need to clean the same\ndf[df['Experience'] < 0]['Experience'].count()\n","5c6318a1":"# Cheching the correlation of Experience with other variables.\n\ndf.corr()['Experience']","8e2b1c63":"# Creating a list of ages.\nages = df[df['Experience'] < 0]['Age'].unique().tolist()\n\n# Creating a list of indexes.\nindexes = df[df['Experience'] < 0].index.tolist()\n\n# Replacing Negative Value with median.\nfor i in indexes:\n    for x in ages:\n        df.loc[i,'Experience'] = df[(df.Age == x) & (df.Experience > 0)].Experience.median()\n        ","2f2e1ff9":"# checking if there are records with negative experience\ndf[df['Experience'] < 0]['Experience'].count()","8b7cc572":"df.isna().sum()","df74f4a2":"# Outlier is defined as Data points above or below than 1.5 times the Inter Quartile Range of the data.\nnumerical = ['Age','Income','Experience','CCAvg','Mortgage']\nQ1 = df[numerical].quantile(0.25)\nQ3 = df[numerical].quantile(0.75)\nIQR = Q3 - Q1\nout = (df[numerical] < (Q1 - 1.5 * IQR)) | (df[numerical] > (Q3 + 1.5 * IQR))\nout.sum()","4cd57313":"sns.boxplot(x=df['Income'])","ed211998":"sns.boxplot(x=df['CCAvg'])","16fdffde":"sns.boxplot(x=df['Mortgage'])","2a982deb":"df[numerical].hist(bins=15, figsize=(20, 10), layout=(2, 3));","2c30e862":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['Family'],hue=df['Personal Loan'],ax=axes[1],order = df[\"Family\"].value_counts().index)\nplt.legend()\nax.set_title('Family vs Personal Loan')\nax = sns.countplot(df['Family'],ax=axes[0],order = df[\"Family\"].value_counts().index)\nplt.legend()\nax.set_title('Family Countplot')\nplt.show()","e0b67439":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['Education'],hue=df['Personal Loan'],ax=axes[1],order = df[\"Education\"].value_counts().index)\nplt.legend()\nax.set_title('Education vs Personal Loan')\nax = sns.countplot(df['Education'],ax=axes[0],order = df[\"Education\"].value_counts().index)\nplt.legend()\nax.set_title('Education_Level Countplot')\nplt.show()","ae5552b7":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['Securities Account'],hue=df['Personal Loan'],ax=axes[1])\nplt.legend()\nax.set_title('Security Account vs Personal Loan')\nax = sns.countplot(df['Securities Account'],ax=axes[0])\nplt.legend()\nax.set_title('Security Account Countplot')\nplt.show()","b6bd51d4":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['CD Account'],hue=df['Personal Loan'],ax=axes[1])\nplt.legend()\nax.set_title('CD Account vs Personal Loan')\nax = sns.countplot(df['CD Account'],ax=axes[0])\nplt.legend()\nax.set_title('Countplot for Certificate of Deposit Account ')\nplt.show()","d68b5a0f":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['Online'],hue=df['Personal Loan'],ax=axes[1])\nplt.legend()\nax.set_title('Internet Banking vs Personal Loan')\nax = sns.countplot(df['Online'],ax=axes[0])\nplt.legend()\nax.set_title('Countplot for Internet Banking')\nplt.show()","b5579f5e":"# Using sns.countplot() for visualizing the counts\n\nfig,axes = plt.subplots(nrows=1,ncols=2, figsize=(12,6) )\nax = sns.countplot(df['CreditCard'],hue=df['Personal Loan'],ax=axes[1])\nplt.legend()\nax.set_title('CreditCard vs Personal Loan')\nax = sns.countplot(df['CreditCard'],ax=axes[0])\nplt.legend()\nax.set_title('Credit Card Countplot')\nplt.show()","02ee0dad":"sns.catplot(x='Education', y = 'Income', data = df,kind= 'box',hue='Personal Loan')","5da6b659":"sns.catplot(x='Education',y='Mortgage',data=df,hue='Personal Loan',kind='box')","1bcb85a9":"fig,ax = plt.subplots( figsize=(7,5) )\ndf['Personal Loan'].value_counts(sort=False).plot(kind='pie',autopct='%1.1f%%', fontsize= 20,startangle=120)\nplt.legend(['Loan Rejected','Loan Accepted '])\nplt.show()","a9bb264e":"# Plot for Visualising the correlation between variables and Target Column.\n\nfig,ax = plt.subplots( figsize=(16,8) )\nsns.heatmap(df.corr(),annot=True)\nplt.title('Heatmap for Correlation')\nplt.show()","c8fa2994":"# Creating seperate DataSets for Target column('Personal Loan') named as 'y' and rest of the features in 'X' \n\ny = df[\"Personal Loan\"]\nX = df.drop(\"Personal Loan\",axis=1)","09e6e18a":"X.drop('ID',axis=1,inplace=True)","c42619ba":"X.drop('ZIP Code',axis=1,inplace=True)","777c97be":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","89bdcf6b":"sc = StandardScaler()\nX_train_scaled=sc.fit_transform(X_train)\nX_test_scaled= sc.transform(X_test)","6f82f73d":"lr = LogisticRegression()\nlr.fit(X_train_scaled,y_train)\nlr_pred = lr.predict(X_test_scaled)\nlr_accu_test = lr.score(X_test_scaled,y_test)\nlr_accu_train = lr.score(X_train_scaled,y_train)\nlr_f1 = f1_score(y_test,lr_pred)\nprint(\"Training Accuracy :\", lr_accu_train)\nprint(\"Testing accuracy  :\",lr_accu_test)\nprint(\"F1 Score :\",lr_f1)\nprint(\"Logistic Regression Confusion matrix :\\n\\n\", confusion_matrix(y_test,lr_pred))","1cdc93d9":"nb = GaussianNB()\nnb.fit(X_train_scaled,y_train)\nnb_predict = nb.predict(X_test_scaled)\nnb_accu_test = nb.score(X_test_scaled,y_test)\nnb_accu_train = nb.score(X_train_scaled,y_train)\nnb_f1 = f1_score(y_test,nb_predict)\nprint(\"Training Accuracy :\", nb_accu_train)\nprint(\"Testing accuracy  :\",nb_accu_test)\nprint(\"F1 Score :\",nb_f1)\nprint(\"Naive Bayes Confusion matrix :\\n\\n\", confusion_matrix(y_test,nb_predict))","4aab7aa2":"# Plotting a graph between various n_neighbors and accuracy scores.\n\nneighbors = np.arange(1,21)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled,y_train)\n    train_accuracy[i] = knn.score(X_train_scaled,y_train)\n    test_accuracy[i] = knn.score(X_test_scaled, y_test)\n    \nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","98852894":"#testing accuracy is the highest for 'n_neighbors'=3\nknn= KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_scaled,y_train)\nknn_predict = knn.predict(X_test_scaled)\nknn_accu_test = knn.score(X_test_scaled,y_test)\nknn_accu_train = knn.score(X_train_scaled,y_train)\nknn_f1 = f1_score(y_test,knn_predict)\nprint(\"Training Accuracy :\", knn_accu_train)\nprint(\"Testing accuracy  :\",knn_accu_test)\nprint(\"F1 Score :\",knn_f1)\nprint(\"KNN Confusion matrix :\\n\\n\", confusion_matrix(y_test,knn_predict))","c255cac0":"# Creating CART model with max_depth = 5\n\ndt = DecisionTreeClassifier(criterion='entropy',max_depth=1)\ndt.fit(X_train_scaled,y_train)\ndt_pred = dt.predict(X_test_scaled)\ndt_training = dt.score(X_train_scaled,y_train)\ndt_testing = dt.score(X_test_scaled,y_test)\ndt_precision = precision_score(y_test,dt_pred)\ndt_f1 = f1_score(y_test,dt_pred)\nprint(\"Traing Accuracy :\", dt_training)\nprint(\"Testing Accuracy :\",dt_testing )\nprint(\"F1 Score: \",dt_f1 )\nprint('Decision Tree Confusion matrix :\\n\\n',confusion_matrix(y_test, dt_pred) )","cfc9fc6f":"rf = RandomForestClassifier(criterion='entropy',max_depth=50,n_estimators=50)\nrf.fit(X_train_scaled,y_train)\nrf_pred = rf.predict(X_test_scaled)\nrf_training = rf.score(X_train_scaled,y_train)\nrf_testing = rf.score(X_test_scaled,y_test)\nrf_precision = precision_score(y_test,rf_pred)\nrf_f1 = f1_score(y_test,rf_pred)\nprint(\"Traing Accuracy :\", rf_training)\nprint(\"Testing Accuracy :\",rf_testing )\nprint(\"F1 Score: \",rf_f1 )\nprint('Random Forest Confusion matrix :\\n\\n',confusion_matrix(y_test, rf_pred) )","f8743aff":"print('F1 Score\\n :',classification_report(y_test, rf_pred))","4ddda3c6":"bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=500,bootstrap=True,max_samples=100)\nbg.fit(X_train_scaled,y_train)\nbg_pred = bg.predict(X_test_scaled)\nbg_training = bg.score(X_train_scaled,y_train)\nbg_testing = bg.score(X_test_scaled,y_test)\nbg_precision = precision_score(y_test,bg_pred)\nbg_f1 = f1_score(y_test,bg_pred)\nprint(\"Traing Accuracy :\", bg_training)\nprint(\"Testing Accuracy :\",bg_testing )\nprint(\"F1 Score: \",bg_f1 )\nprint('Bagging Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, bg_pred) )","475b236d":"print('F1 Score\\n :',classification_report(y_test, bg_pred))","1d8d0d04":"ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),n_estimators=10,learning_rate=0.5)\nada.fit(X_train_scaled,y_train)\nada_pred = ada.predict(X_test_scaled)\nada_training = ada.score(X_train_scaled,y_train)\nada_testing = ada.score(X_test_scaled,y_test)\nada_f1 = f1_score(y_test,ada_pred)\nada_precision = precision_score(y_test,ada_pred)\nprint(\"Traing Accuracy :\", ada_training)\nprint(\"Testing Accuracy :\",ada_testing )\nprint(\"F1 Score: \",ada_f1 )\nprint('AdaBoost Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, ada_pred) )\n","cd69b330":"print('F1 Score\\n :',classification_report(y_test, ada_pred))","29578fe3":"gbc = GradientBoostingClassifier(learning_rate=0.02,n_estimators=65)\ngbc.fit(X_train_scaled,y_train)\ngbc_pred = gbc.predict(X_test_scaled)\ngbc_training = gbc.score(X_train_scaled,y_train)\ngbc_testing = gbc.score(X_test_scaled,y_test)\ngbc_f1 = f1_score(y_test,gbc_pred)\ngbc_precision = precision_score(y_test,gbc_pred)\nprint(\"Traing Accuracy :\", gbc_training)\nprint(\"Testing Accuracy :\",gbc_testing )\nprint(\"F1 Score: \",gbc_f1 )\nprint('AdaBoost Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, gbc_pred) )\n","4297fdab":"print('F1 Score\\n :',classification_report(y_test, gbc_pred))","33214f65":"# ROC for Random Forest\nrf_prob = rf.predict_proba(X_test_scaled)\nfpr,tpr,thresh=roc_curve(y_test,rf_prob[:,1])\nauc1 = auc(fpr,tpr)\nprint(\"Area under the curve for Random Forest \", auc1)\n\n#ROC for Bagging\nbg_prob = bg.predict_proba(X_test_scaled)\nfpr1,tpr1,thresh1=roc_curve(y_test,bg_prob[:,1])\nauc2 = auc(fpr1,tpr1)\nprint(\"Area under the curve for Bagging \", auc2)\n\n#ROC for AdaBoost\nada_prob = ada.predict_proba(X_test_scaled)\nfpr2,tpr2,thresh2=roc_curve(y_test,ada_prob[:,1])\nauc3 = auc(fpr2,tpr2)\nprint(\"Area under the curve for AdaBoost \", auc3)\n\n#ROC for GradientBoost\ngbc_prob = gbc.predict_proba(X_test_scaled)\nfpr3,tpr3,thresh3=roc_curve(y_test,gbc_prob[:,1])\nauc4 = auc(fpr3,tpr3)\nprint(\"Area under the curve for  Gradient \", auc4)\n\nlr_prob = lr.predict_proba(X_test_scaled)\nlr_fpr,lr_tpr,lr_thresh=roc_curve(y_test,lr_prob[:,1])\nlr_auc = auc(lr_fpr,lr_tpr)\nprint(\"Area under the curve for  Logistic Regression \", lr_auc)\n\ndt_prob = dt.predict_proba(X_test_scaled)\ndt_fpr,dt_tpr,dt_thresh=roc_curve(y_test,dt_prob[:,1])\ndt_auc = auc(dt_fpr,dt_tpr)\nprint(\"Area under the curve for Decision Trees \", dt_auc)\n\n\nknn_prob = knn.predict_proba(X_test_scaled)\nknn_fpr,knn_tpr,knn_thresh=roc_curve(y_test,knn_prob[:,1])\nknn_auc = auc(knn_fpr,knn_tpr)\nprint(\"Area under the curve for KNN \", knn_auc)\n\nnb_prob = nb.predict_proba(X_test_scaled)\nnb_fpr,nb_tpr,nb_thresh=roc_curve(y_test,nb_prob[:,1])\nnb_auc = auc(nb_fpr,nb_tpr)\nprint(\"Area under the curve for Naive \", nb_auc)\n","f4d75489":"#Plot the ROC curve \nplt.clf()\nfig, ax= plt.subplots(nrows = 3, ncols = 2, figsize = (15,15))\nax[0,0].plot(fpr, tpr, label='AUC area = %0.2f' % auc1)\nax[0,0].plot([0, 1], [0, 1], 'k--')\nax[0,0].set_xlabel('False Positive Rate')\nax[0,0].set_ylabel('True Positive Rate')\nax[0,0].set_title('ROC for Random Forest')\nax[0,0].legend(loc=\"lower right\")\n\nax[0,1].plot(fpr1, tpr1, label='AUC = %0.2f' % auc2)\nax[0,1].plot([0, 1], [0, 1], 'k--')\nax[0,1].set_xlabel('False Positive Rate')\nax[0,1].set_ylabel('True Positive Rate')\nax[0,1].set_title('ROC for Bagging')\nax[0,1].legend(loc=\"lower right\")\n\nax[1,0].plot(fpr2, tpr2, label='AUC = %0.2f' % auc3)\nax[1,0].plot([0, 1], [0, 1], 'k--')\nax[1,0].set_xlabel('False Positive Rate')\nax[1,0].set_ylabel('True Positive Rate')\nax[1,0].set_title('ROC for AdaBoost')\nax[1,0].legend(loc=\"lower right\")\n\nax[1,1].plot(fpr3, tpr3, label='AUC = %0.2f' % auc4)\nax[1,1].plot([0, 1], [0, 1], 'k--')\nax[1,1].set_xlabel('False Positive Rate')\nax[1,1].set_ylabel('True Positive Rate')\nax[1,1].set_title('ROC Gradient')\nax[1,1].legend(loc=\"lower right\")\n\nax[2,0].plot(lr_fpr, lr_tpr, label='AUC = %0.2f' % lr_auc)\nax[2,0].plot([0, 1], [0, 1], 'k--')\nax[2,0].set_xlabel('False Positive Rate')\nax[2,0].set_ylabel('True Positive Rate')\nax[2,0].set_title('ROC for Logistic Regression')\nax[2,0].legend(loc=\"lower right\")\n\nax[2,1].plot(knn_fpr, knn_tpr, label='AUC = %0.2f' % knn_auc)\nax[2,1].plot([0, 1], [0, 1], 'k--')\nax[2,1].set_xlabel('False Positive Rate')\nax[2,1].set_ylabel('True Positive Rate')\nax[2,1].set_title('ROC for KNN')\nax[2,1].legend(loc=\"lower right\")\n\nplt.show()\n\n","71cc3502":"print(\"Logistic Regression: \\n\",confusion_matrix(y_test,lr_pred))\nprint(\"\\n Gaussian Naive Bayes: \\n\",confusion_matrix(y_test,nb_predict))\nprint(\"\\n K Nearest Neighbour: \\n\",confusion_matrix(y_test,knn_predict))\nprint('\\n Decision Tree Confusion matrix : \\n',confusion_matrix(y_test, dt_pred) )\nprint('\\n Random Forest Confusion matrix : \\n',confusion_matrix(y_test, rf_pred) )\nprint('\\n Bagging Classifier Confusion matrix :  \\n',confusion_matrix(y_test, bg_pred) )\nprint('\\n AdaBoost Classifier Confusion matrix : \\n',confusion_matrix(y_test, ada_pred) )\nprint('\\n Gradient Boost Classifier Confusion matrix :  \\n',confusion_matrix(y_test, gbc_pred) )\n\n\n\n\n#tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()","7548c411":"df_compare = pd.DataFrame([[lr_accu_train,lr_accu_test,lr_f1],\n                           [nb_accu_train,nb_accu_test,nb_f1],\n                           [knn_accu_train,knn_accu_test,knn_f1],\n                           [dt_training,dt_testing,dt_f1],\n                           [rf_training,rf_testing,rf_f1],\n                           [bg_training,bg_testing,bg_f1],\n                          [ada_training,ada_testing,ada_f1],[gbc_training,gbc_testing,gbc_f1]],\n    columns=['Training Accuracy','Testing Accuracy','F1Score'],\n                       index=['Logistic Regression','Gaussian Naive Bayes','KNearest Neighbour',\n                              'DecisionTrees',\n                              'RandomForest','BaggingClassifier','AdaBoost','GradientBoosting'])\ndf_compare","696e3359":"# Top 3 models with Highest testing accuracy are - \ndf_compare.sort_values(ascending=False,by=['Testing Accuracy'])['Testing Accuracy'].head(3)","35865df9":"# Top 3 models with Highest training accuracy are - \ndf_compare.sort_values(ascending=False,by=['Training Accuracy'])['Training Accuracy'].head(3)","a3562ba3":"# Top 3 models with highest F1 Score are \ndf_compare.sort_values(ascending=False,by=['F1Score'])['F1Score'].head(3)","6c8b66b4":"##### Corelation amaong the attributes","0b8d5524":"#### Scaling the features","384ef5dc":"#### Comparision of the models","0875fd01":"* Age feature is normally distributed with majority of customers falling between 30 years and 60 years of age. We can confirm     this by looking at the describe statement above, which shows mean is almost equal to median\n* Experience is normally distributed with more customer having experience starting from 8 years. Here the mean is equal to         median. There are negative values in the Experience. This could be a data input error as in general it is not possible to       measure negative years of experience. We can delete these values, because we have 3 or 4 records from the sample.\n* Income is positively skewed. Majority of the customers have income between 45K and 55K. We can confirm this by saying the mean   is greater than the median\n* CCAvg is also a positively skewed variable and average spending is between 0K to 10K and majority spends less than 2.5K\n* Mortgage 70% of the individuals have a mortgage of less than 40K. However the max value is 635K\n\n","3b505b9b":"#### Relationship of some numerical features with target feature","9701ed31":"* The dataset is biased\n* The percentage of customers who has accepted the loan is very less","b9a3f0b6":"###  Data Cleaning\n\n#### Handling missing values \/ unknown \/ negative values in data","d1854547":"Most of the customers uses Online Banking","9af08354":" * Majority of the customers have family size 1.\n * More number of family size have accepted the loan","6b9ea8b1":"GradientBoost Classifier","34fa6d4d":"Finding the best value for n_neighbors (hyperparameter)","c76d8ea0":"* Income , CD Account and CCAvg column  are slightly corelated with the Target column\n* Others columns such as Income and CCavg are positively corelated.\n* Experirnce and Age are highly corelated","f34deb70":"### Splitting the data","493f1c32":"* Information of the attributes\n  The attributes can be divided accordingly :\n\nThe variable ID does not add any interesting information. There is no association between a person's customer ID and loan, also it does not provide any general conclusion for future potential loan customers. We can neglect this information for our model prediction.\nThe binary category have five variables as below:\n\n* Personal Loan - Did this customer accept the personal loan offered in the last campaign? This is our target variable\n* Securities Account - Does the customer have a securities account with the bank?\n* CD Account - Does the customer have a certificate of deposit (CD) account with the bank?\n* Online - Does the customer use internet banking facilities?\n* Credit Card - Does the customer use a credit card issued by UniversalBank?\n\nInterval variables are as below:\n\n* Age - Age of the customer\n* Experience - Years of experience\n* Income - Annual income in dollars\n* CCAvg - Average credit card spending\n* Mortage - Value of House Mortgage\n\nOrdinal Categorical Variables are:\n\n* Family - Family size of the customer\n* Education - education level of the customer\n\nThe nominal variable is :\n\n* ID\n* Zip Code\n","df88008e":"* Dataset has no missing values\n* As we have observed from the 5 point summary of the data Experience column has negative values which has no meaning.","32168076":"From the above chart it seems that customer who has personal loan have high mortgage","ca614760":"Decision Trees ","2a3e3fb8":"Age,CCAvg and Mortgate contains many outliers.","19afd16d":"#### Naive Bayes","f4887ec8":"* Objective given as per the document is to predict liability of customers considering minimum possible budget.\n* Considering the domain we can say that the model which has less number of False Positive can be considered as a good model.\n* KNN is more close in achieving the objective.(Higher accuray + Least False Positive)\n* Type 1 and Type 2 errors are less for KNN than the other models, as observed from the confusion matrix. \n* KNN has good values in almost every evaluation tests.\n","9b4f5172":"More than 2000 customres has Undergraduate and very less to accept for loan\nCustomers who are Graduate and Professional are more likely to accept loan than undergraduate.","5bd0be37":"##   Distribution of Numerical Features","96dcb852":"AdaBoost Classifier","9de30d6c":" Testing as well as training accuracy is highest for Random Forest.\n \n \n ","2b9bb4f8":"#### Detecting Outliers\n","9f978a34":"#### Distribution of the target column","41ebbf36":"### Separating the target column","f6fb8178":"#### Reasons why Random Forest is performing better than other models are \n * The given dataset has many outlier, and random forest is one of the classification algorithm which can perform better in presence of      outliers.\n * There are many attributes in the dataset which are corelated with each other for example Experience & Age , Income & CCAvg.\n   Naive Bayes Algorithm assumes that the features are independent which is not true in this dataset, hence its performance is not \n   good..\n","28aac554":"Random Forest","dde47513":"Experience has positive corelation with Age.","c442d5c8":"#### Confusion matrix for models","48ef2157":"### Creating different ML Classification models","95b536f0":"#### KNN","1d10713b":"Most of the customer do not have CD Account.\nAlmost al customers who has CD Account has accepted the loan ","81b91ac2":"* From the above plot we can infer that customer having annual income in the range of (120 and 170)$ has accepted the loan.\n* It seems the customers whose education level is 1 is having more income. However customers who has taken the personal loan have the same income levels","c410dd5c":"More than 4000 customers do not have Securities Account","01d629e4":"Most of the customer do not use credit card offered by the bank.\n","16a2e694":"#### Logistic Regression","c24bf335":"### Conclusion","61f5de2d":"### Distribution of categorical features and their relation with the target feature","de3264ec":"* We can replace negative 'Experience' values with the median of total remaining 'Experience' values but it will be not appropriate. So, we will derive various medians of Experience data on the basis of group of respective Age of Customers.\n* Steps Taken to derive median of Experiences on the basis of Age groups -: \n    1. Creating a group of 'Age' values where  negative values in 'Experience' exists.\n    2. Creating a list of indexes of negative values in 'Experience'.\n    3. Replace negative 'Experience' values with the 'Experience' meadian.\n","a55feeab":"Bagging Classifier"}}