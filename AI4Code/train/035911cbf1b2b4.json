{"cell_type":{"b2fe3d72":"code","9df8fafb":"code","4a0c6b98":"code","ef3c8e45":"code","58b29a88":"code","d30706b0":"code","eb6b8f91":"code","9db5d87d":"code","fda49b1f":"code","49c72d61":"code","ebec2efc":"code","b75e8f63":"code","17649c24":"code","4d4f1f67":"code","8cbe0c09":"code","c96b05b3":"code","3331b315":"code","2db63346":"code","7941ee05":"code","973b460f":"code","efafbff0":"code","77081ef8":"code","5b1b05ad":"code","b21e025a":"code","65868afa":"code","c86865f7":"code","3406336e":"code","091a7c69":"code","0dabea92":"code","d39fc25c":"code","d993d6f5":"code","8d668e9f":"code","2f58d72f":"code","2c4e58e7":"code","24466b24":"code","d2c95094":"code","e2386e7f":"code","737ca5ef":"code","0fb40698":"code","62c44467":"code","212eed31":"code","846376a1":"markdown","0e5ece5d":"markdown","d8008d02":"markdown","161dd3c8":"markdown","d84e1798":"markdown","731b3044":"markdown","4f15950d":"markdown","3b92fa38":"markdown","30e6a586":"markdown","8b016924":"markdown","32baf960":"markdown","8576edbe":"markdown","db9bee3a":"markdown","d40957d9":"markdown","2c0857d8":"markdown","2537b8e2":"markdown","220fbbb6":"markdown","95890b2a":"markdown","4d297e2a":"markdown","27a25a45":"markdown","9fbebf1d":"markdown","0d985c3b":"markdown","b16263a3":"markdown"},"source":{"b2fe3d72":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","9df8fafb":"train.head(10)","4a0c6b98":"train.describe()","ef3c8e45":"train.info()\nprint(\"\\n============ Missing Values ==============\\n\")\nprint(train.isnull().sum())","58b29a88":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","d30706b0":"def stacked_bar(feature_name):\n    survived = train[train['Survived'] == 1][feature_name].value_counts()\n    dead = train[train['Survived'] == 0][feature_name].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True,figsize=(10,5))","eb6b8f91":"stacked_bar('Sex')","9db5d87d":"stacked_bar('Pclass')","fda49b1f":"stacked_bar('Embarked')","49c72d61":"stacked_bar('Parch')","ebec2efc":"stacked_bar('SibSp')","b75e8f63":"# combining train and test dataset before extract title from name\n\ntrain_test_data = [train, test] \n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","17649c24":"train['Title'].value_counts()","4d4f1f67":"test['Title'].value_counts()","8cbe0c09":"#Convert categorical value of title into numeric\n\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","c96b05b3":"# Drop feature name & ticket from train and test dataset\n\ntrain.drop(['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\ntest.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)","3331b315":"#Convert categorical value of sex into numeric\n\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","2db63346":"stacked_bar('Embarked')","7941ee05":"train['Embarked'].fillna('S', inplace = True)\ntest['Embarked'].fillna('S', inplace = True)","973b460f":"#Convert categorical value of Embarked into numeric\n\nembark_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embark_mapping)","efafbff0":"#fill age missing value with median value for each title\n\ntrain['Age'].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace = True)\ntest['Age'].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace = True)","77081ef8":"#Convert Age into Categorical feature using bin\n\nfor dataset in train_test_data:\n    dataset.loc[dataset['Age'] <= 17, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 17) & (dataset['Age'] <= 28), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 40), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 60), 'Age'] = 3,\n    dataset.loc[dataset['Age'] > 62, 'Age'] = 4,","5b1b05ad":"#Combine SibSp with Parch into Familysize\n\ntrain[\"Familysize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"Familysize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","b21e025a":"#Scale the feature values into 0 to 5 range\n\nfamilysize_mapping = {0: 0, 1: 0.5, 2: 1, 3: 1.5, 4: 2, 5: 2.5, 6: 3, 7: 3.5, 8: 4, 9: 4.5, 10: 5, 11: 5.5, 12: 6}\nfor dataset in train_test_data:\n    dataset['Familysize'] = dataset['Familysize'].map(familysize_mapping)","65868afa":"#Drop SibSp and Parch from Dataset\n\ntrain.drop(['SibSp','Parch'], axis=1, inplace=True)\ntest.drop(['SibSp','Parch'], axis=1, inplace=True)","c86865f7":"#fill Fare missing value with median value for each Pclass\n\ntrain['Fare'].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\ntest['Fare'].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)","3406336e":"#Binning Fare numerical feature into categorical\n\nfor dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 20, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 20) & (dataset['Fare'] <= 40), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 40) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[dataset['Fare'] > 100, 'Fare'] = 3","091a7c69":"#Check missing values before modelling \n\ntrain.info()\nprint(\"\\n=============================\\n\")\ntest.info()","0dabea92":"correlation_matrix = train.corr().round(2)\nplt.figure(figsize = (16,5))\nsns.heatmap(data=correlation_matrix, annot=True)","d39fc25c":"#Slice Target feature from training data set\n\ntarget = train['Survived']\ntrain = train.drop('Survived', axis=1)","d993d6f5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn import model_selection\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nimport numpy as np","8d668e9f":"# Set models array and its parameter\n\nrand_state = 15\nmodels = []\nmodels.append((\"Logistic Regression\", LogisticRegression(random_state=rand_state)))\nmodels.append((\"KNN\", KNeighborsClassifier(n_neighbors=rand_state)))\nmodels.append((\"Decision Tree\", DecisionTreeClassifier(random_state=rand_state)))\nmodels.append((\"Random Forest\", RandomForestClassifier(random_state=rand_state)))\nmodels.append((\"AdaBoost\", AdaBoostClassifier(random_state=rand_state)))\nmodels.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=rand_state)))\nmodels.append((\"XG Boosting\", xgb.XGBClassifier()))\nmodels.append((\"SVM\", SVC(random_state=rand_state)))","2f58d72f":"# Train the data using k-fold cross validation\n\nkfold = model_selection.KFold(n_splits=10)\nmodel_name = []\nmodel_avgscore = []","2c4e58e7":"for name, model in models:\n    cv_results = model_selection.cross_val_score(model,train,target,scoring=\"accuracy\",cv=kfold)\n    print(\"\\n\"+name)\n    print(\"Avg_score : \"+str(cv_results.mean()))\n    model_name.append(name)\n    model_avgscore.append(cv_results.mean())","24466b24":"# Visualize the result\n\ncv_df = pd.DataFrame({\"AverageScore\":model_avgscore,\"Model\":model_name})\nsns.barplot(\"AverageScore\",\"Model\",data=cv_df,)","d2c95094":"best_model = cv_df.sort_values(by=\"AverageScore\",ascending=False).iloc[0]","e2386e7f":"print(\"Best Model = \"+best_model.Model)\nprint(\"Average Score = \"+str(best_model.AverageScore))","737ca5ef":"svm = SVC()\nsvm.fit(train,target)","0fb40698":"test_id = test['PassengerId']\ntest_data = test.drop('PassengerId', axis=1)\nprediction = svm.predict(test_data)","62c44467":"submission = pd.DataFrame({\"PassengerId\":test_id, \"Survived\":prediction}).set_index(\"PassengerId\")","212eed31":"submission.to_csv('submission.csv')","846376a1":"## 3. Testing","0e5ece5d":"People whose **siblings or spouse** > 2 more likely to survived","d8008d02":"#### Fill missing values in Fare with median respected to Pclass category then convert values into categorical to normalize the outlier","161dd3c8":"#### From **Embarked** bar chart we can see that most of passenger board from S we could fill the missing values with **S**","d84e1798":"#### Extract title feature from names","731b3044":"From the heatmaps we can see that **Fare** has the highest correlation with survivality followed by **Pclass, Sex and Title**","4f15950d":"People aboarded __**Alone**__ more likely dead","3b92fa38":"#### Fill Age missing values with median of title categories then convert Age **numerical** into **categorical** feature","30e6a586":"it seems **_Female_** has higher chance of survive than **_Male_** ","8b016924":"Load train and test data set from kaggle with pandas","32baf960":"## 2. Modelling","8576edbe":"#### From **parent\/child** and **sibling\/spouse** bar chart we could examine people who aboard alone more likely to die.\n#### So we can combine this two features into Familysize feature then scale it to fit the model later","db9bee3a":"### Feature Engineering","d40957d9":"# References","2c0857d8":"From modelling score above we choose **SVM Classifier** to predict the result of the test set","2537b8e2":"Analyze features then choose some that have strong corelation with target feature or ground truth. Sometimes we might be able to extract new feature<br \/> from existing features or combine several ones.<br \/><br \/>\nTransform categorical features into numeric then create **feature vector** which is an n-dimensional vector of features before fitting it into the model.","220fbbb6":"## 1. Exploratory Data Analysis (EDA)","95890b2a":"* [Minsuk Heo Kaggle - Titanic](https:\/\/github.com\/minsuk-heo\/kaggle-titanic)\n\n* [Code A Star Titanic Survivors Data Wrangling](https:\/\/codeastar.com\/data-wrangling)","4d297e2a":"People aboarded form **S** more likely dead and from __C__ slightly has chance to survived","27a25a45":"#### All features normalized and missing values already filled time to see features correlation","9fbebf1d":"### Visualize data to gain feature insights","0d985c3b":"**_Class1_** Passengers more likely survived than other class<br \/>\n\n**_Class2_** Passengers has highest dead count","b16263a3":"# Titanic Survivor Prediction"}}