{"cell_type":{"839a4a78":"code","0ee86f31":"code","0afeab98":"code","391334a2":"code","cf6a6575":"code","4da4a571":"code","a5f42983":"code","cc6a9714":"code","c4b2ade5":"code","7f0d8c9c":"code","7d22fea8":"code","d2254a28":"code","266166e9":"code","ea941ac3":"code","2da309e8":"code","760512e0":"code","cc06a2c8":"code","99fe4ef7":"code","9a26f392":"code","144b8ca8":"code","8627e1a3":"code","354a1a95":"code","f3ea1dec":"code","f46a3ef8":"code","19009baa":"code","8ca4e02a":"code","ec6dcb87":"code","b9146586":"code","d1a590c2":"code","b7e6ec1a":"code","c86b59f1":"code","541bc63e":"code","f8eb3336":"code","009349f8":"code","5fa080f6":"code","801aab68":"code","ae5ea87e":"code","9b2ad93b":"code","904b7b9c":"code","764923a0":"code","3b566fad":"code","657697b8":"code","84cf7520":"code","469897a2":"code","90d96b38":"code","1afc67b9":"code","d5ff5912":"code","83747e42":"code","52f3b417":"code","80c7b78a":"code","40b543f0":"code","e63b7d8c":"code","275c6b4b":"code","2a8b4139":"code","83285891":"code","f4271159":"markdown","31d8a90d":"markdown","bf5ecafb":"markdown","d4e55d14":"markdown","338572f5":"markdown","189c097e":"markdown","a4ce3c51":"markdown","3fde1015":"markdown","7ed1d2c6":"markdown","bf10df2d":"markdown","cd274795":"markdown","eac1af45":"markdown","ec08d692":"markdown","47d519a6":"markdown","5a3a6b9d":"markdown","6540aa4d":"markdown","a77059b1":"markdown","f270e9ce":"markdown","4acd2d0e":"markdown","9bed988d":"markdown","413640ab":"markdown","0709a475":"markdown","b30f192b":"markdown","1220df4e":"markdown","84f04bf0":"markdown","e2d4ce26":"markdown","de7d80e3":"markdown","c7c98a01":"markdown","8ec43205":"markdown","2943f4fa":"markdown","f5175806":"markdown","6f4ebc63":"markdown"},"source":{"839a4a78":"import pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","0ee86f31":"# Copy from previous notebook\ndef create_sub(preds, fn=None):\n    df =  pd.DataFrame({'Id': range(len(preds)), 'Expected': preds})\n    return df","0afeab98":"# The pre-processed data should now be stored here\n!ls \/kaggle\/input\/notebook-1\n\n# This might not work (yet)?","391334a2":"# if the pre-processed data cannot be loaded, we can repeat the preprocessing steps from before:\n\ndf_train = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/kit-mathsee-ml-2021\/pp_test.csv', index_col=0)\n\ndef preproc(df_in, means=None, stds=None, drop_vars=['time', 'station']):\n    df = df_in.copy()\n    if 't2m_obs' in df.columns: df.dropna(subset=['t2m_obs'], inplace=True)\n    df['sm_fc_mean'].replace(np.nan, df['sm_fc_mean'].mean(), inplace=True)\n    \n    y = df.pop('t2m_obs') if 't2m_obs' in df.columns else None\n    X = df.drop(drop_vars, 1)\n    \n    if means is None: means = X.mean()\n    if stds  is None: stds  = X.std()\n    \n    X = (X - means) \/ stds\n    return X, y, means, stds\n\nsplit_date = '2015-01-01'\nX_train, y_train, means, stds = preproc(df_train[df_train.time < split_date])\nX_valid, y_valid, _, _ = preproc(df_train[df_train.time >= split_date], means, stds)\nX_test, _, _, _ = preproc(df_test, means, stds)\nX_train.shape, X_valid.shape, X_test.shape","cf6a6575":"def mse(y_true, y_pred): return ((y_true - y_pred)**2).mean()","4da4a571":"mse(y_valid, df_train.t2m_fc_mean[df_train.time >= '2015-01-01'])","a5f42983":"from sklearn.linear_model import LinearRegression","cc6a9714":"# Create the model object\nlr = LinearRegression()","c4b2ade5":"# Fit the model parameters with the training data\nlr.fit(X_train, y_train)","7f0d8c9c":"X_train.columns","7d22fea8":"lr.coef_","d2254a28":"lr.intercept_","266166e9":"# Make predictions for validation set\npreds = lr.predict(X_valid)","ea941ac3":"mse(preds, y_valid)","2da309e8":"# Submit to Kaggle\ndf_sub = create_sub(lr.predict(X_test), 'lr.csv'); \ndf_sub.head()","760512e0":"lr.score(X_valid, y_valid)","cc06a2c8":"def print_scores(m, X_train=X_train, X_valid=X_valid):\n    print('Train R2 = ', m.score(X_train, y_train), \n          ', Valid R2 = ', m.score(X_valid, y_valid), ', Valid MSE = ', \n          mse(m.predict(X_valid), y_valid))","99fe4ef7":"print_scores(lr)","9a26f392":"sns.pairplot(\n    df_train[::1000], \n    x_vars=['t2m_fc_mean', 'orog', 'gh_pl500_fc_mean', 'cape_fc_mean', 'ssr_fc_mean', 'sm_fc_mean', 'u10_fc_mean'], \n    y_vars=['t2m_obs']\n);","144b8ca8":"from sklearn.tree import DecisionTreeRegressor, plot_tree","8627e1a3":"dt = DecisionTreeRegressor(max_depth=3)","354a1a95":"dt.fit(X_train, y_train)","f3ea1dec":"from sklearn.tree import plot_tree","f46a3ef8":"fig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(dt, filled=True, ax=ax, fontsize=12, feature_names=X_train.columns);","19009baa":"dt = DecisionTreeRegressor()","8ca4e02a":"%time dt.fit(X_train, y_train)","ec6dcb87":"print_scores(dt)","b9146586":"dt.tree_.node_count","d1a590c2":"dt = DecisionTreeRegressor(min_samples_leaf=200)","b7e6ec1a":"dt.fit(X_train, y_train)","c86b59f1":"print_scores(dt)","541bc63e":"from sklearn.ensemble import RandomForestRegressor","f8eb3336":"rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)","009349f8":"%time rf.fit(X_train, y_train)","5fa080f6":"print_scores(rf)","801aab68":"rf = RandomForestRegressor(n_estimators=10, n_jobs=-1, min_samples_leaf=100)","ae5ea87e":"%time rf.fit(X_train, y_train)","9b2ad93b":"print_scores(rf)","904b7b9c":"rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=20)","764923a0":"%time rf.fit(X_train, y_train)","3b566fad":"print_scores(rf)","657697b8":"# Submit to Kaggle\ndf_sub = create_sub(rf.predict(X_test), 'rf.csv'); df_sub.head()","84cf7520":"from sklearn.inspection import permutation_importance","469897a2":"permutation_importance(rf, X_train, y_train, n_repeats=1,n_jobs=-1)","90d96b38":"rf.feature_importances_","1afc67b9":"# Create a new dataframe for easy plotting\nfi_df = pd.DataFrame(\n    data={'Feature': X_train.columns, 'Feature importance': rf.feature_importances_}, \n    columns = ['Feature', 'Feature importance']\n)","d5ff5912":"fi_df.sort_values('Feature importance', inplace=True, ascending=False)","83747e42":"sns.barplot(data=fi_df, x='Feature importance', y='Feature')\nplt.xscale('log')","52f3b417":"X_valid.shape","80c7b78a":"X_pdp = X_valid.copy()","40b543f0":"X_pdp['station_alt'] = -1\nrf.predict(X_pdp).mean()","e63b7d8c":"X_pdp['station_alt'] = 1\nrf.predict(X_pdp).mean()","275c6b4b":"from sklearn.inspection import plot_partial_dependence","2a8b4139":"order = np.argsort(rf.feature_importances_)[::-1]","83285891":"fig, ax = plt.subplots(figsize=(18, 3))\nplot_partial_dependence(rf, X_valid[::1000], order[1:8], feature_names=X_train.columns, grid_resolution=5, n_jobs=-1, n_cols=7, ax=ax)\nplt.tight_layout()","f4271159":"###  Feature importance: Partial dependence\n\nPartial dependence plots tell you: \"All else being equal, how does changing one input variable change the output?\"\n\nTo compute this, one starts with the trained model and the validation dataset. To compute, e.g. the partial dependence with respect to the first feature, one progressively changes the value of this feature and checks how this affects the mean prediction.\n","31d8a90d":"We can see that the training and validation score are roughly similar, indicating that the linear regression doesn't seem to be very prone to overfitting.","bf5ecafb":"Does this fit with our meteorological intuition?\n\nRemember that the model knows nothing about meteorology but only learns correlations in the training dataset.","d4e55d14":"## (Multiple) linear regression\n\n$y = \\sum_i a_i x_i + b$","338572f5":"## Decicion trees\n\nLinear regression is a good start but often the relationships we are trying to model are not linear. One way to still use linear regression is to transform the data (e.g. log, square) but this only works if we know what kind of relationship we have. Real world relationships are often much more difficult and unintuitive.\n\nLet's actually look at some relationships in our data.","189c097e":"#### n_estimators\n\nThe number of trees. ","a4ce3c51":"#### min_samples_leaf\n\nThe minimum number of sample in each leaf.","3fde1015":"### sklearn API\n\nAll sklearn models work in exactly the same way. \n1. Create model\n2. Fit model to training data\n3. Compute predictions\n\n\nIf you have problems choosing the right estimator take a look at this handy map:\n\n\nhttps:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/","7ed1d2c6":"So ideally we want an algorithm that can model any kind of relationship. One such algorithm is a random forest. But before we look at forests, let's look at an individual (decision) tree.","bf10df2d":"Let's submit our new best model to kaggle.","cd274795":"Now we are fitting several decision trees. This can be done in parallel by several processors. `n_jobs = -1` uses all available processors.","eac1af45":"## Feature importance\n\nGetting a good prediction is nice, but ideally we would also understand why we are getting the prediction.\n\nFor this we can look at feature importance. Two common techniques to identify important features are permutation importance and the Gini importance (mean decrease in impurity). \n\nIn permutation importance, you randomly shuffle each column of the dataset and check by how much the prediction skill decreases. This is simply a measure of how important each input feature is for making a good prediction. In other words, without this feature how much worse would the prediction get?\n\nGini importance calculates each features importance as the sum over the number of splits (across all tress) that include the feature, proportionally to the number of samples it splits. \n\nNote: `rf.feature_importances` != permutation importance. If you want to apply this to your research it is worth reading through these references: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py","ec08d692":"Now the parameters $a_i$ and $b$ have been fit to our training data. ","47d519a6":"### The R$^2$ score\n\nThe R$^2$ score is a classical, common skill metric in statistics and machine learning: https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination","5a3a6b9d":"## Your turn\n\nAs before, try to open up a new notebook and reproduce the basic steps, looking at this notebook only when necessary.\n\n1. Import all the data and train a linear regression model\n2. Train a decision tree and visualize it\n3. Train a random forest. Change the hyperparameters and see how this affects your score.\n4. Submit a random forest prediction to Kaggle.\n\nOnce you are done with this, here are some further things to try to improve your score.\n\n1. Try to implement a gradient boosting model for regression (Hint: sklearn.ensemble.GradientBoostingRegressor).\n2. The different stations have very different characteristics. Try building a model (e.g. linear regression) for each station individually. See if you can improve the validation and test score\n3. Try adding some time information (for example the month as a feature. Does this improve the forecast?\n4. Create an ensemble of a linear regression and a random forest. Does this help?\n5. Be creative.","6540aa4d":"- `n_estimators`: Number of trees\n- `n_jobs`: How many processors to use to train trees in parallel. -1 uses all cores.\n\nTraining random forests can take a few minutes. Add verbose=1 to get a status report.\n","a77059b1":"Let's start with a very shallow tree which we can visualize. Again we are using the exact same sklearn API.","f270e9ce":"### Reduce the complexity of the tree\n\n\nSo how can we prevent overfitting? One way would be to reduce the complexity of the tree.\n\nOne way to do this for a decision tree is to increase the minimum number of samples in each leaf.","4acd2d0e":"### Visualize a tree","9bed988d":"So we have a perfect score on the training dataset, but a pretty awful score on the validation set. This is an extreme case of overfitting!\n\nBut this is to be expected. We built our tree to perfectly fit the training set, so that only one sample remains in the final leafs.","413640ab":"### Build a full tree\n\nLet's now build a full tree. We will split the samples until only 1 sample is left in each leaf. This is going to take a little bit.","0709a475":"# Notebook 2 - Linear regression and random forests with scikit-learn\n\nNow that we have prepared our data we can start building statistical models.\n\nWe will start with the simplest model: Linear regression","b30f192b":"### Fit a sample model","1220df4e":"## Random forests\n\nThe idea behind random forests is to have many individual decision trees that are all a little different. How are they different. At each split, only a random subset of features and samples is taken into account for deciding the best split. Because the errors of the individual trees will be independent from each other, averaging the prediction will reduce the error.","84f04bf0":"### Hyperparameters\n","e2d4ce26":"Again, we see that we are overfitting quite badly. As before, we can tune the hyper-parameters to prevent overfitting","de7d80e3":"As expected, by far the most important feature is `t2m_fc_mean`. Does the ordering of the remaining features make meteorological sense?\n\nThere some other really cool ways of interpreting a random forest. For more information check out this notebook: https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/ml1\/lesson2-rf_interpretation.ipynb","c7c98a01":"So we can see that a higher station altitude seems to lead to slightly lower temperatures (note the normalization we performed!).\n\nWe can use sklearns partial dependence funtion to automate this process for a range of values and features.","8ec43205":"Since computing this can take a while we will pick only the most important features but not t2m_fc_mean because we know it's a lot more important and the relationship is linear.","2943f4fa":"So its a little better but actually there is a much better way of doing this. Instead of having just one tree, let's build an entire forest.","f5175806":"- value: Mean target of all samples in that node\n- MSE: MSE if value was used as a prediction for all samples\n\nThe first splits are entirely based on `t2m_fc_mean`. This makes sense since this is by far the most informative variable.\n","6f4ebc63":"### Compute baseline mse\n\nBefore we start let's check which score we have to beat."}}