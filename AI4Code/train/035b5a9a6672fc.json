{"cell_type":{"6cdafcff":"code","f5ce9d21":"code","42b2071e":"code","86f23160":"code","fb6323e7":"code","1ec6e509":"code","142541c9":"code","a1508ced":"code","c83c3054":"code","60e612f0":"code","114784d6":"code","8f210180":"code","4e080a97":"code","7ef2e7bd":"code","c26feca9":"code","24d3abb5":"code","3648903e":"code","9a1a01f6":"code","4f16a104":"code","01e7028e":"code","72606ff8":"code","d4130a0d":"code","70926a85":"code","c8a4278e":"code","99309c4e":"code","941b8819":"code","001f1a26":"code","46033b95":"code","397b25ed":"code","d024b80b":"code","acdc24b6":"code","cc26625b":"code","d185f1ea":"code","67684cc5":"code","f3be5153":"code","892bb5e0":"code","2a1cdd39":"code","be29255d":"code","1e9a3ae4":"code","3c8ffe05":"code","271e65b1":"code","5598f08a":"code","fd64ce44":"code","74838782":"code","d705aabb":"code","ae3cc468":"code","ad489967":"code","d28ad1b9":"code","99f9fe03":"code","39c5e59b":"code","4d5782df":"code","d69a4fc4":"code","6ddd1a91":"code","863fced7":"code","fe7f32d7":"code","582d9afa":"code","1793da1c":"code","d4f8d5d4":"code","3e6d7a38":"code","2dc37b6b":"code","52b248cb":"code","28235b16":"code","620f08e6":"code","883d310a":"code","049ee7fb":"code","3f8741f7":"code","243f9797":"code","01dd00e0":"code","9700ad75":"code","3acbe948":"code","257cacd9":"code","6ef896ed":"code","d36ca030":"code","24a9a459":"code","b5f0d642":"code","66ef2006":"code","452d949b":"code","5562e37e":"code","9af2bf94":"code","c27f631d":"code","4cf5cba9":"code","13dc7420":"code","455a9fee":"code","e011e425":"code","2f639600":"code","b6835f1f":"code","d27cffab":"code","9248182a":"code","b7ef5ba6":"code","fba7c4de":"code","bfbca27c":"code","4fa227d8":"code","141a6537":"code","13beadd4":"code","2c6a13d2":"code","d3f97009":"code","e2b381c7":"code","4446af8e":"code","c7551b96":"code","775d1bf3":"code","8241f989":"code","9720a0bf":"code","3d673cb4":"code","d17303f2":"code","7d31a8e6":"code","185d9519":"code","1bb1727a":"code","e93b1287":"code","e97d5f2a":"code","5b88d456":"code","6cc85642":"code","0598d2d7":"code","9ef9bd67":"code","9e99bc71":"code","b9e1ae9f":"markdown","801530c2":"markdown","5f76dc4e":"markdown","c24ddd39":"markdown","3326a90b":"markdown","02f6aa85":"markdown","be6b6eec":"markdown","c1a0b000":"markdown","060b127d":"markdown","fc00dfee":"markdown","42c9f3ca":"markdown","17b5839b":"markdown","a8309b54":"markdown","15b30d99":"markdown","43e93889":"markdown","5a75561c":"markdown","0c4b22ad":"markdown","12eb4c68":"markdown","3b68ec47":"markdown","06e8f54b":"markdown","6b453ef5":"markdown","a94178f2":"markdown","72a4bbff":"markdown","40f6e512":"markdown","4f77bf86":"markdown","f5ae9921":"markdown","cc6488b5":"markdown"},"source":{"6cdafcff":"! pip install calmap","f5ce9d21":"import calmap","42b2071e":"! pip install squarify","86f23160":"import squarify","fb6323e7":"import os\n    \n# Main libraries that we will use in this kernel\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n# # garbage collector: free some memory is needed\nimport gc\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# statistical package and some useful functions to analyze our timeseries\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.stattools as stattools\n\nimport time\n\nfrom string import punctuation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1ec6e509":"punctuation","142541c9":"PATH_DATA = \"..\/input\/nuclio07-dsc-0421\"","a1508ced":"def print_files():\n    for dirname, _, filenames in os.walk(PATH_DATA):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","c83c3054":"# Let's see how many different files we are dealing with\nprint_files()","60e612f0":"# import the df\nshops = pd.read_csv(os.path.join(PATH_DATA, \"shops.csv\"))\nshops.shape","114784d6":"shops.sample(10)","8f210180":"# We don't have any duplicates in the shop_name field\nshops.shape[0] == len(shops[\"shop_name\"].unique())","4e080a97":"# However inspecting the df by name, we can see that shop_id 10 and 11 are very similar. Later we will try and group them once we inspect the sales per shop\nshops[shops[\"shop_id\"].isin([10, 11])]","7ef2e7bd":"# The same happens with the shops with shop_id 23 and 24\nshops[shops[\"shop_id\"].isin([23, 24])]","c26feca9":"# No missing values in the shops df\nshops.isnull().sum().sum()","24d3abb5":"city_dict = {\n    2 : 'Adygea',\n    3 : 'Balashikha',\n    4 : 'Volzhski',\n    5 : 'Vlogda',\n    6 : 'Voronezh',\n    7 : 'Voronezh',\n    8 : 'Voronezh',\n    9 : 'Comercio Ambulante',\n    11 : 'Zhukovsky',\n    12 : 'Tienda Online',\n    13 : 'Kazan',\n    14 : 'Kazan',\n    15 : 'Kaluga',\n    16 : 'Kolomna',\n    17 : 'Krasnoyarsk',\n    18 : 'Krasnoyarsk',\n    19 : 'Kursk',\n    20 : 'Mosc\u00fa',\n    21 : 'Mosc\u00fa',\n    22 : 'Mosc\u00fa',\n    24 : 'Mosc\u00fa',\n    25 : 'Mosc\u00fa',\n    26 : 'Mosc\u00fa',\n    27 : 'Mosc\u00fa',\n    28 : 'Mosc\u00fa',\n    29 : 'Mosc\u00fa',\n    30 : 'Mosc\u00fa',\n    31 : 'Mosc\u00fa',\n    32 : 'Mosc\u00fa',\n    33 : 'Mytishchi',\n    34 : 'Nizhny Novgorod',\n    35 : 'Nizhny Novgorod',\n    36 : 'Novosibirsk',\n    37 : 'Novosibirsk',\n    38 : 'Omsk',\n    39 : 'Rostov Na Donu',\n    40 : 'Rostov Na Donu',\n    41 : 'Rostov Na Donu',\n    42 : 'San Petersburgo',\n    43 : 'San Petersburgo',\n    44 : 'Samara',\n    45 : 'Samara',\n    46 : 'Posad',\n    47 : 'Surgut',\n    48 : 'Tomsk',\n    49 : 'Tyumen',\n    50 : 'Tyumen',\n    51 : 'Tyumen',\n    52 : 'Ufa',\n    53 : 'Ufa',\n    54 : 'Khimki',\n    55 : 'Tienda Online',\n    56 : 'Checos',\n    57 : 'Yakutsk',\n    58 : 'Yakutsk',\n    59 : 'Yaroslavl'\n}","3648903e":"def fix_shops(shops, city_dict):\n    '''\n    This function modifies the shops df inplace.\n    It correct's 3 shops that we have found to be 'duplicates'\n    and also creates a few more features: extracts the city and encodes it using LabelEncoder\n    '''\n    shops = shops[~shops[\"shop_id\"].isin([0, 1, 10, 23])]\n\n    # replace all the punctuation in the shop_name columns\n    shops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n    \n    # extract the city name\n    shops[\"city\"] = shops[\"shop_id\"].map(city_dict)\n    \n    # encode it using a simple LabelEncoder\n    shops[\"city_code\"] = LabelEncoder().fit_transform(shops['city'])\n    \n    return shops","9a1a01f6":"shops[\"city\"] = shops[\"shop_id\"].map(city_dict)","4f16a104":"shops.dropna(inplace = True, axis = 0)","01e7028e":"lista_1 = [\"a\", \"b\", \"c\"]\nlista_2 = [1, 2, 3, 4]\n\nfor i in range(len(lista_1)):\n    val1 = lista_1[i]\n    val2 = lista_2[i]\n    \n    # print(val1, val2)\n    \nfor val1, val2 in zip(lista_1, lista_2):\n    print(val1, val2)","72606ff8":"numbers = range(31)\n\nlista_pares = []\n\nfor num in numbers:\n    if num%2 == 0:\n        lista_pares.append(num)\nlista_pares","d4130a0d":"lista_pares = [num for num in numbers if num%2 == 0]","70926a85":"lista_pares","c8a4278e":"unique_cities = shops[\"city\"].unique()\ncities_id = range(len(unique_cities))\n\ncity_id_dict = {city_name:city_id for city_name, city_id in zip(unique_cities, cities_id)}\n\nshops[\"city_id\"] = shops[\"city\"].map(city_id_dict)\nshops.head()","99309c4e":"shops[\"city_id_2\"] = LabelEncoder().fit_transform(shops[\"city\"])","941b8819":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer","001f1a26":"# SimpleImputer??","46033b95":"SimpleImputer()","397b25ed":"OneHotEncoder(sparse = False).fit_transform(shops[[\"city\"]])","d024b80b":"shops.head()","acdc24b6":"shops.head()","cc26625b":"shops.drop([\"city\", \"city_id\", \"city_id_2\"], inplace = True, axis = 1)","d185f1ea":"# apply our function to the shops_df\nshops = fix_shops(shops, city_dict)","67684cc5":"shops.head()","f3be5153":"# import df\nitems_category = pd.read_csv(os.path.join(PATH_DATA, \"item_categories.csv\"))\nitems_category.shape","892bb5e0":"items_category.head()","2a1cdd39":"# We don't have any duplicates in the item_category_name field\nitems_category.shape[0] == len(items_category[\"item_category_name\"].unique())","be29255d":"items_category.shape[0]","1e9a3ae4":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items_category.shape[0]","3c8ffe05":"# If we take a closer look, we can see that we have a lot of Play Station categories: like accesories, games and so on. We have the same categories for XBOX and also for PC Games.\n# A lot of categories have to deal with books, presents and computer software and music (CD).\n# We will generate later some features by parsing the names and making groupedby features.\nitems_category","271e65b1":"# If we apply a simple lambda function and extract the everything that contains PS, we will get 16 different categories for PlayStation\nitems_category[\"PS_flag\"] = items_category[\"item_category_name\"].apply(lambda x: True if \"PS\" in x else False)\nitems_category[items_category[\"PS_flag\"] == True]","5598f08a":"# No missing values in the items_category df\nitems_category.isnull().sum().sum()","fd64ce44":"# import df\nitems = pd.read_csv(os.path.join(PATH_DATA, \"items.csv\"))\nitems.shape","74838782":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items.shape[0]\nitems.head()","d705aabb":"# No missing values in the items category\nitems.isnull().sum().sum()","ae3cc468":"# Let's see the top 10 and bottom 10 item categories\nitems_gb = items.groupby(\"item_category_id\").size().sort_values(ascending = False).to_frame()\nitems_gb.head()","ad489967":"items_gb.rename(columns = {0:\"counts\"}, inplace = True)","d28ad1b9":"top_10 = items_gb[:10]","99f9fe03":"bottom_10 = items_gb[-10:]","39c5e59b":"top_10 = top_10.append(bottom_10)\ntop_10 = top_10.sort_values(\"counts\", ascending = False)","4d5782df":"top_10.reset_index()","d69a4fc4":"# We can notice that in the top 10 most popular items products we have PS3\n# At the same time, in the bottom 10 products, we can find 2 PS2.\n# This means, that we have to be careful while generating features like PS\npd.merge(top_10, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")","6ddd1a91":"# import df\nsales = pd.read_csv(os.path.join(PATH_DATA, \"sales_train.csv\"))\nsales.shape","863fced7":"sales.sample(10)","fe7f32d7":"sales.info()","582d9afa":"# No null values in the sales df\n\n# Is this True?\n\nsales.isnull().sum()","1793da1c":"sorted(list(sales[\"item_cnt_day\"].unique()))[:20]","d4f8d5d4":"del shops, items_category, items, sales\ngc.collect()","3e6d7a38":"# a simple function that creates a global df with all joins and also shops corrections\ndef create_df(path, city_dict):\n    '''\n    This is a helper function that creates the train df.\n    '''\n    # import all df\n    shops = pd.read_csv(os.path.join(path, \"shops.csv\"))                    \n    items_category = pd.read_csv(os.path.join(path, \"item_categories.csv\"))\n    items = pd.read_csv(os.path.join(path, \"items.csv\"))\n    sales = pd.read_csv(os.path.join(path, \"sales_train.csv\"))\n    \n    # fix shop_id in sales so that we can leater merge the df\n    \n    d_shops = {0:57, 1:58, 10:11, 23:24}\n    sales[\"shop_id\"] = sales[\"shop_id\"].map(lambda shop_id: d_shops[shop_id] if shop_id in d_shops.keys() else shop_id)\n    sales_shape_start = sales.shape[0]\n    shops = fix_shops(shops, city_dict) # fix the shops as we have seen before\n    \n    # create df by merging the previous dataframes\n    df = pd.merge(items, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")\n    df = pd.merge(sales, df, left_on = \"item_id\", right_on = \"item_id\")\n    df = pd.merge(df, shops, left_on = \"shop_id\", right_on = \"shop_id\")\n    \n    # sort the values\n    df.sort_values(by = [\"shop_id\", \"date\"], ascending = True, inplace = True)\n    df_shape_end = df.shape[0]\n    \n    assert sales_shape_start == df_shape_end, \"You have created a cartessian!\"\n    \n    return df","2dc37b6b":"df = create_df(path = PATH_DATA, city_dict = city_dict)","52b248cb":"# It seems that there are no null values, however this is not fully true. \n# As we will see in the next section, when we groupby and plot the data, there are a lot of months where there have been no sales so basically it's a null value, and we have to impute zero sales for that month.\ndf.isnull().sum().sum()","28235b16":"df.head()","620f08e6":"df.info()","883d310a":"# Let's group by Month and see all the sales\n\n# resample in timeseries is the same as groupby\n# in order it to work, we must set the date column as index, and it must be a datetime format (strings are not valid)\n# when we resample it, we can pass D: daily, W: weekly or M: monthly\n# we can then perform operation on the 'resampled' columns like\n# sum, mean and others.\n\n# calculate the monthly sales\ndf[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")","049ee7fb":"df[\"Year\"] = df[\"date\"].dt.year","3f8741f7":"df[\"Month\"] = df[\"date\"].dt.month","243f9797":"df.groupby([\"Year\", \"Month\"])","01dd00e0":"df.head().T","9700ad75":"df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"M\").sum()","3acbe948":"# resample the data on a monthly basis\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"M\").sum()\n\n# plot the data using matplotlib\nplt.figure(figsize = (10, 6))\nplt.plot(x, color = \"blue\", label = \"Monthly sales\")\nplt.title(\"Monthly sales\")\nplt.legend();","257cacd9":"# perform the same operations but on a weekly basis\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"W\").sum()\n\nplt.figure(figsize = (10, 6))\nplt.plot(x.index, x, color = \"blue\", label = \"Weekly sales\")\nplt.title(\"Weekly sales\")\nplt.legend();","6ef896ed":"russian_holidays_start = [\ndatetime.datetime(2013, 1, 1),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 1),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 1),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","d36ca030":"russian_holidays_end = [\ndatetime.datetime(2013, 1, 8),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 8),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 8),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","24a9a459":"resampled_df = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"D\").sum()\nresampled_df[\"MA_7\"] = resampled_df[\"item_cnt_day\"].rolling(window = 7).mean()\nresampled_df.head(10)","b5f0d642":"def create_ma_df(df, column_to_filter, iterable, period):\n\n    short_df = df[df[column_to_filter] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(period)[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    short_df[f\"MA3{period}\"] = short_df[\"item_cnt_day\"].rolling(window = 3).mean()\n    short_df[f\"MA4{period}\"] = short_df[\"item_cnt_day\"].rolling(window = 4).mean()\n    short_df[f\"MA5{period}\"] = short_df[\"item_cnt_day\"].rolling(window = 5).mean()\n    \n    return short_df","66ef2006":"# ITERABLE_COLUMN = \"shop_name\"\n# ITERABLE_COLUMN = \"item_category_name\"\nITERABLE_COLUMN = \"city\"\n\nfor iterable in sorted(list(df[ITERABLE_COLUMN].unique()))[:5]:\n\n    #######################################################################################\n    # Monthly sales\n    #######################################################################################\n    \n    # create the size of the figure\n    plt.figure(figsize = (30, 10))\n    plt.subplot(1, 2, 1)\n    \n    # create a df with ma sales\n    short_df = create_ma_df(df, ITERABLE_COLUMN, iterable, \"M\")\n    \n    # preparing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    average_3_months = short_df[\"MA3M\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Monthly sales\")\n    plt.plot(dates, average_3_months, '.-', label = \"Average sales of the last 3 months\")\n\n    # get current axis and plot the areas\n    ax = plt.gca()\n    alpha = 0.2\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')    \n       \n    # add title and show legend    \n    plt.title('Monthly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Monthly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by month\")\n    plt.legend()\n    \n    #######################################################################################\n    # Weekly sales\n    #######################################################################################\n    \n    plt.subplot(1, 2, 2)\n    \n    # create a df with ma sales\n    short_df = create_ma_df(df, ITERABLE_COLUMN, iterable, \"W\")\n    \n    # preparing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    average_3_weeks = short_df[\"MA3W\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Weekly sales\")\n    plt.plot(dates, average_3_weeks, '.-', label = \"Average sales of the last 3 weeks\")\n    \n    # get current axis and plot the areas\n    ax = plt.gca()\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')\n    \n    # add title and show legend\n    plt.title('Weekly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Weekly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by week\")\n    plt.legend()\n    \n    # general sales\n    plt.show()","452d949b":"resampled_df[\"diff\"] = resampled_df[\"item_cnt_day\"].diff()\/resampled_df[\"item_cnt_day\"].shift(1)\nresampled_df[\"lag1\"] = resampled_df[\"item_cnt_day\"].shift(3)\nresampled_df.head()","5562e37e":"# we can observe a general trend of decrasing sales.\n# let's add a second axis to see the variation of intradays sales\n\n# select the columns of interest\ndf_var = df[[\"date\", \"item_cnt_day\"]]\n\n# convert to datetime\ndf_var[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n\n# set date as index\ndf_var.set_index(\"date\", inplace = True)\n\n# resample\/groupby by date and convert to frame the total daily sales\ndf_var = df_var.resample(\"M\")[\"item_cnt_day\"].sum().to_frame()\n\n# calculate the intra week variation between total sales\ndf_var[\"Variation\"] = df_var[\"item_cnt_day\"].diff()\/df_var[\"item_cnt_day\"].shift(1)\n\ndf_var.head()","9af2bf94":"# separate x and y\ny_sales = df_var[\"item_cnt_day\"]\ny_variation = df_var[\"Variation\"]\n\n# instanciate the figure\nfig = plt.figure(figsize = (15, 10))\nax = fig.add_subplot(111)\n\n# plot the total sales\nplot1 = ax.plot(y_sales, label = \"Total monthly sales\", color = \"blue\", alpha = 0.5)\n\n# create a secondary axis and plot the variation data\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(y_variation, label = \"Intra - month variation of sales\", color = \"red\", alpha = 0.5)\n\n# create a common legend for both plots\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# add a custom title to the plot\nax.set_title(\"Total monthly sales and variation\");","c27f631d":"# start with the regular df\ndf_for_question_1 = create_df(PATH_DATA, city_dict)","4cf5cba9":"# calendar heatmaps are really useful to see the overall activity for a certain period of time per day and per month.\n# let's build one using python.\n# we will be using the calmap package for this, because it makes it extremenly easy to plot this data\n# select the columns\ndf_calendar = df[[\"date\", \"item_cnt_day\"]]\n\n# set date as index and resample\ndf_calendar.set_index(\"date\", inplace = True)\n# notice that this time, we don't convert it to_frame()\n# df_calendar is a pandas series\n# THIS IS IMPORTANT since calmap expects a series\n# with a datetime index and the values to plot\ndf_calendar = df_calendar.resample(\"D\")[\"item_cnt_day\"].sum()\ndf_calendar.head()","13dc7420":"# ----------------------------------------------------------------------------------------------------\n# plot the data using calmap\ncalmap.calendarplot(df_calendar, # pass the series\n                    fig_kws = {'figsize': (16,10)}, \n                    yearlabel_kws = {'color':'black', 'fontsize':14}, \n                    subplot_kws = {'title':'Total sales per year'});","455a9fee":"[2, 4, 6, 8, 10]\n[4, 6, 8, 10]\n[6, 8, 10]\n\n# [2, 4, 6, 8, 10]\n\n# [1, 2, 3]","e011e425":"df.head()","2f639600":"df[(df[\"shop_id\"] == 6) & (df[\"item_id\"] == 3141)].shape","b6835f1f":"gb_df = df.groupby([\"shop_id\", \"item_id\"])\ngb_df.get_group((6, 3141)).shape","d27cffab":"# This plot are fundamental in timeseries analysis.\n# Basically here we compare the a series again itself but with some lags.\n# These are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n# More info: \n# https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using the built in plots from the stats module\n\n# The AutoCorrelation plot: compares a value v with the value v but n times in the past.\nplot_acf(df.set_index(\"date\").resample(\"M\")[\"item_cnt_day\"].sum(), ax = ax1, lags = 14)\n\n# The Parcial AutoCorrelation plot: partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nplot_pacf(df.set_index(\"date\").resample(\"M\")[\"item_cnt_day\"].sum(), ax = ax2, lags = 14);","9248182a":"# This code snippets show you have to calculate the Partial Autocorrelation\n# Partial Autocorrelation can be very counter intuitive since in some of our steps we are fitting a linear model\n# to predict the values of t - 2 using t - 1\n# Wait, what? Why we use values from yesterday to predict values before yesterday?\n# Basically because we assume that our timeseries is auto regressive. This means that the data at point t captures\n# all the variance\/information from all the previuos data points.\n# This way, t - 1, must have captured all the variance from previous points, thus t - 2, and so t - 1 becomes\n# a good predictor for values from t - 2.","b7ef5ba6":"# create a dataframe with total sales per day (all shops and all items)\ndf_total_sales = df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\n\n# rename the column item_cnt_day to total_sales\ndf_total_sales.columns = [\"total_sales\"]\n","fba7c4de":"df_total_sales.head()","bfbca27c":"\n# create a few features that we need in order to calculate the parcial autocorrelation\ndf_total_sales[\"T-1\"] = df_total_sales[\"total_sales\"].shift(1)\ndf_total_sales[\"T-2\"] = df_total_sales[\"total_sales\"].shift(2)\n\n# we have a few nan for the first 2 rows so we must drop them\nprint(df_total_sales.shape)\ndf_total_sales.dropna(axis = \"rows\", inplace = True)\nprint(df_total_sales.shape)","4fa227d8":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"total_sales\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"total_sales_from_T-1\"] = predictions","141a6537":"df_total_sales.head()","13beadd4":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"T-2\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"T-2_from_T-1\"] = predictions","2c6a13d2":"df_total_sales.head()","d3f97009":"# calculate the residual\n# this means: total_sales - total_sales_from_T-1\n# and: T-2 - \"T-2_from_T-1\"\ndf_total_sales[\"Residual_total_sales_T-1\"] = df_total_sales[\"total_sales\"] - df_total_sales[\"total_sales_from_T-1\"]\n\n# this step is very important based on the asumptions we have about many of the timeseries\n# for more information I recommend this read\n# https:\/\/towardsdatascience.com\/understanding-partial-auto-correlation-fa39271146ac\ndf_total_sales[\"Residual_T-2_T-1\"] = df_total_sales[\"T-2\"] - df_total_sales[\"T-2_from_T-1\"]","e2b381c7":"df_total_sales.head()","4446af8e":"# calculathe the parcial autocorrelation using manual method\nmanual_pacf = df_total_sales.corr(method = \"pearson\")[\"Residual_total_sales_T-1\"][\"Residual_T-2_T-1\"]\nprint(\"Manual parcial autocorrelation method {}\".format(round(manual_pacf, 5)))\n\n# calculate the parcial autocorrelation using statsmodel package\nstats_pacf = pacf(df_total_sales['total_sales'], nlags = 2)[2]\nprint(\"Parcial autocorrelation method using stats package {}\".format(round(stats_pacf, 5)))","c7551b96":"df_timeindex = df.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum().to_frame()\n\n# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex[\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https:\/\/stackoverflow.com\/questions\/45184055\/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex.index.tolist()\n\n# set x_ticks\nax.set_xticks(xtick_location);","775d1bf3":"# start with the regular df\ndf_for_question_2 = create_df(PATH_DATA, city_dict)","8241f989":"# prepare the data\n\n# extract each year using dt.year\ndf[\"YEAR\"] = df[\"date\"].dt.year\n\nshort_df = df[df[\"YEAR\"] == 2014][[\"item_cnt_day\", \"city\"]]\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\nmy_values = short_df[\"item_cnt_day\"]\nmy_pct = short_df[\"item_cnt_day\"]\/short_df[\"item_cnt_day\"].sum()\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(city, sales\/1000, round(pct, 2)*100) for city, sales, pct in zip(short_df.index, my_values, my_pct)]\n\nplt.figure(figsize = (30, 8))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by city and their % over total sales in 2014\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()","9720a0bf":"labels","3d673cb4":"df[[\"city\", \"city_code\"]].drop_duplicates()","d17303f2":"# treemaps are very useful to see the difference and the weights of categories\n# but they don't give us that much of information about the distribution of each category\n# let's use boxplot to see the distribution of a city\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"city\",\n            y = \"item_cnt_day\", \n            data = df[(df[\"YEAR\"] == 2013) & (df[\"city_code\"] == 9)]);","7d31a8e6":"# start with the regular df\ndf_for_question_3 = create_df(PATH_DATA, city_dict)","185d9519":"gb_df_ = df[[\"date\", \"item_id\", \"item_cnt_day\"]].pivot_table(\n    index = \"date\",\n    columns = \"item_id\",\n    values = \"item_cnt_day\",\n    aggfunc = np.sum\n).isnull() * 1\n\norder_of_columns = list(gb_df_.sum().sort_values().index)\ngb_df_[order_of_columns]","1bb1727a":"# This plot will help us visualize the missing values for each datetime and item_id\n# This is the most granular plots possible, since we will be seeing individual sales by day and item_id\n# This plot can be very consufing, but the main point is to show all the \"missing values\" we have\n# We have seen previously in our EDA, athat when we groupby and resamples our sales, we might think\n# that we don't have any missing values. But its not true, we only have the reported sales\n# This means that, if we have a shop or item_id that only had 3 sales per year, when we resample\n# our df by day, pandas will generate additional days with null sales.\n# those null sales is what we want to plot here\n# the values are ordered from less nulls to more nulls\n\ngb_df_ = df.pivot_table(index = [\"date\"], columns = ['item_id'], values = \"item_cnt_day\", aggfunc = sum).isnull()\norder_of_columns = list(gb_df_.sum().sort_values().index)\ngb_df_ = gb_df_[order_of_columns]\n\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(gb_df_, cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by item_id and day\");","e93b1287":"# create a smaller df\nshort_df = df[[\"date\", \"item_cnt_day\", \"item_category_name\"]]\n\n# set the date to be the index (to resample later)\nshort_df.set_index(\"date\", inplace = True)\n\n# groupby by shop_name\ngb = short_df.groupby(\"item_category_name\")\n\n# resample the df by month sales (resample = groupby by months in timeseries)\ngbr = gb.resample(\"M\")[\"item_cnt_day\"].sum()\n\n# unstack the gbr to have columns name\ngbr = gbr.unstack(level = -1).T\n\n# sort the values, from no nulls to more null values\norder_of_columns = list(gbr.isnull().sum().sort_values().index)\n\n# change the order of the df\ngbr = gbr[order_of_columns]","e97d5f2a":"# let's plot the null values for each shop\nplt.figure(figsize=(20, 10))\n\n# this lines gbr.unstack(level = -1).T.isnull()*1\n# converts any null to 1 and the rest will be 0\nsns.heatmap(gbr.isnull()*1, cmap = \"inferno\", cbar = True).set_title(\"Null values by item category and Month\");","5b88d456":"# let's look at outliers for item sales\n# We will use boxplots because they are very useful to see the distribution of values\nplt.figure(figsize = (10,4))\nsns.boxplot(x = df[\"item_cnt_day\"]);","6cc85642":"# let's look at outliers for item price\nplt.figure(figsize = (10,4))\nplt.xlim(df[\"item_price\"].min(), df[\"item_price\"].max()*1.1)\nsns.boxplot(x = df[\"item_price\"]);","0598d2d7":"# joint plot is another very convenient way to plot the relationship between 2 variables\n# but because we have huge outliers, we don't see them \n# https:\/\/seaborn.pydata.org\/generated\/seaborn.jointplot.html\nplt.figure(figsize = (10,4))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","9ef9bd67":"# let's filter the outliers and make the same joint plot\ndf = df[(df[\"item_price\"] < np.percentile(df[\"item_price\"], q = 99)) & (df[\"item_cnt_day\"] >= 0) & (df[\"item_cnt_day\"] < np.percentile(df[\"item_cnt_day\"], q = 99))]","9e99bc71":"# we have removed the outliers and now \nplt.figure(figsize = (10, 10))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","b9e1ae9f":"<a id = \"decomp_weekly\"><\/a>\n# Timeseries decomposition plots: weekly sales\n[Go back to the Table of Contents](#table_of_contents)","801530c2":"<a id =\"table_of_contents\"><\/a>\n# Table of contents\n\n\n[Imports](#imports)\n\n[Quick look at shops df](#quick_look_shops)\n\n[Fix shops df and generate some features](#fix_shops)\n\n[Quick look at item category df](#quick_look_item_cat)\n\n[Quick look at items df](#quick_look_item)\n\n[Quick look at sales df](#quick_look_sales)\n\n[Joining df](#join_df)\n\n[Exploratory Data Analysis (EDA)](#eda)\n\n[Viz of sales per week, month of shops and item_category columns](#sales_viz)\n\n[Total sales and the variation on secondary axis](#sales_viz_2_axis)\n\n--> [Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.](#question_1)\n\n[Calendar heatmap](#calendar_heatmap)\n\n[Timeseries autocorrelation and partial autocorrelation plots: daily sales](#corr_plots_daily)\n\n[Manually calculate the Partial Autocorrelation](#autocorrelation_calculation)\n\n[Timeseries decomposition plots: weekly sales](#decomp_weekly)\n\n--> [Question 2: Create a decomposition plot for a city of weekly sales](#question_2)\n\n[Visualizing the most important cities](#viz_cities)\n\n--> [Question 3: Create a treemap plot for item_category and the total combined sales](#question_3)\n\n[Visualizing nulls values](#viz_null_values)\n\n[Visualization of outliers](#viz_outliers)\n\n[Conclusion](#conclusion)","5f76dc4e":"<a id =\"question_1\"><\/a>\n# Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.\n[Go back to the Table of Contents](#table_of_contents)","c24ddd39":"<a id = \"viz_null_values\"><\/a>\n# Visualizing nulls values\n[Go back to the Table of Contents](#table_of_contents)","3326a90b":"<a id = \"eda\"><\/a>\n# Exploratory Data Analysis (EDA)\n[Go back to the Table of Contents](#table_of_contents)","02f6aa85":"# Welcome to this kernel\n\nThe goal of this kernel is very simple. It aims to provide some useful insights about the data and hopefully can guide you into what features to generate and how to tackle the modelling part.","be6b6eec":"<a id = \"calendar_heatmap\"><\/a>\n# Calendar heatmap\n[Go back to the Table of Contents](#table_of_contents)","c1a0b000":"<a id = \"quick_look_sales\"><\/a>\n# Quick look at sales df\n[Go back to the Table of Contents](#table_of_contents)","060b127d":"<a id = \"join_df\"><\/a>\n# Joining df\n[Go back to the Table of Contents](#table_of_contents)","fc00dfee":"<a id = \"question_3\"><\/a>\n# Question 3: Create a treemap plot for item_category and the total combined sales\n\n<span style=\"color:red\">If the % of a category over total is less 1%, don't put any label!!!<\/span>\n\n[Go back to the Table of Contents](#table_of_contents)","42c9f3ca":"<a id = \"autocorrelation_calculation\"><\/a>\n# Manually calculate the Partial Autocorrelation\n[Go back to the Table of Contents](#table_of_contents)","17b5839b":"<a id = \"viz_outliers\"><\/a>\n# Visualization of outliers\n[Go back to the Table of Contents](#table_of_contents)","a8309b54":"<a id = \"fix_shops\"><\/a>\n# Fix shops df and generate some features\n[Go back to the Table of Contents](#table_of_contents)","15b30d99":"<a id = \"quick_look_shops\"><\/a>\n# Quick look at shops df\n[Go back to the Table of Contents](#table_of_contents)","43e93889":"Analyzing data on a weekly basis, gives us much more information. We can see more variation between weeks, but the main point stays the same: we have spines in January and sales that go down overtime.","5a75561c":"<a id = \"quick_look_item\"><\/a>\n# Quick look at items df\n[Go back to the Table of Contents](#table_of_contents)","0c4b22ad":"Treemaps are a very useful and visual tools to see different categories and their overall importance in a dataset.\nAlso, they are very cool and easy to make using Python and squarify.","12eb4c68":"<a id = \"sales_viz_2_axis\"><\/a>\n# Total sales and the variation on secondary axis\n[Go back to the Table of Contents](#table_of_contents)","3b68ec47":"<a id = \"corr_plots_daily\"><\/a>\n# Timeseries autocorrelation and partial autocorrelation plots: daily sales\n[Go back to the Table of Contents](#table_of_contents)","06e8f54b":"<a id = \"sales_viz\"><\/a>\n# Viz of sales per week, month of shops and item_category columns\n[Go back to the Table of Contents](#table_of_contents)","6b453ef5":"<a id = \"conclusion\"><\/a>\n# Conclusion\n[Go back to the Table of Contents](#table_of_contents)\n\nAfter taking a look at the sales data, here are some conclusion we can extract:\n\n1. We see that the total sales increase over time. This is very important because, we have to create features for our model that catch this trend.\n\n2. We have seen that the sales present huge spikes in Christmas season. Datetime features can help a lot our model.\n\n3. Data has a lot of missing values and we have not found a specific or category affected by this. More likely it's just the nature of the data.\n\n4. Top 3 cities capture more than 50% of total sales. City based features can be very helpful for the model.\n\n5. The top 3 categories represent more than 40% of total sales: they are Movies, PC Games and Music.\n\n6. Data presents outliers at the sales and price level. Before generating features or training a model, data must be cleaned properly.\n\n7. We have seen thanks to our calendar plots that we a small increase in sales on the weekends. We do see however bigger sales on 14 of February or 9 of May (holidays).","a94178f2":"<a id = \"question_2\"><\/a>\n# Question 2: Create a decomposition plot for a city of weekly sales\n[Go back to the Table of Contents](#table_of_contents)","72a4bbff":"<a id = \"imports\"><\/a>\n# Imports\n[Go back to the Table of Contents](#table_of_contents)","40f6e512":"From our very first and simple figure, we can already extract very useful information.\n* First of all, we can see big spikes in January, like to be motivated with national holidays in Russia.\n* Second: we see a general trend to decline in our timeseries. If you are planning to use a parametrical model, you must take into account this.","4f77bf86":"In the next plots we will represent the monthly sales (left plot) and weekly sales (right plot) for each shop. \n\nIn the light red\/pink areas of each plot, we will mark the national holidays in Russia and see if there is any connection with sales spikes.","f5ae9921":"<a id = \"viz_cities\"><\/a>\n# Visualizing the most important cities\n[Go back to the Table of Contents](#table_of_contents)","cc6488b5":"<a id = \"quick_look_item_cat\"><\/a>\n# Quick look at items_category df\n[Go back to the Table of Contents](#table_of_contents)"}}