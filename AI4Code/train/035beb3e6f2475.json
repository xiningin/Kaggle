{"cell_type":{"67324840":"code","c182e844":"code","6df0286a":"code","a60400a3":"code","1e68ec1b":"code","b254d3db":"code","b2d755b5":"code","f144facd":"code","33bb4dec":"code","94edb552":"code","906c2f16":"code","e0ab8efd":"code","f6cbdfe6":"code","2296b0f3":"code","6fc29113":"code","b9b65981":"code","15f6efd8":"code","b0c96681":"code","1a164d44":"code","617f40bf":"code","7934767f":"code","14f99221":"code","5a776a16":"code","94c55a96":"code","fbc1237f":"code","512cf071":"code","327827c3":"code","47d66ade":"code","226c6619":"code","4738abc1":"code","6410a3ba":"code","3b737434":"code","d3faaaf5":"markdown","840f95ae":"markdown","91120330":"markdown","5a761ca2":"markdown","fc92d410":"markdown","909ca00b":"markdown","77116d64":"markdown","a215bed8":"markdown","288754ab":"markdown","ffcee754":"markdown","58eaa7c0":"markdown","e7385516":"markdown","5edb7ab1":"markdown","d7e76c22":"markdown","07dcc6c3":"markdown","4b811faa":"markdown","9541468c":"markdown","11c55a35":"markdown","35101645":"markdown","83562298":"markdown","a24f2bcc":"markdown","fc534f28":"markdown","52831d5f":"markdown","726e83c7":"markdown","5126736b":"markdown","1d0f3e37":"markdown","7001e066":"markdown","2a1ea018":"markdown","71a1bf29":"markdown","c6ba1230":"markdown","eef83f78":"markdown","37e47cd9":"markdown","9c397875":"markdown","d3477c42":"markdown"},"source":{"67324840":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM,GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D,Flatten,Bidirectional,SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords","c182e844":"PATH = '..\/input\/spooky-author-identification'\ntrain = pd.read_csv(f'{PATH}\/train.zip')\ntest = pd.read_csv(f'{PATH}\/test.zip')\nsample = pd.read_csv(f'{PATH}\/sample_submission.zip')","6df0286a":"train.head()","a60400a3":"test.head()","1e68ec1b":"sample.head()","b254d3db":"def multiclass_logloss(actual,predicted,eps=1e-15):\n    \n    #converting the 'actual' values to binary values if it's \n    #not binary values\n    \n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0],predicted.shape[1]))\n        \n        for i, val in enumerate(actual):\n            actual2[i,val] = 1\n        actual = actual2\n    \n    #clip function truncates the number between\n    #a max number and min number\n    clip = np.clip(predicted,eps,1-eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0\/ rows * vsota ","b2d755b5":"encoder = preprocessing.LabelEncoder()\ny = encoder.fit_transform(train[\"author\"].values)","f144facd":"# we will use 10% of data for testing\nX_train, X_test, y_train, y_test = train_test_split(train.text.values,y,random_state=42,test_size=0.1,shuffle=True)","33bb4dec":"from sklearn.feature_extraction.text import CountVectorizer\n#we are going to use this example as our documents.\n\ncat_in_the_hat_docs=[\n       \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n      ]\n\n#make object of countvectorizer\ncv = CountVectorizer()\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)","94edb552":"#now let's look at the  unique words countvectorizer was able to find\ncv.vocabulary_","906c2f16":"count_vector.shape","e0ab8efd":"#using cumstom stopword list\ncustom_stop_words = [\"all\",\"in\",\"the\",\"is\",\"and\"]\n\ncv = CountVectorizer(cat_in_the_hat_docs,stop_words=custom_stop_words)\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\ncount_vector.shape","f6cbdfe6":"#have a look at the stop words\ncv.stop_words","2296b0f3":"cv = CountVectorizer(cat_in_the_hat_docs,min_df=2) #word that has occur in only one document\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\n\n#now let's look at stop words\ncv.stop_words_\n#see the difference of _ at the end it's because we used min_df \n#instead of custom stop_words","6fc29113":"#using max_df\ncv = CountVectorizer(cat_in_the_hat_docs,max_df=0.5) #present in more than 50% of documents\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\n\ncv.stop_words_","b9b65981":"#using Tfidtrasnformer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#we will use this toy example\ndocs=[\"the house had a tiny little mouse\",\n      \"the cat saw the mouse\",\n      \"the mouse ran away from the house\",\n      \"the cat finally ate the mouse\",\n      \"the end of the mouse story\"\n     ]\n\ncv = CountVectorizer(docs,max_df=0.5)\n\ncount_vector = cv.fit_transform(docs)\nprint(count_vector.shape)\n\n#calculate idf values\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(count_vector)\n\ndf_idf = pd.DataFrame(tfidf_transformer.idf_,index=cv.get_feature_names(),columns=[\"idf_weights\"])\ndf_idf\n","15f6efd8":"#using tfid_vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\ntfidf_vectorizer = TfidfVectorizer(smooth_idf=True,use_idf=True)\ntfidf_vectorizer.fit_transform(docs)\n\n#as you can see we don't need CountVectorizer in TfidfVectorizer\n\ndf_idf = pd.DataFrame(tfidf_vectorizer.idf_,index=tfidf_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\ndf_idf","b0c96681":"# we can also pass countvectorizer parameters in TfidVectorizer\ntfv = TfidfVectorizer(min_df=3,max_features=None,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',\n                      ngram_range=(1,3),use_idf=1,smooth_idf=1,stop_words='english')\n\n# max_features confines maximum number of words \n\ntfv.fit(list(X_train) + list(X_test))\nX_train_tfv = tfv.transform(X_train)\nX_test_tfv = tfv.transform(X_test)\n\n\n","1a164d44":"# Fitting Logistic Regression on TFIDF\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nclf.fit(X_train_tfv,y_train)\nprediction = clf.predict_proba(X_test_tfv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","617f40bf":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\nctv.fit(list(X_train)+list(X_test))\nX_train_ctv = ctv.transform(X_train)\nX_test_ctv = ctv.transform(X_test)","7934767f":"clf = LogisticRegression(C=1.0)\nclf.fit(X_train_ctv,y_train)\nprediction = clf.predict_proba(X_test_ctv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","14f99221":"clf = MultinomialNB()\nclf.fit(X_train_tfv,y_train)\n\nprediction = clf.predict_proba(X_test_tfv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","5a776a16":"clf = MultinomialNB()\nclf.fit(X_train_ctv,y_train)\n\nprediction = clf.predict_proba(X_test_ctv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","94c55a96":"svd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(X_train_tfv)\nX_train_svd = svd.transform(X_train_tfv)\nX_test_svd = svd.transform(X_test_tfv)\n\nscl = preprocessing.StandardScaler()\nscl.fit(X_train_svd)\n\nX_train_svd_scl = scl.transform(X_train_svd)\nX_test_svd_scl = scl.transform(X_test_svd)","fbc1237f":"svm = SVC(C=1.0,probability=True)\n\nsvm.fit(X_train_svd_scl,y_train)\nprediction = svm.predict_proba(X_test_svd_scl)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","512cf071":"clf = xgb.XGBClassifier(max_depth=7,n_estimators=200,colsample_bytree=0.8,subsample=0.8,nthread=10,learning_rate=0.1)\n\nclf.fit(X_train_tfv.tocsc(),y_train)\nprediction = clf.predict_proba(X_test_tfv.tocsc())\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","327827c3":"clf = xgb.XGBClassifier(max_depth=7,n_estimators=200,colsample_bytree=0.8,subsample=0.8,nthread=10,learning_rate=0.1)\n\nclf.fit(X_train_svd,y_train)\nprediction = clf.predict_proba(X_test_svd)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","47d66ade":"# as Multiclass_logloss is user defined we need to define our own scorer for grid search\n# greater_is_better is True by default but for our smaller the value of logloss better the result\n\nmll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False,needs_proba=True)","226c6619":"svd = decomposition.TruncatedSVD()\n\nscl = preprocessing.StandardScaler()\n\nlr_model = LogisticRegression()\n\nclf = pipeline.Pipeline([('svd',svd),\n                         ('scl',scl),\n                         ('lr',lr_model)])","4738abc1":"params_grid = {'svd__n_components':[120,180],\n               'lr__C':[0.1,1.0,10],\n               'lr__penalty':['l1','l2']}","6410a3ba":"model = GridSearchCV(estimator=clf,param_grid=params_grid,scoring=mll_scorer,verbose=10,n_jobs=-1,iid=True,refit=True,cv=2)\n\n#fitting the model\nmodel.fit(X_train_tfv,y_train)\n\nprint('Best score: %0.3f' % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(params_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","3b737434":"nb = MultinomialNB()\n\nclf = pipeline.Pipeline([('nb',nb)])\n\nparams_grid = {'nb__alpha':[0.001,0.01,0.1,1,10,100]}\n\nmodel  = GridSearchCV(estimator=clf,param_grid=params_grid,scoring=mll_scorer,verbose=10,n_jobs=-1,refit=True,cv=2)\n\nmodel.fit(X_train_tfv,y_train)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(params_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","d3faaaf5":"Fitting on counts","840f95ae":"## Tfidftransformer and Tfidfvectorizer\n\nThis are the classes which calculates tfid scores for the documents.\n\nThere is only little difference between both of them<br\/>\nTfidtransformer uses CountVectorizer that we have created manually<br\/>\n\nBut Tfidvectorizer performs CountVectorizer function internally and no need to create<br\/>\nmanual CountVectorizer object.","91120330":"## Importing Imp libraries\n\nI have just imported every possibly used library of NLP <br\/>\nwe will see what each imported library and class do eventually.<br\/>\n\nI am following [this](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) notebook by abhishek thakur. ","5a761ca2":"Grid Search for Navi Bayes","fc92d410":" ##  Function for Multiclass-logloss","909ca00b":"## Navie Bayes\n\nFitting on tfidf","77116d64":"Fitting svm ","a215bed8":"# Natural Language Processing \n    \n    Natural language is the language we speak or write in, Natural Language Processing is a study where\n    we make computer understand how our language works and what are the rules on which our language is based\n    \n    Natural Language is hard to understand for computers because Natural Language has very less rules.\n    and sometimes it requires the context inorder to understand the Language.\n    \n    Natural Language Processing has been around for 50 years, but still there is room for improvement.\n    \n    Natural Language Processing is so hard that Turing should have included it in the test of intellegence.\n    \n    \n        \n    ","288754ab":"## Back to Competition ","ffcee754":"### Using Max_df (Max document frequency)\n\nAs we have removed words which are less frequent we can remove words<br\/>\nwhich are too frequent among documents using max_df parameter.","58eaa7c0":"## SVM\n\nSVM is very slow algorithm so it takes lot of time to fit so we will<br\/> \nuse Singular Value Decomposition before applying SVM<br\/>\nand we will also standardize the data.\n","e7385516":"# NLP Challange.\n \n    I have decided a idealistic challange for myself that is learning NLP in 1 week \n    then participating in new \"Jigsaw Multilingual Toxic comment classification challange\"\n    \n    I am going to write all the things I will learn in different notebooks.\n ","5edb7ab1":"Fitting on svd","d7e76c22":"Let's remove stopwords from the docs\n\nThere are three ways in which we can remove stop_words in CountVectorizer\n\n1. using custom stop_words list\n2. using sklearn stopword list (not recomended)\n3. using min_df and max_df for removing stopwords (highly recommended)\n","07dcc6c3":"**we will labelencode the author column using LabelEncoder from scikit-learn**","4b811faa":"##  Mertics for calculating loss\n\nHere kaggle is using multiclass log loss for calculating the error.\n\nLog Loss is the mathematical function which is used to measure the error rate in the classification problem, means we need to get minimum value of log loss. Log Loss is different because here we need to calculate the probability of each class rather than predicting the class. \n\nThe twist about the logloss is that it highly penalize the wrong values. that is if in a binary classification if we have probability of 50\/ 50 an after that if probability shits 60\/40 to the wrong side,\nthat is to wrong class it highly penalizes it.\n\nThe graph of the probability vs logloss is shown below.\n\n![image](https:\/\/datawookie.netlify.app\/img\/2015\/12\/log-loss-curve.png)\n\n![image2](https:\/\/miro.medium.com\/max\/1192\/1*wilGXrItaMAJmZNl6RJq9Q.png)\n\nas this graph shows as the probability of the correct classification increases logloss decreases gradually but if the probability decreases logloss increase exponentially highly penalizing the output.\n\nso it is better to have overall good clasification values, rather than some excellent classification values and many worst classification values.\n\nto learn more about logloss read [this](https:\/\/datawookie.netlify.app\/blog\/2015\/12\/making-sense-of-logarithmic-loss\/) article.\n\n\nHere the difference is that instead of using Binary LogLoss Kaggle is usinig Multi logloss which is for Multiclass classification\n\nand it's formula is\n![image3](https:\/\/miro.medium.com\/max\/1162\/0*i2_eUc_t8A1EJObd.png)\n","9541468c":"grid of parameters","11c55a35":"Logistic regression on count vector","35101645":"**we will make train test split of the data using train_test_split function of scikit which has parameter test_size which decides fraction of the values to use as test data**","83562298":"## Grid Search\n\nWe will do hyperparameter optimization using sklearn GridSearchCV<br\/>\nand Data Pipeline is used here to keep code clean","a24f2bcc":"In above output number shown are not counts of words they are<br\/>\ntheir position (column no.) in the matrix","fc534f28":"## What is Singular Value Decomposition?\n\nSingular value decomposition  or svd is a matrix decompostion technique<br\/>\nsvd converts a matrix A  of mxn in dot product of three matrix (U . sigma . V^T) <br\/>\n\nHere U is mxm matrix<br\/>\nsigma is diagonal matrix of nxm <br\/>\nV^T is transpost of nxn matrix <br\/>\n\nThen reduction to k columns is done by taking sigma with k columns.<br\/>\nor V^T with k rows. and preforming one of below function\n\nT = U dot sigma or T = A dot V^T\n\nT is final reduced matrix.\n\nTo know about SVD in detail read [this](https:\/\/machinelearningmastery.com\/singular-value-decomposition-for-machine-learning\/) article.\n","52831d5f":"we are creating parameter search for logistic regression <br\/>\nwith SVD of 120 and 180 lr of 0.1,1,10 and l1 and l2 penalty.","726e83c7":"## Term Frequency(tf) and Inverse document frequency(idf).\n\n**what is term frequeny(tf)?**: \n* term frequency if simply a number which indicates how many times a number<br\/>\n  occur in the document. So it just count number of occurance of the words. It is used to know what words are<br\/>\n  important in the documents.\n* Term Frequency is divided by the total number of words for normaizing, it could also be divided by max fequency<br\/>\n  or average frquency.\n\n**what is inverse documents frequency?**\n* The problem with the term freuency is that in a language there are many connecters common words like \"the\",\"is\",\"and\"<br\/>\n   and this words do not highlight the context of the sentence. one way to solve this problem is to remove stop words<br\/>\n   and other is to multiply it with idf score.\n* Idf score of a word is a number determining how unique this word is to given document.\n* It is calculated as log(N\/DT) where N is total number of documents and DT is number of doc containing the word.\n\nso tf*idf will highlight the words which gives us context of the data.\n","5126736b":"## Xgboost\n\nFitting on tfidf","1d0f3e37":" ##  Loading dataset\n \n **About Data and compitition**\n    Here the challange is simply to predict which author wrote <br\/>\n    given sentence \n    \n  we are given training data and testing data in separate files\n  training data contains \"id\" of each sentence \"text\" the sentence\n  and author(EAP: Edgar Allan Poe, HPL: HP Lovecraft; \n  MWS: Mary Wollstonecraft Shelley)","7001e066":"**Basic steps performed by countvectorizer**\n1. lowecase words (lowercase=False if not want to lower)\n2. uses utf-8 encoding\n3. perform tokenization (converts text to small chunk of text)\n4. word level tokenization (converts each word to token)\n5. ignores single character such as \"a\" and \"I\"","2a1ea018":"Good thing about CountVectorizer is that it allows you to make your own<br\/>\npreprocessor and tokenizer and add it to 'tokenizer' and 'preprocessor' parameter\n\nand other thing is if you just want presence and absence of words instead of counts<br\/>\nyou can set binary=True in parameters","71a1bf29":"### using MIN_DF (minimum document frequency)\nMin_df will look for the words which have low occurence in the documents<br\/>\nlike a name of the person which is occured only once in all the documents coult<br\/>\nbe removed.\n\nit can be done by setting min_df argument which can be absolute value like 1,2,3<br\/>\nor could be 0.25 means less than 25% of documents\n","c6ba1230":"## LogisticRegression Model","eef83f78":"# countvectorizer\n\nBasic function of countvectorizer is to make a sparce matrix of word count for each document.<br\/>\nfor example there are two docs like \"say hello to dog\", and \"say hello to everyone\".\n\nCountvectorizer will create a column for each unique word in all the documents<br\/>\nand rows for each document. with values of count of each word<br\/>\nso countvectorizer could perform as a feature to machine learning model<br\/>\n\nso we will have 5 columns(unique words) and 2 rows(no of documents)<br\/>\n\nsay | hello | to | dog | everyone<br\/>\n   1      1     1     1       0    <br\/>\n   1      1     1     0       1    <br\/>\n   \nHere instead of column names as \"words\" countvectorizer assigns number to each word<br\/>\nlike say=1, hello=2, to =3 ....\n\nNow we will look at various ways we can use countvectorizer    ","37e47cd9":"The shape changed from (5,43) to (5,40) as the stop words are removed","9c397875":"There are too many stop words because our document list is small","d3477c42":"This is the end of this notebook one, next notebook we will learn about advance stuff<br\/>\nlike word vectors , word embeddings and Deep Learning Models"}}