{"cell_type":{"962440fa":"code","ba2b3f03":"code","513e8077":"code","a35a999b":"code","9fad1964":"code","c0f069ad":"code","07695fea":"code","2bcf4605":"code","311ae253":"code","4b075b0c":"code","4dfef49e":"code","cb6e2b94":"code","09af8058":"code","e5d175da":"code","157a554f":"markdown","a5169448":"markdown","9a0c5a76":"markdown","0581ce22":"markdown","eab9e040":"markdown","50d0cb9f":"markdown","3f67d4ac":"markdown"},"source":{"962440fa":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128","ba2b3f03":"plt.rcParams['figure.figsize'] = (12, 9)","513e8077":"os.listdir('..\/input\/test\/')","a35a999b":"train = pd.read_csv('..\/input\/train\/train.csv')\ntest = pd.read_csv('..\/input\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/test\/sample_submission.csv')","9fad1964":"labels_breed = pd.read_csv('..\/input\/breed_labels.csv')\nlabels_state = pd.read_csv('..\/input\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/state_labels.csv')","c0f069ad":"train_image_files = sorted(glob.glob('..\/input\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/train_sentiment\/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('..\/input\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/test_sentiment\/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","07695fea":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n\n# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ train_df_ids.shape[0]))","2bcf4605":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ test_df_ids.shape[0]))\n\n\n# Metadata:\ntest_df_ids = test[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ test_df_ids.shape[0]))\n\n\n\n# Sentiment:\ntest_df_ids = test[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ test_df_ids.shape[0]))\n\n\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","311ae253":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n\n        if self.extract_sentiment_text:\n            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns').sum()\n        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n        \n        file_sentiment.update(file_sentences_sentiment)\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        if self.extract_sentiment_text:\n            df_sentiment['text'] = file_sentences_text\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = '..\/input\/{}_sentiment\/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob('..\/input\/{}_metadata\/{}*.json'.format(mode, pet_id)))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_metadata_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","4b075b0c":"# Unique IDs from train and test:\ndebug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\n# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","4dfef49e":"# Extend aggregates and improve column naming\naggregates = ['mean', 'sum', 'var']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntrain_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntest_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","cb6e2b94":"print(train_metadata_gr.shape, test_metadata_gr.shape)\nprint(\"sentiment\", train_sentiment_gr.shape, test_sentiment_gr.shape)","09af8058":"\n\ntrain_metadata_gr = train_metadata_gr.merge(\n    train_metadata_desc, how='left', on='PetID')\nprint(\"Train_metadata_gr\",train_metadata_gr.shape)\n\ntrain_sentiment_gr = train_sentiment_gr.merge(\n    train_sentiment_desc, how='left', on='PetID')\nprint(\"Train_sentiment_gr\",train_sentiment_gr.shape)\n\ntest_metadata_gr = test_metadata_gr.merge(\n    test_metadata_desc, how='left', on='PetID')\nprint(\"Test_metadata_gr\",test_metadata_gr.shape)\n\ntest_sentiment_gr = test_sentiment_gr.merge(\n    test_sentiment_desc, how='left', on='PetID')\nprint(\"Test_sentiment_gr\",test_sentiment_gr.shape)","e5d175da":"train_metadata_gr.to_csv('train_dfs_metadata.csv', index=False)\ntrain_sentiment_gr.to_csv('train_dfs_sentiment.csv', index=False)\ntest_metadata_gr.to_csv('test_dfs_metadata.csv', index=False)\ntest_sentiment_gr.to_csv('test_dfs_sentiment_gr.csv', index=False)","157a554f":"Hi, these kernel is forked by **BaselineModeling** And just copy the part of extra Meta-images and sentiment featuers.\nAnd save it to .csv","a5169448":"### group extracted features by PetID:","9a0c5a76":"### load mapping dictionaries:","0581ce22":"### additional data:\n\nWe have also additional information about pets available in form of:\n\n- images\n- metadata\n- sentiment\n\nIntegration of those will enable us to possibly improve the score.\nInformation derived from example from images should be very important, as picture of a pet influences the way we look at an animal in a significant way.","eab9e040":"### train analysis:","50d0cb9f":"### data parsing & feature extraction:\n\nAfter taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train\/test DFs.","3f67d4ac":"### load core DFs (train and test):"}}