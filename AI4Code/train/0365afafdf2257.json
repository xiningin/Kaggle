{"cell_type":{"cc7bc10c":"code","5b2a62d6":"code","70ccbd95":"code","08267f10":"code","073038a5":"code","bbc930a4":"code","4dc9fcb8":"code","d9147a72":"code","7def720c":"code","42169d03":"code","6e2c347b":"code","95818059":"markdown"},"source":{"cc7bc10c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b2a62d6":"jobs = pd.read_csv('\/kaggle\/input\/data-analyst-jobs\/DataAnalyst.csv')","70ccbd95":"import re\nimport matplotlib.pyplot as plt\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport copy\nimport networkx as nx ","08267f10":"jobs.drop('Unnamed: 0', axis = 1, inplace = True)\njobs = jobs.drop_duplicates(subset = ['Job Description','Job Title','Location'], keep = 'first') \n","073038a5":"#Let's remove Capitals\njobs['Job Description'] = jobs['Job Description'].str.lower()\n\n#Let's remove all non-word charachters\n\nregex = re.compile('[^a-zA-Z\\']')\n\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: regex.sub(' ', x))\n\n\n#The Equal Oppritunity tagline may skew our results, let's remove it\nequal_emp = 'Kelly is an equal opportunity employer committed to employing a diverse workforce, including, but not limited to, minorities, females, individuals with disabilities, protected veterans, sexual orientation, gender identity. Equal Employment Opportunity is The Law.'\nequal_emp = equal_emp.lower().split(' ')\n\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: [item for item in x.split() if item.lower() not in equal_emp])\n\n#and then re-join our Job Descriptions\njobs['Job Description'] = jobs['Job Description'].apply(lambda x: ' '.join(x))\n\n\n## There are way too many skills to analyze so we're going to group them by some arbitrary (but hopefully accurate) categories I came up with. \n\nskill_types= {}\n\nskill_types['Statistics'] = ['matlab',\n 'statistical',\n 'models',\n 'modeling',\n 'statistics',\n 'analytics',\n 'forecasting',\n 'predictive',\n 'r',\n 'pandas',\n 'statistics',\n 'statistical',\n 'Julia']\n\nskill_types['Machine Learning'] = ['datarobot',\n 'tensorflow',\n 'knime',\n 'rapidminer',\n 'mahout',\n 'logicalglue',\n 'nltk',\n 'networkx',\n 'rapidminer',\n 'scikit',\n 'pytorch',\n 'keras',\n 'caffe',\n 'weka',\n 'orange',\n 'qubole',\n 'ai',\n 'nlp',\n 'ml',\n 'neuralnetworks',\n 'deeplearning']\n\n\nskill_types['Data Visualization'] = ['tableau',\n 'powerpoint',\n 'Qlik',\n 'looker',\n 'powerbi',\n 'matplotlib',\n 'tibco',\n 'bokeh',\n 'd3',\n 'octave',\n 'shiny',\n 'microstrategy']\n\n\nskill_types['Data Engineering'] = ['etl',\n 'mining',\n 'warehousing',\n 'cloud',\n 'sap',\n 'salesforce',\n 'openrefine',\n 'redis',\n 'sybase',\n 'cassandra',\n 'msaccess',\n 'databasemanagement',\n 'aws',\n 'ibmcloud',\n 'azure',\n 'redshift',\n 's3',\n 'ec2',\n 'rds',\n 'bigquery',\n 'googlecloudplatform',\n 'googlecloudplatform',\n 'hadoop',\n 'hive',\n 'kafka',\n 'hbase',\n 'mesos',\n 'pig',\n 'storm',\n 'scala',\n 'hdfs',\n 'mapreduce',\n 'kinesis',\n 'flink']\n\n\nskill_types['Software Engineer'] = ['java',\n 'javascript',\n 'c#',\n 'c',\n 'docker',\n 'ansible',\n 'jenkins',\n 'nodejs',\n 'angularjs',\n 'css',\n 'html',\n 'terraform',\n 'kubernetes',\n 'lex',\n 'perl',\n 'cplusplus']\n\n\nskill_types['SQL'] = ['sql',\n 'oracle',\n 'mysql',\n 'oraclenosql',\n 'nosql',\n 'postgresql',\n 'plsql',\n 'mongodb']\n\n\n\n\nskill_types['Trait Skills'] = ['Learning',\n 'TimeManagement',\n 'AttentiontoDetail',\n 'ProblemSolving',\n 'criticalthinking']\n\n\n\nskill_types['Social Skills']= ['teamwork',\n 'team'\n 'communication',\n 'written',\n 'verbal',\n 'writing',\n 'leadership',\n 'interpersonal',\n 'personalmotivation',\n 'storytelling']\n\nskill_types['Business'] = ['excel',\n 'bi',\n 'reporting',\n 'reports',\n 'dashboards',\n 'dashboard',\n 'businessintelligence'\n 'business']\n\n#### For some reason some of the dictionary values are uppercase so we have to correct that\n\nfor k,v in skill_types.items():\n    skill_types[k] = [skill.lower() for skill in skill_types.get(k)]\n\n# Refined Job descriptions\n\n## Now that we have our extensive list, we need to use it to modify our Job Descriptions\n\ndef refiner(desc):\n    desc = desc.split()\n    \n    two_word = ''\n    \n    newskills = []\n    \n    for word in desc:\n        two_word = two_word + word \n        for key,value in skill_types.items():\n            if((word in value) or (two_word in value)):\n                newskills.append(key)\n                \n        #check for the two worders, like 'businessintelligence'        \n        two_word = word\n                \n    return list(set(newskills))\n\n#Now all we have to do is apply this do our Job Description\njobs['refined skills'] = jobs['Job Description'].apply(refiner)\njobs['refined skills']","bbc930a4":"def apriori_df(series, min_support):\n    lisolis =[]\n    series.apply(lambda x: lisolis.append(list(x)))\n    \n    from mlxtend.preprocessing import TransactionEncoder\n\n    te = TransactionEncoder()\n    te_ary = te.fit(lisolis).transform(lisolis)\n    df = pd.DataFrame(te_ary, columns=te.columns_)\n\n\n    from mlxtend.frequent_patterns import apriori\n\n    freq_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n    \n    return freq_itemsets\n\n","4dc9fcb8":"\nfrequent_itemsets = apriori_df(jobs['refined skills'],.1)\n\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))","d9147a72":"import seaborn as sns\n\n_ = frequent_itemsets[frequent_itemsets['length'] == 1]\n_['itemsets'] = _['itemsets'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nax = sns.barplot(x=\"itemsets\", y=\"support\", data= _);\nax.set_xticklabels(ax.get_xticklabels(), rotation=75);","7def720c":"single_items = frequent_itemsets[frequent_itemsets['length'] == 1]\nsingle_items['itemsets'] = single_items['itemsets'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\nsingle_items['itemsets'] = single_items['itemsets'].str.replace(' ','\\n', regex = True)","42169d03":"def make_network_graph(min_conviction):\n    rules = association_rules(apriori_df(jobs['refined skills'],.15), metric=\"conviction\", min_threshold=.5)\n    # 1 to 1 \n    rules['alength'] = rules['antecedents'].apply(lambda x: len(x))\n    rules['clength'] = rules['consequents'].apply(lambda x: len(x))\n    rules = rules[(rules['alength'] == 1) & (rules['clength'] == 1)]\n    rules['antecedents'] = rules['antecedents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\n    rules['antecedents'] = rules['antecedents'].str.replace(' ','\\n', regex = True)\n    rules['consequents'] = rules['consequents'].astype(\"unicode\").str.replace('[\\(\\)\\'\\{\\}]|frozenset','', regex = True)\n    rules['consequents'] = rules['consequents'].str.replace(' ','\\n', regex = True)\n\n\n    #Make some edges, now that we've run apriori\n    weighted_edges = []\n\n    G = nx.DiGraph()\n    \n    for x in range(len(rules)):\n        if(rules.iloc[x,8] > min_conviction):\n            weighted_edges.append((rules.iloc[x,0], rules.iloc[x,1], rules.iloc[x,8]))\n        \n        else:\n            G.add_node(rules.iloc[x,1])\n            G.add_node(rules.iloc[x,0])\n        \n        \n     \n    G.add_weighted_edges_from(weighted_edges)\n    \n    #Change node size according to support\n    for i in list(G.nodes()): \n        G.nodes[i]['support'] = single_items.loc[(single_items['itemsets'] == i), 'support'].values\n        \n    node_size = [60000*nx.get_node_attributes(G, 'support')[v] for v in G] \n    edge_width = [(G[u][v]['weight']- 1)*10.5 if((G[u][v]['weight']- 1) > .1) else 0 for u, v in G.edges() ]\n\n    pos = nx.circular_layout(G)\n    plt.figure(figsize=(27,22))\n\n\n    pos_shadow = copy.deepcopy(pos)\n    shift_amount = 0.008\n    for idx in pos_shadow:\n        pos_shadow[idx][0] += shift_amount\n        pos_shadow[idx][1] -= shift_amount\n\n    nx.draw_networkx_nodes(G, pos_shadow, node_color='k', alpha=0.3,  node_size = node_size)   \n\n    nx.draw_networkx_nodes(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\n\n    nx.draw_networkx_labels(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, size = 20, font_size = 28, width = edge_width, font_weight = 'bold', font_color = 'darkorange')\n\n    nx.draw_networkx_edges(G, pos, with_label = True, node_size = node_size,connectionstyle='arc3, rad = .03',arrowsize=60, width = edge_width)\n    plt.title('Skills', size = 50)\n    plt.axis('off')\n    plt.plot();","6e2c347b":"make_network_graph(1.5)","95818059":" # Full Walkthrough: https:\/\/www.kaggle.com\/josephgutstadt\/skills-for-a-data-scientist-analyst\n    \n## Full Dataset https:\/\/www.kaggle.com\/josephgutstadt\/data-jobs\n\nThanks for reading! I hope this was helpful. If you have any concerns, criticism, or just ideas please share. I'd love to hear your thoughts."}}