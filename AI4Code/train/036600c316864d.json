{"cell_type":{"ed75b5af":"code","ee43457a":"code","9e2192c4":"code","15e21ee4":"code","338991e2":"code","56f1d85e":"code","67db001b":"code","d0dd2010":"code","f732c7b0":"code","7a87f15f":"code","0536dadf":"code","c186e660":"code","e884c4e4":"code","6a6e4d1e":"code","28870b3b":"code","921ebbbc":"code","116e05a3":"code","7b377ec1":"code","0aa7eb95":"code","a1cbab50":"code","aaab38ce":"code","07c94cdc":"code","5631b374":"code","8167bc14":"code","0cb8e6ac":"code","479a6504":"code","1ed414cb":"markdown","ac8f265b":"markdown","bb9e8421":"markdown","153fee37":"markdown","215b6268":"markdown","4e6d997b":"markdown","cb34bbe1":"markdown","7974e359":"markdown","d9a19601":"markdown","79d196a0":"markdown","b0f9d3a3":"markdown","3d8d60bc":"markdown","c10fd108":"markdown","5aee8241":"markdown","15213362":"markdown","f53dfb8a":"markdown","0e0bc2b0":"markdown","7aa2c00d":"markdown","74a62652":"markdown","66db9718":"markdown","14d05984":"markdown","0386a1bc":"markdown","c8b64e9e":"markdown","35145ed0":"markdown","819d051c":"markdown","42b32ec8":"markdown","a0421260":"markdown","1962b8ba":"markdown","a8b6c62e":"markdown","97d2e24a":"markdown","8c0771d7":"markdown"},"source":{"ed75b5af":"import datetime as dt\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\n\nimport os\nfrom keras.applications import xception\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport cv2\nfrom scipy.stats import uniform\n\nfrom tqdm import tqdm\nfrom glob import glob\n\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking\nfrom keras.utils import np_utils, to_categorical\n\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n","ee43457a":"#copying the pretrained models to the cache directory\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\n#copy the Xception models\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models\/\n#show\n!ls ~\/.keras\/models","9e2192c4":"dir_kaggle ='..\/input\/face-mask-detection'\ndata_kaggle ='..\/input\/face-mask-detection\/dataset'\nwith_mask ='....\/input\/face-mask-detection\/dataset\/with_mask'\nwithout_mask='..\/input\/face-mask-detection\/dataset\/without_mask'\n\n\nclass_data= ['with_mask','without_mask']\nlen_class_data = len(class_data)","15e21ee4":"image_count = {}\ntrain_data = []\n\nfor i , class_data in tqdm(enumerate(class_data)):\n    class_folder = os.path.join(data_kaggle,class_data)\n    label = class_data\n    image_count[class_data] = []\n    \n    for path in os.listdir(os.path.join(class_folder)):\n        image_count[class_data].append(class_data)\n        train_data.append(['{}\/{}'.format(class_data, path), i, class_data])","338991e2":"#show image count\nfor key, value in image_count.items():\n    print('{0} -> {1}'.format(key, len(value)))","56f1d85e":"#create a dataframe\ndf = pd.DataFrame(train_data, columns=['file', 'id', 'label'])\ndf.shape\ndf.head()","67db001b":"\n#masking function\ndef create_mask_for_image(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    lower_hsv = np.array([0,0,250])\n    upper_hsv = np.array([250,255,255])\n    \n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    return mask\n\n#image  deskew function\ndef  deskew_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#image  gray  function\ndef  gray_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.cvtColor(image,  cv2.COLOR_BGR2GRAY)\n    return output\/255\n\n#image  thresh  function\ndef  thresh_image(image):\n    img = read_img(df['file'][250],(255,255))\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    output = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV) #+cv.THRESH_OTSU)\n    return output\n\n\n#image  rnoise  function\ndef  rnoise_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#image  dilate  function\ndef  dilate_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n\n#image  erode  function\ndef  erode_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n\n#image  opening  function\ndef  opening_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#image canny function\ndef  canny_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n\n#image segmentation function\ndef segment_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n\n#sharpen the image\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp\n\n\n\n# function to get an image\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_kaggle, filepath), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img)\n    return img","d0dd2010":"nb_rows = 3\nnb_cols = 5\nfig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 5));\nplt.suptitle('SAMPLE IMAGES');\nfor i in range(0, nb_rows):\n    for j in range(0, nb_cols):\n        axs[i, j].xaxis.set_ticklabels([]);\n        axs[i, j].yaxis.set_ticklabels([]);\n        axs[i, j].imshow((read_img(df['file'][np.random.randint(400)], (255,255)))\/255.);\nplt.show();","f732c7b0":"#get an image\nimg = read_img(df['file'][12],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_mask);\n","7a87f15f":"#get an image\nimg = read_img(df['file'][13],(255,255))\n\n#segmentation\nimage_segmented = segment_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SEGMENTED', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_segmented);","0536dadf":"#get an image\nimg = read_img(df['file'][14],(255,255))\n#deskew\nimage_deskew = deskew_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DESKEW', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_deskew);","c186e660":"#get an image\nimg = read_img(df['file'][105],(255,255))\n\n#gray\nimage_gray = gray_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GRAY', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_gray);","e884c4e4":"#get an image\nimg = read_img(df['file'][250],(255,255))\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV) #+cv.THRESH_OTSU)\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('THRESH', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(thresh);","6a6e4d1e":"#get an image\nimg = read_img(df['file'][275],(255,255))\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('RNOISE', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_rnoise);","28870b3b":"#get an image\nimg = read_img(df['file'][275],(255,255))\n\n#rnoise\nimage_canny = canny_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('RNOISE', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_canny);","921ebbbc":"#get an image\nimg = read_img(df['file'][12],(255,255))\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SHARPEN', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_sharpen);","116e05a3":"#get an image\nimg = read_img(df['file'][60],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\n#segmentation\nimage_segmented = segment_image(img)\n\n\n#deskew\nimage_deskew = deskew_image(img)\n\n#gray\nimage_gray = gray_image(img)\n\n#thresh\nimage_thresh = thresh_image(img)\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\n#canny\nimage_canny = canny_image(img)\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\nfig, ax = plt.subplots(1, 9, figsize=(15, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('MASK', fontsize=9)\nax[2].set_title('SEGMENTED', fontsize=9)\nax[3].set_title('DESKEW', fontsize=9)\nax[4].set_title('GRAY', fontsize=9)\nax[5].set_title('THREST', fontsize=9)\nax[6].set_title('RNOISE', fontsize=9)\nax[7].set_title('CANNY', fontsize=9)\nax[8].set_title('SHARPEN', fontsize=9)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_mask);\nax[2].imshow(image_segmented);\nax[3].imshow(image_deskew);\nax[4].imshow(image_gray );\nax[5].imshow(thresh);\nax[6].imshow(image_rnoise);\nax[7].imshow(image_canny);\nax[8].imshow(image_sharpen);\n","7b377ec1":"#get an image\nimg = read_img(df['file'][321],(255,255))\n\n#erode\nimage_erode = erode_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('ERODE', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_erode);","0aa7eb95":"#get an image\nimg = read_img(df['file'][175],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DILATE', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_dilate);","a1cbab50":"#get an image\nimg = read_img(df['file'][55],(255,255))\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('OPENING', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(image_opening);","aaab38ce":"#get an image\nimg = read_img(df['file'][60],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\n#erode\nimage_erode = erode_image(img)\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 4, figsize=(10, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('DILATE', fontsize=9)\nax[2].set_title('ERODE', fontsize=9)\nax[3].set_title('OPENING', fontsize=9)\n\n\nax[0].imshow(img\/255);\nax[1].imshow(image_dilate);\nax[2].imshow(image_erode);\nax[3].imshow(image_opening);\n","07c94cdc":"\n#get an image\nimg = read_img(df['file'][135],(255,255))\n\n#Blur\nblur = cv2.blur(img,(5,5))\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BLUR IMAGE', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(blur);","5631b374":"#get an image\nimg = read_img(df['file'][5],(255,255))\n\n#GaussianBlur\nGblur = cv2.GaussianBlur(img,(5,5),0)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GAUSSIAN BLUR', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(Gblur);","8167bc14":"#get an image\nimg = read_img(df['file'][10],(255,255))\n\n#medianBlur\nblur_image_median = cv2.medianBlur(img,5)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MEDIAN BLUR', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(blur_image_median);","0cb8e6ac":"\n#get an image\nimg = read_img(df['file'][15],(255,255))\n\n#BILATERAL FILTER\nbilblur = cv2.bilateralFilter(img,9,75,75)\n\nfig, ax = plt.subplots(1, 2, figsize=(5, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BILATERAL FILTER', fontsize=12)\n\nax[0].imshow(img\/255);\nax[1].imshow(bilblur);\n","479a6504":"#get an image\nimg = read_img(df['file'][20],(255,255))\n\nfig, ax = plt.subplots(1, 5, figsize=(10, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('BLUR IMAGE', fontsize=9)\nax[2].set_title('GAUSSIAN BLUR', fontsize=9)\nax[3].set_title('MEDIAN BLUR', fontsize=9)\nax[4].set_title('BILATERAL FILTER', fontsize=9)\n\n\nax[0].imshow(img\/255);\nax[1].imshow(blur);\nax[2].imshow(Gblur);\nax[3].imshow(blur_image_median);\nax[4].imshow(bilblur);\n","1ed414cb":"# Show Result","ac8f265b":"# Averaging","bb9e8421":"# What is Computer Vision?\n\n![https:\/\/skywell.software\/wp-content\/uploads\/2019\/07\/computer-vision-machine-learning-1024x630.jpg](https:\/\/skywell.software\/wp-content\/uploads\/2019\/07\/computer-vision-machine-learning-1024x630.jpg)\n\nComputer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do[[1](https:\/\/en.wikipedia.org\/wiki\/Computer_vision)]. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions ( Reinhard Klette (2014))\n\n\n","153fee37":"SHOW SAMPLE IMAGES","215b6268":"# Median Filtering\n\nHere, the function cv2.medianBlur() computes the median of all the pixels under the kernel window and the central pixel is replaced with this median value. This is highly effective in removing salt-and-pepper noise.","4e6d997b":"# Comparison of color space transformation","cb34bbe1":"# **Why image preprocessing?**\n\nThe acquired data are usually messy and come from different sources. To feed them to the Machine Learning model (or neural network), they need to be standardized and cleaned up.\n","7974e359":"# Opening\n\nOpening is just another name of erosion followed by dilation. It is useful in removing noise","d9a19601":"# **What is image processing?**\n\nIn machine learning projects in general, you usually go through a data preprocessing or cleaning step. The goal of this step is to make your data ready for the ML model to make it easier to analyze and process computationally, as it is with images. Based on the problem you\u2019re solving and the dataset in hand, there\u2019s some data massaging required before you feed your images to the ML model.\n\nImage processing could be simple tasks like image resizing.In order to feed a dataset of images to a convolutional network, they must all be the same size. Other processing tasks can take place like geometric and color transformation or converting color to grayscale and many more.","79d196a0":"# sharpen","b0f9d3a3":"# gray","3d8d60bc":"# deskew","c10fd108":"# rnoise","5aee8241":"# References\n\n[1] https:\/\/en.wikipedia.org\/wiki\/Computer_vision\n\n[2] http:\/\/szeliski.org\/Book\/\n\n[3] https:\/\/opencv-python-tutroals.readthedocs.io\/\n\n\n","15213362":"# Comparison of Image Blurring","f53dfb8a":"Practical example","0e0bc2b0":"# 2. Morphological Transformations","7aa2c00d":"This is done by convolving the image with a normalized box filter. It simply takes the average of all the pixels under kernel area and replaces the central element with this average. This is done by the function cv2.blur() or cv2.boxFilter(). ","74a62652":"Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise. It actually removes high frequency content (e.g: noise, edges) from the image resulting in edges being blurred when this is filter is applied. (Well, there are blurring techniques which do not blur edges). OpenCV provides mainly four types of blurring techniques.","66db9718":"# erosion\n\nThe basic idea of erosion is just like soil erosion only, it erodes away the boundaries of foreground object (Always try to keep foreground in white)","14d05984":"# 3. Image Blurring (Image Smoothing)","0386a1bc":"# dilate\n\nIt is just opposite of erosion. Here, a pixel element is \u20181\u2019 if atleast one pixel under the kernel is \u20181\u2019. So it increases the white region in the image or size of foreground object increases. Normally, in cases like noise removal, erosion is followed by dilation.","c8b64e9e":"# segmented","35145ed0":"# Bilateral Filtering\n\nAccording to [3] cv2.bilateralFilter() is highly effective at noise removal while preserving edges","819d051c":"\n# 1. Image Transform\n\nThis work tried to transform images from one color-space to another. The transformation you can see below:\n* mask\n* segment\n* deskew\n* gray\n* thresh\n* rnoise\n* canny\n* sharpen\n\nAccording to [2] while the color image can be treated arbitrary vector value functions or collections of independent bands, it usually makes sense to think about them as highly correlated signals with strong connections to the image formation process, sensor design, and Human perception. Consider example brightening picture by adding a constant value to all three channels. In fact, adding the same value to each color channel not only increases the apparent intensity of each pixel, but it cal also affects the picture hue and saturation.\n\n","42b32ec8":"# mask","a0421260":"# canny ","1962b8ba":"# Comparison of Morphological Transformations","a8b6c62e":"# thresh","97d2e24a":"# **Data preprocessing techniques might include:**\n\nin certain problems you\u2019ll find it useful to lose unnecessary information from your images to reduce space or computational complexity.\n\nFor example, converting your colored images to grayscale images. This is because in many objects, color isn\u2019t necessary to recognize and interpret an image. Grayscale can be good enough for recognizing certain objects. Because color images contain more information than black and white images, they can add unnecessary complexity and take up more space in memory (Remember how color images are represented in three channels, which means that converting it to grayscale reduces the number of pixels that need to be processed).","8c0771d7":"# Gaussian Filtering\nIt is done with the function, cv2.GaussianBlur(). "}}