{"cell_type":{"16409eb4":"code","59859b9a":"code","c60301df":"code","65015f7d":"code","a6afeb89":"code","a23ab97e":"code","0bc6304e":"code","f345cc18":"code","6281bbc6":"code","003387bb":"code","9e84ec88":"code","b98d385b":"code","5640b3bd":"code","50778327":"code","c1f3301c":"code","4de9759e":"code","38075c1f":"code","a9cb7ebe":"code","31c6aaeb":"markdown","0bca83d2":"markdown","868c9103":"markdown","8346a9ab":"markdown","76563464":"markdown","be1ce2bf":"markdown","f7a5cad7":"markdown","36eecb23":"markdown"},"source":{"16409eb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59859b9a":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as tud","c60301df":"trainRaw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntestRaw = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nanswer = pd.read_csv('\/kaggle\/input\/titanic-answer\/answers.csv')","65015f7d":"trainRaw.info(null_counts=True)","a6afeb89":"nTrain = trainRaw.shape[0]\nfull = pd.concat([trainRaw, testRaw],\n                 axis=0)\n\n# Name process \nfull['Title'] = full['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\nTitle_Dict = {}\nTitle_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\nTitle_Dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\nTitle_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\nTitle_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\nTitle_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\nTitle_Dict.update(dict.fromkeys(['Master','Jonkheer'], 'Master'))\nfull['Title'] = full['Title'].map(Title_Dict)\n\n# ADD FamilySize feature\nfull['FamilySize']=full['SibSp']+full['Parch']+1\ndef Fam_label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 7)) | (s == 1):\n        return 1\n    elif (s > 7):\n        return 0\nfull['FamilyLabel']=full['FamilySize'].apply(Fam_label)\n\n#Cabin process\n\nfull['Cabin'] = full['Cabin'].fillna('Unknown')\nfull['Deck']=full['Cabin'].str.get(0)\n\n#Ticket process\n\nTicket_Count = dict(full['Ticket'].value_counts())\nfull['TicketGroup'] = full['Ticket'].apply(lambda x:Ticket_Count[x])\ndef Ticket_Label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 8)) | (s == 1):\n        return 1\n    elif (s > 8):\n        return 0\n\nfull['TicketGroup'] = full['TicketGroup'].apply(Ticket_Label)\n\n#Age process\nfrom sklearn.ensemble import RandomForestRegressor\nage_df = full[['Age', 'Pclass','Sex','Title']]\nage_df=pd.get_dummies(age_df)\nknown_age = age_df[age_df.Age.notnull()]\nunknown_age = age_df[age_df.Age.isnull()]\ny = known_age['Age']\nX = known_age.drop(['Age'],axis=1)\nrfr = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)\nrfr.fit(X, y)\npredictedAges = rfr.predict(unknown_age.drop(['Age'],axis=1))\nunknown_age['Age']=predictedAges\nfull.loc[ (full.Age.isnull()), 'Age' ] = predictedAges \n\n#Embarked process\nfull['Embarked'] = full['Embarked'].fillna('C')\n\n#Fare process\nfare=full[(full['Embarked'] == \"S\") & (full['Pclass'] == 3)].Fare.median()\nfull['Fare']=full['Fare'].fillna(fare)\n\n#\u540c\u7ec4\u8bc6\u522b\nfull['Surname']=full['Name'].apply(lambda x:x.split(',')[0].strip())\nSurname_Count = dict(full['Surname'].value_counts())\nfull['FamilyGroup'] = full['Surname'].apply(lambda x:Surname_Count[x])\nFemale_Child_Group=full.loc[(full['FamilyGroup']>=2) & ((full['Age']<=12) | (full['Sex']=='female'))]\nMale_Adult_Group=full.loc[(full['FamilyGroup']>=2) & (full['Age']>12) & (full['Sex']=='male')]\n\n\nFemale_Child_Group=Female_Child_Group.groupby('Surname')['Survived'].mean()\nDead_List=set(Female_Child_Group[Female_Child_Group.apply(lambda x:x==0)].index)\n\nMale_Adult_List=Male_Adult_Group.groupby('Surname')['Survived'].mean()\nSurvived_List=set(Male_Adult_List[Male_Adult_List.apply(lambda x:x==1)].index)\n\ntrain=full.loc[full['Survived'].notnull()]\ntest=full.loc[full['Survived'].isnull()]\ntest.loc[(test['Surname'].apply(lambda x:x in Dead_List)),'Sex'] = 'male'\ntest.loc[(test['Surname'].apply(lambda x:x in Dead_List)),'Age'] = 60\ntest.loc[(test['Surname'].apply(lambda x:x in Dead_List)),'Title'] = 'Mr'\ntest.loc[(test['Surname'].apply(lambda x:x in Survived_List)),'Sex'] = 'female'\ntest.loc[(test['Surname'].apply(lambda x:x in Survived_List)),'Age'] = 5\ntest.loc[(test['Surname'].apply(lambda x:x in Survived_List)),'Title'] = 'Miss'\n###########################################################################\n\nall_data=pd.concat([train, test])\nint2obj_dic = ['FamilyLabel', 'TicketGroup','Pclass']\nall_data[int2obj_dic] = all_data[int2obj_dic].astype('object')\nall_data=all_data\nall_data=all_data[['Survived','Pclass','Sex','Age','Fare','Embarked','Title','FamilyLabel','Deck','TicketGroup']]\nall_data=pd.get_dummies(all_data)","a23ab97e":"all_data.info(null_counts=True)","0bc6304e":"Pclass_dic = ['Pclass_1', 'Pclass_2', 'Pclass_3']\nall_data[Pclass_dic] = all_data[Pclass_dic]\/3\nSex_dic = ['Sex_female', 'Sex_male']\nall_data[Sex_dic] = all_data[Sex_dic]\/2\nEmbarked_dic = ['Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_data[Embarked_dic] = all_data[Embarked_dic]\/3\nTitle_dic = ['Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\nall_data[Title_dic] = all_data[Title_dic]\/6\nFamilyLabel_dic = ['FamilyLabel_0', 'FamilyLabel_1', 'FamilyLabel_2']\nall_data[FamilyLabel_dic] = all_data[FamilyLabel_dic]\/3\nDeck_dic = ['Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T','Deck_U' ]\nall_data[Deck_dic] = all_data[Deck_dic]\/9\nTicketGroup_dic = ['TicketGroup_0', 'TicketGroup_1', 'TicketGroup_2']\nall_data[TicketGroup_dic] = all_data[TicketGroup_dic]\/3","f345cc18":"##Functions\ndef minmaxscaler(data):\n    min = np.amin(data)\n    max = np.amax(data)    \n    return (data - min)\/(max-min)\n\ndef feature_normalize(data):\n    mu = np.mean(data,axis=0)\n    std = np.std(data,axis=0)\n    return (data - mu)\/std\n\ndef unnormalized_show(img):\n    img = img * std + mu     # unnormalize\n    npimg = img.numpy()\n    plt.figure()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))","6281bbc6":"all_data.Age = all_data.Age.apply(lambda x : 70 if x > 70 else x)\nall_data.Age.plot()","003387bb":"all_data.Fare = all_data.Fare.apply(lambda x : 300 if x > 300 else x)\nall_data.Fare.plot()","9e84ec88":"all_data.Age = minmaxscaler(all_data.Age)\nall_data.Fare = minmaxscaler(all_data.Fare)","b98d385b":"all_data.Age.plot()","5640b3bd":"train=all_data[all_data['Survived'].notnull()]\ntest=all_data[all_data['Survived'].isnull()].drop('Survived',axis=1)\ntest_submssion=testRaw[['PassengerId']]\ny = train.Survived\nX = train.drop(['Survived'],axis=1)","50778327":"class DATASET(tud.Dataset):\n    def __init__(self, train, test):\n        self.X = torch.from_numpy(np.asarray(train)).float()\n        self.y= torch.from_numpy(np.asarray(test)).float()\n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n    def __len__(self):\n        return len(self.y)\n    \nD_IN, H1, H2, H3, D_OUT = 31, 100, 100, 10, 1\nDROP_PROB1, DROP_PROB2 = 0.5, 0.5\nclass Base_net(nn.Module):\n    def __init__(self):\n        super(Base_net, self).__init__()\n        self.linear1 = nn.Linear(D_IN, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, H3)\n        self.linear4 = nn.Linear(H3, D_OUT)\n        self.dp1 = nn.Dropout(DROP_PROB1)\n        self.dp2 = nn.Dropout(DROP_PROB2)\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        x = F.leaky_relu(self.linear2(x))\n        x = self.dp1(x)\n        x = F.leaky_relu(self.linear3(x))\n        x = self.dp2(x)\n        x = F.leaky_relu(self.linear4(x))\n        x = torch.sigmoid(x)\n        return x","c1f3301c":"#FULL_TRAIN\nfull_dataset = DATASET(X, y)\nanswer_dataset = DATASET(test, answer.Survived)\nfull_dataloador = tud.DataLoader(full_dataset, batch_size=64, shuffle=False, drop_last=False)\n\nLEN_FULL_TRAIN = len(full_dataset.y)\nLEN_ANSWER = len(answer_dataset.y)\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 2e-4\n\nmodel = Base_net()\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=WEIGHT_DECAY, lr=LEARNING_RATE)\nloss_func = nn.BCELoss()\nfor epoch in range(1000):\n    model.train()\n    for batch_x, batch_y in full_dataloador:\n        barch_y_pred = model(batch_x)\n        loss = loss_func(barch_y_pred, batch_y.unsqueeze(dim=1))\n        optimizer.zero_grad()   \n        loss.backward()         \n        optimizer.step()\n    \n    model.eval()\n    #Train data acc\n    train_pred = model(full_dataset.X)\n    train_pred = train_pred.detach().apply_(lambda x : 1 if x > 0.6 else 0)\n    train_acc = np.sum(train_pred.numpy().flatten() == full_dataset.y.numpy().flatten())\/ LEN_FULL_TRAIN\n    #Answer data acc\n    answer_pred = model(answer_dataset.X)\n    answer_pred = answer_pred.detach().apply_(lambda x : 1 if x > 0.6 else 0)\n    answer_acc = np.sum(answer_pred.numpy().flatten() == answer_dataset.y.numpy().flatten())\/ LEN_ANSWER\n    if epoch % 20 == 0 :\n        print(\"epoch: {} | loss: {} | train acc: {} | answer acc: {}\".format(epoch, loss.item(), train_acc, answer_acc))\n    if answer_acc > 0.81 :\n        print(\"epoch: {} | loss: {} | train acc: {} | answer acc: {}\".format(epoch, loss.item(), train_acc, answer_acc))\n        break","4de9759e":"test = torch.from_numpy(np.asarray(test)).float()\nmodel.eval()\ntest_submssion.loc[:,'Survived'] = model(test).detach().numpy().flatten()\ntest_submssion.loc[:,'Survived'] = test_submssion.loc[:,'Survived'].apply(lambda x : 1 if x > 0.5 else 0)","38075c1f":"test_submssion.info()","a9cb7ebe":"test_submssion.to_csv('\/kaggle\/working\/submssion.csv',index=False)","31c6aaeb":"## Normalized","0bca83d2":"### After repeated attempts, the best level of the network is 0.818 and the average is 0.8","868c9103":"## Feature engineer","8346a9ab":"### Normalized","76563464":"### Predict","be1ce2bf":"## Creat Dataset and Net class","f7a5cad7":"## training set and test set split\n","36eecb23":"## Outlier handling"}}