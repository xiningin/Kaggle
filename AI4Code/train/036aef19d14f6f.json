{"cell_type":{"436cf3bb":"code","a7f9354b":"code","d12f351b":"code","3cd3d201":"code","d26f8e11":"code","5b053231":"code","3ad22046":"code","f97548e6":"code","3a3137bd":"code","81c8dd1b":"code","1cbf9f5c":"code","6062bbd0":"code","bdc32497":"code","c30a70c2":"code","2fc77d7c":"code","bba011e0":"code","1aad6e7e":"code","ddc95070":"code","bf651e85":"code","a30aa5d4":"code","c2886b62":"code","df477e65":"code","42ebb8f0":"code","a5213fc6":"code","0980c4a2":"code","c4497b2c":"code","2e3381c8":"code","9a23e1b6":"code","822c62c6":"code","ce090b03":"code","103bb9d3":"code","b36fefa8":"code","c5e74a98":"code","1fd31996":"code","b3495250":"code","59adf68e":"code","392631ee":"code","6be745ad":"code","2579db32":"markdown","fa865b1c":"markdown","b2d5021b":"markdown","4fee3cfb":"markdown","7580f766":"markdown","8bf2e30b":"markdown","e8ffb574":"markdown","232387f6":"markdown","1ced6b16":"markdown","dcb39975":"markdown"},"source":{"436cf3bb":"import pandas as pd\nimport numpy as np","a7f9354b":"# Input from csv\ndf = pd.read_csv('..\/input\/voted-kaggle-dataset.csv')\n\n# sample data\nprint(df['Description'][0])","d12f351b":"df['Title'][0]","3cd3d201":"df.columns","d26f8e11":"df.head()","5b053231":"# shape of data frame\nlen(df)","3ad22046":"# is there any NaN values\ndf.isnull().sum()","f97548e6":"# nan value in Description\ndf.Description.isnull().sum()","3a3137bd":"df.Tags[0]","81c8dd1b":"#REMOVE NaN VALUES\ndf['Description'].dropna(inplace=True,axis=0)\n\n# check if there is any NaN values\ndf.Description.isnull().sum()","1cbf9f5c":"# REMOVE EMPTY STRINGS:\nblanks = []  # start with an empty list\n\nfor rv in df['Description']:  # iterate over the DataFrame\n    if type(rv)==str:            # avoid NaN values\n        if rv.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\nprint(blanks)\ndf['Description'].drop(blanks, inplace=True)","6062bbd0":"from nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\npattern = r'\\b[^\\d\\W]+\\b'\n# \\b is word boundry\n# [^] is neget\n# \\d is digit and \\W is not word\n\n#tokenize from nltk\ntokenizer = RegexpTokenizer(pattern)\n#I created by myself\ndef tokenizer_man(doc,remove_stopwords=False):\n    doc_rem_puct = re.sub(r'[^a-zA-Z]',' ',doc)\n    words = doc_rem_puct.lower().split()    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))     \n        words = [w for w in words if not w in stops]\n    return words\n\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()","bdc32497":"#NTLK stopwords\n\n#check how many stopwords you have\nstops1=set(stopwords.words('english'))\nprint(stops1)\n#lenght of stopwords\nlen(stopwords.words('english'))","c30a70c2":"#adding new element to the set\nstops1.add('newWords') #newWord added into the stopwords\nprint(len(stops1))","2fc77d7c":"raw = str(df['Description'][0]).lower()\ntokens = tokenizer.tokenize(raw)\n\" \".join(tokens)\nlen(tokens)","bba011e0":"#test manual \nstring=df['Description'][0]\nvocab = tokenizer_man(string)\n\" \".join(vocab)\nlen(vocab)","1aad6e7e":"remove_words = ['data','dataset','datasets','content','context','acknowledgement','inspiration']","ddc95070":"# list for tokenized documents in loop\ntexts = []\n\n# loop through document list\nfor i in df['Description'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # remove stop words from tokens\n    stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n    \n    # remove word containing only single char\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n    \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\nprint(texts[0])","bf651e85":"len(texts)","a30aa5d4":"df['desc_preprocessed'] = \"\"\nfor i in range(len(texts)):\n    df['desc_preprocessed'][i] = ' '.join(map(str, texts[i]))","c2886b62":"print(df['desc_preprocessed'][0])","df477e65":"df.shape","42ebb8f0":"df.columns","a5213fc6":"from sklearn.feature_extraction.text import TfidfVectorizer","0980c4a2":"tfidf = TfidfVectorizer(max_df=0.9,min_df=2,stop_words='english')","c4497b2c":"dtm = tfidf.fit_transform(df['desc_preprocessed'])\n\ndtm","2e3381c8":"from sklearn.decomposition import NMF,LatentDirichletAllocation","9a23e1b6":"nmf_model = NMF(n_components=7,random_state=42)\nnmf_model.fit(dtm)","822c62c6":"LDA = LatentDirichletAllocation(n_components=7,random_state=42)\nLDA.fit(dtm)","ce090b03":"len(tfidf.get_feature_names())","103bb9d3":"# words for NMF modeling\nfor index,topic in enumerate(nmf_model.components_):\n    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","b36fefa8":"# words for LDA modeling\nfor index,topic in enumerate(LDA.components_):\n    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","c5e74a98":"topic_results = nmf_model.transform(dtm)\ndf['NMF_Topic'] = topic_results.argmax(axis=1)","1fd31996":"LDA_topic_results = LDA.transform(dtm)\ndf['LDA_Topic'] = LDA_topic_results.argmax(axis=1)","b3495250":"mytopic_dict = {0:'public',\n                1:'sports',\n                2:'machine_learning',\n                3:'neuron_network',\n                4:'politic',\n                5:'economy',\n                6:'text analysis'\n               }\n\ndf['topic_label_NMF']=df['NMF_Topic'].map(mytopic_dict)","59adf68e":"df.head(-5)","392631ee":"df['LDA_Topic'].unique()","6be745ad":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nfirst_topic = nmf_model.components_[0]\nfirst_topic_words = [tfidf.get_feature_names()[i] for i in first_topic.argsort()[:-15 - 1 :-1]]\n\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4000,\n                          height=2500\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","2579db32":"## Read the data","fa865b1c":"# Data preprocessing","b2d5021b":"# 1.Initiating Tokenizer and Lemmatizer\n\nInitiate the tokenizer, stop words, and lemmatizer from the libraries.\n\n* Tokenizer is used to split the sentences into words.  \n* Lemmatizer (a quite similar term to Stemmer) is used to reduce words to its base form.   \nThe simple difference is that Lemmatizer considers the meaning while Stemmer does not. \n","4fee3cfb":"# LDA modelling","7580f766":"# Topic Modeling \n\nTopic modeling is a statistical model to discover the abstract \"topics\" that occur in a collection of documents.  \nI will be focusing on two medhods seen as follows. \n* LDA \n* Non-negative matrix factorization  ","8bf2e30b":"# Non-negative Matrix Factorization","e8ffb574":"# Import libraries\n\nI used LDA model from sklearn. Other option is using gensim.","232387f6":"# Feature Extraction","1ced6b16":"## Perform Tokenization, Words removal, and Lemmatization","dcb39975":"# Displaying Topics "}}