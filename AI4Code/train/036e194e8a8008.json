{"cell_type":{"754de85a":"code","d10cdff1":"code","80f74684":"code","2edb7f67":"code","a85d1fef":"code","215c8023":"code","72ecc749":"code","914b8065":"code","59d6d80e":"code","e26d9fe7":"code","423e7277":"code","9c77ce1f":"code","de4638ba":"code","95b936ba":"code","baa032bc":"code","bb351f4a":"code","343e8c37":"code","6c99b036":"code","405fcdb3":"code","fac56845":"code","2ae18cd1":"code","2b1c4cb6":"code","9927a24c":"code","c3824e9f":"code","7782e41d":"code","f17885d9":"code","e3d41da7":"code","f19b6bce":"code","53b6c53b":"code","1286d794":"code","c1074d3c":"code","f79b0354":"code","65f70da4":"code","10061b04":"code","dd45590b":"code","4839aad7":"code","ecf28173":"code","c356deb8":"code","cdac6482":"code","5951aa71":"code","67286045":"code","e9249d28":"code","681c1993":"code","b9fd0e4d":"markdown","c2d415b2":"markdown","5fc30fa2":"markdown","4f915c51":"markdown","8b93cdea":"markdown","f161942a":"markdown","e3fffc89":"markdown"},"source":{"754de85a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d10cdff1":"import gym\nimport numpy as np\nimport sys\nfrom gym.envs.toy_text import discrete\nfrom collections import defaultdict, namedtuple\nimport itertools\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport io\nimport random","80f74684":"UP = 0\nRIGHT = 1\nDOWN = 2\nLEFT = 3\nUP_RIGHT = 4\nRIGHT_DOWN = 5\nDOWN_LEFT = 6\nLEFT_UP = 7\n\nACAO = [[-1, 0],[0, 1],[1, 0],[0, -1],[-1, 1],[1, 1],[1, -1],[-1, -1]]","2edb7f67":"class WindyGridworldEnv(discrete.DiscreteEnv):\n\n    metadata = {'render.modes': ['human', 'ansi']}\n    #Limita as coodenadas em 'coord' ao shape de self\n    def _limit_coordinates(self, coord):\n        coord[0] = min(coord[0], self.shape[0] - 1)\n        coord[0] = max(coord[0], 0)\n        coord[1] = min(coord[1], self.shape[1] - 1)\n        coord[1] = max(coord[1], 0)\n        return coord\n    \n\n    def _calculate_transition_prob(self, current, delta, winds): # posicao, dire\u00e7\u00e3o, ventos e define a recompensa\n        # soma posicao atual com direcao e vento (direcao * intensidade)\n        new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)] # efeito do vento aqui\n        new_position = self._limit_coordinates(new_position).astype(int)\n        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n        #is_done = tuple(new_position) == (3, 7)   #??????? Posi\u00e7\u00e3o final?\n        is_done = tuple(new_position) == self.terminal_state   #??????? Posi\u00e7\u00e3o final?\n        return [(1.0, new_state, (5.0 if is_done else -1.0), is_done)]  #  P[s][a] == [(probability, nextstate, reward, done), ...]\n\n    def __init__(self,terminal_state, shape):\n        \n        self.terminal_state = terminal_state\n        self.shape = shape  # define o formato\n\n        nS = np.prod(self.shape) # = linhas * colunas nr de estados\n        nA = 8 # nr de a\u00e7\u00f5es\n        \n        # define os ventos\n        def _winds():\n\n            # estabelece valores aleatorios para o vento\n            r = random.choice([-1,0,1])\n            winds = np.full(self.shape,r)\n\n            # com valor zero seta algumas colunas com valores especificos\n            if r == 0:\n                winds[:,[3,4,5,8]] = 1\n                winds[:,[6,7]] = 2\n            #print(winds)\n            return winds\n            \n\n        P = {}\n        for s in range(nS): # percorre todos os estados\n            position    = np.unravel_index(s, self.shape) # Transforma estado em coordenadas ((n \/\/ nr_colunas), (n % nr_colunas))\n            P[s]        = { a : [] for a in range(nA) }  # {0: [], 1: [], 2: [], 3: []}\n            P[s][UP]    = self._calculate_transition_prob(position, [-1, 0], _winds())\n            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], _winds())\n            P[s][DOWN]  = self._calculate_transition_prob(position, [1, 0], _winds())\n            P[s][LEFT]  = self._calculate_transition_prob(position, [0, -1], _winds())\n            P[s][UP_RIGHT]    = self._calculate_transition_prob(position, [-1, 1], _winds())\n            P[s][RIGHT_DOWN] = self._calculate_transition_prob(position, [1, 1], _winds())\n            P[s][DOWN_LEFT]  = self._calculate_transition_prob(position, [1, -1], _winds())\n            P[s][LEFT_UP]  = self._calculate_transition_prob(position, [-1, -1], _winds())\n\n        isd = np.zeros(nS) # cria um vetor isd\n        \n        # Transforma coordenadas (3,0) em posicao no vetor e seta para 1\n        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0 \n        \n        # chama a superclasse e inicializa com parametros\n        super(WindyGridworldEnv, self).__init__(nS, nA, P, isd) \n        \n    def render(self, mode='human', close=False):\n        self._render(mode, close)\n\n    def _render(self, mode='human', close=False):\n        if close:\n            return\n\n        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n\n        for s in range(self.nS):\n            position = np.unravel_index(s, self.shape)\n            # print(self.s)\n            if self.s == s:\n                output = \" x \"\n            elif position == (3,7):\n                output = \" T \"\n            else:\n                output = \" o \"\n\n            if position[1] == 0: # se primeira coluna\n                output = output.lstrip()\n            if position[1] == self.shape[1] - 1: # se ultima coluna\n                output = output.rstrip()\n                output += \"\\n\"\n\n            outfile.write(output)\n        outfile.write(\"\\n\")","a85d1fef":"env = WindyGridworldEnv(terminal_state = (3, 7), shape = (7, 10) )\n\nprint(env.reset())\nenv.render()\nprint(env.step(1))\nenv.render()\n'''\nprint(env.step(1))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n\nprint(env.step(2))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n'''\n","215c8023":"# Para MC deve se executar n vezes e depois usar a historia.\n# Deve-se guardar estado inicial, acao, recompensa, estado final\n\ndef generateEpisode(p_init_state):\n    episode = []\n    observation = p_init_state\n    passos = 0\n    recompensa = 0\n    while True:\n        init_state = np.unravel_index(observation, env.shape)\n        action = env.action_space.sample() # pega acao aleatoria\n        observation, reward, done, info = env.step(action) # pega resultado da acao\n        final_state = np.unravel_index(observation, env.shape)\n        episode.append([list(init_state), action, reward, list(final_state)])\n        passos += 1 # conta os passos\n        recompensa += reward # soma as recompensas\n        \n        #if reward >  -1:\n            #print('reward maior que -1: ' + str(reward))\n        #env.render() # plota os mapas\n        if done:\n          #print(\"Epis\u00f3dio terminou depois de {} passos\".format(t+1))\n          break\n\n    return episode,passos,recompensa","72ecc749":"env = WindyGridworldEnv(terminal_state = (3, 7), shape = (7, 10) ) # inicializacao\ninit_state = env.reset()\nenv.close()","914b8065":"#generateEpisode(p_init_state=init_state)","59d6d80e":"def testaLimites(position, grid_size):\n    return (np.array(position)<0).any() or (np.array(position)>=np.array(grid_size)).any()","e26d9fe7":"# terminal_states_dict = estados terminais e seus valores\n# value_function = fun\u00e7\u00e3o de valor\n# grid_size = tamanho do grid\n# actions = lista de a\u00e7\u00f5es\ndef get_greedy_policy(terminal_state, value_function, grid_size,  actions):\n    \n    # cria lista de estados ([linha,coluna] [...]...)\n    grid_indexes = [np.arange(val) for val in grid_size]\n    states = np.array(np.meshgrid(*grid_indexes)).T.reshape(-1, 2)\n  \n    # para cada posicao no grid, pega a dire\u00e7\u00e3o de maior ganho\n    greedy_policy = {tuple(state): list() for state in states}\n  \n    for state in states:\n        biggest_value = float('-inf')\n        if not (state == np.asarray(terminal_state)).all():\n            for action in actions:\n                position = np.array(state) + np.array(action)\n                #if is_out_of_the_grid(position, grid_size):\n                if testaLimites(position,grid_size):\n                    continue\n\n                i, j = position\n                value = round(value_function[i][j], 8)\n\n                if value >= biggest_value:\n                    if value > biggest_value:\n                        greedy_policy[state[0], state[1]].clear()\n                    greedy_policy[state[0], state[1]].append(action)\n                    biggest_value = value\n\n    return greedy_policy","423e7277":"import random\ndef atualiza_posicao(posicao,value_function, acoes, grid_size, terminal_state, lista_posicoes):\n    max = float(\"-inf\")\n    pesos = []\n    posicoes = []\n    #print(4)\n    #print(posicao)\n    for acao in acoes:\n        \n        nova_posicao = tuple(np.array(posicao) + acao)\n        #print(5)\n        if testaLimites(nova_posicao,grid_size):\n            #print(6)\n            continue\n      #  elif (nova_posicao in np.array(lista_posicoes)):\n            #print(7)\n            #continue\n        else:\n            #print(1)\n            #print(nova_posicao)\n            peso = value_function[nova_posicao[0],nova_posicao[1]]\n            pesos.append(peso)\n            posicoes.append(nova_posicao)\n            #print(2)\n            #print(nova_posicao,peso)\n\n    posicao_melhor = random.choices(posicoes, weights=[x-2*min(pesos)+1 for x in pesos])[0]\n    \n    #print(posicoes,pesos,posicao_melhor)\n    #print(posicao_melhor)\n    \n    if posicao_melhor == terminal_state:\n        reward = 5.0\n    else:\n        reward = -1\n        \n    return reward,posicao_melhor\n            ","9c77ce1f":"def greedyMC(init_state,value_function,acoes,grid_size,terminal_state):\n    #print(1)\n    #init_state = np.unravel_index(env.reset(), env.shape)\n    posicao = np.unravel_index(env.reset(), env.shape)\n    posicao_final = env.terminal_state\n    contador = 0\n    passos = [posicao]\n    recompensa = 0\n    #print(posicao, posicao_final)\n    while (posicao != posicao_final) and contador < 1000:\n        reward,posicao = atualiza_posicao(posicao,value_function,acoes,grid_size,terminal_state,passos)\n        contador += 1\n        recompensa += reward\n        #print(contador,recompensa)\n        passos.append(posicao)\n        if reward ==0:\n            return contador,-1*contador\n        #if (posicao == posicao_final):\n    #print('chegou')\n    return contador,recompensa","de4638ba":"def run_monte_carlo(iteracoes,alpha,init_state):\n    value_function = np.zeros(env.shape)\n    # O epis\u00f3dio \u00e9 uma lista com os passos do agente.\n    stats = [[],[],[],[],[]]\n    caminho = []\n    for iteracao in range(iteracoes):\n        episode,passos_episodio,recompensa_episodio = generateEpisode(p_init_state=init_state)\n        caminho.append(episode)\n        G = 0\n        #print(episode)\n        episode = episode[::-1] # inverte a lista\n        for i, (initial_position, action, reward, final_position) in enumerate(episode):\n            G += reward  # Usa a propriedade recursiva do horizonte sem desconto\n            #print('G = ' + str(G))\n            # First-visit MC: s\u00f3 considera o retorno obtido se o estado n\u00e3o foi visitado\n            past_positions = [x[0] for x in episode[i+1:]]\n            if initial_position not in past_positions:\n                position_tuple = tuple(initial_position)\n                #print('value = ' + str(value_function[position_tuple] + alpha * (G - value_function[position_tuple])) + 'posicao= ' + str(position_tuple))\n                value_function[position_tuple] = value_function[position_tuple] + alpha * (G - value_function[position_tuple])\n        value_function[env.terminal_state] = 5.0 # Recompensa na chegada\n\n        #passos_greedy,recompensa_greedy = greedyMC(init_state,value_function,ACAO,env.shape,env.terminal_state)\n        greedy_policy = get_greedy_policy(env.terminal_state, value_function, env.shape,  ACAO)\n        inicio = np.unravel_index(init_state, env.shape)\n        passos_greedy, recompensa_greedy, end = f_recompensa_greedy(inicio,greedy_policy,env.shape, value_function)\n        print(iteracao, end='\\r')\n        #if iteracao in (0,1,10,100,1000):\n            #print(' ')\n            #print(value_function)\n        if not end:\n            continue\n        stats[0].append(iteracao)\n        stats[1].append(passos_episodio)\n        stats[2].append(recompensa_episodio)    \n        stats[3].append(passos_greedy)\n        stats[4].append(recompensa_greedy)\n    \n    return value_function,stats\n        \n","95b936ba":"#inicio = np.unravel_index(env.reset(), env.shape)\n#a = get_greedy_policy(env.terminal_state, value_function, env.shape,  ACAO)\ndef f_recompensa_greedy(inicio,greedy_policy,tamanho, value_function):\n    posicao = inicio\n    recompensa = 0\n    passos = 0\n    end = False\n    #print(inicio)\n    lista_visitados = []\n    while len(greedy_policy[posicao]) > 0:\n        posicoes = np.array(posicao) + greedy_policy[posicao]\n        novas_posicoes = [item for item in posicoes.tolist() if item not in lista_visitados and not testaLimites(item, tamanho)]\n        if len(novas_posicoes) == 0:\n            break\n        posicao = tuple(random.choice(novas_posicoes))\n        \n        if posicao == env.terminal_state:\n            recompensa += 5\n            end = True\n            #print('***************************')\n        else:\n            recompensa += -1    \n\n        lista_visitados.append(list(posicao))\n        passos +=1\n        #print(posicoes,posicao)\n    return passos, recompensa, end\n","baa032bc":"##########################################################\nvalue_function,stats=run_monte_carlo(50000,0.1,env.reset())\n##########################################################","bb351f4a":"a = get_greedy_policy(env.terminal_state, value_function, env.shape,  ACAO)","343e8c37":"#a","6c99b036":"value_function","405fcdb3":"[item for item in (np.array((1,2))+[[1,1],[0,0]]).tolist() if item not in [[1,2],[3,4]]]","fac56845":"(np.array((1,2))+[[1,1],[0,0]]).tolist()","2ae18cd1":"fig1 = plt.figure(figsize=(10,5))\n#plt.xscale('log')\nplt.scatter(stats[0],stats[4])\nplt.xlabel(\"N\u00famero de episodios de explora\u00e7\u00e3o\")\nplt.ylabel(\"Recompensa com a\u00e7\u00e3o greedy\")\nplt.title(\"Recompensa com a\u00e7\u00e3o greedy vs n\u00famero de episodios de explora\u00e7\u00e3o\")","2b1c4cb6":"fig1 = plt.figure(figsize=(10,5))\n#plt.xscale('log')\nplt.scatter(stats[0],[a\/b for a,b in zip(stats[4],stats[3])])\nplt.xlabel(\"N\u00famero de episodios de explora\u00e7\u00e3o\")\nplt.ylabel(\"Recompensa m\u00e9dia por passo com a\u00e7\u00e3o greedy\")\nplt.title(\"Recompensa m\u00e9dia com a\u00e7\u00e3o greedy vs n\u00famero de episodios de explora\u00e7\u00e3o\")","9927a24c":"fig1 = plt.figure(figsize=(10,5))\n#plt.xscale('log')\nplt.scatter(stats[0],stats[3])\nplt.xlabel(\"N\u00famero de episodios de explora\u00e7\u00e3o\")\nplt.ylabel(\"passos com a\u00e7\u00e3o greedy\")\nplt.title(\"Passos com a\u00e7\u00e3o greedy vs n\u00famero de episodios de explora\u00e7\u00e3o\")","c3824e9f":"posicao = np.unravel_index(env.reset(), env.shape)\nlista=[posicao]","7782e41d":"print(np.flip([env.shape[0],0]+[-1,1]*(np.array(posicao)+[0,1])))\nprint(value_function)\nreward,posicao = atualiza_posicao(posicao,value_function, ACAO, env.shape, env.terminal_state, lista)\nlista.append(posicao)\nprint(np.flip([env.shape[0],0]+[-1,1]*(np.array(posicao)+[0,1])))","f17885d9":"for lista","e3d41da7":"print(np.flip([env.shape[0],0]+[-1,1]*(np.array(posicao)+[0,1])))","f19b6bce":"import random\nrandom.choices( [1,2,3,4,5], weights=(10, 20, 30, 40, 50))","53b6c53b":"#greedyMC(env.reset,value_function,ACAO,env.shape,env.terminal_state)","1286d794":"# retorna uma fun\u00e7\u00e3o cujo argumento \u00e9 o estado\ndef make_epsilon_greedy_policy(Q, epsilon, nA):\n    def policy_fn(observation):\n        A = np.ones(nA, dtype=float) * epsilon \/ nA\n        best_action = np.argmax(Q[observation])\n        A[best_action] += (1.0 - epsilon)\n        return A\n    return policy_fn","c1074d3c":"def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    \n    Stats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n    stats = Stats(\n        episode_lengths=np.zeros(num_episodes),\n        episode_rewards=np.zeros(num_episodes))\n    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n    \n    for i_episode in range(num_episodes):\n        if (i_episode + 1) % 100 == 0:\n            print(\"\\rEpis\u00f3dio {}\/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n            sys.stdout.flush()\n        \n        state = env.reset()\n        action_probs = policy(state)\n        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n        \n        for t in itertools.count():\n            next_state, reward, done, _ = env.step(action)\n            # probabilidade das proximas acoes \n            next_action_probs = policy(next_state) \n            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n            \n            stats.episode_rewards[i_episode] += reward\n            stats.episode_lengths[i_episode] = t\n            \n            td_target = reward + discount_factor * Q[next_state][next_action]\n            td_delta = td_target - Q[state][action]\n            Q[state][action] += alpha * td_delta\n    \n            if done:\n                break\n                \n            action = next_action\n            state = next_state        \n    \n    return Q, stats","f79b0354":"env = WindyGridworldEnv(terminal_state = (3, 7), shape = (7, 10))\nQ, stats = sarsa(env, 200)","65f70da4":"def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n    fig1 = plt.figure(figsize=(10,5))\n    plt.plot(stats.episode_lengths)\n    plt.xlabel(\"Epis\u00f3dio\")\n    plt.ylabel(\"Comprimento do epis\u00f3dio\")\n    plt.title(\"Comprimento do epis\u00f3dio no tempo\")\n    if noshow:\n        plt.close(fig1)\n    else:\n        plt.show(fig1)\n\n    fig2 = plt.figure(figsize=(10,5))\n    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n    plt.plot(rewards_smoothed)\n    plt.xlabel(\"Epis\u00f3dio\")\n    plt.ylabel(\"Recompensa por epis\u00f3dio (suavizada)\")\n    plt.title(\"Recompensa por epis\u00f3dio no tempo (Suavizada com uma janela de tamanho {})\".format(smoothing_window))\n    if noshow:\n        plt.close(fig2)\n    else:\n        plt.show(fig2)\n\n    fig3 = plt.figure(figsize=(10,5))\n    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n    plt.xlabel(\"Passos no tempo\")\n    plt.ylabel(\"Epis\u00f3dio\")\n    plt.title(\"Epis\u00f3dios por passos no tempo\")\n    if noshow:\n        plt.close(fig3)\n    else:\n        plt.show(fig3)\n\n    return fig1, fig2, fig3","10061b04":"plot_episode_stats(stats)","dd45590b":"def q_learning(env, num_episodes, discount_factor = 1.0,\n               alpha = 0.6, epsilon = 0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n   \n    Stats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n    stats = Stats(\n        episode_lengths=np.zeros(num_episodes),\n        episode_rewards=np.zeros(num_episodes))  \n       \n    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n       \n    for ith_episode in range(num_episodes):\n        if (ith_episode + 1) % 100 == 0:\n            print(\"\\rEpis\u00f3dio {}\/{}.\".format(ith_episode + 1, num_episodes), end=\"\")\n            sys.stdout.flush()\n\n        state = env.reset()\n           \n        for t in itertools.count():\n            action_probabilities = policy(state)\n\n            action = np.random.choice(np.arange(\n                      len(action_probabilities)),\n                       p = action_probabilities)\n   \n            next_state, reward, done, _ = env.step(action)\n   \n            stats.episode_rewards[ith_episode] += reward\n            stats.episode_lengths[ith_episode] = t\n               \n            best_next_action = np.argmax(Q[next_state])    \n            td_target = reward + discount_factor * Q[next_state][best_next_action]\n            td_delta = td_target - Q[state][action]\n            Q[state][action] += alpha * td_delta\n   \n            if done:\n                break\n                   \n            state = next_state\n       \n    return Q, stats","4839aad7":"Q, stats = q_learning(env, 1000)","ecf28173":"plot_episode_stats(stats)","c356deb8":"UP = 0\nRIGHT = 1\nDOWN = 2\nLEFT = 3\n\nclass CliffWalkingEnv(discrete.DiscreteEnv):\n\n    metadata = {'render.modes': ['human', 'ansi']}\n\n    def _limit_coordinates(self, coord):\n        coord[0] = min(coord[0], self.shape[0] - 1)\n        coord[0] = max(coord[0], 0)\n        coord[1] = min(coord[1], self.shape[1] - 1)\n        coord[1] = max(coord[1], 0)\n        return coord\n\n    def _calculate_transition_prob(self, current, delta):\n        new_position = np.array(current) + np.array(delta)\n        new_position = self._limit_coordinates(new_position).astype(int)\n        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n        reward = -100.0 if self._cliff[tuple(new_position)] else -1.0\n        is_done = self._cliff[tuple(new_position)] or (tuple(new_position) == (3,11))\n        return [(1.0, new_state, reward, is_done)]\n\n    def __init__(self):\n        self.shape = (4, 12)\n\n        nS = np.prod(self.shape)\n        nA = 4\n\n        self._cliff = np.zeros(self.shape, dtype=np.bool)\n        self._cliff[3, 1:-1] = True\n\n        P = {}\n        for s in range(nS):\n            position = np.unravel_index(s, self.shape)\n            P[s] = { a : [] for a in range(nA) }\n            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n\n        isd = np.zeros(nS)\n        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\n\n        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n\n    def render(self, mode='human', close=False):\n        self._render(mode, close)\n\n    def _render(self, mode='human', close=False):\n        if close:\n            return\n\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n\n        for s in range(self.nS):\n            position = np.unravel_index(s, self.shape)\n            # print(self.s)\n            if self.s == s:\n                output = \" x \"\n            elif position == (3,11):\n                output = \" T \"\n            elif self._cliff[position]:\n                output = \" C \"\n            else:\n                output = \" o \"\n\n            if position[1] == 0:\n                output = output.lstrip() \n            if position[1] == self.shape[1] - 1:\n                output = output.rstrip() \n                output += \"\\n\"\n\n            outfile.write(output)\n        outfile.write(\"\\n\")","cdac6482":"env = CliffWalkingEnv()\n\nprint(env.reset())\nenv.render()\n\nprint(env.step(0))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n\nprint(env.step(1))\nenv.render()\n\nprint(env.step(2))\nenv.render()","5951aa71":"env = CliffWalkingEnv()\nQ, stats = q_learning(env, 1000)","67286045":"plot_episode_stats(stats)","e9249d28":"Q, stats = sarsa(env, 1000)","681c1993":"plot_episode_stats(stats)","b9fd0e4d":"## Monte Carlo","c2d415b2":"## Windy Gridworld","5fc30fa2":"## Q-learning e Sarsa no The Cliff","4f915c51":"# TD\n\nmo monte carlo VS \u00e9 calculado em funcao do que vai ganhar nos passos futuros da trajetoria.\nO valor do estado \u00e9 a soma das recompensas futuras\n\nLinks:\n* https:\/\/github.com\/dennybritz\/reinforcement-learning.git\n* https:\/\/pythonprogramming.net\/q-learning-reinforcement-learning-python-tutorial\/\n\nhttps:\/\/ai-mrkogao.github.io\/reinforcement%20learning\/openaigymtutorial\/\n\nhttps:\/\/towardsdatascience.com\/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n\nhttps:\/\/blog.paperspace.com\/getting-started-with-openai-gym\/\n\nhttps:\/\/www.deep-teaching.org\/notebooks\/reinforcement-learning\/exercise-monte-carlo-frozenlake-gym\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/reinforcement-learning-introduction-monte-carlo-learning-openai-gym\/\n\nhttps:\/\/ailever.readthedocs.io\/en\/latest\/_modules\/gym\/envs\/toy_text\/discrete.html\n","8b93cdea":"1. Este exerc\u00edcio refere-se a uma compara\u00e7\u00e3o do desempenho de Monte Carlo, Sarsa e Q-learning em uma vers\u00e3o do cen\u00e1rio do Windy Gridworld.\n\nUm windy gridworld onde o agente tamb\u00e9m pode se mover pelas diagonais \u00e9 chamado de **Windy Gridworld with King's Moves**. Quando o efeito do vento \u00e9 estoc\u00e1stico, o cen\u00e1rio \u00e9 chamado de **Stochastic Windy Gridworld**. Neste cen\u00e1rio, o vento, ao inv\u00e9s de ter um impacto definido deterministicamente, tem um comportamento estoc\u00e1stico: pode mover o agente 1 c\u00e9lula para cima, 1 c\u00e9lula para baixo ou manter o valor de vento especificado na coluna. Cada uma dessas situa\u00e7\u00f5es tem uma probabilidade 1\/3 de acontecer.\n\nImplemente uma vers\u00e3o do **Stochastic Windy Gridworld with King's Moves** com base na classe do Windy Gridworld de exemplo apresentada em sala de aula. Fa\u00e7a an\u00e1lises para responder \u00e1s seguintes quest\u00f5es:\n* Qual dos tr\u00eas algoritmos performou melhor no cen\u00e1rio? Houve diferen\u00e7as significativas na velocidade da converg\u00eancia dos algoritmos?\n* Considere o melhor algoritmo dentre os tr\u00eas e fa\u00e7a uma an\u00e1lise sobre o impacto de ter mais a\u00e7\u00f5es a disposi\u00e7\u00e3o para o agente. As a\u00e7\u00f5es adicionais aceleram a converg\u00eancia do algoritmo?\n\n**Dica: a estocasticidade do vento tem que ser especificada ou na inicializa\u00e7\u00e3o atrav\u00e9s da estrutura de probabilidade de transi\u00e7\u00e3o, ou tem que ser simulada em alguma fun\u00e7\u00e3o que seja chamada em tempo de intera\u00e7\u00e3o do agente com o ambiente**","f161942a":"## Sarsa (TD(0) para estado-a\u00e7\u00e3o) no Windy Gridworld","e3fffc89":"## Q-learning no Windy Gridworld"}}