{"cell_type":{"3053de11":"code","2715f5bc":"code","81dc25d1":"code","c6ce4a5c":"code","d3845ab9":"code","07ca6a1c":"code","97157d73":"code","fffbc2df":"code","2549e803":"code","e4a8a88d":"code","6b600184":"code","823f8bee":"code","b91d52b4":"code","aa1ce5bf":"code","ed17a2e4":"code","96d8a9a6":"code","7f80ec97":"code","ebc7b8bb":"code","7e92fe87":"code","c416921c":"code","f667161b":"code","b576e9d2":"code","ce8a95cf":"code","84801033":"code","047b9a4a":"code","0fd31336":"code","9fbd8ed5":"code","94a0846a":"code","c2fc75bb":"code","c2ee074f":"code","d120423a":"code","f290ed54":"code","1f98f53a":"code","56e4f6a6":"code","0a39a795":"code","ef9c23c5":"code","65a1a106":"code","25f3717f":"code","6beadc12":"code","b836cff9":"code","de335a4d":"code","811620ac":"code","ecf36848":"code","5de96264":"code","33fa37af":"code","8f9b9252":"code","5afe151b":"code","51bc7cfd":"code","dea549fa":"code","c7798575":"code","234bc2aa":"code","f9a40586":"code","32f27c7c":"code","3bddbafd":"code","9b37fb6d":"code","96e16d54":"code","d7a5b006":"code","2ad1ac04":"code","210ef743":"code","6b42bf36":"code","415c4860":"code","cb7931d7":"code","59e42e11":"code","fc7b8d57":"code","2b0f7d2c":"code","fa7296a9":"code","6c7445e1":"code","39ad94f0":"code","b9d36a13":"markdown","c1fa7a2d":"markdown","d450a21b":"markdown","50188654":"markdown","6e2abed8":"markdown","dd6be487":"markdown","3e8e116b":"markdown","4b3af9b1":"markdown","f8b02638":"markdown","66d3e86b":"markdown","76d65641":"markdown","28f4c0de":"markdown","155c35d4":"markdown","0adc0c74":"markdown","47a852ba":"markdown","faa29901":"markdown","97d86e0f":"markdown","dce9de43":"markdown","abf981d6":"markdown","59026ae9":"markdown","9e2888a7":"markdown","729033a8":"markdown","90e91b95":"markdown","35188656":"markdown","ce58669b":"markdown","df7fff57":"markdown","8ecb0a1b":"markdown","e8243f80":"markdown","08ae7af5":"markdown","bc7ecc7f":"markdown","1e46b0fa":"markdown","3af78424":"markdown","62614333":"markdown","c247eaef":"markdown","b197717c":"markdown","e795f02f":"markdown","4c79611e":"markdown","5585fe1e":"markdown"},"source":{"3053de11":"import os\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data.dataloader import default_collate\n# from torchvision.datasets import ImageFolder\nimport torchvision.transforms as tt\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()","2715f5bc":"data_path = '\/kaggle\/input\/coronahack-chest-xraydataset\/'\nimg_path = data_path + 'Coronahack-Chest-XRay-Dataset\/Coronahack-Chest-XRay-Dataset'\ntrain_img_dir = img_path + '\/train'\ntest_img_dir = img_path + '\/test'\nimg_dir = os.listdir(img_path)\ndf_meta = pd.read_csv(data_path+'Chest_xray_Corona_Metadata.csv')\ndf_meta_summary = pd.read_csv(data_path+'Chest_xray_Corona_dataset_Summary.csv')\n","81dc25d1":"df_meta.tail()","c6ce4a5c":"df_meta.info()","d3845ab9":"missing_entries = df_meta.isnull().sum()","07ca6a1c":"missing_entries.plot(kind=\"barh\");","97157d73":"#replace null data points to 'unknown'\ndf_meta.fillna('unknown', inplace=True)\ndf_meta.isnull().sum()","fffbc2df":"print((df_meta['Label_1_Virus_category']).value_counts())\nprint((df_meta['Label_2_Virus_category']).value_counts())","2549e803":"train_data = df_meta[df_meta['Dataset_type']=='TRAIN']\ntest_data = df_meta[df_meta['Dataset_type']=='TEST']\nassert train_data.shape[0] + test_data.shape[0] == df_meta.shape[0]\nprint(f\"Shape of train data: {train_data.shape}\")\nprint(f\"Shape of test data: {test_data.shape}\")\n\ntrain_data.sample(5)","e4a8a88d":"test_data.sample(5)","6b600184":"sns.countplot(train_data['Label_1_Virus_category']);","823f8bee":"sns.countplot(train_data['Label_2_Virus_category']);","b91d52b4":"sns.countplot(test_data['Label_1_Virus_category']);","aa1ce5bf":"sns.countplot(test_data['Label_2_Virus_category']);","ed17a2e4":"train_data.loc[train_data['Label'].eq('Normal'), 'class'] = 'healthy';\ntrain_data.loc[(train_data['class'].ne('healthy') & train_data['Label_1_Virus_category'].eq('bacteria')), 'class'] = 'bacteria';\ntrain_data.loc[(train_data['class'].ne('healthy') & train_data['class'].ne('bacteria') & train_data['Label_2_Virus_category'].eq('COVID-19')), 'class'] = 'COVID-19';\ntrain_data.loc[(train_data['class'].ne('healthy') & train_data['class'].ne('bacteria') & train_data['class'].ne('COVID-19')), 'class'] = 'other';\n","96d8a9a6":"target_dict = {'healthy' : 0,\n               'bacteria' : 1,\n               'COVID-19' : 2,\n               'other' : 3}\ntrain_data['target'] = train_data['class'].map(target_dict);","7f80ec97":"sns.countplot(train_data['class']);","ebc7b8bb":"train_data.sample(10)","7e92fe87":"def plot_images(path,class_str,numdisplay):\n    fig, ax = plt.subplots(numdisplay,2, figsize=(15,2.5*numdisplay))\n    for row,file in enumerate(path):\n        image = plt.imread(file)\n#         print(image.shape)\n        ax[row,0].imshow(image, cmap=plt.cm.bone)\n        ax[row,1].hist(image.ravel(), 256, [0,256])\n        ax[row,0].axis('off')\n        if row == 0:\n            ax[row,0].set_title('Images')\n            ax[row,1].set_title('Histograms')\n    fig.suptitle('Class='+class_str,size=16)\n    plt.show()    \n","c416921c":"def display_class_images(img_path,dataset,train_or_test_str,classlabel,numdisplay):\n    path = dataset[dataset['class']==classlabel]['X_ray_image_name'].values\n    sample_path = path[:numdisplay]\n    img_dir = img_path+\"\/\"+train_or_test_str\n    sample_path = list(map(lambda x: os.path.join(img_dir,x), sample_path))\n    plot_images(sample_path,classlabel,numdisplay)\n\n","f667161b":"display_class_images(img_path,train_data,\"train\",\"healthy\",4)","b576e9d2":"display_class_images(img_path,train_data,\"train\",\"COVID-19\",4)","ce8a95cf":"display_class_images(img_path,train_data,\"train\",\"bacteria\",4)","84801033":"display_class_images(img_path,train_data,\"train\",\"other\",4)","047b9a4a":"class CustomDataSet(Dataset):\n    def __init__(self, main_dir,meta_data, transform):\n        self.main_dir = main_dir\n        self.meta_data = meta_data\n        self.transform = transform\n        self.total_imgs = os.listdir(main_dir)\n\n    def __len__(self):\n        return len(self.meta_data)\n\n    def __getitem__(self, idx):\n        meta_data = self.meta_data.iloc[idx] \n        filename = meta_data['X_ray_image_name']\n        try:\n            file_idx = self.total_imgs.index(filename)\n        except:\n            print(\"Data not found!\")\n            return None        \n        img_loc = os.path.join(self.main_dir, self.total_imgs[file_idx])\n        image = Image.open(img_loc).convert(\"RGB\")\n        image = image.resize((128,128))\n        tensor_image = self.transform(image)\n        tensor_label = torch.tensor(meta_data['target'].item())\n        return tensor_image, tensor_label\n\ndef my_collate(batch):\n    \"Puts each data field into a tensor with outer dimension batch size\"\n    batch = filter (lambda x:x is not None, batch)\n    return default_collate(list(batch))\n","0fd31336":"batch_size=32","9fbd8ed5":"calc_normalization_stats = False # Set this if you want to evaluate the stats","94a0846a":"stats = ((0.0093, 0.0093, 0.0092),(0.4827, 0.4828, 0.4828)) # std_dev and mean of images per channel. See below for evaluation.","c2fc75bb":"if calc_normalization_stats:\n    train_tfms = tt.ToTensor()\nelse:\n    train_tfms = tt.Compose([tt.RandomCrop(128, padding=8, padding_mode='edge'), tt.ToTensor(), tt.Normalize(*stats, inplace = True)])\ntest_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats, inplace = True)])","c2ee074f":"train_data.shape","d120423a":"train_ds, test_ds = train_test_split(train_data, test_size=0.25,random_state= 1, shuffle = True)\ntrain_ds, test_ds = train_ds.reset_index(drop=True), test_ds.reset_index(drop=True)\ntrain_ds.shape, test_ds.shape\n\n","f290ed54":"train_dataset = CustomDataSet(train_img_dir, train_ds, transform=train_tfms)    \ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,                            \n                          num_workers=0, pin_memory=True, collate_fn=my_collate)\n\ntest_dataset = CustomDataSet(train_img_dir, test_ds, transform=test_tfms)\ntest_loader = DataLoader(test_dataset , batch_size=2*batch_size, shuffle=False, \n                         num_workers=0, pin_memory=True, collate_fn=my_collate)","1f98f53a":"if calc_normalization_stats:\n    mean_per_batch = []\n    stdev_per_batch = []\n    num_batches = 0\n    for idx, (images,labels) in enumerate(train_loader):\n        mean_per_batch.append(torch.std_mean(images,[0,2,3])[1])\n        stdev_per_batch.append(torch.std_mean(images,[0,2,3])[0])\n\n    channel_std,channel_mean = torch.std_mean(torch.stack(mean_per_batch),0)\n    print(channel_std,channel_mean) ","56e4f6a6":"def show_batch(dl):\n    for images,labels in dl:\n        print(images.shape, labels.shape)\n        fig, ax = plt.subplots(figsize=(8,8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images,nrow=8).permute(1,2,0))\n        break","0a39a795":"show_batch(train_loader)","ef9c23c5":"show_batch(test_loader)","65a1a106":"len(train_loader)","25f3717f":"len(test_loader)","6beadc12":"def show_example(img, label):\n    print('Label: ', \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","b836cff9":"import torch.nn as nn\nimport torch.nn.functional as F","de335a4d":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item()\/len(preds))","811620ac":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","ecf36848":"# class CnnModel(ImageClassificationBase):\n#     def __init__(self):\n#         super().__init__()\n#         self.network = nn.Sequential(\n#             nn.Conv2d(3, 128, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2), # output: 256 x 64 x 64\n\n#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2), # output: 512 x 32 x 32\n\n#             nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2), # output: 1024 x 16 x 16\n\n#             nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.Conv2d(2048, 2048, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2), # output: 2048 x 8 x 8\n\n\n#             nn.Flatten(), \n#             nn.Linear(2048*8*8, 512),\n#             nn.ReLU(),\n#             nn.Linear(512, 128),\n#             nn.ReLU(),\n#             nn.Linear(128, 4))\n        \n#     def forward(self, xb):\n#         return self.network(xb)","5de96264":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)\n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)\n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(16), \n                                        nn.Flatten(), \n                                        nn.Linear(512, num_classes))\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out","33fa37af":"#model = ResNet9(3,4)\n#model","8f9b9252":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","5afe151b":"device = get_default_device()\ndevice","51bc7cfd":"train_loader = DeviceDataLoader(train_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","dea549fa":"@torch.no_grad()\ndef evaluate(model, test_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in test_loader]\n    return model.validation_epoch_end(outputs)\n\n# Simple fit function, was used with the simple CNN model:\n# def fit(epochs, lr, model, train_loader, test_loader, opt_func=torch.optim.SGD):\n#     history = []\n#     optimizer = opt_func(model.parameters(), lr)\n#     for epoch in range(epochs):\n#         # Training Phase \n#         model.train()\n#         train_losses = []\n#         for batch in train_loader:\n#             loss = model.training_step(batch)\n#             train_losses.append(loss)\n#             loss.backward()\n#             optimizer.step()\n#             optimizer.zero_grad()\n#         # Validation phase\n#         result = evaluate(model, test_loader)\n#         result['train_loss'] = torch.stack(train_losses).mean().item()\n#         model.epoch_end(epoch, result)\n#         history.append(result)\n#     return history\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n# More sophisticated fit function, with the following features:\n# Learning rate scheduling, weight decay, gradient clipping\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","c7798575":"# model = to_device(CnnModel(), device)\nmodel = to_device(ResNet9(3, 4), device)","234bc2aa":"load = False\n\nif load:\n    PATH = 'COVID-19_classification-resnet9.pth'\n    model.load_state_dict(torch.load(PATH))","f9a40586":"history = [evaluate(model,test_loader)]\nhistory","32f27c7c":"num_epochs = 25\nopt_func = torch.optim.Adam\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4","3bddbafd":"#history = fit(num_epochs, lr, model, train_loader, test_loader, opt_func)\nhistory += fit_one_cycle(num_epochs, max_lr, model, train_loader, test_loader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","9b37fb6d":"torch.save(model.state_dict(), 'COVID-19_classification-resnet9.pth')","96e16d54":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","d7a5b006":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","2ad1ac04":"plot_accuracies(history)","210ef743":"plot_losses(history)","6b42bf36":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","415c4860":"plot_lrs(history)","cb7931d7":"@torch.no_grad()\ndef get_all_preds_and_targets(model, loader):\n    all_preds = torch.tensor([])\n    all_targets = torch.tensor([])\n    for batch in loader:\n        images, labels = batch\n\n        outputs = model(images)\n        _, preds = torch.max(outputs, dim=1)\n        all_preds = torch.cat((all_preds, preds),dim=0)\n        all_targets = torch.cat((all_targets, labels),dim=0)\n    return all_preds, all_targets","59e42e11":"device = torch.device('cpu');\ntest_loader = DeviceDataLoader(test_loader, device);\nmodel = to_device(model, device);","fc7b8d57":"with torch.no_grad():\n    predictions, targets = get_all_preds_and_targets(model, test_loader)","2b0f7d2c":"sns.countplot(predictions.numpy());","fa7296a9":"sns.countplot(targets.numpy());","6c7445e1":"#let's print a classification report\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(targets, predictions))","39ad94f0":"con_mat = confusion_matrix(targets, predictions)\ncon_mat = con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (10,10))\nplt.title('CONFUSION MATRIX')\nsns.heatmap(con_mat, cmap='coolwarm',\n            yticklabels=['Healthy', 'Bacteria','COVID-19','other virus'],\n            xticklabels=['Healthy', 'Bacteria','COVID-19','other virus'],\n            annot=True);","b9d36a13":"#### A simple convolutional neural network\n\n*note*: This was my first attempt. Leave it here for reference, but commented out.","c1fa7a2d":"Test that (untrained) model gives (meaningless) predictions as output:","d450a21b":"The trend indicates that our model isn't overfitting quite yet.\n\nLet's also look at the evolution of the learning rate, as adjusted by the learning rate scheduler:","50188654":"## Project: Multiclass Classification of X-Ray Images\n\n*Disclaimer*: This is my first public Kaggle project. Therefore, comments are very welcome. \n\nThe first part (data loading and inspecting) was inspired by \n\nhttps:\/\/www.kaggle.com\/digvijayyadav\/deep-learning-and-transfer-learning-on-covid-19.\n\nThe latter part (neutral network) was guided by an amazing pyTorch tutorial by freeCodeCamp.org,\n\nhttps:\/\/www.youtube.com\/watch?v=GIsg-ZUy0MY&t=23737s .\n\nThanks!\n\n\n## Outline & Summary\n* We use the CoronaHack-Chest X-Ray-Dataset, https:\/\/www.kaggle.com\/praveengovi\/coronahack-chest-xraydataset, to classify the X-ray images in four classes: *healthy*, *bacteria*, *COVID-19* and *other*.\n* After investigating the data, we find that the provided test set does not distinguish between virus-caused pneumonia cases, i.e., we cannot separate between the ''*COVID-19*'' class and the ''*other*'' class in the test set. Therefore, we shall only make use of the available training data, and separate it into a training and a validation\/test set.\n* We then setup a convolutional neural network with residual blocks. Concretely, we use the ResNet9 architecture.\n* In addition, we employ the following techniques: *Data normalization*, *Data augmentation* (padding and random crop), *batch normalization*, *learning rate scheduling*, *weight decay* and *gradient clipping*. Note that the exact setup and hyperparameters for these techniques are not yet optimized, therefore, there may still be room for improvement in the performance. \n* We use the Adam optimizer.\n* After 25 training epochs, we evaluate the performance of the model. We achieve an accuracy of about 83%. We calculate the confusion matrix, and see that COVID-19 cases are identified correctly to 77%.\n\n\n","6e2abed8":"Now, calculate the confusion matrix:","dd6be487":"We only have 58 images for COVID-19 (which are all in the training set). Adding more data on this class would certainly improve the model performance.","3e8e116b":"After about 1h of training (25 epochs), the accuracy converges to about 83%.","4b3af9b1":"#### Load previously fitted parameters (if needed):","f8b02638":"## Train the model","66d3e86b":"We can now start the training. We use the Adam optimizer, which uses momentum and adaptive learning rates for faster training. We set the number of epochs to 25.","76d65641":"### Check the data batches ","28f4c0de":"*note*: We resized the images to 128 x 128 pixels. The original images are much larger. More information would be retained when taking a larger pixel size, to the disadvantage of a more CPU\/GPU-intensive training. This may lead to a better performance.","155c35d4":"## Outlook\n\nThere are certainly many possibilities for improvements:\n\n1. All hyperparameters have not been fine-tuned yet. One could also try more\/different settings for the data augmentation and normalization. Also, a different ratio between training and test data may lead to a better performance.\n\n2. Increasing the training set. Obviously, the number of COVID-19 images is a bit low. Adding more data should definitely improve the 77% identification rate.\n\n3. Try a different achitecture?\n\nDo you have more ideas? I would be happy to hear\/read your comments!\n","0adc0c74":"### Detour: Get statistics for data normalization\n\n(needed only in initial run)\n\nCalculate mean and standard deviation over all train images, later used for normalization (see above).","47a852ba":"Save the trained parameters:","faa29901":"Let's display some of the X-Ray images.","97d86e0f":"*Note:* A problem occurs in the test set where all virus-caused pneumonia are not further labeled as COVID-19 (i.e., all Label_2_Virus_category entries are missing\/unknown). We therefore don't know whether all virus-caused pneumonia cases are caused by COVID-19 or by another virus, and hence we cannot assess the accuracy with the validation set. For this reason, we discard the provided test set and split instead the training data into a training set and a validation set.","dce9de43":"## Setting up the Model Training","abf981d6":"For the training set we apply a randomized data augmentations: We pad each image by 8 pixels, then take a random crop of size 128 x 128 pixels. The padding is done in *edge* mode. \n\nFurthermore, we normalize the data, as described above.","59026ae9":"### Loading and Inspecting the Data\nLoad Covid-19 X-Ray dataset and create training and test data objects:","9e2888a7":"We cannot use a convenient DataLoader for images like *ImageFolder* from torchvision.datasets as the data is not ordered in subfolders named after the classes. In fact, the class labels are given in the metadata file, and the images are in another directory. We therefore write a custom dataset:","729033a8":"## Setting up Neural Network","90e91b95":"Some notes on the employed techniques:\n\n**Learning rate scheduling**: The learning rate is changed after every batch of training. We use the *one cycle learning rate policy* strategy, which starts at a low learning rate, gradually increasing it for about 30% of epochs, then gradually decreasing it to a very low learning rate until the end. After training we shall plot the learning rate as a function of training batch number.\n\n**Weight decay**: A regularization technique that prevents the weights from becoming too large. This is done by adding a penalty proportional to the weights magnitude to the loss function.\n\n**Gradient clipping**: This constrains the gradients to a limited range, thus preventing too large gradient values leading to drastic changes in the back-propagation.","35188656":"#### A model with residual blocks (ResNet9)\n\n![resnet-9](https:\/\/github.com\/lambdal\/cifar10-fast\/raw\/master\/net.svg?sanitize=true)","ce58669b":"(*note*: The rows indicate the true class, the columns give the predicted class.)\n\nCOVID-19 cases are identified correctly to about 77% of the time. Given the relatively small data sample on COVID-19 cases in the training set (58), I think this is not too bad.","df7fff57":"## Define output classes\n\nWe aim at classifying four distinct categories: COVID-19 cases vs. healthy vs. bacteria-caused vs. other virus-caused pneumonia.\n","8ecb0a1b":"We now do a random split of the available dataset (''train_data'') into a training set (''train_ds'', 75%) and a validation\/test set (''test_ds'', 25%).","e8243f80":"Finally, we would like to calculate the confusion matrix. For this, we need to obtain the true class labels and the predicted classes from our trained model for the test set.","08ae7af5":"Many entries in the Label_1_Virus_category and Label_2_Virus_category columns are NaNs. We want to replace them with a string 'unknown'.","bc7ecc7f":"## Display X-Ray Images","1e46b0fa":"Comment this in if you want to see the model summary:","3af78424":"Set the batch size. (Note: The batch size may still be tuned to optimize performance.)","62614333":"### Data normalization and augmentation\n\nThe following stats are used for the normalization of the input data. Further below we calculate the mean and the standard deviation for each of the three channels (RGB). These are set here as hard-coded values.","c247eaef":"Let's first look at the count of the class predictions and the true values:","b197717c":"## Setting up GPU device","e795f02f":"## Performance evaluation","4c79611e":"### Load Libraries","5585fe1e":"Let's first look at the evolution of the accuracy and the loss in the training\/validation set."}}