{"cell_type":{"d2a31b30":"code","daca1ff0":"code","9a852794":"code","d9d9de70":"code","0f4421f8":"code","45fb4e2f":"code","12f56369":"code","2aebbe28":"code","23db3174":"code","f16a454f":"code","97bde95e":"code","911ee836":"code","6dde0b16":"code","5f202d0d":"code","5f56727d":"code","adc0491b":"code","616bb59b":"code","f776062b":"code","22399501":"code","bb9db778":"code","1712ad6e":"code","8c7513a6":"code","7fdfb7d0":"code","2ddd89f1":"code","9609ea7f":"code","0f6f4e1e":"code","37d3631b":"code","cd8cc24c":"code","d324c689":"markdown","7b7db4b3":"markdown","dad1ce11":"markdown","6071c837":"markdown","396ce5cf":"markdown","739e69f4":"markdown","4641e60c":"markdown","46c8ffd8":"markdown","69fc1c85":"markdown","37b88d65":"markdown","ab0a72ef":"markdown","9a748ab5":"markdown","7f26bfd4":"markdown","9156a91f":"markdown","910ecc20":"markdown","63ad261f":"markdown","bbb9f4b2":"markdown","e2ab1883":"markdown","c7fd643b":"markdown","927cfc43":"markdown"},"source":{"d2a31b30":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import roc_curve","daca1ff0":"plt.rc('figure',figsize=(18,11))\nsns.set_context('paper',font_scale=2)","9a852794":"df = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\ndf.head()","d9d9de70":"potable_count = np.sum(df['Potability'] == 1)\nunpotable_count = np.sum(df['Potability'] == 0)\nplt.bar(['Potable', 'Unpotable'], [potable_count, unpotable_count]);","0f4421f8":"axes = pd.plotting.scatter_matrix(df)\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(90)\n    ax.yaxis.label.set_rotation(0)\n    ax.yaxis.label.set_ha('right')","45fb4e2f":"corr = df.corr()\ncorr_mask = np.triu(np.ones(shape=corr.shape), k=0)\nsns.heatmap(corr, annot=True, mask=corr_mask);","12f56369":"# Don't include the dependent variable.\npredictors_names = df.columns.values[:-1]\nclass_0_mask = df['Potability'] == 0\nfig, axes = plt.subplots(ncols=3, nrows=3, figsize=(25, 25))\nbins = 30\nfor predictor, ax in zip(predictors_names, axes.flatten()):\n    ax.hist(df[predictor][class_0_mask], alpha=0.3, label='Not potable', bins=bins, density=True)\n    ax.hist(df[predictor][~class_0_mask], alpha=0.3, label='Potable', bins=bins, density=True)\n    ax.set_xlabel(predictor)\n    ax.legend()\nfig.suptitle('Distribution of features in classes', fontsize=30, y=0.91);","2aebbe28":"df_predictors = df.iloc[:, :-1]\ndf_labels = df.iloc[:, -1]\nmsno.matrix(df_predictors);","23db3174":"df.describe()","f16a454f":"df.loc[df['ph'] <= 1]","97bde95e":"df.loc[df['ph'] >= 13]","911ee836":"df = df[(df['ph'] > 1) & (df['ph'] < 13)]\ndf.describe()","6dde0b16":"df_predictors = df.iloc[:, :-1]\ndf_labels = df.iloc[:, -1]\nX_train, X_test, Y_train, Y_test = train_test_split(df_predictors, df_labels, test_size=0.30,\n                                                    stratify=df_labels)\nX_train_imputed = X_train.copy()\nX_train_imputed_no_tri = X_train.drop(columns='Trihalomethanes').copy()\nX_test_imputed = X_test.copy()\nX_test_imputed_no_tri = X_test.drop(columns='Trihalomethanes').copy()","5f202d0d":"# Convenience function to maintain a df after some sklearn transform().\ndef np_to_df(arr: np.array, df_original: pd.DataFrame):\n    return pd.DataFrame(arr, index=df_original.index, columns=df_original.columns)","5f56727d":"# Since the distinction between classes in dependent variables is quite small I think it will be best for\n# the number of neighbours to be large.\nn_neighbors = 20\nimputer_all = KNNImputer(n_neighbors=n_neighbors)\nimputer_no_tri = KNNImputer(n_neighbors=n_neighbors)\n\nX_train_imputed = np_to_df(arr=imputer_all.fit_transform(X_train_imputed), df_original=X_train_imputed)\nX_test_imputed = np_to_df(arr=imputer_all.transform(X_test_imputed), df_original=X_test_imputed)\nX_train_imputed_no_tri = np_to_df(arr=imputer_no_tri.fit_transform(X_train_imputed_no_tri),\n                                  df_original=X_train_imputed_no_tri)\nX_test_imputed_no_tri = np_to_df(arr=imputer_no_tri.transform(X_test_imputed_no_tri),\n                                 df_original=X_test_imputed_no_tri)","adc0491b":"datasets_labels = ['all predictors', 'no Trihalomethanes']\ndatasets_train = [X_train_imputed, X_train_imputed_no_tri]\ndatasets_test = [X_test_imputed, X_test_imputed_no_tri]\nimputed_predictors = ['ph', 'Sulfate', 'Trihalomethanes']","616bb59b":"print(f'Skewness in unimputed predictors\\n{X_train_imputed.drop(columns=imputed_predictors).skew()}')\nfor data, label in zip(datasets_train, datasets_labels):\n    if label == 'no Trihalomethanes':\n        columns = ['ph', 'Sulfate']\n    else:\n        columns = ['ph', 'Sulfate', 'Trihalomethanes']\n    print(f'Skewness in imputed predictors of dataset \"{label}\"')\n    print(data[columns].skew())","f776062b":"box_cox_all = PowerTransformer(method='box-cox', standardize=True)\nbox_cox_all.fit(X_train_imputed)\nbox_cox_tri = PowerTransformer(method='box-cox', standardize=True)\nbox_cox_tri.fit(X_train_imputed_no_tri)\nprint(f'Lambdas for dataset with all predictors')\nfor col, lam in zip(X_train_imputed.columns, box_cox_all.lambdas_):\n    print(f'{col}: {lam}')\nprint(f'\\nLambdas for dataset without Trihalomethanes')\nfor col, lam in zip(X_train_imputed_no_tri.columns, box_cox_tri.lambdas_):\n    print(f'{col}: {lam}')","22399501":"X_train_imputed = np_to_df(box_cox_all.transform(X_train_imputed), X_train_imputed)\nX_test_imputed = np_to_df(box_cox_all.transform(X_test_imputed), X_test_imputed)\nX_train_imputed_no_tri = np_to_df(box_cox_tri.transform(X_train_imputed_no_tri), X_train_imputed_no_tri)\nX_test_imputed_no_tri = np_to_df(box_cox_tri.transform(X_test_imputed_no_tri), X_test_imputed_no_tri)","bb9db778":"datasets_labels = ['all predictors', 'no Trihalomethanes']\ndatasets_train = [X_train_imputed, X_train_imputed_no_tri]\nunimputed_predictors = X_train_imputed.drop(columns=imputed_predictors)\nprint(f'Moments of unimputed predictors:\\n'\n      f'\\tSkewness\\n{unimputed_predictors.skew()}\\n'\n      f'\\tMean\\n{unimputed_predictors.mean()}\\n'\n      f'\\tStd\\n{unimputed_predictors.std()}\\n')\nfor data, label in zip(datasets_train, datasets_labels):\n    if label == 'no Trihalomethanes':\n        columns = ['ph', 'Sulfate']\n    else:\n        columns = ['ph', 'Sulfate', 'Trihalomethanes']\n    print(f'Moments of imputed predictors of dataset \"{label}\"\\n'\n          f'\\tSkewness\\n{data[columns].skew()}\\n'\n          f'\\tMean\\n{data[columns].mean()}\\n'\n          f'\\tStd\\n{data[columns].std()}')","1712ad6e":"datasets_train = [X_train_imputed, X_train_imputed_no_tri]\ndatasets_test = [X_test_imputed, X_test_imputed_no_tri]\n\nfig, axes = plt.subplots(ncols=2)\nfor train, test, df_label, ax in zip(datasets_train, datasets_test, datasets_labels, axes):\n    forest_score = RandomForestClassifier().fit(train, Y_train).score(test, Y_test)\n    svm_score = SVC().fit(train, Y_train).score(test, Y_test)\n    logistic_score = LogisticRegression().fit(train, Y_train).score(test, Y_test)\n    knn_score = KNeighborsClassifier().fit(train, Y_train).score(test, Y_test)\n    qda_score = QuadraticDiscriminantAnalysis().fit(train, Y_train).score(test, Y_test)\n    \n    pos = np.arange(5)\n    accuracies = [forest_score, svm_score, logistic_score, knn_score, qda_score]\n    ax.bar(pos, accuracies)\n    ax.set_xticks(pos)\n    ax.set_xticklabels(['Forest', 'SVM', 'Logistic', 'KNN', 'QDA'])\n    ax.set_title(df_label)\n    ax.set_ylim(top=0.8)\n    rects = ax.patches\n    for rect, value in zip(rects, accuracies):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width() \/ 2, height, f'{value:0.2f}', ha='center', va='bottom')","8c7513a6":"def get_model_cv_acc(model, X: pd.DataFrame, Y: pd.DataFrame, splits=10):\n    pipe = Pipeline([('imputer', KNNImputer(n_neighbors=20)), \n                     ('transformer', PowerTransformer(method='box-cox',standardize=True)), ('model', model)])\n    scores = cross_val_score(pipe, X, Y, cv=splits)\n    return scores.mean()\n\ndf_predictors = df.iloc[:, :-1]\ndf_labels = df.iloc[:, -1]","7fdfb7d0":"print(f'Accuracy of cross-validated QDA model = '\n      f'{100 * get_model_cv_acc(QuadraticDiscriminantAnalysis(), df_predictors, df_labels):.2f}%')","2ddd89f1":"n_kernels = 3\nkernels = ['poly', 'rbf', 'sigmoid']\naccuracy_means = np.empty(n_kernels)\nfor i in range(n_kernels):\n    accuracy_means[i] = get_model_cv_acc(SVC(kernel=kernels[i]), df_predictors, df_labels)\n\npos = np.arange(n_kernels)\nfig, ax = plt.subplots()\nax.bar(pos, accuracy_means)\nax.set_ylabel('Accuracy')\nax.set_xticks(pos)\nax.set_xticklabels(kernels)\nax.set_ylim(top=0.8)\nfor rect, value in zip(ax.patches, accuracy_means):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width() \/ 2, height, f'{value:0.2f}', ha='center', va='bottom')","9609ea7f":"c_values = np.linspace(0.5, 1.5, 15)\nscores = []\nfor c in c_values:\n    scores.append(get_model_cv_acc(SVC(kernel='rbf', C=c), df_predictors, df_labels))\nscores = np.array(scores)\n# plt.plot(c_values, scores_stds[0])\nplt.plot(c_values, scores)\nplt.xlabel('C penalty')\nplt.ylabel('Accuracy')\nprint(f'Best accuracy of {100 * scores.max():.2f}% for C = {c_values[scores.argmax()]}')","0f6f4e1e":"nums_neighbors = np.arange(20, 50)\nscores = []\nfor neighbors in nums_neighbors:\n    scores.append(get_model_cv_acc(KNeighborsClassifier(n_neighbors=neighbors), df_predictors, df_labels))\nscores = np.array(scores)\nplt.plot(nums_neighbors, scores)\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nprint(f'Best accuracy of {100 * scores.max():.2f}% for '\n      f'{nums_neighbors[scores.argmax()]} neighbors')","37d3631b":"n_estimators = np.arange(200, 401, 20)\nmax_depth = np.arange(10, 111, 10)\nmin_samples_split = np.arange(3, 10)\nmin_samples_leaf = np.arange(2, 10)\ngrid = {'n_estimators': n_estimators,\n        'max_depth': max_depth,\n        'min_samples_split': min_samples_split,\n        'min_samples_leaf': min_samples_leaf}\n\nforest_search = make_pipeline(KNNImputer(n_neighbors=20),\n                              PowerTransformer(method='box-cox', standardize=True),\n                              RandomizedSearchCV(estimator=RandomForestClassifier(),\n                                                 param_distributions=grid,\n                                                 n_iter=100,\n                                                 cv=5,\n                                                 verbose=2))\nforest_search.fit(df_predictors, df_labels)","cd8cc24c":"best_params = forest_search['randomizedsearchcv'].best_params_\nbest_forest = forest_search['randomizedsearchcv'].best_estimator_\nbest_score = forest_search['randomizedsearchcv'].best_score_\nprint(f'The best accuracy of {100 * best_score:.2f}% is achieved by forest with parameters:\\n{best_params}')","d324c689":"All ranges of the values seem possible except for ph. While it's hard for me to make some guess about what the real distribution of ph of water bodies is, it also seems highly improbable that it would be 0 or 14, since 0 wouldn't be water at all, but a strong acid and 14 would be a strong base. I don't think water bodies are made out of  car battery acid and drain cleaner fluid.","7b7db4b3":"Since none of the lambda values are near zero, all predictors should be transformed.","dad1ce11":"# Model building\n1. Model selection\n2. Model tuning\n3. Class probabilities","6071c837":"### Skewness, center and scale","396ce5cf":"I'm gonna base my assumption about the probable range of ph of water on this image found here: https:\/\/www.medicalnewstoday.com\/articles\/327185#ph-of-drinks\n# ![image.png](attachment:image.png)\n<br>\nWater ph can possibly have a wide range, but I think that anything below 1 and above 13 is improbable, so I will delete those observations.","739e69f4":"# Basic information about the data","4641e60c":"# EDA","46c8ffd8":"Based on the results I can say that Trihalomethanes does not bring much information to the models, but it also does not hinder them so I might as well just leave it.<br>\nThe only model not worth pursuing further is Logistic Regression. It does not score well and I wouldn't expect that to change since there is hardly any linear relationship in the data. <br>\nFor QDA I can only perform cross validation since it does not have any tuning parameters.","69fc1c85":"### Make sure that all values of the predictors are physically probable and possible.","37b88d65":"### Split the data","ab0a72ef":"Might consider dropping Trihalomethanes, since the differences in distributions seem more like random noise rather than anything informative.","9a748ab5":"The missing data seems to be related to specific predictors rather than samples. Distribution of missing data in samples is somewhat uniform.\n<br><br>\nThere is little correlation between predictors, so I don't think there is a point in looking for predictors correlated with those with missing values to impute data from them. I will impute the missing values from all predictors using KNN.","7f26bfd4":"### QDA cross validation","9156a91f":"# Conclusions\nThe best accuracy is achieved by a SVM with the penalty equal to 1,21. The accuracy of 66.58% is better than the baseline considering proportions of the classes, but not very significantly.","910ecc20":"## Tuning KNN","63ad261f":"# Preprocessing\n1. Split data.\n2. Impute data.\n3. Center, scale, skewness.\n\n#### Create 2 datasets with:\n- all predictors and imputation\n- without Trihalomethanes (because of its distribution and missing values) and imputation","bbb9f4b2":"#### Skewness in original data","e2ab1883":"## Tuning Random Forest\nParameters to tune:\n1. Number of features at every split\n2. Number of trees\n3. Levels in a tree\n4. Minimum number of samples to split a node\n5. Minimum number of samples required at each leaf node","c7fd643b":"## Tuning SVM\n1. Kernel choice\n2. Penalty","927cfc43":"### Impute the data"}}