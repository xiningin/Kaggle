{"cell_type":{"7d9f3b77":"code","d642702b":"code","d972749b":"code","e190eb4b":"code","c3c6f494":"code","fc2c3190":"code","e55742bf":"code","a2e6c92a":"code","895be972":"markdown","c69334be":"markdown","33e24cb9":"markdown","aca34dce":"markdown","fae8281d":"markdown"},"source":{"7d9f3b77":"! pip install -qU tensorflow_datasets","d642702b":"import tensorflow as tf\nimport tensorflow_datasets as tfds","d972749b":"DATA_DIR = \"\/kaggle\/input\/\"\n\nds, ds_info = tfds.load('eurosat\/rgb',\n                        with_info=True,\n                        split='train',\n                        data_dir=DATA_DIR)\n\ntfds.show_examples(ds, ds_info);","e190eb4b":"print(ds_info)","c3c6f494":"BATCH_SIZE = 16\nAUTO = tf.data.experimental.AUTOTUNE\nSHUFFLE_BUFFER = int(ds_info.splits['train'].num_examples * 0.7)\n\nds_train, ds_valid = tfds.load('eurosat\/rgb',\n                               split=['train[:70%]', 'train[70%:]'],\n                               data_dir=DATA_DIR,\n                               as_supervised=True)\n\ndef preprocess(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nds_train = (ds_train\n            .map(preprocess, AUTO)\n            .cache()\n            .shuffle(SHUFFLE_BUFFER)\n            .repeat()\n            # Augmentations go here .map(augment, AUTO)\n            .batch(BATCH_SIZE, drop_remainder=True)\n            .prefetch(AUTO))\n\nds_valid = (ds_valid\n            .map(preprocess, AUTO)\n            .cache()\n            .batch(BATCH_SIZE)\n            .prefetch(AUTO))","fc2c3190":"import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nNUM_CLASSES = ds_info.features['label'].num_classes\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(),\n    layers.Conv2D(filters=16, kernel_size=5, padding='same', activation='elu'),\n    layers.MaxPool2D(),\n    \n    layers.BatchNormalization(),\n    layers.Conv2D(32, 3, padding='same', activation='elu'),\n    layers.Conv2D(32, 3, padding='same', activation='elu'),\n    layers.MaxPool2D(),\n    \n    layers.BatchNormalization(),\n    layers.Conv2D(64, 3, padding='same', activation='elu'),\n    layers.Conv2D(64, 3, padding='same', activation='elu'),\n    layers.MaxPool2D(),\n    \n    layers.Flatten(),\n    layers.Dense(128, activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(64, activation='elu'),\n    layers.Dropout(0.5),\n    layers.Dense(NUM_CLASSES, activation='softmax')\n])\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy'],\n)","e55742bf":"EPOCHS = 50\nSTEPS_PER_EPOCH = int(ds_info.splits['train'].num_examples * 0.7) \/\/ BATCH_SIZE\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=7, min_delta=0.001, restore_best_weights=True)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=[early_stopping],\n)","a2e6c92a":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot();","895be972":"## Load Data ##\n\nWe'll load the data with the TensorFlow Datasets loader. It is not included in the default Kaggle environment, so let's install it first.","c69334be":"## Introduction\n\nThis is the EuroSAT remote sensing dataset in TFRecords format. TFRecords are TensorFlow's data format for high-performance training with GPUs and TPUs.","33e24cb9":"## Define Model ##","aca34dce":"## Construct Pipeline ##","fae8281d":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"}}