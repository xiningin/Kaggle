{"cell_type":{"549a3bed":"code","c3e5238f":"code","acbc7615":"code","d589a8a5":"code","2bdee41f":"code","03f88a88":"code","7dca6a18":"code","5b730a0c":"code","fad18c37":"code","158c3951":"code","dc24b1fa":"code","5278e18a":"code","b90056ed":"code","192a53b3":"code","bd7c0be9":"code","5538bec6":"code","1d58bda8":"code","a311c777":"code","66c726b0":"code","c6062180":"code","d375ce45":"code","ebd6ca57":"code","eb941044":"markdown","2f17376b":"markdown","0b455706":"markdown","3decbbaa":"markdown","4d25b4c8":"markdown"},"source":{"549a3bed":"!pip install -qq git+https:\/\/www.github.com\/ildoonet\/tf-pose-estimation","c3e5238f":"!pip install -qq pycocotools","acbc7615":"%load_ext autoreload\n%autoreload 2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})","d589a8a5":"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\ndef load_graph1(model_file):\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n\n    return graph\n\ndef read_tensor_from_image_file1(file_name, input_height=299, input_width=299,input_mean=0, input_std=255):\n    input_name = \"file_reader\"\n    output_name = \"normalized\"\n    file_reader = tf.read_file(file_name, input_name)\n    if file_name.endswith(\".png\"):\n        image_reader = tf.image.decode_png(file_reader, channels = 3,\n                                       name='png_reader')\n    elif file_name.endswith(\".gif\"):\n        image_reader = tf.squeeze(tf.image.decode_gif(file_reader,\n                                                  name='gif_reader'))\n    elif file_name.endswith(\".bmp\"):\n        image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')\n    else:\n        image_reader = tf.image.decode_jpeg(file_reader, channels = 3,\n                                        name='jpeg_reader')\n    float_caster = tf.cast(image_reader, tf.float32)\n    dims_expander = tf.expand_dims(float_caster, 0);\n    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n    sess = tf.Session()\n    result = sess.run(normalized)\n\n    return result\n\ndef load_labels1(label_file):\n    label = []\n    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n    for l in proto_as_ascii_lines:\n        label.append(l.rstrip())\n    return label\n\ndef classify_scene(image_file):\n    \n# =============================================================================\n#   Note : Provide your own absolute file path for the following\n#   You can choose the retrained graph of either as v1.0 or v2.0 \n#   Both models are retrained inception models (on my procured dataset)\n#   v1.0 was trained for 500 epocs on a preliminary dataset of poses.\n#   v2.0 was trained for 4000 epocs on a dataset containing the previous dataset\n#   and more.\n# =============================================================================\n  # Change the path to your convenience\n    #file_path = os.path.abspath(os.path.dirname(__file__))\n    path = '..\/input\/labels\/'\n    model_file = path+'output_graph_20000.pb'\n    label_file = path+'output_labels.txt'\n    input_height = 299\n    input_width = 299\n    input_mean = 128\n    input_std = 128\n    input_layer = \"Mul\"\n    output_layer = \"final_result\"\n\n    graph = load_graph1(model_file)\n    t = read_tensor_from_image_file1(image_file,\n                                  input_height=input_height,\n                                  input_width=input_width,\n                                  input_mean=input_mean,\n                                  input_std=input_std)\n\n    input_name = \"import\/\" + input_layer\n    output_name = \"import\/\" + output_layer\n    input_operation = graph.get_operation_by_name(input_name);\n    output_operation = graph.get_operation_by_name(output_name);\n\n    with tf.Session(graph=graph) as sess:\n        start = time.time()\n        results = sess.run(output_operation.outputs[0],\n                      {input_operation.outputs[0]: t})\n        end=time.time()\n    results = np.squeeze(results)\n    top_k = results.argsort()[-5:][::-1]\n    labels = load_labels1(label_file)\n\n    print('\\nEvaluation time (1-image): {:.3f}s\\n'.format(end-start))\n    template = \"{} (score={:0.5f})\"\n    for i in top_k:\n        print(template.format(labels[i], results[i]))\n\n\n    return template.format(labels[top_k[0]], results[top_k[0]])\n","2bdee41f":"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\ndef load_graph(model_file):\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n\n    return graph\n\ndef read_tensor_from_image_file(image_file, input_height=299, input_width=299,\n\t\t\t\tinput_mean=0, input_std=255):\n    \n    float_caster = tf.cast(image_file, tf.float32)\n    dims_expander = tf.expand_dims(float_caster, 0);\n    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n    sess = tf.Session()\n    result = sess.run(normalized)\n\n    return result\n\ndef load_labels(label_file):\n    label = []\n    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n    for l in proto_as_ascii_lines:\n        label.append(l.rstrip())\n    return label\n\ndef classify_pose(image_file):\n    \n# =============================================================================\n#   Note : Provide your own absolute file path for the following\n#   You can choose the retrained graph of either as v1.0 or v2.0 \n#   Both models are retrained inception models (on my procured dataset)\n#   v1.0 was trained for 500 epocs on a preliminary dataset of poses.\n#   v2.0 was trained for 4000 epocs on a dataset containing the previous dataset\n#   and more.\n# =============================================================================\n  # Change the path to your convenience\n    #file_path = os.path.abspath(os.path.dirname(__file__))\n    path =  '..\/input\/labels\/'#os.path.join(file_path, '..\/input\/labels\/')\n    model_file = path+'retrained_graph.pb'\n    label_file = path+'retrained_labels.txt'\n    input_height = 224\n    input_width = 224\n    input_mean = 128\n    input_std = 128\n    input_layer = \"input\"\n    output_layer = \"final_result\"\n\n    graph = load_graph(model_file)\n    t = read_tensor_from_image_file(image_file,\n                                  input_height=input_height,\n                                  input_width=input_width,\n                                  input_mean=input_mean,\n                                  input_std=input_std)\n\n    input_name = \"import\/\" + input_layer\n    output_name = \"import\/\" + output_layer\n    input_operation = graph.get_operation_by_name(input_name);\n    output_operation = graph.get_operation_by_name(output_name);\n\n    with tf.Session(graph=graph) as sess:\n        start = time.time()\n        results = sess.run(output_operation.outputs[0],\n                      {input_operation.outputs[0]: t})\n        end=time.time()\n    results = np.squeeze(results)\n\n    labels = load_labels(label_file)\n\n    print('\\nEvaluation time (1-image): {:.3f}s\\n'.format(end-start))\n    template = \"{} (score={:0.5f})\"\n    label = ''\n    if results[0] > results[1]:\n        label = labels[0]\n        result = results[0]\n    else:\n        label = labels[1]\n        result = results[1]\n\n    return template.format(label, result)\n","03f88a88":"%matplotlib inline\nimport tf_pose\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm_notebook\nfrom PIL import Image\nimport numpy as np\nimport os\n\ndef video_gen(in_path):\n    c_cap = cv2.VideoCapture(in_path)\n    while c_cap.isOpened():\n        ret, frame = c_cap.read()\n        if not ret:\n            break\n        yield c_cap.get(cv2.CAP_PROP_POS_MSEC), frame[:, :, ::-1]\n    c_cap.release()\n","7dca6a18":"video_paths = glob('..\/input\/videos-pose\/*.mp4')\nc_video = video_gen(video_paths[0])\nfor _ in range(200):\n    c_ts, c_frame = next(c_video)\nplt.imshow(c_frame)","5b730a0c":"from PIL import Image\n#import numpy as np\n#img_w, img_h = 200, 200\n#data = np.zeros((img_h, img_w, 3), dtype=np.uint8)\n#data[100, 100] = [255, 0, 0]\n#img = Image.fromarray(c_frame, 'RGB')\n#img.save('test.png')\n#img.show()\n\n#classify_scene('test.png')","fad18c37":"from tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\ntfpe = tf_pose.get_estimator()\n\nhumans_vid = tfpe.inference(npimg=c_frame, upsample_size=4.0)\nprint(humans_vid)","158c3951":"new_vid_img = TfPoseEstimator.draw_humans(c_frame[:, :, ::-1], humans_vid, imgcopy=False)\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nax1.imshow(new_vid_img[:, :, ::-1])","dc24b1fa":"body_to_dict = lambda c_fig: {'bp_{}_{}'.format(k, vec_name): vec_val \n                              for k, part_vec in c_fig.body_parts.items() \n                              for vec_name, vec_val in zip(['x', 'y', 'score'],(part_vec.x, 1-part_vec.y, part_vec.score))}\nc_fig = humans_vid[0]\nbody_to_dict(c_fig)","5278e18a":"MAX_FRAMES = 200\nbody_pose_list = []\nfor vid_path in tqdm_notebook(video_paths, desc='Files'):\n    c_video = video_gen(vid_path)\n    c_ts, c_frame = next(c_video)\n    out_path = '{}_out.avi'.format(os.path.split(vid_path)[0])\n    \n    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc('M','J','P','G'),10,(c_frame.shape[1], c_frame.shape[0]))\n    \n    for (c_ts, c_frame), _ in zip(c_video, tqdm_notebook(range(MAX_FRAMES), desc='Frames')):\n        bgr_frame = c_frame[:,:,::-1]\n        humans = tfpe.inference(npimg=bgr_frame, upsample_size=4.0)\n        for c_body in humans:\n            body_pose_list += [dict(video=out_path, time=c_ts, **body_to_dict(c_body))]\n        new_image = TfPoseEstimator.draw_humans(bgr_frame, humans, imgcopy=False)\n        pose_class = classify_pose(new_image)\n    \n        cv2.putText(new_image,\"Current predicted pose is : %s\" %(pose_class),(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n        #cv2.putText(new_image,\"Predicted Scene: %s\" %(scene_class),(10, 30),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n        out.write(new_image)\n    out.release()\n","b90056ed":"import pandas as pd\nbody_pose_df = pd.DataFrame(body_pose_list)\nbody_pose_df#.describe()","192a53b3":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, c_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_ax.plot(c_rows['time'], c_rows['bp_{}_y'.format(i)], label='x {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","bd7c0be9":"#body_pose_df[:400]","5538bec6":"fig, m_axs = plt.subplots(1, 2, figsize=(15, 5))\nfor c_ax, (c_name, n_rows) in zip(m_axs, body_pose_df.groupby('video')):\n    for i in range(17):\n        c_rows = n_rows.query('bp_{}_score>0.6'.format(i)) # only keep confident results\n        c_ax.plot(c_rows['bp_{}_x'.format(i)], c_rows['bp_{}_y'.format(i)], label='BP {}'.format(i))\n    c_ax.legend()\n    c_ax.set_title(c_name)","1d58bda8":"body_pose_df.to_csv('body_pose.csv', index=False)","a311c777":"def img_pose(frame):\n    humans_img = tfpe.inference(npimg=frame, upsample_size=4.0)\n    new_image = TfPoseEstimator.draw_humans(imgrgb[:, :, ::-1], humans_img, imgcopy=False)\n    return new_image","66c726b0":"img_paths = glob('..\/input\/images\/*.jpg')\nfor path in img_paths:\n    frame=cv2.imread(path,cv2.COLOR_BGR2RGB)\n    imgrgb = frame[:,:,::-1]\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    #plt.imshow(imgrgb)\n    ax.imshow(imgrgb)\n    \n    new_image = img_pose(frame)\n    fig, ax2 = plt.subplots(1, 1, figsize=(8, 8))\n    ax2.imshow(new_image[:, :, ::-1])","c6062180":"humans_img = tfpe.inference(npimg=frame, upsample_size=4.0)\nprint(humans_img)","d375ce45":"def webcam():\n    cap = cv2.VideoCapture(0)\n\n    while(cap.isOpened()):\n        # Capture frame-by-frame\n        ret, image = cap.read()\n        if not ret:\n            break\n        yield cap.get(cv2.CAP_PROP_POS_MSEC), frame[:, :, ::-1]\n    cap.release()\n\n        ","ebd6ca57":"video_capture = cv2.VideoCapture(0)","eb941044":"**Image input**","2f17376b":"**Input through Webcam**","0b455706":"**Video input**","3decbbaa":"## Libraries we need\nInstall tf_pose and pycocotools","4d25b4c8":"# Overview\nThe kernel shows how to use the [tf_pose_estimation](https:\/\/github.com\/ildoonet\/tf-pose-estimation) package in Python on a series of running videos."}}