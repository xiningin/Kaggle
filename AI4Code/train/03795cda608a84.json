{"cell_type":{"053ce186":"code","815e4b0e":"code","0c596ba1":"code","09c6e2ce":"code","0ed332f4":"code","82889c47":"code","49705e26":"code","67bcacbc":"code","29f8a067":"code","fb791e0f":"code","c6519b16":"code","008c6bef":"code","8973abc1":"code","7c4e9d02":"code","0d0de467":"code","5e304b39":"code","035b7028":"code","7449e8f6":"code","a3e446b0":"code","4e385b9c":"code","fe1df790":"code","ca85223a":"code","0de8a1a4":"code","42bbae69":"code","5be819cf":"code","f0fa1674":"code","09b9ac26":"code","8f28305d":"code","1bdcb606":"code","a2da02cf":"code","a4afccee":"code","9be07d4b":"code","45a11a28":"code","5265a8e3":"code","c7ede58f":"markdown","7b078c10":"markdown","b0e92907":"markdown","6583fbfc":"markdown","e8b32b32":"markdown","9d1dcf38":"markdown","5c118607":"markdown","c51d4c65":"markdown","28dccacb":"markdown","5a917e85":"markdown","6c27653a":"markdown","1fea728f":"markdown","9b759a05":"markdown","91742b40":"markdown","8b3b91f5":"markdown","d458a169":"markdown","883b99b6":"markdown","353ead69":"markdown","9dedf6c9":"markdown","41f3e762":"markdown","f0475400":"markdown","040903f6":"markdown","729f2fb7":"markdown","3b331ecf":"markdown","53ab45d7":"markdown","7929633f":"markdown","4731d3f8":"markdown","59f86e89":"markdown","af68855d":"markdown","78f35ad0":"markdown","0621d7f8":"markdown","91c340f6":"markdown","64b41f10":"markdown"},"source":{"053ce186":"import pandas as pd\n\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head(5)","815e4b0e":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","0c596ba1":"train.drop([\"Id\"], axis = 1, inplace = True)\ntest.drop([\"Id\"], axis = 1, inplace = True)","09c6e2ce":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr = train.corr()\nfig, ax = plt.subplots(figsize=(12,9))     \nsns.heatmap(corr, vmax=.8, square=True);","0ed332f4":"miss=train[[\"GarageYrBlt\",\n\"GarageCond\",\n\"GarageType\",\n\"GarageFinish\",\n\"GarageQual\",\n\"BsmtFinType2\",\n\"BsmtExposure\",\n\"BsmtQual\",\n\"BsmtCond\",\n\"BsmtFinType1\"]]\nmiss.head()","82889c47":"sns.displot(train['Electrical']);","49705e26":"sns.displot(train['MasVnrArea']);","67bcacbc":"sns.displot(train['MasVnrType']);","29f8a067":"train.drop((missing_data[missing_data['Total'] > 1]).index,1, inplace = True)\ntrain.drop(train.loc[train['Electrical'].isnull()].index, inplace = True)","fb791e0f":"sns.displot(train['SalePrice'],kde=True)","c6519b16":"import numpy as np\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","008c6bef":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(train,test_size=0.2)\ntrain_y=train_set[\"SalePrice\"]\ntrain_x=train_set.drop([\"SalePrice\"], axis=1)\ntest_y=test_set[\"SalePrice\"]\ntest_x=test_set.drop([\"SalePrice\"], axis=1)","8973abc1":"train_set_num=train_x.select_dtypes(include=[np.number])\ntrain_set_ob=train_x.select_dtypes(include=[object])\nnum_column=list(train_set_num.columns)\nob_column=list(train_set_ob.columns)","7c4e9d02":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_column),\n        (\"cat\", OrdinalEncoder(), ob_column),\n    ])\n\ntrain_prepared = full_pipeline.fit_transform(train_x)\ntest_x = full_pipeline.fit_transform(test_x)","0d0de467":"from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb","5e304b39":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\ndef eval(model,test_x,test_y):\n    ymul=model.predict(test_x)\n    print(\"Model score: %.4f\" % model.score(test_x,test_y))  # This is just shorthand for the R2 score\n    print(\"Mean absolute error: %.4f\" % mean_squared_error(ymul,test_y))\n    print(\"R2-score: %.4f\" % r2_score(test_y , ymul) )\n    score=np.sqrt(-cross_val_score(model, train_prepared, train_y, scoring=\"neg_mean_squared_error\", cv = 10))\n    print(\"RMSE score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","035b7028":"lin_reg=LinearRegression()\nlin_reg.fit(train_prepared,train_y)\neval(lin_reg,test_x,test_y)","7449e8f6":"sgd_reg=SGDRegressor(max_iter=1000,tol=1e-3,penalty=None,eta0=0.1)\nsgd_reg.fit(train_prepared,train_y)\neval(sgd_reg,test_x,test_y)","a3e446b0":"ridge_reg=Ridge(alpha=1,solver=\"cholesky\")\nridge_reg.fit(train_prepared,train_y)\neval(ridge_reg,test_x,test_y)","4e385b9c":"lasso_reg=Lasso(alpha =0.1, random_state=1)\nlasso_reg.fit(train_prepared,train_y)\neval(lasso_reg,test_x,test_y)","fe1df790":"elastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5)\nelastic_net.fit(train_prepared,train_y)\neval(elastic_net,test_x,test_y)","ca85223a":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint\n\nparam_grid= {'alpha': randint(low=0.0, high=300.0), }\nrnd_search = RandomizedSearchCV(ridge_reg, param_distributions=param_grid, n_iter=300, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(train_prepared, train_y)\nprint(\"RandomSearch best\", rnd_search.best_estimator_)\n\nparam_grid = [ {'alpha': [0,11]}  ]\ngrid_search = GridSearchCV(ridge_reg, param_grid, cv=5, scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(train_prepared, train_y)\nprint(\"GridSearch best\", grid_search.best_estimator_)","0de8a1a4":"ridge_reg=Ridge(alpha=242,solver=\"cholesky\")\nridge_reg.fit(train_prepared,train_y)\neval(ridge_reg,test_x,test_y)","42bbae69":"param_grid= {'alpha': randint(low=0.0005, high=100),}\nrnd_search = RandomizedSearchCV(lasso_reg, param_distributions=param_grid,n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(train_prepared, train_y)\nprint(\"RandomSearch best\", rnd_search.best_estimator_)\n\nparam_grid = [{'alpha': [0.0005,0.1,0.2,10,51]} ]\ngrid_search = GridSearchCV(lasso_reg, param_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(train_prepared, train_y)\nprint(\"GridSearch best\", grid_search.best_estimator_)","5be819cf":"lasso_reg=Lasso(alpha =0.0005, random_state=1)\nlasso_reg.fit(train_prepared,train_y)\neval(lasso_reg,test_x,test_y)","f0fa1674":"param_grid= {'alpha': randint(low=0.1, high=1),'l1_ratio': randint(low=0.1, high=1)}\n#rnd_search = RandomizedSearchCV(elastic_net, param_distributions=param_grid,n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n#rnd_search.fit(train_prepared, train_y)\n\nparam_grid = [{'alpha': [0.1,0.2],'l1_ratio': [0.1,0.5,0.9,1] } ]\ngrid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(train_prepared, train_y)\nprint(\"GridSearch best\", grid_search.best_estimator_)","09b9ac26":"elastic_net = ElasticNet(alpha=0.1,l1_ratio=0.1)\nelastic_net.fit(train_prepared,train_y)\neval(elastic_net,test_x,test_y)","8f28305d":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(max_depth=5)\ntree_reg.fit(train_prepared,train_y)\neval(tree_reg,test_x,test_y)","1bdcb606":"from sklearn.ensemble import RandomForestRegressor\n\nrmdtree_reg = RandomForestRegressor(max_depth=5)\nrmdtree_reg.fit(train_prepared,train_y)\neval(rmdtree_reg,test_x,test_y)","a2da02cf":"from sklearn.ensemble import BaggingRegressor\n\nbag_reg=BaggingRegressor(base_estimator=Ridge(alpha=242,solver=\"cholesky\"), n_estimators=500)\nbag_reg.fit(train_prepared,train_y)\neval(bag_reg,test_x,test_y)","a4afccee":"from sklearn.ensemble import AdaBoostRegressor\n\nregr = AdaBoostRegressor(random_state=0, n_estimators=100)\nregr.fit(train_prepared,train_y)\neval(regr,test_x,test_y)","9be07d4b":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost.fit(train_prepared,train_y)\neval(GBoost,test_x,test_y)","45a11a28":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4, gamma=0.05, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.78, n_estimators=2200,\n                             reg_alpha=0.46, reg_lambda=0.85,\n                             subsample=0.52, verbosity=0,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(train_prepared,train_y)\neval(model_xgb,test_x,test_y)","5265a8e3":"from sklearn.ensemble import StackingRegressor\n\nmodel_xgb2 = xgb.XGBRegressor(colsample_bytree=0.4, gamma=0.05, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.78, n_estimators=2200,\n                             reg_alpha=0.46, reg_lambda=0.85,\n                             subsample=0.52, verbosity=0,\n                             random_state =7, nthread = -1)\nrmdtree_reg2 = RandomForestRegressor(max_depth=5)\nbag_reg2=BaggingRegressor(base_estimator=Ridge(alpha=242,solver=\"cholesky\"), n_estimators=500)\nregr2 = AdaBoostRegressor(random_state=0, n_estimators=100)\n\nestimators = [('RandomForst',rmdtree_reg2),\n              ('addboost', regr2),\n              ('xgboost', bag_reg2) \n             ] \nstacked = StackingRegressor(estimators=estimators, final_estimator=model_xgb2)\nstacked.fit(train_prepared,train_y)\neval(stacked,test_x,test_y)","c7ede58f":"A random forest algorthim is a ensemble of decesion trees trained using the bagging method. This involves using the same algorithm and training them on a subets of the training data, once this is done the ensemble makes predictions by agregating the result (most frequent for classification and average for regression).  ","7b078c10":"# House Prices\nThis is my work based on the housing prices competition on kaggle https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques","b0e92907":"##### Ridge Regression","6583fbfc":"##### Lasso Regression","e8b32b32":"### Linear Regression based models","9d1dcf38":"##### AdaBoost \nUsing adaboost is when the next training instance pays more atention to previous training instances where it underfitted. This will keep going focussing on the harder to fit cases. For example when the first algorithm trains it will have some misslassified training instances, on the next instance it will increase the weight of the missclassified instances and then trains again. This will produces the final model.","5c118607":"##### Gradient Boosting\nThis works in a very similar was as adaboost but trys to fit to the residual erros made by the previous predictor. ","c51d4c65":"#### Bagging","28dccacb":"##### Stacking models \nThis can work in 2 ways, you can use trival functions to affregate predications together or train a model to perform the aggregation instead. ","5a917e85":"### Ensemble Learning","6c27653a":"I have also done a small trail using bagging regressor for the ridge model, 0.1558 (0.0472) was before without bagging. This does decrease a little when bagging included","1fea728f":"This plot is right skewed (which you can measure). As we know that machine learning algorithms prefer normally distributed values this will need to transformed by taking the log.","9b759a05":"I would have expected this to have a reduced error which is surprising, I will try and investigate further","91742b40":"## Preparing data for Machine learning\nWe need to prepare the data for use in the machine learning models. First we need to split the data into a test and train set. I will then use a pipeline to transform all the data in a efficient way. For the numerical data I will just use Standard Scalar, this will scale all the values to be in the same range. This is useful for instance if we where comparing age (0-100) and wage (0-100000) these would have a different effect due to the different ranges of values. The ordinal encoder will change all the categorical variables into numbers so they can be used in the machine learning algorithm.","8b3b91f5":"A optimerized version of this algorithm exists in xgboost","d458a169":"A decision tree is normally used for classification but can also but done for regression. A decision tree works by asking a intial question at the root node for instance in this case is the sale condition normal and then depending on the answer you go down one of the nodes to another question if needed (for simple datasets it may not be needed to be able to divide the data up). You measure the impurity of the node called gini, if the node is complety pure then it would =0. In skearn it uses the CART algorithm, this works by splitting the data set into 2 subsets using a single feature and a threshold, it then continues to do this until the max depth is reached or a split will not reduce the impurity. To use this for regression it is very similar, however instead of predicting a class to assign to it predicts a value. You have to be careful though as not setting a maximum depth would normally result in overfitting.","883b99b6":"When trying to fit a LinearRegression model you are trying to minise the cost function which is the mean square error. There are 2 main ways of doing this mathmatically. These are the normal equation and Singular Value Decompostion, with the implementation in sklearn using SVD. These however have the problem that they scale porely with large data sets. ","353ead69":"As a number of them have significant numbers of missing values, I will remove these variables. I have defined that above 15% is a good value to remove from. To deal with the observations for electrical, MassVnrType and Area I will first plot these variables to see if the mean makes sense to use as a replacement. I will also plot a correlation plot to see if missing variables can be described by others.","9dedf6c9":"##### Elastic Net Regression","41f3e762":"## Missing Data\nFirst I will look at the number of missing values in each column and the percentage of values that are missing","f0475400":"## Machine Learning Models\nNow that is ready I will start building the models and uses cross_val_score to be able to find the root mean square error. ","040903f6":"As there is no way of finding these missing values and that these are not the most import features for prediction we will drop these columns. Also should be noted that garage year built is very strongly correlated to house build year.","729f2fb7":"#### Evaluation Scores\n* Mean Absolute Error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it\u2019s just average error.\n* Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It\u2019s more popular than Mean Absolute Error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n* Root Mean Squared Error (RMSE). RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers\n* R-squared is not an error, but rather a popular metric to measure the performance of your regression model. It represents how close the data points are to the fitted regression line. The higher the R-squared value, the better the model fits your data. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).","3b331ecf":"#### Optimizing the hyperparameters\nAs the regularized models have hyperparameters we need to choose them. Just based on knowledge we would probably get to the correct area of the model but they will never be perfect. Luckily sklearn has a built in method called GridSearchCV and RandomizedSeachCV that will allow you to find the correct values better. GridSearchCV takes a list of values for the hyperparameters you want to try while RandomizedSeachCV takes a range of values and the number of times to run to get the best soluation. The works very well with small numbers of hyperparamters.","53ab45d7":"Sometimes when we make a model it will overfit the data. A good way to reduce this is to regularize the model (constrain it). For a linear model the normal way is to constrain the weights of the model. The first one I will try in this is ridge regression, in this a term is added to the cost function to keep the model weights as small as possible. They hyperparameters alpha controles how much you want this model to regularize, =0 is just linear regression and =1 means all the weights will be close to 0. ","7929633f":"## Data Visulization\nFirst we need to look at sale price as this is what we are trying to predict.","4731d3f8":"When the data set is large you can train the model instead using gradient decent. The basic idea of this is to tweak the feature of the cost function iteratively to minimize the cost function. The anology that is often given is if you are on a mountian and trying to find the way down, you could look at the steepness of the slope and go where the steepness is greatest. The most important parameter of this method is the learning rate, this is because a small value will take a long time to converge to the correct value. While a large one the model would bounce around the optimal soluation making the model diverge. Another problem with this method is if the cost function is not a nice regular bowl. This could lead to the soluation actually being a local minumum rather than a global minumum. However this isnt a problem with the MSE cost function as it happens to be a convex function. This means that if you pick 2 point on the curve the line segment joining them would never cross the curve.                                           \nIf you use the whole training set for this then it is known as batch gradient decent, this however has the problem of taking a long time with large data sets. To try and solve this problem we can use Stochastic gradient descent. In this you pick a random instance from training set and calulate the gradiants based only on that instance. This increases the speed of the calculation as it only does it on a small amount of data. However due to the randomness of this procress the cost function will bounce around and will never settle at the global miniumum, this however can sometimes help if the cost function has local minimums it can bounce out of it. The method is impletmented in SGDRegressor.           \nYou can also have a mini-batch gradient decent, this computes of the cost function on a small random subset of the overall data.  ","59f86e89":"Elastic net is the middle ground between lasso and ridge. The l1_ratio defines the mixing between them (=1 is just lasso)","af68855d":"# Future work\n* Try to find all the transformations that could help improve the model","78f35ad0":"#### Boosting\nThis combineds several week learners into a stronger leaner, with the general idea is to train models one after another with each trying to correct the previouses error. ","0621d7f8":"Lasso regression is next and this has the characteristic that the least important feature weights tend towards 0, this allows for automatic feature selection which sometimes can be useful.","91c340f6":"### Decsion tree based regression","64b41f10":"Masonry veneer and MasVnrArea: Masonry veneer are not important features and are impossible to get replace the variable, this is because it is categorical for one while the area there is a large peak around 0. These will be removed, the 1 entry for electrical will also be removed because again it cant be replaced."}}