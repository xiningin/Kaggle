{"cell_type":{"165d2dea":"code","b503bf35":"code","934a29ca":"code","67f48185":"code","166aebf7":"code","19e51dbe":"code","bc3895b2":"code","02b5e656":"code","5934976f":"code","7d86598c":"code","b7ff1051":"code","aa038aa3":"code","2ee1cff9":"code","f31430aa":"code","f19590ad":"code","38b21b39":"code","09ca6e7c":"code","f1f33242":"code","aa52e6a1":"code","b6d928d1":"code","bd866674":"code","8c56e215":"code","31712d7f":"code","d8c5ab21":"code","bfb409d7":"code","c601c54f":"code","8f4bf206":"code","3beda249":"code","eb6eef6c":"code","03d72e97":"code","ec0fb33a":"code","1a9c3f7f":"code","e44ef945":"code","eecf7019":"code","2b9446a5":"code","8b96de30":"code","a2c455bd":"code","f3c8aa3c":"code","4dd2a54a":"code","e325d3e7":"code","dd4f667b":"code","24a3e59b":"code","c39b2891":"code","2c4b117d":"markdown","8d8056f6":"markdown","614da6ba":"markdown","9441975e":"markdown","cb5e9552":"markdown"},"source":{"165d2dea":"import os\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport random","b503bf35":"data_parent = '..\/input\/adience-benchmark-gender-and-age-classification\/AdienceBenchmarkGenderAndAgeClassification\/'\nprint(os.listdir(data_parent))","934a29ca":"\nfold_0 = pd.read_csv(os.path.join(data_parent, 'fold_0_data.txt'), sep='\\t')\nfold_1 = pd.read_csv(os.path.join(data_parent, 'fold_1_data.txt'),sep='\\t')\nfold_2 = pd.read_csv(os.path.join(data_parent, 'fold_2_data.txt'),sep='\\t')\nfold_3 = pd.read_csv(os.path.join(data_parent, 'fold_3_data.txt'),sep='\\t')\nfold_4 = pd.read_csv(os.path.join(data_parent, 'fold_4_data.txt'),sep='\\t')\ntotal_data = pd.concat([fold_0, fold_1, fold_2, fold_3, fold_4], ignore_index=True)\ntotal_data.head()","67f48185":"print('[+] length of the file:', len(total_data))\nprint('[+] unique values of Age:')\nprint(total_data.age.unique())\nprint('===================================================')\nprint('[+] Number of None Values in Age:')\nprint((total_data.age == 'None').sum())\nprint('[+] unique values of Gender:')\nprint(total_data.gender.unique())\nprint('===================================================')\nprint('[+] Number of nan values in Gender:')\nprint(total_data.gender.isna().sum())","166aebf7":"total_data.groupby('gender')['gender'].count().plot.pie(figsize=(10, 10))","19e51dbe":"total_data.groupby('age')['age'].count().plot.pie(figsize=(10, 10))","bc3895b2":"sample_num = 200\nim_name = total_data.iloc[sample_num].original_image\nim_path = os.path.join(data_parent, 'faces',str(total_data.iloc[sample_num].user_id), 'coarse_tilt_aligned_face.' + str(total_data.iloc[sample_num].face_id) + '.' + im_name)\nprint('[+] Image path:', im_path)\nimage = cv2.imread(im_path)\nprint('[+] Image shape:', image.shape)\nprint('[!] Age:', total_data.iloc[sample_num].age, 'Gender:', total_data.iloc[sample_num].gender)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","02b5e656":"images = []\nfor _ in range(16):\n    sample_num = random.randint(0, len(total_data))\n    im_name = total_data.iloc[sample_num].original_image\n    im_path = os.path.join(data_parent, 'faces',str(total_data.iloc[sample_num].user_id), 'coarse_tilt_aligned_face.' + str(total_data.iloc[sample_num].face_id) + '.' + im_name)\n    image = cv2.imread(im_path)\n    age = total_data.iloc[sample_num].age\n    gender = total_data.iloc[sample_num].gender\n    n_col = 4\n    n_rows = 4\n    images.append((image, age, gender))\n    \nfig, axs = plt.subplots(ncols=n_col, nrows=n_rows, figsize=(30,30))\ncount = 0\nfor i in range(n_rows):\n      for j in range(n_col):\n        axs[i][j].imshow(cv2.cvtColor(images[count][0], cv2.COLOR_BGR2RGB))\n        axs[i][j].set_title(f'Age: {images[count][1]}, Gender: {images[count][2]}')\n        count+=1\nplt.show()","5934976f":"age_mapping = [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), ('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), ('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), ('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\nage_mapping_dict = {each[0]: each[1] for each in age_mapping}\n\ndrop_labels = []\nfor idx, each in enumerate(total_data.age):\n    if each == 'None':\n        drop_labels.append(idx)\n    else:\n        total_data.age.loc[idx] = age_mapping_dict[each]\ntotal_data = total_data.drop(labels=drop_labels, axis=0) #droped None values\ntotal_data.age.value_counts(dropna=False)","7d86598c":"total_data = total_data.dropna()\ntotal_data['full_path'] = total_data.apply(lambda x: os.path.join(data_parent, 'faces', str(x.user_id), 'coarse_tilt_aligned_face.' + str(x.face_id) + '.' + x.original_image), axis=1)\ntotal_data.age.unique(), len(total_data.age.unique()), total_data.gender.unique()","b7ff1051":"\ngender_map = {'f':0, \n             'm':1,\n             'u':2}\nage_map = {\n    '0-2'  :0,\n    '4-6'  :1,\n    '8-13' :2,\n    '15-20':3,\n    '25-32':4,\n    '38-43':5,\n    '48-53':6,\n    '60+'  :7\n}\ntotal_data.gender = total_data.gender.replace(gender_map)\ntotal_data.age=total_data.age.replace(age_map)","aa038aa3":"gender_labels = total_data.gender.values.tolist()\nage_labels= total_data.age.values.tolist()\ntrain_paths = total_data.full_path.values.tolist()\nlen(gender_labels), gender_labels[0],len(age_labels),age_labels[0], train_paths[0]","2ee1cff9":"from sklearn.preprocessing import OneHotEncoder\nshuffle_list = list(zip(train_paths, gender_labels,age_labels))\nshuffle_list = random.sample(shuffle_list, len(train_paths))\ntrain_paths, gender_labels,age_labels = zip(*shuffle_list)\nage_labels = np.array(list(age_labels)).reshape((-1, 1))\nenc= OneHotEncoder()\nage_labels = enc.fit_transform(age_labels).toarray() \ngender_labels = np.array(list(age_labels)).reshape((-1, 1))\nenc= OneHotEncoder()\ngender_labels = enc.fit_transform(gender_labels).toarray() ","f31430aa":"train_split = 0.75\ntrain_sample = int(train_split * len(total_data))\n\ntrain_data = train_paths[:train_sample]\nvalidation_data = train_paths[train_sample:]\n\ntrain_labels_gender = gender_labels[:train_sample]\nvalidation_labels_gender = gender_labels[train_sample:]\n\ntrain_labels_age=age_labels[:train_sample]\nvalidation_labels_age=age_labels[train_sample:]\nprint(\"train data count:\")\nlen(train_data), len(train_labels_gender), len(train_labels_age)","f19590ad":"import tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nimport multiprocessing","38b21b39":"train_dataset = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels_age)))\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((list(validation_data),list(validation_labels_age)))","09ca6e7c":"for path, target1 in train_dataset.take(1):\n    print(path, target1)","f1f33242":"def preprocess_func(path, label_age):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n    return image, label_age","aa52e6a1":"def preprocess_func(path, label_age):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n    return image, label_age","b6d928d1":"import multiprocessing\ntrain_batches = train_dataset.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)\nvalidation_batches = validation_dataset.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)","bd866674":"for image, target1 in train_batches.take(1):\n    print(image.shape, target1.shape)\n    image = tf.squeeze(image[0])\n    print(target1[0])\n\n    plt.imshow(image)\n    plt.show()\n    break","8c56e215":"#model = tf.keras.models.Sequential([\n #   tf.keras.layers.Conv2D(8, 3, padding='same', strides=2, activation='relu', input_shape=(128, 128, 3)),\n  #  tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu'),\n  #  tf.keras.layers.MaxPooling2D(),\n   # tf.keras.layers.Dropout(0.3),\n    \n    #tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n    #tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n    #tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.3),\n    \n    #tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n    #tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n    #tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.35),\n    \n    #tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n    #tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n    #tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.40),\n    \n    #tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n    #tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n   # tf.keras.layers.MaxPooling2D(),\n   # tf.keras.layers.Dropout(0.45),\n    \n    #tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n    #tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n   # tf.keras.layers.MaxPooling2D(),\n  #  tf.keras.layers.Dropout(0.5),\n    \n  #  tf.keras.layers.Flatten(),\n #   tf.keras.layers.Dense(8, activation = 'softmax')\n#])\n\n#model.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n#model.summary()","31712d7f":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(8, 3, padding='same', strides=1, activation='relu', input_shape=(128, 128, 3)),\n    tf.keras.layers.Conv2D(16, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.4),\n    \n    tf.keras.layers.Conv2D(16, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.Conv2D(32, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.45),\n    \n    tf.keras.layers.Conv2D(32, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.Conv2D(64, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(8, activation = 'softmax')\n])\n\nmodel.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nmodel.summary()","d8c5ab21":"tf.keras.utils.plot_model(model, show_shapes=True)","bfb409d7":"#model = tf.keras.models.Sequential([\n #   tf.keras.layers.Conv2D(8, 3, padding='same', strides=1, activation='relu', input_shape=(128, 128, 3)),\n  #  tf.keras.layers.Conv2D(16, 3, padding='same', strides=1, activation='relu'),\n   # tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.3),\n    \n    #tf.keras.layers.Conv2D(16, 3, padding='same', strides=1, activation='relu'),\n    #tf.keras.layers.Conv2D(32, 3, padding='same', strides=1, activation='relu'),\n    #tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.35),\n    \n    #tf.keras.layers.Conv2D(32, 3, padding='same', strides=1, activation='relu'),\n    #tf.keras.layers.Conv2D(64, 3, padding='same', strides=1, activation='relu'),\n    #tf.keras.layers.MaxPooling2D(),\n    #tf.keras.layers.Dropout(0.45),\n    \n    #tf.keras.layers.Flatten(),\n    #tf.keras.layers.Dense(8, activation = 'softmax')\n#])\n\n#model.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n#model.summary()","c601c54f":" history = model.fit(train_batches, epochs=40, validation_data = validation_batches)","8f4bf206":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')","3beda249":"image_path = validation_data[7]\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (128, 128)) \/ 255.0\nplt.imshow(image)\nplt.show()","eb6eef6c":"image = np.expand_dims(image, 0)\nprediction = model.predict(image)","03d72e97":"index = np.argmax(prediction)\ndecoding = {0:'0-2', 1:'4-6', 2:'8-13',3:'15-20',4:'25-32',5:'38-43',6:'48-53',7:'60+'}\n\nprint('[+] prediction is :', decoding[index]) ","ec0fb33a":"#len(train_data), len(train_labels_gender)\n#len(validation_data), len(validation_labels_gender)","1a9c3f7f":"train_split = 0.75\ntrain_sample = int(train_split * len(total_data))\n\ntrain_data = train_paths[:train_sample]\nvalidation_data = train_paths[train_sample:]\n\ntrain_labels_gender = gender_labels[:train_sample]\nvalidation_labels_gender = gender_labels[train_sample:]\n\n#train_labels_age=age_labels[:train_sample]\n#validation_labels_age=age_labels[train_sample:]\nprint(\"train data count:\")\nlen(train_data), len(train_labels_gender)","e44ef945":"train_dataset2 = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels_gender)))\nvalidation_dataset2 = tf.data.Dataset.from_tensor_slices((list(validation_data),list(validation_labels_gender)))","eecf7019":"for path, target1 in train_dataset2.take(1):\n    print(path, target1)","2b9446a5":"def preprocess_func(path, label_gender):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n    return image, label_gender","8b96de30":"train_batches2 = train_dataset2.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)\nvalidation_batches2 = validation_dataset2.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)","a2c455bd":"for image, target1 in train_batches.take(1):\n    print(image.shape, target1.shape)\n    image = tf.squeeze(image[0])\n    print(target1[0])\n\n    plt.imshow(image)\n    plt.show()\n    break","f3c8aa3c":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(8, 3, padding='same', strides=2, activation='relu', input_shape=(128, 128, 3)),\n#     tf.keras.layers.MaxPooling2D(),\n#      tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n     #tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n#     tf.keras.layers.MaxPooling2D(),\n#      tf.keras.layers.Dropout(0.45),\n    \n    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n     tf.keras.layers.Dropout(0.4),\n        \n     tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n      tf.keras.layers.Conv2D(258, 3, padding='same', activation='relu'),\n     tf.keras.layers.MaxPooling2D(),\n     #tf.keras.layers.Dropout(0.5), \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(3, activation = 'softmax')\n])\n\nmodel.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nmodel.summary()","4dd2a54a":" history = model.fit(train_batches2, epochs=30, validation_data = validation_batches2)","e325d3e7":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.2, 1])\nplt.legend(loc='lower right')","dd4f667b":"image_path = validation_data[7]\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (128, 128)) \/ 255.0\nplt.imshow(image)\nplt.show()","24a3e59b":"image = np.expand_dims(image, 0)\nprediction = model.predict(image)","c39b2891":"index = np.argmax(prediction)\ndecoding = {0:'f', 1:'m', 2:'u'}\n\nprint('[+] prediction is :', decoding[index]) ","2c4b117d":"**gender classification with results**","8d8056f6":"**Age classification (training with results)**","614da6ba":"# Age chart","9441975e":"# change age and gender mapping ","cb5e9552":"# Gender chart"}}