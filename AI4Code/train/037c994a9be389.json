{"cell_type":{"f7d9d4f5":"code","f7d25271":"code","c5237844":"code","2c878d4a":"code","bc67a896":"code","06b40f6a":"code","584c666e":"code","2a134054":"code","e66a050e":"code","fc7c86fa":"code","0055d9ce":"code","de21b606":"code","fbc54bd6":"code","7c9714e4":"code","505b400b":"code","8de37488":"code","a73965a1":"code","14888a3a":"code","3c8a07ad":"code","ae2f0236":"code","766832a2":"code","e1da673a":"code","1600226f":"code","aee40b95":"code","09547402":"code","60397ef0":"code","cb3fbfb0":"code","21fdf2dd":"code","c5fd2156":"code","c4b11939":"code","074fa3a7":"code","1ce0f9e0":"code","91ef2534":"code","d64bd559":"code","a1a94e15":"code","8428ae99":"code","c5fd3c25":"code","beaf1ea7":"code","bd965211":"code","f7272abb":"code","6589421f":"code","6eb7a6c2":"code","35e77501":"code","fa08eb2e":"code","50c071d9":"code","c2318f52":"code","a0fdee4f":"code","8df00895":"code","887c3531":"code","38837918":"code","66510656":"code","9dc230d5":"code","5a0dc4e3":"code","53675224":"code","e5fec69f":"code","e99483c2":"code","63de104f":"code","e05b4211":"code","b5256fc0":"markdown","30d95fbb":"markdown","a2584c73":"markdown","fc4aae44":"markdown","f7cfd278":"markdown","e792ec88":"markdown","529fa4fe":"markdown","dd3304a6":"markdown","302a11dc":"markdown","d0c8379c":"markdown","4824d51b":"markdown","1eb02834":"markdown","3461f69a":"markdown","8ca0e610":"markdown","6720b8b4":"markdown","a0f8a43a":"markdown","41203817":"markdown","9f7e0c7a":"markdown","77c6577d":"markdown","6cf9fdcf":"markdown","751a32bc":"markdown","6413b9d5":"markdown","c8e0d009":"markdown","14b0f097":"markdown"},"source":{"f7d9d4f5":"import string\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\n\n\n%matplotlib inline","f7d25271":"dataset_path = '..\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv'\ndata = pd.read_csv(dataset_path)\n\ndata","c5237844":"data.dropna(subset=['body'], inplace=True)","2c878d4a":"data['original_body'] = data['body']","bc67a896":"data.shape","06b40f6a":"nlp = spacy.blank('en')","584c666e":"import re\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","2a134054":"def remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","e66a050e":"def remove_stop_words(text):\n    doc = nlp(text)\n    return \" \".join([token.text for token in doc if not token.is_stop])","fc7c86fa":"def lemmatize_words(text):\n    doc = nlp(text)\n    return \" \".join([token.lemma_ for token in doc])","0055d9ce":"remove_spaces = lambda x : re.sub('\\\\s+', ' ', x)","de21b606":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)","fbc54bd6":"remove_double_quotes = lambda x : x.replace('\"', '')\nremove_single_quotes = lambda x : x.replace('\\'', '')\ntrim = lambda x : x.strip()","7c9714e4":"other_chars = ['*', '#', '&x200B', '[', ']', '; ',' ;' \"&nbsp\", \"\u201c\",\"\u201c\",\"\u201d\", \"x200b\"]\ndef remove_other_chars(x: str):\n    for char in other_chars:\n        x = x.replace(char, '')\n    \n    return x","505b400b":"def lower_case_text(text):\n    return text.lower()","8de37488":"funcs = [\n    remove_urls, \n    remove_punctuation,\n    remove_stop_words, \n    remove_emoji, \n    remove_double_quotes, \n    remove_single_quotes,\n    lower_case_text,\n    remove_other_chars,\n    lemmatize_words,\n    remove_spaces,\n    trim]\n\nfor fun in funcs:\n    data['body'] = data['body'].apply(fun)","a73965a1":"# reset indexes (again)\ndata.reset_index(inplace=True)\ndata.drop(['index'], axis=1, inplace=True)\n\ndata","14888a3a":"''.join(char for char in data.body.loc[4] if char in string.printable)","3c8a07ad":"body_list = data.body.tolist()","ae2f0236":"body_list[0]","766832a2":"from collections import Counter \n\ncounter = Counter()\n\nfor body in body_list:\n    doc = nlp(body)\n    counter.update([token.text for token in doc])","e1da673a":"most_common_unigrams = counter.most_common()[0:30]\nwords = [item[0] for item in most_common_unigrams]\nfreq = [item[1] for item in most_common_unigrams]","1600226f":"plt.figure(figsize=(8, 25))\nsns.barplot(y=words, x=freq, color='red')","aee40b95":"from wordcloud import WordCloud, STOPWORDS","09547402":"fig_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='lightgrey', \n                          colormap='viridis', width=800, height=600\n                         ).generate(' '.join(body_list))\n\nplt.figure(figsize=(10, 7), frameon=True)\nplt.imshow(fig_wordcloud)\nplt.axis('off')\nplt.show()","60397ef0":"def generate_ngrams(text, n_gram=2):\n    token = [token for token in text.lower().split(' ') if token != '']\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","cb3fbfb0":"bigram_counter = Counter()\n\nfor body in body_list:\n    bigram_counter.update(generate_ngrams(body, 2))","21fdf2dd":"most_common_bigrams = bigram_counter.most_common()[0:30]\nbigrams = [item[0] for item in most_common_bigrams]\nbi_freq = [item[1] for item in most_common_bigrams]","c5fd2156":"plt.figure(figsize=(8, 25))\nsns.barplot(y=bigrams, x=bi_freq, color='green')","c4b11939":"trigram_counter = Counter()\n\nfor body in body_list:\n    trigram_counter.update(generate_ngrams(body, 3))","074fa3a7":"most_common_trigrams = trigram_counter.most_common()[0:30]\ntrigrams = [item[0] for item in most_common_trigrams]\ntri_freq = [item[1] for item in most_common_trigrams]","1ce0f9e0":"plt.figure(figsize=(8, 25))\nsns.barplot(y=trigrams, x=tri_freq, color='blue')","91ef2534":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy import linalg\nfrom sklearn import decomposition\nimport fbpca\n\nnumber_of_topics = 10\nnum_top_words = 8\nvectorizer = TfidfVectorizer()","d64bd559":"vectors = vectorizer.fit_transform(body_list).todense()\nvocab = np.array(vectorizer.get_feature_names())","a1a94e15":"vectors.shape, vocab.shape","8428ae99":"def show_topics(a, vocab, ngram=False):\n    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n    topic_words = ([top_words(t) for t in a])\n    if not ngram:\n        return [' '.join(t) for t in topic_words]\n    else:\n        return [' - '.join(t) for t in topic_words]","c5fd3c25":"u, s, v = fbpca.pca(vectors, number_of_topics)","beaf1ea7":"show_topics(v[:10], vocab)","bd965211":"show_topics(v[:10], vocab)[np.argmax(u[107])]","f7272abb":"data.original_body.loc[107]","6589421f":"show_topics(v[:10], vocab)[np.argmax(u[2179])]","6eb7a6c2":"data.original_body.loc[2179]","35e77501":"bigram_vectorizer = TfidfVectorizer(ngram_range=(2,2))","fa08eb2e":"bigrams_vectors = bigram_vectorizer.fit_transform(body_list).todense()\nbigrams_vocab = np.array(bigram_vectorizer.get_feature_names())","50c071d9":"u1, s1, v1 = fbpca.pca(bigrams_vectors, number_of_topics)","c2318f52":"show_topics(v1[:10], bigrams_vocab, ngram=True)","a0fdee4f":"show_topics(v1[:10], bigrams_vocab, ngram=True)[np.argmax(u1[280])]","8df00895":"data.original_body.loc[280]","887c3531":"del v1, u1, s1, bigrams_vectors, bigrams_vocab","38837918":"docs = [body.split() for body in body_list]","66510656":"from gensim.corpora import Dictionary\n\ndic = Dictionary(docs)","9dc230d5":"corpus = [dic.doc2bow(doc) for doc in docs]","5a0dc4e3":"from gensim.models import LdaModel\n\nmodel = LdaModel(corpus=corpus, id2word=dic, num_topics=number_of_topics, chunksize=2500, passes=5, random_state=1)","53675224":"for (topic, words) in model.print_topics():\n    print(topic+1, \":\", words, '\\n\\n')","e5fec69f":"original_body_list = data.original_body.tolist()","e99483c2":"for (text, doc) in zip(original_body_list[:9], docs[:9]):\n    print('\\033[1m' + 'Text: ' + '\\033[0m', text)\n    print('\\033[1m' + 'Topics: ' + '\\033[0m', [(topic+1, prob) for (topic, prob) in model[dic.doc2bow(doc)] if prob > 0.15])\n    print('\\n')","63de104f":"print('\\033[1m' + 'Text: ' + '\\033[0m', original_body_list[2179])\nprint('\\033[1m' + 'Topic: ' + '\\033[0m', [(topic+1, prob) for (topic, prob) in model[dic.doc2bow(docs[2179])] if prob > 0.1])","e05b4211":"print('\\033[1m' + 'Text: ' + '\\033[0m', original_body_list[107])\nprint('\\033[1m' + 'Topic: ' + '\\033[0m',[(topic+1, prob) for (topic, prob) in model[dic.doc2bow(docs[107])] if prob > 0.1])","b5256fc0":"## Most frequent ngrams\n\nNow that most of the meaningless words have been removed, let's see which are the most frequent unigrams.","30d95fbb":"## Bigrams matrix\n\nInstead of decomposing only the **term-document** matrix, let's now try to apply the **SVD** decomposition to the **bigram-document** matrix just to see what the outcomes look like.","a2584c73":"# EDA\n\nBefore we go straight to the model build phase, let's see how's the data look like, just to have an overview.","fc4aae44":"# WSB Topic Modelling\n\nFrom the mess that we've experienced in the financial markets in February 2020 due to the Redditers horde, I've got curious about the whys and hows this event happen.\n\nIn this notebook, I'll try to satisfy my curiosity by trying to find what are the main topics in the body of the wallstreetbets posts.","f7cfd278":"## Training \n\nNow it's time to train our topic model. We do this with the following parameters:\n\n- **corpus**: the bag-of-word representations of our documents\n- **id2token**: the mapping from indices to words\n- **num_topics**: the number of topics we want the model to identify\n- **chunksize**: the number of documents the model sees for every update\n- **passes**: the number of times we show the total corpus to the model during training\n- **random_state**: we use a seed to ensure reproducibility.","e792ec88":"As you might guess in the top 3 we have the name of the stock that made r\/wsb famous, we are talking about `GME`!\n\nWhereas if you look further in the top 40 you might encounter words like: stock, market, sell, share, trading, ... which are all words related to the financial world.","529fa4fe":"As we can see the topics that we've obtained from the decomposition of the bigram matrix are way more interpretable compared to the topic that we've obtained from the unigram matrix decomposition. For instance the first topic might be related to the weekly thread posting since it contains bigrams such as *best daily*, *discussion thread*, and so on ... While are a few topic that contains the bigram *f#ck robinhood*, those topic might be related to negative comments on the broker robinhood.","dd3304a6":"While also in this case **topic 4** and **topic 5** are quite right, **topic 9** I think is out of place since this post isn't a daily trading discussion.","302a11dc":"While in the topics assigned to post **2179** is also making sense: both **NOK** and **AMC** appear in the topic.","d0c8379c":"Let's now see what are the most common **bigrams** and **trigrams** in the dataset.","4824d51b":"To build out LDA model we are going to use a fantastic library for topic modelling: [Gensim](https:\/\/radimrehurek.com\/gensim\/intro.html)! But before to build and train we to to build a dictionary of the vocabulary the the model is going to use. A dictionary in NLP is simply a mapping between words and their integer ids.","1eb02834":"# Topic Modelling with LDA\n\nAnother method used for finding topics in documents is the [**Latent Dirichlet Allocation**](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) (**LDA**). LDA is a generative statistical model where documents are represented as a mixture of topics and a topic is a bunch of words. Those topics reside within a hidden, also known as a latent layer. \n\n![lda_img.png](attachment:823b8d03-4129-4f14-a36f-007e757f34eb.png)\n\nLDA looks at a document to determine a set of topics that are likely to have generated that collection of words. So, if a document uses certain words that are contained in a topic, you could say the document is about that topic. ","3461f69a":"# Conclusion \n\nFinding patterns and understanding the hidden structure of data is a complicated task. Especially when we are dealing with messy and unstructured data as text. Topic models such as Latent Dirichlet Allocation or matrices decomposition are useful techniques to discover the most prominent topics in such documents. While these results are often very revealing already, it's also possible to use them as a starting point, for example for a labeling exercise for supervised text classification. Although traditional topic models are lacking in more semantic information (they don't use word embeddings, for instance), they should be in every NLPer's toolkit as a really quick way of getting insights into large collections of documents.","8ca0e610":"The topics assigned to post **107** kinda match what the post is expressing because both **AMC** and **GME** are present in the text.","6720b8b4":"Finally, let's inspect the topics the model recognizes in some of the individual documents. Here we see how LDA tends to assign a high probability to a low number of topics for each document, which makes its results very interpretable.","a0f8a43a":"# Topic Modelling using SVD\n\nA common technique for find topics in text data is through matrix decomposition. Matrices decomposition are factorizations that \"decompose\" a matrix into a product of simpler matrices. This factorization can be exact (the product of the simpler matrices gives back the original matrix) or not exact (the product of the simpler matrices gives back something similar to the matrix). \n\nMatrices decompositions are discovered to be useful in topic modeling because the simpler matrices capture some kind of hidden relationship between the documents and the words.  \n\nInstead of using a **term-document** matrix (which is a mathematical matrix that describes the frequency of terms that occur in a collection of documents), we will use \n[Topic Frequency-Inverse Document Frequency](http:\/\/www.tfidf.com\/) (TF-IDF) as a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how common\/rare the term is.\n\nWe'll decompose the tf-idf matrix using a famous matrix decomposition called the Singular Value Decomposition (SVD). If you know a little about linear algebra, you can see the SVD decomposition as a generalization of the eigen decomposition for non square matrices. \n\nThe result of the SVD decomposition are three matrices as shown here:\n\n![svd.png](attachment:39bd9db0-e463-48c6-bbc2-b8ade940f722.png)\n\nIn our case the first matrix $U$ is called the ***document-to-topic*** matrix, it is a $document \\times topic$ matrix, and it captures the probabilities of topics for each document.\n\nThe $\\Sigma$ matrix is a non-negative square matrix is called ***topic-to-topic*** matrix, with $topic \\times topic$ dimension and it captures the importance of each topic.\n\nWhile the last matrix, the $V$ matrix is called the ***topic-to-word*** matrix, and this last matrix captures the probabilities that each word appears in a topic.\n\nThe values in the decomposed matrices don't have to be interpreted as probabilities, because they can be negative, but despite that, I think that gives a better idea of the meaning of these matrices.","41203817":"The topics are obtained by taking the words with highest \"probability\" form the matrix `v` (the **topic-to-words** matrix).  \n\nThose are the topics that we've got from the matrix decomposion. As we can see most of them contains at least one the most talked stocks in the subreddit such as GME, AMC, NOK, BB, ... but also words such as holding, moon, buy, stock are appear in the topics. \n\nNow let's see if the found topics represents, or at least try to do it, the body of the wsb posts.","9f7e0c7a":"# Text Cleaning & Preprocessing\n\nOne of the most crucial phases when dealing with unstructured data such as text is the cleaning\/preprocessing step. Sometimes this process is even more important than the model-building part.\n\nSince both the methodologies that I'll use to find the topics in wsb posts are simple models, it is better to remove words that don't carry much information about the post itself such as punctuation, stop words, and others...\n\nIn this section of the kernel, I'm going to clean the `body` of the DataFrame preparing it for the successive phases.\n\nThe cleaning steps that I'm going to apply are:\n- Removal of URLs\n- Removal of punctuation\n- Removal of emojis\n- Lower casing\n- Removal of stopwords\n- Lemmatization\n- Removal of other non-meaningful characters","77c6577d":"the topics assigned by the LDA are not 100% spot-on, **topic 5** seems to be appropriate but **topic 9** seems to be out of place.\n\nLet's see what kind of topic the LDA is going to choose for the post **280** tested with the bigram matrix decomposition.","6cf9fdcf":"Here the topic should be about a negative review on Robinhood, but the assigned topic haven't any words related to Robinhood or negativity. I think that topic 8 (or 9) should be more appropriate in this case.","751a32bc":"Now let's see what topic is going to assign to the post-**2179**, previously tested with the svd decomposition for the unigram term-document matrix.","6413b9d5":"# References\n\n- [Getting Started with Text Preprocessing](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing\/comments#1272867)\n- [Fast.ai NLP Course Topic Modelling Lesson 2](https:\/\/www.youtube.com\/watch?v=tG3pUwmGjsc&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=2&t=2s)\n- [Fast.ai NLP Course Topic Modelling Lesson 3](https:\/\/www.youtube.com\/watch?v=lRZ4aMaXPBI&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=3)\n- [NLP with Disaster Tweets by gunes evitan](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#3.-Target-and-N-grams)\n- [Discovering and Visualizing Topics in Texts with LDA](https:\/\/github.com\/nlptown\/nlp-notebooks\/blob\/master\/Discovering%20and%20Visualizing%20Topics%20in%20Texts%20with%20LDA.ipynb)","c8e0d009":"First of all, let's all the columns of the dataframe that have empty body.","14b0f097":"Now that our model is trained, let's output the topics that he has learnt. For each topic we'll print the 10 most significant words, hence mathematically speaking the words with the highest probability to appear in the topic. This is showing some interesting patterns already: **topic 9** will likely related to daily trading discussion thread; **topic 5** and **topic 4** will be related to markets, stocks, options, and everything realted to finance; **topic 10** seems linked to cannabis and marijuana since tickers related to the latter appears in the topic."}}