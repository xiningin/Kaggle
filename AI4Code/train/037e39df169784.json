{"cell_type":{"fd056391":"code","3fbc7796":"code","f6a04bc4":"code","63145605":"markdown","4455fe0e":"markdown","bca3da44":"markdown","20bb2f6a":"markdown"},"source":{"fd056391":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter, MonthLocator\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom datetime import datetime\n\ndataset = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\")\ntotal_sales = dataset.loc[:, [\"date\", \"num_sold\"]].groupby(\"date\").sum()\ntotal_sales.index = pd.to_datetime(total_sales.index)\n\nfig, ax = plt.subplots(figsize=(20, 3))\nax.plot(total_sales, label=\"Total Sales\", c=\"#2B2D42\", lw=1)\nax.margins(x=0)\nax.legend(loc=2, edgecolor=\"#FFF\")\nfor label in ax.xaxis.get_ticklabels():\n    label.set_horizontalalignment('left')","3fbc7796":"decompose_result_mult = seasonal_decompose(total_sales, model=\"multiplicative\")\n\ntrend = decompose_result_mult.trend\nseasonal = decompose_result_mult.seasonal\n# trend.plot(figsize=(20, 3))\n# seasonal.plot(figsize=(20, 3), c=\"#2B2D42\")\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") \/ pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots(figsize = (20, 3))\n#     print(freqencies)\n#     print(spectrum)\n    ax.step(freqencies, spectrum, color=\"#2B2D42\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"A\",\n            \"SA\",\n            \"Q\",\n            \"BM\",\n            \"M\",\n            \"BW\",\n            \"W\",\n            \"SW\",\n        ],\n        rotation=0,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    ax.text(max(freqencies)-150, max(spectrum), \n            \"\"\"\n            A  - Annual (1)\n            SA - Semi Annual (2)\n            Q  - Quarterly (4)\n            BM - Bi Monthly (6)\n            M  - Monthly (12)\n            BW - Bi Weekly (26)\n            W  - Weekly (52)\n            SW - Semi Weekly (104)\n            \"\"\", ha=\"left\", va=\"top\")\n    return ax\n\n\n\nfig = plt.figure(tight_layout=True, figsize=(20, 5))\ngs = gridspec.GridSpec(2, 2)\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[0:, 1])\nax1.plot(trend, c=\"#2B2D42\", label=\"Trend\")\nax2.plot(seasonal, c=\"#2B2D42\", label=\"Seasonal\")\nfor i in [ax1, ax2, ax3]:\n    i.margins(x=0)\n    i.legend(loc=2, edgecolor=\"#FFF\")\nplot_periodogram(total_sales.num_sold, ax=ax3);\n\nplt.show()","f6a04bc4":"finland_dataset = dataset.loc[dataset[\"country\"]==\"Finland\", :]\nnorway_dataset = dataset.loc[dataset[\"country\"]==\"Norway\", :]\nsweden_dataset = dataset.loc[dataset[\"country\"]==\"Sweden\", :]\n\naverage_sales_finland = finland_dataset.loc[:, [\"date\", \"num_sold\"]].groupby(\"date\").mean()\naverage_sales_finland.index = pd.to_datetime(average_sales_finland.index)\n# average_monthly_sales_finland = average_sales_finland.resample('M').mean()\n# average_monthly_sales_finland.index = pd.to_datetime(average_monthly_sales_finland.index.strftime('%Y-%m'))\n\naverage_sales_norway = norway_dataset.loc[:, [\"date\", \"num_sold\"]].groupby(\"date\").mean()\naverage_sales_norway.index = pd.to_datetime(average_sales_norway.index)\n# average_monthly_sales_norway = average_sales_norway.resample('M').mean()\n# average_monthly_sales_norway.index = pd.to_datetime(average_monthly_sales_norway.index.strftime('%Y-%m'))\n\naverage_sales_sweden = sweden_dataset.loc[:, [\"date\", \"num_sold\"]].groupby(\"date\").mean()\naverage_sales_sweden.index = pd.to_datetime(average_sales_sweden.index)\n# average_monthly_sales_sweden = average_sales_sweden.resample('M').mean()\n# average_monthly_sales_sweden.index = pd.to_datetime(average_monthly_sales_sweden.index.strftime('%Y-%m'))\n\naverage_sales_finland_sectioned = average_sales_finland[\"2017-10-01\":\"2018-04-01\"]\naverage_sales_finland_sectioned.index = pd.to_datetime(average_sales_finland_sectioned.index)\n\naverage_sales_norway_sectioned = average_sales_norway[\"2017-10-01\":\"2018-04-01\"]\naverage_sales_norway_sectioned.index = pd.to_datetime(average_sales_norway_sectioned.index)\n\naverage_sales_sweden_sectioned = average_sales_sweden[\"2017-10-01\":\"2018-04-01\"]\naverage_sales_sweden_sectioned.index = pd.to_datetime(average_sales_sweden_sectioned.index)\n\nfig, ax = plt.subplots(3, 2, figsize=(20, 9), gridspec_kw={'width_ratios': [4, 1]})\nfig.subplots_adjust(wspace = 0.02, hspace= 0.2)\ndatemin = datetime(2017, 10, 1)\ndatemax = datetime(2018, 3, 1) \nfor i in range(3):\n    for j in range(2):\n        ax[i][j].set_ylim(100, 1600)\n        ax[i][j].set_yticks([100, 600, 1100, 1600])\n#         if i!=2 and j!=1:\n#         ax[i][j].set_xticks([])\n        ax[i][j].margins(x=0)\n        if j==1:\n            ax[i][j].set_yticks([])\n        for k in ['right', 'top']:\n            ax[i][j].spines[k].set_visible(False)\n        if j==1:\n            ax[i][j].set_xticks([])\n            ax[i][j].spines[\"left\"].set_visible(False)\n            ax[i][j].spines[\"bottom\"].set_visible(False)\n        for label in ax[i][j].xaxis.get_ticklabels():\n            label.set_horizontalalignment('left')\n            \nax[0][0].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-10-01'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-01'), format='%Y-%m-%d'), facecolor ='#707070', alpha = 0.2)\nax[0][0].plot(average_sales_finland[:\"2017-10-01\"], label=\"Finland\", c=\"#2B2D42\", lw=1)\nax[0][0].plot(average_sales_finland[\"2018-04-01\":], label=\"Finland\", c=\"#2B2D42\", lw=1)\nax[0][0].plot(average_sales_finland_sectioned, c=\"red\", lw=1)\nax[0][0].plot(average_sales_norway, label=\"Norway\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[0][0].plot(average_sales_sweden, label=\"Sweden\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[0][0].text(datetime(2015, 1, 10), 1500, \"Finland\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[0][0].text(datetime(2015, 1, 10), 1400, \"Total sales in Finland\\nThrough years\", fontsize=10, ha=\"left\", va=\"top\")\n# ax[0][0].scatter([datetime(2015, 12, 30), datetime(2016, 12, 31), datetime(2017, 12, 30)], average_sales_finland.loc[['2015-12-30', '2016-12-31', '2017-12-30'], :], c=\"red\", marker=\"3\")\n\nax[1][0].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-10-01'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-01'), format='%Y-%m-%d'), facecolor ='#707070', alpha = 0.2)\nax[1][0].plot(average_sales_finland, label=\"Finland\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[1][0].plot(average_sales_norway[:\"2017-10-01\"], label=\"Norway\", c=\"#2B2D42\", lw=1)\nax[1][0].plot(average_sales_norway[\"2018-04-01\":], label=\"Norway\", c=\"#2B2D42\", lw=1)\nax[1][0].plot(average_sales_norway_sectioned, c=\"red\", lw=1)\nax[1][0].plot(average_sales_sweden, label=\"Sweden\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[1][0].text(datetime(2015, 1, 10), 1500, \"Norway\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[1][0].text(datetime(2015, 1, 10), 1400, \"Total sales in Norway\\nThrough years\", fontsize=10, ha=\"left\", va=\"top\")\n\nax[2][0].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-10-01'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-01'), format='%Y-%m-%d'), facecolor ='#707070', alpha = 0.2)\nax[2][0].plot(average_sales_finland, label=\"Finland\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[2][0].plot(average_sales_norway, label=\"Norway\", c=\"#707070\", alpha=0.3, lw=0.5)\nax[2][0].plot(average_sales_sweden[:\"2017-10-01\"], label=\"Sweden\", c=\"#2B2D42\", lw=1)\nax[2][0].plot(average_sales_sweden[\"2018-04-01\":], label=\"Sweden\", c=\"#2B2D42\", lw=1)\nax[2][0].plot(average_sales_sweden_sectioned, c=\"red\", lw=1)\nax[2][0].text(datetime(2015, 1, 10), 1500, \"Sweden\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[2][0].text(datetime(2015, 1, 10), 1400, \"Total sales in Seden\\nThrough years\", fontsize=10, ha=\"left\", va=\"top\")\n\nax[0][1].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-9-25'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-5'), format='%Y-%m-%d'), ymin=-0.5, facecolor ='#707070', alpha = 0.2)\nax[0][1].plot(average_sales_finland_sectioned, c=\"red\", lw=1)\nax[0][1].text(datetime(2017, 10, 1), 1500, \"Semi-annual view\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[0][1].text(datetime(2017, 10, 1), 1400, \"Oct 2017 - Mar 2018\", fontsize=8, ha=\"left\", va=\"top\")\n\nax[1][1].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-9-25'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-5'), format='%Y-%m-%d'), ymin=-0.5, facecolor ='#707070', alpha = 0.2)\nax[1][1].plot(average_sales_norway_sectioned, c=\"red\", lw=1)\nax[1][1].text(datetime(2017, 10, 1), 1500, \"Semi-annual view\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[1][1].text(datetime(2017, 10, 1), 1400, \"Oct 2017 - Mar 2018\", fontsize=8, ha=\"left\", va=\"top\")\n\nax[2][1].axvspan(pd.to_datetime(160, origin=pd.Timestamp('2017-9-25'), format='%Y-%m-%d'), pd.to_datetime(160, origin=pd.Timestamp('2018-04-5'), format='%Y-%m-%d'), ymin=-0.5, facecolor ='#707070', alpha = 0.2)\nax[2][1].plot(average_sales_sweden_sectioned, c=\"red\", lw=1);\nax[2][1].text(datetime(2017, 10, 1), 1500, \"Semi-annual view\", fontsize=12, ha=\"left\", va=\"center\", weight=\"bold\")\nax[2][1].text(datetime(2017, 10, 1), 1400, \"Oct 2017 - Mar 2018\", fontsize=8, ha=\"left\", va=\"top\");","63145605":"### **Total Sales through locations**\nThree locations are invloved in the data **[Finland, Norway, Sweden]**. These locations has it's part in sales in that area. For being detail, there is a visualization below showing **Norway is a better seller** than other two locations. **Finland seems to be the lowest** of them all. These differences is more useful in recognizing the place where sales could be focused more.\n\nThe below visualization also iunclude **semi-annual view** of all locations [October 2017 - March 2018], which zooms the data more clearly on a single peak.","4455fe0e":"### **Kaggle sales** \n###### **(If you know about to data, Skip this part. Please move to the introduction)**<br \/>\nThere are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. They've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, They've collected some data and are asking us to build forecasting models to help them decide.\n\nThey want us to help with figuring out whether KaggleMart or KaggleRama should become the official Kaggle outlet!\n\nUnderstanding the data might help us see the problem more clearly. The data is well managed(nothing to complaint about), perfectly captured(Thanks to kaggle). These type of data are helpful in practising time series analysis for beginners who want to puch their levels. Let's fall back to the data,\n\n##### **Features**\n* **row_id** - Representing individual **id** for each row\n* **date** - Date of the sales **(Year-Month-Day)**\n* **country** - Holding three unique values where the sales are recorded **[Finland, Norway, Sweden]**\n* **store** - Denoting the store chain which kaggle represented **[KaggleMart, KaggleRama]**\n* **product** - Various products in stores **[Kaggle Mug, Kaggle Hat, Kaggle Sticker]**\n* **num_sold** - Total number of respective product sold in that **particular day**\n\n### **Introduction**\nThis notebook is only focused on EDA of this particular data. Although, it's not same as other EDA. The plots used in this notebooks are found to be better visualized and explained. Just a simple line plot with few improvements mayhelp us see the data more clearly than before.\n\n##### **Table of contents**\n* Periodic total sales of kaggle products *(averaging all countries, stores and products)*\n    * Reviewing seasonality\n* Periodic total sales of kaggle products through countries *(averaging all stores and products)*\n    * Country influence\n    * Semi-Annual view of each country sales\n    \n\n### **Total Sales**\nThe sales are spreaded into three different countries and different products. By observing the data more clearly we can understand that there are **18 entries in same date**. These duplications*(not officially)* of date are caused by the combinations of **3 countries, 2 stores and 3 products (3*2*3=18 entries)**. The below figure is sum of all these 18 entries in same date showing the total sales of kaggle products.","bca3da44":"##### **Reviewing seasonality**\nIt doesn't need to be detailed, we can diretly find the seasonality. Anyhow, lets dig more through the plot. From the above section we can simply say the bigger sales are at the start of the year(approximately could be around the end of the year). The **highest sale** is on **29th december 2018**, simply assuming the sale would be lot bigger than this count next year.\n\nSeasonality, would be more important task in this data. We could find a lot more seasonality just by viewing the data plot. But, the biggest problem is to find whether the data is showing additive or multiplicative seasonality. On sense, our data is showing **low variations through large period** to put it under multiplicative, at the same time, it could be increasing slightly every year through multiplicative. Hence, we trapped under a confusion. But, we have to choose one here.\n\nI'm gonna make a decision as **multiplicative seasonality**, as we could possibly find more sales in future years.","20bb2f6a":"### **Conclusion**\n\nWe found some important things, Let's list the findlings now\n* **Annual seasonality** is found through data\n* The highest total sale is on **29th december 2018**\n* The sales are reaching **high on start of the year**\n* **Finland** is the **lowest** in sales\n* **Norway** is the **best seller**\n\nThese findlings could help you more on understanding how to well use the perfect model for the data. I hope the visualizations did it's job on make you feel good. Help me with how I can change these visualizations more informatical and what topics(question) I can include."}}