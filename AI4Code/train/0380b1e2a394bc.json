{"cell_type":{"b5c8030b":"code","d4eb6444":"code","0b2206dd":"code","37f6385e":"code","2eee6b65":"code","02d4455d":"code","225bb44c":"code","a3347339":"code","e4705490":"code","4839c7f7":"code","17f499d4":"code","abc83b4f":"code","ddbbc2fd":"code","db8826e4":"code","5cd0ad3a":"code","abd09fa8":"code","cee4607c":"code","66d1ac91":"code","a83ee49b":"code","be471fc5":"code","3db4bb55":"code","740a301b":"code","7bdc6f52":"code","f9250ec9":"code","cd6cee16":"code","8a2178f6":"code","ba786c02":"code","8cff3f46":"code","3b0586c5":"code","bd4557a5":"code","abd5b675":"code","a0ab0825":"code","9d520017":"code","e57b1a58":"code","7e7d943a":"code","6ce50e03":"code","537554e4":"code","3dee375c":"code","50a41745":"code","2de9b121":"code","4d89dc7e":"code","f6c7568b":"code","980efc65":"code","b53b769d":"code","c32c9bfa":"code","58073be8":"code","1f7016da":"code","4b51a18a":"code","85be6345":"code","cc86638f":"code","f35c51dd":"code","0fdbcd8e":"code","e379188c":"code","0d9a9193":"code","8318e7d7":"code","1407aa81":"code","01f79479":"code","844ccf53":"code","dc1a1cd7":"code","e05659fc":"code","430dc1c6":"code","22c31051":"code","41a07d6a":"code","5bf8a871":"code","ff6f5b44":"code","8e2ae3fd":"code","bdee3468":"code","1023471c":"markdown","b01713bc":"markdown","397eaf68":"markdown","bd926061":"markdown","cf7c541b":"markdown","05fc464b":"markdown","db7db6b2":"markdown","304780c0":"markdown","9bff20d6":"markdown","f894c963":"markdown","06a48617":"markdown","a1ecaf50":"markdown","42236b27":"markdown","a15b8126":"markdown","c833bf7f":"markdown","810c0578":"markdown","e62eb86f":"markdown","0e8fec67":"markdown","1ae9c3e3":"markdown","cc44181d":"markdown","6a0a6794":"markdown","d27a1268":"markdown","f37746d4":"markdown","47c00d9e":"markdown","f417c8b0":"markdown","98548658":"markdown","0ba7ebb1":"markdown","c0b984dd":"markdown"},"source":{"b5c8030b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4eb6444":"!pip install rich","0b2206dd":"import re\nimport warnings\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nfrom scipy import stats\nimport missingno as msno\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom rich.console import Console\nfrom rich.theme import Theme\nfrom rich import pretty\n\nfrom PIL import Image\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.metrics import (\n    mean_squared_error as mse, \n    make_scorer, \n    accuracy_score, \n    confusion_matrix\n)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\n\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer","37f6385e":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 2021\nseed_everything(seed)","2eee6b65":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 150)","02d4455d":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplotly_discrete_sequence = px.colors.qualitative.G10","225bb44c":"colors = [primary_blue, primary_blue2, primary_blue3, primary_grey, primary_black, primary_bgcolor, primary_green]\nsns.palplot(sns.color_palette(colors))","a3347339":"sns.palplot(sns.color_palette(plotly_discrete_sequence))","e4705490":"plt.rcParams['figure.dpi'] = 120\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['axes.facecolor'] = primary_bgcolor","4839c7f7":"custom_theme = Theme({\n    \"info\" : \"italic bold blue\",\n    \"succeed\": \"italic bold green\",\n    \"danger\": \"bold red\"\n})\n\nconsole = Console(theme=custom_theme)\n\npretty.install()","17f499d4":"train_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\ntrain_df.shape","abc83b4f":"train_df.head()","ddbbc2fd":"target_column = 'target'\n\ntrain_df.head()","db8826e4":"msno.bar(train_df, color=primary_blue, sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","5cd0ad3a":"fig2 = ff.create_distplot([train_df[target_column]], [target_column], colors=[primary_blue],\n                             bin_size=.05, show_rug=False)\nplot_title = f\"<span style='font-size:30px; font-family:Serif'><b>{target_column.capitalize()}<\/b> resume<\/span>\"\n\nfig = go.Figure()\n\nmean_value = train_df[target_column].mean()\n\nfig.add_vrect(\n    x0=train_df[target_column].quantile(0.25), \n    x1=train_df[target_column].quantile(0.75), \n    annotation_text=\"IQR\", \n    annotation_position=\"top left\",\n    fillcolor=primary_grey, \n    opacity=0.25, \n    line_width=2,\n)\n\nfig.add_trace(go.Scatter(\n    fig2['data'][1],\n    line=dict(\n        color=primary_blue, \n        width=1.5,\n    ),\n    fill='tozeroy'\n))\n\nfig.add_vline(\n    x=mean_value, \n    line_width=2, \n    line_dash=\"dash\", \n    line_color=primary_black\n)\nfig.add_annotation(\n    yref=\"y domain\",\n    x=mean_value,\n    # The arrow head will be 40% along the y axis, starting from the bottom\n    y=0.5,\n    axref=\"x\",\n    ayref=\"y domain\",\n    ax=mean_value + 0.2*mean_value,\n    ay=0.6,\n    text=f\"<span>{target_column.capitalize()} mean<\/span>\",\n    arrowhead=2,\n)\nfig.add_annotation(\n    xref=\"x domain\", yref=\"y domain\",\n    x=0.98, y=0.98,\n    text=f\"<span><b>Skew: %.2f<\/b><\/span>\"%(train_df[target_column].skew()),\n    bordercolor=primary_black,\n    borderwidth=1.5, borderpad=2.5,\n    showarrow=False,\n)\n\nfig.update_layout(\n    showlegend=True,\n    title_text=plot_title\n)\n\nfig.show()","abd09fa8":"###### Helpers not used:\n\n#fig.add_annotation(\n#    yref=\"y3 domain\",\n#    xref=\"x3\",\n#    x=q1_value,\n#    # The arrow head will be 40% along the y axis, starting from the bottom\n#    y=0.95,\n#    axref=\"x3\",\n#    ayref=\"y3 domain\",\n#    ay=0.95,\n#    ax=q1_value + abs(0.2*q1_value),\n#    text=\"Interquartile range (IQR)\",\n#    arrowhead=3,\n#)\n\n\n#fig.add_annotation(\n#    yref=\"y3 domain\",\n#    xref=\"x3\",\n#    x=mean_value,\n#    y=0.5,\n#    axref=\"x3\",\n#    ayref=\"y3 domain\",\n#    ax=mean_value + 0.2*mean_value,\n#    ay=0.6,\n#    text=f\"<span>{feature.capitalize()} mean<\/span>\",\n#    arrowhead=3,\n#)\n\n\n#fig.add_shape(go.layout.Shape(\n#    type=\"line\",\n#    yref=\"y3 domain\",\n#    xref=\"x\",\n#    x0=mean_value,\n#    y0=0,\n#    x1=mean_value,\n#    y1=1,\n#    line=dict(\n#        color=primary_black, \n#        width=2, \n#        dash=\"dash\"\n#    ),\n#), row=3, col=1)","cee4607c":"def generate_feature_resume(df, feature):\n    \n    plot_title = f\"<span style='font-size:30px; font-family:Serif'><b>{feature.capitalize()}<\/b> resume<\/span>\"\n    (osm, osr), (slope, intercept, r) = stats.probplot(df[feature], plot=None)\n    \n    q1_value = train_df[feature].quantile(0.25)\n    mean_value = train_df[feature].mean()\n    fig2 = ff.create_distplot([df[feature]], [feature], colors=[primary_blue],\n                             bin_size=.05, show_rug=False)\n\n    fig = make_subplots(\n        rows=3, cols=2,\n        specs=[\n            [{\"rowspan\": 2}, {\"rowspan\": 2}],\n            [None, None],\n            [{\"colspan\": 2}, None]\n        ],\n        subplot_titles=(\n            \"Quantile-Quantile Plot\",\n            \"Box Plot\",\n            \"Distribution Plot\"\n        )\n    )\n\n    fig.add_trace(go.Scatter(\n        x=osm,\n        y=slope*osm + intercept,\n        mode='lines',\n        line={\n            'color': '#c81515',\n            'width': 2.5\n        }\n\n    ), row=1, col=1)\n    \n    ## QQ-Plot\n    fig.add_trace(go.Scatter(\n        x=osm,\n        y=osr,\n        mode='markers',\n        marker={\n            'color': primary_blue\n        }\n    ), row=1, col=1)\n\n    ## Box Plot\n    fig.add_trace(go.Box(\n        y=df[feature], \n        name='',\n        marker_color = primary_blue\n    ), row=1, col=2)\n\n    ## Distribution plot\n    fig.add_trace(go.Scatter(\n        fig2['data'][1],\n        line=dict(\n            color=primary_blue, \n            width=1.5,\n        ),\n        fill='tozeroy'\n    ), row=3, col=1)\n    \n    ## InterQuartile Range (IQR)\n    fig.add_vrect(\n        x0=df[feature].quantile(0.25), \n        x1=df[feature].quantile(0.75), \n        annotation_text=\"IQR\", \n        annotation_position=\"top left\",\n        fillcolor=primary_grey, \n        opacity=0.25, \n        line_width=2,\n        row=3, col=1,\n    )\n    \n    ## Mean line\n    fig.add_vline(\n        x=mean_value,\n        line_width=2, \n        line_dash=\"dash\", \n        line_color=primary_black,\n        annotation_text=\"Mean\", \n        annotation_position=\"bottom right\",\n        row=3, col=1,\n    )\n    \n    fig.add_annotation(\n        xref=\"x domain\", yref=\"y domain\",\n        x=0.98, y=0.98,\n        text=f\"<span style='font-family:Serif>Skew: %.2f<\/span>\"%(df[feature].skew()),\n        showarrow=False,\n        bordercolor=primary_black,\n        borderwidth=1, borderpad=2,\n        row=3, col=1,\n    )\n    \n    fig.update_layout(\n        showlegend=False, \n        title_text=plot_title,\n        height=720,\n    )\n\n    fig.show()","66d1ac91":"generate_feature_resume(train_df, target_column)","a83ee49b":"# As there is a f*** 0 in the standard_error feature, we are going to change it with the value of the next lowest element\ntrain_df.loc[train_df['standard_error'] == 0, 'standard_error'] = train_df['standard_error'].sort_values(ascending=True).iloc[1]","be471fc5":"generate_feature_resume(train_df, \"standard_error\")","3db4bb55":"sns.jointplot(\n    x=train_df['target'], \n    y=train_df['standard_error'], \n    kind='hex',\n    height=8,\n    edgecolor=primary_grey,\n    color=primary_blue\n)\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\", size=20)\nplt.subplots_adjust(top=0.95)\nplt.show()","740a301b":"train_df['excerpt_len'] = train_df['excerpt'].apply(\n    lambda x : len(x)\n)\ntrain_df['excerpt_word_count'] = train_df['excerpt'].apply(\n    lambda x : len(x.split(' '))\n)","7bdc6f52":"fig = ff.create_distplot(\n    [train_df['excerpt_len']], \n    ['excerpt_len'], \n    bin_size=12, \n    show_rug=False,\n    colors=[primary_blue],\n)\nfig.update_layout(\n    showlegend=False, \n    title_text=f\"<span style='font-size:30px; font-family:Serif'><b>Excerpt<\/b> length<\/span>\",\n)\nfig.show()","f9250ec9":"fig = ff.create_distplot(\n    [train_df['excerpt_word_count']], \n    ['excerpt_word_count'], \n    bin_size=2, \n    show_rug=False,\n    colors=[primary_blue],\n)\nfig.update_layout(\n    showlegend=False, \n    title_text=f\"<span style='font-size:30px; font-family:Serif'><b>Excerpt<\/b> word count<\/span>\",\n)\nfig.show()","cd6cee16":"# Special thanks to https:\/\/www.kaggle.com\/tanulsingh077 for this function\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","8a2178f6":"train_df['excerpt_clean'] = train_df['excerpt'].apply(clean_text)\ntrain_df.head()","ba786c02":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n    \ntrain_df['excerpt_clean'] = train_df['excerpt_clean'].apply(remove_stopwords)\ntrain_df.head()","8cff3f46":"stemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_text(text):\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text","3b0586c5":"train_df['excerpt_clean'] = train_df['excerpt_clean'].apply(stemm_text)\ntrain_df.head()","bd4557a5":"def preprocess_data(text, strip=False):\n    # Clean puntuation, urls, and so on\n    text = clean_text(text)\n    # Remove stopwords\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    # Stemm all the words in the sentence\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    \n    if strip:\n        text = text.strip()\n    \n    return text","abd5b675":"train_df['excerpt_clean'] = train_df['excerpt_clean'].apply(preprocess_data)\ntrain_df.head()","a0ab0825":"console.print('First, lets see the original text:')\nconsole.print(train_df['excerpt'][0], style='info')\n\nconsole.print('And now, lets see the clean text:')\nconsole.print(train_df['excerpt_clean'][0], style='succeed')","9d520017":"def get_top_n_grams(vect, corpus, n):\n    # Use the CountVectorizer to create a document-term matrix\n    dtm = vect.fit_transform(corpus)\n\n    dtm_sum = dtm.sum(axis=0) \n    dtm_freq = [(word, dtm_sum[0, idx]) for word, idx in vect.vocabulary_.items()]\n    dtm_freq =sorted(dtm_freq, key = lambda w: w[1], reverse=True)\n    return dict(sorted(dtm_freq[:n], key = lambda w: w[1], reverse=False))\n\ndef plot_top_grams(grams, groups, title):\n    fig = go.Figure(go.Bar(\n        x=list(grams.values()), y=list(grams.keys()),\n        orientation='h',\n    ))\n    # Customize aspect\n    fig.update_traces(\n        marker_color=groups[2]*[primary_grey] + groups[1]*[primary_blue2] + groups[0]*[primary_blue], \n        marker_line_color=primary_blue3,\n        marker_line_width=1, \n        opacity=0.6\n    )\n    fig.update_layout(\n        title_text=f\"<span style='font-size:30px; font-family:Serif'>{title}<\/span>\"\n    )\n    fig.show()","e57b1a58":"vect = CountVectorizer()\ntop_unigrams = get_top_n_grams(vect, train_df['excerpt_clean'], 15)\n\nplot_top_grams(top_unigrams, [1,5,9], \"Top 15 <b>Unigrams<\/b>\")","7e7d943a":"vect = CountVectorizer(ngram_range=(2, 2))\ntop_bigrams = get_top_n_grams(vect, train_df['excerpt_clean'], 15)\n\nplot_top_grams(top_bigrams, [3,7,5], \"Top 15 <b>Bigrams<\/b>\")","6ce50e03":"vect = CountVectorizer(ngram_range=(3, 3))\ntop_trigrams = get_top_n_grams(vect, train_df['excerpt_clean'], 15)\n\nplot_top_grams(top_trigrams, [3,4,8], \"Top 15 <b>Trigrams<\/b>\")","537554e4":"book_mask = np.array(Image.open('\/kaggle\/input\/masksforwordclouds\/book-logo-1.jpg'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=book_mask,\n)\nwc.generate(' '.join(text for text in train_df.loc[:, 'excerpt_clean']))\nplt.figure(figsize=(18,10))\nplt.title('Top words', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","3dee375c":"rmse = lambda y_true, y_pred: np.sqrt(mse(y_true, y_pred))\nrmse_loss = lambda Estimator, X, y: rmse(y, Estimator.predict(X))","50a41745":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nx = train_df['excerpt_clean']\ny = train_df['target']\n\nprint(len(x), len(y))","2de9b121":"# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","4d89dc7e":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    train_df['excerpt_clean'], \n    train_df['target'], \n    scoring=rmse_loss\n).mean()\n\nconsole.print(f'Train Score for CountVectorizer(1,1): {val_score}')","f6c7568b":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    train_df['excerpt_clean'], \n    train_df['target'], \n    scoring=rmse_loss\n).mean()\n\nconsole.print(f'Train Score for CountVectorizer(1,1): {val_score}')","980efc65":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    train_df['excerpt_clean'], \n    train_df['target'], \n    scoring=rmse_loss\n).mean()\n\nconsole.print(f'Train Score for CountVectorizer(1,1): {val_score}')","b53b769d":"# Now create the train and test dtm\nvect = CountVectorizer()\nvect.fit(x_train)\n\n# Use the trained to create a document-term matrix from train and test sets\nx_train_dtm = vect.transform(x_train)\nx_test_dtm = vect.transform(x_test)","c32c9bfa":"vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)","58073be8":"model = make_pipeline(\n    TfidfVectorizer(ngram_range=(1,1)),\n    LinearRegression()\n)\n\nval_score = cross_val_score(\n    model, \n    train_df['excerpt_clean'], \n    train_df['target'], \n    scoring=rmse_loss\n).mean()\n\nconsole.print(f'Train Score for TfidfVectorizer(1,1): {val_score}')","1f7016da":"model = make_pipeline(\n    TfidfVectorizer(ngram_range=(1,2)),\n    LinearRegression()\n)\n\nval_score = cross_val_score(\n    model, \n    train_df['excerpt_clean'], \n    train_df['target'], \n    scoring=rmse_loss\n).mean()\n\nconsole.print(f'Train Score for TfidfVectorizer(1,1): {val_score}')","4b51a18a":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\n\ntfidf_transformer.fit(x_train_dtm)\nx_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n\nx_train_tfidf","85be6345":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam","cc86638f":"texts = train_df['excerpt_clean']\ntarget = train_df['target']","f35c51dd":"# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(texts)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","0fdbcd8e":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(texts), \n    length_long_sentence, \n    padding='post'\n)\n\ntrain_padded_sentences","e379188c":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions","0d9a9193":"# Now we will load embedding vectors of those words that appear in the\n# Glove dictionary. Others will be initialized to 0.\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","8318e7d7":"from sklearn.pipeline import Pipeline\nimport xgboost as xgb\n\npipe = Pipeline([\n    ('tfid', TfidfVectorizer(ngram_range=(1,2))),  \n    ('model', xgb.XGBRegressor(\n        learning_rate=0.1,\n        max_depth=7,\n        n_estimators=80,\n        use_label_encoder=False,\n        eval_metric='rmse',\n    ))\n])\n\n# Fit the pipeline with the data\npipe.fit(x_train, y_train)","1407aa81":"y_pred = pipe.predict(x_test)\n\nconsole.print(f'Score for XGBoost with TfidfVectorizer(1,2): {rmse(y_test, y_pred)}')","01f79479":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    train_padded_sentences, \n    target, \n    test_size=0.25\n)","844ccf53":"# Model from https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm\/data\n\ndef glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'linear'))\n    \n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \n    return model\n\nmodel = glove_lstm()\nmodel.summary()","dc1a1cd7":"# Load the model and train!!\n\nmodel = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 10,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","e05659fc":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","430dc1c6":"plot_learning_curves(history, [['loss', 'val_loss'],['root_mean_squared_error', 'val_root_mean_squared_error']])","22c31051":"# Sampling Function\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n\n# Convert to tf.data.Dataset\ndef get_dataset(df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=256):\n    \n    texts = [preprocess_data(text, True) for text in df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(texts, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (df[target_column], df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset","41a07d6a":"# TPU or GPU detection\n# Detect hardware and create the ad-hoc strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    print('Using GPU strategy...')\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","5bf8a871":"BATCH_SIZE = 8\nLEARNING_RATE = 1e-3\nEPOCHS = 5\nSTEPS=50\nN_FOLDS = 5\nES_PATIENCE = 7\nSEQ_LEN = 256\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/'\nproper_names = ['fayre', 'roger', 'blaney']","ff6f5b44":"\ndef model_fn(encoder, seq_len=256):\n    input_ids = layers.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = layers.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(\n        inputs=[input_ids, input_attention_mask], \n        outputs=outputs\n    )\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer, \n        loss=losses.MeanSquaredError(), \n        metrics=[metrics.RootMeanSquaredError()]\n    ) \n    return model\n\n\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","8e2ae3fd":"tf.keras.utils.plot_model(\n    model,\n    show_shapes=True, show_dtype=False,\n    show_layer_names=False, rankdir='TB', \n    expand_nested=False\n)","bdee3468":"%%script false --no-raise-error\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n\noof_pred = []; oof_labels = []\nhistory_list = []; test_pred = []\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train_df)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    # Model\n    backend.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(\n        x=get_dataset(\n            train_df.loc[idxT], \n            tokenizer, \n            repeated=True, \n            is_sampled=True, \n            batch_size=BATCH_SIZE, \n            seq_len=SEQ_LEN\n        ), \n        validation_data=get_dataset(\n            train_df.loc[idxV], \n            tokenizer, \n            ordered=True, \n            batch_size=BATCH_SIZE, seq_len=SEQ_LEN\n        ), \n        steps_per_epoch=STEPS, \n        callbacks=[es, checkpoint], \n        epochs=EPOCHS,  \n        verbose=2\n    ).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(\n        train_df.loc[idxV], \n        tokenizer, \n        ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN\n    )\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    oof_pred.append(model.predict(x_oof)['logits'])\n\n    # Test predictions\n    test_ds = get_dataset(\n        test_df, \n        tokenizer, \n        labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN\n    )\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","1023471c":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Baseline Model and Comparison<\/p>\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n\nWe'll do that in three steps using the bag-of-words model:\n\n1. Count how many times does a word occur in each message (Known as term frequency)\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\nEach vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n\n![vectorizer.png](attachment:fe0ac5b9-f924-4a7c-b8e3-306485322784.png)","b01713bc":"Again, the best selection for `ngram` parameter is the tuple `(1,2)`.","397eaf68":"### Pad_sequences\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences\n\n```python\ntf.keras.preprocessing.sequence.pad_sequences(\n    sequences, maxlen=None, dtype='int32', padding='pre',\n    truncating='pre', value=0.0\n)\n```\n\nThis function transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list.\n\n```python\n>>> sequence = [[1], [2, 3], [4, 5, 6]]\n>>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\narray([[1, 0, 0],\n       [2, 3, 0],\n       [4, 5, 6]], dtype=int32)\n```","bd926061":"<a id='3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.2 Stemming \ud83d\udee0<\/p>\n\n### Stemming\/ Lematization\nFor grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes*. Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\n**Stemming** usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.\n\n**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n\n![stemming-lematization.png](attachment:e2f093fa-6bec-42b3-963b-a9fbc60761f0.png)\n\nAs far as the meaning of the words is not important for this study, we will focus on stemming rather than lemmatization.\n\n### Stemming algorithms\n\nThere are several stemming algorithms implemented in NLTK Python library:\n1. **PorterStemmer** uses *Suffix Stripping* to produce stems. **PorterStemmer is known for its simplicity and speed**. Notice how the PorterStemmer is giving the root (stem) of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. But if you look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because *PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems*. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.\n2. One can generate its own set of rules for any language that is why Python nltk introduced **SnowballStemmers** that are used to create non-English Stemmers!\n3. **LancasterStemmer** (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.","cf7c541b":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">CommonLit Readability \ud83d\udcdd A complete Analysis<\/p>\n\n![nlp-header.png](attachment:4a916c95-e05d-4636-98fa-3e82cb1066ce.png)\n\n**Natural Language Processing or NLP** is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.","05fc464b":"As we can see, it seems that the best result is achived using `ngram_range=(1,2)` which has much sense as we are going to see in the following section.>","db7db6b2":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='6'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">6. Modeling<\/p>","304780c0":"<a id='5.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">5.1 Tunning CountVectorizer<\/p>\n\nCountVectorizer has a few parameters you should know.\n\n1. **stop_words**: Since CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words like \u2018the\u2019, \u2018and\u2019, etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don\u2019t take those words into account. Stop words are just a list of words you don\u2019t want to use as features. You can set the parameter stop_words=\u2019english\u2019 to use a built-in list. Alternatively you can set stop_words equal to some custom list. This parameter defaults to None.\n\n2. **ngram_range**: An n-gram is just a string of n words in a row. E.g. the sentence \u2018I am Groot\u2019 contains the 2-grams \u2018I am\u2019 and \u2018am Groot\u2019. The sentence is itself a 3-gram. Set the parameter ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram_range is (1,1). In a recent project where I modeled job postings online, I found that including 2-grams as features boosted my model\u2019s predictive power significantly. This makes intuitive sense; many job titles such as \u2018data scientist\u2019, \u2018data engineer\u2019, and \u2018data analyst\u2019 are 2 words long.\n\n3. **min_df, max_df**: These are the minimum and maximum document frequencies words\/n-grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit. min_df defaults to 1 (int) and max_df defaults to 1.0 (float).\n\n4. **max_features**: This parameter is pretty self-explanatory. The CountVectorizer will choose the words\/features that occur most frequently to be in its\u2019 vocabulary and drop everything else. \n\nYou would set these parameters when initializing your CountVectorizer object as shown below.","9bff20d6":"<a id='2.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.2 Target and Std_err Distributions \ud83d\udcf8<\/p>\n","f894c963":"<a id='4.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">4.1 Top Words \ud83d\udcdd<\/p>","06a48617":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Preprocessing \u2699\ufe0f<\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.","a1ecaf50":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Tokens visualization \ud83d\udcca<\/p>\n\nLet's see which are the top tockens using count vectorizer and ranking based on the appearence. They idea is to have a first overview of the relevance of each word or tuple of words.","42236b27":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\nNow we are going to take a look about the target distribution, missings, messages length and so on.","a15b8126":"<a id='5.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">5.3 Word Embeddings: GloVe<\/p>","c833bf7f":"### GloVe\n\nGloVe method is built on an important idea,\n\n> You can derive semantic relationships between words from the co-occurrence matrix.\n\nTo obtain a vector representation for words we can use an unsupervised learning algorithm called **GloVe (Global Vectors for Word Representation)**, which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.\n\nWord embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space.\n\nThus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.","810c0578":"<a id='4.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">4.2 WordCloud \ud83c\udf1f<\/p>","e62eb86f":"We need to perform **tokenization** - the processing of segmenting text into sentences of words. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing.\n\n![tokenization.jpeg](attachment:156686a1-ffd0-4f85-be0a-d1fd31ea8a64.jpeg)","0e8fec67":"<a id='5.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">5.2 TF-IDF<\/p>\n\nIn information retrieval, tf\u2013idf, **TF-IDF**, or TFIDF, **short for term frequency\u2013inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n\n**tf\u2013idf** is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf\u2013idf.\n\n![tf-idf.png](attachment:ed3d959e-bd1b-446f-af2f-775e4364b2b7.png)","1ae9c3e3":"<a id='2.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.3 Excert overview \ud83d\udd0e<\/p>","cc44181d":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.1 Cleaning the corpus \ud83d\udee0<\/p>","6a0a6794":"<a id='6.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">6.1 TF-IDF with XGBoost<\/p>","d27a1268":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='8'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">8. ToBERTa TF \ud83d\udee0<\/p>\n\nSpecial thanks to @sauravmaheshkar and @dimitreoliveira for such amazing work and thanks for sharing your code. It was ery helpful.","f37746d4":"<a href=\"#table-of-content\">back to table of content<\/a>\n<a id='7'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">7. LSTM<\/p>","47c00d9e":"<a id=\"table-of-content\"><\/a>\n\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n    * [2.1 Missing values](#2.1)\n    * [2.2 Target and Std_err Distributions \ud83d\udcf8](#2.2)\n    * [2.3 Excert overview \ud83d\udd0e](#2.3)\n* [3. Data Preprocessing \u2699\ufe0f](#3)\n    * [3.1 Cleaning the corpus \ud83d\udee0](#3.1)\n    * [3.2 Stemming \ud83d\udee0](#3.2)\n    * [3.3 All together \ud83d\udee0](#3.3)\n* [4. Tokens visualization \ud83d\udcca](#4)\n    * [4.1 Top Words \ud83d\udcdd](#4.1)\n    * [4.2 WordCloud \ud83c\udf1f](#4.2)\n* [5. Vectorization](#5)\n    * [5.1 Tunning CountVectorizer](#5.1)\n    * [5.2 TF-IDF](#5.2)\n    * [5.3 Word Embeddings: GloVe](#5.3)\n* [6. Modeling](#6)\n    * [6.1 XGBoost](#6.1)\n* [7. LSTM with Glove](#7)\n* [8. RoBERTa](#8)\n\nTo be continued..","f417c8b0":"<a id='2.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.1 Missing values<\/p>\n\nAs we can see, the only missing values are in: `url_legal` and `license`. For now, we are going to do an analysis based on the `excerpt` text so we can go ahead.","98548658":"### Stopwords\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some examples removing stopwords are:\n\n![stopwords.png](attachment:a023a8e1-af19-4555-875a-8fc533b0c580.png)","0ba7ebb1":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.","c0b984dd":"<a id='3.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3.3 All together \ud83d\udee0<\/p>"}}