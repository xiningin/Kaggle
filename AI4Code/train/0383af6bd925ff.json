{"cell_type":{"bed7428b":"code","5734cf92":"code","77d5be9d":"code","862ca97b":"code","2dae137f":"code","19229692":"code","62b0b08e":"code","38deb6f0":"code","c971ae96":"code","53114ded":"code","c01e2816":"code","7a84f589":"code","28ce043a":"code","a1407fe5":"code","0be29337":"code","2da84fce":"code","f33a7e02":"code","3313d488":"code","df20e0e6":"code","efa07eb8":"code","a45a2b82":"code","6ad70902":"markdown","59b88ea5":"markdown","40b73a1a":"markdown","566c493c":"markdown","d8d91f57":"markdown","e6bec9ee":"markdown","d4e0b929":"markdown","386ca86b":"markdown","682f219e":"markdown","fe0efde5":"markdown","228e8641":"markdown","06c56428":"markdown","ff7585a5":"markdown","a9487e09":"markdown","ee4198bd":"markdown","8a064bd9":"markdown","46a6aa99":"markdown","9838a2bd":"markdown","929d8b16":"markdown","1f054543":"markdown","53f31ed3":"markdown","745b71f9":"markdown"},"source":{"bed7428b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5734cf92":"# Load d\u0169 li\u1ec7u v\u00e0o dataframe\ntrain_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","77d5be9d":"train_df.info()","862ca97b":"print('5 h\u00e0ng d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean c\u1ee7a t\u1eadp train')\ntrain_df.head()","2dae137f":"print('5 h\u00e0ng d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean c\u1ee7a t\u1eadp test')\ntrain_df.head()","19229692":"plt.figure(figsize=(10,9))\nsns.countplot(x='target', data=train_df)\nprint('Sincere questions: ', train_df[train_df['target'] == 0].shape[0])\nprint('Insincere questions: ', train_df[train_df['target'] == 1].shape[0])\nprint('Total questions: ', train_df.shape[0])","62b0b08e":"print('Ph\u1ea7n tr\u0103m s\u1ed1 c\u00e2u h\u1ecfi h\u1eefu \u00edch: {}%'.format(100 - round(train_df['target'].mean()*100, 2)))\nprint('\\nPh\u1ea7n tr\u0103m s\u1ed1 c\u00e2u h\u1ecfi \u0111\u1ed9c h\u1ea1i: {}%'.format(round(train_df['target'].mean()*100, 2)))","38deb6f0":"from wordcloud import WordCloud\nprint(\"Word cloud th\u1ec3  hi\u1ec7n c\u00e1c c\u00e2u h\u1ecfi sincere: \")\nsincere_wordcloud = WordCloud(width=700, height=500, background_color='black', min_font_size=10).generate(str(train_df[train_df[\"target\"] == 0][\"question_text\"]))\nplt.figure(figsize=(10,9), facecolor=None)\nplt.imshow(sincere_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","c971ae96":"print(\"Word cloud th\u1ec3  hi\u1ec7n c\u00e1c c\u00e2u h\u1ecfi insincere: \")\ninsincere_wordcloud = WordCloud(width=700, height=500, background_color='black', min_font_size=10).generate(str(train_df[train_df[\"target\"] == 1][\"question_text\"]))\nplt.figure(figsize=(10,9), facecolor=None)\nplt.imshow(insincere_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","53114ded":"from sklearn.utils import resample\n# Chia l\u1ea1i t\u1eadp d\u1eef li\u1ec7u\nsincere = train_df[train_df.target == 0]\ninsincere = train_df[train_df.target == 1]\ndf_train_sampled = pd.concat([resample(sincere,replace = True,n_samples = len(insincere)*4), insincere])\n","c01e2816":"import re\nimport nltk\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nwordnet_lemmatizer = WordNetLemmatizer()\nimport re, string\ntokenizer = ToktokTokenizer()\nstemmer = SnowballStemmer('english')\nstopword_list = nltk.corpus.stopwords.words('english')\n","7a84f589":"def clean_text(text):\n\n    # Xo\u00e1 HTML Tags\n    text = re.sub(re.compile('<.*?>'), '', text)\n\n    # Xo\u00e1  [\\], ['], [\"]\n    text = re.sub(r'\\\\', '', text)\n    text = re.sub(r'\\\"', '', text)\n    text = re.sub(r'\\'', '', text)\n\n    # Xo\u00e1 ch\u1eef s\u1ed1\n    text = re.sub('[0-9]{5,}','#####', text);\n    text = re.sub('[0-9]{4,}','####', text);\n    text = re.sub('[0-9]{3,}','###', text);\n    text = re.sub('[0-9]{2,}','##', text);\n\n    # Xo\u00e1 k\u00fd t\u1ef1 la m\u00e3\n    roman = re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$');\n    text = roman.sub(r'', text);\n\n    # Chuy\u1ec3n c\u00e1c k\u00fd t\u1ef1 v\u1ec1 d\u1ea1ng in th\u01b0\u1eddng\n    text = text.strip().lower()\n\n    # Xo\u00e1 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n    filters = '!\"\\'#$%@&*()+_-;:<=>.?{}|`\\\\^\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n    \n    #token \n    #tokens = tokenizer.tokenize(text)\n    #tokens = [token.strip() for token in tokens]\n   # tokens = [token for token in tokens if token not in stop_words]\n   # tokens = [stemmer.stem(token) for token in tokens]\n    #tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n    \n    return text","28ce043a":"df_train_sampled['preprocessed_questions'] = df_train_sampled['question_text'].apply(clean_text)","a1407fe5":"print('C\u1ed9t preprocessed_ question ch\u1ee9a c\u00e1c c\u00e2u h\u1ecfi \u0111\u00e3 \u0111\u01b0\u1ee3c l\u00e0m s')\ndf_train_sampled.head()","0be29337":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nX = df_train_sampled['preprocessed_questions']\ny = df_train_sampled.target\ntest = test_df['question_text']\nX_train, X_test, y_train, y_test = train_test_split(df_train_sampled['preprocessed_questions'], df_train_sampled['target'], test_size=0.3)","2da84fce":"cnt_vectorizer = CountVectorizer()\n#Fitting count vectorizer to both training and test sets (semi-supervised learning)\ncnt_vectorizer.fit(list(X_train) + list(X_test))\nX_train = cnt_vectorizer.transform(X_train) \nX_test = cnt_vectorizer.transform(X_test)","f33a7e02":"df_train_sampled[\"preprocessed_questions\"].head()\n","3313d488":"import sklearn.metrics as metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score","df20e0e6":"from sklearn.naive_bayes import MultinomialNB\nMulNB = MultinomialNB()\nMulNB.fit(X_train,y_train)\ntest_predictions = MulNB.predict(X_test)\ntest_acc = accuracy_score(y_test, test_predictions) \ntest_f1 = f1_score(y_test, test_predictions)\nprint('Precision: %.3f' % precision_score(y_test, test_predictions))\nprint('Recall: %.3f' % recall_score(y_test,test_predictions))\nprint(f\"Testing accuracy:  {test_acc:.2%}, F1: {test_f1:.4f}\")\ny_pred_proba = MulNB.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","efa07eb8":"from sklearn.linear_model import LogisticRegression\n# Fitting a simple Logistic Regression\nlogisticRegression = LogisticRegression(max_iter = 5000)\nlogisticRegression.fit(X_train,y_train)\ntest_predictions = logisticRegression.predict(X_test)\ntest_acc = accuracy_score(y_test, test_predictions) \ntest_f1 = f1_score(y_test, test_predictions) \nprint(f\"Testing accuracy:  {test_acc:.2%}, F1: {test_f1:.4f}\")\nprint('Precision: %.3f' % precision_score(y_test, test_predictions))\nprint('Recall: %.3f' % recall_score(y_test,test_predictions))\ny_pred_proba = logisticRegression.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n","a45a2b82":"x_val = cnt_vectorizer.transform(test_df['question_text'])\nvalidation_predictions = logisticRegression.predict(x_val)\nsubmission = pd.DataFrame({'qid':test_df['qid'], 'prediction':validation_predictions })\nsubmission.to_csv('submission.csv', index=False)\nsubmission","6ad70902":"# 1. Problem Description\n  M\u1ed9t v\u1ea5n \u0111\u1ec1 t\u1ed3n t\u1ea1i \u0111\u1ed1i v\u1edbi b\u1ea5t k\u1ef3 trang web l\u1edbn n\u00e0o hi\u1ec7n nay l\u00e0 l\u00e0m th\u1ebf n\u00e0o \u0111\u1ec3 x\u1eed l\u00fd n\u1ed9i dung \u0111\u1ed9c h\u1ea1i v\u00e0 g\u00e2y chia r\u1ebd. Quora mu\u1ed1n gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y tr\u1ef1c ti\u1ebfp \u0111\u1ec3 gi\u1eef cho n\u1ec1n t\u1ea3ng c\u1ee7a h\u1ecd tr\u1edf th\u00e0nh m\u1ed9t n\u01a1i m\u00e0 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 c\u1ea3m th\u1ea5y an to\u00e0n khi chia s\u1ebb ki\u1ebfn th\u1ee9c c\u1ee7a h\u1ecd v\u1edbi th\u1ebf gi\u1edbi.$$$$\n  Y\u00eau c\u1ea7u c\u1ee7a b\u00e0i to\u00e1n l\u00e0 ph\u00e2n lo\u1ea1i c\u00e2u h\u1ecfi c\u00f3 ph\u1ea3i l\u00e0 c\u00e2u h\u1ecfi toxic hay kh\u00f4ng\n       => **\u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n classifier**\n* D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o s\u1ebd l\u00e0 c\u00e1c c\u00e2u h\u1ecfi (d\u1eef li\u1ec7u d\u1ea1ng text)\n* \u0110\u1ea7u ra l\u00e0 0:1\n\n>**\u0110\u1ec3 \u0111\u00e1nh gi\u00e1 b\u00e0i to\u00e1n, \u1edf \u0111\u00e2y ta d\u00f9ng f1_score**\n![](https:\/\/www.digital-mr.com\/media\/cache\/5e\/b4\/5eb4dbc50024c306e5f707736fd79c1e.png) \n$$$$**Precision** \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m Positive m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n l\u00e0 Positive\n$$precision=\\frac{TP}{TP + FP}$$\n**Recall** \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 t\u1ec9 l\u1ec7 s\u1ed1 \u0111i\u1ec3m Positive m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n \u0111\u00fang tr\u00ean t\u1ed5ng s\u1ed1 \u0111i\u1ec3m th\u1eadt s\u1ef1 l\u00e0 Positive (hay t\u1ed5ng s\u1ed1 \u0111i\u1ec3m \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n l\u00e0 Positive ban \u0111\u1ea7u)\n$$recall=\\frac{TP}{TP + FN}$$\n**F1-score**<br>\nTuy nhi\u00ean, ch\u1ec9 c\u00f3 Precision hay ch\u1ec9 c\u00f3 Recall th\u00ec kh\u00f4ng \u0111\u00e1nh gi\u00e1 \u0111\u01b0\u1ee3c ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh.\n>* Ch\u1ec9 d\u00f9ng Precision, m\u00f4 h\u00ecnh ch\u1ec9 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n cho m\u1ed9t \u0111i\u1ec3m m\u00e0 n\u00f3 ch\u1eafc ch\u1eafn nh\u1ea5t. Khi \u0111\u00f3 Precision = 1, tuy nhi\u00ean ta kh\u00f4ng th\u1ec3 n\u00f3i l\u00e0 m\u00f4 h\u00ecnh n\u00e0y t\u1ed1t.\n>* Ch\u1ec9 d\u00f9ng Recall, n\u1ebfu m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m \u0111\u1ec1u l\u00e0 positive. Khi \u0111\u00f3 Recall = 1, tuy nhi\u00ean ta c\u0169ng kh\u00f4ng th\u1ec3 n\u00f3i \u0111\u00e2y l\u00e0 m\u00f4 h\u00ecnh t\u1ed1t.<br>\nKhi \u0111\u00f3 F1-score \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng. F1-score l\u00e0 trung b\u00ecnh \u0111i\u1ec1u h\u00f2a (harmonic mean) c\u1ee7a precision v\u00e0 recall (gi\u1ea3 s\u1eed hai \u0111\u1ea1i l\u01b0\u1ee3ng n\u00e0y kh\u00e1c 0). F1-score \u0111\u01b0\u1ee3c tinh theo c\u00f4ng th\u1ee9c:\n$$\\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$$","59b88ea5":"# 3. Trainning","40b73a1a":"# Submission","566c493c":"* Chia t\u1eadp d\u1eef li\u1ec7u ra th\u00e0nh 2 ph\u1ea7n train, test v\u1edbi test_size = 0.3","d8d91f57":"**Vectorization:** Tr\u01b0\u1edbc khi \u0111\u01b0a v\u00e0o hu\u1ea5n luy\u1ec7n ta c\u1ea7n chuy\u1ec3n d\u1eef li\u1ec7u t\u1eeb d\u1ea1ng v\u00f4 h\u01b0\u1edbng sang d\u1ea1ng vector<br>\n**M\u00f4 h\u00ecnh t\u00fai t\u1eeb (bag-of-words)** M\u00f4 h\u00ecnh bag-of-words (BOW) l\u00e0 c\u00e1ch bi\u1ec3u di\u1ec5n bi\u1ebfn v\u0103n b\u1ea3n t\u00f9y \u00fd th\u00e0nh c\u00e1c vect\u01a1 c\u00f3 \u0111\u1ed9 d\u00e0i c\u1ed1 \u0111\u1ecbnh b\u1eb1ng c\u00e1ch \u0111\u1ebfm s\u1ed1 l\u1ea7n m\u1ed7i t\u1eeb xu\u1ea5t hi\u1ec7n, kh\u00f4ng quan t\u00e2m \u0111\u1ebfn ng\u1eef ph\u00e1p v\u00e0 th\u1eadm ch\u00ed tr\u1eadt t\u1ef1 t\u1eeb nh\u01b0ng v\u1eabn gi\u1eef t\u00ednh \u0111a d\u1ea1ng.\n>B\u01b0\u1edbc 1: X\u00e1c \u0111\u1ecbnh t\u00fai t\u1eeb v\u1ef1ng g\u1ed3m: the, cat, sat, in, hat, with <br>\n      B\u01b0\u1edbc 2: \u0110\u1ebfm s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ea3u c\u00e1c t\u1eeb\n![](https:\/\/miro.medium.com\/max\/1536\/1*3IACMnNpwVlCl8kSTJocPA.png)  \n* M\u00f4 h\u00ecnh t\u00fai t\u1eeb th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong c\u00e1c ph\u01b0\u01a1ng ph\u00e1p ph\u00e2n lo\u1ea1i t\u00e0i li\u1ec7u trong \u0111\u00f3 s\u1ef1 xu\u1ea5t hi\u1ec7n (t\u1ea7n su\u1ea5t) c\u1ee7a m\u1ed7i t\u1eeb \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t \u0111\u1eb7c tr\u01b0ng \u0111\u1ec3 \u0111\u00e0o t\u1ea1o m\u00e1y ph\u00e2n lo\u1ea1i\n* **\u01afu \u0111i\u1ec3m:** \u0110\u01a1n gi\u1ea3n, t\u00ednh to\u00e1n nhanh, d\u1ec5 t\u00ednh to\u00e1n. Kh\u00f4ng quan t\u00e2m \u0111\u1ebfn v\u1ecb tr\u00ed v\u00e0 ng\u1eef c\u1ea3nh<br>\n\n**CountVectorize:** \n* \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 chuy\u1ec3n \u0111\u1ed5i m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp c\u00e1c t\u00e0i li\u1ec7u v\u0103n b\u1ea3n th\u00e0nh m\u1ed9t vect\u01a1 c\u00f3 s\u1ed1 l\u01b0\u1ee3ng thu\u1eadt ng\u1eef \/ m\u00e3 th\u00f4ng b\u00e1o. N\u00f3 c\u0169ng cho ph\u00e9p x\u1eed l\u00fd tr\u01b0\u1edbc d\u1eef li\u1ec7u v\u0103n b\u1ea3n tr\u01b0\u1edbc khi t\u1ea1o bi\u1ec3u di\u1ec5n vect\u01a1. Ch\u1ee9c n\u0103ng n\u00e0y l\u00e0m cho n\u00f3 tr\u1edf th\u00e0nh m\u1ed9t m\u00f4-\u0111un bi\u1ec3u di\u1ec5n t\u00ednh n\u0103ng r\u1ea5t linh ho\u1ea1t cho v\u0103n b\u1ea3n\n","e6bec9ee":"Nh\u1eadn x\u00e9t:\n* \u1ede \u0111\u00e2y ta nh\u1eadn \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n so v\u1edbi m\u00f4 h\u00ecnh Naice Bayes\n* Thu\u1eadt to\u00e1n c\u0169ng ch\u1ea1y ch\u1eadm h\u01a1n","d4e0b929":"# Work Flow\n1. <a href=\"#1.-Problem-Description\">Problem Description <\/a>\n2. <a href=\"#2.-Data-Processing\">Data Processing <\/a>\n   * <a href=\"#Cleaning-Data\">Cleaning Data<\/a>\n   * <a href=\"#Vectorization\">Vectorization<\/a><br>\n3. <a href=\"#3.-Training\">Data Processing <\/a>\n   * <a href=\"#Naive-Bayes\">Naive Bayes<\/a>\n   * <a href=\"#LogisticRegression\">LogisticRegression<\/a>$$$$\n<a href=\"#Submission\">Submission <\/a>\n","386ca86b":"**Logistic Regression** l\u00e0 1 thu\u1eadt to\u00e1n ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 g\u00e1n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng cho 1 t\u1eadp h\u1ee3p gi\u00e1 tr\u1ecb r\u1eddi r\u1ea1c (nh\u01b0 0, 1, 2, ...). M\u1ed9t v\u00ed d\u1ee5 \u0111i\u1ec3n h\u00ecnh l\u00e0 ph\u00e2n lo\u1ea1i Email, g\u1ed3m c\u00f3 email c\u00f4ng vi\u1ec7c, email gia \u0111\u00ecnh, email spam, ...\n* C\u0169ng gi\u1ed1ng nh\u01b0 c\u00e1c thu\u1eadt to\u00e1n Linear (t\u00ecm v\u00e0 t\u1ed1i \u01b0u w) v\u1edbi c\u00f4ng th\u1ee9c chung: $$y = f(w^{T}x),(w \\in [w_{0}, w_{1},... w_{n}])$$\n* \u0110\u1ea7u ra d\u1ef1 \u0111o\u00e1n c\u1ee7a logistic regression th\u01b0\u1eddng \u0111\u01b0\u1ee3c vi\u1ebft chung d\u01b0\u1edbi d\u1ea1ng:$$y = \\theta (w^{T}x)$$\nTrong \u0111\u00f3 \u03b8 \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 logistic function.\n\n**Sigmoid function**<br>\n ![]( https:\/\/nickmccullum.com\/images\/python-deep-learning\/deep-learning-activation-functions\/sigmoid-function.png)\n\n* Nh\u01b0 v\u1eady sau khi t\u00ednh \u0111\u01b0\u1ee3c ph\u1ea7n tuy\u1ebfn t\u00ednh l\u00e0 $w^{T}x$, ph\u1ea7n tuy\u1ebfn t\u00ednh n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o h\u00e0m sigmoid($\\sigma()$). H\u00e0m $\\sigma()$ s\u1ebd tr\u1ea3 v\u1ec1 x\u00e1c su\u1ea5t m\u00e0 c\u00e2u h\u1ecfi l\u00e0 h\u1eefu \u00edch ($y = 0$) hay \u0111\u1ed9c h\u1ea1i ($y = 1$) v\u1edbi: $$P(y_{1}|x_{i};w) = \\sigma(w^{T}x); P(y_{0}|x_{i};w) = 1 - \\sigma(w^{T}x)$$\n* B\u00e0i to\u00e1n quay tr\u1edf v\u1ec1 v\u1edbi vi\u1ec7c **t\u1ed1i \u01b0u h\u00e0m m\u1ea5t m\u00e1t L** v\u1edbi m\u1ee5c ti\u00eau l\u00e0m cho $P(y|x_{i}) = \\sigma(w^{T}x) = \\widehat{y}$ c\u00e0ng ng\u00e0y c\u00e0ng l\u1edbn (ti\u1ebfn g\u1ea7n t\u1edbi 1): $$L = -log(P(y|x_{i}))$$ $$<=> \\frac{\\partial L}{\\partial w} = (\\widehat{y} - y)x = (\\sigma(w^{T}x) - y)x$$ $$=> w = w + \\lambda(\\widehat{y} - y)x$$\n* V\u1edbi $\\lambda$ l\u00e0 h\u1ec7 h\u1ecdc learning_rate (l\u00e0 m\u1ed9t con s\u1ed1 nh\u1ecf th\u01b0\u1eddng n\u1eb1m trong kho\u1ea3ng t\u1eeb 0.01 \u0111\u1ebfn 0.0001). <br>","682f219e":"# LogisticRegression","fe0efde5":"**D\u1eef li\u1ec7u:**\n* T\u1eadp d\u1eef li\u1ec7u train g\u1ed3m 13006122 h\u00e0ng x 3 c\u1ed9t\n* 3 c\u1ed9t g\u1ed3m 2 c\u1ed9t d\u1eef li\u1ec7u v\u0103n b\u1ea3n v\u00e0 1 c\u1ed9t d\u1eef li\u1ec7u s\u1ed1\n* C\u1ed9t pid ch\u1ee9a id c\u1ee7a c\u00e2u h\u1ecfi\n* C\u1ed9t questions_text ch\u1ee9a n\u1ed9i dung c\u00e2u h\u1ecfi\n* C\u1ed9t target c\u00f3 2 gi\u00e1 tr\u1ecb (0 n\u1ebfu l\u00e0 c\u00e2u h\u1ecfi insincere, 1 n\u1ebfu l\u00e0 c\u00e2u h\u1ecfi sincere) ","228e8641":"**Th\u00eam m\u1ed9t s\u1ed1 th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft**","06c56428":"* Nh\u01b0 ta \u0111\u00e3 ph\u00e2n t\u00edch \u1edf ph\u1ea7n tr\u01b0\u1edbc, t\u1eadp d\u1eef li\u1ec7u \u0111ang b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng. Ta c\u1ea7n ph\u1ea3i chia l\u1ea1i t\u1eadp train sao cho c\u00e2n b\u1eb1ng h\u01a1n\n* \u1ede \u0111\u00e2y ta chia l\u1ea1i theo t\u1ec9 l\u1ec7 4:1 (4 h\u1eefu \u00edch : 1 toxic)","ff7585a5":"  **Nh\u1eadn x\u00e9t:**\n* S\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi sincere chi\u1ebfm 93,81% l\u1edbn h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi insincere \n* D\u1eef li\u1ec7u \u0111ang b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng nghi\u00eam tr\u1ecdng\n> S\u1eed d\u1ee5ng th\u01b0\u1edbc \u0111o \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh l\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c (accuracy) s\u1ebd kh\u00f4ng ph\u00f9 h\u1ee3p. <br>\nVi\u1ec7c t\u1eadp d\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng nghi\u00eam tr\u1ecdng c\u00f3 th\u1ec3 d\u1eabn t\u1edbi s\u1ef1 d\u1ef1 b\u00e1o k\u00e9m ch\u00ednh x\u00e1c c\u1ee7a nh\u00f3m thi\u1ec3u s\u1ed1 (insincere question) \n\n \n **H\u01b0\u1edbng gi\u1ea3i quy\u1ebft:**\n* Chia l\u1ea1i t\u1eadp d\u1eef li\u1ec7u theo t\u1ec9 l\u1ec7 4:1","a9487e09":"# Cleaning Data","ee4198bd":"# 2. Data Processing","8a064bd9":"\u0110\u00e1nh gi\u00e1:\n* T\u1ed1c \u0111\u1ed9 ch\u1ea1y m\u00f4 h\u00ecnh kh\u00e1 nhanh  do m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n\n* \u0110i\u1ec3m accuracy kh\u00e1 l\u00e0 cao do t\u1eadp d\u1eef li\u1ec7u m\u1ea5t c\u00e2n b\u1eb1ng\n* F1 score \u0111\u1ea1t \u0111\u01b0\u1ee3c \u1edf m\u1ee9c trung b\u00ecnh","46a6aa99":"**Load d\u1eef li\u1ec7u v\u00e0o data frame**","9838a2bd":"# Ph\u00e2n t\u00edch d\u1eef li\u1ec7u b\u1eb1ng Word Cloud\n* Word Cloud l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt tr\u1ef1c quan h\u00f3a d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 bi\u1ec3u di\u1ec5n d\u1eef li\u1ec7u v\u0103n b\u1ea3n, trong \u0111\u00f3 k\u00edch th\u01b0\u1edbc c\u1ee7a m\u1ed7i t\u1eeb cho bi\u1ebft t\u1ea7n su\u1ea5t ho\u1eb7c t\u1ea7m quan tr\u1ecdng c\u1ee7a n\u00f3","929d8b16":"# Naive Bayes\n* **Naive Bayes Classification** (NBC) l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n d\u1ef1a tr\u00ean \u0111\u1ecbnh l\u00fd Bayes v\u1ec1 l\u00fd thuy\u1ebft x\u00e1c su\u1ea5t  \u0111\u1ec3 \u0111\u01b0a ra c\u00e1c ph\u00e1n \u0111o\u00e1n c\u0169ng nh\u01b0 ph\u00e2n lo\u1ea1i d\u1eef li\u1ec7u d\u1ef1a tr\u00ean c\u00e1c d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c quan s\u00e1t v\u00e0 th\u1ed1ng k\u00ea. Naive Bayes Classification l\u00e0 m\u1ed9t trong nh\u1eefng thu\u1eadt to\u00e1n \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng r\u1ea5t nhi\u1ec1u trong c\u00e1c l\u0129nh v\u1ef1c Machine learning d\u00f9ng \u0111\u1ec3 \u0111\u01b0a c\u00e1c d\u1ef1 \u0111o\u00e1n ch\u00ednh x\u00e1c nh\u1ea5t d\u1ef1 tr\u00ean m\u1ed9t t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c thu th\u1eadp, v\u00ec n\u00f3 kh\u00e1 d\u1ec5 hi\u1ec3u v\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c cao. N\u00f3 thu\u1ed9c v\u00e0o nh\u00f3m Supervised Machine Learning Algorithms (thu\u1eadt to\u00e1n h\u1ecdc c\u00f3 h\u01b0\u1edbng d\u1eabn), t\u1ee9c l\u00e0 m\u00e1y h\u1ecdc t\u1eeb c\u00e1c v\u00ed d\u1ee5 t\u1eeb c\u00e1c m\u1eabu d\u1eef li\u1ec7u \u0111\u00e3 c\u00f3.$$$$\n 1. G\u1ecdi D l\u00e0 t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n, trong \u0111\u00f3 m\u1ed7i ph\u1ea7n t\u1eed d\u1eef li\u1ec7u X \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng m\u1ed9t vector ch\u1ee9a n gi\u00e1 tr\u1ecb thu\u1ed9c t\u00ednh A1, A2,...,An = {x1,x2,...,xn}\n 2. Gi\u1ea3 s\u1eed c\u00f3 m l\u1edbp C1, C2,..,Cm. Cho m\u1ed9t ph\u1ea7n t\u1eed d\u1eef li\u1ec7u X, b\u1ed9 ph\u00e2n l\u1edbp s\u1ebd g\u00e1n nh\u00e3n cho X l\u00e0 l\u1edbp c\u00f3 x\u00e1c su\u1ea5t h\u1eadu nghi\u1ec7m l\u1edbn nh\u1ea5t. C\u1ee5 th\u1ec3, b\u1ed9 ph\u00e2n l\u1edbp Bayes s\u1ebd d\u1ef1 \u0111o\u00e1n X thu\u1ed9c v\u00e0o l\u1edbp Ci n\u1ebfu v\u00e0 ch\u1ec9 n\u1ebfu:\n$$P(Ci|X) > P(Cj|X) (1<= i, j <=m, i != j)$$\n 3. Gi\u00e1 tr\u1ecb n\u00e0y s\u1ebd t\u00ednh d\u1ef1a tr\u00ean \u0111\u1ecbnh l\u00fd Bayes.\n\u0110\u1ec3 t\u00ecm x\u00e1c su\u1ea5t l\u1edbn nh\u1ea5t, ta nh\u1eadn th\u1ea5y c\u00e1c gi\u00e1 tr\u1ecb P(X) l\u00e0 gi\u1ed1ng nhau v\u1edbi m\u1ecdi l\u1edbp n\u00ean kh\u00f4ng c\u1ea7n t\u00ednh. Do \u0111\u00f3 ta ch\u1ec9 c\u1ea7n t\u00ecm gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t c\u1ee7a P(X|Ci) * P(Ci). Ch\u00fa \u00fd r\u1eb1ng P(Ci) \u0111\u01b0\u1ee3c \u01b0\u1edbc l\u01b0\u1ee3ng b\u1eb1ng |Di|\/|D|, trong \u0111\u00f3 Di l\u00e0 t\u1eadp c\u00e1c ph\u1ea7n t\u1eed d\u1eef li\u1ec7u thu\u1ed9c l\u1edbp Ci. N\u1ebfu x\u00e1c su\u1ea5t ti\u1ec1n nghi\u1ec7m P(Ci) c\u0169ng kh\u00f4ng x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c th\u00ec ta coi ch\u00fang b\u1eb1ng nhau P(C1) = P(C2) = ... = P(Cm), khi \u0111\u00f3 ta ch\u1ec9 c\u1ea7n t\u00ecm gi\u00e1 tr\u1ecb P(X|Ci) l\u1edbn nh\u1ea5t.<br>\n 4. Khi s\u1ed1 l\u01b0\u1ee3ng c\u00e1c thu\u1ed9c t\u00ednh m\u00f4 t\u1ea3 d\u1eef li\u1ec7u l\u00e0 l\u1edbn th\u00ec chi ph\u00ed t\u00ednh to\u00e0n P(X|Ci) l\u00e0 r\u1ea5t l\u1edbn, d\u00f3 \u0111\u00f3 c\u00f3 th\u1ec3 gi\u1ea3m \u0111\u1ed9 ph\u1ee9c t\u1ea1p c\u1ee7a thu\u1eadt to\u00e1n Naive Bayes gi\u1ea3 thi\u1ebft c\u00e1c thu\u1ed9c t\u00ednh \u0111\u1ed9c l\u1eadp nhau. Khi \u0111\u00f3 ta c\u00f3 th\u1ec3 t\u00ednh:\n$$P(X|Ci) = P(x1|Ci)...P(xn|Ci)$$\n","1f054543":"**Nh\u1eadn x\u00e9t:** Ta th\u1ea5y c\u00e1c c\u00e2u h\u1ecfi trong c\u1ed9t question_text gi\u1eefa t\u1eadp train v\u00e0 t\u1eadp test l\u00e0 kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng","53f31ed3":"# Vectorization","745b71f9":"Sau khi \u0111\u00e1nh gi\u00e1 ta th\u1ea5y d\u1eef li\u1ec7u c\u00f2n kh\u00e1 ph\u1ee9c t\u1ea1p v\u00e0 nhi\u1ec1u nhi\u1ec5u. \u0110\u1ec3 \u0111\u01a1n gi\u1ea3n ho\u00e1 d\u1eef li\u1ec7u ta c\u00f3 th\u1ec3 th\u1ef1c hi\u1ec7n m\u1ed9t s\u1ed1 b\u01b0\u1edbc sau:\n* Chuy\u1ec3n c\u00e1c t\u1eeb in hoa th\u00e0nh in th\u01b0\u1eddng\n* Chuy\u1ec3n c\u00e1c t\u1eeb r\u00fat g\u1ecdn v\u1ec1 d\u1ea1ng ho\u00e0n ch\u1ec9nh\n* C\u00e1c t\u1eeb d\u1eebng (a, an, the...) xu\u1ea5t hi\u1ec7n r\u1ea5t nhi\u1ec1u trong c\u00e2u h\u1ecfi nh\u01b0ng kh\u00f4ng li\u00ean quan \u0111\u1ebfn n\u1ed9i dung c\u00e2u h\u1ecfi n\u00ean c\u1ea7n \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf\n* C\u00e1c s\u1ed1 v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t c\u0169ng t\u01b0\u01a1ng t\u1ef1 nh\u01b0 t\u1eeb d\u1eebng kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn \u00fd ngh\u0129a c\u00e2u h\u1ecfi\n* Chuy\u1ec3n c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng t\u1eeb g\u1ed1c"}}