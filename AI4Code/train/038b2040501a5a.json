{"cell_type":{"cfb1f935":"code","75d8b0d3":"code","a3ac5623":"code","9e3ec8a7":"code","0b66f236":"code","9608c2ef":"code","37b07667":"code","00c1cd9f":"code","70ececd2":"code","7ba69ad1":"code","b943bf50":"code","9b6f5561":"code","b2faa2ed":"code","51ef8788":"code","1d3e6ef6":"code","9ddf77b5":"code","ed8cb62b":"code","3d96d326":"code","07e4b560":"code","b85d38d8":"code","4869f118":"code","eafa90cd":"code","ff6a196c":"code","2fe734d5":"code","8e304fa1":"code","ebc38908":"markdown","1f874741":"markdown","2a5b8fdb":"markdown","03298e26":"markdown","548f0ae0":"markdown","4ae67217":"markdown","e837382a":"markdown","3b8a59a9":"markdown","ac1b0f02":"markdown","aa3d175b":"markdown","886be3eb":"markdown","44d82cbc":"markdown","22ebc1ee":"markdown","7a77409e":"markdown","8ae450b2":"markdown","1aa34a60":"markdown","3b54b749":"markdown","216e676f":"markdown","2c3d137e":"markdown","0db5193c":"markdown","e9c179d1":"markdown","e2bde50a":"markdown","a6d6c765":"markdown","57eb9910":"markdown","73d6de08":"markdown","fb5380f8":"markdown","f6992397":"markdown","347a160d":"markdown","f5287ab1":"markdown"},"source":{"cfb1f935":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","75d8b0d3":"import os\n\ndata = pd.read_csv('..\/input\/coursera-ml\/ex1data1.txt', header=None, names=['Population', 'Profit'])\ndata.head()","a3ac5623":"data.describe()","9e3ec8a7":"data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))","0b66f236":"def computeCost(X, y, theta):\n    inner = np.power(((X * theta.T) - y), 2)\n    return np.sum(inner) \/ (2 * len(X))","9608c2ef":"data.insert(0, 'Ones', 1)","37b07667":"# set X (training data) and y (target variable)\ncols = data.shape[1]\nX = data.iloc[:,0:cols-1]\ny = data.iloc[:,cols-1:cols]","00c1cd9f":"X.head()","70ececd2":"y.head()","7ba69ad1":"X = np.matrix(X.values)\ny = np.matrix(y.values)\ntheta = np.matrix(np.array([0,0]))","b943bf50":"theta","9b6f5561":"X.shape, theta.shape, y.shape","b2faa2ed":"computeCost(X, y, theta)","51ef8788":"def gradientDescent(X, y, theta, alpha, iters):\n    temp = np.matrix(np.zeros(theta.shape))\n    parameters = int(theta.ravel().shape[1])\n    cost = np.zeros(iters)\n    \n    for i in range(iters):\n        error = (X * theta.T) - y\n        \n        for j in range(parameters):\n            term = np.multiply(error, X[:,j])\n            temp[0,j] = theta[0,j] - ((alpha \/ len(X)) * np.sum(term))\n            \n        theta = temp\n        cost[i] = computeCost(X, y, theta)\n        \n    return theta, cost","1d3e6ef6":"alpha = 0.01\niters = 1000","9ddf77b5":"g, cost = gradientDescent(X, y, theta, alpha, iters)\ng","ed8cb62b":"computeCost(X, y, g)","3d96d326":"x = np.linspace(data.Population.min(), data.Population.max(), 100)\nf = g[0, 0] + (g[0, 1] * x)\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(x, f, 'r', label='Prediction')\nax.scatter(data.Population, data.Profit, label='Traning Data')\nax.legend(loc=2)\nax.set_xlabel('Population')\nax.set_ylabel('Profit')\nax.set_title('Predicted Profit vs. Population Size')","07e4b560":"fig, ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost, 'r')\nax.set_xlabel('Iterations')\nax.set_ylabel('Cost')\nax.set_title('Error vs. Training Epoch')","b85d38d8":"\ndata2 = pd.read_csv('..\/input\/coursera-ml\/ex1data2.txt', header=None, names=['Size', 'Bedrooms', 'Price'])\ndata2.head()","4869f118":"data2 = (data2 - data2.mean()) \/ data2.std()\ndata2.head()","eafa90cd":"# add ones column\ndata2.insert(0, 'Ones', 1)\n\n# set X (training data) and y (target variable)\ncols = data2.shape[1]\nX2 = data2.iloc[:,0:cols-1]\ny2 = data2.iloc[:,cols-1:cols]\n\n# convert to matrices and initialize theta\nX2 = np.matrix(X2.values)\ny2 = np.matrix(y2.values)\ntheta2 = np.matrix(np.array([0,0,0]))\n\n# perform linear regression on the data set\ng2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)\n\n# get the cost (error) of the model\ncomputeCost(X2, y2, g2)","ff6a196c":"fig, ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost2, 'r')\nax.set_xlabel('Iterations')\nax.set_ylabel('Cost')\nax.set_title('Error vs. Training Epoch')","2fe734d5":"from sklearn import linear_model\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)","8e304fa1":"x = np.array(X[:, 1].A1)\nf = model.predict(X).flatten()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.plot(x, f, 'r', label='Prediction')\nax.scatter(data.Population, data.Profit, label='Traning Data')\nax.legend(loc=2)\nax.set_xlabel('Population')\nax.set_ylabel('Profit')\nax.set_title('Predicted Profit vs. Population Size')","ebc38908":"Credits to : https:\/\/github.com\/jdwittenauer\/ipython-notebooks\/tree\/master\/exercises\/ML ","1f874741":"Instead of implementing these algorithms from scratch, we could also use scikit-learn's linear regression function.  Let's apply scikit-learn's linear regressio algorithm to the data from part 1 and see what it comes up with.","2a5b8fdb":"Here's what theta looks like.","03298e26":"For this task we add another pre-processing step - normalizing the features.  This is very easy with pandas.","548f0ae0":"Finally we can compute the cost (error) of the trained model using our fitted parameters.","4ae67217":"We can take a quick look at the training progess for this one as well.","e837382a":"So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules defined in the text.","3b8a59a9":"the dataset conatains 2 columns : Profit and population. ","ac1b0f02":"Let's take a quick look at the shape of our matrices.","aa3d175b":"Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients.","886be3eb":"Now let's repeat our pre-processing steps from part 1 and run the linear regression procedure on the new data set.","44d82cbc":"Now let's plot the linear model along with the data to visually see how well it fits.","22ebc1ee":"Now let's compute the cost for our initial solution (0 values for theta).","7a77409e":"Exercise 1 also included a housing price data set with 2 variables (size of the house in square feet and number of bedrooms) and a target (price of the house).  Let's use the techniques we already applied to analyze that data set as well.","8ae450b2":"Let's take a look to make sure X (training set) and y (target variable) look correct.","1aa34a60":"Initialize some additional variables - the learning rate alpha, and the number of iterations to perform.","3b54b749":"Now let's implement linear regression using gradient descent to minimize the cost function.\n* Linear regression : In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.\n* Gradient descent : Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent is generally attributed to Cauchy, who first suggested it in 1847,[1] but its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944.[2]\n\nThe equations implemented in the following code samples are detailed in [ex1.pdf](https:\/\/github.com\/jdwittenauer\/ipython-notebooks\/blob\/master\/exercises\/ML\/ex1.pdf).","216e676f":"First we'll create a function to compute the cost of a given solution (characterized by the parameters theta).","2c3d137e":"Let's plot it to get a better idea of what the data looks like.","0db5193c":"## Linear regression with one variable","e9c179d1":"## Linear regression with multiple variables","e2bde50a":"Here's what the scikit-learn model's predictions look like.","a6d6c765":"import libraries","57eb9910":"Now let's run the gradient descent algorithm to fit our parameters theta to the training set.","73d6de08":"# Machine Learning Exercise 1 - Linear Regression","fb5380f8":"The cost function is expecting numpy matrices so we need to convert X and y before we can use them.  We also need to initialize theta.","f6992397":"In this exercise we'll :\n* implement simple linear regression using gradient descent \n* apply it to an example problem.\n* extend our implementation to handle multiple variables \n* apply it to a slightly more difficult example","347a160d":"Now let's do some variable initialization.","f5287ab1":"Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."}}