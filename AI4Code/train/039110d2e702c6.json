{"cell_type":{"6a516b5b":"code","bcb1f60e":"code","b9757606":"code","e4119a80":"code","5b593814":"code","d626ed29":"code","65ceb7e4":"code","422c5ed1":"code","9ad65d7c":"code","67b4f9c8":"code","8bde090c":"code","5f759b20":"code","d1b9bde3":"code","2cc6e510":"code","222929d3":"code","98c410e5":"code","37db1784":"code","9b04ce28":"code","fd0d21f8":"code","cfb8f95d":"code","972d1ce5":"code","f04dfe21":"code","11dfe888":"code","dd7ca949":"code","66654a71":"code","8f9b3942":"code","a04a678c":"code","313c7795":"code","f6357c68":"code","1327b74e":"code","c68b9710":"code","3bd5b2f9":"code","8a3ea870":"code","a46aa0c5":"code","119f1c8d":"code","b8f33303":"code","5333c2d8":"code","1969a4ca":"markdown","b2fef802":"markdown","ce5813b2":"markdown","ef690c21":"markdown","5feafdd3":"markdown","ae1cc6bf":"markdown","04c5f3ee":"markdown","5994bf9f":"markdown"},"source":{"6a516b5b":"import numpy as np \nimport pandas as pd \nimport bz2\nimport gc\nimport chardet\nimport re\nimport os\nimport pickle\nprint(os.listdir(\"..\/input\"))","bcb1f60e":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.models import model_from_json\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","b9757606":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","e4119a80":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","5b593814":"del train_file, test_file","d626ed29":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","65ceb7e4":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n                                                       \nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","422c5ed1":"del train_file_lines, test_file_lines\ntrain_sentences[0]","9ad65d7c":"gc.collect()","67b4f9c8":"# Create train and test dataframes\n#Na_train = {'Sentence': train_sentences, 'Label': train_labels}\n#Nav_train = pd.DataFrame(Na_train)\n\n#Na_test = {'Sentence': test_sentences, 'Label': test_labels}\n#Nav_test = pd.DataFrame(Na_test)\n\n#Nav_train.head()\n#Nav_train.to_csv(\"amazon_train.csv\")\n","8bde090c":"#train_pos = Nav_train[Nav_train['Label'] == 1]\n#train_pos = Nav_train['Sentence']\n#train_neg = Nav_train[Nav_train['Label'] == 0]\n#train_neg = Nav_train['Sentence']\n\n#test_pos = Nav_test[Nav_test['Label'] == 1]\n#test_pos = Nav_test['Sentence']\n#test_neg = Nav_test[Nav_test['Label'] == 0]\n#test_neg = Nav_test['Sentence']","5f759b20":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(binary = True)\ncv.fit(train_sentences)\nlog_train=cv.transform(train_sentences)\nlog_test =cv.transform(test_sentences)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n","d1b9bde3":"#for c in [0.01, 0.05, 0.025, 0.5, 1]:\n#lr = LogisticRegression(C=0.05)\n#lr.fit(log_train, train_labels)","2cc6e510":"\n# save the model to disk\nfilename = 'finalized_model.sav'\n#pickle.dump(lr, open(filename, 'wb'))\n \n# some time later...\n\n","222929d3":" \n# load the model from disk\nlr_model = pickle.load(open(\"..\/input\/sentiment-analysis-logregre-vs-cudnnlstm\/finalized_model.sav\", 'rb'))","98c410e5":"\nprint(\"The product is  good: RESULT:\")\ntesting = \"the product is  good\"\ntesting = [testing]\nprint(lr_model.predict(cv.transform(testing)))\n\nprint(\"The product is not good: RESULT:\")\ntesting = \"The product is not good\"\ntesting = [testing]\n\nprint(lr_model.predict(cv.transform(testing)))\n","37db1784":"acc=(accuracy_score(test_labels, lr_model.predict(log_test)))\nprint(\"Accuracy using LogisticRegression is\", acc)","9b04ce28":"max_features = 20000\nmaxlen = 100","fd0d21f8":"tokenizer = text.Tokenizer(num_words=max_features)","cfb8f95d":"tokenizer.fit_on_texts(train_sentences)","972d1ce5":"tokenized_train = tokenizer.texts_to_sequences(train_sentences)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","f04dfe21":"print(tokenized_train[0])\nprint(X_train[0])","11dfe888":"tokenized_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","dd7ca949":"EMBEDDING_FILE = '..\/input\/glovetwitter100d\/glove.twitter.27B.100d.txt'","66654a71":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","8f9b3942":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","a04a678c":"\ndel tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, word_index, embeddings_index, all_embs, nb_words\ngc.collect()","313c7795":"batch_size = 2048\nepochs = 7\nembed_size = 100","f6357c68":"gc.collect()","1327b74e":"def cudnnlstm_model(conv_layers = 2, max_dilation_rate = 3):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n    x = Dropout(0.25)(x)\n    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n    x = prefilt\n    for strides in [1, 1, 2]:\n        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n    x = concatenate([x_f, x_b])\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['binary_accuracy'])\n\n    return model\n\ncudnnlstm_model = cudnnlstm_model()\ncudnnlstm_model.summary()","c68b9710":"weight_path=\"early_weights.hdf5\"\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\ncallbacks = [checkpoint, early_stopping]","3bd5b2f9":"#cudnnlstm_model.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)","8a3ea870":"cudnnlstm_model.load_weights(weight_path)\nscore, acc = cudnnlstm_model.evaluate(X_test, test_labels, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","a46aa0c5":" \n# serialize model to JSON\nmodel_json = cudnnlstm_model.to_json()\nwith open(\"cudnnlstm_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\ncudnnlstm_model.save_weights(\"cudnnlstm_model.h5\")\nprint(\"Saved model to disk\")\n \n# later...","119f1c8d":"print(os.listdir())","b8f33303":"loaded_model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['binary_accuracy'])\n","5333c2d8":"#print(tokenized_test[0])\nloaded_model.predict( sequence.pad_sequences(tokenizer.texts_to_sequences(X_test[0],maxlen=maxlen))","1969a4ca":"\n## Cleaning and Feature Extraction","b2fef802":"# Read & Preprocess data","ce5813b2":"# Conclusion\nThe logesstic regression model gave accuracy of 90% wherease the CuDNNLSTM gave accuracy of 93%. The deep learning model using GPU to perform sentiment annalysis and can be trained with more iteration to give higher accuracy than 93%","ef690c21":"## Create Lists containing Train & Test sentences","5feafdd3":"## Convert from raw binary strings to strings that can be parsed","ae1cc6bf":"# 2: Using CuDNNLSTM MODEL","04c5f3ee":"# 1: Using Logistic Regression\n\n## Vectorization of data","5994bf9f":"### Separate Positive and Negative tweets"}}