{"cell_type":{"76ca7676":"code","0bb69b16":"code","92b2c6e0":"code","097cc51b":"code","250bd542":"code","f6a8207a":"code","60d97f6c":"code","ddcda13e":"code","a25b6227":"code","5dbc09e6":"code","2c89fe92":"code","1d7791b0":"code","8e0a344f":"code","71bed6d5":"code","065c9730":"code","062ee12a":"code","8db01087":"code","6f3cd513":"code","cf7b9aed":"code","946feec1":"code","ce082dbe":"code","5aa14e7f":"code","6ed3905e":"code","b62fe4b5":"markdown"},"source":{"76ca7676":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0bb69b16":"train = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')","92b2c6e0":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef merge_calendar(df, col):\n    return pd.merge(df, calendar[[col+'_enc',col]] , how = 'left', on = col + '_enc')\ndef merge_id(df, col):\n    return pd.merge(df, id_df[[col+'_enc',col+'_id']] , how = 'left', on = col + '_enc')","097cc51b":"#train = reduce_mem_usage(train)\n#train = train.drop(['d_%s' %(i) for i in range(1,1101)], axis = 1)\n\ncalendar['date'] = calendar['date'].map(lambda x:pd.to_datetime(x, format = '%Y-%m-%d'))\ncalendar = calendar.sort_values('date')\ncalendar['day_num'] = LabelEncoder().fit_transform(calendar['date'])\n\n\nid_df = train[['id','item_id','dept_id','cat_id','store_id','state_id']]\nid_df['id_enc'] = LabelEncoder().fit_transform(id_df['id'])\nid_df['item_enc'] = LabelEncoder().fit_transform(id_df['item_id'])\nid_df['dept_enc'] = LabelEncoder().fit_transform(id_df['dept_id'])\nid_df['cat_enc'] = LabelEncoder().fit_transform(id_df['cat_id'])\nid_df['store_enc'] = LabelEncoder().fit_transform(id_df['store_id'])\nid_df['state_enc'] = LabelEncoder().fit_transform(id_df['state_id'])","250bd542":"from itertools import product\n'''\ncreate TimeSeriesMatrx\n\nday_num of matrix = day_num of calendar (sorted data column of calendar to make sure)\n\n\n\n\n'''\n\nmatrix = []\n\nid_arr = list(id_df['id_enc'])\nfor i in range(813):\n\n    matrix.append(list(product([i], id_arr)))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns = ['day_num','id'])\n\nmatrix[['day_num', 'id']] = matrix[['day_num', 'id']].astype('int16')\n\nmatrix['day_num'] = matrix['day_num'] + 1100\n\nmatrix['sales'] = np.hstack(train.loc[:, 'd_1101':].values)\n\nmatrix['sales'] = matrix['sales'].astype('int16')","f6a8207a":"calendar = calendar.fillna('none')\nlol_cols = ['wday', 'month', 'year', 'd',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'day_num']\nused_cols = []\n\nfor col in lol_cols:\n    if calendar[col].dtype == object:\n        col_name = col + '_enc'\n        calendar[col_name] = LabelEncoder().fit_transform(calendar[col])\n        used_cols.append(col_name)\n    else:\n        used_cols.append(col)\n        \ncal_merge = reduce_mem_usage(calendar[used_cols])\nmatrix = pd.merge(matrix, cal_merge, how = 'left', on = 'day_num')\nid_cols = ['id_enc', 'item_enc', 'dept_enc', 'cat_enc','store_enc', 'state_enc']\nmatrix = matrix.rename(columns = {\"id\":\"id_enc\"})\nmatrix = pd.merge(matrix, reduce_mem_usage(id_df[id_cols]), how = 'left', on = 'id_enc')","60d97f6c":"matrix.head()","ddcda13e":"lol = matrix.groupby('event_name_1_enc')['sales'].agg(['count','mean']).sort_values('mean').reset_index()\nlol = merge_calendar(lol, 'event_name_1')\nlol = lol.drop_duplicates().reset_index().drop('index', axis = 1)","a25b6227":"lol.head(5)","5dbc09e6":"lol.tail(20)","2c89fe92":"matrix.head(2)","1d7791b0":"lel = matrix[matrix.event_name_1_enc == 21].groupby('dept_enc')['sales'].agg(['mean','count']).sort_values('mean')\nlel = merge_id(lel, 'dept')\nlel = lel.drop_duplicates().reset_index().drop('index', axis = 1)","8e0a344f":"lel.head(10)","71bed6d5":"lel.tail(10)","065c9730":"lel = matrix[matrix.event_name_1_enc == 21].groupby('dept_enc')['sales'].agg(['mean','count']).sort_values('mean')\nlel = merge_id(lel, 'dept')\nlel = lel.drop_duplicates().reset_index().drop('index', axis = 1)","062ee12a":"lel.head()","8db01087":"lel","6f3cd513":"id_df.head()","cf7b9aed":"def get_name(name, entry):\n    dept_dict = id_df[[name + '_id',name + '_enc']].drop_duplicates()\n    dept_dict = dict((y,x) for x,y in list(zip(dept_dict[name + '_id'], dept_dict[name + '_enc'])))\n    return dept_dict[entry]","946feec1":"loldf = matrix.groupby(['day_num','item_enc'])['sales'].sum().reset_index()","ce082dbe":"loldf","5aa14e7f":"\nfor item_id in matrix.item_enc.unique()[:10]:\n    plt.figure(figsize = (14,4))\n    plt.title(get_name('item', item_id))\n    plt.plot(loldf[loldf['item_enc'] == item_id].reset_index().['sales'])\n    \n    ","6ed3905e":"for item_id in matrix.item_enc.unique()[:10]:\n    plt.figure(figsize = (14,4))\n    plt.title(get_name('item', item_id))\n    plt.plot(loldf[loldf['item_enc'] == item_id].reset_index()['sales'])","b62fe4b5":"Hi guys, here i am gonna fix the data into traditional timeseries format with day_num\/ all features for each day_num\/ sales. i've merged all features into the data set so it should be easy for you guys to do EDA and stuff. I was afraid there would not be enough memory so I only transformed the data of the last 810-ish days, but it turned out my resulting dataframe with 27.8m rows and 19 feature columns is taking up only 3.3 GB of RAM, so you could transform the whole dataset if you want, it'd take 9 GB at most.\n\nI wrote a merge function for you to merge basic groupby dataframes and be able to look at the string features, since i had to label encoded all of the object features in the matrix to minimize memory consumption.you can write your own merge functions or merge by yourself of course.\n\nreduce_mem_usage is of course not mine, it's a popular reduce_mem_usage function but i'm too lay to put the source... sorry guys... will do later\nHave fun guys"}}