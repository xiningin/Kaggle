{"cell_type":{"753cdb0a":"code","ae5d6de9":"code","3b4ea3d1":"code","f31e1d5c":"code","6698d23b":"code","1617aa6b":"code","e93f93eb":"code","a5b76fcc":"code","be4b3536":"code","bb7510b6":"code","a2605b31":"code","b221472c":"code","b7f3cd7f":"code","c5cb62aa":"code","dfe30ee4":"code","c09fd364":"code","58306134":"code","0cfaf1ac":"code","3a21d439":"code","13ee98ef":"code","b1ba70c2":"code","32c84a36":"code","bf203903":"code","7523ac45":"code","b9bfed63":"code","2c38e6d9":"markdown","e7706e43":"markdown","ace5079b":"markdown","97d36820":"markdown","91449f74":"markdown","ef790bbc":"markdown","f08cb3ec":"markdown","d32b2e6d":"markdown","b5780bca":"markdown","0ffdf67d":"markdown","9a0de6aa":"markdown","74dc8d7b":"markdown","3f7ac73c":"markdown","a6eee431":"markdown","9c14f9a1":"markdown","db6991e3":"markdown","785332e0":"markdown","3a7935c7":"markdown","cb7a43d7":"markdown","5f196b71":"markdown"},"source":{"753cdb0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \nfrom collections import Counter\n\n#Importing Models\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom xgboost import XGBClassifier\n\n#Importing preprocessors\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\n\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV, RepeatedStratifiedKFold, StratifiedShuffleSplit,cross_val_score\nfrom sklearn.metrics import confusion_matrix , classification_report, make_scorer\nfrom sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\nfrom sklearn.metrics import plot_roc_curve ,roc_curve, roc_auc_score\nfrom sklearn.metrics import fbeta_score\nfrom yellowbrick.classifier import ClassificationReport\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.combine import SMOTETomek,SMOTEENN\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours\nfrom imblearn.over_sampling import SMOTE\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae5d6de9":"df = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\", index_col='Id')\nsss=StratifiedShuffleSplit(n_splits=1,test_size=0.25,random_state=42)\n\nfor train_index, test_index in sss.split(df,df[\"Risk_Flag\"], ):\n    sss_train = df.iloc[train_index]\n    sss_test = df.iloc[test_index]\n    \nx_train= sss_train.drop(['Risk_Flag','Profession','STATE','CITY'], axis=1)\nx_test= sss_test.drop(['Risk_Flag','Profession','STATE','CITY'], axis=1)\ny_train = sss_train.Risk_Flag\ny_test = sss_test.Risk_Flag\ncount = Counter(y_train)\ndf\n","3b4ea3d1":"df.info()","f31e1d5c":"df.describe()","6698d23b":"r = df.groupby('Risk_Flag')['Risk_Flag'].count()\nplt.pie(r, explode=[0.05, 0.1], labels=['Non-Defaulter', 'Defaulter'], radius=1.5, autopct='%1.1f%%',  shadow=True)\n;\nplt.show()\n","1617aa6b":"\ndf.hist(figsize = (20, 15))\nplt.tight_layout()\n","e93f93eb":"plt.figure(figsize=(10,18))\ndf['Profession'].value_counts().plot(kind='barh')\n\n\n","a5b76fcc":"pd.crosstab(df.Risk_Flag,df.Car_Ownership).plot(kind= \"bar\",figsize = (10,6))","be4b3536":"pd.crosstab(df.Risk_Flag,df.CURRENT_JOB_YRS).plot(kind= \"bar\",figsize = (10,6) )","bb7510b6":"pd.crosstab(df.Risk_Flag,df.House_Ownership).plot(kind= \"bar\",figsize = (10,6) , color = [\"salmon\",\"lightblue\",\"blue\"] )","a2605b31":"fig, ax = plt.subplots( figsize = (12,8) )\n\ncorr_matrix = df.corr()\ncorr_heatmap = sns.heatmap( corr_matrix, cmap = \"YlGnBu\", annot=True, ax=ax, annot_kws={\"size\": 14})\n\nplt.show()","b221472c":"cat_var = ['Married\/Single','House_Ownership','Car_Ownership']\nnum_var = list(x_train.select_dtypes(include =['int64','float64']).columns)\n\ny_train,y_test = np.array(y_train), np.array(y_test)\nprint(num_var)\nprint(cat_var)","b7f3cd7f":"\npreprocessor= ColumnTransformer(transformers=[('o',OneHotEncoder(sparse=False,handle_unknown='ignore'),cat_var),('s',MinMaxScaler(), num_var)], remainder='passthrough')\nx_arr = preprocessor.fit_transform(x_train)\nx_arr_test = preprocessor.transform(x_test)","c5cb62aa":"new_names1 = preprocessor.transformers_[0][1].get_feature_names()\nnew_x = pd.DataFrame(x_arr)\nnew_x.rename(columns= {i:new_names1[i] for i in range(7)}, inplace=True)\nnew_x","dfe30ee4":"# def evaluate_model(X, y, model):\n#     # define evaluation procedure\n#     cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n#     # define the model evaluation metric\n#     metric = make_scorer(f2_measure)\n#     # evaluate model\n#     scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n#     return scores\ndef f2_measure(y_true, y_pred): #Modified version of F1 measure for more weight on false negatives\n    return fbeta_score(y_true, y_pred, beta=2)\nscale_pos = count[0] \/ count[1] \n\nmodels = [RandomForestClassifier(class_weight='balanced', random_state=42), LogisticRegression(solver='lbfgs', class_weight='balanced', random_state=42),\nKNeighborsClassifier(n_jobs=-1,weights='distance'), XGBClassifier(scale_pos_weight = scale_pos, verbosity=0, silent=True) ]\n#intial cost modified base models to evaluate\n\nmodel_names = ['RandomForestClassifier','LogisticRegression','KNeighborsClassifier', 'XGBClassifier' ]\n\nfor x in range(len(models)) :\n    model = models[x] \n    model.fit(x_arr,y_train)\n    y_pred = model.predict(x_arr_test)\n    print('> %s : f2_score: %.3f , Accuracy score: %.3f , Roc_Auc_Score: %.3f' \n        %(model_names[x],f2_measure(y_test,y_pred),accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred)))\n","c09fd364":"under_sampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\nx_arr,y_train=under_sampler.fit_resample(x_arr,y_train)\n\nprint(\"No. of samples after Under-Sampling:\",y_train.shape)\nsns.countplot(x = y_train)\n","58306134":"over_sampler = SMOTE(random_state=42, n_jobs=-1, sampling_strategy=1)\nx_arr,y_train = over_sampler.fit_resample(x_arr,y_train)\nprint(\"No. of samples after Over-Sampling:\",y_train.shape)\nsns.countplot(x = y_train)\n","0cfaf1ac":"model = RandomForestClassifier(n_jobs=-1, random_state=42,class_weight='balanced')\nmodel.fit(x_arr,y_train)\nprint(\"Fitting done\")\ny_pred = model.predict(x_arr_test)\nprint('> %s : f2_score: %.3f , Accuracy score: %.3f , Roc_Auc_Score: %.3f ' %('Resampled_result',f2_measure(y_test,y_pred),accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred)))\n","3a21d439":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","13ee98ef":"rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, cv = 3, verbose=True, random_state=42, n_jobs = -1)\nrf_random.fit(x_arr,y_train)\nbest_random = rf_random.best_estimator_\nbest_random.fit(x_arr,y_train)\nprint(\"Fitting done\")\nprint(rf_random.best_params_)\ny_pred = best_random.predict(x_arr_test)\nprint('> %s , f2_score: %.3f , Accuracy score: %.3f , Roc_Auc_Score: %.3f ' %('Random_search_result',f2_measure(y_test,y_pred),accuracy_score(y_test,y_pred),roc_auc_score(y_test,y_pred)))\n","b1ba70c2":"def load_data(file_path):\n    data=pd.read_csv(file_path,index_col='Id')\n    for train_index, test_index in sss.split(df,df[\"Risk_Flag\"]):\n        sss_train = df.iloc[train_index]\n        sss_test = df.iloc[test_index]\n    X_train= sss_train.drop(['Risk_Flag','Profession','STATE','CITY'], axis=1)\n    X_val= sss_test.drop(['Risk_Flag','Profession','STATE','CITY'], axis=1)\n    Y_train = sss_train.Risk_Flag\n    Y_val = sss_test.Risk_Flag\n    return X_train,X_val,Y_train,Y_val\n\nX_train,X_val,Y_train,Y_val  = load_data('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\nY_train,Y_val = np.array(Y_train), np.array(Y_val)\n\nfinal_model = RandomForestClassifier(n_estimators= 400, min_samples_split= 10, min_samples_leaf= 1, max_features='sqrt',\n                                     class_weight='balanced',n_jobs=-1,random_state=42,max_depth= 60, bootstrap=False)\nconfig =[('pre',preprocessor),('u',under_sampler),('o',over_sampler),('m',final_model)]\npipeline = Pipeline(steps=config)\npipeline.fit(X_train,Y_train)\nY_pred = pipeline.predict(X_val)\n\nprint('> %s : f2_score: %.3f , Accuracy score: %.3f , Roc_Auc_Score: %.3f ' \n      %('Final_evaluation_score',f2_measure(Y_val,Y_pred),accuracy_score(Y_val,Y_pred),roc_auc_score(Y_val,Y_pred)))\n","32c84a36":"classes = [\"0\", \"1\"]\nvisualizer = ClassificationReport(pipeline, classes=classes, support=True)\nvisualizer.fit(X_train, Y_train)        # Fit the visualizer and the model\nvisualizer.score(X_val, Y_val)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and show the figure\n\n","bf203903":"#Visualizing confusion matrix using seaborn\nsns.heatmap(confusion_matrix(Y_val,Y_pred),annot=True,cbar=False,cmap= 'YlGnBu')\nplt.xlabel(\"Predicted label\") # predictions go on the x-axis\nplt.ylabel(\"True label\") # true labels go on the y-axis \nplt.title(\"Confusion  Matrix\")","7523ac45":"plot_roc_curve(pipeline,X_val,Y_val)","b9bfed63":"test_data=pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv', index_col='ID')\ntest_data = test_data.drop(['Profession','STATE','CITY'], axis=1)\ny_final = pipeline.predict(test_data)\nmy_submission = pd.DataFrame({'id': test_data.index, 'risk_flag': y_final})\nmy_submission.to_csv('submission.csv', index=False)","2c38e6d9":"> **Evaluation after resampling**","e7706e43":"**Check for null elements**","ace5079b":"# **Configuring Final Pipeline for analysis**","97d36820":"There's some collinearity between Current Job years and Experience, one of them can be removed from the data.","91449f74":"# **Resampling Data for target class balancing**","ef790bbc":"**Basic stats of data**","f08cb3ec":"# **Exploratory Data Analysis**","d32b2e6d":"# **Evaluating different baseline models**","b5780bca":"**Numerical columns:** Scaled using MinMaxScaler\n\n\n**Categorical columns:**\n(i) Profession, State and City Columns were dropped because of their high cardinality\n(i) Remaining columns were encoded using One hot encoding","0ffdf67d":"**Submission of Test Results**","9a0de6aa":"All numerical features are uniformly distributed and there's a heavy imbalance in the target class.","74dc8d7b":"# **Preprocessing Data**","3f7ac73c":"To Undersample majority class (i.e class 0) : **RandomUnderSampler**\n\nTo Oversample majority class (i.e class 0) : **SMOTE (Synthetic Minority Over-sampling technique)**","a6eee431":"# **Hyperparameter tuning using 3 fold RandomSearchCV**","9c14f9a1":"Models are trained with cost-sensitive methods to account for class imbalance.\n\nCross validation proved to be computationally expensive for this size of dataset so a shuffled stratified split was done.","db6991e3":"The number of defaulters is higher among those who have rented homes as can be expected in real life scenario.","785332e0":"As seen from the results, Baseline RandomForestClassifier model does a better job than other models with higher f2_score and roc_auc_score of 0.713 and 0.841 respectively.","3a7935c7":"Random_search_result , f2_score: 0.724 , Accuracy score: 0.873 , Roc_Auc_Score: 0.851  \n\nThere is hardly any improvement in the results after random search, one of the reasons could be the loss of data because of resampling \nbut if the data samples were higher in number than it would be computationally very expensive to find optimal hyperparameters using RandomSearchCV.","cb7a43d7":"> **Seperating Numerical and Categorical columns**","5f196b71":"**Visualizing preprocessed data in DataFrame**"}}