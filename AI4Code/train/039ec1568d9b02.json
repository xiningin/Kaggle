{"cell_type":{"01a8564f":"code","20ff6ade":"code","6e5749c9":"code","f0dab928":"code","910c8f6d":"code","307e93bb":"code","71f33084":"code","ad088d9c":"code","3e861421":"code","36f9f7be":"code","c21b16ff":"code","debf051a":"code","c2ddf05d":"code","821db299":"code","5cda4cd8":"code","c30dce93":"code","fecaf164":"code","30000f94":"code","cf544654":"code","817a5aa8":"code","065f1a59":"code","f8de116c":"code","73a6e487":"code","46cabc89":"code","3bfff6ec":"code","33f80bbc":"code","1df81471":"markdown"},"source":{"01a8564f":"import numpy as np \nimport pandas as pd ","20ff6ade":"import numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, merge\nfrom keras.layers import LSTM, GRU, Bidirectional, Input, Concatenate, concatenate\nfrom keras import optimizers\nfrom keras import layers\nfrom keras import Model\nfrom keras.datasets import imdb\nfrom keras.callbacks import History, ReduceLROnPlateau","6e5749c9":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","f0dab928":"history = History()\nrlrop = ReduceLROnPlateau(patience=5)","910c8f6d":"def _f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP \/ (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP \/ (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","307e93bb":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndf_train.head()","71f33084":"def clean(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', \" \", text)\n    text = re.sub(r'@\\w+',' ',text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub(r'<.*?>',' ', text)\n    return text","ad088d9c":"df_train.shape, df_test.shape","3e861421":"location_train = df_train['location']\nlocation_test = df_test['location']\nlocation_total = pd.concat([location_train, location_test], ignore_index=True)\nlocation_enc = pd.get_dummies(location_total)\nlocation_train = location_enc.iloc[:len(df_train)]\nlocation_test = location_enc.iloc[len(df_train):]\nlocation_train.shape, location_test.shape, df_train.shape, df_test.shape","36f9f7be":"kw_total = pd.concat([df_train['keyword'], df_test['keyword']])\nkw_enc = pd.get_dummies(kw_total)\nkw_train = kw_enc.iloc[:len(df_train)]\nkw_test = kw_enc.iloc[len(df_train):]","c21b16ff":"X_train, X_valid, y_train, y_valid = train_test_split(df_train['text'], df_train['target'], random_state=100)\nloc_train, loc_valid = train_test_split(location_train, random_state=100)\nkw_train, kw_valid = train_test_split(kw_train, random_state=100)\nX_train.shape, X_valid.shape, loc_train.shape, loc_valid.shape, kw_train.shape, kw_valid.shape","debf051a":"max_l = 0\nfor i in X_train:\n    if len(i) > max_l: max_l = len(i)\n        \nmax_l","c2ddf05d":"X_train = X_train.apply(clean).values\nX_valid = X_valid.apply(clean).values\nX_test = df_test['text']\nX_test = X_test.apply(clean).values","821db299":"num_words = 13000\nmax_len = 140\ntokenizer = Tokenizer(num_words=num_words)\n\ntokenizer.fit_on_texts(X_train)\n\ntrain_seq = tokenizer.texts_to_sequences(X_train)\nvalid_seq = tokenizer.texts_to_sequences(X_valid)\ntest_seq = tokenizer.texts_to_sequences(X_test)\n\ntrain_seq_pad = sequence.pad_sequences(train_seq, maxlen=max_len)\nvalid_seq_pad = sequence.pad_sequences(valid_seq, maxlen=max_len)\ntest_seq_pad = sequence.pad_sequences(test_seq, maxlen=max_len)","5cda4cd8":"text_input = Input(shape=(max_len,)) \nkw_input = Input(shape=(kw_train.shape[1],))\nmeta_input = Input(shape=(loc_train.shape[1],))\n\nemb = Embedding(output_dim=256, input_dim=num_words, input_length=max_len)(text_input) \nemb = LSTM(64, dropout=.2, return_sequences=True)(emb)\n# emb = GRU(64, dropout=.4, return_sequences=True)(emb)\ntext_out = GRU(32, dropout=.4)(emb)\n\nkw_out = Dense(128, activation='sigmoid')(kw_input)\nconcat = concatenate([text_out, kw_out])\nconcat = Dense(128, activation='sigmoid')(concat)\nconcat = layers.BatchNormalization()(concat)\nclassifier = Dense(32, activation='tanh')(concat) \noutput = Dense(1, activation='sigmoid')(classifier) \nconcat_model = Model(inputs=[text_input, kw_input], outputs=[output])","c30dce93":"concat_model.compile(loss = 'binary_crossentropy',\n                     optimizer = optimizers.Adam(0.0001),\n                     metrics = [_f1])","fecaf164":"concat_model.fit([train_seq_pad, kw_train], y_train, \n                 batch_size=64, verbose=1,\n                 epochs=20, callbacks=history,\n                 validation_data=([valid_seq_pad, kw_valid], y_valid))","30000f94":"fig, ax = plt.subplots(1, 2, figsize=(12,5))\nax[0].plot(history.history['loss'])\nax[0].plot(history.history['val_loss'])\nax[1].plot(history.history['auc'])\nax[1].plot(history.history['val_auc'])\nax[1].set_ylim((.5, 1.1))\nax[0].set_title('Loss'), ax[1].set_title('AUC')","cf544654":"def save_submission(model, input_, fname):\n    y_pred = model.predict(input_)\n    df_test['target'] = np.where(y_pred > .5, 1, 0)\n    submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n    sub = submission.drop('target', axis=1).merge(df_test[['id','target']], on='id')\n    sub.to_csv(f'{fname}.csv', index=False)\n#     submission.head()","817a5aa8":"class Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, features, hidden):\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        \n        score = self.W1(features) + self.W2(hidden_with_time_axis)\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n        \n        context_vector = attention_weights + features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","065f1a59":"seq_inp = Input(shape=(max_len,), dtype=\"int32\")\nemb_seq = Embedding(input_dim=num_words, output_dim=64)(seq_inp)","f8de116c":"lstm = Bidirectional(LSTM(128, return_sequences=True), name=\"bi_lstm_0\")(embedded_sequences)\n\nlstm, forw_h, forw_c, back_h, back_c = Bidirectional(LSTM(64, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)","73a6e487":"state_h = Concatenate()([forw_h, back_h])\nstate_c = Concatenate()([forw_c, back_c])\ncontext_vector, attention_weights = Attention(6)(lstm, state_h)\ndense1 = Dense(64, activation=\"relu\")(context_vector)\ndropout = layers.Dropout(0.4)(dense1)\noutput = Dense(1, activation=\"sigmoid\")(dropout)\n  \nat_model = Model(inputs=sequence_input, outputs=output)","46cabc89":"at_model.compile(loss = 'binary_crossentropy',\n                     optimizer = optimizers.RMSprop(rho=.8, momentum=.05),\n                     metrics = [_f1])","3bfff6ec":"at_model.fit(train_seq_pad, y_train, epochs=50, \n             validation_data=(valid_seq_pad, y_valid), \n             verbose=2, callbacks=history)","33f80bbc":"fig, ax = plt.subplots(1, 2, figsize=(12,5))\nax[0].plot(history.history['loss'])\nax[0].plot(history.history['val_loss'])\nax[1].plot(history.history['_f1'])\nax[1].plot(history.history['val__f1'])\nax[1].set_ylim((.5, 1.1))\nax[0].set_title('Loss'), ax[1].set_title('AUC')","1df81471":"## LSTM with attention layer"}}