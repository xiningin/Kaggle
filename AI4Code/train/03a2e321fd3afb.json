{"cell_type":{"91cb15b2":"code","86bf9472":"code","a3ba4279":"code","b3f2cbff":"code","abaf2bc9":"code","8f946ea7":"code","9885a1d3":"code","f4ecf68e":"code","bd30e55f":"code","633f4143":"code","4c5e381d":"code","027cdae1":"code","0a600faf":"code","8c4510af":"code","6fa364e7":"code","fb670cad":"code","7f6b2c0e":"code","32ae16ca":"code","9e123c99":"code","02644112":"code","207e2e79":"code","0647f025":"code","f0d11d3b":"code","3d4b3a2f":"code","5cb35a7f":"code","6b1366a0":"code","a23291df":"code","8b566ea2":"code","23c94275":"code","87732a73":"code","0103a8ca":"code","ddc57e64":"code","4e357916":"code","6a673cd1":"code","7caa44ca":"code","3c856b3b":"code","9e66046e":"code","46d90e4d":"code","37696563":"code","ab00cddd":"code","81c4c613":"code","c97a0052":"code","f92955a6":"code","401a1c53":"code","50fe9bd3":"code","913c9ee6":"code","da5a4f86":"code","02bd085d":"code","9843e1b2":"code","7aceca71":"code","f1dcf0a3":"code","84805222":"code","813b28f9":"code","202b07c0":"code","17a72c64":"code","35835973":"code","b7bbd45b":"code","bc2583ec":"code","e75f68c1":"code","4adc3c1d":"code","78e5d0af":"code","f8fda2c5":"code","af95d9ae":"code","164635a5":"code","f92d2e60":"code","59415400":"code","17e4afb4":"code","fd875306":"code","df988a13":"code","326dde9d":"code","2f01a2c1":"code","912acee7":"code","90f15787":"code","8c7d9d52":"code","35f04eb1":"code","fec68059":"code","6de19bfc":"code","fc71e0ad":"code","1c17b159":"code","3a5becca":"code","0e7980be":"code","1ed6b750":"markdown","7dbfb4f7":"markdown","ffee93a1":"markdown","656b934e":"markdown","d8986cd9":"markdown","fbcd4182":"markdown","016f73f4":"markdown","f05c150a":"markdown","66ab6d6d":"markdown","d9640fbf":"markdown","25b0fd88":"markdown","f7cfde5d":"markdown","754f928e":"markdown","43012c6e":"markdown","9e431f85":"markdown","dec0d4d0":"markdown","82ab6646":"markdown","834a9afc":"markdown","7685e0ff":"markdown","a8762e35":"markdown","6e8fa665":"markdown","4d468f0f":"markdown","24ed579b":"markdown","5c5a3a2b":"markdown","19f9bdcf":"markdown","ced95420":"markdown","057c02ac":"markdown","b4bfaad1":"markdown","a01463da":"markdown","bb63ba7e":"markdown","6f99dfca":"markdown","7cb6ac28":"markdown","1527118e":"markdown","75e30017":"markdown","e9058e65":"markdown","e0ccb7b8":"markdown","b5ec2a39":"markdown","d9364734":"markdown","ca62d524":"markdown","604d8365":"markdown","2b850ce8":"markdown","f020d748":"markdown","1329fd78":"markdown","590578dc":"markdown","5e189343":"markdown","d76f5650":"markdown","2b9c6a08":"markdown","4435d211":"markdown","95c9f265":"markdown","3b88d8f4":"markdown","14822862":"markdown","f104ac2b":"markdown"},"source":{"91cb15b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86bf9472":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nfrom textblob import TextBlob\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.sparse.linalg import svds\n\n# # importing pre-trained universal sentence encoder\n# embed_use = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")\n\n# dataframe display settings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting style\nplt.style.use('ggplot')\nsns.set_style('darkgrid')","a3ba4279":"# checking for GPU\ntf.test.is_gpu_available()","b3f2cbff":"# defining function to tokenize the text\ndef tokenize_text(text, keep_punctuation=\"\"):\n    '''\n    This function takes text string as input and:\n        1. Extracts the text by excluding the HTML tags\n        2. Converts the casing of all characters to lower casing\n        3. Removes puncuation, special characters, digits, etc, if any values are passed to keep_punctuation param, ignores them\n        4. If any specific punctuations are retained, treat them as seperate word\n        5. Split the text string into (list of) words\n        6. Remove stopwords\n    '''\n    # compiling the string match for HTML tag\n    tag = re.compile(\"<.*?>\")\n    # excluding the HTML tags and retaining only the text\n    text = re.sub(tag, \" \", text)\n    \n    # converting the casing to lower\n    text_lower = text.lower()\n    \n    # cleaning text\n    text_lower_cleaned = re.sub(\"[^a-z\" + keep_punctuation + \"]\", \" \", text_lower)\n    \n    # converting the retained punctuations as seperate words\n    if keep_punctuation != \"\":\n        # adding a single space infront of every punctuation to treat it as a seperate word\n        for l in keep_punctuation:\n            text_lower_cleaned = re.sub(l, \" \"+l, text_lower_cleaned)\n    \n    # splitting text string into words\n    text_lower_cleaned_words = text_lower_cleaned.split()\n    \n    # stopwords, converting into set for faster comparision\n    stops = set(stopwords.words(\"english\"))\n    \n    # removing stopwords\n    tokens = [word for word in text_lower_cleaned_words if word not in stops]\n    \n    # returning the list of tokens\n    return tokens","abaf2bc9":"# reading data from csv and creating a dataframe\ndf = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\n\n# dataframe dimensions\nprint(f\"This dataframe has {df.shape[0]} rows and {df.shape[1]} columns.\")","8f946ea7":"# columns and data types\ndf.info()","9885a1d3":"# sample rows\ndf.sample(5)","f4ecf68e":"# missing values summary\npd.DataFrame(zip(df.columns,\n                 df.isna().any(),\n                 df.isna().sum()\\\n                     \/ df.shape[0]\\\n                     * 100),\n            columns=['Column', 'Has Missing Values?', '% Missing Values'])\\\n    .sort_values('% Missing Values', ascending=False)","bd30e55f":"# visual summary of % of missing values in each column\n(df.isna()\\\n         .sum()\\\n         \/ df.shape[0]\\\n         * 100)\\\n    .sort_values()\\\n    .plot(kind='barh', figsize=(12,8))\nplt.xticks(np.arange(0,110,10), fontsize=15, fontweight='bold')\nplt.xlabel(\"% of missing values\", fontsize=15, fontweight='bold')\nplt.yticks(fontsize=15, fontweight='bold')\nplt.title(\"Summary of Missing Values\", fontsize=20, fontweight='bold')\nplt.show()","633f4143":"# replacing NULLs with empty string\ndf.fillna(\"\", inplace=True)","4c5e381d":"# # dataframe for sentiment analysis\n# df_text = pd.DataFrame(df['Summary'] + \" \" + df['Text'], columns=['ReviewText'])\n\n# # creating the label and appending to the dataframe\n# df_text['Label'] = df['Score'].apply(lambda x: \"Positive\" if x >= 4 else \"Negative\")","027cdae1":"# # compiling the string match for HTML tag\n# tag = re.compile(\"<.*?>\")\n\n# # removing the HTML tags and retaining only the text\n# df_text['CleanedText'] = df_text['ReviewText'].apply(lambda x: re.sub(tag, \" \", x))","0a600faf":"# # class distribution\n# plt.subplots(1,2, figsize=(15,6))\n\n# # countplot to visualize the no. of observations under each class\n# plt.subplot(1,2,1)\n# ax = sns.countplot(df_text['Label'])\n# #plt.xticks([0,1], labels=['Negative', 'Positive'])\n# plt.title('No. of obervations in each class', fontsize=15)\n# for i in ax.patches:\n#     # get_x pulls left or right; get_height pushes up or down\n#     ax.text(i.get_x()+0.3, i.get_height(), str(round(i.get_height(), 2)), fontsize=15, color='black')\n\n# # pie chart to visualize the percentage distribution of each class\n# plt.subplot(1,2,2)\n# plt.pie(df_text['Label'].value_counts(), labels=['Positive', 'Negative'], autopct='%.2f', explode=[0, 0.05])\n# plt.title('Percentage distribution of each class', fontsize=15)\n\n# plt.show()","8c4510af":"# # comparing the frequently used words in each class\n# plt.subplots(1,2, figsize=(18,8))\n\n# # word cloud - positive sentiment class\n# plt.subplot(1,2,1)\n# corpus = \" \".join(df_text.loc[(df_text['Label'] == \"Positive\"), \"CleanedText\"].to_numpy().tolist())\n# wordcloud = WordCloud(width=800, height=720, background_color=\"black\").generate(corpus)\n# plt.imshow(wordcloud, interpolation='bilinear')\n# plt.axis('off')\n# plt.title(\"Frequently used words - Positive Reviews\", fontsize=18, fontweight='bold')\n\n# # word cloud - negative sentiment class\n# plt.subplot(1,2,2)\n# corpus = \" \".join(df_text.loc[(df_text['Label'] == \"Negative\"), \"CleanedText\"].to_numpy().tolist())\n# wordcloud = WordCloud(width=800, height=720, background_color=\"black\").generate(corpus)\n# plt.imshow(wordcloud, interpolation='bilinear')\n# plt.axis('off')\n# plt.title(\"Frequently used words - Negative Reviews\", fontsize=18, fontweight='bold')\n\n# plt.show()","6fa364e7":"# # splitting into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(df_text['CleanedText'],\n#                                                     df_text['Label'],\n#                                                     test_size=0.2,\n#                                                     stratify=df_text['Label'],\n#                                                     random_state=42)","fb670cad":"# # positive reviews\n# positive_reviews = X_train[y_train == \"Positive\"].sample(n=sum(y_train == \"Negative\"), replace=False, random_state=42)\n\n# # negative reviews\n# negative_reviews = X_train[y_train == \"Negative\"]\n\n# # creating a down-sampled train set\n# df_pos = pd.DataFrame(positive_reviews, columns=['CleanedText'])\n# df_pos['Label'] = \"Positive\"\n# df_neg = pd.DataFrame(negative_reviews, columns=['CleanedText'])\n# df_neg['Label'] = \"Negative\"\n# df_train = df_pos.append(df_neg).reset_index(drop=True)\n# df_train = df_train.sample(frac=1, random_state=33)\n\n# # X_train and y_train\n# X_train = df_train['CleanedText']\n# y_train = df_train['Label']","7f6b2c0e":"# # encoding target variable\n# y_train = pd.get_dummies(y_train)\n# y_test = pd.get_dummies(y_test)","32ae16ca":"# # creating embeddings using universal sequential encoder\n# X_train = np.array([tf.reshape(embed_use([text]), [-1]).numpy() for text in tqdm(X_train)])","9e123c99":"# # creating embeddings using universal sequential encoder\n# X_test = np.array([tf.reshape(embed_use([text]), [-1]).numpy() for text in tqdm(X_test)])","02644112":"# # constructing a feed-forward neural network\n# model_predict_sentiment = keras.Sequential()\n# model_predict_sentiment.add(keras.layers.Dense(units=256, input_shape=(512, ), activation='relu'))\n# model_predict_sentiment.add(keras.layers.Dropout(rate=0.2))\n# model_predict_sentiment.add(keras.layers.Dense(units=128, activation='relu'))\n# model_predict_sentiment.add(keras.layers.Dropout(rate=0.2))\n# model_predict_sentiment.add(keras.layers.Dense(2, activation='softmax'))\n# model_predict_sentiment.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.001), metrics=['accuracy'])\n# model_predict_sentiment.summary()","207e2e79":"# # fitting the model with train data\n# train_history = model_predict_sentiment.fit(X_train, y_train,\n#                                               epochs=15,\n#                                               batch_size=16,\n#                                               validation_split=0.1,\n#                                               verbose=1,\n#                                               shuffle=True)","0647f025":"# training loss and accuracy","f0d11d3b":"# evaluating on test data","3d4b3a2f":"# # creating embeddings for review text\n# input_embeddings = np.array([tf.reshape(embed_use([text]), [-1]).numpy() for text in tqdm(df_text['CleanedText'])])","5cb35a7f":"# predicting the sentiment using the model","6b1366a0":"# HelpfulnessRatio\ndf['HelpfulnessRatio'] = df['HelpfulnessNumerator'] \/ df['HelpfulnessDenominator']\n\n# replacing the NaN values with 0\ndf['HelpfulnessRatio'].fillna(0, inplace=True)","a23291df":"# SummaryLength\ndf['SummaryLength'] = df['Summary'].str.len()","8b566ea2":"# SummaryTokens\ndf['SummaryTokens'] = df['Summary'].apply(lambda x: tokenize_text(x, keep_punctuation=\"!\"))","23c94275":"# TextLength\ndf['TextLength'] = df['Text'].str.len()","87732a73":"# TextTokens\ndf['TextTokens'] = df['Text'].apply(lambda x: tokenize_text(x, keep_punctuation=\"!\"))","0103a8ca":"# ReviewText\ndf['ReviewText'] = df['Summary'] + \" \" + df['Text']","ddc57e64":"# ReviewLength\ndf['ReviewLength'] = df['ReviewText'].str.len()","4e357916":"# ReviewTokens\ndf['ReviewTokens'] = df['SummaryTokens'] + df['TextTokens']","6a673cd1":"# NoOfTokens\ndf['NoOfTokens'] = df['ReviewTokens'].apply(lambda x: len(x))","7caa44ca":"# NoOfPunctuations\ndf['NoOfPunctuations'] = df['ReviewTokens'].apply(lambda x: x.count(\"!\"))","3c856b3b":"# dataframe for sentiment analysis\ndf_text = pd.DataFrame(df['Summary'] + \" \" + df['Text'], columns=['ReviewText'])\n\n# compiling the string match for HTML tag\ntag = re.compile(\"<.*?>\")\n\n# removing the HTML tags and retaining only the text\ndf_text['CleanedText'] = df_text['ReviewText'].apply(lambda x: re.sub(tag, \" \", x))\n\n# extracting polarity from the review text\ndf_text['Polarity'] = df_text['CleanedText'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# normalizing the polarity values to get sentiment score\ndf['SentimentScore'] = df_text['Polarity'].apply(lambda x: (x + 1) \/ 2)","9e66046e":"# keeping on the required columns for analysis\n# dropping Id and Time\ndf_eda = df.drop(columns=['Id','Time'])\n\n# new dataframe dimensions\nprint(f\"This dataframe has {df_eda.shape[0]} rows and {df_eda.shape[1]} columns.\")","46d90e4d":"# rearranging the columns\ndf_eda = df_eda[['ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','HelpfulnessRatio',\n                 'Score','Summary','SummaryLength','SummaryTokens',\n                 'Text','TextLength','TextTokens',\n                 'ReviewText','ReviewLength','ReviewTokens',\n                 'NoOfTokens','NoOfPunctuations']]\n\n# sample rows\ndf_eda.sample(5)","37696563":"# no. of unique products and users\ndf[['ProductId','UserId']].describe().T","ab00cddd":"# summary of review scores\ndf[['HelpfulnessNumerator','HelpfulnessDenominator','HelpfulnessRatio','Score']].describe().T","81c4c613":"# plotting the values\nplt.subplots(1,3, figsize=(22,5))\n\n# HelpfulnessNumerator\nplt.subplot(1,3,1)\nsns.distplot(df['HelpfulnessNumerator'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Value\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Helpfulness Numerator\", fontsize=15, fontweight='bold')\n\n# HelpfulnessDenominator\nplt.subplot(1,3,2)\nsns.distplot(df['HelpfulnessDenominator'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Value\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Helpfulness Denominator\", fontsize=15, fontweight='bold')\n\n# Score\nplt.subplot(1,3,3)\nsns.distplot(df['HelpfulnessRatio'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Value\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - HelpfulnessRatio\", fontsize=15, fontweight='bold')\n\nplt.show()","c97a0052":"# Score (ratings)\nplt.figure(figsize=(12,6))\nsns.distplot(df['Score'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Value\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Score (Ratings)\", fontsize=15, fontweight='bold')\nplt.show()","f92955a6":"# summary of summary text and review text length\ndf_eda[['SummaryLength','TextLength','ReviewLength','NoOfTokens','NoOfPunctuations']].describe().T","401a1c53":"# distribution of summary text and review text length values\nplt.subplots(2,2, figsize=(18,12))\n\n# summary text\nplt.subplot(2,2,1)\nsns.boxplot(df_eda['SummaryLength'], whis=3)\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Text Length\", fontsize=12, fontweight='bold')\nplt.title(\"Box plot - Summary Text Length\", fontsize=15, fontweight='bold')\n\nplt.subplot(2,2,2)\nsns.distplot(df_eda['SummaryLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Text Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Summary Text Length\", fontsize=15, fontweight='bold')\n\n# review text\nplt.subplot(2,2,3)\nsns.boxplot(df_eda['TextLength'], whis=3)\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Text Length\", fontsize=12, fontweight='bold')\nplt.title(\"Box plot - Review Text Length\", fontsize=15, fontweight='bold')\n\nplt.subplot(2,2,4)\nsns.distplot(df_eda['TextLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Text Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Text Length\", fontsize=15, fontweight='bold')\n\nplt.show()","50fe9bd3":"# frequently used words in review summary\nlist_tokens = df_eda['SummaryTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=1280, height=720, background_color=\"black\").generate(corpus)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Summary\", fontsize=20, fontweight='bold')\nplt.show()","913c9ee6":"# frequently used words in review text\nlist_tokens = df_eda['TextTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=1280, height=720, background_color=\"black\").generate(corpus)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\nplt.show()","da5a4f86":"# correlation plot of numerical values\nplt.figure(figsize=(12,8))\ncorr_var = df_eda[['HelpfulnessNumerator','HelpfulnessDenominator','HelpfulnessRatio',\n                   'Score',\n                   'SummaryLength','TextLength','NoOfTokens','NoOfPunctuations']].corr()\nsns.heatmap(corr_var, annot=True, square=True, fmt='.2f', cmap='RdYlGn', vmin=-1, mask=np.triu(corr_var, 1))\nplt.xticks(fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.title(\"Heatmap - Correlation among numerical variables\", fontsize=20, fontweight='bold')\nplt.show()","02bd085d":"plt.subplots(1,2, figsize=(15,6))\n\n# helpfulness vs review summary length\nplt.subplot(1,2,1)\ndf_eda.groupby('SummaryLength')['HelpfulnessNumerator'].mean().plot()\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Summary Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Avg. Helpfulness Count\", fontsize=12, fontweight='bold')\nplt.title(\"Review Summary Length vs Helpfulness\", fontsize=18, fontweight='bold')\n\n# helpfulness vs review text length\nplt.subplot(1,2,2)\ndf_eda.loc[df_eda['TextLength'] < 1000].groupby('TextLength')['HelpfulnessNumerator'].mean().plot()\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Text Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Avg. Helpfulness Count\", fontsize=12, fontweight='bold')\nplt.title(\"Review Text Length vs Helpfulness\", fontsize=18, fontweight='bold')\n\nplt.show()","9843e1b2":"plt.subplots(1,2, figsize=(17,6))\n\n# score vs no. of tokens\nplt.subplot(1,2,1)\ndf_eda.groupby('Score')['NoOfTokens'].mean().plot(marker='o')\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Score (rating)\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Avg. No. of Tokens\", fontsize=12, fontweight='bold')\nplt.title(\"No. of Tokens in Review vs Score (rating)\", fontsize=18, fontweight='bold')\n\n# helpfulness vs review text length\nplt.subplot(1,2,2)\ndf_eda.groupby('Score')['NoOfPunctuations'].mean().plot(marker='o')\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Score (rating)\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Avg. No. of Punctuations\", fontsize=12, fontweight='bold')\nplt.title(\"No. of Punctuations in Review vs Score (rating)\", fontsize=18, fontweight='bold')\n\nplt.show()","7aceca71":"# summarizing the metrics product wise\ndf_product_summary = df_eda.groupby('ProductId')\\\n                            .agg({'UserId' : 'count',\n                                  'HelpfulnessRatio' : 'mean',\n                                  'Score' : 'mean',\n                                  'SummaryLength' : 'mean',\n                                  'SummaryTokens' : 'sum',\n                                  'TextLength' : 'mean',\n                                  'TextTokens' : 'sum',\n                                  'ReviewLength' : 'mean',\n                                  'ReviewTokens' : 'sum',\n                                  'NoOfTokens' : 'mean',\n                                  'NoOfPunctuations' : 'mean'})","f1dcf0a3":"# top 100 products\ntop_100_products = df_product_summary[df_product_summary['UserId'] > 50]\\\n                        .sort_values('Score', ascending=False)\\\n                        .head(100)","84805222":"# wordcloud, review length, no. of tokens, no. of punctuations, no. of reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = top_100_products['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of no. of user reviews per product\nplt.subplot(3,2,3)\nsns.distplot(top_100_products['UserId'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of User Reviews\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of User Reviews\", fontsize=15, fontweight='bold')\n\n# distribution of average ratings (scores) for each product\nplt.subplot(3,2,4)\nsns.distplot(top_100_products['Score'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. Product Rating\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. Product Rating\", fontsize=15, fontweight='bold')\n\n# distribution of average review text length for each product\nplt.subplot(3,2,5)\nsns.distplot(top_100_products['ReviewLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. Review Text Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. Review Text Length\", fontsize=15, fontweight='bold')\n\n# distribution of average number of punctuations used in review text for each product\nplt.subplot(3,2,6)\nsns.distplot(top_100_products['NoOfPunctuations'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. No. of Punctuations\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. No. of Punctuations\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","813b28f9":"# bottom 100 products\nbottom_100_products = df_product_summary[df_product_summary['UserId'] > 15]\\\n                            .sort_values('Score')\\\n                            .head(100)","202b07c0":"# wordcloud, review length, no. of tokens, no. of punctuations, no. of reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = bottom_100_products['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of no. of user reviews per product\nplt.subplot(3,2,3)\nsns.distplot(bottom_100_products['UserId'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of User Reviews\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of User Reviews\", fontsize=15, fontweight='bold')\n\n# distribution of average ratings (scores) for each product\nplt.subplot(3,2,4)\nsns.distplot(bottom_100_products['Score'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. Product Rating\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. Product Rating\", fontsize=15, fontweight='bold')\n\n# distribution of average review text length for each product\nplt.subplot(3,2,5)\nsns.distplot(bottom_100_products['ReviewLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. Review Text Length\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. Review Text Length\", fontsize=15, fontweight='bold')\n\n# distribution of average number of punctuations used in review text for each product\nplt.subplot(3,2,6)\nsns.distplot(bottom_100_products['NoOfPunctuations'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Avg. No. of Punctuations\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Avg. No. of Punctuations\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","17a72c64":"df_eda.head(1)","35835973":"# subsetting positive reviews\npositive_reviews = df_eda[(df_eda['Score'] >= 4) & (df_eda['HelpfulnessDenominator'] > 25)]","b7bbd45b":"# profile of positive reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = positive_reviews['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of length of summary text\nplt.subplot(3,2,3)\nsns.distplot(positive_reviews['SummaryLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Summary Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Summary Length\", fontsize=15, fontweight='bold')\n\n# distribution of length of review text\nplt.subplot(3,2,4)\nsns.distplot(positive_reviews['TextLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Review Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Length\", fontsize=15, fontweight='bold')\n\n# distribution of no. of tokens in the combined review text\nplt.subplot(3,2,5)\nsns.distplot(positive_reviews['NoOfTokens'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of Tokens\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of Tokens\", fontsize=15, fontweight='bold')\n\n# distribution of helpfulness ratio values\nplt.subplot(3,2,6)\nsns.distplot(positive_reviews['HelpfulnessRatio'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Helpfulness Ratio\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Helpfulness\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","bc2583ec":"# subsetting negative reviews\nnegative_reviews = df_eda[(df_eda['Score'] < 3) & (df_eda['HelpfulnessDenominator'] > 25)]","e75f68c1":"# profile of negative reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = negative_reviews['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of length of summary text\nplt.subplot(3,2,3)\nsns.distplot(negative_reviews['SummaryLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Summary Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Summary Length\", fontsize=15, fontweight='bold')\n\n# distribution of length of review text\nplt.subplot(3,2,4)\nsns.distplot(negative_reviews['TextLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Review Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Length\", fontsize=15, fontweight='bold')\n\n# distribution of no. of tokens in the combined review text\nplt.subplot(3,2,5)\nsns.distplot(negative_reviews['NoOfTokens'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of Tokens\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of Tokens\", fontsize=15, fontweight='bold')\n\n# distribution of helpfulness ratio values\nplt.subplot(3,2,6)\nsns.distplot(negative_reviews['HelpfulnessRatio'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Helpfulness Ratio\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Helpfulness\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","4adc3c1d":"# comparing the stats\npositive_reviews_stats = [positive_reviews['SummaryLength'].mean(),\n                          positive_reviews['TextLength'].mean(),\n                          positive_reviews['NoOfTokens'].mean(),\n                          positive_reviews['NoOfPunctuations'].mean(),\n                          positive_reviews['HelpfulnessRatio'].mean()]\n\nnegative_reviews_stats = [negative_reviews['SummaryLength'].mean(),\n                          negative_reviews['TextLength'].mean(),\n                          negative_reviews['NoOfTokens'].mean(),\n                          negative_reviews['NoOfPunctuations'].mean(),\n                          negative_reviews['HelpfulnessRatio'].mean()]\n\npd.DataFrame(zip(positive_reviews_stats, negative_reviews_stats),\n             index=['Avg. Summary Length', 'Avg. Review Length', 'Avg. No. of Tokens', 'Avg. No. of Punctuations', 'Avg. Review Helpfulness'],\n             columns=['Positive Reviews', 'Negative Reviews']).round(2)","78e5d0af":"# subsetting top 1000 most helpful reviews\nmost_helpful_reviews = df_eda[(df_eda['HelpfulnessDenominator'] > 50)]\\\n                        .sort_values('HelpfulnessRatio', ascending=False)\\\n                        .head(1000)","f8fda2c5":"# profile of most helpful reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = most_helpful_reviews['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of length of summary text\nplt.subplot(3,2,3)\nsns.distplot(most_helpful_reviews['SummaryLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Summary Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Summary Length\", fontsize=15, fontweight='bold')\n\n# distribution of length of review text\nplt.subplot(3,2,4)\nsns.distplot(most_helpful_reviews['TextLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Review Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Length\", fontsize=15, fontweight='bold')\n\n# distribution of no. of tokens in the combined review text\nplt.subplot(3,2,5)\nsns.distplot(most_helpful_reviews['NoOfTokens'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of Tokens\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of Tokens\", fontsize=15, fontweight='bold')\n\n# distribution of product rating\nplt.subplot(3,2,6)\nsns.distplot(most_helpful_reviews['Score'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Product Rating\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Product Rating\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","af95d9ae":"# subsetting top 1000 least helpful reviews\nleast_helpful_reviews = df_eda[(df_eda['HelpfulnessDenominator'] > 50)]\\\n                        .sort_values('HelpfulnessRatio', ascending=True)\\\n                        .head(1000)","164635a5":"# profile of least helpful reviews\nplt.subplots(3,2, figsize=(15,15))\n\n# word cloud\nplt.subplot(3,2,1)\nplt.subplot2grid((3,2), (0, 0), colspan=2)\nlist_tokens = least_helpful_reviews['ReviewTokens'].apply(lambda x: \",\".join(word for word in x)).values\ncorpus = \",\".join(string for string in list_tokens)\nwordcloud = WordCloud(width=2500, height=720, background_color=\"black\").generate(corpus)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Frequently used words (Word Cloud) - Review Text\", fontsize=20, fontweight='bold')\n\n# distribution of length of summary text\nplt.subplot(3,2,3)\nsns.distplot(least_helpful_reviews['SummaryLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Summary Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Summary Length\", fontsize=15, fontweight='bold')\n\n# distribution of length of review text\nplt.subplot(3,2,4)\nsns.distplot(least_helpful_reviews['TextLength'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Length of Review Text\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Review Length\", fontsize=15, fontweight='bold')\n\n# distribution of no. of tokens in the combined review text\nplt.subplot(3,2,5)\nsns.distplot(least_helpful_reviews['NoOfTokens'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"No. of Tokens\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - No. of Tokens\", fontsize=15, fontweight='bold')\n\n# distribution of product rating\nplt.subplot(3,2,6)\nsns.distplot(least_helpful_reviews['Score'], kde_kws={\"color\": \"k\", \"lw\": 2, \"label\": \"KDE\"})\nplt.xticks(fontsize=12, fontweight='bold')\nplt.xlabel(\"Product Rating\", fontsize=12, fontweight='bold')\nplt.yticks(fontsize=12, fontweight='bold')\nplt.ylabel(\"Density\", fontsize=12, fontweight='bold')\nplt.title(\"Density plot - Product Rating\", fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","f92d2e60":"# comparing the stats\nmost_helpful_reviews_stats = [most_helpful_reviews['SummaryLength'].mean(),\n                              most_helpful_reviews['TextLength'].mean(),\n                              most_helpful_reviews['NoOfTokens'].mean(),\n                              most_helpful_reviews['Score'].mean(),\n                              most_helpful_reviews['NoOfPunctuations'].mean()]\n\nleast_helpful_reviews_stats = [least_helpful_reviews['SummaryLength'].mean(),\n                               least_helpful_reviews['TextLength'].mean(),\n                               least_helpful_reviews['NoOfTokens'].mean(),\n                               least_helpful_reviews['Score'].mean(),\n                               least_helpful_reviews['NoOfPunctuations'].mean()]\n,\npd.DataFrame(zip(most_helpful_reviews_stats, least_helpful_reviews_stats),\n             index=['Avg. Summary Length', 'Avg. Review Length', 'Avg. No. of Tokens', 'Avg. Product Rating', 'Avg. No. of Punctuations'],\n             columns=['Most Helpful Reviews', 'Least Helpful Reviews']).round(2)","59415400":"# summary stats for each user\ndf_user_summary = df_eda.groupby('UserId')\\\n                        .agg({'ProductId' : 'count',\n                              'HelpfulnessDenominator' : 'mean',\n                              'HelpfulnessRatio' : 'mean',\n                              'Score' : 'mean',\n                              'SummaryLength' : 'mean',\n                              'TextLength' : 'mean',\n                              'NoOfTokens' : 'mean',\n                              'NoOfPunctuations' : 'mean'})","17e4afb4":"# middle 50% of users\nmask = [False]*64014 + [True]*64014*2 + [False]*64014 + [False]*3\n\n# comparision of metrics\npd.DataFrame(zip(df_user_summary[(df_user_summary['ProductId'] > 3) &\n                                 (df_user_summary['HelpfulnessDenominator'] > 10) &\n                                 (df_user_summary['HelpfulnessRatio'] > 0.8)].describe().T['mean'].values,\n                 df_user_summary.sort_values('HelpfulnessRatio')[mask].describe().T['mean'].values),\n             index=df_user_summary.describe().T.index,\n             columns=['Top Rated User Review', 'Avg. User Review']).iloc[[2,3,4,5,6,7]]","fd875306":"# model score\ndf['ModelScore'] = df['SentimentScore'] * (df['Score'] \/ 5)","df988a13":"# function to filter rows less than min frequencies\ndef filter_min_frequencies(df_filter, feature, min_freq):\n    feature_frequencies = df_filter[feature].value_counts()\n    df_filter = df_filter[df_filter[feature].isin(feature_frequencies[feature_frequencies > min_freq].index)]\n    return df_filter","326dde9d":"# subsetting data for modelling\ndf_model = df[['ProductId','UserId','ModelScore']]","2f01a2c1":"# setting threshold for min frequencies\nmin_freq = 10\n\n# product frequencies\nproduct_frequencies = df_model['ProductId'].value_counts()\n# user frequencies\nuser_frequencies = df_model['UserId'].value_counts()\n\n# appending product and user frequencies to dataframe\ndf_model['product_frequency'] = df_model['ProductId'].apply(lambda x: product_frequencies[x])\ndf_model['user_frequency'] = df_model['UserId'].apply(lambda x: user_frequencies[x])\n\n# filtering products and users less than min frequency threshold\nwhile ((df_model['ProductId'].value_counts(ascending=True)[0] < min_freq) or (df_model['UserId'].value_counts(ascending=True)[0] < min_freq)):\n    df_model = filter_min_frequencies(df_model, \"ProductId\", min_freq)\n    df_model = filter_min_frequencies(df_model, \"UserId\", min_freq)\n\n# encoding ProductId and UserId\ndf_model['UID'] = pd.factorize(df_model['UserId'])[0]\ndf_model['PID'] = pd.factorize(df_model['ProductId'])[0]\n\n# dictionary - PID to ProductId\npid_to_productid = df_model[['ProductId','PID']].drop_duplicates().set_index('PID')\n\n# splitting into train and test sets\nX = df_model[['UID','PID']]\ny = df_model['ModelScore']\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=911)\ntrain = np.array(X_train.join(y_train))\ntest = np.array(X_test.join(y_test))\n\n# model matrix\nmodel_matrix = np.array(X.join(y))\n\n# model matrix dimensions\nmodel_products = df_model['ProductId'].nunique()\nmodel_users = df_model['UserId'].nunique()\n\n# constructing user-product matrix\nuser_product_matrix = np.zeros([model_users, model_products])\n\nfor row in model_matrix:\n    uid, pid, score = row\n    uid, pid = int(uid), int(pid)\n    if user_product_matrix[uid][pid] < score:\n        user_product_matrix[uid][pid] = score","912acee7":"# function to calculate RMSE\ndef evaluate_rmse(matrix_pred):\n    # in sample (train) RMSE\n    sigma_train, sigma_pred = [], []\n    for row in train:\n        u, p, s = row\n        u, p = int(u), int(p)\n        sigma_train.append(s)\n        sigma_pred.append(matrix_pred[u][p])\n    rmse_in_sample = np.sqrt(mean_squared_error(sigma_train, sigma_pred))\n    \n    # out sample (test) RMSE\n    sigma_test, sigma_pred = [], []\n    for row in test:\n        u, p, s = row\n        u, p = int(u), int(p)\n        sigma_test.append(s)\n        sigma_pred.append(matrix_pred[u][p])\n    rmse_out_sample = np.sqrt(mean_squared_error(sigma_test, sigma_pred))\n    \n    return rmse_in_sample, rmse_out_sample","90f15787":"# function to plot confusion matrix\ndef plot_confusion_matrix(matrix_pred):\n    y_test, y_pred = [], []\n    \n    for row in test:\n        u, p, s = row\n        u, p = int(u), int(p)\n        if s >= 0.75:\n            y_test.append(1)\n        else:\n            y_test.append(0)\n        if matrix_pred[u][p] >= 0.75:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n    \n    # confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    # normalizing\n    cm = cm \/ cm.sum(axis=1).reshape(-1,1)\n    # plotting\n    cm = pd.DataFrame(cm,\n                      columns=['Not Recommended','Recommended'],\n                      index=['Not Recommended','Recommended'])\n\n    cm = cm.round(2)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, cmap=\"RdYlGn\", annot=True, cbar=False, square=True)\n    plt.xlabel(\"Predicted values\", fontsize=15, fontweight='bold')\n    plt.xticks(fontsize=12, fontweight='bold')\n    plt.ylabel(\"Actual values\", fontsize=15, fontweight='bold')\n    plt.yticks(fontsize=12, fontweight='bold')\n    plt.title(\"Confusion Matrix\", fontsize=20, fontweight='bold')\n    plt.show()","8c7d9d52":"# function to recommend products\ndef recommend_products(results, uid, top_n=10):\n    # checking if the user has bought at least 10 items in the past\n    if uid in df_model['UID'].values:\n        top_n_products = np.argpartition(results[uid], -top_n)[-top_n:]\n        user_id = df_model.loc[(df_model['UID'] == uid), 'UserId'].head(1).values[0]\n        products = [pid_to_productid.iloc[pid, 0] for pid in top_n_products]\n        print(f\"For the user {user_id} top {top_n} recommended products are:\\n{products}\")\n        \n    else:\n        print(\"This user has not bought anything, please use other methods.\")","35f04eb1":"# average user scores for each product\navg_product_scores = np.mean(user_product_matrix,axis=0).reshape(1,-1)\n\n# subtracting avg. product scores from the the user product matrix\ndemeaned_matrix = user_product_matrix - avg_product_scores","fec68059":"# choosing the no. of factors\n# SVD\nU, S, VT = svds(demeaned_matrix, k=1101)\n\n# sigma values\ns = np.diag(S)\ns = np.diag(s)[::-1]\n\n# visualizing the information carried\nplt.subplots(1,2, figsize=(18,6))\nplt.subplot(1,2,1)\nplt.semilogy(s)\nplt.title(\"Singular Values\")\nplt.ylabel(\"Information Stored\")\n\nplt.subplot(1,2,2)\nplt.plot(np.cumsum(s) \/ np.sum(s))\nplt.title(\"Singular Values - Cumulative Sum\")\nplt.ylabel(\"Infomation Stored\")\nplt.show()","6de19bfc":"# SVD with 200 factors\nU, S, VT = svds(demeaned_matrix, k=200)\n\n# fetching sigma value\nS = np.diag(S)\n\n# constructing the predicted matrix using sigma values from SVD\nmatrix_pred = np.dot(np.dot(U, S), VT) + avg_product_scores\n\n# scaling the values to [0-1] range\nmms = MinMaxScaler(feature_range=(0,1))\nmatrix_pred = mms.fit_transform(matrix_pred).round(2)","fc71e0ad":"# understanding the matrices\nprint(\"Input Matrix:\\t\\t\\t\", demeaned_matrix.shape)\nprint(\"Left Singular Vectors (U):\\t\", U.shape)\nprint(\"Eigen Values (Sigma):\\t\\t\", S.shape)\nprint(\"Right Singular Vectors (VT):\\t\", VT.shape)","1c17b159":"# RMSE\ntrain_rmse, test_rmse = evaluate_rmse(matrix_pred)\nprint(f\"RMSE train: {train_rmse}\\nRMSE test: {test_rmse}\")","3a5becca":"# plotting confusion matrix\nplot_confusion_matrix(matrix_pred)","0e7980be":"# testing for a user\nrecommend_products(matrix_pred, 1512)","1ed6b750":"# Feature engineering","7dbfb4f7":"### Bivariate and Multivariate analysis","ffee93a1":"### Characteristics (profile) of negative reviews","656b934e":"**Observations:**\n\n* `Summary` and `ProfileName` have missing values. Also, these columns have less than 1% missing values.\n* We can ignore the missing values, it wouldn't affect our analysis and predictions. But for computation purposes, let's replace the NULLs with empty stings.","d8986cd9":"# Missing values","fbcd4182":"### ReviewText\n\nConcatnating `Summary` and `Text` columns to get the complete `ReviewText`.","016f73f4":"**Observations:**\n\n* From the above two plots (wordclouds), we can see that majority of the user sentiment is positive.\n* From the analysis, we can see that users care more about taste, delicious, and gluten free aspects of the product than price and shipping.\n* Most of the products are related to dog food, cat food, coffee, tea, peanut butter, potato chips, cereal, chocolate, soup, etc.","f05c150a":"### Characteristics (profile) of positive reviews","66ab6d6d":"### Characteristics (profile) of least helpful reviews","d9640fbf":"### Characteristics (profile) of most useful reviews","25b0fd88":"### NoOfTokens\n\nCalculate the number of tokens (words excluding stopswords) in each review.","f7cfde5d":"### Preparing data","754f928e":"### SummaryTokens\n\nLet's tokenize the `Summary` text.","43012c6e":"### Model to predict sentiment\n\nLet's build a Feed-forward Neural Network model and train it on the sentence embeddings to predict the sentiment.","9e431f85":"### HelpfulnessRatio\n\nLet's calculate the `HelpfulnessRatio` value by diving the `HelpfulnessNumerator` with `HelpfulnessDenominator`.\n\n$$HelpfulnessRatio = \\frac{HelpfulnessNumerator}{HelpfulnessDenominator}$$","dec0d4d0":"### Extracting sentiment for all the reviews","82ab6646":"# Sentiment analysis\n\nLet's build a model to extract the sentiment from the user review text. We will be using *Universal Sentence Encoder*, a pre-trained model, to transform the review text into fixed-length 512 dimension sentence embeddings.","834a9afc":"**Observations:**\n\n* So, we have 74,258 unique products, sweet!\n* And, there are 2,56,059 unique users.","7685e0ff":"### TextTokens\n\nLet's tokenize the `Text` text.","a8762e35":"### Score (rating) vs No. of tokens and punctuations","6e8fa665":"### Average review user Vs Most helpful review users","4d468f0f":"**Observations:**","24ed579b":"**Observations:**\n\n* There is no or negligeble correlation among metrics.\n* There are no interesting observations.","5c5a3a2b":"**Observations:**\n\n* Review `Summary` text length:\n    * Distribution of  review `Summary` text length values is slightly right skewed.\n    * Most of the values, 50% of values around the median (IQR), are having text length between 13 and 30 characters.\n<br><br>    \n* Review `Text` length:\n    * Distribution of review `Text` length values is heavily right skewed.\n    * Most of the values, 50% of values around the median (IQR), are having text length between 179 and 527 characters.","19f9bdcf":"### Scoring metric","ced95420":"### Positive reviews Vs. Negative reviews","057c02ac":"# Recommendation engines","b4bfaad1":"### Subsetting the data","a01463da":"**Observations:**\n","bb63ba7e":"# Exploratory data analysis","6f99dfca":"# Reading data","7cb6ac28":"# User defined functions","1527118e":"**Observations:**\n\n* We can see that as review `Summary` or `Text` length increases, `HelpfulnessNumerator` (no. of users who found the review helpful) also increases (slightly).\n* Meaning, users find the review helpful as the details on product increase.","75e30017":"### TextLength\n\nLet's calculate the length (no. of characters) in the actual review `Text`.","e9058e65":"**Observations:**\n\n* Users who leave a neutral `Score` (product rating), 3 (neutral score), are in general leaving a more detailed review.\n* Users who leave an extreme `Score` (product rating), either 1 (very bad) or 5 (very good), are in general leaving less detailed review comparatively.\n* Also, users who leave a neutral `Score` (product rating), 3 (neutral score), are in general using few punctuations (\"!\") in their review.\n* Users who leave an extreme `Score` (product rating), either 1 (very bad) or 5 (very good), are in general using more punctuations (\"!\") in their review.\n* Using punctuation (\"!\") can be ascribed to strong emotion.","e0ccb7b8":"### NoOfPunctuations\n\nLet's count the number of punctuations (!) used in the review text.","b5ec2a39":"**Observations:**\n\n* We can see that *love* is a common word in positive reviews.\n* Most of the user reviews are about product taste and flavor (makes sense since these are food products) and some others are about price, smell, texture.\n* We can see that words like *bad*, *disappointment* appear in the negative reviews.\n* There is not much difference in terms of tokens, since both the positive and negative reviews talk about similar aspects of a product.","d9364734":"### ReviewTokens\n\nLet's tokenize the `Review` text.","ca62d524":"**Observations:**\n\n* Majority of `Helpfulness` scores are zero. And they are heavily right skewed.\n* Majority of `HelpfulnessRatio` values are zero, since we have seen that majority of the `Helpfulness` score were zero.\n* Excluding these values, among non-zero `Helpfulness` scores, majority of the `HelpfulnessRatio` values are 1. Meaning, most of the reviews are helpful for the users, i.e. quality reviews.","604d8365":"**Observations:**\n\n* We can see that the initial 350 sigma values are capturing most of the information. Let's choose k as 350 factors.","2b850ce8":"### Modelling","f020d748":"### SentimentScore\n\nLet's extract sentiment score from the user review text.","1329fd78":"### Basic summary statistics (or) Univariate analysis","590578dc":"### Characteristics of top rated products","5e189343":"**Observations:**\n\n* Majority of `Score` (product rating) values are 5 (good to know products on Amazon are good).\n* We can also notice that distribution of product ratings is similar of inverted bell-curve, meaning, there are more number of 1 rating scores than 2 rating scores.","d76f5650":"# Libraries","2b9c6a08":"### ReviewLength\n\nLet's calculate the length (no. of characters) in the complete `Review` text.","4435d211":"### SummaryLength\n\nLet's calculate the length (no. of characters) in the `Summary` text.","95c9f265":"### SVD","3b88d8f4":"### Most helpful reviews Vs Least helpful reviews","14822862":"### Helpfulness vs Review summary and text","f104ac2b":"### Preparing data\n\nLet's preprocess the review text and prepare the data for training a FNN model to predict the sentiment.\n\n1. Creating a binary label from the score values.\n2. Split into train and test sets.\n3. Treat class imbalance by down-sampling the majority class.\n4. Transform the review text into embedded vectors using *Universal Sentence Encoder*."}}