{"cell_type":{"c25d9b14":"code","44e97303":"code","498404d1":"code","af4ca0e9":"code","3e1c2431":"code","6f427f90":"code","a8804332":"code","1f6edb1c":"code","e573ab7e":"code","c08a011e":"code","aae5d973":"code","0db4fae2":"code","5152d706":"code","315f6e68":"code","f153b19f":"code","9192467b":"code","95c3563e":"code","fb3de448":"code","5b99aa8b":"code","1258414a":"code","5f01a3d7":"code","57c870c2":"code","addc19ba":"code","037e9842":"code","a5f76774":"code","b67855d6":"code","37f269f9":"code","bcb836f0":"code","478e82fc":"code","429cecad":"code","8c913f33":"code","ac01e03b":"code","0014dff9":"code","b8073cff":"code","6cf89aea":"code","84f34c49":"code","64e7eb9c":"code","5d7b29f1":"code","b2dc6897":"code","cc3ae8a4":"code","ead70b25":"code","7f193467":"code","eeca714d":"markdown","71aadb4e":"markdown","41d5bec5":"markdown","a288693a":"markdown","ca00ecd8":"markdown","f6df1419":"markdown","8ef440da":"markdown","6e10ca89":"markdown","82f8c63c":"markdown","c3ca73cf":"markdown","e49fd413":"markdown","fe71b464":"markdown","a3262357":"markdown","bb2da23b":"markdown","b22a8ea4":"markdown","036a935d":"markdown","b893c98d":"markdown","1139b77b":"markdown","50cca5cd":"markdown","6cf824bd":"markdown","f2023e1b":"markdown","f5e58056":"markdown","69ccaced":"markdown","8137936b":"markdown","d6f32480":"markdown","628c6791":"markdown","96866dd4":"markdown","5c131fd6":"markdown"},"source":{"c25d9b14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","44e97303":"!echo \"Quick viewing of given raw data\"\n!echo \"## train.csv ## \" ; head ..\/input\/train.csv; echo \"...\" ; tail ..\/input\/train.csv ; wc -l ..\/input\/train.csv ; echo\n!echo \"## test.csv ##\"   ; head ..\/input\/test.csv ; echo \"...\" ; tail ..\/input\/test.csv ; wc -l ..\/input\/test.csv  ; echo\n!echo \"## sample_submission.csv ##\";head ..\/input\/sample_submission.csv;echo \"...\" ; tail ..\/input\/sample_submission.csv ;wc -l ..\/input\/sample_submission.csv","498404d1":"# import related libraries\n\n# dates\nfrom pandas import datetime\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns # advanced vizs\n%matplotlib inline\n\n# statistics\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# time series analysis\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# prophet by Facebook\nfrom fbprophet import Prophet","af4ca0e9":"# Import data\ntrain_data_csv = \"..\/input\/train.csv\"\ntest_data_csv = \"..\/input\/test.csv\"\nsample_submission_csv = \"..\/input\/sample_submission.csv\"\n\ntrain = pd.read_csv(train_data_csv, parse_dates = True,\n                    low_memory = False, index_col = 'date')\ntest = pd.read_csv(test_data_csv, parse_dates = True,\n                   low_memory = False, index_col = 'date')\nsubmission = pd.read_csv(sample_submission_csv)","3e1c2431":"print(\"Check imported data\")\nprint()\nprint(\"In total:\")\nprint(\"train.shape {} \".format(train.shape))\nprint(\"test.shape {} \".format(test.shape))\nprint(\"submission.shape {} \".format(submission.shape))\nprint()\nprint(\"train.columns {} \".format(train.columns))\nprint(\"test.colmuns {} \".format(test.columns))\nprint(\"submission.colmuns {} \".format(submission.columns))\nprint()\nprint(\"train.index {} \".format(train.index))\nprint(\"test.index {} \".format(test.index))\nprint(\"submission.index {} \".format(submission.index))\n\n","6f427f90":"pd.set_option(\"display.max_rows\", 20)","a8804332":"train.head(500)","1f6edb1c":"test.head(500)","e573ab7e":"submission.head(500)","c08a011e":"# rows which contains NA column\ntrain[train.isna().any(axis=1)]","aae5d973":"# rows which contains NA column\ntest[test.isna().any(axis=1)]","0db4fae2":"# rows which contains NA column\nsubmission[submission.isna().any(axis=1)]","5152d706":"# describe - note, store and item are factor\ntrain.describe()","315f6e68":"# describe - note, store and item are factor\ntest.describe()","f153b19f":"# describe - note, this submission data is sample\nsubmission.describe()","9192467b":"pd.set_option(\"display.precision\", 1)","95c3563e":"# Pivot\npd.pivot_table(train, index='item', columns='store', aggfunc='count')","fb3de448":"pd.pivot_table(train, index='item', columns='store', aggfunc='min')","5b99aa8b":"pd.pivot_table(train, index='item', columns='store', aggfunc='max')","1258414a":"pd.pivot_table(train, index='item', columns='store', aggfunc='median')","5f01a3d7":"sns.set(style = \"ticks\")# to format into seaborn \nc = '#386B7F' # basic color for plots\nplt.figure(figsize = (12, 13))\n\nplt.subplot(311)\ncdf = ECDF(train['store'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('store'); plt.ylabel('ECDF');\n\nplt.subplot(312)\ncdf = ECDF(train['item'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('item'); plt.ylabel('ECDF');\n\nplt.subplot(313)\ncdf = ECDF(train['sales'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\", color = c);\nplt.xlabel('sales'); plt.ylabel('ECDF');\n\n","57c870c2":"train['store'].hist()","addc19ba":"train['item'].hist()","037e9842":"train['sales'].hist()","a5f76774":"# check small sales values\ntrain[train['sales'] < 2]","b67855d6":"# data extraction\ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekOfYear'] = train.index.weekofyear\ntrain['DayOfYear'] = train.index.dayofweek\ntrain['is_month_start'] = train.index.is_month_start\ntrain['is_month_end'] = train.index.is_month_end\ntrain['is_month_end'] = train.index.is_month_end\ntrain['days_from_epoch'] = (train.index - pd.Timestamp(\"1970-01-01\")).days","37f269f9":"# sales trends\nsns.catplot(data = train, x = 'Year', y = \"sales\", kind='point')","bcb836f0":"# sales trends\n# sns.factorplot(data = train, x = 'Month', y = \"sales\")\nsns.catplot(data = train, x = 'Month', y = \"sales\", kind='point')","478e82fc":"# sales trends\n# sns.factorplot(data = train, x = 'Day', y = \"sales\")\nsns.catplot(data = train, x = 'Day', y = \"sales\", kind='point')","429cecad":"# sales trends, for each store\nsns.catplot(data = train, x = 'Year', y = \"sales\", col='store', kind='point')","8c913f33":"# sales trends, for each item\nsns.catplot(data = train, x = 'Year', y = \"sales\", row='item', kind='point')","ac01e03b":"# sales trends, for each store x item\nsns.catplot(data = train, x = 'Year', y = \"sales\",\n            row = 'item', col='store', kind='point')","0014dff9":"# timeseries plot\ndef tsplot(tsdf, title):\n    from scipy import signal\n    t = tsdf.index\n    y = tsdf['sales']\n    yd = signal.detrend(y)\n    plt.figure(figsize=(4,3))\n    plt.plot(t, y, label=\"Original Data\")\n    plt.plot(t, y-yd, \"--r\", label=\"Trend\")\n    plt.axis(\"tight\")\n    plt.legend(loc=0)\n    plt.title(title)\n    plt.show()\n    return","b8073cff":"for s in train['store'].unique():\n    tmpdf = train[train['store']==s]\n    # for i in tmpdf['item'].unique():\n    for i in range(1,3):\n        tmp2df = tmpdf[tmpdf['item']==i]\n        tsplot(tmp2df, \"store ID {} and item ID {}\".format(s,i))","6cf89aea":"train.columns","84f34c49":"train_X = train.copy(deep=True)\ndel train_X['sales']\ntrain_y = train['sales']","64e7eb9c":"# data extraction\ntest['Year'] = test.index.year\ntest['Month'] = test.index.month\ntest['Day'] = test.index.day\ntest['WeekOfYear'] = test.index.weekofyear\ntest['DayOfYear'] = test.index.dayofweek\ntest['is_month_start'] = test.index.is_month_start\ntest['is_month_end'] = test.index.is_month_end\ntest['is_month_end'] = test.index.is_month_end\ntest['days_from_epoch'] = (test.index - pd.Timestamp(\"1970-01-01\")).days","5d7b29f1":"test_X = test.copy(deep=True)\ndel test_X['id']\ntest_X.columns","b2dc6897":"from sklearn.ensemble import GradientBoostingRegressor\n\nclf = GradientBoostingRegressor(n_estimators=100, learning_rate=0.25,\n        max_depth=1).fit(train_X, train_y)","cc3ae8a4":"pred_y = clf.predict(test_X)","ead70b25":"print (\"Predict \",pred_y)","7f193467":"# Write submission file\nout_df = pd.DataFrame({'id': test['id'].astype(np.int32), 'sales': pred_y})\nout_df.to_csv('submission.csv', index=False)","eeca714d":"### Data preparation","71aadb4e":"Month-end is good timing to sale.","41d5bec5":"## Quick viewing of given raw data before importing","a288693a":"all sales value is 52 as sample.","ca00ecd8":"Their items sold well in summer.","f6df1419":"## ECDF: empirical cumulative distribution function","8ef440da":"## Pivotal analysis","6e10ca89":"# Exploratory Data Analysis (Data understanding)\n\n- Quick viewing of given raw data before importing\n- First glance at given data set\n - Check shape of data, columns, index\n - Viewing raw data\n - Check NaN\n - Check describe\n- Pivotal analysis\n- Check ECDF: empirical cumulative distribution function\n- Check Histgram\n- Check trend\n- Check timeseries plot\n- Conclusion of EDA","82f8c63c":"### Result of factor plot\n- Sales values are increasing.\n- Each items and shop has a individual increase rate.","c3ca73cf":"### Check NaN","e49fd413":"## Check Histgram","fe71b464":"### Check shape of data, columns, index","a3262357":"## Conclusion of EDA\n\n- 10 different stores and 50 different items\n- Training period : 2013-01-01 to 2017-12-31\n- Test period: 2018-01-01 to 2018-03-31\n- No missing data\n- Given data (stores sales data and items sales data) are stacked into one column\n- sales data is increasing year by year\n- Monday is lowest sales day. Sunday is highest sales day.\n- Most store's sales is increasing\n- Most item's sales is increasing\n- Sales of month end is larger than other days\n- Sales in summer is larger than other seasons","bb2da23b":"## First impression of result of EDA\n- There are 10 different stores and 50 different items. Thus we have to predict 500 different value for same day. There is two approaches. One way is generate 500 different model to predict 500 different sales values. Another way is generate only one model to predict 500 different sales values.\n\n## Gradient Boosting Decision Tree(GBDT)\n- This is good baseline model in competition.\n- Fortunately Desision-Tree type model can  handle such kind of data.\n- However decision tree does not compute any regression coefficients like linear regression, so trend modeling is not possible. Thus it is necessary to detrend time series. (Below, detrending is not yet applied)\n","b22a8ea4":"## Check trend","036a935d":"### Check timeseries plot","b893c98d":"### result of quick viewing\n- Data have header\n- train.csv has three columns\n- test.csv data has three columns, but has ID column instead of sales\n- sample_submission.csv has two columns. it's id and sales.\n- Number of rows of test.csv and number of rows of sample_submission.csv are same. \n- Maybe, test.csv is test_X, and sample_submission.sales is test_y.\n- training period : 2013-01-01 to 2017-12-31 (5 years)\n- test period : 2018-01-01 to 2018-03-31 (3 month)","1139b77b":"# Modeling approach (my base-line)","50cca5cd":"train, test and submission data do not contain NA value","6cf824bd":"only one row has 0 sales value. store ID is 6 and item ID is 4.","f2023e1b":"It looks like increasing year by year","f5e58056":"References:\n- https:\/\/www.kaggle.com\/elenapetrova\/time-series-analysis-and-forecasts-with-prophet\n- https:\/\/petolau.github.io\/Regression-trees-for-forecasting-time-series-in-R\/\n","69ccaced":"### Viewing raw data\nIt is important.","8137936b":"# Demand prediction for multi-store and multi-item\n\nThis kernel is for Kaggle's Store Item Demand Forecasting Challenge\n\n## Data Description\nThe objective of this competition is to predict 3 months of item-level sales data at different store locations.\n\nFile descriptions\n\n* train.csv - Training data\n* test.csv - Test data (Note: the Public\/Private split is time based)\n* sample_submission.csv - a sample submission file in the correct format\n\nData fields\n\n* date - Date of the sale data. There are no holiday effects or store closures.\n* store - Store ID\n* item - Item ID\n* sales - Number of items sold at a particular store on a particular date.","d6f32480":"## Creating new feature for farther analysis","628c6791":"Minimum sales value is 0.\nIt is necessary to check distribution of sales values.","96866dd4":"### Check describe","5c131fd6":"# First glance at given data set\nIn this section we go through given data, handle missing values"}}