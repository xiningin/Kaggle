{"cell_type":{"52925672":"code","18a73797":"code","3fcdc8bf":"code","2cff09e7":"code","22e727a2":"code","8f3f1077":"code","4a45ac23":"code","6d8a9c98":"code","f910c6e7":"code","bcd5c16f":"code","87cd4b3e":"code","67233e2e":"code","9cc74d45":"code","f0787c61":"code","3f25e1c9":"code","fcee0f3a":"code","8cf3fc76":"code","b48666dd":"code","9cdde177":"code","9fcfc096":"code","71b6fb0e":"code","38fd70cf":"code","b100cfad":"code","7fc59238":"code","7b90e75c":"code","ad978b10":"code","55b156e1":"code","fdbac00d":"code","6830ac10":"code","3dca3ab6":"code","df92de2b":"code","0582bc73":"markdown","b5dde248":"markdown","cd17f77b":"markdown","09f99133":"markdown","71ff21bb":"markdown","5cd84ba1":"markdown"},"source":{"52925672":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18a73797":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n!pip install --user catboost\nimport catboost\nfrom catboost import *","3fcdc8bf":"os.chdir('\/kaggle\/input\/amazon-employee-access-challenge\/')","2cff09e7":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","22e727a2":"train.head()","8f3f1077":"print(train.isnull().sum())\ntest.isnull().sum()","4a45ac23":"test['ROLE_CODE'].unique()","6d8a9c98":"# !pip install klib\n# import klib","f910c6e7":"df = train.drop(columns=['ACTION'])\ntrain_x = train.drop(columns=['ACTION'])\ntrain_y = train['ACTION'] \ntest_x = test.drop(columns=['id'])\n# test_y = test['ACTION']","bcd5c16f":"train.head()","87cd4b3e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =  train_test_split(train_x, train_y)","67233e2e":"# Logistic Regression\n# Not suitable at all due to categorical variables\n\nfrom sklearn import linear_model as lm\nfrom sklearn.metrics import accuracy_score\nmodel = lm.LogisticRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict_proba(test_x)\n# model.score(X_test, y_test)\n# accuracy_score(predictions, y_test)\n\n# 0.50 accuracy","9cc74d45":"test.drop(columns=['id']).shape","f0787c61":"print(f\"{X_train.shape}, {X_test.shape}, {test.drop(columns=['id']).shape}\")\ntest_x = test.drop(columns=['id'])\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","3f25e1c9":"# Naive Bayes doesn't work here due to an extra category in test_data\n\nfrom sklearn import naive_bayes\nmodel = naive_bayes.CategoricalNB()\n# clf = model.fit(X_train, y_train)\n# print(f'{clf.score(X_test,y_test)}')\n# predictions = clf.predict(test_x)","fcee0f3a":"# Decision Tree\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=30)\nclf = model.fit(X_train, y_train)\nprint(f'{clf.score(X_test,y_test)}')\npredictions = clf.predict(test_x)\n\n# 0.679 Score on test dataset","8cf3fc76":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators = 300)\nclf = model.fit(X_train, y_train)\nprint(f'{clf.score(X_test, y_test)}')\npredictions = clf.predict_proba(test_x)\n\n## Score: 0.67333 on Binary Classification \n## Score: 0.85201 on Probabilitistic outputs","b48666dd":"os.chdir('\/kaggle\/working')\nos.makedirs('\/kaggle\/working\/amazon')\nos.listdir('.')","9cdde177":"from catboost.utils import create_cd\nfeature_names = dict()\nfor column, name in enumerate(train):\n    if column == 0:\n        continue\n    feature_names[column - 1] = name\ndataset_dir = '.\/amazon'   \ncreate_cd(\n    label=0, \n    cat_features=list(range(1, train.columns.shape[0])),\n    feature_names=feature_names,\n    output_path=os.path.join(dataset_dir, 'train.cd')\n)","9fcfc096":"X = train.drop(columns=['ACTION'])\ny = train.ACTION\ncat_features = list(range(0, X.shape[1]))\nprint(cat_features)","71b6fb0e":"pool1 = Pool(data=X, label=y, cat_features=cat_features)\npool2 = Pool(\n    data=os.path.join('\/kaggle\/input\/amazon-employee-access-challenge\/', 'train.csv'), \n    delimiter=',', \n    column_description=os.path.join(dataset_dir, 'train.cd'),\n    has_header=True\n)\npool3 = Pool(data=X, cat_features=cat_features)\n\n# Fastest way to create a Pool is to create it from numpy matrix.\n# This way should be used if you want fast predictions\n# or fastest way to load the data in python.\n\nX_prepared = X.values.astype(str).astype(object)\n# For FeaturesData class categorial features must have type str\n\npool4 = Pool(\n    data=FeaturesData(\n        cat_feature_data=X_prepared,\n        cat_feature_names=list(X)\n    ),\n    label=y.values\n)\n\nprint('Dataset shape')\nprint('dataset 1:' + str(pool1.shape) +\n      '\\ndataset 2:' + str(pool2.shape) + \n      '\\ndataset 3:' + str(pool3.shape) +\n      '\\ndataset 4: ' + str(pool4.shape))\n\nprint('\\n')\nprint('Column names')\nprint('dataset 1:')\nprint(pool1.get_feature_names()) \nprint('\\ndataset 2:')\nprint(pool2.get_feature_names())\nprint('\\ndataset 3:')\nprint(pool3.get_feature_names())\nprint('\\ndataset 4:')\nprint(pool4.get_feature_names())","38fd70cf":"# train, test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)","b100cfad":"from catboost import CatBoostClassifier\nmodel = CatBoostClassifier(\n    iterations=3000,\n    random_seed=1,\n    learning_rate=0.03,\n    loss_function = 'CrossEntropy'\n#     custom_loss=['AUC', 'Accuracy']\n)\nmodel.fit(\n    X_train, y_train,\n    cat_features=cat_features,\n    eval_set=(X_validation, y_validation),\n    verbose=50,\n#     plot=True\n)","7fc59238":"model.get_feature_importance(prettified=True)","7b90e75c":"# Cross Validation\nfrom catboost import cv\n\nparams = {}\nparams['loss_function'] = 'Logloss'\nparams['iterations'] = 93\nparams['custom_loss'] = 'AUC'\nparams['random_seed'] = 63\nparams['learning_rate'] = 0.5\n\ncv_data = cv(\n    params = params,\n    pool = Pool(X, label=y, cat_features=cat_features),\n    fold_count=5,\n    shuffle=True,\n    partition_random_seed=0,\n#     plot=True,\n    stratified=False,\n    verbose=False\n)","ad978b10":"cv_data","55b156e1":"best_value = np.min(cv_data['test-Logloss-mean'])\nbest_iter = np.argmin(cv_data['test-Logloss-mean'])\n\nprint('Best validation Logloss score, not stratified: {:.4f}\u00b1{:.4f} on step {}'.format(\n    best_value,\n    cv_data['test-Logloss-std'][best_iter],\n    best_iter)\n)","fdbac00d":"test = pd.read_csv('\/kaggle\/input\/amazon-employee-access-challenge\/test.csv')\npredictions = model.predict_proba(test_x)","6830ac10":"np.unique(predictions, return_counts= True)","3dca3ab6":"predictions = predictions[:,1]","df92de2b":"os.chdir('\/kaggle\/working\/')\nos.curdir\nsol = pd.DataFrame(predictions)\n\n\nsol = sol.rename(columns={0:'Action'})\nsol.index = range(1, 58922,1)\nsol = sol.rename_axis('Id')\nsol.to_csv('submission.csv')","0582bc73":"# Catboost implementation","b5dde248":"We have our train and test data  \nLet's go ahead with models, DT and naive bayes to be applied first","cd17f77b":"### EDA on features  \nWe don't see any null values present in the training or test data","09f99133":"Catboost is a boosting algorithm. It is better than lightGBM, XGBoost on various tasks or at least on par with them. It's particularly useful here because of it's ability to work with categorical data.  \n\nCatboost tutorial [link](https:\/\/github.com\/catboost\/tutorials\/blob\/master\/classification\/classification_tutorial.ipynb)","71ff21bb":"We finally get a accuracy score of 0.90126","5cd84ba1":"The columns contain categorical values. A naive bayes, Decision Tree solution as starting point should be taken. "}}