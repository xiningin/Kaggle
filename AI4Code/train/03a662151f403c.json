{"cell_type":{"6bd89483":"code","e39e9c64":"code","b8f8e751":"code","131571d1":"code","82dcbbb9":"code","b90a90da":"code","464d69c3":"code","87e9d967":"code","a2fd553a":"code","e7dcd2b2":"code","0f146fae":"code","61cc352a":"code","d101d765":"code","09d614ea":"code","0c398ad1":"code","3c4f0ba6":"code","307ac52f":"code","d203c654":"code","96353b33":"code","6df2ed22":"code","f557323b":"code","89b119e6":"code","cc2506f1":"code","934546c0":"code","c49ebed8":"code","37aed04e":"code","3c85c21d":"code","e2768795":"code","7ddbcf6f":"code","7c0f8513":"code","eef098f8":"code","88af38fc":"code","b4adeb32":"code","b5e6efb2":"code","3eac1ee0":"code","43e40fdd":"code","20b16a60":"code","efeab6b3":"code","b7a8e910":"code","24d31ddd":"code","a7bbfbf4":"code","55c6deee":"code","4985ebb4":"code","64c46910":"code","948c9b28":"code","f9d405fa":"code","518818dd":"code","febb87cf":"code","73011e8b":"code","575defec":"code","1974a8c4":"code","de335750":"code","c9471bef":"code","31183ee1":"code","b05ce4ce":"code","616a4d66":"code","88a8cf6e":"code","ed487615":"code","f0add0ca":"code","28b40ca9":"code","32d6806b":"code","4df0600e":"code","e34adce1":"code","2a7366d0":"code","5dba8b50":"code","8c81e570":"code","aa053d8c":"code","53b1bd37":"code","4785cc93":"code","e1ba0815":"code","5e010646":"code","24d72cf8":"code","7483bb40":"code","b2d9d88e":"code","7f2731c8":"code","2e2c18f4":"code","5b14ffc2":"markdown","dcf45d4e":"markdown","150341d6":"markdown","f3ed2ed5":"markdown","ff0a31b7":"markdown","004818cd":"markdown","acb70221":"markdown","4f65811d":"markdown","db442cfd":"markdown","a908903b":"markdown","5d568ca0":"markdown","02b0f7b0":"markdown","0b13a18d":"markdown","c190eadc":"markdown","e30f67d6":"markdown","eb5cca7c":"markdown","1ebeebd0":"markdown","4471d174":"markdown","6ef493c2":"markdown","0911c068":"markdown","4e3b3017":"markdown","35fd0c42":"markdown","d5f73e07":"markdown","5516ebd6":"markdown","1b331019":"markdown","9f3d9653":"markdown","d5327fe0":"markdown","fd1ab6ba":"markdown","7273e4d7":"markdown","df75f19b":"markdown","10c4e764":"markdown","6b9c7d86":"markdown","0eb80370":"markdown","5c251143":"markdown","a3325645":"markdown","040a7bf8":"markdown","eda5e1b5":"markdown","f3013726":"markdown","ad782ae0":"markdown","da63d23a":"markdown","c0ca71af":"markdown","be97a6e9":"markdown","7ce86110":"markdown","564fa3be":"markdown","3591b150":"markdown","5d1dcdc5":"markdown","e4b43071":"markdown","fd9d53e3":"markdown","de953ecd":"markdown","0afe0bf7":"markdown","933b1c9b":"markdown","d169da81":"markdown","5aa77c3d":"markdown","6fd74f0e":"markdown"},"source":{"6bd89483":"from bs4 import BeautifulSoup\nimport requests\nimport json\nimport pandas as pd\nimport numpy as np\nimport re\nimport os","e39e9c64":"import warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\",category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=UserWarning)\nwarnings.filterwarnings(\"ignore\",category=RuntimeWarning)","b8f8e751":"num_list = [ \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\"]","131571d1":"webpage_links = []\nfor i in range(len(num_list)):\n  link = \"https:\/\/www.thehindu.com\/archive\/web\/2021\/02\/{0}\/\".format(num_list[i])\n  webpage_links.append(link)","82dcbbb9":"webpage_links","b90a90da":"URL_list = []\nfor link in webpage_links:\n  page = requests.get(link)\n  soup = BeautifulSoup(page.content, \"html.parser\")\n  for item in soup.find_all(attrs={'class':'archive-list'}):\n    for link in item.find_all(href=True):\n        href=link.get('href')\n        URL_list.append(href)","464d69c3":"len(URL_list)","87e9d967":"json_files = []\nfor i in range(len(URL_list)):\n  json_files.append('thehindu_feb_02_file_{0}.json'.format(i))","a2fd553a":"for i in range(len(URL_list)):\n  news_data = {\"text\" : []}\n  page = requests.get(URL_list[i])\n  soup = BeautifulSoup(page.content, \"html.parser\")\n\n#SCRAPING THE TITLE OF THE WEBPAGE\n  title = soup.find('h1')\n  if title!= None:\n    news_data[\"text\"].append(title.text)\n\n#SCRAPING THE SUB TITLE OF THE WEBPAGE\n  sub_title = soup.find_all('h2')\n  if sub_title != None:\n    for st in sub_title:\n      news_data[\"text\"].append(st.text)\n\n      \n#SCRAPING CONTENT FROM PARAGRAPH TAGS WITH A SPECIFIC ID\n  regex = re.compile('.*content-body-.*')\n  for item in soup.find_all(\"div\", {'id': regex}):\n    for p in item.find_all('p'):\n      news_data[\"text\"].append(p.text)  \n\n#STORING THE CONTENT FROM EACH WEBPAGE IN A SEPARATE JSON FILE\n  with open(json_files[i], 'w') as jsonfile:\n    json.dump(news_data, jsonfile)\n\n#SAVING THE JSON FILES IN GOOGLE DRIVE\n  !cp {json_files[i]} \/content\/drive\/MyDrive\/JSON_FILES2\n","e7dcd2b2":"raw_text = []\nfor js in json_files:\n   with open(js) as json_file:\n        json_text = json.load(json_file)\n        raw_text.append(json_text['text'])","0f146fae":"#To save the data from Feb 1 to Feb 7\nwith open('raw_text_01.json', 'w') as jsonfile:\n   json.dump(raw_text, jsonfile)\n!cp raw_text_01.json \/content\/drive\/MyDrive\/RAW_TEXT","61cc352a":"#To save the data from Feb 8 to Feb 14\nwith open('raw_text_02.json', 'w') as jsonfile:\n    json.dump(raw_text, jsonfile)\n!cp raw_text_02.json \/content\/drive\/MyDrive\/RAW_TEXT","d101d765":"tab_head = [\"File_Scraped\", \"Corresponding URL\"]\ndf_data = pd.DataFrame(zip(json_files, URL_list), columns = tab_head)\ndf_data[:10]","09d614ea":"df_data.to_csv('URLs_to_file_mapping.csv', sep ='|')\n!cp URLs_to_file_mapping.csv \/content\/drive\/MyDrive\/CSV_FILES","0c398ad1":"new_df = pd.read_csv('URLs_to_file_mapping.csv')\nnew_df.head(10)","3c4f0ba6":"new_df.shape","307ac52f":"raw_text = []\nf1 = open(\"..\/input\/web-scraped-news-articles\/raw_text_01.json\")\ndata1 = json.load(f1)\nfor i in data1:\n  raw_text.append(i) \n\nf2 = open(\"..\/input\/web-scraped-news-articles\/raw_text_02.json\")\ndata2 = json.load(f2)\nfor i in data2:\n  raw_text.append(i)","d203c654":"len(raw_text)","96353b33":"length = []\nfor files in raw_text:\n  c = len(str(files).split())\n  length.append(c)","6df2ed22":"import seaborn as sns\nsns.set(rc={'figure.figsize':(15,5)})\nsns.boxplot(length, width = 0.8, showmeans = True)","f557323b":"def detect_outlier(data):\n    # find q1 and q3 values\n    q1, q3 = np.percentile(sorted(data), [5, 95])\n \n    # compute IRQ\n    iqr = q3 - q1\n \n    # find lower and upper bounds\n    lower_bound = q1 - (1.5 * iqr)\n    upper_bound = q3 + (1.5 * iqr)\n \n    outliers = [x for x in data if x <= lower_bound or x >= upper_bound]\n    print(\"Lower Bound: \", lower_bound)\n    print(\"Upper Bound: \",upper_bound)\n    return outliers\n \n# input data\ndetect_outlier((length))","89b119e6":"#Removing the text having length greater than 1786 words\nmodified_text = []\nfor text in raw_text:\n  c = len(str(text).split())\n  if c <= 1786:\n    modified_text.append(text)\nlen(modified_text)","cc2506f1":"length = [i for i in length if i < 1786]\nsns.set(rc={'figure.figsize':(15,5)})\nsns.boxplot(length, width = 0.8, showmeans=True)","934546c0":"#FOR STOP WORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n#FOR LEMMATIZATION\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n#FOR STEMMING\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\n#FOR BIGRAMS\nfrom nltk.util import bigrams\nnltk.download('punkt')\nnltk.download('wordnet')\n","c49ebed8":"def data_preprocessing(raw_text):\n  filtered_text = []     #TO STORE THE PROCESSED DATA    \n  for t in raw_text:\n    filtered_sentence = \"\"\n    stemmed_list = []\n    lemmatized_list = []\n    \n    sentence = str(t)\n    # Remove new line characters\n    for word in sentence:\n      word = word.strip()\n\n    #Data Cleansing\n    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n\n\n    #Removing numbers\n    sentence = re.sub(r'[0-9]', '', sentence)\n    \n    #Tokenization\n    words = nltk.word_tokenize(sentence)\n\n    #Lowercase\n    for word in words:\n      word.lower()\n    \n    #Stop words removal\n    words = [w for w in words if not w in stop_words]\n    \n    #Stemming\n    for word in words:\n        stemmed_word = stemmer.stem(word)\n        stemmed_list.append(stemmed_word)\n        \n    #Lemmatization\n    for s_word in stemmed_list:\n        lemmatized_word = lemmatizer.lemmatize(s_word)\n        lemmatized_list.append(lemmatized_word)\n\n    \n    lemmatized_list = [i for i in lemmatized_list if len(i) > 3]\n    \n    filtered_text.append(lemmatized_list)\n  return filtered_text","37aed04e":"filtered_text = data_preprocessing(modified_text)","3c85c21d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk import FreqDist","e2768795":"# function to plot most frequent terms\ndef freq_words(x, terms = 30):\n  all_words = ' '.join([text for text in x])\n  all_words = all_words.split()\n\n  fdist = FreqDist(all_words)\n  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n\n  # selecting top 20 most frequent words\n  d = words_df.nlargest(columns=\"count\", n = terms) \n  plt.figure(figsize=(20,5))\n  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n  ax.set(ylabel = 'Count')\n  plt.show()","7ddbcf6f":"words_list = []\nfor line in filtered_text:\n  for word in line:\n    words_list.append(word)","7c0f8513":"freq_words(words_list)","eef098f8":"extra_stopwords = ['crore', 'first', 'said', 'work', 'also', 'case', 'would', 'take', 'time', 'last', 'year', 'three', 'make', 'nthe', 'need', 'even',\n                   'issu', 'well', 'come', 'made', 'come', 'howev', 'work', 'februari', 'includ','like']\nstop_words.extend(extra_stopwords)","88af38fc":"words_list = [w for w in words_list if not w in stop_words]","b4adeb32":"freq_words(words_list)","b5e6efb2":"filtered_text2 = []\nfor sentence in filtered_text:\n  sentence = [w for w in sentence if not w in extra_stopwords]\n  filtered_text2.append(sentence)","3eac1ee0":"len(filtered_text2)","43e40fdd":"words_string = \" \".join(words_list)","20b16a60":"import wordcloud\n\nwordcloud = wordcloud.WordCloud()\nwordcloud.generate(words_string)\nplt.figure( figsize=(15,10) )\nplt.rcParams[\"axes.grid\"] = False\nplt.imshow(wordcloud)","efeab6b3":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","b7a8e910":"# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 25 times or more).\nbigram = Phrases(filtered_text2, min_count=25)\nfor idx in range(len(filtered_text2)):\n    for token in bigram[filtered_text2[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            if token not in filtered_text2[idx]:\n              filtered_text2[idx].append(token)","24d31ddd":"import nltk\nfrom nltk.util import ngrams\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nword_fd = nltk.FreqDist(words_list)\nbigram_fd = nltk.FreqDist(nltk.bigrams(words_list))\n\nbigram_fd.most_common()[:25]","a7bbfbf4":"trigrams=ngrams(words_list,3)\ntrigram_fd = nltk.FreqDist(trigrams)\ntrigram_fd.most_common()[:20]","55c6deee":"filtered_text2","4985ebb4":"#TO SAVE THE FILTERED TEXT IN A JSON FILE\nwith open('filtered_text.json', 'w') as jsonfile:\n   json.dump(filtered_text2, jsonfile)\n!cp filtered_text.json \/content\/drive\/MyDrive\/RAW_TEXT","64c46910":"filtered_text = []\nf1 = open(\"\/content\/drive\/MyDrive\/RAW_TEXT\/filtered_text.json\")\ndata1 = json.load(f1)\nfor i in data1:\n  filtered_text.append(i)","948c9b28":"from sklearn.model_selection import train_test_split\ntraining_set, test_set  = train_test_split(filtered_text, test_size=0.1, random_state = 0)","f9d405fa":"import csv\nimport pandas as pd\ndata = pd.read_csv(\"\/content\/drive\/MyDrive\/CSV_FILES\/URLs_to_file_mapping.csv\", header = None, delimiter = \"|\" )\ndata_d = data[1:]","518818dd":"df = pd.DataFrame(zip(filtered_text))\ndf_data = pd.merge(data_d,df, right_index=True, left_index=True)","febb87cf":"df_data","73011e8b":"from sklearn.model_selection import train_test_split\ntraining_set, test_set  = train_test_split(df_data, test_size=0.1, random_state = 0)","575defec":"test_set.to_csv('test_set.csv')","1974a8c4":"len(filtered_text)","de335750":"len(test_set)","c9471bef":"#IMPORTING THE LIBRARIES\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","31183ee1":"id2word = corpora.Dictionary(training_set)\n\n#Removing very rare words\nid2word.filter_extremes(no_below=20, no_above=0.5)\n\n# Create Corpus\ntexts = training_set\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]","b05ce4ce":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","616a4d66":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=14, \n                                           random_state= 342,\n                                           iterations = 50,\n                                           chunksize=1000,\n                                           passes=120,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\n# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=training_set, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","88a8cf6e":"from sklearn.externals import joblib\n \n# Save the model as a pickle in a file\njoblib.dump(lda_model, '\/content\/drive\/MyDrive\/Models\/final_model.pkl')","ed487615":"from sklearn.externals import joblib\nlda_model = joblib.load(open('\/content\/drive\/MyDrive\/Models\/final_model.pkl', 'rb'))","f0add0ca":"import os\ndef install_java_jdk():\n  !apt-get install -y openjdk-8-jdk-headless -qq > \/dev\/null\n  os.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/java-8-openjdk-amd64\"\n  !java -version\n\ninstall_java_jdk","28b40ca9":"!wget http:\/\/mallet.cs.umass.edu\/dist\/mallet-2.0.8.zip\n!unzip mallet-2.0.8.zip","32d6806b":"from google.colab import drive\ndrive.mount('\/content\/gdrive')\n%cd gdrive","4df0600e":"os.environ['MALLET_HOME'] = '\/content\/mallet-2.0.8'\nmallet_path = '\/content\/mallet-2.0.8\/bin\/mallet'","e34adce1":"def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","2a7366d0":"model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=training_set, start=5, limit=40, step=5)","5dba8b50":"# Show graph\nimport matplotlib.pyplot as plt\nlimit=40; start=5; step=5;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","8c81e570":"# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 3))","aa053d8c":"# Print the Keyword in the topics\nfrom pprint import pprint\npprint(lda_model.print_topics(num_words=15))\ndoc_lda = lda_model[corpus]","53b1bd37":"Topics = [\"0.\tMedical Health\",\n\"1.\tPolitical Issues\",\n\"2.\tSports \u2013 Cricket\",\n'3.\tNews and Politics \u2013 Elections',\n'4.\tNew and Politics \u2013 Law',\n'5.\tNews and Politics \u2013 National News',\n'6.\tBusiness and Finance',\n'7.\tRegion\/State',\n'8.\tTechnology and Computing',\n'9.\tEducation',\n'10.\tRegion\/State',\n'11.\tMovies',\n'12.\tInternational News',\n'13.\tNational News - Projects'\n]","4785cc93":"table_head = ['Topic', 'Keywords']","e1ba0815":"df = pd.DataFrame(zip(Topics, lda_model.print_topics()), columns= table_head)\ndf","5e010646":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=None):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=training_set)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\ndf_dominant_topic.head(10)","24d72cf8":"pip install pyLDAvis","7483bb40":"# Visualize the topics\nimport pyLDAvis.gensim_models\npyLDAvis.enable_notebook()\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\nLDAvis_prepared.display()","b2d9d88e":"for t in range(14):\n    plt.figure()\n    plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(t, 20))))\n    plt.axis('off')\n    plt.title(Topics[t], fontsize = 18)\n    plt.show()","7f2731c8":"Topic_distribution= []\nfor i in range(30):\n  print(lda_model[id2word.doc2bow(test_set[i])][0])\n  Topic_distribution.append(lda_model[id2word.doc2bow(test_set[i])][0])","2e2c18f4":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=test_set, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","5b14ffc2":"VISUALISING USING BOX PLOT","dcf45d4e":"### FREQUENCY DISTRIBUTION OF BIGRAMS:","150341d6":"### Saving the model","f3ed2ed5":"### FREQUENCY DISTRIBUTION OF TRIGRAMS:","ff0a31b7":"### SAVING THE RAW TEXT IN A SEPARATE .JSON FILE","004818cd":"### Loading the saved model","acb70221":"The length of most documents lies between 200-450 words.\nThe mean of length of all the documents is around 350 words.","4f65811d":"### VIEWING THE RAW DATA","db442cfd":"## TESTING THE MODEL ON TEST SET","a908903b":"### EXTRACTING THE USEFUL CONTENT FROM THE URLs:","5d568ca0":"BOX PLOT","02b0f7b0":"## SPLITTING THE DATA INTO TRAINING SET AND TEST SET","0b13a18d":"DETECTING THE OUTLIERS","c190eadc":"### FREQUENCY DISTRIBUTION PLOT","e30f67d6":" ### Coherence Scores obtained during parameter tuning\n\n<table>\n<tr>\n<th>num_topics<\/th><th>random_state<\/th>\n<th>iterations<\/th><th>passes<\/th>\n<th>alpha<\/th><th>chunksize<\/th>\n<th>Coherence Score<\/th><th>Perplexity<\/th>\n<\/tr>\n<tr>\n<td>25<\/td><td>42<\/td><td>50<\/td><td>20<\/td><td>auto<\/td><td>1000<\/td><td>0.566<\/td><td>-7.166<\/td>\n<\/tr>\n<td>20<\/td><td>42<\/td><td>100<\/td><td>150<\/td><td>auto<\/td><td>1000<\/td><td>0.587<\/td><td>-7.206<\/td>\n<\/tr>\n<tr>\n<td>15<\/td><td>42<\/td><td>50<\/td><td>100<\/td><td>auto<\/td><td>1000<\/td><td>0.595<\/td><td>-7.202<\/td>\n<\/tr>\n<tr>\n<td>15<\/td><td>42<\/td><td>50<\/td><td>120<\/td><td>auto<\/td><td>1000<\/td><td>0.595<\/td><td>-7.202<\/td>\n<\/tr>\n<tr>\n<td>15<\/td><td>342<\/td><td>50<\/td><td>120<\/td><td>auto<\/td><td>1000<\/td><td>0.597<\/td><td>-7.222<\/td>\n<\/tr>\n<tr>\n<td>14<\/td><td>563<\/td><td>50<\/td><td>120<\/td><td>auto<\/td><td>1000<\/td><td>0.590<\/td><td>-7.226<\/td>\n<\/tr>\n\n<tr>\n<td>14<\/td><td>342<\/td><td>50<\/td><td>120<\/td><td>auto<\/td><td>1000<\/td><td>0.610<\/td><td>-7.225<\/td>\n<\/tr>\n\n\n\n\n\n","eb5cca7c":"## DATA PREPROCESSING","1ebeebd0":"### WORD CLOUD","4471d174":"NUM TOPICS = 14","6ef493c2":"## LDA MALLET","0911c068":"### COMPUTING COHERENCE VALUE FOR LDA MALLET","4e3b3017":"### IMPORTING THE LIBRARIES","35fd0c42":"# TOPIC MODELING","d5f73e07":"Extending the stop words list:","5516ebd6":"### PERFORMING THE TEXT PREPROCESSING","1b331019":"### CREATING A LIST OF ALL THE URLS FROM THE WEBPAGES OF 14 DAYS:\n### FROM FEB 1, 2021 TO FEB 14, 2021\n","9f3d9653":"## LDA MODEL","d5327fe0":"## Naming the topics with IAB Taxonomy.","fd1ab6ba":"From the distribution plot, it is observed that there are some irrelevant words present in high frequency in the text. Hence, to improve our model, we can remove these words.","7273e4d7":"## VIEWING THE TOPICS IN LDA MODEL","df75f19b":"### Computing Model Perplexity and Coherence Score","10c4e764":"CALCULATING THE LENGTH OF ALL THE FILES","6b9c7d86":"# COMPONENT 1 - WEB SCRAPING MODULE","0eb80370":"### CREATING A LIST OF JSON FILES","5c251143":"### VISUALISE TOPICS","a3325645":"## BUILDING THE LDA TOPIC MODEL","040a7bf8":"Getting the file names for the test set elements to map back to the corresponding URLs.","eda5e1b5":"OUTLIERS REMOVAL","f3013726":"## EXTRACTING NEWS DATA FOR TWO WEEKS (Feb 1, 2021 to Feb 14, 2021)","ad782ae0":"## Word Clouds of Top 20 Keywords in Each Topic","da63d23a":"## OUTLIERS DETECTION AND REMOVAL","c0ca71af":"### LIST OF URLS OF 14 DAYS:\n\nThe urls of the web pages are as follows:\n\nhttps:\/\/www.thehindu.com\/archive\/web\/2021\/02\/01\/\n\nhttps:\/\/www.thehindu.com\/archive\/web\/2021\/02\/02\/ \n\n................\n\nhttps:\/\/www.thehindu.com\/archive\/web\/2021\/02\/14\/\n\nSince the urls follow a similar pattern, we can create the list of all the urls as follows:","be97a6e9":"## CREATING A DATAFRAME TO STORE THE FILE NAMES AND THE CORRESPONDING URLs","7ce86110":"# Component 2 - Topic Model","564fa3be":"### TOPIC DISTRIBUTION FOR TEST SET","3591b150":"This notebook includes the code for web scraping as well. But you can skip that part and start with preprocessing, since the scraped aticles are available as the dataset.","5d1dcdc5":"### STORING THE TEXT FROM EACH FILE IN A LIST","e4b43071":"### TERM FREQUENCY","fd9d53e3":"### Calculating the Coherence Score on Test Set","de953ecd":"Out of all the models, LDA Model with number of topics = 14 has the best performance with Coherence Score: 0.610 . Thus, we proceed furthur with the same model.","0afe0bf7":"The LDA Model is giving a coherence score of 0.5089 on the test set.","933b1c9b":"### STORING THE DATAFRAME IN .CSV FILE","d169da81":"### DISPLAYING THE CONTENT IN .CSV FILE","5aa77c3d":"### Finding the dominant topic in each documnet","6fd74f0e":"### BIGRAMS AND TRIGRAMS FORMATION"}}