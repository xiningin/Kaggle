{"cell_type":{"6dcde925":"code","93a0496f":"code","6bc66622":"code","094f7bbd":"code","d50577b2":"code","ef8ebfc6":"code","768c97b6":"code","9ec58508":"markdown","ff4e6511":"markdown","7b7510cc":"markdown"},"source":{"6dcde925":"# importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings('ignore')","93a0496f":"#%% reading data\n\ndef get_data():\n    full_train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', index_col = 'id', usecols = ['id','text','target'])\n    full_test_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv', index_col = 'id', usecols = ['id','text'])\n    # there are no null values in either the train set or the test set.\n    print('Total null values in the train set - '+str(full_train_data.isnull().sum().sum()))\n    print('Total null values in the test set - '+str(full_test_data.isnull().sum().sum()))\n    return full_train_data, full_test_data","6bc66622":"#%% preprocessing data\n\ndef clean_data(full_train_data, full_test_data):\n    \n    def tweet_clean(tweet):\n        # convert every tweet to lower case\n        tweet = ''.join([j.lower() for j in tweet])\n        # remove website links\n        tweet = re.sub('www.|https:\/\/|http:\/\/|.com|t.co\/','',tweet)    \n        # remove all punctuation \n        tweet = ''.join([j for j in tweet if j not in string.punctuation])    \n        # remove all digits\n        tweet = ''.join([j for j in tweet if j not in string.digits])    \n        # remove stopwords\n        tweet = ' '.join([j for j in tweet.split() if j not in stopwords.words('english')])    \n        # remove non ASCII characters\n        tweet = ''.join([j for j in tweet if ord(j) < 128])\n        return tweet\n    \n    full_train_data['text'] = full_train_data['text'].apply(lambda x: tweet_clean(x))\n    full_test_data['text'] = full_test_data['text'].apply(lambda x: tweet_clean(x))\n    return full_train_data, full_test_data ","094f7bbd":"# model to be used for tweet analysis\n\ndef nlp_model(full_train_data, full_test_data):\n    \n    CV = CountVectorizer(analyzer = 'char_wb', ngram_range = (1,5))     # parameters of CV can be played around with\n    X = CV.fit_transform(full_train_data['text'])\n    y = full_train_data['target']\n    X_valid = CV.transform(full_test_data['text'])\n    \n    tfidf = TfidfTransformer()\n    X = tfidf.fit_transform(X)\n    X_valid= tfidf.transform(X_valid)\n    \n    return X, y, X_valid","d50577b2":"#%% data scaling\n    \ndef scaled_data(X, X_valid):\n    from sklearn.preprocessing import StandardScaler\n    scaler_X = StandardScaler()\n    X = scaler_X.fit_transform(X)\n    X_valid = scaler_X.transform(X_valid)\n    return X, X_valid, scaler_X  ","ef8ebfc6":"#%% regressor functions\n    \ndef regressor_fn_optimised(X, y, X_valid, choice):      \n    \n    if choice == 1:                    \n        def regressor_fn(C):            \n            regressor = LogisticRegression(C = C)      \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5)\n            return cval.mean()\n        pbounds = {'C': (0.1, 10)}\n        \n    elif choice == 2:                    \n        def regressor_fn(alpha):            \n            regressor = RidgeClassifier(alpha = alpha)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5)\n            return cval.mean()\n        pbounds = {'alpha': (1, 1.1)}\n        \n    elif choice == 3:    \n                \n        def regressor_fn(n_neighbors):     \n            n_neighbors = int(n_neighbors)\n            regressor = KNeighborsClassifier(n_neighbors = n_neighbors)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5)\n            return cval.mean()\n        pbounds = {'n_neighbors': (2,10)}\n        \n    elif choice == 4:           \n        def regressor_fn(n_estimators, max_depth):     \n            max_depth, n_estimators = int(max_depth), int(n_estimators)\n            regressor = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5, n_jobs = -1)\n            return cval.mean()\n        pbounds = {'n_estimators': (10, 500), 'max_depth': (2,20)}\n        \n    elif choice == 5: \n        X, X_valid, scaler_X = scaled_data(X, X_valid)      \n        def regressor_fn(C, gamma):            \n            regressor = SVC(C = C, kernel = 'rbf', gamma = gamma)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5)\n            return cval.mean()\n        pbounds = {'C': (0.1, 100), 'gamma': (0.01, 100)}\n        \n    elif choice == 6:\n        def regressor_fn(learning_rate, max_depth, n_estimators):            \n            max_depth, n_estimators = int(max_depth), int(n_estimators)\n            regressor = LGBMClassifier(learning_rate = learning_rate, max_depth = max_depth, n_estimators = n_estimators)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 5)\n            return cval.mean()\n        pbounds = {'learning_rate': (0.01, 1), 'max_depth': (2,40), 'n_estimators': (10, 500)}        \n        \n    else:\n        def regressor_fn(learning_rate, max_depth, n_estimators):            \n            max_depth, n_estimators = int(max_depth), int(n_estimators)\n            regressor = XGBClassifier(learning_rate = learning_rate, max_depth = max_depth, n_estimators = n_estimators)        \n            cval = cross_val_score(regressor, X, y, scoring = 'balanced_accuracy', cv = 3)\n            return cval.mean()\n        pbounds = {'learning_rate': (0.01, 1), 'max_depth': (2,50), 'n_estimators': (10, 500)}\n    \n    optimizer = BayesianOptimization(regressor_fn, pbounds, verbose = 2)\n    optimizer.probe(params = {'C':1}, lazy = True)\n    optimizer.maximize(init_points = 5, n_iter = 10)    \n    # change next line in accordance with choice of regressor made\n    # y_valid_pred = RandomForestClassifier(max_depth = int(optimizer.max['params']['max_depth']), n_estimators = int(optimizer.max['params']['max_depth'])).fit(X, y).predict(X_valid)\n    y_valid_pred = LogisticRegression(C = optimizer.max['params']['C']).fit(X, y).predict(X_valid)\n    \n    return y_valid_pred, optimizer.max","768c97b6":"if __name__ == '__main__':\n    full_train_data, full_test_data = get_data()\n    full_train_data, full_test_data = clean_data(full_train_data, full_test_data)\n    X, y, X_valid = nlp_model(full_train_data, full_test_data)\n    # uncomment the following line if the model's hyper parameters have to be optimised. \n#     y_valid_pred, optimal_params = regressor_fn_optimised(X, y, X_valid, choice = 1)\n    #comment the following line if regressor_fn_optimised is being run\n    y_valid_pred = LogisticRegression().fit(X, y).predict(X_valid)\n    df = pd.DataFrame({'Id':full_test_data.index, 'Target':y_valid_pred})\n    df.to_csv('prediction.csv', index = False)","9ec58508":"# NLP models - CountVectorizer and TF - IDF","ff4e6511":"# An elaborate EDA is not done as it has already been done many times and multiple kernels can be found. Data cleaning and processing is done in the funciton clean_data. ","7b7510cc":"# This notebook does a manual basic analysis of the train and test data sets. Conventional machine learning models are used. Inline comments are used to explain the code. "}}