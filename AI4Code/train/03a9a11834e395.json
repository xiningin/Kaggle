{"cell_type":{"97ed105b":"code","38f29855":"code","689043b5":"code","7cae5074":"code","0f482101":"code","5c993fbb":"code","3fc6f9b1":"code","5371e0ea":"code","084e81cc":"code","1b6c86f5":"code","1b677931":"code","f4095629":"code","bbe94a7f":"code","6ed48ce3":"code","2b230d20":"code","2cdca82c":"code","f2d616ef":"code","3eaab964":"code","4b53fbce":"code","d272dc40":"code","9440a404":"code","b130dea0":"code","cb8cd9ae":"code","eece4faf":"code","e8c42ca2":"markdown","d010c702":"markdown","7cdfedce":"markdown","fede990d":"markdown","79fbc332":"markdown","84f815aa":"markdown","d2f73698":"markdown","970ec772":"markdown","6871628f":"markdown","69dd3335":"markdown","e9ec2372":"markdown","b7862035":"markdown","8af2c4a6":"markdown","ec09b48f":"markdown","fb4dc7e3":"markdown"},"source":{"97ed105b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nimport matplotlib.pyplot as plt\n\n# Keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Dropout\nfrom keras import callbacks\n\n# Scoring\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score,classification_report\n\n# Removes warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","38f29855":"# Using DataTable for faster loading\ntrain_df = dt.fread('..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas()\ntest_df = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()","689043b5":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","7cae5074":"# Reduce Memory Usage\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","0f482101":"train_df.head()","5c993fbb":"train_df.info()","3fc6f9b1":"train_df.describe()","5371e0ea":"train_df.shape","084e81cc":"test_df.shape","1b6c86f5":"missing_train = train_df.isnull().sum().sum()\nmissing_test = test_df.isnull().sum().sum()\nprint('Total missing value in train dataset is:', missing_train)\nprint('Total missing value in test dataset is:', missing_test)","1b677931":"train_df['target'].value_counts()","f4095629":"X = train_df.drop([\"id\", \"target\"], axis=1)\ny = train_df['target']\n\n# freeing up some memory\ndel train_df","bbe94a7f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","6ed48ce3":"#scaling the data\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","2b230d20":"# create model\nmodel = Sequential()\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='swish'))\nmodel.add(Dense(128, activation='swish'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile model\n\nmodel.compile(optimizer = \"adam\",loss = 'binary_crossentropy', metrics = ['AUC'])\n\nearlystopping = callbacks.EarlyStopping(monitor='val_loss',\n                                        mode=min,\n                                        verbose=1,\n                                        patience=83)","2cdca82c":"# Fit the model\nhistory = model.fit(X_train, y_train,validation_data=(X_test,y_test), batch_size = 2048, epochs = 2000,callbacks =[earlystopping])","f2d616ef":"# summarize history for acc\nplt.plot(history.history['auc'])\nplt.plot(history.history['val_auc'])\nplt.title('Model AUC')\nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","3eaab964":"test_df = test_df.drop([\"id\"], axis=1)","4b53fbce":"test_df = scaler.transform(test_df)","d272dc40":"pred = model.predict(test_df)","9440a404":"sub=pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsub","b130dea0":"sub['target'] = pred","cb8cd9ae":"sub.head()","eece4faf":"sub.to_csv(\"submission.csv\",index=False)","e8c42ca2":"### Load Dataset","d010c702":"## 1. Introduction","7cdfedce":"Source and credit to https:\/\/www.kaggle.com\/mirichoi0218\/ann-making-model-for-binary-classification#7.-ANN-Model-Summary-&-Compare","fede990d":"> <center><img src=\"https:\/\/elogeel.files.wordpress.com\/2010\/05\/050510_1627_multilayerp1.png\" width=\"500px\"><\/center>","79fbc332":"## 4. Data Splitting","84f815aa":"### Memory Reduction","d2f73698":"## 7. Submission","970ec772":"### Import Libraries","6871628f":"## 2. Data Acquisition","69dd3335":"* The Artificial Neural Network consists of an input layer, a hidden layer, and an output layer.","e9ec2372":"## 3. EDA","b7862035":"## 5. Building the ANN","8af2c4a6":"<div align='left'><font size=\"3\" color=\"#000000\"> The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.\n<\/font><\/div>","ec09b48f":"# **Tabular Playground Series - Nov 2021 with ANN**","fb4dc7e3":"## 6. Training the ANN"}}