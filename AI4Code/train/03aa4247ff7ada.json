{"cell_type":{"ba77b992":"code","a02e6d02":"code","f1da5dd7":"code","a36e5319":"code","695a0e2c":"code","665b3557":"code","7d8d92c3":"code","2182a8c2":"code","4101afda":"code","f64485a2":"code","59270076":"code","05f9cdeb":"code","ab8e3bb6":"code","af05624c":"code","379cba75":"code","d59f5f11":"code","f67b2311":"code","47739fc7":"code","a331557a":"code","f98317e8":"code","88abb164":"code","dd63b6c7":"code","0ee140b9":"code","e8d95105":"code","a075913f":"code","76dc8a82":"code","c1521ede":"code","a080cd03":"code","37c40562":"code","3017b2ff":"code","c9141d5e":"code","28a14444":"markdown","b6b07175":"markdown","dc7aa489":"markdown","f1fefffb":"markdown","78391c61":"markdown","8a7fb3f5":"markdown","44f945b0":"markdown","0ff5107b":"markdown","1b505d27":"markdown","115f8131":"markdown","dbd39ac0":"markdown"},"source":{"ba77b992":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a02e6d02":"# We'll start with a simple library import\nimport os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display\n\n# Set the seed value for experiment reproducibility.\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)","f1da5dd7":"DATASET_PATH = 'data\/mini_speech_commands'\n\ndata_dir = pathlib.Path(DATASET_PATH)\nif not data_dir.exists():\n    tf.keras.utils.get_file(\n        'mini_speech_commands.zip',\n        origin=\"http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/mini_speech_commands.zip\",\n        extract=True,\n        cache_dir='.', cache_subdir='data')","a36e5319":"commands = np.array(tf.io.gfile.listdir(str(data_dir)))\ncommands = commands[commands != 'README.md']\nprint('Commands:', commands)","695a0e2c":"filenames = tf.io.gfile.glob(str(data_dir) + '\/*\/*')\nfilenames = tf.random.shuffle(filenames)\nnum_samples = len(filenames)\nprint('Number of total examples:', num_samples)\nprint('Number of examples per label:',\n      len(tf.io.gfile.listdir(str(data_dir\/commands[0]))))\nprint('Example file tensor:', filenames[0]);","665b3557":"train_files = filenames[:6400]\nval_files = filenames[6400: 6400 + 800]\ntest_files = filenames[-800:]\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))","7d8d92c3":"test_file = tf.io.read_file(DATASET_PATH+'\/down\/0a9f9af7_nohash_0.wav')\ntest_audio, _ = tf.audio.decode_wav(contents=test_file)\ntest_audio.shape","2182a8c2":"def decode_audio(audio_binary):\n    # Decode WAV-encoded audio files to `float32` tensors, normalized\n    # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n    audio, _ = tf.audio.decode_wav(contents=audio_binary)\n    # Since all the data is single channel (mono), drop the `channels`\n    # axis from the array.\n    return tf.squeeze(audio, axis=-1)","4101afda":"def get_label(file_path):\n    parts = tf.strings.split(\n        input=file_path,\n        sep=os.path.sep)\n    # Note: You'll use indexing here instead of tuple unpacking to enable this\n    # to work in a TensorFlow graph.\n    return parts[-2]","f64485a2":"def get_waveform_and_label(file_path):\n    label = get_label(file_path)\n    audio_binary = tf.io.read_file(file_path)\n    waveform = decode_audio(audio_binary)\n    return waveform, label","59270076":"AUTOTUNE = tf.data.AUTOTUNE\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_files)\n\nwaveform_ds = files_ds.map(\n    map_func=get_waveform_and_label,\n    num_parallel_calls=AUTOTUNE)","05f9cdeb":"rows = 3\ncols = 3\nn = rows * cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    label = label.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","ab8e3bb6":"def get_spectrogram(waveform):\n    # Zero-padding for an audio waveform with less than 16,000 samples.\n    input_len = 16000\n    waveform = waveform[:input_len]\n    zero_padding = tf.zeros(\n        [16000] - tf.shape(waveform),\n        dtype=tf.float32)\n    # Cast the waveform tensors' dtype to float32.\n    waveform = tf.cast(waveform, dtype=tf.float32)\n    # Concatenate the waveform with `zero_padding`, which ensures all audio\n    # clips are of the same length.\n    equal_length = tf.concat([waveform, zero_padding], 0)\n    # Convert the waveform to a spectrogram via a STFT.\n    spectrogram = tf.signal.stft(\n        equal_length, frame_length=255, frame_step=128)\n    # Obtain the magnitude of the STFT.\n    spectrogram = tf.abs(spectrogram)\n    # Add a `channels` dimension, so that the spectrogram can be used\n    # as image-like input data with convolution layers (which expect\n    # shape (`batch_size`, `height`, `width`, `channels`).\n    spectrogram = spectrogram[..., tf.newaxis]\n    return spectrogram","af05624c":"for waveform, label in waveform_ds.take(1):\n    label = label.numpy().decode('utf-8')\n    spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","379cba75":"def plot_spectrogram(spectrogram, ax):\n    if len(spectrogram.shape) > 2:\n        assert len(spectrogram.shape) == 3\n        spectrogram = np.squeeze(spectrogram, axis=-1)\n    # Convert the frequencies to log scale and transpose, so that the time is\n    # represented on the x-axis (columns).\n    # Add an epsilon to avoid taking a log of zero.\n    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)","d59f5f11":"fig, axes = plt.subplots(2, figsize=(20, 16))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\n\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.show();","f67b2311":"def get_spectrogram_and_label_id(audio, label):\n    spectrogram = get_spectrogram(audio)\n    label_id = tf.argmax(label == commands)\n    return spectrogram, label_id","47739fc7":"spectrogram_ds = waveform_ds.map(\n  map_func=get_spectrogram_and_label_id,\n  num_parallel_calls=AUTOTUNE)","a331557a":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(spectrogram.numpy(), ax)\n    ax.set_title(commands[label_id.numpy()])\n    ax.axis('off')\n\nplt.show()","f98317e8":"def preprocess_dataset(files):\n    files_ds = tf.data.Dataset.from_tensor_slices(files)\n    output_ds = files_ds.map(\n        map_func=get_waveform_and_label,\n        num_parallel_calls=AUTOTUNE)\n    output_ds = output_ds.map(\n        map_func=get_spectrogram_and_label_id,\n        num_parallel_calls=AUTOTUNE)\n    return output_ds","88abb164":"train_ds = spectrogram_ds\nval_ds = preprocess_dataset(val_files)\ntest_ds = preprocess_dataset(test_files)","dd63b6c7":"batch_size = 64\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)","0ee140b9":"train_ds = train_ds.cache().prefetch(AUTOTUNE)\nval_ds = val_ds.cache().prefetch(AUTOTUNE)","e8d95105":"for spectrogram, _ in spectrogram_ds.take(1):\n    input_shape = spectrogram.shape\nprint('Input shape:', input_shape)\nnum_labels = len(commands)\n\n# Instantiate the `tf.keras.layers.Normalization` layer.\nnorm_layer = layers.Normalization()\n# Fit the state of the layer to the spectrograms\n# with `Normalization.adapt`.\nnorm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    # Downsample the input.\n    layers.Resizing(32, 32),\n    # Normalize.\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()","a075913f":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)","76dc8a82":"EPOCHS = 10\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)","c1521ede":"metrics = history.history\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.show()","a080cd03":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n    test_audio.append(audio.numpy())\n    test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)","37c40562":"y_pred = np.argmax(model.predict(test_audio), axis=1)\ny_true = test_labels\n\ntest_acc = sum(y_pred == y_true) \/ len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","3017b2ff":"confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=commands,\n            yticklabels=commands,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","c9141d5e":"sample_file = data_dir\/'no\/01bb6a2a_nohash_0.wav'\n\nsample_ds = preprocess_dataset([str(sample_file)])\n\nfor spectrogram, label in sample_ds.batch(1):\n    prediction = model(spectrogram)\n    plt.bar(commands, tf.nn.softmax(prediction[0]))\n    plt.title(f'Predictions for \"{commands[label[0]]}\"')\n    plt.show()","28a14444":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">CNN and Metrics<\/p>","b6b07175":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Converting signals into spectrograms<\/p>","dc7aa489":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Prepare to Neural Net<\/p>","f1fefffb":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Prediction<\/p>","78391c61":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Import data<\/p>","8a7fb3f5":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Confusion matrix<\/p>","44f945b0":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Read the audio files and their labels<\/p>","0ff5107b":"**Our CNN has an accuracy of 84 percent and can predict simple words.**","1b505d27":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Audio recognition<\/p>","115f8131":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">The end<\/p>","dbd39ac0":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Plot waves<\/p>"}}