{"cell_type":{"d4ab5d42":"code","0b12bc19":"code","d652a63b":"code","1abadfcb":"code","21c2c0e2":"code","d3d91a73":"code","97c46f08":"code","5b71a341":"code","57fff706":"code","979ede43":"code","07caf026":"code","fe60616a":"code","12a5eaa0":"code","64d36e0a":"code","d42cfe36":"code","40e3801a":"code","e82852e1":"code","ba759c4e":"code","af932397":"code","b2ebe9be":"code","a32cfc7e":"code","e0fdc8ce":"code","5ac1d4d0":"code","57f20020":"markdown","99bcdff1":"markdown","5c7dae80":"markdown","dc9920b9":"markdown","97ee72e7":"markdown","75528199":"markdown","1d2aa9e7":"markdown","ba325dc0":"markdown","121e7376":"markdown","c4d33e76":"markdown","20e87802":"markdown","1c11584c":"markdown"},"source":{"d4ab5d42":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score,make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix #Criar matriz de confusao\nfrom sklearn.metrics import plot_confusion_matrix# Desenha","0b12bc19":"df= pd.read_csv(\"\/kaggle\/input\/telco-customer-churn-fixed\/Telco-Customer-Churn.csv\")\ndf.head()","d652a63b":"df.drop(['Unnamed: 21'],axis=1,inplace=True)\ndf.drop(['customerID'],axis=1,inplace=True)\ndf.dtypes","1abadfcb":"#Good way of checking for Nan values\n#len(df.loc[df['TotalCharges']== ' '])\ndf.loc[(df['TotalCharges']== ' '),'TotalCharges']=0\n#len(df.loc[df['tenure']== ' '])\ndf['TotalCharges']=pd.to_numeric(df['TotalCharges'])\ndf.dtypes","21c2c0e2":"#Just replacing the empty parts of certain data values  to make then numeric later\ndf.replace(' ', '_',regex=True,inplace=True)\ndf.replace('-','_',regex=True,inplace=True)\ndf.head()","d3d91a73":"#Copy function pretty good, to not delete from the df itself that you are working on\nX=df.drop('Churn',axis=1).copy()\nX.head()","97c46f08":"y=df['Churn'].copy()\ny.replace('Yes', '1',regex=True,inplace=True)\ny.replace('No', '0',regex=True,inplace=True)\ny=pd.to_numeric(y)\ny.dtype","5b71a341":"X.dtypes\n# User advised, the conversion of categorical data is called one hot encoding which is basically, the conversion of the data to binary code\n#Avoid using continuos data, bcs the categories 4 and 3 are going to be more likely clustered together. Thant 4 and 1, bcs of the way the xgbooost tree works\n#Therefore we convert then into binary avoiding that issue. And giving then the same weight\n#Columns tranformer, changes the column itself and it is from SciKit(memento LEARN IT GODAMMIT)\n#Get dummies creates new columns based solely\n","57fff706":"pd.get_dummies(X,columns=['PaymentMethod']).head()\n#This is the conversion to binary, for payment methods\n#You see this divide the categories in 4 column, one for each category. And changes it to binary, good function","979ede43":"X_encoded= pd.get_dummies(X,columns=['gender',\n                                    'Partner',\n                                    'Dependents',\n                                    'PhoneService',\n                                    'MultipleLines',\n                                    'InternetService',\n                                    'OnlineSecurity',\n                                    'OnlineBackup',\n                                    'DeviceProtection',\n                                    'TechSupport',\n                                    'StreamingTV',\n                                    'StreamingMovies',\n                                    'Contract',\n                                    'PaperlessBilling',\n                                    'PaymentMethod'])\nX_encoded.head()\n","07caf026":"y.unique()","fe60616a":"sum(y)\/len(y)","12a5eaa0":"X_train,X_test,y_train,y_test=train_test_split(X_encoded,y,random_state=42,stratify=y)\n#Random state is the amount of time we going to shuffle the godamm data before picking it up it is set to 42 to see if i at least get an approximate value to Joshs\n#Stratify is basically making sure that the amount of people that left the company in the training set, is the same amount of people in the testing set. \n#The some goes otherwise.","64d36e0a":"#Makin sure that stratify technique worked\nsum(y_train)\/len(y_train)","d42cfe36":"sum(y_test)\/len(y_test)","40e3801a":"clf_xgb=xgb.XGBClassifier(objective='binary:logistic',missing=1,seed=42)\nclf_xgb.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_metric='aucpr',eval_set=[(X_test,y_test)])\n#Verbose means tell me everything, in this algorythimn we are doing early stoppage basically we stop building after we dont have any considerable decrease in the loss function","e82852e1":"plot_confusion_matrix(clf_xgb,X_test,y_test,values_format='d',display_labels=['Stayed','Left'])","ba759c4e":"param_grid={\n    'max_depth':[3,4,5],\n    'learning_rate':[0.1,0.01,0.05],\n    'gamma':[0,0.25,1.0],\n    'reg_lambda':[0,1.0,10.0],\n    'scale_pos_weight':[1,3,5],\n}\n#Round 2 of parameters\nparam_grid2={\n    'max_depth':[4],\n    'learning_rate':[0.1,0.5,1],\n    'gamma':[0.25],\n    'reg_lambda':[10.0,20,100],\n    'scale_pos_weight':[3]\n}","af932397":"# optimal_params = GridSearchCV(\n# estimator=xgb.XGBClassifier(objective='binary:logistic',seed=42,subsample=0.9,colsample_bytree=0.5),\n# param_grid=param_grid,\n# scoring='roc_auc',\n# verbose=0,\n# n_jobs=10,\n# cv=3)\n# #Cv means croos validation hmmkay?\n# #Subsample to select just a part of the data, colsample just to move things a little faster\n# #Just so you guy know so you dont have to run this big ball of parameters best gamma=0.25, learn_rate=0.1,max_depth=4, reg_lambda=10\n# optimal_params.fit(X_train,\n#                    y_train,\n#                    early_stopping_rounds=10,\n#                    eval_metric='auc',\n#                    eval_set=[(X_test,y_test)],\n#                   )\n# print(optimal_parms.best_params_)","b2ebe9be":"clf_xgb=xgb.XGBClassifier(seed=42,\n                          objective='binary:logistic',\n                          gamma=0.25,\n                          learning_rate=0.1,\n                          max_depth=4,\n                          reg_lambda=10,\n                          scale_pos_weight=3,\n                          subsample=0.9,\n                          colsample_bytree=0.5)","a32cfc7e":"clf_xgb.fit(X_train,\n               y_train,\n               verbose=True,\n               early_stopping_rounds=10,\n               eval_metric='aucpr',\n               eval_set=[(X_test,y_test)])","e0fdc8ce":"plot_confusion_matrix(clf_xgb,X_test,y_test,values_format='d',display_labels=['Stayed','Left'])","5ac1d4d0":"clf_xgb=xgb.XGBClassifier(seed=42,\n                          objective='binary:logistic',\n                          gamma=0.25,\n                          learning_rate=0.1,\n                          max_depth=4,\n                          reg_lambda=10,\n                          scale_pos_weight=3,\n                          subsample=0.9,\n                          colsample_bytree=0.5,\n                          n_estimators=1)\n#N_estimator=1 to make sure is just one\nclf_xgb.fit(X_train,y_train)\n\nbst=clf_xgb.get_booster()\nfor importance_type in ('weight','gain','cover','total_gain','total_cover'):\n    print('%s:'% importance_type,bst.get_score(importance_type=importance_type))\nnode_params={'shape':'box',\n            'style':'filled,rounded',\n            'fillcolor':'#78cbe'}\nleaf_params={'shape':'box',\n            'style':'filled',\n            'fillcolor':'#e48038'}\n#Num trees =0 because you know necessity\nxgb.to_graphviz(clf_xgb,num_trees=0,size='10,10',condition_node_params=node_params,leaf_node_params=leaf_params)","57f20020":"# Final model","99bcdff1":"Loading the data","5c7dae80":"* Important thingy this line of code doesnt correspond with Joshs, so lets keep it that way\n* Bcs his doesnt make any sense, the value given to positive values is 0\n* And the value negative is 1 \n* One important thing to remind myself later is that missing data in Xgboost is evaluated and associated with the tree that has the biggest Gain.\n* The datapoints that have missing data are going to be clustered with the threshhold that has the biggest Gain\n* And that is how xgboost handles missing data\n","dc9920b9":"### Okay dividing the data into, the prediction model and the classification model\n* X=Columns used for classification, features\n* y=Columns used for prediction, or the churn column  which is basically the chances of someone leaving, or label","97ee72e7":"Xgboost project for Data Science\n\n* In this project, i will be analysing the Telco Churn Dataset\n* The telco churn is a dataset about a false telephone company, and it is customers\n* I will build up a prediction and a classification, props to the man the myth Josh Starmer\n* Who proposed the project!\n* The algorythm in use will be Xgboost\n* The dataset has an type data issue so it is gonna be fun\n* Also also, the datatype used is a little diferent than joshs cuz that is what i could find so also a few different changes\n* Also also also, just a personal project of mine to display nothing to serious\n* Since the data is small we will not be using aditional hardware mechanics, to run the code faster\n* Hope you guys enjoy\n**Signed Sirmicalau or yappy or Francisco**\n","75528199":"### And now lets apply the get dummies function, to every single categorical line","1d2aa9e7":"# Lets try drawing a tree","ba325dc0":"### Time for a little bit of data conversion","121e7376":"Checking up for nan values on numeric columns, and substituting then","c4d33e76":"Droping weird column of the data that i dont know what it is about\nand unecessary ones","20e87802":"### Building a preliminary model\nThis is big brain time, so basically what we going to do is take consideration the proportions of yes and nos","1c11584c":"## Optimization\n* Lets try to improve tehe way we are currently training our dataset so lets try to find some good values\n* For a certain amount of parameter, like lambda, gamma, learning rate and max_depth.\n* We going to find which one is the best one by cross validation, spliting the data and seeing which one is the best, with all the possibilities given\n* So basically the gridsearchcv function iHA"}}