{"cell_type":{"d57b9c6d":"code","919b0589":"code","45a9b050":"code","8a6bb8d9":"code","7f811c82":"code","46cbd28d":"code","d81d6a00":"code","7ed0ff2f":"code","5964f3cf":"code","a7b36382":"code","a9e2e726":"code","ba8579aa":"code","d324eecf":"code","ce426d62":"code","3f58157e":"code","002f55ad":"code","b997e1c0":"code","82410264":"code","c8168377":"code","55d2f467":"code","8410c326":"code","862e069c":"code","d8f7964f":"code","6315476c":"code","869692da":"code","8fdc9ea0":"code","ef8c186f":"code","ab25053f":"code","984518de":"code","471ead49":"code","1003ce12":"code","5b620f57":"code","f3f840d9":"code","3343516f":"code","28994e50":"code","70e2e9ca":"code","6a77b661":"code","83d1133d":"code","279d568d":"code","df652dc4":"code","8cd5988b":"code","3c3a836a":"code","a82cc742":"code","b95892cf":"code","d8f1d730":"code","d3cabaa5":"code","ff806fe8":"code","eeee6959":"code","09ffe399":"code","c9ebf7a9":"code","f5674399":"code","98935f6a":"code","2c734391":"code","2b1dc603":"code","7aefb509":"code","b7e089da":"code","7d28ff6d":"code","ea39dfa6":"code","56f34590":"code","827be1cf":"code","f42cab12":"code","cd23121e":"code","83146da0":"code","1f167a76":"code","1bab0895":"markdown","f8cb1b5a":"markdown","167f9dfc":"markdown","e4b3420e":"markdown","82a515bf":"markdown","218fc2e5":"markdown","5717b31e":"markdown","1188ff27":"markdown","934b134a":"markdown","68debf67":"markdown","9bf7667e":"markdown","8bd74d15":"markdown","cd6c8b64":"markdown","c8b3bab3":"markdown","e1937cba":"markdown","b707352b":"markdown","cb30a577":"markdown","d81f0429":"markdown","bac36fb6":"markdown","49b31afe":"markdown","818c7ed8":"markdown","6b42f6c8":"markdown"},"source":{"d57b9c6d":"# Check the version of pyspark\n# Pyspark Version should be: 2.4.0\n!pyspark --version\n","919b0589":"# Import pyspark\nimport pyspark\nfrom pyspark import SparkConf\nfrom pyspark.ml.feature import (\n    StringIndexer,\n    OneHotEncoder,\n    VectorAssembler,\n    StandardScaler,\n)\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, ClusteringEvaluator\n\nfrom pyspark.sql.types import FloatType, StringType, DoubleType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n\n# Import numpy, pandas, matpoltlib, seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom sklearn import metrics\n\n# import xgboost\nimport xgboost as xgb\n\n# Import keras\nimport keras\nfrom keras import Sequential, regularizers\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.utils import np_utils\n\nsns.set_style(\"whitegrid\")  # Set style of seaborn\n","45a9b050":"# Create SparkSession in local environment\nspark = (\n    SparkSession.builder.appName(\"Telecom_Churn\")  # create a name\n    .config(\"spark.executor.memory\", \"10g\")  # Maximum amount of memory allowed\n    .config(\"spark.executor.cores\", \"3\")  # Maximum cpu threads allowed\n    .config(\"spark.cores.max\", \"3\")\n    .config(\"spark.driver.memory\", \"10g\")\n    .config(\"spark.sql.broadcastTimeout\", \"9000\")\n    .getOrCreate()\n)\nSparkConf().getAll()\n","8a6bb8d9":"# Define a load founction\ndef load(path, show_num=None, printable=True):\n    \"\"\"\n    read csv file, drop null values, print the length of the rows\n    \"\"\"\n    data = spark.read.csv(path, header=\"true\", inferSchema=True)\n    data = data.na.drop()\n    if show_num:\n        data.show(show_num)\n\n    if printable == True:\n        data.printSchema()\n        print(\"Length:\", data.count())\n    return data\n","7f811c82":"# Define the path of data source and load into spark dataframe\npath = \"..\/input\/cell2celltrain.csv\"\ndf = load(path)\ndf.select(\"MonthlyRevenue\").describe().show()\n","46cbd28d":"# Delete the rows with 'NA' values\nfor column in df.columns:\n    if df.schema[column].dataType == pyspark.sql.types.StringType():\n        df = df.filter(df[column] != \"NA\")\n","d81d6a00":"# Check the description of Churn column\ndf.select(\"Churn\").describe().show()\n","7ed0ff2f":"# Going through all columns required to be transfer into numerical values\ncolumns_string = [\n    column\n    for column in df.columns\n    if df.schema[column].dataType == pyspark.sql.types.StringType()\n]\ncolumns_string\n","5964f3cf":"# create the view of df under spark\ndf.createOrReplaceTempView(\"df\")\n\n# check the distribution of picked columns\nspark.sql(\n    \"SELECT \\\n            Churn, \\\n            CAST (avg(MonthlyRevenue) as decimal(8,2)) as avg_MonthlyRevenue, \\\n            CAST (avg(MonthlyMinutes) as decimal(8,2)) as avg_MonthlyMinutes, \\\n            CAST (avg(CurrentEquipmentDays) as decimal(8,2)) as avg_CurrentEquipmentDays, \\\n            CAST (avg(TotalRecurringCharge) as decimal(8,2)) as avg_TotalRecurringCharge, \\\n            CAST (avg(OverageMinutes) as decimal(8,2)) as avg_OverageMinutes, \\\n            CAST (avg(RoamingCalls) as decimal(8,2)) as avg_RoamingCalls \\\n            FROM df GROUP BY Churn\"\n).show()\n","a7b36382":"# Check the values of string columns\ndf.select([column for column in df.columns if column in columns_string]).show(1)\n","a9e2e726":"# Based on the values of the string columns, select the list of columns for StringIndexer\ncolumns_for_indexer = [\n    \"Churn\",\n    \"ChildrenInHH\",\n    \"HandsetRefurbished\",\n    \"HandsetWebCapable\",\n    \"TruckOwner\",\n    \"RVOwner\",\n    \"Homeownership\",\n    \"BuysViaMailOrder\",\n    \"RespondsToMailOffers\",\n    \"OptOutMailings\",\n    \"NonUSTravel\",\n    \"OwnsComputer\",\n    \"HasCreditCard\",\n    \"NewCellphoneUser\",\n    \"NotNewCellphoneUser\",\n    \"OwnsMotorcycle\",\n    \"MadeCallToRetentionTeam\",\n    \"MaritalStatus\",\n]\n","ba8579aa":"# Loop the list and use StringIndexer encodes the string columns of labels(Yes or No) to columns of label indices(1 or 0)\nfor column in columns_for_indexer:\n    indexer = StringIndexer(inputCol=column, outputCol=column + \"Index\")\n    df = indexer.fit(df).transform(df)\n","d324eecf":"# Drop the original string columns of labels\ndf = df.select([column for column in df.columns if column not in columns_for_indexer])\n","ce426d62":"# Check the Churn column again to confirm the result\ndf.select(\"ChurnIndex\").describe().show()\n","3f58157e":"# Replace unknown values to 0 for Price Column\ndf = df.replace(to_replace={\"Unknown\": \"0\"}, subset=[\"HandsetPrice\"])\n","002f55ad":"# Delete the rows with 'NA' values\nfor column in df.columns:\n    if df.schema[column].dataType == pyspark.sql.types.StringType():\n        df = df.filter(df[column] != \"Unknown\")\n","b997e1c0":"# Another Iteration for feature engineering\ncolumns_string = [\n    column\n    for column in df.columns\n    if df.schema[column].dataType == pyspark.sql.types.StringType()\n]\ncolumns_string\n","82410264":"# Mapping string values into integer based on its values\nmapping_PrizmCode = {\"Other\": \"0\", \"Suburban\": \"1\", \"Town\": \"2\", \"Rural\": \"3\"}\ndf = df.replace(to_replace=mapping_PrizmCode, subset=[\"PrizmCode\"])\n","c8168377":"# Prepare the list of columns to be transformed into float\ncolumns_to_float = [\n    \"MonthlyRevenue\",\n    \"MonthlyMinutes\",\n    \"TotalRecurringCharge\",\n    \"DirectorAssistedCalls\",\n    \"OverageMinutes\",\n    \"RoamingCalls\",\n    \"PercChangeMinutes\",\n    \"PercChangeRevenues\",\n]\n","55d2f467":"# Transform ServiceArea and CreditRating columns only to keep the int value\ndf = df.withColumn(\"ServiceArea\", df[\"ServiceArea\"].substr(-3, 3))\ndf = df.withColumn(\"CreditRating\", df[\"CreditRating\"].substr(1, 1))\n","8410c326":"# Transform the type of columns to float\nfor column in columns_to_float:\n    df = df.withColumn(column, df[column].cast(\"float\"))\n","862e069c":"# Create a Pandas DataFrame for data visualization\ndf_pd = df.toPandas()\n","d8f7964f":"# Before transform the type of Occupation column, let's check it's distribution first\ngraph = df_pd[[\"Occupation\", \"MonthlyRevenue\"]]\nax = sns.barplot(x=\"Occupation\", y=\"MonthlyRevenue\", data=graph)\n","6315476c":"# Transform string values to numbers using mapping\ntemp = (\n    df_pd.loc[:, [\"Occupation\", \"MonthlyRevenue\"]]\n    .groupby(\"Occupation\")\n    .mean()\n    .sort_values([\"MonthlyRevenue\"], ascending=[0])\n)\n\nmapping_Occupation = dict([temp.index[i], str(i)] for i in range(len(temp)))\nprint(mapping_Occupation)\n\ndf = df.replace(to_replace=mapping_Occupation, subset=[\"Occupation\"])\n","869692da":"df.describe()\n","8fdc9ea0":"# Prepare the list of columns to be transformed into integer\ncolumns_to_int = [\n    \"Handsets\",\n    \"HandsetModels\",\n    \"CurrentEquipmentDays\",\n    \"AgeHH1\",\n    \"AgeHH2\",\n    \"HandsetPrice\",\n    \"ServiceArea\",\n    \"CreditRating\",\n    \"PrizmCode\",\n    \"Occupation\",\n    \"ChurnIndex\",\n    \"ChildrenInHHIndex\",\n    \"HandsetRefurbishedIndex\",\n    \"HandsetWebCapableIndex\",\n    \"TruckOwnerIndex\",\n    \"RVOwnerIndex\",\n    \"HomeownershipIndex\",\n    \"BuysViaMailOrderIndex\",\n    \"RespondsToMailOffersIndex\",\n    \"OptOutMailingsIndex\",\n    \"NonUSTravelIndex\",\n    \"OwnsComputerIndex\",\n    \"HasCreditCardIndex\",\n    \"NewCellphoneUserIndex\",\n    \"NotNewCellphoneUserIndex\",\n    \"OwnsMotorcycleIndex\",\n    \"MadeCallToRetentionTeamIndex\",\n    \"MaritalStatusIndex\",\n]\n","ef8c186f":"# Transform the type of columns to integer\nfor column in columns_to_int:\n    df = df.withColumn(column, df[column].cast(\"int\"))\n","ab25053f":"# Go through the dataset check the distribution of each feature\nfor i in range(len(df.columns) \/\/ 6 + 1):\n    df.describe(df.columns[6 * i : 6 * (i + 1)]).show()\n","984518de":"# Create a new Pandas DataFrame for data visualization\ndf_pd = df.toPandas()\n","471ead49":"# Define Catplot function and Barplot function\ndef cat_plot(feature, cut=12):\n    temp = df_pd.loc[:, [feature, \"ChurnIndex\"]]\n    temp[feature + \"_binned\"] = pd.qcut(temp[feature], cut, duplicates=\"drop\")\n    ax = sns.catplot(\n        x=\"ChurnIndex\",\n        y=feature + \"_binned\",\n        data=temp,\n        kind=\"bar\",\n        height=5,\n        aspect=2.7,\n    )\n\n\ndef bar_plot(feature, cut=False, logscale=False, drop_zero=False):\n    if drop_zero:\n        temp = df_pd[df_pd[feature] != 0].loc[:, [feature, \"ChurnIndex\"]]\n    else:\n        temp = df_pd.loc[:, [feature, \"ChurnIndex\"]]\n\n    if cut > 0:\n        temp[feature + \"_binned\"] = pd.qcut(temp[feature], cut, duplicates=\"drop\")\n        ax = sns.barplot(x=feature + \"_binned\", y=\"ChurnIndex\", data=temp)\n    else:\n        ax = sns.barplot(x=feature, y=\"ChurnIndex\", data=temp)\n\n    if logscale:\n        ax.set_yscale(\"log\")\n","1003ce12":"# Use defined Catplot function plot the distribution of MonthsInService\ncat_plot(\"MonthsInService\", cut=12)\n","5b620f57":"# Use defined Barplot function plot the distribution of MonthlyMinutes\nbar_plot(\"MonthlyMinutes\", cut=5, logscale=True, drop_zero=True)\n","f3f840d9":"# Use defined Barplot function plot the distribution of OverageMinutes\nbar_plot(\"OverageMinutes\", cut=7, logscale=True, drop_zero=True)\n","3343516f":"# Use defined Barplot function plot the distribution of CreditRating\nbar_plot(\"CreditRating\", cut=5, logscale=True)\n","28994e50":"# Use defined Barplot function plot the distribution of AgeHH1\nbar_plot(\"AgeHH1\", cut=6, logscale=True, drop_zero=True)\n","70e2e9ca":"# Subplots the distribution of HandsetRefurbished, HandsetWebCapable and MadeCallToRetentionTeam into one graph\nfeature = [\n    \"HandsetRefurbishedIndex\",\n    \"HandsetWebCapableIndex\",\n    \"MadeCallToRetentionTeamIndex\",\n    \"ChurnIndex\",\n]\ntemp = df_pd.loc[:, feature]\ntemp.columns = [\n    \"HandsetRefurbished\",\n    \"HandsetWebCapable\",\n    \"MadeCallToRetentionTeam\",\n    \"ChurnIndex\",\n]\n\nfig, axs = plt.subplots(figsize=(10, 5), ncols=3)\nsns.barplot(x=\"HandsetRefurbished\", y=\"ChurnIndex\", data=temp, ax=axs[0])\nsns.barplot(x=\"HandsetWebCapable\", y=\"ChurnIndex\", data=temp, ax=axs[1])\nsns.barplot(x=\"MadeCallToRetentionTeam\", y=\"ChurnIndex\", data=temp, ax=axs[2])\n","6a77b661":"# Create column list for VectorAssembler\ninputcols = [\n    column for column in df.columns if column not in [\"CustomerID\", \"ChurnIndex\"]\n]\n","83d1133d":"# Use vectorAssembler transform the given list of columns into a single vector column.\nvectorAssembler = VectorAssembler(inputCols=inputcols, outputCol=\"features\")\nfeatures_vectorized = vectorAssembler.transform(df)\nfeatures_vectorized = features_vectorized.withColumnRenamed(\"ChurnIndex\", \"label\")\n","279d568d":"# StandardScaler\nfrom pyspark.ml.feature import StandardScaler\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(features_vectorized)\n\n# Normalize each feature to have unit standard deviation.\nscaledData = scalerModel.transform(features_vectorized)\nscaledData.show()\n","df652dc4":"# Check the distribution of label\nscaledData.select(\"label\").groupBy(\"label\").count().collect()\n","8cd5988b":"# Down-sampling\nscaledData_ds, _ = scaledData.filter(scaledData[\"label\"] == 0.0).randomSplit([0.4, 0.6])\nscaledData_ds = scaledData.filter(scaledData[\"label\"] == 1.0).union(scaledData_ds)\nscaledData_ds.select(\"label\").groupBy(\"label\").count().collect()\n","3c3a836a":"# Split dataset into train and test set\ntrain, test = scaledData_ds.randomSplit([0.8, 0.2])\n","a82cc742":"# Define plot roc curve function\ndef plot_roc_curve(model, train, test):\n    predictions = model.transform(test)\n    predictions_train = model.transform(train)\n\n    results = predictions.select([\"probability\", \"label\"])\n    results_train = predictions_train.select([\"probability\", \"label\"])\n\n    ## prepare score-label set\n    results_collect = results.collect()\n    results_list = [(float(i[0][0]), 1.0 - float(i[1])) for i in results_collect]\n\n    results_collect_train = results_train.collect()\n    results_list_train = [\n        (float(i[0][0]), 1.0 - float(i[1])) for i in results_collect_train\n    ]\n\n    fpr = dict()\n    fpr_train = dict()\n    tpr = dict()\n    tpr_train = dict()\n    roc_auc = dict()\n    roc_auc_train = dict()\n\n    y_test = [i[1] for i in results_list]\n    y_score = [i[0] for i in results_list]\n\n    y_train = [i[1] for i in results_list_train]\n    y_score_train = [i[0] for i in results_list_train]\n\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    roc_auc = auc(fpr, tpr)\n\n    fpr_train, tpr_train, _ = roc_curve(y_train, y_score_train)\n    roc_auc_train = auc(fpr_train, tpr_train)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label=\"ROC curve on testing set (area = %0.2f)\" % roc_auc)\n    plt.plot(\n        fpr_train,\n        tpr_train,\n        label=\"ROC curve on training set (area = %0.2f)\" % roc_auc_train,\n    )\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic example\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n","b95892cf":"# Validation for hyper-parameter tuning.\n# Randomly splits the input dataset into train and validation sets,\n# and uses evaluation metric on the validation set to select the best model.\nrf = RandomForestClassifier(\n    featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxBins=16\n)\n\n# Use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using the evaluator.\nparamGrid = (\n    ParamGridBuilder()\n    .addGrid(rf.maxDepth, [8, 10, 12])\n    .addGrid(rf.minInstancesPerNode, [1, 3, 5, 10])\n    .addGrid(rf.numTrees, [50, 100])\n    .build()\n)\n\n# In this case the estimator is BinaryClassificationEvaluator\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs = TrainValidationSplit(\n    estimator=rf,\n    estimatorParamMaps=paramGrid,\n    evaluator=BinaryClassificationEvaluator(),\n    trainRatio=0.8,\n)  # 80% of the data will be used for training, 20% for validation.\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel = tvs.fit(train)\n","d8f1d730":"# From the best model take chosen hyper-parameters \nbest_model = model.bestModel.extractParamMap()\n\nfor key in best_model.keys():\n    print(str(key).split(\"_\")[-1], \":\", best_model[key])\n","d3cabaa5":"# Use defined function plot roc curve and print roc auc score\nplot_roc_curve(model, train, test)\n","ff806fe8":"# Evaluate the feature importances and plot the charts for most important 20 features\nimportance = model.bestModel.featureImportances\nfeatures_list = inputcols\nfeature_importance = pd.DataFrame(\n    data={\"features_list\": features_list, \"importance\": importance}\n)\nfeature_importance = feature_importance.sort_values(by=\"importance\", ascending=False)[\n    :20\n]\nsns.catplot(\n    x=\"importance\",\n    y=\"features_list\",\n    data=feature_importance,\n    kind=\"bar\",\n    height=5,\n    aspect=2.7,\n)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.xlabel(\"Feature Importance\", fontsize=15)\n","eeee6959":"# Validation for hyper-parameter tuning.\n# Randomly splits the input dataset into train and validation sets,\n# and uses evaluation metric on the validation set to select the best model.\ngbt = GBTClassifier(\n    featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\"\n)\n\n# Use a ParamGridBuilder to construct a grid of parameters to search over.\n# TrainValidationSplit will try all combinations of values and determine best model using the evaluator.\nparamGrid = (\n    ParamGridBuilder()\n    .addGrid(gbt.maxDepth, [8, 10])\n    .addGrid(gbt.minInstancesPerNode, [5, 20, 50])\n    .addGrid(gbt.maxIter, [10, 20])\n    .build()\n)\n\n# In this case the estimator is BinaryClassificationEvaluator\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs = TrainValidationSplit(\n    estimator=gbt,\n    estimatorParamMaps=paramGrid,\n    evaluator=BinaryClassificationEvaluator(),\n    trainRatio=0.8,\n)  # 80% of the data will be used for training, 20% for validation.\n\n# Run TrainValidationSplit, and choose the best set of parameters.\nmodel = tvs.fit(train)\n\n# Make predictions on test data. model is the model with combination of parameters that performed best.\nmodel.transform(test).select(\"features\", \"label\", \"prediction\").show(10)\n","09ffe399":"# From the best model take chosen hyper-parameters \nbest_model = model.bestModel.extractParamMap()\n\nfor key in best_model.keys():\n    print(str(key).split(\"_\")[-1], \":\", best_model[key])\n","c9ebf7a9":"# Use defined function plot roc curve and print roc auc score\nplot_roc_curve(model, train, test)\n","f5674399":"# Transofrm scaledData into pandas Dataframe so we can use XGBoost and Keras\ndataset = scaledData.toPandas()\n\n# Create revenue loss column as label\ndataset[\"revenue_loss\"] = dataset.label * dataset.MonthlyRevenue\n\n# Create the list of features\nfeature_list = [\n    column\n    for column in dataset.columns\n    if column\n    not in [\"CustomerID\", \"label\", \"features\", \"revenue_loss\", \"scaledFeatures\"]\n]\n\n# Create X, y, and split into train\/test sets\nX = dataset.loc[:, feature_list]\ny = dataset.revenue_loss\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n","98935f6a":"# Use GridSearch method to figure the best hyper-parameters\n# Parameters to search over with cross-validation\ngrid_params = [\n    {\n        \"max_depth\": [4, 5],\n        \"learning_rate\": [0.1, 1],\n        \"n_estimators\": [10, 100],\n        \"reg_lambda\": [100, 10],\n        \"objective\": [\"reg:linear\"],\n    }\n]\n\nxgbr = GridSearchCV(xgb.XGBRegressor(), grid_params, cv=5, scoring=\"r2\")\nxgbr.fit(X_train, y_train)\ny_pred = xgbr.predict(X_test)\n\nprint(\"Mean Absolute Error: %r\\n\" % metrics.mean_absolute_error(y_test, y_pred))\nprint(\"Best parameter values: %r\\n\" % xgbr.best_params_)\n","2c734391":"# Plot a chart of how many of total revenue loss can be captured with our predicted highest risk group\ntemp = X_test.copy()\ntemp[\"revenue_loss\"] = y_test\ntemp[\"y_pred\"] = y_pred\nn = 20\ntemp[\"y_pred_bin\"] = pd.qcut(temp[\"y_pred\"], n, duplicates=\"drop\")\nl = len(temp[\"y_pred_bin\"].unique())\nlabels = [l + 1 - i for i in range(1, l + 1)]\ntemp[\"y_pred_bin\"] = pd.qcut(temp[\"y_pred\"], n, labels=labels, duplicates=\"drop\")\nindex_amount = temp.index[(temp.y_pred_bin == 1)]\nfig = plt.figure(figsize=(10, 5))\nlabels.sort()\nax = sns.barplot(\n    x=\"y_pred_bin\", y=\"revenue_loss\", data=temp, estimator=np.sum, order=labels\n)\nplt.xlabel(\"Predicted Risk Level\", fontsize=15)\nplt.ylabel(\"revenue_loss\", fontsize=15)\nper = 100 * sum(temp.y_pred_bin == 1) \/ len(temp.y_pred_bin)\nprint(\n    \"Highest Risk Group(%.2f%% of total clients) can capture %.2f%% of total revenue_loss\"\n    % (\n        per,\n        100 * temp[(temp.y_pred_bin == 1)].revenue_loss.sum() \/ temp.revenue_loss.sum(),\n    )\n)\n","2b1dc603":"a = 0.0005\ndrop = 0.2\nn = X_train.shape[1]\n\nmodel = Sequential(\n    [\n        Dense(\n            int(n * 1.2),\n            input_dim=n,\n            kernel_regularizer=regularizers.l1_l2(l1=a, l2=a),\n            activation=\"relu\",\n        ),\n        Dropout(drop),\n        Dense(\n            int(n), kernel_regularizer=regularizers.l1_l2(l1=a, l2=a), activation=\"relu\"\n        ),\n        Dropout(drop),\n        Dense(\n            int(n * 0.8),\n            activation=\"relu\",\n            kernel_regularizer=regularizers.l1_l2(l1=a, l2=a),\n        ),\n        Dense(1),\n    ]\n)\n","7aefb509":"# Prints a summary representation of model\nmodel.summary()\n","b7e089da":"# Configures the model for training and fit the training set\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"])\nhistory = model.fit(\n    X_train, y_train, validation_split=0.2, epochs=5, verbose=1, shuffle=True\n)\n","7d28ff6d":"# plot loss metrics\nax1 = plt.plot(history.history[\"loss\"])\nax2 = plt.plot(history.history[\"val_loss\"])\nplt.legend([\"loss\", \"val_loss\"])\n","ea39dfa6":"# Use model redict test set\ny_pred = model.predict(X_test)\ny_test = np.array(y_test)\n","56f34590":"# Plot a chart of how many of total revenue loss can be captured with our predicted highest risk group\ntemp = X_test.copy()\ntemp[\"revenue_loss\"] = y_test\ntemp[\"y_pred\"] = y_pred\nn = 20\ntemp[\"y_pred_bin\"] = pd.qcut(temp[\"y_pred\"], n, duplicates=\"drop\")\nl = len(temp[\"y_pred_bin\"].unique())\nlabels = [l + 1 - i for i in range(1, l + 1)]\ntemp[\"y_pred_bin\"] = pd.qcut(temp[\"y_pred\"], n, labels=labels, duplicates=\"drop\")\nindex_amount = temp.index[(temp.y_pred_bin == 1)]\nfig = plt.figure(figsize=(10, 5))\nlabels.sort()\nax = sns.barplot(\n    x=\"y_pred_bin\", y=\"revenue_loss\", data=temp, estimator=np.sum, order=labels\n)\nplt.xlabel(\"Predicted Risk Level\", fontsize=15)\nplt.ylabel(\"revenue_loss\", fontsize=15)\nper = 100 * sum(temp.y_pred_bin == 1) \/ len(temp.y_pred_bin)\nprint(\n    \"Highest Risk Group(%.2f%% of total clients) can capture %.2f%% of total revenue_loss\"\n    % (\n        per,\n        100 * temp[(temp.y_pred_bin == 1)].revenue_loss.sum() \/ temp.revenue_loss.sum(),\n    )\n)\n","827be1cf":"# Trains a k-means model\nkmeans = KMeans(featuresCol=\"scaledFeatures\", k=4, initSteps=2, tol=0.0001, maxIter=20)\nmodel = kmeans.fit(scaledData)\n","f42cab12":"# Make predictions\npredictions = model.transform(scaledData)\n","cd23121e":"# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n","83146da0":"# Create a Pandas DataFrame for data visualization\ncluster = predictions.toPandas()\n","1f167a76":"# Compare the different distributions of each cluster\ntemp_list = list(feature_importance.iloc[:5, 0])\ntemp_list.append(\"prediction\")\ntemp = cluster.loc[:, temp_list]\n\nfig, axs = plt.subplots(figsize=(15, 5), ncols=len(temp_list) - 1)\n\nfor i in range(len(temp_list) - 1):\n    ax = sns.barplot(x=\"prediction\", y=temp_list[i], data=temp, ax=axs[i])\n    ax.set_xlabel(\"cluster\", fontsize=12)\n    ax.set_ylabel(temp_list[i], fontsize=12, color=\"blue\")\n","1bab0895":"## 1. Overview and Goals <a name=\"introduction\"><\/a>","f8cb1b5a":"The Data Source I've chosen here is an open source data(cell2cell) by Teradata center for customer relationship management at Duke University.\n\nCell2Cell dataset is preprocessed and a balanced version provided for analyzing Process. consists of 71,047 instances and 58 attributes. We can try with data set, try Machine learning algorithms and deep leaning algorithms find measures like accuracy, ROC, AUC\n\nTelecom Industry faces fierce com-petition in satisfying its customers. The role of churn prediction system is not only restricted to accurately predict churners but also to interpret customer churn behavior.\n\nChurn management in the telecom services industry is used to securing the customers for a company. In essence, proper customer management presumes an ability to forecast the customer decides to move to another service provider, a measurement of customer profitability, and a set of strategic and tactic retention measures to reduce the movement.\n\nIn practice, we can segment its customers by profitability and focus retention management only on those profitable segments, and score the entire customer base with propensity to churn and prioritize the retention effort based on profitability and churn propensity.\n\nThe origin link of the dataset is https:\/\/www.kaggle.com\/jpacse\/datasets-for-churn-telecom\n","167f9dfc":"## 2. Data Source and Use Case <a name=\"data\"><\/a>","e4b3420e":"### 5.4 Use XGBoost Regressor Algorithm Build Revenue Loss Prediction Model","82a515bf":"Customer acquisition and **retention** is a key concern for many industries, especially acute in the strongly competitive and quick growth **telecommunications** industry. Meanwhile, since the **cost** of retaining a good customer is much lower than acquiring a new one, it is very profit-effective to input valuable resource on the Retention Campaign.\n\nCustomers churn for various different reasons. Experience tells us that once the customer has made up their mind, retention becomes really hard. Therefore, managing churn in a **proactive** process is essential for the growth of the customer base and profit.\n\nThe primary goal of churn analysis is usually to create a list of contracts that are likely to be cancelled in the near future. \n\nThe customers holding these contracts are then targeted with special incentives designed to deter cancellation.\n\nAt a more sophisticated level, the telecommunications company attempts to detect the reasons for an expected cancellation, because this information may help customize the offer.\n\n<img style=\"float: left;\" src='https:\/\/parcusgroup.com\/public\/wysiwyg\/images\/Telecom_Customer_Churn_Prediction_Models.jpg' width='50%'>\n","218fc2e5":"## 4. Analysis & Visualization <a name=\"Analysis&Visualization\"><\/a>","5717b31e":"### 5.5 Use Keras deploy Nural networks to predict revenue loss","1188ff27":"## 6. Customer Segmentation  <a name=\"Segmentation\"><\/a>","934b134a":"Since the data set is highly skewed \u2013 we have much more Negative training samples than Positive training samples \u2013 we will need to try out some strategies that counter the unbalance.\n\nHere I'm going to use one of the simplest method called down-samplings, which means we just randomly filter out some of the majority cases.","68debf67":"## 3. ETL (Extract Transform Load) <a name=\"etl\"><\/a>","9bf7667e":"The first step of this module is data preprocessing. We'll start from VectorAssembler, which is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like Random Forest and Gradient-Boosted Trees. Next, since we're going to use K-Means and Neural Network for the modeling, we're going to use StandardScaler transforms the dataset of Vector rows, normalizing each feature to have unit standard deviation and\/or zero mean. Last, we'll use down-samplings method to deal with our unbalance dataset.\n\nNext Step is modeling, in this project we'll use Random Forest Classifier and GBTs Classifier to predict customer Churn; Use XGBoost Regressor and Neural Network to predict Revenue Loss.\n\nIn the last part of each modeling iteration is evaluate the performance of each prediction model, in order to do so, we will split the dataset in 2 parts: training set (80% of the entries in the dataset) and test set (20% of the entries in the dataset). We train the model on the training set and then we test the model on the test set. I decided to use the ROC curve(A receiver operating characteristic curve) to test the model performance for classification problem which is to predict customer Churn in our case; And use mean absolute error for regression program which is predicted revenue loss in our case, then I'll plot a chart of how many of total revenue loss can be covered with our predicted highest risk group.\n","8bd74d15":"## Table of contents\n* [1. Overview and Goals](#introduction)\n* [2. Data Source and Use Case](#data)\n* [3. ETL (Extract Transform Load)](#etl)\n* [4. Analysis & Visualization](#Analysis&Visualization)\n* [5. Model Training and Evaluation](#Modeling)\n* [6. Customer Segmentation](#Segmentation)\n* [7. Results and Discussion](#results)","cd6c8b64":"### 5.1 Data Preprocessing","c8b3bab3":"Due to the lack of missing some important feature, we can see the overall classification performance is not perfect enough. However, we can still catch the highest risk group so the stakeholders can focus on those groups based on the predicted risk level, so they can put less retention cost but save more revenue loss.\n\nFrom the result of the segmentation model, we can see that all customers have been separated into different clusters perfectly. Customer segmentation begins with gathering and analyzing data and ends with acting on the information gathered in a way that is appropriate and effective. So the next step in real business is based on these clusters and distributions, following with analytical technique such as Uplift Modeling to models the difference in behavior between target and control groups, so the business owner can customize their up-sale and cross-sale strategy. ","e1937cba":"### 5.2 Random Forest Classifier","b707352b":"## 5. Prediction Model Training and Evaluation <a name=\"Modeling\"><\/a>","cb30a577":"<br>\n<left><bold><font size=\"5\"> Customer Churn Prediction and Segmentation for Telecom<\/font><\/bold><\/left>\n<br>\n<br>\n","d81f0429":"### 5.3 Gradient-Boosted Trees(GBTs) Classifier","bac36fb6":"In this module, we'll use KMeans to build a Customer Segmentation model, and then we can compare the different distributions of each cluster.\n\nCustomer segmentation is the practice of dividing customers into groups of individuals that are similar in specific ways relevant to marketing, such as age, months in service, services usage and services spending.\n\nBased on the customer segmentation, the telecommunications company can customize their marketing efforts for different cluster, and gain a deeper understanding of their customers' preferences in order to more accurately tailor marketing materials.\n","49b31afe":"Before go to next step, I'd like to do some Analysis & Visualization of the features we just cleaned. So we can have better understanding on the business aspect.\n\nBased on the industry experience, usually there're five dimensions and listed below are the significant variables.\n-\tCustomer demography: Age, Tenure, Gender, Location, Zip code, etc.\n-\tBill and payment: Monthly fee, Billing amount, Count of overdue payment, payment method, Billing type, etc.\n-\tNetwork\/TV\/Phone usage records: Network use frequency, network flow, usage time and period, internet average speed, In-net call duration, Call type, etc.\n-\tCustomer care\/service: Service call number, service type, service duration, account change count\n-\tCompetitors information: Offer detail under similar plan, etc\n\nAs we can see based on our original database, we're lack of several features such as competitors information. In the real business projects I experienced that is extremely important, however since we're only showing technical process here, I'll just leave it.","818c7ed8":"## 7. Results and Discussion <a name=\"results\"><\/a>","6b42f6c8":"Before Modeling and Visualization, first of all we need follow the data engineering process, specificlly ETL process, which stands for Extract, transform, and load variables from identified data items.\n\nExtract is the process of reading data from a database. In this stage, the data is collected, often from multiple and different types of sources.\n\nTransform is the process of converting the extracted data from its previous form into the form it needs to be in so that it can be placed into another database. Transformation occurs by using rules or lookup tables or by combining the data with other data.\n\nLoad is the process of writing the data into the target database.\n\n<img style=\"float: left;\" src='https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/data-guide\/images\/etl.png' width='50%'>\n<br>"}}