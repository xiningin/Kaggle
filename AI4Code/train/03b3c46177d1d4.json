{"cell_type":{"a3b8bae1":"code","cf1342e6":"code","61439e9e":"code","b739aa89":"code","bc922f69":"code","7d442271":"code","a0e7f7f1":"code","6519ac7d":"code","1f9a6fb2":"code","5bac9d43":"code","8a8e65b5":"code","68585c3b":"code","a2d0537a":"code","01c3e2f7":"markdown","d086e799":"markdown","3d079e87":"markdown","b1a66ffd":"markdown","d3f93b91":"markdown","ba590868":"markdown","1c6f2eee":"markdown","7200b80d":"markdown","870b7622":"markdown","e2d8985e":"markdown","97ea1537":"markdown","e490c5e8":"markdown"},"source":{"a3b8bae1":"import numpy as np # linear algebra\nfrom scipy import stats\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.layers.core import Dense, Activation\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\nprint ('import completed')","cf1342e6":"model_1 = Sequential()\nmodel_1.add(Dense(3, input_shape = (1,),  activation = 'tanh'))\n#model_1.add(Dense(3, input_shape = (1,),  activation = 'relu'))\nmodel_1.add(Dense(1))\nmodel_1.compile(optimizer = 'sgd' , loss = 'mean_squared_error')\nprint(model_1.summary())","61439e9e":"# A feature is an input variable, in this case x\nx=np.linspace(start=-1, stop=1, num=50)\n# A label is the thing we're predicting, in this case y\ny=np.square(x)\nplt.plot(x,y)","b739aa89":"# train the model\nhistory = model_1.fit(x, y, epochs = 20000, verbose = 0)\n# Predict the values\ny_pred = model_1.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","bc922f69":"# A feature is an input variable, in this case x\nx=np.linspace(start=-1, stop=1, num=50)\n# A label is the thing we're predicting, in this case y\ny=np.sin(x*3)\nplt.plot(x,y)","7d442271":"model_3 = Sequential()\nmodel_3.add(Dense(3, input_shape = (1,),  activation = 'tanh'))\n#model_3.add(Dense(3, input_shape = (1,),  activation = 'relu'))\nmodel_3.add(Dense(1))\nmodel_3.compile(optimizer = 'sgd' , loss = 'mean_squared_error')\n# train the model\nhistory = model_3.fit(x, y, epochs = 20000, verbose = 0)\n# Predict the values\ny_pred = model_3.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","a0e7f7f1":"# A feature is an input variable, in this case x\nx=np.linspace(start=-1, stop=1, num=50)\n# A label is the thing we're predicting, in this case y\ny=np.abs(x)\nplt.plot(x,y)","6519ac7d":"model_2 = Sequential()\nmodel_2.add(Dense(3, input_shape = (1,),  activation = 'tanh'))\n#model_2.add(Dense(3, input_shape = (1,),  activation = 'relu'))\nmodel_2.add(Dense(1))\nmodel_2.compile(optimizer = 'sgd' , loss = 'mean_squared_error')\n# train the model\nhistory = model_2.fit(x, y, epochs = 20000, verbose = 0)\n# Predict the values\ny_pred = model_2.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","1f9a6fb2":"# A feature is an input variable, in this case x\nx=np.linspace(start=-1, stop=1, num=50)\ny=np.heaviside(x,0)\nplt.plot(x,y)","5bac9d43":"model_4 = Sequential()\nmodel_4.add(Dense(3, input_shape = (1,),  activation = 'tanh'))\n#model_4.add(Dense(3, input_shape = (1,),  activation = 'relu'))\nmodel_4.add(Dense(1))\nmodel_4.compile(optimizer = 'sgd' , loss = 'mean_squared_error')\n# train the model\nhistory = model_4.fit(x, y, epochs = 20000, verbose = 0)\n# Predict the values\ny_pred = model_4.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","8a8e65b5":"history = model_4.fit(x, y, epochs = 200000, verbose = 0)\n# Predict the values\ny_pred = model_4.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","68585c3b":"# A feature is an input variable, in this case x\nx=np.linspace(start=-1, stop=1, num=50)\n# A label is the thing we're predicting, in this case y\ny=stats.norm.pdf(x, loc=0, scale=1.5) #loc = mean; scale = standard deviation\nplt.plot(x,y)","a2d0537a":"model_5 = Sequential()\nmodel_5.add(Dense(3, input_shape = (1,),  activation = 'tanh'))\n#model_5.add(Dense(3, input_shape = (1,),  activation = 'relu'))\nmodel_5.add(Dense(1))\nmodel_5.compile(optimizer = 'sgd' , loss = 'mean_squared_error')\n# train the model\nhistory = model_5.fit(x, y, epochs = 200000, verbose = 0)\n# Predict the values\ny_pred = model_5.predict(x)\n# Plot the predicted values (red) together with the actual values (blue)\nplt.plot(x,y_pred, 'r',x,y,'b')","01c3e2f7":"# f(x) = Bell curve","d086e799":"# (d) f(x) = H(x) where H(x) is the Heaviside step function","3d079e87":"If you are new to Neural Networks you might want to have a look at my other notebooks:  \nhttps:\/\/www.kaggle.com\/charel\/learn-neural-networks-by-example-mnist-digits  \nhttps:\/\/www.kaggle.com\/charel\/learn-by-example-rnn-lstm-gru-time-series  \nhttps:\/\/www.kaggle.com\/charel\/learn-by-example-reinforcement-learning-with-gym  \n","b1a66ffd":"# Pattern recognition and machine learning, Christopher M. Bischop\nChapter 5 Neural Networks, page 230\/231  \n\n\" Neural networks are therefore said to be universal approximators. For example, a two-layer network with linear outputs can uniformly approximate any continuous function on a compact input domain to arbitrary accuracy provided the network has a sufficiently large number of hidden units.\"  \n\n\" Although such theorems are reassuring, the key problem is how to find suitable parameter values given a set of training data\"  \n\nFigure 5.3 Illustration of the capability of a multilayer perceptron to approximate four different functions comprising (a) f(x) = x^2, (b) f(x) = sin(x), (c), f(x) = |x|, and (d) f(x) = H(x) where H(x) is the Heaviside step function. In each case, N = 50 data points, shown as blue dots, have been sampled uniformly in x over the interval (\u22121, 1) and the corresponding values of f(x) evaluated. These data points are then used to train a twolayer network having **3 hidden units** with \u2018tanh\u2019 activation functions and linear output units. The resulting network functions are shown by the red curves, and the outputs of the three hidden units are shown.\n\nSource: [Pattern recognition and machine learning, Christopher M. Bischop](https:\/\/www.springer.com\/us\/book\/9780387310732)\n","d3f93b91":"#  f(x) = |x|","ba590868":"To conclude, I never realized that with 3 neurons you can approximate all these functions. My results are not yet as perfectly aligned as the pictures in the book, but they are sufficiently close to buy the figure 5.3 illustration of capability.  \nIt begs the question if we need al these bigger and bigger neural network architectures (needing more and more data) to estimate. Interesting food for thought.\n","1c6f2eee":"Next I need to train the network and run the predictions. In this case I am not concerned about overfitting, would like to test how well a network of just  3 neurons can approximate any continuous function on a compact input domain. So no need for a train and validation testset split, or limitations on the amount of epochs.","7200b80d":"That does not yet come close, let's apply some brute force and run 200000 epochs","870b7622":"Welcome to my fourth notebook on Kaggle. I did record my notes so it might help others in their journey to understand Neural Networks by examples. This notebook is my way to contribute back to the Kaggle platform and community.\n\nNoticed that many people (including myself) have the tendency to use more neurons with more layers as the direction to improve the network. So I was always wondering why I need so many neurons\/layers?\nSo when studying chapter 5 of Pattern recognition and machine learning, Christopher M. Bischop I came along an interesting experiment with only 1 hidden layer with 3 neurons which I wanted to try out for myself in this notebook.\n\n","e2d8985e":"# (a) f(x) = x^2","97ea1537":"10 trainable parameters... (compared to a the dense neural network I used in my MNIST motebook with  185,300 parameters)  ","e490c5e8":"# (b) f(x) = sin(x)"}}