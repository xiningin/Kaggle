{"cell_type":{"00ef88c0":"code","5c100c07":"code","54ee0e48":"code","6ba4467f":"code","68529a32":"code","56fea43b":"code","539e5941":"code","3fa4ab4b":"code","80abbe8e":"code","9009643e":"code","c6f6a6a3":"code","b8a42e04":"code","169a10e0":"code","35e2a112":"code","19344a61":"code","7b701932":"code","808d86fe":"code","c828d591":"code","7c5a24ec":"code","675bd778":"code","6da48758":"code","d1223c70":"code","50d0f214":"code","33be413e":"code","e9fcb8ef":"code","7ab6f527":"code","42d54e81":"code","21de92fc":"code","66f8281a":"code","b4ba88fc":"code","fe8310d4":"code","18bd2ee8":"code","33b4cf34":"code","bfbfaae4":"code","11df8776":"code","70ef0100":"code","1095eb5d":"code","9b4abe61":"code","a5012de6":"code","5adcbcfc":"code","d3842418":"code","04bf89e9":"code","a5aea294":"code","ff6786c7":"code","f6732d9b":"code","7a4279d0":"code","ec5fb817":"code","d8850d4b":"code","e988c391":"code","884445df":"code","00abf3c0":"code","ef6bf340":"code","230ab4f9":"markdown","514786db":"markdown","28bed9a8":"markdown","b073ced7":"markdown","61a62eed":"markdown"},"source":{"00ef88c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nimport time\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport gc\nfrom tqdm import tqdm\n\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n# Any results you write to the current directory are saved as output.\n\n# Any results you write to the current directory are saved as output.","5c100c07":"df_train=pd.read_csv(\"..\/input\/trainPrice.csv\")","54ee0e48":"df_train.head()","6ba4467f":"df_train.info()","68529a32":"sch=pd.read_csv(\"..\/input\/Schools.csv\")","56fea43b":"sch.info()","539e5941":"df_test=pd.read_csv(\"..\/input\/testPrice.csv\")","3fa4ab4b":"df_test.info()","80abbe8e":"df_test.head()","9009643e":"subwy=pd.read_csv(\"..\/input\/Subways.csv\")","c6f6a6a3":"subwy.head()","b8a42e04":"subwy.info()","169a10e0":"subm=pd.read_csv(\"..\/input\/submissionPrice.csv\")","35e2a112":"subm.head()","19344a61":"print(df_train.shape,df_test.shape,subm.shape,subwy.shape,sch.shape)","7b701932":"df_train.transaction_real_price.describe()","808d86fe":"plt.figure(figsize=(12, 5))\nplt.hist(df_train.transaction_real_price, bins=200)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('transaction_real_price')\nplt.show()","c828d591":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=df_train.transaction_real_price)\nplt.show()","7c5a24ec":"df_train.tail()","675bd778":"!ls ..\/input\/","6da48758":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)","d1223c70":"miss_per = {}\nfor k, v in dict(df_train.isna().sum(axis=0)).items():\n    if v == 0:\n        continue\n    miss_per[k] = 100 * float(v) \/ len(df_train)\n    \nimport operator \nsorted_x = sorted(miss_per.items(), key=operator.itemgetter(1), reverse=True)\nprint (\"There are \" + str(len(miss_per)) + \" columns with missing values\")\n\nkys = [_[0] for _ in sorted_x][::-1]\nvls = [_[1] for _ in sorted_x][::-1]\ntrace1 = go.Bar(y = kys, orientation=\"h\" , x = vls, marker=dict(color=\"#d6a5ff\"))\nlayout = go.Layout(title=\"Missing Values Percentage\", \n                   xaxis=dict(title=\"Missing Percentage\"), \n                   height=400, margin=dict(l=300, r=300))\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)","50d0f214":"df_train.isnull().values.any()","33be413e":"df_test.isnull().values.any()","e9fcb8ef":"# some out of range int is a good choice\ndf_train.fillna(-999, inplace=True)\ndf_test.fillna(-999, inplace=True)","7ab6f527":"df_train.head()","42d54e81":"df_train.dtypes\n","21de92fc":"df_test.dtypes","66f8281a":"df_train[\"heat_type\"].value_counts()","b4ba88fc":"df_train[\"heat_type\"] = df_train[\"heat_type\"].astype('object')\ndf_train.dtypes","fe8310d4":"df_train=pd.get_dummies(df_train, columns=[\"heat_type\"])\n","18bd2ee8":"df_test=pd.get_dummies(df_test, columns=[\"heat_type\"])\n","33b4cf34":"df_train=pd.get_dummies(df_train, columns=[\"front_door_structure\"])","bfbfaae4":"df_test=pd.get_dummies(df_test, columns=[\"front_door_structure\"])","11df8776":"df_train=pd.get_dummies(df_train, columns=[\"heat_fuel\"])","70ef0100":"df_test=pd.get_dummies(df_test, columns=[\"heat_fuel\"])","1095eb5d":"df_train.head()","9b4abe61":"df_train['elapsed_time'] = ([2019]- df_train['year_of_completion'])\ndf_test['elapsed_time'] = ([2019] - df_test['year_of_completion'])\ndf_train.head()","a5012de6":"df_train=df_train.drop(['key', 'city','transaction_year_month','transaction_date','year_of_completion','address_by_law','room_id'],axis=1)","5adcbcfc":"df_train=df_train.drop(['apartment_id'],axis=1)","d3842418":"df_train.head()","04bf89e9":"df_test=df_test.drop(['city','transaction_year_month','transaction_date','year_of_completion','address_by_law','room_id'],axis=1)","a5aea294":"df_test=df_test.drop(['apartment_id'],axis=1)","ff6786c7":"df_test.head()","f6732d9b":"df_train.shape,df_train.size,df_test.shape,df_test.size","7a4279d0":"df_test.tail()","ec5fb817":"train_X=df_train.drop(['transaction_real_price'],axis=1)","d8850d4b":"test_X=df_test.drop(['transaction_real_price','key'],axis=1)","e988c391":"train_y=df_train['transaction_real_price']","884445df":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\n\n\npred_test = 0\nkf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)\nfor dev_index, val_index in kf.split(df_train):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5.","00abf3c0":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","ef6bf340":"sub = pd.DataFrame()\nsub['key'] = df_test['key']\nsub['transaction_real_price']= pred_test\nsub.to_csv('submission.csv',index=False)","230ab4f9":"Target","514786db":"Let's take a look at the graph of the distribution:","28bed9a8":" not normal-looking distribution","b073ced7":"In this section, let us see if there are any distribution change between train and test sets with respect to year of completion","61a62eed":"Let's look at the \"violin\" version of the same plot."}}