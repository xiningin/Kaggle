{"cell_type":{"4bd9d28b":"code","ea5461c8":"code","84d34164":"code","3614a075":"code","9e58b0f0":"code","1a95c681":"code","9a5cd84a":"code","45a5fc71":"code","fb71aa6b":"code","328a6354":"code","e6b16926":"code","736bbd1f":"code","c6599734":"code","809fb9ee":"code","f2fc8146":"code","0535bd14":"code","d223d304":"code","f2f57c48":"code","19cc6bfb":"code","ca8b0df0":"code","511c75de":"code","c018af0b":"code","dbcc427c":"code","528370da":"markdown","5fedc181":"markdown","d5aa2ad2":"markdown","4bdd99b9":"markdown","2edba30f":"markdown","b47e7f19":"markdown","fae1eb52":"markdown","291443da":"markdown","1ebc67ae":"markdown","cfde4777":"markdown","6fb76b07":"markdown","120fbe0c":"markdown","3cfccb23":"markdown","ea0246cb":"markdown","670cf7cf":"markdown","5b178e55":"markdown","169c47f9":"markdown","5761d369":"markdown","a343b2ba":"markdown","6c559967":"markdown","4e0e5635":"markdown","e247a04a":"markdown","2529eccf":"markdown"},"source":{"4bd9d28b":"#Standard imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#to show the large number of features in this dataset\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ea5461c8":"import h2o\nfrom h2o.automl import H2OAutoML\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n# Number of threads, nthreads = -1, means use all cores on your machine\n# max_mem_size is the maximum memory (in GB) to allocate to H2O\nh2o.init(nthreads = -1, max_mem_size = 16)","84d34164":"#Based on this great kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings      \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n\n    return df, NAlist","3614a075":"# import Dataset\ntrain_identity= pd.read_csv(\"..\/input\/train_identity.csv\", index_col='TransactionID')\ntrain_transaction= pd.read_csv(\"..\/input\/train_transaction.csv\", index_col='TransactionID')\ntest_identity= pd.read_csv(\"..\/input\/test_identity.csv\", index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')","9e58b0f0":"# Creat our train & test dataset\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","1a95c681":"#Delete the imports to free up memory\ndel train_identity,train_transaction,test_identity, test_transaction","9a5cd84a":"train, NAlist = reduce_mem_usage(train)","45a5fc71":"test, NAlist = reduce_mem_usage(test)","fb71aa6b":"train.head()","328a6354":"train.shape","e6b16926":"hf_train = h2o.H2OFrame(train)\nhf_test = h2o.H2OFrame(test)\n#encode the binary repsonse as a factor\nhf_train['isFraud'] = hf_train['isFraud'].asfactor()","736bbd1f":"# Partition data into 70%, 15%, 15% chunks\n# Setting a seed will guarantee reproducibility\n\nsplits = hf_train.split_frame(ratios=[0.7, 0.15], seed=1)  \n\ntrain_x = splits[0]\nvalid = splits[1]\ntest_x = splits[2]","c6599734":"print(train_x.nrow)\nprint(valid.nrow)\nprint(test_x.nrow)","809fb9ee":"y = 'isFraud'\nx = list(hf_train.columns)\nx.remove(y)","f2fc8146":"# Number of CV folds (to generate level-one data for stacking)\nnfolds = 5","0535bd14":"my_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\",\n                                      ntrees=10,\n                                      max_depth=3,\n                                      min_rows=2,\n                                      learn_rate=0.2,\n                                      nfolds=nfolds,\n                                      fold_assignment=\"Modulo\",\n                                      keep_cross_validation_predictions=True,\n                                      seed=1)\nmy_gbm.train(x=x, y=y, training_frame=train_x)","d223d304":"my_rf = H2ORandomForestEstimator(ntrees=100,\n                                 nfolds=nfolds,\n                                 fold_assignment=\"Modulo\",\n                                 keep_cross_validation_predictions=True,\n                                 seed=1)\nmy_rf.train(x=x, y=y, training_frame=train_x)","f2f57c48":"ensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial\",\n                                       base_models=[my_gbm, my_rf])\nensemble.train(x=x, y=y, training_frame=train_x)","19cc6bfb":"stack_test = ensemble.model_performance(test_x)","ca8b0df0":"perf_gbm_test = my_gbm.model_performance(test_x)\nperf_rf_test = my_rf.model_performance(test_x)\nbaselearner_best_auc_test = max(perf_gbm_test.auc(), perf_rf_test.auc())\nstack_auc_test = stack_test.auc()\nprint(\"Best Base-learner Test AUC:  {0}\".format(baselearner_best_auc_test))\nprint(\"Ensemble Test AUC:  {0}\".format(stack_auc_test))","511c75de":"# Generate predictions on test set \npred = ensemble.predict(hf_test)","c018af0b":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission.shape","dbcc427c":"sample_submission['isFraud'] = pred['p1'].as_data_frame().values\nsample_submission.to_csv('h2o_automl_submission_3.csv', index=False)","528370da":"### Converting the pandas dataframe (memory reduced) to H2OFrame","5fedc181":"# Creating a stacked Emsemble Model","d5aa2ad2":"The default number of trees in an H2O Random Forest is 50, here, we have taken ntrees=100 so this RF will be twice as big as the default. Usually increasing the number of trees in an RF will increase performance as well. ","4bdd99b9":"### Memory reduction for Test","2edba30f":"# H2O AutoML tutorial on IEEE-CIS Fruad detection compitition","b47e7f19":"# Data Prep","fae1eb52":"The size of train dataset reduced by almost 65% after the operation (for the sake of simplicity, size reduction logs have not been printed)","291443da":"Notice that split_frame() uses approximate splitting not exact splitting (for efficiency), so these are not exactly 70%, 15% and 15% of the total rows.","1ebc67ae":"We shall first import the data in a pandas dataframe, carry out a memory reduction operation on the dataset (pandas dataframe) and then load it in a H2OFrame.","cfde4777":"# Compare to base learner performance on the test set","6fb76b07":"The size of train dataset reduced by almost 76% after the operation (for the sake of simplicity, size reduction logs have not been printed)","120fbe0c":"Eval ensemble performance on the test data","3cfccb23":"### Memory reduction for train","ea0246cb":"<img src='https:\/\/image.slidesharecdn.com\/joeamsautoml-171107063815\/95\/using-h2o-automl-for-kaggle-competitions-10-638.jpg'><\/img>","670cf7cf":"### Partition data","5b178e55":"Also, encoding the response variable to factor is important bacause otherwise H2O will asssume it as numeric and will train a regression model instead of a classifiaction model.","169c47f9":"# Loading the H2O library and Starting a local H2O cluster (on your machine)","5761d369":"### Train a stacked ensemble using the GBM and RF above","a343b2ba":"### Setting the target and predictor variables\n\nIn H2O, we use y to designate the response variable and x to designate the list of predictor columns.","6c559967":"### Train and cross-validate a RF","4e0e5635":"This kernel acts as a baseline for H2o AutoML stacked ensembles'. One can further build high performance models by adding\/removing base learners, hyperparameter tuning etc.\n\n**Please upvote the kernel if you found it helpful.**","e247a04a":"### The objective of this kernel is to detect fraudulant transactions taking place on a ecommerce site using H2O AutoML\n\n### This kernel helps provide a fair understanding of the H2O's AutoML and it's syntactic nitty gritties. ","2529eccf":"# 1. Generate a 2-model stacked ensemble (GBM + RF)**\n\n### Train and cross-validate a GBM"}}