{"cell_type":{"e714019c":"code","f55a3ac2":"code","e1254cfa":"code","5ea310e2":"code","d0aacb9d":"code","ada6c93b":"code","ab869c88":"code","43f221b7":"code","06b2ccf3":"code","e558b154":"code","0e40cb4e":"code","8f44fd61":"code","3d4f21fc":"code","a4b1b049":"code","13cdcb16":"code","e8c52df8":"code","a863f5a6":"code","5d019bc9":"code","6d0b11ac":"code","7998b874":"code","af1ba590":"code","4e899118":"code","88bead99":"code","05859599":"code","b66739ae":"code","d6c9d69a":"code","caab866b":"code","eace4260":"code","0353f2e7":"code","a91788d4":"code","c2ba6bea":"code","2640afc9":"code","b34e8071":"code","bb172b13":"code","17c3541c":"code","371ab945":"code","498b7b0d":"code","75f51840":"code","020f3f3c":"code","ef4d12d6":"code","58213114":"code","e16d06df":"code","6b652835":"code","2c124fea":"code","4379e495":"code","3922116a":"code","a08b93f1":"code","778b88d1":"code","9c10bb35":"code","f3fcc106":"code","e957e139":"code","63fbd96e":"code","56e3019f":"code","b6fa4326":"markdown","de638e3e":"markdown","67753147":"markdown","e8619a66":"markdown","60b88c37":"markdown","1e29d210":"markdown","2219de36":"markdown","cae5c6aa":"markdown","2481b3c6":"markdown","cd886d57":"markdown","b7ea23ef":"markdown","dbc7b74c":"markdown","c90030b6":"markdown","a4edbb12":"markdown","e148ff9c":"markdown","dc8e7e10":"markdown","937b8acf":"markdown","7ab4a31c":"markdown"},"source":{"e714019c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f55a3ac2":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing","e1254cfa":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","5ea310e2":"train_df.head()","d0aacb9d":"test_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","ada6c93b":"test_df.head()","ab869c88":"train_df.info()","43f221b7":"test_df.info()","06b2ccf3":"plt.style.use('ggplot')\nimport seaborn\nx = train_df.target.value_counts()\n\nseaborn.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","e558b154":"fig,(ax1,ax2) =plt.subplots(1,2,figsize =(10,5))\ntweet_len = train_df[train_df['target'] ==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('Disaster Tweets')\ntweet_len =train_df[train_df['target'] ==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='orange')\nax2.set_title('Normal Tweets')\nfig.suptitle('Words of Tweets')\nplt.show()","0e40cb4e":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nword = train_df[train_df['target'] ==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nseaborn.distplot(word.map(lambda x : np.mean(x)),ax = ax1,color='red')\nax1.set_title('Disaster Tweets')\nword = train_df[train_df['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nseaborn.distplot(word.map(lambda x : np.mean(x)),ax = ax2,color ='blue')\nax2.set_title('Normal Tweets')\nfig.suptitle('Average word length in each tweet')\nplt.show()","8f44fd61":"from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter","3d4f21fc":"def create_corpus(target):\n    corpus =[]\n    \n    for x in train_df[train_df['target'] == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n    ","a4b1b049":"stop=set(stopwords.words('english'))","13cdcb16":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\n\n       ","e8c52df8":"x,y= zip(*top)\nplt.bar(x,y)","a863f5a6":"corpus = create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n","5d019bc9":"x,y = zip(*top)\nplt.bar(x,y)","6d0b11ac":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\npunct = string.punctuation\n\nfor i in corpus:\n    if i in punct:\n        dic[i]+=1\n\nx,y =zip(*dic.items())\nplt.bar(x,y)\n","7998b874":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\npunct = string.punctuation\n\nfor i in corpus:\n    if i in punct:\n        dic[i]+=1\n\nx,y =zip(*dic.items())\nplt.bar(x,y,color ='green')\n","af1ba590":"counter = Counter(corpus)\nmost = counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:50]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","4e899118":"seaborn.barplot(x=y,y=x) #x axis is count and y axis is word\nplt.show()","88bead99":"def get_top_tweet_bigrams(corpus ,n = None):\n    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis =0)\n    \n    words_freq = [(word,sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq,key = lambda x : x[1], reverse = True)\n    \n    return words_freq[:n]\n    \n\n    ","05859599":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams = get_top_tweet_bigrams(train_df['text'])[:10]\n\nx,y = map(list,zip(*top_tweet_bigrams))\n\nseaborn.barplot(x=y,y=x)\nplt.show()\n","b66739ae":"df = pd.concat([train_df,test_df])\ndf.shape","d6c9d69a":"\nimport re\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n","caab866b":"df['text'] = df['text'].apply(lambda x : remove_URL(x))","eace4260":"def remove_HTML(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n    ","0353f2e7":"df['text'] = df['text'].apply(lambda x:remove_HTML(x))","a91788d4":"def remove_emojis(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'',text)\n\n\n    \n                       \n                    ","c2ba6bea":"df['text'] = df['text'].apply(lambda x: remove_emojis(x))","2640afc9":"#Reference\n#https:\/\/stackoverflow.com\/questions\/34293875\/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate","b34e8071":"import string\ndef remove_punc(text):\n    translator=str.maketrans('','',string.punctuation)\n    return text.translate(translator)\n\n\n    ","bb172b13":"df['text'] = df['text'].apply(lambda x: remove_punc(x))","17c3541c":"!pip install pyspellchecker","371ab945":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\n\ndef correct_spelling(text):\n    corrected_text=[]\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n","498b7b0d":"#df['text'] = df['text'].apply(lambda x: correct_spelling(x))","75f51840":"from nltk.tokenize import word_tokenize\nimport gensim\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\n","020f3f3c":"def create_corpus(df):\n    corpus=[]\n    for train_df in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(train_df) if ((word.isalpha() ==1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus\n                                                                      \n    ","ef4d12d6":"corpus = create_corpus(df)","58213114":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","e16d06df":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences ,maxlen = MAX_LEN ,truncating = 'post',padding = 'post')\n","6b652835":"word_index = tokenizer_obj.word_index\nprint('Number of unique words :  ' ,len(word_index))","2c124fea":"num_words = len(word_index) +1\nembedding_matrix = np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec\n","4379e495":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","3922116a":"model = Sequential()\nembedding = Embedding(num_words,100,embeddings_initializer = Constant(embedding_matrix),input_length = MAX_LEN,\n                     trainable = False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64,dropout = 0.2 ,recurrent_dropout = 0.2))\nmodel.add(Dense(1,activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate  =1e-5)\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = optimizer , metrics = ['accuracy'])\n          \n          \n          \n          ","a08b93f1":"model.summary()","778b88d1":"train = tweet_pad[:train_df.shape[0]]\ntest = tweet_pad[train_df.shape[0]:]\n","9c10bb35":"X_train,X_test,y_train,y_test =train_test_split(train,train_df['target'].values,test_size = 0.2)\nprint('Shape of train: ' ,X_train.shape)\nprint('Shape of validation:', X_test.shape)","f3fcc106":"history = model.fit(X_train, y_train , batch_size = 4 ,epochs=15,validation_data = (X_test, y_test),verbose =2)","e957e139":"sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\nsample.head()","63fbd96e":"y_pred = model.predict(test)\ny_pred = np.round(y_pred).astype(int).reshape(3263)\n\nsub = pd.DataFrame({'id': sample['id'].values.tolist(),'target' : y_pred})\n\nsub.to_csv('submission.csv',index = False)\n\n","56e3019f":"sub.head()","b6fa4326":"**Finding the common words used**","de638e3e":"Needed a lot odf data cleaning","67753147":"Checking punctuations in real disaster tweets","e8619a66":"**\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.**","60b88c37":"**Removing HTML tags**","1e29d210":"**Spelling Correction** Installing pyspellchecker to check spelling","2219de36":"**Common stopwords in tweets** \nFirst analysing the tweets with class 0","cae5c6aa":"Checking punctuations in Not disaster tweets","2481b3c6":"**Removing Punctuations**","cd886d57":"**Character count of Tweets**","b7ea23ef":"**Removing Emojis**","dbc7b74c":"Class 0 (  No disaster) is more than Class 1 (Disaster)","c90030b6":"**Average word length in each tweet**","a4edbb12":"**N Gram Analysis**\nGoing to do a bigram (n =2) over the tweets dataset","e148ff9c":"**Analysing the tweets with class 1**","dc8e7e10":"**Removing urls**","937b8acf":"**Data Cleaning**\nTweets have to be cleaned before modelling. Going to do spelling correction,removing pinctuations,removing html tags and removing emojis","7ab4a31c":"**Analysing Punctuations**"}}