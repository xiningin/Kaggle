{"cell_type":{"78622dea":"code","02188106":"code","97b7eff9":"code","cebd2f9f":"code","689cb6cc":"code","1d9a068b":"code","7f5c966b":"code","72d0954d":"code","e9eb9ed6":"code","e1b27e60":"code","c07e75be":"code","1a48cb27":"code","d4d4a941":"code","81219382":"code","d562e615":"code","fdd9be9b":"code","d2643a70":"code","6e2af0b9":"code","3f07410d":"code","d2ef4587":"code","24e983cd":"code","458ea4b6":"code","0ed598f5":"code","4316c262":"code","d8039add":"code","00e6d070":"code","5e022bf9":"markdown","a0290d96":"markdown","e0fd5b05":"markdown","8069bc10":"markdown","245b2bc0":"markdown","625b28a6":"markdown","cb8140ae":"markdown","dc0fd17f":"markdown","23705a2a":"markdown","3811efac":"markdown","fa693f64":"markdown","b670a924":"markdown","64a9e952":"markdown","848e0edd":"markdown","8e93a798":"markdown","e8f4d2b1":"markdown","ad9c57f5":"markdown"},"source":{"78622dea":"#Basic\nimport pandas as pd\nimport numpy as np\nimport emoji\nimport math\nimport os\n\n#NLP\nimport nltk\nfrom collections import Counter\nfrom itertools import combinations, islice\n\n#Graphics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport wordcloud as wc\nfrom matplotlib.patches import Patch\n\n#ML\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","02188106":"#Read the data\ndtypes = {\n    'article_host': 'str', \n    'article_name': 'str', \n    'article_subtitle': 'str', \n    'hashtags': 'object',\n    'img-label': 'str', \n    'img_src': 'str', \n    'linked_profiles': 'object', \n    'links': 'object', \n    'text': 'str', \n    'text_tokenized': 'object', \n    'text_tokenized_filtered': 'object',\n    'text_tokenized_lemmatized': 'object', \n    'page_name': 'category', \n    'page_name_adjusted': 'category',\n    'text_length': 'int32', \n    'wc': 'int32', \n    'sc': 'int32',\n    'sixltr': 'int32'\n}\nposts = pd.read_csv('..\/input\/posts_full.csv', index_col=False, dtype=dtypes, \n                    parse_dates=['timestamp']).drop('Unnamed: 0', axis=1)\nposts.head()","97b7eff9":"#Linked_profiles were written as NaN if empty\nposts['linked_profiles'] = posts.linked_profiles.fillna('[]')\n#Change list columns to lists\nlist_cols = ['hashtags', 'links', 'linked_profiles', 'text_tokenized', \n             'text_tokenized_filtered', 'text_tokenized_lemmatized']\nfor col in list_cols:\n    posts[col] = posts[col].apply(eval)","cebd2f9f":"#Number of emojis in text\nposts['num_emojis'] = posts.text_tokenized.apply(\n    lambda x: len([e for e in x if e in emoji.UNICODE_EMOJI]))\n\n#Whether post has an image\nposts['has_img'] = ~posts.img_src.isnull()\n\n#Domain from article host\ndef get_domain(host):\n    if type(host) != str:\n        return host\n    host = host.replace('http:\/\/', '').replace('https:\/\/', '').split('\/')[0]\n    if '|' in host:\n        host = host.split('|')[0].split('.')[-1].lower()\n    elif '.' in host:\n        host = host.split('.')[-1].lower()\n    else:\n        return np.nan\n    return host if host.isalpha() else np.nan\nposts['article_domain'] = posts.article_host.apply(get_domain).astype('category')\n\n#Fix punc info\npuncs = [('periods', '.'), ('exclamations', '!'), ('questionms', '?'), \n         ('equals', '='), ('dollars', '$')]\nfor name, punc in puncs:\n    posts['percent_' + name] = posts.text_tokenized.apply(\n        lambda words: words.count(punc)) \/ posts.num_tokens\n\n#Percent All Caps\nposts['percent_all_caps'] = posts.text_tokenized.apply(\n    lambda tokens: [token.isupper() for token in tokens].count(True) \/ \n                    len(tokens) if len(tokens) else 0)","689cb6cc":"#Scale 'num_' features by number of words to reduce dependence on how long the text is \nskip_percs = {'num_words', 'num_tokens'}\nfor nc in [n for n in posts.columns if n.startswith('num_') and n not in skip_percs]:\n    percent_column_name = 'percent_' + '_'.join(nc.split('_')[1:])\n    if percent_column_name not in posts.columns:\n        replacement = (posts[nc] \/ posts.num_words).apply(\n            lambda x: x if not math.isinf(x) else 0)\n        posts[percent_column_name] = replacement\n    posts.drop(nc, axis=1, inplace=True)\n\n#Take log of positive, exponential columns [text_length, num_words, n]\nfor c in ['text_length', 'num_words', 'num_tokens']:\n    posts[c + '_log'] = pd.Series(np.log(posts[c])).replace([np.inf, -np.inf], 0)\n    posts.drop(c, axis=1, inplace=True)","1d9a068b":"#Improve look of graph by cutting spines and adding opaque grid\ndef pretty_axis(ax, visible_spines=False, y_grid=True, y_grid_alpha=0.4):\n    for g in ax.spines:\n        ax.spines[g].set_visible(visible_spines)\n    ax.yaxis.grid(y_grid, alpha=y_grid_alpha)","7f5c966b":"#Describe the data\nposts.describe()","72d0954d":"#Plot numerical data with violin plots\ndef plot_comparison_violins(df, ncols=3, figsize=None, filename='Violin Comparisons.png', ax_mod=None):\n    num = df.select_dtypes(['float64', 'int64', 'int16']).fillna(0)\n    num['anti_vax'] = df.anti_vax.apply(\n        lambda x: 'Anti-Vax' if x else 'Normal').astype('category')\n    nrows = int(np.ceil(len(num.columns[:-1]) \/ ncols))\n    blank = np.zeros(num.shape[0])\n    if not figsize:\n        figsize=(21, 6 * nrows)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    for i, col in enumerate(num.columns[:-1]):\n        ax = None\n        if ncols * nrows == 1:\n            ax = axes\n        elif nrows == 1:\n            ax = axes[i % ncols]\n        else:\n            ax = axes[int(i \/ ncols), i % ncols]\n        sns.violinplot(x=blank, y=col, hue='anti_vax', data=num, \n                       split=True, ax=ax, orient='v', legend=False)\n        ax.xaxis.set_visible(False)\n        ax.legend()\n        ax.set_title(col)\n        ax.set_ylabel('')\n        pretty_axis(ax, y_grid_alpha=0.3)\n        if ax_mod:\n            ax_mod(ax)\n    for i in range(len(num.columns) - 1, nrows * ncols):\n        fig.delaxes(axes[int(i \/ ncols), i % ncols])\n    fig.tight_layout()\n    fig.savefig(filename, bbox_inches='tight')\nplot_comparison_violins(posts[[c for c in posts.columns if \n                               c.startswith('percent_') or c == 'anti_vax'][:12]])","e9eb9ed6":"stylistic_columns = ['percent_hashtags', 'percent_linked_profiles', \n                     'percent_links', 'percent_emojis', 'anti_vax']\nax = posts[stylistic_columns].groupby('anti_vax').mean().transpose().plot(\n    kind='bar', color=['b', 'r'], figsize=(12, 4))\npretty_axis(ax, y_grid_alpha=0.3)\nax.set_title('Distribution of Stylistic Features')\nax.legend(handles=[Patch(facecolor='red', label='Anti_Vax'), \n                   Patch(facecolor='blue', label='Normal')])\nplt.setp(ax.xaxis.get_majorticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\nax.set_xticklabels(['Percent Hashtags', 'Percent Linked Profiles', 'Percent Links', 'Percent Emojis']);","e1b27e60":"#Scale numerical data normally\nnumerical_columns = posts.select_dtypes(['float64', 'float32', \n                                         'int64', 'int32', 'int16']).columns\nscaler = StandardScaler()\nscaled_features = pd.DataFrame(scaler.fit_transform(posts[numerical_columns].fillna(0)), \n                               columns=numerical_columns)\nscaled_features = scaled_features.join(\n    posts.select_dtypes(['bool'])).drop('anti_vax', axis=1)\nscaled_features.head()","c07e75be":"#Run PCA-2 on scaled data for plotting\npca = PCA(n_components=2)\ndecomposed = pca.fit_transform(scaled_features)\npca_df = pd.DataFrame(decomposed, columns=['x', 'y'])\npca_df['anti_vax'] = posts.anti_vax.apply(\n    lambda x: 'Anti-vax' if x else 'Normal').astype('category')\n\n#Plot PCA\nplt.figure(figsize=(21, 10), dpi=80)\npath = plt.scatter(pca_df.x, pca_df.y, alpha=0.3, cmap='bwr',\n                   c=pca_df.anti_vax.apply(lambda x: 'red' if \n                                           x == 'Anti-vax' else 'blue'))\npath.axes.set_title('Principle Component Analysis on Two Components');\nplt.legend(handles=[Patch(facecolor='red', edgecolor='r', label='Anti-Vax'), \n                    Patch(facecolor='blue', label='Normal')])\nfor g in path.axes.spines:\n    path.axes.spines[g].set_visible(False)\nplt.axis('off')\nplt.savefig('PCA.png', bbox_inches='tight')","1a48cb27":"corr = scaled_features.corr()\nfig, ax = plt.subplots(figsize=(26, 15))\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, ax=ax, cmap='Spectral', \n            square=True, center=0.0)\nax.set_title('Correlation Heatmap for Numerical Post Data')\nfig.tight_layout()\nfig.savefig('Correlation Heatmap.png', bbox_inches='tight');","d4d4a941":"#Show correlation with Anti-Vax\nfeature_corr = scaled_features.corrwith(posts.anti_vax).abs(\n                    ).sort_values(ascending=False).head(40)\n\n#Generate colors for the bars based upon where the features came from\ncolor_categories = {'text_': 'black', 'num_pos': 'orange', 'num_': 'yellow', \n                    'ttr': 'red', 'sentiment_': 'green', \n                    'percent_': 'maroon', 'readability_': 'lightgreen', \n                    'has_': 'coral', 'other': 'blue'}\ndef generate_categories(col):\n    for cc in color_categories:\n        if col.startswith(cc):\n            return cc\n    return 'other'\ncolumn_cats = [generate_categories(c) for c in feature_corr.index]\ncolumn_colors = [color_categories[c] for c in column_cats]\n\n#Legend for the plot\nhandles = []\nfor c in set(column_cats):\n    handles.append(Patch(facecolor=color_categories[c], \n                         label=c.replace('_', '')))\n\nfig, ax = plt.subplots(figsize=(21, 15))\nfeature_corr.plot.bar(ax=ax, width=0.8, rot=75, color=column_colors)\nax.set_ylim((0.15, 0.34))\nax.tick_params(axis='both', which='major', labelsize=15)\nax.set_title('Feature Correlation with Anti-Vax', fontsize=16)\nax.set_ylabel('Correlation')\npretty_axis(ax)\nax.legend(handles=handles, prop={'size': 20})\nfig.tight_layout()\nfig.savefig('Feature Correlation.png', bbox_inches='tight');","81219382":"plot_comparison_violins(scaled_features[[feature_corr.index[0]]].join(posts.anti_vax), \n                        ncols=1, filename=feature_corr.index[0] + ' Comparison.png', \n                        figsize=(4, 3), ax_mod=lambda ax: ax.legend(loc='center right'))","d562e615":"cov_pca = PCA(n_components=len(scaled_features.columns))\ncov_pca.fit(scaled_features)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\nexplained_var = pd.Series(cov_pca.explained_variance_ratio_, \n                          index=scaled_features.columns).sort_values(ascending=True)\nfig, ax = plt.subplots(figsize=(21, 15))\nfig.tight_layout()\nax = explained_var[explained_var > 0.01].plot.barh(ax=ax)\nax.set_title('Explained Variance by Feature')\npretty_axis(ax, y_grid=False)\nax.xaxis.grid(True, alpha=0.3)\nax.yaxis.grid(False)\nfig.savefig('Explained Variance.png', bbox_inches='tight');","fdd9be9b":"observing_domains = {'au', 'com', 'ca', 'gov', 'edu', 'net', 'blog', \n                     'org', 'ly', 'uk', 'news', 'ie', 'info', 'tv', 'co'}\ndomain_posts = posts.loc[~posts.article_domain.isnull(), \n                         ['anti_vax', 'article_domain']]\ndomain_posts['article_domain'] = domain_posts.article_domain.apply(\n    lambda x: x if x in observing_domains else 'other').astype('category')\n\n#Graph the domains\nfig, ax = plt.subplots(figsize=(21, 10))\ndomain_posts = domain_posts.groupby(['anti_vax']).article_domain.value_counts(\n                                            ).unstack(fill_value=0).transpose()\ndomain_posts['sum'] = domain_posts[True] + domain_posts[False]\ndomain_posts = domain_posts.sort_values(by='sum', \n                                        ascending=False).drop('sum', axis=1)\ndomain_posts.plot(kind='bar', ax=ax, stacked=False, \n                  color=['blue', 'red'], width=0.9, rot=0)\npretty_axis(ax)\nax.set_xlabel('')\nax.legend(handles=[Patch(facecolor='blue', label='Normal'), \n                   Patch(facecolor='red', label='Anti-Vax')])\nax.set_title('Distribution of Domain Suffixes')\nfig.savefig('Domain Distribution.png', bbox_inches='tight')","d2643a70":"#Get bigrams of a group posts\nstop_words = set(nltk.corpus.stopwords.words(\"english\")).union(\n    wc.STOPWORDS).union(observing_domains).union({'http', 'https'})\ndef bigrams(posts, most_common=20):\n    w = posts.text_tokenized_lemmatized.apply(\n        lambda words: [word for word in words if word not in stop_words and not word.isdigit()])\n    def count_bigrams(words):\n        return Counter(zip(words, islice(words, 1, None)))\n    return w.apply(count_bigrams).sum().most_common(most_common)\nsample_size, most_common = 3000, 50\nanti_vax_bigrams = bigrams(posts[posts.anti_vax].sample(n=sample_size), \n                           most_common=most_common)\nnormal_bigrams = bigrams(posts[~posts.anti_vax].sample(n=sample_size), \n                         most_common=most_common)","6e2af0b9":"#Show Anti_Vax Bigrams\nwordcloud = wc.WordCloud(stopwords=stop_words, background_color='white', \n                         width=2100, height=600).fit_words({' '.join(couple): count for \n                                                            couple, count in anti_vax_bigrams})\nfig = plt.figure(figsize=(21, 6))\nplt.imshow(wordcloud)\nplt.axis('off')\nfig.savefig('Anti-Vax Bigram Wordcloud.png', bbox_inches='tight')\nplt.show()","3f07410d":"#Show Normal Bigrams\nwordcloud = wc.WordCloud(stopwords=stop_words, background_color='white', \n                         width=2100, height=600).fit_words({' '.join(couple): count for \n                                                            couple, count in normal_bigrams})\nfig = plt.figure(figsize=(21, 6))\nplt.imshow(wordcloud)\nplt.axis('off')\nfig.savefig('Normal Bigram Wordcloud.png', bbox_inches='tight')\nplt.show()","d2ef4587":"#Get pairs for a group of posts\ndef pairs(posts, most_common=20):\n    w = posts.text_tokenized_lemmatized.apply(\n        lambda words: [word for word in words if \n                       word not in stop_words and not word.isdigit()])\n    def pairs(words):\n        return list(combinations(set(words), 2))\n    counter = Counter()\n    for ind, val in w.apply(pairs).iteritems():\n        counter.update(val)\n    return counter.most_common(most_common)\nsample_size = 30000\nanti_vax_pairs = pairs(posts[posts.anti_vax].sample(n=sample_size), most_common=None)\nnormal_pairs = pairs(posts[~posts.anti_vax].sample(n=sample_size), most_common=None)","24e983cd":"#Clean pairs and convert to a dataframe\npairs = {}\nfor anti in anti_vax_pairs:\n    a, b = anti[0]\n    if b < a:\n        a, b = b, a\n    pairs[(a, b)] = [anti[1], 0]\nfor norm in normal_pairs:\n    a, b = norm[0]\n    if b < a:\n        a, b = b, a\n    if (a, b) not in pairs:\n        pairs[(a, b)] = [0, 0]\n    pairs[(a, b)] = [pairs[(a, b)][0], norm[1]]\npairs = pd.DataFrame([(pair, count[0], count[1]) for pair, count in pairs.items()], columns=['Pair', 'Anti-Vax Count', 'Normal Count'])\npairs['Difference'] = pairs['Anti-Vax Count'] - pairs['Normal Count']\npairs['Abs_Difference'] = pairs.Difference.abs()\npairs.set_index('Pair', inplace=True)\npairs.head()","458ea4b6":"#View Anti-Vax heavy pairs\nfig, axes = plt.subplots(3, 1, figsize=(21, 10))\npair_plots = {\n    'Anti-Vax Dominant Pairs': pairs.sort_values(['Difference', 'Anti-Vax Count'], \n                                                 ascending=False).iloc[:15, 0], \n    'Normal Dominant Pairs': pairs.sort_values(['Difference', 'Normal Count'], \n                                               ascending=[True, False]).iloc[:15, 1],\n    'Common Pairs': pairs[(pairs['Anti-Vax Count'] != 0) & \n                          (pairs['Normal Count'] != 0)].sort_values(['Abs_Difference', \n                                                                     'Normal Count'], \n                                                                    ascending=[True, False]).iloc[:40, :2]\n}\nfor i, ((title, data), color) in enumerate(zip(pair_plots.items(), ['r', 'b', ['r', 'b']])):\n    data.plot(kind='bar', ax=axes[i], color=color)\n    pretty_axis(axes[i])\n    axes[i].set_title(title)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('Count')\n    plt.setp(axes[i].xaxis.get_majorticklabels(), rotation=30, ha=\"right\", \n             rotation_mode=\"anchor\") \nfig.tight_layout()\nfig.savefig('Pairs Comparison.png', bbox_inches='tight');","0ed598f5":"from scipy.stats import ttest_ind\ndef test_significant_scores(df):\n    anti_vax, normal = df[df.anti_vax], df[~df.anti_vax]\n    data = pd.DataFrame({'Feature': df.columns})\n    data['t_value'] = data.Feature.apply(lambda f: ttest_ind(anti_vax[f], normal[f], equal_var=False))\n    data['p_value'] = data.t_value.apply(lambda x: x[1])\n    data['t_value'] = data.t_value.apply(lambda x: x[0])\n    return data\nfeature_tests = test_significant_scores(scaled_features.join(posts.anti_vax))\nfeature_tests['Significant'] = feature_tests.p_value <= 0.05\nfeature_tests.set_index('Feature', inplace=True)\nfeature_tests.to_csv('Significance.csv')\nfeature_tests.head()","4316c262":"feature_tests[feature_tests.p_value > 0.05].sort_values(by='p_value', ascending=False)","d8039add":"feature_tests[feature_tests.p_value <= 0.05].sort_values(by='t_value', ascending=True).head()","00e6d070":"#Add categorical dummy variables for which hashtags were in the text\n#hashtags = set(posts.hashtags.sum())\n#for hashtag in hashtags:\n#    scaled_features['hashtag_' + hashtag] = posts.hashtags.apply(lambda x: hashtag in x)\n#Add categorical dummy variables for the domain of the article linked\n#scaled_features = scaled_features.join(pd.get_dummies(posts.article_domain))\n#TODO: Add Bigram dummies","5e022bf9":" ## PCA on 2 Components (No Encodings)","a0290d96":"### Data Cleaning","e0fd5b05":"# Determine Significant Scores through Independence t-Tests","8069bc10":"## Bigram Analysis","245b2bc0":"## Distribution of Domain Usage","625b28a6":"# Data Visualization","cb8140ae":"### Correlation Heatmap - No Features Removed","dc0fd17f":"### Fix List columns\nColumns stored as lists are read back as strings instead of lists","23705a2a":"## Stylistic Features","3811efac":"# Compile All Features (Future Work)","fa693f64":"## Feature Correlation with Anti-Vax (No Encodings)","b670a924":"### View Feature Violin Plots","64a9e952":"## Feature Variance Analysis","848e0edd":"# Analyzing Facebook Language used by Anti-Vaccine Pages","8e93a798":"## Distribution of Strongest Feature","e8f4d2b1":"## Pair Analysis","ad9c57f5":"### More Features\nAdd additional features not found in dataset"}}