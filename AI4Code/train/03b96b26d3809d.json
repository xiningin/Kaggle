{"cell_type":{"8fc681ce":"code","607f481b":"code","e246e4e1":"code","c93a1952":"code","84f2cbec":"code","26e19715":"code","35cf08e5":"code","f642de8c":"code","594e8684":"code","b1fcfea5":"code","59b069b2":"code","962f97fe":"code","e6ff00c5":"code","98d3706b":"code","09df13b3":"code","82cc991e":"code","46bf67ca":"code","c0efb71a":"code","d92da79c":"code","571fd190":"code","11c894df":"code","42981de9":"code","3aeccbd1":"code","0f454aec":"code","6c314b76":"code","f7e192d4":"code","c69015c2":"code","a2f4b38f":"code","15ba2635":"code","de0592a3":"code","aea1cb15":"code","3f5e5a75":"code","b3eab539":"code","3ac985b0":"code","c3802ce9":"code","9731f9e4":"code","310d37ce":"code","712521a3":"code","23471259":"code","b45e69f4":"code","f521ba86":"code","acab3196":"code","158d1a39":"code","1dc2bcd7":"code","8c33db22":"code","5107b7bb":"markdown","c19cc720":"markdown","6e03cf49":"markdown","53da01f7":"markdown","64f99da1":"markdown","9bf7c520":"markdown","4b696965":"markdown","55f543f9":"markdown","fd8b0d7a":"markdown","312babc7":"markdown","6c2d4a3a":"markdown","1713656b":"markdown","5e80502d":"markdown","1b452b48":"markdown","f097a17c":"markdown"},"source":{"8fc681ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","607f481b":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nimport lightgbm\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e246e4e1":"dataset = pandas.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndataset.head(10)","c93a1952":"dataset.drop(\"enrollee_id\", axis=1, inplace=True)","84f2cbec":"dataset.info()","26e19715":"dataset.describe().T","35cf08e5":"dataset.isnull().sum()","f642de8c":"dataset.isnull().sum().sum()","594e8684":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","b1fcfea5":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","59b069b2":"for c in categorical_list:\n    bar_plot(c)","962f97fe":"numerical_int64 = (dataset.dtypes == \"int64\")\nnumerical_int64_list = list(numerical_int64[numerical_int64].index)\n\nprint(\"Categorical variables:\")\nprint(numerical_int64_list)","e6ff00c5":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","98d3706b":"for n in numerical_int64_list:\n    plot_hist(n)","09df13b3":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","82cc991e":"for n in numerical_float64_list:\n    plot_hist(n)","46bf67ca":"plt.figure(figsize=(50,50))\nj = 0\n\nfor i in categorical_list:\n    colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fbdf70','#ac9fd0','#8b7470']\n    \n    labels = dataset[i].value_counts().index\n    sizes = dataset[i].value_counts().values\n    \n    unique = len(dataset[i].unique())\n    if(unique == 2):\n        myexplode = [0.1, 0]\n    if(unique == 3):\n        myexplode = [0.1, 0,0]\n    if(unique == 4):\n        myexplode = [0.1,0,0,0]\n    \n    plt.subplot(4,3,j+1)\n    plt.pie(sizes, labels=labels, shadow = True, startangle=90, colors=colors, autopct='%1.1f%%',textprops={'fontsize': 12})\n    plt.title(f'Distribution of {i}',color = 'black',fontsize = 25)\n    j += 1","c0efb71a":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('gender = Female')\ndataset.groupby('gender').target.value_counts().loc['Female'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('gender = Male')\ndataset.groupby('gender').target.value_counts().loc['Male'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('gender = Other')\ndataset.groupby('gender').target.value_counts().loc['Other'].plot(kind='bar')","d92da79c":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('relevent_experience = Has relevent experience')\ndataset.groupby('relevent_experience').target.value_counts().loc['Has relevent experience'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('relevent_experience = No relevent experience')\ndataset.groupby('relevent_experience').target.value_counts().loc['No relevent experience'].plot(kind='bar')","571fd190":"plt.figure(figsize=(25,15))\nplt.subplot(2,3,1)\nplt.title('enrolled_university = no_enrollment')\ndataset.groupby('enrolled_university').target.value_counts().loc['no_enrollment'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('enrolled_university = Full time course')\ndataset.groupby('enrolled_university').target.value_counts().loc['Full time course'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('enrolled_university = Part time course')\ndataset.groupby('enrolled_university').target.value_counts().loc['Part time course'].plot(kind='bar')","11c894df":"plt.figure(figsize=(30,15))\nplt.subplot(2,3,1)\nplt.title('education_level = Graduate')\ndataset.groupby('education_level').target.value_counts().loc['Graduate'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('education_level = Masters')\ndataset.groupby('education_level').target.value_counts().loc['Masters'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('education_level = High School')\ndataset.groupby('education_level').target.value_counts().loc['High School'].plot(kind='bar')\n\nplt.subplot(2,3,4)\nplt.title('education_level = Phd')\ndataset.groupby('education_level').target.value_counts().loc['Phd'].plot(kind='bar')\n\nplt.subplot(2,3,5)\nplt.title('education_level = Primary School')\ndataset.groupby('education_level').target.value_counts().loc['Primary School'].plot(kind='bar')","42981de9":"plt.figure(figsize=(30,15))\nplt.subplot(2,3,1)\nplt.title('major_discipline = STEM')\ndataset.groupby('major_discipline').target.value_counts().loc['STEM'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('major_discipline = Business Degree')\ndataset.groupby('major_discipline').target.value_counts().loc['Business Degree'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('major_discipline = Arts')\ndataset.groupby('major_discipline').target.value_counts().loc['Arts'].plot(kind='bar')\n\nplt.subplot(2,3,4)\nplt.title('major_discipline = Humanities')\ndataset.groupby('major_discipline').target.value_counts().loc['Humanities'].plot(kind='bar')\n\nplt.subplot(2,3,5)\nplt.title('major_discipline = No Major')\ndataset.groupby('major_discipline').target.value_counts().loc['No Major'].plot(kind='bar')\n\nplt.subplot(2,3,6)\nplt.title('major_discipline = Other')\ndataset.groupby('major_discipline').target.value_counts().loc['Other'].plot(kind='bar')","3aeccbd1":"plt.figure(figsize=(30,15))\nplt.subplot(2,3,1)\nplt.title('company_type = Pvt Ltd')\ndataset.groupby('company_type').target.value_counts().loc['Pvt Ltd'].plot(kind='bar')\n\nplt.subplot(2,3,2)\nplt.title('company_type = Funded Startup')\ndataset.groupby('company_type').target.value_counts().loc['Funded Startup'].plot(kind='bar')\n\nplt.subplot(2,3,3)\nplt.title('company_type = Early Stage Startup')\ndataset.groupby('company_type').target.value_counts().loc['Early Stage Startup'].plot(kind='bar')\n\nplt.subplot(2,3,4)\nplt.title('company_type = Other')\ndataset.groupby('company_type').target.value_counts().loc['Other'].plot(kind='bar')\n\nplt.subplot(2,3,5)\nplt.title('company_type = Public Sector')\ndataset.groupby('company_type').target.value_counts().loc['Public Sector'].plot(kind='bar')\n\nplt.subplot(2,3,6)\nplt.title('company_type = NGO')\ndataset.groupby('company_type').target.value_counts().loc['NGO'].plot(kind='bar')","0f454aec":"plt.figure(figsize=(30,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['training_hours'], color = 'red', kde = True).set_title('training_hours Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['city_development_index'], color = 'green', kde = True).set_title('city_development_index Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['target'], color = 'blue', kde = True).set_title('target Interval and Counts')","6c314b76":"def missing_values_percentage(feature):\n    missing_values_number = dataset[feature].isnull().sum()\n    if(missing_values_number > 0):\n        print(f'There are {missing_values_number} missing values in \"{feature}\" column.')\n        missing_value_percentage = 100 * missing_values_number \/ len(dataset['gender'])\n        print(f'{missing_value_percentage:.2f}% of the \"{feature}\" feature are missing values')\n        print(\"*****\"*20)","f7e192d4":"features = list(dataset.columns)\n\nfor c in features:\n    missing_values_percentage(c)","c69015c2":"def fill_missings(feature):\n    missing_values_number = dataset[feature].isnull().sum()\n    if(missing_values_number > 0):\n        \n        missing_value_percentage = 100 * missing_values_number \/ len(dataset['gender'])\n\n        if(missing_value_percentage <= 0):\n            #dataset.dropna(subset = [feature], inplace = True)\n            missing_values_number = dataset[feature].isnull().sum()\n            missing_value_percentage = 100 * missing_values_number \/ len(dataset['gender'])\n            print(f'There are {missing_values_number} missing values in \"{feature}\" column.')\n            print(f'{missing_value_percentage:.2f}% of the \"{feature}\" feature are missing values')\n        else:\n            dataset[feature].fillna(method ='bfill', limit = 4, inplace = True)\n            missing_values_number = dataset[feature].isnull().sum()\n            if(missing_values_number > 0):\n                dataset[feature].fillna(method ='ffill', limit = 4, inplace = True)\n                \n                missing_values_number = dataset[feature].isnull().sum()\n                missing_value_percentage = 100 * missing_values_number \/ len(dataset['gender'])\n                print(f'There are {missing_values_number} missing values in \"{feature}\" column.')\n                print(f'{missing_value_percentage:.2f}% of the \"{feature}\" feature are missing values')\n                \n        print(\"*****\"*20)\n        ","a2f4b38f":"for c in features:\n    fill_missings(c)","15ba2635":"dataset.isnull().sum()","de0592a3":"dataset.agg(['skew']).T","aea1cb15":"skews = ['city_development_index', 'training_hours']","3f5e5a75":"from scipy.stats import norm, skew, boxcox\nfor i in skews:\n    sns.set_style('darkgrid')\n    sns.distplot(dataset[i], fit = norm)\n    plt.title('Skeweed')\n    plt.show()\n    (mu, sigma) = norm.fit(dataset[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()\n    \n    dataset[i], lam = boxcox(dataset[i])\n\n    sns.set_style('darkgrid')\n    sns.distplot(dataset[i], fit = norm)\n    plt.title('Transformed')\n    plt.show()\n    (mu, sigma) = norm.fit(dataset[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()","b3eab539":"dataset.agg(['skew']).T","3ac985b0":"cat_var =  ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level', \n            'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\n\nfor i in range (0, len(cat_var)):\n    print(f'Unique Values for {cat_var[i]}', dataset[f'{cat_var[i]}'].unique())","c3802ce9":"onehotencoder = OneHotEncoder()","9731f9e4":"one_hot = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level', \n            'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\n\nfor i in range(0, len(one_hot)):\n    dataset[f'{one_hot[i]}'] = pd.Categorical(dataset[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(dataset[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded')\n    dataset.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    dataset = pd.concat([dataset, dummies], axis=1)","310d37ce":"dataset","712521a3":"X = dataset.drop([\"target\"],axis =1)\ny = dataset[\"target\"]","23471259":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","b45e69f4":"pipeline_GaussianNB = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_GaussianNB\",GaussianNB())])\n\npipeline_BernoulliNB = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_BernoulliNB\",BernoulliNB())])\n\npipeline_LogisticRegression = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_LogisticRegression\",LogisticRegression())])\n\npipeline_RandomForest = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_RandomForest\",RandomForestClassifier())])\n\npipeline_DecisionTree = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_DecisionTree\",DecisionTreeClassifier())])\n\npipeline_KNN = Pipeline([(\"scaler\",StandardScaler()),\n                     (\"pipeline_KNN\",KNeighborsClassifier())])\n\npipeline_GBC = Pipeline([(\"scaler\",StandardScaler()), (\n                        \"pipeline_GBC\",GradientBoostingClassifier())])\n\npipeline_SGD = Pipeline([(\"scaler\",StandardScaler()), \n                        (\"pipeline_SGD\",SGDClassifier(max_iter=5000, random_state=0))])\n\npipeline_LGBM = Pipeline([(\"scaler\",StandardScaler()), \n                        (\"pipeline_NN\",lightgbm.LGBMClassifier())])\n\npipelines = [pipeline_GaussianNB, pipeline_BernoulliNB, pipeline_LogisticRegression, pipeline_RandomForest, pipeline_DecisionTree, pipeline_KNN, pipeline_GBC, pipeline_SGD, pipeline_LGBM]\n\npipe_dict = {0: \"GaussianNB\", 1: \"BernoulliNB\", 2: \"LogisticRegression\",3: \"RandomForestClassifier\", 4: \"DecisionTreeClassifier\",\n            5: \"KNeighborsClassifier\", 6: \"GradientBoostingClassifier\", 7:\"Stochastic Gradient Descent\", 8: \"LGBM\"}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'LGBM']\n\ni= 0\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)\n    print(f'{pipe_dict[i]}')\n    print(\"Train Score of %s: %f     \" % (pipe_dict[i], pipe.score(X_train, y_train)*100))\n    trainScores.append(pipe.score(X_train, y_train)*100)\n    \n    print(\"Validation Score of %s: %f\" % (pipe_dict[i], pipe.score(X_valid, y_valid)*100))\n    validationScores.append(pipe.score(X_valid, y_valid)*100)\n    \n    print(\"Test Score of %s: %f      \" % (pipe_dict[i], pipe.score(X_test, y_test)*100))\n    testScores.append(pipe.score(X_test, y_test)*100)\n    print(\" \")\n    \n    y_predictions = pipe.predict(X_test)\n    conf_matrix = confusion_matrix(y_predictions, y_test)\n    print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n    \n    tn = conf_matrix[0,0]\n    fp = conf_matrix[0,1]\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n\n    total = tn + fp + tp + fn\n    real_positive = tp + fn\n    real_negative = tn + fp\n\n    accuracy  = (tp + tn) \/ total # Accuracy Rate\n    precision = tp \/ (tp + fp) # Positive Predictive Value\n    recall    = tp \/ (tp + fn) # True Positive Rate\n    f1score  = 2 * precision * recall \/ (precision + recall)\n    specificity = tn \/ (tn + fp) # True Negative Rate\n    error_rate = (fp + fn) \/ total # Missclassification Rate\n    prevalence = real_positive \/ total\n    miss_rate = fn \/ real_positive # False Negative Rate\n    fall_out = fp \/ real_negative # False Positive Rate\n    \n    print('Evaluation Metrics:')\n    print(f'Accuracy    : {accuracy}')\n    print(f'Precision   : {precision}')\n    print(f'Recall      : {recall}')\n    print(f'F1 score    : {f1score}')\n    print(f'Specificity : {specificity}')\n    print(f'Error Rate  : {error_rate}')\n    print(f'Prevalence  : {prevalence}')\n    print(f'Miss Rate   : {miss_rate}')\n    print(f'Fall Out    : {fall_out}')\n\n    print(\"\") \n    print(f'Classification Report: \\n{classification_report(y_predictions, y_test)}\\n')\n    print(\"\")\n\n    print(\"*****\"*20)\n    i +=1","f521ba86":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'LGBM']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","acab3196":"table = pd.DataFrame({'Model': modelNames, 'Train': trainScores, 'Validation': validationScores, 'Test': testScores})\ntable","158d1a39":"cv_results_acc = []\n\nfor i, model in enumerate(pipelines):\n    cv_score = cross_val_score(model, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    cv_results_acc.append(cv_score.mean()*100)\n    print(\"%s: %f\" % (pipe_dict[i], cv_score.mean()*100))","1dc2bcd7":"table_cv = pd.DataFrame({'Model': modelNames, 'CV Score': cv_results_acc})\ntable_cv","8c33db22":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('CV Scores Means', fontweight='bold', size = 24)\n\nbarWidth = 0.5\n \nbars2 = cv_results_acc\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\n \nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='black',  yerr=0.5,ecolor=\"black\",capsize=10)\n\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Light GBM']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","5107b7bb":"# Conclusion\n\nI made Visualization and Machine Learning Classification on this notebook. \n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\nThank you for your time.","c19cc720":"# Skewness","6e03cf49":"# HR Analytics - Classification Algorithms Pipeline\n\nIn this notebook, I examined HR Analytics: Job Change of Data Scientist dataset. Firstly, I visualized the data, then filled missing values and I handled skewness problem. I applied One-Hot Encoding to some features. After train-test split, I applied 9 classification algorithms to this dataset. You will find out train-validation-test scores, confusion matrixes, evaluation metrics and classification reports belongs to every algorithm. Also, I applied Cross Validation to all algorithms. I visualized the results.\n\n## If you have questions please ask them on the comment section.\n\n## I will be glad if you can give feedback.","53da01f7":"# Basic Data Analysis","64f99da1":"# Pipelines","9bf7c520":"# Missing Values","4b696965":"# Read Datas & Explanation of Features & Information About Datasets","55f543f9":"## Variable Descriptions\n\n","fd8b0d7a":"## Cross Validation","312babc7":"#### Numerical Variables","6c2d4a3a":"# Train-Test Split","1713656b":"# One-Hot Encoding","5e80502d":"### Univariate Variable Analysis\n\n* Categorical Variables: 'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n\n* Numerical Variables: 'id', 'hypertension', 'heart_disease', 'stroke'","1b452b48":"#### Categorical Variables","f097a17c":"# Importing the Necessary Libraries"}}