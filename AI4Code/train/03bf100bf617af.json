{"cell_type":{"859eecfa":"code","aae72ea3":"code","5f117feb":"code","031c2b3a":"code","b38a1ed1":"code","1f7ccea9":"code","0d808fdb":"code","72656c4d":"code","89002cdd":"code","a335f582":"code","d610f315":"code","1d3b70a6":"code","9e1260cc":"code","49b1db12":"code","0e5f304e":"code","b1872a30":"code","1287d75a":"code","e4846176":"code","f618526b":"code","6a90351e":"code","1178eba6":"code","3b591817":"code","b4070543":"code","a8870ef4":"code","7993be32":"code","8606adf0":"code","b6926078":"code","07945fa3":"code","05f06695":"code","1209a541":"code","d5bc0f94":"code","ac4dd77c":"code","31bebbb3":"code","63c8a0f8":"code","dbe77785":"code","be446376":"code","0633ba50":"code","97938e45":"code","ce4a259c":"code","d7488e11":"code","fe7b75ad":"code","06c6454c":"code","3cbc4120":"code","23917a30":"code","1509725b":"code","c1fff0d7":"code","b995909a":"code","66976d38":"code","d0f88916":"code","8dbc8d73":"code","bda910a0":"code","4b4b5e6c":"code","94228819":"markdown","fd57af30":"markdown","cc359463":"markdown","ad4ccf4b":"markdown","38364683":"markdown"},"source":{"859eecfa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aae72ea3":"df = pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\",index_col = 0)\ndf.head()","5f117feb":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,SimpleRNN,Flatten\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nimport scikitplot as skplt\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer \nfrom nltk.stem import PorterStemmer, LancasterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport matplotlib.pyplot as plt \nfrom textblob import TextBlob\nimport nltk\nimport warnings\nwarnings.filterwarnings('ignore') ","031c2b3a":"# only take the review column as predictor and recommended IND as target in the dataset \n# rename the predictor and target\ndata = df[[\"Title\",\"Review Text\",\"Recommended IND\"]]\ndata = data.rename(columns = {\"Review Text\":\"text\",\"Recommended IND\":\"sentiment\"})\ndata.head()","b38a1ed1":"# calculate the null values in the dataset\ndata.text.isna().sum()","1f7ccea9":"# the null values only occupies a very small proportion thus we can directly delete them\ndata = data[~data.text.isna()]","0d808fdb":"print(data.sentiment.isna().sum(),data.text.isna().sum())\n# now no na values","72656c4d":"# let's check whether the sentiment columns contain other values or not\ndata.sentiment.unique()\n# Great, no other values, just binary result","89002cdd":"def count_exclamation_mark(string_text):\n    count = 0\n    for char in string_text:\n        if char == '!':\n            count += 1\n    return count","a335f582":"# calculate the ! number in text\ndata['count_exc'] = data['text'].apply(count_exclamation_mark)\ndata.head(5)","d610f315":"\n# transfer all the text into lower case\ndata['text'] = data['text'].str.lower()\n# clear all the non-related notation\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata.head(6)","1d3b70a6":"# generate new feature, the length of text\ndata['text_length'] = data['text'].apply(len)\ndata.head()","9e1260cc":"# view the distribution of the target variable\nprint(len(data[data.sentiment == 1]))\nprint(len(data[data.sentiment == 0 ]))","49b1db12":"# generate new feature, the polarity of the one review text\ndata['Polarity'] = data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndata.head(5)","0e5f304e":"# manully set the stop words in this situation\nstop_words = list(set(stopwords.words('english')))\nclothes_list =['dress','sweater','shirt',\n               'skirt','material', 'white', 'black',\n              'jeans', 'fabric', 'color','order', 'wear']\n\nfor i in clothes_list:\n    stop_words.append(i)\n","b1872a30":"def stopwords_removal(messy_str):\n    messy_str = word_tokenize(messy_str)\n    return [word.lower() for word in messy_str \n            if word.lower() not in stop_words ]","1287d75a":"# remove all the words which are in the stop word list\ndata['text'] = data['text'].apply(stopwords_removal)\ndata.head()","e4846176":"# stemming transformation of text\nporter = PorterStemmer()\ndef stem_update(text_list):\n    text_list_new = []\n    for word in text_list:\n        word = porter.stem(word)\n        text_list_new.append(word) \n    return text_list_new","f618526b":"data['text'] = data['text'].apply(stem_update)\ndata['text'].head()","6a90351e":"data['text'] = data['text'].apply(lambda x:' '.join(x))\ndata['text'].head()","1178eba6":"# create word cloud\npos_df = data[data.sentiment== 1]\nneg_df = data[data.sentiment== 0]\npos_df.head(3)","3b591817":"pos_words =[]\nneg_words = []\n\nfor review in pos_df.text:\n    pos_words.append(review) \npos_words = ' '.join(pos_words)\npos_words[:60]\n\nfor review in neg_df.text:\n    neg_words.append(review)\nneg_words = ' '.join(neg_words)\nneg_words[:200]","b4070543":"# word cloud for positive word\nwordcloud = WordCloud().generate(pos_words)\n\nwordcloud = WordCloud(background_color=\"white\",max_words=len(pos_words),\\\n                      max_font_size=40, relative_scaling=.5, colormap='summer').generate(pos_words)\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","a8870ef4":"# word cloud for negative word\nwordcloud = WordCloud().generate(neg_words)\n\nwordcloud = WordCloud(background_color=\"white\",max_words=len(neg_words),\\\n                      max_font_size=40, relative_scaling=.5, colormap='gist_heat').generate(neg_words)\nplt.figure(figsize=(13,13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","7993be32":"# tokenlizing(vectorizing) the text, which transforms the data into tensor format\nsamples = data[\"text\"].tolist()\nmaxlen = 100\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)#transfer string into number\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nX = pad_sequences(sequences, maxlen=maxlen)","8606adf0":"# generate the target label\nlabels =  pd.get_dummies(data['sentiment']).values\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', labels.shape)","b6926078":"# generate the random dataset\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata2 = X[indices]\nlabels = labels[indices]","07945fa3":"data2.shape# contains all the text information","05f06695":"a = data[['Polarity','text_length']].values #contains the created features information\na.shape","1209a541":"# the train,val,test is the data about text feature and train2,val2,test2 is the data about two creating new features,finailly we merge them together \ntraining_samples = 11320\nvalidation_samples = 15848\nx_train = data2[:training_samples]\nx_train2 = a[:training_samples]\ny_train = labels[:training_samples]\nx_val = data2[training_samples: validation_samples] \nx_val2 = a[training_samples: validation_samples] \ny_val = labels[training_samples: validation_samples]\nx_test = data2[validation_samples:]\nx_test2 = a[validation_samples:]\ny_test = labels[validation_samples:]\n# for text feature, we still need following preprocessing step\nx_train = pad_sequences(x_train, maxlen=maxlen)\nx_val = pad_sequences(x_val, maxlen=maxlen)\n\n","d5bc0f94":"# concat all the features\nx_train = np.hstack((x_train2,x_train))\nx_val = np.hstack((x_val2,x_val))\nx_test = np.hstack((x_test2,x_test))","ac4dd77c":"# This is the baseline, cause in the dataset, lable 1 occupies 82% percent\n(np.sum(data['sentiment'] == 1)\/data.shape[0]) * 100","31bebbb3":"x_train.shape","63c8a0f8":"\n# First, let's build the simple embedding model\ndef build_model():\n    model = Sequential()\n    model.add(Embedding(max_words, 102, input_length=maxlen+2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    return model\n\n","dbe77785":"model = build_model()\nmodel.summary()\nhistory = model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","be446376":"\n# First, let's build the simple NN model. Considering that the word vector will be a sparse matrix thus we add one embedding layer\ndef build_model():\n    \n    model = Sequential()\n    model.add(Embedding(max_words, 102,input_length=maxlen+2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    return model\n","0633ba50":"model = build_model()\nmodel.summary()\nhistory = model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","97938e45":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n#  training loss keep decreasing while val loss keep increasing , overfitting","ce4a259c":"model.evaluate(x_test, y_test)","d7488e11":"# recursion NN is a classic method to process text problem\ndef build_RNN():\n    model = Sequential() \n    model.add(Embedding(max_words, 102, input_length=maxlen+2)) \n    model.add(Dropout(0.3))\n    model.add(SimpleRNN(32)) \n    model.add(Dropout(0.3))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) \n    return model","fe7b75ad":"RNN_model = build_RNN()\nRNN_model.summary()\nhistory_RNN = RNN_model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","06c6454c":"acc = history_RNN.history['acc']\nval_acc = history_RNN.history['val_acc']\nloss = history_RNN.history['loss']\nval_loss = history_RNN.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n# training loss keeps decreasing while val loss keeps decreasing,overfitting","3cbc4120":"RNN_model.evaluate(x_test, y_test)","23917a30":"# RNN and embedding model both exist some problems, let's try LSTM, another advanced version of RNN\ndef build_LSTM():\n    embed_dim = 128\n    lstm_out = 196\n    max_features = 2000\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim,input_length = x_train.shape[1]))\n    model.add(SpatialDropout1D(0.4))\n    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(2,activation='softmax'))\n    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    return model\n","1509725b":"LSTM_model = build_LSTM()\nLSTM_model.summary()\nhistory_LSTM = LSTM_model.fit(x_train, y_train,\n                    epochs=7,\n                    batch_size=64,\n                    validation_data=(x_val, y_val))","c1fff0d7":"acc = history_LSTM.history['accuracy']\nval_acc = history_LSTM.history['val_accuracy']\nloss = history_LSTM.history['loss']\nval_loss = history_LSTM.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n# training loss decreases and val loss flucates,better than previous two ","b995909a":"LSTM_model.evaluate(x_test, y_test)","66976d38":"# Because our dataset is not balanced, thus we use another metrics---tpr and tnr to evalute our model","d0f88916":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"Embedding model's ablity to identify the positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct\/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_count*100, \"%\")","8dbc8d73":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = RNN_model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"RNN's ablity to identify the positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct\/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_count*100, \"%\")","bda910a0":"pos_count, neg_count, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(x_test)):\n    \n    result = LSTM_model.predict(x_test[x].reshape(1,x_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(y_test[x]):\n        if np.argmax(y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(y_test[x]) == 0:\n        neg_count += 1\n    else:\n        pos_count += 1\n\n\nprint(\"LSTM ablity to identify positive samples and negative samples\")\nprint(\"pos_acc\", pos_correct\/pos_count*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_count*100, \"%\")\n","4b4b5e6c":"# let's try some interest samples\nreview_sample_1 = 'the poor quality and size is not suitable! '\nreview_sample_2 = 'Oh! nice experience'\nreview_sample_3 = 'ehh...OK OK, price is cheap! quality is also\"cheap\"'\nreview_sample_4 = 'good! very good! everything is good! Only one thing is not very great:what I buy is a shirt but get a pant'\ndef get_result(review):\n    print(review)\n    #vectorizing the review by the pre-fitted tokenizer instance\n    length = np.array(len(review))\n    polarity = np.array(TextBlob(review).sentiment.polarity)\n    length = length.reshape(1,-1)\n    polarity = polarity.reshape(1,-1)\n    rw = tokenizer.texts_to_sequences([review])\n    #padding the review to have exactly the same shape as `embedding_2` input\n    rw = pad_sequences(rw, maxlen=100, dtype='int32', value=0)\n    rw = np.hstack((rw,length,polarity))\n    sentiment = LSTM_model.predict(rw,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n        print(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n        print(\"positive\")\nfor i in [review_sample_1,review_sample_2,review_sample_3,review_sample_4]:\n    get_result(i)","94228819":"Group member:\nYifei Zhou\nYinghong Xu\nQian Qiao\nDaniel Saunders","fd57af30":"we can clearly see that LSTM has better loss performance, test accuracy and more powerful ability to identify the negative samples\nthus our final model is LSTM","cc359463":"2.feature preprocessing","ad4ccf4b":"1. *load data and packages","38364683":"we split the dataset into three parts, training ,validation , test, the ratio is : 75% :25 :25%"}}