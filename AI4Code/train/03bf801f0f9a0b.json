{"cell_type":{"e80df101":"code","12fb40c0":"code","f057ecb8":"code","a2fc1dd3":"code","a7e57dc9":"code","e340cad2":"code","a209d09f":"code","55e9dca6":"code","218771b3":"code","98d7cb96":"code","d4e459a6":"code","416e6f37":"code","5f444ac0":"code","9057b0c2":"code","3bab2aae":"code","97540b80":"code","cf76845d":"code","78ea2f4c":"code","7cd83c4e":"code","3fcdcae1":"code","51a14088":"code","101a7eb3":"code","3a752bf1":"code","e4270ae2":"code","83860c98":"code","2ee04dbc":"code","5e5e11bc":"code","8198c214":"code","16f4725c":"code","7716e85e":"code","a9b4272d":"code","580b1750":"code","ada4bd5d":"code","30ea19f0":"code","38a03436":"code","6f6a3fe9":"code","651709be":"code","58dc2943":"code","956246ef":"code","225d17b7":"code","b5a0b8ec":"code","6bf84fe2":"code","e9ffb624":"code","732e0308":"code","e337a71d":"code","8b87889d":"code","d8336169":"code","c3514b79":"code","9e0c7313":"code","f4259179":"code","d69572f8":"code","826db9e9":"code","ebabf900":"code","72fa708f":"code","ad8fdc19":"code","74497c34":"code","a055436c":"code","ee9f902b":"code","aef74e1a":"code","a8f1ea0b":"code","26ba665f":"code","711a7e00":"code","b133fa43":"code","29958b81":"code","6bdd2d43":"code","d6265728":"code","072ec490":"code","00972376":"code","c4e9935a":"code","9f71914b":"code","fe62e11a":"code","c61e8f8a":"code","b9dc3b4d":"code","b5bdb4d9":"code","a6470886":"code","3589b78e":"code","723dd4ba":"code","84606b1c":"code","b8732c8f":"code","00d8c2f5":"code","3fb9504f":"code","bb6a7a70":"code","1c6c4a4b":"code","e794807e":"code","34950d6f":"code","cbb7f3b7":"code","c51c2cea":"code","ff88b1fd":"code","15b9455a":"code","b45ab902":"code","75465823":"code","7ce7f94d":"code","1c9283cd":"code","ae81547a":"code","ad034567":"code","1e670fa5":"code","ffee8129":"code","5924b1eb":"code","e04faa68":"code","664d376d":"code","af982c57":"code","53304d14":"code","a2f9f49f":"code","4ce3906e":"code","80785413":"code","7239357e":"code","74d528f0":"code","f847d0d2":"code","be42941b":"code","a673567e":"code","c14e2ad4":"code","56c72e2d":"code","317a57fe":"code","8aee51c5":"code","648bce8c":"code","48c433fa":"code","1c6b1729":"code","09923780":"code","39e74afc":"code","02d5af75":"code","481a192c":"code","56d070ad":"code","e606026b":"code","8915f138":"code","baccac81":"code","0dbe4856":"code","cf697b60":"code","c57e72e5":"code","20d086a5":"code","bf794f29":"code","3844cbf0":"code","6128ad6b":"markdown","b4056e7a":"markdown","19f5e961":"markdown","b0003ec5":"markdown","874a48f5":"markdown","bb623cbe":"markdown","fbfe4088":"markdown","19f4f6fd":"markdown","722975d1":"markdown"},"source":{"e80df101":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","12fb40c0":"!pip install \"..\/input\/kerasswa\/keras-swa-0.1.2\"  > \/dev\/null","f057ecb8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport gc\nimport glob\nimport os\nimport sys\nimport string\nimport random\nfrom tqdm import tqdm_notebook\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder, minmax_scale, MultiLabelBinarizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom scipy.stats import spearmanr, rankdata\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1 = CPU only\n\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim import utils\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers\n\n\nimport tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)\n\nimport tensorflow_hub as hub\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom keras import Model\nfrom swa.keras import SWA\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport pickle    \n\ndef save_obj(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\ndef load_obj(name ):\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)                \n","a2fc1dd3":"#INPUT_PATH=\"\/kaggle\/input\/\"\nINPUT_PATH=\"..\/input\/\"\ntrain = pd.read_csv(INPUT_PATH+'google-quest-challenge\/train.csv')\ntest = pd.read_csv(INPUT_PATH+'google-quest-challenge\/test.csv')\nsubmission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')","a7e57dc9":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title','question_body','answer']","e340cad2":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","a209d09f":"train = clean_data(train, input_columns)\ntest = clean_data(test, input_columns)","55e9dca6":"train.head(3)","218771b3":"train['question_body'][0]","98d7cb96":"from transformers import BertTokenizer, AdamW, BertModel, BertForSequenceClassification,BertPreTrainedModel, BertConfig \nfrom transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n\nfrom transformers.optimization import get_linear_schedule_with_warmup","d4e459a6":"# word count in title, body and answer\nfor colname in ['question_title', 'question_body', 'answer']:\n    newname = colname + '_word_len'\n    \n    train[newname] = train[colname].str.split().str.len()\n    test[newname] = test[colname].str.split().str.len()\n\n    \ndel newname, colname","416e6f37":"for colname in ['question', 'answer']:\n\n    # check for nonames, i.e. users with logins like user12389\n    train['is_'+colname+'_no_name_user'] = train[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    test['is_'+colname+'_no_name_user'] = test[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    \n\ncolname = 'answer'\n# check lexical diversity (unique words count vs total )\ntrain[colname+'_div'] = train[colname].apply(lambda s: len(set(s.split())) \/ len(s.split()) )\ntest[colname+'_div'] = test[colname].apply(lambda s: len(set(s.split())) \/ len(s.split()) )","5f444ac0":"train.head(2)","9057b0c2":"## domain components\ntrain['domcom'] = train['question_user_page'].apply(lambda s: s.split(':\/\/')[1].split('\/')[0].split('.'))\ntest['domcom'] = test['question_user_page'].apply(lambda s: s.split(':\/\/')[1].split('\/')[0].split('.'))\n\n# count components\ntrain['dom_cnt'] = train['domcom'].apply(lambda s: len(s))\ntest['dom_cnt'] = test['domcom'].apply(lambda s: len(s))\n\n# extend length\ntrain['domcom'] = train['domcom'].apply(lambda s: s + ['none', 'none'])\ntest['domcom'] = test['domcom'].apply(lambda s: s + ['none', 'none'])\n\n# components\nfor ii in range(0,4):\n    train['dom_'+str(ii)] = train['domcom'].apply(lambda s: s[ii])\n    test['dom_'+str(ii)] = test['domcom'].apply(lambda s: s[ii])\n    \n# clean up\ntrain.drop('domcom', axis = 1, inplace = True)\ntest.drop('domcom', axis = 1, inplace = True)","3bab2aae":"import nltk\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\n# shared elements\ntrain['q_words'] = train['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\ntrain['a_words'] = train['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\ntrain['qa_word_overlap'] = train.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\ntrain['qa_word_overlap_norm1'] = train.apply(lambda s: s['qa_word_overlap']\/(1 + len(s['a_words'])), axis = 1)\ntrain['qa_word_overlap_norm2'] = train.apply(lambda s: s['qa_word_overlap']\/(1 + len(s['q_words'])), axis = 1)\ntrain.drop(['q_words', 'a_words'], axis = 1, inplace = True)\n\ntest['q_words'] = test['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\ntest['a_words'] = test['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\ntest['qa_word_overlap'] = test.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\ntest['qa_word_overlap_norm1'] = test.apply(lambda s: s['qa_word_overlap']\/(1 + len(s['a_words'])), axis = 1)\ntest['qa_word_overlap_norm2'] = test.apply(lambda s: s['qa_word_overlap']\/(1 + len(s['q_words'])), axis = 1)\ntest.drop(['q_words', 'a_words'], axis = 1, inplace = True)","97540b80":"## Number of characters in the text ##\ntrain[\"question_title_num_chars\"] = train[\"question_title\"].apply(lambda x: len(str(x)))\ntest[\"question_title_num_chars\"] = test[\"question_title\"].apply(lambda x: len(str(x)))\ntrain[\"question_body_num_chars\"] = train[\"question_body\"].apply(lambda x: len(str(x)))\ntest[\"question_body_num_chars\"] = test[\"question_body\"].apply(lambda x: len(str(x)))\ntrain[\"answer_num_chars\"] = train[\"answer\"].apply(lambda x: len(str(x)))\ntest[\"answer_num_chars\"] = test[\"answer\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"question_title_num_stopwords\"] = train[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"question_title_num_stopwords\"] = test[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"question_body_num_stopwords\"] = train[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"question_body_num_stopwords\"] = test[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"answer_num_stopwords\"] = train[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"answer_num_stopwords\"] = test[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain[\"question_title_num_punctuations\"] =train['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"question_title_num_punctuations\"] =test['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain[\"question_body_num_punctuations\"] =train['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"question_body_num_punctuations\"] =test['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain[\"answer_num_punctuations\"] =train['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"answer_num_punctuations\"] =test['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"question_title_num_words_upper\"] = train[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"question_title_num_words_upper\"] = test[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"question_body_num_words_upper\"] = train[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"question_body_num_words_upper\"] = test[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"answer_num_words_upper\"] = train[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"answer_num_words_upper\"] = test[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))","cf76845d":"list(train.columns)","78ea2f4c":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","7cd83c4e":"train.head(4)","3fcdcae1":"cont_cols = [\n'question_title_word_len',\n'question_body_word_len',\n'answer_div',\n'answer_word_len',\n'qa_word_overlap',\n 'qa_word_overlap_norm1',\n 'qa_word_overlap_norm2',\n 'question_title_num_chars',\n 'question_body_num_chars',\n 'answer_num_chars',\n 'question_title_num_punctuations',\n 'question_body_num_punctuations',\n 'answer_num_punctuations',\n 'question_title_num_stopwords',\n 'question_body_num_stopwords',\n 'answer_num_stopwords',\n 'question_title_num_words_upper',\n 'question_body_num_words_upper',\n 'answer_num_words_upper'\n \n   \n]\n\ncat_cols = [\n 'is_question_no_name_user',\n 'is_answer_no_name_user',    \n'dom_cnt',\n 'dom_0',\n 'dom_1',\n 'dom_2',\n 'dom_3' \n       \n]","51a14088":"train.shape[1] == len(cat_cols+cont_cols+targets+input_columns)+8","101a7eb3":"def constructLabeledSentences(data):\n    sentences=[]\n    for index, row in data.iteritems():\n        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n    return sentences\n\ndef textClean(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n    \ndef cleanup(text):\n    text = textClean(text)\n    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n    return text\n\ntrain_question_body_sentences = constructLabeledSentences(train['question_body'])\ntrain_question_title_sentences = constructLabeledSentences(train['question_title'])\ntrain_answer_sentences = constructLabeledSentences(train['answer'])\n\ntest_question_body_sentences = constructLabeledSentences(test['question_body'])\ntest_question_title_sentences = constructLabeledSentences(test['question_title'])\ntest_answer_sentences = constructLabeledSentences(test['answer'])","3a752bf1":"from gensim.models import Doc2Vec\n\nall_sentences = train_question_body_sentences + \\\n                train_answer_sentences + \\\n                test_question_body_sentences + \\\n                test_answer_sentences\n\nText_INPUT_DIM=128\ntext_model = Doc2Vec(min_count=1, window=5, vector_size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, epochs=5,seed=1)\ntext_model.build_vocab(all_sentences)\ntext_model.train(all_sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)","e4270ae2":"from nltk.stem.wordnet import WordNetLemmatizer\n\ndef normalize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN') or tag.startswith('PRP'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            continue\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos).lower())\n    return lemmatized_sentence","83860c98":"def normalize_vectorize(df, columns: list):\n    for col in columns:\n        print(col)\n        df[col+'_norm'] = df[col].apply(lambda x: ' '.join(set(normalize_sentence(word_tokenize(x)))))\n        df[col+'_vec'] = df[col].apply(lambda x: text_model.infer_vector([x]))\n\n    return df\n\ntrain = normalize_vectorize(train, input_columns)\ntest = normalize_vectorize(test, input_columns)","2ee04dbc":"train.head()","5e5e11bc":"#bert embedings\ntry:\n    pbe = load_obj(\"..\/input\/questembeddings\/precomputed_bert_embeddings\")\n    train_question_body_dense = pbe['train_question_body_dense']\n    train_answer_dense = pbe['train_answer_dense']\n    train_question_title_dense = pbe['train_question_title_dense']\n    test_question_body_dense = pbe['test_question_body_dense']\n    test_answer_dense = pbe['test_answer_dense']\n    test_question_title_dense = pbe['test_question_title_dense']\nexcept:\n    print(\"Load failed, build embedding\")\n    def sigmoid(x):\n        return 1 \/ (1 + math.exp(-x))\n\n    def chunks(l, n):\n        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n        for i in range(0, len(l), n):\n            yield l[i:i + n]\n\n    def fetch_vectors(string_list, batch_size=64):\n        \n#         DEVICE = torch.device(\"cuda\")\n#         tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n#         model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n#         model.to(DEVICE)\n        # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n        DEVICE = torch.device(\"cuda\")\n        tokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")\n        bert_config = BertConfig.from_json_file('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json')\n        bert_config.num_labels =30\n\n        model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/')\n\n        model = BertModel.from_pretrained(model_path, config=bert_config)\n        model.to(DEVICE)\n\n            \n\n        fin_features = []\n        for data in tqdm_notebook(chunks(string_list, batch_size)):\n            tokenized = []\n            for x in data:\n                x = \" \".join(x.strip().split()[:300])\n                tok = tokenizer.encode(x, add_special_tokens=True)\n                tokenized.append(tok[:512])\n\n            max_len = 512\n            padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized], dtype='int64')\n            attention_mask = np.where(padded != 0, 1, 0)\n            input_ids = torch.tensor(padded).to(DEVICE)\n            attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n            with torch.no_grad():\n                last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n            features = last_hidden_states[0][:, 0, :].cpu().numpy()\n            fin_features.append(features)\n\n        fin_features = np.vstack(fin_features)\n        return fin_features\n\n    train_question_body_dense = fetch_vectors(train.question_body.values)\n    train_answer_dense = fetch_vectors(train.answer.values)\n    train_question_title_dense = fetch_vectors(train.question_title.values)\n\n\n    test_question_body_dense = fetch_vectors(test.question_body.values)\n    test_answer_dense = fetch_vectors(test.answer.values)\n    test_question_title_dense = fetch_vectors(test.question_title.values)\n\n    precomputed_bert_embeddings = {\n        'train_question_body_dense': train_question_body_dense,\n        'train_answer_dense': train_answer_dense,\n        'train_question_title_dense': train_question_title_dense,\n        'test_question_body_dense': test_question_body_dense,\n        'test_answer_dense': test_answer_dense,\n        'test_question_title_dense': test_question_title_dense,\n\n    }\n\n    #save_obj(precomputed_bert_embeddings,\"..\/input\/questembeddings\/precomputed_bert_embeddings\")","8198c214":"# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    #293+239+30 = 508 + 4 = 512\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm_notebook(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        np.asarray(input_ids, dtype=np.int32),\n        np.asarray(input_masks, dtype=np.int32),\n        np.asarray(input_segments, dtype=np.int32),\n    ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","16f4725c":"tokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")\ninput_categories = list(train.columns[[1,2,5]]); input_categories","7716e85e":"%%time\noutputs = compute_output_arrays(train, columns = targets)\ninputs = compute_input_arays(train, input_categories, tokenizer, max_sequence_length=512)\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512)","a9b4272d":"inputs[0], inputs[1], inputs[2]","580b1750":"test_inputs[0], test_inputs[1], test_inputs[2]","ada4bd5d":"inputs[0].shape, inputs[1].shape, inputs[2].shape, test_inputs[0].shape, test_inputs[1].shape, test_inputs[2].shape","30ea19f0":"inputs_feats = np.hstack((inputs[0], inputs[1], inputs[2]))\ntest_inputs_feats = np.hstack((test_inputs[0], test_inputs[1], test_inputs[2]))","38a03436":"inputs_feats.shape, test_inputs_feats.shape","6f6a3fe9":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","651709be":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport scipy\nfrom sklearn.metrics import log_loss\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nfrom sklearn.decomposition import TruncatedSVD\n\ntrain_text_1 = train['question_body']\ntest_text_1 = test['question_body']\nall_text_1 = pd.concat([train_text_1, test_text_1])\n\ntrain_text_2 = train['answer']\ntest_text_2 = test['answer']\nall_text_2 = pd.concat([train_text_2, test_text_2])\n\ntrain_text_3 = train['question_title']\ntest_text_3 = test['question_title']\nall_text_3 = pd.concat([train_text_3, test_text_3])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_1)\n\ntrain_word_features_1 = word_vectorizer.transform(train_text_1)\ntest_word_features_1 = word_vectorizer.transform(test_text_1)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_2)\n\ntrain_word_features_2 = word_vectorizer.transform(train_text_2)\ntest_word_features_2 = word_vectorizer.transform(test_text_2)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_3)\n\ntrain_word_features_3 = word_vectorizer.transform(train_text_3)\ntest_word_features_3 = word_vectorizer.transform(test_text_3)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_1)\n\ntrain_char_features_1 = char_vectorizer.transform(train_text_1)\ntest_char_features_1 = char_vectorizer.transform(test_text_1)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_2)\n\ntrain_char_features_2 = char_vectorizer.transform(train_text_2)\ntest_char_features_2 = char_vectorizer.transform(test_text_2)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_3)\n\ntrain_char_features_3 = char_vectorizer.transform(train_text_3)\ntest_char_features_3 = char_vectorizer.transform(test_text_3)\n\ntrain_features_tfidf = hstack([train_char_features_1, train_word_features_1, train_char_features_2, train_word_features_2,train_char_features_3, train_word_features_3])\ntest_features_tfidf = hstack([test_char_features_1, test_word_features_1, test_char_features_2, test_word_features_2,test_char_features_3, test_word_features_3])\n\npca = TruncatedSVD(n_components=128, n_iter=10)\ntf_idf_text_train = pca.fit_transform(train_features_tfidf)\ntf_idf_text_test = pca.fit_transform(test_features_tfidf)","58dc2943":"tf_idf_text_train.shape, tf_idf_text_test.shape","956246ef":"torch.cuda.empty_cache() # release all gpu memory from pytorch","225d17b7":"# universal sentence encoder\n\ntry:\n    embeddings_train = load_obj(\"..\/input\/questembeddings\/use_embeddings_train\")\n    embeddings_test = load_obj(\"..\/input\/questembeddings\/use_embeddings_test\")\nexcept:\n    print(\"Load failed, build embedding\")\n    try:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge4\/'\n        embed = hub.load(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)))[\"outputs\"]\n            return keras.backend.concatenate([results])\n    except:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge3\/'\n        embed = hub.Module(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n            return keras.backend.concatenate([results])\n\n    embeddings_train = {}\n    embeddings_test = {}\n    for text in input_columns:\n        print(text)\n        train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n\n        curr_train_emb = []\n        curr_test_emb = []\n        batch_size = 4\n        ind = 0\n        while ind*batch_size < len(train_text):\n            curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1\n\n        ind = 0\n        while ind*batch_size < len(test_text):\n            curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1    \n\n        embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n        embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n\n    del embed\n    K.clear_session()\n    gc.collect()\n    \n    #save_obj(embeddings_train,\"..\/input\/questembeddings\/use_embeddings_train\")\n    #save_obj(embeddings_test,\"..\/input\/questembeddings\/use_embeddings_test\")","b5a0b8ec":"embeddings_train","6bf84fe2":"find = re.compile(r\"^[^.]*\")\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntabular_cols = ['question_user_name', 'answer_user_name', \n               'netloc_1', 'netloc_2', 'netloc_3',\n               'category', 'host']\n\n\n    \nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\nabs_dist = lambda x, y: np.abs(x-y).sum(axis=1)\nsum_dist = lambda x, y: (x+y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    \n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    \n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    \n    abs_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    \n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n  \n    sum_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    \n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n\n    l2_dist(train_question_body_dense, train_answer_dense),\n    cos_dist(train_question_body_dense, train_answer_dense),\n    abs_dist(train_question_body_dense, train_answer_dense),\n    sum_dist(train_question_body_dense, train_answer_dense),\n    \n    l2_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n    abs_dist(train_question_body_dense, train_question_title_dense),\n    sum_dist(train_question_body_dense, train_question_title_dense),\n    \n    l2_dist(train_answer_dense, train_question_title_dense),\n    cos_dist(train_answer_dense, train_question_title_dense),\n    abs_dist(train_answer_dense, train_question_title_dense),\n    sum_dist(train_answer_dense, train_question_title_dense),\n]).T\n\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    \n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    \n    \n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    \n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n   \n    \n    abs_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    \n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n\n    \n    sum_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    \n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n\n    \n    l2_dist(test_question_body_dense, test_answer_dense),\n    cos_dist(test_question_body_dense, test_answer_dense),\n    abs_dist(test_question_body_dense, test_answer_dense),\n    sum_dist(test_question_body_dense, test_answer_dense),\n    \n    l2_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n    abs_dist(test_question_body_dense, test_question_title_dense),\n    sum_dist(test_question_body_dense, test_question_title_dense),\n    \n    l2_dist(test_answer_dense, test_question_title_dense),\n    cos_dist(test_answer_dense, test_question_title_dense),\n    abs_dist(test_answer_dense, test_question_title_dense),\n    sum_dist(test_answer_dense, test_question_title_dense),\n]).T","e9ffb624":"dist_features_train.shape, tf_idf_text_train.shape","732e0308":"for k, item in embeddings_train.items():\n    print(len(item))","e337a71d":"train_question_body_dense.shape, train_answer_dense.shape","8b87889d":"possible_features_train = [\n    [item for k, item in embeddings_train.items()],\n    [ dist_features_train ],\n    [ train_question_body_dense ],\n    [ train_answer_dense ],\n    [ inputs_feats ],\n    [ [x for x in train.question_body_vec.values] ],\n    [ [x for x in train.question_title_vec.values] ],\n    [ [x for x in train.answer_vec.values] ],\n    \n    [ tf_idf_text_train ]\n    \n]\npossible_features_test = [\n    [item for k, item in embeddings_test.items()],\n    [ dist_features_test ],\n    [ test_question_body_dense ],\n    [ test_answer_dense ],\n    [ test_inputs_feats ],\n    [ [x for x in test.question_body_vec.values] ],\n    [ [x for x in test.question_title_vec.values] ],\n    [ [x for x in test.answer_vec.values] ],\n    \n    [ tf_idf_text_test ]\n]\n\ndef get_train_test(split=0.8):\n    total_len = len(possible_features_train)\n    r_idx = random.sample(range(total_len), int(total_len * split))\n    \n    train = [ train_question_title_dense ]\n\n    test =  [ test_question_title_dense ]\n\n    for i in r_idx:\n        train += possible_features_train[i]\n        test += possible_features_test[i]\n        \n    return np.hstack(train),np.hstack(test)\n\nX_train,X_test = get_train_test()\ny_train = train[targets].values","d8336169":"X_train.shape, X_test.shape","c3514b79":"X_train = pd.DataFrame(data = X_train)\nX_train.columns = [str(col) + '_col' for col in X_train.columns]\n\nX_test = pd.DataFrame(data=X_test)\nX_test.columns = [str(col) + '_col' for col in X_test.columns]","9e0c7313":"train[tabular_cols].head()","f4259179":"pd_concat_train = [X_train, train[targets], train[tabular_cols], train[cat_cols], train[cont_cols]]\npd_concat_test = [X_test, test[tabular_cols], test[cat_cols], test[cont_cols]]\n\n\nfinal_train_df = pd.concat(pd_concat_train, axis=1)\nfinal_test_df = pd.concat(pd_concat_test, axis=1)","d69572f8":"final_train_df.shape, final_test_df.shape","826db9e9":"valid_sz = 1000\nvalid_idx = range(len(final_train_df)-valid_sz, len(final_train_df))\nvalid_idx","ebabf900":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm","72fa708f":"def to_gauss(x): return np.sqrt(2)*erfinv(x)  #from scipy\n\ndef normalize(data, exclude=None):\n    # if not binary, normalize it\n    norm_cols = [n for n, c in data.drop(exclude, 1).items() if len(np.unique(c)) > 2]\n    n = data.shape[0]\n    for col in norm_cols:\n        sorted_idx = data[col].sort_values().index.tolist()# list of sorted index\n        uniform = np.linspace(start=-0.99, stop=0.99, num=n) # linsapce\n        normal = to_gauss(uniform) # apply gauss to linspace\n        normalized_col = pd.Series(index=sorted_idx, data=normal) # sorted idx and normalized space\n        data[col] = normalized_col # column receives its corresponding rank\n    return data","ad8fdc19":"list(final_train_df[tabular_cols+targets+cat_cols].columns)","74497c34":"exclude = list(final_train_df[tabular_cols+targets+cat_cols].columns)\nnorm_data_train = normalize(final_train_df, exclude=exclude)\n\nexclude = list(final_test_df[tabular_cols+cat_cols].columns)\nnorm_data_test = normalize(final_test_df, exclude=exclude)","a055436c":"gc.collect()","ee9f902b":"tabular_cols+cat_cols","aef74e1a":"cont_names = list(X_train.columns)+cont_cols\ncat_names = tabular_cols+cat_cols\ndep_var = targets\nprocs = [FillMissing, Categorify]\n\ntest_tab = TabularList.from_df(norm_data_test, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\ndata = (TabularList.from_df(norm_data_train, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_idx(valid_idx)\n        .label_from_df(cols=dep_var)\n        .add_test(test_tab)\n        .databunch(bs=32))","a8f1ea0b":"data.show_batch()","26ba665f":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","711a7e00":"def _get_ranks(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n        Efficiently calculates the ranks of the data.\n        Only sorts once to get the ranked data.\n        \n        :param arr: A 1D NumPy Array\n        :return: A 1D NumPy Array containing the ranks of the data\n    \"\"\"\n    temp = arr.argsort()\n    ranks = np.empty_like(temp)\n    ranks[temp] = np.arange(len(arr))\n    return ranks","b133fa43":"def spearmans_rho_custom(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n    \"\"\"\n        Efficiently calculates the Spearman's Rho correlation using only NumPy\n        \n        :param y_true: The ground truth labels\n        :param y_pred: The predicted labels\n    \"\"\"\n    # Get ranked data\n    true_rank = _get_ranks(y_true)\n    pred_rank = _get_ranks(y_pred)\n    \n    cost = np.corrcoef(true_rank, pred_rank)[1][0]\n    loss = cost.mean()\n    loss = loss*(-1)\n    return loss","29958b81":"class Regress_Loss(torch.nn.Module):\n    \n    def __init__(self):\n        super(Regress_Loss,self).__init__()\n        \n    def forward(self,x,y):\n        true_rank = _get_ranks(x)\n        pred_rank = _get_ranks(y)\n        \n        cost = np.corrcoef(true_rank, pred_rank)[1][0]\n        loss = cost.mean()\n        loss = loss*(-1)\n        return loss","6bdd2d43":"def myloss(input,target):\n    #alpha of 0.5 means half weight goes to first, remaining half split by remaining 15\n    x = input\n    y = target\n    vx = x - torch.mean(x)\n    vy = y - torch.mean(y)\n\n    cost = torch.sum(vx * vy) \/ (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n    loss = cost.mean() \n    return loss","d6265728":"class Regress_Loss_1(torch.nn.Module):\n    \n    def __init__(self):\n        super(Regress_Loss_1,self).__init__()\n        \n    def forward(self,x,y):\n        x = input\n        y = target\n        vx = x - torch.mean(x)\n        vy = y - torch.mean(y)\n        \n        cost = torch.sum(vx * vy) \/ (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n        loss = cost.mean()\n        loss = loss*(-1)\n        return loss","072ec490":"from fastai.callbacks import *\n\nlearn_tfidf = tabular_learner(data, \n                              layers=[64, 128], \n                              ps=[0.30, 0.3], \n                              emb_drop=0.10, use_bn=False,\n                              callback_fns=ReduceLROnPlateauCallback)\n\nlearn_tfidf.crit = Regress_Loss\n# learn_tfidf.crit = Regress_Loss_1\nlearn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","00972376":"lr = 5e-2\nlearn_tfidf.fit_one_cycle(10, max_lr=lr,  pct_start=0.3, wd = 1.)","c4e9935a":"learn_tfidf.recorder.plot_losses()","9f71914b":"pred_test_tfidf, lbl_test_tfidf = learn_tfidf.get_preds(ds_type=DatasetType.Test)","fe62e11a":"pred_test_tfidf = np.clip(pred_test_tfidf, 0.00001, 0.999999)\npred_test_tfidf.shape","c61e8f8a":"pred_test_tfidf","b9dc3b4d":"learn_tfidf_2 = tabular_learner(data, \n                              layers=[512, 256], \n                              ps=[0.30, 0.3], \n                              emb_drop=0.10, use_bn=False,\n                              callback_fns=ReduceLROnPlateauCallback)","b5bdb4d9":"#learn_tfidf.crit = Regress_Loss\nlearn_tfidf_2.crit = Regress_Loss_1\nlearn_tfidf_2.lr_find()\nlearn_tfidf_2.recorder.plot(suggestion=True)","a6470886":"lr = 5e-2\nlearn_tfidf_2.fit_one_cycle(10, max_lr=lr,  pct_start=0.3, wd = 1.)","3589b78e":"learn_tfidf_2.recorder.plot_losses()","723dd4ba":"pred_test_tfidf_2, lbl_test_tfidf_2 = learn_tfidf_2.get_preds(ds_type=DatasetType.Test)","84606b1c":"pred_test_tfidf_2 = np.clip(pred_test_tfidf_2, 0.00001, 0.999999)\npred_test_tfidf_2.shape","b8732c8f":"pred_test_tfidf_2","00d8c2f5":"learn_tfidf_3 = tabular_learner(data, \n                              layers=[512], \n                              ps=[0.30], \n                              emb_drop=0.10, use_bn=False,\n                              callback_fns=ReduceLROnPlateauCallback)","3fb9504f":"#learn_tfidf.crit = Regress_Loss\nlearn_tfidf_3.crit = Regress_Loss_1\nlearn_tfidf_3.lr_find()\nlearn_tfidf_3.recorder.plot(suggestion=True)","bb6a7a70":"lr = 3e-2\nlearn_tfidf_3.fit_one_cycle(10, max_lr=lr,  pct_start=0.3, wd = 1.)","1c6c4a4b":"learn_tfidf_3.recorder.plot_losses()","e794807e":"pred_test_tfidf_3, lbl_test_tfidf_3 = learn_tfidf_3.get_preds(ds_type=DatasetType.Test)","34950d6f":"pred_test_tfidf_3 = np.clip(pred_test_tfidf_3, 0.00001, 0.999999)\npred_test_tfidf_3.shape","cbb7f3b7":"pred_test_tfidf_3","c51c2cea":"test_preds_fastai = (pred_test_tfidf+ pred_test_tfidf_2 + pred_test_tfidf_3)\/3","ff88b1fd":"test_preds_fastai","15b9455a":"# submission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')\n# submission[targets] = test_preds_fastai\n# submission.head(20)","b45ab902":"# submission.to_csv(\"submission.csv\", index = False)","75465823":"data_init = data","7ce7f94d":"x = int(len(norm_data_train)*.9)\ntrain_df = norm_data_train.iloc[:x]\ntest_df = norm_data_train.iloc[x:]","1c9283cd":"skf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor train_index, test_index in skf.split(norm_data_train.index):\n    \n    X_train, X_test = norm_data_train.iloc[train_index], norm_data_train.iloc[test_index]\n    y_train, y_test = norm_data_train[targets].iloc[train_index], norm_data_train[targets].iloc[test_index]    ","ae81547a":"X_train.head()","ad034567":"\nval_pct = []\ntest_pct = []\n\n\nfor train_index, val_index in skf.split(train_df.index, train_df[dep_var]):\n    data_fold = (TabularList.from_df(train_df, cat_names=cat_names.copy(),\n                                  cont_names=cont_names.copy(), procs=procs,\n                                  processor=data_init.processor) # Very important\n              .split_by_idxs(train_index, val_index)\n              .label_from_df(cols=dep_var)\n              .databunch())\n    \n    data_test = (TabularList.from_df(test_df, cat_names=cat_names.copy(),\n                                  cont_names=cont_names.copy(), procs=procs,\n                                  processor=data_init.processor) # Very important\n              .split_none()\n              .label_from_df(cols=dep_var))\n    \n    data_test.valid = data_test.train\n    data_test = data_test.databunch()\n    \n    learn_f = tabular_learner(data_fold, layers=[200, 100], \n                        ps=[0.3, 0.3], emb_drop=0.3, use_bn=False)\n    \n    learn_f.fit_one_cycle(10, max_lr=1e-3,  pct_start=0.5, wd = 1, callbacks = [SaveModelCallback(learn_f)])\n    \n    val = learn_f.validate()\n    \n    learn_f.data.valid_dl = data_test.valid_dl\n    \n    test = learn_f.validate()\n    \n    val_pct.append(val)\n    test_pct.append(test)","1e670fa5":"print(f'Validation\\nmean: {np.mean(val_pct)}\\nstd: {np.std(val_pct)}')","ffee8129":"print(f'Test\\nmean: {np.mean(test_pct)}\\nstd: {np.std(test_pct)}')","5924b1eb":"data = (TabularList.from_df(norm_data_train, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_idx(valid_idx)\n        .label_from_df(cols=dep_var)\n        .add_test(test_tab)\n        .databunch(bs=32))","e04faa68":"learn_f = tabular_learner(data, layers=[200, 100], \n                        ps=[0.3, 0.3], emb_drop=0.3, use_bn=False)\nlearn_f.load('bestmodel')","664d376d":"learn_f.fit_one_cycle(1, max_lr=1e-3,  pct_start=0.5, wd = 1)","af982c57":"pred_test_tfidf, lbl_test_tfidf = learn_f.get_preds(ds_type=DatasetType.Test)","53304d14":"pred_test_tfidf = np.clip(pred_test_tfidf, 0.00001, 0.999999)\npred_test_tfidf.shape","a2f9f49f":"pred_test_tfidf","4ce3906e":"# submission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')\n# submission[targets] = pred_test_tfidf\n# submission.head(20)","80785413":"# submission.to_csv(\"submission.csv\", index = False)","7239357e":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom lightgbm import LGBMRegressor\n\nseed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","74d528f0":"# Categorical boolean mask\ncategorical_feature_mask = norm_data_train.dtypes=='object'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = norm_data_train.columns[categorical_feature_mask].tolist()\ncategorical_cols","f847d0d2":"type(norm_data_train.question_user_name[0])","be42941b":"# Categorical boolean mask\ncategorical_feature_mask = norm_data_train.dtypes=='object'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = norm_data_train.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nnorm_data_train[categorical_cols] = norm_data_train[categorical_cols].apply(lambda col: le.fit_transform(col))\nnorm_data_train[categorical_cols].head(10)\n","a673567e":"# Categorical boolean mask\ncategorical_feature_mask = norm_data_test.dtypes=='object'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = norm_data_test.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nnorm_data_test[categorical_cols] = norm_data_test[categorical_cols].apply(lambda col: le.fit_transform(col))\nnorm_data_test[categorical_cols].head(10)","c14e2ad4":"X_train = norm_data_train.loc[:, ~norm_data_train.columns.isin(targets)].values\ny_train = norm_data_train[targets].values\nX_test = norm_data_test.values","56c72e2d":"X_train.shape, y_train.shape, X_test.shape","317a57fe":"norm_data_train.head()","8aee51c5":"# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","648bce8c":"from keras.losses import *\ndef bce(t,p):\n    return binary_crossentropy(t,p)\n\ndef custom_loss(true,pred):\n    bce = binary_crossentropy(true,pred)\n    return bce + logcosh(true,pred)\n\ndef swish(x):\n    return K.sigmoid(x) * x\n\ndef relu1(x):\n    return keras.activations.relu(x, alpha=0.0, max_value=1., threshold=0.0)\n\ndef create_model1():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(64, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(128, activation='elu',kernel_initializer='lecun_normal')(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adam(lr=5e-4,clipnorm=1.4)\n    model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n    return model\n\n\ndef create_model2():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(256, activation='elu',\n              kernel_initializer='lecun_normal', \n              #kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adam(lr=5e-4,clipnorm=1.4)\n    model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n    return model\n\n\ndef create_model3():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(200, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              )(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adam(lr=5e-4,clipnorm=1.4)\n    model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n    return model\n\ndef create_model4():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',\n              kernel_initializer='lecun_normal', \n              )(input1)\n    x = Dropout(0.2)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adam(lr=5e-4,clipnorm=1.4)\n    model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n    return model\n\ndef create_model5():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(4096, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adam(lr=5e-4,clipnorm=1.4)\n    model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n    return model","48c433fa":"all_predictions = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model1()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model1()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n    \nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model2()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model2()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n\nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model3()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model3()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n\nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model4()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model4()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n\nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model5()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model5()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))","1c6b1729":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds\/max_val + 1e-12","09923780":"test_preds","39e74afc":"test_preds = torch.from_numpy(test_preds) * 0.50 + pred_test_tfidf * 0.50","02d5af75":"test_preds","481a192c":"submission = pd.read_csv(INPUT_PATH+'google-quest-challenge\/sample_submission.csv')\nsubmission[targets] = test_preds\nsubmission.head(20)","56d070ad":"submission.to_csv(\"submission.csv\", index = False)","e606026b":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, DataLoader\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport os\nimport math\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm\nfrom tqdm import trange","8915f138":"type(X_train), type(y_train), type(X_test)","baccac81":"# train valid split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","0dbe4856":"X_train = torch.tensor(X_train)\ny_train = torch.tensor(y_train, dtype=torch.float32)\n\nX_val = torch.tensor(X_valid)\ny_val = torch.tensor(y_valid, dtype=torch.float32)\n\nX_test = torch.tensor(X_test)","cf697b60":"batch_size = 32\n\ntrain_data = TensorDataset(X_train, y_train)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(X_val, y_val)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)","c57e72e5":"class py_net(torch.nn.Module):\n    \n    def __init__(self):\n        super(py_net, self).__init__()\n        self.fc1 = nn.Linear(X_train.shape[1], 200)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.25)\n        self.fc2 = nn.Linear(200, 100)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.25)\n        self.fc3 = nn.Linear(100, 30)\n        self.prelu3 = nn.PReLU()\n        self.out = nn.Linear(30, len(targets))\n        self.out_act = nn.Sigmoid()\n    \n    def forward(self, input_):\n        a1 = self.fc1(input_)\n        h1 = self.relu1(a1)\n        dout1 = self.dropout1(h1)\n        a2 = self.fc2(dout1)\n        h2 = self.relu2(a2)\n        dout2 = self.dropout2(h2)\n        a3 = self.fc3(dout2)\n        h3 = self.prelu3(a3)\n        a4 = self.out(h3)\n        y = self.out_act(a4)\n        return y\n    \n    def predict(self, x):\n        pred = self.forward(x)\n        return torch.tensor(pred)          \n    ","20d086a5":"model = py_net()\nmodel.cuda()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1)","bf794f29":"from scipy.stats import spearmanr\ndef train(model, num_epochs,\\\n          optimizer,\\\n          train_dataloader, valid_dataloader,\\\n          train_loss_set=[], valid_loss_set = [],\\\n          lowest_eval_loss=None, start_epoch=0,\\\n          device=\"cuda\"\n          ):\n    \n    \n    crit_function = nn.BCEWithLogitsLoss()\n    model.to(device)\n    \n    for i in trange(num_epochs, desc = 'Epoch'):\n        actual_epoch = start_epoch + i\n        \n        model.train()\n        \n        tr_loss = 0\n        \n        num_train_samples = 0\n        \n        t = tqdm(total = len(train_data), desc = 'Training:', position=0)\n        \n        for step, batch in enumerate(train_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            X_batch, y_batch = batch\n            \n            optimizer.zero_grad()\n            \n            y_pred = model(X_batch.float())\n            \n            loss = crit_function(y_pred, y_batch)\n            \n            tr_loss += loss.item()\n            \n            num_train_samples = y_batch.size(0)\n            \n            loss.backward()\n            \n            optimizer.step()\n            \n            t.update(n = X_batch.shape[0])\n            \n        t.close()\n        \n        epoch_train_loss = tr_loss\/num_train_samples\n        train_loss_set.append(epoch_train_loss)\n\n        print(\"Train loss: {}\".format(epoch_train_loss))\n        \n        # validation\n        \n        model.eval()\n        eval_loss = 0\n        num_eval_samples = 0\n        \n        v_preds = []\n        v_labels = []\n        \n        # Evaluate data for one epoch\n        t = tqdm(total=len(val_data), desc=\"Validating: \", position=0)\n        for batch in val_dataloader:\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            X_batch, y_batch = batch\n            # Telling the model not to compute or store gradients,\n            # saving memory and speeding up validation\n            with torch.no_grad():\n                # Forward pass, calculate validation loss\n                preds = model(X_batch.float())\n                loss = crit_function(preds, y_batch)\n                v_labels.append(y_batch.cpu().numpy())\n                v_preds.append(preds.cpu().numpy())\n                # store valid loss\n                eval_loss += loss.item()\n                num_eval_samples += y_batch.size(0)\n            t.update(n=y_batch.shape[0])\n        t.close()\n        \n        v_labels = np.vstack(v_labels)\n        v_preds = np.vstack(v_preds)\n        print(v_labels.shape)\n        print(v_preds.shape)\n        rho_val = np.mean([spearmanr(v_labels[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0]),\n                                            v_preds[:, ind] + np.random.normal(0, 1e-7, v_preds.shape[0])).correlation for ind in range(v_preds.shape[1])]\n                                )\n        epoch_eval_loss = eval_loss\/num_eval_samples\n        valid_loss_set.append(epoch_eval_loss)\n\n        print(\"Epoch #{}, training BCE loss: {}, validation BCE loss: ~{}, validation spearmanr: {}\"\\\n                .format(0, epoch_train_loss, epoch_eval_loss, rho_val))\n\n        if lowest_eval_loss == None:\n            lowest_eval_loss = epoch_eval_loss\n           \n        else:\n            if epoch_eval_loss < lowest_eval_loss:\n                lowest_eval_loss = epoch_eval_loss\n         \n        print(\"\\n\")\n\n    return model, train_loss_set, valid_loss_set","3844cbf0":"model, train_loss_set, valid_loss_set = train(model=model,\\\n                                              num_epochs = 100,\n                                              optimizer = optimizer,\n                                              train_dataloader = train_dataloader,\n                                              valid_dataloader = val_dataloader,\n                                              device='cuda'\n                                              )","6128ad6b":"# Importing Libraries","b4056e7a":"# Feature Engineering","19f5e961":"# Pytorch - experiment","b0003ec5":"# Keras","874a48f5":"# Embeddings","bb623cbe":"# TFIDF Embeddings","fbfe4088":"# Universal Sentence Encoder embeddings","19f4f6fd":"# BERT Embeddings","722975d1":"# K fold"}}