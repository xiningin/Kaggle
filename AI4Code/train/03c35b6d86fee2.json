{"cell_type":{"4a1af3a0":"code","6cd186ad":"code","cf84581c":"code","77dd609c":"code","6e434ada":"code","03ae82f1":"code","76fbdc3f":"code","a2acc888":"code","b6877bb0":"code","889fb5ca":"code","c56900d8":"code","0d9ed1c9":"code","48268998":"code","5def9790":"code","d04a8937":"code","4ff6852a":"code","4416cf68":"code","fcf335a0":"code","979fe96c":"code","e946d3ef":"code","51ed01f6":"code","41051fc5":"code","95d9f962":"code","b0d42559":"code","3b6e3876":"code","0f274d0c":"code","1a077d4d":"code","ea88d021":"markdown","9cb961da":"markdown","b92888ca":"markdown","6d0a679d":"markdown","c2b65be5":"markdown","8e4d946b":"markdown","7222bbcc":"markdown","c5dd3036":"markdown","d6b422a1":"markdown","d900a279":"markdown","a7476569":"markdown","38a7fc34":"markdown","8e9b7586":"markdown","9b7afcf2":"markdown","4bed82e9":"markdown","b0ac2cf3":"markdown","d91b09e2":"markdown","2f0d49fb":"markdown","d69bf540":"markdown","2d363d66":"markdown","1c9e1b08":"markdown","e6240697":"markdown","220b6da6":"markdown","342f7196":"markdown","6bb7f59e":"markdown","38969f7f":"markdown","6a52e408":"markdown"},"source":{"4a1af3a0":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD,Adam\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline","6cd186ad":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","cf84581c":"INPUT_DIR = '..\/input\/sugiura-lab-first-competition\/'\n\nPATH = {\n    'train': os.path.join(INPUT_DIR, 'train.csv'),\n    'sample_submission': os.path.join(INPUT_DIR, 'sample_submission.csv'),\n    'train_image_dir': os.path.join(INPUT_DIR, 'train_images\/train_images'),\n    'test_image_dir': os.path.join(INPUT_DIR, 'test_images\/test_images'),\n}\n\nID = 'fname'\nTARGET = 'label'\n\nSEED = 42\nseed_everything(SEED)\n\n# GPU settings for PyTorch (explained later...)\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Parameters for neural network. We will see the details later...\nPARAMS = {\n    'valid_size': 0.2,\n    'batch_size': 64,\n    'epochs': 5,\n    'lr': 0.001,\n    'valid_batch_size': 256,\n    'test_batch_size': 256,\n}","77dd609c":"train_df = pd.read_csv(PATH['train'])\nsample_submission_df = pd.read_csv(PATH['sample_submission'])","6e434ada":"print(f'number of train data: {len(train_df)}')\nprint(f'number of test data: {len(sample_submission_df)}')","03ae82f1":"print(f'number of unique label: {train_df[TARGET].nunique()}')","76fbdc3f":"sns.countplot(train_df[TARGET])\nplt.title('train label distribution')\nplt.show()","a2acc888":"train_df.head()","b6877bb0":"sample = train_df.groupby(TARGET).first().reset_index()\n\nfig, ax = plt.subplots(2, 5)\nfig.set_size_inches(4 * 5, 4 * 2)\n\nfor i, row in sample.iterrows():\n    fname, label = row[ID], row[TARGET]\n    img = cv2.imread(os.path.join(PATH['train_image_dir'], fname))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ax[i\/\/5,i%5].imshow(img, 'gray')\n    ax[i\/\/5,i%5].set_title(f'{fname} - label: {label}')","889fb5ca":"print(f'shape of image: {img.shape}')","c56900d8":"class KMNISTDataset(Dataset):\n    def __init__(self, fname_list, label_list, image_dir, transform=None):\n        super().__init__()\n        self.fname_list = fname_list\n        self.label_list = label_list\n        self.image_dir = image_dir\n        self.transform = transform\n        \n    # Dataset\u3092\u5b9f\u88c5\u3059\u308b\u3068\u304d\u306b\u306ftorch.utils.data.Dataset\u3092\u7d99\u627f\u3059\u308b\n    # __len__\u3068__getitem__\u3092\u5b9f\u88c5\u3059\u308b\n    \n    def __len__(self):\n        return len(self.fname_list)\n    \n    def __getitem__(self, idx):\n        fname = self.fname_list[idx]\n        label = self.label_list[idx]\n        \n        image = cv2.imread(os.path.join(self.image_dir, fname))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        if self.transform is not None:\n            image = self.transform(image)\n        # __getitem__\u3067\u30c7\u30fc\u30bf\u3092\u8fd4\u3059\u524d\u306btransform\u3067\u30c7\u30fc\u30bf\u306b\u524d\u51e6\u7406\u3092\u3057\u3066\u304b\u3089\u8fd4\u3059\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8\n        return image, label","0d9ed1c9":"# \u5165\u529b\u306f28*28\u306e\u767d\u9ed2\u753b\u50cf\u306710\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim=28*28, hidden_dim=128, output_dim=10):\n        super().__init__()\n        # nn.Linear\u306f fully-connected layer (\u5168\u7d50\u5408\u5c64)\u306e\u3053\u3068\u3067\u3059\uff0e\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.activation = nn.ReLU()\n    \n    def forward(self, x):\n        # 1\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        \n        return x","48268998":"# \u4ee5\u4e0b\u3092\u57cb\u3081\u3066\u307f\u3088\u3046\n# \u4eca\u56de\u306e\u7814\u4fee\u3067\u306f\n# \u30e2\u30c7\u30eb\u3068\u3057\u3066\u5165\u529b\u304b\u3089\u51fa\u529b\u30c1\u30e3\u30cd\u30eb\u65706, kernel_size5\u306e\u7573\u307f\u8fbc\u307f\u5c64\u2192Maxpooling(2\u00d72)\u2192\u51fa\u529b\u30c1\u30e3\u30cd\u30eb\u657012, kernel_size3\u306e\u7573\u307f\u8fbc\u307f\u5c64\n# \u2192 MaxPooling(2\u00d72)\u21921\u6b21\u5143\u306b\u3059\u308b\u2192Linear\u306710\u6b21\u5143\u51fa\u529b\n# \u3068\u3044\u3046\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044(stride\u306a\u3069\u306f\u8003\u3048\u306a\u3044\u3067\u304f\u3060\u3055\u3044)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # \u51fa\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u65706, kernel size 5\u306eCNN\u3092\u5b9a\u7fa9\u3059\u308b\n        # \u7573\u307f\u8fbc\u307f\u306e\u5b9a\u7fa9\u306fPytorch\u306e\u5834\u5408torch.nn.Conv2d\u3067\u884c\u3044\u307e\u3059\u3002\u30d2\u30f3\u30c8:\u767d\u9ed2\u753b\u50cf\u3068\u306f\u30c1\u30e3\u30cd\u30eb\u6570\u3044\u304f\u3064\u304b\u306f\u81ea\u5206\u3067\u8003\u3048\u3088\u3046\n        # \u516c\u5f0fdocument\u3067\u4f7f\u3044\u65b9\u3092\u78ba\u8a8d\u3059\u308b\u529b\u3092\u3064\u3051\u3066\u307b\u3057\u3044\u306e\u3067\u3001\u81ea\u5206\u3067conv2d\u306a\u3069\u306e\u4f7f\u3044\u65b9\u306f\u8abf\u3079\u3088\u3046\n        self.conv1 = nn.Conv2d()\n        # \u51fa\u529b\u30c1\u30e3\u30cd\u30eb\u657012, kernel_size 3\u306eCNN\u3092\u5b9a\u7fa9\u3059\u308b \u4e0a\u8a18\u3068\u540c\u69d8\u306b\u4eca\u5ea6\u306f\u81ea\u5206\u3067\u66f8\u3044\u3066\u307f\u3088\u3046\n        \n        \n        # Maxpooling\u306e\u5b9a\u7fa9(foward\u3067\u3059\u308b\u306e\u3067\u3082\u3069\u3063\u3061\u3067\u3082)\n        \n        \n        # Linear\u306e\u5b9a\u7fa9\n        # \u7dda\u5f62\u5909\u63db\u3092\u884c\u3046\u5c64\u3092\u5b9a\u7fa9\u3057\u3066\u3042\u3052\u307e\u3059: y = Wx + b\n        # self.conv1, conv2\u306e\u3042\u3068\uff0cmaxpooling\u3092\u901a\u3059\u3053\u3068\u3067\uff0c\n        # self.fc1\u306b\u5165\u529b\u3055\u308c\u308bTensor\u306e\u6b21\u5143\u306f\u4f55\u306b\u306a\u3063\u3066\u3044\u308b\u304b\u8a08\u7b97\u3057\u3066\u307f\u3088\u3046\uff01\n        # \u3053\u308c\u309210\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\uff0c10\u6b21\u5143\u306b\u5909\u63db\u3059\u308b\u3088\u3046\u306aLinear\u5c64\u3092\u5b9a\u7fa9\u3057\u307e\u3059\n        \n        self.fc1 = nn.Linear()\n\n    \n    def forward(self, x):\n        batch_size = x.shape[0]\n        # forward\u95a2\u6570\u306e\u4e2d\u3067\u306f\uff0c\uff0c\u5165\u529b x \u3092\u9806\u756a\u306b\u30ec\u30a4\u30e4\u30fc\u306b\u901a\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\u307f\u3066\u3044\u304d\u307e\u3057\u3087\u3046\uff0e    \n        # \u307e\u305a\u306f\uff0c\u753b\u50cf\u3092CNN\u306b\u901a\u3057\u307e\u3059\n        x = self.conv1(x)\n\n        # \u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066relu\u3092\u4f7f\u3044\u307e\u3059\n        x = F.relu(x)\n        \n        # \u6b21\u306b\uff0cMaxPooling\u3092\u304b\u3051\u307e\u3059\uff0e\n        \n        \n        # 2\u3064\u76ee\u306eConv\u5c64\u306b\u901a\u3057\u307e\u3059\n        \n        \n        # MaxPooling\u3092\u304b\u3051\u307e\u3059\n        \n        \n         # \u5c11\u3057\u30c8\u30ea\u30c3\u30ad\u30fc\u306a\u3053\u3068\u304c\u8d77\u304d\u307e\u3059\uff0e\n        # CNN\u306e\u51fa\u529b\u7d50\u679c\u3092 fully-connected layer \u306b\u5165\u529b\u3059\u308b\u305f\u3081\u306b\n        # 1\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u306b\u3057\u3066\u3084\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\n        # \u6b63\u78ba\u306b\u306f\uff0c\u3000(batch_size, channel, height, width) --> (batch_size, channel * height * width)\n        x = x.view(batch_size, -1)\n        \n        # linear\u3068\u6d3b\u6027\u5316\u95a2\u6570\u306b\u901a\u3057\u307e\u3059\n        \n        x = F.relu(x)\n        return x","5def9790":"net = Net()","d04a8937":"train_df, valid_df = train_test_split(\n    train_df, test_size=PARAMS['valid_size'], random_state=SEED, shuffle=True\n)\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)","4ff6852a":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    # numpy.array\u3067\u8aad\u307f\u8fbc\u307e\u308c\u305f\u753b\u50cf\u3092PyTorch\u7528\u306eTensor\u306b\u5909\u63db\u3057\u307e\u3059\uff0e\n    transforms.Normalize((0.5, ), (0.5, ))\n    #\u6b63\u898f\u5316\u306e\u51e6\u7406\u3082\u52a0\u3048\u307e\u3059\u3002\n])\n\ntrain_dataset = KMNISTDataset(train_df[ID], train_df[TARGET], PATH['train_image_dir'], transform=transform)\nvalid_dataset = KMNISTDataset(valid_df[ID], valid_df[TARGET], PATH['train_image_dir'], transform=transform)\n\n# DataLoader\u3092\u7528\u3044\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u306e\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u307e\u3059\u3002shuffle\u3092true\u306b\u3059\u308b\u3053\u3068\u3067\u30c7\u30fc\u30bf\u3092shuffle\u3057\u3066\u304f\u308c\u307e\u3059\ntrain_dataloader = DataLoader(train_dataset, batch_size=PARAMS['batch_size'], shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=PARAMS['valid_batch_size'], shuffle=False)","4416cf68":"model = MLP().to(DEVICE)","fcf335a0":"# model = model.to(\"cuda\")\n# tensor = tensor.to(\"cuda\")","979fe96c":"optim = SGD(model.parameters(), lr=PARAMS['lr'])\ncriterion = nn.CrossEntropyLoss()","e946d3ef":"def accuracy_score_torch(y_pred, y):\n    y_pred = torch.argmax(y_pred, axis=1).cpu().numpy()\n    y = y.cpu().numpy()\n\n    return accuracy_score(y_pred, y)","51ed01f6":"for epoch in range(PARAMS['epochs']):\n    # epoch\u30eb\u30fc\u30d7\u3092\u56de\u3059\n    model.train()\n    train_loss_list = []\n    train_accuracy_list = []\n    \n    for x, y in tqdm(train_dataloader):\n        # \u5148\u307b\u3069\u5b9a\u7fa9\u3057\u305fdataloader\u304b\u3089\u753b\u50cf\u3068\u30e9\u30d9\u30eb\u306e\u30bb\u30c3\u30c8\u306edata\u3092\u53d6\u5f97\n        x = x.to(dtype=torch.float32, device=DEVICE)\n        y = y.to(dtype=torch.long, device=DEVICE)\n        \n        # pytorch\u3067\u306f\u901a\u5e38\u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u884c\u3046\u524d\u306b\u6bce\u56de\u52fe\u914d\u3092\u30bc\u30ed\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\n        optim.zero_grad()\n        # \u9806\u4f1d\u64ad\u3092\u884c\u3046\n        y_pred = model(x)\n        # loss\u306e\u5b9a\u7fa9 \u4eca\u56de\u306fcross entropy\u3092\u7528\u3044\u307e\u3059\n        loss = criterion(y_pred, y)\n        # \u8aa4\u5dee\u9006\u4f1d\u64ad\u3092\u884c\u306a\u3063\u3066\u30e2\u30c7\u30eb\u3092\u4fee\u6b63\u3057\u307e\u3059(\u8aa4\u5dee\u9006\u4f1d\u64ad\u306b\u3064\u3044\u3066\u306fhttp:\/\/hokuts.com\/2016\/05\/29\/bp1\/)\n        loss.backward() # \u9006\u4f1d\u64ad\u306e\u8a08\u7b97\n        # \u9006\u4f1d\u64ad\u306e\u7d50\u679c\u304b\u3089\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\n        optim.step()\n        \n        train_loss_list.append(loss.item())\n        train_accuracy_list.append(accuracy_score_torch(y_pred, y))\n    \n    model.eval()\n    valid_loss_list = []\n    valid_accuracy_list = []\n\n    for x, y in tqdm(valid_dataloader):\n        x = x.to(dtype=torch.float32, device=DEVICE)\n        y = y.to(dtype=torch.long, device=DEVICE)\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n        \n        valid_loss_list.append(loss.item())\n        valid_accuracy_list.append(accuracy_score_torch(y_pred, y))\n    \n    print('epoch: {}\/{} - loss: {:.5f} - accuracy: {:.3f} - val_loss: {:.5f} - val_accuracy: {:.3f}'.format(\n        epoch,\n        PARAMS['epochs'], \n        np.mean(train_loss_list),\n        np.mean(train_accuracy_list),\n        np.mean(valid_loss_list),\n        np.mean(valid_accuracy_list)\n    ))","41051fc5":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, ), (0.5, ))\n])\n\ntest_dataset = KMNISTDataset(\n    sample_submission_df[ID],\n    sample_submission_df[TARGET],\n    PATH['test_image_dir'],\n    transform=transform\n)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=PARAMS['test_batch_size'], shuffle=False)","95d9f962":"model.eval()\npredictions = []\n\nfor x, _ in test_dataloader:\n    x = x.to(dtype=torch.float32, device=DEVICE)\n    \n    with torch.no_grad():\n        y_pred = model(x)\n        y_pred = torch.argmax(y_pred, axis=1).cpu().numpy()\n        y_pred = y_pred.tolist()\n        \n    predictions += y_pred","b0d42559":"sample_submission_df[TARGET] = predictions","3b6e3876":"sample_submission_df.to_csv('submission.csv', index=False)\nfrom IPython.display import FileLink\nFileLink('submission.csv')","0f274d0c":"sns.countplot(sample_submission_df[TARGET])\nplt.title('test prediction label distribution')\nplt.show()","1a077d4d":"fig, ax = plt.subplots(2, 5)\nfig.set_size_inches(4 * 5, 4 * 2)\n\nfor i, row in sample_submission_df.iloc[:10,:].iterrows():\n    fname, label = row[ID], row[TARGET]\n    img = cv2.imread(os.path.join(PATH['test_image_dir'], fname))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ax[i\/\/5,i%5].imshow(img, 'gray')\n    ax[i\/\/5,i%5].set_title(f'{fname} - label: {label}')","ea88d021":"\u3068\u308a\u3042\u3048\u305a\u3053\u306enotebook\u3067pytorch\u306b\u6163\u308c\u307e\u3057\u3087\u3046\uff01\uff01\uff01\n\n\u308f\u304b\u3089\u306a\u3044\u3053\u3068\u304c\u3042\u3063\u305f\u3089\u3069\u3093\u3069\u3093\u8cea\u554f\u3057\u3066\u304f\u3060\u3055\u3044\u306d\n\n\u307e\u305a\u306f\u4e0a\u304b\u3089\u9806\u306b\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046","9cb961da":"kaggle\u63d0\u51fa\u7528\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306acsv\u3092\u4f5c\u6210\u3057\u307e\u3057\u3087\u3046","b92888ca":"\u8a66\u3057\u306btrain\u30c7\u30fc\u30bf\u306e\u5185\u5bb9\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3057\u3087\u3046","6d0a679d":"* Transforms: \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u62c5\u5f53\n\n* Dataset: \u30c7\u30fc\u30bf\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u8fd4\u3059\u30e2\u30b8\u30e5\u30fc\u30eb(transforms\u3067\u524d\u51e6\u7406\u3057\u305f\u3082\u306e\u3092\u8fd4\u3059\uff09\n\n* DataLoader: \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u56fa\u3081\u3066\u8fd4\u3059\u30e2\u30b8\u30e5\u30fc\u30eb","c2b65be5":"# Input Modules","8e4d946b":"\u81ea\u5206\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3044\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u3053\u3053\u3067\u7d44\u307f\u307e\u3059\u3002\n\n\u57fa\u672c\u7684\u306bPyTorch\u3067\u306f\uff0c\u30e2\u30c7\u30eb\u3092\u30af\u30e9\u30b9\u3068\u3057\u3066\u5ba3\u8a00\u3057\u307e\u3059\uff0e\nPyTorch\u3092\u4f7f\u3046\u3068\u304d\u306f\uff0cnn.Module\u3092\u7d99\u627f\u3057\u305f\u30af\u30e9\u30b9\u3092\u5ba3\u8a00\u3057\u3066\uff0e\u305d\u306e\u4e2d\u306bforward\u95a2\u6570\u3092\u4f5c\u308a\u307e\u3059\uff0e\n\n\u4f7f\u3044\u305f\u3044\u30ec\u30a4\u30e4\u30fc\uff08CNN\u3068\u304b\uff0cDense\u3068\u304b\uff0cactivation function\uff09\u3092__init__\u306e\u306a\u304b\u3067\u5b9a\u7fa9\u3057\u3066\uff0c\nforward\u306e\u4e2d\u3067\u306f\uff0c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306b\u884c\u308f\u305b\u305f\u3044\u51e6\u7406\u3092\u8a18\u8f09\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\n1. __init__(self): \u30af\u30e9\u30b9\u304b\u3089\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\u3057\u305f\u3068\u304d\u306b\u81ea\u52d5\u7684\u306b\u5b9f\u884c\u3055\u308c\u308b\u95a2\u6570\u3067\u3059\uff0e\nPyTorch\u3067\u306f\u57fa\u672c\u7684\u306b\u3053\u3053\u3067\u30e2\u30c7\u30eb\u306e\u4e2d\u8eab\u3092\u8a2d\u5b9a\u3057\u3066\u3084\u308a\u307e\u3059\uff0e\n\u57fa\u672c\u7684\u306b\u306f\u300c\u5b66\u7fd2\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u91cd\u307f\u300d\u3092\u6301\u3063\u3066\u3044\u308b\u5c64\u304c\u3053\u3053\u3067\u8a2d\u5b9a\u3055\u308c\u307e\u3059\uff0e\n\u4ed6\u306b\u3082\uff0c\u6d3b\u6027\u5316\u95a2\u6570\u306a\u3069\u3092\u3053\u3053\u3067\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\n\n1. forward(self, x) (\u9806\u4f1d\u64ad\u95a2\u6570)\nPyTorch\u3067\u306f\uff0c\u5b9f\u969b\u306b\u4f55\u304b\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u529b\u3057\u305f\u5834\u5408\u306b\u306f\u3053\u306e\u95a2\u6570\u306e\u4e2d\u8eab\u304c\u5b9f\u884c\u3055\u308c\uff0c\u8a08\u7b97\u7d50\u679c\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\uff0e\n\n\u3082\u306e\u3059\u3054\u30fc\u3044\u7c21\u7565\u5316\u3057\u3066\u304b\u304f\u3068\uff0c\u3053\u3093\u306a\u611f\u3058\u306b\u306a\u308a\u307e\u3059\uff0e\n\nmodel = Net() <-- \u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\u3059\u308b\n\noutput = model(input_images) <-- \u3053\u308c\u3092\u5b9f\u884c\u3059\u308b\u3068\uff0c\ninput_images \u304c forward()\u306b\u6e21\u3055\u308c\u3066\uff0c\u8a08\u7b97\u7d50\u679c\u304c output \u306b\u6e21\u3055\u308c\u308b","7222bbcc":"\u3053\u3053\u304c\u57cb\u3081\u308c\u305f\u3089\u4e0b\u8a18\u3067 model = MLP().to(DEVICE)\u306b\u306a\u3063\u3066\u308b\u90e8\u5206\u306eMLP\u3092Net()\u306b\u66f8\u304d\u76f4\u3057\u3066submission\u3057\u3066\u307f\u307e\u3057\u3087\u3046!\u3000\uff11\u56de\u76ee\u3088\u308a\u7cbe\u5ea6\u306f\u4e0a\u304c\u3063\u305f\u304b\u306a\uff1f","c5dd3036":"\u5b9f\u969b\u306bDataset\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002\ntransforms\u306f\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u3059\u308b\u3082\u306e\u3067\u3057\u305f\u306d\u3002","d6b422a1":"# Data Loader\u306e\u4f5c\u6210","d900a279":"train\u30c7\u30fc\u30bf\u306e\u5927\u304d\u3055\u3092\u78ba\u8a8d\u3057\u307e\u3059\n\nkaggle\u3092\u3084\u308b\u969b\u306b\u306f\uff08\u7814\u7a76\u3084\u308b\u4e0a\u3067\u3082\uff1f\uff09\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30b5\u30a4\u30ba\u306a\u3069\u306e\u78ba\u8a8d\u306f\u91cd\u8981\u306a\u306e\u3067\u30de\u30b9\u30c8\u3067\u3084\u308c\u308b\u3068\u826f\u3044\u3067\u3059\u306d","a7476569":"Pytorch\u306b\u306ftransforms, Dataset, Dataloader\u3068\u3044\u3046\u4e09\u7a2e\u306e\u795e\u5668\u304c\u3042\u3063\u3066\u3053\u308c\u3092\u4f7f\u3044\u3053\u306a\u305b\u308b\u3088\u3046\u306b\u306a\u308b\u3068\u30c7\u30fc\u30bf\u30ed\u30fc\u30c9\u5468\u308a\u304c\u975e\u5e38\u306b\u697d\u306b\u30b9\u30de\u30fc\u30c8\u306b\u306a\u308a\u307e\u3059\uff0e (\u591a\u5206tensorflow\u306b\u306f\u306a\u3044\uff1f\uff09","38a7fc34":"train\u30c7\u30fc\u30bf\u306e\u4e2d\u8eab\u3092\u78ba\u8a8d\u3057\u307e\u3059\n","8e9b7586":"\u5b9f\u969b\u306b\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3092\u884c\u3044\u307e\u3059","9b7afcf2":"pytorch\u3067\u306fdata\u306e\u30ed\u30fc\u30c9\u3068\u524d\u51e6\u7406\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3082\u306e\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u3056\u3063\u304f\u308a\u3068\u3057\u305f\u8aac\u660e\u3059\u308b\u306e\u3067\u521d\u3081\u307e\u3057\u3066\u306e\u4eba\u306f\u307b\u3048\u3048\u3048\u305d\u3046\u306a\u3093\u3060\u304f\u3089\u3044\u3067\u805e\u3044\u3068\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u4ee5\u4e0b\u306eqiita\u8a18\u4e8b\u306a\u3069\u3082\u5206\u304b\u308a\u3084\u3059\u3044\u306e\u3067\u307f\u3066\u52c9\u5f37\u3059\u308b\u306e\u3082\u304a\u3059\u3059\u3081\u3067\u3059\u3002\nhttps:\/\/qiita.com\/takurooo\/items\/e4c91c5d78059f92e76d\n","4bed82e9":"# Model\u30af\u30e9\u30b9\u306e\u5b9a\u7fa9","b0ac2cf3":"train\u30c7\u30fc\u30bf\u3092train\u3068valid\u306b\u5206\u5272\u3057\u307e\u3059\u3002python\u3067\u306fvalid_size\u306a\u3069\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u30e9\u30f3\u30c0\u30e0\u306btrain\u3068valid\u3078\u306e\u5206\u5272\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u8cea\u554ftrain\u30c7\u30fc\u30bf, valid\u30c7\u30fc\u30bf, test\u30c7\u30fc\u30bf\u3063\u3066\u306a\u3093\u3067\u3057\u305f\u3063\u3051\uff1f","d91b09e2":"\u65e9\u901f\u5b66\u7fd2\u3092\u56de\u3057\u3066\u307f\u307e\u3057\u3087\u3046\n\n","2f0d49fb":"PyTorch\u3067\u306fGPU\u3092\u4f7f\u3046\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002GPU\u306b\u660e\u793a\u7684\u306b\u9001\u308b\u5fc5\u8981\u304c\u3042\u308b\u3093\u3067\u3059\u306d\u3002","d69bf540":"\u4e88\u6e2c\u3068\u6b63\u89e3\u3092\u5165\u529b\u3057\u3066accuracy\u3092\u8fd4\u3059\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\n","2d363d66":"train_df\u306btrain\u7528\u30c7\u30fc\u30bf\u3092\u5165\u308c\u307e\u3059","1c9e1b08":"\u7d9a\u3044\u3066\u4eca\u56de\u306f\uff11\uff10\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\u30e9\u30d9\u30eb\u6570\u306e\u78ba\u8a8d\u3082\u3057\u307e\u3059\n","e6240697":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30e9\u30d9\u30eb\u6570\u3092\u53ef\u8996\u5316\u3057\u3066\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u5168\u3066\u540c\u3058\u679a\u6570\u3067\u504f\u308a\u304c\u306a\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u306d\u3002","220b6da6":"\u3053\u3053\u307e\u3067\u5b9f\u884c\u3067\u304d\u305f\u3089\u6b21\u306f\u81ea\u5206\u3067CNN\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u52d5\u304b\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","342f7196":"\u4e88\u6e2c\u7d50\u679c\u3092\u78ba\u8a8d\u3057\u307e\u3059","6bb7f59e":"\u5165\u529b\u30c7\u30fc\u30bf\u306e\u753b\u50cf\u30b5\u30a4\u30ba\u306e\u78ba\u8a8d\u3092\u3057\u307e\u3059\u3002\n\n\u5165\u529b\u30b5\u30a4\u30ba\u306f\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u4e0a\u3067\u5fc5\u9808\u306a\u306e\u3067\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046","38969f7f":"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3082\u540c\u69d8\u306btransform\u306a\u3069\u3092\u65bd\u3057\u3066\u304a\u304d\u307e\u3059","6a52e408":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u5b9a\u7fa9\u3068\u30cf\u30a4\u30d1\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5b9a\u7fa9\u3092\u884c\u3044\u307e\u3059\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u306f\u5b66\u7fd2\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf(\u91cd\u307f\u3084\u30d0\u30a4\u30a2\u30b9\u306a\u3069\uff09\u3068\u81ea\u5206\u3067\u5024\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308b\u30cf\u30a4\u30d1\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u5b58\u5728\u3057\u307e\u3059\u3002\n\n\u30cf\u30a4\u30d1\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4ee5\u4e0b\u306b\u5b9a\u7fa9\u3057\u307e\u3059\u3002"}}