{"cell_type":{"8f66b00f":"code","4f784793":"code","53585a9f":"code","e08d50ed":"code","4499cc45":"code","cc5c4470":"code","4fffe1ad":"code","554aeab1":"code","2324d1b8":"code","246c6554":"code","ee9e2f91":"code","413d11a2":"code","970b9c81":"code","e4952a12":"code","f151f3d3":"code","f59a262e":"code","e52505dd":"code","c7206faf":"code","4f631e38":"code","facac796":"code","b7eabb23":"code","5f65cfee":"code","94319dcf":"code","63d03a3f":"code","7c78f5e9":"code","e5acf95b":"code","8612c566":"code","4bf1015a":"code","6dbe9ebc":"code","630c40a6":"code","00902675":"code","8f355ebf":"code","ba1c92e9":"code","021983d8":"code","9e277e2c":"code","86465239":"code","53b70aa6":"code","19868d80":"code","1699d6b5":"code","cdee25dc":"code","8427a1a6":"code","e1b810f4":"code","4d15afdc":"code","f5b6bb16":"code","1cb7eecc":"code","03206016":"code","3104752d":"code","279fd6cf":"markdown","93a18384":"markdown","f7672add":"markdown","2fa530f2":"markdown","3ce9246b":"markdown","1e37de85":"markdown","4a4a494b":"markdown","55f3f4e8":"markdown","bc56af3d":"markdown","03dc4b17":"markdown","2061a56d":"markdown","07c9a39f":"markdown","240be4f3":"markdown"},"source":{"8f66b00f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4f784793":"df_gender = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","53585a9f":"df_gender","e08d50ed":"y_test = df_gender.drop(columns=['PassengerId'])\ny_test","4499cc45":"df_test","cc5c4470":"x_test = df_test.drop(columns='PassengerId')\nx_test","4fffe1ad":"df_train","554aeab1":"x_train = df_train.drop(columns=['PassengerId'])\nx_train","2324d1b8":"x_train.isnull().sum()","246c6554":"x_test.isnull().sum()","ee9e2f91":"x_train.describe()","413d11a2":"# Fill null value in Age with medean value \"28\"\nx_train.Age = x_train.Age.fillna(value=28)\n\n# Fill null value in Cabin with \"0\" and other value with \"1\"\nx_train.Cabin = x_train.Cabin.fillna(value=0)\nfor i in range(0, len(x_train)):\n    if x_train.Cabin[i] != 0:\n        x_train.Cabin[i] = 1\n\n# Change \"Sex\" Value female = 0 & male = 1\nsex = {'male': 1, 'female':0}\nx_train.Sex = x_train.Sex.replace(sex)\n\n# Change \"Embarked\" value C=0, Q=1, S=2\nembarked = {'C':0, 'Q':1, 'S':2}\nx_train.Embarked = x_train.Embarked.replace(embarked)\nx_train","970b9c81":"x_train.isna().sum()","e4952a12":"x_train.Embarked.describe()","f151f3d3":"# Fill Null value in Embarked with \"2\"\nx_train.Embarked = x_train.Embarked.fillna(value=2)\nx_train.isnull().sum()","f59a262e":"x_test.describe()","e52505dd":"# Fill null value in Age with medean value \"28\"\nx_test.Age = x_test.Age.fillna(value=28)\n\n# Fill null value in Cabin with \"0\" and other value with \"1\"\nx_test.Cabin = x_test.Cabin.fillna(value=0)\nfor i in range(0, len(x_test)):\n    if x_test.Cabin[i] != 0:\n        x_test.Cabin[i] = 1\n\n# Change \"Sex\" Value female = 0 & male = 1\nsex = {'male': 1, 'female':0}\nx_test.Sex = x_test.Sex.replace(sex)\n\n# Change \"Embarked\" value C=0, Q=1, S=2\nembarked = {'C':0, 'Q':1, 'S':2}\nx_test.Embarked = x_test.Embarked.replace(embarked)\n\n# Fill null value in 'Fare' with mean value 35.627188\nx_test.Fare = x_test.Fare.fillna(value=35.627188)\n\nx_test","c7206faf":"x_train.isna().sum()","4f631e38":"# Now we run machine learning model so that we don't need \"Name\", \"Ticket\"\ny_train = x_train['Survived']\nx_train = x_train.drop(columns=['Name', 'Ticket', 'Survived'])\nx_test = x_test.drop(columns=['Name', 'Ticket'])\ny_test = y_test['Survived']\n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","facac796":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","b7eabb23":"models = {'Logistic Regression': LogisticRegression(),\n          'Random Forest Classifier': RandomForestClassifier(),\n          'K-Neighbors Classifier': KNeighborsClassifier()}\n\nscore = {}\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    score[name] = model.score(x_test, y_test)\n    \nscore","5f65cfee":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","94319dcf":"score = pd.DataFrame(score, index=['accuracy'])\nscore.T.plot.bar()","63d03a3f":"# Let's tune KNN\ntrain_score = []\ntest_score = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbours\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    # Fit the algorithm\n    knn.fit(x_train, y_train)\n    # Update the training scores list\n    train_score.append(knn.score(x_train, y_train))\n    # Update the testing scores list\n    test_score.append(knn.score(x_test, y_test))","7c78f5e9":"train_score","e5acf95b":"test_score","8612c566":"plt.figure(figsize=(15,7))\nsns.lineplot(x=neighbors, y=test_score)\nsns.lineplot(x=neighbors, y=train_score)\nplt.xticks(ticks=np.arange(1,21,1))\nplt.xlabel('Number of neighbors')\nplt.ylabel('Model Score')\nplt.legend(['Test Score', 'Train Score'])\nprint(f'Maximum KNN Score on the test data:{max(test_score)*100:.2f}%')","4bf1015a":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {'C': np.logspace(-4, 4, 20),\n                'solver': ['liblinear']}\n\n# Tune LogisticRegression \nnp.random.seed(42)\n\n# setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(x_train, y_train)","6dbe9ebc":"rs_log_reg.best_params_","630c40a6":"rs_log_reg.score(x_test, y_test)","00902675":"# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {'n_estimators': np.arange(10, 1000, 50),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split': np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2)}\n\n# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(x_train, y_train)","8f355ebf":"# Find the best hyperparameters\nrs_rf.best_params_","ba1c92e9":"# Evaluate the randomized search randomforestClassifier model\nrs_rf.score(x_test, y_test)","021983d8":"# Diffrent hyperparameters for our LogisticRegression model\nlog_reg_grid = {'C': np.logspace(-4,4,30),\n                'solver': ['liblinear']}\n\n# setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid= log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(x_train, y_train)","9e277e2c":"gs_log_reg.best_params_","86465239":"gs_log_reg.score(x_test, y_test)","53b70aa6":"# make predictions with tuned model\ny_preds = gs_log_reg.predict(x_test)","19868d80":"y_preds","1699d6b5":"y_test","cdee25dc":"plot_roc_curve(gs_log_reg, x_test, y_test)","8427a1a6":"confusion_matrix(y_test, y_preds)","e1b810f4":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cbar=False, fmt='.0f')\nplt.ylabel('Predicted Label')\nplt.xlabel('True Label')","4d15afdc":"print(classification_report(y_test, y_preds))","f5b6bb16":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=1.3738237958832638, solver='liblinear')\n\nclf.fit(x_train, y_train)","1cb7eecc":"clf.coef_","03206016":"# Match coef's of features to columns\nfeature_dict = dict(zip(x_test.columns, list(clf.coef_[0])))\nfeature_dict","3104752d":"feature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title='Feature Importance', legend=False)","279fd6cf":"# Import Data","93a18384":"# Choose a Model","f7672add":"## Hyperparameter Tuning with GridSearchCV\n\nLogisticRegression model provides the best scores, try and improve them again using GridSearchCV...","2fa530f2":"## Feature Importance \n\nfor LogisticRegression model...","3ce9246b":"# Data Preparation","1e37de85":"### Variable Notes\n\n* pclass: A proxy for socio-economic status (SES)\n    * 1st = Upper\n    * 2nd = Middle\n    * 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n    * Sibling = brother, sister, stepbrother, stepsister\n    * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n    * Parent = mother, father\n    * Child = daughter, son, stepdaughter, stepson\n\n* Some children travelled only with a nanny, therefore parch=0 for them.","4a4a494b":"# Overview\n\n### The data has been split into two groups:\n\n> training set (train.csv)\n> test set (test.csv)\n\n* The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\n\n* The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n\n* We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.","55f3f4e8":"## HyperParameters Tuning (by hand)","bc56af3d":"# Parameter Tuning","03dc4b17":"### Data Dictionary\n\n* Variable  --->\tDefinition    --->\tKey\n* survival  --->\tSurvival      --->    0 = No, 1 = Yes\n* pclass    --->\tTicket class  --->\t1 = 1st, 2 = 2nd, 3 = 3rd\n* sex       --->\tSex\t\n* Age       --->\tAge in years\t\n* sibsp     --->\t# of siblings \/ spouses aboard the Titanic\t\n* parch     --->\t# of parents \/ children aboard the Titanic\t\n* ticket    --->\tTicket number\t\n* fare      --->\tPassenger fare\t\n* cabin     --->\tCabin number\t\n* embarked  --->\tPort of Embarkation   --->\tC = Cherbourg, Q = Queenstown, S = Southampton\n","2061a56d":"## Evaluting our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* classification report\n* Precision\n* Recall\n* F1-Score\n\n....and it would be great it cross-validaton was used where possible\n\nto make comaparistions and evaluate our trained model, first we need to make predictions.","07c9a39f":"# Train & Evaluate the Model","240be4f3":"## Hyperparameter tunning with RandomizedSearchCV\n\nTune the model:\n* LogiticRegression()\n* RandomForestClassifier()\n\n... using RandomizedSearchCV"}}