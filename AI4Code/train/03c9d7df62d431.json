{"cell_type":{"e99a8e6e":"code","33100947":"code","70548a5c":"code","1acbffd7":"code","8b206a2a":"code","5a8ec01c":"code","f0ed6e61":"code","8f17efd8":"code","06b132b6":"code","52632cd0":"code","29814f97":"code","2cf6933f":"code","77798829":"code","ea714236":"code","a1107120":"code","e7421fbd":"code","1c2959ef":"code","d9d066ce":"markdown","01ef41ec":"markdown","a25b6013":"markdown","018f1653":"markdown","fc04cd02":"markdown","830c607b":"markdown","090eb5cd":"markdown","fe90eb0f":"markdown","bc65ae5e":"markdown","e482ab8c":"markdown","75091127":"markdown","720e4472":"markdown"},"source":{"e99a8e6e":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom nltk import sent_tokenize\nfrom tqdm import tqdm\nfrom scipy import spatial\nfrom operator import itemgetter\ntqdm.pandas()\n%matplotlib inline","33100947":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","70548a5c":"movie = pd.read_csv('..\/input\/wikipedia-movie-plots\/wiki_movie_plots_deduped.csv')\nfull_mov = pd.read_csv('..\/input\/movie-database\/full_mov_db.csv')\nfull_mov.head()","1acbffd7":"movie.head()","8b206a2a":"len(movie)","5a8ec01c":"## Drop duplicates\nmovie = movie.drop_duplicates(subset='Plot', keep=\"first\")\nlen(movie)","f0ed6e61":"movie.reset_index(inplace=True)\nmovie.drop(columns=['index'],inplace=True)\nmovie.head()","8f17efd8":"movie['word count'] = movie['Plot'].apply(lambda x : len(x.split()))","06b132b6":"movie['word count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='no of plots',\n    title='Plot Word Count Distribution')","52632cd0":"movie['Origin\/Ethnicity'].value_counts().iplot(kind='bar')","29814f97":"movie['Release Year'].value_counts().iplot(kind='bar')","2cf6933f":"import re\ndef clean_plot(text_list):\n    clean_list = []\n    for sent in text_list:\n        sent = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-.:;<=>?@[\\]^`{|}~\"\"\"), '',sent)\n        sent = sent.replace('[]','')\n        sent = re.sub('\\d+',' ',sent)\n        sent = sent.lower()\n        clean_list.append(sent)\n    return clean_list","77798829":"plot_emb_list = []\nwith tf.Graph().as_default():\n    embed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\")\n    messages = tf.placeholder(dtype=tf.string, shape=[None])\n    output = embed(messages)\n    with tf.Session() as session:\n        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        for plot in tqdm_notebook(full_mov['Plot']):\n            sent_list = sent_tokenize(plot)\n            clean_sent_list = clean_plot(sent_list)\n            sent_embed = session.run(output, feed_dict={messages: clean_sent_list})\n            plot_emb_list.append(sent_embed.mean(axis=0).reshape(1,512))            \nfull_mov['embeddings'] = plot_emb_list\nfull_mov.head()","ea714236":"full_mov.to_pickle('.\/updated_mov_df_use_2.pkl')","a1107120":"def similar_movie(movie_name,topn=5):\n    plot = full_mov.loc[movie_name,'Plot'][:1][0]\n    with tf.Graph().as_default():\n        embed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\")\n        messages = tf.placeholder(dtype=tf.string, shape=[None])\n        output = embed(messages)\n        with tf.Session() as session:\n            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n            sent_list = sent_tokenize(plot)\n            clean_sent_list = clean_plot(sent_list)\n            sent_embed2 = (session.run(output, feed_dict={messages: clean_sent_list})).mean(axis=0).reshape(1,512)\n            similarities = []\n            for tensor,title in zip(full_mov['embeddings'],full_mov.index):\n                cos_sim = 1 - spatial.distance.cosine(sent_embed2,tensor)\n                similarities.append((title,cos_sim))\n            return sorted(similarities,key=itemgetter(1),reverse=True)[1:topn+1]","e7421fbd":"full_mov.set_index('Title', inplace=True)","1c2959ef":"similar_movie('Interstellar')","d9d066ce":"### Pickling the embeddings for future (re)use","01ef41ec":"# **Movie Recommender System based on Wiki-plots using sentence embeddings**\n![recommended movie?](https:\/\/alvinalexander.com\/sites\/default\/files\/2017-09\/netflix-christmas-movie-suggestions.jpg)*You may like these?*\n\n#### Dataset contains movie plots scraped from wikipedia from 1902-2017 from approximately 22 regions along with important metadata\n<!-- blank line -->\n----\n## Content (plot) based Recommender System\n### We embed all sentences within the plots using Google's [Universal Sentence Encoder](https:\/\/arxiv.org\/abs\/1803.11175) and compare the input plot's associated embeddings using [Cosine similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity)","a25b6013":"### **Imports**","018f1653":"### **Embedding using USE** (for more info refer - [USE tutorial](https:\/\/colab.research.google.com\/github\/tensorflow\/hub\/blob\/master\/examples\/colab\/semantic_similarity_with_tf_hub_universal_encoder.ipynb))","fc04cd02":"## **The results seem to make sense**\n### You can use other embeddings like BERT\/ELMo\/Flair to get better\/different results\n### An upvote will be appreciated :)","830c607b":"### Testing our model - using Interstellar's plot","090eb5cd":"### **Plot text preprocessing**","fe90eb0f":"### ** bare-bones EDA**","bc65ae5e":"## Similar Movie function","e482ab8c":"### Plotly imports","75091127":"### Dropping duplicate plots (multi lingual movie releases generally have wiki pages for more than one language versions)","720e4472":"### Loading Dataset"}}