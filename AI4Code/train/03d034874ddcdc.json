{"cell_type":{"686608ea":"code","fb0ccf82":"code","71f31a2e":"code","8d8b14c5":"code","610f5518":"code","e4d56a07":"code","d5c37363":"code","d5f7dd0e":"code","1c069744":"code","7c770824":"code","38106c0b":"code","8211a80e":"code","31a28ff1":"code","b5305b77":"code","a9d524e1":"code","7cdba284":"code","94cdf4ba":"code","8beabab0":"code","d098c1e8":"code","6eb22a63":"code","60ad37ea":"code","412a6ce2":"code","b175df02":"code","9e3a816f":"code","8c41a0b3":"code","a2157cbd":"code","2a3145a5":"code","08de151e":"code","c963e153":"code","f5b33d10":"code","7ab3df5f":"code","8065102f":"code","dee7f09a":"code","21234b63":"code","9bca7a4c":"markdown","77ba843e":"markdown","410f82bf":"markdown","0e8e7e7c":"markdown","9fc286d4":"markdown","d6d7f8b1":"markdown","219375b4":"markdown","54d71bc5":"markdown","437d7e05":"markdown","069f1d71":"markdown","77f9485b":"markdown","ac4e3b57":"markdown","12fd6f92":"markdown","ecf29527":"markdown","01507ef0":"markdown","ced7a998":"markdown","f85f341e":"markdown","4819bf61":"markdown","5917696b":"markdown","8f19b97a":"markdown","06e53274":"markdown","1412f98e":"markdown"},"source":{"686608ea":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as pyo\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import plot_precision_recall_curve,plot_roc_curve\n%matplotlib inline","fb0ccf82":"df = pd.read_csv('..\/input\/ethereum-frauddetection-dataset\/transaction_dataset.csv')\npd.set_option('display.max_columns', None)\ndf.head()","71f31a2e":"df.drop(['Unnamed: 0','Index'],axis =1,inplace = True)\ndf.head()","8d8b14c5":"# Checking the counts of each label (fraud and non-fraud)\n\nprint(df['FLAG'].value_counts())\n\nprint()\n#Checking if there is rows without determining whether the transaction is fraud or not\n\nprint(df['FLAG'].isnull().sum())","610f5518":"#Important information about each feature\n\ndf.describe()","e4d56a07":"# Looking at the number if null values of each feature\n\nprint (df.isnull().sum()[df.isnull().sum()>0])\n\nprint()\n# Checking the number of unique values on these specific categorical features\n\nprint(df[' ERC20_most_rec_token_type'].nunique())\nprint(df[' ERC20 most sent token type'].nunique())","d5c37363":"#checking for dupplicated values\ndf[df.duplicated()== True]\ndf.drop_duplicates(inplace = True)\n\n### Renaming The Features Names To The Same Names Without Spaces\ndf_copy = df.copy()\nfor i in df_copy.columns:\n    df_copy.rename(columns=str.strip,inplace = True)\n    \n### Making Mean Imputatinons To Numeric Values\nfor i in df_copy.columns:\n    if df_copy[i].isnull().sum() == 829:\n        df_copy[i].replace({np.NaN:df_copy[i].mean()},inplace=True)\n    else:\n        pass\n    \n    \n#checking again the number of nulls, expecting only 2 categorical values to stay with null values\nprint(df_copy.isnull().sum()[df_copy.isnull().sum()>0])\n\n\n#looking at the value counts of the categorical feature\nprint(f\"Value Counts of Categorical feature: \\n{df_copy['ERC20_most_rec_token_type'].value_counts()}\")","d5f7dd0e":"#cleaning the categorical feature - changing 0 values to null, cause a 0 value doesnt mean anything in categorical features\ndf_copy['ERC20_most_rec_token_type'].replace({'0':np.NaN},inplace = True)\ndf_copy['ERC20 most sent token type'].replace({'0':np.NaN},inplace = True)","1c069744":"#looking for nulls this feature\nprint(df_copy['ERC20 most sent token type'].isnull().sum()\/len(df_copy))\nprint(df_copy['ERC20 most sent token type'].max)","7c770824":"#making a list of the features which is their variance is equal to 0\nto_drop = list(df_copy.var()[df_copy.var() == 0].keys())\nprint(to_drop)\n\n\n#droping those featues\ndf_copy.drop(list(df_copy.var()[df_copy.var() == 0].keys()),inplace = True, axis = 1)\ndf_copy.describe()\n\n#we also can see that we still have 5 features that their std are very close to 0 ,\n#we want to drop them also","38106c0b":"df_copy.drop(['min value sent to contract','max val sent to contract','avg value sent to contract'\\\n              ,'total ether sent contracts','ERC20 uniq sent addr.1', 'Address']\\\n             ,axis =1 , inplace = True)","8211a80e":"# filling with mode the categorical missing vaue\ndf_copy = df_copy.fillna(df_copy.mode().iloc[0])\ndf_copy.isnull().sum()[df_copy.isnull().sum()>0]","31a28ff1":"corr = df_copy.corr()\ncorr_df = corr[corr>0.6].dropna(axis = 1 ,thresh = 2)\ncorr_df = corr_df[corr_df != 1]\nto_drop = corr_df[corr_df>0.9]\nto_drop.values[to_drop.values>0]\nto_drop.unstack().sort_values(kind=\"quicksort\")[to_drop.unstack()>0]","b5305b77":"## Correlation Analysis\n\ncorr = df_copy.corr()\ncorr_df = corr[corr>0.6].dropna(axis = 1 ,thresh = 2)\ncorr_df = corr_df[corr_df != 1]\nto_drop = corr_df[(corr_df>0.6) & (corr_df<0.9)]\nto_drop.values[to_drop.values>0]\nto_drop = to_drop.unstack().sort_values(kind=\"quicksort\")[to_drop.unstack()>0]\n\n\ndf_droped_corr = df_copy.drop(['ERC20 total ether sent','ERC20 avg val sent','ERC20 uniq rec token name','ERC20 min val sent',\\\n                              'ERC20 max val sent','ERC20 max val rec'], axis =1)\n\n\ndf.copy = df_droped_corr.copy()\nto_drop","a9d524e1":"plt.subplots(figsize = (8, 6))\nsns.set(style = 'darkgrid')\nsns.scatterplot(data = df_copy,x = 'Unique Received From Addresses', y= 'Received Tnx',hue = 'FLAG' )\nplt.show()\n\n\nplt.subplots(figsize = (8, 6))\nsns.set(style = 'whitegrid')\nsns.scatterplot(data = df_copy,x = 'Unique Sent To Addresses', y= 'Sent tnx',hue = 'FLAG' )\nplt.show()\n\n\n\nplt.subplots(figsize = (8, 6))\nsns.scatterplot(data = df_droped_corr,x = 'ERC20 uniq sent addr', y= 'Total ERC20 tnxs',hue = 'FLAG' )\nplt.show()\n\n\n\nplt.subplots(figsize = (8, 6))\nsns.scatterplot(data = df_droped_corr,x = 'Unique Received From Addresses', y= 'Received Tnx',hue = 'FLAG' )\nplt.show()\n\n\n\nplt.subplots(figsize = (8, 6))\nsns.scatterplot(data = df_droped_corr,x = 'Sent tnx', y= 'Unique Sent To Addresses',hue = 'FLAG' )\nplt.show()\n\n\nplt.subplots(figsize = (8, 6))\nsns.scatterplot(data = df_droped_corr,x = 'ERC20 uniq rec addr', y= 'Total ERC20 tnxs',hue = 'FLAG' )\nplt.show()\n\n\nplt.subplots(figsize = (8, 6))\nsns.scatterplot(data = df_droped_corr,x = 'total transactions (including tnx to create contract', y= 'Received Tnx',hue = 'FLAG' )\nplt.show()\n\n","7cdba284":"# looking at the correlation to the label\ndf_copy.corr()['FLAG'].sort_values(ascending= False)","94cdf4ba":"# Correlation metrix for each label - fraud and non-fraud\nsample = df_copy[df_copy['FLAG']==1]\ncorr = sample.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(18,10))\n    sns.heatmap(corr,  mask=mask, annot=False, cmap='magma', center=0, linewidths=0.1, square=True)\n\n# Non-Fraudulant Correlation\nsample = df_copy[df_copy['FLAG']==0]\ncorr = sample.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(18,10))\n    sns.heatmap(corr,  mask=mask, annot=False, cmap='magma', center=0, linewidths=0.1, square=True)","8beabab0":"# standardiztion - we will try to model the data with and without standartization and then we will decide which we should choose\n# making a copy of the dataset ith only the numeric values for future calculations\n#df_numeric_values = df_copy.select_dtypes(exclude = 'object')\n#df_without_label = df_numeric_values.drop(columns = ['FLAG']) # we'll normalize the data without 'label' column\n#original_columns = df_without_label.columns # to keep the original features' names\n#original_indexes = df_without_label.index # to keep the original samples' id\n\n# standardiztion\n#standard_scaler = StandardScaler() # we initialize our scaler\n#standard_scaler.fit(df_without_label) # we fit our scaler\n#df_without_label = standard_scaler.transform(df_without_label) # we transform our dataframe using the scaler we have just fit\n\n#df_without_label = pd.DataFrame(df_without_label) # turning the normalized data to dataframe (instead of array)\n#df_without_label.columns = original_columns # returning the original features' names\n#df_without_label.index = original_indexes # returning the original samples' id\n#df_norm = pd.concat([df_without_label, df_copy['FLAG']], axis = 1) # adding back the labels\n#df_norm\n","d098c1e8":"# as we saw at the table represent the correlation to the label, the below feature is the most correlated to the label\n# we will try to find outliers from visualizing the relatioinship between this feature and the label","6eb22a63":"# Finding some outliers due to the scatter plots I ploted before.\n# later we will check if deleting them will improve our model results\ndf_copy[(df_copy['Received Tnx']>3000)&(df_copy['FLAG'] == 1)& (df_copy['Received Tnx']<4000)].index","60ad37ea":"sns.set(style = 'darkgrid')\ndata1 = df_copy['Avg min between received tnx'][df_copy['FLAG']==0]\ndata2 = df_copy['Avg min between received tnx'][df_copy['FLAG']==1]\nplt.subplots(figsize = (14, 8))\nboxplot = sns.boxplot(data=[data1,data2])\nboxplot = sns.stripplot(data=[data1,data2] ,marker=\"o\", alpha=0.3, color=\"blue\")\nboxplot.axes.set_title(\"Distribution of Avg min between received tnx\", fontsize=16)\nboxplot.set_xlabel(\"Conditions\", fontsize=14)\nboxplot.set_ylabel(\"Values\", fontsize=14)\nplt.show()\n\n\ndata1 = df_copy['Time Diff between first and last (Mins)'][df_copy['FLAG']==0]\ndata2 = df_copy['Time Diff between first and last (Mins)'][df_copy['FLAG']==1]\nplt.subplots(figsize = (14, 8))\nboxplot = sns.boxplot(data=[data1,data2])\nboxplot = sns.stripplot(data=[data1,data2] ,marker=\"o\", alpha=0.3, color=\"blue\")\nboxplot.axes.set_title(\"Distribution of Time Diff between first and last (Mins)\", fontsize=16)\nboxplot.set_xlabel(\"Conditions\", fontsize=14)\nboxplot.set_ylabel(\"Values\", fontsize=14)\nplt.show()\n\n\ndata1 = df_copy['Avg min between received tnx'][df_copy['FLAG']==0]\ndata2 = df_copy['Avg min between received tnx'][df_copy['FLAG']==1]\nplt.subplots(figsize = (14, 8))\nboxplot = sns.boxplot(data=[data1,data2])\nboxplot = sns.stripplot(data=[data1,data2] ,marker=\"o\", alpha=0.3, color=\"blue\")\nboxplot.axes.set_title(\"Distribution of Avg min between received tnx\", fontsize=16)\nboxplot.set_xlabel(\"Conditions\", fontsize=14)\nboxplot.set_ylabel(\"Values\", fontsize=14)\nplt.show()\n\n","412a6ce2":"outliers_1 = df_copy['Time Diff between first and last (Mins)'][\n    (df_copy['FLAG'] == 1) & \n                                (df_copy['Time Diff between first and last (Mins)']>600000)]\ndf_copy.drop(outliers_1.index, axis = 0, inplace = True)\n\n\n\noutliers_2 = df_copy[(df_copy['FLAG']==1)&(df_copy['Avg min between received tnx']>60000)]\n#df_for_changes.drop(outliers_2.index, axis = 0, inplace = True)\n\n\noutliers_3 = df_copy[(df_copy['Received Tnx']>8000)&(df_copy['FLAG'] ==1)]\noutliers_3\nprint(outliers_1)\n\nprint(df_copy['Avg min between received tnx'][(df_copy['FLAG']==0)& (df_copy['Avg min between received tnx'] >400000)].index)\nprint()\nprint(df_copy['Avg min between received tnx'][(df_copy['FLAG']==1)& (df_copy['Avg min between received tnx'] >140000)].index)","b175df02":"# Exchange features with more than 50 categories (unique values)\ndf_1 = df_copy.copy()\nlabel_encoding_features = [] # the list will hold the categorical features that we'll exchange by 'Label Encoding'\ncategorial_features_dict = {} # the dict will hold feature as keys, and transform categories dict as values\n\nfor feature in df_copy[['ERC20_most_rec_token_type','ERC20 most sent token type']].columns:\n    if df_copy[feature].nunique() >= 50: # if there are more than different 50 categories - we'll use 'Label Encoding'\n        label_encoding_features.append(feature)\n        categorial_features_dict[feature] = {} # the dict will hold the original values as keys, and running number (category) as values\n        i = 1 # running number (category)\n        for sample in df_copy[feature]:\n            if sample not in categorial_features_dict[feature].keys() and sample is not np.nan: # we want to replace each value (that isn't 'null') by the dict\n                categorial_features_dict[feature][sample] = i\n                i += 1\n\n        df_copy[feature].replace(categorial_features_dict[feature], inplace = True) # exchange categories in the original dataframe\n        df_copy[feature].replace(categorial_features_dict[feature], inplace = True) # exchange categories in df_categorical\n","9e3a816f":"dropping_corr=df_copy.drop(['ERC20 min val rec','ERC20 uniq sent token name','min val sent','ERC20 avg val rec',\\\n             'total ether balance','ERC20 max val rec','ERC20 total Ether received'],axis =1)\n\n\ncorr_decesion = pd.DataFrame(df_copy.corr()['FLAG'].sort_values(ascending= False))\nd = corr_decesion[(corr_decesion['FLAG']<0.02) & (corr_decesion['FLAG']>-0.02)].index\n\n\ndropping_corr = df_copy.drop(d,axis =1)\ndropping_corr ","8c41a0b3":"# Logistic Regression with the 2 categorical features\nsns.set(style = 'white')\n\nX = df_copy.drop(['FLAG',],axis =1)\nY = df_copy['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\n\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nfrom sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(max_iter = 10000)\nlog_model.fit(X_train,y_train)\ny_pred = log_model.predict(X_test)\n\n\nprint(classification_report(y_test,y_pred))\n\n\nplot_confusion_matrix(log_model,X_test,y_test)","a2157cbd":"##Random Forest with MinMax Normalization\nX = df_copy.drop(['FLAG'],axis =1)\nY = df_copy['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\n\n\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nrfc = RandomForestClassifier(n_estimators=25,max_features= 5,random_state = 101)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\n\nprint(classification_report(y_test,y_pred))\n\n\n\nplot_confusion_matrix(rfc,X_test,y_test)\n\nplot_roc_curve(rfc,X_test,y_test)","2a3145a5":"#SVM model, not good as the random forest\n\nfrom sklearn.svm import SVC\n\nX = df_copy.drop([185, 9806],axis = 0).drop(['FLAG'],axis =1)\nY = df_copy.drop([185, 9806],axis = 0)['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nmodel = SVC(kernel ='rbf', C=9)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nplot_confusion_matrix(model,X_test,y_test)\n\nprint(classification_report(y_test,y_pred))\nprint(plot_roc_curve(model,X_test,y_test))","08de151e":"# Grid search for SVM\nparam_grid = {'C':[i for i in range(1,10,1)],'kernel':['linear','rbf','poly']}\ngrid = GridSearchCV(model,param_grid,scoring = 'recall')\ngrid.fit(X_train,y_train)","c963e153":"grid.best_params_","f5b33d10":"#PCA and then trying to improve the random forest model, that is already wonderfull - the results without PCA are better\n\nX = df_copy.drop([185, 9806],axis = 0).drop(['FLAG'],axis =1)\nY = df_copy.drop([185, 9806],axis = 0)['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.decomposition import PCA\npca_model = PCA(n_components= 5)\nX = pca_model.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\nrfc = RandomForestClassifier(n_estimators=25,max_features= 5,random_state = 101)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rfc,X_test,y_test)\n\nprint(classification_report(y_test,y_pred))","7ab3df5f":"# finding more outliers and trying to improve the model\nfor i in df_copy.drop('FLAG',axis =1).columns:\n    IQR = np.percentile(df_copy[i],75) -  np.percentile(df_copy[i],25)\n    lower_limit = np.percentile(df_copy[i],25) - 1.5*IQR\n    upper_limit = np.percentile(df_copy[i],75) + 1.5*IQR\n    outliers_a = df_copy[i][df_copy[i] > upper_limit].shape \\\n    + df_copy[i][df_copy[i] < lower_limit].shape\n    if outliers_a[0]\/df_copy['Unique Sent To Addresses'].shape[0] <= 0.07:\n        print(i)\n        \n        \n        \noutliers_a = df_copy['avg val sent'][df_copy['avg val sent'] > upper_limit].shape \\\n    + df_copy['avg val sent'][df_copy['avg val sent'] < lower_limit].shape\noutliers_a\n        \n    \n    \n    \noutliers_a = df_copy['avg val sent'][df_copy['avg val sent'] > upper_limit].index\ndf_droped_corr.drop(outliers_a,axis = 0)","8065102f":"sns.set(style = 'white')\n##Random Forest after deleting the outliers - not better than the model without deleting the outliers\nX = df_copy.drop(outliers_a,axis = 0).drop(['FLAG'],axis =1)\nY = df_copy.drop(outliers_a,axis = 0)['FLAG']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nrfc = RandomForestClassifier(n_estimators=25,max_features= 5,random_state = 101)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rfc,X_test,y_test)","dee7f09a":"sns.set(style = 'white')\n##Random Forest after deleting the outliers - THE BEST RESULTS\n\nX = df_copy.drop([185, 9806],axis = 0).drop(['FLAG'],axis =1)\nY = df_copy.drop([185, 9806],axis = 0)['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nrfc = RandomForestClassifier(n_estimators=25,max_features= 5,random_state = 101)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rfc,X_test,y_test)\n\nprint(plot_roc_curve(rfc,X_test,y_test))","21234b63":"# This is The dataset after deleting many features with small correlation to the label\n# we can see that the results are slightly less good then the original dataset\n# but it is a very small change, so we may consider using this one due to the small number of features\n\n\nsns.set(style = 'white')\n\nX = dropping_corr.drop([185, 9806],axis = 0).drop(['FLAG'],axis =1)\nY = dropping_corr.drop([185, 9806],axis = 0)['FLAG']\nX_train, X_test, y_train, y_test = train_test_split(X, Y ,test_size = 0.3, random_state = 42)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nrfc = RandomForestClassifier(n_estimators=25,max_features= 5,random_state = 101)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rfc,X_test,y_test)\n\nprint(classification_report(y_test,y_pred))","9bca7a4c":"### we want found some strong outliers for the fraud transactions, we will try to find them and drop these rows","77ba843e":"# PCA","410f82bf":"# Table of Contents\n* [Import Libraries](#Import-Libraries)\n* [Exploration](#Exploration)\n* [Correlation Matrix of the numerical features](#Correlation-Matrix-of-the-numerical-features)\n* [Data Pre-Processing](#Data-Preprocessing)\n* [Heatmap](#Heatmap)\n* [Standartization](#Standartization)\n* [Dealing With Outliers](#Dealing-With-Outliers)\n* [Label Encoding](#Label-Encoding)\n* [Modeling](#Modeling)\n* [Best Model-Random Forest](#Best-Model-Random-Forest)\n* [PCA](#PCA)\n* [Back To Pre-proccessing](#Back-To-Pre-proccessing)\n* [Conclusion](#Conclusion)\n\n","0e8e7e7c":"#### We can see that the data is imbalanced, and it's something important we should consider later when modeling","9fc286d4":"In this notebook, we wanted to make a classification, and to understand which Ethereum transaction is a \nfraud, and which is not. We did EDA, we chose which attributes we should use to predict the model, and \nwhich features we should avoid. We had to deal with outliers in the data set, and we had to be aware of \ncorrelation issues, that can make us decide to avoid some features, that are highly correlated with others. \nThen we used 3 different models \u2013 Logistic Regression, SVM, and Random Forest\nThe Random Forest Model did the best work for us. to conclude, we think that we answer the question \nwell, and we could predict fraudulent transactions. For future work, we suggest adding the volatility of \nEthereum to the consideration, for example in times there is the high volatility of the crypto, it may be \nthe time that the fraudulent transactions are increasing.","d6d7f8b1":"# Standartization","219375b4":"# Dealing With Outliers","54d71bc5":"# Exploration","437d7e05":"![imgonline-com-ua-CompressToSize-89jVk0NP0Vp.jpg](attachment:a1e733b0-6a10-47e0-a092-c6a647491440.jpg)","069f1d71":"# Back To Pre-proccessing","77f9485b":"### the list of features above are also with varaincae of 0 (very close to),so we dont want to consider constant features in our classification midel","ac4e3b57":"##### we can see that we have a value called 'none' as one of the top values of the column, so we should take it into account when modeling, maybe we will change it late to null also, and then make mode imputation to those none values","12fd6f92":"# Label Encoding","ecf29527":"## Removing Features With Small Correlation To The Label","01507ef0":"#### We learned a lot from the std describing for each feature, there are few features that all of their values are around the value of 0 ,so we may not use them when modeling","ced7a998":"# Correlation Matrix of the numerical features","f85f341e":"# Import Libraries","4819bf61":"# Conclusion","5917696b":"# Modeling","8f19b97a":"# Data Preprocessing","06e53274":"# Heatmap","1412f98e":"#### After checking model results i decided to use min-max normalization"}}