{"cell_type":{"744b0f20":"code","563914d0":"code","b70c687a":"code","c268625a":"code","2307fa7d":"code","2b1604be":"code","22d91d3d":"code","68775b3d":"code","6206fc75":"code","edfd33aa":"code","0ea44084":"code","3fcacf86":"code","0d41dc50":"code","8a3f2718":"code","015de534":"code","df43e035":"code","12a359e6":"code","d7d7ee39":"code","0a80132f":"code","6c07421c":"code","8070bac2":"code","b1298f82":"code","1f27015e":"code","08c76eda":"code","8fb934bd":"code","8900fd5f":"code","a42be5b3":"code","c84edeab":"code","3088e0dd":"code","2095c3ef":"code","631a98ae":"code","f9ccf627":"code","0c3ca832":"code","08553ece":"code","ef579a41":"code","387c228d":"code","28a50048":"code","0332fbb3":"code","e7db2b89":"code","d0b0e0ec":"code","e1d85a7c":"code","4bcaab69":"code","977b0391":"code","acdf1996":"code","2e43e2b7":"code","a9f3df52":"code","5d69ca25":"code","8118c745":"code","b927c62c":"code","eb3e2e18":"code","71978374":"markdown","92b8ce05":"markdown","bdfb03c6":"markdown"},"source":{"744b0f20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","563914d0":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\nfrom sklearn.model_selection import train_test_split\n                               \nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom statsmodels.genmod.generalized_linear_model import GLM\nfrom statsmodels.genmod import families","b70c687a":"df = pd.read_csv('..\/input\/imbalanced-data-practice\/aug_train.csv')","c268625a":"df.info()","2307fa7d":"df.head()","2b1604be":"df['Response'].value_counts()","22d91d3d":"{x: len(df[x].unique()) for x in df.select_dtypes('object').columns}","68775b3d":"{x: len(df[x].unique()) for x in df.select_dtypes('number').columns}","6206fc75":"{x: df[x].unique() for x in df.select_dtypes('number').columns if len(df[x].unique()) == 2}","edfd33aa":"{x: df[x].unique() for x in df.select_dtypes('object').columns}","0ea44084":"def preprocessing(df):\n    df = df.copy()\n    df.drop(columns = ['id'], inplace=True)\n    df['male_id'] = np.where(df['Gender'] == 'Male', 1, 0)\n\n    df['veh_damage_id'] = np.where(df['Vehicle_Damage'] == 'Yes', 1, 0)\n\n    for name, val in zip(['lt_1_yr', 'btwn_1_2_years', 'plus2yrs'], ['< 1 Year', '1-2 Year', '> 2 Years']):\n        df[f'vehicle_age__{name}'] = np.where(df['Vehicle_Age'] == val, 1, 0)\n        \n    df.drop(columns = ['Gender', 'Vehicle_Damage', 'Vehicle_Age'], inplace=True)\n\n\n    return df.drop(columns = ['Response']), df['Response']","3fcacf86":"X, y = preprocessing(df)    ","0d41dc50":"df2.head()","8a3f2718":"df2.isnull().sum(axis=0)","015de534":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7, \n                                                    shuffle=True, \n                                                    random_state=1)","df43e035":"\ntrain_d_matrix = xgb.DMatrix(X_train, label = y_train)\ntest_d_matrix = xgb.DMatrix(X_test, label = y_test)\n\nxgb_skl = XGBClassifier(objective = 'binary:logistic')\n\nxgb_skl.fit(X_train, y_train)","12a359e6":"\n# i.e. now train this using xgb package using the same params\nparams = {'base_score': 0.5,\n 'booster': 'gbtree',\n 'colsample_bylevel': 1,\n 'colsample_bynode': 1,\n 'colsample_bytree': 1,\n 'gamma': 0,\n 'gpu_id': -1,\n 'importance_type': 'gain',\n 'interaction_constraints': '',\n 'learning_rate': 0.300000012,\n 'max_delta_step': 0,\n 'max_depth': 6,\n 'min_child_weight': 1,\n 'missing': np.nan,          \n 'monotone_constraints': (),    \n 'n_jobs': 4,    \n 'num_parallel_tree': 1,    \n 'tree_method': 'exact',    \n 'validate_parameters': 1,    \n 'objective': 'binary:logistic',\n 'random_state': 0,\n 'reg_alpha': 0,\n 'reg_lambda': 1,\n 'scale_pos_weight': 1,\n 'seed': 0,\n 'subsample': 1}\n\n\nmodel = xgb.train(dtrain  = train_d_matrix,\n            num_boost_round  = 100,\n            params = params)","d7d7ee39":"glm_mod = GLM(y_train, \n              X_train, \n              family=families.Binomial()).fit(atol=1e-10)\n","0a80132f":"preds = xgb_skl.predict(X_test)\nsum(y_test)\/len(y_test)","6c07421c":"\ndef get_sklearn_perf(model):\n    preds = model.predict(X_test)\n    return f1_score(y_test, preds), accuracy_score(y_test, preds)\n\ndef get_xgb_perf(model):\n    preds = (model.predict(test_d_matrix) >= 0.5) * 1\n    return f1_score(y_test, preds), accuracy_score(y_test, preds)\n\n\ndef get_glm_perf(model):\n    preds = (model.predict(X_test) >= 0.5) * 1\n    return f1_score(y_test, preds), accuracy_score(y_test, preds)\n\n# so now just to test if this lift result in the logit space gives the same result as predict! and it does! so this is the motivation of why you can use this for multi-class!\nprob_1 = y_train.sum() \/ len(y_train)\nprob_0 = 1 - prob_1\nlogit_1 = np.log(prob_1\/(1-prob_1))\nlogit_0 = np.log(prob_0\/(1-prob_0))\n\n\npreds = xgb_skl.predict_proba(X_test)\npreds_logits = np.log(preds\/(1-preds))\npreds_logits[:,0] = preds_logits[:,0]\/logit_0\npreds_logits[:,1] = preds_logits[:,0]\/logit_1\npreds_class =np.argmax(preds_logits, axis=1)\nnp.unique(preds_class, return_counts=True)\nall(preds_class == xgb_skl.predict(X_test))","8070bac2":"sklearns_results = {}\nxgb_results = {}\nglm_results = {}\n\nf1, acc = get_sklearn_perf(xgb_skl)\nxgb_f1, xbg_acc = get_xgb_perf(model)\nglm_f1, glm_acc = get_glm_perf(glm_mod)\n\nsklearns_results['benchmark'] = {'f1' : f1, 'acc': acc}\nxgb_results['benchmark'] = {'f1' : xgb_f1, 'acc': xbg_acc}\nglm_results['benchmark'] = {'f1' : glm_f1, 'acc': glm_acc}","b1298f82":"sklearns_results","1f27015e":"xgb_results","08c76eda":"glm_results","8fb934bd":"X_train_backup = X_train.copy()\ny_train_backup = y_train.copy()\n\nX_train = X_train_backup.copy()\ny_train = y_train_backup.copy()\n\n\nmajority_indexes = y_train[y_train==0].index\nminority_indexes = y_train[y_train==1].index\nn_maj = len(majority_indexes)\nn_min = len(minority_indexes)\n\n\nmaj_class_indices_to_keep = y_train[majority_indexes].sample(n_min, random_state=221).index.tolist()\n\n\nX_train_sampled = X_train.copy()\ny_train_sampled = y_train.copy()\nX_train_sampled = X_train_sampled.loc[maj_class_indices_to_keep + minority_indexes.tolist()]\ny_train_sampled = y_train_sampled.loc[maj_class_indices_to_keep + minority_indexes.tolist()]\n\n\nxgb_skl = XGBClassifier(objective = 'binary:logistic')\nxgb_skl.fit(X_train_sampled, y_train_sampled)\n\nglm_mod = GLM(y_train_sampled, \n              X_train_sampled,\n              family=families.Binomial()).fit(atol=1e-10)\n\n\ntrain_d_matrix = xgb.DMatrix(X_train_sampled, label = y_train_sampled)\nmodel = xgb.train(dtrain  = train_d_matrix,\n            num_boost_round  = 100,\n            params = params)\n\nf1, acc = get_sklearn_perf(xgb_skl)\nxgb_f1, xbg_acc = get_xgb_perf(model)\nglm_f1, glm_acc = get_glm_perf(glm_mod)\n\nsklearns_results['downsampling'] = {'f1' : f1, 'acc': acc}\nxgb_results['downsampling'] = {'f1' : xgb_f1, 'acc': xbg_acc}\nglm_results['downsampling'] = {'f1' : glm_f1, 'acc': glm_acc}","8900fd5f":"print(sklearns_results)\nprint(xgb_results)\nprint(glm_results)","a42be5b3":"######################\n#\n# now for uplsampling!\n#\n######################\nmin_class_indices_to_keep = y_train[minority_indexes].sample(n_maj, replace=True, random_state=221).index.tolist()\n\n\nX_train_sampled = X_train.copy()\ny_train_sampled = y_train.copy()\n\nX_train_sampled = X_train_sampled.loc[min_class_indices_to_keep + majority_indexes.tolist()]\ny_train_sampled = y_train_sampled.loc[min_class_indices_to_keep + majority_indexes.tolist()]\n\ny_train_sampled.value_counts()\n\n\n# this increases training time\nxgb_skl = XGBClassifier(objective = 'binary:logistic')\nxgb_skl.fit(X_train_sampled, y_train_sampled)\n\n\n\nglm_mod = GLM(y_train_sampled, \n              X_train_sampled,\n              family=families.Binomial()).fit(atol=1e-10)\n\n\n\ntrain_d_matrix = xgb.DMatrix(X_train_sampled, label = y_train_sampled)\nmodel = xgb.train(dtrain  = train_d_matrix,\n                    num_boost_round  = 100,\n                    params = params)\n\nf1, acc = get_sklearn_perf(xgb_skl)\nxgb_f1, xbg_acc = get_xgb_perf(model)\n\nsklearns_results['oversampling'] = {'f1' : f1, 'acc': acc}\nxgb_results['oversampling'] = {'f1' : xgb_f1, 'acc': xbg_acc}\nglm_results['oversampling'] = {'f1' : glm_f1, 'acc': glm_acc}","c84edeab":"print(sklearns_results)\nprint(xgb_results)\nprint(glm_results)","3088e0dd":"########### ok now for combining the two appraoches -> so you want to downsample some 0s and upsample some 1s\n########### ok now for combining the two appraoches -> so you want to downsample some 0s and upsample some 1s\n########### ok now for combining the two appraoches -> so you want to downsample some 0s and upsample some 1s\n\nmid_point = int((n_maj - n_min)\/2)\n# so we want to downample this many from maj class and upsample this many from min class\nn_maj - mid_point\nn_min + mid_point\n\n# it's an effort doing this!\n\n\n\n\nmaj_class_indices_to_keep = y_train[majority_indexes].sample(n_maj - mid_point, random_state=221).index.tolist()\nmin_class_indices_to_keep = y_train[minority_indexes].sample(n_min + mid_point, replace=True, random_state=221).index.tolist()\n\n\nX_train_sampled = X_train.copy()\ny_train_sampled = y_train.copy()\nX_train_sampled = X_train_sampled.loc[maj_class_indices_to_keep + min_class_indices_to_keep]\ny_train_sampled = y_train_sampled.loc[maj_class_indices_to_keep + min_class_indices_to_keep]\ny_train_sampled.value_counts()\n\n\n# this increases training time\nxgb_skl = XGBClassifier(objective = 'binary:logistic')\nxgb_skl.fit(X_train_sampled, y_train_sampled)\n\n\n\nglm_mod = GLM(y_train_sampled, \n              X_train_sampled,\n              family=families.Binomial()).fit(atol=1e-10)\n\n\n\ntrain_d_matrix = xgb.DMatrix(X_train_sampled, label = y_train_sampled)\nmodel = xgb.train(dtrain  = train_d_matrix,\n                    num_boost_round  = 100,\n                    params = params)\n\nf1, acc = get_sklearn_perf(xgb_skl)\nxgb_f1, xbg_acc = get_xgb_perf(model)\n\nsklearns_results['both'] = {'f1' : f1, 'acc': acc}\nxgb_results['both'] = {'f1' : xgb_f1, 'acc': xbg_acc}\nglm_results['both'] = {'f1' : glm_f1, 'acc': glm_acc}","2095c3ef":"print(sklearns_results)\nprint(xgb_results)\nprint(glm_results)","631a98ae":"## finally for focal loss\n## finally for focal loss\n## finally for focal loss\n\n\n# this class was taken directly from the github of the python package that implements it\n# I'm not interested in using the package - just the loss to see if it works hence why I've taken just that bit\n\n# https:\/\/github.com\/jhwjhw0123\/Imbalance-XGBoost\n    \nclass Focal_Binary_Loss():\n    '''\n    The class of focal loss, allows the users to change the gamma parameter\n    '''\n\n    def __init__(self, gamma_indct):\n        '''\n        :param gamma_indct: The parameter to specify the gamma indicator\n        '''\n        self.gamma_indct = gamma_indct\n\n    def robust_pow(self, num_base, num_pow):\n        # numpy does not permit negative numbers to fractional power\n        # use this to perform the power algorithmic\n\n        return np.sign(num_base) * (np.abs(num_base)) ** (num_pow)\n\n    def focal_binary_object(self, pred, dtrain):\n        gamma_indct = self.gamma_indct\n        # retrieve data from dtrain matrix\n        label = dtrain.get_label()\n        # compute the prediction with sigmoid\n        sigmoid_pred = 1.0 \/ (1.0 + np.exp(-pred))\n        # gradient\n        # complex gradient with different parts\n        g1 = sigmoid_pred * (1 - sigmoid_pred)\n        g2 = label + ((-1) ** label) * sigmoid_pred\n        g3 = sigmoid_pred + label - 1\n        g4 = 1 - label - ((-1) ** label) * sigmoid_pred\n        g5 = label + ((-1) ** label) * sigmoid_pred\n        # combine the gradient\n        grad = gamma_indct * g3 * self.robust_pow(g2, gamma_indct) * np.log(g4 + 1e-9) + \\\n               ((-1) ** label) * self.robust_pow(g5, (gamma_indct + 1))\n        # combine the gradient parts to get hessian components\n        hess_1 = self.robust_pow(g2, gamma_indct) + \\\n                 gamma_indct * ((-1) ** label) * g3 * self.robust_pow(g2, (gamma_indct - 1))\n        hess_2 = ((-1) ** label) * g3 * self.robust_pow(g2, gamma_indct) \/ g4\n        # get the final 2nd order derivative\n        hess = ((hess_1 * np.log(g4 + 1e-9) - hess_2) * gamma_indct +\n                (gamma_indct + 1) * self.robust_pow(g5, gamma_indct)) * g1\n\n        return grad, hess\n    ","f9ccf627":"train_d_matrix = xgb.DMatrix(X_train, label = y_train)\ntest_d_matrix = xgb.DMatrix(X_test, label = y_test)","0c3ca832":"params['objective_func'] = 'binary:logitraw'\nparams['eval_metric'] = 'logloss'","08553ece":"# gamma is tuning parameter so just doing a simple search for a few values -> not oerforming great! will pip install package!\nfor idx, gamma in enumerate([0.25, 0.5, 0.75, 1.0,1.25, 1.5]):\n    focal_loss = Focal_Binary_Loss(gamma_indct = gamma)\n    grad, hess = focal_loss.focal_binary_object(model.predict(train_d_matrix),train_d_matrix)\n\n\n    model_focal = xgb.train(params,  # any other tree method is fine.\n            dtrain=train_d_matrix,\n            num_boost_round=100,\n            obj=focal_loss.focal_binary_object)\n\n    xgb_f1, xbg_acc = get_xgb_perf(model_focal)\n    xgb_results[f'focal-{gamma}'] = {'f1' : xgb_f1, 'acc': xbg_acc}\n    print(xgb_results[f'focal-{gamma}'])","ef579a41":"xgb_results","387c228d":"!pip install imbalance-xgboost","28a50048":"from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n","0332fbb3":"from sklearn.model_selection import GridSearchCV\nxgboster_focal = imb_xgb(special_objective='focal')\nxgboster_weight = imb_xgb(special_objective='weighted')\nCV_focal_booster = GridSearchCV(xgboster_focal, {\"focal_gamma\":[1.0,1.5,2.0,2.5,3.0]})\nCV_weight_booster = GridSearchCV(xgboster_weight, {\"imbalance_alpha\":[1.5,2.0,2.5,3.0,4.0]})","e7db2b89":"CV_focal_booster.fit(X_train.to_numpy(), y_train.to_numpy())\nopt_focal_booster = CV_focal_booster.best_estimator_","d0b0e0ec":"opt_focal_booster","e1d85a7c":"preds = opt_focal_booster.predict_determine(X_test.to_numpy(), y=None) ","4bcaab69":"print(f1_score(y_test, preds))\nprint(accuracy_score(y_test, preds))","977b0391":"xgb_results","acdf1996":"# so the above doesn't do better than either of the more sophisticated approaches!\n# focal point approach not better than sampling things differently!!!!!!\n# but there is something interesting going on as the acc hasn't changed! i.e. you've gained on f-score without loosing on accuracy!!!!!","2e43e2b7":"from sklearn.metrics import confusion_matrix\nconf_matrix = pd.DataFrame(confusion_matrix(y_test, preds), index= [0, 1], columns = [0,1])\nconf_matrix = conf_matrix.loc[[1, 0], [1, 0]]\nconf_matrix.loc[1, 'recall'] = conf_matrix.loc[1, 1] \/ conf_matrix.loc[1, ].sum()\nconf_matrix.loc[0, 'recall'] = conf_matrix.loc[0, 0] \/ conf_matrix.loc[0, ].sum()\nconf_matrix\n\nconf_matrix_focal = conf_matrix.copy()\nconf_matrix_focal","a9f3df52":"CV_weight_booster.fit(X_train.to_numpy(), y_train.to_numpy())\nopt_weight_booster = CV_weight_booster.best_estimator_","5d69ca25":"opt_weight_booster","8118c745":"preds = opt_weight_booster.predict_determine(X_test.to_numpy(), y=None) ","b927c62c":"print(f1_score(y_test, preds))\nprint(accuracy_score(y_test, preds))","eb3e2e18":"# wow! this weighted objective function has prooven to have superior results!!!!!!!\n# wow! this weighted objective function has prooven to have superior results!!!!!!!\n# wow! this weighted objective function has prooven to have superior results!!!!!!!\n# wow! this weighted objective function has prooven to have superior results!!!!!!!","71978374":"- approach 1 - downsampling\n- approach 2 - upsampling\n- approach 3 - both\n- approach 4 - using focal ","92b8ce05":"### aim\n\n- I want to test different ways of addressing imbalance and I want to replicate the sklearn xgboost api with the xgb package\n- I want to test the benefits of using the focal cost function for xgboost\n- Also I want to explore setting predicted class by using the pred probs and converting them to logits and compare them to the original class probabilities in logits\n\n\n## conclusion\n\nThere was a benefit from downsampling but the biggest gains came from using the xgboost class imbalance approached using the package here!\nnot sure how it sets up cost function but it's worth looking into!\nhttps:\/\/pypi.org\/project\/imbalance-xgboost\/","bdfb03c6":"## train test split"}}