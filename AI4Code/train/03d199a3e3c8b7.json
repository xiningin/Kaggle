{"cell_type":{"235b50e7":"code","6da79213":"code","75f1a448":"code","daf7d61b":"code","00545d7d":"code","b85a74dc":"code","30e75967":"code","48510d8b":"code","ae4d9aea":"code","5f5abb00":"code","10df1b08":"code","70abaabf":"code","80e2e0e5":"code","6e43bcbe":"code","82b9031a":"code","740d440e":"code","117b34c2":"code","ed3ecc4c":"code","360983fc":"code","cab5e9e6":"code","c51d32dd":"code","1a12aaff":"code","532b8228":"code","af392352":"code","0b5b832d":"code","b5d15ed7":"code","73dd20f6":"code","85cd51fe":"code","9f30d5d1":"code","8343e7f0":"code","fe5112e8":"code","d2d2c20d":"code","ba4b65c8":"code","df2c4237":"code","2cd78b87":"code","539400f2":"code","44cd6b5a":"markdown","bb0bd667":"markdown","3e453839":"markdown","f25a1233":"markdown","677a7015":"markdown","058f736b":"markdown","40d8c69e":"markdown","8aa946cb":"markdown","2bafd627":"markdown","c5e99e96":"markdown","c68c9c25":"markdown","4dd6f59e":"markdown","b9344530":"markdown","8229e61f":"markdown","16da2c54":"markdown","645ea679":"markdown","844ff2d9":"markdown"},"source":{"235b50e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","6da79213":"df.describe()","75f1a448":"# Good No Null Values!\ndf.isnull().sum().max()","daf7d61b":"df.columns","00545d7d":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","b85a74dc":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","30e75967":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","48510d8b":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","ae4d9aea":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","5f5abb00":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","10df1b08":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","70abaabf":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\n\n\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","80e2e0e5":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","6e43bcbe":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","82b9031a":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","740d440e":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","117b34c2":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","ed3ecc4c":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","360983fc":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","cab5e9e6":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","c51d32dd":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","1a12aaff":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","532b8228":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","af392352":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","0b5b832d":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","b5d15ed7":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n    \n    \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","73dd20f6":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","85cd51fe":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","9f30d5d1":"undersample_y_score = log_reg.decision_function(original_Xtest)","8343e7f0":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","fe5112e8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nensemble = {\n    \"RandomForest\": RandomForestClassifier(),\n    \"GradientBoosting\": GradientBoostingClassifier(),\n    \"Bagging\": BaggingClassifier(),\n    \"ExtraTree\": ExtraTreesClassifier()\n}","d2d2c20d":"from sklearn.model_selection import cross_val_score\n\n\nfor key, ensemble in ensemble.items():\n    ensemble.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Ensemble: \", ensemble.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","ba4b65c8":"# Random Forest Classifier \nparam_rfc={\n            'max_depth': [3, None],\n            'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000),\n            'max_features': (2,4,6)\n}\ngrid_rfc = GridSearchCV(RandomForestClassifier(), param_rfc)\ngrid_rfc.fit(X_train,y_train)\nrfc = grid_rfc.best_estimator_\n\n# Gradient Boosting Classifier\nparam_gbc = {\n    'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000),\n    'max_depth': [1,2,3,4,5]\n}\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_gbc)\ngrid_gbc.fit(X_train, y_train)\ngbc = grid_gbc.best_estimator_\n\n#Bagging Classifer\nparam_bagg = {\n    'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n}\n\ngrid_bagg = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),\n                                     n_estimators = 100, max_features = 0.5),\n                   param_bagg)\ngrid_bagg.fit(X_train, y_train)\nbagg = grid_bagg.best_estimator_","df2c4237":"# Overfitting Case\n\nrfc_score = cross_val_score(rfc, X_train, y_train, cv=5)\nrfc_score.mean()\n","2cd78b87":"gbc_score = cross_val_score(gbc, X_train, y_train, cv=5)\ngbc_score.mean()","539400f2":"bagg_score = cross_val_score(bagg, X_train, y_train, cv=5)\nbagg_score.mean()","44cd6b5a":"<h1 align=\"center\"> Credit Fraud Detector <\/h1>\n\n**Note:** There are still aspects of this kernel that will be subjected to changes. I've noticed a recent increase of interest towards this kernel so I will focus more on the steps I took and why I took them to make it clear why I took those steps.\n\n<h2>Before we Begin:  <\/h2>\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards this subject and will look for more efficient ways so that our models are able to detect more accurately both fraud and non-fraud transactions.\n\n<h2> Introduction <\/h2>\nIn this kernel we will use various predictive models to see how accurate they  are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n\n\n<h2> Our Goals: <\/h2>\n<ul>\n<li> Understand the little distribution of the \"little\" data that was provided to us. <\/li>\n<li> Create a 50\/50 sub-dataframe ratio of \"Fraud\" and \"Non-Fraud\" transactions. (NearMiss Algorithm) <\/li>\n<li> Determine the Classifiers we are going to use and decide which one has a higher accuracy. <\/li>\n<li>Create a Neural Network and compare the accuracy to our best classifier. <\/li>\n<li>Understand common mistaked made with imbalanced datasets. <\/li>\n<\/ul>\n\n\n<h2> Outline: <\/h2>\nI. <b>Understanding our data<\/b><br>\na) [Gather Sense of our data](#gather)<br><br>\n\nII. <b>Preprocessing<\/b><br>\na) [Scaling and Distributing](#distributing)<br>\nb) [Splitting the Data](#splitting)<br><br>\n\nIII. <b>Random UnderSampling and Oversampling<\/b><br>\na) [Distributing and Correlating](#correlating)<br>\nb) [Anomaly Detection](#anomaly)<br>\nc) [Dimensionality Reduction and Clustering (t-SNE)](#clustering)<br>\nd) [Classifiers](#classifiers)<br>\ne) [A Deeper Look into Logistic Regression](#logistic)<br>\nf) [Oversampling with SMOTE](#smote)<br><br>\n\nIV. <b>Testing <\/b><br>\na) [Testing with Logistic Regression](#testing_logistic)<br>\nb) [Neural Networks Testing (Undersampling vs Oversampling)](#neural_networks)\n\n<h2>Correcting Previous Mistakes from Imbalanced Datasets: <\/h2>\n<ul>\n<li> Never test on the oversampled or undersampled dataset.<\/li>\n<li>If we want to implement cross validation, remember to oversample or undersample your training data <b>during<\/b> cross-validation, not before! <\/li>\n<li> Don't use <b>accuracy score <\/b> as a metric with imbalanced datasets (will be usually high and misleading), instead use <b>f1-score, precision\/recall score or confusion matrix <\/b><\/li>\n<\/ul>\n\n\n<h2> References: <\/h2>\n<ul> \n<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aur\u00e9lien G\u00e9ron (O'Reilly). CopyRight 2017 Aur\u00e9lien G\u00e9ron  <\/li>\n<li><a src=\"https:\/\/www.youtube.com\/watch?v=DQC_YE3I5ig&t=794s\" > Machine Learning - Over-& Undersampling - Python\/ Scikit\/ Scikit-Imblearn <\/a>by Coding-Maniac<\/li>\n<li><a src=\"https:\/\/www.kaggle.com\/lane203j\/auprc-5-fold-c-v-and-resampling-methods\"> auprc, 5-fold c-v, and resampling methods\n<\/a> by Jeremy Lane (Kaggle Notebook) <\/li>\n<\/ul>","bb0bd667":"\uc55e\uc11c \ub098\uc628 Classifier\ub4e4 \ubaa9\ub85d\uc5d0 \uc559\uc0c1\ube14 \ub0b4\uc6a9\uc774 \ub530\ub85c \ub4e4\uc5b4\uac00 \uc788\uc9c0 \uc54a\uace0, imblearn \uad00\ub828 \ub0b4\uc6a9\uc774 \ub4e4\uc5b4 \uc788\ub294 \uad00\uacc4\ub85c, \ubcc4\ub3c4\ub85c \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8, \uadf8\ub798\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305, \ubc30\uae45, \uc5d1\uc2a4\ud2b8\ub77c \ud2b8\ub9ac\ub97c \ud65c\uc6a9\ud55c classifier \ucf54\ub4dc\ub97c \uc784\uc2dc\ub85c(?) \uc791\uc131\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","3e453839":"\ubaa8\ub378 \uc608\uce21\ub825\uc744 \ub192\uc774\uae30 \uc704\ud574 cross validation\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \uc0ac\uc6a9\ud558\uace0 \ub098\uba74 \uc544\uae4c\ubcf4\ub2e4 \uc608\uce21\ub825\uc774 \ub192\uc544\uc9c0\ub294 \uac78 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","f25a1233":"<h2> Scaling and Distributing <\/h2>\n<a id=\"distributing\"><\/a>\nIn this phase of our kernel, we will first scale the columns comprise of <b>Time<\/b> and <b>Amount <\/b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n\n<h3> What is a sub-Sample?<\/h3>\nIn this scenario, our subsample will be a dataframe with a 50\/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n\n<h3> Why do we create a sub-Sample?<\/h3>\nIn the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n<ul>\n<li><b>Overfitting: <\/b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. <\/li>\n<li><b>Wrong Correlations:<\/b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. <\/li>\n<\/ul>\n\n<h3>Summary: <\/h3> \n<ul>\n<li> <b>Scaled amount <\/b> and <b> scaled time <\/b> are the columns with scaled values. <\/li>\n<li> There are <b>492 cases <\/b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. <\/li>\n<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. <\/b><\/li>\n<\/ul>","677a7015":"## A Deeper Look into LogisticRegression:\n<a id=\"logistic\"><\/a>\nIn this section we will ive a deeper look into the <b> logistic regression classifier<\/b>.\n\n\n### Terms:\n<ul>\n<li><b>True Positives:<\/b> Correctly Classified Fraud Transactions <\/li>\n<li><b>False Positives:<\/b> Incorrectly Classified Fraud Transactions<\/li>\n<li> <b>True Negative:<\/b> Correctly Classified Non-Fraud Transactions<\/li>\n<li> <b>False Negative:<\/b> Incorrectly Classified Non-Fraud Transactions<\/li>\n<li><b>Precision: <\/b>  True Positives\/(True Positives + False Positives)  <\/li>\n<li><b> Recall: <\/b> True Positives\/(True Positives + False Negatives)   <\/li>\n<li> Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.<\/li>\n<li><b>Precision\/Recall Tradeoff: <\/b> The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect. <\/li>\n<\/ul>\n\n### Summary:\n<ul>\n<li> <b>Precision starts to descend<\/b> between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score. <\/li>\n\n<\/ul>","058f736b":"train set\uacfc test set\uc744 train_test_split\uc73c\ub85c \ub098\ub214","40d8c69e":"<h2> Classifiers (UnderSampling):  <\/h2>\n<a id=\"classifiers\"><\/a>\nIn this section we will train four types of classifiers and decide which classifier will be more effective in detecting <b>fraud transactions<\/b>.  Before we have to split our data into training and testing sets and separate the features from the labels.\n\n## Summary: \n<ul>\n<li> <b> Logistic Regression <\/b> classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression) <\/li>\n<li><b> GridSearchCV <\/b> is used to determine the paremeters that gives the best predictive score for the classifiers. <\/li>\n<li> Logistic Regression has the best Receiving Operating Characteristic score  (ROC), meaning that LogisticRegression pretty accurately separates <b> fraud <\/b> and <b> non-fraud <\/b> transactions.<\/li>\n<\/ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>wider the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias)<\/b><\/li>\n<li><b> Logistic Regression Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>","8aa946cb":"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future.","2bafd627":"**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!","c5e99e96":"## Gather Sense of Our Data:\n<a id=\"gather\"><\/a>\nThe first thing we must do is gather a <b> basic sense <\/b> of our data. Remember, except for the <b>transaction<\/b> and <b>amount<\/b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.   \n\n<h3> Summary: <\/h3>\n<ul>\n<li>The transaction amount is relatively <b>small<\/b>. The mean of all the mounts made is approximately USD 88. <\/li>\n<li>There are no <b>\"Null\"<\/b> values, so we don't have to work on ways to replace values. <\/li>\n<li> Most of the transactions were <b>Non-Fraud<\/b> (99.83%) of the time, while <b>Fraud<\/b> transactions occurs (017%) of the time in the dataframe. <\/li>\n<\/ul>\n\n<h3> Feature Technicalities: <\/h3>\n<ul>\n<li> <b>PCA Transformation: <\/b>  The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).<\/li>\n<li> <b>Scaling:<\/b> Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)<\/li>\n<\/ul>","c68c9c25":"\uadf8\ub9ac\ub4dc \uc11c\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd5c\uc801\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \ucc3e\ub294\ub2e4. (\uc6b0\ub9ac \uc2a4\ud130\ub514\ub54c\ub3c4 \uad00\ub828\ud574\uc11c \uc5b8\uae09\ud55c \uae30\uc5b5\uc774 \uc788\uc2b5\ub2c8\ub2e4.)","4dd6f59e":"\uc624\ubc84\ud53c\ud305\ub41c \uacbd\uc6b0\ub97c \uc870\uc815\ud574\uc900\ub2e4.","b9344530":"##  Equally Distributing and Correlating: \n<a id=\"correlating\"><\/a>\nNow that we have our dataframe correctly balanced, we can go further with our <b>analysis<\/b> and <b>data preprocessing<\/b>.","8229e61f":"## Random Under-Sampling:\n<img src=\"http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/_images\/sphx_glr_plot_random_under_sampler_001.png\">\n\nIn this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset <\/b> and thus avoiding our models to overfitting.\n\n#### Steps:\n<ul>\n<li>The first thing we have to do is determine how <b>imbalanced<\/b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  <\/li>\n<li>Once we determine how many instances are considered <b>fraud transactions <\/b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions<\/b> to the same amount as fraud transactions (assuming we want a 50\/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  <\/li>\n<li> After implementing this technique, we have a sub-sample of our dataframe with a 50\/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data<\/b> to see if our models can maintain a certain accuracy everytime we run this script.<\/li>\n<\/ul>\n\n**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss<\/b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)","16da2c54":"### Splitting the Data (Original DataFrame)\n<a id=\"splitting\"><\/a>\nBefore proceeding with the <b> Random UnderSampling technique<\/b> we have to separate the orginal dataframe. <b> Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.<\/b> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.  ","645ea679":"\uc55e\uc11c \ucd94\ucd9c\ud55c new_df\uc5d0\uc11c \ubaa9\uc801 \ubcc0\uc218\uc778 Class\ub97c Y \ubcc0\uc218\ub85c \uc9c0\uc815\ud558\uace0 \ub098\uba38\uc9c0 \uc790\ub8cc\ub97c X\ub85c \uc9c0\uc815.","844ff2d9":"\ud55c\ubc88\uc5d0 \uc5ec\ub7ec \ubaa8\ub378\uc5d0 \ub300\ud574 \ubaa8\ub378 \uc608\uce21\ub825\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574 simple classifiers\ub97c \ubb36\uc5b4 \uc0ac\uc6a9."}}