{"cell_type":{"79d1223a":"code","f91e394e":"code","89c867c1":"code","207e4e6f":"code","45a55c05":"code","02923987":"code","e42e4ebd":"code","18e24f02":"code","237d4468":"code","46cc436a":"code","02d70026":"code","e680787d":"code","058e1228":"code","3da2f79f":"code","d0b6f9f5":"code","d1ee59a5":"code","d0f0cca1":"code","1b7fcd9c":"code","d892fa5d":"code","e8caf6e4":"code","ef0dcdbf":"code","da4e837d":"code","3d2ffedf":"code","31ff2037":"code","1cbf92b2":"markdown","38e1d9d0":"markdown","28fddc3c":"markdown","8fbdda52":"markdown","2250055f":"markdown","40dd0759":"markdown","4b99fbac":"markdown","956a3fa0":"markdown","800eb175":"markdown","e2166c3c":"markdown","01d1f71f":"markdown","046eb25b":"markdown","ecf178f4":"markdown","17aeea71":"markdown"},"source":{"79d1223a":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport xgboost as xgb\n\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom geopy.distance import vincenty\n\nimport gc","f91e394e":"#######\n# read in the data\n######\nhousing = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')\nhousing.head()\n\n\ncity_lat_long = pd.read_csv('..\/input\/california-housing-feature-engineering\/cal_cities_lat_long.csv')\ncity_pop_data = pd.read_csv('..\/input\/california-housing-feature-engineering\/cal_populations_city.csv')\ncounty_pop_data = pd.read_csv('..\/input\/california-housing-feature-engineering\/cal_populations_county.csv')","89c867c1":"#########\n# Engineer more features here prior to \n# passing data in for imputation and one hot encoding\n#########\n\ncity_coords = {}\n\nfor dat in city_lat_long.iterrows():\n    row = dat[1]\n    if row['Name'] not in city_pop_data['City'].values:   \n        continue           \n    else: \n        city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))\n\n\"\"\"\nNOTE: originally I had the code shown below, but I had to change it because I only want to \ndeal with cities with both location and population data.\n\ncity_coords = {}\nfor dat in city_lat_long.iterrows():\n    row = dat[1]\n    city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))\n\n# how I deiscovered the need for the change\npresent = []\nabsent = []\nfor city in city_coords.keys():\n    if city in city_pop_data['City'].values:\n        present.append(city)\n    else:\n        absent.append(city)\nlen(present)\nlen(absent)\nabsent\n\"\"\"","207e4e6f":"# Two functions (vincenty--see https:\/\/en.wikipedia.org\/wiki\/Vincenty%27s_formulae AND )\n# 1. Take two lat long tuples as input\n    # Return the distance between the two\n    # vincenty(tuple1, tuple2)\n    \n# example\nnewport_ri = (41.49008, -71.312796)\ncleveland_oh = (41.499498, -81.695391)\n\nx = vincenty(newport_ri, cleveland_oh)\nprint(\"x: \", x) # distance stored in km, should see units in output\n\ntype(x.kilometers) # is it a float?","45a55c05":"#2. Take a dict[city] = (lat, long) of locations and a tuple of (lat, long)\n    # Run #1 for each comparison and return a tuple with the closest city's\n    # key + value and  distance between points.\n    # The time complexity here is O(n) (linear, see https:\/\/en.wikipedia.org\/wiki\/Time_complexity#Linear_time)\n    # It's not particularly efficient and makes me weep--I am thinking of ways to improve it.\n\ndef closest_point(location, location_dict):\n    \"\"\" Take a tuple of latitude and longitude and \n        compare to a dictonary of locations where\n        key = location name and value = (lat, long)\n        Returns tuple of (closest_location , distance) \"\"\"\n    closest_location = None\n    for city in location_dict.keys():\n        distance = vincenty(location, location_dict[city]).kilometers\n        if closest_location is None:\n            closest_location = (city, distance)\n        elif distance < closest_location[1]:\n            closest_location = (city, distance)\n    return closest_location","02923987":"test = (39.524325, -122.293592) #likely 'Willows'\nclosest_point(test, city_coords)","e42e4ebd":"# Run #2 (closest_point()) to determine both the nearest city, and then\n    # also the nearest city with 1 million people (subset of original dict)\n\ncity_pop_dict = {}\nfor dat in city_pop_data.iterrows():\n    row = dat[1]\n    city_pop_dict[row['City']] =  row['pop_april_1990']\n\nbig_cities = {}\n\nfor key, value in city_coords.items():\n    if city_pop_dict[key] > 500000:\n        big_cities[key] = value","18e24f02":"#######\n# Add closest city data to dataframes\n#######\n\nhousing['close_city'] = housing.apply(\n    lambda x: closest_point((x['latitude'], x['longitude']), city_coords), \n    axis = 1\n)\nhousing['close_city_name'] = [x[0] for x in housing['close_city'].values]\nhousing['close_city_dist'] = [x[1] for x in housing['close_city'].values]\nhousing['close_city_pop'] = [city_pop_dict[x] for x in housing['close_city_name'].values]\n\nhousing = housing.drop('close_city', axis=1)\nhousing.head()\n\n\n#add the data relating to the points to the closest big city\nhousing['big_city'] = housing.apply(\n    lambda x: closest_point((x['latitude'], x['longitude']), big_cities),\n    axis = 1\n)\nhousing['big_city_name'] = [x[0] for x in housing['big_city'].values]\nhousing['big_city_dist'] = [x[1] for x in housing['big_city'].values]\n\nhousing = housing.drop('big_city', axis=1)","237d4468":"# Load plot background image; load new df for graphing\n\ncalifornia_img=mpimg.imread('..\/input\/california-housing-feature-engineering\/california.png')\n\nhousing_plot = housing[['longitude','population','latitude', 'close_city_name','big_city_name','big_city_dist','median_house_value']]","46cc436a":"housing_plot.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n            s=housing_plot['population']\/100, label='population', figsize=(10,7),\n            c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.legend() \nplt.show()","02d70026":"city_lat_long['Population'] = [city_pop_dict[x] if x in city_pop_dict.keys() \n                                                else 0 for x in city_lat_long['Name'].values]\n\n# graph of city locations and size\ncity_lat_long.plot(kind='scatter', x='Longitude', y='Latitude',  alpha=0.4,\n                s=housing_plot['population']\/50, label='population', figsize=(10,7))\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.legend() \nplt.show()","e680787d":"####\n# graph of vectors connecting points to the nearest city\n\ncity_lat_long.plot(kind='scatter', x='Longitude', y='Latitude',  alpha=0.4,\n                    s=housing_plot['population']\/100, label='population', figsize=(10,7))\n\nfor line in housing.iterrows():\n    dat = line[1]\n    x1 = dat['longitude']\n    y1 = dat['latitude']\n    p2 = city_coords[dat['close_city_name']]\n    x2 = p2[1]\n    y2 = p2[0]\n    plt.plot([x1,x2], [y1, y2], 'k-', linewidth=0.1)\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.show()","058e1228":"####\n# graph of the vectors connecting districts to the nearest major city\n# and a barplot of distance to the nearest major city\n\ncity_lat_long.plot(kind='scatter', x='Longitude', y='Latitude',  alpha=0.4,\n                    s=housing_plot['population']\/100, label='population', figsize=(10,7))\n\nfor line in housing.iterrows():\n    dat = line[1]\n    x1 = dat['longitude']\n    y1 = dat['latitude']\n    p2 = big_cities[dat['big_city_name']]\n    x2 = p2[1]\n    y2 = p2[0]\n    plt.plot([x1,x2], [y1, y2], 'k-', linewidth=0.1)\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.show()","3da2f79f":"#####\n# Alter existing features, do train\/test split.\n#####\n\n# Divide by 1.5 to limit the number of income categories\nhousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] \/ 1.5)\n\n# Label those above 5 as 5\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n\n# Must fix column name with < symbol\nhousing['ocean_proximity'][housing['ocean_proximity'] == '<1H OCEAN'] = 'LessThan1h'\n\n# Make stratified split of the data\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    train_set = housing.loc[train_index]\n    test_set = housing.loc[test_index]\n\nfor set_ in (train_set, test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n\ngc.collect()\n\ndef housing_data_clean(input_df):\n    input_df['rooms_per_household'] = input_df['total_rooms']\/input_df['households']\n    input_df['bedrooms_per_household'] = input_df['total_bedrooms']\/input_df['households']\n    input_df['bedrooms_per_room'] = input_df['total_bedrooms']\/input_df['total_rooms']\n    input_df['population_per_household'] = input_df['population']\/input_df['households']\n    input_df = input_df.drop(['total_bedrooms','total_rooms'], axis=1)\n    return input_df\n\ntrain_set = housing_data_clean(train_set)\ntrain_set.head()\n\n# Do the same to the test set so they remain consistent with one another\ntest_set = housing_data_clean(test_set)\n\nX_train = train_set.drop('median_house_value', axis=1)\ny_train = train_set['median_house_value'].values.astype(float)\n\nX_test = test_set.drop('median_house_value', axis=1)\ny_test = test_set['median_house_value'].values.astype(float)\n\n\n#####\n# fill numerical values\n#####\n\ndef fill_median(dataframe, cols):\n    \"\"\" impute the mean for a list of columns in the dataframe\"\"\"\n    for i in cols:\n        dataframe[i].fillna(dataframe[i].median(skipna=True), inplace = True)\n    return dataframe\n\ndef cols_with_missing_values(dataframe):\n    \"\"\" query a dataframe and find the columns that have missing values\"\"\"\n    return list(dataframe.columns[dataframe.isnull().any()])\n\ndef fill_value(dataframe, col, val):\n    \"\"\" impute the value for a list column in the dataframe\"\"\"\n    \"\"\"  use this to impute the median of the train into the test\"\"\"\n    dataframe[i].fillna(val, inplace = True)\n    return dataframe\n\nmissing_vals = cols_with_missing_values(X_train)\nX_train = fill_median(X_train, missing_vals)\n\nfor i in missing_vals:\n    X_test = fill_value(X_test, i, X_train[i].median(skipna=True))","d0b6f9f5":"#####\n# One hot encode the categorical cols\n#####\n\nencoder1 = LabelBinarizer()\nencoded_ocean_train_1hot = encoder1.fit_transform(X_train['ocean_proximity'])\n\n# Just using transform to ensure that the categories are sorted and used the same as in the train\/fit\nencoded_ocean_test_1hot = encoder1.transform(X_test['ocean_proximity']) \n\nencoder2 = LabelBinarizer()\nencoded_train_close_city = encoder2.fit_transform(X_train['close_city_name'])\nencoded_test_close_city = encoder2.transform(X_test['close_city_name'])\n\nencoder3 = LabelBinarizer()\nencoded_train_big_city = encoder3.fit_transform(X_train['big_city_name'])\nencoded_test_big_city = encoder3.transform(X_test['big_city_name'])\n\nall_classes = list(encoder1.classes_) + [x + '_city' for x in encoder2.classes_ ]  + [x + '_Bigcity' for x in encoder3.classes_]\n\ntrain_bool_data = np.concatenate((encoded_ocean_train_1hot, encoded_train_close_city, encoded_train_big_city), axis=1)\ntest_bool_data = np.concatenate((encoded_ocean_test_1hot, encoded_test_close_city, encoded_test_big_city), axis=1)\n\ntrain_cat_df = pd.DataFrame(train_bool_data,\n        index = X_train.index, columns = all_classes)\n\ntest_cat_df = pd.DataFrame(test_bool_data,index = X_test.index, columns = all_classes)","d1ee59a5":"###\n# Combine and scale the dfs\n###\n\nX_train.drop(['ocean_proximity', 'close_city_name', 'big_city_name'], axis=1, inplace=True)\nX_test.drop(['ocean_proximity', 'close_city_name', 'big_city_name'], axis=1, inplace=True)\n\nnum_cols = X_train.columns\nnum_train_index = X_train.index\nnum_test_index = X_test.index\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train,index=num_train_index, columns=num_cols)\nX_test = pd.DataFrame(X_test,index=num_test_index, columns=num_cols)\n\nX_train = pd.concat([X_train, train_cat_df], axis=1)\nX_test = pd.concat([X_test, test_cat_df], axis=1)\n\ngc.collect()","d0f0cca1":"dtrain = xgb.DMatrix(X_train, y_train)","1b7fcd9c":"dtest = xgb.DMatrix(X_test)","d892fa5d":"y_mean = np.mean(y_train)","e8caf6e4":"#Seb Boving's suggestion for using the nthreads parameter\nimport multiprocessing\n\nn_cpus_avaliable = multiprocessing.cpu_count()\nprint(f'We\\'ve got {n_cpus_avaliable} CPUs to work with.')","ef0dcdbf":"xgb_params = {\n    'eta':  0.05,\n    'max_depth': 8,\n    'subsample': 0.80, \n    'objective':  'reg:linear',\n    'eval_metric': 'rmse', # root mean square error\n    'base_score':  y_mean,\n    'nthread': n_cpus_avaliable\n}","da4e837d":"model = xgb.train(xgb_params, dtrain, num_boost_round=1648)","3d2ffedf":"xgb_pred = model.predict(dtest)","31ff2037":"test_mse = np.mean(((xgb_pred - y_test)**2))\ntest_rmse = np.sqrt(test_mse)\nprint(f'Final test RMSE: {test_rmse} with 1648 prediction rounds used') # root mean square error (RMSE)","1cbf92b2":"I've  commented out the cross validation as it leads to the kernel getting killed for long runtime. When I run this locally, the optimal number of rounds is 10039 for a learning rate of 0.01. For a learning rate of 0.05, 1648 is the best learning rate (but the overall result has a higher rmse) I run the 0.05 learning rate variant below, again due to the time constraints.","38e1d9d0":"$46146.47 was the root mean squared error for a single xgboost algorithm run on the original dataset. So by adding in some new features, and training a roughly equivalent model, we have greatly decreased the rmse\n\nRunning this xgboost model locally, I achieved the following results:\n\nfinal test rmse: 41476.90 with 1648 prediction rounds used and a learning rate of 0.05\n\nfinal test rmse: 40781.61 with 10039 prediction rounds used and a learning rate of 0.01\n\nFrom the large reduction of rmse compared to the previous models I have built ( ~12 percent improvement), we can see that the engineered features are definitely contributing to the predictive ability of the model. Through the simple addition of some population and geospatial data we have reduced the error. The original dataset did contain latitude and longitude data, but we have greatly enhanced these data by giving the models information on the size of the nearest town, the distance to the nearest town, and as well as data on the distance to the nearest big city.\n","28fddc3c":"# Visualizing the data\n\n## 1. Map of the location and population of housing districts, along with a heatmap to show where homes are the most expensive.\n\nThis plot is almost directly from Aur\u00e9lien G\u00e9ron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow' (Chapter 2).  It gives a good representation of the original dataset and provides a reference prior to our discussion of the engineered features below.","8fbdda52":"The code below iterates through the coordinates dataframe and builds a dictonary with the city names as the keys and a tuple of the corresponding (latitude, longitude) as the values. ","2250055f":"## 2. Below we have a graph showing the size and location of the cities in the dataset.","40dd0759":"This next function just takes the vincenty function and uses it to compare a set of coordinates to all of the coordinates stored in a dictonary of locations to determine which location in the dictonary is closest.","4b99fbac":"## 4. Other Engineered Features - Distance to the big city\n\nThis set of features shows which big city (San Francisco, San Jose, San Diego or Los Angeles... those are all really religious spanish names!) a given district is closest to. The cutoff for 'big city' was 500,000 people (this is 1990 data remember). The black lines here represent the same information as the previous plot, and each district was given a value for the distance to the nearest big city (Vincenty) and a categorical for the name of the nearest big city.  I decided to add these features in because the proximity to a large city is somthing I intuitively thought would impact a house price. Access to things like major aiports and major entertainment events\/venues could all have a small but important effect of the price of a home.\n\nLooking at the map below, we can see that this clustering also has an interesting side effect of effectively splitting California into quarters. So having the categorical San Francisco as the closest big city is roughly equivalent to being from the northern 1\/4 of the state, while the San Diego category is roughly equivalent to the bottom 1\/4 of California (I said roughy... don't get out the rulers!). So this feature may be adding some extra structure to the data in ways I had not initially intended.","956a3fa0":"The large block of code below does some cleaning on the existing features, performs a train test split, one-hot encodes the categorical variables, scales the numerical variables and then recombines the data to produce the final, cleaned version of the train and test dataframes","800eb175":"## 3. Engineered Features - Length of vectors between districts and nearest town (Vincenty's formulae)\n\nThe map below provides a visualization of three of the features added to the data. The black lines represent the vectors connecting the districts to the nearest town. Based on these lines, the distance to the nearest town (length of black line), the population of the nearest town (point the black line arrives at) and a categorical for the name of the nearest town were assigned to the district.\n\nThese features help to capture the general trend of houses getting cheaper the further away from downtown one gets (a fairly universal pattern... with plenty of exceptions). They can also aid the model by providing a rough representation of more abstract aspects associated with position in town, such as the commute time for people working in city centers. The nearest town can also capture important effects such as 'who do you pay property taxes to?' and 'what is the infastructure like for this municipality?' which are the types of things that could have impacts of the values of homes.","e2166c3c":"cv_result = xgb.cv(xgb_params, \n                   dtrain, \n                  nfold=5,\n                   num_boost_round=20000,\n                   early_stopping_rounds=50,\n                   verbose_eval=10, \n                   show_stdv=False)\n\nnum_boost_rounds = len(cv_result)\n","01d1f71f":"The dictonaries and functions we created are then applied to the main housing dataframe to make the new data columns. This is done mainly using the pandas .apply() function.","046eb25b":"# Introduction and goals\n\nAt the end of last semester I gave a two part tutorial to my department's R users group where we introduced the basics of machine learning, data cleaning and paramater tuning by following along with an R derivative of  'Hands-On Machine learning with Scikit-Learn and TensorFlow'  by Aur\u00e9lien G\u00e9ron and then building some XGBoost models to try to improve predictions through the use of different algorithms, parameter tuning, and some basic ensembling. One of the follow up exercises I had suggested was trying to add in more features to improve the models not only through tuning algorithms, but through the use of better data.\n\nOn my spare time I started sniffing around and found two datasets that make a good starting point for trying to add to the original features and build better predictive models. And since I was now doing this outside of the confines of the R users group, I have switched over to python for the selfish reason of wanting to keep my python data visualization skills sharp.\n\nThe new data I used:\n\nA list of the latitude and longitude of cities in California: \nhttp:\/\/52.26.186.219\/internships\/useit\/content\/cities-california-latitude-and-longitude\n\nHistorical population data for cities in California (including the year 1990, which is the year the original housing price data was collected :\nhttp:\/\/www.dof.ca.gov\/Reports\/Demographic_Reports\/documents\/2010-1850_STCO_IncCities-FINAL.xls\n\nI have cleaned the originals (a.k.a. de-excel'd) and put them on Kaggle here: https:\/\/www.kaggle.com\/camnugent\/california-housing-feature-engineering\n\nThere is the potential to add a lot of different features based on these dataframes, but my goal was to focus on engineering the following (predominantly geospatial) features for each data point:\n1. The name of the nearest city.\n2. The distance from the district's coordinates to the nearest city.\n3. The population of the nearest city.\n4. The nearest 'big city' (a big city is categorized as >500,000 residents in the given year, 1990).\n5. The distance to the nearest 'big city'.\n\nBelow I walk through the code I used to add these 5 bits of information to each housing district's list of predictors. I then develop some maps\/plots to help visualize the new features and see what information was added, and finally I train an xgboost model on the expanded feature set and assess its performance relative to the previous models I had built using the original data only.\n\nBefore we start a few limitations to note on the data we are adding -  I am not sure if these are comprehensive lists of the cities in California. In fact there is imperfect overlap between the coordinates and population data (so some of the smaller towns are not prefectly represented in the added features), so the data aren't perfect but do a fair job of representing the large\/medium cities and towns in California\n\nIf there are more good features you can think of engineering I encourage you to give the notebook a fork and try them out!\n","ecf178f4":"This next bit of code builds a dictonary of (city, population) (key, value) pairs and then goes through and builds a separate coordinate dictonary for only the big cities.\n\nWe use the April 1990 population data below, as the housing price information that we are trying to model  is based off of the 1990 California Census.","17aeea71":"Two functions are needed to find the distance to the nearest town, first one to determine the distance between two sets of coordinates, and second a function to try all pairwise comparisons and figure out which city is the closest.\n\nFor the first function,  geopy's vincenty function is used to determine the distance between two points on the earths surface using Vincenty's formulae (it accounts for the fact the world is a spheroid when doing the vector math). A quick example is shown below (form the geopy docs)"}}