{"cell_type":{"5238245e":"code","34f8b890":"code","ad07d42c":"code","3c930ad4":"code","42807925":"code","651f783b":"code","94bafc60":"code","de8b2fb4":"code","46cc42c4":"code","a9c47887":"code","1bfe0034":"code","e8973cfa":"code","07d65248":"code","0b804f29":"code","a7b5422d":"code","05f43037":"code","2ca3cb20":"code","0b16d2f9":"code","661f40c1":"code","beae94fe":"code","281be518":"code","b79c3bef":"code","a852d657":"code","855e2b3e":"markdown","2668d903":"markdown","e191191c":"markdown","c13faaa9":"markdown","b84ba142":"markdown","80997a9f":"markdown","45495f78":"markdown","3a3f05c5":"markdown","b37145b5":"markdown","e9e9f5a4":"markdown","44559891":"markdown"},"source":{"5238245e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34f8b890":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ad07d42c":"dataset = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","3c930ad4":"dataset","42807925":"dataset.info()","651f783b":"dataset.isnull().sum()","94bafc60":"plt.figure(figsize= (10,10))\nsns.heatmap(dataset.corr(), annot= True)","de8b2fb4":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","46cc42c4":"x","a9c47887":"y","1bfe0034":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","e8973cfa":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","07d65248":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","0b804f29":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","a7b5422d":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['Naive Bayes', GaussianNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier()])\n\nlst= []\n\nfor m in range(len(models)):\n  a= []\n  model = models[m][1]\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  cm = confusion_matrix(y_test, y_pred)\n  accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)\n  print(models[m][0])\n  print(cm)\n  print('Accuracy Score',accuracy_score(y_test, y_pred))\n  print('')\n  print(\"Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n  print('')\n  print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n  print('')\n  print('-----------------------------------')\n  print('')\n  a.append(models[m][0])\n  a.append((accuracy_score(y_test, y_pred))*100) \n  a.append(accuracies.mean()*100)\n  a.append(accuracies.std()*100)\n  lst.append(a)","05f43037":"lst","2ca3cb20":"df = pd.DataFrame(lst, columns= ['Model', 'Accuracy', 'Mean Accuracy', 'Std. Deviation'])","0b16d2f9":"df.sort_values(by= ['Accuracy', 'Mean Accuracy'], inplace= True, ascending= False)","661f40c1":"df","beae94fe":"rf = RandomForestClassifier()\nsvm = SVC()\nlr = LogisticRegression()","281be518":"data = [(rf, [{'n_estimators': [50, 100, 200, 300, 500], 'criterion': ['gini', 'entropy'], 'random_state':[0]}]), \n        (svm, [{'C': [0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf'], 'random_state':[0]}]),\n        (lr, [{'C': [0.1, 0.5, 1.0], 'random_state':[0]}])]","b79c3bef":"from sklearn.model_selection import GridSearchCV","a852d657":"for i,j in data:\n  grid = GridSearchCV(estimator = i , param_grid = j , scoring = 'accuracy',cv = 10)\n  grid.fit(x_train,y_train)\n  best_accuracy = grid.best_score_\n  best_parameters = grid.best_params_\n  print('{} BestAccuracy : {:.2f}%'.format(i,best_accuracy*100))\n  print('BestParameters : ',best_parameters)","855e2b3e":"# **Selection of Models**","2668d903":"# **Importing Libraries**","e191191c":"# **Heat Map Correlation**","c13faaa9":"**Making Data Frame.**","b84ba142":"# **Feature Scaling**","80997a9f":"# **Splitting dataset into Train and Test set**","45495f78":"**Therefore, after applying GridSearch we can confirm that RandomForest is best suited model on the dataset and gives best accuracy of 84.75%.**","3a3f05c5":"# **Importing Dataset**","b37145b5":"**Below shows the values of models in Descending Order.**","e9e9f5a4":"**Applying Grid Search on Top 3 above models for best parameters and model selection.**\n1. Random Forest\n2. SVM\n3. Logistic Regression","44559891":"**Checking if there are any NULL values.**"}}