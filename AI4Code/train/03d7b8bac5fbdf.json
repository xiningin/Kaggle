{"cell_type":{"a0cfb708":"code","370c3364":"code","9831d009":"code","0e9684fd":"code","83dfc9cd":"code","a8848f1a":"code","c38863e9":"code","b7c69e11":"code","931264d4":"code","730d3111":"code","b3e3e171":"markdown","75b3acf2":"markdown","ac3d8896":"markdown","e198e0b4":"markdown","2c42d35a":"markdown","18d939cf":"markdown","93d43367":"markdown","2f1885df":"markdown","06bbfc2c":"markdown","2b7e5e52":"markdown","9bd02d0d":"markdown","293c9984":"markdown"},"source":{"a0cfb708":"pip install  mglearn","370c3364":"import mglearn\nmglearn.plots.plot_linear_regression_wave()","9831d009":"X, y = mglearn.datasets.load_extended_boston()\nprint(\"X.shape: {}\".format(X.shape))\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = LinearRegression().fit(X_train, y_train)","0e9684fd":"print(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))","83dfc9cd":"print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))","a8848f1a":"from sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","c38863e9":"ridge10 = Ridge(alpha=10).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))","b7c69e11":"ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))","931264d4":"import numpy as np\nfrom sklearn.linear_model import Lasso\nlasso = Lasso().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))","730d3111":"# we increase the default setting of \"max_iter\",\n# otherwise the model would warn us that we should increase max_iter.\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))","b3e3e171":"# 3-Lasso regression\n* An alternative to Ridge for regularizing linear regression is Lasso.\n* As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called **L1 regularization**.\n* When using the lasso, some coefficients are exactly zero.\n* This means some features are entirely ignored by the model.\n\n\n","75b3acf2":"# **6-Regression**\n* In regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).\n* An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. \n* If there is continuity between possible outcomes, then the problem is a regression problem.","ac3d8896":"# 2-Ridge regression\n* Ridge regression is also a linear model for regression.\n* In ridge regression,though, the coefficients (w) are chosen not only so that they predict well on the training data, but also to fit an additional constraint.\n* We also want the magnitude of coefficients to be as small as possible; in other words, all entries of w should be close to zero.\n* Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well.\n* This constraint is an example of what is called **regularization**. \n* **Regularization** means explicitly restricting a model to avoid overfitting.","e198e0b4":"# 1-Linear regression\n* Linear regression, or ordinary least squares (OLS), is the simplest and most classic linear method for regression.\n* Linear regression finds the parameters w and b that minimize the mean squared error between predictions and the true regression targets, y, on the training set.\n* The mean squared error is the sum of the squared differences between the predictions and the true values.","2c42d35a":"* As you can see, the training set score of Ridge is lower than for LinearRegression, while the test set score is higher.\n* Ridge is a more restricted model, so we are less likely to overfit. A less complex model means worse performance on the training set, but better generalization.\n* The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set.\n* we used the default parameter alpha=1.0.\n* The optimum setting of alpha depends on the particular dataset we are using.\n* Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization.\n","18d939cf":"* For very small values of alpha, coefficients are barely restricted at all, and we end up with a model that resembles LinearRegression:","93d43367":"### [**Back to course home**](https:\/\/www.kaggle.com\/ammarnassanalhajali\/machine-learning-in-python-course-1)","2f1885df":"## The Boston Housing\n* The task associated with this dataset is to predict the median value of homes in several Boston neighborhoods in the 1970s, using information such as crime rate, proximity to the Charles River, highway accessibility, and so on. The dataset contains 506 data points, described by 104 features:","06bbfc2c":"* As you can see, Lasso does quite badly, both on the training and the test set. This indicates that we are underfitting, and we find that it used only 4 of the 105 features.\n* the Lasso also has a regularization parameter, alpha, that controls how strongly coefficients are pushed toward zero.\n* the previous example, we used the default of alpha=1.0. To reduce underfitting, let\u2019s try decreasing alpha.\n* When we do this, we also need to increase the default setting of max_iter (the maximum number of iterations to run):","2b7e5e52":"## Linear Models for Regression \nFor regression, the general prediction formula for a linear model looks as follows:\n\n### \u0177 = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n* Here, x[0] to x[p] denotes the features of a single data point, w and b are parameters of the model that are learned.\n* \u0177 is the prediction the model makes. \n\n* For a dataset with a single feature,this is:\n\n### \u0177 = w[0] * x[0] + b","9bd02d0d":"* which you might remember from high school mathematics as the equation for a line. Here, \n* w[0] is the slope and b is the y-axis offset.","293c9984":"* **Note**:The particular kind used by ridge regression is known as **L2 regularization**."}}