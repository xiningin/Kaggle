{"cell_type":{"0f055e33":"code","40a4e382":"code","02a9240e":"code","dcc0e1e6":"code","07e1721d":"code","14a6db75":"code","454477b2":"code","66ac8cfb":"code","2f1e6b9e":"code","394298cd":"code","4f4166c3":"code","96314025":"code","f862c826":"code","e62dd073":"code","bf7af0f0":"code","ebc00aac":"code","ccfb8419":"code","bdd83903":"code","9706122b":"code","97cfa795":"code","f7396fe6":"code","b740291c":"code","672035f6":"code","3db80199":"code","eded2ea3":"code","2e1812fc":"code","60d553ae":"code","5984e34c":"code","d6210128":"code","ed4653b8":"code","7056efd1":"code","8eee181b":"code","333921a3":"code","db77613c":"code","75bb49a4":"code","e5ca7566":"code","9e936d0a":"code","d0014610":"code","3602ceee":"code","bf6a812c":"code","f316da86":"code","c1ab5a7e":"code","3a4ba98c":"code","4d7a384a":"code","adc802bc":"code","81c8c4e5":"code","5dfb5e76":"code","c6617999":"code","283b5d7b":"code","c6602fef":"code","cd92dc97":"code","127e0bd4":"code","5f204bd1":"code","21513f13":"code","d5a82ecf":"code","1a097534":"code","62145ab3":"code","2b2eb812":"code","b9f4a9cd":"code","647003d2":"code","b107d40d":"code","2ec15c2a":"code","68876d01":"code","86622b1b":"code","58fa9d90":"code","9b582420":"code","af9c6b22":"code","d4bbb35c":"code","2dd98841":"code","51b3f7e7":"code","0b7641ce":"code","6c5b6168":"code","0710bf6f":"code","a43fc7e9":"code","7614a1ad":"code","0215f261":"code","570c8c97":"code","13835eb3":"code","35b8aff3":"code","064e18db":"code","1d30b9b0":"code","7e6c6e69":"code","6c63bad9":"code","9b51c228":"code","34e40fea":"code","6cd70d21":"code","434d4a89":"code","e7b791fd":"code","8ee0bfc8":"code","fed59549":"code","f23791af":"code","2e6930a7":"code","2dc8af38":"code","b068befb":"code","ff22fb6c":"code","4a1c4e2d":"code","4991152c":"code","960df942":"code","62102216":"code","7635de84":"code","b9aade62":"code","b7ee14aa":"code","b1ad9d55":"code","e2637d98":"code","23c8e2cd":"code","9a860a92":"code","a165ad32":"code","3fd3ec13":"code","7fe266b0":"code","3cee8275":"code","8c8ec8f2":"code","da6fa1fd":"code","6462c896":"code","5a0476c1":"code","470b8e48":"code","8ebfb165":"code","fc996237":"code","b295df64":"code","8dc1fbe9":"code","3df0dfc7":"code","b01764b2":"code","e0d21c2b":"code","a6cc395b":"code","64093730":"code","c1b609d4":"code","5090838a":"code","ef7a73b4":"code","c9e37f14":"code","5d69d7e1":"code","26c6d4dc":"code","60643397":"code","86589354":"code","1f151bc7":"code","9e391172":"code","2979ec51":"code","54a7ec5e":"code","1613676e":"code","0fdf9b52":"code","477c4231":"code","6b508ab1":"markdown","42a2eb33":"markdown","250060cd":"markdown","98cc11cd":"markdown","60e51169":"markdown","9c63f522":"markdown","bb6e7331":"markdown","a8478e95":"markdown","83f12c35":"markdown","9156f819":"markdown","fa09a6cb":"markdown","b806b06b":"markdown","5d2e52d2":"markdown","c4812e50":"markdown","0a55d971":"markdown","166a7881":"markdown","c52b43e2":"markdown","9db2dba4":"markdown","6a85fc75":"markdown","d626a1c9":"markdown","78932514":"markdown","c139e105":"markdown","0863e918":"markdown","830921b1":"markdown","757ee223":"markdown","98ff4725":"markdown","5369d313":"markdown","21cf42a0":"markdown","5ca0170f":"markdown","87056e78":"markdown","3639df25":"markdown","7e6be58d":"markdown","10716dc4":"markdown","976d572a":"markdown","74c687b8":"markdown","bf36542d":"markdown","5643b168":"markdown","1255441d":"markdown","d0acf775":"markdown","13859cf2":"markdown","0dec9370":"markdown","be30c1ad":"markdown","8eed9bb0":"markdown","2057dad5":"markdown","afd140ab":"markdown","eeec41e1":"markdown","40b6d080":"markdown","56a5fc2e":"markdown","70521aa2":"markdown","40c68cf8":"markdown","322713c3":"markdown","b318be03":"markdown","a3b725c3":"markdown","a736f291":"markdown","5f0d3657":"markdown"},"source":{"0f055e33":"# common imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning imports\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator\nfrom sklearn import metrics\n\n# display setup\npd.set_option(\"display.max_columns\", None) # the None parameter displays unlimited columns\nsns.set(style=\"whitegrid\") # for plots","40a4e382":"# read the csv file\ndf = pd.read_csv(r\"..\/input\/hotel-booking-demand\/hotel_bookings.csv\")","02a9240e":"# display the first 5 rows for a quick look\ndf.head()","dcc0e1e6":"# DataFrame shape (rows, columns)\n# understand the amount of data we are working with\ndf.shape","07e1721d":"# description of data\ndf.info()","14a6db75":"# summary of the numerical attributes\n# null values are ignored\ndf.describe()","454477b2":"# a histogram plot for each numerical attribute\ndf.hist(bins=50, figsize=(20,15))\nplt.tight_layout()\nplt.show()","66ac8cfb":"# use sklearn train_test_split function to split the data\n# the reason for selecting 0.15 as the test size is because the dataset is very large\n# the random state parameter ensures that data will be shuffled and split the same way in each run\ntrain_set, test_set = train_test_split(df, test_size=0.15, random_state=42)","2f1e6b9e":"print(\"Number of instances in training set: \", len(train_set))\nprint(\"Number of instances in testing set: \", len(test_set))","394298cd":"# deep copy of the training set\ndf2 = train_set.copy()","4f4166c3":"df2.head(2)","96314025":"# the methods below calculate the number of missing values\nmissing_values = df2.isna().sum()\nmissing_values = missing_values[missing_values != 0]\nmissing_values","f862c826":"# replace missing values\n\n# can assume that there were no children\ndf2.fillna({\"children\": 0}, inplace=True)\n\n# missing countries can be labeled unknown\ndf2.fillna({\"country\": \"Unknown\"}, inplace=True)\n\n# missing agent ID can be zero, presuming the booking was made privately\ndf2.fillna({\"agent\": 0}, inplace=True)\n\n# missing company ID can be zero (for the same reason as agent ID)\ndf2.fillna({\"company\": 0}, inplace=True)","e62dd073":"# check that the values were filled\ndf2.isna().sum()","bf7af0f0":"# method creates a correlations matrix\ncorr_matrix = df2.corr()","ebc00aac":"# looking at attributes correlation with is_canceled feature\ncorr_matrix[\"is_canceled\"].sort_values(ascending=False)","ccfb8419":"# experimenting with attribute combinations\n\n# create a column with total amount of guests\ndf2[\"guests_stayed\"] = df2[\"adults\"] + df2[\"children\"] + df2[\"babies\"]\n\n# create a column with total nights booked\ndf2[\"nights_stayed\"] = df2[\"stays_in_week_nights\"] + df2[\"stays_in_weekend_nights\"]","bdd83903":"# looking at the correlation matrix again with the added columns\ncorr_matrix = df2.corr()\ncorr_matrix[\"is_canceled\"].sort_values(ascending=False)","9706122b":"# hist plot of lead time\n# kde = kernel density estimation (displays distribution function, density curve)\n# shows the distribution and highest concentration points\nplt.figure(figsize=(10,5))\nlead_time = df2['lead_time']\nlead_time = pd.DataFrame(sorted(lead_time, reverse = True), columns = ['Lead'])\nsns.histplot(lead_time, kde=True)\nplt.title(\"Lead Time\", size=20)\nplt.xlabel(\"lead time days\", size=15)\nplt.tight_layout()\nplt.show()","97cfa795":"# divides lead time by less than 100 days, 100-355 days and 365 or more days\nlead_time_1 = df2[df2[\"lead_time\"] < 100]\nlead_time_2 = df2[(df2[\"lead_time\"] >= 100) & (df2[\"lead_time\"] < 365)]\nlead_time_3 = df2[df2[\"lead_time\"] >= 365]","f7396fe6":"# calculates cancellations according to lead time groups\nlead_cancel_1 = lead_time_1[\"is_canceled\"].value_counts()\nlead_cancel_2 = lead_time_2[\"is_canceled\"].value_counts()\nlead_cancel_3 = lead_time_3[\"is_canceled\"].value_counts()","b740291c":"# hist plot for each lead time group\nfig, (bx1, bx2, bx3) = plt.subplots(1,3,figsize=(21,6))\nsns.histplot(lead_time_1[\"lead_time\"], ax = bx1, kde=True)\nbx1.set_title(\"lead_time [0,100) days\", size=20)\nsns.histplot(lead_time_2[\"lead_time\"], ax = bx2, kde=True)\nbx2.set_title(\"lead_time [100,365) days\", size=20)\nsns.histplot(lead_time_3[\"lead_time\"], ax = bx3, kde=True)\nbx3.set_title(\"lead_time [365,max) days\", size=20)\nplt.tight_layout()\nplt.show()","672035f6":"# total count of lead time according to cancellation\ntotal_lead_days_cancel = pd.DataFrame(data=[lead_cancel_1,lead_cancel_2,lead_cancel_3],\n             index=[\"[0,100) days\", \"[100,365) days\", \"[365,max) days\"])\ntotal_lead_days_cancel","3db80199":"# pie plot for each lead time group\nfig, ax = plt.subplots(1,3, figsize=(21,6))\nax[0].pie(np.array([total_lead_days_cancel[0][0], total_lead_days_cancel[1][0]]),\n          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n          colors=['forestgreen', 'firebrick'])\nax[0].set_title(\"lead_time [0,100) days\", size=20)\nax[1].pie(np.array([total_lead_days_cancel[0][1], total_lead_days_cancel[1][1]]),\n          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n          colors=['forestgreen', 'firebrick'])\nax[1].set_title(\"lead_time [100,365) days\", size=20)\nax[2].pie(np.array([total_lead_days_cancel[0][2], total_lead_days_cancel[1][2]]),\n          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n          colors=['forestgreen', 'firebrick'])\nax[2].set_title(\"lead_time [365,max) days\", size=20)\nplt.tight_layout()\nplt.show()","eded2ea3":"# get previous cancellations column\nprev_cancel = df2[\"previous_cancellations\"]","2e1812fc":"# sort the index values\nprev_cancel.value_counts().sort_index()","60d553ae":"print(\"Cancellation Rates:\\n\")\nprint('Never canceled =' ,str(round(df2[df2['previous_cancellations']==0]\n                                            ['is_canceled'].mean()*100,2))+' %')\nprint('Canceled once =' ,str(round(df2[df2['previous_cancellations']==1]\n                                            ['is_canceled'].mean()*100,2))+' %')\nprint('Canceled more than 10 times:',str(round(df2[df2['previous_cancellations']>10]\n                                            ['is_canceled'].mean()*100,2))+' %')\nprint('Canceled more than 11 times:' ,str(round(df2[df2['previous_cancellations']>11]\n                                            ['is_canceled'].mean()*100,2))+' %')","5984e34c":"# create a list with previous cancellations indices\nprev_cancel_index = df2[\"previous_cancellations\"].value_counts().index.to_list()\n# sort the list\nprev_cancel_index.sort()\n\n# calculate the average percentage of cancellations for each value in the DataFrame\npercentage_prev_cancel= []\nfor i in prev_cancel_index:\n    percentage_prev_cancel.append((round(df2[df2[\"previous_cancellations\"]==i][\"is_canceled\"].mean()*100,2)))","d6210128":"# create a DataFrame with the results\ndf_prev_cancel = pd.DataFrame(percentage_prev_cancel, index=prev_cancel_index, columns=[\"Previous Cancellations %\"])\ndf_prev_cancel","ed4653b8":"# plot previous cancellations by percentages\ndf_prev_cancel.plot(figsize= (10,5), linewidth=3)\nplt.title(\"Previous Cancellations\", size=20)\nplt.xlabel(\"Number of Previous Cancellations\", size=15)\nplt.ylabel(\"%\", size=15)\nplt.tight_layout()\nplt.show()","7056efd1":"# number of instances for each value\ndf2[\"total_of_special_requests\"].value_counts()","8eee181b":"# group by cancellations\nis_canceled = df2.groupby(by=\"is_canceled\")","333921a3":"# get groups according to binary outcome\ncanceled = is_canceled.get_group(1)\nnot_canceled = is_canceled.get_group(0)","db77613c":"# count values for each outcome\nspecial_requests_0 = not_canceled[\"total_of_special_requests\"].value_counts()\nspecial_requests_1 = canceled[\"total_of_special_requests\"].value_counts()","75bb49a4":"# create a DataFrame for each outcome\ndf_special_requests_0 = pd.DataFrame(special_requests_0.values, index=special_requests_0.index, columns=[\"not_canceled\"])\ndf_special_requests_1 = pd.DataFrame(special_requests_1.values, index=special_requests_1.index, columns=[\"canceled\"])","e5ca7566":"# join both DataFrames side by side\ndf_special_requests= df_special_requests_0.join(df_special_requests_1)","9e936d0a":"# add total of both outcomes\nspecial_requests_total = df_special_requests[\"not_canceled\"] + df_special_requests[\"canceled\"]\n\n# calculate percentage of cancellations for each number of requests value individually\nspecial_requests_percentage = []\nfor i in special_requests_total.index:\n    special_requests_percentage.append(round((special_requests_1[i]\/special_requests_total[i])*100,2))\nspecial_requests_percentage","d0014610":"# add percentages as new column in DataFrame\ndf_special_requests.join(pd.DataFrame(special_requests_percentage, index=df_special_requests.index,\n             columns=[\"cancellations %\"]))","3602ceee":"# plot special requests according to cancellations\nplt.figure(figsize=(10,5))\nsns.countplot(x=df2[\"total_of_special_requests\"], hue=df2[\"is_canceled\"])\nplt.title(\"Special Requests\", size=20)\nplt.xlabel(\"Number of Special Requests\", size=15)\nplt.legend([\"not canceled\", \"canceled\"])\nplt.tight_layout()\nplt.show()","bf6a812c":"# number of instances for each value\ndf2[\"required_car_parking_spaces\"].value_counts().sort_index()","f316da86":"# count values for each outcome with previous groupby\nparking_spaces_0 = not_canceled[\"required_car_parking_spaces\"].value_counts()\nparking_spaces_1 = canceled[\"required_car_parking_spaces\"].value_counts()","c1ab5a7e":"# value counts for non canceled instances\nparking_spaces_0.sort_index()","3a4ba98c":"# value counts for canceled instances\nparking_spaces_1","4d7a384a":"# pie plot of cancellations with zero required parking spaces\nplt.pie(x=[parking_spaces_0[0], parking_spaces_1[0]], labels=[\"not_canceled\", \"canceled\"],autopct='%1.1f%%',\n        startangle=90, colors=['forestgreen', 'firebrick'])\nplt.title(\"Zero Required Parking Spaces Cancellations\", size=20)\nplt.tight_layout()\nplt.show()","adc802bc":"# number of instances for each value\ndf2[\"booking_changes\"].value_counts().sort_index()","81c8c4e5":"# count values for each outcome with previous groupby\nbooking_changes_0 = not_canceled[\"booking_changes\"].value_counts()\nbooking_changes_1 = canceled[\"booking_changes\"].value_counts()","5dfb5e76":"# count index of not canceled\nlen(booking_changes_0.index)","c6617999":"# count index of canceled\nlen(booking_changes_1.index)","283b5d7b":"# fill missing values\n# the outcome 0 has more values\n# filling the values will enable joining the dataframes later\ndf_booking_changes_1 = pd.DataFrame(booking_changes_1, index=booking_changes_0.index)\ndf_booking_changes_1.fillna({\"booking_changes\": 0}, inplace=True)\nbooking_changes_1 = pd.Series(df_booking_changes_1[\"booking_changes\"])","c6602fef":"# add total of both outcomes\nbooking_changes_total = booking_changes_0 + booking_changes_1\n\n# calculate percentage of cancellations for each number of booking changes individually\npercentage_booking_changes = []\nfor i in booking_changes_total.index:\n    percentage_booking_changes.append(round((booking_changes_1[i]\/booking_changes_total[i])*100,2))","cd92dc97":"# create a DataFrame with the percentage of cancellations\ndf_percentage_booking_changes = pd.DataFrame(percentage_booking_changes, index=booking_changes_total.index,\n                                             columns=[\"cancellations %\"])","127e0bd4":"# create a DataFrame for each outcome\ndf_booking_changes_0 = pd.DataFrame(booking_changes_0.values, index=booking_changes_0.index, columns=[\"not_canceled\"])\ndf_booking_changes_1 = pd.DataFrame(booking_changes_1.values, index=booking_changes_1.index, columns=[\"canceled\"])","5f204bd1":"# join all three DataFrames side by side\ndf_booking_changes = df_booking_changes_0.join\\\n    ([df_booking_changes_1, df_percentage_booking_changes])\n\n# remove rows with 0% cancellations\ndf_booking_changes = df_booking_changes[df_booking_changes[\"cancellations %\"]!=0]\ndf_booking_changes","21513f13":"df2[df2[\"adr\"]==0][\"reservation_status\"].value_counts()","d5a82ecf":"df2[df2[\"adr\"]==0][\"is_canceled\"].value_counts()","1a097534":"df2[\"hotel\"].value_counts()","62145ab3":"# a plot of the number of instances for each hotel according to cancellations\nplt.figure(figsize=(10,5))\nsns.countplot(x=df2[\"hotel\"], hue=df2[\"is_canceled\"])\nplt.title(\"Hotel Cancellations\", size=20)\nplt.legend([\"not canceled\", \"canceled\"])\nplt.tight_layout()\nplt.show()","2b2eb812":"ordered_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\nresort_canceled_percent = []\ncity_canceled_percent = []\n\n# divide cancellation outcome by hotel and month of arrival\nresort_1 = canceled[canceled[\"hotel\"]==\"Resort Hotel\"][\"arrival_date_month\"].value_counts()\nresort_0 = not_canceled[not_canceled[\"hotel\"]==\"Resort Hotel\"][\"arrival_date_month\"].value_counts()\ncity_1 = canceled[canceled[\"hotel\"]==\"City Hotel\"][\"arrival_date_month\"].value_counts()\ncity_0 = not_canceled[not_canceled[\"hotel\"]==\"City Hotel\"][\"arrival_date_month\"].value_counts()\n\n# calculate cancellation percentage according to hotel\nfor i in ordered_months:\n    resort_canceled_percent.append(round((resort_1[i] \/ (resort_0[i]+resort_1[i]))*100,2))\n    city_canceled_percent.append(round((city_1[i]\/(city_0[i]+city_1[i]))*100,2))\n\n# create a DataFrame with the cancellation percentage of each hotel\ndf_resort_cancel = pd.DataFrame(resort_canceled_percent, index=ordered_months, columns=[\"Resort Hotel Canceled %\"])\ndf_city_cancel = pd.DataFrame(city_canceled_percent, index=ordered_months, columns=[\"City Hotel Canceled %\"])\n\n# join DataFrames\ndf_hotel_cancel = df_resort_cancel.join(df_city_cancel)\ndf_hotel_cancel","b9f4a9cd":"# plot meal according to cancellations\nplt.figure(figsize=(10,5))\nsns.countplot(x=df2[\"meal\"], hue=df2[\"is_canceled\"])\nplt.title(\"Cancellations According to Meal Booked\", size=20)\nplt.xlabel(\"meal\", size=15)\nplt.legend([\"not canceled\", \"canceled\"])\nplt.tight_layout()\nplt.show()","647003d2":"df2[\"market_segment\"].value_counts()","b107d40d":"# calculate cancellation percentage according to market segment\nmarket_segment_percent = []\n\nmarket_segment_1 = canceled[\"market_segment\"].value_counts()\nmarket_segment_total = df2[\"market_segment\"].value_counts()\n\nfor i in market_segment_total.index:\n    market_segment_percent.append(str(i+\": \") +\n                    str(round((market_segment_1[i]\/market_segment_total[i])*100,2)))\nmarket_segment_percent","2ec15c2a":"df2[\"distribution_channel\"].value_counts()","68876d01":"# calculate cancellation percentage according to distribution channel\ndistribution_channel_percent = []\n\ndistribution_channel_1 = canceled[\"distribution_channel\"].value_counts()\ndistribution_channel_total = df2[\"distribution_channel\"].value_counts()\n\nfor i in distribution_channel_total.index:\n    distribution_channel_percent.append(str(i+\": \") +\n                    str(round((distribution_channel_1[i]\/distribution_channel_total[i])*100,2)))\ndistribution_channel_percent","86622b1b":"df2[\"customer_type\"].value_counts()","58fa9d90":"# calculate cancellation percentage according to customer type\ncustomer_type_percent = []\n\ncustomer_type_1 = canceled[\"customer_type\"].value_counts()\ncustomer_type_total = df2[\"customer_type\"].value_counts()\n\nfor i in customer_type_total.index:\n    customer_type_percent.append(str(i+\": \") +\n                    str(round((customer_type_1[i]\/customer_type_total[i])*100,2)))\ncustomer_type_percent","9b582420":"# plot of cancellations according to room type\nplt.figure(figsize=(10,5))\nsns.countplot(x=df2[\"reserved_room_type\"], hue=df2[\"is_canceled\"])\nplt.title(\"Cancellations According to Room Type\", size=20)\nplt.legend([\"not canceled\", \"canceled\"], loc=1)\nplt.tight_layout()\nplt.show()","af9c6b22":"df2[\"deposit_type\"].value_counts()","d4bbb35c":"# calculate deposit type instances percentage in data\ndeposit_percent = round(df2[\"deposit_type\"].value_counts()\/len(df[\"deposit_type\"])*100,4)\ndeposit_percent","2dd98841":"# use groupby to divide according to deposit type\ndeposit = df2.groupby(by=\"deposit_type\")\nnon_refund = deposit.get_group(\"Non Refund\")\nrefundable = deposit.get_group(\"Refundable\")\nno_deposit = deposit.get_group(\"No Deposit\")","51b3f7e7":"# calculate number of cancellations according to deposit type\nno_deposit_0 = (no_deposit[\"is_canceled\"]==0).sum()\nno_deposit_1 = (no_deposit[\"is_canceled\"]==1).sum()\nnon_refund_0 = (non_refund[\"is_canceled\"]==0).sum()\nnon_refund_1 = (non_refund[\"is_canceled\"]==1).sum()\nrefundable_0 = (refundable[\"is_canceled\"]==0).sum()\nrefundable_1 = (refundable[\"is_canceled\"]==1).sum()\nall_canceled = no_deposit_1 + non_refund_1 + refundable_1\nall_not_canceled = no_deposit_0 + non_refund_0 + refundable_0","0b7641ce":"# check that all values were calculated\nall_canceled + all_not_canceled == df2[\"deposit_type\"].size","6c5b6168":"# create a DataFrame with the number of instances for each deposit type\ndf_deposit_type = pd.DataFrame(index=[\"Not Canceled\", \"Canceled\"])\ndf_deposit_type[\"no_deposit\"] = [no_deposit_0, no_deposit_1]\ndf_deposit_type[\"non_refund\"] = [non_refund_0, non_refund_1]\ndf_deposit_type[\"refundable\"] = [refundable_0, refundable_1]\ndf_deposit_type","0710bf6f":"# pie plot of cancellations according to deposit type\ncancel_labels = [\"cancelled\", \"not_cancelled\"]\nfig, dx = plt.subplots(1,3, figsize=(21,6))\ndx[0].pie(np.array([no_deposit_1, no_deposit_0]), labels=cancel_labels, autopct='%1.1f%%', startangle=90,\n          colors=['firebrick', 'forestgreen'])\ndx[0].set_title(\"No Deposit Cancellations\", size=20)\ndx[1].pie(np.array([non_refund_1, non_refund_0]), labels=cancel_labels, autopct='%1.1f%%', startangle=90,\n          colors=['firebrick', 'forestgreen'])\ndx[1].set_title(\"Non Refund Cancellations\", size=20)\ndx[2].pie(np.array([refundable_1, refundable_0]), labels=cancel_labels, autopct='%1.1f%%', startangle=90,\n          colors=['firebrick', 'forestgreen'])\ndx[2].set_title(\"Refundable Cancellations\", size=20)\nplt.tight_layout()\nplt.show()","a43fc7e9":"df2[\"country\"].unique().size","7614a1ad":"canceled[\"country\"].value_counts()","0215f261":"# calculate countries by number of instances that appear in data\ncountry_1 = (df2[\"country\"].value_counts() <= 1).sum()\ncountry_10 = (df2[\"country\"].value_counts() <= 10).sum()\ncountry_50 = (df2[\"country\"].value_counts() <= 50).sum()\ncountry_100 = (df2[\"country\"].value_counts() <= 100).sum()\ncountry_1000 = (df2[\"country\"].value_counts() <= 1000).sum()\n\nprint(\"Number of countries with one or less instances:\", country_1,\n      \"\\nNumber of countries with 10 or less instances:\", country_10,\n      \"\\nNumber of countries with 50 or less instances:\", country_50,\n      \"\\nNumber of countries with 100 or less instances:\", country_100,\n      \"\\nNumber of countries with 1000 or less instances:\", country_1000)","570c8c97":"# clean copy of the training set\ndf3 = train_set.copy()","13835eb3":"# custom transformer removes instances with zero guests\n\nclass RemoveZeroGuests(TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        XData = X.loc[((X[\"adults\"]) + (X[\"children\"]) + (X[\"babies\"])) > 0]\n        return XData","35b8aff3":"df3.shape","064e18db":"# use transformer to remove instances with zero guests stayed\ndf3 = RemoveZeroGuests().fit_transform(df3)","1d30b9b0":"df3.shape","7e6c6e69":"# separate predictors from target values\n\n# drop- creates a copy without changing the training set\nX_train = df3.drop(\"is_canceled\", axis=1)\n\n# create a deep copy of the target values\ny_train = df3[\"is_canceled\"].copy()","6c63bad9":"num_features = [\"lead_time\", \"stays_in_weekend_nights\", \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n                \"is_repeated_guest\", \"previous_cancellations\", \"previous_bookings_not_canceled\", \"adr\",\n                \"required_car_parking_spaces\", \"total_of_special_requests\"]\n\ncat_features = [\"hotel\", \"arrival_date_month\", \"arrival_date_week_number\", \"meal\", \"market_segment\",\n                \"distribution_channel\", \"reserved_room_type\", \"deposit_type\", \"customer_type\"]","9b51c228":"# Undefined\/SC both represent no meal package and can be combined\n\nclass ReplaceMeal(TransformerMixin):\n\n    def fit(self,X, y=None):\n        return self\n\n    def transform(self, X):\n        XData = X.copy()\n        XData[\"meal\"].replace(\"Undefined\", \"SC\", inplace=True)\n        return XData","34e40fea":"# SimpleImputer constant default fills values with zero\n# MinMaxScaler normalizes data (rescales between 0-1)\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"constant\")),\n    (\"min_max\", MinMaxScaler())\n])","6cd70d21":"# SimpleImputer fills missing values with 'Unknown'\n# OneHotEncoder converts categories to a numeric dummy array\n# (one binary attribute per category)\ncat_pipeline = Pipeline([\n    (\"meal\", ReplaceMeal()),\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])","434d4a89":"# column transformer:\n# features generated by each transformer will be concatenated to form a single feature space\n# columns of the original feature matrix that are not specified are dropped\nfull_pipeline = ColumnTransformer([\n    (\"numerical\", num_pipeline, num_features),\n    (\"categorical\", cat_pipeline, cat_features)\n])","e7b791fd":"# transform training data using pipeline\nX_train_prepared = full_pipeline.fit_transform(X_train)\n\n# transform training data without fit for testing\nX_tr_testing = full_pipeline.transform(X_train)","8ee0bfc8":"# function prints scores\ndef display_evaluation(actual, pred):\n    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(actual, pred), \"\\n\")\n    print(\"Classification Report:\\n\", metrics.classification_report(actual, pred))","fed59549":"# instantiate classifier\n# default k=5\nknn = KNeighborsClassifier()","f23791af":"# fit the training set\nknn.fit(X_train_prepared, y_train)","2e6930a7":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", knn.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","2dc8af38":"# predict using the training data\nknn_pred = knn.predict(X_tr_testing)","b068befb":"# use function to show results\ndisplay_evaluation(y_train, knn_pred)","ff22fb6c":"# instantiate KNN model using distance instead of uniform\n# distance means closer instances have a larger weight\n# uniform weighs all instances equally\n# default k=5\nknn = KNeighborsClassifier(weights=\"distance\")","4a1c4e2d":"# fit the training set\nknn.fit(X_train_prepared, y_train)","4991152c":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", knn.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","960df942":"# predict using the training data\nknn_pred_2 = knn.predict(X_tr_testing)","62102216":"# use function to show results\ndisplay_evaluation(y_train, knn_pred_2)","7635de84":"# max features default is sqrt (number of features selected per split)\n# bootstrap default is true (resampling data true)\n# n estimators default is 100 (number of decision tree classifiers)\nrf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)","b9aade62":"# fit the training set\nrf_clf.fit(X_train_prepared, y_train)","b7ee14aa":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", rf_clf.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","b1ad9d55":"# predict using the training data\nrf_pred = rf_clf.predict(X_tr_testing)","e2637d98":"# use function to show results\nprint(display_evaluation(y_train, rf_pred))","23c8e2cd":"# parameters for random search\nparam_dist_rf = [{\"n_estimators\": [10, 50, 100, 500], \"max_features\": [\"sqrt\", 8, 16], \"bootstrap\": [True, False]}]","9a860a92":"# instantiate randomized search\nrf_cv = RandomizedSearchCV(rf_clf, param_dist_rf, n_iter=10, random_state=42, cv=5, scoring=\"f1\")","a165ad32":"# fit the training set\nrf_cv.fit(X_train_prepared, y_train)","3fd3ec13":"# show the best score\nrf_cv.best_score_","7fe266b0":"# show the best estimator parameters\nrf_clf = rf_cv.best_estimator_\nrf_clf","3cee8275":"# show results for each iteration\ncvres = rf_cv.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)","8c8ec8f2":"# predict using training data\nrf_pred_2 = rf_clf.predict(X_tr_testing)","da6fa1fd":"# display evaluation scores\ndisplay_evaluation(y_train, rf_pred_2)","6462c896":"# pair the feature names with the results from randomized search\nfeature_importance = rf_cv.best_estimator_.feature_importances_\nfeatures = num_features+cat_features\nsorted(zip(feature_importance,features), reverse=True)","5a0476c1":"# features left\n\nnum_features_2 = [\"lead_time\", \"stays_in_weekend_nights\", \"stays_in_week_nights\", \"adults\", \"children\",\n                  \"previous_cancellations\", \"adr\", \"required_car_parking_spaces\", \"total_of_special_requests\"]\n\ncat_features_2 = [\"hotel\", \"arrival_date_month\"]\n\n\n# category pipeline without meal transformer\ncat_pipeline_2 = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\n# pipeline with new features\nfull_pipeline_2 = ColumnTransformer([\n    (\"numerical\", num_pipeline, num_features_2),\n    (\"categorical\", cat_pipeline_2, cat_features_2)\n])","470b8e48":"# transform data with new pipeline\nX_train_prepared_2 = full_pipeline_2.fit_transform(X_train)\nX_tr_testing_2 = full_pipeline_2.transform(X_train)","8ebfb165":"# instantiate model\nrf_clf_2 = RandomForestClassifier(random_state=42, n_jobs=-1)","fc996237":"# fit the training set\nrf_clf_2.fit(X_train_prepared_2, y_train)","b295df64":"# predictions on training data\nrf_pred_3 = rf_clf_2.predict(X_tr_testing_2)","8dc1fbe9":"# display evaluation scores\ndisplay_evaluation(y_train, rf_pred_3)","3df0dfc7":"# parameters for random search\nparam_dist = [{\"n_estimators\": [10, 50, 100, 500], \"max_features\": [4, 8, 16], \"bootstrap\": [True, False]}]","b01764b2":"# instantiate randomized search\nrf_cv_2 = RandomizedSearchCV(rf_clf_2, param_dist, cv=5, n_iter=10, scoring=\"f1\", random_state=42)","e0d21c2b":"# fit the training set\nrf_cv_2.fit(X_train_prepared_2, y_train)","a6cc395b":"# show the best score\nrf_cv_2.best_score_","64093730":"# show the best estimator parameters\nrf_clf_3 = rf_cv_2.best_estimator_\nrf_clf_3","c1b609d4":"# show results for each iteration\ncvres = rf_cv_2.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)","5090838a":"# predict using training data\nrf_pred_4 = rf_clf_3.predict(X_tr_testing_2)","ef7a73b4":"# display evaluation scores\ndisplay_evaluation(y_train, rf_pred_4)","c9e37f14":"# dummy classifier\n# classifies every instance as not canceled\n# BaseEstimator allows to set and get estimator parameters\nclass NeverCanceledClassifier(BaseEstimator):\n\n    def fit(self, X, y=None):\n        pass\n\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=int)","5d69d7e1":"# instantiate dummy classifier\nnever_canceled = NeverCanceledClassifier()","26c6d4dc":"# fit the training set\nnever_canceled.fit(X_train, y_train)","60643397":"# predict using dummy classifier\nnever_canceled_pred = never_canceled.predict(X_train)","86589354":"# evaluate scores for comparison\n# can't assess using F1 score\n# precision will divide by zero\nprint(\"Accuracy:\", metrics.accuracy_score(y_train, never_canceled_pred))\nprint(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_train, never_canceled_pred))","1f151bc7":"# use predictions to get precision recall curve values\nrf_scores = rf_cv_2.best_estimator_.predict_proba(X_tr_testing_2)[:, 1]\nprecisions, recalls, thresholds = metrics.precision_recall_curve(y_train, rf_scores)","9e391172":"# plot precision recall curve\nplt.figure(figsize=(10,5))\nplt.plot(precisions, recalls, linewidth=3)\nplt.title(\"Precision Recall Curve for Hotel Cancellations\", size=20)\nplt.xlabel(\"Recall\", size=15)\nplt.ylabel(\"Precision\", size=15)\nplt.tight_layout()\nplt.show()","2979ec51":"# separate test set predictors and labels\nX_test = test_set.drop(\"is_canceled\", axis=1)\ny_test = test_set[\"is_canceled\"].copy()","54a7ec5e":"# transform test set\nX_test_prep = full_pipeline_2.transform(X_test)","1613676e":"final_model = rf_cv_2.best_estimator_\nfinal_model","0fdf9b52":"# predict test set\nfinal_predictions = final_model.predict(X_test_prep)","477c4231":"# evaluate predictions\ndisplay_evaluation(y_test, final_predictions)","6b508ab1":"> ### Splitting the Data:\n>> Before further analysis let's split the data into a training set and a testing set.\n>> This will ensure avoidance of bias that could occur from learning the data as a whole.","42a2eb33":"# 1. Getting the Data","250060cd":"> ### Observations:\n>> * Market segment cancellation rates are highest amongst travel agencies and tour operators.\n>> * Distribution channel cancellation rates are highest amongst groups, travel agencies and tour operators.\n>> * Customer type cancellation rates are highest amongst transient\n>> (meaning the booking is not part of a group or contract and is not associated to another transient booking).\n>> * The room type \"A\" is canceled most frequently.","98cc11cd":"> ### Removing the Following Columns:\n>> #### Numerical Attributes:\n>> * arrival_date_year: This category references towards certain years. This could be\n>> problematic for instances during years that do not appear in the training data, or\n>> perhaps have bias towards certain years specifically due to the unequal amounts of\n>> observations in the training data.\n>> * arrival_date_day_of_month: The column arrival date week of month generalizes this.\n>> * booking_changes: Could change over time, potentially causing data leakage.\n>> * days_in_waiting_list: Could constantly change over time. Additionally, there are many\n>> instances. This could prevent the model from generalizing.\n>> * agent & company: Represented by an ID. These columns are uninformative since they\n>> contain a substantial amount of various numerical values without having an actual\n>> numerical meaning. Since other columns (such as market segment) indicate the type of\n>> reservation, these columns won't be needed.\n>>\n>> #### Categorical Attributes:\n>> * country: There are many categories, most with few instances. In order to make a model\n>> that generalizes, it is better to dismiss this category.\n>> * assigned_room_type: Similar to reserved_room_type and seems like the reserved room is\n>> a more suitable choice.\n>> * reservation_status: Major data leakage! The categories are Check-Out, Canceled and No-Show.\n>> This is exactly what we are trying to predict.\n>> * reservation_status_date: This is the date when the reservation status was last changed,\n>> and therefore is irrelevant.","60e51169":"> ### Cancellations According to Required Car Parking Spaces","9c63f522":"> ### Observations:\n>> * While a large amount of bookings with no changes were canceled, this category can change overtime\n>> which could possibly be a source of leakage.","bb6e7331":"> ### What is the Random Forest Classification Model?\n\nForests are based on multiple decision trees, so it is vital to first understand how decision\ntrees work.\n\nA decision tree is a non-linear model built by constructing many linear boundaries.\nThe tree works as a sequence of yes or no, true or false questions that progress down\nthe tree until reaching a predicted class. The data is split into nodes based on\nfeature values. This model is good for occasions when there is no single linear line that\ncan divide the data. Gini Impurity of a node represents the probability that a randomly chosen\nsample would be incorrectly classified, so the goal is to reduce this as much as possible.\n\nUsing a single decision tree could cause overfitting of the training data. For example,\na decision tree could create a leaf node (the predicted class) for each instance.\nUsing a forest could help generalize better to new data. The random forest model\nsamples random points and subsets of features when training. Then, the predictions are made\nby averaging the predictions made by each decision tree.","a8478e95":"<h1 style=\"text-align: center;\">EDA of Hotel Bookings and ML to Predict Cancellations<\/h1>","83f12c35":"#### Model 2: KNN","9156f819":"> ### Cancellations According to Country of Origin","fa09a6cb":"> ### Cancellations According to Meal Booked","b806b06b":"#### Model 5: Random Forest Classifier","5d2e52d2":"# 4. Training and Evaluating Models","c4812e50":"> #### Observations:\n>> * The non refund values and graph look a bit off. It almost seems that the values\n>> for canceled and not canceled were accidentally switched!\n>> In light of this, it might be better to evaluate the model both with and without this\n>> feature.","0a55d971":"> ### Numerical Attributes:","166a7881":"> ### Observations:\n>> * Nearly half of the bookings without special requests are canceled.\n>> * There are fewer cancellations when the number of special requests increases.","c52b43e2":"> ### Observations:\n>> * There are 175 unique countries. This indicates that the data is representative\n>> worldwide, contrary to a specific region.\n>> * More than half of the instances have 50 or fewer observations in the DataFrame.\n>> * A model would likely generalize better if we avoid using this column.","9db2dba4":"> ### Observations:\n>> * The BB (Bed & Breakfast) meal is most common. It is also most frequently canceled.","6a85fc75":"> ### Cancellations According to Lead Time","d626a1c9":"> ### Missing Features:","78932514":"# 2. Understanding and Visualizing the Data\n> ##### *The motivation for this section is to gain more insights.*","c139e105":"> Initial observations from the histograms:\n>> 1. Some weeks have more bookings. This could be because of holiday or summer seasons, when people tend to travel more.\n>> 2. According to the lead_time plot, most bookings were made shortly before arrival.\n>> 3. Bookings tend to be without children or babies.\n>> 4. It seems that the most accommodations are two weeks long or shorter.\n>> 5. While most bookings were not canceled, there are thousands of instances that were.","0863e918":"> ### Cancellations According to Total of Special Requests","830921b1":"> Deposit type is pretty high on the list which raises speculation. As seen earlier,\n> the cancellation rate was nearly 100% in the category Non Refund.\n> Lets train a model without this feature.\n>\n> Additionally, lets train a model without the parameters that have less\n> than 0.005 feature importance. If the training error is nearly the\n> same when using fewer features, it might be more efficient to\n> train a model without them.","757ee223":"> Accuracy is less relevant for an imbalanced classification problem.\n> Evaluating by a metric that represents the data better is important.\n>\n> Chosen evaluation metric:\n>\n> The F1 Score is calculated by using precision (the accuracy of the positive predictions) and\n> recall (the ratio of positive instances correctly classified) accuracy.\n> This metric gives a higher value towards false positives rather than false negatives.","98ff4725":"#### Random Search Cross Validation 2","5369d313":"#### Model 1: KNN","21cf42a0":"> ### Observations:\n>> The percentages show that when there are more previous cancellations, there is\n>> a substantially higher chance the customer will cancel again.","5ca0170f":"#### Model 4: Random Forest Classifier","87056e78":"> #### Resources:\n> 1. Hotel Booking Demand Dataset <a href=\"https:\/\/www.kaggle.com\/jessemostipak\/hotel-booking-demand\"\n> title=\"Kaggle\">link<\/a>\n> 2. Hotel Booking Demand Article <a href=\"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340918315191\"\n> title=\"Article\">link<\/a>\n> 3. Average Daily Rate Article <a href=\"https:\/\/www.investopedia.com\/terms\/a\/average-daily-rate.asp\"\n> title=\"Investopedia\">link<\/a>\n> 4. Random Forest Article <a href=\"https:\/\/towardsdatascience.com\/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76\" title=\"towardsdatascience\">link<\/a>","3639df25":"> #### Observations:\n>> * Most bookings occur about 5 days prior to arrival.\n>> * When the lead time is larger the chances for cancellation increase.\n>> * The amount of bookings is steady overall between 20-100 days, then drops.","7e6be58d":"> # Objective\n> ## Predicting if a booking will be canceled.\n>> ### Chosen Feature:\n>> #### *is_canceled* column\n>>> 0 means the booking was not canceled\n>>>\n>>> 1 means the booking was canceled\n>> ### Motive:\n>> Like any business, hotels are also looking to gain profit. A model that predicts if the booking\n>> is likely to be canceled could be a good indication for hotels, as they\n>> may prefer to accept the lower risk bookings first.","10716dc4":"#### Random Search Cross Validation 1","976d572a":"### Any feedback, suggestions, questions? Leave a comment below!\n### Upvote if you liked this notebook, learned something new or found it useful!","74c687b8":"> ### Observations:\n>> * There are more instances for City Hotel than Resort Hotel in the data.\n>> * City Hotel has a higher cancellation rate according to arrival months.","bf36542d":"#### Model 3: Random Forest Classifier","5643b168":"> ### Observations:\n>> * Dividing the instances into groups according to cancellations shows that canceled\n>> bookings were only ones without required parking spaces.\n>> * This could potentially be a bad indication for cancellations. The model could learn\n>> that a booking can be canceled **only** if no parking spaces were required, which does not\n>> necessarily have to be the case.","1255441d":"# 3. Data Cleaning","d0acf775":"> ### Correlations with is_canceled Attribute - Overview:\n>> The strongest positive correlations (0.1 or more) are:\n>> * lead_time\n>> * previous_cancellations\n>>\n>> The strongest negative correlations (-0.1 or less) are:\n>> * total_of_special_requests\n>> * required_car_parking_spaces\n>> * booking_changes\n>>\n> The attribute combinations tested (guests stayed and nights stayed) both had weak correlations.","13859cf2":"> ### Cancellations According to Market Segment, Distribution Channel, Customer Type and Room Type","0dec9370":"> ### Cancellations According to Previous Cancellations","be30c1ad":"# 5. Evaluating the Test Set","8eed9bb0":"> ### Features in the DataFrame:\n>> 1. hotel: Resort Hotel or City Hotel\n>> 2. is_canceled: Value indicating if the booking was canceled (1) or not (0)\n>> 3. lead_time: Number of days between the booking date to the arrival date\n>> 4. arrival_date_year: Year of arrival\n>> 5. arrival_date_month: Month of arrival\n>> 6. arrival_date_week_number: Week number according to year of arrival\n>> 7. arrival_date_day_of_month: Day of arrival\n>> 8. stays_in_weekend_nights: Number of weekend nights booked (Saturday or Sunday)\n>> 9. stays_in_week_nights: Number of week nights booked (Monday to Friday)\n>> 10. adults: Number of adults\n>> 11. children: Number of children\n>> 12. babies: Number of babies\n>> 13. meal: Type of meal booked\n>> 14. country: Country of origin\n>> 15. market_segment: Market segment designation, typically influences the price sensitivity\n>> 16. distribution_channel: Booking distribution channel, refers to how the booking was made\n>> 17. is_repeated_guest: Value indication if the booking was from a repeated guest (1) or not (0)\n>> 18. previous_cancellations: Number of previous cancellations prior to current booking\n>> 19. previous_bookings_not_canceled: Number of previous booking not canceled prior to current booking\n>> 20. reserved_room_type: Code of room type reserved\n>> 21. assigned_room_type: Code for the type of room assigned to the booking\n>> 22. booking_changes: Number of changes made to the booking since entering the hotel management system\n>> 23. deposit_type: Type of deposit made for the reservation\n>> 24. agent: ID of the travel agency that made the booking\n>> 25. company: ID of the company\/organization that made the booking or is responsible for payment\n>> 26. days_in_waiting_list: Number of days booking was in the waiting list until it was confirmed\n>> 27. customer_type: Type of booking\n>> 28. adr: Average Daily Rate (the sum of transactions divided by the number of nights stayed)\n>> 29. required_car_parking_spaces: Number of car parking spaces requested\n>> 30. total_of_special_requests: Number of special requests made by the customer\n>> 31. reservation_status: Last reservation status (Canceled, Check-Out, No-Show)\n>> 32. reservation_status_date: Date at which the last status was set\n>>\n>>> ##### *Understanding the features could help gain insight on how to treat null values.*","2057dad5":"> ### Overview:\n>> ####  Removing nearly half of the features did not drastically change the score.\n>> * The model prior to feature selection had an F1 score of 0.99.\n>> * The model after feature selection has an F1 score of 0.98.\n>> * The model performs better than the dummy classifier.","afd140ab":"> ### Cancellations According to Booking Changes","eeec41e1":"> So far, the performance of the KNN model using distance weights instead of uniform\n> drastically improved the results.","40b6d080":"> ### Cancellations According to Hotels and Arrival Month","56a5fc2e":"#### Feature Importance","70521aa2":"> ### Understanding the ADR Feature\n>> Since this feature is not entirely clear from the description on Kaggle,\n>> I've decided to further assess it.\n>>\n>> The Average Daily Rate (ADR) is typically calculated by taking the average revenue\n>> earned from the rooms and dividing by the number of rooms sold (excluding rooms occupied\n>> by staff).\n>>\n>> Since it is not clear if an ADR of zero indicates that the booking was canceled or\n>> if the hotel did not gain profit, I will look at instances listed with an ADR\n>> of zero. This should provide enough insight to see if this feature should be removed\n>> during before model evaluations.","40c68cf8":"#### Dummy Classifier\n> The dummy classifier serves as an indication and comparison for model performance.","322713c3":"> ### Observations:\n>> * Most bookings are labeled as checked-out and not canceled when the ADR was zero.\n>> This concludes the previous speculation.","b318be03":"> ### Categorical Attributes:","a3b725c3":"> ### Cancellations According to Deposit Type","a736f291":"> In a first observation it is clear that some features have\n> missing values (i.e. \"company\" and \"agent\" columns).\n> We will need to take care of this later.","5f0d3657":"> The Random Forest Classification model performed slightly better than the second\n> KNN model. The next step is to find the hyperparameters\n> that provide the best results.\n>\n> Since the dataset is rather large, and it would take a long time\n> to run all estimators, using randomized search cv is the ideal\n> option. The size of the dataset is also the reason I have neglected\n> randomized search for the KNN model.\n>\n> The randomized search runs an amount of iterations specified\n> and tries random combinations of the attributes listed."}}