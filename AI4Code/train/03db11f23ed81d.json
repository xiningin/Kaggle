{"cell_type":{"f54b4fbd":"code","a2411e0e":"code","d3f8ff00":"code","1f7ce217":"code","6f15139b":"code","107db903":"code","818c05e4":"code","5dfcc843":"code","be107819":"code","f2d078fd":"code","a7cce805":"code","52a6f3b7":"code","6479f7c2":"code","e0408c40":"code","4f0feecf":"code","f12cbca1":"code","bc201042":"code","3ec64980":"code","f81facfa":"code","ba79bfa5":"code","5ed9f3bc":"code","f0bbb657":"code","c2bbef08":"code","322d4e14":"code","4c4abcf3":"code","b7ca70ab":"code","2b2fbee0":"code","38b4c154":"code","b3eeac81":"code","ff5ad2a7":"code","cb7fb10f":"code","167006b1":"code","a99c5302":"code","4474528b":"code","482c10e9":"code","43b54117":"code","5b941568":"code","ccb67f3b":"code","4324ed7c":"code","4855c1ae":"code","596a4a55":"markdown","95ab364f":"markdown","1d2f6118":"markdown","4063a896":"markdown","0f08a777":"markdown","4d5f8b4c":"markdown","0280df59":"markdown","13fd6da7":"markdown","7cebcaff":"markdown","a75cf578":"markdown","42685810":"markdown","90ef5b1d":"markdown","ca669651":"markdown","398ad75b":"markdown","db6dd8b2":"markdown","3c2fe5a0":"markdown"},"source":{"f54b4fbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2411e0e":"import sqlite3\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm #Process bar\ntqdm.pandas()","d3f8ff00":"con = sqlite3.connect('\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite') # connecting to database\n\nraw_reviews = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3 LIMIT 6000\"\"\", con) # Loading only 6000 rows for fast computing","1f7ce217":"raw_reviews.head()","6f15139b":"## Converting Score to positive (for Score 5 or 4) and negitive (for Score 1 or 2)\n\nreviews = raw_reviews\nreviews['Score'] = reviews['Score'].apply(lambda x : 'positive' if x>3 else 'negitive')","107db903":"reviews.head()","818c05e4":"reviews.shape","5dfcc843":"# removing Duplicated\n# Some rows are duplicated as amazone use the same review for the different varient of the same product\n\nreviews.drop_duplicates(subset={'UserId', 'ProfileName', 'Text', 'Summary', 'Score', 'Time'}, inplace=True)","be107819":"reviews.shape","f2d078fd":"reviews.info()","a7cce805":"reviews['Score'].value_counts()","52a6f3b7":"# Histogram\n\nsns.FacetGrid(reviews, height=5).map(plt.hist, 'Score').add_legend()","6479f7c2":"# Removing Garbage\nimport re\ndef rem(sen):\n    sen = re.sub(r'<.*?>',' ',sen)#remove HTLM tags\n    sen = re.sub(r'^https?:\\\/\\\/.*[\\r\\n\\s]', ' ', sen)#remove links\n    \n    ##Changing decontracted words\n    sen = re.sub(r\"n\\'t\", \" not\", sen)\n    sen = re.sub(r\"\\'re\", \" are\", sen)\n    sen = re.sub(r\"\\'s\", \" is\", sen)\n    sen = re.sub(r\"\\'d\", \" would\", sen)\n    sen = re.sub(r\"\\'ll\", \" will\", sen)\n    sen = re.sub(r\"\\'t\", \" not\", sen)\n    sen = re.sub(r\"\\'ve\", \" have\", sen)\n    sen = re.sub(r\"\\'m\", \" am\", sen)\n    \n    sen = re.sub(r'\\W',' ',sen)#remove special characters\n\n    #### Removing alpha numeric words and non english words and len<=2 words\n    \n    new_sen = \"\"\n    for word in sen.split():\n        if word.isalpha():\n            if len(word)>2:\n                new_sen += word + \" \"\n            else:\n                continue\n        else:\n            continue\n            \n    return new_sen.lower() #Converting to Lowercase","e0408c40":"# Testing our rem function\nprint(reviews['Text'][497])\nprint()\nprint()\nprint(rem(reviews['Text'][497]))","4f0feecf":"# performing rem() function on Text and Summary Coloumns\nreviews['Text'] = reviews['Text'].progress_apply(rem)\nreviews['Summary'] = reviews['Summary'].progress_apply(rem)","f12cbca1":"reviews.iloc[2]","bc201042":"from nltk.corpus import stopwords\n\n#We are removing not, nor no from stopwords\n\ns_words = set(stopwords.words('english'))\nprint('length of stopwordes before removing is {}'.format(len(s_words)))\n\ns_words = s_words.difference({'no','not','nor'})\nprint('length of stopwordes after removing is {}'.format(len(s_words)))\n","3ec64980":"def rem_swords(s, s_words = s_words):\n    new_s = \"\"\n    for word in s.split():\n        if word not in s_words:\n            new_s += word + \" \"\n        else:\n            continue\n    return new_s\n\n##Testing function\nprint(reviews['Text'][5])\nprint();print()\nprint(rem_swords(reviews['Text'][5]))","f81facfa":"#Removing StopWords from Text and Summary\n\nreviews['Text'] = reviews['Text'].progress_apply(rem_swords)\nreviews['Summary'] = reviews['Summary'].progress_apply(rem_swords)","ba79bfa5":"reviews['Text'][4]","5ed9f3bc":"from nltk.stem import SnowballStemmer","f0bbb657":"stemmer = SnowballStemmer('english')\n\ndef stemming(sen, stemmer = stemmer):\n    new_s = \"\"\n    for word in sen.split():\n        new_s += stemmer.stem(word) + \" \"\n    return new_s","c2bbef08":"#Testing stemming()\n\nprint(reviews['Text'][5])\nprint(\"=> length before stemming {}\".format(len(reviews['Text'][5].split())))\nprint()\nprint()\nprint(stemming(reviews['Text'][5]))\nprint(\"=> length after stemming {}\".format(len(stemming(reviews['Text'][5]).split())))","322d4e14":"#Stemming Text and Summary Coloumns\nreviews['Text'] = reviews['Text'].progress_apply(stemming)\nreviews['Summary'] = reviews['Summary'].progress_apply(stemming)","4c4abcf3":"#This will be used later for vectorization, It will be a list of all document is corpus\npreprocess_text = []\nfor sen in tqdm(reviews['Text'].values):\n    preprocess_text.append(sen.strip())\npreprocess_summary  = []\nfor sen in tqdm(reviews['Summary'].values):\n    preprocess_summary.append(sen.strip())\n\n","b7ca70ab":"from sklearn.feature_extraction.text import CountVectorizer #Class for generating Bag of words Vector","2b2fbee0":"text_vectorizor = CountVectorizer()\nsummary_vectorizor = CountVectorizer()\n\n# Creating Bag of Word vector for Text\ntext_sparse_matrix = text_vectorizor.fit_transform(preprocess_text)\n\n# Creating Bag of Word vector for Summary\nsummary_sparse_matrix = summary_vectorizor.fit_transform(preprocess_summary)","38b4c154":"print(\"Text vector\")\nprint(text_vectorizor.get_feature_names()[:10]) #Some features of text Vector\nprint(\"Summary vector\")\nprint(summary_vectorizor.get_feature_names()[:10]) #Some features of Summary vector","b3eeac81":"text_vector = text_sparse_matrix.toarray() #Converting text sparse matrix to numpy array\n\nsummary_vector = summary_sparse_matrix.toarray() # Converting summary sparse matrix to numpy array\n\nprint(\"Shape of text vector is {}\".format(text_vector.shape))\nprint(\"Shape of summary vector is {}\".format(summary_vector.shape))","ff5ad2a7":"from sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler","cb7fb10f":"# Standarization of Vector\nstd_text = StandardScaler()\nstandarized_text = std_text.fit_transform(text_vector)\nprint(\"shape of text_vector is {} and shape of standarized text is {}\".format(text_vector.shape,standarized_text.shape))\n\nstd_summary = StandardScaler()\nstandarized_summary = std_summary.fit_transform(summary_vector)\nprint(\"shape of summary_vector is {} and shape of standarized summary is {}\".format(summary_vector.shape,standarized_summary.shape))","167006b1":"from sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","a99c5302":"#splitting training and test data 7:3\nX_train, X_test, y_train, y_test = train_test_split(standarized_text, reviews['Score'], test_size=0.3, random_state=21)\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","4474528b":"#3 fold cross validation on train data to find the best K\nneig = [i for i in range(1,len(X_train),2)]\ncv_score = []\nfor i in neig:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    score = cross_val_score(knn,X_train, y_train, cv=3, scoring='accuracy',n_jobs=-1)\n    print(score)\n    print(i)\n    cv_score.append(score.mean())","482c10e9":"cv_score.index(max(cv_score))","43b54117":"#Best K = 7\n# Let's see accuracy on test data\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train,y_train)\n\n#predicting test data\nX_test_pred = knn.predict(X_test)\nacc = accuracy_score(y_test ,X_test_pred, normalize=True)*float(100)\nprint(\"=> Accuracy on test data is: \",acc)","5b941568":"#splitting training and test data 7:3\nX_train, X_test, y_train, y_test = train_test_split(standarized_summary, reviews['Score'], test_size=0.3, random_state=21)\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","ccb67f3b":"#3 fold cross validation on train data to find the best K\nneig = [i for i in range(1,22,2)]\ncv_score = []\nfor i in neig:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    score = cross_val_score(knn,X_train, y_train, cv=3, scoring='accuracy',n_jobs=-1)\n    print(score)\n    print(i)\n    cv_score.append(score.mean())","4324ed7c":"print(cv_score.index(max(cv_score)),max(cv_score)*100)","4855c1ae":"#Best K = 13\n# Let's see accuracy on test data\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier(n_neighbors=13)\nknn.fit(X_train,y_train)\n\n#predicting test data\nX_test_pred = knn.predict(X_test)\nacc = accuracy_score(y_test ,X_test_pred, normalize=True)*float(100)\nprint(\"=> Accuracy on test data is: \",acc)","596a4a55":"#### Standarizing Bag Of Words","95ab364f":"> **KNN on Bag of words**","1d2f6118":"### 2.1 Bag of Words","4063a896":"### **Stemming** ( Snowball Stemming )","0f08a777":"## 1.1 Data deduplication","4d5f8b4c":"## Loading DataSet","0280df59":"## 2. Data Featurization","13fd6da7":"## 1.2. Data Preprocessing\n1. Removing Html tags fro Text and Summary\n2. Removing Special Characters\n3. Removing Numeric Characters\n4. Removing words lengeth of 2 or less\n5. Removing Links from Document\n6. Check all the Words are English and not alpha Numeric\n7. Finally Removing StopWords\n8. Stemming (Snowball Stemming)","7cebcaff":"- **Tasks Done**\n\n    1. Removing Html tags fro Text and Summary&#x2714;\n    2. Removing Special Characters&#x2714;\n    3. Removing Numeric Characters&#x2714;\n    4. Removing words lengeth of 2 or less&#x2714;\n    5. Removing Links from Document&#x2714;\n    6. Check all the Words are English and not alpha Numeric&#x2714;\n    7. Finally Removing StopWords&#x2718;\n    8. Stemming (Snowball Stemming)&#x2718;","a75cf578":"# 1. EDA","42685810":"### **Removing stopwords**","90ef5b1d":"### Preprocessing\n- Tasks Performed\n\n    1. Removing Html tags fro Text and Summary&#x2714;\n    2. Removing Special Characters&#x2714;\n    3. Removing Numeric Characters&#x2714;\n    4. Removing words lengeth of 2 or less&#x2714;\n    5. Removing Links from Document&#x2714;\n    6. Check all the Words are English and not alpha Numeric&#x2714;\n    7. Finally Removing StopWords&#x2714;\n    8. Stemming (Snowball Stemming)&#x2714;","ca669651":"**KNN for summary**","398ad75b":"# Amazon Fine Food reviews rating prediction\n\n- **Objective**\n        Predict the Rating( Positive or negitive) based on the review","db6dd8b2":"> DataSet is not Balanced as 5028 are **Positive** Reviews and 972 are **Negitive** Reviews","3c2fe5a0":"**KNN for text**"}}