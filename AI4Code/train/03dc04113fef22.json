{"cell_type":{"a91a43d4":"code","a07723de":"code","b692ac8e":"code","8621586f":"code","4f11fcb1":"code","07180d14":"code","b0f98178":"code","0ada1da0":"code","dd686168":"code","babed6ab":"code","96e83962":"code","4a6d8d72":"code","5479f77e":"code","8e3d74b5":"code","d17bf6a2":"code","061db290":"code","1f646b49":"code","8cc46ebe":"code","a1d656ea":"code","eabd29d5":"code","0afd68c6":"code","20a765af":"code","b3f8ddd7":"code","4212780c":"code","566d6345":"code","f15cd991":"code","9a921af0":"code","094500d5":"code","77bcf73d":"code","b2ed2edf":"code","3e1712b9":"code","dee4ff97":"code","fa324086":"code","1e642d45":"code","adc8e431":"code","ecb5a134":"code","954c79d5":"code","78b79d55":"code","457d09dc":"code","35cb5550":"code","2af09c20":"code","ec5ed8cb":"code","94a837c9":"code","3f76eaf8":"code","a737be4b":"code","e6a51af4":"code","a0609fca":"code","1747fbcc":"code","958f08e4":"code","bebec94c":"code","6e0675d0":"code","0a922a8f":"markdown","37a21c6c":"markdown","def5f9bb":"markdown","ede2e572":"markdown","5f236d0f":"markdown","52c6d7cb":"markdown","4fce941a":"markdown","e73c5d9c":"markdown","5668942b":"markdown","49b1bd04":"markdown","b3c51248":"markdown","a105b0b1":"markdown","f46f1e44":"markdown","a25dffde":"markdown","fabbebcb":"markdown","20225466":"markdown","721bfa37":"markdown","60bbe98f":"markdown","b64c69db":"markdown","e0733b1d":"markdown","fb7e2e13":"markdown","8eb7e7eb":"markdown","912beb64":"markdown","af3f0f8b":"markdown","771eecfb":"markdown","b4e6ee99":"markdown","a0b18cc1":"markdown","4334cfd5":"markdown","4b612c9f":"markdown","6b55e3d9":"markdown","779483f7":"markdown","a594c08c":"markdown","fe7cc817":"markdown","ea9cd6fe":"markdown","cfe07f9e":"markdown","b20a3510":"markdown"},"source":{"a91a43d4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a07723de":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","b692ac8e":"%config Completer.use_jedi = False","8621586f":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","4f11fcb1":"display(train.head(5))\nprint(train.shape)\nprint(train.columns.values)","07180d14":"train.info()","b0f98178":"train.describe(include=['object'])","0ada1da0":"plt.figure(figsize=(10, 5))\ntrain['Survived'].value_counts().plot(kind='barh')\nplt.title('Target Variable')\nplt.show()","dd686168":"plt, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nfor ax, i in zip(axes.flatten(), range(0, 2)):\n    sns.distplot(train[train['Survived'] == i]['Age'].dropna(), ax=ax, axlabel='Age of Survived ' + str(i))","babed6ab":"sns.barplot(x='Pclass', y='Survived', data=train)\nplt.show()","96e83962":"train.head(5)","4a6d8d72":"print('Data size before deletion: {}'.format(train.shape))\n\ntrain.drop(columns=['Ticket', 'Cabin'], inplace=True)\ntest.drop(columns=['Ticket', 'Cabin'], inplace=True)\n\nprint('Data size after deletion: {}'.format(train.shape))","5479f77e":"train['Name'].head(5)","8e3d74b5":"train['Name'].str.extract('([A-Za-z]+)\\.', expand=False).unique()","d17bf6a2":"train['Title'] = train['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntest['Title'] = test['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","061db290":"train['Title'].value_counts()","1f646b49":"for dataset in [train, test]:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","8cc46ebe":"train['Title'].value_counts()","a1d656ea":"train[['Title', 'Survived']].groupby(['Title'], as_index=False) \\\n                            .mean() \\\n                            .sort_values(by='Survived', ascending=False)","eabd29d5":"for dataset in [train, test]:\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}).fillna(0)\n    \ntrain.head(5)","0afd68c6":"print('Data size before deletion: {}'.format(train.shape))\n\nfor dataset in [train, test]:\n    dataset.drop(columns=['Name'], inplace=True) #, 'PassengerId'\n\nprint('Data size after deletion: {}'.format(train.shape))","20a765af":"train.head(5)","b3f8ddd7":"# we need to code Sex column in numbers\nfor dataset in [train, test]:\n    dataset['Sex'] = dataset['Sex'].map({'male': 1, 'female': 0})","4212780c":"train.head()","566d6345":"np.sum(train['Age'].isnull())","f15cd991":"for dataset in [train, test]:\n    dataset['Age'] = dataset['Age'].fillna(np.median(dataset['Age'].median()))","9a921af0":"train.head(5)","094500d5":"np.sum(train['Age'].isnull())","77bcf73d":"for dataset in [train, test]:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","b2ed2edf":"train.head(5)","3e1712b9":"for dataset in [train, test]:\n    dataset['Alone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'Alone'] = 1","dee4ff97":"train.head(5)","fa324086":"train = train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest = test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)","1e642d45":"print('Data size after deletion: {}'.format(train.shape))","adc8e431":"train.head()","ecb5a134":"train['Embarked'].value_counts()","954c79d5":"for dataset in [train, test]:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","78b79d55":"for dataset in [train, test]:\n    dataset['Embarked'] = dataset['Embarked'].map({'S' : 0,\n                                                   'C' : 1,\n                                                   'Q' : 2}) \\\n                                             .astype('int')","457d09dc":"train.head()","35cb5550":"sns.heatmap(train.corr(), annot=True)\nplt.show()","2af09c20":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","ec5ed8cb":"y = train['Survived']\nX = train.drop(columns=['Survived'])\nX_test = test","94a837c9":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)","3f76eaf8":"print('Training sample:', X_train.shape)\nprint('Validation sample:', X_val.shape)","a737be4b":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\npred = logreg.predict(X_val)\naccuracy_score(y_val, pred)","e6a51af4":"from sklearn.tree import DecisionTreeClassifier","a0609fca":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\n\npred = clf.predict(X_val)\naccuracy_score(y_val, pred)","1747fbcc":"from sklearn.ensemble import RandomForestClassifier","958f08e4":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\npred = rf.predict(X_val)\naccuracy_score(y_val, pred)","bebec94c":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest['Survived'] = 0\ntest.loc[test['Sex'] == 'female','Survived'] = 1\ndata_to_submit = pd.DataFrame({\n    'PassengerId':test['PassengerId'],\n    'Survived':test['Survived']\n})","6e0675d0":"data_to_submit.to_csv('csv_to_submit.csv', index = False)\nprint('Saved file: ' + filename)","0a922a8f":"We've done all we can do with the **Name** and **PassengerId** features. Let's drop them:","37a21c6c":"We can see that infants have better survival rate as the people aged ~80 years: they have a 100% chance of survival. \nOur hypothesis is confirmed.","def5f9bb":"Now I want to turn my attention to the **Embarked** feature.","ede2e572":"Well, we have 12 features, some of which are very useful and affect survivability. \nFor instance, **Age** - obviously a child will have priority for a place in the boat. \nAlso, status of the person's cabin, because people in the first class will have more chances to survive.\n\nWe will definitely check the correlation of all features with the target variable, just a little later.\n\nNote that not all attributes are numbers. We have words, categorical variables etc. Let's take a look at the data types:","5f236d0f":"Pandas allows you to read any csv file in one line, then we can do whatever we want.","52c6d7cb":"Let's get started. First, we have to import the libraries and look at the data:","4fce941a":"Let's create a new feature: **FamilySize** based on **SibSp** and **Parch**:","e73c5d9c":"We have to verify the result of the model somehow, so we do a delayed sampling and make a validation on it.\n\nThis is the simplest way, but you can read about other validation methods here:\n\nhttps:\/\/towardsdatascience.com\/supervised-machine-learning-model-validation-a-step-by-step-approach-771109ae0253\n\nhttps:\/\/www.geeksforgeeks.org\/cross-validation-machine-learning\/","5668942b":"## 2. Data Cleaning & Feature Engineering","49b1bd04":"## 3. Modelling.","b3c51248":"And a feature for those who have travelled without family \u2013 **Alone**","a105b0b1":"The same approach we used for the previous categorical features:","f46f1e44":"There are a lot of null valies in **'Age'**, we must fix it:","a25dffde":"## 1. Data Observation.","fabbebcb":"1. Logistic Regression","20225466":"3. Random Forest","721bfa37":"Let's create a separate feature thereof:","60bbe98f":"Let's deal with categorical variable \u2013 **Sex**. 1 will be male, 0 \u2013 female. ","b64c69db":"That's better.","e0733b1d":"Good job! Now let's look at the correlations for all features:","fb7e2e13":"After that, we can delete useless columns:","8eb7e7eb":"## 4. Submission","912beb64":"2. Decision tree","af3f0f8b":"Good!","771eecfb":"By this table we can draw some conclusion, e.g. \n* Every person's name on the Titanic is unique, \n* The Sex trait takes two values, male and female. \n* Staterooms are not unique because many people were in groups and shared one cabin with several of them.\n\nAs you can see, we haven't done anything yet, but despite that fact, we know about our data a lot! :D It's a very important skill in Data Science: trying to draw preliminary conclusions from the data.\n\nLet's visualize out Target Variable:","b4e6ee99":"##### I decided to use the following algorithms:\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest","a0b18cc1":"Number of values is too high. I implement the For Loop below: \n\nThose titles that are rare will be replaced by **'Rare'** value, whereas the rest will be called by common names. ","4334cfd5":"The **Name** feature seems useless at first glance. But let's think how we can benefit from it. As you can see below, every **Name** contains titles like 'Mr', 'Mrs' etc.\n\nMaybe if there's a doctor (Dr.) on the boat, he might have a better chance of surviving?","4b612c9f":"Let's clean up the data a bit. \nIt's really important to remove attributes that don't provide information for the model at all. \nI mean **Ticket** and **Cabine** features (don't forget about \"inplace=True\", for proper removal).","6b55e3d9":"We need to code 'Title' in numbers, because putting words into a Machine Learning model isn't a good idea.\n\n\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5","779483f7":"**float64** & **int64** are numbers, whereas **object** is related with words or letters.\n\nFurthermore, we have another problem: **NaN values**. These are missing values in our dataset, and they can hurt out model a lot.\n\nNext, The **Survived** feature is our target. It takes the value 0 or 1, 0 for death, 1 for surviving, and we will predict it.","a594c08c":"Here I used median value (not 0, because it will break the results of the model). \n\nMoreover, I would like to point out that there are many ways of dealing with the 0 value, but in this kernel I've chosen the clearest and easiest =)","fe7cc817":"Well, without too much effort, we:\n1. Processed the data, \n2. Created a couple of our own features, \n3. Visualized them \n4. Trained 3 machine learning models. \n\nSure, it's not the best score you can get, but this notebook give you basic knowledge about processes in Kaggle competition.\n\nWe can improve this result. For example, deal with missing data individually for each feature rather than in a cycle, create new 'features' based on existing ones, or even find parameters that will increase the score of the model in the leaderboard, when we try another models.\n\nFinding the best solution at Kaggle competitions is a whole art that can be mastered by combining different techniques with non-traditional methods. Good luck!\n\n## <font color=\"green\">Stay tuned and don't forget to <b>UPVOTE<\/b> this kernel =)<\/font>","ea9cd6fe":"Being in **first** class gives you a better chance of surviving than in **third** one.","cfe07f9e":"## Welcome to my Titanic EDA for Beginners!\nThis kernel covers simple Data Exploration & Visualization, Feature Engineering.\n\nThe main goal of this notebooks is not to show you the best score, but to give a clear explanation of the things that you need in [Titanic](https:\/\/www.kaggle.com\/c\/titanic) competition.\n\n## <font color=\"green\">If this notebook were useful for you, please <b>UPVOTE<\/b> it =)<\/font>","b20a3510":"Let's check the survival rate of each group:"}}