{"cell_type":{"a6a547aa":"code","1a1b8ffc":"code","e935ee32":"code","2cd6dbdf":"code","e78a1a52":"code","7e916da4":"code","0bac173d":"code","079d168a":"code","61837bf0":"code","e9f11c71":"code","b594270c":"code","e14326a1":"code","f06b7e9d":"code","c3a15401":"code","ddede735":"code","9d81e522":"code","1c780995":"code","9fe6853b":"code","d061898e":"code","ecad2b2f":"code","9209ca42":"code","52fcc822":"code","b722a41a":"code","b1f05e81":"code","ed5b7419":"code","c8844fcd":"code","21ed762a":"code","7f0e5360":"code","db03db69":"code","f5e3bc3c":"code","e6a1e500":"code","d23eaba7":"code","8e241e3a":"markdown","064daaee":"markdown","54ce3ce6":"markdown","a21f158b":"markdown","2b2929cf":"markdown","569cb4ff":"markdown","822f38b5":"markdown","201fc013":"markdown","f9ac8acc":"markdown","75a08a44":"markdown","d3e1ae19":"markdown","42612003":"markdown","059f0419":"markdown","36f4c5c5":"markdown","4f5c260e":"markdown","326c5702":"markdown","5166d6f7":"markdown","05b8d40d":"markdown","a81ff3fe":"markdown","4d5392a7":"markdown","8b9ac38d":"markdown","ecd2a122":"markdown","c8ea7daa":"markdown"},"source":{"a6a547aa":"import pandas as pd\nimport numpy as np","1a1b8ffc":"df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\", header=0, index_col=0)\ndf.head(10)","e935ee32":"# Get the feature and target columns\nfeature_cols = df.columns[:-1]\ntarget_cols = df.columns[-1]\n\n# Get the data as a numpy matrix\nfeatures = df[feature_cols].to_numpy(dtype = np.float32)\ntarget = df[target_cols].to_numpy(dtype = np.float32)\nprint(f\"Data shape: features -> {features.shape}, and Target -> {target.shape}\") ","2cd6dbdf":"from sklearn.preprocessing import StandardScaler","e78a1a52":"# Create the StandardScaler object\nscaler = StandardScaler()\n\n# Transform the features\nfeatures = scaler.fit_transform(features)","7e916da4":"from sklearn.model_selection import train_test_split","0bac173d":"X_train, X_val, Y_train, Y_val = train_test_split(features, target, train_size = 0.90, random_state = 42)\nprint(f\"Train data: {X_train.shape}, {Y_train.shape}, \\nValidation data: {X_val.shape}, {Y_val.shape}\")","079d168a":"import torch\nfrom torch.utils.data import Dataset\n","61837bf0":"class TabularDataset(Dataset):\n    def __init__(self, x, y, transform = None, target_transform = None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, idx):\n        feature = self.x[idx, :]\n        target = self.y[idx]\n        target = np.array([target], dtype = np.float32) \n\n        if self.transform:\n            feature = self.transform(feature)\n        if self.target_transform:\n            target = self.target_transform(target)\n        \n        return torch.from_numpy(feature), torch.from_numpy(target)\n    \n    def __len__(self):\n        return len(self.x)","e9f11c71":"Train_dataset = TabularDataset(X_train, Y_train)\nVal_dataset = TabularDataset(X_val, Y_val)","b594270c":"from torch.utils.data import DataLoader","e14326a1":"# Let's create the dataloader for train and test datasets\ntrain_dataloader = DataLoader(Train_dataset, batch_size = 128*5, shuffle = True)\nval_dataloader = DataLoader(Val_dataset, batch_size = 128*5, shuffle = True)","f06b7e9d":"# Let's see what can be done with these iterators\ndataitr = iter(train_dataloader)\nfeatures, labels = dataitr.next()\nprint(f\"Features: {features.shape} \\nLabels: {labels.shape}\")","c3a15401":"import torch.nn as nn","ddede735":"# Create Model subclass to define the network\nclass Model(nn.Module):\n    def __init__(self, in_features = 100):\n        super().__init__()\n\n        # Define possible layers configuration\n        self.fc1 = nn.Linear(in_features, 150)\n        self.fc2 = nn.Linear(150, 90)\n        self.fc3 = nn.Linear(90, 70)\n        self.fc4 = nn.Linear(70, 50)\n        self.fc5 = nn.Linear(50, 30)\n        self.fc6 = nn.Linear(30, 20)\n        self.fc7 = nn.Linear(20, 10)\n        self.fc8 = nn.Linear(10, 5)\n        self.fc9 = nn.Linear(5, 1)\n        \n        # Define activations, classifier layer, \n        # and if required then regularizations\n        self.activation = nn.SELU() # Activations\n        self.classifier = nn.Sigmoid() # Classifier\n        self.dropout = nn.Dropout(p=0.1) # Regularization\n    \n    def forward(self, x):\n        \"\"\"\n        Function implements the `forward` pass of a network.\n        While training this will run with gradient enabled, to backprop,\n        otherwise while testing this is used with torch.no_grad() to infer on the query.\n        \"\"\"\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        x = self.activation(self.fc3(x))\n        x = self.activation(self.fc4(x))\n        x = self.activation(self.fc5(x))\n        x = self.activation(self.fc6(x))\n        x = self.activation(self.fc7(x))\n        x = self.activation(self.fc8(x))\n        x = self.classifier(self.fc9(x))\n        \n        return x","9d81e522":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"{device} device will be used.\")","1c780995":"# Transfer the model parameters and properties to selected device\nmodel = Model().to(torch.device(device))","9fe6853b":"try:\n    from torchsummary import summary\nexcept:\n    print(\"Installing Torchsummary..........\")\n    ! pip install torchsummary\n    from torchsummary import summary","d061898e":"summary(model, (100,))","ecad2b2f":"def get_acc(acc_type = 'val'):\n    correct = 0\n    total = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        dl = val_dataloader if acc_type == 'val' else train_dataloader\n        for data in dl:\n            features, labels = data\n            features = features.to(device)\n            labels = labels.to(device)\n\n            # calculate outputs by running images through the network\n            outputs = model(features)\n\n            # the class with the highest energy is what we choose as prediction\n            pivot = torch.tensor([0.5]).to(device)\n            value = torch.tensor([0.0]).to(device)\n            predicted = torch.heaviside(outputs.data-pivot, value)\n            \n            # print(predicted, labels)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return 100*correct\/total\n","9209ca42":"def init_weights(layer):\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight.data)\n\nmodel.apply(init_weights)","52fcc822":"load_prev = False\nif load_prev:\n    model.load_state_dict(torch.load('.\/basemodel'))\n    print(model.eval())","b722a41a":"import torch.optim as optim\n\ncriterion = nn.BCELoss() # Loss function\nparams_list = model.parameters() # model parameters\n\n## We can apply custom learning rate or any other perameters to each layer, use the following:\n# params_list = [\n#     {'params': model.fc1.parameters(), 'lr': 0.01},\n#     {'params': model.fc1_1.parameters(), 'lr': 0.01},\n#     {'params': model.fc2.parameters(), 'lr': 0.005},\n#     {'params': model.fc2_2.parameters(), 'lr': 0.005},\n#     {'params': model.fc3.parameters(), 'lr': 0.001},\n#     {'params': model.fc3_3.parameters(), 'lr': 0.001},\n#     {'params': model.fc4.parameters(), 'lr': 0.005},\n#     {'params': model.fc4_4.parameters(), 'lr': 0.005},\n#     {'params': model.fc5.parameters(), 'lr': 0.001},\n#     {'params': model.fc5_5.parameters(), 'lr': 0.001},\n# ]\noptimizer = optim.AdamW(params_list, lr=0.0007, weight_decay=0.01) # Optimizer","b1f05e81":"for epoch in range(700):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.to(torch.device(device))\n        labels = labels.to(torch.device(device))\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 400 == 399:    # print every 400 mini-batches\n            print('[%d, %5d] loss: %.3f, val accuracy: %.3f' %\n                  (epoch + 1, i + 1, running_loss \/ 400, get_acc('val')))\n            running_loss = 0.0\n\nprint('Finished Training')","ed5b7419":"print(' Validation accuracy of the network: %f %%' % (\n    get_acc('val')))\nprint(' Train accuracy of the network: %f %%' % (\n    get_acc('train')))","c8844fcd":"torch.save(model.state_dict(), \".\/basemodel\")","21ed762a":"model.state_dict()","7f0e5360":"df_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\", header=0, index_col=0)\ndf_test.head(10)","db03db69":"features = scaler.transform(np.float32(df_test.values))\ntest_dataset = TabularDataset(features, np.ones((len(features),)))\ntest_dataloader = DataLoader(test_dataset, batch_size = 128)","f5e3bc3c":"result = []\nwith torch.no_grad():\n    for data in test_dataloader:\n        features = data[0].to(device)\n\n        # calculate outputs by running images through the network\n        outputs = model(features)\n\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        result.extend(predicted.cpu().detach().numpy())\n","e6a1e500":"df_result = pd.DataFrame(np.array([df_test.index.tolist(), result]).T, columns = ['id', 'target'])\ndf_result.head(5)","d23eaba7":"df_result.to_csv(\"submission.csv\", index=False)","8e241e3a":"## Read the data\n- Using Pandas `read_csv` functionality to grab the data","064daaee":"## Lets work with pytorch and get our model ready","54ce3ce6":"## Create the Model\n- In `PyTorch` the model can be created either in `nn.Sequential` or as a subclass of `nn.Module` with implementation of `forward` function.\n- The former is easy to deal with, but the later provides **flexibility**.","a21f158b":"### We can specify the accelerator device for the model","2b2929cf":"## Standardize the data\n- Bring all of the features to `0 mean`, and `standard deviation 1`\n- It's requred, other wise the model will be `ralatively over attentive` towards features with `larger scales`.","569cb4ff":"## Define the optimization algorithm, and loss function","822f38b5":"## Create a Pytorch Dataset\n- Pytorch needs a dataset as a subclass of `torch.utils.data.Dataset`.\n- The subclass implements some functions to augument the `Dataset` class for custom datasets.\n- They are generally of 2 types \n    1. Iterable-style : Implements the methods `__iter__()` [Useful when we can't read the data randomly]\n    2. Map-style : Implements the method `__getitem__()`, and `__len__()` [Heavily used for dataset where we can access data through indexing.]\n- In this case `Map-style` dataset subclass is created, to give features, and target values at the query index.","201fc013":"### Normalize it, and get the pytorch dataset, and dataloader","f9ac8acc":"## Save the model parameters for future usage\n- This may be relaoded as further continuation of the training process","75a08a44":"# Simple Neural Network implementation on Pytorch","d3e1ae19":"## Make the Dataloaders\n- Dataloader modalit is used in `PyTorch` to acces the bulk data with required additional operation on overall datasets like shuffle the dataset.\n- Dataloader provides iterable object, to acces the data while training or testing process.","42612003":"## Implement training loop","059f0419":"### Save the dataframe as a csv file","36f4c5c5":"## Let's infer on the test set and submit the predictions","4f5c260e":"### Load test data","326c5702":"### Infer the results","5166d6f7":"### Create results as a pandas DataFrame","05b8d40d":"## Get the model description","a81ff3fe":"## Check validation and train accuracy","4d5392a7":"## Split the data into train and validation set\n- As we know that there isn't any **validation set** given.\n- We need to verify the performance of the model on **unseen dataset**.\n- Thus we need to make validation dataset from the given training data.","8b9ac38d":"## Initialize the model weights","ecd2a122":"## Separate out features and corresponding target values\n- In pandas one can simply provide a `list of columns` as an index to `pd.DataFrame` to get all the data associated to given columns as an index\n- `features_cols = [\"f0\", \"f1\", ...]`\n- `target_cols = \"target\"`","c8ea7daa":"## We can also load previously stored model\n- set `load_prev` to `True`"}}