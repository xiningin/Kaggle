{"cell_type":{"c809bbbf":"code","dd861876":"code","12df7275":"code","b4185700":"code","e60d7f84":"code","b297b718":"code","aa95c914":"code","2181418d":"code","354b531f":"code","cf002e0e":"code","83d02238":"code","d33b4e4f":"code","b6516346":"code","b2aa8c93":"code","abef8912":"code","5c741b5a":"code","e11bb871":"code","c9b05ed4":"code","1f49dce1":"code","3a89cd2c":"code","40692064":"code","141186d9":"code","db94129e":"code","f568a77c":"code","4f28ae12":"code","439b68e5":"code","3eec19cd":"code","e5ccac87":"code","6404584d":"code","14f73a34":"code","7509e84e":"code","8c8799ab":"code","8093eb0f":"code","2b6034f0":"code","3f6802f3":"markdown","2040a46d":"markdown","e53fdc18":"markdown","e2f6546a":"markdown","67c82d13":"markdown","a21b98d8":"markdown","c53ecc8b":"markdown","56cf9ad3":"markdown","d05842c1":"markdown","8476d3e9":"markdown","2a9cad5d":"markdown","4b782d00":"markdown","54000460":"markdown","b3813066":"markdown","53b23cc3":"markdown","f460e4de":"markdown","9eda82df":"markdown","69d47d7f":"markdown","87b7db33":"markdown","3190a36b":"markdown","782982a3":"markdown","2d16b2a2":"markdown","32852681":"markdown","bdc1273b":"markdown","b2dea419":"markdown","ebc9049c":"markdown"},"source":{"c809bbbf":"import numpy as np \nimport pandas as pd\nimport dask.bag as db\nimport plotly.express as px","dd861876":"lines=db.read_text(\"\/kaggle\/input\/arxiv\/*.json\") \n","12df7275":"lines.take(2) ## Looks at first two records","b4185700":"import json\n\nrecords=lines.map(lambda x:json.loads(x))\n\nrecords.take(2)\n\n","e60d7f84":"print(\"Type of First Record After JSON LOADS \",type(lines.take(1)[0]))\nprint(\"Type of First Record After JSON LOADS \",type(records.take(1)[0]))","b297b718":"records_count=records.count()\nrecords_count","aa95c914":"print(\"Number of Records in ArXiv Data is \",records_count.compute())\n","2181418d":"records.map(lambda x:x['submitter']).frequencies(sort=True).compute() ### The map function here extracts only the submitter information and the frequencies function is applied on it","354b531f":"records.map(lambda x:x['submitter']).frequencies(sort=True).topk(k=10,key=1).compute()","cf002e0e":"records.map(lambda x:x['categories']).frequencies(sort=True).topk(k=20,key=1).compute()","83d02238":"extract_latest_version=lambda x:x['versions'][-1][\"created\"] ## Here -1 indicates the last element in the versions. ","d33b4e4f":"data_after_2015=records.filter(lambda x:int(extract_latest_version(x).split(\" \")[3])>=2015)","b6516346":"print(\"Number of Papers Published Since 2015 \",data_after_2015.count().compute())","b2aa8c93":"data_after_2015.map(lambda x:x['categories']).frequencies(sort=True).topk(k=20,key=1).compute()","abef8912":"extract_latest_version_year=lambda x:x['versions'][-1][\"created\"].split(\" \")[3]","5c741b5a":"pub_by_year=records.map(extract_latest_version_year).frequencies().to_dataframe(columns=['submission_year','num_submissions']).compute()","e11bb871":"pub_by_year.head()","c9b05ed4":"pub_by_year=pub_by_year.sort_values(by=\"submission_year\")\npub_by_year.head()","1f49dce1":"\npx.line(x='submission_year',y='num_submissions',data_frame=pub_by_year,title=\"Distribution of Paper Published on Arxiv By Year\")","3a89cd2c":"ai_category_list=['stat.ML','cs.LG','cs.AI']","40692064":"ai_docs = (records.filter(lambda x:any(ele in x['categories'] for ele in ai_category_list)==True))","141186d9":"print(\"Total Papers published in AI&ML \",ai_docs.count().compute())","db94129e":"ai_docs_by_year=ai_docs.map(extract_latest_version_year).frequencies().to_dataframe(columns=['submission_year','num_submissions']).compute()","f568a77c":"ai_docs_by_year=ai_docs_by_year.sort_values(by=\"submission_year\")\nai_docs_by_year.head()","4f28ae12":"px.line(x='submission_year',y='num_submissions',data_frame=ai_docs_by_year,title=\"AI & ML Paper Published on Arxiv By Year\")","439b68e5":"## Extracting author parsed for the first paper to look at the structure\nauthors=records.map(lambda x:x[\"authors_parsed\"]).take(1)[0]\nauthors","3eec19cd":"[\" \".join(a) for a in authors]","e5ccac87":"get_authors =lambda x: [' '.join(a).strip() for a in x['authors_parsed']]","6404584d":"ai_authors=ai_docs.map(get_authors).flatten().frequencies(sort=True).topk(k=20,key=1).to_dataframe(columns=['authors','num_submissions']).compute()","14f73a34":"ai_authors.head()","7509e84e":"ai_authors = ai_authors.sort_values('num_submissions', ascending=True)","8c8799ab":"px.bar(y=\"authors\",x=\"num_submissions\",data_frame=ai_authors,orientation=\"h\")","8093eb0f":"get_metadata = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'],\n                  'abstract':x['abstract'],\n                 'version':x['versions'][-1]['created']}\n\nai_papers=ai_docs.map(get_metadata).to_dataframe().compute()","2b6034f0":"ai_papers.head()","3f6802f3":"As we can see there has been an exponential growth is the field of AI&ML since 2015.","2040a46d":"### Compute the Number of Records\n\nAs with all lazy Dask collections, we need to call **compute** to actually evaluate our result. \n\nThe **take** method used above is also like compute and will also trigger computation.","e53fdc18":"### What if we wanted only the Top 10 Submitters??\n\nDask Bag has a topk() function which allows us to get the top k by frequencies. ","e2f6546a":"In the above function we gave *.json instead of the actual filename, to ensure that if they are more than one json files, Dask Bag can read it. Also, each json will become its own partition. \n\nUsing * instead of the filename, ensures even if the filename changes, the json file is read","67c82d13":"### Who are the Top Submitters?\n\n**frequencies()** function allows us to Count number of occurrences of each distinct element. By Setting sort=True in frequencies() we can sort in descending order of frequencies","a21b98d8":"###     Extracting MetaData of AI, ML papers into DataFrame","c53ecc8b":"In the above example, we performed a chain computation  involving three steps\n\n**1. we first extracted the submitter information using map()**\n\n**2. Then applied frequencies() function to count distinct occurrent of Each Submitter**\n\n**3. We used to compute() function to start the computation**","56cf9ad3":"#### Getting top categories on the filtered data","d05842c1":"### Conclusion\n\nWe saw a few ways in which we can use Dask Bag to perform aggregate queries on large datasets without having to load the data into memory.\nWe can do further aggregations as well using groupby(). ","8476d3e9":"#### Get the papers that belong to AI ML Categories","2a9cad5d":"### Look at the First 5 records of data\n\nSimilar to head() function in Pandas DataFrame, Dask Bag has take() function.","4b782d00":"### Reading the Data using Dask Bag\n\n**read_text** is used to read file in dask bag. The contents of the file will be read as string","54000460":"### Number of Papers that have been published each Year\n\n\nLet us extract the data and store it in a dataframe - this will allow us to visualise the trend.\n\nThe **to_dataframe()** function, allows to convert the data returned by Dask Bag into a dataframe.\n\n","b3813066":"### What are the top 20 categories in which journals are published?","53b23cc3":"#### Filtering Data Since 2010","f460e4de":"We can see that each record is a string. We will now have to convert it into dict format, to allow us to query the data. We can use json.loads() to convert each record to dict format and use the **map** function to apply it to all records.\n\n### Convert the Records to Dict or JSON Format","9eda82df":"###   Top Authors publishing in AI and ML \n\nauthors parsed, contains a list of list of authors. We can join them and get a list of Authors for a paper. We can then use flatten to concatenate nested list of authors across all papers to get the frequency","69d47d7f":"**\"Yoshua Bengio\" ,Canadian Scientist** who was the co-receipient of the **Turing Award in 2018** is the leading author in AI and ML papers. ","87b7db33":"### Introduction\n\nArXiv data consists of 1.7million papers, this means that data is too huge to fit into memory and using standard pandas DataFrame may not be feasible.\nThis is where Dask or PySpark RDD can come in handy. In this notebook, we will look at using **dask.bag** to perform computations to analyse the ArXiv Data.\n\n \n**Dask Bag** implements operations like map, filter, groupby and aggregations on collections of Python objects. It does this in parallel and in small memory using Python iterators. It is similar to a parallel version of itertools or a Pythonic version of the PySpark RDD.\n\n### About the Data\n\n**arXiv** is an **open-access repository** of electronic preprints (known as e-prints) approved for posting after moderation, but not full peer review. \n\nIt consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. \n\nIn many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. \n\nThis dataset consists of metadata of 1.7 million papers present on arXiv. The metadata includes the version of the journal, when the version was created, the submitted of the paper, the authors, the title and the abstract of the paper. It also includes category the paper. \n\n","3190a36b":"### Loading the Libraries","782982a3":"When we simply call the **count()** function without calling compute() it does not actually perform any computation.","2d16b2a2":"Over the Years, number of papers published have been increasing. Let us how for a given category say AI and ML has this trend been.\n \n###  Papers published by Year for AI and ML\n\nthe categories that belong to AI and ML are represented by 'stat.ML','cs.LG','cs.AI'. A paper may belong to multiple categories and this is a string. \n\nSo we need to check if any of the three categories are present in the category string and filter out those paper","32852681":"We can see that the metadata is extracted into a dataframe. \n\n","bdc1273b":"We can see that each record is now a dictionary and we can access each attribute by the key","b2dea419":"AstroPhysics is the category in which most papers are published. Except for Computer Vision(cs.CV), AI and ML papers are not in the top 20 categories.\n\nIs it because, it is in the recent years they have gained traction? \n\n### What are the Top 20 Categories in which Journals are published since 2015?\n\n\nFRom the data, getting the year of publication is not straightforward, we have to extract the year from \"versions\" tag and \"versions\" also contains information about multiple version. Let us for our purpose get the \"created\" date from the last version. \n\n\nDate is of format : Wed, 27 Jun 2012 19:59:59 GMT . To get the year, we can just split the date on space and get the third index\n\nOnce we have the date, we have to filter by Year and then compute the top categories.","ebc9049c":"There is a huge shift in the top 20 in the last 5 years. ML and most of the CV papers have come in the last 5 years. "}}