{"cell_type":{"9a05885b":"code","187dba9c":"code","314b6fc3":"code","e71efd8f":"code","8fe20f7f":"code","589ddf62":"code","23eea43a":"code","1c363896":"code","3c95dcbe":"code","628a4f8f":"code","7c8e8b6c":"code","fb1f56c9":"code","3288b863":"code","73a13f97":"code","7b53718e":"code","b0276b55":"code","51c53a31":"code","297601bd":"code","f179e295":"markdown","ec269cac":"markdown","8779c775":"markdown","5eba03bf":"markdown","087241f8":"markdown","4715706a":"markdown","a72b127a":"markdown","df3c5d3f":"markdown","b6eee085":"markdown","9312c8ff":"markdown","81e817cc":"markdown","1af2ec8d":"markdown"},"source":{"9a05885b":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport time","187dba9c":"# Opening file, reading, eliminating whitespaces, and splitting by '\\n', which in turn creates list\nlabels = open('\/kaggle\/input\/obj.names').read().strip().split('\\n')  # list of class names\n\n# # Check point\nprint(labels)","314b6fc3":"# Defining paths to the weights and configuration file with model of Neural Network\nweights_path = '\/kaggle\/input\/yolov4-obj.weights'\nconfiguration_path = '\/kaggle\/input\/yolov4-obj.cfg'\n\n# Setting minimum probability to eliminate weak predictions\nprobability_minimum = 0.5\n\n# Setting threshold for non maximum suppression\nthreshold = 0.3","e71efd8f":"network = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)\n\n# Getting names of all layers\nlayers_names_all = network.getLayerNames()  # list of layers names\n\n# # Check point\nprint(layers_names_all)","8fe20f7f":"# Getting only output layers names that we need from YOLO algorithm\nlayers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names\n\n# Check point\nprint(layers_names_output)  ","589ddf62":"# Our image initially is in RGB format\n# But now we open it in BGR format as function 'cv2.imread' opens it so\nimage_input = cv2.imread('\/kaggle\/input\/test_img\/car.jpg')\n\n# Getting image shape\nimage_input_shape = image_input.shape\n\n# Check point\nprint(image_input_shape) ","23eea43a":"# Showing RGB image but firstly converting it from BGR format\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()","1c363896":"blob = cv2.dnn.blobFromImage(image_input, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n\n# Check point\nprint(image_input.shape)  \nprint(blob.shape)  ","3c95dcbe":"# Check point\n# Slicing blob and transposing to make channels come at the end\nblob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\nprint(blob_to_show.shape)  # (416, 416, 3)\n\n# Showing 'blob_to_show'\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 5.0)\nplt.imshow(blob_to_show)\nplt.show()","628a4f8f":"# Calculating at the same time, needed time for forward pass\nnetwork.setInput(blob)  # setting blob as input to the network\nstart = time.time()\noutput_from_network = network.forward(layers_names_output)\nend = time.time()\n\n# Showing spent time for forward pass\nprint('YOLO v4 took {:.3f} seconds'.format(end - start))","7c8e8b6c":"# Check point\nprint(type(output_from_network))  # <class 'list'>\nprint(type(output_from_network[0]))  # <class 'numpy.ndarray'>","fb1f56c9":"# Seed the generator - every time we run the code it will generate by the same rules\n# In this way we can keep specific colour the same for every class\nnp.random.seed(42)\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\n# Check point\nprint(colours.shape)  \nprint(colours[0])  ","3288b863":"# Preparing lists for detected bounding boxes, obtained confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []","73a13f97":"# Getting spacial dimension of input image\nh, w = image_input_shape[:2]  # Slicing from tuple only first two elements\n\n# Check point\nprint(h, w)  ","7b53718e":"for result in output_from_network:\n    # Going through all detections from current output layer\n    for detection in result:\n        # Getting class for current object\n        scores = detection[5:]\n        class_current = np.argmax(scores)\n\n        # Getting confidence (probability) for current object\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current > probability_minimum:\n            # Scaling bounding box coordinates to the initial image size\n            # YOLO data format keeps center of detected box and its width and height\n            # That is why we can just elementwise multiply them to the width and height of the image\n            box_current = detection[0:4] * np.array([w, h, w, h])\n\n            # From current box with YOLO format getting top left corner coordinates\n            # that are x_min and y_min\n            x_center, y_center, box_width, box_height = box_current.astype('int')\n            x_min = int(x_center - (box_width \/ 2))\n            y_min = int(y_center - (box_height \/ 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)","b0276b55":"# It is needed to make sure the data type of the boxes is 'int'\n# and the type of the confidences is 'float'\n# https:\/\/github.com\/opencv\/opencv\/issues\/12789\nresults = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n# Check point\n# Showing labels of the detected objects\nfor i in range(len(class_numbers)):\n    print(labels[int(class_numbers[i])])\n\n# Saving found labels\nwith open('found_labels.txt', 'w') as f:\n    for i in range(len(class_numbers)):\n        f.write(labels[int(class_numbers[i])])","51c53a31":"# Checking if there is at least one detected object\nif len(results) > 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Getting current bounding box coordinates\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n        # Preparing colour for current bounding box\n        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n\n        # Drawing bounding box on the original image\n        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n                      colour_box_current, 2)\n\n        # Preparing text with label and confidence for current bounding box\n        text_box_current = '{} : {:.2f}%'.format(labels[int(class_numbers[i])], confidences[i])\n\n        # Putting text with label and confidence on the original image\n        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n                0.5, colour_box_current, 2)","297601bd":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()","f179e295":"## Implementing forward pass with our blob and only through output layers","ec269cac":"# YOLO v4 - Licence Plates Detection","8779c775":"## Plot RGB image with bounding boxes and labels","5eba03bf":"## Loading class labels from file","087241f8":"## Getting blob from input image","4715706a":"## Implementing non maximum suppression of given boxes and corresponding scores","a72b127a":"## Draw bounding boxes and labels","df3c5d3f":"## Loading libraries","b6eee085":"## Going through all output layers after feed forward and answer from network","9312c8ff":"## Colours for representing every detected object","81e817cc":"## Loading trained YOLO Licence Plate Detector ","1af2ec8d":"## Loading input image "}}