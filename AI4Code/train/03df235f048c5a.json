{"cell_type":{"2a44d301":"code","c25d060b":"code","e5760a45":"code","5cc6e0bf":"code","75cd3881":"code","d22bf5a3":"code","387730dc":"code","99f0dfa1":"code","a136b519":"code","e956e27d":"code","108d75e2":"code","94210a20":"code","534c6425":"code","a07013bf":"code","894325af":"code","e0483dcf":"code","8565562f":"code","da75e689":"code","17c181dd":"code","4d529913":"code","5dd38b78":"markdown","9b5804da":"markdown","ba91e4eb":"markdown","958f1a7b":"markdown","47a0a4b0":"markdown","3f57be28":"markdown","503e9024":"markdown","e113a471":"markdown","9137a7d9":"markdown","a2eb9df4":"markdown","29141a4e":"markdown","8d962ffd":"markdown","10fae161":"markdown","57823b08":"markdown","ae8b0c3e":"markdown","7d53a9d7":"markdown","a8442b8c":"markdown","44778942":"markdown","a1656e89":"markdown","33980ef6":"markdown","e0e3c35b":"markdown","ca1dd532":"markdown","e3fbafee":"markdown","62d71bc3":"markdown","970b946c":"markdown","7c88c7b0":"markdown"},"source":{"2a44d301":"%%capture\n!pip install pycaret[full]\n\nimport os\nimport warnings\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\nfrom pathlib import Path\n\nimport dateutil.easter as easter\nfrom pycaret.regression import *\n\n# Mute warnings\nwarnings.filterwarnings(\"ignore\")","c25d060b":"!tree ..\/input\/","e5760a45":"data_dir = Path('..\/input\/tabular-playground-series-jan-2022')\nholiday_dir = Path('..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519')\ngdp_dir = Path('..\/input\/gdp-20152019-finland-norway-and-sweden')\n\ntrain = pd.read_csv(\n    data_dir \/ 'train.csv',\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n        'num_sold': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n    index_col='row_id'\n)\n\ntest = pd.read_csv(\n    data_dir \/ \"test.csv\",\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n    index_col='row_id'\n)\n\ntarget_col = train.columns.difference(test.columns)[0]\n\nholiday_data = pd.read_csv(holiday_dir \/ 'holidays.csv')\n\ngdp = pd.read_csv(\n    gdp_dir \/ 'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')","5cc6e0bf":"eda = setup(data=train, target=target_col, session_id=123 , profile=True, silent=True)","75cd3881":"# Categorical features\ncategorical_cols = train.select_dtypes('category').columns.tolist()","d22bf5a3":"K_FOLDS = 3\nGDP_EXPONENT = 1.2120618918594863 \n# c.f https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\n\ngdp.columns = gdp.columns.str[4:]\ngdp = gdp.apply(lambda x: x**GDP_EXPONENT)\nscaler = gdp.iloc[K_FOLDS+1] \/ gdp\ngdp_map = scaler.stack().to_dict()\n\ntrain[target_col] = pd.Series(\n    list(zip(train.date.dt.year,train.country))\n).map(gdp_map) * train[target_col]\n\ntrain[target_col] = np.log1p(train.num_sold)","387730dc":"def holiday_features(holiday_df, df):\n    \n    fin_holiday = holiday_df.loc[holiday_df.country == 'Finland']\n    swe_holiday = holiday_df.loc[holiday_df.country == 'Sweden']\n    nor_holiday = holiday_df.loc[holiday_df.country == 'Norway']\n    \n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    \n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    \n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-5, 65)\n    \n    # Last Sunday of May (Mother's Day)\n    sun_may_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-5-31')),\n        2016: pd.Timestamp(('2016-5-29')),\n        2017: pd.Timestamp(('2017-5-28')),\n        2018: pd.Timestamp(('2018-5-27')),\n        2019: pd.Timestamp(('2019-5-26'))\n    })\n    #new_df['days_from_sun_may'] = (df.date - sun_may_date).dt.days.clip(-1, 9)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-06-24')),\n        2016: pd.Timestamp(('2016-06-29')),\n        2017: pd.Timestamp(('2017-06-28')),\n        2018: pd.Timestamp(('2018-06-27')),\n        2019: pd.Timestamp(('2019-06-26'))\n    })\n    df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-11-1')),\n        2016: pd.Timestamp(('2016-11-6')),\n        2017: pd.Timestamp(('2017-11-5')),\n        2018: pd.Timestamp(('2018-11-4')),\n        2019: pd.Timestamp(('2019-11-3'))\n    })\n    df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return df\n\ntrain = holiday_features(holiday_data, train)\ntest  = holiday_features(holiday_data, test)","99f0dfa1":"train = pd.get_dummies(train, columns=categorical_cols)\ntest  = pd.get_dummies(test, columns=categorical_cols)","a136b519":"def new_date_features(df):\n    df['year'] = df.date.dt.year \n    df['quarter'] = df.date.dt.quarter\n    df['month'] = df.date.dt.month  \n    df['week'] = df.date.dt.week \n    df['day'] = df.date.dt.day  \n    df['weekday'] = df.date.dt.weekday\n#     df['day_of_week'] = df.date.dt.dayofweek  \n    df['day_of_year'] = df.date.dt.dayofyear  \n#     df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_month'] = df.date.dt.days_in_month  \n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    df['is_friday'] = np.where((df['weekday'] == 4), 1, 0)\n    \n    df.drop('date', axis=1, inplace=True)\n    \n    return df\n    \ntrain = new_date_features(train)\ntest  = new_date_features(test)","e956e27d":"display(train, test)","108d75e2":"def smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) \/ 2\n    \n    return np.mean(numerator \/ denominator)*100","94210a20":"NB_MODELS = 3\n\nmodels = []\n\nfor i in range (NB_MODELS):\n    print ('Fit Model', i)\n    reg = setup(\n        data = train,\n        target = target_col,\n        data_split_shuffle = False, \n        create_clusters = False,\n        fold_strategy = 'groupkfold',\n        fold_groups = 'year',\n        use_gpu = True,\n        silent = True,\n        fold = K_FOLDS,\n        n_jobs = -1,\n    )\n    \n    add_metric('SMAPE', 'SMAPE', smape, greater_is_better=False)\n    \n    models.append(create_model('catboost'))","534c6425":"plot_model(models[0], 'feature')","a07013bf":"interpret_model(models[0])","894325af":"blend = blend_models(models)","e0483dcf":"final_blend = finalize_model(blend)","8565562f":"plot_model(final_blend, 'error')","da75e689":"# Fit-Based Weights Geo-Rounded\n# from https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n\n    return result_array","17c181dd":"y_pred = np.expm1(\n    predict_model(final_blend, data=test)['Label']\n)\n\ny_pred = geometric_round(np.array(y_pred).transpose()).astype(int)\ny_pred","4d529913":"submission = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nsubmission[target_col] = y_pred\n\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission","5dd38b78":"Next, the cardinality of each categorical feature is quite low, and that we do not want to impose an ordinal order, **one-hot encoding** may be a good way to encode our categorical features.","9b5804da":"### Interpret the model","ba91e4eb":"\ud83d\udccc This part has been updated and largely inspired by these notebooks:\n> * [TPSJAN22-03 Linear Model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) & [TPSJAN22-06 LightGBM Quickstart](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart) by [AmbrosM](https:\/\/www.kaggle.com\/ambrosm)<br \/>\n> * [TPS Jan 22 - EDA + modelling](https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-eda-modelling) by [Samuel Cortinhas](https:\/\/www.kaggle.com\/samuelcortinhas)\n> * [TPS Jan 2022 CatBoost with PyCaret](https:\/\/www.kaggle.com\/bernhardklinger\/tps-jan-2022-catboost-with-pycaret) by [Bernhard Klinger](https:\/\/www.kaggle.com\/bernhardklinger)","958f1a7b":"Submissions are evaluated on SMAPE between forecasts and actual values.","47a0a4b0":"___\n# <a name=\"Loading\">\ud83d\udcda Loading libraries and files<\/a>","3f57be28":"### Submission","503e9024":"It has been shown that the GDP helps to improve the our mode. Thus, let's transform our target considering the **GDP deflator**, which is a measure of inflation.","e113a471":"**Tip:** Since the SMAPE evaluation metric is asymmetric. In this case, underestimated values are much more penalized than overestimated values. Then, feel free to round your predictions **up** to the nearest value, or use any rounding technique that may be relevant.<br \/>\n<br \/>\n\ud83d\udccc You will find more by having a glance to these awesome notebooks: \n> * [SMAPE Weirdness](https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness) by [CPMP](https:\/\/www.kaggle.com\/cpmpml)\n> * [TPS Jan 2022: A simple average model (no ML)](https:\/\/www.kaggle.com\/carlmcbrideellis\/tps-jan-2022-a-simple-average-model-no-ml) by [Carl McBride Ellis](https:\/\/www.kaggle.com\/carlmcbrideellis).\n> * [\ud83c\udf2a Ensembling and rounding techniques comparison](https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison) by [Fergus Findley](https:\/\/www.kaggle.com\/fergusfindley)","9137a7d9":"### Initializing setup","a2eb9df4":"### Blending the models","29141a4e":"# TPS-Jan22 | CatBoost using PyCaret\n\n# \ud83d\udcdd Agenda\n>1. [\ud83d\udcda Loading libraries and files](#Loading)\n>2. [\ud83d\udd0d Exploratory Data Analysis with PyCaret](#EDA)\n>3. [\u2699\ufe0f Feature Engineering](#FeatureEngineering)\n>4. [\ud83c\udfcb\ufe0f Model Training & Inference](#TrainingInference)","8d962ffd":"___\n# <a name=\"FeatureEngineering\">\u2699\ufe0f Feature Engineering<\/a>","10fae161":"Since we have a <code>date<\/code>-typed feature here, and models are rarely able to use dates and times as they are, we would benefit from encoding it as categorical variables as this can often yield useful information about temporal patterns.\n\nFurthermore, time-series data (such as product sales) often have distributions that differs from week days to week-ends for example, it is likely that using the day of the week as a new feature is a relevant option we have.","57823b08":"# What is PyCaret?","ae8b0c3e":"### Finalization & Inference","7d53a9d7":"___\n# <a name=\"TrainingInference\">\ud83c\udfcb\ufe0f Model Training & Inference<\/a>","a8442b8c":"We are dealing with time-series data, therefore it is relevant to consider the impact of holidays, which naturally play a large role in business activities.","44778942":"[PyCaret](https:\/\/pycaret.org\/) is an open source Python machine learning library inspired by the caret R package.\n\nThe goal of the caret package is to automate the major steps for evaluating and comparing machine learning algorithms for classification and regression. The main benefit of the library is that a lot can be achieved with very few lines of code and little manual configuration. The PyCaret library brings these capabilities to Python.\n\n\ud83d\udccc According to the [PyCaret official website](https:\/\/pycaret.org\/guide\/):\n> PyCaret is an open-source, **low-code** machine learning library in Python that aims to reduce the cycle time from hypothesis to insights. It is well suited for **seasoned data scientist**s who want to increase the productivity of their ML experiments by using PyCaret in their workflows or for **citizen data scientists** and those **new to data science** with little or no background in coding. PyCaret allows you to go from preparing your data to deploying your model within seconds using your choice of notebook environment.\n\nThe PyCaret library automates many steps of a machine learning project, such as:\n* Defining the data transforms to perform `setup()`\n* Evaluating and comparing standard models `compare_models()`\n* Tuning model hyperparameters `tune_model()`\n\nAs well as many more features not limited to creating ensembles, saving models, and deploying models.","a1656e89":"![PyCaret logo](https:\/\/raw.githubusercontent.com\/pycaret\/pycaret\/master\/docs\/images\/logo.png)","33980ef6":"Analyzing feature importance:","e0e3c35b":"Finally, here are our datasets.","ca1dd532":"![SMAPE formula](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20211120224204\/smapeformula.png)","e3fbafee":"___\n# <a name=\"EDA\">\ud83d\udd0d Exploratory Data Analysis with PyCaret<\/a>","62d71bc3":"Analyzing the prediction error:","970b946c":"Interpretation of the model, based on the [SHapley Additive exPlanations (SHAP)](https:\/\/shap.readthedocs.io\/en\/latest\/):\n\n> **SHAP** is an approach that aims at **explaining the output of any Machine Learning model.** This tool has the particularity of connecting game theory with local explanations by unifying several old methods such as LIME, DeepLIFT and Shapley value (in a cooperative game, Shapley value gives (in a cooperative game, Shapley value gives a fair distribution of payoffs to the players).","7c88c7b0":"\ud83d\udccc According to the [PyCaret documentation](https:\/\/pycaret.org\/blend-models\/):\n> Blending models is a method of ensembling which uses consensus among estimators to generate final predictions. The idea behind blending is to combine different machine learning algorithms and use a majority vote or the average predicted probabilities in case of classification to predict the final outcome."}}