{"cell_type":{"7edc1d79":"code","83097b1c":"code","101337f8":"code","d82aab83":"code","2acd1faf":"code","51188bad":"code","0bb3e5ed":"code","7406ac54":"code","d85b8f81":"code","e3aca09c":"code","48c21072":"code","c94aa31a":"code","4cb2a721":"code","ea5ec17f":"code","0012155f":"code","678729e0":"code","9772e75c":"code","44d19b1e":"code","02a8e564":"code","5a801354":"code","6a3e954c":"code","83b60f18":"code","b2d4af65":"code","3800f3bc":"code","5587fac8":"code","d07ff1a2":"markdown","0083b750":"markdown","81a2afd1":"markdown","23ad22cc":"markdown","9be7be94":"markdown","a0537399":"markdown","c9e3a24d":"markdown","98698c23":"markdown","36dcc810":"markdown","daddac2a":"markdown","301a64d6":"markdown","a0cee9a4":"markdown","040667b2":"markdown"},"source":{"7edc1d79":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nimport catboost\nfrom catboost import Pool\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport itertools\nfrom scipy import interp\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\n\n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","83097b1c":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","101337f8":"# merge \ntrain_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"Train shape : \"+str(train_df.shape))\nprint(\"Test shape  : \"+str(test_df.shape))","d82aab83":"train_df[\"diff_V319_V320\"] = np.zeros(train_df.shape[0])\n\ntrain_df.loc[train_df[\"V319\"]!=train_df[\"V320\"],\"diff_V319_V320\"] = 1\n\ntest_df[\"diff_V319_V320\"] = np.zeros(test_df.shape[0])\n\ntest_df.loc[test_df[\"V319\"]!=test_df[\"V320\"],\"diff_V319_V320\"] = 1\n\ntrain_df[\"diff_V109_V110\"] = np.zeros(train_df.shape[0])\n\ntrain_df.loc[train_df[\"V109\"]!=train_df[\"V110\"],\"diff_V109_V110\"] = 1\n\ntest_df[\"diff_V109_V110\"] = np.zeros(test_df.shape[0])\n\ntest_df.loc[test_df[\"V109\"]!=test_df[\"V110\"],\"diff_V109_V110\"] = 1","2acd1faf":"pd.set_option('display.max_columns', 500)","51188bad":"# GPreda, missing data\ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","0bb3e5ed":"display(missing_data(train_df), missing_data(test_df))","7406ac54":"#fillna\ntrain_df = train_df.fillna(-999)\ntest_df = test_df.fillna(-999)","d85b8f81":"# From https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\n\ndef make_day_feature(df, offset=0.58, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6.\n    \"\"\"\n    days = df[tname] \/ (3600 * 24)\n    encoded_days = np.floor(days - 1 + offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23.\n    \"\"\"\n    hours = df[tname] \/ (3600)\n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","e3aca09c":"plt.hist(train_df['TransactionDT'] \/ (3600 * 24), bins=1800)\nplt.xlim(70, 78)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,1000)","48c21072":"train_df['Weekday'] = make_day_feature(train_df)\ntrain_df['Hour'] = make_hour_feature(train_df)","c94aa31a":"plt.plot(train_df.groupby('Hour').mean()['isFraud'])\nplt.xlabel('Encoded hour')\nplt.ylabel('Fraction of fraudulent transactions')","4cb2a721":"test_df['Weekday'] = make_day_feature(test_df)\ntest_df['Hour'] = make_hour_feature(test_df)","ea5ec17f":"train_df = train_df.sort_values('TransactionDT').drop('TransactionDT', axis=1)\ntest_df = test_df.sort_values('TransactionDT').drop('TransactionDT', axis=1)","0012155f":"del train_transaction, train_identity, test_transaction, test_identity","678729e0":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain'\n         ]\n\nfor col in i_cols:\n    if col in train_df and col in test_df:\n        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n        fq_encode = temp_df[col].value_counts().to_dict()   \n        train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n        test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)","9772e75c":"%%time\n# Check if Transaction Amount is common or not (we can use freq encoding here)\n# In our dialog with model we are telling to trust or not to these values  \nvalid_card = train_df['TransactionAmt'].value_counts()\nvalid_card = valid_card[valid_card>10]\nvalid_card = list(valid_card.index)\n    \ntrain_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\ntest_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n\n# For our model current TransactionAmt is a noise (even when features importances are telling contrariwise)\n# There are many unique values and model doesn't generalize well\n# Lets do some aggregations\ni_cols = ['card1','card2','card3','card5']\n\nfor col in i_cols:\n    for agg_type in ['mean', 'std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n        \n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n    \n        train_df[new_col_name] = train_df[col].map(temp_df)\n        test_df[new_col_name]  = test_df[col].map(temp_df)","44d19b1e":"%%time\n# Let's look on bank addres and client addres matching\n# card3\/card5 bank country and name?\n# Addr2 -> Clients geo position (country)\n# Most common entries -> normal transactions\n# Less common etries -> some anonaly\n\ntrain_df['bank_type'] = train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\ntest_df['bank_type']  = test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n\ntrain_df['address_match'] = train_df['bank_type'].astype(str)+'_'+train_df['addr2'].astype(str)\ntest_df['address_match']  = test_df['bank_type'].astype(str)+'_'+test_df['addr2'].astype(str)\n\nfor col in ['address_match','bank_type']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    temp_df[col] = np.where(temp_df[col].str.contains('nan'), np.nan, temp_df[col])\n    temp_df = temp_df.dropna()\n    fq_encode = temp_df[col].value_counts().to_dict()   \n    train_df[col] = train_df[col].map(fq_encode)\n    test_df[col]  = test_df[col].map(fq_encode)\n\ntrain_df['address_match'] = train_df['address_match']\/train_df['bank_type'] \ntest_df['address_match']  = test_df['address_match']\/test_df['bank_type']","02a8e564":"# Label Encoding\nfor f in train_df.columns:\n    if  train_df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n        train_df[f] = lbl.transform(list(train_df[f].values))\n        test_df[f] = lbl.transform(list(test_df[f].values))  \ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()","5a801354":"features = list(train_df)\nfeatures.remove('isFraud')\nfeatures.remove('bank_type')\ntarget = 'isFraud'","6a3e954c":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","83b60f18":"param_cb = {\n        'learning_rate': 0.2,\n        'bagging_temperature': 0.1, \n        'l2_leaf_reg': 30,\n        'depth': 12, \n        #'max_leaves': 48,\n        'max_bin':255,\n        'iterations' : 1000,\n        'task_type':'GPU',\n        'loss_function' : \"Logloss\",\n        'objective':'CrossEntropy',\n        'eval_metric' : \"AUC\",\n        'bootstrap_type' : 'Bayesian',\n        'random_seed':1337,\n        'early_stopping_rounds' : 100,\n        'use_best_model': True \n}","b2d4af65":"print('CatBoost GPU modeling...')\nstart_time = timer(None)\nplt.rcParams[\"axes.grid\"] = True\n\nnfold = 7\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\ncms= []\ntprs = []\naucs = []\ny_real = []\ny_proba = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\nfor train_idx, valid_idx in skf.split(train_df, train_df.isFraud.values):\n    print(\"\\nfold {}\".format(i))\n\n    \n    trn_data = Pool(train_df.iloc[train_idx][features].values,label=train_df.iloc[train_idx][target].values)\n    val_data = Pool(train_df.iloc[valid_idx][features].values,label=train_df.iloc[valid_idx][target].values)   \n\n    clf = catboost.train(trn_data, param_cb, eval_set= val_data, verbose = 300)\n\n    oof[valid_idx]  = clf.predict(train_df.iloc[valid_idx][features].values)   \n    oof[valid_idx]  = np.exp(oof[valid_idx]) \/ (1 + np.exp(oof[valid_idx]))\n    \n    predictions += clf.predict(test_df[features]) \/ nfold\n    predictions = np.exp(predictions)\/(1 + np.exp(predictions))\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n    accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    precisions.append(precision_score(train_df.iloc[valid_idx][target].values ,oof[valid_idx].round()))\n    f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Roc curve by fold\n    f = plt.figure(1)\n    fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n\n    # Precion recall by folds\n    g = plt.figure(2)\n    precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n    y_real.append(train_df.iloc[valid_idx][target].values)\n    y_proba.append(oof[valid_idx])\n    plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n    \n    i= i+1\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Features imp\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.get_feature_importance()\n    fold_importance_df[\"fold\"] = nfold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n# Metrics\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\n#ROC\nf = plt.figure(1)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Catboost ROC curve by folds')\nplt.legend(loc=\"lower right\")\n\n# PR plt\ng = plt.figure(2)\nplt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\ny_real = np.concatenate(y_real)\ny_proba = np.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nplt.plot(recall, precision, color='blue',\n         label=r'Mean P|R')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Catboost P|R curve by folds')\nplt.legend(loc=\"lower left\")\n\n# Confusion maxtrix & metrics\nplt.rcParams[\"axes.grid\"] = False\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title= 'CatBoost Confusion matrix [averaged\/folds]')\n# Timer end    \ntimer(start_time)","3800f3bc":"\"\"\"\nplt.style.use('dark_background')\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n    .groupby(\"Feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:30].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('CatBoost Features importance (averaged\/folds)', fontsize=18)\nplt.tight_layout()\n\"\"\"","5587fac8":"sample_submission['isFraud'] = predictions\nsample_submission.to_csv('submission_IEEE.csv')","d07ff1a2":"# <a id='3'>3. Feature importance<\/a> ","0083b750":"## CONFUSION MATRIX","81a2afd1":"## CV 7 FOLDS AND METRICS","23ad22cc":"# Remove timestamp","9be7be94":"## DATASETS","a0537399":"# Check if Transaction Amount is common or not","c9e3a24d":"# Anomaly Search in geo information","98698c23":"## ENCODING","36dcc810":"Useful links:\n* https:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again\n* https:\/\/www.kaggle.com\/vincentlugat\/ieee-catboost-gpu-baseline-5-kfold\n\n\nhttps:\/\/www.kaggle.com\/luisfredgs\/ieee-cis-fraud-detection-catboost?scriptVersionId=18955915 - 0.9322\nhttps:\/\/www.kaggle.com\/luisfredgs\/ieee-cis-fraud-detection-catboost?scriptVersionId=18995695 - 0.9373","daddac2a":"# <a id='4'>4. Submission<\/a> ","301a64d6":"# Frequence encoding","a0cee9a4":"## MERGE, MISSING VALUE, FILL NA","040667b2":"# Make day and hour features"}}