{"cell_type":{"d431b266":"code","85898572":"code","de00f7c0":"code","63f3a2fe":"code","ab6d6404":"code","c2ac5847":"code","6c7c48b5":"code","dde40b62":"code","c1f087b0":"code","64bf9b69":"code","2945785f":"code","54e54c8b":"code","99f280a2":"code","b230045e":"code","f82ce90d":"code","34aa1a71":"code","63e19cda":"code","ebfadbc3":"code","8ab0ddca":"code","f9e071e5":"code","4da32833":"code","ded80ca2":"code","e832008e":"code","ece7eb79":"code","6a1b341a":"code","54c92bc2":"code","0a323cc1":"code","c6850af5":"code","81b82cee":"code","4e704118":"code","8adb792c":"code","65c59a29":"code","f1e9225d":"code","b4c4e65b":"code","0ba04c50":"code","1df64ad7":"code","fd4a20dd":"code","4f697d8e":"code","fe765c71":"code","09c8fa56":"markdown","f21a24f1":"markdown","d5aedd9d":"markdown","f88f5b3f":"markdown","4848a01e":"markdown","57cbf0a4":"markdown","a2f863c8":"markdown","dd4a530e":"markdown","c50b76c9":"markdown","374b32b6":"markdown","62f52fd6":"markdown","81147cca":"markdown","c0d71c75":"markdown","6293c239":"markdown","a773dab2":"markdown","13fe12ba":"markdown","2b4de681":"markdown","ad98129d":"markdown","ef36f033":"markdown","c61f1d62":"markdown","4ba86186":"markdown","21e4b805":"markdown","6b651737":"markdown","971ae812":"markdown"},"source":{"d431b266":"!pip install textstat","85898572":"import numpy as np #| linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport spacy \nfrom spacy.tokenizer import Tokenizer\nfrom wordcloud import WordCloud\nimport textstat\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nclass color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'","de00f7c0":"train_data = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_data.head()","63f3a2fe":"train_data.isnull().sum()","ab6d6404":"train_data.describe()","c2ac5847":"train_data.excerpt[0]","6c7c48b5":"train_data['target_bins'] = pd.qcut(train_data['target'],\n                              q=[0, .2, .4, .6, .8, 1],\n                              labels=[\"very_difficult\",\"difficult\",\"medium\",\"easy\",\"very_easy\"])\ng = sns.displot(train_data, x=\"target\", hue=\"target_bins\", kind=\"kde\", fill=True)\nplt.title(\"Target bins Ditribution\", fontsize = 15)\ng.fig.set_size_inches(15,7)","dde40b62":"nlp = spacy.load(\"en_core_web_sm\")\n\nexcerpt_tokens = []\nfor doc in nlp.pipe(train_data[\"excerpt\"][:], disable = [\"ner\"]):\n    excerpt_tokens.append(doc)\n\ntrain_data[\"parsed_excerpt\"] = excerpt_tokens  ","c1f087b0":"most_freq_words = pd.read_csv(\"\/kaggle\/input\/english-word-frequency\/unigram_freq.csv\")\nmost_freq_words.head()","64bf9b69":"rank_tuples = list(most_freq_words.apply(lambda x : (x.word,x.name), axis = 1))\nfrequent_words_rank_dict = dict(rank_tuples)","2945785f":"def is_useful_word(word):\n    if word.is_punct == False and word.is_stop != True and word.text !=\"\\n\":\n        return True\n    return False\n\n\n \n\nexcerpt_processed_words = []\nfor i,sentnece in enumerate(excerpt_tokens):\n    excerpt_processed_words.append([])\n    for word in sentnece:        \n        if is_useful_word(word):\n            excerpt_processed_words[i].append(word.text)\n","54e54c8b":"sentence_vocab_score = []\nfor sentence in excerpt_processed_words:\n    temp_sum = 0\n    for word in sentence:\n        temp_sum+=frequent_words_rank_dict.get(word.lower(), 0)\n    sentence_vocab_score.append(temp_sum\/len(sentence))\n        \ntrain_data[\"vocab_score\"] = sentence_vocab_score","99f280a2":"sorted_train_data = train_data.sort_values(\"target\", axis = 0)\nsorted_train_data.plot.scatter(x = \"target\", y = \"vocab_score\")\nplt.title(\"Vocab score vs Target\", fontsize = 15)\n\nprint(\"correlation : \",sorted_train_data[\"target\"].corr(sorted_train_data[\"vocab_score\"]))\n","b230045e":"def n_max_elements_avg(list1, N):\n    final_list = []\n    if len(list1) > N:\n        for i in range(0, N): \n            max1 = 0\n\n            for j in range(len(list1)):     \n                if list1[j] > max1:\n                    max1 = list1[j];\n\n            list1.remove(max1);\n            final_list.append(max1)\n    else:\n        final_list = list1\n          \n    return sum(final_list)\/len(final_list)\n\ndef avg_len_n_sents(x, n = 1,punct = False):\n    sents_len = []\n    for sent in x.sents:\n        sent_len = 0\n        for word in sent:\n            if word.is_punct == False:\n                sent_len += 1\n        \n        sents_len.append(sent_len)  \n    return n_max_elements_avg(sents_len, n)\n\n","f82ce90d":"fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(7,11), constrained_layout = True)\n# plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n\nfor n in range(6):\n    col = str(n+1)+\"_sent_avg\"\n    sorted_train_data[col] = sorted_train_data.parsed_excerpt.apply(lambda x : avg_len_n_sents(x, n = n+1))\n\n    sorted_train_data.plot.scatter(x = \"target\", y = col, title = \\\n                                   \"top {} len, corr = {}\"\\\n                                   .format(n+1, sorted_train_data[\"target\"].corr(sorted_train_data[col])), ax = axes[int(n\/2),n%2])\n","34aa1a71":"# noun verb sentences\ndef get_filtered_text(word, tags):\n#     print(word, tags, word.pos_)\n    if tags != []:\n        if word.pos_ in tags:\n             return word.text\n        else:\n            return \"\" \n    else:\n        return word.text\n    \ndef generate_word_cloud_input(spacy_excerpt,tags = []):\n    word_cloud_input = \"\"\n    for sentence in spacy_excerpt:\n        for word in sentence:\n            filtered_text = get_filtered_text(word,tags)\n#             print(filtered_text)\n            if is_useful_word(word) and filtered_text != \"\":\n                word_cloud_input += filtered_text + \" \"\n    return word_cloud_input\n                \n                ","63e19cda":"fig = plt.figure(figsize = (15, 15), facecolor = None)\nwordcloud_obj = WordCloud(width = 800, height = 800,\n                    background_color ='white',\n                    min_font_size = 10)\n\nPOS = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\nfor i,pos in enumerate(POS):\n    word_cloud = wordcloud_obj.generate(generate_word_cloud_input(train_data[\"parsed_excerpt\"], [pos]))\n    ax = fig.add_subplot(2,2,i+1)  \n    ax.imshow(word_cloud)\n    plt.title(\"Word Cloud for {}\".format(pos),fontsize=20)\n\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 10)\nplt.show()\n","ebfadbc3":"from nltk import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport gensim\n\ndef spacy_POS_to_wordnet_POS(nltk_tag):\n    if nltk_tag == 'ADJ':\n        return wordnet.ADJ\n    elif nltk_tag == 'VERB':\n        return wordnet.VERB\n    elif nltk_tag == 'NOUN':\n        return wordnet.NOUN\n    elif nltk_tag == 'ADV':\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \nlemmentizer = WordNetLemmatizer()\n\ntopic_modelling_docs = []\nfor i, sentence in enumerate(train_data[\"parsed_excerpt\"]):\n    topic_modelling_docs.append([])\n    for word in sentence:\n        if is_useful_word(word):\n            topic_modelling_docs[i].append(\\\n                        lemmentizer.lemmatize(word.text, pos = spacy_POS_to_wordnet_POS(word.pos_)))            ","8ab0ddca":"from gensim.models import TfidfModel,LdaMulticore\nfrom gensim.corpora import Dictionary\n\nnum_topics = 5\ndct = Dictionary(topic_modelling_docs)\nword_freq_corpus = [dct.doc2bow(doc) for doc in topic_modelling_docs]\nmodel = TfidfModel(word_freq_corpus)\ntfidf_corpus = [model[word_freq_sentence] for word_freq_sentence in word_freq_corpus]\nlda_model = LdaMulticore(tfidf_corpus, num_topics=num_topics, id2word=dct, passes=2, workers=2)","f9e071e5":"for topic_num in range(num_topics):\n    topic_words = [dct[id_] for id_, weight in lda_model.get_topic_terms(topic_num, topn=15)]\n    print(color.BOLD, \"Topic {} Top Words : \".format(topic_num), color.END, \", \".join(topic_words))","4da32833":"# train_data[\"punctuation_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : sum([word.is_punct for word in x]))\n# train_data[\"starting_pos\"] = train_data[\"parsed_excerpt\"].apply(lambda x : x[0].pos_)","ded80ca2":"pos_list = ['ADJ','ADP','ADV','AUX','CCONJ','DET','NOUN','NUM','PART','PRON','PROPN','PUNCT','SCONJ','VERB']\n\nfor pos in pos_list:\n    train_data[\"sents_starting_with_{}\".format(pos)] = train_data[\"parsed_excerpt\"].apply(lambda x : x[0].pos_ == pos if len(x) > 0 else \"null\")\n    train_data[\"{}_count\".format(pos)] = train_data[\"parsed_excerpt\"].apply(lambda x : sum([word.pos_ == pos for word in x]))\n\n","e832008e":"pos_list = [\"NOUN\", \"VERB\",'ADJ','ADV',\"PRON\",\"SCONJ\",\"PROPN\",\"PUNCT\",'AUX','CCONJ','DET','NUM']\npos_count_cols = []\nfor pos in pos_list:\n    pos_count_cols.append(\"{}_count\".format(pos))\n\nfig, ax =plt.subplots(4,3, figsize=(15,20))\n\nfor i,col in enumerate(pos_count_cols):\n    s = sns.boxplot(x=\"target_bins\", y = col, data=train_data, ax = ax[int(i\/3),i%3], order = [\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n    s.set_title(\"{} vs difficulty\".format(col))\n    s.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n    \n\n\nplt.tight_layout(pad = 2)\nfig.show()\n\n","ece7eb79":"train_data[\"word_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : len(x))\ntrain_data[\"sent_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : len((list(x.sents))))","6a1b341a":"fig, ax =plt.subplots(1,2, figsize=(10,5))\ns = sns.boxplot(x=\"target_bins\", y=\"word_count\", data=train_data, ax = ax[0])\ns.set_title(\"Word Count vs Difficulty\")\ns.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n\ns = sns.boxplot(x=\"target_bins\", y=\"sent_count\", data=train_data, ax = ax[1])\ns.set_title(\"Sentence Count vs Difficulty\")\ns.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n\n\nplt.tight_layout(pad = 4)\nfig.show()\n","54c92bc2":"def syllable_count(word):\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count\n\ndef get_avg_syllabel(doc):\n    syllabel_sum = 0 \n    for word in doc:\n        syllabel_sum += syllable_count(word.text)\n    return syllabel_sum\/len(doc)\n\ntrain_data[\"avg_syllabel_len\"] = train_data[\"parsed_excerpt\"].apply(get_avg_syllabel)\ns = sns.boxplot(x=\"target_bins\", y=\"avg_syllabel_len\", data=train_data)\ns.set_title(\"Average Syllabel Count vs Difficulty\")\nfig.show()\n\n","0a323cc1":"train_data[\"question_mark_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : sum([word.text == \"?\" for word in x]))\ntrain_data[\"excalmation_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : sum([word.text == \"!\" for word in x]))\ntrain_data[\"comma_count\"] = train_data[\"parsed_excerpt\"].apply(lambda x : sum([word.text == \",\" for word in x]))","c6850af5":"fig, ax =plt.subplots(1,3, figsize=(20,7))\ns = sns.violinplot(x=\"target_bins\", y=\"question_mark_count\", data=train_data, ax = ax[0])\ns.set_title(\"? mark vs Difficulty\")\ns.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n\ns = sns.violinplot(x=\"target_bins\", y=\"excalmation_count\", data=train_data, ax = ax[1])\ns.set_title(\"! mark vs Difficulty\")\ns.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n\ns = sns.violinplot(x=\"target_bins\", y=\"comma_count\", data=train_data, ax = ax[2])\ns.set_title(\"Comma vs Difficulty\")\ns.set_xticklabels(rotation=30, labels=[\"very_difficult\", \"difficult\",\"medium\", \"easy\" ,\"very_easy\"])\n\nplt.tight_layout(pad = 2)\nfig.show()\n","81b82cee":"train_data[\"unique_words\"] = train_data[\"parsed_excerpt\"].apply(lambda x : len(set([word.text for word in x])))","4e704118":"plt.figure(figsize=(8,4)) # this creates a figure 8 inch wide, 4 inch high\ns = sns.violinplot(x=\"target_bins\", y=\"unique_words\", data=train_data)\ns.set_title(\"Unique Words vs Difficulty\")\nplt.show()\n","8adb792c":"reading_tests = {\n    \"flesch_reading_ease\" : textstat.flesch_reading_ease,\n    \"smog_index\" : textstat.smog_index,\n    \"flesch_kincaid_grade\" : textstat.flesch_kincaid_grade,\n    \"coleman_liau_index\" : textstat.coleman_liau_index,\n    \"automated_readability_index\" : textstat.automated_readability_index,\n#     \"dale_chall_readability_score\" : textstat.dale_chall_readability_score # this test take a lot of time to compute\n}\n\nfor test, test_func in reading_tests.items():\n    print(\"running {} test\".format(test))\n    train_data[test] = train_data[\"parsed_excerpt\"].apply(lambda x : test_func(x.text))\n","65c59a29":"sorted_train_data = train_data[[\"parsed_excerpt\"]+list(reading_tests)+[\"target\"]].sort_values(\"target\")\nsorted_train_data","f1e9225d":"def print_scores(data_tuple):\n    print(color.BOLD,\"Given Score : \", color.END)\n    print(\"\\tTarget : \", data_tuple.target)\n    print(color.BOLD, \"Test Scores : \", color.END)\n    print(\"\\tflesch_reading_ease : \\t\\t\", data_tuple.flesch_reading_ease)\n    print(\"\\tsmog_index : \\t\\t\\t\", data_tuple.smog_index)\n    print(\"\\tflesch_kincaid_grade : \\t\\t\", data_tuple.flesch_kincaid_grade)\n    print(\"\\tcoleman_liau_index : \\t\\t\", data_tuple.coleman_liau_index)\n    print(\"\\tautomated_readability_index : \\t\", data_tuple.automated_readability_index)\n    print(color.BOLD, \"Original Text : \", color.END)\n    print(\"\\tText\", data_tuple.parsed_excerpt.text)\n    \n","b4c4e65b":"print_scores(sorted_train_data.iloc[20])\n","0ba04c50":"print_scores(sorted_train_data.iloc[500])\n","1df64ad7":"print_scores(sorted_train_data.iloc[1002])\n","fd4a20dd":"print_scores(sorted_train_data.iloc[1700])\n","4f697d8e":"print_scores(sorted_train_data.iloc[2534])\n","fe765c71":"corr = train_data[list(reading_tests.keys())+[\"target\"]].corr()\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","09c8fa56":"- I was unable to observe any specific topics based on the topic words from LDA\n\n> \ud83d\udccc I would be glad if you can __point me out in the comments if there are any hidden topics__ in these topic words. Also you can tweak some parameters of LDA and see if you can find any topics\n\n- Find the documentation on LDA parameters [here](https:\/\/radimrehurek.com\/gensim\/models\/ldamulticore.html)\n","f21a24f1":"# Unique words plot","d5aedd9d":"### Randomly check scores for different excerpts\n","f88f5b3f":"### Sort dataframe based on target score","4848a01e":"## Preprocess and store enriched form of text using Spacy\n","57cbf0a4":"![](https:\/\/i1.wp.com\/www.josephineelia.com\/wp-content\/uploads\/2016\/01\/the_art_of_reading.jpg?resize=1080%2C675&ssl=1)","a2f863c8":"## Plotting punctuations count","dd4a530e":"## Read and Have a quick view over data \ud83d\udc40 ","c50b76c9":"# Overview\n\nThis notebook is a attempt to do data exploration and understand what makes a document difficult to read. Apart form data exploration I tried to do topic modelling and understand if there are any specific topics which are difficult to read. \n\nthere are several aspects which makes reading a document difficult. Most important of them are\n1. parts of speech \n2. rarity of word \n\n## Parts of speech\n- Data analysis on POS tags of the excerpts revealed the significance of pos tags in readability\n- for instance consider these sentence \n    - \"I went to shop to buy soap\"\n    - \"I went to shop to buy nivea soap\"\n    - \"I quickly went to shop to buy white nivea soap\"\n- Althought above three sentences mention same thing adding more POS tags will add more details in a single sentnece andmakes it difficult to read\n- In the same context we are going to see various patterns in POS tags and analyse what make a sentence less readable.\n\n## Rarity of word\n- since less frequently used words are difficult to read. They play an important role in determining the readability of excerpts\n- consider the word __\"puny\"__ and the word __\"tiny\"__. Althought these two words have same meaning using tiny will make a sentence easy to read than using __puny__\n\n## Other aspects of text\n- Apart from these there are several other attributes of text which also contribute to the readability\n- Features like\n    - Average syllabel count in each word\n    - Uniqueness of word\n    - Number of pnctuations\n    - word count\n    - sentence sount\n    - unique words count\n    - puncttuations counts\n- Understanding how these attributes of text correlate to the readability will help in better understanding of given data\n\n\n\n__So why late, Lets jump into data exploration__\n\n\n","374b32b6":"Here we can observe the there is a correlation of __-0.29__ between vocab score and target value.\n\n<div class=\"alert success-alert\">\n    \n    This means that tougher words will increase the toughness of reading\n    \n<\/div>","62f52fd6":"# CommonLit Readability Prize","81147cca":"# Flesch reading score Index meaning\n![](https:\/\/www.researchgate.net\/profile\/Anealka-Hussin\/publication\/329921610\/figure\/tbl1\/AS:708098725531652@1545835300816\/Descriptive-Categories-used-in-the-Flesch-Reading-Ease-Formula.png)\n\n> \ud83d\udccc Except _flesch_reading_ease_ score for _all other scores indices_ smaller score means easy to read","c0d71c75":"## Average Syllabel count\n- we can observe that the average syllabel count is higher for tougher excerpts\n","6293c239":"Below data frame has the frequency of different words\n> __Assumption__ : Difficult words make it difficult to read the excerpts","a773dab2":"# Standard reading tests","13fe12ba":"### Correlation plot for standard tests & target","2b4de681":"## Visualising POS tags and their relation to readability\n- In below plots we can observe that more nuons and verbs will be present in a tougher excerpt\n- Also less number of verbs will be present in a toucher sentence\n- Hence we can clearly inder that more number of nouns and verbs and less number of verbs in a sentence could lead to dificult to read excerpts\n    - for instance we can write a sentence like this\n         - \"_jack went to shopping. jill went to fishing_\". obeserve there are two verbs.\n         - we can write the same sentence above in more compact way like. \"jack and jill went to shopping and fishing respectively\".\n    - Hence more compact sentneces possibly have fewer verbs and more nouns and make it difficult to read\n- In SCONJ plot we can observe that very simple sentences will have fewer conjunctions. Indeed it is true that distinct sentences are comparatively easy to read than long sentences joined by conjunctions.\n\n> \ud83d\udccc There is alot going on in below plots and I may not be able to point out everything. You can always explore more and post your observetions in the comments","ad98129d":"## Does difficult words makes it difficult to read ?","ef36f033":"### Box plots on average word and sentence count in a excerpt","c61f1d62":"# Topics of the excerpts\n\n- Below word clouds show various POS tags and their respective word clouds\n- Observing carefully we can understand that there are no diversified topics in the text because almost all the word clouds talk about mundane things that arepresent in everyday life. So no special topics like science, healthcare etc\n- As much as i understand these excerpts are about short stories for kids","4ba86186":"# topic modelling using LDA\n- Below topics modelling shows ","21e4b805":"### Convert the readability score (target) column into bins\n- Lets break the readability scoresinto groups for simplicity\n    1. very_difficult\n    2. difficult\n    3. medium\n    4. easy\n    5. very_easy","6b651737":"# Import Libraries \ud83d\udcd5","971ae812":"# Does longer sentence make it difficult to read ?\n\n- Below plots show the plots of average length of top n sentences\n- we can observe a negative correlation because longer sentences are toughrs to read\n- However average length of top 3 sentences have highest negative correlation. Maybe top 3 sentences length average is a good approximation to meature the toughness of readability\n"}}