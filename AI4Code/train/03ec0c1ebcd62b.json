{"cell_type":{"09583de2":"code","033d2b56":"code","063030e3":"code","5527853b":"code","521a9a1d":"code","2fb65e1e":"code","9a99012f":"code","3f65cd5c":"code","1df9477b":"code","c5b44939":"code","8a4f6241":"code","b9b5130f":"markdown","3b270e7f":"markdown","23d1bf2e":"markdown","73505832":"markdown","68289808":"markdown","0f0b0807":"markdown","ac817ed5":"markdown","c1b09062":"markdown"},"source":{"09583de2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport csv\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","033d2b56":"#Made By: Eduardo Bourget\n\n#Get the data given \ntraining_file = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/train.csv')\ntest_file = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/test.csv')\n\ndata_file = training_file.copy()\n\n#How does the data actually good\ndata_file.head()","063030e3":"#Drop the ID part as it is not needed\n\ndata_file.drop('id', axis =1 , inplace = True)\ntest_file.drop('id', axis =1 , inplace = True)","5527853b":"#Now, get the X and Y features that represent the data that we are looking for \n#In this case we look for the bankrupt part \n\nX_train = data_file.loc[:, data_file.columns!='Bankrupt']\ny_train = data_file.loc[:,'Bankrupt']","521a9a1d":"#Plot on a hist chart\nplt.hist(data_file.iloc[:,-1])","2fb65e1e":"#checking by boxplots\n\ntraining_describe_std = training_file.describe().loc['std',:]\nex_coles = training_describe_std[training_describe_std>10000].index.values\nregular_coles = [col for col in training_file.columns[:-1] if col not in ex_coles]\n\n#Now plot it\ntraining_file.loc[:,regular_coles].boxplot()","9a99012f":"#check for NULL values\n\ndf = pd.DataFrame(data_file)\ncount = len(df.notnull())\nreal = len(data_file)\nleft = real - count\n\n\nprint('There are ' + str(count) + ' out of ' + str(real) + ' features in this CSV File!')\nprint('And there are ' + str(left) + ' that are NULL or mising in this CSV File' )","3f65cd5c":"#begin with model\n\ndTree = DecisionTreeClassifier(criterion= 'entropy', max_depth=7)\ndTree.fit(X_train, y_train)\n\ndTreePredict = dTree.predict_proba(test_file)","1df9477b":"#Scores and confusion matrix for the decision tree model\n\nXtrain_predict = dTree.predict(X_train)\n\nprint('F1 Score: ' + str(metrics.f1_score(y_train, Xtrain_predict)))\nprint('AUC Score: ' + str(metrics.roc_auc_score(y_train, Xtrain_predict)))\nprint('Accuracy Score: ' + str(metrics.accuracy_score(y_train, Xtrain_predict)))\n\n#Matrix\nprint()\nprint(confusion_matrix(y_train, Xtrain_predict))\nprint(classification_report(y_train, Xtrain_predict))\n\n#make a chart representing the ROC Curve\nfpr, tpr, _ =metrics.roc_curve(y_train, Xtrain_predict)\naucChart1 = metrics.roc_auc_score(y_train, Xtrain_predict)\nplt.plot([0,1], [0,1], 'r--')\nplt.plot(fpr, tpr, label = 'AUC for Chart = %0.3f' %aucChart1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend\n\n\n#submit\nprint()\nprint(\"Submitting Results...\")\noutput = pd.DataFrame({'Bankrupt' : dTreePredict[:,1]})\noutput.to_csv('DecisionTree_Prediction.csv', index=True, index_label = 'id')\nprint(\"Submission complete!\")\n\n#End model","c5b44939":"#Begin with Random Forest Model\n\nrForest = RandomForestClassifier(criterion= 'entropy', max_depth=8, verbose = 2)\nrForest.fit(X_train, y_train)\n\nrForest_predict = rForest.predict_proba(test_file)","8a4f6241":"#Scores and Confusion matrix for the decision tree model\n\n\nforestPredict = rForest.predict(X_train)\n\nprint('F1 Score: ' + str(metrics.f1_score(y_train, forestPredict)))\nprint('AUC Score: ' + str(metrics.roc_auc_score(y_train, forestPredict))) \nprint('Accuracy Score: ' + str(metrics.accuracy_score(y_train, forestPredict)))\n\n\n#print matrix\nprint() \nprint(confusion_matrix(y_train, forestPredict))\n\nprint(classification_report(y_train, Xtrain_predict))\n\n#make a chart representing the ROC Curve\nfpr, tpr, _ =metrics.roc_curve(y_train, forestPredict)\naucChart1 = metrics.roc_auc_score(y_train, forestPredict)\nplt.plot([0,1], [0,1], 'r--')\nplt.plot(fpr, tpr, label = 'AUC for Chart = %0.3f' %aucChart1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend\n\n\n#submit\nprint()\nprint(\"Submitting Results... \")\noutput = pd.DataFrame({'Bankrupt' : rForest_predict[:,1]}) \noutput.to_csv('RandomForest_Prediction.csv', index=True, index_label = 'id')\nprint(\"Submission complete!\")\n\n#End the Random Forest Model","b9b5130f":"# **Now we should see how the original data is represented and look for outliers**\n\n","3b270e7f":"# Standardize or Normalize the data? \n\nAs Standardizing assumes that the data has a Gaussian bell curve distribution, I wouldn't touch that without having that said information explicitly \n\nNow for normalizing the data, I tried some code for this in my own enviroment and it did not give me as good results as what I have now. Maybe some other code might help\nbut I would just go with the data provided ","23d1bf2e":"The score is not that bad! But lets see how this model compares with the Random Forests!\n\n# Random Forest Model","73505832":"Now we can see that we dont have any missing data and proceed to the actual models","68289808":"# Begin with the Decision Tree Model","0f0b0807":"**This makes it seem like the data does not have outliers at all as the data is not that far off from each other. But we can check in another way...**","ac817ed5":"# In this chart we can see that we dont have any noticable outliers as they would simply appear out of the line \n# But we can check directly if there is any NULL data","c1b09062":"Clearly we can see that the random forests is the best in this case. But that could greatly vary if the data had any outliers or if the data was more manipulated "}}