{"cell_type":{"383eaf3e":"code","28c132ea":"code","ad7a7aad":"code","3f73e2cb":"code","e4a680ed":"code","f5908882":"code","26f0176f":"code","16cb30ef":"code","dab54480":"code","d5933091":"code","939bb7c4":"code","d17a154e":"code","e98044b9":"code","131fd3c6":"code","ea70438f":"code","4273d891":"code","30f318c9":"code","db56d5e7":"code","8b5cc154":"code","0ca44a63":"code","53dd8bd8":"code","69a59c68":"code","edde7530":"code","29c08ab3":"code","ff00901d":"code","1baa86b7":"code","f1720095":"code","4096fff0":"code","94a497b9":"code","e79c6111":"code","33fc7520":"code","50412b86":"code","e7e2e1e1":"code","9c89e777":"code","6025d17c":"code","6bc53169":"code","a7ca9f09":"code","bf7d9871":"code","4ae1f00a":"code","efd5a466":"code","15bf5a02":"code","99d6c133":"code","2e8a334f":"code","62a16572":"code","50d4d310":"code","1fd40ae0":"code","e8a0a38b":"code","b1efac2b":"code","e9b15623":"code","726632e0":"code","e21535f8":"code","e012652d":"code","f76769ee":"code","fbabff43":"code","155c1410":"code","f2ca5621":"code","c95ffb90":"code","0ef5443f":"code","9bea2062":"code","edc968fe":"code","7043688f":"code","0761df4f":"code","919c0087":"code","2e7ee1ec":"code","4730594c":"code","934fdcfe":"code","fb869174":"code","862701a0":"code","67384cfe":"code","b043f069":"code","7e30b79d":"code","b5f7af15":"code","e9bf2f43":"code","186423a6":"code","44e488aa":"code","f4178ad8":"code","b624d5cf":"code","3ab897a2":"code","6190a816":"code","b834b067":"code","b6fe7167":"code","9a82bb8c":"code","6a5f2251":"code","b6d8ed4c":"code","03a7a6da":"code","bd7fc237":"code","c834f941":"code","a04c6ee7":"code","a0c59d1f":"code","ae1844a2":"code","2098a363":"markdown","dffa4f6d":"markdown","0afe9190":"markdown","0421f168":"markdown","ee243841":"markdown","48f373b1":"markdown","fb5c55a2":"markdown","5288774a":"markdown","450255ee":"markdown","06873a9b":"markdown","1c83d3d4":"markdown","4cfc9f7b":"markdown","5f55a43a":"markdown","07cad703":"markdown","f9aa52ad":"markdown","8798f71c":"markdown","d04b2d10":"markdown","35050e4c":"markdown","67e756f5":"markdown","27b39419":"markdown","21c96551":"markdown","ee8e5e52":"markdown","aba33380":"markdown","b60e5783":"markdown","0a4fe9bd":"markdown","8779fe86":"markdown","878c9de6":"markdown","d2171b79":"markdown","6a41c98e":"markdown","60f7ba70":"markdown","8ff742ec":"markdown","9f7e4c23":"markdown","a8202372":"markdown","600d331a":"markdown","e0251c16":"markdown","45240985":"markdown","8dd930fc":"markdown","a3c1b487":"markdown","4be60fef":"markdown","95757556":"markdown","b2c380c0":"markdown","4d6e6c51":"markdown","8b45dd3f":"markdown","9ca480a8":"markdown","788ab608":"markdown","0dce9114":"markdown","a3f21353":"markdown","04a7ad5b":"markdown","41fa022f":"markdown","b0ea5a2f":"markdown","b839a90a":"markdown","7d0f7754":"markdown","ee238e0e":"markdown","dc03f23e":"markdown","6be35dd8":"markdown","f5448616":"markdown","1c0c6e94":"markdown","6c5a48ce":"markdown","76c51a45":"markdown"},"source":{"383eaf3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly import subplots, tools\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode, plot, iplot\n \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom pathlib import Path\nimport glob\nimport random\nimport itertools\nfrom itertools import islice\nfrom operator import itemgetter\nimport networkx as nx\nfrom networkx.algorithms.community.centrality import girvan_newman\n\nsns.set_theme(style=\"whitegrid\")\nsns.color_palette(\"cubehelix\")\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","28c132ea":"def show_values_on_bars(labels, axs, h_v=\"v\", space=0.4, labelsign=\"\"):\n    \"\"\"\n        annotates a bar chart\n        \n        Arguments\n        ----------\n        labels: a list of texts to show with each bar\n        axs: matplotlib axis\n        h_v: orientation (h: horizontal, v: vertical)\n        space: space before the label\n        labelsign: label suffix (default: '')\n    \"\"\"\n    \n    def _show_on_single_plot(ax):\n        i = 0\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = \"{:.1f}{}\".format(labels[i], labelsign) ##int(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\", va=\"top\", fontsize=11, fontweight=\"semibold\") \n                i += 1\n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - float(space)\n                value = \"{:.1f}{}\".format(labels[i], labelsign) ##int(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\", va=\"center\", fontsize=11, fontweight=\"semibold\")\n                i += 1\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","ad7a7aad":"def getdata(dfl, colname, colmap=None, encode=True, dropcols=[\"None\", \"OTHER\"]):\n    \"\"\"\n        returns one or more columns from the given dataframe\n        \n        Arguments\n        ----------\n        dfl: a dataframe\n        colname: name or prefix of the column\n        colmap: mapping of the column names (default = None)\n        encode: encode the column values or not (default = True)\n        dropcols: column names to drop (default = [\"None\", \"OTHER\"])\n    \"\"\"\n    \n    le = preprocessing.LabelEncoder()\n    cols = dfl.columns[ dfl.columns.str.startswith(colname) ].to_list()\n    data = dfl.loc[1:, cols]\n    \n    if colmap and len(colmap) > 0:\n        data = data.rename(columns=colmap)\n    \n    if encode == True:\n#         data = data.apply(le.fit_transform)\n        data = data.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n        data = (data == 0) * 1\n    \n    if dropcols and len(dropcols) > 0:\n        data.drop(columns=dropcols, axis=1, inplace=True)\n    \n    return data","3f73e2cb":"def get_cluster(dfl):\n    \"\"\"\n        returns a cluster name based on Python\/R\/MATLAB expertise and data analysis\n        \n        Arguments\n        ----------\n        dfl: a dataframe\n    \"\"\"\n    \n    col1 = \"Python_R_MATLAB\"\n    col2 = \"Analyze and understand data\"\n    \n    if dfl[col1] > 0 and dfl[col2] > 0:\n        return 'Python_R_MATLAB & Analysis'\n    elif dfl[col1] > 0 and dfl[col2] == 0:\n        return 'Lack Analysis'\n    elif dfl[col1] == 0 and dfl[col2] > 0:\n        return 'Lack Python_R_MATLAB'\n    else:\n        return 'Lack Both'","e4a680ed":"def show_bar(grp, xcol, ycol, labelcol, xlabel, ylabel, title, labelsign=\"\", figsize=(10, 8)):\n    \"\"\"\n        draws a labelled bar chart (orientation: horizontal)\n        \n        Arguments\n        ----------\n        grp: dataframe for plotting\n        xcol: x-axis column name\n        ycol: y-axis column name\n        labelcol: label column name\n        xlabel: x-axis label of the plot\n        ylabel: y-axis label of the plot\n        title: title of the plot\n        labelsign: suffix of the label text (default: '')\n        figsize: dimension of the matplotlib plot (default: (10, 8))\n    \"\"\"\n    \n    sns.set_theme(style=\"whitegrid\")\n    fig = plt.figure(figsize=figsize)\n\n    ax = sns.barplot( x=xcol\n                     ,y=ycol\n                     ,data=grp\n                     ,orient=\"h\"\n                     ,palette=\"cubehelix\"\n                   )\n\n    if labelcol and len(labelcol) > 0:\n        show_values_on_bars(grp.loc[:, labelcol].to_list(), ax, \"h\", 0.3, labelsign=labelsign)\n        \n    plt.ylabel(ylabel, fontweight=\"normal\")\n    plt.xlabel(xlabel, fontweight=\"normal\")\n    plt.title(title, fontweight=\"normal\")\n\n    plt.show()","f5908882":"def aggregate(dfl, groupbycols, colname, agg=\"count\"):\n    \"\"\"\n        returns a dataframe with an aggregated measure grouped by one or more columns\n        \n        Arguments\n        ----------\n        dfl: a dataframe\n        groupbycols: an array of one or more column name(s)\n        colname: a column name on which the aggregation to be performed\n        agg: the name of the aggregated measure (default: count)\n        \n        Return Values\n        -------------\n        grouped: a grouped dataframe with the aggregated measure and percent columns\n    \"\"\"\n    \n    grouped = dfl.groupby(groupbycols)[colname].agg([agg])\\\n                                               .reset_index()\\\n                                               .sort_values(by=[agg], ascending=False)\n    \n    grouped.rename(columns={colname: agg}, inplace=True)\n    total = (grouped[agg]).sum()\n    grouped[\"percent\"] = grouped[agg] * 100. \/ total\n    \n    return grouped\n\ndef count_and_show(dfl, groupbycols, colname, ycol, xlabel, ylabel, title, labelsign=\"\", figsize=(10, 8)):\n    \"\"\"\n      shows a count plot of the dataframe\n      \n      Arguments\n      ----------\n      dfl: a dataframe\n      groupbycols: an array of one or more column name(s)\n      colname: a column name on which the aggregation to be performed\n      ycol: y-axis column name\n      xlabel: x-axis label of the plot\n      ylabel: y-axis label of the plot\n      title: title of the plot\n      labelsign: suffix of the label text (default: '')\n      figsize: dimension of the matplotlib plot (default: (10, 8))\n    \"\"\"\n    \n    xcol=\"count\"\n    \n    grouped = aggregate(dfl\n                       ,groupbycols\n                       ,colname\n                       )\n\n    print( grouped.head() )\n    \n    show_bar(grp=grouped\n            ,xcol=xcol\n            ,ycol=ycol\n            ,labelcol=\"percent\"\n            ,xlabel=xlabel\n            ,ylabel=ylabel\n            ,title=title\n            ,labelsign=labelsign\n            ,figsize=figsize\n            )","26f0176f":"def get_joint_prob(dfg, groupbycols, colname):\n    \"\"\"\n        returns the joint probability distributions of the columns in a dataframe\n        \n        Arguments\n        ---------\n        dfg: an encoded dataframe (0-1 format)\n        groupbycols: an array of one or more column name(s)\n        colname: a column name on which the aggregation to be performed\n        \n        Return Values\n        -------------\n        grouped: a dataframe with the joint probabilities of the column combinations in dfg\n        p: a column in grouped specifying the joint probability of the column value combinations\n        rowsum: a column in grouped specifying how many columns are on (i.e., 1) in each column value combination\n    \"\"\"\n    grouped = dfg.groupby(groupbycols)[colname].count().reset_index()\n    grouped.rename(columns={colname: \"count\"}, inplace=True)\n    grouped[\"p\"] = grouped[\"count\"] \/ grouped[\"count\"].sum()\n    grouped[\"rowsum\"] = grouped[groupbycols].apply(sum, axis=1)\n    \n    return grouped\n\ndef get_adjacency_list(dfg, cols):\n    \"\"\"\n        returns the adjacency list of the dataframe\n        \n        Arguments\n        ---------\n        dfg: an encoded dataframe (0-1 format)\n        cols: an array of selected column names\n        \n        Return Value\n        ------------\n        df_edges: a dataframe with ['Source', 'Target', 'Weight'] columns\n        Source: the name of the source column\n        Target: the name of the target column\n        Weight: the joint probability of Source and Target occurring together\n    \"\"\"\n    ## https:\/\/stackoverflow.com\/questions\/28253779\/python-pandas-setting-dataframe-index-and-column-names-from-an-array\n    length = len(cols)\n    df_edges = pd.DataFrame(np.arange(length * length).reshape(length, length))\n    df_edges.index = cols\n    df_edges.columns = cols\n    df_edges.loc[:, :] = 0\n    \n    df_edges = df_edges.stack().reset_index()\n    df_edges.columns = ['Source', 'Target', 'Weight']\n    \n    df_edges['Weight'] = df_edges[['Source', 'Target']].apply(lambda x: get_weight(dfg, x['Source'], x['Target'], 'p'), axis=1)\n    \n    return df_edges\n\ndef get_weight(dfw, source, target, weight='p'):\n    \"\"\"\n        returns the joint probability of source and target in dfw\n        \n        Arguments\n        ---------\n        dfw: a dataframe with column p (i.e., probability), source and target\n        source: name of the source node\n        target: name of the target node\n        weight: the probability of source and target happening together\n    \"\"\"\n    if source == target:\n       return dfw.loc[ dfw[source] == 1, weight].sum()\n    else:\n        return dfw.loc[ (dfw[source] == 1) & (dfw[target] == 1) ,weight].sum()\n    \ndef get_importance(df_edgelist):\n    \"\"\"\n        returns the normalized marginal probability of each node\n        \n        Arguments\n        ---------\n        df_edgelist: a dataframe in adjacency list format [Source, Target, Weight] columns\n        \n        Return Value\n        ------------\n        Importance: a column, specifying the relative value of each node with respect to the max weight\n    \"\"\"\n    df_imp = df_edgelist[ df_edgelist[\"Source\"] == df_edgelist[\"Target\"] ]\n    df_imp[\"Importance\"] = df_imp[\"Weight\"] * 100. \/ df_imp[\"Weight\"].max()\n    df_imp = df_imp.sort_values(by=[\"Importance\"], ascending=False)\n    return df_imp\n    \ndef get_pair_importance(df_edgelist):\n    \"\"\"\n        returns the pairwise normalized probability of nodes\n        \n        Arguments\n        ---------\n        df_edgelist: a dataframe in adjacency list format [Source, Target, Weight] columns\n        \n        Return Value\n        ------------\n        Importance: a column, specifying the relative value of a nodepair or an edge with respect to the max edge-weight\n    \"\"\"\n    df_imp = get_importance(df_edgelist)\n    dictw = df_imp[[\"Source\", \"Importance\"]].set_index(\"Source\")[\"Importance\"].to_dict()\n    \n    df_joint_imp = df_edgelist[ (df_edgelist[\"Source\"] != df_edgelist[\"Target\"]) ]\n    df_joint_imp[\"Importance\"] = df_joint_imp[\"Weight\"] * 100. \/ df_joint_imp[\"Weight\"].max()\n    df_joint_imp[\"Displacement\"] = df_joint_imp[[\"Source\", \"Target\"]].apply(lambda x: dictw[x[\"Target\"]] - dictw[x[\"Source\"]], axis=1)\n    df_joint_imp[\"SortedPairName\"] = df_joint_imp[[\"Source\", \"Target\"]].apply(lambda x: \"_\".join(sorted([x[\"Source\"], x[\"Target\"]])), axis=1)\n    \n    df_joint_imp = df_joint_imp.sort_values(by=[\"Displacement\"], ascending=True) ## sort so that the most important source in (source, target) remains at the top\n    df_joint_imp.drop_duplicates(subset=[\"SortedPairName\"], keep=\"first\", inplace=True) ## keep the pair with most important source\n     \n    df_joint_imp[\"Pair\"] = df_joint_imp[[\"Source\", \"Target\"]].apply(lambda x: \"( {}, {} )\".format(x[\"Source\"], x[\"Target\"]), axis=1 )\n    df_joint_imp = df_joint_imp.sort_values(by=[\"Importance\"], ascending=False)\n    \n    return df_joint_imp\n\ndef sum_and_percent(dfg, groupbycols, colname, sort=False):\n    \"\"\"\n        returns a dataframe summed by one or more columns\n        \n        Arguments\n        ----------\n        dfl: a dataframe\n        groupbycols: an array of one or more column name(s)\n        colname: a column name on which the summation is to be performed\n        sort: sort the group or not (default: False)\n        \n        Return Value\n        ------------\n        group: a grouped dataframe with sum and percent columns\n    \"\"\"\n    group = dfg.groupby(groupbycols)[colname].sum().reset_index()\n\n    group[\"percent\"] = group[colname] * 100. \/ group[colname].sum()\n    \n    if sort == True:\n        group = group.sort_values(by=[\"percent\"], ascending=False)\n        \n    return group","16cb30ef":"# generate random colors\ncolors_ = lambda n: list(map(lambda i: \"#\" + \"%06x\" % random.randint(0, 0xFFFFFF),range(n)))\n\ndef heaviest(G):\n    \"\"\"\n        returns the max weight edge in graph G\n\n        Argument\n        --------\n        G: a networkx graph\n    \"\"\"\n    u, v, w = max(G.edges(data=\"weight\"), key=itemgetter(2))\n\n    return (u, v)\n\ndef graph(nodelist, edgelist):\n    \"\"\"\n        return a networkx graph and edge weights\n        \n        Arguments\n        ---------\n        nodelist: a list of nodes\n        edgelist: a list of weighted edges\n    \"\"\"\n    G = nx.Graph()\n    G.add_nodes_from(nodelist)\n    G.add_weighted_edges_from(edgelist)\n\n    edge_labels=dict( [((u,v,), d['weight'])\n                      for u,v,d in G.edges(data=True)] )\n    \n    return G, edge_labels\n\n## https:\/\/stackoverflow.com\/questions\/952914\/how-to-make-a-flat-list-out-of-a-list-of-lists\ndef get_partition(G, k):\n    \"\"\"\n        returns a partition of graph G with k influencers' ecosystem separated out from the rest\n        \n        Arguments\n        ---------\n        G: a networkx graph\n        k: number of influencing nodes\n        \n        Return Values\n        -------------\n        a dictionary: (key, value) -> (node_name, group_name), group_name=0 is the dominant group and group_name=1 is the dominated group\n        an array: two groups of node names \n    \"\"\"\n    communities = girvan_newman(G, most_valuable_edge=heaviest) \n    limited = itertools.takewhile(lambda c: len(c) > 1, communities)\n\n    for com in limited:\n        node_groups = [sorted(c) for c in com]\n        group_0 = list(filter(lambda a: len(a) == 1, node_groups))\n        \n        if len(group_0) == k:\n            group_0 = [x[0] for x in group_0]\n            group_1 = list(filter(lambda a: len(a) > 1, node_groups))\n            group_1 = np.concatenate(group_1).tolist()\n            partition = [(n, 0) for n in group_0]\n            partition += [(n, 1) for n in group_1]\n            \n            return dict(partition), [group_0, group_1]\n            \n## finds cluster of nodes with highest joint probabilities and dominance\ndef find_cluster_dominance(nodelist, edgelist, cluster_size=3):\n    \"\"\"\n        returns a segregated graph with two clusters, namely, dominant and dominated\n        \n        Arguments\n        ---------\n        nodelist: a list of node names\n        edgelist: a list of edges\n        cluster_size: number of dominants or influencers \n        \n        Return Values\n        -------------\n        G: a networkx graph\n        partition: a dict of (node_name, group_name), group_name=0 is the dominant\n        colors: color code of the nodes\n    \"\"\"\n#     color_map = [\"#CCE5FF\", \"#A9A1E5\"] \n    color_map = [\"#CCE5FF\", \"lavenderblush\"] \n    G, edge_labels = graph(nodelist, edgelist)\n    \n    values = list(G.nodes())\n    colors = [color_map[0]] * len(values)\n    \n    if cluster_size >= len(nodelist) - 1:\n       partition = dict([(n, 0) for n in values])\n       return G, partition, colors \n    \n    partition, group = get_partition(G, k=cluster_size) \n    \n    for n in group[1]: \n        colors[ values.index(n) ] = color_map[1]\n            \n    for u, v, d in edgelist:   \n        g_u = partition[u]\n        g_v = partition[v]\n        if g_u != g_v :\n            G.remove_edge( u, v )\n        \n    return G, partition, colors  \n\n## https:\/\/stackoverflow.com\/questions\/14283341\/how-to-increase-node-spacing-for-networkx-spring-layout\ndef draw_network(G, pos, nodecolors, mapp=None, zoom=1, random_seed=42):\n    \"\"\"\n        draws a networkx graph\n        \n        Arguments\n        ---------\n        G: a networkx graph\n        pos: position of the nodes in the graph\n        nodecolors: a list of colors for each node\n        mapp: shorthand names of the node labels (default=None)\n        zoom: size of the nodes (default=1)\n        random_seed: random seed for reproducing the layout of the graph (default=42)\n    \"\"\"\n    sns.set_style(\"white\")\n    fig = plt.figure(figsize=(16, 12))\n    # build a rectangle in axes coords\n    left, width = .25, .5\n    bottom, height = .25, .5\n    right = left + width\n    top = bottom + height\n\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    # axes coordinates: (0, 0) is bottom left and (1, 1) is upper right\n    p = patches.Rectangle(\n        (left, bottom), width, height,\n        fill=False, transform=ax.transAxes, clip_on=False\n        )\n\n    ax.add_patch(p)\n\n    # Need to create a layout when doing\n    # separate calls to draw nodes and edges\n    \n#     pos = nx.spring_layout(G, scale=2, k=0.15, iterations=2, seed=42)\n#     pos = nx.spring_layout(G, scale=2, k=0.15, iterations=1, seed=random_seed)\n    \n    nx.draw_networkx_nodes(G, pos, cmap=plt.get_cmap('jet'), edgecolors = \"k\", linewidths = [2],\n                           node_color = nodecolors, node_shape=\"o\", node_size = 5000 * zoom\n                          ) ## \"#CCE5FF\", #184070\n    \n    if mapp and len(mapp) > 0:\n        nx.draw_networkx_labels(G, pos, labels=mapp, font_color=\"#184973\", font_weight='bold', font_size=13)\n    else:\n        nx.draw_networkx_labels(G, pos, font_color=\"#184973\", font_weight='bold', font_size=13)\n        \n    nx.draw_networkx_edges(G, pos, width=2)\n    edge_labels = nx.get_edge_attributes(G,'weight')\n    nx.draw_networkx_edge_labels(G, pos, edge_labels = edge_labels, font_color=\"#184973\", font_weight='bold', font_size=13)\n    \n    ax.text(  0.5*(left+right), 0.5*(bottom+top), \"influences\"\n            , ha=\"center\", va=\"center\", rotation=0, size=20\n            , bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"white\", ec=\"b\", lw=2)\n            , transform=ax.transAxes)\n    \n    plt.show()\n    \n    \ndef find_and_draw_cluster_dominance(nodelist, edgelist, mapp=None, cluster_size=3, zoom=1, random_seed=42):\n    \"\"\"\n        finds out the influencers in a graph\n        \n        Arguments\n        ---------\n        nodelist: a list of node names\n        edgelist: a list of edges\n        mapp: shorthand names of the labels (default=None)\n        cluster_size: number of dominants or influencers (default=3)\n        zoom: size of the nodes (default=1)\n        random_seed: random seed for reproducing the layout of the graph (default=42)\n    \"\"\"\n    \n    g, partition, nodecolors = find_cluster_dominance(nodelist, edgelist, cluster_size)\n    pos = community_layout(g, partition, random_seed=random_seed)\n\n    draw_network(g, pos, nodecolors, mapp=mapp, zoom=zoom, random_seed=random_seed)","dab54480":"# https:\/\/stackoverflow.com\/questions\/43541376\/how-to-draw-communities-with-networkx\ndef community_layout(g, partition, random_seed=42):\n    \"\"\"\n    Compute the layout for a modular graph.\n\n\n    Arguments:\n    ----------\n    g -- networkx.Graph or networkx.DiGraph instance\n        graph to plot\n\n    partition -- dict mapping int node -> int community\n        graph partitions\n\n\n    Returns:\n    --------\n    pos -- dict mapping int node -> (float x, float y)\n        node positions\n\n    \"\"\"\n\n    pos_communities = _position_communities(g, partition, scale=2., seed=random_seed)\n\n    pos_nodes = _position_nodes(g, partition, scale=1., seed=random_seed)\n\n    # combine positions\n    pos = dict()\n    for node in g.nodes():\n        pos[node] = pos_communities[node] + pos_nodes[node]\n\n    return pos\n\ndef _position_communities(g, partition, **kwargs):\n\n    # create a weighted graph, in which each node corresponds to a community,\n    # and each edge weight to the number of edges between communities\n    between_community_edges = _find_between_community_edges(g, partition)\n\n    communities = set(partition.values())\n    hypergraph = nx.DiGraph()\n    hypergraph.add_nodes_from(communities)\n    for (ci, cj), edges in between_community_edges.items():\n        hypergraph.add_edge(ci, cj, weight=len(edges))\n\n    # find layout for communities\n    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n\n    # set node positions to position of community\n    pos = dict()\n    for node, community in partition.items():\n        pos[node] = pos_communities[community]\n\n    return pos\n\ndef _find_between_community_edges(g, partition):\n\n    edges = dict()\n\n    for (ni, nj) in g.edges():\n        ci = partition[ni]\n        cj = partition[nj]\n\n        if ci != cj:\n            try:\n                edges[(ci, cj)] += [(ni, nj)]\n            except KeyError:\n                edges[(ci, cj)] = [(ni, nj)]\n\n    return edges\n\ndef _position_nodes(g, partition, **kwargs):\n    \"\"\"\n    Positions nodes within communities.\n    \"\"\"\n\n    communities = dict()\n    for node, community in partition.items():\n        try:\n            communities[community] += [node]\n        except KeyError:\n            communities[community] = [node]\n\n    pos = dict()\n    for ci, nodes in communities.items():\n        subgraph = g.subgraph(nodes)\n        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n        pos.update(pos_subgraph)\n\n    return pos\n\ndef test(nodelist, edgelist, cluster_size, random_seed=42):\n    g, partition, nodecolors = find_cluster_dominance(nodelist, edgelist, cluster_size)\n    pos = community_layout(g, partition, random_seed=random_seed)\n\n    draw_network(g, pos, nodecolors, mapp=None, random_seed=random_seed)\n    return","d5933091":"map_language = {  'Q7_Part_1': 'Python'\n                , 'Q7_Part_2': 'R'\n                , 'Q7_Part_3': 'SQL'\n                , 'Q7_Part_4': 'C'\n                , 'Q7_Part_5': 'C++'\n                , 'Q7_Part_6': 'Java'\n                , 'Q7_Part_7': 'Javascript'\n                , 'Q7_Part_8': 'Julia'\n                , 'Q7_Part_9': 'Swift'\n                , 'Q7_Part_10': 'Bash'\n                , 'Q7_Part_11': 'MATLAB'\n                , 'Q7_Part_12': 'None'\n                , 'Q7_OTHER': 'OTHER'\n               }\n\nmap_viztools = {  'Q14_Part_1': 'Matplotlib'\n                , 'Q14_Part_2': 'Seaborn'\n                , 'Q14_Part_3': 'Plotly \/ Plotly Express'\n                , 'Q14_Part_4': 'Ggplot \/ ggplot2'\n                , 'Q14_Part_5': 'Shiny'\n                , 'Q14_Part_6': 'D3js'\n                , 'Q14_Part_7': 'Altair'\n                , 'Q14_Part_8': 'Bokeh'\n                , 'Q14_Part_9': 'Geoplotlib'\n                , 'Q14_Part_10': 'Leaflet \/ Folium'\n                , 'Q14_Part_11': 'None'\n                , 'Q14_OTHER': 'OTHER'\n               }\n\nmap_mlalgorithm = { \"Q17_Part_1\": \"Linear or Logistic Regression\"\n                   ,\"Q17_Part_2\": \"Decision Trees or Random Forests\"\n                   ,\"Q17_Part_3\": \"Gradient Boosting Machines (xgboost, lightgbm, etc)\"\n                   ,\"Q17_Part_4\": \"Bayesian Approaches\"\n                   ,\"Q17_Part_5\": \"Evolutionary Approaches\"\n                   ,\"Q17_Part_6\": \"Dense Neural Networks (MLPs, etc)\"\n                   ,\"Q17_Part_7\": \"Convolutional Neural Networks\"\n                   ,\"Q17_Part_8\": \"Generative Adversarial Networks\"\n                   ,\"Q17_Part_9\": \"Recurrent Neural Networks\"\n                   ,\"Q17_Part_10\": \"Transformer Networks (BERT, gpt-3, etc)\"\n                   ,\"Q17_Part_11\": \"None\"\n                   ,\"Q17_OTHER\": \"OTHER\"\n                  }\n\nmap_mlframework = { \"Q16_Part_1\": \"Scikit-learn\"\n                   ,\"Q16_Part_2\": \"TensorFlow\"\n                   ,\"Q16_Part_3\": \"Keras\"\n                   ,\"Q16_Part_4\": \"PyTorch\"\n                   ,\"Q16_Part_5\": \"Fast.ai\"\n                   ,\"Q16_Part_6\": \"MXNet\"\n                   ,\"Q16_Part_7\": \"Xgboost\"\n                   ,\"Q16_Part_8\": \"LightGBM\"\n                   ,\"Q16_Part_9\": \"CatBoost\"\n                   ,\"Q16_Part_10\": \"Prophet\"\n                   ,\"Q16_Part_11\": \"H2O.3\"\n                   ,\"Q16_Part_12\": \"Caret\"\n                   ,\"Q16_Part_13\": \"Tidymodels\"\n                   ,\"Q16_Part_14\": \"JAX\"\n                   ,\"Q16_Part_15\": \"PyTorch Lightning\"\n                   ,\"Q16_Part_16\": \"Huggingface\"\n                   ,\"Q16_Part_17\": \"None\"\n                   ,\"Q16_OTHER\": \"OTHER\"\n                  }\n\nmap_activity = { \"Q24_Part_1\": \"Analyze and understand data\"\n                ,\"Q24_Part_2\": \"Build and\/or run the data infrastructure\"\n                ,\"Q24_Part_3\": \"Build prototypes\"\n                ,\"Q24_Part_4\": \"Build and\/or run a machine learning service\"\n                ,\"Q24_Part_5\": \"Experimentation and iteration of ML methods\"\n                ,\"Q24_Part_6\": \"Research ML\"\n                ,\"Q24_Part_7\": \"None\"\n                ,\"Q24_OTHER\": \"OTHER\"\n              }\n\nmap_learning = { \"Q40_Part_1\": \"Coursera\"\n               , \"Q40_Part_2\": \"edX\"\n               , \"Q40_Part_3\": \"Kaggle Learn Courses\"\n               , \"Q40_Part_4\": \"DataCamp\"\n               , \"Q40_Part_5\": \"Fast.ai\"\n               , \"Q40_Part_6\": \"Udacity\"\n               , \"Q40_Part_7\": \"Udemy\"\n               , \"Q40_Part_8\": \"LinkedIn Learning\"\n               , \"Q40_Part_9\": \"Cloud-certification (AWS, Azure, GCP, or similar)\"\n               , \"Q40_Part_10\": \"University Courses\"\n               , \"Q40_Part_11\": \"None\"\n               , \"Q40_OTHER\": \"OTHER\"\n              }","939bb7c4":"viztools_short = { 'Matplotlib': 'Matplotlib'\n                 ,'Seaborn': 'Seaborn'\n                 ,'Plotly \/ Plotly Express': 'Plotly'\n                 ,'Ggplot \/ ggplot2': 'Ggplot'\n                 ,'Shiny': 'Shiny'\n                 ,'D3js': 'D3js'\n                 ,'Altair': 'Altair'\n                 ,'Bokeh': 'Bokeh'\n                 ,'Geoplotlib': 'Geoplotlib'\n                 ,'Leaflet \/ Folium': 'Leaflet'\n                }\n\nactivity_short = {\"Analyze and understand data\": \"Analysis\"\n                  ,\"Build and\/or run the data infrastructure\": \"Infrastructure\"\n                  ,\"Build prototypes\": \"Prototype\"\n                  ,\"Build and\/or run a machine learning service\": \"ML Service\"\n                  ,\"Experimentation and iteration of ML methods\": \"Experiment ML\"\n                  ,\"Research ML\": \"Research ML\"\n                 }\n\nmlalgo_short = {\"Linear or Logistic Regression\": \"Lin\/Log Regression\"\n                ,\"Decision Trees or Random Forests\": \"Decision Trees\"\n                ,\"Gradient Boosting Machines (xgboost, lightgbm, etc)\": \"Gradient Boost\"\n                ,\"Bayesian Approaches\": \"Bayesian\"\n                ,\"Convolutional Neural Networks\": \"CNN\"\n                ,\"Dense Neural Networks (MLPs, etc)\": \"Dense NN\"\n                ,\"Evolutionary Approaches\": \"Evolution\"\n                ,\"Generative Adversarial Networks\": \"GAN\"\n                ,\"Recurrent Neural Networks\": \"RNN\"\n                ,\"Transformer Networks (BERT, gpt-3, etc)\": \"Transformer\"\n               }\n\nlearning_short = {\"Coursera\": \"Coursera\"\n                  ,\"edX\": \"edX\"\n                  ,\"Kaggle Learn Courses\": \"Kaggle\"\n                  ,\"DataCamp\": \"DataCamp\"\n                  ,\"Fast.ai\": \"Fast.ai\"\n                  ,\"Udacity\": \"Udacity\"\n                  ,\"Udemy\": \"Udemy\"\n                  ,\"LinkedIn Learning\": \"LinkedIn\"\n                  ,\"Cloud-certification (AWS, Azure, GCP, or similar)\": \"Cloud-cert\"\n                  ,\"University Courses\": \"University\"\n                }","d17a154e":"path = Path(\"\/kaggle\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv\")\n# path = Path(\"..\/kaggle\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv\")\ndf_responses = pd.read_csv(path)\ndf_responses.head(3)","e98044b9":"print(\"dataframe shape: \", df_responses.shape)\nprint()\nprint(\"dataframe columns: \", df_responses.columns.to_list()[:5])","131fd3c6":"df_prog = getdata(  dfl=df_responses\n                  , colname=\"Q7\"\n                  , colmap=map_language\n                  , encode=True\n                  , dropcols=[\"None\", \"OTHER\"]\n                 )\n\ndf_prog[\"Python_R_MATLAB\"] = df_prog[[\"Python\", \"R\", \"MATLAB\"]].apply(sum, axis=1)\n\nprint(\"df_prog shape: \", df_prog.shape)\n\ndf_prog.head()","ea70438f":"df_activity = getdata(  dfl=df_responses\n                      , colname=\"Q24\"\n                      , colmap=map_activity\n                      , encode=True\n                      , dropcols=[\"None\", \"OTHER\"]\n                     )\nprint(\"df_activity shape: \", df_activity.shape)\n\ndf_activity.head()","4273d891":"df = pd.concat([df_prog[\"Python_R_MATLAB\"], df_activity[\"Analyze and understand data\"]], axis=1)\ndf[\"DS\"] = ((df[\"Python_R_MATLAB\"] > 0) & (df[\"Analyze and understand data\"] > 0)) * 1\ndf.head()","30f318c9":"print(\"Data Science Workforce: Count: {}, Percent: {:.2f}%\".format( (df[\"DS\"] > 0).sum(), (df[\"DS\"] > 0).sum() * 100. \/ df.shape[0]))","db56d5e7":"df[\"cluster\"] = df.apply(get_cluster, axis=1)\ndf.head()","8b5cc154":"count_and_show( dfl=df\n               ,groupbycols=[\"cluster\"]\n               ,colname=\"DS\"\n               ,ycol=\"cluster\"\n               ,xlabel=\"Occurrence\"\n               ,ylabel=\"Cluster Type\"\n               ,title=\"Cluster Type vs. Occurrence (Entire Workforce)\"\n               ,labelsign=\"%\"\n               ,figsize=(6, 4)\n              )","0ca44a63":"cols = ['Analyze and understand data',\n        'Build and\/or run the data infrastructure', \n        'Build prototypes',\n        'Build and\/or run a machine learning service',\n        'Experimentation and iteration of ML methods', \n        'Research ML'\n       ]\n\ndf_enthus = pd.concat([df[[\"DS\", \"cluster\"]], df_activity], axis=1)\ndf_enthus = df_enthus.loc[(df_enthus[\"cluster\"] == \"Lack Analysis\"), cols ]\ndf_enthus[\"DS_Activity\"] = ((df_enthus['Build prototypes'] == 1) | \n                            (df_enthus['Experimentation and iteration of ML methods'] == 1) |\n                            (df_enthus['Research ML'] == 1)) * 1.\n\ndf_enthus[\"NonDS_Activity\"] = ( (df_enthus['Build and\/or run the data infrastructure'] == 1) |\n                                (df_enthus['Build and\/or run a machine learning service'] == 1)) * 1.\n\n# https:\/\/stackoverflow.com\/questions\/29960733\/how-to-convert-true-false-values-in-dataframe-as-1-for-true-and-0-for-false\nvar_on_off = {0: 'Off' , 1: 'On' }\ndf_enthus[\"DS_Activity\"] = df_enthus[\"DS_Activity\"].map(var_on_off)\ndf_enthus[\"NonDS_Activity\"] = df_enthus[\"NonDS_Activity\"].map(var_on_off)\n\ncount_and_show( dfl=df_enthus[[\"DS_Activity\", \"NonDS_Activity\"]]\n               ,groupbycols=[\"DS_Activity\"]\n               ,colname=\"NonDS_Activity\"\n               ,ycol=\"DS_Activity\"\n               ,xlabel=\"Occurrence\"\n               ,ylabel=\"DS Activity\"\n               ,title=\"DS Activity vs. Occurrence (Data Enthusiasts)\"\n               ,labelsign=\"%\"\n               ,figsize=(6, 2)\n              )","53dd8bd8":"df_job = getdata( dfl=df_responses\n                , colname=\"Q5\"\n                , colmap=None\n                , encode=False\n                , dropcols=None\n               )\n\ndf_job.head()","69a59c68":"df = pd.concat([df, df_job], axis=1)\ndf.head()","edde7530":"title = [\"Data Scientist\", \"Data Analyst\"]\n\ncount_and_show(  df[df[\"Q5\"].isin(title)]\n                ,groupbycols=[\"cluster\"]\n                ,colname=\"DS\"\n                ,ycol=\"cluster\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Cluster Type\"\n                ,title=\"Cluster Type vs. Occurrence (Job Title: Data Scientist & Data Analyst)\"\n                ,labelsign=\"%\"\n                ,figsize=(6, 4)\n              )","29c08ab3":"df_industry = getdata( dfl=df_responses\n                     , colname=\"Q20\"\n                     , colmap=None\n                     , encode=False\n                     , dropcols=None\n                    )\n\ndf_industry = df_industry.loc[:, \"Q20\"]\ndf_industry.head()","ff00901d":"df = pd.concat([df, df_industry], axis=1)\ndf.head()","1baa86b7":"count_and_show(  df[df[\"DS\"] > 0]\n                ,groupbycols=[\"Q20\"]\n                ,colname=\"DS\"\n                ,ycol=\"Q20\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Industry\"\n                ,title=\"Industry vs. Occurrence (Data Science Workforce)\"\n                ,labelsign=\"%\"\n                ,figsize=(10, 12)\n              )","f1720095":"count_and_show(  df[df[\"DS\"] > 0]\n                ,groupbycols=[\"Q5\"]\n                ,colname=\"DS\"\n                ,ycol=\"Q5\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Occupation\"\n                ,title=\"Occupation vs. Occurrence (Data Science Workforce)\"\n                ,labelsign=\"%\"\n                ,figsize=(10, 10)\n              )","4096fff0":"df_edu = getdata( dfl=df_responses\n                , colname=\"Q4\"\n                , colmap=None\n                , encode=False\n                , dropcols=None\n               )\n\ndf_edu = df_edu.loc[:, \"Q4\"]\ndf_edu.head()","94a497b9":"df = pd.concat([df, df_edu], axis=1)\ndf.head()","e79c6111":"count_and_show(  df[df[\"DS\"] > 0]\n                ,groupbycols=[\"Q4\"]\n                ,colname=\"DS\"\n                ,ycol=\"Q4\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Education\"\n                ,title=\"Education vs. Occurrence (Data Science Workforce)\"\n                ,labelsign=\"%\"\n                ,figsize=(10, 6)\n              )","33fc7520":"df_progexp = getdata( dfl=df_responses\n                    , colname=\"Q6\"\n                    , colmap=None\n                    , encode=False\n                    , dropcols=None\n                   )\n\ndf_progexp.head()","50412b86":"df = pd.concat([df, df_progexp], axis=1)\ndf.head()","e7e2e1e1":"count_and_show(  df[df[\"DS\"] > 0]\n                ,groupbycols=[\"Q6\"]\n                ,colname=\"DS\"\n                ,ycol=\"Q6\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Programming Experience\"\n                ,title=\"Programming Experience vs. Occurrence (Data Science Workforce)\"\n                ,labelsign=\"%\"\n                ,figsize=(8, 4)\n              )","9c89e777":"df_mlexp = getdata( dfl=df_responses\n                  , colname=\"Q15\"\n                  , colmap=None\n                  , encode=False\n                  , dropcols=None\n                 )\n\ndf_mlexp.head()","6025d17c":"df = pd.concat([df, df_mlexp], axis=1)\ndf.head()","6bc53169":"count_and_show(  df[df[\"DS\"] > 0]\n                ,groupbycols=[\"Q15\"]\n                ,colname=\"DS\"\n                ,ycol=\"Q15\"\n                ,xlabel=\"Occurrence\"\n                ,ylabel=\"Machine Learning Experience\"\n                ,title=\"Machine Learning Experience vs. Occurrence (Data Science Workforce)\"\n                ,labelsign=\"%\"\n                ,figsize=(8, 6)\n              )","a7ca9f09":"df.head()","bf7d9871":"cols = [ \"Python\", \"R\", \"SQL\"\n        , \"C\", \"C++\", \"Java\"\n        , \"Javascript\", \"Julia\", \"Swift\"\n        , \"Bash\", \"MATLAB\" ] \n\ndf_lang = pd.concat([df_prog[cols], df[\"DS\"]], axis=1)\ndf_lang.head()","4ae1f00a":"grouped = get_joint_prob( df_lang.loc[df_lang[\"DS\"] > 0, :]\n                        , cols\n                        , \"DS\"\n                       )\n\ngrouped.head()","efd5a466":"grouped_langs = sum_and_percent(  dfg=grouped\n                                , groupbycols=[\"rowsum\"]\n                                , colname=\"count\"\n                               )\n\ngrouped_langs.head()","15bf5a02":"show_bar(  grp=grouped_langs\n         , xcol=\"count\"\n         , ycol=\"rowsum\"\n         , labelcol=\"percent\"\n         , xlabel=\"Occurrence\"\n         , ylabel=\"Languages\"\n         , title=\"Languages vs. Occurrence (Data Science Workforce)\"\n         , labelsign=\"%\"\n         , figsize=(10, 8)\n        )","99d6c133":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","2e8a334f":"df_imp = get_importance(df_edges)\ndf_imp.head()","62a16572":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Language\"\n         , title=\"Language vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 8)\n        )","50d4d310":"df_joint_imp = get_pair_importance(df_edges)\ndf_joint_imp.head()","1fd40ae0":"show_bar(  grp=df_joint_imp[:10]\n         , xcol=\"Importance\"\n         , ycol=\"Pair\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Language Pair\"\n         , title=\"Language Pair vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 8)\n        )","e8a0a38b":"edgelist = list( df_joint_imp.loc[  df_joint_imp[\"Importance\"] > 0\n                                  , [\"Source\", \"Target\", \"Importance\"]].apply(lambda x: (x[\"Source\"], x[\"Target\"], round(x[\"Importance\"], 1)), axis=1) )\nnodelist = df_joint_imp[\"Source\"].unique()\n\n# edgelist[:4], nodelist\nfind_and_draw_cluster_dominance(nodelist, edgelist, mapp=None, cluster_size=3, zoom=1.2, random_seed=41)","b1efac2b":"print(\"(Python, SQL, Javascript): {:.2f}%\".format( grouped.loc[((grouped[\"Python\"] == 1) & (grouped[\"SQL\"] == 1) & (grouped[\"Javascript\"] == 1)), \"p\"].sum() * 100.))\n\nprint(\"(Python, SQL, R): {:.2f}%\".format( grouped.loc[((grouped[\"Python\"] == 1) & (grouped[\"SQL\"] == 1) & (grouped[\"R\"] == 1)), \"p\"].sum() * 100.))","e9b15623":"df_viz = getdata( dfl=df_responses\n                , colname=\"Q14\"\n                , colmap=map_viztools\n                , encode=True\n                , dropcols=[\"None\", \"OTHER\"]\n               )\n\ndf_viz = pd.concat([df_viz, df_prog[[\"Python\", \"R\"]], df[\"DS\"]], axis=1)\ndf_viz.head()","726632e0":"df_viz.shape","e21535f8":"# ((df_viz[\"DS\"] > 0) & ((df_viz[\"Python\"] > 0) | (df_viz[\"R\"] > 0))).sum()\n(df_viz[\"DS\"] > 0).sum()","e012652d":"cols = ['Matplotlib', 'Seaborn', 'Plotly \/ Plotly Express',\n       'Ggplot \/ ggplot2', 'Shiny', 'D3js', 'Altair', 'Bokeh',\n       'Geoplotlib', 'Leaflet \/ Folium']\n\ngrouped = get_joint_prob( df_viz.loc[(df_viz[\"DS\"] > 0), :]\n                        , cols\n                        , \"DS\"\n                        )\n\ngrouped.head()","f76769ee":"grouped_vizlibs = sum_and_percent( grouped\n                                 , [\"rowsum\"]\n                                 , \"count\"\n                                 )\n\ngrouped_vizlibs.head()","fbabff43":"show_bar(  grp=grouped_vizlibs\n         , xcol=\"count\"\n         , ycol=\"rowsum\"\n         , labelcol=\"percent\"\n         , xlabel=\"Occurrence\"\n         , ylabel=\"Visualization Libraries\"\n         , title=\"Visualization Libraries vs. Occurrence (Data Science Workforce)\"\n         , labelsign=\"%\"\n         , figsize=(10, 8)\n        )","155c1410":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","f2ca5621":"df_imp = get_importance(df_edges)\ndf_imp.head()","c95ffb90":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Language\"\n         , title=\"Language vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 8)\n        )","0ef5443f":"df_joint_imp = get_pair_importance(df_edgelist=df_edges)\ndf_joint_imp.head()","9bea2062":"show_bar(  grp=df_joint_imp[:10]\n         , xcol=\"Importance\"\n         , ycol=\"Pair\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Visualization Lib Pair\"\n         , title=\"Visualization Lib Pair vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 8)\n        )","edc968fe":"edgelist = list( df_joint_imp.loc[  df_joint_imp[\"Importance\"] > 0\n                                  , [\"Source\", \"Target\", \"Importance\"]].apply(lambda x: (x[\"Source\"], x[\"Target\"], round(x[\"Importance\"], 1)), axis=1) )\n\nnodelist = df_joint_imp[\"Source\"].unique()\n\nfind_and_draw_cluster_dominance(nodelist, edgelist, mapp=viztools_short, cluster_size=3, zoom=1.2, random_seed=41)","7043688f":"df_mlalgo = getdata(  dfl=df_responses\n                    , colname=\"Q17\"\n                    , colmap=map_mlalgorithm\n                    , encode=True\n                    , dropcols=[\"None\", \"OTHER\"]\n                   )\n\ndf_mlalgo = pd.concat([df_mlalgo, df[\"DS\"]], axis=1)\ndf_mlalgo.head()","0761df4f":"cols = ['Linear or Logistic Regression',\n       'Decision Trees or Random Forests',\n       'Gradient Boosting Machines (xgboost, lightgbm, etc)',\n       'Bayesian Approaches', 'Evolutionary Approaches',\n       'Dense Neural Networks (MLPs, etc)',\n       'Convolutional Neural Networks', 'Generative Adversarial Networks',\n       'Recurrent Neural Networks',\n       'Transformer Networks (BERT, gpt-3, etc)'\n       ]\n\ngrouped = get_joint_prob( df_mlalgo.loc[(df_mlalgo[\"DS\"] > 0), :]\n                        , cols\n                        , \"DS\"\n                        )\n\ngrouped.head()","919c0087":"grouped_mlalgo = sum_and_percent(grouped\n                                ,groupbycols=[\"rowsum\"]\n                                ,colname=\"count\"\n                                )\ngrouped_mlalgo.head()","2e7ee1ec":"show_bar(  grp=grouped_mlalgo\n         , xcol=\"count\"\n         , ycol=\"rowsum\"\n         , labelcol=\"percent\"\n         , xlabel=\"Occurrence\"\n         , ylabel=\"ML Algorithm\"\n         , title=\"ML Algorithm vs. Occurrence (Data Science Workforce)\"\n         , labelsign=\"%\"\n         , figsize=(10, 8)\n        )","4730594c":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","934fdcfe":"df_imp = get_importance(df_edges)\ndf_imp.head()","fb869174":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"ML Algorithm\"\n         , title=\"ML Algorithm vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 8)\n        )","862701a0":"df_joint_imp = get_pair_importance(df_edges)\n\nedgelist = list( df_joint_imp.loc[  df_joint_imp[\"Importance\"] > 0\n                                  , [\"Source\", \"Target\", \"Importance\"]].apply(lambda x: (x[\"Source\"], x[\"Target\"], round(x[\"Importance\"], 1)), axis=1) )\n\nnodelist = df_joint_imp[\"Source\"].unique()\n\n# result = map(lambda a: mlalgo_short[a], nodelist)\n# labels = [val for val in result]\nfind_and_draw_cluster_dominance(nodelist, edgelist, mapp=mlalgo_short, cluster_size=3, zoom=3, random_seed=41)","67384cfe":"df_mllib = getdata(  dfl=df_responses\n                    , colname=\"Q16\"\n                    , colmap=map_mlframework\n                    , encode=True\n                    , dropcols=[\"None\", \"OTHER\"]\n                   )\n\ndf_mllib = pd.concat([df_mllib, df[\"DS\"]], axis=1)\ndf_mllib.head()","b043f069":"cols = ['Scikit-learn', 'TensorFlow', 'Keras', 'PyTorch', 'Fast.ai',\n       'MXNet', 'Xgboost', 'LightGBM', 'CatBoost', 'Prophet', 'H2O.3',\n       'Caret', 'Tidymodels', 'JAX', 'PyTorch Lightning', 'Huggingface']\n\ngrouped = get_joint_prob( df_mllib.loc[(df_mllib[\"DS\"] > 0), :]\n                        , cols\n                        , \"DS\"\n                        )\n\ngrouped.head()","7e30b79d":"grouped_mllib = sum_and_percent(grouped\n                               ,groupbycols=[\"rowsum\"]\n                               ,colname=\"count\"\n                               )\n\ngrouped_mllib.head()","b5f7af15":"show_bar(  grp=grouped_mllib\n         , xcol=\"count\"\n         , ycol=\"rowsum\"\n         , labelcol=\"percent\"\n         , xlabel=\"Occurrence\"\n         , ylabel=\"ML Framework\"\n         , title=\"ML Framework vs. Occurrence (Data Science Workforce)\"\n         , labelsign=\"%\"\n         , figsize=(10, 12)\n        )","e9bf2f43":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","186423a6":"df_imp = get_importance(df_edges)\ndf_imp.head()","44e488aa":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"ML Framework\"\n         , title=\"ML Framework vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 12)\n        )","f4178ad8":"df_joint_imp = get_pair_importance(df_edgelist=df_edges)\nedgelist = list( df_joint_imp.loc[  df_joint_imp[\"Importance\"] > 0\n                                  , [\"Source\", \"Target\", \"Importance\"]].apply(lambda x: (x[\"Source\"], x[\"Target\"], round(x[\"Importance\"], 1)), axis=1) )\n\nnodelist = df_joint_imp[\"Source\"].unique()\n\n# edgelist[:4], nodelist\nfind_and_draw_cluster_dominance(nodelist, edgelist, mapp=None, cluster_size=3, zoom=2.2, random_seed=41)","b624d5cf":"df_activities = pd.concat([df_activity, df[\"DS\"]], axis=1)\ndf_activities.head()","3ab897a2":"cols = ['Analyze and understand data',\n        'Build and\/or run the data infrastructure', \n        'Build prototypes',\n        'Build and\/or run a machine learning service',\n        'Experimentation and iteration of ML methods', \n        'Research ML'\n       ]\n\ngrouped = get_joint_prob( df_activities.loc[(df_activities[\"DS\"] > 0), :]\n                        , cols\n                        , \"DS\"\n                        )\n\ngrouped.head()","6190a816":"grouped_activities = sum_and_percent(grouped\n                                    ,groupbycols=[\"rowsum\"]\n                                    ,colname=\"count\"\n                                    )\n\ngrouped_activities.head()","b834b067":"show_bar(  grp=grouped_activities\n         , xcol=\"count\"\n         , ycol=\"rowsum\"\n         , labelcol=\"percent\"\n         , xlabel=\"Occurrence\"\n         , ylabel=\"Activity\"\n         , title=\"Activity vs. Occurrence (Data Science Workforce)\"\n         , labelsign=\"%\"\n         , figsize=(8, 4)\n        )","b6fe7167":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","9a82bb8c":"df_imp = get_importance(df_edges)\ndf_imp.head()","6a5f2251":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Activity\"\n         , title=\"Activity vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(8, 4)\n        )","b6d8ed4c":"df_joint_imp = get_pair_importance(df_edges)\ndf_joint_imp.head()","03a7a6da":"edgelist = list( df_joint_imp.loc[  df_joint_imp[\"Importance\"] > 0\n                                  , [\"Source\", \"Target\", \"Importance\"]].apply(lambda x: (x[\"Source\"], x[\"Target\"], round(x[\"Importance\"], 1)), axis=1) )\n\nnodelist = df_joint_imp[\"Source\"].unique()\n\n# edgelist[:4], nodelist\nfind_and_draw_cluster_dominance(nodelist, edgelist, mapp=activity_short, cluster_size=3, zoom=2.4, random_seed=41)","bd7fc237":"df_learning = getdata( dfl=df_responses\n                     , colname=\"Q40\"\n                     , colmap=map_learning\n                     , encode=True\n                     , dropcols=[\"None\", \"OTHER\"]\n                    )\n\ndf_learning = pd.concat([df_learning, df[\"DS\"]], axis=1)\ndf_learning.head()","c834f941":"cols = ['Coursera', 'edX', 'Kaggle Learn Courses', 'DataCamp', 'Fast.ai',\n        'Udacity', 'Udemy', 'LinkedIn Learning',\n        'Cloud-certification (AWS, Azure, GCP, or similar)',\n        'University Courses']\n\ngrouped = get_joint_prob( df_learning.loc[(df_learning[\"DS\"] > 0), :]\n                        , cols\n                        , \"DS\"\n                        )\n\ngrouped.head()","a04c6ee7":"df_edges = get_adjacency_list(grouped, cols)\ndf_edges.head()","a0c59d1f":"df_imp = get_importance(df_edges)\ndf_imp.head()","ae1844a2":"show_bar(  grp=df_imp\n         , xcol=\"Importance\"\n         , ycol=\"Source\"\n         , labelcol=\"Importance\"\n         , xlabel=\"Relative Importance\"\n         , ylabel=\"Learn Platform\"\n         , title=\"Learn Platform vs. Relative Importance (Data Science Workforce)\"\n         , labelsign=\"\"\n         , figsize=(10, 6)\n        )","2098a363":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li> <p>The Top 3 mostly seen activities are: Analyze and understand data, Build prototypes, and Build\/run data infrastructure. <\/p><\/li>\n<\/ul>    \n<\/div>","dffa4f6d":"## <a id=\"section_dystopia\">2. Is the Data Science World in Dystopia?<\/a>\n\nWe have described the core notion of Data Science in an earlier section ([1. Data Science Definition Revisited](#section_ds_definition)). Now let's explore whether and by how much, the Kaggle Survey responses conform to it - that is, perform data analysis regularly with Analytic Programming (Python\/R\/MATLAB). It would be worthy too to perform a health-checkup of the Data Scientist\/Analyst Workforce in this data.\n\nNote that, because of our attention to the specific choices, we have decided to exclude the None and Other columns for questions with multiple eligible answers throughout our analysis.","0afe9190":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The Top 3 most popular languages are: Python, SQL and R.<\/p><\/li>\n\n<li><p> In contrast, the Top 3 most influential languages are: Python, SQL and Javascript. <\/p><\/li>\n\n<li><p> So, the question is: which 3 should we pick? The answer may vary based on personal preferences and tasks at hand. But if it is coming from an absolute beginner, we may argue as this. Functionalitywise there is no significant difference between Python and R. Therefore, picking any of these analytic programming languages is fine. SQL and Javascript pertain to two different dimensions, namely, Database and Web Scripting. So, one is supposed to get more value if the 3-pack consists of an Analytic Language, Database Programming and Web Scripting. <\/p>\n\n<p> <b>Example.<\/b> The joint probabiity of (Python, SQL, R) is 17.77%. If the R specific tasks can be done with Python and Javascript is learnt instead of R, then one can pick (Python, SQL, Javascript). So, the payoff  for picking (Python, SQL, Javascript) is: pr(Python, SQL, R) + pr(Python, SQL, Javascript) = 17.77% + 11.39% = 29.16%. This implies, the (Python, SQL, Javascript) combo can accomplish more tasks than the (Python, SQL, R) combo (29.16% with the former vs. 17.77% with the latter). Note that, Python is more influential than R in this Kaggle Survey data.<\/p> <\/li>\n\n<\/ul>\n<\/div>","0421f168":"<b>Find the most influential 3-pack cluster(s)<\/b>","ee243841":"## Department of Data-induced Maladies and Injuries\n![Kaggle_Survey_MAIN.png](attachment:9cc9f07e-cb50-49ca-810b-6e225a2eaaf0.png)","48f373b1":"<div class=\"alert alert-block alert-danger\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Only 14.8% of the Data Enthusiasts perform DS activities regularly, however, without any prior analysis and\/or understanding of data. Here DS activities refer to Building prototypes, Research and Experimentation of ML methods. <b>Seems data is in wrong hands!<\/b><\/p><\/li>\n    \n<li><p> Most of the Data Enthusiasts have been left out from actively participating in the DS activities in their regular jobs (85.2%). This triggers the question: <b> Is there an ongoing depletion in the Data Science job market? <\/b> <\/p><\/li>\n<\/ul>\n<\/div>","fb5c55a2":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n    <li><p> Top 2: Data Scientist (29.7%) and Data Analyst (17.6%) <\/p><\/li>\n    <li><p> Professionals with the DS skillset are engaged in so many interesting roles (not only Data Scientist and Data Analyst); a few of the titles somewhat capture the Data Science intuition, e.g., Machine Learning Engineer, Research Scientist, Data Engineer and Statistician.<\/p><\/li>\n<\/ul>\n<\/div>\n<div class=\"alert alert-block alert-warning\">\n<ul>\n    <li><p> However, some others seem to get masked (Managers, DBA, Software Engineer and Developer Advocate).<\/p><\/li>\n    <li><p> Threat: Job search and hiring by title may not serve the intended purpose.<\/p><\/li>\n<\/ul>\n<\/div>","5288774a":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The 3 mostly used Programming Languages are: Python, SQL and R. This implies the utmost importance of Data Extraction\/Manipulation and Analytical abilities. <p><\/li>\n\n<li><p> Others are used occasionally, maybe in conjuction with Python\/R\/SQL. <\/p><\/li>\n\n<li><p> Note that, MATLAB lies almost at the bottom of the chart, which implies it is relatively less popular. The reason behind this is MATLAB is not a freeware and therefore, is not used much in personal tasks. <\/p><\/li>\n<\/ul>\n<\/div>","450255ee":"### <a id=\"section_enthusiasts\">2.2 Data Enthusiasts: An Overlooked Thread<\/a>\n\nNow let's explore the condition of the Data Enthusiasts more closely.","06873a9b":"## <a id=\"section_resources\">Helpful Resources<\/a>\n\n1. 7 tips to make your notebook better: https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\/discussion\/279327\n\n1. Previous year winning notebooks: https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\/discussion\/278542\n\n1. 8 Popular Storytelling Themes from Past Competitions: https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\/discussion\/278727\n\n1. Tips from the (Kaggle Survey) Triple Winner. I'm a big fan of him.: https:\/\/www.kaggle.com\/c\/kaggle-survey-2021\/discussion\/278889\n\n1. A deep learning of Deep Learning: https:\/\/www.kaggle.com\/rohanrao\/a-deep-learning-of-deep-learning\n\n1. independent probability: https:\/\/stackoverflow.com\/questions\/68844193\/check-if-random-variables-are-independent-python\n\n1. labels on the graph: https:\/\/stackoverflow.com\/questions\/20133479\/how-to-draw-directed-graphs-using-networkx-in-python\n\n1. separate node labels on the graph: https:\/\/stackoverflow.com\/questions\/14665767\/networkx-specific-nodes-labeling\n\n1. draw communities: https:\/\/stackoverflow.com\/questions\/43541376\/how-to-draw-communities-with-networkx\n\n1. node shape: https:\/\/stackoverflow.com\/questions\/30344592\/networkx-how-to-change-the-shape-of-the-node\n\n1. girvan_newman: https:\/\/networkx.org\/documentation\/stable\/reference\/algorithms\/generated\/networkx.algorithms.community.centrality.girvan_newman.html\n\n1. show all columns and rows: https:\/\/stackoverflow.com\/questions\/49188960\/how-to-show-all-columns-names-on-a-large-pandas-dataframe\n\n1. columns with prefix: https:\/\/stackoverflow.com\/questions\/27275236\/pandas-best-way-to-select-all-columns-whose-names-start-with-x\n\n1. categorical axes: https:\/\/plotly.com\/python\/categorical-axes\/\n\n1. k-modes clustering: https:\/\/www.kaggle.com\/ashydv\/bank-customer-clustering-k-modes-clustering\n\n1. opportunity analysis: https:\/\/www.kaggle.com\/sadeka007\/spillography-when-freemium-gets-premium-v5-1\n\n1. outlier detection:\n    1. https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/\n\n    1. https:\/\/machinelearningmastery.com\/model-based-outlier-detection-and-removal-in-python\/\n\n    1. https:\/\/machinelearningmastery.com\/how-to-identify-outliers-in-your-data\/\n    \n1. jupyter cheatsheet: \n    1. https:\/\/www.ibm.com\/docs\/en\/db2-event-store\/2.0.0?topic=notebooks-markdown-jupyter-cheatsheet\n    \n    1. https:\/\/www.kaggle.com\/shubhamksingh\/create-beautiful-notebooks-formatting-tutorial\n    \n    1. https:\/\/github.com\/showdownjs\/showdown\/wiki\/Showdown's-Markdown-syntax\n    \n    1. https:\/\/www.w3schools.com\/html\/tryit.asp?filename=tryhtml_lists_nested\n    \n1. font: https:\/\/matplotlib.org\/stable\/tutorials\/text\/text_props.html\n\n1. default font params: https:\/\/stackoverflow.com\/questions\/3899980\/how-to-change-the-font-size-on-a-matplotlib-plot\n\n1. annotation: https:\/\/matplotlib.org\/stable\/tutorials\/text\/annotations.html#sphx-glr-tutorials-text-annotations-py\n\n1. matplotlib colors: https:\/\/stackoverflow.com\/questions\/22408237\/named-colors-in-matplotlib\n\n1. map 0-1 to values: https:\/\/stackoverflow.com\/questions\/29960733\/how-to-convert-true-false-values-in-dataframe-as-1-for-true-and-0-for-false\n\n1. figure aspect ratio: https:\/\/stackoverflow.com\/questions\/26163702\/how-to-change-figuresize-using-seaborn-factorplot","1c83d3d4":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The most popular 3-pack combo is (Matplotlib, Seaborn, Plotly). Sounds absolutely Pythonish! <\/p><\/li>\n\n<li><p> The most influential 3-pack combo is (Matplotlib, Seaborn, Shiny). Shiny is a CRAN package for generating interactive plots in R. <\/p><\/li>\n\n<li><p> Now, it is time to find out which way we should go. Note that, both probabilistic and influence based combos include  static and interactive libraries. Based on the finding in the Programming section, we have decided to pick either the Python packages or the R packages and not to jumble those up. So, Python users may pick (Matplotlib, Seaborn, Plotly) and R users may pick (Ggplot, Shiny). Note that, both Plotly and Ggplot made their positions in the Top 5 most infuential packages. <\/p><\/li>\n<\/ul>\n<\/div>","4cfc9f7b":"### <a id=\"section_mlexp\">3.5 Machine Learning Experience<\/a>\n\nLet's explore their experience in Machine Learning methods.","5f55a43a":"<div class=\"alert alert-block alert-danger\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Only 64.2% of the respondents with title Data Scientist\/Analyst conform to the basic DS concept. <\/p><\/li>\n\n<li><p> A notable fraction (27.5%) of Data Scientist\/Analyst do not have analysis in their regular activities. This group conducts DS activities without any prior analysis and\/or understanding of data. <b> Seems data is in wrong hands! <\/b> <\/p><\/li>\n\n<li><p> Others (8.4%) lack analytic programming skill. <b>An alarming situation, no doubt!<\/b> <\/p><\/li>\n<\/ul>\n<\/div>","07cad703":"## Resolve: The Data Science Dystopia\n\nData Science and Machine Learning have become an irrepressible flow in today's world. Heralded by the data-deluge and the affordable high capacity compute power, most companies are paving their ways towards unraveling actionable insights from data. Very indulging, indeed!\n\nHave you ever found yourself lost in the Data Wonderland? How did you feel while going through the survey questionairre (42 Questions)? Here's what I felt:\n\n<div style=\"text-align:center;\" class=\"alert alert-block alert-warning\"><b>\nWith langs and tools all too scary, <br>\nLearning path looks so very dreary.\n<\/b><\/div>\n    \nSo, the BIG question that arose in my mind is this: <br>\n\n<div style=\"text-align:center;\" class=\"alert alert-block alert-info\"><b>\nWith these options and tools available, are we supporting the growth or the distraction of Data Science?\n<\/b><\/div>\n\nTo put it simply: many new dimensions are being added to Data Science over the course of time; such as Cloud Computing, Big Data, ML Frameworks and Packages, to name a few. Does it help us focus on solving the real Data problems efficiently, or diverge our attention to the auxiliaries? If it is the latter, that'd mean Data Science is not serving the purpose it is intended to. This necessitates the need to conduct a health-checkup of the Data Practitioners through the lenses of the core notion of Data Science. This notebook, therefore, is an endeavor towards identifying and resolving the Data Science Dystopia based on the Kaggle Survey Responses 2021.\n \n\n### Disclaimer\n\n1. The insights and conclusions have been drawn here from the Kaggle Survey Data 2021, which may not represent the overall Data Science population. \n\n1. There is a non-zero probability that the responses of this survey may be influenced by personal biases in understanding and judgement.\n\n\n### What is trending\n\n- [1. Data Science Definition Revisited](#section_ds_definition)\n\n- [2. Is the Data Science World in Dystopia?](#section_dystopia)\n    - [2.1 Search: The Fellows of Data Science](#section_basic_analysis)\n    - [2.2 Data Enthusiasts: An Overlooked Thread](#section_enthusiasts)\n    - [2.3 Job title \"Data Scientist\/Analyst\": Compromises in Quality](#section_jobtitle_basic)\n    - [2.4 Summary: Data Science a hype, flaws in recruitment and\/or adoption?](#section_hype_flaws)\n        \n- [3. Data Science Workforce Analysis](#section_workforce)\n    - [3.1 Industry](#section_industry)\n    - [3.2 Job Title](#section_jobtitle)\n    - [3.3 Education](#section_edu)\n    - [3.4 Programming Experience](#section_progexp)\n    - [3.5 Machine Learning Experience\u00b6](#section_mlexp)\n    - [3.6 Summary: In-Demand Skillset, Job Role not Well-Defined, High Entry Bar](#section_indemand)\n    \n- [4. Induction in the Data Science Wonderland](#section_induction)\n    - [4.1 Starter Pack: A 4-piece Data Science Toolkit](#section_starter)\n    - [4.2 Activities](#section_activities)\n    - [4.3 The Data Science Witchcraft Authority](#section_authority)\n    - [4.4 Induction Summary: Strenuous Job, Online Learning Popular, Basic Toolkit](#section_induction_summary)\n      \n- [5. Closing Remarks](#section_closing)","f9aa52ad":"## <a id=\"section_load_data\">Load data<\/a>\n\nLet's load the survey data first.","8798f71c":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Both probability and influence based reasoning are in sync for ML algorithms. The Top 3 picks are: Linear or Logistic Regression, Decision Trees or Random Forests, and Gradient Boosting Machines (xgboost, lightgbm, etc). This implies the prevalence of tabular data based regression and classification tasks in the regular job. <p><\/li>\n<\/ul>\n<\/div>","d04b2d10":"## <a id=\"section_utility\">Utility Functions<\/a>","35050e4c":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p>The Top 3 most popular ML Frameworks are: Scikit-learn, Tensorflow and Keras. Xgboost is not too distant, though (4th place)!<\/p><\/li>\n\n<li><p>The Top 3 most influential Frameworks are: Scikit-learn, Tensorflow and Pytorch.<\/p><\/li>\n\n<li><p>Scikit-learn framework consists of many basic components of Machine Learning, including Linear and Logistic Regression, Decision Trees and Random Forests, etc. This is an absolutely important package for inclusion. Xgboost can be included too for more sophisticated regression and classification.<\/p><\/li>\n\n<li><p>Tensorflow, Keras and Pytorch are Deep Learning Frameworks; a nice to have skill and may be recommended for advanced learners, specifically those whose interests lie in Computer Vision and Natural Language Processing.<\/p><\/li>\n<\/ul>\n    \n<\/div>","67e756f5":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> 7.2% of the DS Workforce do not use any visualization library.<\/p><\/li>\n\n<li><p> But the common trend is to use 1-3 libraries for generating graphs.<\/p><\/li>\n<\/ul>\n<\/div>","27b39419":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The common trend requires using 1-3 languages. <\/p><\/li>\n\n<li><p> Two Programming Languages are used most often (32.6%). <\/p><\/li>\n\n<li><p> Some tasks may require a combo of 3 Languages (24.9%). <\/p><\/li>\n<\/ul>\n<\/div>","21c96551":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> About 15.1% do not use any ML Framework. <\/p><\/li>\n    \n<li><p> However, it is typical to use 1-4 ML Frameworks. <\/p><\/li>\n<\/ul>\n<\/div>","ee8e5e52":"### <a id=\"section_indemand\">3.6 Summary: In-Demand Skillset, Job Role not Well-Defined, High Entry Bar<\/a>\n<ul>\n<li> <p><b>An In-Demand Skillset.<\/b> Data Science skillset is in demand across various industries as well as academia. The Top 3 places where Data Science is practised are: Computers\/Technology (24.3%), Academics\/Education (16.4%) and Accounting\/Finance (10.2%).<\/p> <\/li>\n\n<li> <p><b>Transferrable Skills.<\/b> The Data Science Workforce looks reasonably matured in programming and machine learning. Although there is significant diversity in their assumed roles and industry verticals, these two skills appear to have gained respect over decades.<\/p><\/li>\n    \n<li> <p><b>Job Role is not Well-Defined.<\/b> However, the key vulnerability in the recruitment process is the Job Role: <b>What you see may not be what you get.<\/b> This means, the DS Workforce do not always assume the role of a Data Scientist and\/or Data Analyst. Most often the roles are hazy, such as Machine Learning Engineer, Software Engineer, Product Manager, and even surprisingly, DBA and Developer Advocate! So, job search and hiring by title may not serve the intended purpose.<\/p> <\/li>\n\n<li> <p><b>High Entry Bar.<\/b> The entry bar for joining the DS Workforce is high. A Bachelor's degree is the minimal requirement; however, a Master's degree and\/or a Doctorate can be a plus. Experience in Programming and Machine Learning is highly desired too. It is not difficult to find one with 1-3 years of coding (26.7%) and ML experience (35.7%). The quality of such experience matters too (hasn't been measured in this survey, though).<\/p> <\/li>\n\n<li> <p><b>Entry Bar Compromised.<\/b> It's been observed that a small fraction of the DS Workforce doesn't hold a Bachelor's degree (5.1%).<\/p> <\/li>\n<\/ul>","aba33380":"#### <a id=\"section_mlframe\">4.1.4 ML Framework<\/a>\n\nLet's see which Machine Learning frameworks are used most often.","b60e5783":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li> <p>The Top 3 most popular Learning Platforms are: Coursera, Kaggle Learn Courses, and Udemy. Pick the one based on your preferences, such as rating\/reviews, content quality, pacing and price. <\/p> <\/li>\n<\/ul>\n<\/div>","0a4fe9bd":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Matplotlib and Seaborn occupy the Top 2 places. <\/p><\/li>\n\n<li><p> Plotly and ggplot take the next spots. ggplot is very specific to R users, whereas plotly has its implementation in both Python and R for generating interactive plots. <\/p><\/li>\n<\/ul>\n<\/div>","8779fe86":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> (Python, SQL), (Python, R) and (SQL, R) combos received highest acceptance in the DS community. <\/p> <\/li>\n<\/ul>\n<\/div>","878c9de6":"#### <a id=\"section_programming\">4.1.1 Programming Language<\/a>\n\nLet's explore the Programming Language preference of the DS workforce.","d2171b79":"### <a id=\"section_hype_flaws\">2.4 Summary: Data Science a hype, flaws in recruitment and\/or adoption?<\/a>\n<ul>\n<li><p> Our analysis reveals the fact that only 30.92% of the Survey respondents meet the minimal Data Science requirement. This group  analyzes data with Python\/R\/MATLAB on a regular basis. <\/p><\/li>\n\n<li><p> Majority of the respondents (56.8%) have hands-on experience in analytic programming. They can be termed as Data Enthusiasts who occasionally solve data problems in Kaggle and\/or other platforms. Only 14.8% of this group perform DS activities in regular jobs, however, without any prior analysis and\/or understanding of data. Here DS activities refer to Building prototypes, Research and Experimentation of ML methods. Seems data is in real danger! This condition provokes  the questions: <\/p>\n<div class=\"alert alert-block alert-warning\"><b>\n    <ul>\n    <li><p> Is Data Science still in the hype phase? <\/p><\/li>\n    <li><p> Are there inherent flaws in the recruitment and the adoption of Data Science? <\/p><\/li>\n    <\/ul><\/b>\n<\/div> <\/li>\n\n<li><p> People with Job title \"Data Scientist and Data Analyst\" do not necessarily conform to the basic Data Science notion. Only 64.2% meet the criteria, 27.5% do not perform analysis in their jobs and 8.4% do not even have hands-on experience in analytic programming (alarming, indeed!). This conveys the message: <\/p>\n<div class=\"alert alert-block alert-danger\"><b> \n     The Data Science responsibility is very loosely (and sometimes lousily) performed.\n<\/b><\/div><\/li>\n<\/ul>","6a41c98e":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> <b>Data Scientist.<\/b> Only 30.92% of the respondents satisfy the minimal requirement (cluster: Python\/R\/MATLAB & Analysis); that is, they analyze data and extract insights with one or more analytic programming language (Python\/R\/MATLAB) in their regular jobs. <\/p><\/li>\n\n<li><p> <b>Data Enthusiast.<\/b> Majority of the respondents (56.8%) do not have analysis in their regular jobs. This group can be termed as Data Enthusiasts, who occasionally play around with the DS\/ML problems using Python\/R\/MATLAB.<\/p><\/li>\n\n<li><p> Others (12.2%) lack analytic programming skill.<\/p><\/li>\n<\/ul>\n<\/div>","60f7ba70":"## <a id=\"section_induction\">4. Induction in the Data Science Wonderland<\/a>\n\nOur analysis has given us some hope that Data Science is an in-demand skillset. It's been practised in the academia and industry for decades. The Entry Bar for joining the DS Workforce is very high. However, there are significant flaws in the recruitment and adoption of Data Science; seems the hype has not been entirely over yet. The process followed in the Data Science industry is still in the nascent stage. Saving a spot in the DS community, therefore, demands a great deal of effort. If you are really keen to join the workforce, you are welcome to go through the induction process! \n\nData Science is similar to Witchcraft! With proper induction, you can learn tools and techniques to manipulate and visualize the data, extract valuable insights and recommend viable actions to the stakeholders. This induction pamphlet is designed to get you acquainted with the regular DS activities, gift you a small starter pack to brush on the basic DS skills, and eventually connect you to the DS Witchcraft Authority so that you can learn and practise your skills in a regulated environment, and earn a certification (or magic wand!) to stand ahead in the queue. ","8ff742ec":"<b>Find the most influential 3-pack language cluster(s)<\/b>","9f7e4c23":"<b>Find the most influential 3-pack cluster(s)<\/b>","a8202372":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li> <p>The Top 3 mostly seen activities are: Analyze and understand data, Build prototypes, and Build\/run data infrastructure.<\/p> <\/li>\n\n<li> <p>On the other hand, the Top 3 most influential activities are: Analyze and understand data, Build prototypes, and Experimentation and tuning of ML methods.<\/p> <\/li>\n\n<li> <p>Having an exposure to either of these sets would be advantageous. However, the latter ones are often tightly coupled, because building prototypes requires a thorough understanding of data and experimentation\/tuning of the model parameters.<\/p> <\/li>\n<\/ul>    \n<\/div>","600d331a":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The Top 3 mostly used ML Algorithms are: Linear or Logistic Regression, Decision Trees or Random Forests, and Gradient Boosting Machines (xgboost, lightgbm, etc). <\/p><\/li>\n<\/ul>\n<\/div>","e0251c16":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Most of them are newbies in ML (29.3%). <\/p><\/li>\n\n<li><p> A very small fraction has 10+ experience in ML (4.1%). <\/p><\/li>\n\n<li><p> Some do not use Machine Learning methods (8.5%). This implies Machine Learning is not mandatory; but it is a nice to have skill and may be required to perform some of the regular tasks. <\/p><\/li>\n<\/ul>\n<\/div>","45240985":"<b>Find the most influential 3-pack cluster(s)<\/b>","8dd930fc":"### <a id=\"section_edu\">3.3 Education<\/a>\n\nLet's observe the educational qualification of the DS Workforce.","a3c1b487":"### <a id=\"section_jobtitle\">3.2 Job Title<\/a>\n\nLet's find out the usual Job Title of the DS Workforce.","4be60fef":"### <a id=\"section_activities\">4.2 Activities<\/a>\n\nLet's explore which activities are most common in the Data Wonderland.","95757556":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The DS Workforce looks very qualified in formal education; majority (47.8%) holds a Master's degree. <\/p><\/li>\n\n<li><p> A notable fraction (15.8%) holds a doctorate degree too. <\/p><\/li>\n\n<li><p> But the recommended minimum is still a Bachelor's degree (29.8%). <\/p><\/li>\n<\/ul>\n<\/div>\n<div class=\"alert alert-block alert-danger\">\n<ul>\n<li><p> Threat: A small fraction (5.1%) hasn't yet passed the minimum qualification. <b>Alarming situation, no doubt! <\/b><\/p><\/li>\n<\/ul>\n<\/div>","b2c380c0":"### <a id=\"section_basic_analysis\">2.1 Search: The Fellows of Data Science<\/a>\n\nThis section performs a health-checkup of the Kaggle respondents with respect to the core Data Science concept.","4d6e6c51":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n    <li><p> It is common to have a couple of activities in the regular job. <\/p><\/li>\n    <li><p> But some respondents reported 5-6 activities too in the regular job (16.9%). This is very overwhelming!<\/p><\/li>\n<\/ul>\n<\/div>","8b45dd3f":"## <a id=\"section_ds_definition\">1. Data Science Definition Revisited<\/a>\n\nThe web gets inundated with many exciting news, visualizations and predictions arising from the mining of massive data. The forecasts by www.fivethirtyeight.com come true like prophecies quite often; e.g., the outcome of the US Presidential Election in 2012. So, it is no wonder, a Data Admirer may want to get to know more closely about the Data Scientists, specifically, their responsibilities and qualifications. Here is a couple of definitions (quoted from: cs109, Harvard University).\n\n<div class=\"alert alert-block alert-success\">\n<b>Data Science.<\/b> To gain insights into data through computation, statistics, and visualization.\n<br>\n\n<b>Data Scientist.<\/b> A data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician.\n<br>\n    \n<b>Data Scientist<\/b> = statistician + programmer + coach + storyteller + artist.\n<br>\n<\/div>\n    \nThe more one explores the definitions in this field, the more confusions are supposed to enshroud one's mind. So, instead of making an attempt to be accurate, let's stick to our promise and define Data Science with respect to its  core aspects.\n\n<div class=\"alert alert-block alert-success\">\n<b>Data Science.<\/b> Is mainly concerned with extracting insights from data by means of statistics and programming.\n<\/div>\n\nThe inclusion of statistics and programming here is required for the sake of efficient handling of data and preferably, the huge effusion of data.","9ca480a8":"### <a id=\"section_progexp\">3.4 Programming Experience<\/a>\n\nLet's check the Programming Experience of the DS Workforce.","788ab608":"## <a id=\"section_closing\">5. Closing Remarks<\/a>\n\nOur analysis has unearthed some serious flaws in the Data World. It appears that the hype around Data Science has not been over yet. The proliferation of new innovative tools and technologies have been proved handy, without doubt; however, sometimes we get duped too because of too much dependency on them, such as building prototypes without even conducting an EDA (Exploratory Data Analysis) properly. The result, therefore, is less generalized and flawed insights and predictions, thus causing a fiasco in the decision-making process. Here goes a brief summary of our observations.\n\n<ul>\n<li> <p><b>Data Quality.<\/b> The quality of the extracted insights depends on the data quality to a great extent. If the collected data incorporates biases, flawed understanding or judgement, and\/or does not include the necessary dimensions to answer the specific questions, Data Science can't help much. For instance, survey data may be prone to individual biases in understanding and judgement. If the duration of a survey is too long, the respondents may get distracted. Moreover, if the survey is designed with too many goals in mind, some important questions may be left out (e.g., statistical packages). Hence, it is recommended to design a survey with a couple of concrete objectives in mind and ensure the necessary dimensions are included to answer the relevant questions while keeping the total survey duration to a tolerable degree.<\/p> <\/li>\n\n<li> <p><b>Depletion in the DS Job Field.<\/b> The health-checkup of the Kaggle Respondents has yielded sheer disappointment. Only 30.92% of the Survey Respondents conform to the core concept of Data Science. Majority of them are occasional DS\/ML problem solvers (56.8%); a few of this group perform DS activites in the regular job, however, without caring about the prior analysis and understanding of the data (14.8%) - seems data is in wrong hands! This situation provokes the questions: <br>\n    <ul>\n    <li> <b>Lack of Analysis\/Understanding of Data.<\/b> Is the practical adoption of Data Science inherently flawed?<\/li>\n    <li> <b>Surplus in Data Enthusiasts.<\/b> Is there an ongoing depletion in the Data Science job market?<\/li>\n    <\/ul>\n<\/p><\/li>\n\n<li> <p><b>Flaws in Recruitment.<\/b> There are some serious flaws creeping through the DS recruitment process. The roles are sometimes hazy (e.g., Product\/Project Manager, DBA, Developer Advocate) and do not always indicate the need\/implementation of DS skillset, therefore, job search and hiring by title may not serve the intended purpose. A notable fraction of the group with title Data Scientist\/Analyst do not have expertise in Python\/R\/MATLAB (8.4%). A small group has been found out for whom the minimal educational requirement of a Bachelor's degree had been waived (5.1%).<\/p> <\/li>\n<\/ul>\n\n<p> The inherent inconsistencies and flaws in adoption give us an impression that the Data Science World is in Dystopia. This calls for a need to perform a health-checkup and curation of the Data Science Workforce on a regular interval. Most importantly, we need a proactive strategy to educate the Data Science Practitioners; a well-defined induction process is the need of the hour. Based on the Kaggle Survey data, we have strived to get them acquanited with the regular activities, basic tools and learning platforms. But there is still a lot of to be done here. Just like other STEM fields, the Data Science Practitioners need to follow a set of strictures delineating the exact steps while conducting a specific Data Science activity. Domain knowledge plays an important role in connecting the observations to the real-world phenomena. There may be other nuances which vary from one domain to the other. Hope this work will help us take the bilndfold from around our eyes and send a wakeup call to our conscience for a rigorous execution of Data Science activities. <\/p>","0dce9114":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> The mostly used visualization pair is (Matplotlib, Seaborn). <\/p><\/li>\n\n<li><p> In fact, the top 3 pairs are some combinations of Matplotlib, Seaborn and Plotly. Since the DS Workforce in this Survey is mostly skewed towards Python, libraries related to it are receiving higher importance. <\/p><\/li>\n<\/ul>\n<\/div>","a3f21353":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p>The Top 3 mostly used ML Frameworks are: Scikit-learn, Tensorflow and Keras. Xgboost is not too distant, though (4th place)!<\/p><\/li>\n    \n<\/ul>\n<\/div>","04a7ad5b":"### <a id=\"section_authority\">4.3 The Data Science Witchcraft Authority<\/a>\n\nFor those who are planning to build a career in Data Science, it is recommended to take a few courses. It is similar to learning and practising Data Science in a regulated environment. Earning certifications in these Online Learning Platforms turns out to be very effective for standing ahead in the Job Queue.","41fa022f":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> Data Science Workforce is being employed in various industries, such as, Manufacturing, Medical\/Pharmaceutical, Sales & Marketing, etc. <\/p><\/li>\n<li><p> Top 3: Computers\/Technology is on the lead (24.3%), followed by Academics\/Education (16.4%) and Accounting\/Finance (10.2%). <\/p><\/li> \n<\/ul>\n<\/div>","b0ea5a2f":"### <a id=\"section_induction_summary\">4.4 Induction Summary: Strenuous Job, Online Learning Popular, Basic Toolkit<\/a>\n<ul>\n<li> <p><b>Data Science jobs are very strenuous.<\/b> It is very common to have multiple responsibilities in the regular job. A notable fraction of the respondents reported 5-6 activities (16.9%), which is very overwhelming!<\/p>\n<p>  \n    The most common activities are: Analyze and understand data, Build\/tune prototypes, and Build\/run data infrastructure. \n<\/p> \n<\/li>\n\n\n<li> <p><b>Popularity of Online Learning.<\/b> The existing DS Workforce found Online Learning very helpful. It is, therefore, recommended to take a few courses in Data Science and Machine Learning. It is similar to learning and practising Data Science in a regulated environment. Earning certifications in these Online Learning Platforms turns out to be very effective for standing ahead in the Job Queue.<\/p>\n<p> \n    The Top 3 most popular Learning Platforms are: Coursera, Kaggle Learn Courses, and Udemy. Pick the one based on your preferences, such as rating\/reviews, content quality, pacing and price. \n<\/p>\n<\/li>\n\n<li> <p><b>The 4-piece Data Science Toolkit.<\/b> The four components are: Programming Language, Visualization Libraries, ML Algorithm, and ML Framework. The key Mantra here is:<\/p>\n<div style=\"text-align:center;\" class=\"alert alert-block alert-info\"><b>\nPick the main along with some sides. \n<\/b><\/div>\n    <ul>\n        <li> <b>Programming Language.<\/b> The common trend requires using 1-3 languages. Two Programming Languages are used most often (32.6%). Some tasks may require a combo of 3 Languages (24.9%). Our analysis reveals that one is supposed to get more value if the 3-pack consists of an Analytic Language (Python\/R), Database Programming (SQL) and Web Scripting (Javascript). The Analytic Programming Languages nowadays provide packages for efficient manipulation and statistical handling of the data; for example, pandas, numpy and scipy packages in Python. <\/li> \n        <li> <b>Visualization Libraries.<\/b> About 7.2% of the DS Workforce do not use any visualization libraries. But the common trend is to use 1-3 libraries for generating graphs. (Matplotlib, Seaborn, Plotly) combo is often used by Python users and (Ggplot, Shiny) combo is often used by R users. Note that, these libraries made their positions in the Top 5 most influential visualization libraries, according to the Kaggle Survey 2021. <\/li>\n        <li> <b>ML Algorithm.<\/b> About 12.7% do not use any ML Algorithm. The common trend, however, is to use 2-4 algorithms. The Top 3 picks are: Linear or Logistic Regression, Decision Trees or Random Forests, and Gradient Boosting Machines (xgboost, lightgbm, etc). This implies the prevalence of tabular data based regression and classification tasks in the regular activities (type: build\/tune prototypes). <\/li>\n        <li> <b>ML Framework.<\/b> About 15.1% do not use any ML Framework. However, it is typical to use 1-4 ML Frameworks. Both Scikit-learn and Tensorflow occupied the Top 2 places, according to the Kaggle Survey 2021.   \n          <p>Scikit-learn framework consists of many basic components of Machine Learning, including Linear and Logistic Regression, Decision Trees and Random Forests, etc. This is an absolutely important package for inclusion. Xgboost can be included too for more sophisticated regression and classification. \n          In contrast, Tensorflow is a Deep Learning Framework; a nice to have skill and recommended for advanced learners, specifically whose interests lie in Computer Vision and Natural Language Processing. <\/p><\/li>   \n    <\/ul>\n<\/ul>","b839a90a":"## <a id=\"section_workforce\">3. Data Science Workforce Analysis (Job, Education, Experience)<\/a>\n\nFrom now on, we'll use the term <b>Data Science Workforce<\/b> to point to the group who passed the minimal requirement. Let's find out more about this group in terms of their qualifications in coding and analysis, as well as the type of industry they work on. These insights can help us better prepared for employment.","7d0f7754":"<b>Find the most influential 3-pack cluster(s)<\/b>","ee238e0e":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> About 12.7% do not use any ML Algorithm (or the ones mentioned here). <\/p><\/li>\n\n<li><p> The common trend, however, is to use 2-4 algorithms. <\/p><\/li>\n<\/ul>\n<\/div>","dc03f23e":"### <a id=\"section_industry\">3.1 Industry<\/a>\n\nLet's explore which industries are actively working on Data Science.","6be35dd8":"### <a id=\"section_starter\">4.1 Starter Pack: A 4-piece Data Science Toolkit<\/a>\n\nThe four components in the Starter Pack are:\n\n1. Programming Language\n\n1. Visualization Libraries\n\n1. ML Algorithm\n\n1. ML Framework","f5448616":"<div class=\"alert alert-block alert-success\">\n<b>Remarks.<\/b>\n<ul>\n<li><p> About 43.1% of the Data Science Workforce are young programmers with 3 years of experience at most. <\/p><\/li>\n\n<li><p> A notable fraction has 10+ years experience in programming (22.4%). <\/p><\/li>\n<\/ul>\n<\/div>","1c0c6e94":"#### <a id=\"section_viz\">4.1.2 Visualization Libraries<\/a>\n\nLet's find out the choices of the Visualization Libraries.","6c5a48ce":"#### <a id=\"section_mlalgo\">4.1.3 ML Algorithm<\/a>\n\nLet's explore the landscape of the Machine Learning Algorithm.","76c51a45":"### <a id=\"section_jobtitle_basic\">2.3 Job title \"Data Scientist\/Analyst\": Compromises in Quality<\/a>\n\nThis section performs a health-checkup of the group with Job title Data Scientist and\/or Data Analyst."}}