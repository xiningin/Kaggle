{"cell_type":{"51784a04":"code","8e3526bc":"code","9b07c9f6":"code","5273decd":"code","17e8b379":"code","476aa087":"code","3ef55b71":"code","effb24c5":"code","82cc5da4":"code","edbe0b93":"code","9b93fa0f":"code","d59904bc":"code","5a714c12":"code","2c8cf0d9":"code","b5afcfd1":"code","5cb26735":"code","934d5cf6":"code","81f10974":"code","ea08ef80":"code","7f76be06":"code","6d548088":"code","b76e5b9d":"code","c2c9cdc7":"code","ff7b0e7c":"code","2b675143":"code","e990f0c6":"code","7a6b7cee":"code","36ebd1bb":"code","0185f398":"code","c6829d13":"code","ccc28bff":"code","5e2f8d86":"code","3fc0f31b":"code","87c2e9fe":"code","2a647705":"code","484b1c7f":"code","692d7d1b":"code","2f82b17e":"code","e22a7fd4":"code","6598b32e":"code","27c02040":"code","3fa041af":"code","055069b7":"markdown","35281998":"markdown","f9741790":"markdown","776355a9":"markdown","b7bc1858":"markdown","72b8659b":"markdown","7223dfa7":"markdown","ad924141":"markdown","51cb4e01":"markdown","bd128841":"markdown","bbdc255a":"markdown","1be92105":"markdown","65884132":"markdown","fdf46077":"markdown","c4cb5b65":"markdown"},"source":{"51784a04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8e3526bc":"#load the data and pop the label\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nlabels = train.pop('label')\n\nX_train = train.values\nX_test = test.values","9b07c9f6":"print(train.isnull().any().describe())\nprint(test.isnull().any().describe())\n\n#check the distribution of training labels\nsns.countplot(labels)","5273decd":"\n#reshape for showing images\nimg_X = [X_train[i].reshape((28,28)) for i in range(train.shape[0]) ] #reshape for visualization\n#Visulaize the ith training data\ni=0\nplt.figure()\nplt.imshow(img_X[i],cmap='gray')\nplt.title(labels[i]);\n","17e8b379":"#if we do the one hot vector encoding, we will use 'categorical_crossentropy' for loss function\n#otherwise, with integer labels as our targets, we use  'sparse_categorical_crossentropy'\n\nfrom keras.utils.np_utils import to_categorical\n#encode labels to one hot vector\nY_train = to_categorical(labels,num_classes=10);\nplt.plot(Y_train[0])\nplt.title(labels.iloc[0]);","476aa087":"from sklearn.model_selection import train_test_split\n\n#set therandom seed\nrandom_seed=1;\n#split the train and validation sets\nx_train, x_val, y_train, y_val = train_test_split(X_train,labels, test_size =.2, random_state = random_seed)","3ef55b71":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nscaled_x_train = scaler.transform(x_train)\nscaled_x_train = scaled_x_train.reshape(-1,28,28,1)\n\nscaled_x_val = scaler.transform(x_val)\nscaled_x_val = scaled_x_val.reshape(-1,28,28,1)\n","effb24c5":"# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler #for annealer\n\n\nprint(tf.__version__)","82cc5da4":"#label.unique()#total number of different labels\n\n#build the keras NN model\nmodel =Sequential([\n#     Dense(16,activation='relu',input_shape = (28,28,1)),\n    \n    Conv2D(filters = 64, activation = 'relu',kernel_size=(5,5),padding='Same',input_shape = (28,28,1)),\n    Conv2D(filters = 64, activation = 'relu',kernel_size=(5,5),padding='Same'),\n    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n    Dropout(0.25),\n    \n    Conv2D(filters = 64, activation = 'relu',kernel_size=(3,3),padding='Same'),\n    Conv2D(filters = 64, activation = 'relu',kernel_size=(3,3),padding='Same'),\n    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n    Dropout(0.25),\n    \n    Flatten(),\n    Dense(256, activation='relu'),\n#     BatchNormalization(axis=1),\n#     Dropout(0.3),\n#     Dense(32, activation='relu'),\n# #     BatchNormalization(axis=1),\n    Dropout(0.5),\n    Dense(10, activation='softmax'), #a normalized exponential functions for probability\n])\n\n\n#compile the model\noptimizer = RMSprop(lr=0.001)\n\nmodel.compile(optimizer = optimizer,\n              loss= 'sparse_categorical_crossentropy',\n              metrics = ['acc']\n             )","edbe0b93":"model.summary()","9b93fa0f":"#save only the architecture as a json file\nmodel_json = model.to_json()\nwith open ('CNN_architecture.json','w') as json_file:\n    json_file.write(model_json)","d59904bc":"from keras.models import model_from_json\nensembles_num=15;\nwith open('CNN_architecture.json','r') as f:\n    models_array =[model_from_json(f.read())]*ensembles_num\n\n","5a714c12":"models_array[0:3]","2c8cf0d9":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n","b5afcfd1":"model.save_weights('model_init.h5')","5cb26735":"#train the model\nEPOCHS = 45;\nbatch_size = 84;\n\n# # Display training progress by printing a single dot for each completed epoch\n# class PrintDot(keras.callbacks.Callback):\n#   def on_epoch_end(self, epoch, logs):\n#     if epoch % 10 == 0: print('')\n#     print('.', end='')\n    \n# #set earlystop when val_loss is not improving, patienece is the amount of epochs to check for improvement\n# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nhistory = model.fit(scaled_x_train.reshape(-1,28,28,1), y_train,\n                    batch_size=batch_size,\n                    epochs=EPOCHS,\n                    validation_data = (scaled_x_val.reshape(-1,28,28,1),y_val),\n                    verbose=2,\n                    callbacks = [learning_rate_reduction]\n                   )\n#   callbacks=[\n#               early_stop,\n#               PrintDot()])","934d5cf6":"#train the model\nEPOCHS = 45;\nbatch_size = 84;\n\noptimizer = RMSprop(lr=0.001)\n\nmodels_array[0].compile(optimizer = optimizer,\n              loss= 'sparse_categorical_crossentropy',\n              metrics = ['acc']\n             )\n    \n# #set earlystop when val_loss is not improving, patienece is the amount of epochs to check for improvement\n# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nensemble_histories=[0]*ensembles_num\nensemble_histories[0] = models_array[0].fit(scaled_x_train.reshape(-1,28,28,1), y_train,\n                    batch_size=batch_size,\n                    epochs=EPOCHS,\n                    validation_data = (scaled_x_val.reshape(-1,28,28,1),y_val),\n                    verbose=2,\n                    callbacks = [annealer]\n                   )\n#   callbacks=[\n#               early_stop,\n#               PrintDot()])","81f10974":"#model.save() save a Keras model into a single hdf5 file which contains:\n#architecture, weights, trainging coniguration(loss, optimzer), and the state of the optimizer(which allows to resume training)\nmodel.save('my_CNN.h5')\nmodel.save_weights('model_fitted.h5')\nmodels_array[0].save('array_0.h5')\nmodels_array[0].save_weights('array_0_fitted_weights.h5')","ea08ef80":" #Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\n","7f76be06":" #Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(ensemble_histories[0].history['loss'], color='b', label=\"Training loss\")\nax[0].plot(ensemble_histories[0].history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(ensemble_histories[0].history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(ensemble_histories[0].history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\n","6d548088":"from sklearn.metrics import confusion_matrix\nY_preds = model.predict(x_val.reshape(-1,28,28,1))\nY_preds = np.argmax(Y_preds,axis=1)\nconfusion_mtx = confusion_matrix(y_true=y_val,y_pred=Y_preds)\nclasses = range(10)","b76e5b9d":"sns.heatmap(confusion_mtx,annot=True, fmt = 'd', cmap =plt.cm.Blues)\nplt.tight_layout();\nplt.yticks(rotation=0)\nplt.ylabel('True label');\nplt.xlabel('Predicted label');","c2c9cdc7":"Y_preds = models_array[0].predict(x_val.reshape(-1,28,28,1))\nY_preds = np.argmax(Y_preds,axis=1)\nconfusion_mtx = confusion_matrix(y_true=y_val,y_pred=Y_preds)\nclasses = range(10)\nsns.heatmap(confusion_mtx,annot=True, fmt = 'd', cmap =plt.cm.Blues)\nplt.tight_layout();\nplt.yticks(rotation=0)\nplt.ylabel('True label');\nplt.xlabel('Predicted label');","ff7b0e7c":"# import itertools\n# def plot_confusion_matrix(cm,classes,normalize=False, \n#                           title ='Confusion Matrix',cmap= plt.cm.Blues):\n#     plt.imshow(cm, cmap= cmap)\n#     plt.colorbar()\n\n#     tick_marks = np.arange(len(classes));\n#     plt.xticks(tick_marks, classes);\n#     plt.yticks(tick_marks, classes);\n#     if normalize:\n#         cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n#     thresh = cm.max() \/ 2.\n#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#         plt.text(j, i, cm[i, j],\n#                  horizontalalignment=\"center\",\n#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n#     plt.tight_layout()\n#     plt.ylabel('True label')\n#     plt.xlabel('Predicted label')\n        \n# plot_confusion_matrix(confusion_mtx,classes)","2b675143":"# With data augmentation to prevent overfitting \nfrom keras.preprocessing.image import ImageDataGenerator #For data augmentation\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False  # randomly flip images\n        ) \n\n\ndatagen.fit(scaled_x_train)","e990f0c6":"model.load_weights('model_fitted.h5')\nEPOCHS = 150;\nbatch_size = 42;\n# Fit the model\nhistory = model.fit_generator(datagen.flow(scaled_x_train,y_train, batch_size=batch_size),\n                              epochs = EPOCHS, \n                              validation_data = datagen.flow(scaled_x_val,y_val,batch_size=batch_size),\n                              validation_steps = (x_val.shape[0] \/\/ batch_size),\n                              verbose = 2, steps_per_epoch=x_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","7a6b7cee":"model.save_weights('model_augmented_fitted.h5')","36ebd1bb":" #Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","0185f398":"Y_preds = model.predict(x_val.reshape(-1,28,28,1))\nY_preds = np.argmax(Y_preds,axis=1)\nconfusion_mtx = confusion_matrix(y_val,Y_preds)\nclasses = range(10)\n","c6829d13":"sns.heatmap(confusion_mtx,annot=True, fmt = 'd', cmap =plt.cm.Blues)\nplt.tight_layout();\nplt.yticks(rotation=0)\nplt.ylabel('True label');\nplt.xlabel('Predicted label');","ccc28bff":"EPOCHS = 150;\nbatch_size = 42;\n# Fit the model\nhistory = models_array[0].fit_generator(datagen.flow(scaled_x_train,y_train, batch_size=batch_size),\n                              epochs = EPOCHS, \n                              validation_data = datagen.flow(scaled_x_val,y_val,batch_size=batch_size),\n                              validation_steps = (x_val.shape[0] \/\/ batch_size),\n                              verbose = 2, steps_per_epoch=x_train.shape[0] \/\/ batch_size\n                              , callbacks=[annealer])\n\n #Plot the loss and accuracy curves for training and validation \n\nfig, ax = plt.subplots(2,1)\n\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n\nlegend = ax[0].legend(loc='best', shadow=True)\n\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n\nlegend = ax[1].legend(loc='best', shadow=True)\n\n\n\n","5e2f8d86":"Y_preds = model.predict(x_val.reshape(-1,28,28,1))\n\nY_preds = np.argmax(Y_preds,axis=1)\n\nconfusion_mtx = confusion_matrix(y_val,Y_preds)\n\nclasses = range(10)\nplt.figure()\nsns.heatmap(confusion_mtx,annot=True, fmt = 'd', cmap =plt.cm.Blues)\nplt.tight_layout();\nplt.yticks(rotation=0)\nplt.ylabel('True label');\nplt.xlabel('Predicted label');\n","3fc0f31b":"# import pickle\n# #dump the augmented model\n# with open('augmented_model.pkl','wb') as f:\n#     pickle.dump(model, f)\n\n# #the inital model\n# model.load_weights('model.fitted')\n# with open('CNN_fitted_model.pkl','wb') as f:\n#     pickle.dump(model, f)\n    \n# # and later you can load it\n# with open('filename.pkl', 'rb') as f:\n#     clf = pickle.load(f)","87c2e9fe":"#prepare for submission\n\nscaled_x_test = scaler.transform(X_test)\nscaled_x_test = scaled_x_test.reshape(-1,28,28,1)","2a647705":"\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n# print(submission.head())\n\nsubmission.Label = model.predict_classes(scaled_x_test)\nsubmission.to_csv('submission_augmented_1.csv',index=False)\n\nmodel.load_weights('model_fitted.h5')\nsubmission.Label = model.predict_classes(scaled_x_test)\nsubmission.to_csv('submission_no_aug.csv',index=False)\n","484b1c7f":"# gen = ImageDataGenerator(rotation_range=10,width_shift_range=.1,\n#                          height_shift_range=.1,shear_range=.15,\n#                         zoom_range = .1, channel_shift_range=10.)","692d7d1b":"# i=2;\n# image = scaled_x_train[i];\n# plt.imshow(image.reshape(28,28),cmap='gray')\n\n# #transform the image\n# image1 = gen.apply_transform(image,transform_parameters={'shear':20})\n# plt.figure()\n# plt.imshow(image1.reshape(28,28),cmap='gray')\n\n# image1 = gen.apply_transform(image,transform_parameters={'brightness':20})\n# plt.figure()\n# plt.imshow(image1.reshape(28,28),cmap='gray')","2f82b17e":"# image = X_train.reshape(-1,28,28,1)\n# plt.figure\n# plt.imshow(image.reshape(-1,28,28)[3],cmap='gray')\n# i=3;\n# aug_iter = gen.flow(image[i].reshape(-1,28,28,1))\n# aug_images = np.asarray([next(aug_iter) for i in range(20)])\n# print(aug_images.shape)\n# print(aug_images[0].shape)\n\n# for j in range(20):\n#     plt.subplot(2,10,j+1)\n#     plt.imshow(aug_images[j][0].reshape(28,28))\n    \n# # for i in range(3):\n# #     plt.figure\n# #     plt.imshow(aug_images[i].reshape(28,28))\n# # # np.asarray(aug_images)","e22a7fd4":"# def convolve(image, kernel ):\n#     length, width = image.shape;\n#     k_len = kernel.shape[0];#kernel is square\n    \n#     out_len, out_wid = (length-k_len+1,width-k_len+1)\n#     convolved = np.zeros((out_len,out_wid))\n#     for i in range(length - k_len+1):\n#         for j in range(width-k_len+1):\n#             convolved[i,j] =np.dot(image[i:i+k_len,j:j+k_len].flatten(),kernel.flatten())\n#     return convolved\n                              \n            ","6598b32e":"# test_img = scaled_x_train[0].reshape(28,28)\n\n# plt.imshow(test_img,cmap='gray')\n# kernel =np.array([[-1]*3, [0]*3, [1]*3])\n# print(kernel)\n# conv = convolve(test_img,kernel)\n# plt.figure()\n# plt.imshow(conv,cmap='gray')","27c02040":"# test_img = scaled_x_train[0].reshape(28,28)\n\n# plt.imshow(test_img,cmap='gray')\n# kernel =np.array([[-2]*5,[-1]*5, [0]*5, [1]*5,[2]*5]).T\n# print(kernel)\n# conv = convolve(test_img,kernel)\n# plt.figure()\n# plt.imshow(conv,cmap='gray')","3fa041af":"# img = X_train[45].copy()\n# avg = np.ma.mean(img[img>0])\n# print(avg)\n# img = np.where(img < avg,0,1);\n\n\n\n# plt.imshow(img.reshape((28,28)),cmap ='gray')","055069b7":"## Load the model architecture to models_array","35281998":"It shows no missing data.\nThe label data are uniformly distributed","f9741790":"## pickle the model","776355a9":"## Submission","b7bc1858":"## Save the model architecture","72b8659b":"## 2.4 label encoding","7223dfa7":"# Data preparation\n## 2.1 Load data","ad924141":"## Learning Rate annealer","51cb4e01":"## 2.5 split training and validation","bd128841":"## 2.2 Check for null","bbdc255a":"# 3. CNN\n## 3.1 Define the model","1be92105":"### Save the init weights(random) for later comparison only","65884132":"encode the label to one hot vector","fdf46077":"## 2.3 show an image example","c4cb5b65":"## Understand Data Augmentation"}}