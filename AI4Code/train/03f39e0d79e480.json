{"cell_type":{"61e51313":"code","960d8583":"code","0b426c7c":"code","ad284220":"code","7d0a047d":"code","6d6adf02":"code","22ef33ac":"code","09506d5a":"code","f1985f13":"code","4ebf2b91":"code","e1450445":"code","3570193a":"code","8bb19fc9":"code","1cb3b489":"code","65cbb25e":"code","f5abfb21":"code","039976da":"code","bdb7cc16":"code","9a652165":"code","166096f9":"code","c7252d78":"code","de23b8ef":"code","396b9e5a":"code","89bba922":"code","ffdb74ce":"code","46e8e139":"code","887984a6":"markdown","aea6db0f":"markdown","e95e6a12":"markdown","dc216fa9":"markdown","ef4eebae":"markdown","448c4be9":"markdown","e5008b07":"markdown","7731b470":"markdown","a450c2ef":"markdown","1554d44b":"markdown","ee9e350d":"markdown","3eb15266":"markdown","20a361e4":"markdown","abaf1525":"markdown","dcccbea4":"markdown","fbe2fd67":"markdown","b3b194a8":"markdown","0e489f1c":"markdown","16509ad1":"markdown","37589f81":"markdown","d25297bb":"markdown","bd8ea986":"markdown","2d3de952":"markdown","b6517f41":"markdown","28ba99dc":"markdown"},"source":{"61e51313":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy.stats import anderson\nimport seaborn as sns\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport scipy.stats as stats\npd.pandas.set_option('display.max_columns',None)\nfrom IPython.display import HTML\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split \nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nmain_train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nmain_test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","960d8583":"main_train_df.head(20)","0b426c7c":"train_df = main_train_df\ntrain_df.drop(columns=\"Id\", inplace = True)\ntest_df = main_test_df\nprint(\"Data shape:\",train_df.shape)\nprint()\ntype_ls = []\ntype_dict = {}\nfor col in train_df.columns:\n    if train_df[col].dtype not in type_ls:\n        type_ls.append(train_df[col].dtype)\n        type_dict[train_df[col].dtype] = 1\n    else:\n        counter = type_dict[train_df[col].dtype]\n        type_dict[train_df[col].dtype] = counter + 1\nprint(\"Feature datatypes:\")\nfor key, value in type_dict.items():\n    print(key,\"->\",value)    \nprint()\nprint(\"Features with NA values more than 20%:\")\nfor col in train_df.columns:\n    na_ptg = round(100*(train_df[col].isna().sum()\/len(train_df)),2)\n    if na_ptg >= 20:\n        print(\"{} -> {}\".format(col, na_ptg))\nprint()\nprint(\"Data description:\\n\")\ntrain_df.describe()","ad284220":"numeric_discrete_vars = []\ncategorical_vars = []\nnumeric_vars = []\nfor col in train_df.columns:\n    if train_df[col].dtype == \"object\":\n        categorical_vars.append(col)\n    elif len(train_df[col].unique()) < 25:\n           numeric_discrete_vars.append(col)\n    else:\n        numeric_vars.append(col)\nprint(\"Numeric Discrete variables:\\n\",numeric_discrete_vars)\nprint()\nprint(\"Categorical variables:\\n\",categorical_vars)\nprint()\nprint(\"Numeric variables:\\n\",numeric_vars)","7d0a047d":"corr = train_df[numeric_vars].corr()\ncorr.style.background_gradient(cmap='coolwarm', axis=None)","6d6adf02":"high_cor_feat = set()\nfor i in range(len(corr.columns)):\n    for j in range(i):\n        if abs(corr.iloc[i, j]) > 0.5:\n            colname = corr.columns[i]\n            high_cor_feat.add(colname)\nprint(\"Highly correlated features:\\n\",high_cor_feat)","22ef33ac":"for i in ['TotalBsmtSF', 'GarageArea', 'GrLivArea']:\n    fig, ax = plt.subplots()\n    ax.set_title(\"Area feature with outliers present\", fontsize=15)\n    ax.scatter(x = train_df[i], y = train_df['SalePrice'], alpha = 0.5, c=\"red\")\n    plt.ylabel('SalePrice', fontsize=15)\n    plt.xlabel(i, fontsize=15)\n    plt.show()","09506d5a":"train_NoOutlier = train_df[train_df.GrLivArea < 3500]\nfig, ax = plt.subplots()\nax.set_title(\"GrLivArea with outliers removed\", fontsize=15)\nax.scatter(x=train_NoOutlier['GrLivArea'], y=train_NoOutlier['SalePrice'], alpha = 0.5, c=\"purple\")\nplt.ylabel('SalePrice', fontsize=15)\nplt.show()","f1985f13":"train_NoOutlier = train_df[(train_df.TotalBsmtSF < 2500) & (train_df.TotalBsmtSF > 200)]\nfig, ax = plt.subplots()\nax.set_title(\"TotalBsmtSF with outliers removed\", fontsize=15)\nax.scatter(x=train_NoOutlier['TotalBsmtSF'], y=train_NoOutlier['SalePrice'], alpha = 0.4, c=\"purple\")\nplt.ylabel('SalePrice', fontsize=15)\nplt.show()","4ebf2b91":"plot_df = train_df.copy()\ncol_colr = [\"red\", \"orange\", \"lime\", \"purple\"]\nlabels = [i for i in train_df.columns if \"Yr\" in i or \"Year\" in i]\ncont = 0\nfor year_cols in labels:\n    if \"Yr\" in year_cols or \"Year\" in year_cols: \n        temp_df = plot_df.groupby(year_cols)[\"SalePrice\"].median()\n        temp_df.plot(color = col_colr[cont], figsize=(22,10))\n        plt.xlabel(\"Timeline\", size=15)\n        plt.legend(labels = labels)\n        plt.ylabel(\"Median House Price\", size=15)\n        plt.title(\"House Sale Price and Year features\", size=15)\n        cont += 1","e1450445":"value = train_df[\"SalePrice\"]\nsns.distplot(value, kde=False, fit=stats.norm, color = \"blue\")\nplt.xlabel(\"Skewed SalePrice\", size = 15)","3570193a":"norm_test = anderson(train_df[\"SalePrice\"])\nprint(\"Anderson Darling test of Normality:\")\nnorm_test","8bb19fc9":"SP_Log_Trnsfmd = train_df[\"SalePrice\"].copy()\nSP_Log_Trnsfmd = SP_Log_Trnsfmd.apply(lambda x: np.log1p(x))\nsns.distplot(SP_Log_Trnsfmd, kde=False, fit=stats.norm, color = \"red\")\nplt.xlabel(\"Log transformed SalePrice\", size = 15)","1cb3b489":"print(\"Top 20 important features as per correlation values and SalePrice:\")\ncorr = abs(train_df.corr())\ncorr.sort_values(['SalePrice'], ascending=False, inplace=True)\nprint(corr.SalePrice[1:21])\nImp_feat_df = pd.DataFrame(corr.SalePrice[1:11])\nImp_feat_df.rename(columns = {\"SalePrice\": \"Importance\"}, inplace = True)","65cbb25e":"x = list(Imp_feat_df.index) \ny = list(Imp_feat_df['Importance'])  \nfig = plt.figure(figsize = (12, 5)) \nplt.bar(x, y, color ='blue',  \n        width = 0.4)\nplt.xlabel(\"Features\", size=15) \nplt.ylabel(\"Importance\", size=15) \nplt.title(\"Top 10 features with correlation sorted by SalePrice\", size=15) \nplt.show() ","f5abfb21":"print(\"Top 5 important features and their distributions:\")\nImp_feat_df[\"Feature\"] = Imp_feat_df.index\nImp_feat_df.set_index(np.arange(1,11))\nimportant_feats = [i for i in Imp_feat_df[\"Feature\"]]\nsns.set()\ncols = important_feats[:5]\nsns.pairplot(train_df[cols], height = 2.5)\nplt.show();","039976da":"def imp_feat(name, vars_ls):\n    print(f\"Top 20 important {name} as per correlation values and SalePrice:\")\n    cols = vars_ls\n    cols.append('SalePrice')\n    for c in cols:\n        if c == 'SalePrice' and cols.count(c) > 1:\n            cols.pop()\n    corr = abs(train_df[cols].corr())\n    corr.sort_values(['SalePrice'], ascending=False, inplace=True)\n    print(corr.SalePrice[1:21])\nimp_feat(\"Discrete variables\", numeric_discrete_vars)\nprint()\nimp_feat(\"Mumeric variables\", numeric_vars)","bdb7cc16":"train_df[categorical_vars] = train_df[categorical_vars].fillna('None')","9a652165":"for col in numeric_vars:\n    train_df[col].fillna(train_df[col].median(),inplace=True)","166096f9":"for col in numeric_discrete_vars:\n    train_df[col].fillna(train_df[col].median(),inplace=True)","c7252d78":"train_df[\"SalePrice\"] = train_df[\"SalePrice\"].apply(lambda x: np.log1p(x))\ntrain_df[\"SalePrice\"].skew()","de23b8ef":"for col in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:  \n    train_df[col] = train_df['YrSold'] - train_df[col]","396b9e5a":"for col in numeric_vars:\n    train_df[col] = train_df[col].apply(lambda x: np.log1p(x))","89bba922":"print(\"Unique values in all the categorical variables:\")\n_encoder = preprocessing.LabelEncoder() \nfor col in categorical_vars:  \n    train_df[col] = _encoder.fit_transform(train_df[col])\n    unique = list(set(train_df[col]))\n    print(f\"{col} -> {unique}\")","ffdb74ce":"train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\ntrain_df.fillna(train_df.mean(), inplace=True)\nscale_feats = [cols for cols in train_df.columns if cols != 'SalePrice']\nscale = MinMaxScaler()\nscale.fit(train_df[scale_feats])\nscale.transform(train_df[scale_feats])","46e8e139":"train_df_prepared = pd.DataFrame(scale.transform(train_df[scale_feats]), columns = scale_feats)\ntrain_df_prepared = pd.concat([train_df_prepared, train_df['SalePrice']], axis = 1)\nprint(\"Tranformed and prepared data set:\")\n#train_df_prepared.to_csv(\"train_clean.csv\")\ntrain_df_prepared.head(20)","887984a6":"### 1. Data visualization, Variable analysis and EDA","aea6db0f":"Replacing year related variables with related age in values. The data of getting sold YrSold column is marked as the end date. This is done to gain some coefficient values from those columns. ","e95e6a12":"### 2. Data cleaning and pre-processing","dc216fa9":"Log transform on the numeric features as well in order to remove skewness.","ef4eebae":"* OverallQual and GrLivArea are strongly correlated with the target variable with 0.82 and 0.7.\n* GarageCars, GarageArea, TotalBsmtSF, etc. follows the top 2 important variables w.r.t SalePrice.\n* GarageCars and GarageArea follow an almost similar correlation w.r.t SalePrice. More number of cars one would grab, much larger space that individual would need. That would iventually play an important role deciding the price of his\/her plot.\n* Higher the quality of a house overall OverallQual, higher is the price of a house. The OverallQual has a high correlation with the target variable.\n* The presence of outliers are prominent in every features. Some of them are not showing a linear relationship as well. The ouliers are making those features important.  ","448c4be9":"The dependent variable SalePrice has been Log transformed in order to remove skewness from it.  ","e5008b07":"SalesPrice isn't following a normal distibution as it is right-skewed. From the Aderson Darling test, we have achieved a statistic or around 41.69. We have also obtained a list of critical values which are far less than the statistic value. Thus, we can infer that the distribution in the data isn't Normal, thus fails to accept the null hypothesis. A Log transformation might change the shape of the distribution.","7731b470":"Top 20 highly correlated features w.r.t SalePrice demonstrated seperately as per column data types. We can estimate the important features from the values above shown seperately. ","a450c2ef":"Pic courtsey: https:\/\/plantationhomes.com.au\/ (Plantation Homes)","1554d44b":"### Data summary ","ee9e350d":"### Loaders ","3eb15266":"> * Problem name: House Prices - Advanced Regression Techniques.\n> * Kernel owner: Adipta Biswas ([Linkedin](http:\/\/https:\/\/www.linkedin.com\/in\/adipta-biswas-adiptabiswas\/)).\n> * Notebook: Contains EDA, Variable analysis, visualizations and data cleaning part.\n> * Data set: Download from [here](http:\/\/..\/input\/house-prices-advanced-regression-techniques).\n> * Problem type: Regression.","20a361e4":"* There are outliers present in GrLivArea and GarageArea. These two area features show high correlations.\n* BsmtFinSF1 is highly negatively correlated with BsmtUnfSF. GrLivArea has a high correlation with the TotRmsAbvGrd, GarageCars has a high correlation with the GarageArea and vice-versa.\n* Majority of the GarageArea are between 300 and 700. A large number of GrLivArea are between 1000 and 2500.\n* The Slope between GrLivArea and SalePrice is steeper than GarageArea and SalePrice.","abaf1525":"The NULL values in the numeric variables are replaced with Median of each variables in order to fill the missing values.","dcccbea4":"The NULL values in the categorical variables are replaced with None in order to fill the missing values.","fbe2fd67":"The above plot is demonstrating the top 10 important features w.r.t their correlation factors against the y-variable SalePrice.\nThis is a correlation sorted by SalePrice values, which is our dependent variable. ","b3b194a8":"The NULL values in the discrete variables are replaced with Median of each variables in order to fill the missing values.","0e489f1c":"The space or area related features having a correlation more than 50% have been plotted in the above. The plots  show that they have many outliers. The presence of outliers can often lead to a fluctuating correlation factor. ","16509ad1":"The demonstration done manually shows that the outliers have been removed from GrLivArea and TotalBsmtSF features. They have been plotted against SalePrice.","37589f81":"Log transformation demonstration performed on \"SalePrice\". As the distribution of the dependent variable looks normal, a Linear model can be fit on the variable.","d25297bb":"* From the above time-series visualization, we can see that with an increase in built year or year of establishment of a house YearBuilt, there is an increase in median price. In other words, if a house is recently built, it's median SP is highly likely to be more than a house which was built a decade earlier.\n* The same goes with house garage built year GarageYrBlt and house remodification year YearRemodAdd.\n* The year of selling YrSold is just the opposite as the median price is seen to be decreasing with an increasing in the year of house getting sold, i.e the houses sold recently have lower median price than the houses sold a few years back.","bd8ea986":"* The data provided has 1460 unique data-points along with 81 columns.\n* There are 35 \"Integer\" type columns, 43 columns of type \"Object\" and 3 of \"Float\" type.\n* There are 4 columns with blank values (>50%), they are: Alley (93.77%), PoolQC (99.52%), Fence (80.75%) and MiscFeature (96.3%).","2d3de952":"Min-Max Scaling is applied on the data set to scale all the independent variables under an unified standard. This is to avoid variation in values non-uniformity in units.  ","b6517f41":"Applying label encoding method on all the category features to convert each label into numeric discrete values. ","28ba99dc":"## House Prices - Advanced Regression Techniques : Data exploration and pre-processing\n![House](https:\/\/plantationhomes.com.au\/cms_uploads\/images\/15861_six-stages-of-building-a-home.jpg)"}}