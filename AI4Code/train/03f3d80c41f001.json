{"cell_type":{"5bfa9c9a":"code","fb63fca7":"code","24cd7e5c":"code","d55ed92e":"code","93e556c4":"code","f692df1a":"code","3ee7e647":"code","4bb7efc2":"code","71c0bc68":"code","4ee05de3":"code","c7015637":"code","0fe5a689":"code","433f6d4b":"markdown","66163e11":"markdown","42390496":"markdown","83300ae3":"markdown","36995841":"markdown","e23ca0ea":"markdown","d822853d":"markdown","fa9a0853":"markdown","3f98c1d3":"markdown","aa0f91be":"markdown","1c5e2646":"markdown"},"source":{"5bfa9c9a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# install datatable\n#Model\nfrom lightgbm import LGBMClassifier\n#Metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n#Optimisation\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args","fb63fca7":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] \/ df.shape[0])*100))","24cd7e5c":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","d55ed92e":"basic_EDA(train)","93e556c4":"train['action'] = ((train['resp'].values) > 0).astype(int)\ntrain = train[train['weight'] != 0]\n\nfeatures = [c for c in train.columns if 'feature' in c]\n\nfor i in features:\n    x = train[i].mean()  \n    train[i] = train[i].fillna(x)\n","f692df1a":"def model_run(model,train):\n    \n    timeframe = 400\n\n    #Creating X\n    X = train.copy()\n    #Defining the Target\n    y = train.loc[:,['action']]\n    \n    #Index of training and validation sets\n    #I defined 300 days of training and 100 days for validation\n    index_train = X['date'] <= (X['date'].max() - 200)\n    index_val = (X['date'] > (X['date'].max() - 200)) & (X['date'] <= timeframe)\n    index_test = X['date'] > timeframe\n    \n    #print(i, X['date'].max()-30, X['date'].max())\n    \n    #Removing all other features from X (resp, weight and etc.)\n    X = X.loc[:, X.columns.str.contains('feature')]\n    \n    #Creating training and validation sets\n    X_train = X.loc[index_train]\n    y_train = y.loc[index_train]\n\n    X_val = X.loc[index_val]\n    y_val = y.loc[index_val]\n    \n    X_test = X.loc[index_test]\n    y_test = y.loc[index_test]\n    \n    #Fitting the model\n    model.fit(X_train, y_train.values.ravel(), \n                  eval_metric='auc', \n                  eval_set=[(X_val, y_val.values.ravel())],\n                  verbose=-1)\n    \n    #Using AUC score to evaluate results\n    AUC_score_val = roc_auc_score(model.predict(X_val),y_val.values.ravel())\n    AUC_score_test = roc_auc_score(model.predict(X_test),y_test.values.ravel())  \n    \n    print(\"Validation Score: \", AUC_score_val)\n    print(\"Test Score: \", AUC_score_test)\n    \n    return AUC_score_val","3ee7e647":"# Algorithm Search Space\nspace  = [Real(0, 1, name='reg_alpha'),\n          Real(0, 1, name='reg_lambda'), \n         Integer(10, 500, name = 'max_depth'),\n         Integer(10, 500, name = 'num_leaves')]\n\n# For decimal numbers do Real(0.5, 0.9, name='learning_rate')\n# Make sure you imported the Real numbers in \"from skopt.space import Real, Integer\"\n\n# Allows the GP optm to go through your defined search space\n@use_named_args(space)\n\n# Function that is going to be called by the optmiser\ndef objective(**params):\n    # Model - I already predefined the basics parameters\n    model = LGBMClassifier(objective = 'binary',\n                           metric = 'auc',\n                           learning_rate = 0.03,\n                           early_stopping_round = 50,\n                           n_estimators = 10000,\n                           verbose = -1)#,\n                          #device = 'gpu')\n    \n    # Sets the parameters defined by the \"space variable\"\n    model.set_params(**params)\n\n    # Calls my model function that returns the AUC of this\n    # parameter configuration\n    \n    return 1-(model_run(model,train))","4bb7efc2":"res_gp = gp_minimize(objective, space, n_calls=20, n_initial_points = 10, random_state=0)","71c0bc68":"print(\"Best score=%.4f\" % (1-res_gp.fun))\n\nprint(\"\"\"Best parameters:\n- reg_alpha = %.2f\n- reg_lambda = %.2f\n- max_depth = %d\n- num_leaves = %d\"\"\" % (res_gp.x[0], res_gp.x[1],res_gp.x[2], res_gp.x[3]))","4ee05de3":"#If you would like to plot the results\n\nfrom skopt.plots import plot_convergence\n\nplot_convergence(res_gp)","c7015637":"params = { \n'objective': 'binary', \n\"metric\": \"auc\", \n\"learning_rate\": 0.03,\n\"reg_lambda\":res_gp.x[0],\n\"reg_alpha\": res_gp.x[1],\n\"max_depth\": res_gp.x[2],\n\"num_leaves\": res_gp.x[3],\n\"n_estimators\": 350,\n\"boosting_type\":\"gbdt\",\n'random_state': 0,\n#\"device\": \"gpu\"\n}","0fe5a689":"model = LGBMClassifier()\nmodel.set_params(**params)\n\n#Now that  we use the whole data to train and create final model\n\nX = train.copy()\n#Defining the Target\ny = train.loc[:,['action']]\n\nX = X.loc[:, X.columns.str.contains('feature')]\n\n#Fitting the model\nmodel.fit(X,y.values.ravel())\n\n#Using AUC score to evaluate results\nAUC_score = roc_auc_score(model.predict(X),y.values.ravel())\n\n#The score is higher as it is based on a training set\n#but is just to give an overall idea\nprint(\"AUC Score: \", AUC_score)","433f6d4b":"# Data Preparation and Preprocessing\n\nIf you participating of this challenge, the steps I taken below are already familiar to you.\n\n* First we create the action column with our binary target. If res > 1 then 1, else is zero. \n* Other notebooks have also taken into account resp_1 ... to resp_4. As this is not the goal of this notebook, I will use the simpler approach using only resp as indication of my target\n* As a blunt missing values strategy, I am just replacing all by the mean. Other strategies can also be valid and should be investigated\n* I have not used any normalisation or standardization strategy for now","66163e11":"# Model Definition\n\nThe way I find easier to work with the GP optimisation library is to create a function that defines the model, calls the dataset, trains and output the score. Ideally, you should be using a validation set to choose the best parameters to avoid overfitting to the test set.\n\n* My test set is the last 100 days of the data. Again, this is an example on how to use the optimiser rather than a how-to win the challenge\n* I selected to train and validate with the first 400 days. I will use this timeline to build the training and validation sets\n* I am using this simple strategy to focus on the optimisation of the model. Usually, for time series,  you would have more than one window of training and validation sets. And the parameters should be selected according to the ones that give the better result on all time windows","42390496":"Now instead of using training and validation sets, we make the most of the data made available. We train the whole model again, now with the final parameters defined by the optimiser.","83300ae3":"During the parameter optimisation phase, I set the n_estimators very high (10.000) as the early stopping would control and avoid overfitting. For the final model, I used n_estimators = 350 instead as I am not using early_stopping to avoid overfitting. I did not use early_stopping as it requires the usage of a validation set and I wanted to train the model with the full data. \n\nTry to optmise the n_estimators. Otherwise, use a subset as the validation set and use early stopping. Also, perhaps focusing more effort on regularization parameters and feature engineering are the way to go. \n\nHopefully this has been helpful. I am happy that some people upvoted, thank you!","36995841":"# Optimisation\n\nHere is where the optimisation bits start. As we are using LGBM, which is a tree based model, I choose the following parameters to be optimised:\n\n* max_depth\n* num_leaves\n* regularization params: reg_alpha and lambda\n\nThe parameters to be optimised needs to be saved within a variable. This variable will be the search space of our optimisation algorithm (hence the name \"space\").\n\n* Since all the parameters I chosen are non-decimal values, they need to be defined with the word Integer in front, as shown below\n* If you want to optimise the learning rate, which is decimal, use Real and check the commented example in the code below\n\nFor some reason, it works best if you follow this order when setting up your code (really weird error messages when I tried to change):\n\n1. Define your search space\n1. Add **@use_named_args(space)**. This allows the GP opt. to use the params defined in your space variable\n1. Create the **objective** function\n\nSince the score I chosen is AUC, which is the higher the better, it is not ideal to be used directly in the BO optimisation function. The BO is trying to minimise the output, as such it will chose the parameter values that actually reduce the AUC if we leave it as it is. \n\nTo adapt the AUC score to a minimisation problem, I changed the objective function return output to be **1-AUC Score**. By doing so, the higher the AUC, the smaller the output from the objective function. ","e23ca0ea":"# Data Import\n\nI will skip the EDA analysis, and we will jump straight to the data cleansing, processing and model building phase.","d822853d":"According to Jane Street Group:\n* The set of features, feature_{0...129}, is anonymized and represents real stock market data. Each row is a trading opportunity that we need to predict if the transaction was done (value:1) or not (value: 0). \n* As we can see from the above table, each sample has an associated **weight** and **resp**, representing the return on the trade. \n* In the test set, the **resp** features are not provided, as it is (or they are) our target feature. \n* The **date** feature represents the day of the trade, while **ts_id** represents a time ordering.\n* In the datasets, features with **weight = 0** were added. However such trades will not contribute towards the scoring evaluation.","fa9a0853":"Below is the line of code that calls the GP optmisation. There are more parameters to be defined, however, I find little difference when changing the acquisition function (acq_func) or the optimization method (acq_optimizer), as the default options seemed to work best for this task. \n\nIn addition, I would advise to optimise few parameters at a time. While it may seem counterintuitive, in the long run is the best. \n\nTo elaborate on the remaining parameters (see full description at [skopt-minimize](https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.gp_minimize.html)):\n\n> gp_minimize(func, dimensions, base_estimator=None, n_calls=100, n_random_starts=None, n_initial_points=10, initial_point_generator='random', acq_func='gp_hedge', acq_optimizer='auto', x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=10000, n_restarts_optimizer=5, xi=0.01, kappa=1.96, noise='gaussian', n_jobs=1, model_queue_size=None)\n\n1. **func**: I named **objective** as it is the function we are trying to minimise. Everytime, the GP will run this function, evaluate the results and select the next set of parameters according to the AUC results;\n1. **dimensions**: I named **space**. This is the available search space, where the range of the features are defined. \n1. **n_calls**: Number of times the GP optm will run to find the best results. I would not leave the default as it takes a long time for this dataset. Increase as you gain more confidence with the process. The GP optm advantage is that is converges rapidly to a good enough solution when compared to other optmisation algorithms. As such, it has been widely used for complex problems that cannot afford to be run 500x.\n1. **n_initial_points**: To define the best way forward, the GP optm will first run in some random points within the feature space. After this X number of runs, it will define the best \"next\" point to be evaluated and run for the next **n_calls**.\n","3f98c1d3":"# Create final Optimised Model\n\nBelow we use the output of the BO to define our final model. GPU is turned off as I am using for another Kaggle window :)","aa0f91be":"From the convergence plot we can see that there is not much gain after 20 rounds of optimisation. As such, we could reduce the number of iterations (n_calls) from **20** to **10** to speed up the optimisation process.","1c5e2646":"# Bayesian Optmisation\n\nSimple Notebook showing how to use  Bayesian Optimisation (BO) to select parameters value using Jane Street Market dataset. \n\nWhile here I use LGBM, the same logic can be applied to any ML model. It can also be used to define the ANN architecture. The library I will use is *scikit-optimize*. Information on the library and parameters can be found [here](https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.gp_minimize.html).\n\nThe libraries that need to be imported are:\n\n* from skopt import gp_minimize\n* from skopt.space import Real, Integer\n* from skopt.utils import use_named_args\n\nTo keep this short and to the point, I will not extend on how the BO works. If you would like an in depth explanation, I would suggest to read this paper from Peter Frazier (https:\/\/arxiv.org\/abs\/1807.02811). In addition, the set of lectures of [Nando de Freitas](https:\/\/www.youtube.com\/watch?v=vz3D36VXefI) are extremly valuable and available on Youtube. He also explains Gaussian Process in a marvellous way that blew my mind.\n\nFinally, [Jeff Heaton](https:\/\/www.youtube.com\/watch?v=sXdxyUCCm8s) has a great example on how to use BO to set the architecture of the ANN.\n\nOk, let's get started!"}}