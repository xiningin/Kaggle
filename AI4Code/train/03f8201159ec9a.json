{"cell_type":{"1be58dc0":"code","50d2bbec":"code","6d4e4f2c":"code","563c8471":"code","44bea1f4":"code","6f5de901":"code","edf02190":"code","d7f1767b":"code","c2d1dbb6":"code","d0936bf8":"code","4487065d":"code","5707bc6e":"code","4c219cbd":"code","d1e50c54":"code","8134ae17":"code","2b249296":"code","3903b84e":"code","5aa32294":"code","bffaab35":"code","4037d758":"code","bf69eb59":"code","ee850de1":"code","b858f13e":"code","8709cf97":"code","c0700556":"code","dd8ae531":"code","9439b849":"code","cbe83940":"code","339ed198":"code","829c47ce":"code","5c1f5a1a":"code","8714f311":"code","e15bb910":"code","80298ef9":"code","fdbcb01d":"code","60e35e01":"code","559980f6":"code","cd435823":"code","a50e46de":"code","857b45e1":"code","ede744a4":"code","82563c32":"code","e5d3e34a":"code","0ce23e38":"code","507a0395":"code","a92c86ae":"code","9da048c6":"code","fbc27e86":"code","f3f18dc4":"code","2591ad10":"code","5d7ac9ff":"code","06106de1":"code","c99bb6d9":"code","9c44e733":"code","995b3542":"code","7ba0d82c":"code","7df6ed27":"code","de971716":"code","c11892b0":"code","70521e14":"code","c9c38a82":"code","1c1ac439":"code","b123c426":"code","9b216093":"code","11bf4a24":"code","6178f7dc":"code","494ff238":"markdown","b3603de5":"markdown","9f2f0bd9":"markdown","f681519c":"markdown","18a5c5c8":"markdown","d09aee35":"markdown","29f2a366":"markdown","b2a42726":"markdown","9acd7005":"markdown","518757f8":"markdown","a7f096f2":"markdown","781d0016":"markdown","b538b2ff":"markdown","78f4943a":"markdown","d69a542d":"markdown","8dd5094e":"markdown","23ec1014":"markdown","90e67329":"markdown","a2ebd510":"markdown","15760c64":"markdown","87e479b7":"markdown","e1c269b1":"markdown","b09c8c02":"markdown","1f865c8c":"markdown","63944467":"markdown","6bb5cbfd":"markdown","f1d08ea0":"markdown","d72e3ad6":"markdown","9ceca69a":"markdown","fea682a8":"markdown","f3cde37e":"markdown","a572b02c":"markdown","58bb78c3":"markdown","3536ff74":"markdown","95c62940":"markdown","e575f3b7":"markdown","ecfd0a16":"markdown","611696d4":"markdown","e02b8852":"markdown","5b2c2c2c":"markdown","2e0ff13c":"markdown","9f860686":"markdown","03b16116":"markdown","fc0f1860":"markdown","2da69cdb":"markdown","09589266":"markdown","62170771":"markdown","480bc69a":"markdown","4271a7e8":"markdown","762a7d2f":"markdown","142e66aa":"markdown","1e4acd81":"markdown","734d602c":"markdown","b29b5841":"markdown","e76cb946":"markdown","1deefb10":"markdown","fe328aad":"markdown","ab0bdade":"markdown","ea2e682e":"markdown","753cf22b":"markdown","8a7f46c5":"markdown","f3867cde":"markdown","b09e1151":"markdown","172fc5ff":"markdown","6dc43b24":"markdown"},"source":{"1be58dc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.graph_objects as go\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","50d2bbec":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","6d4e4f2c":"train.sample(10)","563c8471":"print('Train Data Info\\n')\ntrain.info()\nprint('\\n\\n\\nTest Data Info\\n')\ntest.info()","44bea1f4":"print('Train Data\\n')\nprint(train.isnull().sum().sort_values(ascending=False))\nprint('\\nTest Data\\n')\nprint(test.isnull().sum().sort_values(ascending=False))","6f5de901":"fig, ax  = plt.subplots(1,2, figsize=(25,5))\n\nax[0].set_title('Train Nulls')\nsns.barplot(x=train.columns, y=train.isnull().sum(), ax=ax[0])\nax[1].set_title('Test Nulls')\nsns.barplot(x=test.columns, y=test.isnull().sum(), ax=ax[1])\nplt.show()","edf02190":"train.describe()","d7f1767b":"cat1 = ['Survived(obj)', 'Pclass', 'Sex', 'Embarked']\n\ntrain['Survived(obj)'] = ['Yes' if i == 1 else 'No' for i in train.Survived]\n\nfig, axs = plt.subplots(2,2, figsize=(10,7))\n\nc = 0\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        sns.barplot(train[cat1[c]].value_counts().index, train[cat1[c]].value_counts().values, ax=axs[i][j])\n        axs[i][j].set_title(cat1[c])\n        axs[i][j].set_ylabel('Frequency')\n        c += 1\ntrain.drop(['Survived(obj)'], axis=1, inplace=True)","c2d1dbb6":"# cat1 = ['Survived(obj)', 'Pclass', 'Sex', 'Embarked']\n\n# train['Survived(obj)'] = ['Yes' if i == 1 else 'No' for i in train.Survived]\n\n# fig, axs = plt.subplots(2,2, figsize=(15,10))\n\n# c = 0\n# for i in range(axs.shape[0]):\n#     for j in range(axs.shape[1]):\n#         sns.countplot(train[cat1[c]], ax=axs[i][j], hue=train[cat1[c]])\n#         axs[i][j].set_title(cat1[c])\n#         c += 1\n# train.drop(['Survived(obj)'], axis=1, inplace=True)","d0936bf8":"cat2 = ['SibSp','Parch']\n\nfig, axs = plt.subplots(1,2, figsize=(15,10))\n\nfor i in range(axs.shape[0]):\n    axs[i].pie(train[cat2[i]].value_counts().values, labels=train[cat2[i]].value_counts().index, autopct='%1.1f%%')\n    axs[i].set_title(cat2[i])","4487065d":"cat2 = ['Name', 'Ticket', 'Cabin']\n\nfor i in cat2:\n    print(train[i].value_counts(),'\\n\\n')","5707bc6e":"plt.figure(figsize=(12,7))\ntrain[train.Fare > 200].groupby('Name')['Fare'].max().plot(kind='barh', color='black')\nplt.title('People Paid Fares More Than 200\u00a3')\nplt.xlabel('Fare')\nplt.show()","4c219cbd":"num = ['Age','Fare']\n\nfig, axs = plt.subplots(1,2, figsize=(14,6))\n\nfor i in range(axs.shape[0]):\n    sns.distplot(train[num[i]], ax=axs[i], color='blue')\n    axs[i].set_title(num[i])\n    \nf, axs2 = plt.subplots(1,2, figsize=(14,6))\nfor i in range(axs.shape[0]):\n    sns.boxplot(y = num[i], ax=axs2[i], data=train)\n#     sns.swarmplot(y = num[i], ax=axs2[i], data=train, x='Pclass', color='.25')\n    axs2[i].set_title(num[i])","d1e50c54":"train.groupby('Sex')[['Survived']].mean()","8134ae17":"pvt = train.groupby(['Pclass','Sex'], as_index=0)['Survived'].mean()\n\nplt.figure(figsize=(10,7))\nsns.heatmap(pvt.pivot('Pclass','Sex','Survived'), cmap='Blues', annot=True)\n\ntrain.groupby(['Pclass','Sex'])[['Survived']].mean()#.plot(kind='barh')","2b249296":"from collections import Counter as count\n\ndef detect_outliers(data, features):\n    indices = []\n    for f in features:\n        q1 = np.quantile(data[f], .25) #np.nanpercentile(data[f], .25)\n        q3 = np.quantile(data[f], .75) #np.nanpercentile(data[f], .75)\n        iqr = np.abs(q3 - q1)\n        \n        min_val = q1 - 1.5 * iqr\n        max_val = q3 + 1.5 * iqr\n        \n#         indices.extend([idx for idx, i in enumerate(data[f]) if i < min_val or i > max_val])\n        indices.extend(data[(data[f] < min_val) | (data[f] > max_val)].index)\n    \n    outliers_morethan2 = [k for k,v in count(indices).items() if v > 2]\n    \n    return outliers_morethan2","3903b84e":"outlier_indices = detect_outliers(train, ['SibSp','Parch','Fare'])\nprint(len(outlier_indices))\n\ntrain = train.iloc[~train.index.isin(outlier_indices)].reset_index()\n# or\n# traindata = train.drop(outlier_indices, axis=0).reset_index()","5aa32294":"data = pd.concat([train, test], axis=0).reset_index()\n\ndata.isnull().sum().sort_values(ascending=False)[data.isnull().sum() > 0]","bffaab35":"data.groupby(['Pclass','Embarked'])[['Fare']].agg(['mean','median'])","4037d758":"data[data.Embarked.isnull()]","bf69eb59":"data[data.Fare.isnull()]","ee850de1":"plt.figure(figsize=(10,7))\nsns.boxplot(y='Fare', x='Embarked', data=data)\nplt.show()","b858f13e":"data['Embarked'] = data.Embarked.fillna('C')\nprint('Missing values in \"Embarked\" column: ',data.Embarked.isnull().sum())","8709cf97":"data['Fare'] = data.Fare.fillna(data[(data.Pclass == 3) & (data.Embarked == 'S')]['Fare'].median())\n\nprint('Missing values in \"Fare\" column: ',data.Fare.isnull().sum())","c0700556":"print('Null Age values: ',data.Age.isnull().sum())","dd8ae531":"plt.figure(figsize=(8,5))\nsns.boxplot(y=data.Age, x=data.Sex, hue=data.Pclass)\nplt.show()","9439b849":"plt.figure(figsize=(10,5))\nsns.boxplot(y=data.Age, x=data.SibSp)\n\nplt.figure(figsize=(10,5))\nsns.boxplot(y=data.Age, x=data.Parch)\nplt.show()","cbe83940":"data['SexLabeled'] = [0 if i =='female' else 1 for i in data.Sex]\n\nplt.figure(figsize=(10,7))\nsns.heatmap(data[['Age','Parch','Pclass','SibSp','SexLabeled']].corr(), cmap='Blues', annot=True)\nplt.show()","339ed198":"# indices of missing values\nnull_indices = [i for i in data[data.Age.isnull()].index]\n\n\nfor idx in null_indices:\n    age_med = data.Age[(data.Pclass == data.Pclass.iloc[idx]) & (data.SibSp == data.SibSp.iloc[idx]) & (data.Parch == data.Parch.iloc[idx])].median()\n    \n    if not np.isnan(age_med):\n        data.Age.iloc[idx] = age_med\n    else:\n        data.Age.iloc[idx] = data.Age.median()","829c47ce":"print('Missing values in \"Age\" column: ',data.Age.isnull().sum())","5c1f5a1a":"# def age_outliers(data, features):\n#     indices = []\n#     for f in features:\n#         q1 = np.quantile(data[f], .25) #np.nanpercentile(data[f], .25)\n#         q3 = np.quantile(data[f], .75) #np.nanpercentile(data[f], .75)\n#         iqr = np.abs(q3 - q1)\n        \n#         min_val = q1 - 1.5 * iqr\n#         max_val = q3 + 1.5 * iqr\n        \n# #         indices.extend([idx for idx, i in enumerate(data[f]) if i < min_val or i > max_val])\n#         indices.extend(data[(data[f] < min_val) | (data[f] > max_val)].index)\n    \n#     outliers_morethan2 = [k for k,v in collections.Counter(indices).items()]\n    \n#     return outliers_morethan2\n\n\n# data = data.drop(age_outliers(data, ['Age']), axis= 0).reset_index(drop=True)","8714f311":"data['Cabin'] = data.Cabin.fillna('U')\ndata['Cabin'] = [i[0][0] for i in data.Cabin]\n\ndata.Cabin.value_counts(dropna=False)","e15bb910":"plt.figure(figsize=(10,5))\nsns.barplot(data.Cabin, data.Survived)\n\ndata.groupby('Cabin').Survived.agg({'mean','count'})","80298ef9":"data.sample(5)","fdbcb01d":"plt.figure(figsize=(9,6))\nsns.barplot(x=data.SibSp, y=data.Survived)\nplt.title('# of Siblings vs Survival Rate')\n\ndata.groupby('SibSp').Survived.agg(['count','mean'])","60e35e01":"plt.figure(figsize=(9,6))\nsns.barplot(x=data.Parch, y=data.Survived)\nplt.title('# of Family Members vs Survival Rate')\n\ndata.groupby('Parch').Survived.agg(['count','mean'])","559980f6":"plt.figure(figsize=(9,6))\nsns.barplot(x=data.Pclass, y=data.Survived)\nplt.title('# of Ticket Class vs Survival Rate')\n\ndata.groupby('Pclass').Survived.mean()","cd435823":"g = sns.FacetGrid(data=data, col='Survived', size=4)\ng.map(sns.distplot, 'Age')\n\ndata.groupby('Age').Survived.mean().nlargest(15)","a50e46de":"g = sns.FacetGrid(data= data, col='Survived', row='Pclass',size=3.5)\ng.map(plt.hist, 'Age', bins=25)\nplt.show()","857b45e1":"g = sns.FacetGrid(data= data, col='Embarked', size=4)\ng = g.map(sns.barplot, 'Pclass', 'Survived','Sex', palette='Set2')\ng.add_legend()\ng.set_axis_labels('Pclass', 'Survival Rate')\n\ndata.groupby(['Sex','Pclass','Embarked']).Survived.mean()","ede744a4":"g = sns.FacetGrid(data = data, row='Embarked', col='Survived')\ng = g.map(sns.barplot, 'Sex', 'Fare', palette='Set2')\n\ndata.groupby(['Sex','Embarked','Survived']).Fare.mean()","82563c32":"data['Title'] = [i.split('.')[0].split(',')[1].strip() for i in data.Name]","e5d3e34a":"plt.figure(figsize=(9,6))\nsns.countplot(data.Title)\nplt.xticks(rotation=45)\n\ndata.Title.value_counts()","0ce23e38":"data['Title'] = data['Title'].replace([data.Title.value_counts().index[idx] for idx,title in enumerate(data.Title.value_counts()) if data['Title'].value_counts()[idx] < 50], 'other')","507a0395":"data['Title'] = [0 if title=='Mr' else 1 if title=='Mrs' or title=='Miss' else 2 if title=='Master' else 3 for title in data['Title']]","a92c86ae":"plt.figure(figsize=(9,6))\nsns.barplot(x=data['Title'], y=data.Survived)\nplt.xticks(np.arange(data.Title.nunique()),['Mr','Mrs','Master','Other'])\nplt.show()","9da048c6":"data['Family Size'] = data['Parch'] + data['SibSp'] + 1\ndata.head(15)","fbc27e86":"plt.figure(figsize=(9,6))\nsns.barplot(x=data['Family Size'], y=data.Survived)\n\ndata.groupby('Family Size').Survived.mean()","f3f18dc4":"data['Family Size'] = [0 if i==6 or i==5 else 1 if i==1 or i==7 else 2 for i in data['Family Size']]","2591ad10":"sns.barplot(x=data['Family Size'], y=data.Survived)\n\n\ndata.groupby('Family Size').Survived.agg(['count','mean'])","5d7ac9ff":"import string \n\n# string.punctuation : '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\n\nnew_tickets = []\nfor i in data.Ticket:\n    if len(i.split()) > 1:  # if not only digits\n        for j in string.punctuation:\n            i = i.replace(j,'')\n        new_tickets.append(i.split()[0])\n        \n    else: # if only digits\n        new_tickets.append('NC')","06106de1":"data.Ticket = new_tickets","c99bb6d9":"data[['Family Size', 'Parch', 'SibSp']].corr()","9c44e733":"data.head(10)","995b3542":"data.drop(['level_0','Name','index','SexLabeled'], axis=1, inplace=True)\n# data.drop(['level_0','Cabin','Name','PassengerId','index','SexLabeled'], axis=1, inplace=True)\n\ndata.head()","7ba0d82c":"one_hot_list = ['Embarked','Pclass','Sex','Title','Family Size','Cabin','Ticket']\n\ndata = pd.get_dummies(data, columns= one_hot_list, drop_first= True)\n\ndata.head()","7df6ed27":"train = data[~data.Survived.isna()].reset_index(drop=True)\ntest  = data[data.Survived.isna()].drop(['Survived'], axis=1).reset_index(drop=True)\ntest_ID = test['PassengerId']\ntrain.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\nx_train = train.drop(['Survived'], axis=1)\ny_train = train.Survived.values.reshape(-1,1)","de971716":"from sklearn.model_selection import train_test_split\n\nx_trainn, x_val, y_trainn, y_val = train_test_split(x_train, y_train, test_size= 0.25, random_state=3)\n\nsns.barplot(x=['x_train','x_val','y_train','y_val','test'], y=[len(x_train),len(x_val),len(y_train),len(y_val),len(test)])\nplt.title('# of Samples')\nplt.show()","c11892b0":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nclf_list = [KNeighborsClassifier(),\n          LogisticRegression(),\n          LinearDiscriminantAnalysis(),\n          DecisionTreeClassifier(),\n          RandomForestClassifier(),\n          SVC()]\n\nknn_params = {'n_neighbors':np.linspace(1,50,5, dtype=int),\n             'weights':['uniform','distance'], \n             'metric':['euclidean','minkowski']}\n\nlr_params = {'penalty':['l1','l2'],\n            'C':np.logspace(-1,0,6)}\n\nlda_prarams = {'solver':['svd','eigen']}\n\n\ndtc_params = {'criterion':['gini','entropy'],\n             'min_samples_leaf':range(1,65,10), # samples required to be a leaf(not an internal node)\n             'max_depth':range(1,20,3)} \n\nrfc_params = {'n_estimators':range(100,301,100),\n             'bootstrap':[False]}\n\nsvc_params = {'C':np.logspace(0,2,4),\n             'gamma':np.logspace(-1,0,4),\n             'probability':[True]}\n\n\ngrid_params = [knn_params, lr_params, lda_prarams, dtc_params, rfc_params, svc_params]","70521e14":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\ncv_results = []\nbest_estimators = []\n\nfor clf, param in zip(clf_list, grid_params):\n    \n    gs  = GridSearchCV(estimator= clf, param_grid= param, cv= StratifiedKFold(5), scoring='accuracy')\n    gs.fit(x_trainn, y_trainn)\n    \n    cv_results.append(gs.best_score_)\n    best_estimators.append(gs.best_estimator_)\n    print(clf,'\\n\\naccuracy: ',gs.score(x_val, y_val),'\\n\\n')","c9c38a82":"plt.figure(figsize=(8,5))\nsns.pointplot(y=cv_results,x=['K-nearest Neighbor', 'Logistic Regression','LDA','Decision Trees','Random Forest','SVC'])\nplt.title('Accuracies')\nplt.xticks(rotation=30, color='black', fontweight='bold')\nplt.yticks(np.arange(0.5,0.9,0.05))\nplt.show()","1c1ac439":"from sklearn.ensemble import VotingClassifier\n\n\nfor voting in ('hard', 'soft'):\n    globals()['votingclf_' + voting] = VotingClassifier(estimators=[('knn', best_estimators[0]),\n                                             ('logreg', best_estimators[1]), \n                                             ('lda', best_estimators[2]), \n                                             ('dt', best_estimators[3]), \n                                             ('rf', best_estimators[4]),\n                                             ('svc', best_estimators[5])],\n                                             voting=voting)\n    \n    globals()['votingclf_' + voting] = globals()['votingclf_' + voting].fit(x_trainn, y_trainn)\n    print('accuracy for voting = {}: {}'.format(voting, globals()['votingclf_' + voting].score(x_val, y_val)))","b123c426":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\n\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n\nxgb  = XGBClassifier(n_estimators=1000, learning_rate=0.01, reg_lambda=2)\ngbc  = GradientBoostingClassifier(n_estimators=500)\nlgbm = LGBMClassifier(n_estimators=100)\n\nfor i in [xgb, gbc, lgbm]:\n    print(i)\n    print('score: ',i.fit(x_trainn, y_trainn).score(x_val, y_val))\n    print('accuracy: ',sum((i.predict(x_val) == y_val)[0])\/len(y_val))\n    print('auc: ',roc_auc_score(i.predict(x_val), y_val))\n    print('confusion matrix\\n',confusion_matrix(y_val, i.predict(x_val), labels=[0,1]), '\\n\\n')","9b216093":"params = {'loss_function':'Logloss',\n         'eval_metric':'AUC',\n         'verbose':200,\n         'iterations':1000}\n\ncbc  = CatBoostClassifier(**params)\ncbc.fit(x_trainn, y_trainn,\n       eval_set=[(x_trainn, y_trainn), (x_val, y_val)],\n       plot=True)","11bf4a24":"final_estimators = [SVC(), LogisticRegression(), LinearDiscriminantAnalysis()]\n\nestimators = [('xgb', XGBClassifier(n_estimators=1000, learning_rate=0.01, reg_lambda=2)),\n              ('gbc', GradientBoostingClassifier(n_estimators=500)),\n              ('lgb', LGBMClassifier(n_estimators=100))]\n#               ('catb', CatBoostClassifier(**params))]\n\nfor i in final_estimators:\n    sc = StackingClassifier(estimators = estimators,\n                           final_estimator= i,\n                           cv = StratifiedKFold(5, shuffle=True, random_state=3))\n    \n    sc.fit(x_trainn, y_trainn)\n    print(i, '\\nScore: ' ,sc.score(x_val, y_val), '\\n\\n')\n    print('Accuracy: ' ,sum((sc.predict(x_val) == y_val)[0])\/len(y_val),'\\n')\n    print('Confusion Matrix\\n', confusion_matrix(y_val, sc.predict(x_val)),'\\n\\n\\n')\n","6178f7dc":"test_predictions = pd.Series(sc.predict(test), name = \"Survived\").astype(int)\n\nresults = pd.concat([test_ID, test_predictions],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","494ff238":"<font color = 'blue'>\n* Ratio of Survivals w.r.t. Genders","b3603de5":"<font color='blue'>\n* Survival rates of new categories","9f2f0bd9":"<b id=23>\n\n## Train-Test Split","f681519c":"<font color=blue>\n* CatBoost","18a5c5c8":"<b id=29>\n# Submission\n<font color=blue>\nBest score has been obtained by Stacking method with meta-classifier of LDA.","d09aee35":"# Introduction\n<font color=green>\nThis kernel includes exploratory data analysis and prediction of survivals for Titanic competition. \n\n\n---\n\n\n\n\n<font color = 'blue'>\n\nContents\n\n1. [Loading Data](#1)\n2. [Data Description & Overview](#2)\n3. [Univariate Analysis](#3)\n   * [Categorical Variable Analysis](#4)\n   * [Numerical Variable Analysis](#5)\n\n\n\n4. [Basic Data Analysis](#6)\n\n\n5. [Anomaly Detection & Elimination](#8)\n    * [Filling Missing Values](#9)\n        * [Embarked & Fare](#10)\n        * [Age](#11)\n        * [Cabin](#100)\n\n\n\n\n6. [Visualizations & Brainstorming](#12)\n    * [Number of Siblings ~ Survival Rate](#13)\n    * [Number of Family Members ~ Survival Rate](#14)\n    * [Ticket Class ~ Survival Rate](#15)\n    * [Age ~ Survival Rate](#16)\n    * [Pclass-Age-Survived](#7)\n    * [Pclass-Embarked-Sex-Survived](#17)\n    * [Embarked-Sex-Survived-Fare](#18)\n    \n    \n7. [Feature Engineering](#19)\n\n\n8. [Pre-processing](#20)\n    * [Drop Unnecessary Features](#21)\n    * [One-hot Encoding](#22)\n    * [Train-Test Split](#23)\n\n\n9. [Classification](#24)\n    * [Basic ML Classification Algorithms](#25)\n    * [Ensemble Modelling](#26)\n        1. [Voting Classifier](#26)\n        2. [Boosting Methods](#27)\n        3. [Stacking](#28)\n        \n        \n        \n10. [Submission](#29)","29f2a366":"<font color = 'blue'>\n\n* Percentages of Different Numbers of Parents and Siblings","b2a42726":"<b id=18>\n\n## Embarked-Sex-Survived-Fare","9acd7005":"<font color='blue'>\n\n* Although Pclass = 3 has the highest boarding number, the survival rate is the lowest.\n* Pclass = 1 has the highest survival rate.\n\n--- \n<font color='green'>\n\nAs in the Age~Survived analysis above, here it's also seen that children are privileged.","518757f8":"<font color = 'blue'>\n    \nNow let's take a look at the descriptive statistics of the data, outliers will be found and visualized by boxplots later. ","a7f096f2":"<font color= 'blue'>\n* Features Corresponding to Embarked nulls: <font color= 'red'> Fare: 80 - Pclass: 1\n\n\n<font color= 'black'>\nThe closest corresponding Embarkation Port for those values: --> <font color= 'blue'>\n**C** ","781d0016":"<a id = \"10\"><\/a>\n\n### Embarked & Fare\n\n\n<font color= 'blue'>\n* Below table shows **mean and median values of each ticket class and embarkation port** in order to be able to decide what to fill with null values.","b538b2ff":"<font color ='blue'>\n    \n* Having a higher class ticket yields a better survival chance. While it's 0.63 for higher class, it gradually decreased around 0.15 and 0.25 for each class.","78f4943a":"<font color = 'blue'>\n* Value Counts of Other Categorical Features","d69a542d":"Obviously, there are some missing values in several features as well. They will be dealt with later. \n    \nBut for now, let's count and visualize them.","8dd5094e":"<font color = 'red'>\n\n* Number of Survivals w.r.t Ticket Classes and Genders","23ec1014":"<font color = 'blue'>\n* Heatmap of Survivals w.r.t Ticket Classes and Genders","90e67329":"<b id='15'>\n\n## Ticket Class ~ Survival Rate","a2ebd510":"<b id=21>\n\n## Drop Unnecessary Features\n\n<font color='blue'>\n\n---\nIn this section, features given below will be dropped:\n<font color='red'>\n\n* level_0\n* Cabin\n* Name\n* Parch\n* PassengerId\n* SibSp\n* index\n* SexLabeled","15760c64":"<a id = \"11\"><\/a>\n\n## Age","87e479b7":"<font color= 'red' >\n* Let's fill the null values with median of 'Age' values having the same corresponding values of 'Pclass', 'SibSp' and 'Parch'.","e1c269b1":"<b id='13'>\n## Number of Siblings ~ Survival Rate","b09c8c02":"**Data Types**\n\n<font color = 'blue'>\n\n* float64(2) : Age, Fare\n* int64(4)   : PassengerId, Survived, Pclass, SibSp, Parch\n* object(5)  : Name, Sex, Ticket, Cabin, Embarked","1f865c8c":"<font color = 'blue'>\n* Counts of Categorical Features","63944467":"<font color='blue'>\n* Having siblings more than 2 may be thought as an indication of lower survival rate.\n\n---\n<font color='green'>\n    \n0.3 can be a threshold for a new feature extraction from the graph.\n\n\n---\n<font color='red'> \nNotice that the lines in the middle of the bars represent the confidence intervals drawn by bootstrapping, and they are inversely proportional with the count of each category. \n\n<font color='blue'> \n* SE = std \/ (n^(1\/2)\n* confidence interval = {mean(x) -2x(SE), mean(x) +2x(SE)}","6bb5cbfd":"<b id=22>\n\n## One-hot Encoding\n\n\n---\n**Categorical features** must be one-hot encoded before applying data mining techniques.\n\nAlso, one of the one-hot columns must be dropped to avoid multicollinearity leading the lower accuracy of created models.\n\n<font color='blue'>\n* get_dummies() method of Pandas does all these alone.\n* **'drop_first = True'** parameter removes the first dummy variable.\n    \n<font color='green'>\nFeatures to be one-hot encoded:\n* Embarked\n* Pclass\n* Sex\n* Title\n* Family Size\n* Ticket","f1d08ea0":"<font color='green'>\n\nWe can determine some thresholds for rearraging the new feature:\n<font color='blue'>\n\n* 0 if p < 0.2\n* 1 if p < 0.4 \n* 2 if p > 0.4","d72e3ad6":"<font color='red'>\nThe same issue shows up [here](#12) again in some plots. The rightness of the plots can be verified through the information right above them.\n    \n<font color='blue'>\n    \n* Given the ports C and S; passengers who paid more fare tend to have a higher rate of survival regardless of their sex.\n\n* As to the passengers boarded at port C, the survival rate seems to be unassociated with the fare.\n\n* To sum up, the higher fare a passenger pays, the higher survival rate they have.\n \n<font color='green'>\n\nFair can be converted to a categorical feature for training.","9ceca69a":"<font color='blue'>\n* Majority of 20-30 year-olds couldn't survive.\n* In contrast, 30-40 year-olds mostly survived.\n* The highest survival rates are for the infants **(<10)** as seen in the figures.\n\n---\n<font color='green'>\n\nThus, life cycle can be a criterion in predicting survivors, furthermore, this feature can be converted to categorical. ","fea682a8":"We load both train and test as pandas dataframes from relevant paths.","f3cde37e":"<a id = \"8\"><\/a>\n\n# 5. Anomaly Detection & Elimination","a572b02c":"<font color='blue'> \n\n* Having more than 3 parent \/ children may be a criterion of lower survival rate.\n\n\n","58bb78c3":"<font color ='blue'>\n    \nWhile ticket classes got higher, the median of ages gradually increased by nearly same ratio for both males and females.\n<font color='green'>\n\n----\n\n* So, 'Pclass' feature may be convenient to fill null Age values.","3536ff74":"<b id = \"6\">\n\n# 4. Basic Data Analysis","95c62940":"<b id='16'>\n\n## Age ~ Survival Rate","e575f3b7":"<a id = \"3\"><\/a>\n\n# 3. Univariate Analysis\n\n<font color = 'blue'>\n    \n**Categorical Features:** <font color = 'black'>Survived, Pclass, Name, Sex, Ticket, Cabin, Embarked, SibSp, Parch,\n\n<font color = 'blue'>\n**Numerical Features:** <font color = 'black'>PassengerId, Age, Fare","ecfd0a16":"<font color='blue'>\nAs seen in the above graph(heatmap), the features having some correlation with Age are 'Pclass', 'SibSp' and 'Parch' at 0.41, -0.24, -0.15 respectively. ","611696d4":"<a id = \"9\"><\/a>\n\n## Filling Missing Values\n","e02b8852":"### Family Size","5b2c2c2c":"<a id = \"2\"><\/a>\n\n\n\n1. **PassengerId :** unique ID number to each passenger\n1. **Survived :** Survived(1) or Died(0) passenger\n1. **Pclass :** Ticket class of each passenger; 1(upper class), 2(medium class), 3(lower class)\n1. **Name :** Passenger name\n1. **Sex :** Sex of passenger\n1. **Age :** Age of passenger\n1. **SibSp :** # of siblings\n1. **Parch :** # of parents \/ children\n1. **Ticket :** Ticker number\n1. **Fare :** Amount of money spent on ticket\n1. **Cabin :** Cabin number\n1. **Embarked :** Port of embarkation","2e0ff13c":"<b id= 100>\n## Cabin","9f860686":"<b id=25> \n## Basic ML Classification Algorithms","03b16116":"* Embarked","fc0f1860":"<font color='blue'>\n\n* Now, label encoding of new categories.","2da69cdb":"<a id='12'>\n<font color='red'>\n\nAs might have seen in the information above the plot, the **MIDDLE FIGURE** is wrong: hue must be exact opposite.\n\n* Green: Female\n* Orange: Male\n\n------\n<font color='blue'>\n\n* For all classes and embarkation ports, females has much higher survival ratio than males. \n\n\n---\n<font color='green'>\n\nThese features can directly be used in training phase.","09589266":"### Title","62170771":"<a id = \"2\"><\/a>\n\n## 2. Data Description & Overview","480bc69a":"<b id = \"12\">\n\n\n# 6. Visualizations & Brainstorming","4271a7e8":"<b id='7'>\n\n## Pclass-Age-Survived","762a7d2f":"<a id = \"1\"><\/a>\n## 1. Loading Data","142e66aa":"<font color= 'blue'>\n* Features Corresponding to Fare null: <font color= 'red'> Embarked: S - Pclass: 3\n\n\n<font color= 'black'>\nThe closest corresponding Fare for those values: --> <font color= 'blue'>\n**Median = 8.05** ","1e4acd81":"<font color = 'blue'>\n* Distributions of Age and Fare w.r.t. Ticket Class","734d602c":"<b id=20>\n\n# 8. Preprocessing","b29b5841":"<a id = \"4\"><\/a>\n\n## Categorical Variable Analysis\n\n","e76cb946":"<b id=27>\n\n### 2. Boosting Methods\n\n#### Used Methods\n<font color=blue>\n\n* XG Boost\n    \n    \n* Gradient Tree Boosting\n\n\n* LightGBM\n\n\n* CatBoost","1deefb10":"### Ticket\n\n---\n<font color='blue'>\n* Tickets with the same codes will be grouped.\n\n* Tickets without a prior code (only digits) will be group in a category called 'NC'","fe328aad":"<b id=19>\n# 7. Feature Engineering","ab0bdade":"<b id='17'>\n## Pclass-Embarked-Sex-Survived","ea2e682e":"<a id = \"5\"><\/a>\n\n### Numerical Analysis","753cf22b":"<b id=24>\n\n# 9. Classification\n---\n\nMethods that will be used:\n<font color='blue'>\n* Logistic Regression\n* Linear Discriminant Analysis\n* Decision Trees\n* Random Forest\n* Support Vector Machines","8a7f46c5":"<b id=26>\n\n## Ensemble Modelling\n\n### 1. Voting Classifier\n<font color=blue>\n\n\n---\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities to predict the class labels.","f3867cde":"<b id='14'>\n\n## Number of Parent \/ Children ~ Survival Rate","b09e1151":"<font color='blue'>\n    \n'SibSp' and 'Parch' features seem to have a negative and positive correlation with Age respectively.\n<font color='green'>\n\n---\nThese values also can be used to fill null values of Age.\n\n\n\nLet's see the actual relationship among these features.","172fc5ff":"<b id=28>\n\n### 3. Stacking\n","6dc43b24":"<font color='blue'>\n    \n* Aggregate rare titles -->  <font color='red'>whose # <50"}}