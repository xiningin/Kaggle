{"cell_type":{"d22dc237":"code","9dc0e10a":"code","cd48e211":"code","48bb6bec":"code","a2e697b5":"code","abd2a134":"code","33421824":"code","b8757278":"code","f7e356c8":"code","bf659d57":"code","f4a48b46":"code","0e90f012":"code","8e8d8941":"code","14db806f":"code","157f8c84":"code","b9072f89":"code","27de267b":"code","1e5ec718":"code","5cd546c0":"code","f232aaeb":"code","3222cd1c":"code","29c2130b":"code","a9efd241":"code","c84f2564":"code","46ef0ab6":"code","dc7a9c2f":"code","1d67b501":"code","38c2e43f":"code","effbd95f":"code","e1100a0c":"code","1449c499":"code","28f87524":"code","3bedeef4":"code","f1bcc16b":"code","ac4be5a0":"code","fa47f689":"code","644eb874":"code","10992417":"markdown","4697e1f6":"markdown","a671fdec":"markdown","4101e9dd":"markdown","6a5d256d":"markdown","010e4aff":"markdown","8a3119b1":"markdown","4ea7c85b":"markdown","f6fb4011":"markdown","9432d021":"markdown","f4986b04":"markdown","ba02bab1":"markdown","e08f6a6f":"markdown","325ad8d4":"markdown","2d4a5ac3":"markdown","92ad38db":"markdown","7bd5ff51":"markdown","b839ebef":"markdown","9924e4d9":"markdown","11522fd1":"markdown","cde943c5":"markdown","be5e3ae1":"markdown","6c12eca5":"markdown","1da86654":"markdown","3b179173":"markdown","aae45b58":"markdown","48fc787a":"markdown","9f6daa98":"markdown","a6cbfd89":"markdown","075def19":"markdown","19f2cd93":"markdown","4f48ad97":"markdown","61878bbb":"markdown","12f963c5":"markdown"},"source":{"d22dc237":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline ","9dc0e10a":"IMG_ROWS = 28\nIMG_COLS = 28\nNUM_CLASSES = 10\nPATH=\"..\/input\/\"\nprint(os.listdir(PATH))","cd48e211":"train_images = np.load(os.path.join(PATH,'train-imgs.npz'))['arr_0']\ntest_images = np.load(os.path.join(PATH,'test-imgs.npz'))['arr_0']\ntrain_labels = np.load(os.path.join(PATH,'train-labels.npz'))['arr_0']","48bb6bec":"char_df = pd.read_csv(os.path.join(PATH,'classmap.csv'), encoding = 'utf-8')","a2e697b5":"print(\"KMNIST train shape:\", train_images.shape)\nprint(\"KMNIST test shape:\", test_images.shape)\nprint(\"KMNIST train shape:\", train_labels.shape)","abd2a134":"print(\"KMNIST character map shape:\", char_df.shape)","33421824":"char_df","b8757278":"print('Percent for each category:',np.bincount(train_labels)\/len(train_labels)*100)","f7e356c8":"labels = char_df['char']\nf, ax = plt.subplots(1,1, figsize=(8,6))\ng = sns.countplot(train_labels)\ng.set_title(\"Number of labels for each class\")\ng.set_xticklabels(train_labels)\nplt.show()    ","bf659d57":"def plot_sample_images_data(images, labels):\n    plt.figure(figsize=(12,12))\n    for i in tqdm_notebook(range(10)):\n        imgs = images[np.where(labels == i)]\n        lbls = labels[np.where(labels == i)]\n        for j in range(10):\n            plt.subplot(10,10,i*10+j+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(imgs[j], cmap=plt.cm.binary)\n            plt.xlabel(lbls[j])","f4a48b46":"plot_sample_images_data(train_images, train_labels)","0e90f012":"# Since I noticed that there are several null images (all zeros), I decided to remove them \n# from the training set in order to remove noise.\n# There's probably a better way to do this...\n\n# Get indexes of null images. \ntrain_images = train_images.flatten().reshape(-1, 28*28)\nidxs =  np.argwhere(np.all(train_images == 0, axis=1)).flatten()\n\ndef plot_nulls(images, labels):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,5, figsize=(12,12))\n\n    for i, img in enumerate(images):\n        ax[i\/\/5, i%5].imshow(img.reshape(IMG_ROWS,IMG_COLS))\n        ax[i\/\/5, i%5].axis('off')\n        ax[i\/\/5, i%5].set_title(\"Label:{}\".format(labels[i]))\n    plt.show()  \n    \nplot_nulls(train_images[idxs], train_labels[idxs])\n\n#remove images and labels using the index array.\ntrain_images = np.delete(train_images, idxs, 0).reshape(-1, 28, 28) \ntrain_labels = np.delete(train_labels, idxs, 0) \n\nprint(train_images.shape)\nprint(train_labels.shape)\n\nNUM_IMAGES = train_images.shape[0]\n\n# keep preprocessing function for rescaling images.\ndef data_preprocessing(images):\n    num_images = images.shape[0]\n    x_shaped_array = images.reshape(-1, IMG_ROWS, IMG_COLS, 1)\n    out_x = x_shaped_array \/ 255\n    return out_x","8e8d8941":"# Rescale and reshape\nX = data_preprocessing(train_images)\ny = keras.utils.to_categorical(train_labels, NUM_CLASSES)\nX_test = data_preprocessing(test_images)","14db806f":"#90\/10 split worked best for this model. Tried K-Fold, but it did not improve the score and seemed to overfit.\n#Added stratify parameter since a shorter validation set resulted in a bit of class imbalance\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, stratify=y)","157f8c84":"print(\"KMNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\nprint(\"KMNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\nprint(\"KMNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])\n","b9072f89":"def plot_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    f, ax = plt.subplots(1,1, figsize=(12,4))\n    g = sns.countplot(ydf[0], order = np.arange(0,10))\n    g.set_title(\"Number of items for each class\")\n    g.set_xlabel(\"Category\")\n            \n    plt.show()  \n\ndef get_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    # Get the count for each label\n    label_counts = ydf[0].value_counts()\n\n    # Get total number of samples\n    total_samples = len(yd)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = label_counts.index[i]\n        label_char = char_df[char_df['index']==label]['char'].item()\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{}({}):   {} or {}%\".format(label, label_char, count, percent))\n    \nplot_count_per_class(np.argmax(y_train,axis=1))\nget_count_per_class(np.argmax(y_train,axis=1))","27de267b":"plot_count_per_class(np.argmax(y_val,axis=1))\nget_count_per_class(np.argmax(y_val,axis=1))","1e5ec718":"# Nothing surprising in here. A deep CNN.\n# BatchNormalization and Dropout to avoid overfitting.\n\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(name=\"Conv-1-64\", filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (IMG_ROWS,IMG_COLS,1)))\nmodel.add(Conv2D(name=\"Conv-2-64\", filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-1\", pool_size=(2,2), strides=(1,1)))\nmodel.add(BatchNormalization(name=\"BatchNorm-1\"))\n\nmodel.add(Conv2D(name=\"Conv-3-64\", filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(name=\"Conv-4-64\",filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-2\", pool_size=(2,2), strides=(1,1)))\nmodel.add(BatchNormalization(name=\"BatchNorm-2\"))\n\nmodel.add(Conv2D(name=\"Conv-5-128\", filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(name=\"Conv-6-64\",filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-3\", pool_size=(2,2), strides=(1,1)))\nmodel.add(BatchNormalization(name=\"BatchNorm-3\"))\n\nmodel.add(Conv2D(name=\"Conv-7-256\", filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(name=\"Conv-8-256\", filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-4\", pool_size=(2,2), strides=(2,2)))\nmodel.add(BatchNormalization(name=\"BatchNorm-4\"))\n\nmodel.add(Conv2D(name=\"Conv-9-256\", filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(name=\"Conv-10-256\", filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-5\",pool_size=(2,2), strides=(2,2)))\nmodel.add(BatchNormalization(name=\"BatchNorm-5\"))\n\nmodel.add(Conv2D(name=\"Conv-11-256\", filters = 512, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(name=\"Conv-12-256\", filters = 512, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPooling2D(name=\"MaxPool-6\", pool_size=(2,2), strides=(2,2)))\nmodel.add(BatchNormalization(name=\"BatchNorm-6\"))\n\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","5cd546c0":"datagen = keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=10, \n        zoom_range = 0.2, \n        shear_range=0.3,\n        width_shift_range=0.2, \n        height_shift_range=0.2, \n        horizontal_flip=False, # No flipping\n        vertical_flip=False)  # No flipping\n","f232aaeb":"optimizer = keras.optimizers.Adam(lr=0.001, amsgrad=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n\n# Adding callbacks\nhistory = keras.callbacks.History()\nlearning_rate_annealing = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0000001)\n\ncheckpoint = keras.callbacks.ModelCheckpoint('best_model', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\ncallbacks = [learning_rate_annealing, checkpoint]","3222cd1c":"model.summary()","29c2130b":"history = model.fit(datagen.flow(X_train,y_train, batch_size=512),\n          epochs=50,\n          verbose=1,\n          callbacks=[learning_rate_annealing, checkpoint],\n          validation_data=(X_val, y_val))","a9efd241":"model.load_weights('best_model')","c84f2564":"def plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = range(len(acc))\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, 'g', label='Training accuracy')\n    ax[0].plot(epochs, val_acc, 'r', label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, 'g', label='Training loss')\n    ax[1].plot(epochs, val_loss, 'r', label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()\nplot_accuracy_and_loss(history)","46ef0ab6":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_val)\n#get the indices to be plotted\ny_true = np.argmax(y_val,axis=1)","dc7a9c2f":"correct = np.nonzero(predicted_classes==y_true)[0]\nincorrect = np.nonzero(predicted_classes!=y_true)[0]","1d67b501":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","38c2e43f":"target_names = [\"Class {} ({}):\".format(i, char_df[char_df['index']==i]['char'].item()) for i in range(NUM_CLASSES)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","effbd95f":"def plot_images(data_index,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,5, figsize=(12,12))\n\n    for i, indx in enumerate(data_index[:25]):\n        ax[i\/\/5, i%5].imshow(X_val[indx].reshape(IMG_ROWS,IMG_COLS), cmap=cmap)\n        ax[i\/\/5, i%5].axis('off')\n        ax[i\/\/5, i%5].set_title(\"True:{}  Pred:{}\".format(y_true[indx],predicted_classes[indx]))\n    plt.show()    \n\nplot_images(correct, \"Greens\")","e1100a0c":"plot_images(incorrect, \"Reds\")","1449c499":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          cmap=plt.cm.Blues):\n\n    title = 'Confusion matrix'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_true, predicted_classes, classes=[c for c in range(9)])\n\nplt.show()","28f87524":"rand_idx = np.random.randint(test_images.shape[0])\nrand_img = test_images[rand_idx]\n\nplt.imshow(rand_img.reshape(IMG_ROWS,IMG_COLS))\nplt.show()\nprint(rand_img.shape)","3bedeef4":"# Get output of all convolutional blocks (Conv2D, MaxPool and BatchNorm)\nlayer_outputs = [layer.output for layer in model.layers[:24]] \n# Create a new model with only those outputs\nactivation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)","f1bcc16b":"### Taken from here: \n### https:\/\/github.com\/gabrielpierobon\/cnnshapes\/blob\/master\/README.md\n\n# Get the activations for the random image previously selected from the validation set.\nactivations = activation_model.predict(rand_img.reshape(-1, IMG_ROWS, IMG_COLS, 1)) \n\nlayer_names = []\nfor layer in model.layers[:24]:\n    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","ac4be5a0":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_test)","fa47f689":"# This script allows you to generate csv submission file and download it without a commit.\n# Useful if you don't want to wait, or if you didn't use a fixed seed like I did... \n'''\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\nsubmission = pd.read_csv(os.path.join(PATH,\"sample_submission.csv\"))\nsubmission['Class'] = predicted_classes\nsubmission.to_csv(os.path.join(\".\",\"submission.csv\"), index=False)\n\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')\n","644eb874":"submission = pd.read_csv(os.path.join(PATH,\"sample_submission.csv\"))\nsubmission['Class'] = predicted_classes\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","10992417":"### Inspect the model\n\nLet's check the model we initialized.","4697e1f6":"## <a id=\"41\">Class distribution<\/a>\n\nLet's see how many number of images are in each class. \n\n### Train set images class distribution","a671fdec":"# <a id=\"6\">Test prediction<\/a>\n\nLet's use the trained model to predict the labels for the test images.","4101e9dd":"### Data Augmentation\n\nUsing data augmentation increased the accuracy of the model. The value of the parameters are arbitrary, I played around for a bit and these worked fine. Bigger numbers lead to not generalizing well.\nVertical and Horizontal flipping are set to False since different classes of image might end up looking the same.","6a5d256d":"\nLet's visualize few images from the validation set that were correctly classified (25 images).","010e4aff":"# <a id=\"3\">Read the data<\/a>\n\nWe will read the two data files containing the 10-class data, KMNIST, similar to MNIST.\n\nThere are 10 different classes of images, one class for each number between 0 and 9. \n\nImage dimmensions are **28**x**28**.   \n\nThe train set and test set are given in two separate numpy arrays.   \n\nWe are also reading the labels for train set.\n\nAditionally, we will read the character class map for KMNIST, so that we can display the actual characters corresponding to the labels.\n","8a3119b1":"The model used here is simplified, with the purpose to serve as a  baseline. For more complex models, please consult the references.","4ea7c85b":"## <a id=\"42\">Sample images<\/a>\n\n### Train dataset images\n\nLet's plot some samples for the images.","f6fb4011":"# <a id=99>Visualizing Model Activations<\/a>","9432d021":"# <a id=\"7\">Submission<\/a>\n\nLet's prepare the submission.\n","f4986b04":"## <a id=\"54\">Validation accuracy \/ class<\/a>\n\nLet's see in detail how well are the validation set classes predicted.","ba02bab1":"## <a id=\"52\">Train the model<\/a>\n\n### Build the model   \n\n\n\n","e08f6a6f":"The dimmension of the processed train, validation and test set are as following:","325ad8d4":"### Compile the model\nWe then compile the model, with the layers and optimized defined.","2d4a5ac3":"# <a id=\"1\">Introduction<\/a>  \n\n## Dataset\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms.   \n\nKMNIST (Kuzushiji-MNIST or Cursive hiragana-MNIST)  was introduced as an alternative to MNIST. It contains images with the first entries from the 10 main Japanese hiragana character groups.\n\n\n## Content\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.   \n\nThe images are storred in numpy arrays of 50,000 x 28 x 28 and 20,000 x 28 x 28, respectively. The labels for training set are also stored in a numpy array.\n","92ad38db":"Let's see also the class distribution of validation set.","7bd5ff51":"# <a id=\"4\">Data exploration<\/a>","b839ebef":"## Parameters","9924e4d9":"<h1><center><font size=\"6\">Cursive Hiragana Images Classification<\/font><\/center><\/h1>\n\n\n<img src=\"http:\/\/su-cultural-history.up.n.seesaa.net\/su-cultural-history\/image\/P1030717.JPG?d=a48\" width=\"200\"><\/img>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Data exploration<\/a>\n    - <a href='#41'>Class distribution<\/a>\n    - <a href='#42'>Images samples<\/a>\n- <a href='#5'>Model<\/a>  \n    - <a href='#51'>Prepare the model<\/a>  \n    - <a href='#52'>Train the model<\/a>  \n    - <a href='#53'>Validation accuracy and loss<\/a>  \n    - <a href='#54'>Validation accuracy per class<\/a>  \n- <a href='#6'>Test prediction<\/a>     \n- <a href='#7'>Conclusions<\/a>\n- <a href='#8'>References<\/a>","11522fd1":"We identify the predicted class for each image by selecting the column with the highest predicted value.","cde943c5":"## <a id=\"51\">Prepare the model<\/a>\n\n## Data preprocessing\n\nFirst we will do a data preprocessing to prepare for the model.\n\nWe reshape the numpy arrays for images to associate to each image a (28 x 28 x 1) array, with values normalized.","be5e3ae1":"Let's check the class imbalance for the resulted training set.","6c12eca5":"Let's show the character map:","1da86654":"## Split train in train and validation set\n\nWe further split the train set in train and validation set. The validation set will be 20% from the original train set, therefore the split will be train\/validation of 0.9\/0.1.","3b179173":"\n## <a id=\"53\">Validation accuracy and  loss <\/a>\n\n\nWe plot accuracy for validation set compared with the accuracy of training set, for each epoch, on the same graph. Then, we plot loss for validation set compared with the loss for training set. \n","aae45b58":"### Run the model\n\nWe run the model with the training set. We are also using the validation set (a subset from the orginal training set) for validation.","48fc787a":"The dimmension of the character set data file for KMNIST are:","9f6daa98":" <a id=\"8\">References<\/a>\n\n[1] Yan LeCun, MNIST Database, http:\/\/yann.lecun.com\/exdb\/mnist\/  \n[2] DanB, CollinMoris, Deep Learning From Scratch, https:\/\/www.kaggle.com\/dansbecker\/deep-learning-from-scratch  \n[3] DanB, Dropout and Strides for Larger Models, https:\/\/www.kaggle.com\/dansbecker\/dropout-and-strides-for-larger-models  \n[4] BGO, CNN with Keras, https:\/\/www.kaggle.com\/bugraokcu\/cnn-with-keras    \n[5] Gabriel Preda, Simple introduction to CNN for MNIST (99.37%), https:\/\/www.kaggle.com\/gpreda\/simple-introduction-to-cnn-for-mnist-99-37  \n[6] Anokas, KMNIST-MNIST replacement, https:\/\/www.kaggle.com\/aakashnain\/kmnist-mnist-replacement    \n[7] Megan Risdal, Starter: Kuzushiji-MNIST, https:\/\/www.kaggle.com\/mrisdal\/starter-kuzushiji-mnist-ed86cfac-1   \n[8] Kuzushiji-MNIST, project Github repo, https:\/\/github.com\/rois-codh\/kmnist   \n[9] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha, Deep Learning for Classical Japanese Literature, https:\/\/arxiv.org\/abs\/1812.01718  \n\n","a6cbfd89":"We process both the train_data and the test_data.","075def19":"The classes are allmost equaly distributed in the train set (being ~10% each, or ~5000 items).  Let's also plot a graph for these.","19f2cd93":"![](http:\/\/)The dimmension of the original  train,  test set are as following:","4f48ad97":"Let's visualize the images from the validation set that were incorrecly classified (25 images).","61878bbb":"# <a id=\"2\">Load packages<\/a>","12f963c5":"# <a id=\"5\">Model<\/a>\n\nWe start with preparing the model."}}