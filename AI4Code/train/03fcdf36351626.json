{"cell_type":{"a12a90f3":"code","90e40a54":"code","51d1b586":"code","56cb0bad":"code","6a256180":"code","36ec3c0d":"code","575f990e":"code","6c44a4c6":"code","d3b06a9d":"code","6deaccb9":"code","38e561e8":"code","385f7cd2":"code","d1243db2":"code","8d7aa3cd":"code","50daf169":"code","662da860":"code","5f5486ca":"code","b4abe530":"code","87759eea":"code","32501864":"code","3dd1bc0c":"code","c5eda0c1":"markdown","4fe76016":"markdown","7091999e":"markdown","b61c325d":"markdown"},"source":{"a12a90f3":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nimport tensorflow as tf\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics, Model,losses\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)","90e40a54":"# CV\n\ndef mlgskf(FOLDS = 5,SEED = 42):\n    # LOAD FILES\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc<=18].index.sort_values()\n    vc2 = vc.loc[vc>18].index.sort_values()\n\n    # STRATIFY DRUGS 18X OR LESS\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 18X\n    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, \n              random_state=SEED)\n    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    # ASSIGN FOLDS\n    scored['fold'] = scored.drug_id.map(dct1)\n    scored.loc[scored.fold.isna(),'fold'] =\\\n        scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n    scored.fold = scored.fold.astype('int8')\n    return scored\n","51d1b586":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ncp_type_train = train_features['cp_type']\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id'],axis=1)\n\n# Cell and Gene Features\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\n# Label Encoder for categorical cp_dose\n\ncat = 'cp_dose'\nle = preprocessing.LabelEncoder()\nle.fit(train_features[cat])\ntrain_features[cat] = le.transform(train_features[cat])\n\n# Transform categorical\n\ntest_features[cat] = le.transform(test_features[cat])","56cb0bad":"# Data with no MoA, including controls from public test\n\nnon_moa = train_features.iloc[labels_train.sum(axis=1)==0]\nnon_moa = non_moa.append(test_features.loc[test_features['cp_type']=='ctl_vehicle'].drop('cp_type',axis=1))\n\n# Non MoA Stratifier\n\nnon_moa_stratifier = cp_type_train[labels_train.sum(axis=1)==0].append(test_features.loc[test_features['cp_type']=='ctl_vehicle','cp_type'])","6a256180":"# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]","36ec3c0d":"# Scaler for numerical values\n\nscaler = preprocessing.QuantileTransformer(output_distribution='normal')\n\n# Scale train data\ndata_train = scaler.fit_transform(train_features)\n\n# Scale Test data\ndata_test = scaler.transform(test_features.drop('cp_type',axis=1))\n\n# Scale Non MoA data\n\nnon_moa = scaler.transform(non_moa)","575f990e":"n_features = data_train.shape[1]\nn_labels = labels_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]","6c44a4c6":"# Generate swap noise on AutoEncoder inputs\n\ndef swap_noise(data_in,noise_fraction = 0.15, n_splits = 100,seed=42):\n    np.random.seed(seed)\n    data_out = []\n    for data in np.array_split(data_in,n_splits):\n        \n        n_features = data.shape[1]\n        n_samples = data.shape[0]\n\n        swap_cols = np.random.choice(np.arange(n_features),int(noise_fraction*n_features),replace=False)\n        swap_idx = np.random.permutation(np.arange(n_samples))\n        data[:,swap_cols] = data[:,swap_cols][swap_idx,:]\n        data_out.append(data)\n        \n    return np.concatenate(data_out)\n\ndef swap_data(data_in):\n    data = data_in.copy()\n    data[:,cs] = swap_noise(data[:,cs],seed=0)\n    data[:,gs] = swap_noise(data[:,gs],seed=1)\n    return data\n\nnon_moa_train, non_moa_test = train_test_split(non_moa,test_size=0.15,stratify=non_moa_stratifier.values)\nnon_moa_train_swap = swap_data(non_moa_train)\nnon_moa_test_swap = swap_data(non_moa_test)","d3b06a9d":"# Create AutoEncoders\n\nactivation = 'elu'\ndropout = 0.0\n\nclass Autoencoder(Model):\n    def __init__(self, n_features, encoding_dim):\n        super(Autoencoder, self).__init__()        \n        self.encoder = Sequential([\n            layers.Dropout(dropout),\n            layers.BatchNormalization(),\n            layers.Dense(0.5*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n#             layers.Dropout(dropout),\n            layers.Dense(0.75*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n#             layers.Dropout(dropout),\n            layers.Dense(encoding_dim,activation=activation),\n            ])\n        \n        self.decoder = Sequential([\n        layers.Dense(0.75*encoding_dim,activation=activation),\n        layers.BatchNormalization(),\n#         layers.Dropout(dropout),\n        layers.Dense(0.5*encoding_dim,activation=activation),\n        layers.BatchNormalization(),\n#         layers.Dropout(dropout),\n        layers.Dense(n_features)\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\n# Cells\n\nae_reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1E-5)\nae_early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=32, restore_best_weights=True)\n\nencoding_dim = 512\n\nautoencoder_cells = Autoencoder(cs.sum(),encoding_dim)\nautoencoder_cells.compile(optimizer=optimizers.Adam(learning_rate=1E-4), loss='mse')\nhist = autoencoder_cells.fit(non_moa_train_swap[:,cs],non_moa_train[:,cs],batch_size=128, verbose=0, \n                             validation_data = (non_moa_test[:,cs],non_moa_test[:,cs]), epochs=1024,\n                       shuffle=True, callbacks=[ae_reduce_lr, ae_early_stopping])\n\nplt.figure(figsize=(12,8))\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","6deaccb9":"# Genes\n\nencoding_dim = 1536\n\nautoencoder_genes = Autoencoder(gs.sum(),encoding_dim)\nautoencoder_genes.compile(optimizer=optimizers.Adam(learning_rate=1E-4), loss='mse')\nhist = autoencoder_genes.fit(non_moa_train_swap[:,gs],non_moa_train[:,gs],batch_size=128, verbose=0, validation_data = (non_moa_test[:,gs],non_moa_test[:,gs]),\n                             epochs=1024, shuffle=True, callbacks=[ae_reduce_lr, ae_early_stopping])\n\nplt.figure(figsize=(12,8))\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","38e561e8":"# Create Cell features based on autoencoder\n\nautoencoder_cells_train = autoencoder_cells.encoder(data_train[:,cs]).numpy()\nautoencoder_cells_test = autoencoder_cells.encoder(data_test[:,cs]).numpy()\nautoencoder_cells_non_moa_test = autoencoder_cells.encoder(non_moa_test[:,cs]).numpy()\nautoencoder_cells_error = mean_squared_error(non_moa_test[:,cs],autoencoder_cells.decoder(autoencoder_cells_non_moa_test).numpy())\nprint(\"Autoencoder Cells reconstruction error for Non MoA is \" + str(autoencoder_cells_error))\nautoencoder_cells_error = mean_squared_error(data_test[:,cs],autoencoder_cells.decoder(autoencoder_cells_test).numpy())\nprint(\"Autoencoder Cells reconstruction error for test dataset is \" + str(autoencoder_cells_error))","385f7cd2":"# Create Gene Features based on autoencoder\n\nautoencoder_genes_train = autoencoder_genes.encoder(data_train[:,gs]).numpy()\nautoencoder_genes_test = autoencoder_genes.encoder(data_test[:,gs]).numpy()\nautoencoder_genes_non_moa_test = autoencoder_genes.encoder(non_moa_test[:,gs]).numpy()\nautoencoder_genes_error = mean_squared_error(non_moa_test[:,gs],autoencoder_genes.decoder(autoencoder_genes_non_moa_test).numpy())\nprint(\"Autoencoder Genes reconstruction error for Non MoA is \" + str(autoencoder_genes_error))\nautoencoder_genes_error = mean_squared_error(data_test[:,gs],autoencoder_genes.decoder(autoencoder_genes_test).numpy())\nprint(\"Autoencoder Genes reconstruction error for test dataset is \" + str(autoencoder_genes_error))","d1243db2":"# Create Cell Features based on PCA\n\npca_cells = PCA(n_components=64)\npca_cells.fit(non_moa_train[:,cs])\npca_cells_train = pca_cells.transform(data_train[:,cs])\npca_cells_test = pca_cells.transform(data_test[:,cs])\npca_cells_error = mean_squared_error(non_moa_test[:,cs],pca_cells.inverse_transform(pca_cells.transform(non_moa_test[:,cs])))\nprint('PCA Cells Reconstruction Error for Non MoA is ' + str(pca_cells_error))\npca_cells_error = mean_squared_error(data_test[:,cs],pca_cells.inverse_transform(pca_cells.transform(data_test[:,cs])))\nprint('PCA Cells Reconstruction Error for test dataset is ' + str(pca_cells_error))","8d7aa3cd":"# Create Gene Features based on PCA\n\npca_genes = PCA(n_components=256)\npca_genes.fit(non_moa_train[:,gs])\npca_genes_train = pca_genes.transform(data_train[:,gs])\npca_genes_test = pca_genes.transform(data_test[:,gs])\npca_genes_error = mean_squared_error(non_moa_test[:,gs],pca_genes.inverse_transform(pca_genes.transform(non_moa_test[:,gs])))\nprint('PCA Genes Reconstruction Error for Non MoA is ' + str(pca_genes_error))\npca_genes_error = mean_squared_error(data_test[:,gs],pca_genes.inverse_transform(pca_genes.transform(data_test[:,gs])))\nprint('PCA Genes Reconstruction Error for test dataset is ' + str(pca_genes_error))","50daf169":"# Create reconstruction error features\n\nautoencoder_cells_error_train = mean_squared_error(data_train[:,cs].T,autoencoder_cells.decoder(autoencoder_cells.encoder(data_train[:,cs]).numpy()).numpy().T,\n                                                   multioutput='raw_values')\nautoencoder_cells_error_test = mean_squared_error(data_test[:,cs].T,autoencoder_cells.decoder(autoencoder_cells.encoder(data_test[:,cs]).numpy()).numpy().T,\n                                                 multioutput='raw_values')\npca_cells_error_train = mean_squared_error(data_train[:,cs].T,pca_cells.inverse_transform(pca_cells.transform(data_train[:,cs])).T, multioutput='raw_values')\npca_cells_error_test = mean_squared_error(data_test[:,cs].T,pca_cells.inverse_transform(pca_cells.transform(data_test[:,cs])).T, multioutput='raw_values')\n\nautoencoder_genes_error_train = mean_squared_error(data_train[:,gs].T,autoencoder_genes.decoder(autoencoder_genes.encoder(data_train[:,gs]).numpy()).numpy().T,\n                                                   multioutput='raw_values')\nautoencoder_genes_error_test = mean_squared_error(data_test[:,gs].T,autoencoder_genes.decoder(autoencoder_genes.encoder(data_test[:,gs]).numpy()).numpy().T,\n                                                 multioutput='raw_values')\npca_genes_error_train = mean_squared_error(data_train[:,gs].T,pca_genes.inverse_transform(pca_genes.transform(data_train[:,gs])).T, multioutput='raw_values')\npca_genes_error_test = mean_squared_error(data_test[:,gs].T,pca_genes.inverse_transform(pca_genes.transform(data_test[:,gs])).T, multioutput='raw_values')","662da860":"moa_train = data_train[labels_train.sum(axis=1)!=0,:]\n\npca_cells_error_non_moa = mean_squared_error(non_moa_test[:,cs].T,pca_cells.inverse_transform(pca_cells.transform(non_moa_test[:,cs])).T, multioutput='raw_values')\npca_cells_error_moa = mean_squared_error(moa_train[:,cs].T,pca_cells.inverse_transform(pca_cells.transform(moa_train[:,cs])).T, multioutput='raw_values')\nautoencoder_cells_error_non_moa = mean_squared_error(non_moa_test[:,cs].T,autoencoder_cells.decoder(autoencoder_cells_non_moa_test).numpy().T, multioutput='raw_values')\nautoencoder_cells_error_moa = mean_squared_error(moa_train[:,cs].T,autoencoder_cells.decoder(autoencoder_cells.encoder(moa_train[:,cs]).numpy()).numpy().T, \n                                                     multioutput='raw_values')\n\nplt.figure(figsize=(16, 8))\nplt.scatter(pca_cells_error_moa,autoencoder_cells_error_moa,c='red',marker='o')\nplt.scatter(pca_cells_error_non_moa,autoencoder_cells_error_non_moa,c='blue',marker='s')\nplt.legend(['MoA','No MoA'])\nplt.xlabel('PCA Cells Reconstruction Error')\nplt.ylabel('Autoencoder Cells Reconstruction Error');","5f5486ca":"pca_genes_error_non_moa = mean_squared_error(non_moa_test[:,gs].T,pca_genes.inverse_transform(pca_genes.transform(non_moa_test[:,gs])).T, multioutput='raw_values')\npca_genes_error_moa = mean_squared_error(moa_train[:,gs].T,pca_genes.inverse_transform(pca_genes.transform(moa_train[:,gs])).T, multioutput='raw_values')\nautoencoder_genes_error_non_moa = mean_squared_error(non_moa_test[:,gs].T,autoencoder_genes.decoder(autoencoder_genes_non_moa_test).numpy().T, multioutput='raw_values')\nautoencoder_genes_error_moa = mean_squared_error(moa_train[:,gs].T,autoencoder_genes.decoder(autoencoder_genes.encoder(moa_train[:,gs]).numpy()).numpy().T, \n                                                     multioutput='raw_values')\n\nplt.figure(figsize=(16, 8))\nplt.scatter(pca_genes_error_moa,autoencoder_genes_error_moa,c='red',marker='o')\nplt.scatter(pca_genes_error_non_moa,autoencoder_genes_error_non_moa,c='blue',marker='s')\nplt.legend(['MoA','No MoA'])\nplt.xlabel('PCA Genes Reconstruction Error')\nplt.ylabel('Autoencoder Genes Reconstruction Error');\nplt.xlim((0,2))\nplt.ylim((0,3))","b4abe530":"data_train = np.concatenate((data_train[:,:2],pca_cells_train,autoencoder_cells_train,pca_genes_train,autoencoder_genes_train,\n                            pca_cells_error_train[:,np.newaxis],autoencoder_cells_error_train[:,np.newaxis],\n                             pca_genes_error_train[:,np.newaxis],autoencoder_genes_error_train[:,np.newaxis]),axis=1)\ndata_test = np.concatenate((data_test[:,:2],pca_cells_test,autoencoder_cells_test,pca_genes_test,autoencoder_genes_test,\n                            pca_cells_error_test[:,np.newaxis],autoencoder_cells_error_test[:,np.newaxis],\n                            pca_genes_error_test[:,np.newaxis],autoencoder_genes_error_test[:,np.newaxis]),axis=1)","87759eea":"n_labels = labels_train.shape[1]\nn_features = data_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 5E-4\np_max = 1-5E-4\n\n# Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n\n# Generate Seeds\n\nseeds = [148,161,41]\nn_seeds = len(seeds)\n\n# Training Loop\n\nn_folds = 5\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nbias = tf.keras.initializers.Constant(np.log(labels_train.mean(axis=0)))\nfor seed in seeds:\n    labels_grouped = mlgskf()\n    for fold in range(n_folds):\n        train = ((labels_grouped.iloc[non_ctl_idx]['fold']!=0).values)\n        test = ((labels_grouped.iloc[non_ctl_idx]['fold']==0).values)\n        X_train = data_train[train]\n        X_test = data_train[test]\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n\n        # Define NN Model\n\n        model = Sequential()\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.7))        \n        model.add(layers.Dense(1536))\n        model.add(layers.Activation('elu'))\n        model.add(layers.Dropout(0.5))        \n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('elu'))\n        model.add(layers.Dropout(0.5))        \n        model.add(layers.Dense(512))\n        model.add(layers.Activation('elu'))\n        model.add(layers.Dropout(0.5))    \n        model.add(layers.Dense(n_labels,activation='sigmoid',bias_initializer=bias))\n        model.compile(optimizer=optimizers.Adam(learning_rate=1E-5), loss=losses.BinaryCrossentropy(label_smoothing=0.0005), metrics=logloss)\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n        def scheduler(epoch,lr):\n            if epoch%16<9:\n                lr += (1E-3)\/8\n            else:\n                lr -= (1E-3)\/8\n            return lr\n\n        lr_scheduler = callbacks.LearningRateScheduler(scheduler)\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=192,verbose=0,validation_data = (X_test,y_test),callbacks=[lr_scheduler, early_stopping])\n        model.fit(X_train,y_train, batch_size=128, epochs=192,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('Anomaly_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict(data_test)\/(n_folds*n_seeds)        ","32501864":"# Analysis of Training\n\ntf.print('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\n\nhist_trains = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_train = (hists[i]).history['logloss']\n    hist_trains.append(hist_train)\n    hist_lens.append(len(hist_train))\nhist_train = []\nfor i in range(min(hist_lens)):\n    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_train)\n\nhist_vals = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_val = (hists[i]).history['val_logloss']\n    hist_vals.append(hist_val)\n    hist_lens.append(len(hist_val))\nhist_val = []\nfor i in range(min(hist_lens)):\n    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_val)\n\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","3dd1bc0c":"# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('submission.csv', index=False)","c5eda0c1":"# Training\/Inference","4fe76016":"# MoA: Anomaly Detection\n\n* We have a lot of data in this competition which has no MoAs\n* The control data (cp_type = ctl_vehicle) has been unused so far - training the model on this data makes the scores worse.\n* However, this data could be useful in identifying which observations are \"outliers\" i.e likely to have some MoA.\n\n## Anomaly Detection\n\n* The basic idea behind anomaly detection is to create a model which generates expected outputs for the regular examples, and then generates an output with a large deviation in comparison, when fed anomalous data.\n\n* One useful metric to monitor is reconstruction error of an autoencoder - if an autoencoder trained on data without anomalies, with low reconstruction error, suddenly has a large reconstruction error for a data point, it indicates an anomaly. \n\n* Similarly, PCA reconstruction error can also be used as a metric\n\n## Application to MoA:\n\n* Consider data with a MoA as the anomaly. Then, we want to use a model trained on the data with no MoA to generate features.\n* Since we are not actually performing anomaly detection, we do not need the decoding\/reconstruction error computation step\n* Instead, we feed the encoded features as the input to the model.\n* Ideally, the encoded features will be better separated to begin with than the raw features, resulting in improved model performance.\n\n\n### References:\n* https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/185126\n* https:\/\/www.kaggle.com\/konradb\/anomaly-detection","7091999e":"# Visualization of Anomaly Detection\n\n### The scatter plot below checks whether the autoencoder and PCA reconstruction errors separate the datapoints corresponding to MoA or no MoA. Some points are very well separated, while others still have a high overlap. The PCA also currently separates the data better than the autoencoder, but there is potential for more improvement.","b61c325d":"## Updates in V16:\n\n* Change CV to [Grouped Multilabel Stratified KFold](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195) using train_drug.csv\n* Use [Denoising AutoEncoder with swap noise](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195642)\n* Separate AutoEncoders for Cell and Gene features"}}