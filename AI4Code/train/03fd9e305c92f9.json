{"cell_type":{"c1b0df8f":"code","e0689352":"code","0b76dbe5":"code","2214153e":"code","07e0cf85":"code","ccc43901":"code","e8465a4a":"code","97a0c7a3":"code","8da6d28d":"code","5488c9a9":"code","3c1bd2d8":"code","2260b9b8":"code","1fb5afa9":"code","5d21f0ff":"code","7b1dc719":"code","7e252c91":"code","9a2e84f9":"code","6300f0f0":"code","fe6d621c":"code","aca27dc8":"code","b5792660":"code","12e5cc53":"code","cccba33e":"code","daf51c36":"code","f4dbf306":"code","c8aa863c":"code","a39141a7":"code","4c3e90e8":"code","8e89cabb":"code","96de90d6":"code","46a75f9d":"code","97d8df74":"code","0de0df00":"code","c951f5b5":"code","dc3b305a":"code","25436243":"code","70404f1b":"code","9f5cc0e5":"code","476a7ccb":"code","23b33b31":"code","3e2c9b66":"code","2dcec621":"code","9cb6ac15":"code","ca2dfa43":"code","7b9551ec":"code","ae1b1f64":"code","cb7d0b69":"code","5bf8d425":"code","f9481a7f":"code","b0b53eba":"code","0a7f6cb7":"code","6befab20":"code","b704e539":"code","dfe34aa3":"code","e075b153":"code","ba9a8ad9":"code","3c987999":"code","35f4323b":"code","33eb7fb7":"code","f65c67a9":"code","9c4038f4":"code","949ee223":"code","28f4cd4e":"code","d9fa0334":"code","0eed981c":"code","2fa87ee4":"code","ca36a377":"code","224cda11":"code","e4ede406":"code","b2355749":"code","68ea5350":"code","17f5bb8f":"code","f9a9f34f":"code","89070535":"code","fdb30161":"code","05666a5c":"code","74a6fbca":"code","b3bf9e52":"code","b6eb84be":"code","f9cfd9a3":"code","b6a5826d":"code","e243a557":"code","d799e09c":"code","058341f2":"code","d300daee":"code","99ca7829":"code","66456c83":"code","4915385c":"code","79a60513":"code","accdce5f":"markdown","0910a3fc":"markdown","f638d892":"markdown","99083051":"markdown","d17cb698":"markdown","646baa4b":"markdown","6c10aca8":"markdown","e1228430":"markdown","39ff1178":"markdown","1f1ffee8":"markdown","e5684b23":"markdown","c9ff7a34":"markdown","58ba8d46":"markdown","8b98fac1":"markdown","d5915782":"markdown","5b337c83":"markdown","d34e6f90":"markdown","87a5a791":"markdown","0916cf3a":"markdown","0970f532":"markdown"},"source":{"c1b0df8f":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","e0689352":"import pandas as pd\nimport numpy as np \n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0b76dbe5":"# read the data\ntrain_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest_data  = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')","2214153e":"train_data.head()","07e0cf85":"test_data.head()","ccc43901":"train_data.shape","e8465a4a":"train_data.info()","97a0c7a3":"train_data.describe().T","8da6d28d":"index_num = list(train_data.describe().T.index)\ntrain_data_num = train_data[index_num]","5488c9a9":"train_data_num.head()","3c1bd2d8":"import matplotlib.pyplot as plt\ntrain_data.hist(bins=50, figsize=(25,20))\nplt.show()","2260b9b8":"corr_matrix = train_data_num.corr()","1fb5afa9":"corr_matrix[\"SalePrice\"].sort_values(ascending=False)","5d21f0ff":"plt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix,cmap='coolwarm',annot=True,linewidths=1)","7b1dc719":"plt.figure(figsize=(10,6))\ntrain_data_num['SalePrice'].hist(bins=50)","7e252c91":"#MSSubClass=The building class\ntrain_data['MSSubClass'] = train_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain_data['OverallCond'] = train_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntrain_data['YrSold'] = train_data['YrSold'].astype(str)\ntrain_data['MoSold'] = train_data['MoSold'].astype(str)\n\n","9a2e84f9":"train_data.info()","6300f0f0":"index_num = list(train_data.describe().T.index)\ntrain_data_num = train_data[index_num]","fe6d621c":"housing = train_data_num.drop([\"SalePrice\",'Id'], axis=1) # drop labels for training set\nhousing_labels = train_data_num[\"SalePrice\"].copy()","aca27dc8":"housing.head()","b5792660":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","12e5cc53":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n                        ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing)","cccba33e":"housing_num_tr = pd.DataFrame(housing_num_tr,columns= housing.columns,\n                              index=housing.index)","daf51c36":"housing_num_tr.info()","f4dbf306":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX = housing_num_tr #independent columns\ny = housing_labels  #target column i.e price range\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nplt.figure(figsize=(20,20))\nfeat_importances.nlargest(36).plot(kind='barh')\nplt.show()","c8aa863c":"most_30_feat= (feat_importances.nlargest(30)).index","a39141a7":"most_30_feat","4c3e90e8":"train_data_cat = train_data.drop(index_num,axis=1)","8e89cabb":"train_data_cat.info()","96de90d6":"train_data_cat.head()","46a75f9d":"train_data_cat.head()","97d8df74":"train_data_cat.mode()","0de0df00":"train_data.shape","c951f5b5":"train_data_cat.isna().sum()","dc3b305a":"count_20 = (1460\/100)*20","25436243":"count_20","70404f1b":"x = (train_data_cat.isna().sum()) > count_20","9f5cc0e5":"a = x[x!=True]\na","476a7ccb":"index_20 = list(a.index)","23b33b31":"train_cat_20 = train_data_cat[index_20]","3e2c9b66":"train_cat_20.head()","2dcec621":"def clean_20 (data):\n    count__20 = (len(data)\/100)*20\n    x = (data.isna().sum()) > count__20\n    a = x[x!=True]\n    index_20 = list(a.index)\n    clean_data = data[index_20]\n    return clean_data","9cb6ac15":"from sklearn.preprocessing import LabelEncoder\ncols = ( 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional',  'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train_cat_20[c].values)) \n    train_cat_20[c] = lbl.transform(list(train_cat_20[c].values))\n\n# shape        \nprint('Shape train_cat_20: {}'.format(train_cat_20.shape))\n\n\n","ca2dfa43":"train_cat_20 = pd.get_dummies(train_cat_20)\nprint(train_cat_20.shape)","7b9551ec":"train_cat_20.head()","ae1b1f64":"housing_num_tr.head()","cb7d0b69":"train_data_model = housing_num_tr.join(train_cat_20)","5bf8d425":"train_data_model.head()","f9481a7f":"test_data.info()","b0b53eba":"test_data_clean_20 = clean_20(test_data)","0a7f6cb7":"#MSSubClass=The building class\ntest_data_clean_20['MSSubClass'] = test_data_clean_20['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntest_data_clean_20['OverallCond'] = test_data_clean_20['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntest_data_clean_20['YrSold'] = test_data_clean_20['YrSold'].astype(str)\ntest_data_clean_20['MoSold'] = test_data_clean_20['MoSold'].astype(str)\n\n","6befab20":"index_num = list(test_data_clean_20.describe().T.index)\ntest_data_num = test_data_clean_20[index_num]","b704e539":"\ntest_data_num.drop('Id',axis=1,inplace=True)","dfe34aa3":"test_data_cat = test_data_clean_20.drop(index_num,axis=1)\n","e075b153":"test_data_cat.head()","ba9a8ad9":"test_data_tr = num_pipeline.transform(test_data_num)","3c987999":"test_data_num_tr = pd.DataFrame(test_data_tr,columns=test_data_num.columns,\n                                            index = test_data_num.index )","35f4323b":"test_data_num_tr.head()","33eb7fb7":"# cat data\nfrom sklearn.preprocessing import LabelEncoder\ncols = ( 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional',  'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(test_data_cat[c].values)) \n    test_data_cat[c] = lbl.transform(list(test_data_cat[c].values))\n\n# shape        \nprint('Shape test_data_cat: {}'.format(test_data_cat.shape))\n\n\n","f65c67a9":"test_data_cat = pd.get_dummies(test_data_cat)\nprint(test_data_cat.shape)","9c4038f4":"test_data_cat.head()","949ee223":"s=list(test_data_model.columns)\nd=list(train_data_model.columns)\ntr = train_data_model.drop(s,axis=1)\nmiss_cl=(tr.columns)\ntest_data_model = test_data_num_tr.join(test_data_cat)","28f4cd4e":"test_data_model.head()","d9fa0334":"train_data_model=train_data_model.drop(miss_cl,axis=1)","0eed981c":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_data_model, housing_labels)","2fa87ee4":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(train_data_model)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","ca36a377":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","224cda11":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(train_data_model, housing_labels)","e4ede406":"housing_predictions = tree_reg.predict(train_data_model)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","b2355749":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, train_data_model, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","68ea5350":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","17f5bb8f":"lin_scores = cross_val_score(lin_reg, train_data_model, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","f9a9f34f":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(train_data_model, housing_labels)","89070535":"housing_predictions = forest_reg.predict(train_data_model)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","fdb30161":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, train_data_model, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","05666a5c":"scores = cross_val_score(lin_reg, train_data_model, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","74a6fbca":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(train_data_model, housing_labels)","b3bf9e52":"grid_search.best_params_","b6eb84be":"grid_search.best_estimator_","f9cfd9a3":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","b6a5826d":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(train_data_model, housing_labels)","e243a557":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","d799e09c":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","058341f2":"final_model = grid_search.best_estimator_\n\nfinal_predictions = final_model.predict(test_data_model)\n","d300daee":"final_predictions","99ca7829":"submit=pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')","66456c83":"submit['SalePrice']= final_predictions","4915385c":"submit.head()","79a60513":"sumbit.to_csv('house1.csv')","accdce5f":"# Look at the Big Picture \n\nFrame the Problem \n\nSelect a Performance Measure \n\nCheck the Assumptions\n\n# Get the Data \n\nCreate the Workspace\n\nDownload the Data \n\nTake a Quick Look at the Data Structure\n\nCreate a Test Set \n\n# Discover and Visualize the Data to Gain Insights \n\nVisualizing Geographical Data \n\nLooking for Correlations \n\nExperimenting with Attribute Combinations\n\n# Prepare the Data for Machine Learning Algorithms \n\nData Cleaning\n\nHandling Text and Categorical Attributes\n\nCustom Transformers \n\nFeature Scaling\n\nTransformation Pipelines \n\n# Select and Train a Model \n\nTraining and Evaluating on the Training Set\n\nBetter Evaluation Using Cross-Validation \n\n# Fine-Tune The Model \n\nGrid Search \n\nRandomized Search \n\nEnsemble Methods\n\nAnalyze the Best Models and Their Errors \n\nEvaluate Your System on the Test Set\n\n\n ","0910a3fc":"looking at the corrolation ","f638d892":"import the important libraries","99083051":"# Fine-tune The model","d17cb698":"### categorical data ","646baa4b":"Let's look at the score of each hyperparameter combination tested during the grid search:","6c10aca8":"most important feture ","e1228430":"# House price end to end project ","39ff1178":"**prepare the testset** ","1f1ffee8":"**Getting dummy other categorical features**","e5684b23":"### Numerical data","c9ff7a34":"**Transforming some numerical variables that are really categorical**","58ba8d46":"# Select and train a model","8b98fac1":"**Label Encoding some categorical variables that may contain information in their ordering set** ","d5915782":"# clean and standerize the numerical data","5b337c83":"# Get our data\nour data was splitted to train set and test set so we are ready to discover it","d34e6f90":"showing the most important feture fro numerical data","87a5a791":"drop the coulmns that have more than 20% missing values","0916cf3a":"**we specify n_estimators=100**","0970f532":"### clean the categorical data"}}