{"cell_type":{"32d2137d":"code","b1ea1cab":"code","c040ab12":"code","be3bdfb2":"code","d6e0d54f":"code","26fee630":"code","e65f72ec":"code","67968bf2":"code","009ca533":"code","772b87d4":"code","b06004dc":"code","cf62d7f8":"code","823b1b0e":"code","fb58159a":"code","82bddd44":"code","8e3f8f86":"code","734a2ba9":"code","f762fca2":"code","5bbea0ae":"code","42a71a3e":"code","c390faaa":"code","8dbf75c3":"code","136e6cd0":"code","7229123f":"code","5a564ff2":"code","c2cea4ae":"code","f0aea70f":"code","54030c18":"code","28c44d09":"code","5b29a6d5":"code","9c563df7":"code","3f2d0167":"code","d4fd8596":"code","1bcb7f20":"code","039d3659":"code","cd493c67":"code","404828f6":"code","4f87054f":"code","05053b2f":"code","a6f92e9e":"code","073b89f5":"code","eabae6a6":"code","7ce7fbf3":"code","c31d69f1":"code","01515720":"code","7241068b":"code","77334ce4":"code","09f7f15c":"code","4a39e824":"code","7191b6f9":"markdown","0ded9009":"markdown","6348e651":"markdown","4254db75":"markdown","fd27ce47":"markdown","48aa1d5e":"markdown","4601aa1d":"markdown","8d7f9335":"markdown","0abb8cff":"markdown","c4cf8e5b":"markdown","3bfba25b":"markdown","7907f7d8":"markdown","4f3705e4":"markdown","0ff6cbcd":"markdown","a5ecee83":"markdown","fbe133f4":"markdown","9fde31d9":"markdown","1f49f86a":"markdown","4abe1e10":"markdown","db4821c8":"markdown","f6fcb23a":"markdown"},"source":{"32d2137d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1ea1cab":"# from google.colab import drive\n# drive.mount('\/content\/drive')","c040ab12":"pip install shap","be3bdfb2":"import os\nimport shap\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\nfrom keras.models import Model\nfrom keras import optimizers, applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n\n# Set seeds to make the experiment more reproducible.\n\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.compat.v1.set_random_seed(seed)\n\nseed = 0\nseed_everything(seed)\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")","d6e0d54f":"train = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\ntest = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\nprint('Number of train samples: ', train.shape[0])\nprint('Number of test samples: ', test.shape[0])\n\n# Preprocess data\ntrain[\"id_code\"] = train[\"id_code\"].apply(lambda x: x + \".png\")\ntest[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\ntrain['diagnosis'] = train['diagnosis'].astype('str')\ndisplay(train.head())","26fee630":"# Model parameters\nBATCH_SIZE = 8\nEPOCHS = 10\nWARMUP_EPOCHS = 2\nLEARNING_RATE = 1e-4\nWARMUP_LEARNING_RATE = 1e-3\nHEIGHT = 320\nWIDTH = 320\nCANAL = 3\nN_CLASSES = train['diagnosis'].nunique()\nES_PATIENCE = 5\nRLROP_PATIENCE = 3\nDECAY_DROP = 0.5","e65f72ec":"# X_train, X_val = train_test_split(train, test_size=0.2, random_state=seed)","67968bf2":"# In the first step we will split the data in training and remaining dataset\nX_train, X_rem = train_test_split(train, test_size=0.4, random_state=seed)\n\n# Now since we want the valid and test size to be equal (20% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nX_val, X_test = train_test_split(X_rem, test_size=0.5, random_state=seed)","009ca533":"print(X_train.head(5))\nprint(X_val.head(5))\nprint(X_test.head(5))","772b87d4":"print(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","b06004dc":"y_test = X_test['diagnosis']\ndisplay(X_test.head(5))\nX_test = X_test.drop(columns = ['diagnosis'])\ndisplay(X_test.head(5))\ndisplay(y_test.head(5))","cf62d7f8":"train_datagen=ImageDataGenerator(rescale=1.\/255, \n                                 rotation_range=360,\n                                 horizontal_flip=True,\n                                 vertical_flip=True)\n\ntrain_generator=train_datagen.flow_from_dataframe(\n    dataframe=X_train,\n    directory=\"..\/input\/aptos2019-blindness-detection\/train_images\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    target_size=(HEIGHT, WIDTH),\n    seed=0)\n\nvalidation_datagen = ImageDataGenerator(rescale=1.\/255)\n\nvalid_generator=validation_datagen.flow_from_dataframe(\n    dataframe=X_val,\n    directory=\"..\/input\/aptos2019-blindness-detection\/train_images\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    class_mode=\"categorical\", \n    batch_size=BATCH_SIZE,   \n    target_size=(HEIGHT, WIDTH),\n    seed=0)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow_from_dataframe(  \n        dataframe=X_test,\n        directory = \"..\/input\/aptos2019-blindness-detection\/train_images\",\n        x_col=\"id_code\",\n        batch_size=1,\n        class_mode=None,\n        shuffle=False,\n        target_size=(HEIGHT, WIDTH),\n        seed=0)","823b1b0e":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input","fb58159a":"pip install resnet","82bddd44":"\nfrom keras.applications.resnet import ResNet50, preprocess_input\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n\n\ndef create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    base_model = ResNet50(weights='imagenet', \n                                       include_top=False,\n                                       input_tensor=input_tensor)\n    \n\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)\n    x = Dense(2048, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n    model = Model(input_tensor, final_output)\n    \n    return model\n\n\n","8e3f8f86":"\nmodel = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n\nfor layer in model.layers:\n    layer.trainable = False\n\nfor i in range(-5, 0):\n    model.layers[i].trainable = True\n    \nclass_weights = class_weight.compute_class_weight('balanced', np.unique(train['diagnosis'].astype('int').values), train['diagnosis'].astype('int').values)\n\nmetric_list = [\"accuracy\"]\noptimizer = keras.optimizers.Adam(lr=WARMUP_LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=metric_list)\nmodel.summary()","734a2ba9":"STEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\n\nhistory_warmup = model.fit_generator(generator=train_generator,\n                                     steps_per_epoch=STEP_SIZE_TRAIN,\n                                     validation_data=valid_generator,\n                                     validation_steps=STEP_SIZE_VALID,\n                                     epochs=WARMUP_EPOCHS,\n                                     verbose=1).history","f762fca2":"for layer in model.layers:\n    layer.trainable = True\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\nrlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n\ncallback_list = [es, rlrop]\noptimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=metric_list)\nmodel.summary()","5bbea0ae":"history_finetunning = model.fit_generator(generator=train_generator,\n                                          steps_per_epoch=STEP_SIZE_TRAIN,\n                                          validation_data=valid_generator,\n                                          validation_steps=STEP_SIZE_VALID,\n                                          epochs=EPOCHS,\n                                          callbacks=callback_list,\n                                          verbose=1).history","42a71a3e":"# model.save(\"\/content\/drive\/MyDrive\/models\/my_model.h5\")\nmodel.save(\"\/kaggle\/output\/my_model.h5\")","c390faaa":"# model.save(\"\/content\/drive\/MyDrive\/models\/my_model_2nd.h5\")","8dbf75c3":"# new_model = tf.keras.models.load_model(\"\/content\/drive\/MyDrive\/models\/my_model.h5\")\nnew_model = tf.keras.models.load_model(\"\/kaggle\/output\/my_model.h5\")\n","136e6cd0":"# #to save warm up and fine tunning history- try\n# np.save('\/content\/drive\/MyDrive\/models\/history_warmup.npy',history_warmup)\n# np.save('\/content\/drive\/MyDrive\/models\/history_finetunning.npy',history_finetunning)\n\nnp.save('\/kaggle\/output\/history_warmup.npy',history_warmup)\nnp.save('\/kaggle\/output\/history_finetunning.npy',history_finetunning)","7229123f":"#to load saved history to plot graph - try\nhistory_warmup=np.load('history_warmup.npy',allow_pickle='TRUE').item()\nhistory_finetunning=np.load('history_finetunning.npy',allow_pickle='TRUE').item()","5a564ff2":"history = {'loss': history_warmup['loss'] + history_finetunning['loss'], \n           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'], \n           'accuracy': history_warmup['accuracy'] + history_finetunning['accuracy'], \n           'val_accuracy': history_warmup['val_accuracy'] + history_finetunning['val_accuracy']}\n\nsns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n\nax1.plot(history['loss'], label='Train loss')\nax1.plot(history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history['accuracy'], label='Train accuracy')\nax2.plot(history['val_accuracy'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","c2cea4ae":"# Create empty arays to keep the predictions and labels\nlastFullTrainPred = np.empty((0, N_CLASSES))\nlastFullTrainLabels = np.empty((0, N_CLASSES))\nlastFullValPred = np.empty((0, N_CLASSES))\nlastFullValLabels = np.empty((0, N_CLASSES))\n\n# Add train predictions and labels\nfor i in range(STEP_SIZE_TRAIN+1):\n    im, lbl = next(train_generator)\n    scores = model.predict(im, batch_size=train_generator.batch_size)\n    lastFullTrainPred = np.append(lastFullTrainPred, scores, axis=0)\n    lastFullTrainLabels = np.append(lastFullTrainLabels, lbl, axis=0)\n\n# Add validation predictions and labels\nfor i in range(STEP_SIZE_VALID+1):\n    im, lbl = next(valid_generator)\n    scores = model.predict(im, batch_size=valid_generator.batch_size)\n    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n    \n    \nlastFullComPred = np.concatenate((lastFullTrainPred, lastFullValPred))\nlastFullComLabels = np.concatenate((lastFullTrainLabels, lastFullValLabels))\ncomplete_labels = [np.argmax(label) for label in lastFullComLabels]\n\ntrain_preds = [np.argmax(pred) for pred in lastFullTrainPred]\ntrain_labels = [np.argmax(label) for label in lastFullTrainLabels]\nvalidation_preds = [np.argmax(pred) for pred in lastFullValPred]\nvalidation_labels = [np.argmax(label) for label in lastFullValLabels]","f0aea70f":"fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\nlabels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\ntrain_cnf_matrix = confusion_matrix(train_labels, train_preds)\nvalidation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n\ntrain_cnf_matrix_norm = train_cnf_matrix.astype('float') \/ train_cnf_matrix.sum(axis=1)[:, np.newaxis]\nvalidation_cnf_matrix_norm = validation_cnf_matrix.astype('float') \/ validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n\ntrain_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\nvalidation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n\nsns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=ax1).set_title('Train')\nsns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=sns.cubehelix_palette(8), ax=ax2).set_title('Validation')\nplt.show()","54030c18":"print(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds,train_labels, weights='quadratic'))\nprint(\"Validation Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels, weights='quadratic'))\nprint(\"Complete set Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds+validation_preds, train_labels+validation_labels, weights='quadratic'))","28c44d09":"# model.save(\"\/content\/drive\/MyDrive\/models\/my_model.h5\")\nmodel.save(\"\/kaggle\/output\/my_model.h5\")","5b29a6d5":"model.summary()","9c563df7":"n_explain = 2\nvalid_generator.batch_size = 10 # background dataset\nbackground, lbls = next(valid_generator)\n\nsns.set_style(\"white\")\nplt.figure(figsize=[8, 8])\nfor index, image in enumerate(background[:n_explain]):\n    plt.subplot(n_explain, 1, index+1)\n    plt.imshow(image)\n    plt.title(\"Image %s, Label: %s\" % (index, np.argmax(lbls[index])))\n    \nplt.show()","3f2d0167":"n_explain = 3\nvalid_generator.batch_size = 10 # background dataset\nbackground, lbls = next(valid_generator)\nflag=0\nc=1\n\ndef select(background, lbls):\n    flag=0\n    for index, image in enumerate(background[:n_explain]):\n        # print(np.argmax(lbls[index]))\n        if((np.argmax(lbls[index]) == 3) or (np.argmax(lbls[index]) == 4)):\n            flag = flag+1\n    \n    return background, lbls, flag\n\nbackground, lbls, c = select(background, lbls)\nwhile(c!=n_explain):\n    background, lbls = next(valid_generator)\n    background, lbls, c = select(background, lbls)\n    \n\nsns.set_style(\"white\")\nplt.figure(figsize=[8, 8])\nfor index, image in enumerate(background[:n_explain]):\n    plt.subplot(n_explain, 1, index+1)\n    plt.imshow(image)\n    plt.title(\"Image %s, Label: %s\" % (index, np.argmax(lbls[index])))\n    \nplt.show()","d4fd8596":"# from google.colab import drive\n# drive.mount('\/content\/drive')","1bcb7f20":"# explain predictions of the model on \"n_explain\" images\ne = shap.GradientExplainer(model, background)\nshap_values = e.shap_values(background)\n\n# plot the feature attributions\nshap.image_plot(shap_values, -background[:n_explain], labels=lbls, hspace=0.1)","039d3659":"import PIL\nfrom PIL import Image, ImageOps\nimport cv2","cd493c67":"from keras.applications.resnet import preprocess_ip, decode_predictions\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.models import Model, load_model\nfrom keras import backend as K\n\nDATA_PATH = '..\/input\/aptos2019-blindness-detection'\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'test_images')\n\nTRAIN_LABEL_PATH = os.path.join(DATA_PATH, 'test.csv')\ntrain_df = pd.read_csv(TRAIN_LABEL_PATH)","404828f6":"fig, ax = plt.subplots(3, 5, figsize=(20,10))\n\nimage_size = (320, 320)\nstart_index = 1000\nnum_output = 5\n\nfor idx in range(num_output):\n    index = idx\n    index += start_index\n    img_path = os.path.join(TRAIN_IMG_PATH, train_df['id_code'][index]+'.png')\n    \n    # ==================================\n    #   1. Test images visualization\n    # ==================================\n    img = load_img(img_path, target_size=image_size)\n    img = np.expand_dims(img, axis=0)\n    pred_img = preprocess_ip(img)\n    pred = model.predict(pred_img)\n    ax[0][idx].imshow(img[0])\n    ax[0][idx].set_title('ID: {}, Predict: {}'.format(train_df['id_code'][index], np.argmax(pred)))\n    \n    # ==============================\n    #   2. Heatmap visualization \n    # ==============================\n    # Item of prediction vector\n    pred_output = model.output[:, np.argmax(pred)]\n    \n    # Feature map of 'conv_7b_ac' layer, which is the last convolution layer\n    last_conv_layer = model.get_layer('conv5_block3_out')\n    \n    # Gradient of class for feature map output of 'conv_7b_ac'\n    grads = K.gradients(pred_output, last_conv_layer.output)[0]\n    \n    # Feature map vector with gradient average value per channel\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    \n    # Given a test image, get the feature map output of the previously defined 'pooled_grads' and 'conv_7b_ac'\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    \n    # Put a test image and get two numpy arrays\n    pooled_grads_value, conv_layer_output_value = iterate([pred_img])\n    \n    # Multiply the importance of a channel for a class by the channels in a feature map array\n    for i in range(int(pooled_grads.shape[0])):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n        \n    # The averaged value along the channel axis in the created feature map is the heatmap of the class activation\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    \n    # Normalize the heatmap between 0 and 1 for visualization\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n    ax[1][idx].imshow(heatmap)\n    \n    # =======================\n    #   3. Apply Grad-CAM\n    # =======================\n    ori_img = load_img(img_path, target_size=image_size)\n    \n    heatmap = cv2.resize(heatmap, image_size)\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    superimposed_img = heatmap * 0.5 + ori_img\n    cv2.imwrite('.\/grad_cam_result{}.jpg'.format(idx), superimposed_img)\n    grad_img = cv2.imread('.\/grad_cam_result{}.jpg'.format(idx))\n    \n    ax[2][idx].imshow(grad_img)\n    \nplt.show()","4f87054f":"from keras.applications.resnet import preprocess_input, decode_predictions\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.models import Model, load_model\nfrom keras import backend as K\n\nDATA_PATH = '..\/input\/aptos2019-blindness-detection'\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train_images')\n\nTRAIN_LABEL_PATH = os.path.join(DATA_PATH, 'train.csv')\ntest = X_test\ntest.insert(1, \"diagnosis\", y_test, True)","05053b2f":"# X_test = X_test.drop(columns = ['diagnosis'])\ndisplay(test)","a6f92e9e":"tryy = test\ndisplay(tryy)\n","073b89f5":"fig, ax = plt.subplots(3, 5, figsize=(20,10))\n\nimage_size = (320, 320)\nstart_index = 1000\nnum_output = 5\n\nfor idx in range(num_output):\n    index = idx\n    index += start_index\n    img_path = os.path.join(TRAIN_IMG_PATH, test['id_code'][index]+'.png')\n    \n    # ==================================\n    #   1. Test images visualization\n    # ==================================\n    img = load_img(img_path, target_size=image_size)\n    img = np.expand_dims(img, axis=0)\n    pred_img = preprocess_input(img)\n    pred = model.predict(pred_img)\n    ax[0][idx].imshow(img[0])\n    ax[0][idx].set_title('ID: {}, Predict: {}'.format(test['id_code'][index], np.argmax(pred)))\n    \n    # ==============================\n    #   2. Heatmap visualization \n    # ==============================\n    # Item of prediction vector\n    pred_output = model.output[:, np.argmax(pred)]\n    \n    # Feature map of 'conv_7b_ac' layer, which is the last convolution layer\n    last_conv_layer = model.get_layer('conv5_block3_out')\n    \n    # Gradient of class for feature map output of 'conv_7b_ac'\n    grads = K.gradients(pred_output, last_conv_layer.output)[0]\n    \n    # Feature map vector with gradient average value per channel\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    \n    # Given a test image, get the feature map output of the previously defined 'pooled_grads' and 'conv_7b_ac'\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    \n    # Put a test image and get two numpy arrays\n    pooled_grads_value, conv_layer_output_value = iterate([pred_img])\n    \n    # Multiply the importance of a channel for a class by the channels in a feature map array\n    for i in range(int(pooled_grads.shape[0])):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n        \n    # The averaged value along the channel axis in the created feature map is the heatmap of the class activation\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    \n    # Normalize the heatmap between 0 and 1 for visualization\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n    ax[1][idx].imshow(heatmap)\n    \n    # =======================\n    #   3. Apply Grad-CAM\n    # =======================\n    ori_img = load_img(img_path, target_size=image_size)\n    \n    heatmap = cv2.resize(heatmap, image_size)\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    superimposed_img = heatmap * 0.5 + ori_img\n    cv2.imwrite('.\/grad_cam_result{}.jpg'.format(idx), superimposed_img)\n    grad_img = cv2.imread('.\/grad_cam_result{}.jpg'.format(idx))\n    \n    ax[2][idx].imshow(grad_img)\n    \nplt.show()","eabae6a6":"n_explain = 3\nbackground, lbls = next(valid_generator)\n\nsns.set_style(\"white\")\nplt.figure(figsize=[12, 12])\nfor index, image in enumerate(background[:n_explain]):\n    plt.subplot(n_explain, 1, index+1)\n    plt.imshow(image)\n    plt.title(\"Image %s, Label: %s\" % (index, np.argmax(lbls[index])))\n    \nplt.show()","7ce7fbf3":"fig, ax = plt.subplots(3, 5, figsize=(20,10))\n\nimage_size = (320, 320)\nstart_index = 1000\nnum_output = 5\n\nfor idx in range(num_output):\n    index = idx\n    index += start_index\n    img_path = os.path.join(TRAIN_IMG_PATH, train_df['id_code'][index]+'.png')\n    \n    # ==================================\n    #   1. Test images visualization\n    # ==================================\n    img = load_img(img_path, target_size=image_size)\n    img = np.expand_dims(img, axis=0)\n    pred_img = preprocess_input(img)\n    pred = model.predict(pred_img)\n    ax[0][idx].imshow(img[0])\n    ax[0][idx].set_title('ID: {}, Predict: {}'.format(train_df['id_code'][index], np.argmax(pred)))","c31d69f1":"# explain predictions of the model on \"n_explain\" images\ne = shap.DeepExplainer(new_model, background)\nshap_values = e.shap_values(background)\n\n# plot the feature attributions\nshap.image_plot(shap_values, -background[:n_explain], labels=lbls, hspace=0.1)","01515720":"test_generator.reset()\nSTEP_SIZE_TEST = test_generator.n\/\/test_generator.batch_size\npreds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)\npredictions = [np.argmax(pred) for pred in preds]\n\n\nfilenames = test_generator.filenames\nresults = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})\nresults['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])","7241068b":"# Create empty arays to keep the predictions and labels\nlastFullTestPred = np.empty((0, N_CLASSES))\nlastFullTestLabels = np.empty((0, N_CLASSES))\n\n# Add train predictions and labels\nfor i in range(STEP_SIZE_TRAIN+1):\n    im, lbl = next(train_generator)\n    scores = model.predict(im, batch_size=train_generator.batch_size)\n    lastFullTrainPred = np.append(lastFullTrainPred, scores, axis=0)\n    lastFullTrainLabels = np.append(lastFullTrainLabels, lbl, axis=0)\n\n# Add validation predictions and labels\nfor i in range(STEP_SIZE_VALID+1):\n    im, lbl = next(valid_generator)\n    scores = model.predict(im, batch_size=valid_generator.batch_size)\n    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n    \n    \nlastFullComPred = np.concatenate((lastFullTrainPred, lastFullValPred))\nlastFullComLabels = np.concatenate((lastFullTrainLabels, lastFullValLabels))\ncomplete_labels = [np.argmax(label) for label in lastFullComLabels]\n\ntrain_preds = [np.argmax(pred) for pred in lastFullTrainPred]\ntrain_labels = [np.argmax(label) for label in lastFullTrainLabels]\nvalidation_preds = [np.argmax(pred) for pred in lastFullValPred]\nvalidation_labels = [np.argmax(label) for label in lastFullValLabels]","77334ce4":"fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\nlabels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\ntest_cnf_matrix = confusion_matrix(train_labels, train_preds)\n\ntest_cnf_matrix_norm = test_cnf_matrix.astype('float') \/ test_cnf_matrix.sum(axis=1)[:, np.newaxis]\n\ntest_df_cm = pd.DataFrame(test_cnf_matrix_norm, index=labels, columns=labels)\n\nsns.heatmap(test_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=ax1).set_title('Train')\nplt.show()","09f7f15c":"fig = plt.subplots(1, 1, sharex='col', figsize=(24, 8.7))\nsns.countplot(x=\"diagnosis\", data=results, palette=\"inferno\")\nsns.despine()\nplt.show()","4a39e824":"results.to_csv('submission.csv', index=False)\nresults.head(50)","7191b6f9":"# Model parameters","0ded9009":"## Quadratic Weighted Kappa","6348e651":"\n<h1><center>APTOS 2019 Blindness Detection<\/center><\/h1>\n<h2><center>Diabetic retinopathy - SHAP model explainability<\/center><\/h2>\n![](https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/APTOS%202019%20Blindness%20Detection\/aux_img.png)\n\nIn this work, I'll train a baseline ResNet50, evaluate the model, and use SHAP model explainability technique to help us better understand our model's predictions, and how we could further improve its performance.\n\n#### About [SHAP](https:\/\/github.com\/slundberg\/shap) from its source:\n\n<img src=\"https:\/\/raw.githubusercontent.com\/slundberg\/shap\/master\/docs\/artwork\/shap_diagram.png\" width=\"400\">\n\n##### SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](https:\/\/github.com\/slundberg\/shap#citations) for details).","4254db75":"Save Model","fd27ce47":"# Model Evaluation\n\n## Confusion Matrix","48aa1d5e":"# Fine-tune the complete model","4601aa1d":"# SHAP Model explainability\n\n#### About SHAP's DeepExplainer from the [source repository](https:\/\/github.com\/slundberg\/shap#deep-learning-example-with-deepexplainer-tensorflowkeras-models): \n- Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https:\/\/arxiv.org\/abs\/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc.\n\n### First let's see the images that we will explain","8d7f9335":"- The plot above explains five outputs (our five levels of diabetic retinopathy 0-5) for three different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left (they are black because most of the pixels are greater than 0), and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset, here I'm using 10 images) and the current model output. \n- Note that for the images that the label is \"1.0\" (the correct one), we a greater pink area.\n- Labels that have as much pink area as the correct one are labels that our model probably doesn't have a high confidence prediction.","0abb8cff":"## Dependencies","c4cf8e5b":"# Data generator","3bfba25b":"# Train top layers","7907f7d8":"pip install opencv-python","4f3705e4":"# TEST","0ff6cbcd":"# Predictions class distribution","a5ecee83":"## Apply model to test set and output predictions","fbe133f4":"## Train test split","9fde31d9":"## Let's try on a few more images","1f49f86a":"# Model","4abe1e10":"## Load data","db4821c8":"### Now the SHAP explanation","f6fcb23a":"# Model loss graph "}}