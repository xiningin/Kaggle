{"cell_type":{"e967b899":"code","f5ea39c4":"code","3de4a514":"code","54328792":"code","d07d2ced":"code","2142b3a1":"code","caa5c15b":"code","c22ca243":"code","941280f8":"code","7023ba81":"code","67ed59db":"code","e9ef6493":"code","60173a37":"code","829a1f71":"code","16c253f2":"code","cc17ea4b":"code","e1cae692":"code","4274fde2":"code","2896bdd6":"code","af0cb386":"code","c0d46f33":"code","92f8181a":"code","e7f1c99c":"code","66114109":"code","db126b03":"code","bdbbe0ea":"code","cbda0a94":"code","d90b0181":"code","afffbe93":"code","e05d952d":"code","3e5e68cf":"code","99a10c36":"code","93608ff7":"code","574b2173":"code","01b7e229":"code","76d63f9d":"code","f0c3591b":"code","0bff2a9d":"code","441225af":"markdown","5b669231":"markdown","326bf904":"markdown","04941ed6":"markdown","15ffbd9e":"markdown","fcd6c212":"markdown","30f74942":"markdown","5469f259":"markdown","e9be11dd":"markdown","6cd66c08":"markdown","ffa02b39":"markdown","5a43dd9c":"markdown","070e7f42":"markdown","4fa27131":"markdown"},"source":{"e967b899":"import numpy as np\nimport pandas as pd\nimport re\nimport spacy\nfrom tqdm import tqdm\nimport string\nimport ast","f5ea39c4":"from collections import Counter\nimport matplotlib.pyplot as plt\nnlp = spacy.load(\"en_core_web_sm\")\nlen(nlp.Defaults.stop_words) #No of Stop Words in Spacy Package","3de4a514":"data_dir = '..\/input\/'\ntrain = pd.read_csv(data_dir + \"train.csv\")\noverview = pd.read_csv(data_dir + \"game_overview.csv\")\ntest = pd.read_csv(data_dir + \"test.csv\")","54328792":"train.head()","d07d2ced":"overview.head()","2142b3a1":"overview_games = set(overview.title.unique())\ntest_games = set(test.title.unique())\ntrain_games = set(train.title.unique())\nprint(\"Games Titles present in the train but not in overview\", len(train_games - overview_games))\nprint(\"Games Titles present in the test but not in overview\", len(test_games - overview_games))","caa5c15b":"train = pd.merge(train,overview,left_on='title',right_on='title')\ntrain.head()","c22ca243":"test = pd.merge(test,overview,left_on='title',right_on='title')\ntest.head()","941280f8":"isascii = lambda s: len(s) == len(s.encode()) #Lambda function that compares length\nnon_ascii_idx = [idx for idx,user_review in enumerate(train['user_review'].values) if not isascii(user_review)]","7023ba81":"train.iloc[non_ascii_idx] #Heart Emojis are visible hence these strings have non ascii chars","67ed59db":"import unicodedata\ndef remove_accented_chars(text):\n    \"\"\"Takes string as input and return a normalized version of string with ascii chars only.\"\"\" \n    \"\"\"Non-ascii replaced with equivalent ascii.\"\"\"\n    text = unicodedata.normalize('NFKD', text).encode('ascii','ignore').decode('utf-8', 'ignore')\n    return text","e9ef6493":"#Let's covert an example\ntrain.loc[non_ascii_idx[0],'user_review']","60173a37":"remove_accented_chars(train.loc[non_ascii_idx[0],'user_review'])","829a1f71":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\n\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\"he's\": \"he is\",\n\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\n\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\n\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}","16c253f2":"def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text","cc17ea4b":"expand_contractions(\"This can't be true.\")","e1cae692":"remove_url = lambda text : re.sub(\"http[s]*:\/\/[^\\s]+\",\" \",text)","4274fde2":"remove_url(\"Hey!! Checkout the site here https:\/\/www.google.com or https:\/\/youtube.com\")","2896bdd6":"def cleaning_spaces_punct_specialchars(text):\n    text_sent = re.sub(\"[-!\\\"#$%&'()*+,.\/:;<=>?@\\][^_`|}{~']\",\" \",text) #replaces any of these characters with space\n    text_sent = text_sent.replace(\"\\\\\",\" \")\n    text_sent = re.sub(r'\\s+', ' ',text_sent) #Replaces more than one space with single space\n    return text_sent","af0cb386":"cleaning_spaces_punct_specialchars(\"Hi! Chir@ag Ho    w are \\\\you?\")","c0d46f33":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","92f8181a":"sents = ['This is to play the games.', 'This is playing the games.', 'This is where we played the games.']\ndocs = nlp.pipe(sents) #use nlp.pipe(\"sentence\") for only single sentence\nfor doc in docs:\n    for token in doc:\n        print(token.lemma_, end=\" \")\n    print(\"\")","e7f1c99c":"def clean_the_corpus(dataframe,col_name):\n    corpus = list(dataframe[col_name].values)\n    text_sent = [expand_contractions(sent) for sent in corpus]\n    text_sent = [re.sub(\"http[s]*:\/\/[^\\s]+\",\" \",text) for text in text_sent]\n    text_sent = [remove_accented_chars(sent) for sent in text_sent]\n    text_sent = [re.sub(\"[-!\\\"#$%&'()*+,.\/:;<=>?@\\][^_`|}{~']\",\" \",text) for text in text_sent]\n    text_sent = [text.replace(\"\\\\\",\" \") for text in text_sent]\n    text_sent = [re.sub(r'\\s+', ' ',sent) for sent in text_sent]\n    text_sent = list(map(str.lower,text_sent))\n    docs = nlp.pipe(text_sent,disable=[\"ner\",\"parser\"])\n    cleaned_corpus = []\n    for doc in docs:\n        doc_text = []\n        for token in doc:\n            if token.lemma_ != '-PRON-':\n                doc_text.append(token.lemma_)\n            else:\n                doc_text.append(token.text)\n        cleaned_corpus.append(doc_text)\n    cleaned_corpus = [\" \".join(cleaned_text) for cleaned_text in cleaned_corpus]\n    return cleaned_corpus","66114109":"cleaned_title = clean_the_corpus(train,\"title\")\ncleaned_user_review = clean_the_corpus(train, \"user_review\")","db126b03":"cleaned_overview = clean_the_corpus(train,\"overview\")","bdbbe0ea":"list(set(cleaned_title))[:5]","cbda0a94":"train['user_review'].values[0]","d90b0181":"cleaned_user_review[0]","afffbe93":"train['tags'].values[0] #Double Quotes in the beginning and end show it's a string","e05d952d":"print(type(train['tags'].values[0]))","3e5e68cf":"def tag_cleaner(df,col_name):\n    tags = df[col_name].values #list of all strings(tags) assosiated with each row.\n    tags = [ast.literal_eval(tag) for tag in tags] #Evaluates the strings into list of string for each row\n    tags = [\"_\".join(set(tag)) for tag in tags] #If we join by space then the tags like horror and psychological horror\n    #won't have any differences.\n    return np.array(tags) ","99a10c36":"cleaned_tag = tag_cleaner(train,\"tags\")","93608ff7":"cleaned_tag[0]","574b2173":"train_cleaned = pd.DataFrame()\ntrain_cleaned['title'] = cleaned_title\ntrain_cleaned['user_review'] = cleaned_user_review\ntrain_cleaned['overview'] = cleaned_overview\ntrain_cleaned['tag'] = cleaned_tag\ntrain_cleaned['year'] = train['year'].astype(str)\ntrain_cleaned['user_suggestion'] = train['user_suggestion'].values","01b7e229":"train_cleaned.head()","76d63f9d":"test_cleaned_title = clean_the_corpus(test,\"title\")\ntest_cleaned_user_review = clean_the_corpus(test, \"user_review\")\ntest_overview = clean_the_corpus(test, \"overview\")\ntest_cleaned_tag = tag_cleaner(test,\"tags\")\ntest_review_id = test['review_id']","f0c3591b":"test_cleaned = pd.DataFrame()\ntest_cleaned['review_id'] = test_review_id\ntest_cleaned['title'] = test_cleaned_title\ntest_cleaned['user_review'] = test_cleaned_user_review\ntest_cleaned['overview'] = test_overview\ntest_cleaned['tag'] = test_cleaned_tag","0bff2a9d":"test_cleaned.head()","441225af":"### 2. Dealing with contractions\n* Contractions are the contracted form of a word. Can't,isn't,won't are the contractions of cannot, is not and would not respectively. \n* Sometimes fixing the contractions into one standard form can give you the slight edge you need to jump in the rankings of kaggle and AV.\n* Below is the dictionary we will use to map contractions. You can add your own domain specific contractions which will generate better results.","5b669231":"# Meta-data\n* ### Train.csv and Test.csv\n    * **review_id** : Review Serial Number. It has nothing to do with the actual review.\n    * **title** : It's the Game Name.\n    * **year** : Year in which review was published.\n    * **user_review**: Variable Length Text which shows the feeling of a user after buying and playing the game.\n    * **user_suggestion** : tells if the user suggests other to buy the game or not. 1 Means Suggest and 0 Means No. Test.csv doesn't have this data-point since we had to predict user_suggestion for it.\n","326bf904":"Fortunately for us every game in ```train.csv``` and ```test.csv``` has an overview in ```overview.csv```.\n\nLet's merge the data.","04941ed6":"# Understanding Text Cleaning Apporaches\n\nText Analytics is the field of which finds patterns in text data, uncovers hidden insights and make sense of text which is reproducible and useful where text data directly isn't. Text analytics has many advantages \u2013 it\u2019s scalable, meaning you can analyze large volumes of data in a very short time, and allows you to obtain results in real-time. So, apart from gaining insights that help you make confident decisions, you can also resolve issues in a timely manner. \n\nText Analytics suffers from some disadvantages too but the advancement in tools has resolved most of them. The two major problems we still deal with Text Data are as following:\n1. __Unstructured and Unlabelled Data__ : Text Data keeps growing at a very steady rate. Billions of text messages are send to each other everyday so keeping track of what's important and what's not is very difficult. Text Data is also unlabelled which means for Supervised Machine Learning Tasks we have to label the data which is a very costly and slow process.\n2. __Textual Data is Subjective__ : Text is represented in a language which in itself isn't absolute rather just a tool to express your feelings toward something. Feeling something and expressing it in language can be different and it depends on the language of the speaker\/writer.\n\nA Lot of tools are present to deal with above mentioned problems. While doing Natural Language Processing, a learner must keep in mind for the both. We are focusing on the first type of problem today as the second one is more of subjective and psychological issue. \n\n### In the end of this notebook you will understand multiple challenges faced while cleaning the data and techniques to overcome them.","15ffbd9e":"## Data\nData we are going to use is from __[JANATA HACK NLP HOSTED BY ANALYTICS VIDHYA](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-nlp-hackathon\/)__. Get the data from there. Data has three csv files:\n* train.csv\n* overview.csv \n* test.csv\n\nThis data is by Analytics Vidhya and Steam. Review\/Suggestion of the games by the user and the task was to find based on the review whether the user will suggest the game or not.","fcd6c212":"## 5. Various Form of Words\nWe can come across multiple form of same words like play, playing, played etc which essentially intend toward same meaning. We can make our vocabulary drastically smaller if we stemmize or lemmatize the text data.\nStemming and Lemmatization work the same way just convert different form of words into the base word but there is only one difference stemming doesn't result in an actual word everytime but lemmatization always do. Here is an excellent [article](https:\/\/blog.bitext.com\/what-is-the-difference-between-stemming-and-lemmatization\/) which explains the difference.\n\nWe will be using spacy library for this task which makes it very simple.","30f74942":"When we lemmatize a sentence the different form of verb are rounded to the base form and the pronouns have lemma of **-PRON-**","5469f259":"## What about tags?\nTags don't need to be cleaned this way,because they have their unique problem. It looks like there is a list of tags assosiated with a game but if you check the dtype you will be it's a string which has brackets in it. So do what do we do? We directly use ast python package and convert this string-list to a single string of all tags.","e9be11dd":"## Converting all of the previous steps into one single function.\nNow we have learnt about each step seperately we will link all of them into one single step. We will create a function that takes two inputs first the name of dataframe and second the column of dataframe to clean. A series of steps will be applied on the column values and cleaned column will be returned.\n\nSteps will be applied in the following order:\n1. Expanding Contractions\n2. Removing Urls\n3. Removing Accented Chars\n4. Removing Special Chars\n5. Removing Double Backslash\n6. Converting to Lower Case\n7. Lemmatizing the Text\n\n### All of the steps mentioned here can be done quickly with spacy and sometimes the algorithms directly take care of them except lemmatization but the whole point of this notebook is too walk you through the mess of text cleaning and making you comfortable.","6cd66c08":"## Exploring problems with data\n\nOur prime focus here are the ```user_review```, ```tags``` and ```overview``` column in the train and test DataFrames.\n\nWe will fix the following problems if they are present in the data.\n\n1. ### **Dealing with non-ascii characters**\n    * This is a common issue which arises whensomeone writes in another language other than english, uses emojis or any other format that is not within the ascii range. How do we find it, simple trick if we take a string compare it's length to the length of it's 'UTF-8' encoded version, only if the length is same in both case we say the string is all ASCII otherwise no.\n\n    * Once we have spotted non-ascii strings, what do we do with them? \n    **We first normalize the strings**. [Wikipedia - Unicode Equivalence](https:\/\/en.wikipedia.org\/wiki\/Unicode_equivalence) and [Why we need Unicode Normalization?](https:\/\/withblue.ink\/2019\/03\/11\/why-you-need-to-normalize-unicode-strings.html) are two excellent read on this matter. I strongly suggest going through them because these will keeping popping up everytime you do something in Natural Language Processing.\n    \n    Once we have normalized them **we encode the normalized string to ascii** so all non ascii chars get removed. **We decode this string back to 'UTF-8'** because that's the standard format used by programming languages otherwise nature of string will change and many functionalities won't work as designed.","ffa02b39":"## Merging Data Together\n\nBefore merging the data together let's check if any of the game in ```train.csv``` or ```test.csv``` doesn't have an overview. If a games doesn't have an overview we might have to change our strategy from joining them directly to considering what happens to the rows with the missing overview, do we drop them or fill them?","5a43dd9c":"## 4. Dealing with Extra Spaces, Special Characters, Punctuations\n\n* Extra spaces have no information in them, special characters mostly are there for some specific, rare meaning which is not genrally utilized well by conventional Machine Learning Algorithms. Punctations do impart some meaning about the grammatical sense of sentence. You can keep them but here we will see how to remove them all.","070e7f42":"## 3. Dealing with URLs\n\nUrls generally are not the type of text we need because they don't some semantic meaning. They are just shared within the text, however adding some technique that can hit the urls and add a quick short summary that is relevant the review would be awesome but unfortunately I haven't seen any technique like that by now. So we will focus only on removing them and subtituting them with a space character.","4fa27131":"## Meta-Data\n* overview.csv\n    * title : It's the Game Name.\n    * developer : Name of the developer of the game.\n    * publisher : Name of the publisher of the game.\n    * tags : These are the tags assosiated with the game and quickly summarize the game-play and specialities of the game.\n    * overview : Summary of the game as published by the publishers on the STEAM."}}