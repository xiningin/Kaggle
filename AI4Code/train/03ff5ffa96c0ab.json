{"cell_type":{"2bec7354":"code","5779d2f6":"code","451f29e4":"code","8fed2916":"code","b47e9c33":"code","f8ba1403":"code","5cbf2fd2":"code","7510f98b":"code","9f9bc605":"code","a415bc49":"code","48084d0e":"code","516ef1a7":"code","65f927c5":"code","695ce1a3":"code","3e39ed63":"code","b24502f3":"code","50f52466":"code","f29ade02":"code","6845d227":"code","14bb8563":"code","73291356":"code","c10e09e3":"markdown","8e99630e":"markdown","b2f738fd":"markdown","6ac9b747":"markdown","6217cadd":"markdown","1b46c477":"markdown","8ee986d2":"markdown"},"source":{"2bec7354":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5779d2f6":"from sklearn.datasets import load_wine\nwine_data = load_wine()\nwine_df = pd.DataFrame(wine_data.data,\n                      columns= wine_data.feature_names)","451f29e4":"print(wine_df.shape)\nprint(wine_df.columns.values)","8fed2916":"#approach 1\n#wine_df.select_dtypes(exclude=['object']).columns\n\n#approach 2\nnumerical_cols = [col for col in wine_df.columns\n                 if wine_df[col].dtypes == ('float64')]\nprint(numerical_cols)","b47e9c33":"#Approach 1\n#missing_val_in_cols = [col for col in wine_df.columns\n#                        if wine_df[col].isnull().sum()]\n#print(\"Missing Values in Columns: \", missing_val_in_cols)\n\n#Approach 2\nmissing_val_in_cols =  wine_df.columns.isnull().sum()\nprint(\"Missing Values in Columns: \", missing_val_in_cols)","f8ba1403":"#decsribe first 5 columns of data\nwine_df.iloc[:,:5].describe()","5cbf2fd2":"#get missng values  and type of first 5 columns of data using info()\nwine_df.iloc[:,:5].info()","7510f98b":"wine_df.head()","9f9bc605":"import seaborn as sns\nsns.pairplot(wine_df)\n#shows Histogram of individual features, and scatter plot with each other features\n#Used to incpect\/extract useful features","a415bc49":"#we select two features\nX = wine_df[['alcohol','total_phenols']]\nX.head(10)","48084d0e":"#scalling Feature to get better results for clustring\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(X)\nprint(\"Mean : \", scale.mean_)\nprint(\"Standard deviation : \", scale.scale_)\n#Mean of alcohol = 13.00061798              #Standard deviation of alcohol = 2.29511236\n#Mean of total_phenols =  0.80954291        #Standard deviation of total_phenols = 0.62409056","516ef1a7":"print(\"First 5 elements Before scaled!\\n\",X[:5])\n\nscaled_X = scale.transform(X)\nprint(\"*\"*40)\nprint(\"First 5 elements After scaled!\\n\",scaled_X[:5])\n\n#SD formula of scaling is\n# scaling b\/t (0 - 1) = X[i] - mean \/ SD\n# Example scaling of first element of alcohol\n#  = 14.23 - 13.00061798  \/ 0.80954291\n#  = 1.51861254","65f927c5":"from sklearn.cluster import KMeans\n#instantiate the model \nkmeans = KMeans(n_clusters = 3, random_state=1)\n#fit the model\nkmeans.fit(scaled_X)\n#make predictions\ny_pred = kmeans.predict(scaled_X)\nprint(y_pred)\n# there are 65 wines in cluster 0,\n# 60 in cluster 1, and 53 in cluster 2\ncount_1, count_2, count_3= 0,0,0\nfor i in range(len(y_pred)):\n    if y_pred[i] == 1:\n        count_1 = count_1 + 1\n    elif y_pred[i] == 2:\n        count_2 = count_2 + 1\n    else:\n        count_3 = count_3 + 1\nprint(\"Cluster[0] counts=\", count_1)\nprint(\"Cluster[1] counts=\", count_2)\nprint(\"Cluster[2] counts=\", count_3)","695ce1a3":"#cordinates of the three centroids\nkmeans.cluster_centers_","3e39ed63":"import matplotlib.pyplot as plt\n#plot the scaled data\nplt.scatter(scaled_X[:,0], scaled_X[:,1], c= y_pred)\n#identify the centroids\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,0],\n            marker=\"*\", s= 250, c= [0,1,2], edgecolors='k')\n#set x_label\nplt.xlabel('alcohol')\n#set y_label\nplt.ylabel('total phenols')\n#set title\nplt.title('k_means (k=3)')\nplt.show()\n\n#scaled_X[:,0]-->means all rows of column[alcohol]","b24502f3":"#suppose alcohol = 13, total phenol= 2.5, to which clutser it belongs?\ncheck = np.array([[13, 2.5]])\n#scaled the values\nscaled_check = scale.transform(check)\n#prediction\nprint(\"It belongs to Cluster\",kmeans.predict(scaled_check))","50f52466":"#Inertia tells the tightness(sum of squares of the distance from data point to its nearestr centroid)\nprint(\"Inertia at k=3: \",round(kmeans.inertia_,2))","f29ade02":"#calculate inertia for k=1 to k=10\ninertia = []\nRange = np.arange(1,11)\nfor i in Range:\n    km = KMeans(n_clusters=i)\n    km.fit(scaled_X)\n    inertia.append(km.inertia_)\n\n#ploting\nplt.plot(Range, inertia, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n#Optimal k should be that where the inertia is no longer to decrease rapidly\n#in our case k=3 is best","6845d227":"X_all = wine_df\n#scaling\nscale_all = StandardScaler()\nscale_all.fit(X_all)\nscaled_all_X = scale_all.transform(X_all)\n#get optimal k\ninertia = []\nRange = np.arange(1,11)\nfor i in Range:\n    km = KMeans(n_clusters=i)\n    km.fit(scaled_all_X)\n    inertia.append(km.inertia_)\n\n#ploting\nplt.plot(Range, inertia, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n#again optimal k=3","14bb8563":"optimal_k=3\nkmean_all = KMeans(n_clusters = optimal_k, random_state=1)\nkmean_all.fit(scaled_all_X)\ny_pred_all = kmean_all.predict(scaled_all_X)\nprint(y_pred_all)\ncount_all_1, count_all_2, count_all_3= 0,0,0\nfor i in range(len(y_pred)):\n    if y_pred[i] == 1:\n        count_all_1 = count_all_1 + 1\n    elif y_pred[i] == 2:\n        count_all_2 = count_all_2 + 1\n    else:\n        count_all_3 = count_all_3 + 1\nprint(\"Cluster[0] counts=\", count_all_1)\nprint(\"Cluster[1] counts=\", count_all_2)\nprint(\"Cluster[2] counts=\", count_all_3)","73291356":"import matplotlib.pyplot as plt\n#plot the scaled data\nplt.scatter(scaled_all_X[:,0], scaled_all_X[:,1], c= y_pred_all)\n#identify the centroids\nplt.scatter(kmean_all.cluster_centers_[:,0], kmean_all.cluster_centers_[:,0],\n            marker=\"*\", s= 250, c= [0,1,2], edgecolors='k')\n#set x_label\nplt.xlabel('alcohol')\n#set y_label\nplt.ylabel('total phenols')\n#set title\nplt.title('k_means (k=3)')\nplt.show()","c10e09e3":"To inspect results visually","8e99630e":"How many clusters are best for modeling?","b2f738fd":"Modeling with all features ","6ac9b747":"make a test to our predictions","6217cadd":"K_means Modeling","1b46c477":"As we compare, there is same values for both models","8ee986d2":"plot the inertia for different values of k"}}