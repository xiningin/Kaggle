{"cell_type":{"5485a806":"code","6507799f":"code","9da264cd":"code","75c926d4":"code","af5869f9":"code","8ce551ee":"code","9be6d393":"code","0fb7b3b2":"code","9a60a30b":"code","a596ca63":"code","ffccf64a":"code","45e3a9d8":"code","b6448c74":"code","ca747f68":"code","b737dcea":"code","4de621e6":"code","4b926846":"code","08b4449f":"code","e3fcfd25":"code","d98869ec":"code","7d4d124d":"code","61fc2976":"code","ce2a2663":"code","7c94519e":"code","4cde4e40":"code","71d36247":"code","b40e41d4":"code","94818a21":"code","e7e5715b":"code","1219d8ad":"code","97ee3c17":"code","9c572da0":"code","0e54fe70":"code","6db96d58":"code","69618441":"code","f67e7665":"code","60fdbe76":"code","02724ed1":"code","fa60d3a0":"code","14788d43":"markdown","972324a7":"markdown","71aae77a":"markdown","4c36290d":"markdown","4ac5b06d":"markdown","d98d41d2":"markdown","9eada975":"markdown","46f81149":"markdown","425b6e5c":"markdown"},"source":{"5485a806":"import warnings \nwarnings.filterwarnings('ignore')","6507799f":"import tensorflow as tf\nprint('TF version:',tf.__version__)\n\nprint('GPU is','avaliable' if tf.test.is_gpu_available() else 'Not avaliable')","9da264cd":"import numpy as np\nimport IPython.display as display\nimport PIL.Image\n\nfrom tensorflow.keras.preprocessing import image","75c926d4":"# \u56fe\u50cf\u6807\u51c6\u5316\ndef normalize_image(img):\n    img = 255*(img+1.)\/2.\n    return tf.cast(img,tf.uint8)","af5869f9":"# \u5b9a\u4e49\u56fe\u50cf\u53ef\u89c6\u5316\u51fd\u6570    \ndef show_image(img):\n    display.display(PIL.Image.fromarray(np.array(img)))","8ce551ee":"def save_image(img,file_name):\n    PIL.Image.fromarry(np.array(img)).save(file_name)","9be6d393":"# \u5b9a\u4e49\u56fe\u50cf\u566a\u58f0\nimg_noise = np.random.uniform(size = (300,300,3)) + 100.\nimg_noise = img_noise.astype(np.float32) # \u8f6c\u6362\u6210float32\nshow_image(normalize_image(img_noise))","0fb7b3b2":"import os \nimport cv2\nimport matplotlib.pyplot as plt\n\npath = '..\/input\/pokemon-images-and-types\/images\/images'\nimg_name = os.path.join(path,'arceus.png')\nimg = cv2.imread(img_name)\n# img = tf.image.resize(images = img,size = [300,300])\nplt.imshow(img)","9a60a30b":"# \u5bfc\u5165imageNet\u6570\u636e\u96c6\u7684\u56fe\u50cf\u8bc6\u522b\u9884\u8bad\u7ec3InceptionV3 \u6a21\u578b \n# \u53bb\u6389\u9876\u5c42 \u8fd9\u6837\u80fd\u63a5\u53d7\u65b0\u7684\u8bad\u7ec3\u6570\u636eshape\nbase_model = tf.keras.applications.InceptionV3(include_top = False,weights='imagenet')\n# base_model.summary()","a596ca63":"# \u786e\u5b9a\u9700\u8981\u6700\u5927\u5316\u6fc0\u6d3b\u7684\u5377\u79ef\u5c42\nlayer_names = 'conv2d_85'\n\nlayers = base_model.get_layer(layer_names).output\n\nlayers","ffccf64a":"# \u521b\u5efa\u7279\u5f81\u63d0\u53d6\u6a21\u578b\ndream_model = tf.keras.Model(inputs = base_model.input,outputs = layers)\n# dream_model.summary()","45e3a9d8":"# \u635f\u5931\u662f\u9009\u4e2d\u5c42\u7684\u901a\u9053\u8f93\u51fa\ndef calc_loss(img,model):\n    # \u9009\u5b9a\u7b2c13\u901a\u9053\n    channel = 13\n    # \u5bf9\u56fe\u50cf\u505a\u53d8\u5f62 \u7531\uff08300\uff0c300\uff0c3\uff09 \u6269\u5c55\u95ee\uff081\uff0c300\uff0c300\uff0c3\uff09\n    # Inception \u6a21\u578b\u8981\u6c42\u7684\u8f93\u5165\u683c\u5f0f\u662f\uff08batch\uff0cheight\uff0cwidth\uff0cchannel\uff09\n    img = tf.expand_dims(img,axis = 0)\n    \n    # \u56fe\u50cf\u901a\u8fc7\u6a21\u578b\u5411\u524d\u4f20\u64ad\u5f97\u5230\u8ba1\u7b97\u7ed3\u679c\n    # [1,8,8,2048]\n    layer_activations = model(img)\n    # [1,8,8]\n    act = layer_activations[:,:,:,channel]\n    \n    loss = tf.math.reduce_mean(act)\n    \n    return loss","b6448c74":"# \u5b9a\u4e49\u56fe\u50cf\u4f18\u5316\u8fc7\u7a0b\n# \u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u8fdb\u884c\u56fe\u50cf\u8c03\u6574\uff0c\u8be5\u56fe\u50cf\u4f1a\u8d8a\u6765\u8d8a\u591a\u5730\u6fc0\u6d3b\u6a21\u578b\u4e2d\u7684\u6307\u5b9a\u5c42\u548c\u901a\u9053\u7684\u4fe1\u606f\ndef render_deepdream(model,img,steps = 100,step_size = 0.01,verbose = 1):\n    for n in tf.range(steps):\n        # \u6c42\u68af\u5ea6\n        with tf.GradientTape() as tape:\n            # \u5bf9img\u8fdb\u884c\u68af\u5ea6\u53d8\u6362\n            # \u5f97\u5230\u6c42\u5bfc\u53d8\u91cf\n            tape.watch(img)\n            # \u5f97\u5230\u6c42\u5bfc\u516c\u5f0f\n            loss = calc_loss(img,model)\n        # \u8ba1\u7b97\u635f\u5931\u76f8\u5bf9\u4e8e\u8f93\u5165\u56fe\u50cf\u50cf\u7d20\u7684\u68af\u5ea6\n        # \u8fdb\u884c\u6c42\u5bfc\n        gradients = tape.gradient(loss,img)\n        \n        # \u5f52\u4e00\u5316\u68af\u5ea6\u503c\n        # \u5bf9\u6c42\u5bfc\u7ed3\u679c\u505a\u5f52\u4e00\u5316\n        gradients \/= tf.math.reduce_std(gradients) + 1e-8\n        \n        # \u5728\u68af\u5ea6\u4e0a\u5347\u4e2d\uff0c\u635f\u5931\u503c\u8d8a\u6765\u8d8a\u5927 \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u6dfb\u52a0\u635f\u5931\u503c\u5230\u56fe\u50cf\u4e2d \u56e0\u4e3a\u4ed6\u4eec\u7684shape\u76f8\u540c\n        # \u66f4\u65b0\n        img = img + gradients*step_size\n        # \u538b\u7f29\u503c v < -1 \uff1f -1 : v    v > 1? 1 : v\n        img = tf.clip_by_value(img,-1,1)\n        \n        # \u8f93\u51fa\u8fc7\u7a0b\u63d0\u793a\u4fe1\u606f\n        if(verbose == 1):\n            if((n+1)%10 == 0):\n                print(\"Step {}\/{},loss{}\".format(n+1,steps,loss))\n    return img","ca747f68":"# Inception_v3\u5bf9\u8f93\u5165\u7684\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\nimg = tf.keras.applications.inception_v3.preprocess_input(img_noise)\n# \u5c06\u6570\u636e\u8f6c\u4e3a\u5f20\u91cf\nimg = tf.convert_to_tensor(img)\n\n","b737dcea":"path = '..\/input\/pokemon-images-and-types\/images\/images'\nimg_name = os.path.join(path,'arceus.png')\nimg = cv2.imread(img_name)\n# img = tf.image.resize(images = img,size = [300,300])\nimg2 = tf.keras.applications.inception_v3.preprocess_input(img)\n# \u5c06\u6570\u636e\u8f6c\u4e3a\u5f20\u91cf\nimg2 = tf.convert_to_tensor(img2)\nplt.imshow(img2)","4de621e6":"import time\nstart = time.time()\nprint('\u5f00\u59cb\u505a\u68a6')\n\n# \u8c03\u7528\u4f18\u5316\u8fc7\u7a0b\ndream_img = render_deepdream(dream_model,img2,steps = 100,step_size = 0.01)\n\nend = time.time()\nprint('pass:{}'.format(end- start))\n\nprint('\u68a6\u5883\u7ed3\u675f')\n\n# \u6807\u51c6\u5316\u56fe\u50cf\ndream_img = normalize_image(dream_img)\n\n","4b926846":"# \u8ba1\u7b97\u635f\u5931\uff08\u591a\u4e2a\u901a\u9053\u603b\u548c\uff09\ndef calc_loss(img,model):\n    # \u9009\u62e9\u901a\u9053\n    channels = [10,300]\n    # \u5bf9\u56fe\u50cf\u505a\u53d8\u5f62\uff0c\u7531\uff08300\uff0c300\uff0c3\uff09\u6269\u5c55\u5230\uff081\uff0c300\uff0c300\uff0c3\uff09\n    img = tf.expand_dims(img,axis = 0)\n    \n    # \u56fe\u50cf\u901a\u8fc7\u6a21\u578b\u524d\u5411\u4f20\u64ad\u5f97\u5230\u8ba1\u7b97\u7ed3\u679c\n    layer_activations = model(img)\n    \n    # \u5c06\u6240\u6709\u901a\u9053\u7684\u8ba1\u7b97\u7ed3\u679c\u90fd\u52a0\u8d77\u6765\n    losses = []\n    \n    for cn in channels:\n        act = layer_activations[:,:,:,cn]\n        loss = tf.reduce_mean(act)\n        losses.append(loss)\n    \n    return tf.reduce_sum(losses)","08b4449f":"import time\nstart = time.time()\nprint('\u5f00\u59cb\u505a\u68a6')\n\n# \u8c03\u7528\u4f18\u5316\u8fc7\u7a0b\ndream_img = render_deepdream(dream_model,img2,steps = 100,step_size = 0.01)\n\nend = time.time()\nprint('pass:{}'.format(end- start))\n\nprint('\u68a6\u5883\u7ed3\u675f')\n\n# \u6807\u51c6\u5316\u56fe\u50cf\ndream_img2 = normalize_image(dream_img)\n\n","e3fcfd25":"\n_,(ax1,ax2,ax3) = plt.subplots(1,3,figsize = (15,10))\nax1.imshow(img)\nax2.imshow(dream_img)\nax3.imshow(dream_img2)\nplt.show()","d98869ec":"# \u786e\u5b9a\u9700\u8981\u6700\u5927\u5316\u6fc0\u6d3b\u7684\u5377\u79ef\u5c42\nlayer_names = ['mixed3','mixed5']\n\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\nlayers","7d4d124d":"def calc_loss(img,model):\n    img = tf.expand_dims(img,axis = 0)\n    \n    layer_avtivations = model(img)\n    \n    losses = []\n    for act in layer_avtivations:\n        loss = tf.math.reduce_mean(act)\n        losses.append(loss)\n    return tf.reduce_sum(losses)","61fc2976":"import time\nstart = time.time()\nprint('\u5f00\u59cb\u505a\u68a6')\n\n# \u8c03\u7528\u4f18\u5316\u8fc7\u7a0b\ndream_img = render_deepdream(dream_model,img2,steps = 100,step_size = 0.01)\n\nend = time.time()\nprint('pass:{}'.format(end- start))\n\nprint('\u68a6\u5883\u7ed3\u675f')\n\n# \u6807\u51c6\u5316\u56fe\u50cf\ndream_img3 = normalize_image(dream_img)\n\n","ce2a2663":"\n_,(ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize = (15,10))\nax1.imshow(img2)\nax2.imshow(dream_img)\nax3.imshow(dream_img2)\nax4.imshow(dream_img3)","7c94519e":"def read_image(file_name,max_dim = None):\n    img = PIL.Image.open(file_name)\n    if max_dim:\n        img.thumbnail((max_dim,max_dim))\n    return np.array(img)","4cde4e40":"image_file = '..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10004.jpg'\nimg = read_image(image_file,max_dim = 600)\nfig = plt.figure(figsize = (10,10))\nplt.imshow(img)\nplt.show()\nimg = tf.keras.applications.inception_v3.preprocess_input(img)\n# # \u5c06\u6570\u636e\u8f6c\u4e3a\u5f20\u91cf\nimg = tf.convert_to_tensor(img)\n","71d36247":"layer_names = ['mixed3','mixed5']\n\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\nlayers","b40e41d4":"# \u5b9a\u4e49\u56fe\u50cf\u4f18\u5316\u8fc7\u7a0b\n# \u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u8fdb\u884c\u56fe\u50cf\u8c03\u6574\uff0c\u8be5\u56fe\u50cf\u4f1a\u8d8a\u6765\u8d8a\u591a\u5730\u6fc0\u6d3b\u6a21\u578b\u4e2d\u7684\u6307\u5b9a\u5c42\u548c\u901a\u9053\u7684\u4fe1\u606f\ndef render_deepdream(model,img,steps = 100,step_size = 0.01,verbose = 1):\n    for n in tf.range(steps):\n        # \u6c42\u68af\u5ea6\n        with tf.GradientTape() as tape:\n            # \u5bf9img\u8fdb\u884c\u68af\u5ea6\u53d8\u6362\n            # \u5f97\u5230\u6c42\u5bfc\u53d8\u91cf\n            tape.watch(img)\n            # \u5f97\u5230\u6c42\u5bfc\u516c\u5f0f\n            loss = calc_loss(img,model)\n        # \u8ba1\u7b97\u635f\u5931\u76f8\u5bf9\u4e8e\u8f93\u5165\u56fe\u50cf\u50cf\u7d20\u7684\u68af\u5ea6\n        # \u8fdb\u884c\u6c42\u5bfc\n        \n        \n        gradients = tape.gradient(loss,img)\n        # \u5f52\u4e00\u5316\u68af\u5ea6\u503c\n        # \u5bf9\u6c42\u5bfc\u7ed3\u679c\u505a\u5f52\u4e00\u5316\n        gradients \/= tf.math.reduce_std(gradients) + 1e-8\n        \n        # \u5728\u68af\u5ea6\u4e0a\u5347\u4e2d\uff0c\u635f\u5931\u503c\u8d8a\u6765\u8d8a\u5927 \u56e0\u6b64\u53ef\u4ee5\u76f4\u63a5\u6dfb\u52a0\u635f\u5931\u503c\u5230\u56fe\u50cf\u4e2d \u56e0\u4e3a\u4ed6\u4eec\u7684shape\u76f8\u540c\n        # \u66f4\u65b0\n        img = img + (gradients)*step_size\n        # \u538b\u7f29\u503c v < -1 \uff1f -1 : v    v > 1? 1 : v\n        img = tf.clip_by_value(img,-1,1)\n        \n        \n        # \u8f93\u51fa\u8fc7\u7a0b\u63d0\u793a\u4fe1\u606f\n        if(verbose == 1):\n            if((n+1)%10 == 0):\n                print(\"Step {}\/{},loss{}\".format(n+1,steps,loss))\n    return img","94818a21":"import time\nstart = time.time()\nprint('\u5f00\u59cb\u505a\u68a6')\n\n# \u8c03\u7528\u4f18\u5316\u8fc7\u7a0b\ndream_img = render_deepdream(dream_model,img,steps = 200,step_size = 0.01)\n\nend = time.time()\nprint('pass:{}'.format(end- start))\n\nprint('\u68a6\u5883\u7ed3\u675f')\n\n# \u6807\u51c6\u5316\u56fe\u50cf\ndream_img3 = normalize_image(dream_img)","e7e5715b":"fig = plt.figure(figsize = (10,10))\nplt.imshow(dream_img3)\nplt.show()","1219d8ad":"import matplotlib.pyplot as plt\n\nimage_file = '..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10004.jpg'\n\noriginal_img = read_image(image_file,max_dim = 600)\nfig = plt.figure(figsize = (10,10))\nplt.imshow(original_img)\nplt.show()","97ee3c17":"import time\n\nstart = time.time()\n\nOCTAVE_SCALE = 1.3\n\nimg = tf.keras.applications.inception_v3.preprocess_input(original_img)\nimg = tf.convert_to_tensor(img)\n\ninitial_shape = tf.shape(img)[:-1]\n\nfor octave in range(-2,3):\n    new_size = tf.cast(tf.convert_to_tensor(initial_shape),tf.float32)*(OCTAVE_SCALE**octave)\n    img = tf.image.resize(img,tf.cast(new_size,tf.int32))\n    print(img.shape)\n    img = render_deepdream(dream_model,img,steps = 30,step_size = 0.01)\n\n# img = tf.image.resize(img,initial_shape)\n\nimg = normalize_image(img)\n","9c572da0":"fig = plt.figure(figsize = (10,10))\nplt.imshow(img)\nplt.show()","0e54fe70":"def random_roll(img,maxroll = 512):\n    shift = tf.random.uniform(shape = [2],minval=-maxroll,maxval = maxroll,dtype = tf.int32)\n    shift_down,shift_right = shift[0],shift[1]\n    img_rolled = tf.roll(tf.roll(img,shift_right,axis = 1),shift_down,axis = 0)\n    return shift_down,shift_right,img_rolled","6db96d58":"shift_down,shift_right,img_rolled = random_roll(np.array(original_img),512)\nprint(shift_down,shift_right)\nfig = plt.figure(figsize = (10,10))\nplt.imshow(img_rolled)","69618441":"def calc_loss(img,model):\n    img = tf.expand_dims(img,axis = 0)\n    \n    layer_avtivations = model(img)\n    \n    losses = []\n    for act in layer_avtivations:\n        loss = tf.math.reduce_mean(act)\n        losses.append(loss)\n    return tf.reduce_sum(losses)","f67e7665":"# \u5b9a\u4e49\u5206\u5757\u8ba1\u7b97\u7684\u68af\u5ea6\ndef get_tiled_gradients(model,img,tile_size = 150):\n    shift_down,shift_right,img_rolled = random_roll(img,tile_size)\n   \n    # \u521d\u59cb\u5316\u68af\u5ea6\u4e3a0\n    gradients = tf.zeros_like(img_rolled)\n    \n    # \u4ea7\u751f\u5206\u5757\u5750\u6807\u5217\u8868 \n    xs = tf.range(0,img_rolled.shape[0],tile_size)\n    ys = tf.range(0,img_rolled.shape[1],tile_size)\n    for x in xs:\n        for y in ys:\n            with tf.GradientTape() as tape:\n                \n                tape.watch(img_rolled)\n                # \u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u8be5\u56fe\u5757\uff0c\u6700\u540e\u4e00\u5757\u5927\u5c0f\u4f1a\u6309\u5b9e\u9645\u63d0\u53d6\n                img_tile = img_rolled[x:x+int(tile_size), y:y+int(tile_size)]\n                loss = calc_loss(img_tile,model)\n                gradients = gradients + tape.gradient(loss,img_tile)\n    gradients = tf.roll(tf.roll(gradients,-shift_right,axis = 1),-shift_down,axis = 0)\n    \n    gradients \/= tf.math.reduce_std(gradients) + 1e-8\n    \n    return gradients","60fdbe76":"# \u5206\u5757\u7684\u68af\u5ea6d\u8fed\u4ee3\u51fd\u6570\ndef render_deepdream_with_octaves(model,img,steps_per_octave = 100,step_size = 0.01,\n                                 octaves = range(0,3),octave_scale = 1.3):\n    initial_shape = img.shape[:-1]\n    \n    for octave in range(-2,1):\n        new_size = tf.cast(tf.convert_to_tensor(initial_shape),tf.float32)*(OCTAVE_SCALE**octave)\n        img = tf.image.resize(img,tf.cast(new_size,tf.int32))\n        \n        for step in range(100):\n            gradients = get_tiled_gradients(model,img)\n            img = img + gradients*step_size\n            img = tf.clip_by_value(img,-1,1)\n            \n            if((step+1)%10 == 0):\n                print('Octave{},Step{}'.format(octave,step+1))\n        \n    img = tf.image.resize(img,initial_shape)\n    result = normalize_image(img)\n    \n    return result","02724ed1":"\nprint('\u5f00\u59cb\u505a\u68a6')\n\nimg = tf.keras.applications.inception_v3.preprocess_input(original_img)\n\nimg = tf.convert_to_tensor(img)\n\n\nimg = render_deepdream_with_octaves(dream_model,img,steps_per_octave=50)\n\nend = time.time()\n\nprint('pass : {}'.format(str(end- start)))\nprint('\u68a6\u9192\u4e86')\n\nfig = plt.figure(figsize = (10,10))\nplt.imshow(img)","fa60d3a0":"fig = plt.figure(figsize = (10,10))\nplt.imshow(img)","14788d43":"## \u5355\u5c42\u5355\u901a\u9053V3","972324a7":"# \u6784\u5efa\u6a21\u578b","71aae77a":"# Deep Dream \u5e94\u7528\u5b9e\u4f8b","4c36290d":"## DeepDream\u7684\u4e3b\u8981\u60f3\u6cd5\u662f\u9009\u62e9\u4e00\u4e2a\u5377\u79ef\u5c42\u7684\u67d0\u4e2a\u901a\u9053\u6216\u8005\u5377\u79ef\u5c42\uff08\u4e5f\u53ef\u4ee5\u662f\u591a\u4e2a\u7f51\u7edc\u5c42\uff09   \n## \u6539\u53d8\u56fe\u50cf\u50cf\u7d20\uff08\u4e0e\u8bad\u7ec3\u5206\u7c7b\u5668\u6700\u5927\u7684\u533a\u522b\uff09\uff0c\u6765\u6700\u5927\u5316\u9009\u4e2d\u5c42\u6216\u901a\u9053\u7684\u6fc0\u6d3b\u503c","4ac5b06d":"## \u5355\u5c42\u591a\u901a\u9053V2","d98d41d2":"\u5728\u4e0d\u540c\u6bd4\u4f8b\u7684\u56fe\u4e0a\u4f7f\u7528\u68af\u5ea6\u4e0a\u5347\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5728\u5c0f\u6bd4\u4f8b\u4e0a\u6d82\u6210\u7684\u7ed3\u679c\u5408\u5e76\u5230\u66f4\u5927\u6bd4\u4f8b\u7684\u56fe\u4e0a","9eada975":"## \u80cc\u666f\u56fe\u50cf\u8d77\u70b9\u591a\u5c42\u7f51\u7edc\u5168\u901a\u9053","46f81149":"\u5c06\u56fe\u50cf\u62c6\u5206\u4e3a\u591a\u4e2a\u5c0f\u56fe\u5757\u8ba1\u7b97\u68af\u5ea6\uff0c\u6700\u540e\u5c06\u5176\u62fc\u5408\u8d77\u6765\uff0c\u5f97\u5230\u6700\u7ec8\u56fe\u50cf\n","425b6e5c":"## \u591a\u5c42\u5168\u901a\u9053V3"}}