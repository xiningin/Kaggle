{"cell_type":{"da915174":"code","70e3c55c":"code","5ab45d20":"code","094a4bdc":"code","e299211e":"code","483e380e":"code","b011461a":"code","3d746bee":"code","9d9fd5dd":"code","f0d281aa":"code","354965d8":"code","6f2c5d94":"code","d1370972":"code","762d8700":"code","4dfa6b6a":"code","a6c40ee1":"code","c4f4947b":"code","38730c28":"code","9f7b9de5":"code","3160474c":"code","4ad14ce4":"code","8ca27941":"code","ff006be8":"code","b6b279d9":"code","6d5b4ce1":"code","28b840bb":"code","9ac511c9":"code","6fe2f843":"code","c97b8e37":"code","ebc60c7d":"code","cb50649b":"code","04aa2a40":"code","10456ade":"code","492d9729":"code","dbf0f2ff":"code","332f4169":"code","09c74641":"code","e04fb3a9":"code","9f2bacda":"code","3a4e5b07":"code","217a527b":"code","1966f9d9":"code","60add806":"code","2c5d989f":"code","4abe7f29":"code","e9750edf":"code","1bb99e75":"code","a57a84ec":"code","fc091b58":"code","e03d8c77":"code","66442969":"code","2c08b524":"code","ce721e01":"code","1d825f67":"code","1447e591":"code","e980592a":"code","972b02b9":"code","ab085129":"code","797635b2":"code","b9c9b6f5":"code","40a93a46":"code","ed76d4c0":"code","05a71128":"code","131b6ad2":"code","aab792c1":"code","29151692":"code","8bd2d193":"code","bd58ea83":"code","65b0df86":"code","2a1ce1b5":"code","693a9738":"code","d812726c":"code","b85e9b8b":"code","5a3b4b95":"code","2c4f5a61":"code","d26fc663":"code","d1f48ab3":"code","361ccdc5":"code","e8c74818":"code","f42801c1":"code","4b045d1f":"code","7c598740":"code","70714205":"code","9bc66e2a":"code","9b017a00":"code","d7cf29b2":"code","0849f453":"code","32c40e51":"code","063359c3":"code","c1c7fcba":"code","dbaf7b38":"code","9f8423d2":"code","f3ecefb1":"code","afe7b66b":"code","9d37c831":"code","8df71297":"code","3bce991d":"code","a2b9ef28":"code","ced1722b":"code","3253ae7d":"code","3bc67b87":"code","63438b37":"code","18f970a1":"code","b6531ff0":"code","e16c4f46":"code","1e04c121":"code","8f75fb63":"code","ab124900":"code","2eee7c21":"code","f1c66dd3":"code","6b863fcc":"code","5921ace6":"code","4fa8bc7e":"code","bc6e2657":"code","6692877f":"markdown","435ace52":"markdown","0d4e4db5":"markdown","02d0897d":"markdown","b734c956":"markdown","096fdc04":"markdown","b4932891":"markdown","ab9d827e":"markdown","cea27693":"markdown","478e1f48":"markdown","a83f3450":"markdown","ce868e8a":"markdown","b2a89ba7":"markdown","2119383e":"markdown","4c114869":"markdown","2cf99552":"markdown","06f9096f":"markdown","27587ad9":"markdown","2de075a0":"markdown","fb1ced8c":"markdown","16677985":"markdown","44f5d0c1":"markdown","09e04989":"markdown","64e02b2c":"markdown","ac14b096":"markdown","243a5880":"markdown","cc68cb2a":"markdown","c9ce6a28":"markdown","41d09d88":"markdown","e6dcf529":"markdown","44013956":"markdown","49bb8898":"markdown","4e4e7b0e":"markdown","c5ee764d":"markdown","584baaf4":"markdown","9c6517c5":"markdown","d25e6950":"markdown","b5073b5f":"markdown","0bd97839":"markdown","dda74dee":"markdown","4dd95134":"markdown","651bf0cb":"markdown","0d613171":"markdown","d143d7a2":"markdown","91755730":"markdown","555f9a07":"markdown","890bdd4a":"markdown","32fae242":"markdown","adc5e4d1":"markdown","f9d52a93":"markdown","afe4b691":"markdown","da8d74da":"markdown","0d558af1":"markdown","fec5b7bf":"markdown","b50125c4":"markdown","ba5a89a2":"markdown"},"source":{"da915174":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70e3c55c":"df_detail = pd.read_csv(\"\/kaggle\/input\/complaints-data\/complaints-accused_2000-2016_2016-11.csv\")\ndf_victims = pd.read_csv(\"\/kaggle\/input\/complaints-data\/complaints-victims_2000-2016_2016-11.csv\")\ndf_main = pd.read_csv(\"\/kaggle\/input\/complaints-data\/complaints-complaints_2000-2016_2016-11.csv\")","5ab45d20":"df_main.head()","094a4bdc":"df_main.columns","e299211e":"#remove unnecessary info\ndf_main = df_main.drop([\"row_id\", \"location_code\", \"address_number\", \"street\", \"apartment_number\", \"incident_time\", \"complaint_date\",\"closed_date\"], axis=1)","483e380e":"#creating year and dropping date columns\ndf_main['incident_year'] = df_main['incident_date'].apply(lambda x: int(x[0:4]))\ndf_main.drop(['incident_date'], axis = 1, inplace = True)","b011461a":"#pruning our data to only contain CR_ID's >= 1000000 as these are the CR_ID's present across all data sets\ndf_main = df_main[df_main.cr_id >= 1000000]","3d746bee":"df_main.head()","9d9fd5dd":"#open and read CPD beats file\nbeatset = set([])\nbeatf = open(\"\/kaggle\/input\/complaints-data\/beatlist.txt\",'r')\nN = int(beatf.readline()[:-1])\nfor i in range(N):\n    beatset.add(beatf.readline()[:-1])","f0d281aa":"#helper func to easily pull beat names and ensure that each beat is a part of the beats file we previously opened and read\ndef pad0(string, length):\n    if (type(string) != str):\n        string = str(int(string))\n    ans = \"\"\n    for i in range(length-len(string)):\n        ans += '0'\n    ans += string\n    if (ans in beatset):\n        return ans\n    else:\n        return \"asdf\"\n\n#helper func to ensure all complaints in the Chicago-Metro Area\ndef city_state_checker(string):\n    if string[:2] != \"CH\" or string == \"CHICAGO RIDG IL\":\n        return \"bAd\"\n    return \"gOOd\"","354965d8":"#applying above funcs to data for removing data with irrelevant beats and formatting exisiting beats\ndf_main = df_main[~pd.isna(df_main.beat)]\ndf_main.beat = df_main.beat.apply(lambda x: pad0(x, 4))\ndf_main = df_main[df_main.beat != \"asdf\"]","6f2c5d94":"#applying above helper funcs to remove all complaints not in our geographic area of interests\ndf_main = df_main[~pd.isna(df_main.city_state)]\ndf_main.city_state = df_main.city_state.apply(lambda x: city_state_checker(x))\ndf_main = df_main[df_main.city_state != \"bAd\"]","d1370972":"#the above function removed need for 'city_state' column so let's drop\ndf_main = df_main.drop([\"city_state\"], axis=1)","762d8700":"#index by CR ID for easy sorting\/combining\ndf_main = df_main.set_index(\"cr_id\")","4dfa6b6a":"df_main.head()","a6c40ee1":"df_victims.head()","c4f4947b":"#remove null values for race\ndf_victims = df_victims[~pd.isna(df_victims[\"race\"])]","38730c28":"#set index to CR ID for easy sorting\/combining\ndf_victims = df_victims.set_index(\"cr_id\")","9f7b9de5":"df_victims.head()","3160474c":"df_detail.head()","4ad14ce4":"#removing all CR ID < 1000000\ndf_detail = df_detail[df_detail[\"cr_id\"] >= 1000000]","8ca27941":"#set index to CR ID for easy sorting\/combining\ndf_detail = df_detail.set_index(\"cr_id\")","ff006be8":"#drop an unnecessary column\ndf_detail = df_detail.drop([\"complaints-accused_2000-2016_2016-11_ID\", \"UID\"], axis=1)","b6b279d9":"df_detail.head()","6d5b4ce1":"df_main = df_main.sort_index()\ndf_victims = df_victims.sort_index()\ndf_detail = df_detail.sort_index()","28b840bb":"df_main.drop_duplicates(inplace = True)\ndf_detail.drop_duplicates(inplace = True)","9ac511c9":"df_main.head()","6fe2f843":"df_victims.head()","c97b8e37":"df_detail.head()","ebc60c7d":"df_main.shape","cb50649b":"df_victims.shape","04aa2a40":"df_detail.shape","10456ade":"df_main.head()","492d9729":"#graph for beats with most complaints\nbeatFreq = df_main.beat.value_counts()[:20]\nbeatFreq_beats = beatFreq.index\nsns.set(font_scale = 1)\nax = sns.barplot(x = beatFreq_beats, y = beatFreq, order = beatFreq_beats, label = 'small')\nax.set_title('20 Beats with HIGHEST Number of Complaint Reports')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set(xlabel='Beats', ylabel='Number of Complaints')\nax.tick_params(labelsize=8.5)\nplt.show()","dbf0f2ff":"#graph for beats with least complaints\nbeatFreq_low = df_main.beat.value_counts()[-20:]\nbeatFreq_low_beats = beatFreq_low.index\nsns.set(font_scale = 1)\nax = sns.barplot(x = beatFreq_low_beats, y = beatFreq_low, order = beatFreq_low_beats, label = 'small')\nax.set_title('20 Beats with LOWEST Number of Complaint Reports')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set(xlabel='Beats', ylabel='Number of Complaints')\nax.tick_params(labelsize=8.5)\nplt.show()","332f4169":"#counter dict for district with most beats in top 20\ndistricts_high = {}\nfor beat in beatFreq_beats:\n    district = beat[:2]\n    if district in districts_high.keys():\n        districts_high[district] += 1\n    else:\n        districts_high[district] = 1\n#print(districts_high)\ndistricts_high_sorted_keys = sorted(districts_high, key=districts_high.get, reverse=True)\ndistricts_high_sort ={}\nfor district in districts_high_sorted_keys:\n    districts_high_sort[district] = districts_high[district]\nprint(districts_high_sort)","09c74641":"#graph of above counter\nsns.set(font_scale = 1)\nax = sns.barplot(x = list(districts_high_sort.keys()), y = list(districts_high_sort.values()), order = districts_high_sort, label = 'small')\nax.set_title('Districts with HIGHEST Number of Complaints')\nax.set_xticklabels(ax.get_xticklabels())\nax.tick_params(labelsize=8.5)\nax.set(xlabel='Districts', ylabel='Number of Complaints')\nplt.show()","e04fb3a9":"#counter dict for district with most beats in bottom 20\ndistricts_low = {}\nfor beat in beatFreq_low_beats:\n    district = beat[:2]\n    if district in districts_low.keys():\n        districts_low[district] += 1\n    else:\n        districts_low[district] = 1\n#print(districts_low)\ndistricts_low_sorted_keys = sorted(districts_low, key=districts_low.get, reverse=True)\ndistricts_low_sort ={}\nfor district in districts_low_sorted_keys:\n    districts_low_sort[district] = districts_low[district]\nprint(districts_low_sort)","9f2bacda":"#graph of above counter\nsns.set(font_scale = 1)\nax = sns.barplot(x = list(districts_low_sort.keys()), y = list(districts_low_sort.values()), order = districts_low_sort, label = 'small')\nax.set_title('Districts with LOWEST Number of Complaints')\nax.set_xticklabels(ax.get_xticklabels())\nax.tick_params(labelsize=8.5)\nax.set(xlabel='Districts', ylabel='Number of Complaints')\nplt.show()","3a4e5b07":"print(df_main.incident_year.value_counts())","217a527b":"#graph of frequency each year appears across all available complaint data\nax = sns.countplot(x=\"incident_year\", data=df_main)\nax.set_title('Number of Complaints Filed by year')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","1966f9d9":"df_victims.head()","60add806":"#counter for gender frequency across all available complaints\nprint(df_victims.gender.value_counts())","2c5d989f":"#graph of above counter\nax = sns.countplot(x=\"gender\", data=df_victims,  order = df_victims.gender.value_counts().index)\nax.set_title('Breakdown of Complainees by Gender')\nplt.show()","4abe7f29":"#counter for race frequency across all available complaints\nprint(df_victims.race.value_counts())","e9750edf":"#graph of above counter\nax = sns.countplot(x=\"race\", data=df_victims, order = df_victims.race.value_counts().index)\nax.set_title('Breakdown of Complainees by Race')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","1bb99e75":"#helper func to create age ranges\ndef bander(age):\n    if (age<20):\n        return '<20'\n    elif(age>=20 and age<30):\n        return '20-29'\n    elif(age>=30 and age<40):\n        return '30-39'\n    elif(age>=40 and age<50):\n        return '40-49'\n    elif(age>=50 and age<60):\n        return '50-59'\n    elif(age>=60 and age<70):\n        return '60-69'\n    else:\n        return '>=70'","a57a84ec":"#apply helper func to complete complaints data\ndf_victims['age_band'] = df_victims.age.apply(lambda x: bander(x))","fc091b58":"#view of frequency of age band\ndf_victims.age_band.value_counts()","e03d8c77":"#graph of above counter\nage_band_order = ['<20', '20-29','30-39','40-49','50-59','60-69', '>=70' ]\nax = sns.countplot(x=\"age_band\", data=df_victims, order = age_band_order)\nax.set_title('Breakdown of Complainees by Age')\nplt.show()","66442969":"df_victims.drop('age', axis = 1, inplace = True)","2c08b524":"pd.set_option('display.max_rows', None)\ntable = pd.pivot_table(df_victims, index=['race', 'gender', 'age_band'], aggfunc= len, fill_value = 0)\ntable","ce721e01":"df_detail.head()","1d825f67":"#store all the complaint categories in a neat list\ncomplaint_cat_list = list(df_detail.complaint_category.unique())","1447e591":"#create a dict for the above said 3-character key and the corresponding description\ncomplaint_cat_dict = {}\nfor x in complaint_cat_list:\n    desc = str(x)\n    key = desc[:3]\n    DESC = desc[4:]\n    complaint_cat_dict[key] = DESC\nprint(len(complaint_cat_dict))","e980592a":"#remove the descriptions from the complaints\ndf_detail['complaint_key'] = df_detail.complaint_category.apply(lambda x: str(x)[:3])","972b02b9":"#we don't need all full descriptions anymore, so drop\ndf_detail.drop('complaint_category', axis = 1, inplace = True)","ab085129":"#replacing null values with 'OTHER'\ndf_detail['complaint_key'].replace('nan', 'OTHER', inplace = True)","797635b2":"#before removing dups (uncomment if needed but long)\n#df_detail.complaint_key.value_counts()","b9c9b6f5":"#removing some auxilary category\ndf_detail.drop('row_id', axis = 1, inplace = True)","40a93a46":"#creating our no dups dataframe\ndf_detail_no_dups = df_detail.drop_duplicates()","ed76d4c0":"#after removing dups (uncomment if needed but long)\n#df_detail_no_dups.complaint_key.value_counts()","05a71128":"#table of 20 most frequent complaint types in duplicates data\nprint('20 Most Frequent Complaint Types (With Duplicates) \u2014 Table\\n')\nfor key in df_detail.complaint_key.value_counts()[:20].index:\n    if(key == 'OTHER'):\n        print('OTHER')\n        print(\"Frequency: \"+str(df_detail.complaint_key.value_counts()[key]))\n        print('-'*50)\n    else:\n        print(key+\": \"+complaint_cat_dict[key])\n        print(\"Frequency: \"+str(df_detail.complaint_key.value_counts()[key]))\n        print('-'*50)","131b6ad2":"#graph of above data via frequency\nsns.set(font_scale = 1)\nax = sns.barplot(x = df_detail.complaint_key.value_counts()[:20].index, y = df_detail.complaint_key.value_counts()[:20], order = df_detail.complaint_key.value_counts()[:20].index, label = 'small')\nax.set_title('20 Most Frequent Complaint Types (With Duplicates)')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set(xlabel='Complaint Type', ylabel='Frequency')\nax.tick_params(labelsize=8.5)\nplt.show()","aab792c1":"#table of 20 most frequent complaint types in no duplicates data\nprint('20 Most Frequent Complaint Types (No Duplicates) \u2014 Table\\n')\nfor key in df_detail_no_dups.complaint_key.value_counts()[:20].index:\n    print(key+\": \"+complaint_cat_dict[key])\n    print(\"Frequency: \"+str(df_detail_no_dups.complaint_key.value_counts()[key]))\n    print('-'*50)","29151692":"#graph of above data via frequency\nsns.set(font_scale = 1)\nax = sns.barplot(x = df_detail_no_dups.complaint_key.value_counts()[:20].index, y = df_detail_no_dups.complaint_key.value_counts()[:20], order = df_detail_no_dups.complaint_key.value_counts()[:20].index, label = 'small')\nax.set_title('20 Most Frequent Complaint Types (No Duplicates)')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set(xlabel='Complaint Type', ylabel='Frequency')\nax.tick_params(labelsize=8.5)\nplt.show()","8bd2d193":"df_detail.reset_index(level = 0, inplace = True)\ndf_detail_no_dups.reset_index(level = 0, inplace = True)","bd58ea83":"#discipline code counter for duplicate data + sorting (recommended discipline)\ndiscipline_dict_rec = {}\ndiscipline_dict_rec['x Day Suspension'] = 0\ndiscipline_dict_rec['Over 30 Day Suspension'] = 0\ndiscipline_dict_rec['Reprimanded'] = 0\ndiscipline_dict_rec['Administrative Termination'] = 0\ndiscipline_dict_rec['Year-long Suspension or Longer'] = 0\ndiscipline_dict_rec['Seperation'] = 0\ndiscipline_dict_rec['Reinstated'] = 0\ndiscipline_dict_rec['Nothing'] = 0\ndiscipline_dict_rec['Resigned'] = 0\nfor ind in df_detail.index:\n    rec = df_detail['recommended_discipline'][ind]\n    if rec <199:\n        discipline_dict_rec['x Day Suspension'] += 1\n    if rec == 200:\n        discipline_dict_rec['Over 30 Day Suspension'] +=1\n    if rec == 0 or rec == 100:\n        discipline_dict_rec['Reprimanded'] += 1\n    if rec == 300:\n        discipline_dict_rec['Administrative Termination'] +=1\n    if rec == 365:\n        discipline_dict_rec['Year-long Suspension or Longer'] +=1\n    if rec == 400:\n        discipline_dict_rec['Seperation'] += 1\n    if rec == 500:\n        discipline_dict_rec['Reinstated'] += 1\n    if rec == 600 or rec == 900:\n        discipline_dict_rec['Nothing'] += 1\n    if rec == 800:\n        discipline_dict_rec['Resigned'] += 1\ndiscipline_dict_rec_keys = sorted(discipline_dict_rec, key=discipline_dict_rec.get, reverse=True)\ndiscipline_dict_rec_sort ={}\nfor district in discipline_dict_rec_keys:\n    discipline_dict_rec_sort[district] = discipline_dict_rec[district]\nprint(discipline_dict_rec_sort)","65b0df86":"#discipline code counter for duplicate data + sorting (final discipline)\ndiscipline_dict_fin = {}\ndiscipline_dict_fin['x Day Suspension'] = 0\ndiscipline_dict_fin['Over 30 Day Suspension'] = 0\ndiscipline_dict_fin['Reprimanded'] = 0\ndiscipline_dict_fin['Administrative Termination'] = 0\ndiscipline_dict_fin['Year-long Suspension or Longer'] = 0\ndiscipline_dict_fin['Seperation'] = 0\ndiscipline_dict_fin['Reinstated'] = 0\ndiscipline_dict_fin['Nothing'] = 0\ndiscipline_dict_fin['Resigned'] = 0\nfor ind in df_detail.index:\n    fin = df_detail['final_discipline'][ind]\n    if fin <199:\n        discipline_dict_fin['x Day Suspension'] += 1\n    if fin == 200:\n        discipline_dict_fin['Over 30 Day Suspension'] +=1\n    if fin == 0 or fin == 100:\n        discipline_dict_fin['Reprimanded'] += 1\n    if fin == 300:\n        discipline_dict_fin['Administrative Termination'] +=1\n    if fin == 365:\n        discipline_dict_fin['Year-long Suspension or Longer'] +=1\n    if fin == 400:\n        discipline_dict_fin['Seperation'] += 1\n    if fin == 500:\n        discipline_dict_fin['Reinstated'] += 1\n    if fin == 600 or fin == 900:\n        discipline_dict_fin['Nothing'] += 1\n    if fin == 800:\n        discipline_dict_fin['Resigned'] += 1\ndiscipline_dict_fin_keys = sorted(discipline_dict_fin, key=discipline_dict_fin.get, reverse=True)\ndiscipline_dict_fin_sort ={}\nfor district in discipline_dict_fin_keys:\n    discipline_dict_fin_sort[district] = discipline_dict_fin[district]\nprint(discipline_dict_fin_sort)","2a1ce1b5":"#putting both counters into an output with some light analysis \nprint('DISCIPLINE DATA (with duplictes)\\n')\nfor disc in discipline_dict_rec_sort:\n    print(disc)\n    print('Frequency that '+disc+' was the Recommended Discipline: '+str(discipline_dict_rec[disc]))\n    print('Frequency that '+disc+' was the Final Discipline: '+str(discipline_dict_fin[disc]))\n    if discipline_dict_rec[disc]>discipline_dict_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_rec[disc]-discipline_dict_fin[disc])+' ('+ str(abs(discipline_dict_rec[disc]-discipline_dict_fin[disc]))+' more Recommended than Final)')\n    elif discipline_dict_rec[disc]<discipline_dict_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_rec[disc]-discipline_dict_fin[disc])+' ('+ str(abs(discipline_dict_rec[disc]-discipline_dict_fin[disc]))+' more Final than Recommended)')\n    else:\n        print('Frequencies of Recommended and Final are Equal')\n    print('-'*50)","693a9738":"#discipline code counter for non-duplicate data + sorting (recommended discipline)\ndiscipline_dict_no_dups_rec = {}\ndiscipline_dict_no_dups_rec['x Day Suspension'] = 0\ndiscipline_dict_no_dups_rec['Over 30 Day Suspension'] = 0\ndiscipline_dict_no_dups_rec['Reprimanded'] = 0\ndiscipline_dict_no_dups_rec['Administrative Termination'] = 0\ndiscipline_dict_no_dups_rec['Year-long Suspension or Longer'] = 0\ndiscipline_dict_no_dups_rec['Seperation'] = 0\ndiscipline_dict_no_dups_rec['Reinstated'] = 0\ndiscipline_dict_no_dups_rec['Nothing'] = 0\ndiscipline_dict_no_dups_rec['Resigned'] = 0\nfor ind in df_detail_no_dups.index:\n    rec = df_detail['recommended_discipline'][ind]\n    if rec <199:\n        discipline_dict_no_dups_rec['x Day Suspension'] += 1\n    if rec == 200:\n        discipline_dict_no_dups_rec['Over 30 Day Suspension'] +=1\n    if rec == 0 or rec == 100:\n        discipline_dict_no_dups_rec['Reprimanded'] += 1\n    if rec == 300:\n        discipline_dict_no_dups_rec['Administrative Termination'] +=1\n    if rec == 365:\n        discipline_dict_no_dups_rec['Year-long Suspension or Longer'] +=1\n    if rec == 400:\n        discipline_dict_no_dups_rec['Seperation'] += 1\n    if rec == 500:\n        discipline_dict_no_dups_rec['Reinstated'] += 1\n    if rec == 600 or rec == 900:\n        discipline_dict_no_dups_rec['Nothing'] += 1\n    if rec == 800:\n        discipline_dict_no_dups_rec['Resigned'] += 1\ndiscipline_dict_no_dups_rec_keys = sorted(discipline_dict_no_dups_rec, key=discipline_dict_no_dups_rec.get, reverse=True)\ndiscipline_dict_no_dups_rec_sort ={}\nfor district in discipline_dict_no_dups_rec_keys:\n    discipline_dict_no_dups_rec_sort[district] = discipline_dict_no_dups_rec[district]\nprint(discipline_dict_no_dups_rec_sort)","d812726c":"#discipline code counter for non-duplicate data + sorting (final discipline)\ndiscipline_dict_no_dups_fin = {}\ndiscipline_dict_no_dups_fin['x Day Suspension'] = 0\ndiscipline_dict_no_dups_fin['Over 30 Day Suspension'] = 0\ndiscipline_dict_no_dups_fin['Reprimanded'] = 0\ndiscipline_dict_no_dups_fin['Administrative Termination'] = 0\ndiscipline_dict_no_dups_fin['Year-long Suspension or Longer'] = 0\ndiscipline_dict_no_dups_fin['Seperation'] = 0\ndiscipline_dict_no_dups_fin['Reinstated'] = 0\ndiscipline_dict_no_dups_fin['Nothing'] = 0\ndiscipline_dict_no_dups_fin['Resigned'] = 0\nfor ind in df_detail_no_dups.index:\n    fin = df_detail['final_discipline'][ind]\n    if fin <199:\n        discipline_dict_no_dups_fin['x Day Suspension'] += 1\n    if fin == 200:\n        discipline_dict_no_dups_fin['Over 30 Day Suspension'] +=1\n    if fin == 0 or fin == 100:\n        discipline_dict_no_dups_fin['Reprimanded'] += 1\n    if fin == 300:\n        discipline_dict_no_dups_fin['Administrative Termination'] +=1\n    if fin == 365:\n        discipline_dict_no_dups_fin['Year-long Suspension or Longer'] +=1\n    if fin == 400:\n        discipline_dict_no_dups_fin['Seperation'] += 1\n    if fin == 500:\n        discipline_dict_no_dups_fin['Reinstated'] += 1\n    if fin == 600 or fin == 900:\n        discipline_dict_no_dups_fin['Nothing'] += 1\n    if fin == 800:\n        discipline_dict_no_dups_fin['Resigned'] += 1\ndiscipline_dict_no_dups_fin_keys = sorted(discipline_dict_no_dups_fin, key=discipline_dict_no_dups_fin.get, reverse=True)\ndiscipline_dict_no_dups_fin_sort ={}\nfor district in discipline_dict_no_dups_fin_keys:\n    discipline_dict_no_dups_fin_sort[district] = discipline_dict_no_dups_fin[district]\nprint(discipline_dict_no_dups_fin_sort)","b85e9b8b":"#putting both counters into an output with some light analysis \nprint('DISCIPLINE DATA (no duplictes)\\n')\nfor disc in discipline_dict_no_dups_rec_sort:\n    print(disc)\n    print('Frequency that '+disc+' was the Recommended Discipline: '+str(discipline_dict_no_dups_rec[disc]))\n    print('Frequency that '+disc+' was the Final Discipline: '+str(discipline_dict_no_dups_fin[disc]))\n    if discipline_dict_no_dups_rec[disc]>discipline_dict_no_dups_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_no_dups_rec[disc]-discipline_dict_no_dups_fin[disc])+' ('+ str(abs(discipline_dict_no_dups_rec[disc]-discipline_dict_no_dups_fin[disc]))+' more Recommended than Final)')\n    elif discipline_dict_no_dups_rec[disc]<discipline_dict_no_dups_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_no_dups_rec[disc]-discipline_dict_no_dups_fin[disc])+' ('+ str(abs(discipline_dict_no_dups_rec[disc]-discipline_dict_no_dups_fin[disc]))+' more Final than Recommended)')\n    else:\n        print('Frequencies of Recommended and Final are Equal')\n    print('-'*50)","5a3b4b95":"#helper func to determine whether the final discipline is more\/less\/equally severe as recommended\ndef compare(rec, fin):\n    if pd.isnull(rec) or pd.isnull(fin):\n        return 'NO DATA'\n    if rec == fin:\n        return \"EQUAL\"\n    if rec<fin and fin != 600 and fin!= 900 and fin!= 500:\n        return \"GREATER THAN RECOMMENDED\"\n    else: \n        return \"LESS THAN RECOMMENDED\"","2c4f5a61":"#applying helper to both data frames\ndf_detail['compare_discipline'] = df_detail.apply(lambda row: compare(row['recommended_discipline'], row['final_discipline']), axis = 1)\ndf_detail_no_dups['compare_discipline'] = df_detail_no_dups.apply(lambda row: compare(row['recommended_discipline'], row['final_discipline']), axis = 1)","d26fc663":"#printing counter of comparison func in both data sets (as labeled)\nprint('Compare Disciplines for DF_DETAIL (with duplicates)\\n')\nprint(df_detail.compare_discipline.value_counts())\nprint('-'*50)\nprint('Compare Disciplines for DF_DETAIL (without duplicates)\\n')\nprint(df_detail_no_dups.compare_discipline.value_counts())","d1f48ab3":"df_detail.final_finding.unique()","361ccdc5":"#counting the number and percentage of 'SU' findings in data set with duplicate data (recommneded finding)\nsmthCount = 0\nfor ind in df_detail.index:\n    recFin = df_detail['recommended_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_detail['recommended_finding'].notnull())\nprint('Recommended Finding Data (with Duplicates)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","e8c74818":"#counting the number and percentage of 'SU' findings in data set with duplicate data (final finding)\nsmthCount = 0\nfor ind in df_detail.index:\n    recFin = df_detail['final_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_detail['final_finding'].notnull())\n        \nprint('Final Finding Data (with Duplicates)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","f42801c1":"#counting the number and percentage of 'SU' findings in data set without duplicate data (recommneded finding)\nsmthCount = 0\nfor ind in df_detail_no_dups.index:\n    recFin = df_detail_no_dups['recommended_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_detail_no_dups['recommended_finding'].notnull())\n\nprint('Recommended Finding Data (without Duplicates)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","4b045d1f":"#counting the number and percentage of 'SU' findings in data set without duplicate data (final finding)\nsmthCount = 0\nfor ind in df_detail_no_dups.index:\n    recFin = df_detail_no_dups['final_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_detail_no_dups['final_finding'].notnull())\n\nprint('Final Finding Data (without Duplicates)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","7c598740":"#implementing a 'different' coutner for final and recommended finding on data with duplicates\ndiffCount = 0\nfor ind in df_detail.index:\n    recFin = df_detail['recommended_finding'][ind]\n    finFin = df_detail['final_finding'][ind]\n    if recFin == 'SU' and finFin != 'SU' and pd.notnull(finFin):\n        diffCount += 1\n    if finFin == 'SU' and recFin != 'SU' and pd.notnull(recFin):\n        diffCount+=1\ntotLen = len(df_detail['recommended_finding'].notnull())\nprint('Finding Comparison Data (with Duplicates)')\nprint('Number of Incidnets where Recommended and Final Findings Do Not Coincide: '+str(diffCount))\nprint('Total Number of Incidents: '+str(totLen))\nprint('Percentage of Differenct Recommended and Final Findings: '+str(diffCount\/totLen*100)+'%')","70714205":"#implementing a 'different' coutner for final and recommended finding on data without duplicates\ndiffCount = 0\nfor ind in df_detail_no_dups.index:\n    recFin = df_detail_no_dups['recommended_finding'][ind]\n    finFin = df_detail_no_dups['final_finding'][ind]\n    if recFin == 'SU' and finFin != 'SU' and pd.notnull(finFin):\n        diffCount += 1\n    if finFin == 'SU' and recFin != 'SU' and pd.notnull(recFin):\n        diffCount+=1\ntotLen = len(df_detail_no_dups['recommended_finding'].notnull())\nprint('Finding Comparison Data (without Duplicates)')\nprint('Number of Incidnets where Recommended and Final Findings Do Not Coincide: '+str(diffCount))\nprint('Total Number of Incidents: '+str(totLen))\nprint('Percentage of Differenct Recommended and Final Findings: '+str(diffCount\/totLen*100)+'%')","9bc66e2a":"#helper func to compare incidents when final and recommended finding didn't coincide\nnoneList = ['UN', 'NS', 'EX', 'NAF', 'NC']\ndef compareFinding (rec, fin):\n    if pd.isnull(rec) or pd.isnull(fin):\n        return 'NO DATA'\n    if rec == 'SU' and fin != 'SU':\n        return 'LESS THAN RECOMMENDED'\n    if rec != 'SU' and fin == 'SU':\n        return 'GREATER THAN RECOMMENDED'\n    if rec == fin or (rec in noneList and fin in noneList):\n        return 'EQUAL'","9b017a00":"#apply aboce func to both datasets\ndf_detail['compare_findings'] = df_detail.apply(lambda row: compareFinding(row['recommended_finding'], row['final_finding']), axis = 1)\ndf_detail_no_dups['compare_findings'] = df_detail_no_dups.apply(lambda row: compareFinding(row['recommended_finding'], row['final_finding']), axis = 1)","d7cf29b2":"#print a counter of finding comparisons for both dataframes\nprint('Finding Data on Differences between Recommended and Final Findings (with Duplicates)\\n')\nprint(df_detail.compare_findings.value_counts())\nprint('-'*50)\nprint('Finding Data on Differences between Recommended and Final Findings (without Duplicates)\\n')\nprint(df_detail_no_dups.compare_findings.value_counts())\nprint('-'*50)","0849f453":"df_detail.set_index('cr_id', inplace = True)","32c40e51":"df_merged = df_main.merge(df_victims, how='outer', left_index=True, right_index=True)\ndf_merged = df_merged.merge(df_detail, how = 'outer', left_index = True, right_index = True)\ndf_merged.head()","063359c3":"#remove all rows that have a null value\ndf_merged_match = df_merged.dropna(axis = 0, how = 'any')","c1c7fcba":"#see how many actual incidents we are working with\nlen(df_merged_match)","dbaf7b38":"#create counter for most common beats in merged\nmerged_match_beats = df_merged_match.beat.value_counts()","9f8423d2":"#graph above counter with top 20 beats\nsns.set(font_scale = 1)\nax = sns.barplot(x = merged_match_beats[:20].index, y = merged_match_beats[:20], order = merged_match_beats[:20].index, label = 'small')\nax.set_title('20 Beats with HIGHEST Number of Complaint Reports (Merged Data)')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set(xlabel='Beats', ylabel='Number of Complaints')\nax.tick_params(labelsize=8.5)\nplt.show()","f3ecefb1":"#break down top 20 beats into frequency by district\ndistricts_high = {}\nfor beat in merged_match_beats[:20].index:\n    district = beat[:2]\n    if district in districts_high.keys():\n        districts_high[district] += 1\n    else:\n        districts_high[district] = 1\n#print(districts_high)\ndistricts_high_sorted_keys = sorted(districts_high, key=districts_high.get, reverse=True)\ndistricts_high_sort ={}\nfor district in districts_high_sorted_keys:\n    districts_high_sort[district] = districts_high[district]\nprint(districts_high_sort)","afe7b66b":"#graph above created district data\nsns.set(font_scale = 1)\nax = sns.barplot(x = list(districts_high_sort.keys()), y = list(districts_high_sort.values()), order = list(districts_high_sort.keys()), label = 'small')\nax.set_title('Districts with Most Beats in \\'Top 20 Highest Number of Complaint Reports (Merged Data)\\'')\nax.set_xticklabels(ax.get_xticklabels())\nax.set(xlabel='Beats', ylabel='Number of Complaints')\nax.tick_params(labelsize=8.5)\nplt.show()","9d37c831":"#race counter for merged data\ndf_merged_match.race.value_counts()","8df71297":"#graph of above counter\nax = sns.countplot(x=\"race\", data=df_merged_match, order = df_merged_match.race.value_counts().index)\nax.set_title('Breakdown of Complainees by Race (Merged Data)')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","3bce991d":"#gender counter for merged data\ndf_merged_match.gender.value_counts()","a2b9ef28":"#graph of above counter\nax = sns.countplot(x=\"gender\", data=df_merged_match, order = df_merged_match.gender.value_counts().index)\nax.set_title('Breakdown of Complainees by Gender (Merged Data)')\nax.set_xticklabels(ax.get_xticklabels())\nplt.show()","ced1722b":"#age range counter for merged data\ndf_merged_match.age_band.value_counts()","3253ae7d":"#graph of above counter\nage_order = ['<20', '20-29', '30-39', '40-49', '50-59', '60-69','>=70']\nax = sns.countplot(x=\"age_band\", data=df_merged_match, order = age_order)\nax.set_title('Breakdown of Complainees by Age (Merged Data)')\nax.set_xticklabels(ax.get_xticklabels())\nplt.show()","3bc67b87":"#create counter for most common complaint types in merged data\nprint('20 Most Frequent Complaint Types (Merged Data) \u2014 Table\\n')\nfor key in df_merged_match.complaint_key.value_counts()[:20].index:\n    if(key == 'OTHER'):\n        print('OTHER')\n        print(\"Frequency: \"+str(df_merged_match.complaint_key.value_counts()[key]))\n        print('-'*50)\n    else:\n        print(key+\": \"+complaint_cat_dict[key])\n        print(\"Frequency: \"+str(df_merged_match.complaint_key.value_counts()[key]))\n        print('-'*50)","63438b37":"#create counter for least common complaint types in merged data\nprint('20 Least Frequent Complaint Types (Merged Data) \u2014 Table\\n')\nfor key in df_merged_match.complaint_key.value_counts()[-20:].index:\n    if(key == 'OTHER'):\n        print('OTHER')\n        print(\"Frequency: \"+str(df_merged_match.complaint_key.value_counts()[key]))\n        print('-'*50)\n    else:\n        print(key+\": \"+complaint_cat_dict[key])\n        print(\"Frequency: \"+str(df_merged_match.complaint_key.value_counts()[key]))\n        print('-'*50)","18f970a1":"df_merged_match.reset_index(level =0, inplace = True)","b6531ff0":"#counter for recommended disciplines\ndiscipline_dict_rec = {}\ndiscipline_dict_rec['x Day Suspension'] = 0\ndiscipline_dict_rec['Over 30 Day Suspension'] = 0\ndiscipline_dict_rec['Reprimanded'] = 0\ndiscipline_dict_rec['Administrative Termination'] = 0\ndiscipline_dict_rec['Year-long Suspension or Longer'] = 0\ndiscipline_dict_rec['Seperation'] = 0\ndiscipline_dict_rec['Reinstated'] = 0\ndiscipline_dict_rec['Nothing'] = 0\ndiscipline_dict_rec['Resigned'] = 0\nfor ind in df_merged_match.index:\n    rec = df_merged_match['recommended_discipline'][ind]\n    if rec <199:\n        discipline_dict_rec['x Day Suspension'] += 1\n    if rec == 200:\n        discipline_dict_rec['Over 30 Day Suspension'] +=1\n    if rec == 0 or rec == 100:\n        discipline_dict_rec['Reprimanded'] += 1\n    if rec == 300:\n        discipline_dict_rec['Administrative Termination'] +=1\n    if rec == 365:\n        discipline_dict_rec['Year-long Suspension or Longer'] +=1\n    if rec == 400:\n        discipline_dict_rec['Seperation'] += 1\n    if rec == 500:\n        discipline_dict_rec['Reinstated'] += 1\n    if rec == 600 or rec == 900:\n        discipline_dict_rec['Nothing'] += 1\n    if rec == 800:\n        discipline_dict_rec['Resigned'] += 1\ndiscipline_dict_rec_keys = sorted(discipline_dict_rec, key=discipline_dict_rec.get, reverse=True)\ndiscipline_dict_rec_sort ={}\nfor district in discipline_dict_rec_keys:\n    discipline_dict_rec_sort[district] = discipline_dict_rec[district]\nprint(discipline_dict_rec_sort)","e16c4f46":"#counter for final disciplines\ndiscipline_dict_fin = {}\ndiscipline_dict_fin['x Day Suspension'] = 0\ndiscipline_dict_fin['Over 30 Day Suspension'] = 0\ndiscipline_dict_fin['Reprimanded'] = 0\ndiscipline_dict_fin['Administrative Termination'] = 0\ndiscipline_dict_fin['Year-long Suspension or Longer'] = 0\ndiscipline_dict_fin['Seperation'] = 0\ndiscipline_dict_fin['Reinstated'] = 0\ndiscipline_dict_fin['Nothing'] = 0\ndiscipline_dict_fin['Resigned'] = 0\nfor ind in df_merged_match.index:\n    fin = df_merged_match['final_discipline'][ind]\n    if fin <199:\n        discipline_dict_fin['x Day Suspension'] += 1\n    if fin == 200:\n        discipline_dict_fin['Over 30 Day Suspension'] +=1\n    if fin == 0 or fin == 100:\n        discipline_dict_fin['Reprimanded'] += 1\n    if fin == 300:\n        discipline_dict_fin['Administrative Termination'] +=1\n    if fin == 365:\n        discipline_dict_fin['Year-long Suspension or Longer'] +=1\n    if fin == 400:\n        discipline_dict_fin['Seperation'] += 1\n    if fin == 500:\n        discipline_dict_fin['Reinstated'] += 1\n    if fin == 600 or fin == 900:\n        discipline_dict_fin['Nothing'] += 1\n    if fin == 800:\n        discipline_dict_fin['Resigned'] += 1\ndiscipline_dict_fin_keys = sorted(discipline_dict_fin, key=discipline_dict_fin.get, reverse=True)\ndiscipline_dict_fin_sort ={}\nfor district in discipline_dict_fin_keys:\n    discipline_dict_fin_sort[district] = discipline_dict_fin[district]\nprint(discipline_dict_fin_sort)","1e04c121":"#view all discipline data for merged data set + light analysis\nprint('DISCIPLINE DATA (Merged Data)\\n')\nfor disc in discipline_dict_rec_sort:\n    print(disc)\n    print('Frequency that '+disc+' was the Recommended Discipline: '+str(discipline_dict_rec[disc]))\n    print('Frequency that '+disc+' was the Final Discipline: '+str(discipline_dict_fin[disc]))\n    if discipline_dict_rec[disc]>discipline_dict_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_rec[disc]-discipline_dict_fin[disc])+' ('+ str(abs(discipline_dict_rec[disc]-discipline_dict_fin[disc]))+' more Recommended than Final)')\n    elif discipline_dict_rec[disc]<discipline_dict_fin[disc]:\n        print('Difference between Recommended and Final Frequencies: '+str(discipline_dict_rec[disc]-discipline_dict_fin[disc])+' ('+ str(abs(discipline_dict_rec[disc]-discipline_dict_fin[disc]))+' more Final than Recommended)')\n    else:\n        print('Frequencies of Recommended and Final are Equal')\n    print('-'*50)","8f75fb63":"#view of discpline comparison counter\nprint('Compare Disciplines for DF_DETAIL (with duplicates)\\n')\nprint(df_merged_match.compare_discipline.value_counts())","ab124900":"#counter and view for findings (recommended) in merged\nsmthCount = 0\nfor ind in df_merged_match.index:\n    recFin = df_merged_match['recommended_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_merged_match['recommended_finding'].notnull())\nprint('Recommended Finding Data (Merged Data)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","2eee7c21":"#counter and view for findings (final) in merged\nsmthCount = 0\nfor ind in df_merged_match.index:\n    recFin = df_merged_match['final_finding'][ind]\n    if recFin == 'SU':\n        smthCount += 1\ntotLen = len(df_merged_match['final_finding'].notnull())\nprint('Final Finding Data (Merged Data)\\n')\nprint('Number of Findings that were \\'Something\\': '+ str(smthCount))\nprint('Number of Total Findings: ' + str(totLen))\nprint('Percentage of \\'Something\\' Findings: '+str(smthCount\/totLen*100)+'%')","f1c66dd3":"#counter and view for the differences in recommended and final finding\ndiffCount = 0\nfor ind in df_merged_match.index:\n    recFin = df_merged_match['recommended_finding'][ind]\n    finFin = df_merged_match['final_finding'][ind]\n    if recFin == 'SU' and finFin != 'SU' and pd.notnull(finFin):\n        diffCount += 1\n    if finFin == 'SU' and recFin != 'SU' and pd.notnull(recFin):\n        diffCount+=1\ntotLen = len(df_merged_match['recommended_finding'].notnull())\nprint('Finding Comparison Data (Merged Data)')\nprint('Number of Incidnets where Recommended and Final Findings Do Not Coincide: '+str(diffCount))\nprint('Total Number of Incidents: '+str(totLen))\nprint('Percentage of Differenct Recommended and Final Findings: '+str(diffCount\/totLen*100)+'%')","6b863fcc":"#counter of comparisons on findings for merged data\nprint('Finding Data on Differences between Recommended and Final Findings (Merged Data)\\n')\nprint(df_merged_match.compare_findings.value_counts())","5921ace6":"outF = open(\"complaints.txt\", \"w\")","4fa8bc7e":"outF.write('T20 Complaints: \\n')\nfor ind in beatFreq.index:\n    beat = str(ind)\n    outF.write(beat+\" \")\noutF.write('\\n')","bc6e2657":"outF.write('L20 Complaints: \\n')\nfor ind in beatFreq_low.index:\n    beat = str(ind)\n    outF.write(beat+\" \")\noutF.write('\\n')\noutF.close()","6692877f":"Let's put all this data into a useful table...","435ace52":"Now looking at disciplines...\n","0d4e4db5":"Complaint types...","02d0897d":"We see that all the findings mean 'nothing' except for one (excluding null), let's do some analysis on this fact...","b734c956":"*Note: the above graphs represents the frequency of each district in '20 Beats with the Highest Number of Complaints' data (i.e. 4 beats in District 11 were in the 20 Beats with the Highest Number of Complaints' data)*","096fdc04":"We can further classify these incidents where the Recommended and Final Findings do not coincide into when 'SU' was recommended and when 'SU' was the final finding...","b4932891":"Sorting the above graphs by district...(which is the first two digits in the beat name)","ab9d827e":"Lastly, age... \n\n*(We will group ages by certain age ranges in order to make the graphs more readable)*","cea27693":"**Key Takeaways:**\n* There are almost no trends in the data without duplicates, unlike the data with duplicates\n* The differences between recommended and final frequencies within each discipline are all small (<15), thus suggesting that the CPD's disciplinary action follows the recommended action\n* Unsurprisingly, the less severe dispclianry actions are more commonly recommended and implemented than the more sever ones","478e1f48":"Gender...","a83f3450":"Let's look at the incidents where the recommended discipline was a greater punishment, and equal punishment, or a lesser punsihment to the final discipline...","ce868e8a":"**DF_DETAIL**","b2a89ba7":"**Key Takeaways:**","2119383e":"## Looking at all dataframes simulatenously:","4c114869":"Now, race:","2cf99552":"**Key Takeaways:**\n* The prominence of Districts 12, 16, and 19 at the bottom match the trends we saw in the other data","06f9096f":"Create a year category as specific dates are superfluous...","27587ad9":"Findings...","2de075a0":"**Key Takeaways:**\n\n* The difference in percetnages from the data with duplicates to the data without duplicates is quite disparaging, but we can still disect some overall trends:\n    * The percentages of 'Something' findings was lower in the 'Final' category than the 'Recommended' category in both datasets, suggesting that an internal commisions was recommended more often than it was implemented   ","fb1ced8c":"**Key Takeaways:**\n* Districts 2 and  11 in consistently high across all categories we have measured so far...","16677985":"Disciplines...","44f5d0c1":"Looking at complaint types...","09e04989":"Use dict for complaint categories...\n\n*Each complaint has a 3 character alphanumeric key and a longer description. We will only display the 3 character alphanumeric key and keep the corresponding description in a seperate space.*","64e02b2c":"Let's look at the breakdown of complainees by gender, race, and age...\n\nFirst, gender:","ac14b096":"In previously previewing our data, we found that only CR_ID's greater than or equal to 1000000 were present across all data sets. Let's take out the data that doesn't apply...","243a5880":"**Key Takeaways:**\n* We see that with the duplicate data, most recommended disciplines match the final discipline. \n* We also see that a final discipline less severe than the recommended discipline occurs with relative frequency\n* In both cases, the final discipline being more severe than the recommended discipline occured the least","cc68cb2a":"Age...","c9ce6a28":"**Key Takeaways:**\n* We see the majoirty of Recommened and Final Findings do coincide\n* When they happen not to coincicde, we see the Final Finding is less severe than the Recommended Finding roughly 80% of the time that the Recommended and Final findings do not coincide for both datasets","41d09d88":"Turning to the 'Finding' category...here is a key to what each finding code means:\n\n## Finding Code Key\n\nUN, NS, EX, NAF, NC = nothing\n\nSU = something\n\n800 = resigned\n\n600, 900 = nothing\n\n500 = reinstated\n\n400 = separation\n\n365 = ??? a year?\n\n300 = administrative termination\n\n0, 100 = reprimand\n\n200 = over 30 day suspension\n\n< 199 = x day suspension","e6dcf529":"**Key Takeaways:**\n* The most common complaint types were usually complaints involving some form of impropoer arrest or search\n* Rarely do we see any sort of physical abuse or racial prejudice being presented, but such complaints still occur with relative frequency","44013956":"Now, let's count out the number and percentage of incidents where the recommended and final findings do not match.... (this will mean when once of the finding columns is a 'Something' and the other isn't, barring null values)","49bb8898":"# DF_DETAIL Processed Below","4e4e7b0e":"**Important Note:**\n\nWe found many of the complaints here had identical info across all categories. We labeled such instances as 'duplicates'. As we were unable to discern whether these 'duplicate' complaints were unique incidients under the same complaint ID or cloned data, we will run our remaining analysis on a dataset with all data and on data without said 'duplicates', which we will denote in all our outputs as 'No Dups'","c5ee764d":"## DF_MAIN Processed Below","584baaf4":"**DF_MERGED**","9c6517c5":"**DF_VICTIMS**","d25e6950":"# CPD Complaints Analysis\nIn this notebook, we examine (mostly civilian) complaints filed against the Chicago Police Department from 2006 to 2016. This supplements our efforts in our previous notebook: [1. Baseline EDA](https:\/\/www.kaggle.com\/loganchang\/baseline-eda), where we analyze more detailed, but officer-reported tactical response reports (TRRs). Our goal here is to compare and contrast the data from both perspectives, perhaps uncovering patterns such as police underreporting.","b5073b5f":"Starting with beat data...","0bd97839":"# Starting some EDA...","dda74dee":"**DF_MAIN:**","4dd95134":"We won't note any observations until we cross-reference with the beat\/district data in the 'Baseline EDA' notebook, but we can tell you from first glance we already see some intersting overlaps acorss all beat\/district data, which we will go into greater depth with in our cross-reference notebook","651bf0cb":"df_merged_match is a dataset where we merged all of the three previous datasets (using the duplicate data for DF_DETAIL) and only took incidents where all possible information was available. We'll look at the trends within this dataset for the remaining analysis...\n\n\n*Note: This dataset is really not going to tell us anything we don't already know and even be a little misleading. This  dataset is simply a reflection on which incidents are most coherent, not a reflection on the overall trends in all complaints filed within our time period (which would be our previous analysis)*","0d613171":"**Key Takeaways:**\n* The complaints listed here are a lot more 'menial' than the complaints in the data with duplicates, suggesting the types of complaints prevelant in the duplicate data are much more common...\n* The types of complaints listed here are lots of police misuse of equipment or miscellaneous complaints","d143d7a2":"**Key Takeaways:**\n* We see that in the data with duplicates, there are rarely any incidents where the recommended and final findings do not coincide, while in the data without duplicates, this phenomenan happens roughly 10% of the time\n* What this suggests is that there are some incidents where this phenomenan occurs, but is rarely presents in the greater scheme of all CR incidents\n\n*Note: We made no dilineation amongst the 'Nothing' indetifiers although there are 5 differents such identifiers. This is because we really just don't know what they mean :\/*","91755730":"Again, the only overlapping data across all three datasets is for CR IDs greater than or equal to 1000000, so let's get rid of all CR IDs that don't fit this criteria...","555f9a07":"**Key Takeaways:**\n* Complaints are filed by black citizens nearly 6x more than any other race group\n* Roughly twice as many complaints were filed by men than by women\n* The most common subgroup were black males in their 20's, but black males in general filed many complaints\n* We also see elderly citizens (>=70 years old) filed a vast number of complaints across every race and gender category","890bdd4a":"**Key Takeaways:**\n* Trends here aren't exactly clear...but we can see some of the less severe dsciplinary actions (i.e. short suspensions, reprimanding, nothing) or recommended more often than they are actually imposed\n* Conversely, the more severe disciplinary actions (i.e. longer suspensions, administrative termination, resignation) occur more often than they are recommended, though at a realtively low margin exlcuidng resignation.\n* From this, we can conclude more often than not, the CPD takes disciplinary action more severely than what disciplinary action is recommended.\n* We also see that although resignation is rarely recommended, it does happen quite often","32fae242":"Writing the top and bottom 20 beats in complaints to an output file:","adc5e4d1":"Let's see which beats and which districts had the highest and lowest number of complaints...","f9d52a93":"**Key Takeaways:**\n* Complaints are far more frequent since 2005. This could be due to an uptick in complaints in recent years or an increase in digital recoards recently","afe4b691":"We have a file that contains the name of each beat in the CPD. We will use it to make sure each complaint occurs in a relevatn beat....","da8d74da":"## DF_VICTIMS Processed Below","0d558af1":"Now, race...","fec5b7bf":"We will now put all three datasets (using the one with duplicates for DF_DETAIL) into one big dataframe called 'DF_MERGED'. We won't run any new analysis, rather look at the distributions in data where all possible info is available.","b50125c4":"We can look at the frequency of complaint reports filed by year too...","ba5a89a2":"Looking at the Beats with the lowest number of complaints isn't really worthwhile since outside of the top 20 Beats, the vast majority of beats have less than 10 complaints.\n\nInstead, we will just look at the district data of the Top 20 Beats..."}}