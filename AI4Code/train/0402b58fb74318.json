{"cell_type":{"456b5c2d":"code","e6863400":"code","de74b896":"code","f5f4f466":"code","6015f99b":"code","33c3400a":"code","3f73e3a0":"code","be96c9c3":"code","de7a0a4e":"code","98425f48":"code","85f5f75d":"code","527811cf":"code","a76ef31a":"code","bc99a406":"code","d3613524":"code","1078de2f":"code","9666e8b8":"code","c61f9802":"code","cee5e9c4":"code","b7d5e18a":"code","9499e3b0":"code","7451356d":"code","bb9c4be1":"code","19f843cb":"code","af4d63f2":"code","27113602":"code","260197b4":"code","ddd69967":"code","35fe98e2":"code","46e4958c":"code","b288b2f2":"code","40dd7f3c":"code","626c4047":"code","ec774252":"code","e550975b":"code","cc5ef5f8":"code","de23c7f1":"code","ed238993":"code","7cd3713b":"code","c8e2fc28":"code","95d4650a":"code","61d22ff6":"code","03ef0ce0":"code","bee70fd6":"code","decfc3ab":"code","9cee8c8e":"code","92602518":"code","4bb0ab19":"code","cfe6b44e":"code","16ad20cf":"code","d14e40ec":"code","68f2dccd":"code","48b5deab":"code","c1cc8362":"code","35d56ad3":"code","5a8f9a34":"code","a80612a3":"code","5b75af6f":"code","baac4287":"code","894d03b4":"code","f5170203":"code","a9c908e5":"code","d5a8aacb":"code","a581cefc":"code","2ab15732":"code","10b3513a":"code","1df06494":"code","d68e5ca1":"code","24547cbf":"code","a2d5bca5":"code","ad929ca1":"code","83c5a7c5":"code","f054f53b":"code","a6bf9c5a":"code","50be24a2":"code","27e9a29e":"code","7a76d3d6":"code","a7bb7628":"code","07b43141":"code","b9c62a50":"code","b7ac39b0":"code","c8607f62":"code","8620070a":"code","4daef62e":"code","1b6023b6":"code","04d45835":"code","a80abead":"code","01f87ff7":"code","955b3952":"code","18b6c2b7":"code","e18a4066":"code","71eb842a":"code","936b32b8":"code","922bf5ba":"code","b9c753b7":"code","75531c3e":"code","d69c2d5e":"code","a0d185b7":"code","944b874d":"code","53daa141":"code","2d0ec87f":"code","0ab6ec92":"code","287b95f4":"code","19b48afb":"code","c67769ed":"code","9929fa5a":"code","6d2fdc74":"code","277b47e4":"code","fb3099d2":"code","78f3624e":"code","e74a48c1":"code","24b7620c":"code","8677e361":"markdown","2a28ccf7":"markdown","b11820eb":"markdown","bf22ea12":"markdown","23db07c5":"markdown","ce2e6333":"markdown","7243a409":"markdown","a15a752b":"markdown","34ec7a68":"markdown","7db3dd08":"markdown","85de1fb0":"markdown","d036e07f":"markdown","a72a76c7":"markdown","005fbe15":"markdown","db0d9c5b":"markdown","08c00492":"markdown","6c93eddd":"markdown","902d2418":"markdown","f80565b4":"markdown","94cc196b":"markdown","a7e5d963":"markdown","f001835c":"markdown","5ae5ab0a":"markdown","8f45726b":"markdown","74c5490e":"markdown","9c46c1ed":"markdown","3dd212e5":"markdown","c299bac0":"markdown","59ab8055":"markdown","efd94881":"markdown","a7ad82b7":"markdown","02865a01":"markdown","45c0f50a":"markdown","e32ca75c":"markdown","0b365ccc":"markdown","1dfcbfe2":"markdown","ccc84bef":"markdown","95c7f019":"markdown","a3b847bb":"markdown","a17efa73":"markdown","07a52a2e":"markdown","ebc82533":"markdown","0146839a":"markdown","ad2dccf1":"markdown","bca90002":"markdown","2666e505":"markdown","a8fb4de8":"markdown","8cebe889":"markdown","743314a2":"markdown","1b94e576":"markdown","58682869":"markdown","c52d2a9c":"markdown","d68eac85":"markdown","d0f2cb35":"markdown","60b075b9":"markdown","ee2f8921":"markdown","d3f5b090":"markdown","03725cc3":"markdown"},"source":{"456b5c2d":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nfrom functools import partial\nfrom scipy.optimize import fmin as scip_fmin\nfrom tqdm import tqdm\nimport statistics\nimport tempfile\n\n# Visialisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom PIL import Image\n\n# Machine Learning\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\nfrom imblearn.pipeline import Pipeline as imb_pipe\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn import impute\nfrom sklearn import pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import base\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression, mutual_info_classif, mutual_info_regression, SelectKBest, SelectPercentile, VarianceThreshold\nfrom sklearn.decomposition import TruncatedSVD\n# Models\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n#Metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score","e6863400":"data_dir = '..\/input\/jobathon-analytics-vidhya'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","de74b896":"RANDOM_SEED = 42","f5f4f466":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","6015f99b":"seed_everything()","33c3400a":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","3f73e3a0":"train_df.sample(10)","be96c9c3":"train_df.columns","de7a0a4e":"train_df.describe().T","98425f48":"nulls_train = np.sum(train_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()\nprint('There are', len(nullcols_train), 'features with missing values in the training data.')\nprint(f'Columns containing nulls are: {list(nullcols_train.index)}')","85f5f75d":"nulls_train = np.sum(train_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\nnullcols_train = nullcols_train.apply(lambda x: 100*x\/train_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","527811cf":"nulls_train = np.sum(test_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()\nprint('There are', len(nullcols_train), 'features with missing values in the test data.')\nprint(f'Columns containing nulls are: {list(nullcols_train.index)}')","a76ef31a":"nulls_train = np.sum(test_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\nnullcols_train = nullcols_train.apply(lambda x: 100*x\/test_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","bc99a406":"print(list(train_df['Holding_Policy_Duration'].unique()))","d3613524":"train_df['Holding_Policy_Duration'].fillna(0, inplace=True)\ntest_df['Holding_Policy_Duration'].fillna(0, inplace=True)","1078de2f":"print(list(train_df['Holding_Policy_Type'].unique()))","9666e8b8":"train_df['Holding_Policy_Type'].fillna(0, inplace=True)\ntest_df['Holding_Policy_Type'].fillna(0, inplace=True)","c61f9802":"print(list(train_df['Health Indicator'].unique()))","cee5e9c4":"train_df['Health Indicator'].fillna(train_df['Health Indicator'].mode()[0], inplace=True)\ntest_df['Health Indicator'].fillna(test_df['Health Indicator'].mode()[0], inplace=True)","b7d5e18a":"np.sum(train_df.isnull())","9499e3b0":"np.sum(test_df.isnull())","7451356d":"ax = plt.subplots(figsize=(18, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Response', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Response\", size=20);","bb9c4be1":"train_df.nunique()","19f843cb":"g = sns.catplot(x='Response', y='Reco_Policy_Premium', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","af4d63f2":"g = sns.catplot(x='Reco_Policy_Cat', y='Reco_Policy_Premium', hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(30,8)","27113602":"g = sns.catplot(x='Reco_Insurance_Type', y='Reco_Policy_Premium', col='Is_Spouse' ,hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(30,8)","260197b4":"g = sns.catplot(x='Reco_Insurance_Type', y='Reco_Policy_Premium', hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(30,8)","ddd69967":"g = sns.catplot(x='Response', y='Upper_Age', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","35fe98e2":"g = sns.catplot(x='Response', y='Lower_Age', kind='boxen', data=train_df);\ng.fig.set_size_inches(15,8)","46e4958c":"g = sns.catplot(x='Health Indicator', y='Upper_Age', hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(30,8)","b288b2f2":"g = sns.catplot(x='Health Indicator', y='Lower_Age', hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(30,8)","40dd7f3c":"ax = plt.subplots(figsize=(30, 8))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='City_Code', hue='Response', data=train_df);","626c4047":"# Response Rate from Cities\nv = train_df.groupby('City_Code').Response.value_counts().unstack()\nv['Ratio'] = v[1]\/v[0]\nv.reset_index(inplace=True)","ec774252":"v['Ratio'].mean()","e550975b":"ax = plt.subplots(figsize=(30, 8))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='City_Code', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","cc5ef5f8":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Accomodation_Type', hue='Response', data=train_df);","de23c7f1":"# Response Rate from Accomodation Types\nv = train_df.groupby('Accomodation_Type').Response.value_counts().unstack()\nv['Ratio'] = v[1]\/v[0]\nv.reset_index(inplace=True)","ed238993":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Accomodation_Type', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","7cd3713b":"g = sns.catplot(x='Accomodation_Type', y='Reco_Policy_Premium', hue='Response', kind='box', data=train_df);\ng.fig.set_size_inches(10,8)","c8e2fc28":"train_df['Cust_Type'] = train_df['Holding_Policy_Duration'].apply(lambda x : 'New' if x==0 else 'Old')\ntest_df['Cust_Type'] = test_df['Holding_Policy_Duration'].apply(lambda x : 'New' if x==0 else 'Old')","95d4650a":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Cust_Type', hue='Response', data=train_df);","61d22ff6":"# Response Rate from Customer Types\nv = train_df.groupby('Cust_Type').Response.value_counts().unstack()\nv['Ratio'] = v[1]\/v[0]\nv.reset_index(inplace=True)","03ef0ce0":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Cust_Type', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","bee70fd6":"target = ['Response']\nnot_features = ['ID', 'Response', 'kfold']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","decfc3ab":"categorical_features = [\n    'City_Code', 'Accomodation_Type', 'Reco_Insurance_Type', 'Region_Code',\n    'Is_Spouse', 'Health Indicator', 'Holding_Policy_Type', 'Reco_Policy_Cat',\n    'Cust_Type'\n]\nnumerical_features = [feat for feat in features if feat not in categorical_features]\n\nprint(f'Categorical Features: {categorical_features}')\nprint(f'Numeric Features: {numerical_features}')","9cee8c8e":"g = sns.pairplot(train_df[numerical_features + ['Response']], hue='Response')\ng.fig.set_size_inches(10,8)","92602518":"train_df_cor_spear = train_df[numerical_features].corr(method='spearman')\nplt.figure(figsize=(10,8))\nsns.heatmap(train_df_cor_spear, square=True, cmap='coolwarm', annot=True);","4bb0ab19":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df[['Upper_Age', 'Lower_Age']], orient=\"h\", palette=\"Set2\");","cfe6b44e":"plt.figure(figsize=(10,8))\nsns.boxplot(data=train_df['Reco_Policy_Premium'], orient=\"h\", palette=\"Set2\");","16ad20cf":"ax = plt.subplots(figsize=(20, 7))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='City_Code', data=train_df);","d14e40ec":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Accomodation_Type', data=train_df);","68f2dccd":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Reco_Insurance_Type', data=train_df);","48b5deab":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Is_Spouse', data=train_df);","c1cc8362":"ax = plt.subplots(figsize=(20, 7))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Health Indicator', data=train_df);","35d56ad3":"ax = plt.subplots(figsize=(15, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Holding_Policy_Type', data=train_df);","5a8f9a34":"ax = plt.subplots(figsize=(20, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Reco_Policy_Cat', data=train_df);","a80612a3":"ax = plt.subplots(figsize=(8, 5))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Cust_Type', data=train_df);","5b75af6f":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nNUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.Response.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","baac4287":"train_df['Age_Conf_Interval'] = train_df['Upper_Age'] - train_df['Lower_Age']\ntest_df['Age_Conf_Interval'] = test_df['Upper_Age'] - test_df['Lower_Age']","894d03b4":"train_df['Long_Term_Cust'] = train_df['Holding_Policy_Duration'].apply(lambda x : 'Yes' if x=='14+' else 'No')\ntest_df['Long_Term_Cust'] = test_df['Holding_Policy_Duration'].apply(lambda x : 'Yes' if x=='14+' else 'No')","f5170203":"train_df['Reco_Policy_Premium_log'] = np.log(train_df['Reco_Policy_Premium'])\ntest_df['Reco_Policy_Premium_log'] = np.log(test_df['Reco_Policy_Premium'])","a9c908e5":"train_df['Mean_Age'] = (train_df['Upper_Age'] + train_df['Upper_Age'])\/2\ntest_df['Mean_Age'] = (test_df['Upper_Age'] + test_df['Upper_Age'])\/2","d5a8aacb":"train_df['Permium_Per_Age_Year'] = np.log(train_df['Reco_Policy_Premium']\/train_df['Mean_Age'])\ntest_df['Permium_Per_Age_Year'] = np.log(test_df['Reco_Policy_Premium']\/test_df['Mean_Age'])","a581cefc":"train_df['City_Region'] = train_df[\"City_Code\"].astype(str) + '_' + train_df[\"Region_Code\"].astype(str)\ntest_df['City_Region'] = test_df[\"City_Code\"].astype(str) + '_' + test_df[\"Region_Code\"].astype(str)","2ab15732":"train_df['Mean_Age_Bin_10'] = pd.cut(train_df['Mean_Age'], bins=10, labels=False)\ntrain_df['Mean_Age_Bin_50'] = pd.cut(train_df['Mean_Age'], bins=50, labels=False)\ntest_df['Mean_Age_Bin_10'] = pd.cut(test_df['Mean_Age'], bins=10, labels=False)\ntest_df['Mean_Age_Bin_50'] = pd.cut(test_df['Mean_Age'], bins=50, labels=False)","10b3513a":"train_df['Reco_Policy_Premium_Bin_10'] = pd.cut(train_df['Reco_Policy_Premium'], bins=10, labels=False)\ntrain_df['Reco_Policy_Premium_Bin_50'] = pd.cut(train_df['Reco_Policy_Premium'], bins=50, labels=False)\ntest_df['Reco_Policy_Premium_Bin_10'] = pd.cut(test_df['Reco_Policy_Premium'], bins=10, labels=False)\ntest_df['Reco_Policy_Premium_Bin_50'] = pd.cut(test_df['Reco_Policy_Premium'], bins=50, labels=False)","1df06494":"def return_type_def(x):\n    if (x['Reco_Insurance_Type'] == 'Joint') and (x['Is_Spouse'] == 'No'):\n        return 'Family'\n    else:\n        return x['Reco_Insurance_Type']","d68e5ca1":"train_df['Offered_With'] = train_df[['Reco_Insurance_Type', 'Is_Spouse']].apply(return_type_def, axis=1)\ntest_df['Offered_With'] = test_df[['Reco_Insurance_Type', 'Is_Spouse']].apply(return_type_def, axis=1)\n\ntrain_df['Offered_With'] = train_df['Offered_With'].replace('Joint', 'Spouse')\ntest_df['Offered_With'] = test_df['Offered_With'].replace('Joint', 'Spouse')\n\ntrain_df['Offered_With'] = train_df['Offered_With'].replace('Individual', 'Self')\ntest_df['Offered_With'] = test_df['Offered_With'].replace('Individual', 'Self')","24547cbf":"train_df['City_Accomodation'] = train_df[\"City_Code\"].astype(str) + '_' + train_df[\"Accomodation_Type\"].astype(str)\ntest_df['City_Accomodation'] = test_df[\"City_Code\"].astype(str) + '_' + test_df[\"Accomodation_Type\"].astype(str)","a2d5bca5":"def make_whole_number(data):\n    _data = str(int(data))\n    _len_data = len(_data)\n    _data = \"\".join(_data[0] + '0' * (_len_data-1))\n    return int(_data)","ad929ca1":"train_df['Reco_Policy_Premium_Round'] = train_df['Reco_Policy_Premium'].apply(make_whole_number)\ntest_df['Reco_Policy_Premium_Round'] = test_df['Reco_Policy_Premium'].apply(make_whole_number)","83c5a7c5":"target = ['Response']\nnot_features = ['ID', 'Response', 'kfold']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]\nprint(features)","f054f53b":"categorical_features = [\n    'Accomodation_Type', 'Reco_Insurance_Type', 'City_Code', 'Region_Code', \n    'Is_Spouse', 'Health Indicator', 'Holding_Policy_Duration',\n    'Holding_Policy_Type','Reco_Policy_Cat', 'Cust_Type', 'Long_Term_Cust',\n    'City_Region', 'Mean_Age_Bin_10', 'Mean_Age_Bin_50',\n    'Reco_Policy_Premium_Bin_10', 'Reco_Policy_Premium_Bin_50', 'Offered_With',\n    'City_Accomodation'\n]\nnumerical_features = [feat for feat in features if feat not in categorical_features]\n\nprint(f'Categorical Features: {categorical_features}')\nprint(f'Numeric Features: {numerical_features}')","a6bf9c5a":"train_df[categorical_features] = train_df[categorical_features].astype('str')\ntest_df[categorical_features] = test_df[categorical_features].astype('str')","50be24a2":"train_df[numerical_features] = train_df[numerical_features].astype('float64')\ntest_df[numerical_features] = test_df[numerical_features].astype('float64')","27e9a29e":"train_df[target] = train_df[target].astype('float64')","7a76d3d6":"nominal_cols = [\n    'Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse', 'Health Indicator',\n    'Holding_Policy_Type', 'City_Code', 'Region_Code', 'Reco_Policy_Cat',\n    'Cust_Type', 'Long_Term_Cust', 'City_Region', 'Offered_With',\n    'City_Accomodation'\n]\nordinal_cols = [\n    'Holding_Policy_Duration', 'Mean_Age_Bin_10', 'Mean_Age_Bin_50',\n    'Reco_Policy_Premium_Bin_10', 'Reco_Policy_Premium_Bin_50'\n]","a7bb7628":"lbl_enc = preprocessing.LabelEncoder()\nfull_data = pd.concat(\n    [train_df[categorical_features], test_df[categorical_features]],\n    axis=0\n)\n\nfor col in (categorical_features):\n    print(col)\n    if train_df[col].dtype == 'object':\n        lbl_enc.fit(full_data[col].values)\n        train_df[col] = lbl_enc.transform(train_df[col])\n        test_df[col] = lbl_enc.transform(test_df[col])","07b43141":"# ce_helmert = ce.HelmertEncoder(cols=ordinal_cols)\n\n# ce_helmert.fit(train_df[ordinal_cols], train_df[target])\n\n# train_df = train_df.join(ce_helmert.transform(train_df[ordinal_cols]).add_suffix('_helm'))\n# test_df = test_df.join(ce_helmert.transform(test_df[ordinal_cols]).add_suffix('_helm'))","b9c62a50":"# train_df = train_df.drop(ordinal_cols, axis=1)\n# test_df = test_df.drop(ordinal_cols, axis=1)","b7ac39b0":"target = ['Response']\nnot_features = ['ID', 'Response', 'kfold']\ncols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in not_features]","c8607f62":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","8620070a":"ufs = UnivariateFeatureSelction(\n    n_features=0.7,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df[target].values.ravel())\n# X_transformed = ufs.transform(train_df[features])\nselected_features = ufs.return_cols(train_df[features])","4daef62e":"def rate_model(clf, x, cv = StratifiedKFold(n_splits=NUM_SPLITS),\n               fold=0, features=selected_features, target=target):\n    '''\n    Prints out various evaluation metrics for a classification task. Like:-\n    1. Classification Accuracy\n    2. ROC-AUC Score\n    3. Precision\n    4. Recall\n    5. F1 Score\n    6. Confusion matrix\n    All score are calculated in base format. No averaging is performed.\n    \n    clf - Classification Model\n    x - Input features\n    cv - Cross Validation criteria\n    fold - fold number for confusion matrix\n    features - Feature column names\n    target - Target column name\n    '''\n    \n    scoring = {'acc' : 'accuracy', 'roc' : 'roc_auc', 'precision' : 'precision', 'recall' : 'recall', 'f1' : 'f1'}\n    scores = cross_validate(clf, x[features], x[target].values.ravel(), scoring=scoring, cv=cv, return_train_score=False)\n    roc = np.mean(scores['test_roc'])\n    acc = np.mean(scores['test_acc'])\n    prec = np.mean(scores['test_precision'])\n    rec = np.mean(scores['test_recall'])\n    f1 = np.mean(scores['test_f1'])\n    print(f'ROC: {roc}')\n    print(f'Accuracy: {acc}')\n    print(f'Precision: {prec}')\n    print(f'Recall: {rec}')\n    print(f'F-Score: {f1}')","1b6023b6":"def rate_model_with_imbalance(clf, x, cv = StratifiedKFold(n_splits=NUM_SPLITS),\n                              NUM_SPLITS=NUM_SPLITS, features=selected_features,\n                              target=target):\n    '''\n    Prints out various evaluation metrics for a classification task. Like:-\n    1. Classification Accuracy\n    2. ROC-AUC Score\n    3. Precision\n    4. Recall\n    5. F1 Score\n    6. Confusion matrix\n    All score are calculated in base format. No averaging is performed.\n    \n    clf - Classification Model\n    x - Input features\n    cv - Cross Validation criteria\n    fold - fold number for confusion matrix\n    features - Feature column names\n    target - Target column name\n    '''\n    roc = []\n    for fold in range(NUM_SPLITS):\n        df_train = x[x.kfold != fold].reset_index(drop=True)\n        df_valid = x[x.kfold == fold].reset_index(drop=True)\n        oversample = SMOTE()\n        X, y = oversample.fit_resample(df_train[features], df_train[target].values.ravel())\n        clf.fit(X, y)\n        predictions = clf.predict_proba(df_valid[features])[:, 1]\n        print(f'Fold {fold+1} Done! ROC: {roc_auc_score(df_valid[target].values.ravel(), predictions)}')\n        roc.append(roc_auc_score(df_valid[target].values.ravel(), predictions))\n    print(f'Mean ROC: {statistics.mean(roc)}')","04d45835":"def rate_ensemble_model(clfs, weights, X, fold=0, features=selected_features,\n                        target=target):\n    if len(clfs) != len(weights):\n        raise Exception(\"Number of Classifiers and Weights unequal!\")\n    else:\n        X_train = X[X.kfold != fold].reset_index(drop=True)\n        X_valid = X[X.kfold == fold].reset_index(drop=True)\n        \n        preds = np.zeros(X_valid.shape[0])\n        for i, clf in enumerate(clfs):\n            clf.fit(X_train[features], X_train[target].values.ravel())\n            preds += clf.predict_proba(X_valid[features])[:, 1] * weights[i]\n        print(f'AUC Score: {roc_auc_score(X_valid[target].values.ravel(), preds)}')","a80abead":"def prepare_submission_one_model(clf, train_df=train_df, test_df=test_df,\n                                 features=selected_features, target=target):\n    clf.fit(train_df[features], train_df[target].values.ravel())\n    preds = clf.predict_proba(test_df[features])[:, 1]\n    output = pd.DataFrame({'ID': test_df['ID'],\n                           'Response': preds})\n    output.to_csv('submission.csv', index=False)\n    print('Prediction file saved. All the Best!')","01f87ff7":"def prepare_submission_ensemble(clfs, weights, train_df=train_df, test_df=test_df,\n                                features=selected_features, target=target):\n    if len(clfs) != len(weights):\n        raise Exception(\"Number of Classifiers and Weights unequal!\")\n    else:\n        preds = np.zeros(test_df.shape[0])\n        for i, clf in enumerate(clfs):\n            clf.fit(train_df[features], train_df[target].values.ravel())\n            preds += clf.predict_proba(test_df[features])[:, 1] * weights[i]\n        output = pd.DataFrame({'ID': test_df['ID'],\n                               'Response': preds})\n        output.to_csv('submission.csv', index=False)\n        print('Prediction file saved. All the Best!')","955b3952":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nclass OptimizeAUC:\n    def __init__(self):\n        self.coef_ = 0\n        \n    def _auc(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = roc_auc_score(y, predictions)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._auc, X=X, y=y)\n        initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)\n        self.coef_ = scip_fmin(loss_partial, initial_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions","18b6c2b7":"clf = LogisticRegression(C=1.0, max_iter=5000)\nrate_model(clf, train_df, fold=3)","e18a4066":"clf = KNeighborsClassifier(n_neighbors=3)\nrate_model(clf, train_df, fold=3)","71eb842a":"clf = RandomForestClassifier(n_estimators = 100,\n                             random_state=RANDOM_SEED,\n                             n_jobs=-1,\n                             class_weight='balanced_subsample')\nrate_model(clf, train_df, fold=3)","936b32b8":"NUM_BOOSTERS = 1000\n\nclf = xgb.XGBClassifier(max_depth=7,\n                        n_estimators=NUM_BOOSTERS,\n                        colsample_bytree=0.8,\n                        subsample=0.8,\n                        nthread=-1,\n                        learning_rate=0.1)\nrate_model(clf, train_df, fold=3)","922bf5ba":"NUM_BOOSTERS = 1000\nMETRIC = 'auc'\ncat_cols = [i for i, col in enumerate(train_df[selected_features].columns) if col not in numerical_features]\nEARLY_STOPPING = 100\n\nclf = LGBMClassifier(metric=METRIC,\n                     seed=RANDOM_SEED,\n                     n_estimators=NUM_BOOSTERS)\nrate_model(clf, train_df, fold=3)","b9c753b7":"NUM_BOOSTERS = 10000\nEARLY_STOPPING = 300\nMETRIC = 'AUC'\ncat_cols = [i for i, col in enumerate(train_df[selected_features].columns) if col not in numerical_features]\n\nclf = CatBoostClassifier(verbose=0,\n                         n_estimators=NUM_BOOSTERS,\n                         eval_metric=METRIC,\n                         random_seed=RANDOM_SEED,\n                         cat_features=cat_cols)\nrate_model(clf, train_df, fold=3)","75531c3e":"current_fold = 0\ndf_train = train_df[train_df.kfold != current_fold].reset_index(drop=True)\ndf_valid = train_df[train_df.kfold == current_fold].reset_index(drop=True)","d69c2d5e":"clf = CatBoostClassifier(verbose=0,\n                         n_estimators=NUM_BOOSTERS,\n                         eval_metric=METRIC,\n                         random_seed=RANDOM_SEED,\n                         cat_features=cat_cols,\n                         early_stopping_rounds=EARLY_STOPPING)\nclf.fit(df_train[selected_features], df_train.Response,\n        eval_set=(df_valid[selected_features], df_valid.Response),\n        plot=True, verbose=False);\n\nypred = clf.predict_proba(df_valid[selected_features])[:,1]\nroc_auc_score(df_valid.Response, ypred)","a0d185b7":"# clf = BalancedRandomForestClassifier(n_estimators=100)\n# rate_model(clf, train_df, fold=3)","944b874d":"current_fold = 4\n\ndf_train_4 = train_df[train_df.kfold != current_fold].reset_index(drop=True)\ndf_valid_4 = train_df[train_df.kfold == current_fold].reset_index(drop=True)","53daa141":"lgb = LGBMClassifier(metric='auc',\n                     seed=RANDOM_SEED,\n                     n_estimators=NUM_BOOSTERS)\ncb = CatBoostClassifier(verbose=0,\n                        n_estimators=NUM_BOOSTERS,\n                        eval_metric='AUC',\n                        random_seed=RANDOM_SEED,\n                        cat_features=cat_cols)\nxgbc = xgb.XGBClassifier()","2d0ec87f":"lgb.fit(df_train_4[selected_features], df_train_4[target].values.ravel())\ncb.fit(df_train_4[selected_features], df_train_4[target].values.ravel())\nxgbc.fit(df_train_4[selected_features], df_train_4[target].values.ravel())","0ab6ec92":"pred_lgb = lgb.predict_proba(df_valid_4[selected_features])[:, 1]\npred_cb = cb.predict_proba(df_valid_4[selected_features])[:, 1]\npred_xgbc = xgbc.predict_proba(df_valid_4[selected_features])[:, 1]\navg_pred = (pred_lgb + pred_cb + pred_xgbc) \/ 3","287b95f4":"fold4_preds = np.column_stack((\n    pred_lgb,\n    pred_cb,\n    pred_xgbc,\n    avg_pred\n))\n\naucs_fold4 = []\nfor i in range(fold4_preds.shape[1]):\n    auc = roc_auc_score(df_valid_4[target].values.ravel(), fold4_preds[:, i])\n    aucs_fold4.append(auc)\n    \nprint(f\"Fold-2: LGB AUC = {aucs_fold4[0]}\")\nprint(f\"Fold-2: CB AUC = {aucs_fold4[1]}\")\nprint(f\"Fold-2: XGB AUC = {aucs_fold4[2]}\")\nprint(f\"Fold-2: Average Pred AUC = {aucs_fold4[3]}\")","19b48afb":"current_fold = 1\n\ndf_train_1 = train_df[train_df.kfold != current_fold].reset_index(drop=True)\ndf_valid_1 = train_df[train_df.kfold == current_fold].reset_index(drop=True)","c67769ed":"lgb = LGBMClassifier(metric='auc',\n                     seed=RANDOM_SEED,\n                     n_estimators=NUM_BOOSTERS)\ncb = CatBoostClassifier(verbose=0,\n                        n_estimators=NUM_BOOSTERS,\n                        eval_metric='AUC',\n                        random_seed=RANDOM_SEED,\n                        cat_features=cat_cols)\nxgbc = xgb.XGBClassifier()","9929fa5a":"lgb.fit(df_train_1[selected_features], df_train_1[target].values.ravel())\ncb.fit(df_train_1[selected_features], df_train_1[target].values.ravel())\nxgbc.fit(df_train_1[selected_features], df_train_1[target].values.ravel())","6d2fdc74":"pred_lgb = lgb.predict_proba(df_valid_1[selected_features])[:, 1]\npred_cb = cb.predict_proba(df_valid_1[selected_features])[:, 1]\npred_xgbc = xgbc.predict_proba(df_valid_1[selected_features])[:, 1]\navg_pred = (pred_lgb + pred_cb + pred_xgbc) \/ 3","277b47e4":"fold1_preds = np.column_stack((\n    pred_lgb,\n    pred_cb,\n    pred_xgbc,\n    avg_pred\n))\n\naucs_fold1 = []\nfor i in range(fold1_preds.shape[1]):\n    auc = roc_auc_score(df_valid_1[target].values.ravel(), fold1_preds[:, i])\n    aucs_fold1.append(auc)\n    \nprint(f\"Fold-2: LGB AUC = {aucs_fold1[0]}\")\nprint(f\"Fold-2: CB AUC = {aucs_fold1[1]}\")\nprint(f\"Fold-2: XGB AUC = {aucs_fold1[2]}\")\nprint(f\"Fold-2: Average Pred AUC = {aucs_fold1[3]}\")","fb3099d2":"opt = OptimizeAUC()\n\nopt.fit(fold1_preds[:, :-1], df_valid_1[target].values.ravel())\nopt_preds_fold4 = opt.predict(fold4_preds[:, :-1])\nauc = roc_auc_score(df_valid_4[target].values.ravel(), opt_preds_fold4)\nprint(f\"Optimized AUC, Fold 2 = {auc}\")\nprint(f\"Coefficients = {opt.coef_}\")","78f3624e":"for i in range(NUM_SPLITS):\n    rate_ensemble_model(clfs = [lgb, cb, xgbc],\n                        weights = [0.00428773, 3.51047478, 0.34316149],\n                        X = train_df, fold=i)","e74a48c1":"# cb = CatBoostClassifier(verbose=0,\n#                         n_estimators=NUM_BOOSTERS,\n#                         eval_metric='AUC',\n#                         random_seed=RANDOM_SEED,\n#                         cat_features=cat_cols)\n# prepare_submission_one_model(clf)","24b7620c":"lgb = LGBMClassifier(metric='auc',\n                     seed=RANDOM_SEED,\n                     n_estimators=NUM_BOOSTERS)\ncb = CatBoostClassifier(verbose=0,\n                        n_estimators=NUM_BOOSTERS,\n                        eval_metric='AUC',\n                        random_seed=RANDOM_SEED,\n                        cat_features=cat_cols)\nxgbc = xgb.XGBClassifier()\n\nprepare_submission_ensemble(clfs = [lgb, cb, xgbc],\n                            weights = [0.00428773, 3.51047478, 0.34316149])","8677e361":"### B. Holding_Policy_Type","2a28ccf7":"## 3. Random Forest","b11820eb":"### C. Health Indicator\nUnlike the previous two, health indicator is a bit tricky. It can be assumed that this value was derived from pre-existing metric based on some empirical calculations like BMI wherein the user had to enter their details. And this variable can be null when the user decided not to fill those values. There is nothing much we can do about it. Let's pill the NaN with the most common value in that category.","bf22ea12":"The recommended policy premium rises with Age.  \n## 10. Numerical Features Quartiles","23db07c5":"Reco_Policy_Premium has a long tail. We can take logarithm of the feature to reduce the impact of outliers.  \n## 11. Categorical Feature Counts","ce2e6333":"As we suspected the only zero non-number value in the entire range of values in NaN. Thus it can be inferred that those values are pertaining to non-existing customers. And we can fill them up by 0.","7243a409":"## 1. Age Confidence Interval\nSince we have a upper and lowe limit for age we can create a confidence interval for the age.","a15a752b":"## 10. Join City and Accomodation Type\nWhich in practical sense means 'Renting a home in Mumbai' (for example)","34ec7a68":"## 4. XGBoost","7db3dd08":"## 4. Mean Age","85de1fb0":"## 8. Balanced Random Forest","d036e07f":"## 1. Logistic Regression","a72a76c7":"Because the counts of 'Holding_Policy_Type' and 'Holding_Policy_Duration' are similar and like 'Holding_Policy_Duration' there are no indicator of non-existing customers in the 'Holding_Policy_Type' column it can be easily inferred that those are the same customersd and we can assign a type from out side. Let's call them '0'.","005fbe15":"## 6. CatBoost","db0d9c5b":"## 9. Ensemble  \nAll referenced from [here](https:\/\/github.com\/abhishekkrthakur\/approachingalmost)","08c00492":"We can get a naive idea about the type of variables form the definition itself and looking at the data makes it clearer.","6c93eddd":"# Model Building","902d2418":"The situation is almost similar in test set as well. And looking at the feature definition of the missing values I have a hunch that these values are absent because those people are not current customers of the company. Let's look at them one by one:-  \n### A. Holding_Policy_Duration","f80565b4":"Ok, so we see that there are certain policies (type = 3, 13, 16, etc) where we can assume the high premium for that class of policy might be one of the reasons for the lead going cold. However Insurance type has little to no effect on it's own.  \n## 5. Age","94cc196b":"# Features Selection\nNow let's only select the important features to overcome the curse of dimensionality.","a7e5d963":"## 5. LightGBM","f001835c":"Now we can say that the data is \"Nulls' free.  \nNow, since it's a classification problem, let's look at the class imbalance of the training set as it will impact the choice and approach of the model largely.  \n## 2. Class Imbalance","5ae5ab0a":"So almost 40% values in 'Holding_Policy_Duration' and 'Holding_Policy_Type' and about 22% values in 'Health Indicator' are nulls. Let's explore what is the situation in Test data:-","8f45726b":"People renting homes are less likely to buy insurances if the premium is too high.  \n## 8. Existing vs New Customer\nAs we inferred earlier, people having 'Holding_Policy_Duration' as NaN (imputed with 0) are most likely to be new customers. So let's create a synthetic feature signifying the same.","74c5490e":"## 4. Recommended Policy Premium","9c46c1ed":"The distribution of premium price of recommended policy is sort of similar between a lead and no-lead customer. But one other way of looking at this might be:- 'What if the person felt that the premium is too high for a particular category of policy?'  \nSo let's take that into account as well...","3dd212e5":"So we can infer that the upper and Lower age of customer on it's own has a very little impact on the type of response we can get. But's let's try to factor in the health condition as well.","c299bac0":"# Features Encoding\nIrrespective of the column datatype, we can split the data based on their definition into categorical and Numeric Data types.","59ab8055":"The lean to non-lead ratio in every city is below 1, i.e, there are more no-leads as comapred to leads in each and every city. But there are certain cities which perrform better than average like C30, C32, etc.  \n## 7. Accomodation_Type","efd94881":"An older customer is slightly more likely to take up an offered insurance as compared to new ones.  \nFrom the inference of single and bi feature EDA it is evident that there is no clearcut single variable responsible for determining the final class. So it is going to be a uphill task to separate the two clusters.  \nNow let's move to feature engineering and Feature selection part..  \n## 9. Numerical Features Distribution\nLet's explore the relationship of each numerical variable with one another","a7ad82b7":"Similarly, for 'Reco_Insurance_Type' :-","02865a01":"## 9. Joint But Not Spouse\nThere are some people who were offered Joint insurance, but not with their spouse. Let's call that member Family.","45c0f50a":"# KFold Splits  \nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not rix any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","e32ca75c":"Let's see what columns we have in the training data.","0b365ccc":"## 8. Proposed Premium Buckets\nSimilarly proposed policy premium can be bucketed and dictretized into groups.","1dfcbfe2":"Both accomodation types have a similarish response, with home owners slightly more favorable to be leads. What if we combine accomodation_type with Insurance premium. Because logically it should ahve an effect...","ccc84bef":"# Prediction","95c7f019":"# Why this Competition?\nThis competition provides an unique oppertunity for Data Science begiiners to participate in a Hackathon style challenge for Data Science and compete for potentiaql oppertunities in many reputed companies. It also provides the unique oppertunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- **Classification**.\n\n# Problem Statement\nOur client (FinMan Company) wishes to cross-sell health insurance to the existing customers who may or may not hold insurance policies with the company. A policy is recommended to a person when they land on their website, and if the person chooses to fill up a form to apply it is considered as a Positive outcome (Classified as lead). All other conditions are considered Zero outcomes.\n\n## Data Description:-\nWe have the following information regarding the potential-customer and the insurance at any given point in time:\n* Demographics (city, age, region etc.)\n* Information regarding holding policies of the customer\n* Recommended Policy Information\n\n## Expected Outcome:-\n* Build a model to predict whether the person will be interested in their proposed Health plan\/policy given the information above.\n* Grading Metric: **ROC_AUC_SCORE**\n\n## Problem Category:-\nFor the data and objective its is evident that this is a **Binary Classification Problem** in the **Tabular Data** format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey of Lead prediction:-","a3b847bb":"## 3. Recommended Policy Price Log","a17efa73":"## 5. Premium to Age Ratio","07a52a2e":"# Utils","ebc82533":"## 2. k-NN","0146839a":"Now let's convert each of the numeric column into proper datatype...","ad2dccf1":"## 2. Long Term Customer\nWe can create a synthetic variable saying if the person is a long term customer of the company.","bca90002":"Let's fill the missing values with mode.","2666e505":"# Feature Engineering","a8fb4de8":"## 6. City Code and Region Code Combine\nWe can combine the city and region code to create a single variable that represents the locality","8cebe889":"That's a lot of null values. Let's see what that looks like from a percentage stand point.","743314a2":"# Things Tried  \nThe following things were tried during tetsing but didn't work. Hence are removed from final submission:-\n* Encoding:\n    * Label Encoder (Worse result)\n    * One Hot Encoder (Worse result)\n    * Binary Encoder (Worse result)\n    * Mean Endoder (Overfitting)\n    * Frequency Encoder (Overfitting)\n    * Hashing Encoder (Worse result)\n    * Catboost Encoder (Overfitting)\n    * Target Encoder (Overfitting)\n    * Leave One Out Encoder (Overfitting)\n    * Weight Of Evidence Encoder (Overfitting)\n    * M-Estimate Encoder (Overfitting)\n    * Entity Embeddings (No considerable increase in performance)\n\n\n* Class Balancing:\n    * Random Over Sampling (Overfitting)\n    * Random Under Sampling (Worse result)\n    * Random Over and Under Sampling together (Overfitting)\n    * Smote (Overfitting)\n    * Tomek Links (Worse result)\n\n\n* Models:\n    * Deep Learning (Worse result)\n    * Easy Ensemble (Overfitting)","1b94e576":"We can see that there are some Null values in some columns. Which is not good for data ingestion into any model, so let's see the Null situation upfront:-  \n## 1. Null Values","58682869":"Now that the numeric features are ready, moving on to Categorical Features encoding.  \nCategorical Features can be of 2 types:-  \n* Nominal (No significant Order)\n* Ordinal (Order is significant)  \n  \nSo let's split them up first and encode them after.","c52d2a9c":"## 7. Customer Age Buckets\nCustomer ages can be bucketed and dictretized into groups.","d68eac85":"From the column keys in problem statement we know the following information about each of the features:-\n\n Variable      | Definition         \n :-----------  |:------------\n ID\tUnique | Identifier for a row\nCity_Code |\tCode for the City of the customers\nRegion_Code | Code for the Region of the customers\nAccomodation_Type |\tCustomer Owns or Rents the house\nReco_Insurance_Type | Joint or Individual type for the recommended insurance  \nUpper_Age |\tMaximum age of the customer \nLower_Age |\tMinimum age of the customer\nIs_Spouse | If the customers are married to each other (in case of joint insurance) \nHealth_Indicator | Encoded values for health of the customer\nHolding_Policy_Duration | Duration (in years) of holding policy (a policy that customer has already subscribed to with the company)\nHolding_Policy_Type | Type of holding policy\nReco_Policy_Cat\t| Encoded value for recommended health insurance\nReco_Policy_Premium\tAnnual | Premium (INR) for the recommended health insurance\nResponse (Target) |\t0 : Customer did not show interest in the recommended policy,\n| 1 : Customer showed interest in the recommended policy","d0f2cb35":"Using Label encoding for Nominal Data and Helmhert for Ordinal Data","60b075b9":"Okay, so it is an imbalanced set. We have to keep that in mind while modelling and choosing hyper-parameters later.  \n## 3. Feature Value Counts","ee2f8921":"# EDA\nLet's have a basic look around the data we have at hand first","d3f5b090":"**Interesting! So we can infer that there are certain health conditions like X7, X8 and X9 where the chances of getting a positive response is higher only after a particular age limit.**  \n## 6. City Code","03725cc3":"## 11. Recommended Policy Premium to Round Numbers\nLet's convert the recommended policy premium to a whole round number so that we do not factor in the noise and trivialities in the exact number."}}