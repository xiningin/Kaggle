{"cell_type":{"191b123d":"code","7344a766":"code","9835c39e":"code","1c5bb45f":"code","477828f9":"code","3e251c5c":"code","da24b223":"code","47f7f14f":"code","629038b0":"code","6f16723f":"code","c43e4b74":"code","5923d701":"code","03c71310":"code","ad74e894":"code","aee36ed6":"code","b712d81f":"code","61d57d48":"code","ed4e98d6":"code","b6b33f70":"code","2f650465":"code","b461c86d":"code","aa9192d7":"code","d6a46e5e":"code","2d54bf53":"code","17905018":"code","b0464962":"code","95c2d6eb":"code","e9c7879f":"code","2ec56be3":"code","6cf8e7c8":"code","4c93fb61":"code","e9e6176f":"code","a48c0519":"code","21785132":"code","9d8621e0":"code","b03c09e6":"code","26840cb4":"code","3b55b670":"code","fcb59e55":"code","08f99cfa":"code","cfd06659":"code","f1f3873f":"code","afd5043a":"code","5cf4c552":"markdown","a3a85399":"markdown","3594c5c0":"markdown","a7385afa":"markdown","613948bd":"markdown","9bc3d934":"markdown","4eba5b89":"markdown","5792a793":"markdown","abee4347":"markdown","208e661e":"markdown","97b2e387":"markdown","658c17ed":"markdown","b9b667d6":"markdown","a879b974":"markdown","a0eb7140":"markdown","8dc8dcd6":"markdown","c576fa90":"markdown","2285362f":"markdown"},"source":{"191b123d":"#! pip install kaggle\n#! mkdir ~\/.kaggle\n#! cp kaggle.json ~\/.kaggle\/\n#! chmod 600 ~\/.kaggle\/kaggle.json\n","7344a766":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.ensemble import GradientBoostingClassifier","9835c39e":"#! kaggle datasets download fedesoriano\/heart-failure-prediction\n","1c5bb45f":"#! unzip heart-failure-prediction.zip","477828f9":"df  = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","3e251c5c":"def describe(df):                        # Function to explore major elements in a Dataset\n                                         # Will help to find null values present and deal with them\n  columns=df.columns.to_list()           # Function will help to directly find numerical and categorical columns\n  ncol=df.describe().columns.to_list()\n  ccol=[]\n  for i in columns:\n    if(ncol.count(i)==0):\n      ccol.append(i)\n    else:\n      continue\n  print('Name of all columns in the dataframe:')\n  print(columns)\n  print('')\n  print('Number of columns in the dataframe:')\n  print(len(columns))\n  print('')\n  print('Name of all numerical columns in the dataframe:')\n  print(ncol)\n  print('')\n  print('Number of numerical columns in the dataframe:')\n  print(len(ncol))\n  print('')\n  print('Name of all categorical columns in the dataframe:')\n  print(ccol)\n  print('')\n  print('Number of categorical columns in the dataframe:')\n  print(len(ccol))\n  print('')\n  print('------------------------------------------------------------------------------------------------')\n  print('')\n  print('Number of Null Values in Each Column:')\n  print('')\n  print(df.isnull().sum())\n  print('')\n  print('')\n  print('Number of Unique Values in Each Column:')\n  print('')\n  print(df.nunique())\n  print('')\n  print('')\n  print('Basic Statistics and Measures for Numerical Columns:')\n  print('')\n  print(df.describe().T)\n  print('')\n  print('')\n  print('Other Relevant Metadata Regarding the Dataframe:')\n  print('')\n  print(df.info())\n  print('')\n  print('')\n\n\n\n  \n","da24b223":"describe(df)\n","47f7f14f":"oe=['g','r']\nfig = plt.figure(figsize=(15,15))\n\nplt.subplot(3,2,1)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"Sex\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='Sex ', ylabel='Count')\n\nplt.subplot(3,2,2)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"ChestPainType\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='ChestPainType', ylabel='Count')\n\nplt.subplot(3,2,3)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"ExerciseAngina\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='ExerciseAngina', ylabel='Count')\n\nplt.subplot(3,2,4)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"RestingECG\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='RestingECG', ylabel='Count')\n\nplt.subplot(3,2,5)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"ST_Slope\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='ST_Slope', ylabel='Count')\n\nplt.subplot(3,2,6)\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nsns.histplot(data=df, x=\"FastingBS\", hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n#ax.set(xlabel='FastingBS', ylabel='Count')","629038b0":"def outliers(df_column):\n  q75, q25 = np.percentile(df_column, [75 ,25]) \n  iqr = q75 - q25\n  print('q75: ',q75)\n  print('q25: ',q25)\n  print('Inter Quartile Range: ',iqr)\n  print('Outliers lie before', q25-1.8*iqr, 'and beyond', q75+1.8*iqr) \n\n  # Usually 1.5 times IQR is considered, but we have used 1.8 for broader range\n\n  print('Number of Rows with Left Extreme Outliers:', len(df[df_column <q25-1.8*iqr]))\n  print('Number of Rows with Right Extreme Outliers:', len(df[df_column>q75+1.8*iqr]))\n  plt.tight_layout()\n  plt.style.use('seaborn')\n  sns.set_context('notebook')\n  sns.histplot(data=df, x=df_column, hue=\"HeartDisease\",multiple=\"stack\",palette=oe)\n\n  ","6f16723f":"outliers(df['RestingBP']) ","c43e4b74":"# Since outliers are slightly skewed towards the right extreme side, we will keep them for now and just drop the left extreme values\ndf=df[df.RestingBP>=84]\nlen(df)\n\n","5923d701":"df.head()","03c71310":"outliers(df['Cholesterol']) \n# Multiple Values of 0 value are present\n#Since Cholesterol value can't be zero, these values are missing and have to be dealt with prior to model creation\n","ad74e894":"df=df[df.Cholesterol<=500]\ndf.head()\nlen(df)","aee36ed6":"outliers(df['MaxHR'])","b712d81f":"outliers(df['Oldpeak'])","61d57d48":"df[df.Cholesterol==0].mean() #Effort to understand how data looks like where values are missing, i.e. 0 Cholesterol ","ed4e98d6":"df[df.Cholesterol>0].mean()","b6b33f70":"print('Mean: ',df['Cholesterol'].mean())\nprint('Median: ',df['Cholesterol'].median())","2f650465":"df[df['Cholesterol']>0].Cholesterol.mean()","b461c86d":"df.describe().T","aa9192d7":"def OHE(dfcolumn):\n  global df\n  dfcolumn.nunique()\n  len(df.columns)\n  finallencol = (dfcolumn.nunique() - 1) + (len(df.columns)-1)\n  dummies = pd.get_dummies(dfcolumn, drop_first=True, prefix=dfcolumn.name)\n  df=pd.concat([df,dummies],axis='columns')\n  df.drop(columns=dfcolumn.name,axis=1,inplace=True) # We have to drop columns to aviod multi-collinearity\n  if(finallencol==len(df.columns)):\n    print('One Hot Encoding was successful!') \n    print('')\n  else:\n    print('Error in OHE XXXX')\n  return df\n","d6a46e5e":"OHE(df['ChestPainType'])\nOHE(df['Sex'])\nOHE(df['RestingECG'])\nOHE(df['ExerciseAngina'])\nOHE(df['ST_Slope'])\n","2d54bf53":"# Imupute Values for 0 Cholesterol, Method Used: KNN-Imputation \n\ndf['Cholesterol'].replace(to_replace = 0, value =np.nan, inplace=True)\nKNN_imputed = KNNImputer(n_neighbors=5)\nI=KNN_imputed.fit_transform(df)\nCholesterol=[]\nfor i in range(0,len(df)):\n  Cholesterol.append(I[i][2])\ndf['Cholesterol']=Cholesterol ","17905018":"# Find VIF (Variance Inflation Factor) To Check for collinearity among independent variables\nvif = df.copy()\nvif.drop(columns='HeartDisease',axis=1,inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i)\n                          for i in range(len(vif.columns))]\n","b0464962":"vif_data # Standard practice is to deal with variables with VIF >= 10","95c2d6eb":"# Scale Data For Higher Efficiency\n\nfrom sklearn.preprocessing import StandardScaler # Converts Columnar Data into Standard Normal Distribution\nscaler=StandardScaler()\nscaler.fit(vif)\nscaled_data=scaler.transform(vif)\nscaled_data\n\n","e9c7879f":"from sklearn.decomposition import PCA # Reduce Dimensions by Principal Component Analysis To Compensate for Variables with High VIF\npca=PCA(n_components=11)\npca.fit(scaled_data)\nx_pca=pca.transform(scaled_data)\nx_pca","2ec56be3":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)","6cf8e7c8":"from sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state=0)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)","4c93fb61":"from sklearn.metrics import classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(classifier, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()","e9e6176f":"logreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\nprint('')\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n","a48c0519":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 103, stop = 300, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","21785132":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train, y_train)","9d8621e0":"rf_random.best_params_","b03c09e6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)\nclf=RandomForestClassifier(n_estimators=124,min_samples_split= 2,\n                           min_samples_leaf= 1,max_features='sqrt',max_depth=None, bootstrap=False)\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(clf, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()","26840cb4":"\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n\t\t\t'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n\t\t\t'kernel': ['rbf']}\n\ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=5)\ngrid.fit(x_train, y_train)\n","3b55b670":"print(grid.best_params_)\nprint(grid.best_estimator_)\ny_pred = grid.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(grid, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()\n\n","fcb59e55":"import tensorflow as tf\ntf.random.set_seed(0)\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss=tf.keras.losses.binary_crossentropy,\n    optimizer=tf.keras.optimizers.Adam(lr=0.03),\n    metrics=[\n        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n)\n\nhistory = model.fit(x_train, y_train, epochs=100)","08f99cfa":"predictions = model.predict(x_test)\ny_pred = [\n    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n]\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nprint(confusion_matrix(y_test, y_pred))\n\n","cfd06659":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train, y_train)\ny_pred=knn.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(knn, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()","f1f3873f":"gnb = GaussianNB()\ngnb.fit(x_train, y_train)\ny_pred=gnb.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(gnb, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()","afd5043a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['HeartDisease'], test_size=0.25, random_state=0)\nclff = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, max_depth=1, random_state=23)\nclff.fit(x_train, y_train)\ny_pred=clff.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint('')\nprint('------------------------')\nprint('Confusion Matrix')\nprint('------------------------')\nprint('')\nprint(confusion_matrix(y_test, y_pred))\nplot_confusion_matrix(clff, x_test, y_test,cmap=\"binary\") \nplt.grid(False)\nplt.show()\n","5cf4c552":"6.6 KNN Classifier","a3a85399":"6.7 Gaussian Navie Bayes Algorithm","3594c5c0":"2.1 Vizualize Data in Terms of Target Variable","a7385afa":"5. Data Transformation Before Model Building","613948bd":"6.3 Random Forest Classifier","9bc3d934":"2. Explore Dataset","4eba5b89":"1. Import Libraries \n","5792a793":"All models are subject to betterment with more stringent hyper-parameter tuning. This can be achieved by random selection, brute force methods, etc. Various other classifiers can also be used, but the most standard classifiers have been considered in this notebook.","abee4347":"The best recall achieved was **91%** with and average of **89.5%**. An accuracy of **94%** has also been achieved. ","208e661e":"4. Convert Categorical Variables into Numerical by One Hot Encoding","97b2e387":"**6. Final Model and Algorithms Used**","658c17ed":"3. Outlier Detection","b9b667d6":"6.2 Logistic Regression Function","a879b974":"6.8 Gradient Boosting Algorithm","a0eb7140":"6.1 Support Vector Machine (Radial Basis Function Kernel)","8dc8dcd6":"6.5 Neural Network Using Tensor flow ","c576fa90":"Recommend standard practices for data transformation, outlier detection, and null value substitution have been incorporated in this notebook","2285362f":"6.4 Support Vector Machine (Radial Basis Function Kernel) with Grid Search CV"}}