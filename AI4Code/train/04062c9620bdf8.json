{"cell_type":{"b5cc61b5":"code","a0e926cf":"code","2259b0da":"code","b6326e07":"code","cdf5841a":"code","6d3d443e":"code","40629e43":"code","808bcf04":"code","50f0ba93":"code","79f26aaa":"code","8eed4315":"code","a42b1f4b":"code","e856beeb":"code","de710a66":"code","6cf8a7fc":"code","f0235ea5":"code","b9bd603b":"code","1ff7ed0a":"code","44ff1af3":"code","e72a3357":"code","8c00d4e8":"code","c1a9c431":"code","7e13b667":"code","3145ba23":"code","157370a0":"code","b48b3bcc":"code","bf5e31b0":"code","93b64238":"code","cfbf21e2":"code","6b3dbdf6":"code","a3fde023":"code","23324032":"code","cd973bd1":"code","ca76938a":"code","d579a453":"code","88c2468c":"code","a926c140":"code","d247db10":"code","a690dbaa":"code","4ea5d3b7":"code","f5a0a1a6":"code","cac88d89":"code","03324a38":"code","1ac254c4":"code","8da5009f":"code","2896b7e9":"code","59caa763":"code","4269835a":"code","b293f62e":"code","0507b0de":"code","71aafd09":"code","c27ffca4":"code","86d1da8a":"code","27adc4aa":"code","59873dad":"code","77aa5a78":"code","79250711":"code","7284afd7":"code","c4a426a6":"code","b10bcb13":"code","8e7f3f55":"code","462d5a42":"code","d7a85e73":"markdown","9c1efac2":"markdown","5a606f94":"markdown","c84b4806":"markdown","59c75d36":"markdown","d488ccc0":"markdown","4e6580a8":"markdown","579e9f1b":"markdown","9a8656b6":"markdown","d4a5d998":"markdown","15ebc109":"markdown","cdb7238c":"markdown","97bd9826":"markdown","536e443f":"markdown","eb88f13f":"markdown","76759175":"markdown","08ec151d":"markdown","af244a41":"markdown","40753edb":"markdown","e812ffe4":"markdown","408c8058":"markdown","22e91fd2":"markdown","e5b6d739":"markdown","4c9a75e2":"markdown","3dba2cf2":"markdown","1bb3258d":"markdown","4c429f58":"markdown","3611ebe6":"markdown","f3d53d98":"markdown","5dafba36":"markdown","45159b5e":"markdown","ad3f2074":"markdown","70b6b62f":"markdown","29d43a11":"markdown","3e9ea90d":"markdown","a7973eb1":"markdown","7ce0fdea":"markdown","9cd81cf4":"markdown","73e86fb9":"markdown","72d3fcc2":"markdown","ab2ca21d":"markdown","0f2ee412":"markdown","39e3c63e":"markdown","3837d10b":"markdown","4d24c52a":"markdown","015d4e37":"markdown","cdb01074":"markdown","bffdd788":"markdown","def2763e":"markdown","27dcba84":"markdown","f907e8a0":"markdown","2dd29f32":"markdown","5c574ad1":"markdown","b4d764a0":"markdown","6720a76f":"markdown","2247c5f1":"markdown","bd12061b":"markdown","daad6f4d":"markdown","5acafb80":"markdown","090a73be":"markdown","e67bb844":"markdown"},"source":{"b5cc61b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)maxle\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a0e926cf":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport operator ","2259b0da":"df_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')","b6326e07":"print (\"Train data target 1\")\nprint (df_train[df_train['target']==1].count())\n\nprint (\"Train data target 0\")\nprint (df_train[df_train['target']==0].count())","cdf5841a":"all_phrases=df_train[df_train.target != 2]\nall_words = []\nfor t in all_phrases.question_text:\n    all_words.append(t)\nall_words[:4]","6d3d443e":"all_text = pd.Series(all_words).str.cat(sep=' ')","40629e43":"from wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(all_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","808bcf04":"neg_phrases = df_train[df_train.target == 1]\nneg_words = []\nfor t in neg_phrases.question_text:\n    neg_words.append(t)\nneg_words[:4]","50f0ba93":"neg_text = pd.Series(neg_words).str.cat(sep=' ')\nneg_text[:100]","79f26aaa":"from wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(neg_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8eed4315":"pos_phrases = df_train[df_train.target == 0]\npos_words = []\nfor t in pos_phrases.question_text:\n    pos_words.append(t)\npos_words[:4]","a42b1f4b":"pos_text = pd.Series(pos_words).str.cat(sep=' ')\npos_text[:100]","e856beeb":"from wordcloud import WordCloud\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(pos_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","de710a66":"df_train['length'] = df_train['question_text'].str.count(' ') + 1\ndf_test['length'] = df_test['question_text'].str.count(' ') + 1","6cf8a7fc":"print (df_train[df_train['target']==1]['length'].median())\nprint (df_train[df_train['target']==0]['length'].median())","f0235ea5":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    return unknown_words","b9bd603b":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","1ff7ed0a":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\npunct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\npunct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {'examinaton': 'examination',\n                'undergraduation': 'under graduation',\n                'fianc\u00e9': 'fiance',\n                'qoura': 'quora',\n                'bhakts': 'followers',\n                'quorans': 'quora users',\n                'brexit': 'Britain exit',\n                'cryptocurrencies': 'cryptocurrency',\n                'colour': 'color',\n                'centre': 'center',\n                'favourite': 'favorite',\n                'travelling': 'traveling',\n                'counselling': 'counseling',\n                'theatre': 'theater',\n                'cancelled': 'canceled',\n                'labour': 'labor',\n                'organisation': 'organization',\n                'wwii': 'world war 2',\n                'citicise': 'criticize',\n                'youtu ': 'youtube ',\n                'Qoura': 'Quora',\n                'sallary': 'salary',\n                'Whta': 'What',\n                'narcisist': 'narcissist',\n                'howdo': 'how do',\n                'whatare': 'what are',\n                'howcan': 'how can',\n                'howmuch': 'how much',\n                'howmany': 'how many',\n                'whydo': 'why do',\n                'doI': 'do I',\n                'theBest': 'the best',\n                'howdoes': 'how does',\n                'mastrubation': 'masturbation',\n                'mastrubate': 'masturbate',\n                \"mastrubating\": 'masturbating',\n                'pennis': 'penis',\n                'Etherium': 'Ethereum',\n                'narcissit': 'narcissist',\n                'bigdata': 'big data',\n                '2k17': '2017',\n                '2k18': '2018',\n                'qouta': 'quota',\n                'exboyfriend': 'ex boyfriend',\n                'airhostess': 'air hostess',\n                \"whst\": 'what',\n                'watsapp': 'whatsapp',\n                'demonitisation': 'demonetization',\n                'demonitization': 'demonetization',\n                'demonetisation': 'demonetization',\n                'pok\u00e9mon': 'pokemon',\n                'paralizing': 'paralising',\n                'perfeccionism': 'perfectionism',\n                'depreciaton': 'depreciation',\n                'abvicable': 'abdicable',\n                'catanation': 'catenation',\n                'leasership': 'leadership',\n                'webassembly': 'web assembly',\n                'fortitide': 'fortitude',\n                'withdrow': 'withdraw',\n                'bomblasts': 'bomb blasts',\n                'engineerer': 'engineer',\n                'citycarclean': 'city car clean',\n                'billionsites': 'billion sites',\n                'willhandjob': 'will hand job',\n                'fireguns': 'fire guns',\n                'justeat': 'just eat',\n                'ubereats': 'uber eats',\n                'doinformation': 'do information',\n                'freshersworld': 'freshers world',\n                'topicwise': 'topic wise',\n                'excitee': 'excited',\n                'bengalore': 'bangalore',\n                'proproetor': 'proprietor',\n                'migeration': 'migration',\n                'ejectulate': 'Ejaculate',\n                'glucoze': 'glucose',\n                'whatapp': 'whatsapp',\n                'sumup': 'sum up',\n                'besic': 'basic',\n                'experienceed': 'experienced',\n                'feminisam': 'feminism',\n                'kayboard': 'keyboard',\n                'retructuring': 'restructuring',\n                'becomd': 'become',\n                'preidct': 'predict',\n                'statups': 'startups',\n                'superbrains': 'super brains',\n                'becoome': 'become',\n                'gwroth': 'growth',\n                'wakeupnow': 'wake up now',\n                'headpone': 'headphone',\n                'industiry': 'industry',\n                'arichtecture': 'architecture',\n                'simlarity': 'similarity',\n                'walmartlabs': 'walmart labs',\n                'thunderstike': 'thunder stike',\n                'maintanable': 'maintainable',\n                'diffferently': 'differently',\n                'careamics': 'ceramics',\n                'sinnister': 'sinister',\n                'quoras': 'quora',\n                'breakimg': 'breaking',\n                'surggery': 'surgery',\n                'whatwill': 'what will',\n                'adhaar': 'identity',\n                'aidentity': 'identity',\n                'upwork': 'up work',\n                'alshamsi': 'al shamsi',\n                'litecoin': 'cryptocurrency ',\n                'chapterwise': 'chapter wise',\n                'blockchains': 'blockchain',\n                'flipcart': 'flipkart',\n               'Terroristan': 'terrorist Pakistan',\n                'terroristan': 'terrorist Pakistan',\n                'BIMARU': 'Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh',\n                'Hinduphobic': 'Hindu phobic',\n                'hinduphobic': 'Hindu phobic',\n                'Hinduphobia': 'Hindu phobic',\n                'hinduphobia': 'Hindu phobic',\n                'Babchenko': 'Arkady Arkadyevich Babchenko faked death',\n                'Boshniaks': 'Bosniaks',\n                'Dravidanadu': 'Dravida Nadu',\n                'mysoginists': 'misogynists',\n                'MGTOWS': 'Men Going Their Own Way',\n                'mongloid': 'Mongoloid',\n                'unsincere': 'insincere',\n                'meninism': 'male feminism',\n                'jewplicate': 'jewish replicate',\n                'unoin': 'Union',\n                'daesh': 'Islamic State of Iraq and the Levant',\n                'Kalergi': 'Coudenhove-Kalergi',\n                'Bhakts': 'Bhakt',\n                'bhakts': 'Bhakt',\n                'Tambrahms': 'Tamil Brahmin',\n                'Pahul': 'Amrit Sanskar',\n                'SJW': 'social justice warrior',\n                'SJWs': 'social justice warrior',\n                'incel': ' involuntary celibates',\n                'incels': ' involuntary celibates',\n                'emiratis': 'Emiratis',\n                'weatern': 'western',\n                'westernise': 'westernize',\n                'Pizzagate': 'Pizzagate conspiracy theory',\n                'nai\u0308ve': 'naive',\n                'Skripal': 'Sergei Skripal',\n                'Remainers': 'British remainer',\n                'remainers': 'British remainer',\n                'bremainer': 'British remainer',\n                'antibrahmin': 'anti Brahminism',\n                'HYPSM': 'Harvard, Yale, Princeton, Stanford, MIT',\n                'HYPS': 'Harvard, Yale, Princeton, Stanford',\n                'kompromat': 'compromising material',\n                'Tharki': 'pervert',\n                'tharki': 'pervert',\n                'mastuburate': 'masturbate',\n                'Zoe\u0308': 'Zoe',\n                'indans': 'Indian',\n                'xender': 'gender',\n                'Naxali ': 'Naxalite ',\n                'Naxalities': 'Naxalites',\n                'Bathla': 'Namit Bathla',\n                'Mewani': 'Indian politician Jignesh Mevani',\n                'cliche\u0301d': 'cliche',\n                'cliche\u0301': 'cliche',\n                'cliche\u0301s': 'cliche',\n                'Wjy': 'Why',\n                'Fadnavis': 'Indian politician Devendra Fadnavis',\n                'Awadesh': 'Indian engineer Awdhesh Singh',\n                'Awdhesh': 'Indian engineer Awdhesh Singh',\n                'Khalistanis': 'Sikh separatist movement',\n                'madheshi': 'Madheshi',\n                'BNBR': 'Be Nice, Be Respectful',\n                'Bolsonaro': 'Jair Bolsonaro',\n                'XXXTentacion': 'Tentacion',\n                'Padmavat': 'Indian Movie Padmaavat',\n                'Z\u030ciz\u030cek': 'Slovenian philosopher Slavoj \u017di\u017eek',\n                'Adityanath': 'Indian monk Yogi Adityanath',\n                'Brexit': 'British Exit',\n                'Brexiter': 'British Exit supporter',\n                'Brexiters': 'British Exit supporters',\n                'Brexiteer': 'British Exit supporter',\n                'Brexiteers': 'British Exit supporters',\n                'Brexiting': 'British Exit',\n                'Brexitosis': 'British Exit disorder',\n                'brexit': 'British Exit',\n                'brexiters': 'British Exit supporters',\n                'jallikattu': 'Jallikattu',\n                'fortnite': 'Fortnite ',\n                'Swachh': 'Swachh Bharat mission campaign ',\n                'Quorans': 'Quoran',\n                'Qoura ': 'Quora ',\n                'quoras': 'Quora',\n                'Quroa': 'Quora',\n                'QUORA': 'Quora',\n                'narcissit': 'narcissist',\n                # extra in sample\n                'Doklam': 'Tibet',\n                'Drumpf': 'Donald Trump fool',\n                'Drumpfs': 'Donald Trump fools',\n                'Strzok': 'Hillary Clinton scandal',\n                'rohingya': 'Rohingya ',\n                'wumao': 'cheap Chinese stuff',\n                'wumaos': 'cheap Chinese stuff',\n                'Sanghis': 'Sanghi',\n                'Tamilans': 'Tamils',\n                'biharis': 'Biharis',\n                'Rejuvalex': 'hair growth formula',\n                'Feku': 'Fake',\n                'deplorables': 'deplorable',\n                'muhajirs': 'Muslim immigrant',\n                'Gujratis': 'Gujarati',\n                'Chutiya': 'Fucker',\n                'Chutiyas': 'Fucker',\n                'thighing': 'masturbate',\n                '\u5350': 'Nazi Germany',\n                'Pribumi': 'Native Indonesian',\n                'Gurmehar': 'Gurmehar Kaur Indian student activist',\n                'Novichok': 'Soviet Union agents',\n                'Khazari': 'Khazars',\n                'Demonetization': 'demonetization',\n                'demonetisation': 'demonetization',\n                'demonitisation': 'demonetization',\n                'demonitization': 'demonetization',\n                'demonetisation': 'demonetization',\n                'cryptocurrencies': 'cryptocurrency',\n                'Hindians': 'North Indian who hate British',\n                'vaxxer': 'vocal nationalist ',\n                'remoaner': 'remainer ',\n                'bremoaner': 'British remainer ',\n                'Jewism': 'Judaism',\n                'Eroupian': 'European',\n                'WMAF': 'White male married Asian female',\n                'moeslim': 'Muslim',\n                'cishet': 'cisgender and heterosexual person',\n                'Eurocentric': 'Eurocentrism ',\n                'Jewdar': 'Jew dar',\n                'Asifa': 'abduction, rape, murder case ',\n                'marathis': 'Marathi',\n                'Trumpanzees': 'Trump chimpanzee fool',\n                'Crimean': 'Crimea people ',\n                'atrracted': 'attract',\n                'LGBT': 'lesbian, gay, bisexual, transgender',\n                'Boshniak': 'Bosniaks ',\n                'Myeshia': 'widow of Green Beret killed in Niger',\n                'demcoratic': 'Democratic',\n                'raaping': 'rape',\n                'D\u00f6nmeh': 'Islam',\n                'feminazism': 'feminism nazi',\n                'langague': 'language',\n                'Hongkongese': 'HongKong people',\n                'hongkongese': 'HongKong people',\n                'Kashmirians': 'Kashmirian',\n                'Chodu': 'fucker',\n                'penish': 'penis',\n                'micropenis': 'tiny penis',\n                'Madridiots': 'Real Madrid idiot supporters',\n                'Ambedkarite': 'Dalit Buddhist movement ',\n                'ReleaseTheMemo': 'cry for the right and Trump supporters',\n                'harrase': 'harass',\n                'Barracoon': 'Black slave',\n                'Castrater': 'castration',\n                'castrater': 'castration',\n                'Rapistan': 'Pakistan rapist',\n                'rapistan': 'Pakistan rapist',\n                'Turkified': 'Turkification',\n                'turkified': 'Turkification',\n                'Dumbassistan': 'dumb ass Pakistan',\n                'facetards': 'Facebook retards',\n                'rapefugees': 'rapist refugee',\n                'superficious': 'superficial',\n                # extra from kagglers\n                'colour': 'color',\n                'centre': 'center',\n                'favourite': 'favorite',\n                'travelling': 'traveling',\n                'counselling': 'counseling',\n                'theatre': 'theater',\n                'cancelled': 'canceled',\n                'labour': 'labor',\n                'organisation': 'organization',\n                'wwii': 'world war 2',\n                'citicise': 'criticize',\n                'youtu ': 'youtube ',\n                'sallary': 'salary',\n                'Whta': 'What',\n                'narcisist': 'narcissist',\n                'narcissit': 'narcissist',\n                'howdo': 'how do',\n                'whatare': 'what are',\n                'howcan': 'how can',\n                'howmuch': 'how much',\n                'howmany': 'how many',\n                'whydo': 'why do',\n                'doI': 'do I',\n                'theBest': 'the best',\n                'howdoes': 'how does',\n                'mastrubation': 'masturbation',\n                'mastrubate': 'masturbate',\n                'mastrubating': 'masturbating',\n                'pennis': 'penis',\n                'Etherium': 'Ethereum',\n                'bigdata': 'big data',\n                '2k17': '2017',\n                '2k18': '2018',\n                'qouta': 'quota',\n                'exboyfriend': 'ex boyfriend',\n                'airhostess': 'air hostess',\n                'whst': 'what',\n                'watsapp': 'whatsapp',\n                # extra\n                'bodyshame': 'body shaming',\n                'bodyshoppers': 'body shopping',\n                'bodycams': 'body cams',\n                'Cananybody': 'Can any body',\n                'deadbody': 'dead body',\n                'deaddict': 'de addict',\n                'Northindian': 'North Indian ',\n                'northindian': 'north Indian ',\n                'northkorea': 'North Korea',\n                'Whykorean': 'Why Korean',\n                'koreaboo': 'Korea boo ',\n                'Brexshit': 'British Exit bullshit',\n                'shithole': 'shithole ',\n                'shitpost': 'shit post',\n                'shitslam': 'shit Islam',\n                'shitlords': 'shit lords',\n                'Fck': 'Fuck',\n                'fck': 'fuck',\n                'Clickbait': 'click bait ',\n                'clickbait': 'click bait ',\n                'mailbait': 'mail bait',\n                'healhtcare': 'healthcare',\n                'trollbots': 'troll bots',\n                'trollled': 'trolled',\n                'trollimg': 'trolling',\n                'cybertrolling': 'cyber trolling',\n                'sickular': 'India sick secular ',\n                'suckimg': 'sucking',\n                'Idiotism': 'idiotism',\n                'Niggerism': 'Nigger',\n                'Niggeriah': 'Nigger'}\n","44ff1af3":"glove = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nparagram =  '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_news = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n\n\n\ndef load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index\n","e72a3357":"glove_embeddings = load_embed(glove)\nprint (len(glove_embeddings))","8c00d4e8":"train = df_train['question_text']\ntest = df_test['question_text']\ndf = pd.concat([train ,test])\n\nvocab = build_vocab(df)","c1a9c431":"print(\"oov : Glove \")\noov = check_coverage(vocab, glove_embeddings)\n\nadd_lower(glove_embeddings, vocab)\n\nprint(\"oov : \")\noov = check_coverage(vocab, glove_embeddings)\n","7e13b667":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known\n\ndef clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","3145ba23":"def clean_special_chars(text, punct, puncts, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    for p in puncts:\n        text = text.replace(p, f' {p} ')\n        \n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","157370a0":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","b48b3bcc":"import re\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","bf5e31b0":"# Lowering\ndf_train['question_text'] = df_train['question_text'].apply(lambda x: x.lower())\n# Contractions\ndf_train['question_text'] = df_train['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# Special characters\ndf_train['question_text'] = df_train['question_text'].apply(lambda x: clean_special_chars(x, punct, puncts, punct_mapping))\n# Spelling mistakes\ndf_train['question_text'] = df_train['question_text'].apply(lambda x: correct_spelling(x, mispell_dict))\n# Clean Numbers\ndf_train['question_text'] = df_train['question_text'].apply(lambda x: clean_numbers(x))","93b64238":"# Lowering\ndf_test['question_text'] = df_test['question_text'].apply(lambda x: x.lower())\n# Contractions\ndf_test['question_text'] = df_test['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# Special characters\ndf_test['question_text'] = df_test['question_text'].apply(lambda x: clean_special_chars(x, punct,puncts, punct_mapping))\n# Spelling mistakes\ndf_test['question_text'] = df_test['question_text'].apply(lambda x: correct_spelling(x, mispell_dict))\n# clean numbers\ndf_test['question_text'] = df_test['question_text'].apply(lambda x: clean_numbers(x))","cfbf21e2":"train = df_train['question_text']\ntest = df_test['question_text']\ndf = pd.concat([train ,test])\n\nvocab = build_vocab(df)\n\nprint(\"oov : \")\noov = check_coverage(vocab, glove_embeddings)","6b3dbdf6":"#oov","a3fde023":"#train_Y=df_train['target']\n#train_X=df_train['question_text']\nfrom sklearn.model_selection import train_test_split\n#X_train, X_test2, y_train, y_test2 = train_test_split(train_X, train_Y, test_size=0.04, random_state=123)\n#X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.045, random_state=123)\n\ntrain_df, test2_df = train_test_split(df_train, test_size=0.04, random_state=123)\ntrain_df, valid_df = train_test_split(train_df, test_size=0.00001, random_state=123)\n\ntrain_df=train_df.reset_index(drop=True)\nvalid_df=valid_df.reset_index(drop=True)\ntest2_df=test2_df.reset_index(drop=True)\n\nX_train=train_df['question_text'].values\ny_train=train_df['target'].values\n\n\nX_test2=test2_df['question_text'].values\ny_test2=test2_df['target'].values\n\n\nX_valid=valid_df['question_text'].values\ny_valid=valid_df['target'].values\n\n\nX_test=df_test['question_text'].values\n\n\n\nprint (X_train.shape)\nprint (y_train.shape)\n\n\nprint (X_valid.shape)\nprint (y_valid.shape)\n\n\n\nprint (X_test2.shape)\nprint (y_test2.shape)\n\n\nprint (X_test.shape)\n\n\nprint (train_df.shape)\nprint (valid_df.shape)\nprint (test2_df.shape)","23324032":"X_len_train=train_df['length'].values\nX_len_test2=test2_df['length'].values\nX_len_valid=valid_df['length'].values\nX_len_test=df_test['length'].values\n\nprint (X_len_train.shape)\nprint (X_len_test2.shape)\nprint (X_len_valid.shape)\nprint (X_len_test.shape)","cd973bd1":"embedding_dim = 300 # how big is each word vector\nmax_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 # max number of words in a question to use\n\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=max_features)\n#tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train.tolist() + X_valid.tolist()+ X_test2.tolist() + X_test.tolist())","ca76938a":"vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\nprint (vocab_size)","d579a453":"#tokenizer.word_index","88c2468c":"#maxlen=70","a926c140":"def index_to_matrix(embeddings_index,word_index):\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return (embedding_matrix)","d247db10":"glove_embedding_matrix=index_to_matrix(glove_embeddings,tokenizer.word_index)\nembedding_matrix=glove_embedding_matrix","a690dbaa":"from keras.models import Model\nfrom keras.layers import Dense, Embedding, Bidirectional, CuDNNGRU,CuDNNLSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Input, Dropout, Add\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras import layers\nimport keras.callbacks\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint","4ea5d3b7":"def make_model(embedding_matrix, maxlen, embed_size=300, loss='binary_crossentropy'):\n    inp    = Input(shape=(maxlen,))\n    inp2   = Input(shape=(1,))\n    x      = Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    #x      = Bidirectional(CuDNNGRU(256, return_sequences=True))(x)\n    #x      = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x      = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x      = Dropout(0.2)(x)\n    x      = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    #x      = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    #x      = Attention(maxlen)(x)\n    #x      = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    x      = Dropout(0.2)(x)\n    #x      = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    #x      = Dropout(0.25)(x)\n    avg_pl = GlobalAveragePooling1D()(x)\n    max_pl = GlobalMaxPooling1D()(x)\n    concat = concatenate([avg_pl, max_pl])\n    #add=Add()([concat, inp2])\n    #concat = concatenate([avg_pl, max_pl,inp2])      # using sentence length as one of the feature.\n    dense1  = Dense(32, activation=\"relu\")(concat)\n    #dense1  = Dense(32, activation=\"relu\")(concat)\n    concat = concatenate([dense1, inp2])\n    #drop1   = Dropout(0.2)(concat)\n    #dense2  = Dense(8, activation=\"relu\")(drop1)\n    #drop2   = Dropout(0.1)(dense2)\n    #dense3  = Dense(8, activation=\"relu\")(dense1)\n    output = Dense(1, activation=\"sigmoid\")(concat)\n    \n    #model  = Model(inputs=inp, outputs=output)\n    model = Model(inputs=[inp,inp2], outputs=output)\n    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    #model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n    \n    return model","f5a0a1a6":"model = make_model(embedding_matrix,maxlen=70)","cac88d89":"model.summary()","03324a38":"import matplotlib.pyplot as plt \nplt.style.use('ggplot')\n\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    \n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    \n    ## Loss\n    plt.figure(1)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    ## Accuracy\n    plt.figure(2)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","1ac254c4":"from sklearn.model_selection import StratifiedKFold\nseed = 7\nn_splits=5\nnp.random.seed(seed)\nkfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)","8da5009f":"from keras.preprocessing.sequence import pad_sequences\ni=1\nmaxlength={}\nmaxl={}\nfor train, valid in kfold.split(X_train, y_train):\n    #print ((train))\n    if i <=n_splits:\n        maxl[i]=X_len_train[train].max()\n        print (X_len_train[train])\n        maxlength[i]=int(np.quantile(X_len_train[train],0.999))\n        print (\"Running Fold\", i, \"\/\", n_splits)\n        print (\"split 99.9 percentile length\",maxlength[i],\"split max length\",maxl[i])\n        modelname=str(\"Model\") + str(i)\n        #print (train)\n        #print (valid)\n        train_data=[X_train[j] for j in train]\n        valid_data=[X_train[k] for k in valid]\n        \n        #print (train_data[0])\n        #print (valid_data[0])\n        \n        train_data = tokenizer.texts_to_sequences(train_data)\n        valid_data = tokenizer.texts_to_sequences(valid_data)\n        train_data = pad_sequences(train_data, padding='post', maxlen=maxlength[i])\n        valid_data = pad_sequences(valid_data, padding='post', maxlen=maxlength[i])\n        #print (train_data[0])\n        #print (valid_data[0])\n        #x_test2 = pad_sequences(x_test2, padding='post', maxlen=maxlen)\n        #x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n        #print (x_train[train][:10])\n        #print ('before')\n        model = make_model(embedding_matrix,maxlength[i])\n        \n        #print ('after')        \n        #print (\"train\", x_train[train][:1])\n        #print (\"train\", y_train[train][:100])\n        print (\"train\", y_train[train].sum())\n        print (\"validation\",y_train[valid].sum())\n        checkpointer = ModelCheckpoint(filepath=modelname,monitor='val_loss', mode='auto', verbose = 1, save_best_only=True)\n        history = model.fit([train_data,X_len_train[train]], y_train[train],\n                        epochs=5,\n                        validation_data=([valid_data,X_len_train[valid]], y_train[valid]),\n                        batch_size=512,callbacks=[checkpointer])\n        plot_history(history)\n        i=i+1","2896b7e9":"#plot_history(history)","59caa763":"y_pred={}\ny_pred_test={}\n\ncount=0\nfor i in np.arange(1, n_splits+1, 1):\n    try:\n        model.load_weights(str(\"Model\") + str(i))\n        print (str(\"Model\") + str(i))\n        import sklearn\n        from sklearn.metrics import f1_score\n        #y_pred[i] = model.predict(x_test2, batch_size=512, verbose=1)\n        #y_pred_test[i] = model.predict(x_test, batch_size=512, verbose=1)\n        \n        x_test2 = tokenizer.texts_to_sequences(X_test2)\n        x_test = tokenizer.texts_to_sequences(X_test)\n        x_test2 = pad_sequences(x_test2, padding='post', maxlen=maxlength[i])\n        x_test = pad_sequences(x_test, padding='post', maxlen=maxlength[i])\n        \n        y_pred[i] = model.predict([x_test2,X_len_test2], batch_size=512, verbose=1)\n        y_pred_test[i] = model.predict([x_test,X_len_test], batch_size=512, verbose=1)\n       \n        \n        model_f1_score={}\n        for thresh in np.arange(0.1, 0.91, 0.01):\n            thresh = np.round(thresh, 2)\n            #print(\"F1 score at threshold {0} is {1}\".format(thresh, sklearn.metrics.f1_score(y_valid, (y_pred>=thresh).astype(int))))\n            model_f1_score[thresh]=sklearn.metrics.f1_score(y_test2, (y_pred[i]>=thresh).astype(int))\n               \n        model_cutoff=max(model_f1_score, key=model_f1_score.get)\n        print(\"Max F1 score  is {1} found at threshold {0}\".format(model_cutoff, model_f1_score[model_cutoff]))\n        count=count+1\n        #y_pred_final=y_pred_final + y_pred[i]\n    except:\n        pass\n\nprint ('count is ', count)\n","4269835a":"y_pred_final={}\ny_pred_test_final={}\nfor i in np.arange(1, count+1, 1):\n    if (i == 1):\n        y_pred_final=y_pred[i]\n        y_pred_test_final=y_pred_test[i]\n        #print (i)\n    else:\n        y_pred_final=y_pred_final + y_pred[i]\n        y_pred_test_final=y_pred_test_final + y_pred_test[i] \n        #print(i)\n    \n#print (count)\ny_pred_final=y_pred_final\/count\ny_pred_test_final=y_pred_test_final\/count\n","b293f62e":"print (\"Final Model on hold out\") \nmodel_f1_score={}\nfor thresh in np.arange(0.1, 0.91, 0.01):\n    thresh = np.round(thresh, 2)\n    #print(\"F1 score at threshold {0} is {1}\".format(thresh, sklearn.metrics.f1_score(y_valid, (y_pred>=thresh).astype(int))))\n    model_f1_score[thresh]=sklearn.metrics.f1_score(y_test2, (y_pred_final>=thresh).astype(int))\n\nmodel_cutoff=max(model_f1_score, key=model_f1_score.get)\nprint(\"Max F1 score  is {1} found at threshold {0}\".format(model_cutoff, model_f1_score[model_cutoff]))","0507b0de":"y_pred_test2_final_class = (y_pred_final >= model_cutoff).astype(int) \nprint (y_test2.sum())\nprint (y_pred_test2_final_class.sum())\n\nimport sklearn\nfrom sklearn.metrics import f1_score\nprint(sklearn.metrics.f1_score(y_test2, y_pred_test2_final_class)) \n\nprint ('classification report')\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test2, y_pred_test2_final_class))\n\nprint ('Confusion matrix')\nprint(sklearn.metrics.confusion_matrix(y_test2, y_pred_test2_final_class)) \n\nfrom sklearn.metrics import roc_auc_score\nprint ('roc score')\nprint(sklearn.metrics.roc_auc_score(y_test2, y_pred_test2_final_class))","71aafd09":"df=pd.DataFrame(columns=['text','y_actual', 'y_pred','y_pred_prob','length'])","c27ffca4":"df['text'] = X_test2\ndf['y_pred'] =y_pred_test2_final_class\ndf['y_pred_prob'] =y_pred_final\ndf['length'] =X_len_test2\ndf['y_actual'] =y_test2","86d1da8a":"df[(df.y_actual != df.y_pred ) & (df.y_pred_prob >=(model_cutoff-0.3)) & (df.y_pred_prob <=(model_cutoff + 0.3))]['text'].values","27adc4aa":"FN_phrases = df[(df.y_actual == 1) & (df.y_pred == 0)]\nFN_phrases.shape\n\nFN_words = []\nfor t in FN_phrases.text:\n    FN_words.append(t)\nFN_words[:10]\n\nFN_text = pd.Series(FN_words).str.cat(sep=' ')\nFN_text[:100]\n\n","59873dad":"from wordcloud import WordCloud\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(FN_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","77aa5a78":"from collections import Counter\ncounts = Counter(FN_text.split())\nprint(counts)","79250711":"FP_phrases = df[(df.y_actual == 0) & (df.y_pred == 1)]\nFP_phrases.shape\n\n\nFP_words = []\nfor t in FP_phrases.text:\n    FP_words.append(t)\n#print (FP_words[:10])\n\nFP_text = pd.Series(FP_words).str.cat(sep=' ')\n#print (FP_text[:100])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(FP_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","7284afd7":"from collections import Counter\ncounts = Counter(FP_text.split())\nprint(counts)","c4a426a6":"df_test['prediction']= (y_pred_test_final >= model_cutoff).astype(int) ","b10bcb13":"#df_test","8e7f3f55":"df_test=df_test.drop(['question_text'], axis=1)\ndf_test=df_test.drop(['length'], axis=1)","462d5a42":"df_test.to_csv(r'submission.csv', index = False)","d7a85e73":"paragram_embeddings = load_embed(paragram)\nprint (len(paragram_embeddings))","9c1efac2":"# Make model instance","5a606f94":"print(x_train[0])\nprint(X_train[0])\nprint(x_test[0])\nprint(X_test[0])","c84b4806":"# functions to handle special characters","59c75d36":"# Import Libraries","d488ccc0":"# Vocab Size","4e6580a8":"# find accuracy, precision, recall , auc , f1 on holdout","579e9f1b":"checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5',monitor='val_loss', mode='auto', verbose = 1, save_best_only=True)\n","9a8656b6":"# All text - word cloud","d4a5d998":"# Insincere Questions - word cloud","15ebc109":"print(os.listdir(\"..\/input\/embeddings\/wiki-news-300d-1M\"))","cdb7238c":"# Create df with actual and predicted values of holdout","97bd9826":"# Check OOV (out of vocabulary)","536e443f":"import gc\ngc.collect()\ndel glove_embeddings\ngc.collect()","eb88f13f":"# lists to handle contractions, punctuations, mis spelled words","76759175":"# False negative wordcloud","08ec151d":"# Write to csv","af244a41":"# Different Embeddings","40753edb":"# Build vocab again and check coverage after the above data handling","e812ffe4":"# Running model for kfold","408c8058":"# Import Data","22e91fd2":"# Choose Embedding","e5b6d739":"# function to handle wrong spellings","4c9a75e2":"x_train = tokenizer.texts_to_sequences(X_train)\nx_valid = tokenizer.texts_to_sequences(X_valid)\nx_test2 = tokenizer.texts_to_sequences(X_test2)\nx_test = tokenizer.texts_to_sequences(X_test)","3dba2cf2":"# Define Model","1bb3258d":"from nltk.stem import PorterStemmer\nfrom textblob import Word\nstemmer = PorterStemmer()","4c429f58":"# functions to handle contractions","3611ebe6":"# Get tokens for words using keras tokenizer","f3d53d98":"# Split the data into train, holdout","5dafba36":"# Make prediction on holdout, find threshhold cutoff which gives maximum F1 score on hold out, for each model","45159b5e":"# k fold splitting ","ad3f2074":"# function to add embeddings for lowercase words in the embeddings","70b6b62f":"# Pad the sequences with 0s so that all sentences\/sequences have same length for NN","29d43a11":"df_train\n\n                        qid\t       question_text\t                       target\tlength\n0\t00002165364db923c7e6\tHow did Quebec nationalists see their province...\t       0\t          13\n1\t000032939017120e6e44\tDo you have an adopted dog, how would you enco...\t 0\t            16\n2\t0000412ca6e4628ce2cf\tWhy does velocity affect time? Does velocity a...\t         0\t            10\n3\t000042bf85aa498cd78e\tHow did Otto von Guericke used the Magdeburg h...\t  0             \t9\n4\t0000455dfa3e01eae3af\tCan I convert montra helicon D to a mountain b...\t       0\t            15\n5\t00004f9a462a357c33be\tIs Gaza slowly becoming Auschwitz, Dachau or T...\t   0\t            10\n6\t00005059a06ee19e11ad\tWhy does Quora automatically ban conservative ...\t  0\t                18\n7\t0000559f875832745e2e\tIs it crazy if I wash or wipe my groceries off...\t            0\t              14\n8\t00005bd3426b2d0c8305\tIs there such a thing as dressing moderately, ...\t        0\t               18\n9\t00006e6928c5df60eacb\tIs it just me or have you ever been in this ph...\t           0\t             44\n10\t000075f67dd595c3deb5\tWhat can you say about feminism?\t                         0\t                 6","3e9ea90d":"# function to plot accuracy and loss","a7973eb1":"# take average of predictions from all models","7ce0fdea":"# False Positive wordcloud","9cd81cf4":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_valid = pad_sequences(x_valid, padding='post', maxlen=maxlen)\nx_test2 = pad_sequences(x_test2, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)","73e86fb9":"choose from glove or paragram or wiki_news\nembed_pretrained = load_embed(glove)\nprint (len(embed_pretrained))","72d3fcc2":"# Check Coverage in Glove embeddings before and after adding lower words","ab2ca21d":"# function to extract embedding vectors for each word","0f2ee412":"# Getting glove embeddings","39e3c63e":"# Model summary","3837d10b":"# Functions for vocabulary building and checking coverage in different word embeddings","4d24c52a":"# Median length of the sentence in both the classes","015d4e37":"paragram_embedding_matrix=index_to_matrix(paragram_embeddings,tokenizer.word_index)\nembedding_matrix=paragram_embedding_matrix","cdb01074":"# Data processings on DF train","bffdd788":"# False Negative records","def2763e":"# find threshold cutoff on holdout for final model","27dcba84":"# function to handle numerical characters","f907e8a0":"# Add new feature : length of the sentence","2dd29f32":"print(\"oov : paragram \")\noov = check_coverage(vocab, paragram_embeddings)\n\nadd_lower(paragram_embeddings, vocab)\n\nprint(\"oov : \")\noov = check_coverage(vocab, paragram_embeddings)","5c574ad1":"# Import Libraries","b4d764a0":"# Data processings on DF test","6720a76f":"# class distribution","2247c5f1":"# Tokenize the sentences : convert sentences to sequence of tokens (indices or numbers)","bd12061b":"# check records which are wrongly predicted and fall close to threshold","daad6f4d":"# On final test data","5acafb80":"# Build Vocabulary","090a73be":"wiki_embeddings = load_embed(wiki_news)\nprint (len(wiki_embeddings))","e67bb844":"# Sincere Questions - word cloud"}}