{"cell_type":{"a1285bab":"code","48f10d29":"code","901bf4d5":"code","d318515e":"code","e33df731":"code","62f5521d":"code","891c84db":"code","3500e1d5":"code","71667b25":"code","3537e011":"code","cd4be9f7":"code","42792bd6":"code","e8d67c85":"code","bb6ded78":"code","561c89ef":"code","a3ba4f3f":"code","fb94ceef":"code","d121e8e0":"code","b0ebe3de":"code","f3748975":"code","d9d2bbaa":"code","7188caf4":"code","df6f807d":"code","f3db50e2":"code","ace44c46":"code","7317c54c":"code","583eacab":"code","255c9527":"code","045800f0":"code","93815947":"code","0a09c263":"code","f2d95714":"markdown","72d05b0d":"markdown","a585c850":"markdown","af4afadb":"markdown","3ff5b7e9":"markdown","77b44e71":"markdown","b6e1335a":"markdown","b7f10cf3":"markdown","9ffdbdaa":"markdown","2610c917":"markdown","e1ee1a71":"markdown","dd72e5bf":"markdown","f938ae4b":"markdown","859d2ef4":"markdown","6fdfcc66":"markdown","7e08a3c1":"markdown","50bb68d7":"markdown","eed353b4":"markdown","743a67f1":"markdown","1a5c49b0":"markdown","baed52ce":"markdown","e840c591":"markdown","0b6dede6":"markdown","da9079ad":"markdown","57442791":"markdown","bcb7837f":"markdown","5441169b":"markdown","aea428c3":"markdown","532e6a1b":"markdown","6a3fb49e":"markdown","896eeaed":"markdown"},"source":{"a1285bab":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","48f10d29":"breast = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\n\ndf = breast.copy()","901bf4d5":"df.drop([\"Unnamed: 32\",\"id\"],axis = 1, inplace = True)","d318515e":"X = df.drop(\"diagnosis\", axis = 1)\ny = df[\"diagnosis\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, stratify = y, random_state = 42)","e33df731":"logreg = LogisticRegression().fit(X_train,y_train)\ny_pred = logreg.predict(X_test)","62f5521d":"print(classification_report(y_test, y_pred))","891c84db":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","3500e1d5":"y = df.diagnosis\nx = df.drop(\"diagnosis\", axis = 1)\n\nx.head()","71667b25":"ax = sns.countplot(x = \"diagnosis\", data = df)\nplt.show()\n\nb,m = df.diagnosis.value_counts()\nprint(\"Number of Benign: \", b)\nprint(\"Number of Malignant: \", m)\n","3537e011":"x.describe().T","cd4be9f7":"data_dia = y\ndata = x\n\n#standardization\ndata_n2 = (data-data.mean()) \/ data.std()\n\ndata = pd.concat([y,data_n2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","42792bd6":"# second ten part\ndata = pd.concat([y,data_n2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","e8d67c85":"# last ten part\ndata = pd.concat([y,data_n2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# plotting the violin plot\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","bb6ded78":"sns.jointplot(x ='concavity_worst', y = 'concave points_worst', \n              data = x, kind=\"reg\", color=\"#D81B60\");","561c89ef":"sns.jointplot(x = \"symmetry_worst\", y = \"fractal_dimension_worst\",\n              data = x, kind = \"reg\",color=\"#D81B60\" );","a3ba4f3f":"sns.jointplot(x ='concavity_se', y = 'concave points_se', \n              data = x, kind=\"reg\", color=\"#D81B60\");","fb94ceef":"df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","d121e8e0":"df = x.loc[:,['perimeter_mean','area_mean','area_worst',\"concavity_mean\"]]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","b0ebe3de":"data = pd.concat([y,data_n2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","f3748975":"data = pd.concat([y,data_n2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","d9d2bbaa":"data = pd.concat([y,data_n2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(8,8))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","7188caf4":"corr = x.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True,fmt='.2f',mask=mask, cmap=cmap, ax=ax);","df6f807d":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx1 = x.drop(drop_list1, axis = 1 )       \nx1.head()","f3db50e2":"corr = x1.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 6))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True,fmt='.2f',mask=mask, cmap=cmap, ax=ax);","ace44c46":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n\nx_train, x_test, y_train, y_test = train_test_split(x1, y, test_size=0.3, random_state=42)\n\n#n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac_score = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac_score)\n\ncnf_m = confusion_matrix(y_test,clf_rf.predict(x_test))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cnf_m, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","7317c54c":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 4 features\nselect_feature = SelectKBest(chi2, k=4).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","583eacab":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\n\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\n\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\n\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_2, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","255c9527":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","045800f0":"x_train_3 = select_feature.transform(x_train)\nx_test_3 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_3,y_train)\n\nac_3 = accuracy_score(y_test,clf_rf_2.predict(x_test_3))\nprint('Accuracy is: ',ac_2)\n\ncm_3 = confusion_matrix(y_test,clf_rf_2.predict(x_test_3))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_3, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","93815947":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 6 features\nselect_feature = SelectKBest(chi2, k=6).fit(x_train, y_train)\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","0a09c263":"x_train_4 = select_feature.transform(x_train)\nx_test_4 = select_feature.transform(x_test)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_3 = RandomForestClassifier()      \nclr_rf_3 = clf_rf_3.fit(x_train_4,y_train)\n\nac_4 = accuracy_score(y_test,clf_rf_3.predict(x_test_4))\nprint('Accuracy is: ',ac_2)\n\ncm_4 = confusion_matrix(y_test,clf_rf_3.predict(x_test_4))\n\nplt.figure(figsize=(3,3))\nsns.heatmap(cm_4, annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Base Model', color='navy', fontsize=15)\nplt.show()","f2d95714":"In **texture_mean** feature, \n  - median of the **Malignant** and **Benign** looks like separated, so it can be good for classification.<br>\n  \nHowever, in **fractal_dimension_mean** feature, \n  - median of the **Malignant** and **Benign** does not looks like separated so it does not gives good information for classification.","72d05b0d":"I will drop features that coefficients are high from 0.9","a585c850":"- For example, we can easily see that **area mean** feature well separated from each other in terms of classification\n- Hovewer, **symmetry_mean** looks like malignant and benign are mixed so it is hard to classfy while using this feature.","af4afadb":"## **PairGrid**\n**Instead of looking at them individually, let's look at three or four properties in one graph.**","3ff5b7e9":"- With **k = 4**, it gave a better result (0.96) than other k values, predicting 1's much better.","77b44e71":"### **Swarm Plots**","b6e1335a":"## **Violin Plots**","b7f10cf3":"<a id=\"3\"><\/a> <br>\n# **Visualization**\n\n- Before visualization, we need to do **normalization** or **standardization**. Because differences between values of features are very high to observe on plot.\n- I plot features in 3 group and each group includes 10 features to observe better.","9ffdbdaa":"- in here we can make same comment as before, **area_se** almost well separated from each other\n- but for example **fractal_dimension_se** not like that.","2610c917":"# Intro\n\nHello everyone, in this notebook I will try to show how we can work on a multidimensional data and make it less dimensional with **Feature Selection**. First we will try to get information about the feature we have with **data visualization**, then we will try to establish the optimum model with less features with **feature selection**. I try to learn new things every day and improve myself. I may have mistakes, if you come across, please mention it in the comments. Your feedback is very important to me.","e1ee1a71":"<a id=\"5\"><\/a> <br>\n# **Conclusion**\nIn short, in this notebook, I tried to show **Data Visualization** and **Feature Selection** techniques. While we had 33 features in the beginning, we reduced it to 4 using statistics and some algorithms.\n","dd72e5bf":"According to score list we will choose best 4 as follows: \n  - **texture_mean**,\n  - **area_mean**,\n  - **area_se**,\n  - **concavity_worst**\n\nand we build new model","f938ae4b":"* When we looked at Violin plots, the distribution of some features was very close to each other, we looked closer to the relationship between them to better observe this. And we have seen that there is a linear relationship between them.","859d2ef4":"**k = 6**","6fdfcc66":"**Highly Correlated Features**\n\n-  compactness_mean, concavity_mean and concave points_mean then I choose **concavity_mean**\n-  radius_se, perimeter_se and area_se then I choose **area_se**\n-  radius_worst, perimeter_worst and area_worst then I choose **area_worst**\n-  compactness_worst, concavity_worst and concave points_worst then I choose **concavity_worst** \n-  compactness_se, concavity_se and concave points_se then I choose **concavity_se**\n-  texture_mean and texture_worst are correlated then I choose **texture_mean** \n-  area_worst and area_mean I choose **area_mean**","7e08a3c1":"<a id=\"2\"><\/a> <br>\n# **EDA**","50bb68d7":"#### **k=4**","eed353b4":"## **Joint-Plot**\n### **Concavity Worst - Concave Points Worst**","743a67f1":"The model we built using all the features,base model, gives an **accuracy score of 0.94**. Now let's try to make Feature Elimination and see if we can get better results. But first, let's visualize the data and gain an understanding of the features.","1a5c49b0":"<a id=\"1\"><\/a> <br>\n# **Base Model**","baed52ce":"Now lets create again correlation matrix","e840c591":"## **2.Univariate Feature Selection and Random Forest C.**\n- In univariate feature selection, we will use **SelectKBest** that removes all but the **k highest scoring** features.\n- In this method we need to choose how many features we will use.(k) I will try model one by one with \n  - k = 4\n  - k = 5 \n  - k = 6","0b6dede6":"## **1.Correlation Matrix and Random Forest C.**","da9079ad":"- when we looked up all graphs, some features look very similar, some features look very different. The different ones will help during the classification..\n\n- In order to compare two features deeper, lets use **joint plot** for some similar and some different features.","57442791":"Notebook content is as follows:\n\n - [Base Model](#1)\n - [EDA](#2)\n - [Visualization](#3)\n - [Feature Selections and Random Forest Classification](#4)\n - [Conclusion](#5)\n ","bcb7837f":"I will drop unnecessary columns","5441169b":"**k = 5**","aea428c3":"**It gave a better result than before, predicting 1s much better.**","532e6a1b":"### **Random Forest Model**","6a3fb49e":"<a id=\"4\"><\/a> <br>\n# **Feature Selection and Random Forest Classification**\n\nIn this part we will select feature with different methods that are feature selection with,\n-  **Correlation**, \n-  **Univariate Feature Selection**, \n\nAnd we will use **Random Forest Classification** in order to train our model and predict.","896eeaed":"### **Symmetry Worst - Fractal Dimension Worst**"}}