{"cell_type":{"5e59207c":"code","6fa45736":"code","08696625":"code","b5b18346":"code","ed6301d0":"code","138b3f6b":"code","52076dfc":"code","a4a305bd":"code","d401235b":"code","3a6e5a57":"code","2c31a758":"code","ac2005d7":"code","de338b87":"code","c24cecd6":"code","799740da":"code","f24ea060":"code","deccf357":"code","99140e88":"code","9b10a38f":"code","b1fc164e":"code","01ccd7ba":"code","2277d187":"code","78e5b058":"code","195a4804":"code","219416c8":"code","1dca9fdc":"code","6fd177c4":"code","19c8786d":"code","34050569":"code","46fec865":"code","d7bdfcca":"code","f0777968":"code","32b20f6d":"code","21ecccee":"code","d1fa253e":"code","61e7abd5":"code","3345a83f":"code","610d8fd1":"code","716d60fb":"code","124460e4":"code","9ee5d9e7":"code","fe1fcb0b":"code","601ed419":"markdown","7fba6661":"markdown","2a379eb8":"markdown","fd6ecd61":"markdown","387cf202":"markdown","859d51b5":"markdown","4c569c70":"markdown","4304de07":"markdown","15b22bfa":"markdown","de9ce109":"markdown","be7b43bc":"markdown","3f55db42":"markdown","fa782ed0":"markdown","b38686a8":"markdown","04d04f35":"markdown","3ccf278b":"markdown","01afbf8c":"markdown"},"source":{"5e59207c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","6fa45736":"#Dataset view\npath1=\"..\/input\/home-credit-default-risk\/\"\ndata_files=list(os.listdir(path1))\ndf_files=pd.DataFrame(data_files,columns=['File_Name'])\ndf_files['Size_in_MB']=df_files.File_Name.apply(lambda x:round(os.stat(path1+x).st_size\/(1024*1024),2))\ndf_files","08696625":"desc=pd.read_csv(\"..\/input\/columns-description\/columns_description.csv\")","b5b18346":"#All functions\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['Null','Unique_Count','Data_type','Max\/Min','Mean','Std','Skewness','Sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['Null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['Unique_Count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['Data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'Max\/Min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'Mean']=df_fa[col].mean()\n            df.at[col,'Std']=df_fa[col].std()\n            df.at[col,'Skewness']=df_fa[col].skew()\n        df.at[col,'Sample_values']=list(df_fa[col].unique())\n           \n    return(df.fillna('-'))\n\n\ndef drop_corr_col(df_corr):\n    upper = df_corr.where(np.triu(np.ones(df_corr.shape),\n                          k=1).astype(np.bool))\n    # Find index of feature columns with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n    return(to_drop)\n","ed6301d0":"#Reading training data\ntrain=pd.read_csv(path1+'application_train.csv')\nprint('application_train Feature Summary')\nwith pd.option_context('display.max_rows',train.shape[1]):\n    train_fs=feature_summary(train)   ","138b3f6b":"#Understanding target\/dependent variable\npie_labels=['Defaulters-'+str(train['TARGET'][train.TARGET==1].count()),'Non Defaulters-'+str(train['TARGET'][train.TARGET==0].count())]\npie_share=[train['TARGET'][train.TARGET==1].count()\/train['TARGET'].count(),\n           train['TARGET'][train.TARGET==0].count()\/train['TARGET'].count()]\nfigureObject, axesObject = plt.subplots()\npie_colors=('red','green')\npie_explode=(.3,.0)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=0,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Defaulters and Non Defaulters',color='blue')\nplt.show()","52076dfc":"print('FEATURE SUMMARY: Categorical Features')\ncat_features=train_fs[train_fs.Data_type=='object'].index\nprint('Total categorical features:',len(cat_features))\ncat_fs=train_fs[train_fs.Data_type=='object']\ncat_fs['Desc']=cat_fs.index\nfor ind in cat_fs['Desc'].values:\n    cat_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\ndisplay(cat_fs.iloc[:,7:9])\ndisplay(cat_fs.iloc[:,:3])","a4a305bd":"#Replacing space with underscore in all categorical values\nfor col in cat_features:\n    train[col]=train[col].apply(lambda x: str(x).replace(\" \",\"_\"))\n   ","d401235b":"#converting call categorical features into dummies \ncat_train=pd.DataFrame()\nfor col in cat_features:\n    dummy=pd.get_dummies(train[col],prefix=col)\n    cat_train=pd.concat([cat_train,dummy],axis=1)\ncat_train.head()","3a6e5a57":"del dummy\ngc.collect()\nprint('Newly created dummy columns:',len(cat_train.columns))","2c31a758":"%%time\n#creating correlation matrix with absolute values\ncorr=cat_train.corr().abs()\n#identifying features with high correlation value\nto_drop=drop_corr_col(corr)\ncat_train.drop(to_drop,axis=1,inplace=True)\nprint('Drop following features as they have high correlation other columns:\\n',to_drop,'\\n')\nprint('Categorical Features after dropping correlated features:',cat_train.shape)","ac2005d7":"train_X,test_X,train_y,test_y=train_test_split(cat_train,train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Createing a basic LGBM classifier on categorical data. To check can newly created features be consumed by a model')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","de338b87":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [cat_train.columns[i] for i in indices[:138]]\nfig,ax=plt.subplots(figsize=(20,40))\nplt.title(\"Feature Importance - Categorical Features\",fontsize=35)\nplt.ylabel(\"Categorical Features\",fontsize=35)\nplt.xlabel(\"Feature Importance\",fontsize=35)\ndf_fi_cat=pd.DataFrame(model.feature_importances_[indices[:138]],columns=['Feature_imp'])\ndf_fi_cat['names']=names\ndf_fi_cat.sort_values(by='Feature_imp',inplace=True)\nplt.barh(range(138),df_fi_cat['Feature_imp'],align='edge')\nplt.yticks(range(138),df_fi_cat['names'],color='g',fontsize=15)\nfor i in range(0,138,2):\n    ax.get_yticklabels()[i].set_color(\"red\")\n\nplt.show()","c24cecd6":"chi2_selector=SelectKBest(chi2,k=138)\nfeature_kbest=chi2_selector.fit_transform(cat_train,train['TARGET'])\ndf_chi=pd.DataFrame(chi2_selector.scores_,columns=['chi_score'])\ndf_chi['columns']=cat_train.columns\ndf_chi_s=df_chi.sort_values(by='chi_score')","799740da":"fig,ax=plt.subplots(figsize=(20,40))\nplt.title(\"Chi-squared statistics for categorical features\",fontsize=30)\nplt.ylabel(\"Categorical Features\",fontsize=30)\nplt.xlabel(\"Chi-squared statistic\",fontsize=30)\nplt.barh(range(len(df_chi_s['chi_score'])),df_chi_s['chi_score'],align='edge',color='rgbkymc')\nplt.yticks(range(len(df_chi_s['chi_score'])),df_chi_s['columns'],color='g',fontsize=15)\nfor i in range(0,138,2):\n    ax.get_yticklabels()[i].set_color(\"red\")\nplt.show()","f24ea060":"print('Feature with Chi-square statistic less than 1:',len(df_chi_s[df_chi_s.chi_score<1]['columns']))","deccf357":"cat_train.drop(df_chi_s[df_chi_s.chi_score<1]['columns'],axis=1,inplace=True)","99140e88":"train_X,test_X,train_y,test_y=train_test_split(cat_train,train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Score after dropping features with Chi-squared statistic less than 1')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","9b10a38f":"%%time\nshape_1=cat_train.shape[1]\nroc_auc=np.zeros([len(range(10,shape_1,5)),2],float)\nk=0\ndf_chi_s.sort_values(by='chi_score',ascending=False,inplace=True)\nfor i in range(10,shape_1,5):\n    train_X,test_X,train_y,test_y=train_test_split(cat_train[df_chi_s['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc[k][0]=i\n    roc_auc[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])                                \n    k=k+1\n","b1fc164e":"df_roc=pd.DataFrame(roc_auc,columns=['Features','roc_auc_score'])\ndf_roc.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc.head()","01ccd7ba":"df_roc.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Categorical Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc['Features'],df_roc['roc_auc_score'],color='b',linewidth=2)\nplt.hlines(xmin=0,xmax=np.max(roc_auc[:,0]),y=np.max(roc_auc[:,1]),color='g',linestyle='dashed')\ndf_roc.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc)):\n    plt.plot(df_roc.iloc[i,0],df_roc.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc.iloc[i,0],df_roc.iloc[i,1],(('('+str(np.int(df_roc.iloc[i,0]))+','+str(round(df_roc.iloc[i,1],4))+')')),color='r',fontsize=28,rotation=90)\n        plt.vlines(ymin=0.613,ymax=df_roc.iloc[i,1],x=df_roc.iloc[i,0],color='r',linestyle='dotted')\n    if i==16:\n        plt.text(df_roc.iloc[i,0],df_roc.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=28,rotation=90)\n        plt.vlines(ymin=0.613,ymax=df_roc.iloc[i,1],x=df_roc.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","2277d187":"print('FEATURE SUMMARY: Binary Features')\nbin_features=train_fs[((train_fs.Data_type=='int64') | (train_fs.Data_type=='float64')) & (train_fs.Unique_Count==2)].index\nprint('Total binary features (including TARGET):',len(bin_features))\nbin_fs=train_fs[((train_fs.Data_type=='int64') | (train_fs.Data_type=='float64')) & (train_fs.Unique_Count==2)]\nbin_fs['Desc']=bin_fs.index\nfor ind in bin_fs['Desc'].values:\n    bin_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\ndisplay(bin_fs.iloc[:,7:9])\ndisplay(bin_fs.iloc[:,:7])","78e5b058":"corr_bin=train[bin_features[1:]].corr().abs()\nprint('Number of strongly correlated binary features:',drop_corr_col(corr_bin))","195a4804":"train_X,test_X,train_y,test_y=train_test_split(train[bin_features].drop(['TARGET'],axis=1),train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a basic LGBM classifier on binary features')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","219416c8":"indices = np.argsort(model.feature_importances_,)[::-1]\nnames = [train[bin_features].drop(['TARGET'],axis=1).columns[i] for i in indices]\nplt.figure(figsize=(30,10))\nplt.title(\"Feature Importance - Top Binary Features\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel(\"Binary Features\",fontsize=30)\nplt.ylabel(\"Feature Importance\",fontsize=30)\nplt.bar(range(32), model.feature_importances_[indices])\nplt.xticks(range(32), names,rotation=90)\n\nplt.show()","1dca9fdc":"chi2_selector=SelectKBest(chi2,k=32)\nfeature_kbest=chi2_selector.fit_transform(train[bin_features].drop(['TARGET'],axis=1),train['TARGET'])\ndf_chi=pd.DataFrame(chi2_selector.scores_,columns=['chi_score'])\ndf_chi['columns']=bin_features[1:]\ndf_chi_bins=df_chi.sort_values(by='chi_score',ascending=False)","6fd177c4":"plt.figure(figsize=(40,10))\nplt.title(\"Chi-squared statistics for binary features\",fontsize=30)\nplt.xlabel(\"Binary Features\",fontsize=30)\nplt.ylabel(\"Chi-squared statistics\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(len(df_chi_bins['chi_score'])),df_chi_bins['chi_score'],align='edge',color='rgbkymc')\nplt.xticks(range(len(df_chi_bins['chi_score'])),df_chi_bins['columns'],rotation=90,color='g')\nplt.show()","19c8786d":"%%time\nroc_auc_bin=np.zeros([len(range(5,33,3)),2],float)\nk=0\n\nfor i in range(5,33,3):\n    train_X,test_X,train_y,test_y=train_test_split(train[df_chi_bins['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc_bin[k][0]=i\n    roc_auc_bin[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])\n    k=k+1","34050569":"df_roc_bin=pd.DataFrame(roc_auc_bin,columns=['Features','roc_auc_score'])\ndf_roc_bin.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc_bin.head()","46fec865":"df_roc_bin.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Binary Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc_bin['Features'],df_roc_bin['roc_auc_score'],color='b',linewidth=3)\nplt.hlines(xmin=0,xmax=np.max(roc_auc_bin[:,0]),y=np.max(roc_auc_bin[:,1]),color='g',linestyle='dashed')\ndf_roc_bin.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc_bin)):\n    plt.plot(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],\n                 (('('+str(np.int(df_roc_bin.iloc[i,0]))+','+str(round(df_roc_bin.iloc[i,1],4))+')')),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.563,ymax=df_roc_bin.iloc[i,1],x=df_roc_bin.iloc[i,0],color='r',linestyle='dotted')\n    if i==8:\n        plt.text(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.563,ymax=df_roc_bin.iloc[i,1],x=df_roc_bin.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","d7bdfcca":"print('FEATURE SUMMARY: Continuous Features')\ncon_features=train_fs[((train_fs.Data_type=='float64') | (train_fs.Data_type=='int64')) & (train_fs.Unique_Count!=2)].index\nprint('Total continuous features:',len(con_features))\ncon_fs=train_fs[((train_fs.Data_type=='float64') | (train_fs.Data_type=='int64')) & (train_fs.Unique_Count!=2)]\ncon_fs['Desc']=con_fs.index\nfor ind in con_fs['Desc'].values:\n    con_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\nwith pd.option_context('display.max_rows',train.shape[1]):\n    display(con_fs.iloc[:,7:9])\n    display(con_fs.iloc[:,:7])","f0777968":"train_X,test_X,train_y,test_y=train_test_split(train[con_features].drop(['SK_ID_CURR'],axis=1),train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a basic model on continuous features')\nprint('roc auc score',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","32b20f6d":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [train[con_features].drop(['SK_ID_CURR'],axis=1).columns[i] for i in indices[:]]\nplt.figure(figsize=(35,10))\nplt.title(\"Feature Importance - Continuous Features\",fontsize=30)\nplt.xlabel(\"Continuous Features\",fontsize=30)\nplt.ylabel(\"Feature importance\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(72), model.feature_importances_[indices[:]])\nplt.xticks(range(72), names,rotation=90)\n\nplt.show()","21ecccee":"#Updated null values for continuous features their mean value \nfor col in con_features[1:]:\n    if train_fs.at[col,'Null']!=0:\n        train[col]=train[col].fillna(train_fs.at[col,'Mean'])","d1fa253e":"Fvalue_selector=SelectKBest(f_classif,k=72)\nfeature_kbest=Fvalue_selector.fit_transform(train[con_features[1:]],train['TARGET'])\ndf_Fvalue=pd.DataFrame(Fvalue_selector.scores_,columns=['F-value'])\ndf_Fvalue['columns']=con_features[1:]\ndf_Fvalue_s=df_Fvalue.sort_values(by='F-value',ascending=False)","61e7abd5":"plt.figure(figsize=(40,10))\nplt.title(\"F-value for continuous features\",fontsize=30)\nplt.xlabel(\"Continuous Features\",fontsize=30)\nplt.ylabel(\"F-value statistics\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(len(df_Fvalue_s)),df_Fvalue_s['F-value'],align='edge',color='rgbkymc')\nplt.xticks(range(len(df_Fvalue_s)),df_Fvalue_s['columns'],rotation=90,color='g')\nplt.show()","3345a83f":"%%time\nroc_auc_con=np.zeros([len(range(4,73,4)),2],float)\nk=0\n\nfor i in range(4,73,4):\n    train_X,test_X,train_y,test_y=train_test_split(train[df_Fvalue_s['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc_con[k][0]=i\n    roc_auc_con[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])\n    k=k+1","610d8fd1":"df_roc_con=pd.DataFrame(roc_auc_con,columns=['Features','roc_auc_score'])\ndf_roc_con.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc_con.head()","716d60fb":"df_roc_con.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Continuous Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc_con['Features'],df_roc_con['roc_auc_score'],color='b',linewidth=3)\nplt.hlines(xmin=0,xmax=np.max(roc_auc_con[:,0]),y=np.max(roc_auc_con[:,1]),color='g',linestyle='dashed')\ndf_roc_con.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc_con)):\n    plt.plot(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],\n                 (('('+str(np.int(df_roc_con.iloc[i,0]))+','+str(round(df_roc_con.iloc[i,1],4))+')')),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.73,ymax=df_roc_con.iloc[i,1],x=df_roc_con.iloc[i,0],color='r',linestyle='dotted')\n    if i==15:\n        plt.text(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.73,ymax=df_roc_con.iloc[i,1],x=df_roc_con.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","124460e4":"print('Concatenating all categorical, binary and continuous features ')\nfinal_train=pd.concat([cat_train,train[bin_features],train[con_features]],axis=1)\nprint('Shape of final training data set:',final_train.shape)\n\n\ndel train,cat_train,bin_features,model,con_features\ngc.collect()","9ee5d9e7":"train_X,test_X,train_y,test_y=train_test_split(final_train.drop(['TARGET','SK_ID_CURR'],axis=1),final_train['TARGET'])\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a final LGBM classifier on final training dataset')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","fe1fcb0b":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [final_train.columns[i] for i in indices[:50]]\nplt.figure(figsize=(30,10))\nplt.title(\"Feature Importance - Top 50 Features\",fontsize=30)\nplt.xlabel(\"Features\",fontsize=30)\nplt.ylabel(\"Feature Importance\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(50), model.feature_importances_[indices[:50]])\nplt.xticks(range(50), names,rotation=90)\n\nplt.show()","601ed419":"<h2>Creating dummies from categorical features<\/h2>\n* We are converting all categorical features into dummies. \n* Null values also treated as a unique feature value and a separate dummy column created for Null values. \n* So we are not filling Null values, but handling them using dummies\n* Treating Null values as a category","7fba6661":"<h2>Home Credit Business Model, Counties of Operation and Business Challenges<\/h2>\n<table><tr><ol>\n<td><li><b>Business Model<\/b>\n<img src=\"http:\/\/www.homecredit.net\/~\/media\/Images\/H\/Home-Credit-Group\/content-images\/image-signpost\/business-model.jpg\" alt=\"Business Model\"><\/img><\/td>\n<td><li><b>Home Credit has presence in 11 countries<\/b>\n<img src=\"http:\/\/www.homecredit.net\/~\/media\/Images\/H\/Home-Credit-Group\/content-images\/homepage-map-v1.jpg\" alt=\"Count of Operation\"><\/img><\/td>\n<\/ol><\/tr><\/table>","2a379eb8":"<h2>Understanding Data Model<\/h2>\n<ol>\n<li>We need to join <b>application_{train|test}<\/b> (train or test) with other tables to get all possible features\n<li>Below model explains which fields can be used for creating joins.\n","fd6ecd61":"\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png\" alt=\"Count of Operation\" height=\"800\" width=\"800\"><\/img>\n","387cf202":"<h2>Creating baseline model for Continuous features (work in progress)<\/h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value with Null ( 0.7533393) for continuous features.\n<li>Baseline value without Null values (0.752309)\n<li>We will be using ANOVA F-value to identify best continuous features","859d51b5":"![](http:\/\/)<h2>Problem Statement<\/h2>\nThe objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n\n**Supervised:** The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features<br>\n**Classification:** The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n\n<h2>Notebook Objectives:<\/h2>\n<ol>\n<li><font color=\"green\">Understanding Features under application train table<\/font>\n<li><font color=\"green\">Splitting tables into Categorical, Binary and Continuous features<\/font>\n<li><font color=\"green\">Creating a simple classification model(without filling null values) and find a baseline <b>roc_auc_score<\/b><\/font>\n<li><font color=\"green\">Identifying feature importance by feature type for <b>application_train<\/b> table<\/font>\n<li><font color=\"green\">Removing correlated features<\/font>\n<li><font color=\"red\">Identifying featues strongly correlated with TARGET. This helps in separating features holding linear relationship with TARGET. (work in progress)<\/font>\n<li><font color=\"green\">Feature selection on categorical data\/binary data. Using <b>Chi-squared statistic.<\/b><\/font>\n<li><font color=\"green\">Feature selction continuous data. Using <b>ANOVA F-value<\/b><\/font>\n<li><font color=\"green\">Visualization on important features<\/font>\n<li><font color=\"red\">Joining supporting tables and understanding their feature importance as well (work in progress)<\/font>\n<li><font color=\"red\">Filling null value and again checking accuracy (work in progress)<\/font>\n<li><font color=\"green\">Preparing <b>bureau and bureau_balance<\/b> table for joining with application(train\/test) data. Check following link ([http:\/\/www.kaggle.com\/rahullalu\/home-credit-default-risk-preparing-bureau-data](http:\/\/www.kaggle.com\/rahullalu\/home-credit-default-risk-preparing-bureau-data))<\/font>\n<li><font color=\"red\">Preparing <b>previous_application<\/b> table and associated tables for joining with application(train\/test) data. Check following link (work in progress)<\/font>\n<\/ol>\n\n<b><I>Note: Moving analysis on bureau and previous_application tables to other notebooks<\/I><\/b>","4c569c70":"<h2>Identifying top best continuous features (using ANOVA F-value)<\/h2>\n<ul>\n<li>Executing a loop from top 4 features to 72 features with step 4 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n<\/ul>\n<h3>Conclusion:<\/h3>\n<ul>\n<li>As per below analysis best score is 0.0.752309 for top 72 features\n<li>Baseline value for categorical features  0.7533393 with Null values and 0.0.752309 without Null values.\n<li>So in this case we have to select all 72 features\n<\/ul>","4304de07":"<h2>Feature Selection using [Chi-square](http:\/\/www.learn4master.com\/machine-learning\/chi-square-test-for-feature-selection)<\/h2>\n* Chi-square is one of the ways of feature selection for categorical features.\n* Chi-square statistics examines the independence of two categorical vectors.\n* We will calculate Chi-square score for all the features and try to visualize it.\n* Chi-square score is calculated for features with respect to target.\n\n","15b22bfa":"<h2>Creating baseline model for Binary features<\/h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value(0.595040531) for binary features.\n<li>We will be using Chi-squared statistics to identify best binary features","de9ce109":"<h2>Removing correlated categorical features<\/h2>\n<ul>\n<li>Created correlation matrix to identiy features having correlation higher than 0.95\n<li>And dropping all such features","be7b43bc":"<h2>Checking for strongly correlated binary features<\/h2>\n<ul>\n<li>There are no correlated binary features\n<\/ul>\n","3f55db42":"<h2>Creating baseline model for Categorical features<\/h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value(0.638377) for categorical features.\n<li>After dropping categorical features roc_auc_score slightly improved to (0.638434497)\n<li>After dropping categorical features with Chi-square statistic less than 1 roc_auc_score improved to (0.63889890)\n<li>We will be using Chi-squared statistics to identify best categorical features","fa782ed0":"<h2>Identifying top best categorical features (using Chi-squared statistic)<\/h2>\n<ul>\n<li>Executing a loop from top 10 features to 138 features with step 5 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n<\/ul>\n<h3>Conclusion:<\/h3>\n<ul>\n<li>With correlated features best score was 0.639753 for top 115 features\n<li>After removing correlated feature best score is 0.639378 for top 63 features\n<li>Baseline value for categorical features 0.638377\n<li>So selecting top 63 features will improve overall score\n<\/ul>","b38686a8":"<h2>Understanding Features in \"application_train.csv\" using FEATURE SUMMARY table:<\/h2>\n<ol>\n<li>Feature Summary table is generated using custom code\n<li>Idea is to get a consolidated insight on all features. \n<li> Most of the features are Binary, Float values (from 0 to 1) or categorical \n<li>There are lot of missing values. In following sections will be discussing on how to handle missing values..  \n<li>Using summary table we can easily identify following\n<ol>\n<li>Missing values per column\n<li>Unique values per column\n<li>Data type per column\n<li>Minimum and Maximum value\n<li>Mean, Standard Deviation and Skewness\n<li>Sample values\n<\/ol>\n<\/ol>","04d04f35":"<h2>Dataset Overview<\/h2>\n<ol>\n<li>Number of files\n<li>Type of files\n<li>File size\n<\/ol>","3ccf278b":"<h3>Analyzing Target variable: TARGET<\/h3>\n<ol>\n<li>TARGET variable in binary in nature. So this is a binary classification problem.\n<li>Percentage of Defaulters is <b>8.07%<\/b>. Case of Imbalanced dataset.\n<li>Imbalanced dataset is a scenario where the number of observations belonging to one class is significantly lower than those belonging to the other classes.\n<li>How to handle imbalance dataset:\n<u>https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-classification-problem\/<\/u>","01afbf8c":"<h2>Identifying top best binary features (using Chi-squared statistic)<\/h2>\n<ul>\n<li>Executing a loop from top 5 features to 32 features with step 3 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n<\/ul>\n<h3>Conclusion:<\/h3>\n<ul>\n<li>As per below analysis best score is 0.5950405 for top 32 features\n<li>Baseline value for categorical features 0.5950405\n<li>So in this case we have to select all 32 features\n<\/ul>"}}