{"cell_type":{"983c0835":"code","187226ff":"code","91e26358":"code","27365a72":"code","4e5e887a":"code","21d483ae":"code","0ebdd90f":"code","3c18e389":"code","04800530":"code","f0cbc19e":"code","4fe4e8f1":"markdown","9043160f":"markdown","b0fc2137":"markdown","6865752a":"markdown","775a1c63":"markdown"},"source":{"983c0835":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom urllib.request import urlopen # Read csv file\n\n# nltk.download('punkt')\n# nltk.download('stopwords')","187226ff":"# read cv file (pdf or txt)\ncv = urlopen('https:\/\/raw.githubusercontent.com\/iboraham\/indeed-web-scraping\/master\/cv\/cv.txt').read()","91e26358":"# read d.scientist csv\nds = pd.read_csv('..\/input\/data-science-job-market-in-uk\/data_scientist.csv')\nds.head()","27365a72":"# read m.learning engineers csv\nml = pd.read_csv('..\/input\/data-science-job-market-in-uk\/ml.csv')\nml.head()","4e5e887a":"# trim skills and reqs from desc of jobs\ndef empty_count(str_series):\n    counter = 0\n    for ind in str_series:\n        if ind == []:\n            counter+=1\n    return counter\nds_desc=ds.Description.str.lower()\nml_desc=ml.Description.str.lower()\nds_qualifs=ds_desc.str.findall(r'((?:qualifications|requirements|desirable|skills|required|role)+(?:\\:|\\?|\\n)(?:.|\\s)*)')\nml_qualifs=ml_desc.str.findall(r'((?:qualifications|requirements|desirable|skills|required|role)+(?:\\:|\\?|\\n)(?:.|\\s)*)')\nempty_count(ml_qualifs)\/len(ml_qualifs)*100, empty_count(ds_qualifs)\/len(ds_qualifs)*100","21d483ae":"# concat\nml_desc_ready=ml_desc.str.cat(sep=';;;')\nds_desc_ready=ds_desc.str.cat(sep=';;;')","0ebdd90f":"# Make a text corpus containing all words of documents. Use tokenisation and stop word removal.\nimport string\nds_lowers = ds_desc_ready.lower()\nml_lowers = ml_desc_ready.lower()\ncv_lowers = cv.lower()\ncv_lowers = str(cv_lowers)\n\n# punctuation\ntranslator = str.maketrans('', '', string.punctuation)\nds_clean = ds_lowers.translate(translator)\nml_clean = ml_lowers.translate(translator)\ncv_clean = cv_lowers.translate(translator)\n\n# tokenize\nds_tokens = nltk.word_tokenize(ds_clean)\nml_tokens = nltk.word_tokenize(ml_clean)\ncv_tokens = nltk.word_tokenize(cv_clean)\n\n\n# Use stopwords\nfrom nltk.corpus import stopwords\nds_filtered = [w for w in ds_tokens if not w in stopwords.words('english')]\nml_filtered = [w for w in ml_tokens if not w in stopwords.words('english')]\ncv_filtered = [w for w in cv_tokens if not w in stopwords.words('english')]\nds = ' '.join(ds_filtered)\nml = ' '.join(ml_filtered)\ncv = ' '.join(cv_filtered)\n\n\n#Stemming is optional\n'''\nfrom nltk.stem.porter import *\nfrom collections import Counter\nstemmer = PorterStemmer()\nds_stemmed = []\nfor item in ds_filtered:\n    ds_stemmed.append(stemmer.stem(item))\nml_stemmed = []\nfor item in ml_filtered:\n    ml_stemmed.append(stemmer.stem(item))\nds = ' '.join([el[0] for el in Counter(ds_stemmed).most_common(200)])\nml = ' '.join([el[0] for el in Counter(ml_stemmed).most_common(200)])\n'''","3c18e389":"#Convert the documents into tf-idf vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english')\ndocs = [ds,ml,cv]","04800530":"# Find the cosine-similarity between them or any new document for similarity measure.\ntfidf = vect.fit_transform(docs)\nsimilarity = tfidf*tfidf.T\nsimilarity=similarity.toarray()\n\n# another method\nfrom sklearn.metrics.pairwise import cosine_similarity\ncos_sim = cosine_similarity(tfidf)","f0cbc19e":"docs_name = ['ds','ml','cv']\nresults = pd.DataFrame(data=similarity,index=docs_name,columns=docs_name)\nresults","4fe4e8f1":"As a result, I should improve my resume for these fields.","9043160f":"## Read all files","b0fc2137":"## Calculate similarity between cv and jobs desc's","6865752a":"## Import necessary libraries","775a1c63":"## Get desc of the jobs"}}