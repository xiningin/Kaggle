{"cell_type":{"56c61e4c":"code","9e9f3a2c":"code","65ad9523":"code","77cbec07":"code","dedcb56f":"code","32c771b4":"code","c2900bd7":"code","f8ce0892":"code","e4501dc2":"code","90e03886":"code","8430b703":"code","1c2598fc":"code","b26b9506":"code","8b297c92":"code","bfbdd4fc":"code","d4c1d207":"code","290154f4":"code","cf7fd2ff":"code","e72b6d76":"code","4df72efe":"code","fa898a61":"code","cb43c7e2":"code","489caf29":"code","e9177d6e":"code","7c885df0":"code","8c1dda75":"code","d51bd05b":"code","13fc7f08":"code","afe63320":"code","4e131a1f":"code","4caf88e7":"code","ac380429":"code","d6751490":"code","4bd50b10":"code","a15f852e":"code","71c73376":"code","8a7c248c":"code","dcd7edab":"code","bca743ce":"code","0a12858a":"code","4ab95728":"code","1d79291f":"code","fedf2d11":"code","d59b90ba":"code","72fba17d":"code","598fb2dc":"code","445a1401":"code","043964fa":"code","823cbfe3":"code","39e858fa":"code","75192bd5":"code","b55db4a5":"code","b4f17a77":"markdown","b91fdd84":"markdown","e785a48b":"markdown","bb8e780e":"markdown","4dbfdbd6":"markdown","7f0affc1":"markdown","4d965810":"markdown","48b99965":"markdown","921bcfc5":"markdown","af6cf5df":"markdown","381cb662":"markdown","ab44f5f5":"markdown","c5114a10":"markdown","b1723050":"markdown","39face3f":"markdown","de551d25":"markdown","f2e4c36b":"markdown","1c15ba70":"markdown","bed852f5":"markdown","81d9b818":"markdown","a1957956":"markdown","87293e93":"markdown","292c0913":"markdown"},"source":{"56c61e4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9e9f3a2c":"!pip install wordcloud","65ad9523":"# Start with loading all necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nimport csv\n\n\nimport collections\nprint(os.listdir(\"..\/working\/\"))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","77cbec07":"# Load in the dataframe\ndf = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")","dedcb56f":"print(\"Number of rows in data =\",df.shape[0])\nprint(\"Number of columns in data =\",df.shape[1])\nprint(\"\\n\")\nprint(\"**Sample data:**\")\ndf.head()\n","32c771b4":"print(\"There are {} observations and {} features in this dataset. \\n\".format(df.shape[0],df.shape[1]))\n\nprint(\"There are {} words in this dataset such as {}... \\n\".format(len(df.comment_text.unique()),\n                                                                           \", \".join(df.comment_text.unique()[0:1])))\n","c2900bd7":"df[[\"comment_text\"]].head()","f8ce0892":"categories = list(df.columns.values)\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\nax= sns.barplot(categories[2:], df.iloc[:,2:].sum().values)\nplt.title(\"Comments in each category\", fontsize=24)\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Comment Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = df.iloc[:,2:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=18)\nplt.show()","e4501dc2":"rowSums = df.iloc[:,2:].sum(axis=1)\nmultiLabel_counts = rowSums.value_counts()\nmultiLabel_counts = multiLabel_counts.iloc[1:]\nsns.set(font_scale = 2)\nplt.figure(figsize=(15,8))\nax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\nplt.title(\"Comments having multiple labels \")\nplt.ylabel('Number of comments', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\nplt.show()","90e03886":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef clean(s): return re_tok.sub(r' \\1 ', s)","8430b703":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport sys\nimport warnings\ndata = df\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent\ndata['comment_text'] = data['comment_text'].str.lower()\ndata['comment_text'] = data['comment_text'].apply(cleanHtml)\ndata['comment_text'] = data['comment_text'].apply(cleanPunc)\ndata['comment_text'] = data['comment_text'].apply(keepAlpha)","1c2598fc":"df = data.copy()","b26b9506":"# Start with one review:\ntext = df.comment_text[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(text)\n# Display the generated image:\nplt.figure(figsize=(30,50))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8b297c92":"def prepare_text(text_col):\n    ## decide vocab size\n    text = text_col\n    words = []\n    for t in text:\n        words.extend(tokenize(t))\n    ##print(words[:100])\n    vocab = list(set(words))\n    ##print(len(words), len(vocab))\n    words_str1 = ' '.join(str(e) for e in words)  \n    \n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                              collocations=False,\n                              width=2500,\n                              height=1800, background_color=\"white\").generate(words_str1)\n    plt.figure(figsize=(30,50))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","bfbdd4fc":"prepare_text(df['comment_text'])","d4c1d207":"toxic_comments = df.loc[df.toxic != 0]['comment_text']\nsevere_toxic_comments = df.loc[df.severe_toxic != 0]['comment_text']\nobscene_comments = df.loc[df.obscene != 0]['comment_text']\nthreat_comments = df.loc[df.threat != 0]['comment_text']\ninsult_comments = df.loc[df.insult != 0]['comment_text']\nidentity_hate_comments = df.loc[df.identity_hate != 0]['comment_text']","290154f4":"prepare_text(toxic_comments)","cf7fd2ff":"prepare_text(severe_toxic_comments)","e72b6d76":"prepare_text(obscene_comments)","4df72efe":"prepare_text(threat_comments)","fa898a61":"prepare_text(insult_comments)","cb43c7e2":"prepare_text(identity_hate_comments)","489caf29":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nfrom tqdm import tqdm","e9177d6e":"candidate_sentences = df['comment_text']\ncandidate_sentences.shape","7c885df0":"doc = nlp(\"The 22-year-old recently won ATP Challenger tournament.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","8c1dda75":"doc = nlp(\"Nagal won the first set.\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","d51bd05b":"doc = nlp(\"the drawdown process is governed by astm standard d823\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","13fc7f08":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]","afe63320":"get_entities(\"the film had 200 patents\")","4e131a1f":"#entity_pairs = []\n\n#for i in tqdm(candidate_sentences):\n#  entity_pairs.append(get_entities(i))","4caf88e7":"## save paires\n#filename = 'entity_pairs.csv'\n#import csv\n#with open(filename, 'w') as f:\n#   writer = csv.writer(f, delimiter=',')\n#   writer.writerows(entity_pairs)  #considering my_list is a list of lists.","ac380429":"## load paires \nl_entity_pairs = []\ne_file = \"..\/input\/saved-relations\/entity_pairs.csv\" ## read preloaded entity_paires\n#e_file = \"hm_data\/toxic_data\/entity_pairs.csv\" ## read session written entity paires\nwith open(e_file, 'r') as csvfile:\n    entity_pairs_file = csv.reader(csvfile, delimiter=',')\n    for row in entity_pairs_file:\n        for re in row:\n            re = re.replace('\"','')\n            re = eval(re)\n            l_entity_pairs.append(re)      \nentity_pairs = l_entity_pairs","d6751490":"entity_pairs[10:20]","4bd50b10":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n  k = len(matches) - 1\n\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","a15f852e":" #relations = [get_relation(i) for i in tqdm(candidate_sentences)]","71c73376":"## save paires\n#filename = 'relations.csv'\n#import csv\n#with open(filename, 'w') as f:\n#   writer = csv.writer(f, delimiter=',')\n#   writer.writerows(relations)  #considering my_list is a list of lists.\n        ","8a7c248c":"## load relations \ne_file = \"..\/input\/saved-relations\/relations.csv\" ## read preloaded entity_paires\n#e_file = \"hm_data\/toxic_data\/relations.csv\" ## read session written entity paires\nl_relations = []\nwith open(e_file, 'r') as csvfile:\n    relations_file = csv.reader(csvfile, delimiter=',')\n    for row in relations_file:\n        for re in row:\n            l_relations.append(re)\n        #l_relations.append(''.join(row))","dcd7edab":"type(l_relations)","bca743ce":"l_relations[1]","0a12858a":"relations = l_relations","4ab95728":"s = pd.Series(relations).value_counts()\ns[:10]","1d79291f":"## get as much as you want from verbs and their number of links\nprint(s[30:50])","fedf2d11":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","d59b90ba":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","72fba17d":"#plt.figure(figsize=(12,12))\n\n#pos = nx.spring_layout(G)\n#nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n#plt.show()","598fb2dc":"## incoming \nG=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"nigger\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","445a1401":"## outgoing\nG=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"kiss\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","043964fa":"## outging \nG=nx.from_pandas_edgelist(kg_df[kg_df['source']==\"wikipedia\"][:20], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","823cbfe3":"## incoming \nG=nx.from_pandas_edgelist(kg_df[kg_df['target']==\"wikipedia\"][:20], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","39e858fa":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"fuck\"][:30], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","75192bd5":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"suck\"][:25], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","b55db4a5":"## type at local computer \n# python -m tensorboard.main --logdir=models\n## at bti_tf1 enviroment and project folder\n### copy http:\/\/localhost:6006\/ to your browser ","b4f17a77":"### word : wikipedia","b91fdd84":"![test](https:\/\/raw.githubusercontent.com\/ahayek84\/bti_tf1\/master\/image\/2019-10-25_22h03_11.gif)","e785a48b":"### word cloud for each category ","bb8e780e":"### Running essential EDA ","4dbfdbd6":"### Build a Knowledge Graph","7f0affc1":"Counting the number of comments having multiple labels.","4d965810":"## Toxic Comment Exploration Notebook \n* Loading the Data \n* Running essential EDA \n* Word Cloud Visualization \n* Knowldage Graph Visualization \n* Tensorboard Visualization \n","48b99965":"* Preceptorn Nueral Network + Word embedding Kaggle Score : 0.90994 [Press Here](https:\/\/www.kaggle.com\/ahayek84\/toxic-comment-classification-challenge)\n* GRU with Pooling + Word embeding Kaggle Score : 0.95 [Press Here](https:\/\/www.kaggle.com\/ahayek84\/fork-of-toxic-comment-classification-gru)\n* BERT pre-trained model as proof of concept  [Press Here](https:\/\/www.kaggle.com\/ahayek84\/toxic-comment-bert-tf1-proof-of-concept)\n* Bidirectional GRU with Pooling + Glove Kaggle Score : 0.98112 [Press Here](https:\/\/www.kaggle.com\/ahayek84\/toxic-comment-gru-glove)\n* Bidirectional LSTM with Pooling + Glove Kaggle Score : 0.98 [Press Here](https:\/\/www.kaggle.com\/ahayek84\/toxic-comment-lstm-glove)\n\n\n\n","921bcfc5":"### word : suck","af6cf5df":"### Tensorboard Visualization ","381cb662":"### Knowledge Graph Visualization ","ab44f5f5":"Now we count the number of comments under each label. (For detailed code, please refer to the GitHub link of this project.)","c5114a10":"#### Utility functions for text cleaning ","b1723050":"#### Entity Pairs Extraction","39face3f":"### word: fuck","de551d25":"#### Word : nigger","f2e4c36b":"## Explore Key verbs (relations) in the Corpus related Entities","1c15ba70":"## Models and Scores ","bed852f5":"### Relation \/ Predicate Extraction","81d9b818":"## Explore Key words in the Corpus related Entities","a1957956":"WordCloud representation of most used words in each category of comments.","87293e93":"### Loading the Data","292c0913":"### Word Cloud visualization "}}