{"cell_type":{"6006f981":"code","b1acc06e":"code","fb01e6fc":"code","6876f599":"code","b5718bc3":"code","ec91ad63":"code","760f3478":"code","4bd7c9b9":"code","cc44d821":"code","fc7b43cd":"code","d70a9c51":"code","d0808906":"code","7715cc83":"code","22aef291":"code","f4925335":"code","be5d0bef":"code","5a89993b":"code","48a5b7e7":"code","4a74197a":"code","f2d69c35":"code","eed0ea15":"code","848e8563":"code","705f56a4":"code","b6f5a17e":"code","3a1881e2":"code","6b63a08f":"code","32c497ca":"code","83131157":"code","a20f30ee":"code","f2c16252":"code","22d162d4":"code","72720cab":"code","47dd59aa":"code","d954de6b":"code","fd0e79ed":"code","ad07e44a":"code","536a6701":"code","4bc639d4":"code","1eecc060":"code","f34f4b2a":"code","13e64926":"markdown","16f7c600":"markdown","fde4a022":"markdown","2f7fac22":"markdown","8caa66cc":"markdown","1ba0b5d0":"markdown","6bb01abb":"markdown","cde15f68":"markdown","69eb9f92":"markdown","7ae3876f":"markdown","be05c79c":"markdown","2b51fbf8":"markdown","159daab3":"markdown","f8a79041":"markdown","36200e33":"markdown","c198b6f6":"markdown","d8c8d801":"markdown","5d875d0c":"markdown","8ac28a41":"markdown","31fb044e":"markdown","31de6fee":"markdown","5dec66fd":"markdown","981a0c73":"markdown","51fa6c7b":"markdown","c240f8e1":"markdown","5b109890":"markdown","f5aab814":"markdown","68b0203d":"markdown","e604b054":"markdown","058044c2":"markdown","30c2408d":"markdown","34346a5b":"markdown","9f7a837d":"markdown","b7f07e56":"markdown","c7aff9f6":"markdown","6f4ed27a":"markdown","12ab7a09":"markdown","c535a5e3":"markdown","d9ec1a88":"markdown","c85596ce":"markdown","1d2dabe8":"markdown","1701aff1":"markdown","661750db":"markdown","be4fe9c6":"markdown","53743130":"markdown"},"source":{"6006f981":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nSentence_1 = 'Life well spent is life good'\nSentence_2 = 'Life is an art and it is good so far'\nSentence_3 = 'Life is good'\n\n    \nprint(jaccard(Sentence_1,Sentence_2))\nprint(jaccard(Sentence_1,Sentence_3))","b1acc06e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb01e6fc":"!pip install '\/kaggle\/input\/simple-transformers-pypi\/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '\/kaggle\/input\/simple-transformers-pypi\/simpletransformers-0.22.1-py3-none-any.whl' -q","6876f599":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nimport json","b5718bc3":"train = pd.read_csv(r'\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsample_submission = pd.read_csv(r'\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","ec91ad63":"print('Train Data Shape:', train.shape)\nprint('Test Data Shape:', test.shape)","760f3478":"train.describe()","4bd7c9b9":"train.isnull().sum()","cc44d821":"test.isnull().sum()","fc7b43cd":"train.dropna(inplace = True)","d70a9c51":"train['sentiment'].value_counts()","d0808906":"plt.figure(figsize = (13,5))\nplt.subplot(121)\nplt.title('Distribution of sentiments in Train Data')\nsns.countplot(train['sentiment'])\nplt.subplot(122)\nsns.countplot(test['sentiment'])\nplt.title('Distribution of sentiments in Test Data')","7715cc83":"train.head(1)","22aef291":"def clean_data(data):\n    # Removing extra spaces in the beginning of text\n    data = data.strip()\n    # Lower the Text\n    data = data.lower()\n    return data","f4925335":"train['text'] = train['text'].apply(lambda x: clean_data(x))\ntest['text'] = test['text'].apply(lambda x: clean_data(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x: clean_data(x))","be5d0bef":"train['num_words_text'] = train['text'].str.split().apply(lambda x: len(x))\ntest['num_words_text'] = test['text'].str.split().apply(lambda x: len(x))\ntrain['num_words_selected_text'] = train['selected_text'].str.split().apply(lambda x: len(x))","5a89993b":"plt.figure(figsize = (15,20))\nplt.suptitle('Comparing train and test data based on text word length', fontsize = 22)\nplt.subplot(311)\nplt.xlabel('Positive Sentiment Text Length', fontsize = 15)\nplt.ylabel('Distribution', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='positive']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(test[test['sentiment']=='positive']['num_words_text'].values, shade = True, color = 'red', label = 'train')\nplt.subplot(312)\nplt.xlabel('Neutral Sentiment Text Length', fontsize = 15)\nplt.ylabel('Distribution', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='neutral']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(test[test['sentiment']=='neutral']['num_words_text'].values, shade = True, color = 'red', label = 'train')\nplt.subplot(313)\nplt.xlabel('Negaitive Sentiment Text Length', fontsize = 15)\nplt.ylabel('Distribution', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='negative']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(test[test['sentiment']=='negative']['num_words_text'].values, shade = True, color = 'red', label = 'train')","48a5b7e7":"plt.figure(figsize = (15,20))\nplt.suptitle('Comparing train text and selected text word length', fontsize = 22)\nplt.subplot(311)\nplt.xlabel('Positive Sentiment Text Length', fontsize = 15)\nplt.ylabel('Positive Sentiment Selected Text Length', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='positive']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(train[train['sentiment']=='positive']['num_words_selected_text'].values, shade = True, color = 'red', label = 'train')\nplt.subplot(312)\nplt.xlabel('Neutral Sentiment Text Length', fontsize = 15)\nplt.ylabel('Neutral Sentiment Selected Text Length', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='neutral']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(train[train['sentiment']=='neutral']['num_words_selected_text'].values, shade = True, color = 'red', label = 'train')\nplt.subplot(313)\nplt.xlabel('Negaitive Sentiment Text Length', fontsize = 15)\nplt.ylabel('Negative Sentiment Selected Text Length', fontsize = 15)\nsns.kdeplot(train[train['sentiment']=='negative']['num_words_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(train[train['sentiment']=='negative']['num_words_selected_text'].values, shade = True, color = 'red', label = 'train')","4a74197a":"def punctuation_count(data):\n    x = len([w for w in data if w in string.punctuation])\n    return x","f2d69c35":"train['punct_count_text'] = train['text'].apply(lambda x: punctuation_count(x))\ntrain['punct_count_selected_text'] = train['selected_text'].apply(lambda x: punctuation_count(x))","eed0ea15":"plt.figure(figsize = (12,6))\nplt.suptitle('Comparing train text and selected text punctuation length', fontsize = 22)\nplt.xlabel('Punctuation Count', fontsize = 15)\nplt.ylabel('Distribution', fontsize = 15)\nsns.kdeplot(train['punct_count_text'].values, shade = True, color = 'blue', label = 'train')\nsns.kdeplot(train['punct_count_selected_text'].values, shade = True, color = 'red', label = 'train')","848e8563":"positive_tweet = train[train['sentiment']=='positive']\nnegative_tweet = train[train['sentiment']=='negative']\nneutral_tweet = train[train['sentiment']=='neutral']","705f56a4":"def get_top_n_words(corpus, ngram_range = (1,1), n = None):\n    vec = CountVectorizer(ngram_range = ngram_range, stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0)\n    word_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)\n    return word_freq[:n]","b6f5a17e":"pos_unigram = get_top_n_words(positive_tweet['text'], (1,1), 20)\nneutral_unigram = get_top_n_words(neutral_tweet['text'], (1,1), 20)\nneg_unigram = get_top_n_words(negative_tweet['text'], (1,1), 20)\n\ndf1 = pd.DataFrame(pos_unigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neutral_unigram, columns = ['word','count'])\ndf3 = pd.DataFrame(neg_unigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(12,17))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in positive tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in neutral tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df3, orient = 'h',ax = ax3)\nax3.set_title('Most repititve words in negative tweets')\nax3.spines[\"right\"].set_visible(False)\nax3.spines[\"top\"].set_visible(False)\nax3.grid(False)","3a1881e2":"pos_bigram = get_top_n_words(positive_tweet['text'], (2,2), 20)\nneutral_bigram = get_top_n_words(neutral_tweet['text'], (2,2), 20)\nneg_bigram = get_top_n_words(negative_tweet['text'], (2,2), 20)\n\ndf1 = pd.DataFrame(pos_bigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neutral_bigram, columns = ['word','count'])\ndf3 = pd.DataFrame(neg_bigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(12,17))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in positive tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in neutral tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df3, orient = 'h',ax = ax3)\nax3.set_title('Most repititve words in negative tweets')\nax3.spines[\"right\"].set_visible(False)\nax3.spines[\"top\"].set_visible(False)\nax3.grid(False)","6b63a08f":"pos_trigram = get_top_n_words(positive_tweet['text'], (3,3), 20)\nneutral_trigram = get_top_n_words(neutral_tweet['text'], (3,3), 20)\nneg_trigram = get_top_n_words(negative_tweet['text'], (3,3), 20)\n\ndf1 = pd.DataFrame(pos_trigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neutral_trigram, columns = ['word','count'])\ndf3 = pd.DataFrame(neg_trigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(12,17))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in positive tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in neutral tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df3, orient = 'h',ax = ax3)\nax3.set_title('Most repititve words in negative tweets')\nax3.spines[\"right\"].set_visible(False)\nax3.spines[\"top\"].set_visible(False)\nax3.grid(False)","32c497ca":"pos_unigram = get_top_n_words(positive_tweet['selected_text'], (1,1), 20)\nneutral_unigram = get_top_n_words(neutral_tweet['selected_text'], (1,1), 20)\nneg_unigram = get_top_n_words(negative_tweet['selected_text'], (1,1), 20)\n\ndf1 = pd.DataFrame(pos_unigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neutral_unigram, columns = ['word','count'])\ndf3 = pd.DataFrame(neg_unigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(12,17))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in positive selected_text')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in neutral selected_text')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df3, orient = 'h',ax = ax3)\nax3.set_title('Most repititve words in negative selected_text')\nax3.spines[\"right\"].set_visible(False)\nax3.spines[\"top\"].set_visible(False)\nax3.grid(False)","83131157":"stopwords = set(STOPWORDS)\ndef word_cloud(data, title=None):\n    cloud = WordCloud(background_color = 'black',\n                     stopwords = stopwords,\n                     max_words = 200,\n                     max_font_size = 40,\n                     scale = 3).generate(str(data))\n    fig = plt.figure(figsize=(15,15))\n    plt.axis('off')\n    if title:\n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.25)\n        plt.imshow(cloud)\n        plt.show()","a20f30ee":"word_cloud(positive_tweet['text'], 'Most Repeated Words in Positive Text Tweets')","f2c16252":"word_cloud(neutral_tweet['text'], 'Most Repeated Words in Neutral Text Tweets')","22d162d4":"word_cloud(negative_tweet['text'], 'Most Repeated Words in Negative Text Tweets')","72720cab":"train_data = [\n    {\n        'context': \"This tweet sentiment extraction challenge is great\",\n        'qas': [\n            {\n                'id': \"00001\",\n                'question': \"positive\",\n                'answers': [\n                    {\n                        'text': \"is great\",\n                        'answer_start': 43\n                    }\n                ]\n            }\n        ]\n    }\n    ]","47dd59aa":"l = []\ndef start_index(text, selected_text):\n    start_index = text.find(selected_text)\n    l.append(start_index)\n    \nfor i in range(len(train)):\n    start_index(train.iloc[i,1], train.iloc[i,2])","d954de6b":"def ques_ans_format_train(train):\n    output = []\n    for i in range(len(train)):\n        context = train.iloc[i,1]\n        qas = []\n        qid = train.iloc[i,0]\n        ques = train.iloc[i,3]\n        ans = []\n        answer = train.iloc[i,2]\n        answer_start = l[i]\n        ans.append({'text': answer, 'answer_start': answer_start})\n        qas.append({'id': qid, 'question': ques, 'is_impossible': False, 'answers': ans})        \n        output.append({'context': context, 'qas': qas})\n    return output\n\ntrain_json_format = ques_ans_format_train(train)\n# Save as a JSON file\nos.makedirs('data', exist_ok=True)\nwith open('data\/train.json', 'w') as f:\n    json.dump(train_json_format, f)\n    f.close()","fd0e79ed":"test_data = ([\n    {\n        'context': \"Some context as a demo\",\n        'qas': [\n            {'id': '0', 'question': 'neutral'}\n        ]\n    }\n])","ad07e44a":"def ques_ans_format_test(test):\n    output = []\n    for i in range(len(test)):\n        context = test.iloc[i,1]\n        qas = []\n        qid = test.iloc[i,0]\n        ques = test.iloc[i,2]\n        qas.append({'id': qid, 'question': ques})\n        \n        output.append({'context': context, 'qas': qas})\n    return output\n\ntest_json_format = ques_ans_format_test(test)\n# Save as a JSON file\nos.makedirs('data', exist_ok=True)\nwith open('data\/test.json', 'w') as f:\n    json.dump(test_json_format, f)\n    f.close()","536a6701":"from simpletransformers.question_answering import QuestionAnsweringModel\n\nuse_cuda = True \nmodel_path = '\/kaggle\/input\/transformers-pretrained-distilbert\/distilbert-base-uncased-distilled-squad\/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               model_path, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 4,\n                                     'max_seq_length': 128,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model(r'data\/train.json')","4bc639d4":"pred = model.predict(test_json_format)","1eecc060":"df = pd.DataFrame.from_dict(pred)","f34f4b2a":"sample_submission[\"selected_text\"] = df[\"answer\"]\n# new_df = sample_submission.merge(test,how=\"inner\",on=\"textID\")\n# new_df[\"selected_text\"] = np.where((new_df[\"sentiment\"] == \"neutral\"),new_df[\"text\"], new_df[\"selected_text\"])\n# submission = new_df[[\"textID\", \"selected_text\"]]\nsample_submission.to_csv(\"submission.csv\", index = False)\nprint(\"File submitted successfully.\")","13e64926":"#### Converting Train data into specified json Format","16f7c600":"### Getting the submission file ready !!!","fde4a022":"#### Converting Test data into specified json Format","2f7fac22":"As in this competiton it is mandatory to toggle the internet to off position.\nSo, we need to add the offline data which I used in my notebook:\n1. Installing Simple Transformer Package:\n    https:\/\/www.kaggle.com\/jonathanbesomi\/simple-transformers-pypi\n2. Transformer Distilbert Pre-trained Model:\n    https:\/\/www.kaggle.com\/jonathanbesomi\/transformers-pretrained-distilbert","8caa66cc":"### 1.3. Reading the Dataset","1ba0b5d0":"### 2.2. Handling Missing Values","6bb01abb":"### 3.2. Comparing number of words of selected_text and text in Train set","cde15f68":"### Bigram Analysis of text column ","69eb9f92":"It is most easiest and beautiful way to analyze most frequent words occuring in our Dataset","7ae3876f":"## 3. Data Visualiazation","be05c79c":"### 3.3. Comparing number of punctuation marks in Train and Test files","2b51fbf8":"## Model Prepration and Implementation","159daab3":"Input Format Required for Test Data is:","f8a79041":"Determine the words in the tweet which decide the polarity of the tweet.","36200e33":"### Extracting the selected_text from test file","c198b6f6":"This is the format of data neended as input for this dataset for train Data is:","d8c8d801":"### 3.4. Ngram Analysis","5d875d0c":"Dropping is best option here as there is only on missing value","8ac28a41":"### 4.1. Input Data Prepration for the Model","31fb044e":"### 1.1. Understanding Evalution Metric ","31de6fee":"Kaggle always comes with amazing unseen competitons which just blows out your mind. This is my featured competition. At first, after seeing the problem statement, I thought that featured competitons are not for begineers like me because it was not some basic tweet classification problem which we learn from online courses. But then I thought why not to learn some new concepts from other awesome kaggle master's kernels and do some research to get going with the learning process.\nSo, in this kernal I will be covering following things:\n1. Preliminary step\n    1.1. Understanding Evalution Metric\n    1.2. Importing Required Libraries\n    1.3. Reading the Dataset\n    \n2. Exploratory Data Analysis\n    2.1. Descriptive Data Analysis\n    2.2. Handling Missing Values\n    2.3. Distribution of Sentiments\n    2.4. Cleaning the Data\n\n3. Data Visualiazation\n    3.1. Comparing number of words in text of Train and Test files\n    3.2. Comparing number of words of selected_text and text in Train set\n    3.3. Comparing number of punctuation marks in Train and Test files\n    3.4. Ngram Analysis\n    3.5. Word Cloud\n    \n4. Model Implementation\n    4.1. Prepare Data for the Model\n    4.2  Initializing Model\n    4.3. Implementing\n    4.4. Predicting ","5dec66fd":"So, we need to extract the index from the text from where the selected_text starts as this is required field of the input format.\nSo, here is the fxn defined for this task and the index of all texts are saved in a list 'l'  ","981a0c73":"### 2.1. Descriptive Data Analysis","51fa6c7b":"Installing Simple Transformer","c240f8e1":"### Initializing and Training the Model","5b109890":"Ngram Analysis is done using CountVectorizer of Sklearn Library and its documentation can be seen and understood here:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html","f5aab814":"CUDA is a parallel computing platform and application programming interface model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit for general purpose processing. \nFor enabling the GPU we used **use_cuda = True** here.....","68b0203d":"### 2.3. Distribution of Sentiments","e604b054":"## 1. Preliminary Steps","058044c2":"### Unigram Analysis of text column ","30c2408d":"### Unigram Analysis of selected_text column ","34346a5b":"## Objective","9f7a837d":"## 2. Exploratory Data Analysis ","b7f07e56":"![maxresdefault.jpg](attachment:maxresdefault.jpg)\nA simple example using set notation: How similar are these two sets?\n\n\n\nA = {0,1,2,5,6}\nB = {0,2,3,4,5,7,9}\nSolution: J(A,B) = |A\u2229B| \/ |A\u222aB| = |{0,2,5}| \/ |{0,1,2,3,4,5,6,7,9}| = 3\/9 = 0.33","c7aff9f6":"### 1.2. Importing Required Libraries","6f4ed27a":"See the word i'd both in text and selected_text column","12ab7a09":"### 3.1. Comparing number of words in text of Train and Test files","c535a5e3":"It is another way to visualize most frequent words in dataset","d9ec1a88":"### 3.5. Word Cloud Analysis","c85596ce":"### Cleaning the Data","1d2dabe8":"### Trigram Analysis of text column ","1701aff1":"### DO UPVOTE if you like this kernel ...\n\nAs it makes me feel motivated and I will be regularly updating this kernal whenever i get to learn more about the stuff related to this.","661750db":"There are numerous ways to deal with this kind of Dataset that I discovered from other kernels and that I implemented in my kernal:\n1. **Using the Simple Transformer Attention Model's Question Answering Model which uses SQuAD data for this**:\n    and this kernal helped a lot in this: https:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack\/output\n    and it's documentation can be read and understood from here:\n    https:\/\/github.com\/ThilinaRajapakse\/simpletransformers#question-answering\n   \n2. **Another way is using the pre-trained models of BERT, for instance you can look at these kernels:**\n    https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\n    https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch\n    \nA problem can be solved with different a number of ways - Just find the one you can understand first and then try to learn other on as well as learning is an iterative process.","be4fe9c6":"Usually while cleaning data we remove punctuation marks from the text data but not in thus case as selected text contains punctuation marks as well. It's visualization can be seen a few lines below. Another demonstartion for this is:","53743130":"As you may have read in above mentioned github link that for question answering tasks, the input data can be in JSON files or in a Python list of dicts in the correct format and a specicfic format was given which is mandatory as a model takes a particular format as input.\n#### Format attributes: \nEach such dictionary contains two attributes, the \"context\" and \"qas\".\n\n**context**: The paragraph or text from which the question is asked.\n**qas**: A list of questions and answers.\nQuestions and answers are represented as dictionaries. Each dictionary in qas has the following format.\n\n**id**: (string) A unique ID for the question. Should be unique across the entire dataset.\n**question**: (string) A question.\n**is_impossible**: (bool) Indicates whether the question can be answered correctly from the context.\n**answers**: (list) The list of correct answers to the question.\nA single answer is represented by a dictionary with the following attributes.\n\n**answer**: (string) The answer to the question. Must be a substring of the context.\n**answer_start**: (int) Starting index of the answer in the context."}}