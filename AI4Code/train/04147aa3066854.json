{"cell_type":{"1bc951b9":"code","cd2188ef":"code","5ae8c1d4":"code","95adecbb":"code","6ce84952":"code","f920fcd9":"code","346f08e2":"code","d3c26e9f":"code","5a66463f":"code","de2858cf":"code","072f7e4c":"code","73f470f7":"code","523b6053":"code","90b289aa":"code","40f40927":"code","73686804":"code","a1d400e4":"code","ca58f141":"code","8f6e4472":"code","066d4e82":"code","2b5bbed9":"code","5e02ce13":"code","1b3c5d7a":"code","b063e8b0":"code","9c3cec1a":"code","c10388e9":"code","c7bc7a4f":"code","a97aabbd":"code","237b1311":"code","0bbb6877":"code","301d762b":"code","5a91bcb2":"code","a502e81c":"code","39321c59":"code","20f8bd77":"code","b31ac447":"code","e62daeb1":"code","7b021888":"code","8351ac04":"markdown","8eab3b92":"markdown","c4679067":"markdown","8c0dcb46":"markdown","fe4e8a50":"markdown","81c50280":"markdown","a7ff02da":"markdown","b37c9f1e":"markdown","571e5ad8":"markdown","abcb98aa":"markdown","bde8d42c":"markdown","9e98f481":"markdown","c4ea8304":"markdown","f77c7783":"markdown","519e7dfd":"markdown","8f545027":"markdown","c6db3d82":"markdown","133ddceb":"markdown","60a3260e":"markdown","3b4d5bba":"markdown","f9542461":"markdown","54d05466":"markdown","ab359f24":"markdown","1c5aa562":"markdown","cbfe04e3":"markdown","6ce40291":"markdown","166017e7":"markdown","c6ecc503":"markdown","471fe2db":"markdown","5df53a63":"markdown","25914682":"markdown","4e387258":"markdown","27a8f0bc":"markdown","4dc7c886":"markdown","568a2b73":"markdown","ea42f8e5":"markdown","f6098d28":"markdown","c888a585":"markdown","47523b8b":"markdown","6b35371f":"markdown"},"source":{"1bc951b9":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd2188ef":"# Algorithm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Calculate error of models\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\n\n# Train_test_split\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # figures plotting\n%matplotlib inline","5ae8c1d4":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","95adecbb":"df_train.sample(5)","6ce84952":"df_test.describe()","f920fcd9":"df_train.info()","346f08e2":"percent_null_value_train = round(df_train.isnull().sum() \/ len(df_train), 4) * 100\npercent_null_value_test = round(df_test.isnull().sum() \/ len(df_test), 4) * 100\nprint('Percentage of null value in training set:\\n', percent_null_value_train, '\\n')\nprint('-' * 10)\nprint('\\nPercentage of null value in test set:\\n', percent_null_value_test)","d3c26e9f":"df_train_new = df_train.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ndf_test_new = df_test.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)","5a66463f":"# Combine 'Age' column from 2 dataset\nmy_age = df_train_new['Age'].append(df_test_new['Age'])\n\n# Fill NaN with mean value\nmy_new_age = my_age.fillna(np.round(my_age.mean())).tolist()\n\n# Assign 'my_new_age' to df_...['Age']\nm_train = len(df_train_new)\nm_test = len(df_test_new)\n\ndf_train_new['Age'] = my_new_age[:m_train]\ndf_test_new['Age'] = my_new_age[m_train:(m_test + m_train)]","de2858cf":"# 2.2.2.1. Embarked\n# Combine 'Embarked' column from 2 dataset\nmy_embarked = df_train_new['Embarked'].append(df_test_new['Embarked'])\n\n# Find most appearance port of embarkation in both datasets\nfreq_port = my_embarked.mode()[0]\n\n# Fill with freq_port\ndf_train_new['Embarked'] = df_train_new['Embarked'].fillna(freq_port)\n\n# ---------------------- #\n\n# 2.2.2.2. Fare\n# Combine 'Fare' column from 2 dataset\nmy_fare = df_train_new['Fare'].append(df_test_new['Fare'])\n\n# Find most appearance port of embarkation in both datasets\nmedian_fare = my_fare.median()\n\n# Fill with freq_port\ndf_test_new['Fare'] = df_test_new['Fare'].fillna(median_fare)","072f7e4c":"percent_null_train_new = round(df_train_new.isnull().sum() \/ len(df_train_new), 4) * 100\npercent_null_test_new = round(df_test_new.isnull().sum() \/ len(df_test_new), 4) * 100\nprint('Percentage of null value in training set:\\n', percent_null_train_new)\nprint('\\n', '-' * 10)\nprint('\\nPercentage of null value in test set:\\n', percent_null_test_new)","73f470f7":"# List of used columns\ncols = ['Pclass', 'Survived']\n\n# Group survived rate by pclass\npclass_cor = df_train_new[cols].groupby('Pclass')\n\n# Calculate percentage \npclass_survived_rate = (pclass_cor.sum() \/ pclass_cor.count() * 100).reset_index()\npclass_survived_rate","523b6053":"# List of used columns\ncols = ['Sex', 'Survived']\n\n# Group survived rate by pclass\nsex_cor = df_train_new[cols].groupby('Sex')\n\n# Calculate percentage \nsex_survived_rate = (sex_cor.sum() \/ sex_cor.count() * 100).reset_index()\nsex_survived_rate","90b289aa":"# Make a copy of train df\ndf_train_new_copy = df_train_new.copy()\n\n# Add new column, we divived 'Age' into 5 smaller group\ndf_train_new_copy['AgeGroup'] = pd.cut(df_train_new_copy['Age'].astype(int), 5)\n\n# List of used columns\ncols = ['AgeGroup', 'Survived']\n\n# Group survived rate by age\nage_cor = df_train_new_copy[cols].groupby('AgeGroup')\n\n# Calculate percentage \nage_survived_rate = (age_cor.sum() \/ age_cor.count() * 100).reset_index()\nage_survived_rate","40f40927":"# Make a copy of train df\ndf_train_new_copy = df_train_new.copy() \n\n# Add new column, we divived 'Fare' into 4 smaller group\ndf_train_new_copy['FareGroup'] = pd.qcut(df_train_new['Fare'], 4)\n\n# List of used columns\ncols = ['FareGroup', 'Survived']\n\n# Group survived rate by fare group\nfare_cor = df_train_new_copy[cols].groupby('FareGroup')\n\n# Calculate percentage \nfare_survived_rate = (fare_cor.sum() \/ fare_cor.count() * 100).reset_index()\nfare_survived_rate","73686804":"# List of used columns\ncols = ['Embarked', 'Survived']\n\n# Group survived rate by pclass\nembarked_cor = df_train_new[cols].groupby('Embarked')\n\n# Calculate percentage \nembarked_survived_rate = (embarked_cor.sum() \/ embarked_cor.count() * 100).reset_index()\nembarked_survived_rate","a1d400e4":"# Make a copy of train df\ndf_train_new_copy = df_train_new.copy() \n\n# Split title from name\ndf_train_new_copy['Title'] = df_train_new_copy['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Create 'Quantity' column for summing later\ndf_train_new_copy['Quantity'] = 1\n\n# List of used columns\ncols = ['Title', 'Quantity', 'Survived']\n\n# Count quantity of each title\ntitle = df_train_new_copy[cols].groupby('Title').sum().reset_index()\n\n# Create other = sum of title that has quantity < 10\nother = {'Title': 'Other',\n         'Quantity': title[title['Quantity'] < 10].sum()['Quantity'],\n         'Survived': title[title['Quantity'] < 10].sum()['Survived']}\n\n# Drop row that has quantity < 10\ntitle.drop(title[title['Quantity'] < 10].index, inplace=True)\n\n# Add other to df\ntitle = title.append(other, ignore_index=True)\n\n# Add survival rate\ntitle['Survived'] = np.round(title['Survived'] \/ title['Quantity'] * 100, 2)\ntitle = title.rename(columns={'Survived': 'Survival Rate'})\n\ntitle","ca58f141":"# Make a copy of train df\ndf_train_new_copy = df_train_new.copy() \n\n# Split title from name\ndf_train_new_copy['FamilySize'] = df_train_new_copy['SibSp'] + df_train_new_copy['Parch'] + 1\ndf_train_new_copy['IsAlone'] = np.where(df_train_new_copy['FamilySize'] == 1, 1, 0)\n\ndf_train_new_copy","8f6e4472":"# List of used columns\ncols = ['IsAlone', 'Survived']\n\n# Group survived rate by pclass\nsize_cor = df_train_new_copy[cols].groupby('IsAlone')\n\n# Calculate percentage \nsize_survived_rate = (size_cor.sum() \/ size_cor.count() * 100).sort_values(by='Survived', ascending=False).reset_index()\nsize_survived_rate","066d4e82":"# Add 'Age Group'\ndf_train_new['AgeGroup'] = pd.cut(df_train_new['Age'].astype(int), 5).astype(str)\nunique_age_group = df_train_new['AgeGroup'].sort_values().unique().astype(str).tolist()\nunique_age_group","2b5bbed9":"age_conditions = [(df_test_new['Age'] <= 16),\n                  (df_test_new['Age'] <= 32),\n                  (df_test_new['Age'] <= 48),\n                  (df_test_new['Age'] <= 64),\n                  (df_test_new['Age'] > 64)]\n\ndf_test_new['AgeGroup'] = np.select(age_conditions, unique_age_group).astype(str)\ndf_test_new","5e02ce13":"# Add 'Fare Group'\ndf_train_new['FareGroup'] = pd.qcut(df_train_new['Fare'], 4).astype(str)\nunique_fare_group = df_train_new['FareGroup'].sort_values().unique().astype(str).tolist()\nunique_fare_group","1b3c5d7a":"fare_conditions = [(df_test_new['Fare'] <= 7.91),\n                   (df_test_new['Fare'] <= 14.454),\n                   (df_test_new['Fare'] <= 31.0),\n                   (df_test_new['Fare'] <= 512.329)]\n\ndf_test_new['FareGroup'] = np.select(fare_conditions, unique_fare_group).astype(str)\ndf_test_new","b063e8b0":"# Create Title column\ndf_train_new['Title'] = df_train_new['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_test_new['Title'] = df_test_new['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Make list of replace-needed title\nvalid_title = title['Title'].tolist()\nall_train_title = df_train_new['Title'].unique().tolist()\nall_test_title = df_test_new['Title'].unique().tolist()\n\nreplaced_train_title = [tit for tit in all_train_title \n                        if tit not in valid_title]\nreplace_test_title = [tit for tit in all_test_title \n                      if tit not in valid_title]\n\n# Replace \ndf_train_new['Title'].replace(replaced_train_title, 'Other', inplace=True)\ndf_test_new['Title'].replace(replace_test_title, 'Other', inplace=True)\n\ndf_test_new","9c3cec1a":"df_train_new['IsAlone'] = np.where(df_train_new['SibSp'] + df_train_new['Parch'] == 0, 1, 0)\ndf_test_new['IsAlone'] = np.where(df_test_new['SibSp'] + df_test_new['Parch'] == 0, 1, 0)\n\ndf_test_new","c10388e9":"# Columns to drop\ndropped_cols = ['Name', 'Age', 'SibSp', 'Parch', 'Fare']\n\n# Drop columns\ndf_train_new.drop(dropped_cols, axis=1, inplace=True)\ndf_test_new.drop(dropped_cols, axis=1, inplace=True)","c7bc7a4f":"df_train_new.info()","a97aabbd":"train = df_train_new.copy()\ntest = df_test_new.copy()\n\n# List of features need to label encoded\ncols_to_encoded = ['Sex', 'AgeGroup', 'FareGroup']\n\n# Map value\nsex_map = {'male': 1, 'female': 0}\nage_group_map = {'(-0.08, 16.0]': 0, '(16.0, 32.0]': 1, '(32.0, 48.0]': 2, \n                 '(48.0, 64.0]': 3, '(64.0, 80.0]': 4}\nfare_group_map = {'(-0.001, 7.91]': 0, '0': 0, '(7.91, 14.454]': 1, \n                  '(14.454, 31.0]': 2, '(31.0, 512.329]': 3}\n\n# Map list\nmap_list = [sex_map, age_group_map, fare_group_map]\n\nfor idx, col in enumerate(cols_to_encoded):\n    df_train_new[col] = df_train_new[col].map(map_list[idx])\n    df_test_new[col] = df_test_new[col].map(map_list[idx])","237b1311":"df_train_new.sample(5)","0bbb6877":"df_train_new = pd.get_dummies(df_train_new)\ndf_test_new = pd.get_dummies(df_test_new)","301d762b":"df_train_new.drop(columns=['Embarked_S', 'Title_Other'], inplace=True)\ndf_test_new.drop(columns=['Embarked_S', 'Title_Other'], inplace=True)","5a91bcb2":"df_train_new","a502e81c":"y = df_train_new['Survived']\nX = df_train_new.drop(['Survived'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint('Train size: {}'.format(X_train.shape))\nprint('Validation size: {}'.format(X_valid.shape))","39321c59":"def run_and_validate_model(model, X_train, y_train, X_valid, y_valid):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    \n    score = f1_score(y_valid, preds)\n    return score","20f8bd77":"# List of models used\nmodels_used = [SVC(), KNeighborsClassifier(n_neighbors=3), LogisticRegression(), \n               RandomForestClassifier(n_estimators=100),\n               GaussianNB(), Perceptron(), SGDClassifier(), LinearSVC(), \n               DecisionTreeClassifier()]\n\nname_models_used = ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                    'Random Forest', 'Naive Bayes', 'Perceptron', \n                    'Stochastic Gradient Decent', 'Linear SVC', \n                    'Decision Tree']\n\nmodels_score = []\n\n# Run and validate models in a for loop\nfor model in models_used:\n    score = run_and_validate_model(model, X_train, y_train, X_valid, y_valid)\n    models_score.append(score)","b31ac447":"model = pd.DataFrame({'Model': name_models_used, 'Score': models_score})\nmodel = model.sort_values(by='Score', ascending=False).reset_index(drop=True)\nmodel","e62daeb1":"X_test = df_test_new\n\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X, y)\npreds = model.predict(X_test)","7b021888":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": preds\n    })\nsubmission.to_csv('submission.csv', index=False)","8351ac04":"### 3.1.6. `Name`  \nLet see if title from name (e.g., Mrs, Mr) made any impact on chances of survival (since the name itself cannot make any contribution)","8eab3b92":"**PATTERN**: `Miss` and `Mrs` (women) have higher survival rate than other\n\n**WORK TO DO**: \n- Add `Title` column\n- Get dummies `Title` column\n- Drop `Name` column as we utitlize it good enough","c4679067":"# 1. Overview data","8c0dcb46":"## 3.2. Get tasks done","fe4e8a50":"### 3.2.3. Label encoding","81c50280":"**PATTERN:** Women tend to have more survival rate than men \n\n**WORK TO DO:** Get dummies `Sex` column.","a7ff02da":"Set up parameters","b37c9f1e":"See if `IsAlone` has any impacts on survival rate","571e5ad8":"**PATTERN:** The higher entry fee, the higher survival rate\n\n**WORK TO DO:** \n- Add `FareGroup` into both `df_train_new` and `df_test_new` \n- Label encoding `FareGroup` column.\n- Drop `Fare` column as we utilize it good enough until now","abcb98aa":"## 2.2. Fill NaN","bde8d42c":"### 3.2.2. Drop columns","9e98f481":"**AgeGroup**","c4ea8304":"### 3.2.4. Get dummies","f77c7783":"### 3.1.4. `Fare`","519e7dfd":"We use the same approach as the one we use to `Age` attribute","8f545027":"# 4. Modeling, Validating and Implementing","c6db3d82":"Since there are too many null value of `Cabin`, I decide to drop `Cabin` column (even though it can be correlated to `Survived`)\n\n`PassengerID` and `Ticket` are also dropped as they are uncorrelated to `Survived`","133ddceb":"**PATTERN:** Young people tend to have higher survival rate\n\n**WORK TO DO:** \n- Add `AgeGroup` into both `df_train_new` and `df_test_new` \n- Label encoding `AgeGroup` column.\n- Drop `Age` column as we utilize it good enough until now","60a3260e":"### 3.1.7. `SibSp` and `Parch`\nFrom these features, I can create a new feature `IsAlone`","3b4d5bba":"# 3. Analyze correlation\n- `Pclass` to `Survived`\n- `Sex` to `Survived`\n- `Age` to `Survived`\n- `Fare` to `Survived`\n- `Embarked` to `Survived`\n- `Name` to `Survived`\n- `Sibsp` and `Parch` to `Survived`  \n\nAt this step I will test if my hypotheses are correct or not. Then I will decide what to do at the end of each hypothesis","f9542461":"### 3.1.1. `Pclass`","54d05466":"**Title**","ab359f24":"**WORK TO DO:** \n- Add `IsAlone` column\n- Drop `SibSp`, `Parch` columns as we have utilized those good enough","1c5aa562":"From those info, we can see that there is no abnormal data type","cbfe04e3":"**Read csv file**","6ce40291":"**PATTERN:** People embarking from Cherbourg had higher survival rate  \n\n**WORK TO DO:** Get dummies `Embarked` column.","166017e7":"**PATTERN:** People from upper class tend to have more survival rate than other ","c6ecc503":"### 2.2.2. `Embarked` and `Fare`","471fe2db":"### 3.2.1. Add new column","5df53a63":"As `Age` column has multiple values, we can cut it into same size pieces","25914682":"### 3.1.3. `Age`","4e387258":"**IsAlone**","27a8f0bc":"### 3.1.2. `Sex`","4dc7c886":"### 3.1.5. `Embarked`","568a2b73":"## 2.1. Drop column","ea42f8e5":"Drop some columns (I don't want to use drop_first in get_dummies because I want to drop last)","f6098d28":"# 2. Clean data","c888a585":"**FareGroup**","47523b8b":"Final check if I miss some null values","6b35371f":"### 2.2.1. `Age`"}}