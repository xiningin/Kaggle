{"cell_type":{"30bea7d0":"code","16e3019d":"code","39fb0989":"code","abc3994e":"code","11c4a300":"code","fde522bb":"code","04a6d39b":"code","543ca7a9":"code","c1601497":"code","c4c9e09d":"code","c7543d06":"code","34b41253":"code","3cdccfad":"code","62340421":"code","a273e672":"code","6eaaa87f":"code","0133ffff":"code","5785e438":"code","ab27ea40":"code","a1e6f981":"code","51161ded":"code","9fb5f89a":"code","73f52e60":"code","8a1d7549":"code","45a9b323":"code","a8aeabe0":"code","6f2d7715":"code","939b49b2":"code","723f8bf1":"code","d1964d9d":"code","076262a7":"code","f515e6df":"markdown","489ab4ef":"markdown","30309c74":"markdown","c3c3b6e5":"markdown","ddf80cd7":"markdown","f2c6f2d3":"markdown","8ed21384":"markdown","03c9a709":"markdown","ad4ae128":"markdown","36dde65a":"markdown","03555b26":"markdown","125f868f":"markdown","b8370078":"markdown","a8bcf5d9":"markdown","39744cb4":"markdown","3cdcc78d":"markdown","a1561336":"markdown","33a794a2":"markdown","663dd4e6":"markdown","2f016c3c":"markdown","63fe5c65":"markdown","e16dd59c":"markdown","81874d35":"markdown","72200021":"markdown","f2e5d41f":"markdown","52226c3a":"markdown","5b7e4550":"markdown","31e0189d":"markdown","d88cd0b9":"markdown","14154a98":"markdown","ab9ef9aa":"markdown","756f6faa":"markdown","9074649e":"markdown","625b9a5d":"markdown","4082254c":"markdown","a2a015ba":"markdown","39e1ccc4":"markdown","8bda584d":"markdown"},"source":{"30bea7d0":"%pip install -q stylecloud","16e3019d":"import pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport sklearn.metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom transformers import AutoTokenizer, TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\nsns.set_style(\"ticks\")\nsns.set_context(\"notebook\")\nfrom IPython.display import Image\nimport stylecloud\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom tqdm.notebook import tqdm\ntqdm.pandas()","39fb0989":"tmdb_df = pd.read_csv('..\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv', index_col='id')","abc3994e":"tmdb_df = tmdb_df.loc[:,['overview','genres']]\ntmdb_df = tmdb_df.dropna(subset=['overview', 'genres'])","11c4a300":"tmdb_df.head(5)","fde522bb":"def parse_genres(genres_string):\n    genres_dict = eval(genres_string)\n    genres = [i['name'] for i in genres_dict]\n    return genres\n\ntmdb_df['genres'] = tmdb_df['genres'].progress_apply(lambda x: parse_genres(x))\n\ndf = tmdb_df.copy()\ndf.head(5)","04a6d39b":"# Multilabel Binarizer\nmlb = MultiLabelBinarizer()\ngenres_binary = mlb.fit_transform(df['genres'])\ngenres = pd.DataFrame(genres_binary, columns=mlb.classes_, index=df.index)\n\n# Remove genres with low representation\ndf = df.join(genres).drop(['genres', 'Foreign', 'History', 'Music', 'Animation',\n                           'War', 'Western', 'TV Movie', 'Documentary'], axis=1)\ndf = df[df.sum(axis=1)!=0] # Remove movies with no genres\n\n# Remove multiple white spaces in overview\ndf['overview'] = df['overview'].progress_apply(lambda x: re.sub(' +', ' ', x))\ndf.reset_index(inplace=True, drop=True)\n\ndf.head(5)","543ca7a9":"ax = df.iloc[:,1:].sum().sort_values(ascending=False).plot(kind='barh',\n                                                           title='Number of movies per genre')\nax.set_xlabel(\"# Movies\")\nax.set_ylabel(\"Genres\")","c1601497":"sum_df = df.iloc[:,1:].sum(axis=1)\nax = sum_df.value_counts().sort_index().plot(kind='bar', title='Multiple genres per movie')\nax.set_xlabel('# Genres')\nax.set_ylabel('# Movies')\nprint(sum_df.describe().round(2))","c4c9e09d":"chr_lens = df.overview.str.len()\nax = chr_lens.plot(kind='hist', bins=15, title='Number of characters in the overviews')\nax.set_xlabel(\"# Characters\")\nprint(chr_lens.describe().round(2))","c7543d06":"word_lens = df.overview.str.split(\" \").str.len()\nax = word_lens.plot(kind='hist', bins=15, title='Number of words in the overviews')\nax.set_xlabel(\"# Words\")\nprint(word_lens.describe().round(2))","34b41253":"stop_words = set(stopwords.words('english')) # Stop words\n\nstylecloud.gen_stylecloud(text = \" \".join(df.overview.values), icon_name= \"fas fa-cloud\",\n                          palette='cartocolors.qualitative.Bold_10',\n                          background_color='white', custom_stopwords=stop_words,\n                          random_state=9, size=(450,450),\n                          output_name='word_cloud.png')\nImage('.\/word_cloud.png')","3cdccfad":"df_final = df.copy()\ndf_final.shape","62340421":"porter = PorterStemmer() # Stemmer\n\ndef clean(text):\n    text = text.lower() # To lower case\n    # Replace contractions\n    text = re.sub(r\"\\'s\", \" is \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub('\\W', ' ', text) # Replace any character which is not a word character\n    text = re.sub('\\s+', ' ', text) # Replace any whitespace character\n    text = re.sub(' +', ' ', text) # Replace multiple whitespaces\n    text = text.strip() # Delete sorrounding whitespaces\n    tokens = word_tokenize(text) # Tokenize text\n    #  Delete Stop Words and apply stemming\n    new_tokens = [porter.stem(word) for word in tokens if not word in stop_words] \n    text = (\" \").join(new_tokens) # Join tokens\n    return text\n\ndf_final['overview'] = df_final['overview'].progress_apply(lambda x: clean(x))","a273e672":"print('Original: ' + df['overview'][5])\nprint(' ')\nprint('New: ' + df_final['overview'][5])","6eaaa87f":"X = df_final.overview\ny = df_final.drop(['overview'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\nprint(X_train.shape)\nprint(X_test.shape)","0133ffff":"# Ignore words that appear in less than 10 document and more than 80% of the documents\ntfidf = TfidfVectorizer(stop_words=stop_words, max_df=0.8, min_df=10)\nX_train_enc = tfidf.fit_transform(X_train)\nX_test_enc = tfidf.transform(X_test)\n\nprint('Vocabulary: ' + str(X_train_enc.shape[1]))\nprint(' ')\nprint('Original: ' + X_train[3])\nprint(' ')\nprint('Encoded: ' + str(X_train_enc[3]))","5785e438":"clf = OneVsRestClassifier(MultinomialNB())\nclf.fit(X_train_enc, y_train)\ny_pred_proba = clf.predict_proba(X_test_enc)\npredictions = clf.predict(X_test_enc)","ab27ea40":"metrics = ['Ranking Average Precision', 'Ranking Loss', 'Exact Match Ratio', 'Recall', 'Precision', 'F1-Score']\nresults = pd.DataFrame(index=metrics)\nresults.index.name = 'Metrics'\nresults['Naive Bayes'] = 0\nresults['DistilBERT'] = 0","a1e6f981":"results.loc['Ranking Average Precision', 'Naive Bayes'] = sklearn.metrics.label_ranking_average_precision_score(y_test, y_pred_proba)\nresults.loc['Ranking Loss', 'Naive Bayes'] = sklearn.metrics.label_ranking_loss(y_test, y_pred_proba)\nresults.loc['Exact Match Ratio', 'Naive Bayes'] = sklearn.metrics.accuracy_score(y_test, predictions)\nresults.loc['Recall', 'Naive Bayes'] = sklearn.metrics.precision_score(y_test, y_pred=predictions, average='samples')\nresults.loc['Precision', 'Naive Bayes'] = sklearn.metrics.recall_score(y_test, y_pred=predictions, average='samples')\nresults.loc['F1-Score', 'Naive Bayes'] = sklearn.metrics.f1_score(y_test, y_pred=predictions, average='samples')\nresults = results.round(2)\nresults","51161ded":"fig, axes = plt.subplots(3,4, figsize=(14, 10), sharey=True, sharex=True)\nfig.suptitle('Confusion Matrixes - Naive Bayes', fontsize=16)\n\nconfusion_matrixes = sklearn.metrics.multilabel_confusion_matrix(y_test, predictions)\nfor idx,i in enumerate(confusion_matrixes):\n    f1_score = sklearn.metrics.f1_score(y_test.iloc[:,idx], pd.DataFrame(predictions).iloc[:,idx])\n    f1_score = round(f1_score,2)\n    ax = axes.flat[idx]\n    vmax = i[1,0] + i[1,1]\n    sns.heatmap(i, annot=True, fmt='g', vmin=0, vmax=vmax, cmap=\"Blues\", cbar=False,\n                ax=ax).set_title(y_test.columns[idx] + ' (F1-score: ' + str(f1_score) + ')')\n\n# Set common labels\nfig.text(0.5, 0.06, 'Prediction', ha='center', va='center')\nfig.text(0.08, 0.5, 'Real value', ha='center', va='center', rotation='vertical')","9fb5f89a":"X = df.overview\ny = df.drop(['overview'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)","73f52e60":"tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')","8a1d7549":"text = X_train.iloc[0]\nencoded_input = tokenizer(text)\n\nprint('Original: '+ text)\nprint(' ')\nprint('Tokens: ' + str(encoded_input))\nprint(' ')\nprint('Reverse tokens: ' + tokenizer.decode(encoded_input[\"input_ids\"]))","45a9b323":"# Train\nbatch_sentences = X_train.values.tolist()\nX_train = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\nX_train['targets'] = np.array(y_train)\nmax_length = X_train['input_ids'].shape[1]\n\n# Test\nbatch_sentences = X_test.values.tolist()\nX_test = tokenizer(batch_sentences, padding='max_length', max_length=max_length, \n                   truncation=True, return_tensors=\"tf\")\nX_test['targets'] = np.array(y_test)\n\nprint(X_train)","a8aeabe0":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n                                                              num_labels=len(y_train.columns), \n                                                              problem_type=\"multi_label_classification\")","6f2d7715":"train_features = {x: X_train[x] for x in tokenizer.model_input_names}\ntrain_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, X_train[\"targets\"]))\ntrain_tf_dataset = train_tf_dataset.shuffle(len(X_train)).batch(32)\n\ntest_features = {x: X_test[x] for x in tokenizer.model_input_names}\ntest_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, X_test[\"targets\"]))\ntest_tf_dataset = test_tf_dataset.batch(32)","939b49b2":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), # Optimize individual binary crossentropy\n    metrics=tf.keras.metrics.BinaryAccuracy()\n)\n\nmodel.fit(train_tf_dataset, validation_data=test_tf_dataset, epochs=3)","723f8bf1":"def inv_logit(p):\n    return np.exp(p) \/ (1 + np.exp(p))\n\ny_pred_logits = model.predict(test_tf_dataset)\ny_pred_proba = inv_logit(y_pred_logits[0])\n\npredictions = pd.DataFrame(y_pred_proba, columns=y_test.columns, index=y_test.index).copy()\npredictions[predictions>=0.5] = 1\npredictions[predictions<0.5] = 0","d1964d9d":"results.loc['Ranking Average Precision', 'DistilBERT'] = sklearn.metrics.label_ranking_average_precision_score(y_test, y_pred_proba)\nresults.loc['Ranking Loss', 'DistilBERT'] = sklearn.metrics.label_ranking_loss(y_test, y_pred_proba)\nresults.loc['Exact Match Ratio', 'DistilBERT'] = sklearn.metrics.accuracy_score(y_test, predictions)\nresults.loc['Recall', 'DistilBERT'] = sklearn.metrics.precision_score(y_test, y_pred=predictions, average='samples')\nresults.loc['Precision', 'DistilBERT'] = sklearn.metrics.recall_score(y_test, y_pred=predictions, average='samples')\nresults.loc['F1-Score', 'DistilBERT'] = sklearn.metrics.f1_score(y_test, y_pred=predictions, average='samples')\nresults = results.round(2)\nresults","076262a7":"fig, axes = plt.subplots(3,4, figsize=(14, 10), sharey=True, sharex=True)\nfig.suptitle('Confusion Matrixes - DistilBERT', fontsize=16)\n\nconfusion_matrixes = sklearn.metrics.multilabel_confusion_matrix(y_test, predictions)\nfor idx,i in enumerate(confusion_matrixes):\n    f1_score = sklearn.metrics.f1_score(y_test.iloc[:,idx], predictions.iloc[:,idx])\n    f1_score = round(f1_score,2)\n    ax = axes.flat[idx]\n    vmax = i[1,0] + i[1,1]\n    sns.heatmap(i, annot=True, fmt='g', vmin=0, vmax=vmax, cmap=\"Blues\", cbar=False,\n                ax=ax).set_title(y_test.columns[idx] + ' (F1-score: ' + str(f1_score) + ')')\n\n# Set common labels\nfig.text(0.5, 0.06, 'Prediction', ha='center', va='center')\nfig.text(0.08, 0.5, 'Real value', ha='center', va='center', rotation='vertical')","f515e6df":"Load the model","489ab4ef":"Train the neural network:","30309c74":"Create tensorflow datasets and batches","c3c3b6e5":"Select overview and genre column:","ddf80cd7":"Transform genre dict to a simple list:","f2c6f2d3":"# Multi-label text classification with Naive Bayes and DistilBERT\n\nWe predict the genres of a movie from its summary (overview).\n\n- First: Applying a classical Naive Bayes, cleaning (normalization, stop-words and stemming) and transforming the text with TF-IDF.\n- Second: Applying DistilBERT on raw and tokenized text.","8ed21384":"### Number of words","03c9a709":"### Metrics","ad4ae128":"## Results","36dde65a":"### Multilabel Binarizer\nCreate binary targets with multilabel binarizer:","03555b26":"### Word Cloud","125f868f":"## Results","b8370078":"Split in train (80%) and test (20%):","a8bcf5d9":"Create tokenizer:","39744cb4":"### Normalize text\n- Lower-case text.\n- Replacing contractions.\n- Cleaning up special characters.\n- Removing extra whitespaces.\n- Removing stopwords.\n- Stemming.","3cdcc78d":"### Number of characters","a1561336":"### Naive Bayes Model\nNaive Bayes with One vs Rest strategy for multilabel classification:","33a794a2":"Load the data:","663dd4e6":"Shape of the dataset:","2f016c3c":"### TF-IDF\nApply Tf-Idf strategy to encode the overview:","63fe5c65":"# Naive Bayes","e16dd59c":"### Confusion Matrixes (as individual models)","81874d35":"Split dataframe without normalization in train and test (with the same random state to be able to compare models):","72200021":"### Number of genres per movie","f2e5d41f":"# Preprocessing","52226c3a":"Example applying tokenizer (It is applied at word level):","5b7e4550":"### Logits to probabilities and to labels\n\nProbability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5.","31e0189d":"### Import Libraries","d88cd0b9":"### Tokenize","14154a98":"Take a look to our data:","ab9ef9aa":"### Confusion Matrixes (as individual models)","756f6faa":"### Genres distribution","9074649e":"### DistilBERT Neural Network","625b9a5d":"Apply tokenizer to train and test, returning tensors for the neural network","4082254c":"# DistilBERT","a2a015ba":"# Charts of the dataset","39e1ccc4":"### Metrics","8bda584d":"Example of the transformation:"}}