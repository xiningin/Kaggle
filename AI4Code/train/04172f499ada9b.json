{"cell_type":{"c3513ab3":"code","526f01f9":"code","04142a6b":"code","53fdab3f":"code","d7ea6fb0":"code","88e7c1c2":"code","1552a991":"code","cefd5291":"code","649e44fb":"code","b63fd98a":"code","9da279af":"code","cab3ae4b":"code","ac2cba91":"code","7bd86f1e":"code","17e9d7c8":"code","bab14eb2":"code","e795d63e":"code","e692ece6":"code","2edbcb08":"code","5f829b12":"code","a9232947":"code","46628a46":"code","b64c2fa0":"code","85c3e02f":"code","15b31c84":"code","3e8824eb":"code","516367cc":"code","f1800604":"code","1c816aac":"code","41114245":"code","aa5bf253":"code","c63beb1e":"code","2b76889d":"code","429a5c7d":"code","21e88df3":"code","a713abb1":"code","777f2d2a":"code","c3e75537":"code","dcd779c4":"code","257fa50b":"code","86d886e7":"code","76e2995b":"code","b533cf11":"code","4ead5dd9":"code","63adff7a":"code","61f1d754":"code","7d847cdd":"markdown","e6e78cd0":"markdown","7c2c38a5":"markdown","61812877":"markdown","b38017e5":"markdown","e4ea403b":"markdown","2e54fdb7":"markdown","0f6499cc":"markdown","033faff0":"markdown","4243835a":"markdown","f3e13f88":"markdown","68efacf4":"markdown","7e789f59":"markdown","a280f4b6":"markdown","78f69b9c":"markdown","e49f65d5":"markdown","772acc44":"markdown","2424bda7":"markdown","5c9261ec":"markdown","c8d44b9d":"markdown","018fe353":"markdown","ecb19150":"markdown","bb015823":"markdown","8de64883":"markdown","acfe59c7":"markdown","4be1b1cb":"markdown","dc1d8222":"markdown","e93b1227":"markdown","91162f26":"markdown","ccc07f98":"markdown","848544e7":"markdown","fff23fc9":"markdown","4298faa8":"markdown","d9e30c68":"markdown","3c58d7b5":"markdown","2a30e7b8":"markdown","d5617b74":"markdown","ed2bc0b1":"markdown","42f32b04":"markdown","881c9fac":"markdown","264f46c2":"markdown","e7515f35":"markdown","bd258353":"markdown","c753ab2e":"markdown","87a8929e":"markdown","09bb17e4":"markdown"},"source":{"c3513ab3":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","526f01f9":"pip install skompiler","04142a6b":"pip install astor","53fdab3f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,scale, RobustScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,roc_curve, classification_report,mean_squared_error,f1_score,recall_score,precision_score\nimport time\nfrom skompiler import skompile\nfrom joblib import dump, load","d7ea6fb0":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","88e7c1c2":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","1552a991":"df.shape","cefd5291":"df.describe()","649e44fb":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","b63fd98a":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","9da279af":"decision_tree = DecisionTreeClassifier().fit(X_train,y_train)","cab3ae4b":"decision_tree","ac2cba91":"decision_tree.get_params()","7bd86f1e":"decision_tree.get_n_leaves()","17e9d7c8":"decision_tree.get_depth()","bab14eb2":"print(skompile(decision_tree.predict).to(\"python\/code\"))","e795d63e":"plt.figure(figsize=(15,10))\nplot_tree(decision_tree,proportion=True);","e692ece6":"decision_tree","2edbcb08":"y_pred = decision_tree.predict(X_test)","5f829b12":"cm = confusion_matrix(y_test,y_pred)","a9232947":"cm","46628a46":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","b64c2fa0":"accuracy_score(y_test,y_pred)","85c3e02f":"recall_score(y_test,y_pred)","15b31c84":"precision_score(y_test,y_pred)","3e8824eb":"f1_score(y_test,y_pred)","516367cc":"print(classification_report(y_test,y_pred))","f1800604":"decision_tree","1c816aac":"accuracies= cross_val_score(estimator=decision_tree,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","41114245":"decision_tree.predict(X_test)[:10]","aa5bf253":"decision_tree_params ={\"criterion\":[\"gini\",\"entropy\"],\n                       \"max_depth\":[1,2,3,4,5,6,7,8,9,10],\n                       \"min_samples_split\":list(range(1,10))}","c63beb1e":"decision_tree_classifier = DecisionTreeClassifier()\ndecision_tree_cv = GridSearchCV(decision_tree_classifier,decision_tree_params,cv=9,n_jobs=-1,verbose=2)","2b76889d":"start_time = time.time()\n\ndecision_tree_cv.fit(X_train,y_train)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for Decision Tree Classifier cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","429a5c7d":"#best score\ndecision_tree_cv.best_score_","21e88df3":"#best parameters\ndecision_tree_cv.best_params_","a713abb1":"decision_tree_tuned = DecisionTreeClassifier(criterion=\"entropy\",max_depth=2,min_samples_split=2).fit(X_train,y_train)","777f2d2a":"decision_tree_tuned","c3e75537":"y_pred = decision_tree_tuned.predict(X_test)","dcd779c4":"cm = confusion_matrix(y_test,y_pred)","257fa50b":"cm","86d886e7":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","76e2995b":"accuracy_score(y_test,y_pred)","b533cf11":"recall_score(y_test,y_pred)","4ead5dd9":"precision_score(y_test,y_pred)","63adff7a":"f1_score(y_test,y_pred)","61f1d754":"print(classification_report(y_test,y_pred))","7d847cdd":"### Model Tuning","e6e78cd0":"#### Precision","7c2c38a5":"### Prediction","61812877":"### Classification","b38017e5":"If you want to see other algorithms such as:\n\n- Logistic Regression (Theory - Model- Tuning)\n\n- K - Nearest Neighbors(KNN) (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Linear Kernel (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Radial Basis Kernel (Theory - Model- Tuning)\n\n- Ensemble Learning - Random Forests Classification (Theory - Model- Tuning)\n\n- Naive Bayes Classification (Theory - Model)\n\n- XGBoost(Extreme Gradient Boosting) Classification (Theory - Model- Tuning)\n\nPlease visit my [Classification tutorial](https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Classification\/Classification.ipynb)","e4ea403b":"Now we will try to tune our model by using **K-Fold Cross Validation**.","2e54fdb7":"Recall gives us the answer of this question :\n\n**What proportion of actual positives was identified correctly?**\n\nIt is defined as follows: TP \/ (TP+FN)","0f6499cc":"### Theory","033faff0":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","4243835a":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw29atdmY9s4wmI-rc0qQZZb&ust=1628435461495000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKj1g_2Yn_ICFQAAAAAdAAAAABAD).","f3e13f88":"All hyperparameters can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html).","68efacf4":"For a real world example, we will work with **Pima Indians Diabetes** dataset by UCI Machine Learning as before.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","7e789f59":"The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n\nThe formula for the F1 score is: 2 * (precision * recall) \/ (precision + recall)","a280f4b6":"## Decision Tree Classification","78f69b9c":"#### Accuracy","e49f65d5":"Some use-cases:\n\n- Mail classification (spam or not)\n\n- Diagnosis of the sicknesses\n\n- Customer buying prediction (if customer will buy or not)","772acc44":"- **true positive**: These are cases in which we predicted positive, and they are actually positive.\n- **false positive (Type 1 Error)**: We predicted postive, but they are actually negative. (Also known as a \"Type 1 error.\")\n- **true negative**: We predicted negative, and they are actually negative.\n- **false negative (Type 2 Error)**: We predicted negative, but they are actually postive. (Also known as a \"Type 2 error.\")","2424bda7":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n\nAn ROC curve plots TP rates vs. FP rares at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n\n![image.png](attachment:image.png)","5c9261ec":"### Model","c8d44b9d":"Let's see the structure of the tree. It can be seen huge because of feature number.","018fe353":"## Importing Libraries","ecb19150":"#### Confusion Matrix","bb015823":"## Classification and Evaluation Metrics","8de64883":"#### F1 - Score","acfe59c7":"**Notation**: TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.","4be1b1cb":"### Evaluation Metrics","dc1d8222":"#### Recall","e93b1227":"#### ROC Curve (Receiver Operating Characteristic Curve)","91162f26":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**Logistic Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=yIYKR4sgzI8&ab_channel=StatQuestwithJoshStarmer)\n\n- [**The Ultimate Guide to Regression & Classification**](https:\/\/www.superdatascience.com\/blogs\/the-ultimate-guide-to-regression-classification)\n\n- [**Logistic Regression for Machine Learning**](https:\/\/machinelearningmastery.com\/logistic-regression-for-machine-learning\/)\n\n- [**Logistic Regression by Stanford University**](https:\/\/web.stanford.edu\/class\/stats202\/notes\/Classification\/Logistic-regression.html)\n\n- [**What is a Confusion Matrix in Machine Learning?**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)\n\n- [**Classification: Precision and Recall**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n\n- [**Classification: ROC Curve and AUC**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)\n\n- [**AUC-ROC Curve in Machine Learning Clearly Explained**](https:\/\/www.analyticsvidhya.com\/blog\/2020\/06\/auc-roc-curve-machine-learning\/)","ccc07f98":"We can also plot it with *plot_tree()* function of sklearn.","848544e7":"Precision gives us the answer of this question : \n\n**What proportion of positive identifications was actually correct?**\n\nIt is defined as follows: TP \/ (TP+FP)","fff23fc9":"In order to see all rows and columns, we will increase max display numbers of dataframe.","4298faa8":"**Lockdown Example**\n\nFor example, I tried to generate a guide for citizens in the big cities about\ngoing-out permissions during Covid-19 Pandemic. For the sake of simplicity, the users are adult\ncitizens who are older than 20 years.\n\nThe guide first checks the age of the citizen. If the citizen is 65+, then it checks if it is an Out-Day-\nFor65+. A 65+ user is allowed to go out only if it is an Out-Day-For-65+, otherwise he\/she is\nnot allowed to go out.\n\nIf the age of the citizen is not 65+, then the guide checks if that day is a Lockdown-Day. If it is not a\nlockdown day, this younger citizen is allowed to go out. However, if it is a lockdown day, the\nyounger citizens can go out only if he\/she accompanies a 65+ person. Otherwise he is not allowed to go out.\n\nIn this example, *Age of Citizen* is **root node** at the beginning of a tree. It represents entire population being analyzed. From the root node, the population is divided according to age.\n\n*Allowed to go out* and *Not allowed to go out* is **Leaf(Terminal) Node** that does not split anything.Splitting is a process of dividing a node into two or more sub-nodes.\n\n**Parent and Child Node** is a node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.","d9e30c68":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","3c58d7b5":"**Created by Berkay Alan**\n\n**Classification | Decision Tree**\n\n**17 January 2022**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","2a30e7b8":"## Resources","d5617b74":"To compute the points in an ROC curve, we could evaluate a classification model many times with different classification thresholds, but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC.","ed2bc0b1":"Now we will tune our model with GridSearch. We will tune *criterion*, *max_depth* and *min_samples_split* parameters.","42f32b04":"Classification is the process of finding or discovering a model or function which helps in separating the data into multiple categorical classes i.e. discrete values. In classification, data is categorized under different labels according to some parameters given in input and then the labels are predicted for the data. \nThe derived mapping function could be demonstrated in the form of \u201cIF-THEN\u201d rules. The classification process deal with the problems where the data can be divided into binary or multiple discrete labels. ","881c9fac":"![Screen%20Shot%202021-07-24%20at%2012.56.03.png](attachment:Screen%20Shot%202021-07-24%20at%2012.56.03.png)","264f46c2":"Decision Trees are an important type of algorithm for predictive modeling machine learning.\n\nThe representation for the CART model is a binary tree. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction.","e7515f35":"#### AUC (Area under Curve)","bd258353":"The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\nThis is an example of AUC:\n\n![image.png](attachment:image.png)","c753ab2e":"Accuracy is one metric for evaluating classification models. Informally, accuracy is **the fraction of predictions our model got right**.\n\nFormally, accuracy has the following definition: All correct predictions \/ all predictions\n\nFor binary classification, accuracy can also be calculated in terms of positives and negatives as follow: (TP+TN) \/ (TP+FP+FN+TN)","87a8929e":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate our model.  Confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.","09bb17e4":"The Classification Tree (CART) algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees."}}