{"cell_type":{"04571730":"code","d2feadb7":"code","886c399d":"code","ec566538":"code","57718ce0":"code","3efac113":"code","991606c3":"code","9cc12adb":"code","28df0fac":"code","2dce9078":"code","8bee0655":"code","820578f9":"code","2b868683":"code","7ac348ad":"code","d1ad85ef":"code","6919a221":"code","4946321a":"code","ef26c7ec":"code","fa693c13":"code","59714ba8":"code","55fc796d":"markdown","7af01961":"markdown","514c014c":"markdown","797c3e4b":"markdown","819fbad2":"markdown","1a6bee8e":"markdown","d5d42a25":"markdown","dbd64c06":"markdown","9f4adb6e":"markdown","ae5a0dae":"markdown","cb462f6c":"markdown"},"source":{"04571730":"# Installing tensorflow datasets\n!pip install -U tfds-nightly","d2feadb7":"# importing the libraries\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","886c399d":"# downloading the dataset\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","ec566538":"# function to normalize the dataset,i.e the pixel value ranges from 0-1\ndef normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) \/ 255.0\n  input_mask -= 1\n  return input_image, input_mask","57718ce0":"# resizing the train dataset image to H x W = 128 x 128\n# performing image augmentation to increase the dataset size\ndef load_image_train(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)\n    input_mask = tf.image.flip_left_right(input_mask)\n\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask","3efac113":"# resizing the test dataset image to H x W = 128 x 128\ndef load_image_test(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask","991606c3":"# defining some hyperparameters\nTRAIN_LENGTH = info.splits['train'].num_examples\nBUFFER_SIZE = 1000\nBATCH_SIZE = 128\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE","9cc12adb":"# defining the input pipeline\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest = dataset['test'].map(load_image_test)\ntrain_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset = test.batch(BATCH_SIZE)","28df0fac":"# function for showing sample images and its mask from the dataset\ndef display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()","2dce9078":"# train.take(1) method takes the first element f\nfor image, mask in train.take(1):\n  sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])","8bee0655":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.applications.vgg16 import *","820578f9":"def fcn(image_size, ch_in=3, ch_out=3):\n    \n    inputs = Input(shape=(*image_size, ch_in), name='input')\n\n    # Building a pre-trained VGG-16 feature extractor (i.e., without the final FC layers)\n    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n    # Recovering the feature maps generated by each of the 3 final blocks:\n    f3 = vgg16.get_layer('block3_pool').output  \n    f4 = vgg16.get_layer('block4_pool').output  \n    f5 = vgg16.get_layer('block5_pool').output  \n\n    # Replacing VGG dense layers by convolutions:\n    f5_conv1 = Conv2D(filters=4086, kernel_size=7, padding='same',\n                      activation='relu')(f5)\n    f5_drop1 = Dropout(0.5)(f5_conv1)\n    f5_conv2 = Conv2D(filters=4086, kernel_size=1, padding='same',\n                      activation='relu')(f5_drop1)\n    f5_drop2 = Dropout(0.5)(f5_conv2)\n    f5_conv3 = Conv2D(filters=ch_out, kernel_size=1, padding='same',\n                      activation=None)(f5_drop2)\n\n\n    # Using a transposed conv (w\/ s=2) to upscale `f5` into a 14 x 14 map\n    # so it can be merged with features from `f4_conv1` obtained from `f4`:\n    f5_conv3_x2 = Conv2DTranspose(filters=ch_out, kernel_size=4, strides=2,\n                                use_bias=False, padding='same', activation='relu')(f5)\n    f4_conv1 = Conv2D(filters=ch_out, kernel_size=1, padding='same',\n                      activation=None)(f4)\n\n    # Merging the 2 feature maps (addition):\n    merge1 = add([f4_conv1, f5_conv3_x2])\n\n    # We repeat the operation to merge `merge1` and `f3` into a 28 x 28 map:\n    merge1_x2 = Conv2DTranspose(filters=ch_out, kernel_size=4, strides=2,\n                                use_bias=False, padding='same', activation='relu')(merge1)\n    f3_conv1 = Conv2D(filters=ch_out, kernel_size=1, padding='same',\n                      activation=None)(f3)\n    merge2 = add([f3_conv1, merge1_x2])\n\n    # Finally, we use another transposed conv to decode and up-scale the feature map\n    # to the original shape, i.e., using a stride 8 to go from 28 x 28 to 224 x 224 here:\n    outputs = Conv2DTranspose(filters=ch_out, kernel_size=16, strides=8,\n                              padding='same', activation=None)(merge2)\n    \n    fcn_model = Model(inputs, outputs)\n    return fcn_model","2b868683":"# defining the tensorflow distribute strategy\nstrategy = tf.distribute.MirroredStrategy()","7ac348ad":"# calling the model inside the scope\nwith strategy.scope():\n  model = fcn(image_size=(128, 128))\n  # compiling the model with the optimizer, loss function and acc metrics\n  model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","d1ad85ef":"# let's see how our FCN implemented model looks like\n# does it looks like the architecture fig above?\ntf.keras.utils.plot_model(model, show_shapes=True)","6919a221":"# function to create the mask\ndef create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]","4946321a":"# function to show the prediction\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","ef26c7ec":"# Let's see a sample of input image it's true mask and what our model is predicted given the input image\nshow_predictions()","fa693c13":"# calllback to displaying the predictions of our model while training\nclass DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    show_predictions()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n   \n # ReduceLROnPlateau callback\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\n# EarlyStopping callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n","59714ba8":"EPOCHS = 50\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = info.splits['test'].num_examples\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_dataset,\n                          callbacks=[DisplayCallback(), reduce_lr, early_stopping])","55fc796d":"![IMG_20200721_200333.jpg](attachment:IMG_20200721_200333.jpg)","7af01961":"Fully Convolutional Networks (FCNs) owe their name to their architecture, which is built only from locally connected layers, such as convolution, pooling and upsampling. Note that no dense layer is used in this kind of architecture. This reduces the number of parameters and computation time. Also, the network can work regardless of the original image size, without requiring any fixed number of units at any stage, givent that all connections are local. To obtain a segmentation map (output), segmentation networks usually have 2 parts :\n* Downsampling path : capture semantic\/contextual information\n* Upsampling path : recover spatial information\n\nThe downsampling path is used to extract and interpret the context (what), while the upsampling path is used to enable precise localization (where). Furthermore, to fully recover the fine-grained spatial information lost in the pooling or downsampling layers, we often use skip connections.\n\nA skip connection is a connection that bypasses at least one layer. Here, it is often used to transfer local information by concatenating or summing feature maps from the downsampling path with feature maps from the upsampling path. Merging features from various resolution levels helps combining context information with spatial information.","514c014c":"###  <span style=\"color:red\">If you like this kernel then please upvote. It'll motivate me for making more kernels like this.<\/span>","797c3e4b":"    fig FCN architecture \n![Screenshot%20%28144%29.png](attachment:Screenshot%20%28144%29.png)","819fbad2":"This all for this kernel. Hope you enjoyed it.\n\n<span style=\"color:red\">**If you've learned anything then please upvote this notebook. This will motivate me for contributing more kagle kernels to kaggle community.**<\/span>","1a6bee8e":"We're going to use pre-trained VGG-16 model from keras.applications .\nThis model is trained on imagenet dataset. We're loading the VGG-16\nmodel without the last fully connected dense layers.\n\nAnd now let's try to implement the FCN model from looking the\nabove architecure.\nAnd we're using keras functional API for defining the model.","d5d42a25":"Refrences: \n1. https:\/\/www.tensorflow.org\/tutorials\/images\/segmentation\n2. https:\/\/arxiv.org\/abs\/1411.4038\n","dbd64c06":"FCN models have been developed for semantic segmentation.\nThis models uses pre-trained VGG-16 for feature extraction, adding on top some layers to concatenate and upsample the feature maps into pixel-level predictions.\nIn this fig, you can see how it has been used and connected.\n![Screenshot%20%28144%29.png](attachment:Screenshot%20%28144%29.png)","9f4adb6e":"What is Image Segmentation?\n\nImage segmentation is a critical process in computer vision. It involves dividing a visual input into segments to simplify image analysis. Segments represent objects or parts of objects, and comprise sets of pixels, or \u201csuper-pixels\u201d. Image segmentation sorts pixels into larger components, eliminating the need to consider individual pixels as units of observation.","ae5a0dae":"In this kernel we're going to do:\n1. what is image segmentation.\n2. what is FCN(fully Convolutional Networks).\n3. Implementing FCN model from the paper.\n4. Train the model into dataset Oxford-IIIT pet dataset.","cb462f6c":"Now, let's download and prepare our dataset Oxford-IIIT pet dataset.     .\nThe Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200 images for each class. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed.\n\nIt's already available in tensorflow datasets, So we're going to use it."}}