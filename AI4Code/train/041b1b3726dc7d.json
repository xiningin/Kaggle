{"cell_type":{"22afba32":"code","080b84ac":"code","03685a84":"code","790e8394":"code","96157718":"code","242e41b3":"code","c046d206":"code","a2ac5688":"code","5d4c595e":"code","59e21328":"code","73fbd9f6":"code","973cfbd4":"code","d2b4fbdf":"code","9c154dbf":"code","29106f6c":"code","2205f267":"code","97fbb3a9":"code","28acda63":"code","6eaf7988":"code","5a3bf086":"code","0c5f81e3":"code","da45a100":"code","c3d378a2":"code","551d5ad4":"code","d2e4b01c":"code","16759af3":"code","50fa85f9":"markdown","79978952":"markdown","9ec4cbf8":"markdown","8f1a0082":"markdown","94af22cf":"markdown","50d95dc1":"markdown","a9fe5708":"markdown","55efcfb3":"markdown"},"source":{"22afba32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","080b84ac":"import numpy as np \nimport pandas as pd \n\nCalendarDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\", header=0)\nSalesDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\", header=0) #June 1st Dataset","03685a84":"import os, psutil\n\npid = os.getpid()\npy = psutil.Process(pid)\nmemory_use = py.memory_info()[0] \/ 2. ** 30\nprint ('memory GB:' + str(np.round(memory_use, 2)))","790e8394":"CalendarDF['date'] = pd.to_datetime(CalendarDF.date)\n\nTX_1_Sales = SalesDF[['TX_1' in x for x in SalesDF['store_id'].values]]\nTX_1_Sales = TX_1_Sales.reset_index(drop = True)\nTX_1_Sales.info()","96157718":"# Generate MultiIndex for easier aggregration.\nTX_1_Indexed = pd.DataFrame(TX_1_Sales.groupby(by = ['cat_id','dept_id','item_id']).sum())\nTX_1_Indexed.info()","242e41b3":"# Aggregate total sales per day for each sales category\nFood = pd.DataFrame(TX_1_Indexed.xs('FOODS').sum(axis = 0))\nHobbies = pd.DataFrame(TX_1_Indexed.xs('HOBBIES').sum(axis = 0))\nHousehold = pd.DataFrame(TX_1_Indexed.xs('HOUSEHOLD').sum(axis = 0))\nFood.info()","c046d206":"# Merge the aggregated sales data to the calendar dataframe based on date\nCalendarDF = CalendarDF.merge(Food, how = 'left', left_on = 'd', right_on = Food.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Food'})\nCalendarDF = CalendarDF.merge(Hobbies, how = 'left', left_on = 'd', right_on = Hobbies.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Hobbies'})\nCalendarDF = CalendarDF.merge(Household, how = 'left', left_on = 'd', right_on = Household.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Household'})\nCalendarDF.head(10)","a2ac5688":"# Drop dates with null sales data\nCalendarDF = CalendarDF.drop(CalendarDF.index[1941:])\nCalendarDF.reset_index(drop = True)","5d4c595e":"# Aggregate all sales to perform analysis \ntotalSales = pd.DataFrame(columns = ['Sales'])\ntotalSales['Sales'] = CalendarDF['Food'] + CalendarDF['Hobbies'] + CalendarDF['Household']\ntotalSales.index = CalendarDF['date']\ntotalSales.head(10)","59e21328":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Decomposing the sales data from the full time period\nfullDecompose = seasonal_decompose(totalSales, model = 'additive')\nfullDecompose.plot() # plotting the decomposition\npyplot.show()","73fbd9f6":"# Decomposing the sales data from 2011\nis_2011 = CalendarDF.year == 2011\nCalendar2011 = CalendarDF[is_2011] # pull 2011 data from CalendarDF\n\n# Collect total sales data from 2011\nSales2011 = Calendar2011['Food'] + Calendar2011['Hobbies'] + Calendar2011['Household'] \nSales2011.reset_index(drop = True)\nSales2011.index = Calendar2011['date']\n\n# Decomposition\nDecompose2011 = seasonal_decompose(Sales2011, model = 'additive')\nDecompose2011.plot()\npyplot.show()","973cfbd4":"#Decomposing the sales data from February 2011\nis_February = Calendar2011.month == 2\nFebruary2011 = Calendar2011[is_February] # pull February data from Calendar2011\n\n# Collect total sales data from February 2011\nfebruarySales = February2011['Food'] + February2011['Hobbies'] + February2011['Household']\nfebruarySales.reset_index(drop = True)\nfebruarySales.index = February2011['date']\n\n# Decomposition\nfebruaryDecompose = seasonal_decompose(februarySales, model = 'additive')\nfebruaryDecompose.plot()\npyplot.show()","d2b4fbdf":"# To build an ARIMA Model, I have to find the 'p', 'q', and 'd' terms. Details found on Selva Prabhakaran's guide\n\n# Begin by checking if the Total Sales time series is stationary using the Augmented Dickey Fuller test\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nstatTest = adfuller(totalSales)\nprint('ADF Statistic: %f' % statTest[0])\nprint('p-value: %f' % statTest[1])","9c154dbf":"# Since the Total Sales time series does not seem to be stationary (p-value > .05), I perform differencing to find the 'd' and 'q' terms of the ARIMA model\n\nstatTest = adfuller(totalSales.diff().dropna())\nprint('p-value: %f' % statTest[1]) # p-value < .05, set 'd' = 1\n\n# Plot the first differencing to find the 'q' term\nplot_acf(totalSales.diff().dropna()) # Since lag 3 is well above the significance region, set 'q' = 3","29106f6c":"# To find the P term, find the first significant lag by plotting the Partial Autocorrelation of the differenced time series\nplot_pacf(totalSales.diff().dropna()) # the first lag is significant, p = 1","2205f267":"# Building the ARIMA model\nfrom statsmodels.tsa.arima_model import ARIMA\n\n# Segment the total sales data into Train and Test datasets\nsalesTrain = totalSales['20110130':'20160410']\nsalesTest = totalSales['20160411':'20160522']\n\n# Create the model (p,d,q) using p = 1, d =1, q = 3\nmodel = ARIMA(salesTrain.values, order = (1,1,3)) \nmodel_fit = model.fit(disp = 0)\nprint(model_fit.summary())","97fbb3a9":"model = ARIMA(salesTrain.values, order = (1,1,2)) \nmodel_fit = model.fit(disp = 0)\nprint(model_fit.summary())","28acda63":"# Actual vs Fitted\nmodel_fit.plot_predict(dynamic = False)\nplt.show()","6eaf7988":"# Plotting the residuals\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","5a3bf086":"# Using the model to forecast\nfitted = model.fit(disp=-1)\nfc, se, conf = fitted.forecast(42, alpha = .05) # predicting the total sales of the next 42 days\n\n# Make each forecast value as a pandas Series to plot confidence interval\nfc_series = pd.Series(fc, index = salesTest.index)\nlower_series = pd.Series(conf[:,0], index = salesTest.index)\nupper_series = pd.Series(conf[:,1], index = salesTest.index)\n\n# To improve visualization, I only plot the salesTest data against the forecasted values\nplt.plot(salesTest, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","0c5f81e3":"# I use the Pyramid Arima package to perform an auto-SARIMA forecast\n!pip install pmdarima\nimport pmdarima as pm\n\n# We set the seasonality ('m') to 7 in order to account for the weekly seasonality present in the data\nsmodel = pm.auto_arima(salesTrain, start_p = 1, start_q = 1, test = 'adf',\n                      max_p = 3, max_q = 3, m = 7,\n                      start_P = 0, seasonal = True, d = None, D = 1,\n                      trace = True, error_action = 'ignore', suppress_warnings = True,\n                      stepwise = True)\n\nsmodel.summary()","da45a100":"# Forecasting using the SARIMA model\nn_periods = 42 # forecasting the next 42 days of sales\nfitted, confint = smodel.predict(n_periods = n_periods, return_conf_int = True)\nindex_of_fc = pd.date_range(salesTest.index[0], periods = n_periods, freq = 'D') # setting freq = 'D' for daily indexing\n\n# Making series for plotting purposes\nfitted_series = pd.Series(fitted, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)","c3d378a2":"# Plotting the salesTest data against the forecasted values\nplt.plot(salesTest)\nplt.plot(fitted_series, color='darkgreen')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"SARIMA - Total Sales of TX_1\")\nplt.show()","551d5ad4":"#Accuracy metrics\ndef symmetric_mean_absolute_percentage_error(actual,forecast):\n    return 1\/len(actual) * np.sum(2 * np.abs(forecast-actual)\/(np.abs(actual)+np.abs(forecast)))\n\ndef mean_absolute_error(actual, forecast):\n    return np.mean(np.abs(actual - forecast))\n\ndef naive_forecasting(actual, seasonality):\n    return actual[:-seasonality]\n\ndef mean_absolute_scaled_error(actual, forecast, seasonality):\n    return mean_absolute_error(actual, forecast) \/ mean_absolute_error(actual[seasonality:], naive_forecasting(actual, seasonality))","d2e4b01c":"# Using SMAPE to evaluate the ARIMA model\nsymmetric_mean_absolute_percentage_error(salesTest['Sales'], fc)","16759af3":"# Using MASE to evaluate the SARIMA model\nmean_absolute_scaled_error(salesTest.values, fitted, seasonality = 7) # set seasonality to 7 to account for weekly seasonality","50fa85f9":"# **Load in new data ignoring Price and Submission Data**","79978952":"# **Perform Additive Decomposition**\n\nI used Jason Brownlee's guide on decomposition to decompose the total sales data in the following periods: the full time period, one year (2011), and one month (February 2011)","9ec4cbf8":"The forecasted values appear to be a straight line, making the simple ARIMA model an ineffective model in forecasting future sales values. However, the salesTest data lies within the 95% confidence interval of the forecast. Thus, the ARIMA model, while unable to predict exact future sales values, is able to accurately map the linear trend of the data. In order to account for the seasonality present in the sales data, I will build a SARIMA model and measure its effectiveness.","8f1a0082":"# **Preparing the Dataset**","94af22cf":"# **M5 Forecast Accuracy Research**\n\nThis is a continuation of my work on analyzing the sales data of Walmart's TX_1 store (Version 1 found here:https:\/\/www.kaggle.com\/jimmyliuu\/m5-forecast-accuracy-research-version-1). This week, I performed additive decomposition on total sales data. In addition,  I used two forecasting algorithms (ARIMA and SARIMA) to predict TX_1's future total sales based on the June 1st training data.\n\nI followed Jason Brownlee's \"How to Decompose Time Series Data into Trend and Seasonality\" to perform additive decomposition (found here:https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/)\n\nI followed Selva Prabhakaran's guide on Time Series Forecasting to build my ARIMA and SARIMA models (found here:https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/) \n\nI used Boris Shishov's forecasting metrics code to evaluate the accuracy of my models with Symmetric Mean Absolute Percentage Error (sMAPE) and Mean Absolute Scaled Error (MASE) (found \nhere:https:\/\/gist.github.com\/bshishov\/5dc237f59f019b26145648e2124ca1c9)\n\nNext week, I will incorporate the impact of exogenous variables to my forecasting.","50d95dc1":"# **Building the ARIMA Model**","a9fe5708":"Since the MA1 term is not significant, I adjusted q to 2 to improve the MA1 term's significance.","55efcfb3":"The SARIMA model is able to more accuractely predict the fluctuations in total sales values. \n\nI will now write the sMAPE and MASE functions to evaluate the accuracy of the models."}}