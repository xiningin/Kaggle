{"cell_type":{"21be4c95":"code","b4d82268":"code","71289722":"code","09c35bfd":"code","cd7955c9":"code","014ee5cb":"code","d68bf80c":"code","a7c7e812":"code","09271792":"code","7518d0aa":"code","56f38dcc":"code","13aecf91":"code","0f36572f":"code","2e2cfd68":"code","fbcaf4ae":"code","c785515c":"code","00337c21":"code","1810ef73":"code","2f39041f":"code","0f46c4a9":"code","20c37a51":"code","eeecb16b":"code","fb9d7e9a":"code","bf6e962f":"code","a94e40fb":"code","222acc36":"code","0ceef409":"code","30bc198a":"code","fb41f10c":"code","cb803f74":"code","6a4dcddd":"code","a9350629":"code","bda64863":"code","e766ebb5":"code","433b70f5":"code","49cd286b":"code","79ed506a":"code","a5c94eee":"code","a2222561":"code","b9254642":"code","7784bd24":"code","6f557c15":"code","c507179b":"code","8b152e2a":"code","71dd6e8f":"code","54b0a756":"code","070fe3bf":"code","c01760aa":"code","accef083":"code","5c3f1a4b":"code","05003638":"code","4e15f4a4":"markdown","55a64620":"markdown","1b0c7a8e":"markdown","6133df05":"markdown","e85912e6":"markdown","aadcd40e":"markdown","3cbc584b":"markdown","1513bc17":"markdown","5916033f":"markdown","1aa05a94":"markdown","b60fe9aa":"markdown","6f8491e7":"markdown"},"source":{"21be4c95":"import numpy as np \nimport pandas as pd \n\nimport os\n\nimport seaborn as sns\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib\n%config InlineBackend.figure_format = 'svg'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# matplotlib.style.use('seaborn') \n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","b4d82268":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","71289722":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","09c35bfd":"train_features.head()","cd7955c9":"train_targets_scored.head()","014ee5cb":"train_features.shape, train_targets_scored.shape, train_targets_scored.shape","d68bf80c":"train_features.isnull().sum().sum()","a7c7e812":"train_features.sig_id.nunique()","09271792":"(train_features.sig_id != train_targets_scored.sig_id).sum()","7518d0aa":"train_features.set_index('sig_id', inplace=True)\ntest_features.set_index('sig_id', inplace=True)\ntrain_targets_scored.set_index('sig_id', inplace=True)\ntrain_targets_nonscored.set_index('sig_id', inplace=True)","56f38dcc":"train_targets_scored.sum(axis=1).value_counts()","13aecf91":"train_targets_scored.sum(axis=0)","0f36572f":"fig, ax = plt.subplots()\nfig.set_figwidth(10)\nfig.set_figheight(5)\n\nplt.scatter(np.arange(train_targets_scored.shape[1]), train_targets_scored.sum(axis=0)) \nplt.grid(True)\nplt.ylabel('\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0438\u0441\u0445\u043e\u0434\u044b')\nplt.xlabel('\u041d\u043e\u043c\u0435\u0440 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439')\nplt.xticks(np.arange(train_targets_scored.shape[1])[::10])\nplt.show()","2e2cfd68":"train_targets_scored.loc[:, train_targets_scored.sum(axis=0) > 600]","fbcaf4ae":"from collections import Counter\nmoa_types = Counter([name.split('_')[-1] for name in train_targets_scored.columns])","c785515c":"moa_types","00337c21":"train_features.cp_type.value_counts()","1810ef73":"train_targets_scored.loc[train_features[train_features.cp_type == 'ctl_vehicle'].index].sum(axis=0).sum()","2f39041f":"train_features.cp_time.value_counts()","0f46c4a9":"train_features.cp_dose.value_counts()","20c37a51":"def add_statistics(df):\n\n    features_g = [col for col in df.columns if col.startswith('g-')]\n    features_c = [col for col in df.columns if col.startswith('c-')]\n    \n    df['g_sum'] = df[features_g].sum(axis=1)\n    df['g_mean'] = df[features_g].mean(axis=1)\n    df['g_std'] = df[features_g].std(axis=1)\n    df['g_kurt'] = df[features_g].kurtosis(axis=1)\n    df['g_skew'] = df[features_g].skew(axis=1)\n    df['c_sum'] = df[features_c].sum(axis=1)\n    df['c_mean'] = df[features_c].mean(axis=1)\n    df['c_std'] = df[features_c].std(axis=1)\n    df['c_kurt'] = df[features_c].kurtosis(axis=1)\n    df['c_skew'] = df[features_c].skew(axis=1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis=1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis=1)\n    df['gc_std'] = df[features_g + features_c].std(axis=1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis=1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis=1)","eeecb16b":"drop_index = train_features[train_features.cp_type == 'ctl_vehicle'].index\n\ntrain_features_df = train_features.drop(drop_index, axis=0)\ntrain_features_df = train_features_df.drop('cp_type', axis=1)\n\ntrain_target_df = train_targets_scored.drop(drop_index, axis=0)\n\n\ndrop_index = test_features[test_features.cp_type == 'ctl_vehicle'].index\ntest_features_df = test_features.drop(drop_index, axis=0)\ntest_features_df = test_features_df.drop('cp_type', axis=1)","fb9d7e9a":"train_features_df = pd.get_dummies(train_features_df, columns=['cp_time', 'cp_dose'], drop_first=True)\ntest_features_df = pd.get_dummies(test_features_df , columns=['cp_time', 'cp_dose'], drop_first=True)","bf6e962f":"# add_statistics(train_features_df)\nadd_statistics(test_features_df)","a94e40fb":"X_train_all = train_features_df.values\ny_train_all = train_target_df.values\nX_test = test_features_df.values","222acc36":"scaler = StandardScaler()\nX_train_all = scaler.fit_transform(X_train_all)\nX_test = scaler.transform(X_test)","0ceef409":"X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)","30bc198a":"train_all_dataset = TensorDataset(torch.tensor(X_train_all).float(), torch.tensor(y_train_all).float())\ntrain_all_loader = DataLoader(train_all_dataset, batch_size=512)\n\ntrain_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\nval_dataset = TensorDataset(torch.tensor(X_val).float(), torch.tensor(y_val).float())\n\ntrain_loader = DataLoader(train_dataset, batch_size=512)\nval_loader = DataLoader(val_dataset, batch_size=512)","fb41f10c":"x, y = next(iter(train_loader))\nx.shape, y.shape","cb803f74":"class FFNN(nn.Module):\n    def __init__(self, input_size, output_size, dropout_rate=0.1):\n        super().__init__()\n        \n        hidden_size = input_size \/\/ 2\n        \n        self.l1 =  nn.utils.weight_norm(nn.Linear(input_size, hidden_size))\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        \n        self.l2 =  nn.utils.weight_norm(nn.Linear(hidden_size, output_size))\n        self.bn2 = nn.BatchNorm1d(output_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        \n        self.l3 =  nn.utils.weight_norm(nn.Linear(output_size, output_size))\n        \n    \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = self.dropout1(x)\n        x = F.elu(x)\n        \n        x = self.l2(x)\n        x = self.bn2(x)\n        x = self.dropout2(x)\n        x = F.elu(x)\n        \n        x = self.l3(x)\n        \n        return x","6a4dcddd":"model = FFNN(input_size=890, output_size=206)\nmodel(x).shape","a9350629":"def train_model(model, optimizer, loss_function, train_loader, \n                val_loader=None, scheduler=None, epochs=1):\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for n_iter, (x, y) in enumerate(train_loader):\n            model.train()\n            x = x.to(device)\n            y = y.to(device) \n            optimizer.zero_grad()\n            y_pred = model(x)\n            loss = loss_function(y_pred, y)\n            loss.backward()\n            optimizer.step()      \n            running_loss += loss.item()\n        running_loss \/= len(train_loader)   \n        \n        if val_loader is not None:\n            model.eval()  \n            loss = 0.0\n            with torch.no_grad():\n                for (x, y) in val_loader:\n                    x = x.to(device)\n                    y = y.to(device) \n                    y_pred = model(x)\n                    loss += loss_function(y_pred, y).item()\n                loss \/= len(val_loader)\n\n            print(\"Epoch: [{}\/{}] \".format(epoch + 1, epochs),\n                  \"Train loss: {:.6f}\".format(running_loss),\n                  \"Val loss: {:.6f} \".format(loss))\n        else:\n            print(\"Epoch: [{}\/{}] \".format(epoch + 1, epochs),\n                  \"Train loss: {:.6f}\".format(running_loss))\n        if scheduler is not None:\n            scheduler.step()     ","bda64863":"def predict(model, X):\n    model.eval()  \n            \n    with torch.no_grad():\n        X = X.to(device)\n        preds = model(X)\n        y_pred = torch.sigmoid(preds)\n    return y_pred.cpu().numpy()","e766ebb5":"loss_function = nn.BCEWithLogitsLoss()\nmodel = FFNN(input_size=890, output_size=206).to(device)\noptimizer = optim.Adam(lr=0.001, params=model.parameters(), weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer,10, gamma=0.8, last_epoch=-1)","433b70f5":"from sklearn.decomposition import PCA","49cd286b":"# pca = PCA()\n# pca.fit_transform(X_train).shape","79ed506a":"# train_model(model, optimizer, loss_function, train_loader, val_loader, epochs=150, scheduler=scheduler)","a5c94eee":"loss_function = nn.BCEWithLogitsLoss()\nmodel = FFNN(input_size=875, output_size=206).to(device)\noptimizer = optim.Adam(lr=0.001, params=model.parameters(), weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer,10, gamma=0.8, last_epoch=-1)\n\ntrain_model(model, optimizer, loss_function, train_all_loader, epochs=100, scheduler=scheduler)","a2222561":"y_pred = predict(model, torch.tensor(X_test).float())","b9254642":"submission = pd.DataFrame(np.zeros((test_features.shape[0], train_targets_scored.shape[1])),\n                         index=test_features.index, columns=train_targets_scored.columns)","7784bd24":"sample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","6f557c15":"pred_index = test_features[test_features.cp_type != 'ctl_vehicle'].index","c507179b":"len(pred_index)","8b152e2a":"y_pred.shape","71dd6e8f":"submission.shape","54b0a756":"submission.loc[pred_index, :] = y_pred","070fe3bf":"submission.reset_index(inplace=True)","c01760aa":"submission","accef083":"sample_submission.shape","5c3f1a4b":"submission.shape","05003638":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","4e15f4a4":"###  Submission","55a64620":"### FFNN model","1b0c7a8e":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u043e\u043b-\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0441 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u0430\u0440\u0433\u0435\u0442\u0430.","6133df05":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0432\u0441\u0435 \u043b\u0438 id \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b \u0438 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442 \u043b\u0438 id \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u0432","e85912e6":"\u0412\u044b\u0434\u0435\u043b\u044f\u044e\u0442\u0441\u044f \u0442\u0440\u0438 \u0433\u0440\u0443\u043f\u043f\u044b \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u0432 - \u0440\u0435\u0434\u043a\u0438\u0435 (\u0447\u0441\u0438\u043b\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043e\u0442 0 \u0434\u043e 200), \u0447\u0430\u0441\u0442\u044b\u0435 (\u0447\u0438c\u043b\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043e\u0442 200 \u0434\u043e 600), \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 (\u0447\u0438c\u043b\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 > 600). \u041c\u043e\u0436\u0435\u0442, \u043f\u0440\u0438\u0433\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c.","aadcd40e":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 ctrl_vehicle \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 MoAs.","3cbc584b":"### \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","1513bc17":"### \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438","5916033f":"### \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u0442\u0430\u0440\u0433\u0435\u0442\u044b","1aa05a94":"\u0412 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0443\u0434\u0430\u043b\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0441 cp_type == 'ctl_vehicle' \u0438 \u0431\u0443\u0434\u0435\u043c \u0432\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043d\u0438\u0445 \u043d\u0443\u043b\u0435\u0432\u044b\u0435 \u0432\u0435\u0440\u043e\u0442\u044f\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 MoAs.","b60fe9aa":"### Train on all objects","6f8491e7":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u043c\u043e\u0436\u0435\u0442 \u043b\u0438 \u0442\u0430\u0440\u0433\u0435\u0442 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0446\u0435\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439"}}