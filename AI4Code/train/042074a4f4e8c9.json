{"cell_type":{"ee031324":"code","6caae30b":"code","3317d52e":"code","c44f9edf":"code","5fc5afb0":"code","df03fc73":"code","673b7bcc":"code","1f0ce0ca":"code","22d3c195":"code","0975612a":"code","e1ab36a4":"code","ed0677ab":"code","ec162945":"code","cd21c784":"code","71f4ee32":"code","4254726d":"code","2c47dd3d":"code","2534cc33":"code","09084f2d":"code","7cd1ceeb":"code","f0bbc225":"markdown","fea649fe":"markdown","7df2bfcf":"markdown"},"source":{"ee031324":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.feature_selection import f_classif, SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\nSEED = 1126","6caae30b":"raw_train_data = pd.read_csv(\n    \"..\/input\/html-final-data\/train_data.csv\", index_col=0\n    )\n\nraw_test_data = pd.read_csv(\n    \"..\/input\/html-final-data\/test_data.csv\", index_col=0\n    )\n\nX_train = pd.read_csv(\n    \"..\/input\/html-final-dataset-preprocessed\/X_train_preprocessed.csv\", index_col=0\n)\nX_val = pd.read_csv(\n    \"..\/input\/html-final-dataset-preprocessed\/X_val_preprocessed.csv\", index_col=0\n)\nX_test = pd.read_csv(\n    \"..\/input\/html-final-dataset-preprocessed\/X_test_preprocessed.csv\", index_col=0\n)\ny_train = pd.read_csv(\n    \"..\/input\/html-final-dataset-preprocessed\/y_train_preprocessed.csv\", index_col=0\n)\ny_val = pd.read_csv(\n    \"..\/input\/html-final-dataset-preprocessed\/y_val_preprocessed.csv\", index_col=0\n)\ny_train = np.array(y_train).ravel()","3317d52e":"cv = StratifiedKFold(n_splits=5, random_state=SEED)","c44f9edf":"#Ridge regression \nclf_LG = LogisticRegressionCV(\n    cv=cv, penalty = \"l2\", random_state=SEED, solver='liblinear', n_jobs = -1\n    ).fit(X_train, y_train)\nclf_LG.score(X_train, y_train)","5fc5afb0":"print(\"=\"*10, \"Logistic regression result\", \"=\"*10)\ny_val_pred = clf_LG.predict(X_val)\nprecision, recall, fscore, support = score(\n    y_val.to_numpy(), y_val_pred, \n    average='macro'\n    )\nprint('Precision: {:.4f}'.format(precision))\nprint('Recall: {:.4f}'.format(recall))\nprint('F1 score: {:.4f}'.format(fscore))","df03fc73":"#Random forest\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 300, stop = 1000, num = 5)]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 10, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\nRF_params_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'oob_score': [True]}\nprint(RF_params_grid)\n\nRFC = RandomForestClassifier(random_state = SEED)\nRFC_CV = GridSearchCV(\n    estimator = RFC, param_grid = RF_params_grid, \n    cv = cv, verbose = 1, n_jobs = -1\n    )\nRFC_CV.fit(\n    X_train, np.array(y_train).ravel()\n)","673b7bcc":"print(\"=\"*10, \"Random forest result\", \"=\"*10)\ny_val_pred = RFC_CV.predict(X_val)\nprecision, recall, fscore, support = score(\n    y_val.to_numpy(), y_val_pred, \n    average='macro'\n    )\nprint('Precision: {:.4f}'.format(precision))\nprint('Recall: {:.4f}'.format(recall))\nprint('F1 score: {:.4f}'.format(fscore))","1f0ce0ca":"# #SVM\n# SVM_param_grid = {\n#     'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],\n#     'kernel': ['rbf', 'poly', 'sigmoid']\n#     }\n\n# SVC_clf = SVC()\n# SVC_grid = GridSearchCV(SVC_clf, SVM_param_grid, refit=True, verbose=3, n_jobs = -1)\n# SVC_grid.fit(X_train, y_train)","22d3c195":"# y_val_pred = SVC_grid.predict(X_val)\n# precision, recall, fscore, support = score(\n#     y_val.to_numpy(), y_val_pred, \n#     average='macro'\n#     )\n# print('Precision: {:.4f}'.format(precision))\n# print('Recall: {:.4f}'.format(recall))\n# print('F1 score: {:.4f}'.format(fscore))","0975612a":"#KNN\nk_max = round(np.sqrt(X_train.shape[0]))\nk_range = list(range(3, k_max, 3))\nKNN_param_grid = {\n    \"n_neighbors\": k_range,\n    \"weights\": [\"uniform\", \"distance\"],\n    \"metric\": [\"euclidean\", \"manhattan\"]\n}\n\nKNN = KNeighborsClassifier(random_state=SEED)\nKNN_grid = GridSearchCV(KNN, \n    KNN_param_grid, \n    verbose = 1\n    cv = cv, n_jobs = -1).fit(X_train, y_train)","e1ab36a4":"print(\"=\"*10, \"KNN result\", \"=\"*10)\ny_val_pred = KNN_grid.predict(X_val)\nprecision, recall, fscore, support = score(\n    y_val.to_numpy(), y_val_pred, \n    average='macro'\n    )\nprint('Precision: {:.4f}'.format(precision))\nprint('Recall: {:.4f}'.format(recall))\nprint('F1 score: {:.4f}'.format(fscore))","ed0677ab":"GBC_param_grid = {\n    \"loss\":[\"deviance\", \"exponential\"], #exponential = adaboost\n    \"learning_rate\": [0.1],\n#     \"min_samples_split\": np.arange(2, 12, 4),\n    \"min_samples_leaf\": [4],\n    \"max_depth\":[10],\n    \"subsample\":[1],\n    \"n_estimators\":[300]\n    }","ec162945":"GBC = GradientBoostingClassifier(random_state=SEED)\nGBC_grid = GridSearchCV(\n    GBC, GBC_param_grid, \n    cv=cv, n_jobs=-1, verbose = 1 \n).fit(X_train, y_train)","cd21c784":"print(\"=\"*10, \"Gradient boosting result\", \"=\"*10)\ny_val_pred = GBC_grid.predict(X_val)\nprecision, recall, fscore, support = score(\n    y_val.to_numpy(), y_val_pred, \n    average='macro'\n    )\nprint('Precision: {:.4f}'.format(precision))\nprint('Recall: {:.4f}'.format(recall))\nprint('F1 score: {:.4f}'.format(fscore))","71f4ee32":"L2REG = LogisticRegression(C = clf_LG.C_[0], penalty = \"l2\", random_state=SEED)\nRF = RandomForestClassifier(**RFC_CV.best_params_, random_state = SEED)\n#SVM = SVC(**SVC_grid.best_params_)\nKNN = KNeighborsClassifier(**KNN_grid.best_params_, random_state=SEED)\nGBC = GradientBoostingClassifier(**GBC_grid.best_params_, random_state=SEED)\nbase_learners = [\n    (\"Ridge\", L2REG),\n    (\"RF\", RF),\n    (\"GBC\", GBC)\n#     (\"SVM\", SVM),\n    (\"KNN\", KNN)\n]\nstack_clf = StackingClassifier(\n    estimators = base_learners, \n    final_estimator = LogisticRegression(),\n    cv = 10,\n    n_jobs = -1,\n    verbose = 1\n)\nstack_clf.fit(X_train, y_train)","4254726d":"print(\"=\"*10, \"Stacking Result\", \"=\"*10)\ny_val_pred = stack_clf.predict(X_val)\nprecision, recall, fscore, support = score(\n    y_val.to_numpy(), y_val_pred, \n    average='macro'\n    )\nprint('Precision: {:.4f}'.format(precision))\nprint('Recall: {:.4f}'.format(recall))\nprint('F1 score: {:.4f}'.format(fscore))","2c47dd3d":"import pickle\nwith open('stack_clf.pickle', 'wb') as f:\n    pickle.dump(stack_clf, f)","2534cc33":"category2label = {'Attitude': 3, 'Competitor': 1, 'Dissatisfaction': 2, 'Other': 5, 'Price': 4, 'No Churn': 0}\nground_truth_ratio = \\\n    (raw_train_data[\"Churn Category\"].value_counts() \/ raw_train_data.shape[0]).sort_index()\nground_truth_ratio = np.array(ground_truth_ratio.to_list())\nclf_label2category = dict(\n        zip(np.arange(stack_clf.n_classes_), stack_clf.classes_)\n    )","09084f2d":"#Normalized\ntest_prob = stack_clf.predict_proba(X_test)\ntest_prob_normalized = test_prob \/ ground_truth_ratio \ntest_predict_norm = np.argmax(\n    test_prob_normalized, axis=1\n)\n#clf label (number) -> category\ntest_predict = [\n    clf_label2category[label] for label in test_predict\n]\n\n#category -> correct label (number)\ntest_predict = [\n    category2label[category] for category in test_predict\n]\n\nd = {'Customer ID': raw_test_data.index.to_numpy(), 'Churn Category': test_predict_norm}\ndf = pd.DataFrame(data=d)\ndf.to_csv('.\/predict_0115_normalized.csv', index=False)","7cd1ceeb":"#Unnormalized\n\ntest_predict = stack_clf.predict(X_test)\ntest_predict = np.array([int(category[i]) for i in test_predict])\nd = {'Customer ID': raw_test_data.index.to_numpy(), 'Churn Category': test_predict}\ndf = pd.DataFrame(data=d)\ndf.to_csv('.\/predict_0115.csv', index=False)","f0bbc225":"### Level One Model - hyperparameter tuning\n- Ridge Regression\n- Random Forest\n- XGboost\n- Support Vector Machine\n- KNN","fea649fe":"# Stacking model","7df2bfcf":"# Inference"}}