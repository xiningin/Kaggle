{"cell_type":{"3426ecfa":"code","b00ef04c":"code","dd67b67f":"code","dcbdc0f6":"code","40d18b09":"code","7c6ebcd9":"code","ceeb2a4c":"code","7e4db115":"code","5498b4e7":"code","879d1ac3":"code","3b81cf61":"code","6185e93d":"code","8b565074":"code","00a87fa3":"code","3f3c30f3":"code","70fccf42":"code","7c29e4f0":"code","7ffcdde5":"code","6adf9ebb":"code","8c48c64e":"code","05b311e3":"code","2807c657":"code","cf0766c8":"code","c15770c9":"markdown","9e09873c":"markdown","1bde1994":"markdown","717fdf6d":"markdown","1c1e1614":"markdown","670c48aa":"markdown","94d4c536":"markdown","8463df0a":"markdown","c34e202f":"markdown","07acb601":"markdown","fb6dcb25":"markdown","755e0aae":"markdown"},"source":{"3426ecfa":"# Very few imports. This is a pure torch solution!\nimport cv2\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN","b00ef04c":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBASE_DIR = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/\"\n\nNUM_EPOCHS = 12","dd67b67f":"import ast\ndf = pd.read_csv(\"..\/input\/reef-cv-strategy-subsequences-dataframes\/train-validation-split\/train-0.1.csv\")\n\n# string\uc73c\ub85c \ub418\uc5b4\uc788\ub294 annotation\uc744 list of dictionaris\ub85c \ubcc0\ud658 (\uc0ac\uc2e4 train dataset \ub9cc\ub4e4\ub54c \ubbf8\ub9ac \ud574\ub460)\ndf['annotations'] = df['annotations'].apply(ast.literal_eval)\n\n# Create the image path for the row\ndf['image_path'] = \"video_\" + df['video_id'].astype(str) + \"\/\" + df['video_frame'].astype(str) + \".jpg\"\n\ndf.head()","dcbdc0f6":"# is_Train = True -> df_train \/ False -> df_val\ndf_train, df_val = df[df['is_train']], df[~df['is_train']]","40d18b09":"df_train[df_train.annotations.str.len()== 0]","7c6ebcd9":"# The model doesn't support images with no annotations\n# It raises an error that suggest that it just doesn't support them:\n# ValueError: No ground-truth boxes available for one of the images during training\n# I'm dropping those images for now\n# https:\/\/discuss.pytorch.org\/t\/fasterrcnn-images-with-no-objects-present-cause-an-error\/117974\/3\n\n# annotations\uac00 \uc5c6\uc73c\uba74 \ubaa8\ub378\uc774 \uc5d0\ub7ec\ub098\uae30 \ub54c\ubb38\uc5d0 annotations\uc758 \uae38\uc774\uac00 0\uc774\uc0c1\uc778 \uac83\ub9cc \ub0a8\uae41\ub2c8\ub2e4.\ndf_train = df_train[df_train.annotations.str.len() > 0 ].reset_index(drop=True)\ndf_val = df_val[df_val.annotations.str.len() > 0 ].reset_index(drop=True)","ceeb2a4c":"df_train.shape[0], df_val.shape[0]","7e4db115":"df_train.head()","5498b4e7":"row = df_train.iloc[0]\nboxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\nboxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x_max\nboxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y_max\nprint(\"[x_min, y_min, x_max, y_max]\",boxes) \nbox_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\nprint(box_outside_image)","879d1ac3":"class ReefDataset:\n\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.transforms = transforms\n\n    def can_augment(self, boxes):\n        \"\"\" Check if bounding boxes are OK to augment\n        augmentation\uc774 \uac00\ub2a5\ud55c\uc9c0 \ud655\uc778 \ud558\ub294 \ud568\uc218\uc785\ub2c8\ub2e4. annotation\uc774 image\uc758 \uc601\uc5ed \ubc16\uc73c\ub85c \ub098\uac00\uc9c0 \uc54a\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n        \n        For example: image_id 1-490 has a bounding box that is partially outside of the image\n        It breaks albumentation\n        Here we check the margins are within the image to make sure the augmentation can be applied\n        \"\"\"\n        \n        box_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\n        return not box_outside_image\n\n    def get_boxes(self, row):\n        \"\"\"\n        3D foramt\uc758 bboxes\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\n        Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\n        \"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x_max\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y_max\n        return boxes\n    \n    def get_image(self, row):\n        \"\"\"Gets the image for a given row\"\"\"\n        \n        image = cv2.imread(f'{BASE_DIR}\/{row[\"image_path\"]}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0 # normalization\n        return image\n    \n    def __getitem__(self, i):\n\n        row = self.df.iloc[i]\n        image = self.get_image(row)\n        boxes = self.get_boxes(row)\n        \n        n_boxes = boxes.shape[0]\n        \n        # Calculate the area\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # (x_max - x_min) * (y_max - y_min)\n        \n        \n        target = {\n            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n            'area': torch.as_tensor(area, dtype=torch.float32),\n            \n            'image_id': torch.tensor([i]),\n            \n            # There is only one class\n            'labels': torch.ones((n_boxes,), dtype=torch.int64),\n            \n            # Suppose all instances are not crowd\n            'iscrowd': torch.zeros((n_boxes,), dtype=torch.int64)            \n        }\n\n        if self.transforms and self.can_augment(boxes): #transform\uc774 \uc788\uace0 augmentation\uc774 \uac00\ub2a5\ud558\ub2e4\uba74...\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            if n_boxes > 0: # \ud0c0\uac9f\uc774 \uc5ec\ub7ec \uac1c\uc77c \uacbd\uc6b0 stack\ud568\uc218\ub97c \uc774\uc6a9\ud574 \uc313\uc544\uc62c\ub9bc map\ud568\uc218\ub85c tensor \ubcc0\ud658\n                target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        else: # augmentation\uc774 \uc548\ub41c\ub2e4\uba74 \uadf8\ub300\ub85c tensor\ub85c \ubcc0\ud658\n            image = ToTensorV2(p=1.0)(image=image)['image']\n\n        return image, target\n\n    def __len__(self):\n        return len(self.df)","3b81cf61":"# train\uc2dc\uc5d0\ub9cc augmentation \uc9c4\ud589\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","6185e93d":"# Define datasets\nds_train = ReefDataset(df_train, get_train_transform())\nds_val = ReefDataset(df_val, get_valid_transform())","8b565074":"# Let's get an interesting one ;)\ndf_train[df_train.annotations.str.len() > 12].head()","00a87fa3":"image, targets = ds_train[2200]\nimage","3f3c30f3":"targets","70fccf42":"boxes = targets['boxes'].cpu().numpy().astype(np.int32)\nimg = image.permute(1,2,0).cpu().numpy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(img,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(img);","7c29e4f0":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndl_train = DataLoader(ds_train, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\ndl_val = DataLoader(ds_val, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)","7ffcdde5":"def get_model():\n    # load a model; pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    num_classes = 2  # 1 class (starfish) + background\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    model.to(DEVICE)\n    return model\n\nmodel = get_model()","6adf9ebb":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.0025, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\nn_batches, n_batches_val = len(dl_train), len(dl_val)\nvalidation_losses = []\n\n\nfor epoch in range(NUM_EPOCHS):\n    time_start = time.time()\n    loss_accum = 0\n    \n    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n         \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        # Predict\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_accum += loss_value\n\n        # Back-prop\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    # Validation \n    val_loss_accum = 0\n        \n    # Validation \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n            images = list(image.to(DEVICE) for image in images)\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n            \n            val_loss_dict = model(images, targets)\n            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n            val_loss_accum += val_batch_loss.item()\n    \n    # Logging\n    val_loss = val_loss_accum \/ n_batches_val\n    train_loss = loss_accum \/ n_batches\n    validation_losses.append(val_loss)\n    \n    # Save model\n    chk_name = f'fasterrcnn_resnet50_fpn-e{epoch}.bin'\n    torch.save(model.state_dict(), chk_name)\n    \n    \n    elapsed = time.time() - time_start\n    \n    print(f\"[Epoch {epoch+1:2d} \/ {NUM_EPOCHS:2d}] Train loss: {train_loss:.3f}. Val loss: {val_loss:.3f} --> {chk_name}  [{elapsed:.0f} secs]\")   ","8c48c64e":"validation_losses","05b311e3":"np.argmin(validation_losses)","2807c657":"idx = 0\n\nimages, targets = next(iter(dl_val))\nimages = list(img.to(DEVICE) for img in images)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\nboxes = targets[idx]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[idx].permute(1,2,0).cpu().numpy()\n\nmodel.eval()\n\noutputs = model(images)\noutputs = [{k: v.detach().cpu().numpy() for k, v in t.items()} for t in outputs]","cf0766c8":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# Red for ground truth\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n\n    \n# Green for predictions\n# Print the first 5\nfor box in outputs[idx]['boxes'][:5]:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (0, 220, 0), 3)\n\nax.set_axis_off()\nax.imshow(sample);","c15770c9":"## Check one sample","9e09873c":"# Train","1bde1994":"# \ud83d\udc20 Reef - Pytorch Starter - FasterRCNN Train\n\n## A self-contained, simple, pure pytorch \ud83d\udd25 Faster R-CNN implementation with `LB=0.413`\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31703\/logos\/header.png)\n\n#### FasterR-CNN is one of the SOTA models for Object detection.\n\n### In this notebook we present a simple solution using a pure pytorch Faster R-CNN with pretrained weights, and finetuning it for few epochs.\n\nIt is an adapted version of [this notebook](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train) mentioned in [this comment](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/discussion\/290016).\n\n## You can find the [inference notebook here](https:\/\/www.kaggle.com\/julian3833\/coral-reef-pytorch-fasterrcnn-infer-0-xxx).\n\n## Details: \n- FasterRCNN from torchvision\n- Use Resnet50 backbone\n\n**Update**: Added simple train\/validation split in this version, using the \"subsequence\" split of this notebook: [\ud83d\udc20 Reef - CV strategy: subsequences!](https:\/\/www.kaggle.com\/julian3833\/reef-cv-strategy-subsequences)\n\nStill dropping all the images with no objects, as the model doesn't support them out-of-the-box. The other starters are removing the empty images as well, so it might be a general condition of Object Detection. I'm quite noob in the field to be honest.\n\n# Please, _DO_ upvote if you find this useful!!\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n\n#### Changelog\n\n| Version | Description| Dataset| Best LB |\n| --- | ----| --- | --- |\n| [**V8**](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-293?scriptVersionId=80517118)  | 2 epochs - Save last epoch | [coral-reef-pytorch-starter-fasterrcnn-weights](https:\/\/www.kaggle.com\/julian3833\/coral-reef-pytorch-starter-fasterrcnn-weights)| `0.293`|\n| [**V16**](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-293?scriptVersionId=80601095) | 4 epochs - Save all epochs | [reef-starter-torch-fasterrcnn-4e](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-4e)| `0.361` |\n| [**V17**](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-369?scriptVersionId=80604402) | Add **validation**. 95-5 split. 8 epochs, keeping track of validation loss. | [reef-starter-torch-fasterrcnn-8e](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-8e)| `0.369` |\n| [**V19**](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-369?scriptVersionId=80610403) | 12 epochs, lower LR | [reef-starter-torch-fasterrcnn-12e](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-12e)| `0.413` |\n| [**V24**](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-369) | V19 with 90-10 train-validation split. Tidy up code. Add Flip. Correct problem with augmentations. | -- | `??` |\n\n\n","717fdf6d":"## Augmentations","1c1e1614":"# Check result","670c48aa":"# Load `df`\n\n### See: [\ud83d\udc20 Reef - CV strategy: subsequences!](https:\/\/www.kaggle.com\/julian3833\/reef-cv-strategy-subsequences)","94d4c536":"# Please, _DO_ upvote if you found it useful!","8463df0a":"# Dataset class","c34e202f":"# Imports","07acb601":"# Create the model","fb6dcb25":"# Constants","755e0aae":"## DataLoaders"}}