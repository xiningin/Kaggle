{"cell_type":{"a76aa95b":"code","fb746aaa":"code","0d089ded":"code","e8ac2353":"code","10ea2527":"code","f3186ce9":"code","8ccf3127":"code","ddd77550":"code","9b7ec429":"code","29b7701b":"code","cece0b3d":"code","790139cb":"code","94142031":"code","012b27b9":"code","7b1dd2c2":"code","b4a70b2b":"code","a40b9a40":"code","b0484a9f":"code","878d2b2f":"code","ccc36ab7":"code","197d4de2":"code","35aceee9":"code","e6c12212":"code","ed24824a":"code","544de44c":"code","4b07e1d8":"code","95fcef91":"code","98ee972f":"code","94720532":"code","659960a5":"code","9cf82348":"code","953d4041":"code","0695fee0":"code","f38daf54":"code","fe82fcee":"code","9c8b3d91":"code","4a223c3f":"code","ffcbd638":"code","77928230":"code","b946086e":"code","b97bb4d0":"code","d1d9734e":"code","c436778e":"code","269cdded":"code","a9f7a914":"code","147a7d7f":"code","60851224":"code","2007d09f":"code","7cdbe0d1":"code","c21612ff":"code","96bf94c4":"code","9384cfe4":"code","683d8116":"code","cc9c39bd":"code","72549fab":"code","1c49ba3f":"code","cfeb5381":"code","5fac7dce":"code","54f52414":"code","c5eec9b6":"code","a700ffdb":"code","e18ad726":"code","adfc56bb":"code","2f5d1b40":"code","edfeffd0":"markdown","f1d24993":"markdown","5da9d733":"markdown","3d0631ed":"markdown","827e7f08":"markdown","9a537c02":"markdown","d2e2f46a":"markdown","97c0af4a":"markdown","ec4bd979":"markdown","6574ccff":"markdown","46d8996d":"markdown","06ced266":"markdown","de07f9f8":"markdown","6bc5e903":"markdown","d01cbb37":"markdown","04b7fc70":"markdown","d93d6569":"markdown","5753cb4f":"markdown","6a5d45cb":"markdown","889116ea":"markdown","31294245":"markdown","0b25cbb0":"markdown","c8a5cf16":"markdown","355fd27c":"markdown","04383ce8":"markdown","c7feb3c8":"markdown","aff2934e":"markdown","73852153":"markdown","d095c063":"markdown","688f4ce7":"markdown","812c581a":"markdown","1a3502c8":"markdown","181261d8":"markdown","8f72805c":"markdown","e3004c13":"markdown","b0e94db2":"markdown","76eaf862":"markdown","2c31726b":"markdown","b1514fbc":"markdown","189bd195":"markdown","66462bbc":"markdown","54c2c425":"markdown","569c6f4a":"markdown","4d7601cc":"markdown","32de80c8":"markdown","9460d246":"markdown","a8d9213c":"markdown","f753b5a5":"markdown","efc19f4c":"markdown","97df7a05":"markdown","451ad05b":"markdown","91b42fc8":"markdown","83d3520c":"markdown","c1ab0992":"markdown","94de86ee":"markdown","83b0759f":"markdown","51a852e2":"markdown","0132fd8a":"markdown","670b7935":"markdown","8878ca19":"markdown"},"source":{"a76aa95b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n%matplotlib inline \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","fb746aaa":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/train.csv')","0d089ded":"train.info()","e8ac2353":"test.info()","10ea2527":"train.isnull().any().describe()","f3186ce9":"test.isnull().any().describe()","8ccf3127":"from sklearn.model_selection import train_test_split","ddd77550":"X_train_split = train.drop(['label'], axis=1).copy()\ny_train_split = train['label'].copy()\n\nX_train, X_validation, y_train, y_validation = train_test_split(X_train_split, y_train_split, test_size=0.1, random_state=42)\n\ndel X_train_split, y_train_split\n\nprint(\"Training Features:\", X_train.shape)\nprint(\"Training Labels:\", y_train.shape)\nprint(\"Validation Features:\", X_validation.shape)\nprint(\"Validation Labels:\", y_validation.shape)\nprint(\"Test Features:\", test.shape)","9b7ec429":"X_train_explore = X_train.copy()\ny_train_explore = y_train.copy()\n\ndel X_train, y_train","29b7701b":"y_train_explore.value_counts().describe()","cece0b3d":"sns.set()\nsns.countplot(x=\"label\", data=y_train_explore.to_frame())","790139cb":"sample_digit = X_train_explore.iloc[2000] # a random instance\nsample_digit_image = sample_digit.values.reshape(28, 28) # reshape it from (784,) to (28,28)\nplt.imshow(sample_digit_image, # plot it as an image\n           cmap = matplotlib.cm.binary,\n           interpolation=\"nearest\")\nplt.axis(\"off\")\nplt.show()","94142031":"X_train_scaled = X_train_explore.copy()\nX_train_scaled = X_train_scaled \/ 255.0\n\nX_train_scaled.head()","012b27b9":"X_train = X_train_scaled.copy()\ny_train = y_train_explore.copy()\n\ndel X_train_explore, X_train_scaled, y_train_explore","7b1dd2c2":"\"\"\"\nfrom sklearn.svm import SVC # Support Vector Classification\n\"\"\"","b4a70b2b":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nsvc_clf = SVC(gamma='auto', random_state=42, verbose=True)\nsvc_clf.fit(X_train, y_train)\n\n\"\"\"","a40b9a40":"\"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier\n\"\"\"","b0484a9f":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nkn_clf = KNeighborsClassifier()\nkn_clf.fit(X_train, y_train)\n\n\"\"\"","878d2b2f":"\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\n\"\"\"","ccc36ab7":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_clf = RandomForestClassifier(random_state=42, verbose=True)\nrf_clf.fit(X_train, y_train)\n\n\"\"\"","197d4de2":"\"\"\"\nfrom sklearn.neural_network import MLPClassifier\n\"\"\"","35aceee9":"# Important parameters\n# hidden layer size\n# activation function\n# alpha -> learning rate\n# random_state -> set to get remove randomness effect for different runs\n# momentum\n# max_iter\n\n\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\n\nmlp_clf = MLPClassifier(random_state=42)\nmlp_clf.fit(X_train, y_train)\n\n\"\"\"","e6c12212":"\"\"\"\nfrom sklearn.metrics import accuracy_score\n\"\"\"","ed24824a":"\"\"\"\nX_validation_scaled = X_validation.copy()\nX_validation_scaled = X_validation_scaled \/ 255.0\n\"\"\"","544de44c":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nsvc_prediction = svc_clf.predict(X_validation_scaled)\nprint(\"SVC Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=svc_prediction))\n\n\"\"\"","4b07e1d8":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nknn_prediction = kn_clf.predict(X_validation_scaled)\nprint(\"KNN Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=knn_prediction))\n\n\"\"\"","95fcef91":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_prediction = rf_clf.predict(X_validation_scaled)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=rf_prediction))\n\n\"\"\"","98ee972f":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nmlp_prediction = mlp_clf.predict(X_validation_scaled)\nprint(\"MLP Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=mlp_prediction))\n\n\"\"\"","94720532":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nX_validation_bw = X_validation.copy()\nX_train_bw = X_train.copy()\n\nX_validation_bw[X_validation_bw > 0] = 1\nX_train_bw[X_train_bw > 0] = 1\n\n\"\"\"","659960a5":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nsvc_bw_clf = SVC(gamma='auto', random_state=42, verbose=True)\nsvc_bw_clf.fit(X_train_bw, y_train)\n\n\"\"\"","9cf82348":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nkn_bw_clf = KNeighborsClassifier()\nkn_bw_clf.fit(X_train_bw, y_train)\n\n\"\"\"","953d4041":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_bw_clf = RandomForestClassifier(random_state=42, verbose=True)\nrf_bw_clf.fit(X_train_bw, y_train)\n\n\"\"\"","0695fee0":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nmlp_bw_clf = MLPClassifier(random_state=42)\nmlp_bw_clf.fit(X_train_bw, y_train)\n\n\"\"\"","f38daf54":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nsvc_bw_prediction = svc_clf.predict(X_validation_bw)\nprint(\"SVC BW Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=svc_bw_prediction))\n\n\"\"\"","fe82fcee":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nknn_bw_prediction = kn_clf.predict(X_validation_bw)\nprint(\"KNN Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=knn_bw_prediction))\n\n\"\"\"","9c8b3d91":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_bw_prediction = rf_clf.predict(X_validation_bw)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=rf_bw_prediction))\n\n\"\"\"","4a223c3f":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nmlp_bw_prediction = mlp_clf.predict(X_validation_bw)\nprint(\"MLP Accuracy:\", accuracy_score(y_true=y_validation ,y_pred=mlp_bw_prediction))\n\n\"\"\"","ffcbd638":"\"\"\"\nfrom sklearn.model_selection import cross_val_score\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"\")\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\"\"\"","77928230":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nsvm_scores = cross_val_score(svc_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, verbose=10)\nsvm_rmse_scores = np.sqrt(-svm_scores)\n\nprint(\"SVM Scores\\n\")\ndisplay_scores(svm_rmse_scores)\n\"\"\"","b946086e":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nkn_scores = cross_val_score(kn_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, verbose=10)\nkn_rmse_scores = np.sqrt(-kn_scores)\n\nprint(\"KNeighbor Scores\\n\")\ndisplay_scores(kn_rmse_scores)\n\n\"\"\"","b97bb4d0":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_scores = cross_val_score(rf_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, verbose=10)\nrf_rmse_scores = np.sqrt(-rf_scores)\n\nprint(\"Random Forest Scores\\n\")\ndisplay_scores(rf_rmse_scores)\n\n\"\"\"","d1d9734e":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nmlp_scores = cross_val_score(mlp_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, verbose=10)\nmlp_rmse_scores = np.sqrt(-mlp_scores)\n\nprint(\"Neural Network Scores\\n\")\ndisplay_scores(mlp_rmse_scores)\n\"\"\"","c436778e":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\"\"\"","269cdded":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nnn_parameter_grid = [\n    {'hidden_layer_sizes': [(100, ), (200, ), (300, )],\n     'solver': ['sgd', 'adam'],\n     'learning_rate_init':[0.0001, 0.001]\n    }\n]\n\nnn_grid_clf = MLPClassifier(random_state=42, verbose=True)\nnn_grid_search = GridSearchCV(nn_grid_clf,\n                              nn_parameter_grid,\n                              cv=3,\n                              scoring='neg_mean_squared_error',\n                              verbose=3)\nnn_grid_search.fit(X_train, y_train)\n\n\"\"\"","a9f7a914":"\"\"\"\n\ncvres = nn_grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n\"\"\"","147a7d7f":"\"\"\"\n\nnn_grid_search.best_params_\n\n\"\"\"","60851224":"\"\"\"\n\nI am commenting out this section since it is taking too much time, but feel free to uncomment and run it.\n\nrf_parameter_grid = [\n    {\n        'n_estimators': [60, 100, 200, 500],\n        'max_features': [12, 30, 100, 300, 'auto']\n    }\n]\n\nrf_grid_clf = RandomForestClassifier(random_state=42, verbose=True)\nrf_grid_search = GridSearchCV(rf_grid_clf,\n                              rf_parameter_grid,\n                              cv=None,\n                              scoring='neg_mean_squared_error',\n                              verbose=2)\nrf_grid_search.fit(X_train, y_train)\n\n\"\"\"","2007d09f":"\"\"\"\n\ncvres = rf_grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n    \n\"\"\"","7cdbe0d1":"\"\"\"\nrf_grid_search.best_params_\n\"\"\"","c21612ff":"nn_tuned_clf = MLPClassifier(hidden_layer_sizes=(300,),\n                            learning_rate_init=0.001,\n                            solver='adam',\n                            random_state=42,\n                            verbose=True)\nnn_tuned_clf.fit(X_train, y_train)","96bf94c4":"nn_tuned_pred = nn_tuned_clf.predict(X_validation_scaled)","9384cfe4":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_fscore_support","683d8116":"nn_precisions, nn_recalls, nn_f_beta_scores, nn_support = precision_recall_fscore_support(y_validation, nn_tuned_pred)\nprint(\"Precision of each class:\", nn_precisions, \"\\n\")\nprint(\"Recall of each class:\", nn_recalls, \"\\n\")\nprint(\"F Scores of each class:\", nn_f_beta_scores, \"\\n\")\nprint(\"Support of each class:\", nn_support, \"\\n\")","cc9c39bd":"f1_score(y_validation, nn_tuned_pred, average=\"micro\") # This is the average","72549fab":"nn_conf_matrix = confusion_matrix(y_validation, nn_tuned_pred)","1c49ba3f":"plt.matshow(nn_conf_matrix, cmap=plt.cm.gray)\nplt.show()","cfeb5381":"nn_row_sums = nn_conf_matrix.sum(axis=1, keepdims=True)\nnn_norm_conf_mx = nn_conf_matrix \/ nn_row_sums","5fac7dce":"np.fill_diagonal(nn_norm_conf_mx, 0) # to keep only the errors we fill diagonal with 0s, since diagonal shows the ones that are correctly classified.\nplt.matshow(nn_norm_conf_mx, cmap=plt.cm.gray)\nplt.show()","54f52414":"\"\"\"\n\nfinal_train = train.copy()\n\nfinal_X_train = final_train.drop(['label'], axis=1).copy()\nfinal_y_train = final_train['label'].copy()\n\ndel final_train\n\nfinal_X_train_scaled = final_X_train.copy()\nfinal_X_train_scaled = final_X_train_scaled \/ 255.0\n\"\"\"\n","c5eec9b6":"\"\"\"\nfrom sklearn.neural_network import MLPClassifier\n\"\"\"","a700ffdb":"\"\"\"\n\nfinal_nn_clf = MLPClassifier(hidden_layer_sizes=(300,),\n                            learning_rate_init=0.001,\n                            solver='adam',\n                            random_state=42)\nfinal_nn_clf.fit(final_X_train_scaled, final_y_train)\n\"\"\"\n","e18ad726":"\"\"\"\nfinal_prediction = final_nn_clf.predict(test)\n\"\"\"","adfc56bb":"\"\"\"\n\nsubmission = pd.DataFrame({\"ImageId\": list(range(1,len(final_prediction)+1)),\n                          \"Label\": final_prediction})\n\"\"\"","2f5d1b40":"\"\"\"\nsubmission.to_csv(\"cnn_mnist_submission.csv\", index=False)\n\"\"\"","edfeffd0":"{'max_features': 30, 'n_estimators': 500}","f1d24993":"As you can see our data is in this given format.","5da9d733":"## 6. Hyperparameter Tuning","3d0631ed":"# 7. Predict and Submit","827e7f08":"### Neural Network","9a537c02":"### KNN","d2e2f46a":"### MLP ","97c0af4a":"SVC BW Accuracy: 0.9226190476190477","ec4bd979":"SVC Accuracy: 0.934047619047619","6574ccff":"We are going to build the final classifier again, using the whole training set (including the validation set this time). Predict result for test set and submit our results.","46d8996d":"# 2. Data Exploration\n\nThis is the part where I get to know the data, how is it formatted, what properties it has etc.","06ced266":"Test set is missing one column, and that is the `label` column, since images are in the form of `28x28` we have `784` feature columns for each image.","de07f9f8":"### 2.2 Check for null values ","6bc5e903":"0.8918650572947011 {'max_features': 12, 'n_estimators': 60}  \n0.8688312407295935 {'max_features': 12, 'n_estimators': 100}  \n0.8511274812964209 {'max_features': 12, 'n_estimators': 200}  \n0.8394221998670452 {'max_features': 12, 'n_estimators': 500}  \n0.8549575316659775 {'max_features': 30, 'n_estimators': 60}  \n0.8316331863350311 {'max_features': 30, 'n_estimators': 100}  \n0.8305668364684866 {'max_features': 30, 'n_estimators': 200}  \n0.8183413344580241 {'max_features': 30, 'n_estimators': 500}  \n0.8433210991019234 {'max_features': 100, 'n_estimators': 60}  \n0.8353309390761112 {'max_features': 100, 'n_estimators': 100}  \n0.8305827622023737 {'max_features': 100, 'n_estimators': 200}  \n0.8200529083460241 {'max_features': 100, 'n_estimators': 500}  \n0.8734775114237132 {'max_features': 300, 'n_estimators': 60}  \n0.8598326402923824 {'max_features': 300, 'n_estimators': 100}  \n0.8533469740973235 {'max_features': 300, 'n_estimators': 200}  \n0.8530524091158835 {'max_features': 300, 'n_estimators': 500}  \n0.85441585602472 {'max_features': 'auto', 'n_estimators': 60}  \n0.838002790716521 {'max_features': 'auto', 'n_estimators': 100}  \n0.8238668665243273 {'max_features': 'auto', 'n_estimators': 200}  \n0.8244767460407266 {'max_features': 'auto', 'n_estimators': 500}  ","d01cbb37":"## 4.4 Neural Network Classifier\n\nWe are going to create the MLP classifier.\nWe are going to `fit()` the training data.\n\nMulti-layer perceptron is the one that requires most modification. Of course default values are already set in its **\\_\\_init\\_\\_** method but it is better if we customize it according to our needs. Of course there are no strict rules these parameters but we are going to try to do our best.\n","04b7fc70":"1.3016777939893878 {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.0001, 'solver': 'sgd'}  \n0.7858874268106391 {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.0001, 'solver': 'adam'}  \n0.9632946111841942 {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'solver': 'sgd'}  \n0.7544751846368469 {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'solver': 'adam'}  \n1.2809553313236808 {'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'solver': 'sgd'}  \n0.7334956530313408 {'hidden_layer_sizes': (200,), 'learning_rate_init': 0.0001, 'solver': 'adam'  }  \n0.8997648134800114 {'hidden_layer_sizes': (200,), 'learning_rate_init': 0.001, 'solver': 'sgd'}  \n0.712455326734661 {'hidden_layer_sizes': (200,), 'learning_rate_init': 0.001, 'solver': 'adam'}  \n1.2637811745483698 {'hidden_layer_sizes': (300,), 'learning_rate_init': 0.0001, 'solver': 'sgd'}  \n0.7171187204774808 {'hidden_layer_sizes': (300,), 'learning_rate_init': 0.0001, 'solver': 'adam'}  \n0.8738408800221489 {'hidden_layer_sizes': (300,), 'learning_rate_init': 0.001, 'solver': 'sgd'}  \n0.7087510439696313 {'hidden_layer_sizes': (300,), 'learning_rate_init': 0.001, 'solver': 'adam'}","d93d6569":"# 5. Evaluate Models","5753cb4f":"### Predict on Test Set","6a5d45cb":"KNN Accuracy: 0.9654761904761905","889116ea":"KNN Accuracy: 0.9630952380952381","31294245":"Converting to BW kinda overwrite Min-Max scaling effect. So these are totally different transformation, and clearly feature scaling is a better approach.","0b25cbb0":"## 7.3 Predict and Submit Results","c8a5cf16":"# 3. Data Preprocessing","355fd27c":"# 1. Introduction\n\nHello everyone! I started this kernel right after I finished reading on **Classification**, and since they say \"MNIST is the `hello world` of classification\", I jumped into this competition to have some hands on experience on that.\n\nThis kernel consists of *7 main parts*, and 5th and 6th are a bit interchangeble. I will try to build 4 different models to classify MNIST images, SVM, KNN, Random Decision Forest and a Neural Network.\n\nSo let's get to work!\n\nPS: I had to comment out many pieces of this notebook since I was not able to `Commit&Run` that way. I once run whole notebook on my local, so I copied the outputs for the parts that I have commented out. Feel free to remove the comments and run the code. ","04383ce8":"Random Forest Accuracy: 0.9166666666666666","c7feb3c8":"### 2.4 Understand Data","aff2934e":"### Random Forest","73852153":"# MNIST Classification","d095c063":"### 2.3 Split the Data\n\nAs I learned; we should always put our test set aside when we are exploring dataset, to prevent our brain to mislead us. Since we are trying to create a solution that generalizes and not memorizes, it is important to modify our data by looking at only to train set and not the test set. Test set should only be used for final evaluation.","688f4ce7":"Scores: [0.63807008 0.75385403 0.70411055 0.6401844  0.81735991 0.72976497\n 0.69986193 0.68349178 0.72595641 0.77671352]\n\nMean: 0.7169367594666385\n\nStandard deviation: 0.05380741584858098","812c581a":"### 3.1 Feature Scaling \/ Normalization","1a3502c8":"These are the accuracy results for the models in their base forms, I mean without any tuning, **RandomForest** and **Neural Network (MLP)** performed well.\n\nThanks to [archaeocharlie](https:\/\/www.kaggle.com\/archaeocharlie) I realised a different type of modification to apply and I changed the scale from grayscale to only black and white.\n\nI am going to apply this and check the results for all other models.","181261d8":"### Get the dataset again and normalize it","8f72805c":"1. Introduction  \n2. Data Exploration  \n    2.1 Load Data  \n    2.2 Check for null values  \n    2.3 Understand data\n3. Data Preprocessing  \n    3.1 Feature Scaling \/ Normalization  \n    3.2 Label Encoding  \n4. Build Models  \n    4.1  SVM  \n    4.2 KNeighbors  \n    4.3 Random Forest  \n    4.4 Neural Network\n5. Evaluate Models  \n    5.1 Cross Validation   \n6. Hyperparameter Tuning  \n7. Predict and Submit  \n    7.1 Confusion Matrix  \n    7.2 Precision, Recall and F1 Scores  \n    7.3 Predict and Submit Results","e3004c13":"## 5.1 Cross Validation","b0e94db2":"The brighter squares represents higher values, meaning higher error rates.  \n\nWe can see that most of the **3s are mis-classified as 5**, some of the **4s are mis-classified as 9s**, and some of **7s are mis-classified as 2**. To solve these issues we may add some new features:\n\nFor example the main difference betweeen 5 and 3 is the postiion of the line in between, in 3s it closer to the middle, in 5s it is a bit higher.  \n\nFor 4 and 9 we can check to see if there are complete circles, which indicates that is a 9.  \n\nFor 7 and 2, the additional line in the bottom is what makes the most of the difference.\n\nTo get a better understanding of the error we may examine the mis-classified examples to what might be the reason for the error.\n\n**PS: Increasing the total number of training samples for each digit would result in a increase in performance. We can also preprocess the images to make sure that they are not rotated and they fit well in the matrix.**","76eaf862":"## 4.1 SVM\n\nWe are going to create the SVM model.  \nWe are going to call `fit()` method with training data.\n\nSVM's SVC uses `One-versus-Rest\/All (OvA\/OvR)` by default, meaning that system trains 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score.\n\nSo to building 10 different classifiers going to take some time.","2c31726b":"Currently we can not talk about this graph so much, since these are the actual values and depending on the frequency of each digit this result may mislead us. It is better to look at the **error rates** and **not the actual number** of errors (mis-classified).\n\nBecause, if we had 1000 5s and 9999 1s, having 999 false classifications (errors) in 5s, and 1000 in 1s would be seen as \"We had less error in 5s\". But if we have used **rates** it would be clearer.","b1514fbc":"MLP Accuracy: 0.9595238095238096","189bd195":"## 7.2 Precision, Recall and F1 Scores","66462bbc":"It looks like only 5 is little less than 4000 and the rest is almost evenly distributed.  \n\n\nWe can move on.","54c2c425":"### Transform Images to Black and White","569c6f4a":"### SVM","4d7601cc":"### Random Forest","32de80c8":"Random Forest Accuracy: 0.9419047619047619","9460d246":"# 4. Build Model\n\nWe are going to build the base models first, then we are going to try to  `fine-tune` them.","a8d9213c":"See [Sklearn Documentation for precision_recall_fscore_support()](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n\nThe **precision** is the ratio `tp \/ (tp + fp)` where tp is the number of true positives and fp the number of false positives. _The precision is intuitively the ability of the classifier **not to label as positive a sample that is negative**._\n\nThe **recall** is the ratio `tp \/ (tp + fn)` where tp is the number of true positives and fn the number of false negatives. _The recall is intuitively the ability of the classifier **to find all the positive samples**._\n\nThe **F-beta score** can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its `best value at 1 and worst score at 0`.\n\nThe F-beta score weights recall more than precision by a factor of beta. `beta == 1.0` means `recall and precision are equally important.`","f753b5a5":"It seems like we do not have any missing values. Perfect!","efc19f4c":"## 4.2 KNeighbors\n\nWe are going to create a K-Nearest Neighbor Classifier.  \nWe are going to `fit()` the data to the model.  \n\nKNNs asks for a parameter `n_neighbors` which tells how many neighbor points should it check around it, and classify itself according to the ones that are closest to it.","97df7a05":"## 4.3 Random Forest\n\nWe are going to build the Random Forest classifier.\nWe are going to call `fit()` to train it.  \n\nRandom Forest is an ensemble machine learning algorithm, it trains many trees under the hood and the picks the one that performs the best. Random Forest has 2-3 parameters that we are going to tune and the better we tune it the better results we get.","451ad05b":"Working with numerical data that is in between `0-1` is more effective for most of the machine learning algortihms than `0-255`.  \nWe can easily scale our features to `0-1` range by dividing to `max` value (255).\n\nWe could use `MinMaxScaler` from `sklearn.preprocessing` but since the formula for that is `(x-min)\/(max-min)` and our `min` is 0, we could directly calculate `x\/max` and that is `x\/255`.  \n\nThis is going to give the same result. So let's do it!\n\n**PS: Do not forget to scale test and validation examples before prediction**","91b42fc8":"### Submit Results","83d3520c":"### Train The Best Model (MLP in this case)","c1ab0992":"### GridSearch","94de86ee":"MLP Accuracy: 0.9745238095238096","83b0759f":"{'hidden_layer_sizes': (300,), 'learning_rate_init': 0.001, 'solver': 'adam'}","51a852e2":"**Recep Inanc, BSc**","0132fd8a":"## 7.1 Confusion Matrix","670b7935":"I am a big fan of GridSearch! You create a set of parameter combinations and you run your model with each of them and get the best parameter combination for that model. So let's do it!","8878ca19":"### 2.1 Load Data"}}