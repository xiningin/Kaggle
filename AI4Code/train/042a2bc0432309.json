{"cell_type":{"c1b95a9e":"code","6ea64691":"code","1185b2bd":"code","f79c9d6b":"code","c2b9b37d":"code","94c58b77":"code","30381039":"code","79982650":"code","a979a76e":"code","2e0e3cc0":"code","6a146992":"code","02bf77c5":"code","1277c95b":"code","05dbe937":"code","a12f6440":"code","d19170cb":"code","c37b74a6":"code","ee675fdc":"code","89e796c5":"code","b67f5efe":"code","5167610e":"code","80b0b1c5":"code","adb3fea1":"code","cb3831e4":"code","60caf9a0":"code","82141333":"markdown","3717f9c4":"markdown","cb00a2fe":"markdown","f34f6e25":"markdown","4b3e0bb8":"markdown","c7d09688":"markdown","abb602fd":"markdown","db8a66e6":"markdown","13d828bd":"markdown","cbb99555":"markdown","4c070a85":"markdown","ac9e49df":"markdown","db1492d7":"markdown","945d6523":"markdown","53cd6cdc":"markdown","5601d0f8":"markdown","3148eedd":"markdown","6798f247":"markdown","a967073b":"markdown","23916f4b":"markdown","6640ceea":"markdown"},"source":{"c1b95a9e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nX = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')","6ea64691":"X.info()","1185b2bd":"numeric_cols_with_missing = [cname for cname in X.columns \n                if X[cname].dtype in ['int64', 'float64']\n                and (X[cname].isnull().any())]\n\nmissing_val_count_by_column = X[numeric_cols_with_missing].isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","f79c9d6b":"sns.kdeplot(data=X['LotFrontage'],shade=True)\nprint(X['LotFrontage'].min())","c2b9b37d":"sns.stripplot(data=X,x=X['MasVnrType'],y=X['MasVnrArea'])\nprint(X['MasVnrType'].value_counts())","94c58b77":"print(X['GarageCars'].value_counts())\nprint(X['GarageArea'].value_counts())","30381039":"sns.regplot(x=X['GarageYrBlt'],y=X['SalePrice'])","79982650":"categorical_cols_with_missing = [cname for cname in X.columns \n                if X[cname].dtype == \"object\"\n                and (X[cname].isnull().any())]\n\nmissing_val_count_by_column = X[categorical_cols_with_missing].isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","a979a76e":"from sklearn.impute import SimpleImputer\n\nnone_imp = SimpleImputer(strategy='constant',fill_value='None')\nmost_freq_imp = SimpleImputer(strategy='most_frequent')\nzero_imp = SimpleImputer(strategy='constant',fill_value=0.0)\nmedian_imp = SimpleImputer(strategy='median')\n\ndef impute(data,imput_type,column_name):\n    if(imput_type == 'zero'):\n        data[[column_name]] = zero_imp.fit_transform(data[[column_name]])\n    elif(imput_type == 'none'):\n        data[[column_name]] = none_imp.fit_transform(data[[column_name]])\n    elif(imput_type == 'most_freq'):\n        data[[column_name]] = most_freq_imp.fit_transform(data[[column_name]])\n    elif(imput_type == 'median'):\n        data[[column_name]] = median_imp.fit_transform(data[[column_name]])","2e0e3cc0":"fill_with_none = ('GarageType','GarageFinish','GarageQual','GarageCond','MasVnrType','Alley',\n                 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','PoolQC','Fence','MiscFeature')\nfill_with_most_freq = ('Electrical',)\nfill_with_zero = ('MasVnrArea',)\nfill_with_median = ('LotFrontage','GarageYrBlt')\nfor x in (X,X_test):\n    for col in (fill_with_none):\n        impute(x,'none',col)\n    for col in (fill_with_zero):\n        impute(x,'zero',col)\n    for col in (fill_with_most_freq):\n        impute(x,'most_freq',col)\n    for col in (fill_with_median):\n        impute(x,'median',col)","6a146992":"missing_val_count_by_column = X_test.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","02bf77c5":"for col in ('BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea'):\n    impute(X_test,'zero',col)\nfor col in ('SaleType','Exterior1st','Exterior2nd','MSZoning','Utilities','KitchenQual','Functional'):\n    impute(X_test,'most_freq',col)","1277c95b":"import category_encoders as ce\ncat_features = [cname for cname in X.columns\n                   if X[cname].dtype == 'object']\n    \ncount_enc = ce.CountEncoder(cols=cat_features)\ncount_enc.fit(X[cat_features])\n\nX = X.join(count_enc.transform(X[cat_features]).add_suffix('_count'))\nX_test = X_test.join(count_enc.transform(X_test[cat_features]).add_suffix('_count'))\n\ntarget_enc = ce.TargetEncoder(cols= cat_features)\ntarget_enc.fit(X=X[cat_features],y=X['SalePrice'])\n\nX = X.join(target_enc.transform(X[cat_features]).add_suffix('_target'))\nX_test = X_test.join(target_enc.transform(X_test[cat_features]).add_suffix('_target'))\n","05dbe937":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\nOH_cols_X = pd.DataFrame(OH_encoder.fit_transform(X[cat_features]))\nOH_cols_X_test = pd.DataFrame(OH_encoder.transform(X_test[cat_features]))\n\n# One-hot encoding removed index; put it back\nOH_cols_X.index = X.index\nOH_cols_X_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nX = X.drop(cat_features, axis=1)\nX_test = X_test.drop(cat_features, axis=1)\n\n# Add one-hot encoded columns to numerical features\nX = pd.concat([X, OH_cols_X], axis=1)\nX_test = pd.concat([X_test, OH_cols_X_test], axis=1)","a12f6440":"y =X[\"SalePrice\"]\nX = X.drop(\"SalePrice\",axis = 1)","d19170cb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X),index=X.index,columns=X.columns)\nX_test = pd.DataFrame(scaler.fit_transform(X_test),index=X_test.index,columns=X_test.columns)","c37b74a6":"import xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom numpy import sort\n\nregr = xgb.XGBRegressor(n_estimators = 1000,learning_rate=0.05)\n\nxgbSelector = xgb.XGBRegressor(n_estimators = 1000,learning_rate=0.05).fit(X,y)\nmodel = SelectFromModel(xgbSelector,prefit=True,threshold='median')\nX_new = model.transform(X)\n    \nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                        index=X.index,\n                                        columns=X.columns)\n    \ncols_to_keep = selected_features.columns[selected_features.var() != 0]\nX_med = X[cols_to_keep]\nX_test_med = X_test[cols_to_keep]\nscores = -1 * cross_val_score(regr,X_med,y,cv=5,scoring='neg_mean_absolute_error')\nprint(scores)","ee675fdc":"xgbSelector2 = xgb.XGBRegressor(n_estimators = 1000,learning_rate=0.05).fit(X,y)\nmodel = SelectFromModel(xgbSelector,prefit=True,threshold='mean')\nX_new = model.transform(X)\n    \nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                        index=X.index,\n                                        columns=X.columns)\n    \ncols_to_keep = selected_features.columns[selected_features.var() != 0]\nX_mean = X[cols_to_keep]\nX_test_mean = X_test[cols_to_keep]\nscores = -1 * cross_val_score(regr,X_mean,y,cv=5,scoring='neg_mean_absolute_error')\nprint(scores)\n\nprint(np.median(sort(xgbSelector.feature_importances_)))\nprint(np.mean(sort(xgbSelector.feature_importances_))) ","89e796c5":"X = X_med\nX_test = X_test_med","b67f5efe":"print(X.columns)","5167610e":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\nparam_distribs = {\n        'n_estimators': [600,700,800,900,1000],\n        'max_depth': [2,3,4,5,6],\n        'learning_rate': reciprocal(0.01,1),\n    }\n\nregressor = xgb.XGBRegressor()\n\nrnd_search = RandomizedSearchCV(regressor, param_distributions=param_distribs,\n                                n_iter=50, cv=5,scoring='neg_mean_absolute_error',\n                                verbose=2,random_state=42,n_jobs=-1)\nrnd_search.fit(X,y)","80b0b1c5":"from sklearn.model_selection import GridSearchCV\nparam_grid = [ {'learning_rate': [0.03,0.04,0.05,0.06],\n              'n_estimators': [800,900,1000],'max_depth': [3,4]}\n]\ngrid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_absolute_error', return_train_score=True, n_jobs =-1)\ngrid_search.fit(X,y)","adb3fea1":"print(rnd_search.best_params_)\nprint(rnd_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n#Let's pick the parameters found with the RandomSearchCV\n\n# best_parameters I found myself (learning_rate= 0.054,n_estimators=1000,max_depth= 3)\nfinal_regressor = xgb.XGBRegressor(learning_rate= 0.06,n_estimators=800,max_depth= 3)","cb3831e4":"scores = -1 * cross_val_score(final_regressor,X,y,cv=6,scoring='neg_mean_absolute_error')\nprint(scores)\nprint(\"Average MAE score (across experiments):\")\nprint(scores.mean())","60caf9a0":"final_regressor.fit(X,y)\npredictions = final_regressor.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                    'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)\n\nprint('Output produced')","82141333":"There is no 0 value and the feature Street: Type of road access to property includes 1460 non-null values,\ntherefore, the NA LotFrontage values are just missing values. So let's replace them by the median value.","3717f9c4":"Let's think about the categorical columns","cb00a2fe":"Let's consider first the missing numerical values","f34f6e25":"We define a function facilitating the imputing task :","4b3e0bb8":"We process to some feature scaling","c7d09688":"We will consider individually each feature, starting by LotFrontage; It is defined by\nLotFrontage: Linear feet of street connected to property; \nSince there the .txt describing the features do not give a us a explaination for the NA values,\n(it could be that the property is not connected to the street), we need to investigate the issue","abb602fd":"Drop the label from the data","db8a66e6":"We already know how to treat the GarageTypes features and the MasVnrType feature; And again the .txt indicates us which NA values are due to an absence of the element concerned by the feature In this case we fill the holes with 'None' values. Otherwise we fill them with the most frequent value","13d828bd":"Let's add 8 None values and fill the holes in MasVnrArea with the 0 value (since only 3 None values are different of 0)","cbb99555":"We use RandomizedSearchCV to guess the best XGBooster regressor","4c070a85":"We should transform these NA value into a NoGarage in categorical features","ac9e49df":"Fortunately there are not a lot of NA values and we know that they all correspond to missing values, for the categorical features. On the other side the NA values in Basement numerical features correspond to NA values (Absence of Basement) in the others Basement categorical features, same goes for the Garage numerical features, so we impute zeros.","db1492d7":"The test set may have NA values in totally different categories","945d6523":"We have to treat the eight NA values in MasVnrArea: Masonry veneer area in square feet feature\nThe MasVnrType: Masonry veneer type feature shows also 8 NA missing values.\n","53cd6cdc":"We process to some feature engineering; Count encoding, target enconding, OH encoding, feature selection, feature scaling.","5601d0f8":"It's the turn of the GarageYrBlt: Year garage was built feature to be investigated.\nThe others Garage values (showing 1379 non null values like GarageYrBlt) in the .txt mentions NA values as the absence of garage.\nBut GarageCars and GarageArea shows 1460 non null values. It is explained by the fact that there are 0 cars, and the area of an absent a garage is equal\nto zero","3148eedd":"Let's see how this regressor performs on X","6798f247":"We process now to a one-hot encoding of categorical values","a967073b":"Since the median value provides a much better performance :","23916f4b":"If we fill the GarageYrBlt NA values with zeros, it will make noise on the graph above and therefore, perturbate the machine's regression line and prediction. So lets pick the median value to fill the holes.","6640ceea":"Feature selection : at first we only keep the features whose important values are higher than the median of them. On a second time with replace the median by the mean and compare the two methods"}}