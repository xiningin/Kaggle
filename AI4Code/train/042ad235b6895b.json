{"cell_type":{"1182a5c8":"code","efde212c":"code","b79887f9":"code","11bd14a9":"code","5f6355f4":"code","4efbb9f6":"code","2fe90bf1":"code","fb6f25ee":"code","4be2d8a3":"code","0a30a652":"code","5824aa32":"code","7cf7ee0c":"code","8dde5c91":"code","87236fbb":"code","23b18d92":"code","a61fff21":"code","97baf62c":"code","b5e1d3e4":"code","634c69da":"code","b2dbe27b":"code","897ef1af":"code","34c6fca0":"code","40714b4b":"code","fd0d01bf":"code","a401cc0e":"code","e784b77a":"code","6ce24486":"code","eeb2029a":"code","05f718a0":"code","bcf7cc55":"code","5240ccac":"code","887475fc":"code","13b254da":"code","417d63a7":"code","3e31fb88":"code","84a51529":"code","b027e21f":"code","d15bbaf2":"code","c8f1dbf2":"code","fecc04da":"code","a4001492":"code","56eb1472":"code","fce70257":"code","7c8573a3":"markdown","bd40014f":"markdown","7fdfd2c5":"markdown","7f005563":"markdown","249b56a7":"markdown","73a2e7b1":"markdown","09f53c6b":"markdown","0e34e718":"markdown","995377ef":"markdown","cfc0e6b8":"markdown","f003d606":"markdown","86007671":"markdown","fa7bccdd":"markdown","c0a07fa3":"markdown","78022a19":"markdown","21db74ee":"markdown","771638b5":"markdown","df2515e0":"markdown","8bf9380c":"markdown","32646239":"markdown","65ba7ad8":"markdown","32be098a":"markdown","1b9fda62":"markdown","efeae825":"markdown","0162e1fe":"markdown","4d24961a":"markdown","0f6d0e6b":"markdown","7012950d":"markdown","f2dfc186":"markdown","7af67673":"markdown","1cf239e6":"markdown","89b4e840":"markdown","aac1114e":"markdown","ec77dec8":"markdown","9a84fcb4":"markdown","eb189c14":"markdown","8ec37e60":"markdown","1553cfd5":"markdown","6ec37424":"markdown","ad97d771":"markdown","18ade947":"markdown","b1364837":"markdown","98966a31":"markdown","e133f813":"markdown","ce4cd5dc":"markdown","4836c9ae":"markdown","6145697c":"markdown","8de8f160":"markdown","af00089f":"markdown","e6d30dd1":"markdown"},"source":{"1182a5c8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier","efde212c":"data= pd.read_csv('..\/input\/sonaralldata\/sonar.all-data.csv', header=None) \ndata.head()","b79887f9":"#missing value checking\ndata.isnull().sum().sum()","11bd14a9":"data.describe()","5f6355f4":"#target value\ndata.iloc[:,-1].unique()","4efbb9f6":"#Turning ROCK and MINE into 0 and 1 (binary)\ndata= data.replace(to_replace=\"R\", value=0, regex=True)\ndata= data.replace(to_replace=\"M\", value=1, regex=True)\ndata.head()","2fe90bf1":"#train-test data\nX= data.iloc[:,:-1]\ny= data.iloc[:,-1]\nX_train, X_test, y_train, y_test= train_test_split(X,y, test_size = 0.2, random_state=0)","fb6f25ee":"#visualizing for 2 Principal Components representing features\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2).fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(pca, y, random_state=0)\nplt.figure(dpi=120)\nplt.scatter(pca[y.values==0,0], pca[y.values==0,1], alpha=1, label='Rock', s=4)\nplt.scatter(pca[y.values==1,0], pca[y.values==1,1], alpha=1, label='Mine', s=4)\nplt.legend()\nplt.title('Sonar Data Set\\nFirst Two Principal Components')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.gca().set_aspect('equal')","4be2d8a3":"knn= KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nprint('train score: {:.4f}'.format(knn.score(X_train, y_train)))\nprint('test score: {:.4f}'.format(knn.score(X_test, y_test)))","0a30a652":"#scaling the data for better accuracy\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","5824aa32":"knn.fit(X_train_scaled, y_train)\nprint('train score: {:.4f}'.format(knn.score(X_train_scaled, y_train)))\nprint('test score: {:.4f}'.format(knn.score(X_test_scaled, y_test)))","7cf7ee0c":"knn_prediction = knn.predict(X_test_scaled)\ncm= confusion_matrix(y_test, knn_prediction)\nprint(cm)","8dde5c91":"#trying k values for best score\nscores=[]\nk_range = range(1,20)\nfor k in k_range:\n    knn= KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n    scores.append(knn.score(X_test_scaled, y_test))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.scatter(k_range, scores)\nplt.xticks([0,5,10,15,20])","87236fbb":"scores","23b18d92":"print(\"max score for k=\", scores.index(max(scores)) +1,\"\\n\",\"max score=\", max(scores))","a61fff21":"t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nplt.figure()\n\nfor s in t:\n\n    scores = []\n    for i in range(1,50):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n        knn.fit(X_train, y_train)\n        scores.append(knn.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\n\nplt.xlabel('Training set proportion (%)')\nplt.ylabel('accuracy')","97baf62c":"log= LogisticRegression().fit(X_train_scaled, y_train)\nprint('logreg train score: {:.3f}'.format(log.score(X_train_scaled, y_train)))\nprint('logreg test score: {:.3f}'.format(log.score(X_test_scaled, y_test)))","b5e1d3e4":"lr_prediction = log.predict(X_test_scaled)\ncm= confusion_matrix(y_test, lr_prediction)\nprint(cm)","634c69da":"#an alternative to visualize confusion matrix\ncm = confusion_matrix(y_test, log.predict(X_test_scaled))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(cm)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted Rock', 'Predicted Mine'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual Rock', 'Actual Mine'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontsize=14)\nplt.show()","b2dbe27b":"nb= GaussianNB().fit(X_train_scaled, y_train)\nprint('clf train score: {:.2f}'.format(nb.score(X_train_scaled, y_train)))\nprint('clf test score: {:.2f}'.format(nb.score(X_test_scaled, y_test)))","897ef1af":"nb_prediction= nb.predict(X_test_scaled)\ncm= confusion_matrix(y_test, nb_prediction)\nprint(cm)","34c6fca0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\nvifs=calc_vif(data)\nvifs[\"VIF\"].sort_values(ascending=False)","40714b4b":"svm= SVC().fit(X_train_scaled, y_train) \nprint('svm train score: {:.2f}'.format(svm.score(X_train_scaled, y_train)))\nprint('svm test score: {}'.format(svm.score(X_test_scaled, y_test)))","fd0d01bf":"svm_prediction = svm.predict(X_test_scaled)\ncm = confusion_matrix(y_test, svm_prediction)\nprint(cm)","a401cc0e":"svm_2= LinearSVC().fit(X_train_scaled, y_train)\nprint('svm train score: {}'.format(svm_2.score(X_train_scaled, y_train)))\nprint('svm test score: {}'.format(svm_2.score(X_test_scaled, y_test)))","e784b77a":"dt= DecisionTreeClassifier().fit(X_train_scaled, y_train) #maxdepth kac dallanma oldugu\nprint('clf train score: {:.2f}'.format(dt.score(X_train_scaled, y_train)))\nprint('clf test score: {:.2f}'.format(dt.score(X_test_scaled, y_test)))","6ce24486":"dt_prediction = dt.predict(X_test_scaled)\ncm = confusion_matrix(y_test, dt_prediction)\nprint(cm)","eeb2029a":"names= X_train.columns\nsorted(zip(map(lambda x: round(x, 4), dt.feature_importances_), names), reverse=True)[:10]","05f718a0":"Importance = pd.DataFrame({'Importance':dt.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance_nonzero = Importance[(Importance.T != 0).any()]\nImportance_nonzero.sort_values(by ='Importance',\n                      axis = 0,\n                      ascending = True).plot(kind = 'barh',\n                                            color = 'b')\n\nplt.xlabel('Variable Importance %')\nplt.ylabel(\"Column Name\")\nplt.gca().legend_ = None","bcf7cc55":"rf= RandomForestClassifier().fit(X_train_scaled, y_train)\nprint('clf train score: {:.2f}'.format(rf.score(X_train_scaled, y_train)))\nprint('clf test score: {:.2f}'.format(rf.score(X_test_scaled, y_test)))","5240ccac":"rf_prediction = rf.predict(X_test_scaled)\ncm = confusion_matrix(y_test,rf_prediction)\nprint(cm)","887475fc":"sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True)[:10]","13b254da":"Importance = pd.DataFrame({'Importance':rf.feature_importances_*100},\n                         index = X_train.columns)\n\n#filtering less than %2 importance to highlight the most important ones\nImportance_mostly = Importance[(Importance.T >= 2).any()]\nImportance_mostly.sort_values(by ='Importance',\n                      axis = 0,\n                      ascending = True).plot(kind = 'barh',\n                                            color = 'g')\n\nplt.xlabel('Variable Importance %')\nplt.ylabel(\"Column Name\")\nplt.gca().legend_ = None","417d63a7":"mlp = MLPClassifier(random_state=1, max_iter=300).fit(X_train_scaled, y_train)\nprint('clf train score: {:.2f}'.format(mlp.score(X_train_scaled, y_train)))\nprint('clf test score: {:.2f}'.format(mlp.score(X_test_scaled, y_test)))","3e31fb88":"mlp_prediction = mlp.predict(X_test_scaled)\ncm = confusion_matrix(y_test,mlp_prediction)\nprint(cm)","84a51529":"pr_dict = {'KNN' : knn_prediction,'Logistic Regression' : lr_prediction,'SVM' : svm_prediction,\n           'Decision Tree' : dt_prediction, 'Random Forest' : rf_prediction,'Naive Bayes' : nb_prediction, 'MLP' : mlp_prediction}\n\nall_predictions = pd.DataFrame(pr_dict)\n\nfinal_prediction = [] #final prediction list\n\nfor i in range(all_predictions.shape[0]):    \n    if all_predictions.mean(axis=1)[i] <= 0.5:\n        final_prediction.append(0)  #means rock\n    else:\n        final_prediction.append(1) #means mine\nall_predictions[\"final pred\"] = final_prediction\nall_predictions[\"real\"] = y_test.values\nall_predictions.head()","b027e21f":"cm = confusion_matrix(final_prediction, y_test)\nprint(cm)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, final_prediction))","d15bbaf2":"models= [knn,log,svm,dt,rf,nb,mlp]\nall_scores = pd.DataFrame(np.zeros((8,2)))\nall_scores.columns = [\"train score\", \"test score\"]\nall_scores.index = ['KNN', 'Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest',\n       'Naive Bayes', 'MLP', \"Final Pred\"]\ntrain_scores=[]\ntest_scores=[]\nfor model in models:\n    model.fit(X_train_scaled,y_train)\n    train_scores.append(model.score(X_train_scaled, y_train))\n    test_scores.append(model.score(X_test_scaled, y_test))\n#for final pred row\ntrain_scores.append(\"-\")\ntest_scores.append(accuracy_score(y_test, final_prediction))\nall_scores[\"train score\"]= train_scores\nall_scores[\"test score\"]= test_scores\nall_scores","c8f1dbf2":"all_scores[\"test score\"].sort_values(ascending=False)","fecc04da":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_validate\n\n# Define dictionary with performance metrics\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score),\n           'recall':make_scorer(recall_score), \n           'f1_score':make_scorer(f1_score)}\n\n# Import required libraries for machine learning classifiers\n\n# Instantiate the machine learning classifiers\nknn_model = KNeighborsClassifier()\nlog_model = LogisticRegression()\nsvc_model = SVC()\ndtr_model = DecisionTreeClassifier()\nrfc_model = RandomForestClassifier()\ngnb_model = GaussianNB()\nmlp_model = MLPClassifier()\n\n# Define the models evaluation function\ndef models_evaluation(X, y, folds):\n    \n    '''\n    X : data set features\n    y : data set target\n    folds : number of cross-validation folds\n    \n    '''\n    \n    # Perform cross-validation to each machine learning classifier\n    knn = cross_validate(knn_model, X, y, cv=folds, scoring=scoring)\n    log = cross_validate(log_model, X, y, cv=folds, scoring=scoring)\n    svc = cross_validate(svc_model, X, y, cv=folds, scoring=scoring)\n    dtr = cross_validate(dtr_model, X, y, cv=folds, scoring=scoring)\n    rfc = cross_validate(rfc_model, X, y, cv=folds, scoring=scoring)\n    gnb = cross_validate(gnb_model, X, y, cv=folds, scoring=scoring)\n    mlp = cross_validate(mlp_model, X, y, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({ 'KNN': [knn['test_accuracy'].mean(),\n                                                knn['test_precision'].mean(),\n                                                knn['test_recall'].mean(),\n                                                knn['test_f1_score'].mean()],\n                                        \n                                        'Logistic Regression':[log['test_accuracy'].mean(),\n                                                               log['test_precision'].mean(),\n                                                               log['test_recall'].mean(),\n                                                               log['test_f1_score'].mean()],\n                                       \n                                      'Support Vector Classifier':[svc['test_accuracy'].mean(),\n                                                                   svc['test_precision'].mean(),\n                                                                   svc['test_recall'].mean(),\n                                                                   svc['test_f1_score'].mean()],\n                                       \n                                      'Decision Tree':[dtr['test_accuracy'].mean(),\n                                                       dtr['test_precision'].mean(),\n                                                       dtr['test_recall'].mean(),\n                                                       dtr['test_f1_score'].mean()],\n                                       \n                                      'Random Forest':[rfc['test_accuracy'].mean(),\n                                                       rfc['test_precision'].mean(),\n                                                       rfc['test_recall'].mean(),\n                                                       rfc['test_f1_score'].mean()],\n                                       \n                                      'Gaussian Naive Bayes':[gnb['test_accuracy'].mean(),\n                                                              gnb['test_precision'].mean(),\n                                                              gnb['test_recall'].mean(),\n                                                              gnb['test_f1_score'].mean()],\n                                       \"MLP\": [mlp['test_accuracy'].mean(),\n                                                mlp['test_precision'].mean(),\n                                                mlp['test_recall'].mean(),\n                                                mlp['test_f1_score'].mean()]},\n                                       index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n                                      \n\n    # Add 'Best Score' column\n    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)\n  \n# Run models_evaluation function\nmodels_evaluation(X, y, 5)","a4001492":"#algorithm comparison with boxplots\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SVM', SVC()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('MLP', MLPClassifier()))\n\nresults = []\nnames = []\nfrom sklearn.model_selection import cross_val_score\nfor name, model in models:\n    kfold = 5\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)","56eb1472":"for i in range(len(results)):\n    print(names[i],results[i].mean())","fce70257":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\nplt.show()","7c8573a3":"## ML ALGORITHMS - Classification (Supervised L. Part 1)","bd40014f":"Final Comments:\nClassification is such a fun era to implement.\nTo choose the best one, we need to consider our data amount, our wanted speed and accuracy, and how much we want to be interpretable.\nThe best one is the most suitable one for our needs. It is not just about the score.\n\nOur Rocks or Mines dataset has all numeric features and it is not considered to be high dimensional. We can choose Logistic regression if we want more explainable results,because of LR is sensitive to outliers, we may need to do outlier detection and feature selection for better accuracy. Also, we can choose RF, MLP to compute fast, and if we can feed our data with more observations they can perform better. An intense preprocessing may heal our algorithms' performance. Hope these explanations can help to take appropriate actions.\n\nTake care of you & your algorithms :)","7fdfd2c5":"![svc.png](attachment:svc.png)","7f005563":"![svm2.png](attachment:svm2.png)","249b56a7":"Without train-test split, we can check the scores by using cross validation:","73a2e7b1":"These algorithms have not optimized parameters, this is just their raw functions. That's why deciding a classifier just by looking at this results are not reliable. Also, Logistic Regression, Decision Tree, Random Forest and MLP has 1.0 training score, so they are overfitting. Even if their test scores are higher, they might not be performed well on a different test set. Also Naive Bayes' assumption is not met in this data.","09f53c6b":"**Why you should use MLP?** <br>\n\n1- Having  a collection of classifiers is that each of those classifiers can engage themselves for finding different patterns in the data and since there are many such units, the network can learn a lot of features from the data. <br>\n2- A conventional classifier would use logistic regression (or a decision tree or an SVM) and would try to fit a linear model for classification. ANN is much better suited to generalize and give better predictions on test data. With enough data, generally better results are possible for ANN. <br>\n3- Very effective for high dimensionality problems.<br>\n4- Powerful tuning options to prevent over- and under-fitting.<br>\n5- Deep learning frameworks are readily available that do the work for you.<br>\n\n**Why you should not use MLP?** <br>\n\n1- Theoretically complex, difficult to implement. <br>\n2- This larger and more complex models typically require significant volumes of data, computation, and training time to learn. (For small data sets, the lbfgs solver tends to be faster, and find more effective weights.). <br>\n3- Careful pre-processing of the input data is needed, to help ensure fast, stable, meaningful solutions to finding the optimal set of weights. <br>\n4- When the features are of very different types, MLP is less of a good choice<br>","0e34e718":"**Why you should use RF?** <br>\n\n1- It can handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method (feature importances).<br>\n2- Like decision trees, it works well with data sets that have a mixture of feature types.<br>\n3- It has methods for balancing errors in data sets where classes are imbalanced.<br>\n4- It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing.<br>\n\n**Why you should not use RF?** <br>\n\n1- Random forest can feel like a black box approach for a statistical modelers, we have very little control on what the model does (can be less interpretable than an individual decision tree).<br>\n2- Training a large number of deep trees can have high computational costs and use a lot of memory.<br>","995377ef":"![nb.png](attachment:nb.png)","cfc0e6b8":"![knn.png](attachment:knn.png)","f003d606":"## DECISION TREE","86007671":"![](http:\/\/blogs.sas.com\/content\/subconsciousmusings\/files\/2017\/04\/machine-learning-cheet-sheet.png)","fa7bccdd":"In the graph, orange lines are median, little circles are outliers (high std).\nRandom Forest, MLP and Logistic Regression may be tried and tuned parameters after to prevent overfitting.","c0a07fa3":"## KNN","78022a19":"Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable.\nA binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function.","21db74ee":"**Why you should use LR?**\n\n1- It is easier to implement, interpret and very efficient to train. <br>\n2- Logistic Regression performs well when the dataset is linearly separable. <br>\n3- Multi-collinearity is not really an issue and can be countered with L2 regularization to an extent.<br>\n4- Logistic regression is less prone to over-fitting but it can overfit in high dimensional datasets. You should consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios.<br>\n5- No hyperparamer tunning needed except for the threshold of class labels which is usually chosen as 0.5.<br>\n\n**Why you should not use LR?**\n\n1- Main limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. (Can't learn non linear decision boundaries)<br>\n2- If the number of observations are lesser than the number of features, Logistic Regression should not be used, otherwise it may lead to overfit.<br>\n3- In LR, features need to be scaled and normalized.<br>\n4- LR is sensitive to the unusual observations: outliers, high leverage, and influential observations. <br>\n5- Appropriate features must be selected before fitting the model or as an alternative the Logistic Rgression model should be regularized with lasso to select the features.<br>","771638b5":"These different score types can be important by different tasks. It is all about our tolerance level for TP,TN,FP,FN scores.","df2515e0":"### importing data","8bf9380c":"## LOGISTIC REGRESSION","32646239":"When K = 1, the prediction is sensitive to noise, outliers, mislabeled data, and other sources of variation in individual data points.For larger values of K, the areas assigned to different classes are smoother and not as fragmented and more robust to noise in the individual points. But possibly with some mistakes, more mistakes in individual points. This is an example of what's known as the bias variance tradeoff.","65ba7ad8":"SVM uses a *kernel trick* to transform the input space to a higher dimensional space as shown on the right. The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x^2=y^2). Now you can easily segregate these points using linear separation.","32be098a":"A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. A diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.","1b9fda62":"Artificial neural networks are built of simple elements called neurons, which take in a real value, multiply it by a weight, and run it through a non-linear activation function. By constructing multiple layers of neurons, each of which receives part of the input variables, and then passes on its results to the next layers, the network can learn very complex functions. Theoretically, a neural network is capable of learning the shape of just any function, given enough computational power. <br>\nA multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.","efeae825":"## RANDOM FOREST","0162e1fe":"K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. ","4d24961a":"**Why you should use KNN?**\n\n1- It is very simple to understand and implement. <br>\n2- KNN is a non-parametric algorithm that don't make strong assumptions. By not making assumptions, they are free to learn any functional form from the training data. <br>\n3- It\u2019s an instance-based learning; k-NN is a memory-based approach. The classifier immediately adapts as we collect new training data. It allows the algorithm to respond quickly to changes in the input during real-time use. <br>\n4- Easy to implement for multi-class problem whereas K-NN adjust to it without any extra efforts. <br>\n5- KNN algorithm gives user the flexibility to choose distance.(Euclidean Distance, Hamming Distance, Manhattan Distance, Minkowski Distance) <br>\n6- It can learn nonlinear decision boundaries. (If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers.)\n\n**Why you should not use KNN?**\n\n1- As dataset grows efficiency or speed of algorithm declines very fast. <br>\n2- Choosing the optimal number of neighbors can be an issue. <br>\n3- KNN doesn\u2019t perform well on imbalanced data. <br>\n4- Very sensitive to outliers as it simply chose the neighbors based on distance criteria. <br>\n5- Necessary that features have the same scale, since absolute differences in features weight the same.","0f6d0e6b":"VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others. So NB assumption is not met. (need to fix this)","7012950d":"![Random-Forest-Algorithm.jpg](attachment:Random-Forest-Algorithm.jpg)","f2dfc186":"![nn.png](attachment:nn.png)","7af67673":"**Why you should use DT?** <br>\n\n1- They are easily visualized and interpreted (visiual outputs, feature selection etc.). <br>\n2- No need to do feature pre-processing or normalization.  <br>\n3- Works well with data sets that have a mixture of feature types (binary, categorical or continuous and with features that are on very different scales. If you deal with a problem where inputs are categorical\/discrete values, it is better to apply tree). <br>\n4- Missing values in attributes can be efficiently handled. <br>\n5- Linearity between independent and dependent variable is not a constraint for prediction. <br>\n\n**Why you should not use DT?** <br>\n\n1- They can still overfit, may not achieve the best generalization performance compared to other methods. (ensemble algorithms can be useful to overcome this issue)<br>\n2- The hierarchical structure of the tree causes results to be very unstable if data is altered slightly (it splits the data based on a single attribute).<br>\n3- Information gain for variables with a high number of subclasses gives a biased response for said attributes.<br>","1cf239e6":"Support Vector Machine(SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. It is mostly used in classification problems called SVC. It is good for image analysis tasks, such as image classification and handwritten digit recognition, also text mining tasks.<br>\nSVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.","89b4e840":"## Classifiers All Together","aac1114e":"### importing libraries","ec77dec8":"![nbb.png](attachment:nbb.png)","9a84fcb4":"## SUPPORT VECTOR MACHINE","eb189c14":"Putting all prediction for test data by all classifiers and making the final prediction by getting mean of all decisions.","8ec37e60":"NB Classifier is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. It is used for many NLP models.<br>\n\n*Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c):*<br>\nP(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).<br>\nP(c) is the prior probability of class.<br>\nP(x|c) is the likelihood which is the probability of predictor given class.<br>\nP(x) is the prior probability of predictor.<br>","1553cfd5":"## NAIVE BAYES","6ec37424":"**Why you should use SVM?** <br>\n\n1- SVM Classifiers offer good accuracy and perform faster prediction compared to Na\u00efve Bayes algorithm.<br>\n2- They also use less memory because they use a subset of training points in the decision phase. <br>\n3- SVM works well with a clear margin of separation and with high dimensional space.<br>\n\n**Why you should not use SVM?** <br>\n\n1- SVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Na\u00efve Bayes. <br>\n2- It works poorly with overlapping classes and is also sensitive to the type of kernel used.<br>\n3- The algorithm is prone for over-fitting, if the number of features is much greater than the number of samples.<br>\n4- SVMs do not directly provide probability estimates, which are desirable in most classification problems.","ad97d771":"Accuracy = TP+TN\/TP+FP+FN+TN <br>\nPrecision = TP\/TP+FP<br>\nRecall = TP\/TP+FN<br>\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)<br>","18ade947":"Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. <br>\nDecision Tree Classifier is a class capable of performing multi-class classification on a dataset.","b1364837":"## Diving into Classification Algorithms","98966a31":"You can check these useful cheatsheet (https:\/\/blogs.sas.com\/content\/subconsciousmusings\/2017\/04\/12\/machine-learning-algorithm-use\/).","e133f813":"![dt.png](attachment:dt.png)","ce4cd5dc":"## NEURAL NETWORK (MLP Classifier)","4836c9ae":"There are three types of Naive Bayes model under the scikit-learn library:<br>\nGaussian: GNBs, continuous values associated with each feature are assumed to be distributed according to a Gaussian (normal) distribution.<br>\nMultinomial: It is used for discrete counts(ex. text classification problem).<br>\nBernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones).<br>\n\nWe use Gaussian NB for this dataset, because values are continous.","6145697c":"LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. But LinearSVC does not accept parameter kernel, as this is assumed to be linear. It also lacks some of the attributes of SVC, like support_.","8de8f160":"![lrr.png](attachment:lrr.png)","af00089f":"**Why you should use NB?** <br>\n\n1- It is easy and fast to predict class of test data set. It also perform well in multi class prediction.<br>\n2- When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.<br>\n3- It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).<br>\n\n**Why you should not use NB?** <br>\n\n1- Assumption of independent predictors, in real life is almost impossible that we get a set of predictors which are completely independent.<br>\n2- If the categorical variable has a category in the test data but not in the train data, the probability of this cat will be assigned zero and prediction is not possible.<br>","e6d30dd1":"**References\/Acknowlegments :**<br>\n\nApplied Machine Learning in Python course in Coursera : https:\/\/www.coursera.org\/learn\/python-machine-learning<br>\nKNN:https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm <br>\nhttps:\/\/www.fromthegenesis.com\/pros-and-cons-of-k-nearest-neighbors\/<br>\nLR:https:\/\/www.i2tutorials.com\/what-are-the-advantages-and-disadvantages-of-logistic-regression\/<br>\nhttps:\/\/www.edvancer.in\/logistic-regression-vs-decision-trees-vs-svm-part2\/<br>\nhttps:\/\/www.quora.com\/What-are-pros-and-cos-of-logistic-regression-and-random-forest<br>\nNurunnabi, Abdul & Ali, A & Imon, A. & Nasser, M.. (2012). Outlier Detection in Logistic Regression. 10.4018\/978-1-4666-1830-5.ch016. <br>\nDT:https:\/\/www.quora.com\/What-are-the-advantages-of-using-a-decision-tree-for-classification<br>\nhttps:\/\/www.researchgate.net\/post\/What_are_pros_and_cons_of_decision_tree_versus_other_classifier_as_KNN_SVM_NN<br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree<br>\nRF:https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forest<br>\nMLP:https:\/\/www.quora.com\/What-is-the-difference-between-an-artificial-neural-network-classifier-and-a-conventional-classifier<br>\nNB:https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/<br>\nhttps:\/\/missinglink.ai\/guides\/neural-network-concepts\/classification-neural-networks-neural-network-right-choice\/<br>\nhttps:\/\/medium.com\/swlh\/naive-bayes-its-mathematical-implementation-12f80319c333<br>\nSVC:https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python<br>\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/<br>\nOTHER RESOURCES:https:\/\/towardsdatascience.com\/machine-learning-classifiers-comparison-with-python-33149aecdbca<br>\nhttps:\/\/www.kaggle.com\/mattcarter865\/sonar-mines-vs-rocks<br>\nhttps:\/\/www.kaggle.com\/metetik\/classification-algorithms-comparison<br>\nhttps:\/\/www.kaggle.com\/aashita\/classification-algorithms<br>\nhttps:\/\/www.kaggle.com\/anniepyim\/essential-classification-algorithms-explained<br>\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/what-is-multicollinearity\/"}}