{"cell_type":{"ddd353cf":"code","f95e0e18":"code","848594d2":"code","7f79cee8":"code","02fe2c83":"code","dc34e1fa":"code","a018f634":"code","dba76de2":"code","f249dd51":"code","f3b09516":"code","fe693ff2":"code","9f161140":"code","41f07d02":"code","df60cb51":"code","f3dff242":"code","b357888e":"code","15f2c9c2":"code","3b6f290e":"code","eb176e85":"code","a2715612":"code","dbf3286d":"code","e40a0dc1":"code","267ba03d":"code","59084121":"code","fb5e4122":"code","35cb755a":"code","5eb44e0f":"code","50c20872":"code","c52027e9":"code","ef5d0ce7":"code","dafc5c45":"code","1a83b1d9":"code","f0f8b6ca":"code","38e3af6d":"code","38528287":"code","aeb4dbe6":"code","57ed518c":"code","d122c5c3":"code","6a938a1a":"code","29ddfb1a":"code","690521ba":"code","dbee22a3":"code","c9670f95":"code","02c198a7":"code","91c6148e":"code","f2fa59dd":"code","617eb8ad":"code","331c292d":"code","07f57530":"code","7ff24fdf":"code","5672b6f0":"code","237b54ba":"code","84b4f226":"code","06d9fd19":"code","a5e270ab":"markdown","d98c8c1c":"markdown","8926f2c3":"markdown","de3bc29d":"markdown","ba84d89a":"markdown","8b27db1e":"markdown","405674cd":"markdown","6d5c5eaf":"markdown","3d1492b5":"markdown","1b50aa20":"markdown","1acdbcc0":"markdown","79253eca":"markdown","ecc85050":"markdown","b126f7d4":"markdown","be351e83":"markdown","e572adb5":"markdown","7c9caeb8":"markdown","b7b5dd34":"markdown","39f16351":"markdown","840e67fe":"markdown","6899a482":"markdown"},"source":{"ddd353cf":"# Importing the Required Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","f95e0e18":"# Importing the train dataset\ndf_train = pd.read_csv(\"..\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_train_data.csv\/Webpages_Classification_train_data.csv\")\ndf_train.drop(columns = \"Unnamed: 0\", inplace = True)\n\n# ISO Alpha 3 code dataset\ncount = pd.read_csv('..\/input\/iso-alpha-3\/tableconvert_csv_pkcsig.csv')\n\n# Importing the test dataset\ndf_test = pd.read_csv(\"..\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_test_data.csv\/Webpages_Classification_test_data.csv\")\ndf_test.drop(columns = \"Unnamed: 0\", inplace = True)","848594d2":"df_train.head()","7f79cee8":"df_test.head()","02fe2c83":"count.head()","dc34e1fa":"print (\"The shape of the train dataset : \", df_train.shape)\nprint (\"The shape of the test dataset : \", df_test.shape)","a018f634":"# Adding a feature which is the ISO_ALPHA_3 code of the countries\ncountries = dict(zip(count['Country'], count['Alpha-3 code']))\n\ndf_train['iso_3'] = df_train['geo_loc']\ndf_train['iso_3'].replace(countries, inplace = True)\ndf_train.head()","dba76de2":"# Doing some preprocessing before Exploratory Data Analysis to get the best DA\ndf_train.https.replace({'yes' : 'HTTPS', 'no' : 'HTTP'}, inplace = True)\ndf_train.head()","f249dd51":"# Class with some preprocessing functions to create some features\nclass preproc:\n    \n    # Counting the Special Characters in the content\n    def count_special(string):\n        count = 0\n        for char in string:\n            if not(char.islower()) and not(char.isupper()) and not(char.isdigit()):\n                if char != ' ':\n                    count += 1\n        return count\n    \n    # Identifying the type of network [A, B, C]\n    def network_type(ip):\n        ip_str = ip.split(\".\")\n        ip = [int(x) for x in ip_str]\n\n        if ip[0]>=0 and ip[0]<=127:\n            return (ip_str[0], \"A\")\n        elif ip[0]>=128 and ip[0]<=191:\n            return (\".\".join(ip_str[0:2]), \"B\")\n        else:\n            return (\".\".join(ip_str[0:3]), \"C\")","f3b09516":"# Adding Feature that shows the Network type\ndf_train['Network']= df_train['ip_add'].apply(lambda x : preproc.network_type(x))\ndf_train['net_part'], df_train['net_type'] = zip(*df_train.Network)\ndf_train.drop(columns = ['Network'], inplace = True)\n\n# Adding Feature that shows the Number of Special Character in the Content\ndf_train['special_char'] = df_train['content'].apply(lambda x: preproc.count_special(x))\ndf_train.head()","fe693ff2":"# Length of the Content\ndf_train['content_len'] = df_train['content'].apply(lambda x: len(x))\ndf_train.head()","9f161140":"df_train.label.replace({'bad' : 'Malicious', 'good' : 'Benign'}, inplace = True)\ndf_train.head()","41f07d02":"# Setting the parameters\nplt.rcParams['figure.figsize'] = [18, 8]\nsns.set(style = 'white', font_scale = 1.3)\nfig, ax = plt.subplots(1, 2)\n\n# Bar graph\nbar = sns.countplot(df_train.label, data = df_train, ax = ax[0], palette = ['coral', 'mediumorchid'])\nbar.set(xlabel = 'Webpage Type', ylabel = 'Count')\nbar.set_title(\"Distribution of Malicious and Benign Webpage\", bbox={'facecolor':'0.8', 'pad':5})\n\n# Creating the labels for the piechart\ntypes = df_train['label'].value_counts()\nlabels = list(types.index)\naggregate = list(types.values)\n# percentage = [(x*100)\/sum(aggregate) for x in aggregate]\n# print (\"The percentages of Benign and Malicious Webpages are : \", percentage)\n\n# Plotting the Piechart to see the percentage distribution of the Webpages\nplt.rcParams.update({'font.size': 16})\nexplode = (0, 0.1)\nax[1].pie(aggregate, labels = labels, autopct='%1.2f%%', shadow=True, explode = explode, colors = ['coral', 'mediumorchid'])\nplt.title(\"Pie Chart for Malicious and Benign Webpage\", bbox={'facecolor':'0.8', 'pad':5})\nplt.legend(labels, loc = 'best')\nplt.tight_layout()\nplt.show()","df60cb51":"# Iso alpha 3 codes are necessary for choropleth plots [not particulary necessary though :)]\ndf = df_train.loc[df_train.label == 'Malicious', :]\n\nval = df.iso_3.value_counts()\nval = pd.DataFrame(val)\nval","f3dff242":"# Choropleth Map\nfig = go.Figure(data = go.Choropleth(\n    locations = val.index,\n    z = val.iso_3,\n#     text = val.index,\n    colorscale = 'reds',\n    autocolorscale = False,\n    reversescale = False,\n    marker_line_color = 'darkgray',\n    marker_line_width = 0.5,\n    colorbar_title = 'Number of Webpages',\n))\n\nfig.update_layout(\n    title_text = 'Malicious Webpages Around the World',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n#         projection_type = 'equirectangular'\n    ),\n    annotations = [dict(\n        x = 0.55,\n        y = 0.1,\n        xref = 'paper',\n        yref = 'paper',\n        text = 'Source: <a href=\"https:\/\/data.mendeley.com\/datasets\/gdx3pkwp47\/2\">\\\n            Dataset [Mendeley Data] <\/a>',\n        showarrow = False\n    )]\n)\n\nfig.show()","b357888e":"df = df_train.loc[df_train.label == 'Benign', :]\n\nval = df.iso_3.value_counts()\nval = pd.DataFrame(val)\nval","15f2c9c2":"# Choropleth Map\nfig = go.Figure(data = go.Choropleth(\n    locations = val.index,\n    z = val.iso_3,\n#     text = val.index,\n    colorscale = 'blues',\n    autocolorscale = False,\n    reversescale = False,\n    marker_line_color = 'darkgray',\n    marker_line_width = 0.5,\n    colorbar_title = 'Number of Webpages',\n))\n\nfig.update_layout(\n    title_text = 'Benign Webpages Around the World',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n#         projection_type = 'equirectangular'\n    ),\n    annotations = [dict(\n        x = 0.55,\n        y = 0.1,\n        xref = 'paper',\n        yref = 'paper',\n        text = 'Source: <a href=\"https:\/\/data.mendeley.com\/datasets\/gdx3pkwp47\/2\">\\\n            Dataset [Mendeley Data] <\/a>',\n        showarrow = False\n    )]\n)\n\nfig.show()","3b6f290e":"# Segregating the dataset for easy ploting\ndf_mal = df_train.loc[df_train.label == 'Malicious', :]\ndf_ben = df_train.loc[df_train.label == 'Benign', :]","eb176e85":"plt.rcParams['figure.figsize'] = [15, 7]\n\nfig, ax = plt.subplots(2, 2)\n\nmal = sns.distplot(df_mal['url_len'], color = 'r', hist = True, rug = False, kde = False, ax = ax[0,0])\nmal.set(title = 'Malicious Webpage URL length distribution')\nben = sns.distplot(df_ben['url_len'], color = 'b', hist = True, rug = False, kde = False, ax = ax[0,1])\nben.set(title = 'Benign Webpage URL length distribution')\n\nsns.kdeplot(df_mal['url_len'], color = 'r', fill = True, ax = ax[1,0])\nsns.kdeplot(df_ben['url_len'], color = 'b', fill = True, ax = ax[1,1])\n\nplt.tight_layout()\nplt.show()","a2715612":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nmal_tld = pd.DataFrame(df_mal.tld.value_counts()[:10])\nben_tld = pd.DataFrame(df_ben.tld.value_counts()[:10])\n\nmal = go.Bar(y = mal_tld.tld, x = mal_tld.index.tolist(), text = mal_tld.tld, marker_color = 'indianred')\nben = go.Bar(y = ben_tld.tld, x = ben_tld.index.tolist(), text = ben_tld.tld, marker_color = 'lightsalmon')\n\nfig = make_subplots(\n    rows = 2, cols = 1, subplot_titles = (\"Most occuring Top Level Domain in Malicious Webpages\", \"Most occuring Top Level Domain in Benign Webpages\"),\n    specs = [[{'type' : 'bar'}], [{'type' : 'bar'}]]\n)\n\nfig.append_trace(mal, row = 1, col = 1)\nfig.append_trace(ben, row = 2, col = 1)\nfig.update_traces(texttemplate = '%{text:.2s}', textposition = 'outside')\nfig.update_layout(uniformtext_minsize = 10, uniformtext_mode = 'hide', width = 1200, height = 900)\n\nfig.show()","dbf3286d":"plt.rcParams['figure.figsize'] = [20, 10]\nsns.set(style = 'whitegrid')\n\nfig, ax = plt.subplots(1, 2)\nkde = sns.kdeplot(df_train.content_len, data = df_train, hue = 'label', vertical = False, fill = True, palette = ['seagreen', 'darkred'], ax = ax[0])\nkde.set(title = 'Kernel Distribution Estimation of Content Length', xlabel = 'Content Length')\n\nvio = sns.violinplot(y = df_train.content_len, x = df_train.label, data = df_train, palette = ['seagreen', 'darkred'], ax = ax[1])\nvio.set(title = 'Violin Plot of Content Length', ylabel = 'Content Length', xlabel = 'Webpage Type')\n\nprint (\"Mean Content Length of Malicious Webpage : \", df_train.loc[df_train.label == 'Malicious', 'content_len'].mean())\nprint (\"Mean Content Length of Benign Webpage    : \", df_train.loc[df_train.label == 'Benign', 'content_len'].mean())\nprint ()","e40a0dc1":"# js_obf_len\nfig, ax = plt.subplots(2, 2)\n\nmal = sns.distplot(df_mal.js_obf_len, hist = True, rug = False, kde = False, color = 'r', ax = ax[0,0])\nmal.set(title = 'Obf JS Length Distribution for Malicious Webpages')\nben = sns.distplot(df_ben.js_obf_len, hist = True, rug = False, kde = False, color = 'b', ax = ax[0,1])\nben.set(title = 'Obf JS Length Distribution for Benign Webpages')\n\nsns.kdeplot(df_mal.js_obf_len, data = df_train, fill = True, color = 'r', ax = ax[1,0])\nsns.kdeplot(df_ben.js_obf_len, data = df_train, fill = True, color = 'b', ax = ax[1,1])","267ba03d":"# js_len\nfig, ax = plt.subplots(2, 2)\n\nmal = sns.distplot(df_mal.js_len, hist = True, rug = False, kde = False, color = 'r', ax = ax[0,0])\nmal.set(title = 'JS Length Distribution for Malicious Webpages')\nben = sns.distplot(df_ben.js_len, hist = True, rug = False, kde = False, color = 'b', ax = ax[0,1])\nben.set(title = 'JS Length Distribution for Benign Webpages')\n\nsns.kdeplot(df_mal.js_len, data = df_train, fill = True, color = 'r', ax = ax[1,0])\nsns.kdeplot(df_ben.js_len, data = df_train, fill = True, color = 'b', ax = ax[1,1])","59084121":"# Violin Plot showing the relation\nplt.rcParams['figure.figsize'] = [18, 8]\nfig, ax = plt.subplots(1, 2)\n\nPAG = sns.violinplot(x = df_train.label, y = df_train.js_obf_len, data = df_train, ax = ax[0])\nPAG.set(title = 'Violin Plot for Obf JS length', xlabel = 'Webpage Type')\nPAG_ = sns.violinplot(x = df_train.label, y = df_train.js_len, data = df_train, ax = ax[1])\nPAG_.set(title = 'Violin Plot for JS length', xlabel = 'Webpage Type');","fb5e4122":"plt.rcParams['figure.figsize'] = [16, 8]\nsns.set(style = 'whitegrid', font_scale = 1.2)\n\nfig, ax = plt.subplots(1, 2)\nbar_1 = sns.countplot(x = 'https', data = df_mal, order = ['HTTPS', 'HTTP'], palette = 'Set2', ax = ax[0])\nbar_1.set(title = 'Bargraph for HTTPS Vs HTTP for Malicious Webpages', xlabel = 'Protocol')\n\nbar_2 = sns.countplot(x = 'https', data = df_ben, order = ['HTTPS', 'HTTP'], palette = 'Set2', ax = ax[1])\nbar_2.set(title = 'Bargraph for HTTPS Vs HTTP for Benign Webpages', xlabel = 'Protocol');","35cb755a":"con = pd.DataFrame(df_train.groupby('iso_3')['content_len'].mean())\ncon.head()","5eb44e0f":"# Choropleth Map\nfig = px.choropleth(con, locations = con.index.tolist(),\n                    color = \"content_len\",\n                    color_continuous_scale = px.colors.sequential.Viridis)\nfig.show()","50c20872":"plt.rcParams['figure.figsize'] = [16, 8]\nsns.set(style = 'whitegrid', font_scale = 1.2)\n\nfig, ax = plt.subplots(1, 2)\nbar_1 = sns.countplot(x = 'net_type', data = df_mal, order = ['A', 'B', 'C'], palette = 'YlOrBr', ax = ax[0])\nbar_1.set(title = 'Network types in Malicious Webpages', xlabel = 'Network Type')\n\nbar_2 = sns.countplot(x = 'net_type', data = df_ben, order = ['A', 'B', 'C'], palette = 'YlOrBr', ax = ax[1])\nbar_2.set(title = 'Network types in Benign Webpages', xlabel = 'Network Type');","c52027e9":"# These are the categorical features that needs to be converted into numeric features for modelling \na = df_train.select_dtypes('object').columns.tolist()\nls = [element for element in a if element not in ['content', 'url', 'ip_add', 'label', 'net_part', 'iso_3']]\nls","ef5d0ce7":"# This le_dict will save the Label Encoder Class so that the same Label Encoder instance can be used for the test dataset\nle_dict = {}\n\nfor feature in ls:\n    le = LabelEncoder()\n    le_dict[feature] = le\n    df_train[feature] = le.fit_transform(df_train[feature])\n\ndf_train.label.replace({'Malicious' : 1, 'Benign' : 0}, inplace = True)\ndf_train.head()","dafc5c45":"# The Final Features which are going to be used for training\ndf_train = df_train[['url_len', 'geo_loc', 'tld', 'who_is', 'https', 'js_len', 'js_obf_len', 'label', 'net_type', 'special_char', 'content_len']]\ndf_train.head()","1a83b1d9":"# Pearson Correlation Heatmap\nplt.rcParams['figure.figsize'] == [18, 16]\nsns.set(font_scale = 1)\n\nsns.heatmap(df_train.corr(method = 'pearson'), annot = True, cmap = \"YlGnBu\");","f0f8b6ca":"# Normalizing the 'content_len' and 'special_char' in training data\nss_dict = {}\n\nfor feature in ['content_len', 'special_char']:\n    ss = StandardScaler()\n    ss_fit = ss.fit(df_train[feature].values.reshape(-1, 1))\n    ss_dict[feature] = ss_fit\n    d = ss_fit.transform(df_train[feature].values.reshape(-1, 1))\n    df_train[feature] = pd.DataFrame(d, index = df_train.index, columns = [feature])\n\ndf_train.head()","38e3af6d":"# preprocessing the test dataset\n# Replacing yes to HTTPS and no to HTTP\ndf_test.https.replace({'yes' : 'HTTPS', 'no' : 'HTTP'}, inplace = True)\n\n# Replacing the label\ndf_test.label.replace({'bad' : 'Malicious', 'good' : 'Benign'}, inplace = True)","38528287":"# Adding Feature that shows the Network type\ndf_test['Network']= df_test['ip_add'].apply(lambda x : preproc.network_type(x))\ndf_test['net_part'], df_test['net_type'] = zip(*df_test.Network)\ndf_test.drop(columns = ['Network'], inplace = True)\n\n# Adding Feature that shows the Number of Special Character in the Content\ndf_test['special_char'] = df_test['content'].apply(lambda x: preproc.count_special(x))","aeb4dbe6":"# Length of the Content\ndf_test['content_len'] = df_test['content'].apply(lambda x: len(x))","57ed518c":"# Using the same label encoders for the features as used in the training dataset\nfor feature in ls:\n    le = le_dict[feature]\n    df_test[feature] = le.fit_transform(df_test[feature])\n\ndf_test.label.replace({'Malicious' : 1, 'Benign' : 0}, inplace = True)\ndf_test.head()","d122c5c3":"# Normalizing the 'content_len' and 'special_char' in testing data\nss_fit = ss_dict['content_len']\nd = ss_fit.transform(df_test['content_len'].values.reshape(-1, 1))\ndf_test['content_len'] = pd.DataFrame(d, index = df_test.index, columns = ['content_len'])\n\nss_fit = ss_dict['special_char']\nd = ss_fit.transform(df_test['special_char'].values.reshape(-1, 1))\ndf_test['special_char'] = pd.DataFrame(d, index = df_test.index, columns = ['special_char'])\n\ndf_test.head()","6a938a1a":"df_test = df_test[['url_len', 'geo_loc', 'tld', 'who_is', 'https', 'js_len', 'js_obf_len', 'label', 'net_type', 'special_char', 'content_len']]\ndf_test.head()","29ddfb1a":"# Configuration Class\nclass config:\n    BATCH_SIZE = 128\n    DEVICE =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    LEARNING_RATE = 2e-5\n    EPOCHS = 20","690521ba":"# Making the custom dataset for pytorch\nclass MaliciousBenignData(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.input = self.df.drop(columns = ['label']).values\n        self.target = self.df.label\n        \n    def __len__(self):\n        return (len(self.df))\n    \n    def __getitem__(self, idx):\n        return (torch.tensor(self.input[idx]), torch.tensor(self.target[idx]))","dbee22a3":"# Creating the dataloader for pytorch\ndef create_dataloader(df, batch_size):\n    cls = MaliciousBenignData(df)\n    return DataLoader(\n        cls,\n        batch_size = batch_size,\n        num_workers = 0\n    )\n\ndf_train_loader = create_dataloader(df_train, batch_size = config.BATCH_SIZE)\ndf_test_loader = create_dataloader(df_test, batch_size = 1) # Here for testing using the batch size as 1","c9670f95":"# DataLoader components\ndf_train_loader.__dict__","02c198a7":"# Making the DNN model\nclass dnn(nn.Module):\n    def __init__(self):\n        super(dnn, self).__init__()\n\n        self.fc1 = nn.Linear(10, 64)\n        self.fc2 = nn.Linear(64, 128)\n        self.fc3 = nn.Linear(128, 128)\n        self.out = nn.Linear(128, 1)\n\n        self.dropout1 = nn.Dropout(p = 0.2)        \n        self.dropout2 = nn.Dropout(p = 0.3)\n        self.batchn1 = nn.BatchNorm1d(num_features = 64)\n        self.batchn2 = nn.BatchNorm1d(num_features = 128)\n\n    def forward(self, inputs):\n\n        t = self.fc1(inputs)\n        t = F.relu(t)\n        t = self.batchn1(t)\n        t = self.dropout1(t)\n        t = self.fc2(t)\n        t = F.relu(t)\n        t = self.batchn2(t)\n        t = self.dropout2(t)\n        t = self.fc3(t)\n        t = F.relu(t)\n        t = self.out(t)\n\n        return t","91c6148e":"# Transfer the model on the device -- 'GPU' if available or Default 'CPU'\nmodel = dnn()\nmodel.to(config.DEVICE)\nprint (model)","f2fa59dd":"# Criterian and the Optimizer for the model\ncriterian = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr = config.LEARNING_RATE)","617eb8ad":"# Simple Binary Accuracy Function\ndef binary_acc(predictions, y_test):\n    y_pred = torch.round(torch.sigmoid(predictions))\n    correct = (y_pred == y_test).sum().float()\n    acc = torch.round((correct\/y_test.shape[0])*100)\n    return acc","331c292d":"# Training function\n\ndef train_model(model, device, data_loader, optimizer, criterian):\n    # Putting the model in training mode\n    model.train()\n\n    for epoch in range(1, config.EPOCHS+1):\n        epoch_loss = 0\n        epoch_acc = 0\n        for X, y in data_loader:\n\n            X = X.to(device)\n            y_ = torch.tensor(y.unsqueeze(1), dtype = torch.float32)\n            y = y_.to(device)\n\n            # Zeroing the gradient\n            optimizer.zero_grad()\n\n            predictions = model(X.float())\n\n            loss = criterian(predictions, y)\n            acc = binary_acc(predictions, y)\n\n            loss.backward() # Calculate Gradient\n            optimizer.step() # Updating Weights\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n        print (f\"Epoch -- {epoch} | Loss : {epoch_loss\/len(data_loader): .5f} | Accuracy : {epoch_acc\/len(data_loader): .5f}\")","07f57530":"# Evaluation Function\n\ndef eval_model(model, device, data_loader):\n    # Putting the model in evaluation mode\n    model.eval()\n\n    y_pred = []\n    y_test_al = []\n\n    with torch.no_grad():\n        for X_test, y_test in data_loader:\n            X_test = X_test.to(device)\n\n            predictions = model(X_test.float())\n            pred = torch.round(torch.sigmoid(predictions))\n\n            y_test_al.append(y_test.tolist())\n            y_pred.append(pred.tolist())\n\n        # Changing the Predictions into list \n        y_test_al = [ele[0] for ele in y_test_al]\n        y_pred = [int(ele[0][0]) for ele in y_pred] # the format of the prediction is [[[0]], [[1]]]\n\n        return (y_test_al, y_pred)","7ff24fdf":"# Training the Model\ntrain_model(model, config.DEVICE, df_train_loader, optimizer, criterian)","5672b6f0":"# Evaluating the model and getting the predictions\ny_test, preds = eval_model(model, config.DEVICE, df_test_loader)\nprint ('Predictions : ', preds[0:10])","237b54ba":"# Classification Report\ncls_report = metrics.classification_report(y_test, preds)\n\nprint (\"\")\nprint (f\"Accuracy : {metrics.accuracy_score(y_test, preds)*100 : .3f} %\") \nprint (\"\")\nprint (\"Classification Report : \")\nprint (cls_report)","84b4f226":"plt.rcParams['figure.figsize'] = [10, 7]\nsns.set(font_scale = 1.2)\n\n# Confusion Matrix\ncm = metrics.confusion_matrix(y_test, preds)\n\n# Plotting the Confusion Matrix\nax = sns.heatmap(cm, annot = True, cmap = 'YlGnBu')\nax.set(title = \"Confusion Matrix\", xlabel = 'Predicted Labels', ylabel = 'True Labels');","06d9fd19":"torch.save(model.state_dict(), 'model.pth')","a5e270ab":"### Choropleth Map showing the Benign Webpages","d98c8c1c":"### Bar graph showing the most used TLD in URLs over Malicious and Benign Webpages","8926f2c3":"#### Saving the model!!","de3bc29d":"Here, it can be seen that Malicious Webpages have more obfuscated javascript code, NOICE!","ba84d89a":"Hmm Interesting Malicious Webpages have large content length","8b27db1e":"### Distribution of Webpage types","405674cd":"#### Visit this website for more info on [Network Classes](https:\/\/docs.oracle.com\/cd\/E19504-01\/802-5753\/6i9g71m2o\/index.html#planning3-fig-11)","6d5c5eaf":"### HTTPS and HTTP protocols distribution","3d1492b5":"We can see that the dataset is highly skewed so choosing the evaluation metrics is an important step as only accuracy will be biased as if model predicts every webpage as Benign still the accuracy will be very high. So we'll focus on the confusion matrix, F1 Score, Precision and Recall.","1b50aa20":"### Content Length Distribution over Malicious and Benign Webpages","1acdbcc0":"## Exploratory Data Analysis","79253eca":"## DNN Model using Pytorch","ecc85050":"The Benign Webpages are more inclined to use HTTPS as it is more secure than the HTTP protocol and for Malicious Webpages, they uses HTTP more!!!","b126f7d4":"#### All Done, Training Time!!","be351e83":"### URL Length (url_len) Distribution","e572adb5":"### Network types over Malicious and Benign Webpages","7c9caeb8":"### Heatmap of Pearson Correlation","b7b5dd34":"## Preprocessing","39f16351":"### Average Content length of the webpages around the world","840e67fe":"### Choropleth Map showing the Malicious Webpages","6899a482":"### js_len and js_obf_len Distribution over Malicious and Benign Webpages"}}