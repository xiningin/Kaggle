{"cell_type":{"715fe725":"code","a5a26319":"code","7a009844":"code","c63f12f6":"code","0e1bff27":"code","fb1dc0e2":"code","cfca0952":"code","90a47e7b":"code","ed75d2d2":"code","a049dde7":"code","d2fb1c92":"code","105b4f76":"code","81ee47a7":"code","128ceffb":"code","bcaaaa94":"code","58911f0a":"code","34698025":"code","457345c9":"markdown","7ea99dd8":"markdown"},"source":{"715fe725":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5a26319":"from simple_pipeline import simple_pipeline","7a009844":"from complex_xgboost import complex_XGBoost_model","c63f12f6":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv', parse_dates=['date']).drop('row_id',axis=1)\nprint(df.shape)\ndf.head()","0e1bff27":"target = 'num_sold'\nmodeltype = 'Regression'","fb1dc0e2":"if isinstance(target, str):\n    cols = [x for x in list(df) if x not in [target]]\nelse:\n    cols = [x for x in list(df) if x not in target]\nX = df[cols]\ny = df[target]\ndf.dtypes","cfca0952":"###  find the y values that have low samples ####\nif modeltype != 'Regression':\n    low_counts = y.value_counts()[(y.value_counts()<=1).values].index\n    print(len(low_counts))\n    ## You need to remove those rows that have just one sample ##\n    X = X[~(y.isin(low_counts))]\n    y = y[~(y.isin(low_counts))]\nX.shape, y.shape","90a47e7b":"from sklearn.model_selection import train_test_split\nif modeltype == 'Regression':\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=999)\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,stratify=y, random_state=999)\nprint(X_train.shape, X_test.shape)","ed75d2d2":"### This returns a pipeline ## you need to fit it to see how well it performs on a hold out dataset\nsmp, mlp = simple_pipeline(X_train, y_train, target, with_model=False, date_to_string=False)","a049dde7":"X_XGB = smp.fit_transform(X)\nprint(X_XGB.shape)\nX_XGB.head(1)","d2fb1c92":"test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv', parse_dates=['date']).drop('row_id',axis=1)\nprint(test.shape)\ntest.head()","105b4f76":"X_XGB_test = smp.transform(test)\nprint(X_XGB_test.shape)\nX_XGB_test.head(1)","81ee47a7":"outputs = complex_XGBoost_model(x_train=X_XGB, y_train=y, x_test=X_XGB_test, log_y=False, \n                                GPU_flag=False, scaler='', enc_method='', n_splits=5, verbose=-1)","128ceffb":"y_pred = outputs[0]\ny_pred.shape","bcaaaa94":"subm = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\nprint(subm.shape)\nsubm.head()","58911f0a":"subm[target] = y_pred\nsubm.head()","34698025":"subm.to_csv('submission.csv', index=False)","457345c9":"from sklearn import set_config\n#### You can display your pipeline in a nice diagram if it is a Jupyter Notebook ##\nset_config(display=\"diagram\")\nmlp","7ea99dd8":"from sklearn.metrics import balanced_accuracy_score, mean_squared_error\nif modeltype == 'Regression':\n    print(np.sqrt(mean_squared_error(y_test, mlp.predict(X_test))))\nelse:\n    print(balanced_accuracy_score(y_test, mlp.predict(X_test)))"}}