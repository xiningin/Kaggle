{"cell_type":{"ecb22972":"code","d95f613c":"code","bc535bf5":"code","09a68856":"code","34d7a0c3":"code","04b61478":"code","6f726b41":"code","d5b42bf5":"code","23c378ce":"code","ab4808a9":"code","2f9cf709":"code","32072d49":"code","475d0a98":"code","8ff96b2b":"code","5c999b43":"code","83e25ca8":"code","f8abe4ce":"code","d6458675":"code","658313dc":"code","39639de2":"code","97c8aa0d":"code","00c60baf":"code","3a5e1e2f":"code","066a9226":"code","50e161eb":"code","80c55909":"code","d105dbb9":"code","00c4adab":"code","ae136c4a":"code","33fb1eb6":"markdown","9727dd90":"markdown","40989477":"markdown","9ecdf4b7":"markdown","2849cb69":"markdown","768eed0b":"markdown","12d07718":"markdown","f78255b4":"markdown","9c612060":"markdown","1cae962a":"markdown"},"source":{"ecb22972":"%cd ..\/input\/food-101\/food-101\/\n!ls food-101\/","d95f613c":"!ls food-101\/images\/","bc535bf5":"from collections import defaultdict\nfrom shutil import copy\nimport os\nimport gc\n\n# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('\/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","09a68856":"%cd \/\nprint(\"Creating train data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/food-101\/meta\/train.txt', '\/kaggle\/input\/food-101\/food-101\/food-101\/images', 'train')","34d7a0c3":"%cd \/\nprint(\"Creating test data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/food-101\/meta\/test.txt', '\/kaggle\/input\/food-101\/food-101\/food-101\/images', 'test')","04b61478":"print(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c\n\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","6f726b41":"from fastai.metrics import accuracy\nfrom fastai.vision import *\n\nBS = 16\nSEED = 678\nNUM_WORKERS = 0\n\nfrom pathlib import Path\npath = Path('\/')","d5b42bf5":"if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\ntfms = get_transforms(flip_vert=False, max_lighting=0.1, max_zoom=1.05, max_warp=0., xtra_tfms=[cutout()])\n\nsrc = (ImageList.from_folder(path\/'train')\n       .split_by_rand_pct(0.2)\n       .label_from_folder())\n\ndata = (src.add_test_folder(test_folder = path\/'test')              \n         .transform(tfms, size=128)\n         .databunch(num_workers=NUM_WORKERS,bs=BS)).normalize(imagenet_stats)","23c378ce":"data.show_batch(3)","ab4808a9":"gc.collect()\n\nlearner = cnn_learner(\n    data,\n    models.resnet50,\n    path=path,\n    metrics=[accuracy],\n    ps = 0.5\n)","2f9cf709":"learner.lr_find()\nlearner.recorder.plot()","32072d49":"lr = 1e-03\nlearner.fit_one_cycle(1, slice(lr))","475d0a98":"learner.save('stage-1-rn50')\nlearner.unfreeze()\nlearner.lr_find()\nlearner.recorder.plot()","8ff96b2b":"learner.fit_one_cycle(2, slice(1e-5, lr\/5))","5c999b43":"learner.save('stage-2-rn50')","83e25ca8":"data = (src.add_test_folder(test_folder = path\/'test')              \n         .transform(tfms, size=256)\n         .databunch(num_workers=NUM_WORKERS,bs=BS)).normalize(imagenet_stats)\n\nlearner.data = data","f8abe4ce":"learner.freeze()\nlearner.lr_find()\nlearner.recorder.plot()","d6458675":"lr=1e-3\/2\nlearner.fit_one_cycle(1, slice(lr))","658313dc":"learner.save('stage-1-256-rn50')\nlearner.unfreeze()\nlearner.fit_one_cycle(2, slice(1e-6, 1e-5\/5))","39639de2":"learner.recorder.plot_losses()\nlearner.save('stage-2-256-rn50')\nlearner.export()","97c8aa0d":"data = (src.add_test_folder(test_folder = path\/'test')              \n         .transform(tfms, size=512)\n         .databunch(num_workers=NUM_WORKERS,bs=BS)).normalize(imagenet_stats)\n\nlearner.data = data\n\nlearner.freeze()\nlearner.lr_find()\nlearner.recorder.plot()","00c60baf":"lr=1e-3\/2\nlearner.fit_one_cycle(1, slice(lr))","3a5e1e2f":"learner.save('stage-1-512-rn50')\nlearner.unfreeze()\nlearner.fit_one_cycle(2, slice(1e-6, 1e-3\/2))","066a9226":"learner.recorder.plot_losses()\nlearner.save('stage-2-512-rn50')\nlearner.export()","50e161eb":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nsrc = (ImageDataBunch.from_folder(path = path, train = 'train', valid = 'test')\n       .split_by_rand_pct(0.2)\n       .label_from_folder())\n\ndata = (src.transform(tfms, size=512)\n         .databunch(num_workers=NUM_WORKERS,bs=BS)).normalize(imagenet_stats)","80c55909":"learner.fit_one_cycle(1)","d105dbb9":"def predict_class(model, images, show = True):\n  for img in images:\n    im = open_image(img)                                     \n\n    pred,_,_ = model.predict(im)\n    if show:\n        plt.imshow(image2np(im.data))                           \n        plt.axis('off')\n        print(pred)\n        plt.show()","00c4adab":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# list all files in dir\nfiles = [y for x in os.listdir(\"\/test\/\") for y in os.listdir(os.path.join(\"\/test\/\", x)) if os.path.isfile(y)]\n\n# select 0.1 of the files randomly \nrandom_files = np.random.choice(files, int(len(files)*.1))","ae136c4a":"predict_class(learner, random_files, True )","33fb1eb6":"### Predictions","9727dd90":"**Test Data Accuracy**","40989477":"### Data Preparation","9ecdf4b7":"**Creating data bunch with size 128 x 128**","2849cb69":"# Food-101 :: [Challenge Link](https:\/\/fellowship.ai\/challenge)\n\n![image.png](attachment:image.png)\n\n\n### Problem Statement\n<p>\nWe introduce a challenging data set of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.\n<\/p>\n\n### Dataset\n\n[Training and Test dataset](http:\/\/data.vision.ee.ethz.ch\/cvl\/food-101.tar.gz) for the challenge. \n<br>\n\n### Evaluation Metric\n \n*** Accuracy ***","768eed0b":"**meta** folder contains the text files - train.txt and test.txt <br>\n**train.txt** contains the list of images that belong to training set <br>\n**test.txt** contains the list of images that belong to test set <br>\n**classes.txt** contains the list of all classes of food <br>","12d07718":"### Model","f78255b4":"**Creating data bunch with size 256 x 256. Feeding the data to the same model we have trained on 128 x 128 images.**","9c612060":"**Creating data bunch with size 512 x 512 (Original Image Size). Feeding the data to the same model we have trained on 128 x 128 & 256 x 256 images.**","1cae962a":"**images** folder contains 101 folders with 1000 images each. <br>\nFollowing Classes of Food items available"}}