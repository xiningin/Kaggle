{"cell_type":{"9634fa63":"code","7831cf29":"code","fbf1a9b3":"code","21da97e9":"code","8d40bf97":"code","6f88517e":"code","d9fe198c":"code","144a6f1c":"code","5c8a387b":"code","69ddf371":"code","121cd838":"code","b4de5291":"code","79afce67":"code","e4007b0a":"code","3d9d98fe":"code","5dd021f3":"code","38dda59d":"code","dfbcd031":"code","27265246":"code","5015cbeb":"code","7a90a701":"code","574f393b":"code","fbb5768d":"code","0dc313bd":"code","3f322d18":"code","f37570d9":"code","a49e410f":"code","ffd4bf17":"code","971d373c":"code","5cba8c39":"code","121e37c6":"code","f545e196":"code","75d03e52":"code","fa0f2d2c":"code","5d1d524f":"code","cb5e32b9":"code","6470e6bb":"code","35ffa36f":"code","07122e5e":"code","7f522541":"code","a532ab62":"code","49e26cac":"code","5f4bad2b":"code","c7ea8bed":"code","b75aa60e":"code","cc8c1874":"code","c294af64":"code","a6a3c20d":"code","0197b0c7":"code","6645d882":"code","ff1f197b":"code","2641a1e1":"code","c8e88718":"code","18a55fd5":"code","477d0983":"code","f4a3769d":"code","ff99bfb0":"code","6ad84f26":"code","a6975fa4":"code","36181942":"code","c4ad7bcf":"code","4e833239":"code","727f19aa":"code","5ecde81d":"code","ca160210":"code","f1eb8eeb":"code","194105b1":"code","e84c327a":"code","950df987":"code","960d7789":"code","e45c1547":"code","4368d5f5":"code","95044e2c":"code","5e84e366":"code","e74898da":"code","5eca63e4":"code","5178f62f":"code","a6e9b8f3":"code","ccf2c404":"code","e327237e":"code","d82dd6ba":"code","d54b8beb":"code","e36e4e7d":"code","4a27a852":"code","c97a1aaf":"code","ee133cc5":"code","6916897e":"code","4f8ffd3a":"code","63936505":"code","9a1e532a":"code","ad95b90e":"code","efaa1c80":"code","33297393":"code","ab9488aa":"code","d1ead783":"code","c69a8ddd":"code","3cb5ce6d":"markdown","81bbcc3f":"markdown","3e53cb8a":"markdown","7f59b4d7":"markdown","19c4cdd8":"markdown","276671db":"markdown","ba7ae61e":"markdown","f9a34a89":"markdown","73215016":"markdown","b793a4e6":"markdown","f3f0edb9":"markdown","57a94e66":"markdown","95cb784f":"markdown","6381f6f9":"markdown","0358cb9a":"markdown","ed654add":"markdown","f2176018":"markdown","aff434ed":"markdown","5cead1ae":"markdown","9a87108b":"markdown","ad01aab8":"markdown","85a80fbb":"markdown","b3296276":"markdown","2de5aff0":"markdown","1a61ea57":"markdown","b033d8d7":"markdown","7560a7e4":"markdown","de54af2f":"markdown","f15e7ee7":"markdown","964a7b4a":"markdown","6ffd993f":"markdown","d0f91618":"markdown","c3d17104":"markdown","56123829":"markdown","f955d188":"markdown","9bf3880d":"markdown","670f0fd1":"markdown","9f8db69b":"markdown","f0d4df64":"markdown","7b46935b":"markdown","fa080efa":"markdown","6f738c27":"markdown","c673eb52":"markdown","ea98a66e":"markdown","eb82cf6c":"markdown","c12249ce":"markdown","74f8e54d":"markdown","c081c043":"markdown","dbfcd054":"markdown","aabe80d0":"markdown","12be24d6":"markdown","4e93214e":"markdown","ffa40590":"markdown","a5bc60b5":"markdown","cf65fdcd":"markdown","622aec53":"markdown","6374d336":"markdown","6fbf2e4e":"markdown","2004dd8c":"markdown","dcb75143":"markdown","2971c4af":"markdown","05a08d66":"markdown","e3bff1fa":"markdown","b42360b5":"markdown","fb9323ad":"markdown","add9bf48":"markdown","0feba423":"markdown"},"source":{"9634fa63":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set_style(\"whitegrid\")","7831cf29":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","fbf1a9b3":"data.shape","21da97e9":"data.head()","8d40bf97":"data.info()","6f88517e":"pd.set_option('display.float','{:.2f}'.format)\ndata.describe()","d9fe198c":"data.target.value_counts()","144a6f1c":"disease = len(data[data['target'] == 1])\nno_disease = len(data[data['target']== 0])\n\nplt.figure(figsize=(12,6))\n\nlabels = 'Have heart disease','Have not heart disease'\nsizes = [disease,no_disease]\nexplode = (0.1, 0) \nplt.pie(sizes, explode=explode, labels=labels, colors=['orangered','skyblue'],\nautopct='%1.2f%%', shadow=True, startangle=90, textprops={'fontsize': 12})\nplt.axis('equal')\nplt.title('Percentage of target', size=16)\nplt.show()","5c8a387b":"data.isna().sum()","69ddf371":"qualitative = []\nquantitative = []\nfor feature in data.columns:\n    if len(data[feature].unique()) <= 8:\n        qualitative.append(feature)\n    else:\n        quantitative.append(feature)","121cd838":"qualitative","b4de5291":"qualitative","79afce67":"top = 15\ncorr = data.corr()\ntop15 = corr.nlargest(top, 'target')['target'].index\ncorr_top15 = data[top15].corr()\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_top15, square=True, ax=ax, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size':12})\nplt.title('Top correlated features of dataset', size=16)\nplt.show()","e4007b0a":"fig = plt.figure(figsize=(16,4))\n\nax1 = fig.add_subplot(121)\nsns.boxplot(y = data['age'], ax=ax1, color='orangered')\ndescribe = data['age'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(122)\nax2.axis('off')\nfont_size = 16\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of age', fontsize=16)\nplt.show()","3d9d98fe":"plt.figure(figsize=(15,8))\nsns.countplot(data['age'], hue=data['sex'], palette=['skyblue','orangered'], saturation=0.8)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Gender count', fontsize=16)\nplt.legend(loc='upper right', fontsize=16, labels=['Female', 'Male'])\nplt.text(30, 11,'Total male: {:.2f}%'. \n         format(((data['sex'].value_counts()[1])\/(len(data)))*100), fontsize=16)\nplt.text(30, 10.5,'Total female: {:.2f}%'. \n         format(((data['sex'].value_counts()[0])\/(len(data)))*100), fontsize=16)\nplt.show()","5dd021f3":"plt.figure(figsize=(12,4))\nlabels = ['female','male']\nsns.countplot(data['sex'], hue=data['target'], palette=['skyblue','orangered'], saturation=0.8)\nplt.xlabel('Sex')\nplt.ylabel('Count')\nplt.title('Target count in genders', fontsize=16)\nplt.legend(loc='upper right', fontsize=16, labels=['No disease', 'Disease'])\nplt.show()","38dda59d":"plt.figure(figsize=(12,6))\npalette=['skyblue','orangered']\ns1=sns.boxenplot(x=data.sex, y=data.age, hue=data.target, palette=palette, linewidth=3)\nhandles = s1.get_legend_handles_labels()[0]\ns1.legend(handles, ['No disease', 'Disease'])\ns1.set_title(\"Sex, age, target boxenplot\",fontsize=16)\nplt.show()","dfbcd031":"fig,ax=plt.subplots(figsize=(24,6))\npalette = ['skyblue','orangered']\n\nplt.subplot(131)\nx1=sns.countplot(x=data.cp,hue=data.target,palette=palette,linewidth=3)\nx1.set_title('Chest pain type vs heart disease',size=16)\nx1.legend(loc='upper right', fontsize=12, labels=['No disease', 'Disease'])\n\nplt.subplot(132)\nx2=sns.countplot(x=data.thal,hue=data.target,palette=palette,linewidth=3)\nx2.set_title('Thalassaemia vs heart disease',size=16)\nx2.legend(loc='upper left', fontsize=12, labels=['No disease', 'Disease'])\n\nplt.subplot(133)\nx3=sns.countplot(x=data.slope,hue=data.target,palette=palette,linewidth=3)\nx3.set_title('Slope of the peak exercise ST segment vs heart disease',size=16)\nx3.legend(loc='upper left', fontsize=12, labels=['No disease', 'Disease'])\n\nplt.show()","27265246":"fig,ax=plt.subplots(figsize=(24,6))\npalette = ['darkblue','darkred']\n\nplt.subplot(1, 3, 1)\ndata['bin_thalach']=pd.cut(data.thalach, bins=[80,100,125,150,175,200])\np1=sns.countplot(x=data.bin_thalach,hue=data.target,palette=palette,linewidth=3)\np1.set_title(\"Thalach vs heart disease\",size=16)\np1.legend(loc='upper left', fontsize=12, labels=['No disease', 'Disease'])\n\n\n\nplt.subplot(1, 3, 2)\ndata['bin_chol']=pd.cut(data.chol, bins=[100,150,200,250,300,350,400])\np2=sns.countplot(x=data.bin_chol,hue=data.target,palette=palette,linewidth=3)\np2.set_title(\"Cholesterol vs heart disease\",size=16)\np2.legend(loc='upper left', fontsize=12, labels=['No disease', 'Disease'])\n\n\n\nplt.subplot(1, 3, 3)\ndata['bin_trestbps']=pd.cut(data.trestbps, bins=[80,100,120,140,160,180,200])\np3=sns.countplot(x=data.bin_trestbps,hue=data.target,palette=palette,linewidth=3)\np3.set_title(\"Trestbps vs heart disease\",size=16)\np3.legend(loc='upper left', fontsize=12, labels=['No disease', 'Disease'])\n\n\n\nplt.show()","5015cbeb":"fig,ax=plt.subplots(figsize=(24,6))\n\nplt.subplot(121)\nold_bins = [0,1,2,3,4,5,6]\ndata['oldpeak_bin']=pd.cut(data.oldpeak, bins=old_bins)\no1=sns.countplot(x=data.oldpeak_bin,hue='target',data=data, palette='bright')\no1.legend(loc='upper right', fontsize=12, labels=['No disease', 'Disease'])\n\n\nplt.subplot(122)\no2 = sns.pointplot(x='slope',y='oldpeak',data=data,hue='target',palette='bright')\nhandles = o2.get_legend_handles_labels()[0]\no2.legend(handles, ['No disease', 'Disease'])\n\nplt.suptitle('Oldpeak, slope vs target', size = 22)\nplt.show()","7a90a701":"plt.figure(figsize=(24,6))\nz1=sns.pointplot(x=data.age, y=data.thalach, hue=data.target, palette='bright', linewidth=3)\nplt.title('Age, thalach vs target',size=22)\nplt.show()","574f393b":"#Dropping columns used only to plots.\ndata.drop(['bin_chol','bin_thalach','bin_trestbps','oldpeak_bin'],axis=1,inplace=True)","fbb5768d":"data[quantitative].head()","0dc313bd":"def iqr(df, column):\n  Q1 = np.percentile(df[column], 25)\n  Q3 = np.percentile(df[column], 75)\n  IQR = Q3 - Q1\n  outlier_step = 1.5 * IQR\n  outliers_index = df[(df[column] < Q1 - outlier_step) | (df[column] > Q3 + outlier_step)].index\n  return outliers_index","3f322d18":"outliers_index = iqr(data,'trestbps')\ndata.drop(outliers_index, inplace=True)\ndata.reset_index(drop=True, inplace=True)","f37570d9":"outliers_index = iqr(data,'chol')\ndata.drop(outliers_index, inplace=True)\ndata.reset_index(drop=True, inplace=True)","a49e410f":"outliers_index = iqr(data,'thalach')\ndata.drop(outliers_index, inplace=True)\ndata.reset_index(drop=True, inplace=True)","ffd4bf17":"outliers_index = iqr(data,'oldpeak')\ndata.drop(outliers_index, inplace=True)\ndata.reset_index(drop=True, inplace=True)","971d373c":"y = data['target']\nX = data.drop('target',axis=1)","5cba8c39":"qualitative.remove('target')\nX = pd.get_dummies(X, columns = qualitative)","121e37c6":"X.head()","f545e196":"X[quantitative] = StandardScaler().fit_transform(X[quantitative])","75d03e52":"X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=42)","fa0f2d2c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\nparams = {'n_neighbors':list(range(1,20)),\n    'p':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'leaf_size':list(range(1,20)),\n    'weights':['uniform', 'distance']}","5d1d524f":"knn_param = GridSearchCV(knn, params, cv=5, n_jobs=-1)","cb5e32b9":"knn_param.fit(X_train, y_train)\n#Best params selected by GridSearchCV\nknn_param.best_params_","6470e6bb":"predict = knn_param.predict(X_test)","35ffa36f":"knn_acc_train = knn_param.score(X_train, y_train)*100\nknn_acc_test = knn_param.score(X_test, y_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(knn_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(knn_acc_test))","07122e5e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,predict))","7f522541":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for K-NN')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","a532ab62":"from sklearn.metrics import roc_auc_score,roc_curve","49e26cac":"y_probabilities = knn_param.predict_proba(X_test)[:,1]","5f4bad2b":"false_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,y_probabilities)","c7ea8bed":"#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('ROC for K-NN')\nplt.plot(false_positive_rate_knn, true_positive_rate_knn, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","b75aa60e":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()","cc8c1874":"params = {'penalty':['l1','l2'],\n         'C':[0.01,0.1,1,10,100],\n         'class_weight':['balanced',None]}\n\nlog_param = GridSearchCV(log,param_grid=params,cv=10, verbose=0)","c294af64":"log_param.fit(X_train,y_train)\n#Best params selected by GridSearchCV\nlog_param.best_params_","a6a3c20d":"predict = log_param.predict(X_test)","0197b0c7":"log_acc_train = log_param.score(X_train, y_train)*100\nlog_acc_test = log_param.score(X_test, y_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(log_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(log_acc_test))","6645d882":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,predict))","ff1f197b":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Logistic Regression')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","2641a1e1":"y_probabilities = log_param.predict_proba(X_test)[:,1]","c8e88718":"false_positive_rate_log,true_positive_rate_log,threshold_log = roc_curve(y_test,y_probabilities)","18a55fd5":"#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('ROC for Logistic Regression')\nplt.plot(false_positive_rate_log, true_positive_rate_log, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","477d0983":"from sklearn.svm import SVC\n\nsvm_clf = SVC(probability=True, kernel='rbf', gamma=0.1, C=1.0)","f4a3769d":"params = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20), \n          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1), \n          \"kernel\":('linear', 'poly', 'rbf')}\n\nsvm_param = GridSearchCV(svm_clf, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")","ff99bfb0":"svm_param.fit(X_train, y_train)\n#Best params selected by GridSearchCV\nsvm_param.best_params_","6ad84f26":"predict = svm_param.predict(X_test)","a6975fa4":"svc_acc_train = svm_param.score(X_train, y_train)*100\nsvc_acc_test = svm_param.score(X_test, y_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(svc_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(svc_acc_test))","36181942":"print(classification_report(y_test,predict))","c4ad7bcf":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Supported Vector Classifier')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","4e833239":"y_probabilities = svm_param.predict_proba(X_test)[:,1]","727f19aa":"false_positive_rate_svc,true_positive_rate_svc,threshold_svc = roc_curve(y_test,y_probabilities)","5ecde81d":"#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('ROC for Supported Vector Classifier')\nplt.plot(false_positive_rate_svc, true_positive_rate_svc, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","ca160210":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier()","f1eb8eeb":"params = {\"criterion\":(\"gini\", \"entropy\"), \n          \"splitter\":(\"best\", \"random\"), \n          \"max_depth\":(list(range(1, 20))), \n          \"min_samples_split\":[2, 3, 4], \n          \"min_samples_leaf\":list(range(1, 20))}\n\ntree_param = GridSearchCV(tree_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3, iid=True)\ntree_param.fit(X_train, y_train)","194105b1":"best_params = tree_param.best_params_\n#Best params selected by GridSearchCV\nbest_params","e84c327a":"tree_param = DecisionTreeClassifier(**best_params)","950df987":"tree_param.fit(X_train, y_train)","960d7789":"predict = tree_param.predict(X_test)","e45c1547":"tree_acc_train = tree_param.score(X_train, y_train)*100\ntree_acc_test = tree_param.score(X_test, y_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(tree_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(tree_acc_test))","4368d5f5":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Decision Tree')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","95044e2c":"y_probabilities = tree_param.predict_proba(X_test)[:,1]","5e84e366":"false_positive_rate_tree, true_positive_rate_tree, threshold_tree = roc_curve(y_test,y_probabilities)","e74898da":"#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_tree, true_positive_rate_tree, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","5eca63e4":"from xgboost.sklearn import XGBClassifier  \n\nxgb = XGBClassifier()","5178f62f":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.25, 0.5, 0.75, 0.99]\nlearning_rate = [0.05, 0.1, 0.15, 0.20]\nmin_child_weight = [1, 2, 3, 4]\n\nparams = {'n_estimators': n_estimators, 'max_depth': max_depth,\n    'learning_rate' : learning_rate, 'min_child_weight' : min_child_weight, \n    'booster' : booster, 'base_score' : base_score}","a6e9b8f3":"xgb_cv = GridSearchCV(xgb, params, cv=5, scoring = 'accuracy',n_jobs =-1, verbose=0)","ccf2c404":"xgb_cv.fit(X_train, y_train)","e327237e":"best_params = xgb_cv.best_params_\n#Best params selected by GridSearchCV\nbest_params","d82dd6ba":"xgb = XGBClassifier(**best_params, silent=1)\nxgb.fit(X_train, y_train)","d54b8beb":"predict = xgb.predict(X_test)","e36e4e7d":"xgb_acc_train = xgb.score(X_train, y_train)*100\nxgb_acc_test = xgb.score(X_test, y_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(xgb_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(xgb_acc_test))","4a27a852":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for XGB Classifier')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","c97a1aaf":"### ROC Curve for XGB Classifier\n\ny_probabilities = xgb.predict_proba(X_test)[:,1]\n\nfalse_positive_rate_xgb, true_positive_rate_xgb, threshold_xgb = roc_curve(y_test,y_probabilities)\n\n#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('ROC for XGB Classifier')\nplt.plot(false_positive_rate_xgb, true_positive_rate_xgb, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","ee133cc5":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers","6916897e":"model = keras.Sequential([\n        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n        layers.Dropout(0.4),\n        layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n        layers.Dense(1,activation='sigmoid')\n])","4f8ffd3a":"model.compile(loss='binary_crossentropy', optimizer='adam',metrics='accuracy')","63936505":"print(\"Fit model on training data\")\nhistory = model.fit(X_train, y_train, batch_size=30, epochs=100, validation_split = 0.2, verbose=1)","9a1e532a":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nax1.set_title('Model Accuracy')\nax1.set_ylabel('Accuracy')\nax1.set_xlabel('Epoch')\nax1.legend(['Train', 'Test'])\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nax2.set_title('Model Loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'test'])\nplt.suptitle(\"Model Accuracy & Loss\",fontsize=16)\n\nplt.show()","ad95b90e":"predict_test = model.predict_classes(X_test)\npredict_train = model.predict_classes(X_train)","efaa1c80":"nn_acc_train = accuracy_score(y_train, predict_train)*100\nnn_acc_test = accuracy_score(y_test, predict_test)*100\n\nprint(\"Train Accuracy {:.2f}%\".format(nn_acc_train))\nprint(\"Test Accuracy {:.2f}%\".format(nn_acc_test))","33297393":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test,predict_test)\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot = True, cmap = 'Blues',\n           fmt = 'g')\n\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Neural Network')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","ab9488aa":"### ROC Curve for Neural Network\n\ny_probabilities = model.predict_proba(X_test)\n\nfalse_positive_rate_nn, true_positive_rate_nn, threshold_nn = roc_curve(y_test,y_probabilities)\n\n#Plotting ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('ROC for Neural Network')\nplt.plot(false_positive_rate_nn, true_positive_rate_nn, linewidth=5, color='red')\nplt.plot([0,1],ls='--',linewidth=5)\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.text(0.2,0.6,'AUC: {:.2f}'.format(roc_auc_score(y_test,y_probabilities)),size= 16)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","d1ead783":"plt.figure(figsize = (16,10))\nplt.title('ROC Curves')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn,label='K-NN')\nplt.plot(false_positive_rate_log,true_positive_rate_log,label='Logistic Regression')\nplt.plot(false_positive_rate_svc,true_positive_rate_svc,label='Supported Vector Classifier')\nplt.plot(false_positive_rate_tree, true_positive_rate_tree,label='Decision Tree Classifier')\nplt.plot(false_positive_rate_xgb, true_positive_rate_xgb,label='XGB Classifier')\nplt.plot(false_positive_rate_nn, true_positive_rate_nn,label='Neural Network Classifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","c69a8ddd":"scores = pd.DataFrame(data=[[\"K-NN\", knn_acc_train, knn_acc_test],\n                            [\"Logistic Regression\", log_acc_train, log_acc_test],\n                            [\"Supported Vector Classifier\", svc_acc_train, svc_acc_test],\n                            [\"Decision Tree Classifier\", tree_acc_train, tree_acc_test],\n                            [\"XGB Classifier\", xgb_acc_train, xgb_acc_test],\n                            [\"Neural Network Classifier\", nn_acc_train, nn_acc_test]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nscores","3cb5ce6d":"# Decision Tree Classifier\nimplemented with GridSearchCV for hyperparameter tunning.","81bbcc3f":"#  XGBoost Classifier\nimplemented with GridSearchCV for hyperparameter tunning.","3e53cb8a":"### Predictions","7f59b4d7":"# 1.Exploratory Data Analysis (EDA)","19c4cdd8":"### Predictions","276671db":"## Corellation Matrix","ba7ae61e":"### ROC Curve for Logistic Regression","f9a34a89":"One of the key applications of machine learning methods is certainly health problems. The heaviest element in the application of black box models by scientists is to explain what affected the result. Since human life is the most important thing in medicine, data scientists must face up to the challenge and be able to answer the fundamental question - **why**?\n\nThe presented data includes numerous qualitative and quantitative features that will allow us to build the machine learning model. However as I mentioned, we work with human data, so the key element in building the model is to understand what really causes heart diseases. Starting with an extensive exploratory data analysing, I will try to answer some heart disease questions. Then the data will be cleaned and inserted into machine learning models .\n\n### Thus, this kernel is more focused for learning purposes and exploration. Please feel free to give advice, recommendations\/ better approaches or whatsover on the code below.","73215016":"### Predictions","b793a4e6":"### Features scaling","f3f0edb9":"## Key quantitative features vs heart disease\nBased on corrplot I took three most corellating quantitative features with target.","57a94e66":"# Data overview","95cb784f":"### Conclusions\n* As the age increases, the thalach slightly decreases\n* In almost every age, for disease samples thalach feature has a higher value.","6381f6f9":"### Conclusion\n* The data examined included mainly men, at the age mentioned. ","0358cb9a":"Great! Dataset is free from NaN values. ","ed654add":"### Split data into features and target","f2176018":"### Train test split","aff434ed":"## Age and thalach","5cead1ae":"# <p style=\"text-align:center;\"> **<font color='orange'>Feel free<\/font>** to ask below <\/p>","9a87108b":"### Confusion Matrix","ad01aab8":"### Confusion Matrix","85a80fbb":"### Report","b3296276":"## Sex, age vs target","2de5aff0":"### ROC Curve for Decision Tree","1a61ea57":"## Missing values","b033d8d7":"### Outliers in cholesterol","7560a7e4":"## If you find this kernel helpful, any **<font color='orange'>UPVOTES<\/font>** would be very much appreciated.","de54af2f":"# Supported Vector Classifier\nimplemented with GridSearchCV for hyperparameter tunning.","f15e7ee7":"#### Conclusion\n* On average, men start having heart problems at an earlier age than women.","964a7b4a":"### Predictions","6ffd993f":"## Oldpeak and Slope\nAccording to the high correlation score between oldpeak and slope, it is worth taking a closer look at this dependence.","d0f91618":"Columns description:\n\n* age - age in years \n* sex - (1 = male; 0 = female) \n* cp - chest pain type \n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) \n* chol - serum cholestoral in mg\/dl \n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n* restecg - resting electrocardiographic results \n* thalach - maximum heart rate achieved \n* exang - exercise induced angina (1 = yes; 0 = no) \n* oldpeak - ST depression induced by exercise relative to rest \n* slope - the slope of the peak exercise ST segment \n* ca - number of major vessels (0-3) colored by flourosopy \n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect \n* target - have disease or not (1=yes, 0=no)","c3d17104":"## ROC Curves for all models","56123829":"### Conclusions\n* Second type of chest pain most often accompanies heart problems.\n* Second thalassaemia type(fixed defect) most often accompanies heart problems.\n* Second slope  most often accompanies heart problems.","f955d188":"# Summary","9bf3880d":"### ROC Curve for K-Nearest Neighbor","670f0fd1":"### Outliers in trestbps","9f8db69b":"## Model scores","f0d4df64":"# K-Nearest Neighbor Algorithm\nimplemented with GridSearchCV for hyperparameter tunning.","7b46935b":"### Predictions","fa080efa":"### Conclusion\n* Based on boxplot we can see that data most often include people of age between 47-61 becouse, these ages lying between I and III percentile (IQR).","6f738c27":"### ROC Curve for Supported Vector Classifier","c673eb52":"## Info","ea98a66e":"### Confusion Matrix","eb82cf6c":"# Logistic Regression\nimplemented with GridSearchCV for hyperparameter tunning.","c12249ce":"# 2.Outliers\nTo detect outliers I will use the IQR method on quantitative features.","74f8e54d":"### Outliers in oldpeak","c081c043":"### Model","dbfcd054":"### Conclusions\n* With the value of oldpeak increases the rate of heart disease decreases\n* With increases of value of slope, heart disease people have lower oldpeak","aabe80d0":"## Categorical values\nApart from target, some of features contianed in the dataset are categorical. Let's discover them.","12be24d6":"## Key categorical features vs heart disease\nBased on corrplot I took three most corellating qualitative features with target.","4e93214e":"### Encode categorical variables","ffa40590":"## Target gender count","a5bc60b5":"## Age visualization","cf65fdcd":"# <p style=\"text-align:center;\">What causes heart problems\ud83d\udc94? <\/p>\n#  <p style=\"text-align:center;\"> Analysis and comprehensive classification <\/p>","622aec53":"### Confusion Matrix","6374d336":"# <p style=\"text-align:center;\">**<font color='orange'>Suggestions<\/font>** are welcome <\/p>","6fbf2e4e":"### Outliers in thalach","2004dd8c":"### Predictions","dcb75143":"# 4.Models","2971c4af":"### Conclusion\n* In the data under study, men more often had heart problems.","05a08d66":"## Target\n\n* 1 - (YES) have heart disease\n* 0 - (NO) have not heart disease","e3bff1fa":"### Conclusions\n* The most frequent heart rate(thalach) suggesting heart disease is 175.\n* Cholesterol levels suggestive of heart disease are most likely to be 250.\n* The most frequent resting blood pressure, which is associated with heart disease, is 140.","b42360b5":"### Confusion Matrix","fb9323ad":"# Neural Network","add9bf48":"<center><img src=\"https:\/\/img.webmd.com\/dtmcms\/live\/webmd\/consumer_assets\/site_images\/article_thumbnails\/slideshows\/did_you_know_this_could_lead_to_heart_disease_slideshow\/650x350_did_you_know_this_could_lead_to_heart_disease_slideshow.jpg\"><\/center>","0feba423":"# 3.Data preparation"}}