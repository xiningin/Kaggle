{"cell_type":{"78284132":"code","8116bd7d":"code","36bcfcc7":"code","dda3eef7":"code","6b7e1ef3":"code","6f200665":"code","b440c874":"code","c12b2506":"code","b1c53402":"code","bff4f3e6":"code","625619d5":"code","fd9ccb36":"code","f8e369d5":"code","1c37cb4c":"code","db62613c":"code","93b4b9da":"code","d83dfad1":"code","9ce8ab98":"code","084b6279":"code","eeda97ac":"code","a52be22b":"markdown","d0492072":"markdown","63294896":"markdown","76a44785":"markdown","00352f9f":"markdown","b32d58e9":"markdown"},"source":{"78284132":"#Using the GPU for faster computation\nimport sys\nprint(sys.version)\ndevice = 'cuda'","8116bd7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torchvision import transforms\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","36bcfcc7":"#Checking for GPU availability\nprint(torch.version.cuda)\nprint(torch.cuda.device_count())\nprint(torch.cuda.is_available())","dda3eef7":"\"\"\"\n#Loading the training data\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\"\"\"","6b7e1ef3":"class DatasetMNIST(Dataset):\n    \n    def __init__(self, file_path, transform=None):\n        self.data = pd.read_csv(file_path)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        # load image as ndarray type (Height * Width * Channels)\n        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n        # in this example, we use ToTensor(), so we define the numpy array like (H, W, C)\n        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28, 28, 1))\n        label = self.data.iloc[index, 0]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image, label\n","6f200665":"transform = transforms.Compose([transforms.ToPILImage(), transforms.RandomAffine(degrees = 2, translate = (0.02, 0.02), scale = (0.98, 1.02)), transforms.ToTensor()])","b440c874":"train_dataset = DatasetMNIST(\"..\/input\/digit-recognizer\/train.csv\", transform)\ntrain_loader = DataLoader(train_dataset, batch_size = 1, shuffle = True)\n\ntrain_transform_x = []\ntrain_transform_y = []\n\nfor xb, yb in train_loader:\n    train_transform_x.append(xb)\n    train_transform_y.append(yb)\n\ntrain_x = torch.cat(train_transform_x, dim=1)  \ntrain_y = torch.cat(train_transform_y, dim=0) \n    \ntrain_x = train_x.reshape(-1, 1, 28, 28)\ntrain_y = train_y.reshape(-1, 1)\n    \ntrain_x = train_x.numpy()\ntrain_y = train_y.numpy()\n    \ntrain_x = train_x\/255.0\n    \nprint(train_x.shape)\nprint(train_y.shape)","c12b2506":"random_seed = 1\n\nX_train, X_cv, Y_train, Y_cv = train_test_split(train_x, train_y, test_size = 0.2, random_state = random_seed)\n","b1c53402":"#Just checking the values with a random index\nX_train = X_train.reshape(-1, 28, 28)\nplt.imshow(X_train[6], cmap = 'gray')\nprint(Y_train[6])\nX_train = X_train.reshape(-1, 1, 28, 28)","bff4f3e6":"#Just checking to see if the relevant shapes are as expected\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_cv.shape)\nprint(Y_cv.shape)\n\n#isinstance(X_train, np.ndarray)\n#isinstance(test, np.ndarray)\n#isinstance(Y_train, np.ndarray)","625619d5":"#Converting numpy arrays to torch tensors\n\nX_train,Y_train, X_cv, Y_cv = map(torch.tensor, (X_train, Y_train, X_cv, Y_cv))","fd9ccb36":"train_ds = TensorDataset(X_train, Y_train)\ntrain_dl = DataLoader(train_ds, batch_size = 128, shuffle = True)\n\nvalid_ds = TensorDataset(X_cv, Y_cv)\nvalid_dl = DataLoader(valid_ds, batch_size = 256)","f8e369d5":"\nclass Mnist_cnn(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5, stride = 1, padding = 2)\n        self.conv1_bn = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size = 5, stride = 1, padding = 2)\n        self.conv2_bn = nn.BatchNorm2d(32)\n        self.conv2_drop = nn.Dropout2d(0.25)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n        self.conv3_bn = nn.BatchNorm2d(64)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n        self.conv4_bn = nn.BatchNorm2d(64)\n        self.conv4_drop = nn.Dropout2d(0.25)\n        self.fc1 = nn.Linear(3136, 256)\n        self.fc1_bn = nn.BatchNorm1d(256)\n        self.fc1_drop = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256, 10)\n    \n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = self.conv1_bn(xb)\n        xb = F.relu(self.conv2(xb))\n        xb = self.conv2_bn(xb)\n        xb = F.max_pool2d(xb, (2, 2))\n        xb = self.conv2_drop(xb)\n        xb = F.relu(self.conv3(xb))\n        xb = self.conv3_bn(xb)\n        xb = F.relu(self.conv4(xb))\n        xb = self.conv4_bn(xb)\n        xb = F.max_pool2d(xb, (2, 2), stride = 2)\n        xb = self.conv4_drop(xb)\n        xb = xb.view(-1, self.num_flat_features(xb))\n        xb = F.relu(self.fc1(xb))\n        xb = self.fc1_bn(xb)\n        xb = self.fc1_drop(xb)\n        xb = self.fc2(xb)\n        \n        return xb\n  \n    def num_flat_features(self, xb):\n        size = xb.size()[1:]\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n    \nmodel = Mnist_cnn()\nprint(model) \nmodel = model.float()\n\n#Passing the model to the GPU\nmodel.to(device)","1c37cb4c":"opt = optim.Adam(model.parameters(), lr = 0.001)\n\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience = 1, verbose = True)\n\nloss_func = nn.CrossEntropyLoss()","db62613c":"def fit(model, epochs, opt, loss_func, train_dl, valid_dl):\n\n    for epoch in range(epochs):\n        #Going into training mode\n        model.train()\n        \n        train_loss = 0\n        train_acc = 0\n       \n        for xb, yb in train_dl:\n            xb = xb.to(device)   #Passing the input mini-batch to the GPU\n            yb = yb.to(device)   #Passing the label mini-batch to the GPU\n            opt.zero_grad()      #Setting the grads to zero to avoid accumulation of gradients\n            out = model(xb.float())\n            loss = loss_func(out, yb.squeeze())    #Squeezing yb so it has dimensions (minibatch_size,)\n            train_loss += loss\n            train_pred = torch.argmax(out, dim = 1)\n            train_pred = train_pred.reshape(train_pred.size()[0], 1) #Setting train_pred to have shape (minibatch_size, 1)\n            train_acc += (train_pred == yb).float().mean()\n            \n            loss.backward()\n            opt.step()\n        \n        lr_scheduler.step(train_loss\/len(train_dl))   #Setting up lr decay  \n        \n        model.eval()            #Going into eval mode                            \n        with torch.no_grad():   #No backprop\n            valid_loss = 0\n            valid_acc = 0\n            \n            for xb, yb in valid_dl:\n                xb = xb.to(device)  \n                yb = yb.to(device)\n                cv_out = model(xb.float())\n                valid_loss += loss_func(cv_out, yb.squeeze())\n                valid_pred = torch.argmax(cv_out, dim = 1)\n                valid_pred = valid_pred.reshape(valid_pred.size()[0], 1)\n                valid_acc += (valid_pred == yb).float().mean()\n        \n        print(\"Epoch \", epoch, \" Training Loss: \", train_loss\/len(train_dl), \"CV Loss: \", valid_loss\/len(valid_dl))\n        print(\"Training Acc: \", train_acc\/len(train_dl), \"CV Acc: \", valid_acc\/len(valid_dl))","93b4b9da":"fit(model, 200, opt, loss_func, train_dl, valid_dl)   #Training for 200 epoch","d83dfad1":"#Deleting some variables of no use later to free some space\ndel train_dl, valid_dl, train_ds, valid_ds, X_train, Y_train, X_cv, Y_cv","9ce8ab98":"test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\ntest = test\/255.0\ntest = test.values.reshape(-1, 1, 28, 28)\ntest = torch.from_numpy(test)","084b6279":"with torch.no_grad():\n    test = test.to(device)    #Passing the entire test set to the GPU\n    test_out = model(test.float())\n    test_pred = torch.argmax(test_out, dim = 1)\n    test_pred = test_pred.reshape(test_pred.size()[0], 1)\n    test_pred_np = test_pred.cpu().numpy()   #Conversion of tensor to numpy array","eeda97ac":"test_pred_np = np.reshape(test_pred_np, test_pred_np.shape[0])  #Reshaping to expected 1D array\ntest_pred_pd = pd.Series(test_pred_np, name = \"Label\")          #Conversion to Pandas Dataframe\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), test_pred_pd], axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\", index=False)","a52be22b":"# Preprocessing of the Dataset\n\nSplitting the given data into the input pixels and the output labels followed by reshaping into image dimensions and converting from pandas dataframe to numpy arrays","d0492072":"# Preprocessing the Test Set\n\nLoading the test set at this time to optimize memory utilization.\nIt is followed by its normalization, reshaping into PyTorch image dimensions, conversion to a numpy array from Pandas Dataframe finally followed by its conversion into PyTorch tensor from the numpy array","63294896":"# Defining the Model\n\nConstructing the CNN by first passing the input through 2 convolutional layers followed by a maxpool layer. This is followed by using 2 more convolutional layers and a maxpool layer. Finally, it is passed through 2 fully connected layers. Dropout and batch normalization is implemented at intermediate stages to allow for the stability of shallower layers and to prevent the overfitting of the training set","76a44785":"# Using TensorDataset and DataLoader\nPyTorch implemented TensorDataset and DataLoader allows for automatically iterating over the relevant mini-batches making the code succinct and cleaner","00352f9f":"# Setting Optimization Hyperparameters and the Loss Function\n\nUsing Adam optimization with the standard learning rate of 0.001. Also, using learning rate decay to decrease the learning rate by the default factor of 0.1 if the cumulative training loss does not decrease after 2 consecutive epochs by setting the patience factor as 1. Also, using the standard cross entropy loss as the loss function","b32d58e9":"# Training Set Split\nSplitting the given training set into a training and a cross-validation set with 80% of the data being used for training and the remaining 20% for the development set"}}