{"cell_type":{"bd9cc2c4":"code","0b8831db":"code","a83d00c5":"code","42ee6bfd":"code","0075bd53":"code","a111c22e":"code","284f7974":"code","806911e2":"code","35520133":"code","f4b8db9b":"code","260cb2cc":"code","e54c7ade":"code","10c1f1ef":"code","c07f1605":"code","74b98803":"markdown","3081b17e":"markdown","e328bb95":"markdown","d02ea1b1":"markdown","64bcc731":"markdown","35ea5da8":"markdown","4e487ec7":"markdown","8642ca97":"markdown"},"source":{"bd9cc2c4":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn import tree\nfrom matplotlib import pyplot as plt","0b8831db":"train_df = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')","a83d00c5":"cat_cols = ['Sex','Embarked']\ntrain_df = pd.get_dummies(train_df, columns=cat_cols)\ntrain_df.head(2)","42ee6bfd":"features = ['Pclass','Age','SibSp','Parch','Fare','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S']\nX = train_df[features]\ny = train_df.Survived\nX.Age.fillna(X.Age.mode()[0], inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","0075bd53":"X_train.head(2)","a111c22e":"titanic_tree = DecisionTreeClassifier()\ntitanic_tree.fit(X_train, y_train)","284f7974":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(titanic_tree) ","806911e2":"titanic_tree.tree_.node_count","35520133":"train_pred = titanic_tree.predict(X_train)\nprint(\"Train accuracy: \", metrics.accuracy_score(y_train, train_pred))\npred = titanic_tree.predict(X_test)\nprint(\"Test accuracy: \", metrics.accuracy_score(y_test, pred))","f4b8db9b":"titanic_tree = DecisionTreeClassifier(ccp_alpha=0.01)\ntitanic_tree.fit(X_train, y_train)","260cb2cc":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(titanic_tree, filled=True,\n                   feature_names=X_train.columns,\n                   class_names=['died','survived'])","e54c7ade":"train_pred = titanic_tree.predict(X_train)\nprint(\"Train accuracy: \", metrics.accuracy_score(y_train, train_pred))\npred = titanic_tree.predict(X_test)\nprint(\"Test accuracy: \", metrics.accuracy_score(y_test, pred))","10c1f1ef":"scores = cross_val_score(titanic_tree, X, y, cv=5)\nprint(scores)\nprint(scores.mean())","c07f1605":"for i in range(1, 10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(i+40)\/50)\n    titanic_tree = DecisionTreeClassifier()\n    titanic_tree.fit(X_train, y_train)\n    pred = titanic_tree.predict(X_test)\n    print('=====================')\n    print(\"Test size: \", (i+40)\/50)\n    print(\"Test accuracy: \", metrics.accuracy_score(y_test, pred))","74b98803":"## Creating Decision Tree Model","3081b17e":"## This is likely overtrained","e328bb95":"## Pruning the tree with cost complexity pruning","d02ea1b1":"## Creating feature and target dataframes","64bcc731":"## Visualizing Tree","35ea5da8":"#### Marginal improvements in test accuracy when using post-pruning","4e487ec7":"## Cross Validation","8642ca97":"## Effect of shrinking training data"}}