{"cell_type":{"c1d72431":"code","60d91674":"code","2ac0f5bc":"code","c8dd56d6":"code","90989994":"code","d4fe52f9":"code","27d7cdad":"code","7c6be0b5":"code","6f026429":"code","ebc5c98e":"code","0d4c7de1":"code","01411acb":"code","fbc58403":"code","531f6e8f":"code","b6dd6396":"markdown","bcba2307":"markdown","70dca3a6":"markdown","f67204a5":"markdown","cc7574d9":"markdown","32e214d4":"markdown","ff1bc69b":"markdown","ac68a144":"markdown","0211681f":"markdown","7bc278ca":"markdown","9320f9a3":"markdown"},"source":{"c1d72431":"import time\n# Pandas is used for data manipulation\nimport pandas as pd\n\n#Graphic library plotly\nimport plotly.graph_objects as go\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Read in data, add header row and display first 5 rows\nfeatures = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')\nfeatures.head(5)\n\n","60d91674":"print('The shape of our features is:', features.shape)","2ac0f5bc":"# Descriptive statistics for each column\nfeatures.describe()","c8dd56d6":"# Initialize figure with subplots\nfrom plotly.subplots import make_subplots\n\n# Add traces\nfig = make_subplots(rows=4, cols=2, start_cell=\"bottom-left\")\n\n\nfig.add_trace(go.Scatter(y=features['GRE Score']),\n              row=1, col=1)\n\nfig.add_trace(go.Scatter(y=features['TOEFL Score']),\n              row=1, col=2)\n\nfig.add_trace(go.Scatter(y=features['University Rating']),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter(y=features['SOP']),\n              row=2, col=2)\n\nfig.add_trace(go.Scatter(y=features['LOR ']),\n              row=3, col=1)\n\nfig.add_trace(go.Scatter(y=features['CGPA']),\n              row=3, col=2)\n\nfig.add_trace(go.Scatter(y=features['Research']),\n              row=4, col=1)\n\nfig.add_trace(go.Scatter(y=features['Chance of Admit ']),\n              row=4, col=2)\n\n# Update yaxis properties\nfig.update_yaxes(title_text=\"GRE Score\", row=1, col=1)\nfig.update_yaxes(title_text=\"TOEFL Score\", row=1, col=2)\nfig.update_yaxes(title_text=\"University Rating\", row=2, col=1)\nfig.update_yaxes(title_text=\"SOP\", row=2, col=2)\nfig.update_yaxes(title_text=\"LOR\", row=3, col=1)\nfig.update_yaxes(title_text=\"CGPA\", row=3, col=2)\nfig.update_yaxes(title_text=\"Research\", row=4, col=1)\nfig.update_yaxes(title_text=\"Chance of Admit\", row=4, col=2)\n\n# Update title and height\nfig.update_layout(title_text=\"Basic plots for data verifying\", height=700)\n\nfig.show()","90989994":"# Use numpy to convert to arrays\nimport numpy as np\n\n# Labels are the values we want to predict\nlabels = np.array(features['Chance of Admit '])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures= features.drop('Chance of Admit ', axis = 1)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n\n# Convert to numpy array\nfeatures = np.array(features)\n\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","d4fe52f9":"\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(train_features, train_labels);\n","27d7cdad":"# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n","7c6be0b5":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","6f026429":"# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\n# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\n# Export the image to a dot file\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n# Use dot file to create a graph\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\n# Write graph to a png file\ngraph.write_png('tree.png')","ebc5c98e":"#Import library\nfrom IPython.display import Image\n\n# Load image from local storage\nImage(filename = \"tree.png\", width = 1400, height = 600)","0d4c7de1":"# Limit depth of tree to 3 levels\nrf_small = RandomForestRegressor(n_estimators=10, max_depth = 3, random_state = 42)\nrf_small.fit(train_features, train_labels)\n\n# Extract the small tree\ntree_small = rf_small.estimators_[5]\n\n# Save the tree as a png image\nexport_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n\ngraph.write_png('small_tree.png');","01411acb":"# Load image from local storage\nImage(filename = \"small_tree.png\", width = 1400, height = 600)","fbc58403":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","531f6e8f":"fig = go.Figure([go.Bar(x=feature_list, y=importances)])\n\nfig.update_layout(\n    title={\n        'text': \"Variable Importances\",\n        'x':0.5,\n        'xanchor': 'center'}\n\n)\n\nfig.update_layout(\n    font_family=\"Times New Roman\",\n    font_color=\"blue\",\n    font_size=20,\n    title_font_color=\"red\",\n)\n\nfig.update_xaxes(title_text=\"Variable\")\nfig.update_yaxes(title_text=\"Importance\")\n\nfig.update_xaxes(title_font_size=20)\nfig.update_yaxes(title_font_size=20)\n\nfig.show()\n","b6dd6396":"**To identify anomalies, we can quickly compute summary statistics.**","bcba2307":"# Visualizing a Single Decision Tree","70dca3a6":"# Used resorces:\nWill Koehrsen : Random Forest in Python\n\nRaul Eulogio, Edits by Brittany Marie Swanson: Random Forest in Python. Wisconsin Breast Cancer Machine Learning","f67204a5":"# Identify Anomalies\/ Missing Data","cc7574d9":"# Graduate Admission\n\nThis dataset is created for prediction of Graduate Admissions from an Indian perspective.\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs.\nThe parameters included are :\n\n    GRE Scores ( out of 340 )\n    TOEFL Scores ( out of 120 )\n    University Rating ( out of 5 )\n    Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n    Undergraduate GPA ( out of 10 )\n    Research Experience ( either 0 or 1 )\n    Chance of Admit ( ranging from 0 to 1 )\n\nThe following Python code loads in the csv data and displays the structure of the data:","32e214d4":"## Make Predictions on the Test Set","ff1bc69b":"# Variable Importances","ac68a144":"## Train Model","0211681f":"**We make basic plots using plotly visualization library**","7bc278ca":"## Features and Targets and Convert Data to Arrays\n## Training and Testing Sets","9320f9a3":"*Limiting the depth of trees*"}}