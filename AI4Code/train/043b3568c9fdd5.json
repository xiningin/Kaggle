{"cell_type":{"3c17d07e":"code","cb15d405":"code","db0f08ba":"code","b3769799":"code","0db10900":"code","2e134327":"code","32942fe8":"code","0dc5fdca":"code","edc1b0a9":"code","765d82b5":"code","c708be27":"code","37387908":"code","6a7b0e49":"code","b2e57ef8":"code","28fd4410":"code","c6652720":"code","6f5266ea":"code","cf37c502":"code","032bb61e":"code","cd324b13":"code","6ce9cb11":"code","bec6e5af":"code","791213e1":"code","faf41329":"code","dd11059a":"code","0d65e31e":"code","c6a37b3c":"code","e460cd57":"code","0894d4fe":"code","ad1087cb":"code","e4941755":"code","d4b26fd2":"code","3f38c40b":"code","972b35a1":"code","078b20d8":"code","1656f9f5":"code","838a3606":"code","0dbdffde":"code","2f76a6d1":"code","dcf6e1c0":"code","24fce600":"code","eb68c4ef":"code","dbc17153":"code","f723a8a6":"code","e683806b":"code","f96e534b":"code","a9931a9f":"code","f454712e":"code","490cc671":"code","9f9b6759":"code","a26f879b":"code","06e794ad":"code","eddce221":"code","ea91fc5f":"code","e84e4cd9":"code","31df112d":"code","6a78dd5c":"code","a5476326":"code","e3938f59":"code","ea5ae9c7":"code","42c9458f":"markdown","c731ccbb":"markdown","3d8a9d7d":"markdown","dee65701":"markdown","b90e6b53":"markdown","a8d541c1":"markdown","71ff11b3":"markdown","7e47b175":"markdown","0a26e0bb":"markdown","4de05114":"markdown","db3aa725":"markdown","983baf13":"markdown","73831ae2":"markdown","716379ac":"markdown","e08ebdf5":"markdown","3089e785":"markdown","0b9dbc03":"markdown","472f1a60":"markdown","578988b4":"markdown","af25dbde":"markdown","c0786f27":"markdown","69e151ad":"markdown","df0ca87e":"markdown","df0a119e":"markdown","c252caa0":"markdown","db27a38c":"markdown","a4a48914":"markdown","efe9a18c":"markdown","c4aabf32":"markdown","1e95123e":"markdown","e85786b0":"markdown","ebc9f208":"markdown","53171135":"markdown","52e1669b":"markdown","989c3e7b":"markdown","09b67143":"markdown","af925824":"markdown"},"source":{"3c17d07e":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom IPython.display import display\nimport random\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport time\n\npd.options.display.max_columns = None","cb15d405":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\ntrain['dataset'] = 'train'\ntest['dataset'] = 'test'\n\ndf = pd.concat([train, test])","db0f08ba":"train.head()","b3769799":"test.head()","0db10900":"print('Number of rows in training set: ', train.shape[0])\nprint('Number of columns in training set: ', train.shape[1] - 1)\nprint('Number of rows in test set: ', test.shape[0])\nprint('Number of columns in test set: ', test.shape[1] - 1)","2e134327":"df.info()","32942fe8":"ds = df.groupby(['cp_type', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_type', 'dataset', 'count']\n\nfig = px.bar(\n    ds, \n    x='cp_type', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_type train\/test counts', \n    width=500,\n    height=400\n)\n\nfig.show()","0dc5fdca":"ds = df.groupby(['cp_time', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_time', 'dataset', 'count']\n\nfig = px.bar(\n    ds, \n    x='cp_time', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_time train\/test counts', \n    width=500,\n    height=400\n)\n\nfig.show()","edc1b0a9":"ds = df.groupby(['cp_dose', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_dose', 'dataset', 'count']\n\nfig = px.bar(\n    ds, \n    x='cp_dose', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_dose train\/test counts', \n    width=500,\n    height=400\n)\n\nfig.show()","765d82b5":"ds = df[df['dataset']=='train']\nds = ds.groupby(['cp_type', 'cp_time', 'cp_dose'])['sig_id'].count().reset_index()\nds.columns = ['cp_type', 'cp_time', 'cp_dose', 'count']\n\nfig = px.sunburst(\n    ds, \n    path=[\n        'cp_type',\n        'cp_time',\n        'cp_dose' \n    ], \n    values='count', \n    title='Sunburst chart for all cp_type\/cp_time\/cp_dose',\n    width=500,\n    height=500\n)\nfig.show()","c708be27":"train_columns = train.columns.to_list()\ng_list = [i for i in train_columns if i.startswith('g-')]\nc_list = [i for i in train_columns if i.startswith('c-')]\n#g_list\n#c_list","37387908":"def plot_set_histograms(plot_list, title):\n    fig = make_subplots(rows=4, cols=3)\n    traces = [go.Histogram(x=train[col], nbinsx=20, name=col) for col in plot_list]\n\n    for i in range(len(traces)):\n        fig.append_trace(traces[i], (i \/\/ 3) + 1, (i % 3) + 1)\n\n    fig.update_layout(\n        title_text=title,\n        height=1000,\n        width=1000\n    )\n    fig.show()","6a7b0e49":"plot_list = [g_list[random.randint(0, len(g_list)-1)] for i in range(50)]\nplot_list = list(set(plot_list))[:12]\n\nplot_set_histograms(plot_list, 'Randomly selected gene expression features distributions')","b2e57ef8":"plot_list = [c_list[random.randint(0, len(c_list)-1)] for i in range(50)]\nplot_list = list(set(plot_list))[:12]\nplot_set_histograms(plot_list, 'Randomly selected cell expression features distributions')","28fd4410":"columns = g_list + c_list\nfor_correlation = list(set([columns[random.randint(0, len(columns)-1)] for i in range(200)]))[:40]\ndata = df[for_correlation]\n\nf = plt.figure(figsize=(19, 17))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=50)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=13)","c6652720":"start = time.time()\ncols = ['cp_time'] + columns\nall_columns = []\nfor i in range(0, len(cols)):\n    for j in range(i+1, len(cols)):\n        if abs(train[cols[i]].corr(train[cols[j]])) > 0.9:\n            all_columns.append(cols[i])\n            all_columns.append(cols[j])\n\nprint(time.time()-start)","6f5266ea":"all_columns = list(set(all_columns))\nprint('Number of columns: ', len(all_columns))","cf37c502":"data = df[all_columns]\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=50)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","032bb61e":"fig = make_subplots(rows=12, cols=3)\ntraces = [go.Histogram(x=train[col], nbinsx=20, name=col) for col in all_columns]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 3) + 1, (i % 3)+1)\n\nfig.update_layout(\n    title_text='Highly correlated features',\n    height=1200\n)\nfig.show()","cd324b13":"train_target = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\n\nprint('Number of rows : ', train_target.shape[0])\nprint('Number of cols : ', train_target.shape[1])\ntrain_target.head()","6ce9cb11":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values().reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the higher number of positive samples (top 50)', \n    height=1000, \n    width=800\n)\nfig.show()","bec6e5af":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the lowest number of positive samples (top 50)', \n    height=1000, \n    width=800\n)\nfig.show()","791213e1":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'count']\nx['count'] = x['count'] * 100 \/ len(train_target)\nfig = px.bar(\n    x, \n    x='column', \n    y='count', \n    orientation='v', \n    title='Percent of positive records for every column in target', \n    height=800, \n    width=1200\n)\nfig.show()","faf41329":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\nfig = px.bar(\n    data, \n    y=data['row'], \n    x=\"count\", \n    title='Number of activations in targets for every sample', \n    width=800, \n    height=500\n)\nfig.show()","dd11059a":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\nfig = px.pie(\n    data, \n    values=100 * data['row']\/len(train_target), \n    names=\"count\", \n    title='Number of activations in targets for every sample (Percent)', \n    width=800, \n    height=500\n)\nfig.show()","0d65e31e":"train_target.describe()","c6a37b3c":"start = time.time()\n\ncorrelation_matrix = pd.DataFrame()\nfor t_col in train_target.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train[col].corr(train_target[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list\n    \nprint(time.time()-start)","e460cd57":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\ncorrelation_matrix","0894d4fe":"maxCol=lambda x: max(x.min(), x.max(), key=abs)\nhigh_scores = correlation_matrix.apply(maxCol, axis=0).reset_index()\nhigh_scores.columns = ['column', 'best_correlation']\n\nfig = px.bar(\n    high_scores, \n    x='column', \n    y=\"best_correlation\", \n    orientation='v', \n    title='Best correlation with train columns for every target column', \n    width=1200,\n    height=800\n)\nfig.show()","ad1087cb":"col_df = pd.DataFrame()\ntr_cols = list()\ntar_cols = list()\nfor col in correlation_matrix.columns:\n    tar_cols.append(col)\n    tr_cols.append(correlation_matrix[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(1).values[0])\n\ncol_df['column'] = tar_cols\ncol_df['train_best_column'] = tr_cols\n\ntotal_scores = pd.merge(high_scores, col_df)\ntotal_scores","e4941755":"count_features = total_scores['train_best_column'].value_counts().reset_index().sort_values('train_best_column')\ncount_features.columns = ['column', 'count']\nfig = px.bar(\n    count_features.tail(33), \n    x='count', \n    y=\"column\", \n    orientation='h', \n    title='Columns from training set with number of high correlations with target columns', \n    width=800,\n    height=700\n)\nfig.show()","d4b26fd2":"target_columns = train_target.columns.tolist()\ntarget_columns.remove('sig_id')\nfor_analysis = [target_columns[random.randint(0, len(target_columns)-1)] for i in range(5)]\ncurrent_corr = correlation_matrix[for_analysis]","3f38c40b":"col_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntar_cols = list()\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[0])\n    tr_second_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[1])\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df","972b35a1":"def plot_scatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[col_df.iloc[index]['column']]\n    analysis['x'] = train[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train[col_df.iloc[index]['train_2_column']]\n    analysis.columns = ['color', col_df.iloc[index]['train_1_column'], col_df.iloc[index]['train_2_column']]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 10\n\n    fig = px.scatter(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'], \n        color=\"color\", \n        size='size', \n        height=800,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","078b20d8":"plot_scatter(col_df, 0)","1656f9f5":"plot_scatter(col_df, 1)","838a3606":"plot_scatter(col_df, 2)","0dbdffde":"plot_scatter(col_df, 3)","2f76a6d1":"plot_scatter(col_df, 4)","dcf6e1c0":"for_analysis = [target_columns[random.randint(0, len(target_columns)-1)] for i in range(5)]\ncurrent_corr = correlation_matrix[for_analysis]\n\ncol_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntr_third_cols = list()\ntar_cols = list()\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[0])\n    tr_second_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[1])\n    tr_third_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[2])\n\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df['train_3_column'] = tr_third_cols\ncol_df","24fce600":"def plot_3dscatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[col_df.iloc[index]['column']]\n    analysis['x'] = train[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train[col_df.iloc[index]['train_2_column']]\n    analysis['z'] = train[col_df.iloc[index]['train_3_column']]\n    analysis.columns = ['color', col_df.iloc[index]['train_1_column'], col_df.iloc[index]['train_2_column'], col_df.iloc[index]['train_3_column']]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 20\n\n    fig = px.scatter_3d(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'],\n        z=col_df.iloc[index]['train_3_column'], \n        color=\"color\", \n        size='size', \n        height=800,\n        width=800,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","eb68c4ef":"plot_3dscatter(col_df, 0)","dbc17153":"plot_3dscatter(col_df, 1)","f723a8a6":"plot_3dscatter(col_df, 2)","e683806b":"plot_3dscatter(col_df, 3)","f96e534b":"plot_3dscatter(col_df, 4)","a9931a9f":"last_term = dict()\nfor item in target_columns:\n    try:\n        last_term[item.split('_')[-1]] += 1\n    except:\n        last_term[item.split('_')[-1]] = 1\n\nlast_term = pd.DataFrame(last_term.items(), columns=['group', 'count'])\nlast_term = last_term.sort_values('count')\nlast_term = last_term[last_term['count']>1]\nlast_term['count'] = last_term['count'] * 100 \/ 206\n\nfig = px.bar(\n    last_term, \n    x='count', \n    y=\"group\", \n    orientation='h', \n    title='Groups in target columns (Percent from all target columns)', \n    width=800,\n    height=500\n)\nfig.show()","f454712e":"answer = list()\nfor group in last_term.group.tolist():\n    agent_list = list()\n    for item in target_columns:\n        if item.split('_')[-1] == group:\n            agent_list.append(item)\n    agent_df = train_target[agent_list]\n    data = agent_df.astype(bool).sum(axis=1).reset_index()\n    answer.append(data[0].max())","490cc671":"ds = pd.DataFrame()\nds['group'] = last_term.group.tolist()\nds['max_value'] = answer\n\nfig = px.bar(\n    ds, \n    x='max_value', \n    y=\"group\", \n    orientation='h', \n    title='Maximum number of active columns in 1 sample for every group', \n    width=800,\n    height=500\n)\nfig.show()","9f9b6759":"categories = train[['cp_type', 'cp_time', 'cp_dose']]\ntar = train_target.copy()\ntar = tar.drop(['sig_id'], axis=1)\nanalysis = pd.concat([categories, tar], axis=1)","a26f879b":"for category in analysis['cp_dose'].unique().tolist():\n    number = 0\n    cols = []\n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_dose'] == category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","06e794ad":"analysis[analysis['cp_dose'] == 'D2']['atp-sensitive_potassium_channel_antagonist'].value_counts()","eddce221":"analysis[analysis['cp_dose']=='D2']['erbb2_inhibitor'].value_counts()","ea91fc5f":"for category in analysis['cp_time'].unique().tolist():\n    number = 0\n    cols = []\n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_time']==category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","e84e4cd9":"analysis[analysis['cp_time'] == 24]['erbb2_inhibitor'].value_counts()","31df112d":"analysis[analysis['cp_time'] == 72]['erbb2_inhibitor'].value_counts()","6a78dd5c":"analysis[analysis['cp_time'] == 24]['atp-sensitive_potassium_channel_antagonist'].value_counts()","a5476326":"analysis[analysis['cp_time'] == 72]['atp-sensitive_potassium_channel_antagonist'].value_counts()","e3938f59":"for category in analysis['cp_type'].unique().tolist():\n    number = 0\n    cols = []\n    for col in analysis.columns:\n        if col in ['cp_type', 'cp_time', 'cp_dose']:\n            continue\n        if len(analysis[analysis['cp_type']==category][col].value_counts()) == 1:\n            number += 1\n            cols.append(col)\n\n    print(category, '. Number of columns with 1 unique value: ', number, '. Columns: ', cols)","ea5ae9c7":"analysis[analysis['cp_type']=='ctl_vehicle']['igf-1_inhibitor'].value_counts()","42c9458f":"### Let's do the same but for 3d plots","c731ccbb":"### Some distribution of randomly selected columns","3d8a9d7d":"### Let's check targets","dee65701":"### Let's visualize them","b90e6b53":"### Let's see some correlation between randomly selected variables","a8d541c1":"<a id=\"7\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>7. Targets & Train features dependecies<\/center><h2>","71ff11b3":"## And we have large correlation matrix","7e47b175":"## Now let's see what columns from training set have the higher number of \"high\" correlations with target columns. Every row from chart means that column `A` `N` times has the best value of correlation with different target columns. ","0a26e0bb":"<a id=\"6\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>6. Train & Targets correlations<\/center><h2>","4de05114":"## Time to find the most correlated features for every target column","db3aa725":"## Let's see what is the higher value (absolute) of correlation for target columns with every column from train set. Every column on chart is max correlation of current target column with all of columns from training set.","983baf13":"## Is it possible to have more than 1 activation for 1 sample in every group?","73831ae2":"### Time to find pairs of features with high correlation","716379ac":"## 2.2 Cp_time feature","e08ebdf5":"<a id=\"2\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>2. Categories Visualization<\/center><h2>","3089e785":"## 2.3 Cp_dose feature","0b9dbc03":"### Let's check target columns with categorical columns from training set ","472f1a60":"### We can see that we have 872 float features 1 integer (cp_time) and 3 categorical (sig_id, cp_type and cp_dose)","578988b4":"### Let's check problematic columns for cp_time = 24 and 72","af25dbde":"## We can see here that about 40% of sample have zeros in all columns and more than 50% have only one active target column.","c0786f27":"### We can extract several group names from target column names. Looks like that last term in column name is definition of a group. Let's extact them and visualize groups with number of columns > 1.","69e151ad":"## We can see that for groups activator, agent, blocker maximum number of active columns in sample is 1.","df0ca87e":"<a id=\"5\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>5. Targets analysis<\/center><h2>","df0a119e":"### Let's select some random columns and see how they deal with pairs of the highly correlated features","c252caa0":"## The biggest number of positive samples for 1 target column is 3.5%. So we deal here with highly imbalanced data.","db27a38c":"<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>3. Gene and cell features distribution<\/center><h2>","a4a48914":"<h1><center>Mechanisms of Action (MoA) Prediction. Data analysis and visualization<\/center><\/h1>\n\n<center><img src=\"https:\/\/pharmacyinnovations.net\/wp-content\/uploads\/pillsdrugs.png\"><\/center>\n","efe9a18c":"### Take a look into training and test sets.","c4aabf32":"## Target columns and pairs of highly correlated features","1e95123e":"## 2.1 cp_type feature","e85786b0":"## We can see that for column ```cp_type``` all records where value is ```ctl_vehicle``` for all targets are 0. The same picture for ```cp_time``` == 72 ana == 24, but only for 2 target columns and for ```cp_dose``` == D2 also for 2 target columns.\n","ebc9f208":"<a id=\"1\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>1. Basic Data Overview<\/center><h2>","53171135":"### Let's check problematic columns for dp_dose = 2","52e1669b":"## We can see that at least 50 target columns have number pf positive samples less than 20 (about 0.1%) !!!","989c3e7b":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1. Basic Data Overview](#1)\n* [2. Categories visualization](#2)\n* [3. Gene and cell features distribution](#3)\n* [4. Training features correlation](#4)\n* [5. Targets analysis](#5)\n* [6. Train & Targets correlations](#6)\n* [7. Targets & Train features dependecies](#7)","09b67143":"### In total we have 35 columns that have correlation with at least another 1 higher than 0.9. Let's visualize them.","af925824":"<a id=\"4\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>4. Training features correlation<\/center><h2>"}}