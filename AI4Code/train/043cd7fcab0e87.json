{"cell_type":{"6bffeb49":"code","6f987d67":"code","298cf860":"code","b0f855c9":"code","63785578":"code","af679395":"code","7c342673":"code","ec38221f":"code","594b37cf":"code","3527309d":"code","852146ee":"code","f0dc6dc9":"code","ceb1c6f4":"code","567e64e4":"code","1d41a01c":"code","345d3a3f":"code","dbe3a3c5":"code","3424443c":"code","0b5fc813":"code","713ba68f":"code","711ea341":"code","5a085253":"code","9be19ab2":"code","8d5b9e52":"code","3dd9a690":"code","82ea956b":"code","2b0438ec":"code","00752302":"code","03f81900":"code","d9990207":"code","80a0ae6a":"code","3f3ce973":"code","2dcf4660":"code","f97894d5":"code","af60a6b5":"code","c50a13de":"code","447168c3":"code","101d0253":"code","d5c5da40":"code","b65abfbb":"code","ef03615f":"code","6858a703":"code","b1a6da35":"code","456ebfd7":"code","872b75d5":"code","200d9b55":"code","2d41045c":"code","e97aba98":"code","f2359604":"code","ccc8e120":"code","10b23477":"code","1ce52eb8":"code","4a5c3b11":"code","19d4771e":"code","d2445baf":"code","e1946afc":"code","71c6e97a":"code","f7566c10":"code","33acc004":"code","19179b4e":"code","23df1c66":"code","b987ecd6":"code","05aaae34":"code","0eee23d5":"code","c96d5ae8":"code","6c50c1f0":"code","bbdd997e":"code","c4c62442":"code","84d77e9f":"code","7f8d1af4":"code","db0d7e51":"code","e75de314":"code","3fb38334":"code","805b7bd1":"code","a8578db6":"code","8e7d7b41":"code","98952dd2":"code","15bb0fac":"code","ff0e69dc":"code","b865ab55":"code","8120ee06":"code","7be79463":"code","57887031":"code","ba1e9c7e":"code","a31e4820":"markdown","8745f970":"markdown","122612a6":"markdown","cf94cdb9":"markdown","79ef0f44":"markdown","b9e9d478":"markdown","ab33ac01":"markdown","037ae4fe":"markdown","3ae8716c":"markdown","374c18a5":"markdown","0173f8fc":"markdown","baa5242b":"markdown","e4f0812f":"markdown","b11d8f2b":"markdown","4cb41d42":"markdown","eb884c15":"markdown","4bbe2235":"markdown","3ab2727c":"markdown","52174776":"markdown","1375486d":"markdown","2b8b1ec5":"markdown","64e510e2":"markdown","5654ce75":"markdown","39ba707e":"markdown","06e4e62a":"markdown","d60060be":"markdown","404762b6":"markdown","531393b8":"markdown","4ae2c7f1":"markdown","d1e6760d":"markdown","26a32376":"markdown","703b1066":"markdown","6b46c76b":"markdown"},"source":{"6bffeb49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f987d67":"trainFile = '\/kaggle\/input\/mkn-ml-2020-competition-1-true-part-2\/comp2_train.csv'\ntestFile = '\/kaggle\/input\/mkn-ml-2020-competition-1-true-part-2\/comp2_test.csv'","298cf860":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt","b0f855c9":"np.random.seed(7)\nnp.set_printoptions(suppress=True)","63785578":"trainData = pd.read_csv(trainFile).to_numpy()\nfeatures = np.array([row[1:] for row in trainData])\ntargets = np.array([row[0] for row in trainData])\n","af679395":"testData = pd.read_csv(testFile).to_numpy()\ntestFeatures = np.array([row[:-1] for row in testData])","7c342673":"data = pd.read_csv(trainFile)\npd.set_option('display.max_rows', data.shape[0]+1)\nprint(data[(data.feature_12 == 1) & (data.feature_13 == 1)].sort_values(['feature_6'])[['target', 'feature_6', 'feature_10', 'feature_11', 'feature_12', 'feature_13']])","ec38221f":"def ShowPicture(features, targets, i, j, sz=3):\n    plt.figure(figsize=(sz, sz))\n    plt.scatter(features[:, i], features[:, j], c=targets, s=100, cmap='autumn', edgecolors=\"black\")\n    plt.xlabel(f'feature {i + 1}')\n    plt.ylabel(f'feature {j + 1}')\n    plt.title(f'Picture for feature {i + 1} and feature {j + 1}')\n\n    plt.show()\n    ","594b37cf":"# for i in range(features.shape[1]):\n#     for j in range(i):\n#         ShowPicture(features, targets, j, i)","3527309d":"ShowPicture(features, targets, 0, 7, 8)","852146ee":"ShowPicture(features, targets, 4, 7, 8)","f0dc6dc9":"ShowPicture(features, targets, 0, 4, 8)","ceb1c6f4":"data[data.feature_1 > 0.7].sort_values('feature_8')[['target', 'feature_1', 'feature_8']]","567e64e4":"def CountWrongPairs(s):\n    if len(s) <= 1:\n        return 0\n    left = s[:len(s) \/\/ 2]\n    right = s[len(s) \/\/ 2:]\n    res = CountWrongPairs(left) + CountWrongPairs(right)\n    curL = 0\n    curR = 0\n    cntZeros = 0\n    s = []\n    \n    def addFromLeft(curL, cntZeros):\n        s.append(left[curL])\n        if left[curL][1] == 0:\n            cntZeros += 1\n        curL += 1\n        return curL, cntZeros\n\n    \n    def addFromRight(curR, res, cntZeros):\n        s.append(right[curR])\n        if right[curR][1] == 1:\n            res += cntZeros\n        curR += 1\n        return curR, res\n\n    while curL < len(left) and curR < len(right):\n        if left[curL][0] < right[curR][0]:\n            curL, cntZeros = addFromLeft(curL, cntZeros)\n        elif left[curL][0] > right[curR][0]:\n            curR, res = addFromRight(curR, res, cntZeros)\n        else:\n            wasL = curL\n            curOnes = 0\n            while curR < len(right) and right[curR][0] == left[wasL][0]:\n                if right[curR][1] == 1:\n                    curOnes += 1\n                curR, res = addFromRight(curR, res, cntZeros)\n            curZeros = 0\n            while curL < len(left) and left[curL][0] == left[wasL][0]:\n                if left[curL][1] == 0:\n                    curZeros += 1\n                curL, cntZeros = addFromLeft(curL, cntZeros)\n            res += curZeros * curOnes \/ 2\n            \n    while curL < len(left):\n        curL, cntZeros = addFromLeft(curL, cntZeros)\n    while curR < len(right):\n        curR, res = addFromRight(curR, res, cntZeros)\n    return res\n            \ndef CalcAucRoc(targets, predictions):\n    assert(len(predictions) == len(targets))\n    sortedPredictions = sorted([[predictions[i], targets[i]] for i in range(len(targets))])\n    cntOnes = np.sum(targets)\n    return CountWrongPairs(sortedPredictions) \/ (cntOnes * (len(targets) - cntOnes))","1d41a01c":"from sklearn.metrics import roc_auc_score\n\nn = 30\nfor i in range(100):\n    yScore = np.random.random_sample((n,))\n    yGoal = np.random.choice(2, n)\n    assert(abs(roc_auc_score(yGoal, yScore) - CalcAucRoc(yGoal, yScore)) < 1e-5)\n\nfor i in range(100):\n    yScore = np.random.choice(4, n) \/ 3\n    yGoal = np.random.choice(2, n)\n    assert(abs(roc_auc_score(yGoal, yScore) - CalcAucRoc(yGoal, yScore)) < 1e-5)","345d3a3f":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputedFeatures = my_imputer.fit_transform(features)\n\nk, itersCount = 800, 100\nres = 0\nfor i in range(itersCount):\n    indices = np.random.permutation(features.shape[0])\n    trainingIDs, testIDs = indices[:k], indices[k:]\n    training, test = imputedFeatures[trainingIDs,:], imputedFeatures[testIDs,:]\n\n    from sklearn import tree\n    clf = tree.DecisionTreeClassifier()\n    clf = clf.fit(training, targets[trainingIDs])\n\n    res += roc_auc_score(targets[testIDs], clf.predict(test))\nprint(res \/ itersCount)","dbe3a3c5":"def SaveResults(features, predict, fileName):\n    global testData\n    result = pd.DataFrame({\n        'target': [predict(row) for row in features],\n        'id': [int(row[-1]) for row in testData]\n    })\n    result.to_csv(fileName + '.csv',index=False)","3424443c":"solver = tree.DecisionTreeClassifier()\nsolver = clf.fit(imputedFeatures, targets)\n\nimputedTestFeatures = my_imputer.fit_transform(testFeatures)\n\nSaveResults(imputedTestFeatures, lambda row: solver.predict([row])[0], 'SklearnDecisionTreeClassifier')","0b5fc813":"print(tree.plot_tree(clf))","713ba68f":"rowsWithNans = [k for k in range(features.shape[1]) if np.isnan(features[:,k]).any()]\nprint(rowsWithNans)\nfor rowID in rowsWithNans:\n    print([x for x in range(features.shape[0]) if np.isnan(features[x,rowID]).any()])","711ea341":"cnt = []\nfor rowID in range(features.shape[0]):\n    if np.isnan(features[rowID]).any():\n        cnt.append(len([k for k in range(features.shape[1]) if np.isnan(features[rowID,k]).any()]))\nprint(cnt)","5a085253":"def ConjugateGradient(A, b):\n    x = np.zeros(A.shape[1])\n    At = A.T\n    v = A @ x - b\n    d = v\n    v_norm = np.dot(v, v)\n    \n    for i in range(len(b)):\n        Ad = A @ d\n        alpha = v_norm \/ (d @ Ad)\n        x = x - alpha * d\n        v = v - alpha * Ad\n        v_norm_new = np.dot(v, v)\n\n        d = v + (v_norm_new \/ v_norm) * d\n        v_norm = v_norm_new\n    return x\n\ndef RunConjugateGradient(features, targets):\n    return ConjugateGradient(features.T @ features, features.T @ targets)\n\ndef KFold(features, targets, k):\n    n = features.shape[0]\n    w = np.zeros(features.shape[1])\n    for i in range(k):\n        curFeatures = np.array([features[j] for j in range(n) if j % k != i])\n        curTargets = np.array([targets[j] for j in range(n) if j % k != i])\n        curW = RunConjugateGradient(curFeatures, curTargets)\n        w = w + curW\n    w = np.array([wi \/ k for wi in w])\n    return w\n\ndef LeaveOneOut(features, targets):\n    n = features.shape[0]\n    w = np.zeros(features.shape[1])\n    for i in range(n):\n        curFeatures = np.array([features[j] for j in range(n) if j != i])\n        curTargets = np.array([targets[j] for j in range(n) if j != i])\n        curW = RunConjugateGradient(curFeatures, curTargets)\n        w = w + curW\n    w = np.array([wi \/ n for wi in w])\n    return w","9be19ab2":"def f(row):\n    return np.concatenate(([1], row))\n\ndef LearnModel(features, targets, k):\n    return RunConjugateGradient(np.array([f(row) for row in features]), targets)\n\ndef CalculatePenalty(features, targets, w, k):\n    prediction = np.array([f(row) @ w for row in features])\n    return np.dot(prediction - targets, prediction - targets) \/ features.shape[0]","8d5b9e52":"from copy import deepcopy","3dd9a690":"def splitDatasetsForFeatureRestoration(features, testFeatures, k, p=0.5):\n    mergedFeatures = np.concatenate((features, testFeatures))\n    rowsWithoutNans = [rowID for rowID in range(mergedFeatures.shape[0]) if not np.isnan(mergedFeatures[rowID]).any()]\n    rowsForPrediction = [rowID for rowID in range(mergedFeatures.shape[0]) if np.isnan(mergedFeatures[rowID, k]).any()]\n    np.random.shuffle(rowsWithoutNans)\n    trainPart = int(len(mergedFeatures) * p)\n    indices = np.array([x for x in range(features.shape[1]) if x != k])\n    return mergedFeatures[rowsWithoutNans[:trainPart],][:,indices], mergedFeatures[rowsWithoutNans[:trainPart],][:,k], \\\n            mergedFeatures[rowsWithoutNans[trainPart:],][:,indices], mergedFeatures[rowsWithoutNans[trainPart:],][:,k], \\\n            mergedFeatures[rowsForPrediction,][:,k]\n\ndef featureRestoration(features, testFeatures):\n    featuresWithNans = [k for k in range(features.shape[1]) if np.isnan(features[:,k]).any()]\n    restoratedFeatures = deepcopy(features)\n    restoratedTestFeatures = deepcopy(testFeatures)\n    for k in featuresWithNans:\n        wBest = None\n        penalty = 0\n        for iter in range(100):\n            trainFeatures, trainTargets, checkFeatures, checkTargets, test = splitDatasetsForFeatureRestoration(features, testFeatures, k)\n            w = LearnModel(trainFeatures, trainTargets, k)\n            curPenalty = CalculatePenalty(checkFeatures, checkTargets, w, k)\n            if wBest is None or penalty > curPenalty:\n                wBest = deepcopy(w)\n                penalty = curPenalty\n        for row in restoratedFeatures:\n            if np.isnan(row[k]).any():\n                kek = np.concatenate((row[:k], row[k+1:]))\n                row[k] = f(kek) @ w\n        for row in restoratedTestFeatures:\n            if np.isnan(row[k]).any():\n                kek = np.concatenate((row[:k], row[k+1:]))\n                row[k] = f(kek) @ w\n    return restoratedFeatures, restoratedTestFeatures\n        \nfeaturesNew, testFeaturesNew = featureRestoration(features, testFeatures)","82ea956b":"def SplitDatasetForTrainAndTest(features, targets, k):\n    indices = np.random.permutation(features.shape[0])\n    trainingIDs, testIDs = indices[:k], indices[k:]\n    return features[trainingIDs,:], features[testIDs,:], targets[trainingIDs], targets[testIDs]\n\n\nk, itersCount = 800, 100\nres = 0\nfor i in range(itersCount):\n    training, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    from sklearn import tree\n    clf = tree.DecisionTreeClassifier()\n    clf = clf.fit(training, trainTargets)\n\n    res += roc_auc_score(testTargets, clf.predict(test))\nprint(res \/ itersCount)","2b0438ec":"solver = tree.DecisionTreeClassifier()\nsolver = clf.fit(featuresNew, targets)\n\ndata = deepcopy(solver.predict(testFeaturesNew))\nres = pd.read_csv('SklearnDecisionTreeClassifier.csv').to_numpy()[:,0]\n\nSaveResults(testFeaturesNew, lambda row: solver.predict([row])[0], 'SklearnDecisionTreeClassifierWithfeatureRestoration1')","00752302":"iterCount = 1000\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    solver = tree.DecisionTreeClassifier(criterion='entropy')\n    solver = clf.fit(train[:,indices], trainTargets)\n    \n    val = roc_auc_score(testTargets, clf.predict(test[:,indices]))\n    if val > res:\n        res = val\n        clfBest = deepcopy(clf)\n    \nprint(res)\nSaveResults(testFeaturesNew, lambda row: clfBest.predict([row[indices]])[0], f'SklearnDecisionTreeClassifierEntropy{k}')","03f81900":"def MajorityFormula(items):\n    cntOnes = 0\n    for item in items:\n        if item[1] == 1:\n            cntOnes += 1\n    return 1 if cntOnes * 2 >= len(items) else 0\n\ndef DistanceFormula(items):\n    summa, norm = 0, 0\n    for item in items:\n        summa += item[1] \/ item[0]\n        norm += 1 \/ item[0]\n    return summa \/ norm\n\n\ndef RunKNN(features, targets, testFeatures, k, formula):\n    result = []\n    for row in testFeatures:\n        items = [[np.dot(feature - row, feature - row), targets[i]] for i, feature in enumerate(features)]\n        items.sort()\n        result.append(formula(items[:k]))\n    return result","d9990207":"# iterCount = 1\n# trainSize = 800\n\n# bestK = None\n# bestAUC = 0\n# for k in range(0, 500):\n#     if k % 20 == 0:\n#         print(bestK, bestAUC)\n#     for iter in range(iterCount):\n#         train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, trainSize)\n#         auc = roc_auc_score(testTargets, RunKNN(train, trainTargets, test, k, DistanceFormula))\n#         if bestK is None or bestAUC < auc:\n#             bestK = k\n#             bestAUC = auc\n# print(\"RESULT:\")\n# print(bestK)\n# print(bestAUC)","80a0ae6a":"from copy import deepcopy\n\nclass Node:\n    Features = []\n    Targets = []\n    DivisionRule = None\n    StopRule = None\n    MarkRule = None\n    \n    Predicate = None\n    \n    LeftSon = None\n    RightSon = None\n    \n    Height = 0\n    Mark = None\n    \n    def __init__(self, features, targets, divisionRule, stopRule, markRule, height=0):\n#         print()\n        self.Features = features\n        self.Targets = targets\n        self.DivisionRule = divisionRule\n        self.StopRule = stopRule\n        self.MarkRule = markRule\n        self.Height = height\n        \n    def Fit(self):\n        if self.StopRule(self):\n            self.Mark = self.MarkRule(self.Features, self.Targets)\n            return\n        \n        self.Predicate = self.DivisionRule(self.Features, self.Targets)\n        if self.Predicate is None:\n            self.Mark = self.MarkRule(self.Features, self.Targets)\n            return\n\n        toLeft, toRight = [], []\n        for i, feature in enumerate(self.Features):\n            if self.Predicate(feature):\n                toLeft.append(i)\n            else:\n                toRight.append(i)\n        \n        if len(toLeft) * len(toRight) == 0:\n            kek = 'toLeft' if len(toLeft) == 0 else 'toRight'\n            raise Exception(f'Invalid DivideRule, {kek} is empty')\n        self.LeftSon = Node(self.Features[toLeft], self.Targets[toLeft], self.DivisionRule, self.StopRule, self.MarkRule, self.Height + 1)\n        self.RightSon = Node(self.Features[toRight], self.Targets[toRight], self.DivisionRule, self.StopRule, self.MarkRule, self.Height + 1)\n        \n        \n        self.LeftSon.Fit()\n        self.RightSon.Fit()\n            \n    def PredictSingle(self, feature):\n        if self.Predicate is None:\n            return self.Mark(feature)\n        return self.LeftSon.PredictSingle(feature) if self.Predicate(feature) else self.RightSon.PredictSingle(feature)\n            \n    def Predict(self, features):\n        return [self.PredictSingle(feature) for feature in features] ","3f3ce973":"def MajorityVote(features, targets):\n    return lambda row: 1 if np.sum(targets) * 2 >= len(targets) else 0","2dcf4660":"def SimpleStopRule(node):\n    onesCount = np.sum(node.Targets)\n    return onesCount == 0 or onesCount == len(node.Targets)","f97894d5":"def h1(q):\n    if q == 0 or q == 1:\n        return 0\n#     return 4 * q * (1 - q)\n    return -(q * np.log(q) + (1 - q) * np.log(1 - q))\n\n\ndef Gain(pl, nl, pr, nr):\n    s = pl + nl + pr + nr\n    return (pl + nl) \/ s * h1(pl\/(pl + nl)) + (pr + nr) \/ s * h1(pr\/(pr + nr))\n\ndef SimpleDivisionRule(features, targets):\n    order = list(range(features.shape[0]))\n    bestK = None\n    minVal = None\n    xBorder = None\n    allOnes = np.sum(targets)\n    for k in range(features.shape[1]):\n        order.sort(key=lambda x: features[x][k])\n        curOnes = 0\n        for c, i in enumerate(order):\n            if targets[i] == 1:\n                curOnes += 1\n            if features[i][k] > features[order[0]][k] and features[i][k] < features[order[-1]][k] != len(order):\n                val = Gain(curOnes, c + 1 - curOnes, allOnes - curOnes, (features.shape[0] - allOnes) - (c + 1 - curOnes))\n                if minVal is None or val < minVal:\n                    bestK = k\n                    minVal = val\n                    xBorder = features[i][k]\n    if bestK is None:\n        return None\n    return lambda row: row[bestK] <= xBorder","af60a6b5":"np.random.seed(0)\niterCount = 1\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    root = Node(train, trainTargets, SimpleDivisionRule, SimpleStopRule, MajorityVote)\n    root.Fit()\n    \n    val = roc_auc_score(testTargets, root.Predict(test[:,indices]))\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    \nprint(res)","c50a13de":"SaveResults(imputedTestFeatures, lambda row: clfBest.Predict([row])[0], 'MyTreeSimpleImputed')","447168c3":"root = Node(imputedFeatures, targets, SimpleDivisionRule, SimpleStopRule, MajorityVote)\nroot.Fit()\n\nSaveResults(imputedTestFeatures, lambda row: root.Predict([row])[0], 'MyTreeSimpleFullTrainImputed')","101d0253":"SaveResults(testFeaturesNew, lambda row: clfBest.Predict([row])[0], 'MyTreeSimple')","d5c5da40":"root = Node(featuresNew, targets, SimpleDivisionRule, SimpleStopRule, MajorityVote)\nroot.Fit()\n\nSaveResults(testFeaturesNew, lambda row: root.Predict([row])[0], 'MyTreeSimpleFullTrain')","b65abfbb":"np.random.seed(0)\niterCount = 1\nk = 800\nres = 0\nclfBest = None\nindices = [0, 2, 3, 6, 7, 10, 11]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    root = Node(train[:,indices], trainTargets, SimpleDivisionRule, SimpleStopRule, MajorityVote)\n    root.Fit()\n    \n    val = roc_auc_score(testTargets, root.Predict(test[:,indices]))\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    \nprint(res)","ef03615f":"SaveResults(testFeaturesNew, lambda row: clfBest.Predict([row])[0], f'MyTreeSimpleBestCV{iterCount}')","6858a703":"def Sigmoid(t):\n    return 1 \/ (1 + np.exp(-t))\n\ndef CalculateLogLikehood(features, targets, w):\n    res = 0\n    for i in range(n):\n        sigm = Sigmoid(features[i] @ w)\n        res += np.log(sigm) if targets[i] == 1 else np.log(1 - sigm)\n    return -res \/ features.shape[0]\n\ndef CalculateGradient(features, targets, w):\n    res = np.zeros(w.shape[0])\n    for i in range(features.shape[0]):\n        res += (-targets[i] + Sigmoid(features[i] @ w)) * features[i]\n    return res \/ features.shape[0]\n\ndef LogisticRegression(features, targets, iterCount = 50):\n    featuresNew = np.concatenate((features, np.array([[1] for i in range(features.shape[0])])), 1)\n    w = np.random.sample(featuresNew.shape[1]) \/ 100000\n    for i in range(iterCount):\n        gradient = CalculateGradient(featuresNew, targets, w)\n        left = 1e-9\n        right = 1e9\n        for j in range(10):\n            m1 = (left * left * right) ** (1\/3)\n            m2 = (left * right * right) ** (1\/3)\n            if CalculateLogLikehood(featuresNew, targets, w - m1 * gradient) < CalculateLogLikehood(featuresNew, targets, w - m2 * gradient):\n                right = m2\n            else:\n                left = m1\n        w = w - left * gradient\n    return w","b1a6da35":"w = LogisticRegression(featuresNew, targets)","456ebfd7":"def Predict(row, w):\n    return Sigmoid(np.concatenate((row, [1])) @ w)","872b75d5":"np.random.seed(0)\n\niterCount = 7\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    w = LogisticRegression(train[:,indices], trainTargets)\n    \n    val = roc_auc_score(testTargets, [Predict(row, w) for row in test[:,indices]])\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    \nprint(res)","200d9b55":"def N(b, w, features, targets):\n    res = 0\n    for i in range(features.shape[0]):\n        res += w[i] * int(b(features[i]) != targets[i])\n    return res\n\ndef f(row, ab):\n    res = 0\n    for a, b in ab:\n        res += a * b(row)\n    return 1 if res > 0 else 0\n\ndef AdaBoost(features, targetsKek, Rule, T = 100):\n    targets = np.array([1 if t == 1 else -1 for t in targetsKek])\n    w = np.ones(features.shape[0]) \/ features.shape[0]\n    res = []\n    for _ in range(T):\n        b = Rule(features, targets, w)\n        penalty = N(b, w, features, targets)\n        a = 1\/2 * np.log((1 - penalty) \/ penalty)\n        wsum = 0\n        for i in range(features.shape[0]):\n            w[i] *= np.exp(-a * targets[i] * b(features[i]))\n            wsum += w[i]\n        w \/= wsum\n        res.append([a, b])\n    return lambda row : f(row, res)","2d41045c":"def RelaxBestStump(order, features, targets, w, k, bestK, bestX, bestRes, isMoreRule, More):\n    res = N(lambda row : -1, w, features, targets)\n    for c, i in enumerate(order):\n        res += w[i] * (int(1 == -targets[i]) - int(-1 == -targets[i]))\n        if c + 1 < len(order) and features[order[c + 1]][k] == features[i][k]:\n            continue\n        if bestRes is None or res < bestRes:\n            bestK = k\n            bestRes = res\n            bestX = features[i][k]\n            isMoreRule = More\n        \n    return bestK, bestX, bestRes, isMoreRule\n\ndef convert01(x):\n    return 1 if x else -1\n\ndef StumpRule(features, targets, w):\n    order = [i for i in range(features.shape[0])]\n    bestK = None\n    bestX = None\n    bestRes = None\n    isMoreRule = True\n    for k in range(features.shape[1]):\n        order.sort(key=lambda i: features[i][k])\n        bestK, bestX, bestRes, isMoreRule = RelaxBestStump(order, features, targets, w, k, bestK, bestX, bestRes, isMoreRule, False)\n        order.reverse()\n        bestK, bestX, bestRes, isMoreRule = RelaxBestStump(order, features, targets, w, k, bestK, bestX, bestRes, isMoreRule, True)\n    return lambda row: convert01(row[bestK] >= bestX) if isMoreRule else convert01(row[bestK] <= bestX)","e97aba98":"# model = AdaBoost(featuresNew, targets, StumpRule)","f2359604":"np.random.seed(0)\n\niterCount = 10\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    model = AdaBoost(train[:,indices], trainTargets, StumpRule, 10)\n\n    val = roc_auc_score(testTargets, [model(row) for row in test[:,indices]])\n#     print([model(row) for row in test[:,indices]])\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    \nprint(res)","ccc8e120":"def PrecheckSubmission(fileName, bestSubmissionFileName='SklearnDecisionTreeClassifier.csv'):\n    res = pd.read_csv(fileName + '.csv' if fileName[-3:] != 'csv' else '').to_numpy()[:,0]\n    bestRes = pd.read_csv(bestSubmissionFileName).to_numpy()[:,0]\n    onesToZeros = 0\n    zerosToOnes = 0\n    for i in range(len(res)):\n        if res[i] == 0 and bestRes[i] == 1:\n            onesToZeros += 1\n        if res[i] == 1 and bestRes[i] == 0:\n            zerosToOnes += 1\n    return f'predictions changes: {np.sum(res != bestRes)}, ones to zeros: {onesToZeros}, zeros to ones: {zerosToOnes}'","10b23477":"StumpBoost = AdaBoost(featuresNew, targets, StumpRule, 100)","1ce52eb8":"SaveResults(testFeaturesNew, lambda row: np.sign(StumpBoost(row)), 'StumpBoost')\nprint(PrecheckSubmission('StumpBoost'))","4a5c3b11":"root = Node(featuresNew, targets, SimpleDivisionRule, SimpleStopRule, MajorityVote)\nroot.Fit()","19d4771e":"SaveResults(testFeaturesNew, lambda row: root.Predict([row])[0], 'FullTrainSimpleDT')\nprint(PrecheckSubmission('FullTrainSimpleDT'))","d2445baf":"def StopRuleHeight(node, h):\n    onesCount = np.sum(node.Targets)\n    if onesCount == 0 or onesCount == len(node.Targets):\n        return True\n    return node.Height >= h\n\ndef GenerateStopRuleHeight(h):\n    return lambda node: StopRuleHeight(node, h)\n\ndef StumpsBoostForLeaves(features, targets):\n    onesCount = np.sum(targets)\n    if onesCount == 0 or onesCount == len(targets):\n        return lambda row: 0 if onesCount == 0 else 1\n    return AdaBoost(features, targets, StumpRule, 10)","e1946afc":"import warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(0)\n\niterCount = 10\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    root = Node(train[:,indices], trainTargets, SimpleDivisionRule, GenerateStopRuleHeight(5), StumpsBoostForLeaves)\n    root.Fit()\n    \n    val = roc_auc_score(testTargets, root.Predict(test[:,indices]))\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    print(res)\n    \nprint(res)","71c6e97a":"fName = f'DTWithStumpsAtLeavesBestCV{iterCount}'\nSaveResults(testFeaturesNew, lambda row: clfBest.Predict([row])[0], fName)\nprint(PrecheckSubmission(fName))","f7566c10":"height = 5\nroot = Node(featuresNew, targets, SimpleDivisionRule, GenerateStopRuleHeight(height), StumpsBoostForLeaves)\nroot.Fit()","33acc004":"fName = f'DTWithStumpsAtLeavesFullTrain{height}'\nSaveResults(testFeaturesNew, lambda row: root.Predict([row])[0], fName)\nprint(PrecheckSubmission(fName))","19179b4e":"import warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(0)\n\niterCount = 1\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\ntrain, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\nroot = Node(train[:,indices], trainTargets, SimpleDivisionRule, GenerateStopRuleHeight(3), StumpsBoostForLeaves)\nroot.Fit()\nprint(roc_auc_score(testTargets, root.Predict(test[:,indices])))","23df1c66":"height = 3\nroot = Node(featuresNew, targets, SimpleDivisionRule, GenerateStopRuleHeight(height), StumpsBoostForLeaves)\nroot.Fit()","b987ecd6":"fName = f'DTWithStumpsAtLeavesFullTrain{height}'\nSaveResults(testFeaturesNew, lambda row: root.Predict([row])[0], fName)\nprint(PrecheckSubmission(fName))","05aaae34":"import warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(0)\n\niterCount = 300\nk = 800\nres = 0\nclfBest = None\nindices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor i in range(iterCount):\n    train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, k)\n    root = Node(train[:,indices], trainTargets, SimpleDivisionRule, GenerateStopRuleHeight(3), StumpsBoostForLeaves)\n    root.Fit()\n    \n    val = roc_auc_score(testTargets, root.Predict(test[:,indices]))\n    if val > res:\n        res = val\n        clfBest = deepcopy(root)\n    if i % 30 == 0:\n        print(i, res)\n    \nprint(res)","0eee23d5":"fName = f'DTWithStumpsBestCV{height}'\nSaveResults(testFeaturesNew, lambda row: clfBest.Predict([row])[0], fName)\nprint(PrecheckSubmission(fName))","c96d5ae8":"def MajorityFormula(items):\n    cntOnes = 0\n    for item in items:\n        if item[1] == 1:\n            cntOnes += 1\n    return 1 if cntOnes * 2 >= len(items) else 0\n\ndef DistanceFormula(items):\n    summa, norm = 0, 0\n    for item in items:\n        summa += item[1] \/ item[0]\n        norm += 1 \/ item[0]\n    return summa \/ norm\n\nnorm = [np.max(featuresNew[:,k]) - np.min(featuresNew[:,k]) for k in range(featuresNew.shape[1])]\n\ndef calcDist(f1, f2):\n#     print(norm)\n    v = (f1 - f2) \/ curnorm\n    return np.dot(v, v)\n        \n\ndef RunKNN(features, targets, testFeatures, k, formula):\n    result = []\n    for row in testFeatures:\n        items = [[calcDist(feature, row), targets[i]] for i, feature in enumerate(features)]\n        items.sort()\n        result.append(formula(items[:k]))\n    return result","6c50c1f0":"# iterCount = 1\n# trainSize = 800\n\n# bestK = None\n# bestAUC = 0\n# for k in range(1, 500):\n#     if k % 20 == 0:\n#         print(bestK, bestAUC)\n#     for iter in range(iterCount):\n#         np.random.seed(0)\n#         train, test, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, trainSize)\n#         auc = roc_auc_score(testTargets, RunKNN(train, trainTargets, test, k, DistanceFormula))\n#         print(auc)\n#         if bestK is None or bestAUC < auc:\n#             bestK = k\n#             bestAUC = auc\n# print(\"RESULT:\")\n# print(bestK)\n# print(bestAUC)","bbdd997e":"import itertools\n\nk = 39\ntrainSize = 800\nnp.random.seed(0)\nttrain, ttest, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, trainSize)\nfor r in range(featuresNew.shape[1 - 1], -1, -1):\n    for i in itertools.combinations(list(range(featuresNew.shape[1])), r):\n        mask = deepcopy(i)\n        train, test = deepcopy(ttrain[:, mask]), deepcopy(ttest[:,mask])\n        curnorm = [norm[x] for x in mask]\n        auc = roc_auc_score(testTargets, RunKNN(train, trainTargets, test, k, DistanceFormula))\n        if auc > 0.86:\n            print(*i, auc)\n    print(r, 'done')","c4c62442":"goodIndices = [\n    ( 0, 2, 3, 4, 5, 6, 7, 8, 10, 11 ), \n    ( 0, 1, 2, 3, 4, 5, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 4, 5, 6, 7, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 8, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 8, 11 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 11 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 12 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 9, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 9, 11 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 10, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 9, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 9, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 10, 11 ), \n    ( 0, 2, 4, 5, 6, 7, 8, 10, 12 ), \n    ( 0, 1, 2, 3, 4, 5, 6, 7 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 9 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 11 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 12 ), \n    ( 0, 1, 2, 4, 5, 6, 7, 8 ), \n    ( 0, 1, 2, 4, 5, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 9 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 11 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 10, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 10, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 11, 12 ), \n    ( 0, 2, 3, 5, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 5, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 5, 6, 7, 8, 11 ), \n    ( 0, 2, 3, 5, 6, 7, 8, 12 ), \n    ( 0, 2, 4, 5, 6, 7, 8, 9 ), \n    ( 0, 2, 4, 5, 6, 7, 8, 10 ), \n    ( 0, 2, 4, 5, 6, 7, 8, 12 ), \n    ( 0, 2, 4, 5, 6, 7, 9, 10 ), \n    ( 0, 2, 4, 5, 6, 7, 10, 12 ), \n    ( 0, 2, 4, 6, 7, 8, 9, 10 ), \n    ( 0, 2, 4, 6, 7, 8, 10, 12 ), \n    ( 0, 3, 4, 5, 6, 7, 8, 9 ), \n    ( 0, 3, 4, 5, 6, 7, 8, 12 ), \n    ( 0, 3, 4, 5, 6, 7, 9, 12 ), \n    ( 2, 3, 4, 5, 6, 7, 8, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7 ), \n    ( 0, 1, 2, 3, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 6, 7, 10 ), \n    ( 0, 1, 2, 4, 5, 6, 7 ), \n    ( 0, 1, 2, 4, 6, 7, 8 ), \n    ( 0, 1, 2, 4, 6, 7, 9 ), \n    ( 0, 1, 2, 4, 6, 7, 10 ), \n    ( 0, 1, 2, 4, 6, 7, 12 ), \n    ( 0, 1, 3, 4, 5, 6, 7 ), \n    ( 0, 1, 3, 4, 6, 7, 8 ), \n    ( 0, 1, 4, 5, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 5, 6, 7 ), \n    ( 0, 2, 3, 4, 5, 7, 8 ), \n    ( 0, 2, 3, 4, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 6, 7, 9 ), \n    ( 0, 2, 3, 4, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 12 ), \n    ( 0, 2, 3, 4, 7, 8, 10 ), \n    ( 0, 2, 3, 5, 6, 7, 8 ), \n    ( 0, 2, 3, 5, 6, 7, 9 ), \n    ( 0, 2, 3, 5, 6, 7, 10 ), \n    ( 0, 2, 3, 5, 6, 7, 11 ), \n    ( 0, 2, 3, 5, 6, 7, 12 ), \n    ( 0, 2, 3, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 6, 7, 8, 11 ), \n    ( 0, 2, 3, 6, 7, 8, 12 ), \n    ( 0, 2, 3, 6, 7, 10, 11 ), \n    ( 0, 2, 3, 6, 7, 10, 12 ), \n    ( 0, 2, 4, 5, 6, 7, 8 ), \n    ( 0, 2, 4, 5, 6, 7, 9 ), \n    ( 0, 2, 4, 5, 6, 7, 10 ), \n    ( 0, 2, 4, 5, 6, 7, 12 ), \n    ( 0, 2, 4, 6, 7, 8, 9 ), \n    ( 0, 2, 4, 6, 7, 8, 10 ), \n    ( 0, 2, 4, 6, 7, 8, 12 ), \n    ( 0, 2, 4, 6, 7, 10, 11 ), \n    ( 0, 2, 5, 6, 7, 8, 9 ), \n    ( 0, 2, 5, 6, 7, 8, 10 ), \n    ( 0, 3, 4, 5, 6, 7, 8 ), \n    ( 0, 3, 4, 5, 6, 7, 9 ), \n    ( 0, 3, 4, 5, 6, 7, 12 ), \n    ( 0, 3, 4, 6, 7, 8, 9 ), \n    ( 0, 3, 4, 6, 7, 8, 12 ), \n    ( 0, 4, 5, 6, 7, 8, 9 ), \n    ( 2, 3, 4, 5, 6, 7, 8 ), \n    ( 2, 3, 4, 5, 6, 7, 10 ), \n    ( 2, 3, 4, 6, 7, 8, 10 ), \n    ( 0, 1, 2, 3, 6, 7 ), \n    ( 0, 1, 2, 4, 6, 7 ), \n    ( 0, 1, 2, 6, 7, 10 ), \n    ( 0, 1, 2, 6, 7, 12 ), \n    ( 0, 1, 3, 4, 6, 7 ), \n    ( 0, 1, 4, 5, 6, 7 ), \n    ( 0, 1, 4, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 6, 7 ), \n    ( 0, 2, 3, 5, 6, 7 ), \n    ( 0, 2, 3, 6, 7, 8 ), \n    ( 0, 2, 3, 6, 7, 9 ), \n    ( 0, 2, 3, 6, 7, 10 ), \n    ( 0, 2, 3, 6, 7, 11 ), \n    ( 0, 2, 3, 6, 7, 12 ), \n    ( 0, 2, 4, 5, 6, 7 ), \n    ( 0, 2, 4, 6, 7, 8 ), \n    ( 0, 2, 4, 6, 7, 9 ), \n    ( 0, 2, 4, 6, 7, 10 ), \n    ( 0, 2, 4, 6, 7, 11 ), \n    ( 0, 2, 4, 6, 7, 12 ), \n    ( 0, 2, 5, 6, 7, 8 ), \n    ( 0, 2, 5, 6, 7, 9 ), \n    ( 0, 2, 5, 6, 7, 10 ), \n    ( 0, 2, 6, 7, 8, 9 ), \n    ( 0, 2, 6, 7, 8, 10 ), \n    ( 0, 3, 4, 5, 6, 7 ), \n    ( 0, 3, 4, 6, 7, 8 ), \n    ( 0, 3, 4, 6, 7, 9 ), \n    ( 0, 3, 4, 6, 7, 11 ), \n    ( 0, 3, 4, 6, 7, 12 ), \n    ( 0, 3, 5, 6, 7, 12 ), \n    ( 0, 4, 5, 6, 7, 8 ), \n    ( 0, 4, 5, 6, 7, 9 ), \n    ( 0, 4, 6, 7, 8, 9 ), \n    ( 2, 3, 4, 5, 6, 7 ), \n    ( 2, 3, 4, 6, 7, 8 ), \n    ( 2, 3, 4, 6, 7, 10 ), \n    ( 2, 4, 5, 6, 7, 8 ), \n    ( 0, 1, 2, 6, 7 ), \n    ( 0, 1, 3, 6, 7 ), \n    ( 0, 2, 3, 6, 7 ), \n    ( 0, 2, 4, 6, 7 ), \n    ( 0, 2, 5, 6, 7 ), \n    ( 0, 2, 6, 7, 8 ), \n    ( 0, 2, 6, 7, 9 ), \n    ( 0, 2, 6, 7, 10 ), \n    ( 0, 2, 6, 7, 11 ), \n    ( 0, 2, 6, 7, 12 ), \n    ( 0, 3, 4, 6, 7 ), \n    ( 0, 3, 5, 6, 7 ), \n    ( 0, 3, 6, 7, 8 ), \n    ( 0, 4, 5, 6, 7 ), \n    ( 0, 4, 6, 7, 8 ), \n    ( 2, 3, 4, 6, 7 ), \n    ( 2, 4, 5, 6, 7 ), \n    ( 2, 4, 6, 7, 8 ), \n    ( 0, 2, 6, 7 ), \n    ( 0, 3, 6, 7 ), \n    ( 0, 4, 6, 7 ), \n    ( 2, 4, 6, 7 ), \n    ( 3, 4, 6, 7 ), \n]","84d77e9f":"k = 39\ntrainSize = 800\niterCount = 10\n\nfor i in goodIndices:\n    mask = deepcopy(i)\n    np.random.seed(0)\n    sumAuc = 0\n    for j in range(iterCount):\n        ttrain, ttest, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, trainSize)\n        train, test = deepcopy(ttrain[:, mask]), deepcopy(ttest[:,mask])\n        curnorm = [norm[x] for x in mask]\n        auc = roc_auc_score(testTargets, RunKNN(train, trainTargets, test, k, DistanceFormula))\n        sumAuc += auc\n    print(*i, sumAuc \/ iterCount)","7f8d1af4":"goodCVIndices = [\n    ( 0, 1, 2, 3, 4, 5, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 4, 5, 6, 7, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 9, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 9, 10 ), \n    ( 0, 1, 2, 3, 4, 5, 6, 7 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 9 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7, 12 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 9 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 8, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 9, 12 ), \n    ( 0, 2, 3, 4, 6, 7, 10, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 10, 12 ), \n    ( 0, 2, 3, 5, 6, 7, 8, 10 ), \n    ( 0, 1, 2, 3, 4, 6, 7 ), \n    ( 0, 1, 2, 3, 6, 7, 8 ), \n    ( 0, 1, 2, 3, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 5, 6, 7 ), \n    ( 0, 2, 3, 4, 6, 7, 8 ), \n    ( 0, 2, 3, 4, 6, 7, 9 ), \n    ( 0, 2, 3, 4, 6, 7, 10 ), \n    ( 0, 2, 3, 4, 6, 7, 11 ), \n    ( 0, 2, 3, 4, 6, 7, 12 ), \n    ( 0, 2, 3, 5, 6, 7, 8 ), \n    ( 0, 2, 3, 5, 6, 7, 9 ), \n    ( 0, 2, 3, 5, 6, 7, 10 ), \n    ( 0, 2, 3, 6, 7, 8, 9 ), \n    ( 0, 2, 3, 6, 7, 8, 10 ), \n    ( 0, 1, 2, 3, 6, 7 ), \n    ( 0, 2, 3, 4, 6, 7 ), \n    ( 0, 2, 3, 5, 6, 7 ), \n    ( 0, 2, 3, 6, 7, 8 ), \n    ( 0, 2, 3, 6, 7, 9 ), \n    ( 0, 2, 3, 6, 7, 10 ), \n    ( 0, 2, 3, 6, 7, 12 ), \n    ( 0, 2, 4, 6, 7, 10 ), \n    ( 0, 3, 4, 6, 7, 9 ), \n    ( 2, 3, 4, 5, 6, 7 ), \n    ( 2, 3, 4, 6, 7, 8 ), \n    ( 2, 3, 4, 6, 7, 10 ), \n    ( 0, 2, 3, 6, 7 ), \n    ( 0, 2, 4, 6, 7 ), \n    ( 0, 3, 4, 6, 7 ), \n    ( 2, 3, 4, 6, 7 ), \n    ( 0, 3, 6, 7 ), \n]","db0d7e51":"result = []","e75de314":"for i in reversed(goodCVIndices[:10]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","3fb38334":"for i in reversed(goodCVIndices[10:20]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","805b7bd1":"for i in reversed(goodCVIndices[20:30]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","a8578db6":"for i in reversed(goodCVIndices[30:40]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","8e7d7b41":"for i in reversed(goodCVIndices[40:50]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","98952dd2":"for i in reversed(goodCVIndices[50:]):\n    mask = deepcopy(i)\n    train = deepcopy(featuresNew[:, mask])\n    curnorm = [norm[x] for x in mask] \n    result.append(RunKNN(train, targets, testFeaturesNew[:,mask], k, DistanceFormula))","15bb0fac":"result = np.array(result)\nresultBackUp = deepcopy(result)","ff0e69dc":"def funk(probs):\n    return np.sum([round(x) if abs(x - round(x)) < 0.25 else x for x in probs]) \/ probs.shape[0]\n\nfinalResult = np.array([funk(result[:,k]) for k in range(result.shape[1])])\n","b865ab55":"cntGood, cntOnes = 0, 0\neps = 0.01\nfor x in finalResult:\n    if x < eps or x > 1 - eps:\n        cntGood += 1\nprint(cntGood)","8120ee06":"def SavePureResult(result, fileName):\n    global testData\n    result = pd.DataFrame({\n        'target': result,\n        'id': [int(row[-1]) for row in testData]\n    })\n    result.to_csv(fileName + '.csv',index=False)","7be79463":"SavePureResult(finalResult, 'LastHope')","57887031":"np.random.seed(0)\n\niterCount = 10\nk = 39\nmask = [0, 2, 3, 4, 6, 7]\nsumAuc = 0\nfor j in range(iterCount):\n    ttrain, ttest, trainTargets, testTargets = SplitDatasetForTrainAndTest(featuresNew, targets, trainSize)\n    train, test = deepcopy(ttrain[:, mask]), deepcopy(ttest[:,mask])\n    curnorm = [norm[x] for x in mask]\n    auc = roc_auc_score(testTargets, RunKNN(train, trainTargets, test, k, DistanceFormula))\n    sumAuc += auc\nprint(sumAuc \/ iterCount)","ba1e9c7e":"SavePureResult(RunKNN(featuresNew[:,mask], targets, testFeaturesNew[:,mask], k, DistanceFormula), 'kek')","a31e4820":"\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 $\\texttt{DivisionRule}, \\texttt{StopRule}$ \u0438 $\\texttt{MarkRule}$ \u043c\u043e\u0436\u043d\u043e \u0441\u0432\u043e\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c","8745f970":"\u042d\u0442\u043e \u043d\u0430\u0431\u0440\u0430\u043b\u043e $0.38$","122612a6":"# \u0410 \u0432\u043e\u0442 \u0438 \u043d\u0430\u0448\u0435 \u0440\u0435\u0448\u0430\u044e\u0448\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e","cf94cdb9":"\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u043d\u0430\u0448 \u043a\u043e\u0434 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u0441\u0447\u0438\u0442\u0430\u0435\u0442","79ef0f44":"\u0422\u0430\u043a, \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0443 \u043c\u0435\u043d\u044f \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0430\u0441\u044c","b9e9d478":"\u0411\u0443\u0434\u0435\u043c \u0441\u0447\u0438\u0442\u0430\u0442\u044c, \u0447\u0442\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043b\u0438","ab33ac01":"### \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0435\u0449\u0435 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u043d\u0430 \u043f\u043d\u044f\u0445","037ae4fe":"# \u0412\u043e\u0442 \u0442\u0443\u0442 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f ","3ae8716c":"# \u0422\u0430\u043a, \u0442\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u0440\u0438\u043a\u0438\u043d\u0435\u043c \u0447\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u043c\u043e\u0436\u0435\u0442 \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c","374c18a5":"\u041d\u0443 \u044d\u0442\u043e \u043d\u0435\u043f\u043b\u043e\u0445\u043e \u0434\u0435\u043b\u0438\u0442, \u0442\u0430\u043c \u0432\u0441\u0435\u0433\u043e \u043e\u0434\u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0447\u043a\u0430 \u0441\u0440\u0435\u0434\u0438 \u043d\u0443\u043b\u0435\u0439 (\u043d\u0430 \u043d\u0435\u0435 \u043c\u044b \u043f\u043e\u043a\u0430 \u0437\u0430\u0431\u044a\u0435\u043c)","0173f8fc":"\u0412\u043e\u0442 \u0442\u0443\u0442 \u044f \u043f\u043e\u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u043b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u043a\u0438\u043d\u0443\u0442\u044c\n\n\u041d\u0443 \u0432\u043e\u0442 \u0442\u0443\u0442 \u043f\u043e\u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c, \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u0442\u043e\u043c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0437\u0430\u0441\u043b\u0430\u0442\u044c \u044d\u0442\u043e","baa5242b":"\u0422\u0430\u043a, \u043d\u0443 \u0431\u0443\u0441\u0442 \u043d\u0430 \u043f\u043d\u044f\u0445 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u0435\u0431\u044f \u043f\u0440\u043e\u044f\u0432\u0438\u043b, \u043a\u0430\u0436\u0435\u0442\u0441\u044f","e4f0812f":"\u041b\u0443\u0447\u0448\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b:\n\n```\n0 2 3 4 6 7 0.9237777102945642\n\n```","b11d8f2b":"\u0412\u0438\u0434\u043d\u043e, \u0447\u0442\u043e $k = 39$ \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0445\u043e\u0440\u043e\u0448\u043e","4cb41d42":"0.93 \u043d\u0430\u0431\u0438\u0440\u0430\u0435\u0442","eb884c15":"## \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0435\u0449\u0435 kNN\n\n\u0410 \u0442\u043e \u043f\u043e\u043a\u0430 \u0447\u0442\u043e \u044f \u043d\u0435 \u0433\u043e\u0442\u043e\u0432 \u0435\u0449\u0435 \u043f\u0438\u0441\u0430\u0442\u044c \u0434\u0435\u0440\u0435\u0432\u044c\u044f","4bbe2235":"\u041d\u0443 \u0432\u043e\u043e\u0431\u0449\u0435\u043c \u0433\u043b\u044f\u0434\u044f \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u0447\u0442\u043e \u0440\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443","3ab2727c":"\u041a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e 174 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c. \u041d\u043e \u0436\u0434\u0430\u043b \u044f \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u043b\u0433\u043e, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u043d\u0430 \u043f\u043b\u044e\u0441\u0430\u0445 \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c. \u0427\u0442\u043e-\u0442\u043e \u043a\u0430\u043a-\u0442\u043e \u0442\u0443\u0445\u043b\u0435\u043d\u044c\u043a\u043e","52174776":"\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0432\u0441\u0435 \u0433\u0440\u0443\u043f\u043f\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u0431\u0440\u0430\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 0.87, \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0434\u043b\u044f \u043d\u0438\u0445 \u0447\u0442\u043e-\u0442\u043e \u0442\u0438\u043f\u0430 $CV$ \u0438 \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0438\u0437 \u043d\u0438\u0445, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e $CV$ \u043d\u0435\u043f\u043b\u043e\u0445\u043e\u0435","1375486d":"\u0418 \u0442\u0443\u0442 \u044f \u043f\u043e\u043d\u044f\u043b, \u0447\u0442\u043e \u044f \u043e\u0448\u0438\u0431\u0441\u044f, \u0442\u0430\u043c \u043d\u0435 $\\texttt{imputedTestFeatures}$ \u043d\u0430\u0434\u043e, \u0430 $\\texttt{testFeaturesNew}$","2b8b1ec5":"\u0422\u0435\u043f\u0435\u0440\u044c \u0432\u043e\u0442 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 (\u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043a\u043e\u0440 \u0431\u043e\u043b\u044c\u0448\u0435 $0.89$). \n\n\u0418 \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0447\u0442\u043e-\u0442\u043e \u0442\u0438\u043f\u0430 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430","64e510e2":"\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0432\u043d\u0430\u0447\u0430\u043b\u0435 \u0440\u0430\u0437\u0431\u0435\u0440\u0435\u043c\u0441\u044f \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438","5654ce75":"\u041c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \u0412\u043e\u0442 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c train \u0438 test \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u044b. \u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0443 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0438 \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0443 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0410 \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0435\u0441\u044f \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u044b \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430.","39ba707e":"### \u0415\u0449\u0435 \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\n\n\u041f\u0440\u043e\u0441\u0442\u043e, \u044f \u0445\u043e\u0447\u0443 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0443\u0437\u043d\u0430\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u043d\u0438\u0445, \u0430 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u0445 \u043d\u0435\u0442 \u0442\u0430\u043a\u043e\u0439 \u043e\u043f\u0446\u0438\u0438","06e4e62a":"\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0435\u0449\u0435 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e \u0441\u0430\u043c\u0443\u044e \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u0430\u0441\u043a\u0443 \u0437\u0430\u0441\u043b\u0430\u0442\u044c","d60060be":"0.91 \u043d\u0430\u0431\u0440\u0430\u043b\u043e\u0441\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u044d\u0442\u043e \u0438 \u043d\u0430\u043f\u0438\u0448\u0435\u043c \u0432\u043d\u0430\u0447\u0430\u043b\u0435 ","404762b6":"# \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f","531393b8":"# \u0412\u043e\u0442 \u0442\u0443\u0442 \u0437\u0430\u043a\u0430\u043d\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u0435\u0434\u043f\u043e\u0434\u0441\u0447\u0435\u0442","4ae2c7f1":"\u041d\u0430\u0431\u0438\u0440\u0430\u0435\u0442 $0.89241$. \u042d\u0442\u043e, \u043f\u043e \u0438\u0434\u0435\u0435, \u043f\u0435\u0440\u0432\u0430\u044f \u043c\u043e\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0435\u0437 \u0432\u043d\u0435\u0448\u043d\u0438\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","d1e6760d":"\u0425\u043c, \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u0441\u0442\u0430\u043b\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u0445\u0443\u0436\u0435. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f 0.89557","26a32376":"\u0412\u043e\u0442 \u044d\u0442\u043e $0.88$ \u043d\u0430\u0431\u0438\u0440\u0430\u0435\u0442","703b1066":"CV \u0441\u0430\u043c\u0430\u044f \u0431\u043e\u043b\u044c\u0448\u0430\u044f \u0434\u043b\u044f \u043d\u0435\u0435 \u0438\u0437 \u0432\u0441\u0435\u0433\u043e, \u0447\u0442\u043e \u0431\u044b\u043b\u043e (0.94). \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u044d\u0442\u043e \u0438 \u0434\u043e\u043b\u0436\u043d\u043e \u043d\u0430\u0431\u0438\u0440\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u0441\u0435\u0433\u043e","6b46c76b":"\u041c\u044b \u0435\u0449\u0435 \u0440\u0430\u0437\u043e\u0431\u044a\u0435\u043c \u043a\u043e\u0434 \u043d\u0430 \u0431\u043b\u043e\u043a\u0438, \u0447\u0442\u043e\u0431\u044b \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0435 \u043a\u043b\u0435\u0442\u043e\u043a (\u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e)"}}