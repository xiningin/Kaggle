{"cell_type":{"9a91250d":"code","3e0e769c":"code","d8e584d4":"code","6a857172":"code","fd286273":"code","d6951f82":"code","25f9eee2":"code","c7d87ba0":"code","f1160145":"code","2c9bac98":"code","f3a4f993":"code","d4c554e5":"code","3d036c64":"code","c58ca19d":"code","61a44db9":"code","e081e745":"code","dbbc1715":"code","a8666147":"code","4458bbc4":"code","4afab217":"code","f9606b80":"code","70270b3f":"code","2db18fa5":"code","c39135bd":"code","686819a8":"code","51252b22":"code","3ba833a1":"code","ab6a93fa":"code","53317e88":"code","6b109d99":"code","ac8fc470":"code","564bd6b9":"code","bfc331f8":"code","380d100d":"code","de757c0f":"code","17880058":"code","d8b4c9c8":"code","502ae8f4":"code","234ff4be":"code","b5e35987":"code","33f7b923":"code","d5e4d80f":"code","ce403c41":"code","dfe62472":"code","a553e891":"code","2abe731e":"code","9741cda9":"code","a3641ae2":"code","724f8d66":"code","ac7299e6":"markdown","e997d893":"markdown","3ce0f343":"markdown","77ba5d7b":"markdown","2267d778":"markdown","bb464aec":"markdown","cc1e3ceb":"markdown","4bbab8f1":"markdown","15a1ffd2":"markdown","8226b65f":"markdown","80eb8b86":"markdown","4a164f38":"markdown","af5dd251":"markdown","10be2b83":"markdown"},"source":{"9a91250d":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt","3e0e769c":"# Added the Dataset from Kaggle Datasets.\nimages_path = '..\/input\/flickr8k-sau\/Flickr_Data\/Images\/'\nimages = glob(images_path+'*.jpg')          # Glob function() used to read the image from the directory and just read the jpg files.\nlen(images)        #Length of the Images","d8e584d4":"images[:5]","6a857172":"for i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])                       # Reading the Path of the Images.\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                    #Converting the Image into RGB.\n    plt.imshow(img)","fd286273":"# ResNet-50 is a convolutional neural network that is 50 layers deep. You can load a pretrained version of the network trained on more than a million images\nfrom keras.applications import ResNet50                     \n\nincept_model = ResNet50(include_top=True)","d6951f82":"incept_model.summary()     #Summary of the model","25f9eee2":"#We have to discard the prediction layer and want the output avg_pooling laye which is the second Last layer.\nfrom keras.models import Model\nlast = incept_model.layers[-2].output\nmodele = Model(inputs = incept_model.input,outputs = last)\nmodele.summary()","c7d87ba0":"#Create a dictionary key will the name of the lmage and the value will be the vector of that image.\nimages_features = {}\ncount = 0\nfor i in images:\n    img = cv2.imread(i)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)            #Cv2 reads the image from the RGB format that the resason to the convert it into RGB.\n    img = cv2.resize(img, (224,224))              #Resize the image because the input size required to give RESNET is (244,244,3).\n    \n    img = img.reshape(1,224,224,3)                 #Reshape to transfer the image to the resnet model.\n    pred = modele.predict(img).reshape(2048,)        #Predicting the image vector of 2048 values.\n        \n    img_name = i.split('\/')[-1]                  #Spliting is done to extract just the name of the image from the complete path of image.\n    \n    images_features[img_name] = pred              #Earlier Create dictionary will contain the image name as the key and the value will the 2048 values of the image vector.\n    \n    count += 1\n    \n    #We are just working on 1500 images from the dataset due to memory limitations so under loop count will be 1499 the loop will be break down. \n    if count > 1499:\n        break\n        \n    elif count % 50 == 0:\n        print(count)\n    \n        ","f1160145":"# Printing the length of the images and its features.\nlen(images_features)","2c9bac98":"# Assigning the path of token.txt\ncaption_path = '..\/input\/flickr8k-sau\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'","f3a4f993":"# Reading the caption_path and spliting it with new line.\ncaptions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')","d4c554e5":"# Ex: The 5 captions different captions for 2 different images.\ncaptions[:10]","3d036c64":"#total Loength of captions of all the images\nlen(captions)","c58ca19d":"#Created a Dictionary which will have image Name as the key and the values will the list of captions of an all the images.\ncaptions_dict = {}\n\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2]                       #Spliting the name of image.\n        caption = i.split('\\t')[1]                             #Spliting the caption.\n        \n        # Appending the all the captions as values in dictionary of a specific image.\n        if img_name in images_features:                            \n            if img_name not in captions_dict:                               \n                captions_dict[img_name] = [caption]\n                \n            else:\n                captions_dict[img_name].append(caption)             \n            \n    except:\n        pass","61a44db9":"# Ex: Checking the five Captions of the image.\ncaptions_dict['1002674143_1b742ab4b8.jpg']","e081e745":"# Total length of dictionary\nlen(captions_dict)","dbbc1715":"#Function to add start and end tokens to all the captions\ndef preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified","a8666147":"#Token added \nfor k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)\ncaptions_dict['1002674143_1b742ab4b8.jpg']","4458bbc4":"# Dictionary which will store vocabulary and it's respective numeric value.\ncount_words = {}\nfor k,vv in captions_dict.items():\n    for v in vv:                       # vv is the list of 5 captions of each image\n        for word in v.split():                 #spliting the captions into words to get vocabulory.\n            if word not in count_words:\n\n                count_words[word] = 0\n\n            else:\n                count_words[word] += 1","4afab217":"# count_words","f9606b80":"len(count_words)","70270b3f":"THRESH = -1\ncount = 1\nnew_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        new_dict[k] = count\n        count += 1","2db18fa5":"new_dict","c39135bd":"len(new_dict)","686819a8":"new_dict['<OUT>'] = len(new_dict) ","51252b22":"len(new_dict)","3ba833a1":"captions_backup = captions_dict.copy()","ab6a93fa":"captions_dict = captions_backup.copy()","53317e88":"for k, vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():  \n            if word not in new_dict:\n                encoded.append(new_dict['<OUT>'])\n            else:\n                encoded.append(new_dict[word])\n\n\n        captions_dict[k][vv.index(v)] = encoded","6b109d99":"# captions_dict","ac8fc470":"from keras.utils import to_categorical                # Converts a class vector (integers) to binary class matrix.\nfrom keras.preprocessing.sequence import pad_sequences                            # pad_sequences is used to ensure that all sequences in a list have the same length     ","564bd6b9":"MAX_LEN = 0\nfor k, vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","bfc331f8":"MAX_LEN","380d100d":"# captions_dict","de757c0f":"Batch_size = 5000\nVOCAB_SIZE = len(new_dict)\n\ndef generator(photo, caption):\n    n_samples = 0\n    \n    X = []          # Ex : <sos>\n    y_in = []       # Ex : cat\n    y_out = []       # predicted word\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range(1, len(v)):\n                X.append(photo[k])           # Append the image features to X variable.\n\n                in_seq= [v[:i]]          # Spliting the word according to the i variable.\n                out_seq = v[i]              # The next word.\n\n                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]   # Ensuring the the captions are of same length by assigning 0.          \n                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]             \n\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n            \n    return X, y_in, y_out\n    \n    ","17880058":"X, y_in, y_out = generator(images_features, captions_dict)","d8b4c9c8":"# Converting in Numpy array for processing  Faster\nX = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')","502ae8f4":"X.shape, y_in.shape, y_out.shape","234ff4be":"#Ex. \nX[1510]","b5e35987":"y_in[2]","33f7b923":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model","d5e4d80f":"embedding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(new_dict)\n\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","ce403c41":"model.fit([X, y_in], y_out, batch_size=512, epochs=100)","dfe62472":"inv_dict = {v:k for k, v in new_dict.items()}","a553e891":"model.save('model.h5')","2abe731e":"model.save_weights('mine_model_weights.h5')","9741cda9":"np.save('vocab.npy', new_dict)","a3641ae2":"def getImage(x):\n    \n    test_img_path = images[x]\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (224,224))\n\n    test_img = np.reshape(test_img, (1,224,224,3))\n    \n    return test_img","724f8d66":"for i in range(5):\n    \n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(new_dict[i])\n\n        encoded = [encoded]\n\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n        \n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","ac7299e6":"# Text Preprocess","e997d893":"## Discarding the Last Layer and the second last layer will be the output:","3ce0f343":"# Create Vocabulary","77ba5d7b":"# **Build Generator Function**","2267d778":"# Importing Libraries ","bb464aec":"# **PREDICTIONS**","cc1e3ceb":"**we have created a deep learning model which can create captions by watching a image. Used keras in order to create the model and used resnet pretrained model which is already available in keras and we have used resnet for extracting the features from the image.and used long short-term memory cells for predicting the captions.**","4bbab8f1":"# Image Preprocess","15a1ffd2":"# **MODEL**","8226b65f":"# Adding start and End Tokens to all the captions:","80eb8b86":"## Transfering all the images to RESNET model to get Vectors","4a164f38":"# Visualizing the Five Images","af5dd251":"# Now assigning numerical values of words in the captions Dictionary","10be2b83":"# Importing the Pre Trained Model RESNET50 to extract the features from the images"}}