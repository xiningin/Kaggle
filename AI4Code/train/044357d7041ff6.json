{"cell_type":{"86a2a780":"code","7e02bfd0":"code","7000d909":"code","7a2a4891":"code","af550b62":"code","3a4d8219":"code","198fb3e4":"code","08fa134a":"code","7a5a0193":"code","48aa61d1":"code","0ba3e4f0":"code","27807529":"code","98ab3a23":"code","e43ef90e":"code","05709763":"code","d336f60a":"code","ee83f09b":"code","a80fe732":"code","64775f9d":"code","3148ab8a":"code","d9c7a46b":"code","57fb6b57":"code","7b472014":"code","ae16bee1":"code","ad851d9c":"code","db556d7f":"code","b63cda2d":"code","d267700f":"code","bdac297a":"code","69d74b66":"code","8b693573":"code","5fa788ef":"code","ac3ad764":"code","eaccc7ef":"code","cc1973b9":"code","57319e9c":"code","3d085be9":"code","dc890e40":"code","5e8aec54":"markdown","77480331":"markdown","5b17a91d":"markdown","0e609b86":"markdown","9e8d7a74":"markdown","6f00fca9":"markdown","24787704":"markdown","786da6d9":"markdown","caf72575":"markdown","2a12ea3c":"markdown","7c977ac9":"markdown","5f891a99":"markdown","a0d89f01":"markdown","57bc7110":"markdown","7f5a52ac":"markdown","0f067769":"markdown","b047367d":"markdown","6748d444":"markdown","d11ee29f":"markdown","ff396c62":"markdown","69b7bafe":"markdown","6c4ff262":"markdown","b56c7586":"markdown","95eec4c6":"markdown","b96f7e80":"markdown","eb92f5e2":"markdown","1a8e39da":"markdown","e3740cc8":"markdown","0f48abf4":"markdown","ff3b190d":"markdown","be8e85f8":"markdown","fc524590":"markdown"},"source":{"86a2a780":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random as rn\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\nfrom plotly.offline import init_notebook_mode, iplot \nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport pycountry\npy.init_notebook_mode(connected=True)\nimport folium \nfrom folium import plugins\n\n#for q-q plot\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.gofplots import qqplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n#vis\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.manifold import TSNE\nfrom scipy import sparse\n\n#Modeling\nimport tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb","7e02bfd0":"train=pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsub=pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ndef get_dataset_information(df):\n    '''\n    Take a first view of dataset\n    '''\n    print(\"Number of Columns in dataset:\",df.shape[1])\n    print(\"Number of Rows in dataset:\",df.shape[0])\n    print(\"Name of Columns in dataset:\",df.columns)\n    print(df.describe())\n    print(df.info())\n    return df.head(5)\n\ndef calculate_missing_percentage(df):\n    ''' calculate missing percentage in data and stored as missing_stat\n    Input: Give dataset, in which you like to check missing values\n    '''\n    missing_stat=df.isnull().sum()\/len(df)*100\n    #convert missing_stat into dataframe\n    prod_count = pd.DataFrame(missing_stat.sort_index())\n    plt.figure()\n    #plot in barplot\n    sns.barplot(x=missing_stat.index, y=missing_stat.values, alpha=0.8)\n    plt.title('Percent Missing')\n    plt.ylabel('Missing', fontsize=12)\n    plt.xlabel('Features', fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()\n    \ndef draw_num_plot(df,column,col):\n    '''Draw KDE plot for given dataset and particular column\n    Inputs: Pass dataframe and particular column and also pass color for making interactive map\n    '''\n    plt.figure(figsize=(15,8))\n    mean=df[column].mean()\n    median=df[column].median()\n    sns.axes_style(\"dark\")\n    ax=sns.kdeplot(data=df, x=column,fill=True,color=col)\n    ax.axvline(mean, color='crimson', linestyle='dotted')\n    ax.axvline(median, color='g', linestyle='-')\n    plt.legend({'Mean':mean,'Median':median})\n    plt.show()\n    \n\ndef q_q_plot(df,column):\n    '''Check data column follow normal distribution or not by using Q-Q plot\n    '''\n    data = df[column]\n    # q-q plot\n    plt.figure(figsize=(15,8))\n    qqplot(data, line='s')\n    \n    plt.show()\n\ndef draw_correlation_matrix(df):\n    plt.figure(figsize=(18,6))\n    #find correlation between data columns\n    corrMatrix = df.corr()\n    #plot by using heatmap\n    sns.heatmap(corrMatrix,annot = True, vmin=-1, vmax=1, center= 0)\n    plt.show()\n    \n\ndef rmse_score(yreal, yhat):\n    return sqrt(mean_squared_error(yreal, yhat))","7000d909":"train.head()","7a2a4891":"get_dataset_information(train)\ncalculate_missing_percentage(train)","af550b62":"get_dataset_information(test)\ncalculate_missing_percentage(test)","3a4d8219":"train['target'].describe()","198fb3e4":"draw_num_plot(train,'target','blue')\nq_q_plot(train,'target')","08fa134a":"train['standard_error'].describe()","7a5a0193":"draw_num_plot(train,'standard_error','red')\nq_q_plot(train,'standard_error')","48aa61d1":"draw_correlation_matrix(train)","0ba3e4f0":"x1 = train['target'].value_counts().index.values\nx2 = train['standard_error'].value_counts().index.values\nsns.jointplot(x=x1,y=x2,data=train,kind='reg',color='crimson')","27807529":"import plotly.graph_objects as go\nfrom fastcore.all import *\n#let's see the extract which have maximum score in target and also see which one have min score in target\n_max_target=pd.DataFrame(train[train['target']==train['target'].max()])\n_min_target=pd.DataFrame(train[train['target']==train['target'].min()])\n_zero_target=pd.DataFrame(train[train['target']==0])\ndef look_at_text(df):\n    fig = go.Figure(data=[go.Table(\n        columnwidth = [200,600],\n        header=dict(values=['Excerpt('+str(df['target'].values[0])+')'],\n                    line_color='darkslategray',\n                    fill_color='darkblue',\n                    align='left',  \n                    font=dict(color='white', size=16)\n                   ),\n        cells=dict(values=df['excerpt'].T, \n                   line_color='darkslategray',\n                   fill_color=['lightgray','white'],\n                   align='left'))\n    ])\n    dslabel   = df['excerpt'].iloc[0]\n    fig.update_layout(width=1000, height=350)\n    fig.update_layout(title=go.layout.Title(text=\"Excerpt with target\", font=dict(\n                family=\"Courier New, monospace\",\n                size=22,\n                color=\"#0000FF\"\n            )))\n    fig.show()\nlook_at_text(_min_target)\nlook_at_text(_zero_target)\nlook_at_text(_max_target)","98ab3a23":"train['excerpt'].describe()","e43ef90e":"from wordcloud import WordCloud\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=1000, max_font_size=100, figure_size=(14.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    \n\n    wordcloud = WordCloud(background_color='black',max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[\"excerpt\"], title=\"Word Cloud of Excerpt\")","05709763":"train_data=train.copy()\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_data=test.copy()","d336f60a":"\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\n\n#Let's extract some meta data to undeerstand more about target relation with text\n## Number of words in the text ##\ntrain[\"num_words\"] = train[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"excerpt\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"excerpt\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"excerpt\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest[\"num_stopwords\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","ee83f09b":"#let's look at train data \ntrain.head(3)","a80fe732":"def kde_plot(meta_data):\n    plt.figure(figsize=(12,6))\n    p1=sns.kdeplot(train[meta_data], shade=True, color=\"darkred\").set_title('Kernel Distribution of '+ meta_data)\n    #p1=sns.kdeplot(test[meta_data], shade=True, color=\"red\")\n    plt.legend(labels=['Train','Test'])\n    \n#plot all the meta data\ncol_list=train.columns[6::]\nfor i in range(len(col_list)):\n    kde_plot(col_list[i])","64775f9d":"draw_correlation_matrix(train)","3148ab8a":"train_data=train.copy()\nimport string\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\ndef preprocess_text(text):\n    text=str(text).lower()\n    text = re.sub('\\n', '', text)\n    return text\n\ndef removestop(text):\n    stop_words = set(stopwords.words('english')) \n  \n    word_tokens = word_tokenize(text) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    filtered_sentence = [] \n  \n    for w in word_tokens: \n        if w not in stop_words: \n            filtered_sentence.append(w) \n    return filtered_sentence\nSTOP_WORDS = set(stopwords.words('english')) # stop words\ndef rem_stop(text):\n    text = ' '.join([i for i in text.split() if i not in STOP_WORDS])\n    return text\ndef remove_punctuation(text):\n    text_clean=\"\".join([i for i in text if i not in string.punctuation])\n    return text_clean\n\ntrain_data['excerpt']=train_data['excerpt'].apply(lambda x:remove_punctuation(x))\ntrain_data['excerpt']=train_data['excerpt'].apply(lambda x:preprocess_text(x))\ntrain_data['excerpt']=train_data['excerpt'].apply(lambda x:removestop(x))","d9c7a46b":"from collections import Counter\ndef find_common_words_and_count(dataframe,col_name):\n    dataframe['temp_list'] = dataframe[col_name].apply(lambda x:str(x).split())\n    top = Counter([item for sublist in dataframe[col_name] for item in sublist])\n    temp = pd.DataFrame(top.most_common(50))\n    temp.columns = ['Common_words','count']\n    temp.style.background_gradient(cmap='Green')\n    return temp\n\nimport plotly.express as px\nfig = px.bar(find_common_words_and_count(train_data,'excerpt'), x=\"count\", y=\"Common_words\", title='Commmon Words in Excerpt', orientation='h', \n             width=1000, height=700,color='Common_words')\nfig.show()\n\nfig = px.treemap(find_common_words_and_count(train_data,'excerpt'), path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","57fb6b57":"from plotly.offline import init_notebook_mode, iplot \ntrace1 = go.Bar(\n                    y=train['license'].value_counts().index,\n                    x =train['license'].value_counts(sort=True).values,\n                    orientation='h',\n                    marker = dict(color='green',\n                                 line=dict(color='skyblue',width=1)),\n                    text = train.index)\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title=\"license\",width=800, height=500, \n                       xaxis= dict(title=\"license\"),\n                       yaxis=dict(autorange=\"reversed\"),\n                       showlegend=False)\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","7b472014":"import warnings\ntrain['url_legal']=train['url_legal'].fillna(\"Missing_source\")\ntrain['source_type']=''\ntrain['website']=''\nfor i in range(len(train)):\n    if train['url_legal'][i]=='Missing_source':\n        train['source_type'][i]='Missing_source'\n        train['website'][i]='Missing_source'\n    elif train['url_legal'][i]!='Missing_source':\n        list_source=train['url_legal'].apply(lambda x: x.split('\/'))[i][2:]\n        train['source_type'][i]=list_source[1]\n        web=train['url_legal'].apply(lambda x: x.split('\/'))[i][2:]\n        train['website'][i]=web[0]","ae16bee1":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfi = make_subplots(\n    rows=2, cols=2,subplot_titles=('source_type','website')\n)\n\nfi.add_trace(go.Bar(x=train['source_type'].value_counts().index, y=train['source_type'].value_counts().values,\n                    marker=dict(color=[4, 5, 6], coloraxis=\"coloraxis\")),\n              1, 1)\n\nfi.add_trace(go.Bar(x=train['website'].value_counts().index, y=train['website'].value_counts().values,\n                    marker=dict(color=[2, 3, 5], coloraxis=\"coloraxis\")),\n              1, 2)\n\nfi.update_layout(coloraxis=dict(colorscale='Bluered_r'),height=500, showlegend=False)\n\nsource_type = train['source_type'].value_counts().reset_index()\nwebsite = train['website'].value_counts().reset_index()\n\npie_men = go.Pie(labels=source_type['index'],values=source_type['source_type'],name='source_type',hole=0.8,domain={'x': [0,0.56]})\n\npie_women = go.Pie(labels=website['index'],values=website['website'],name=\"website\",hole=0.8,domain={'x': [0.52,1]})\n\nlayout = dict(title = 'Features_Extraction_from_URL', font=dict(size=10), legend=dict(orientation=\"h\"),\n              annotations = [dict(x=0.2, y=0.5, text='source_type', showarrow=False, font=dict(size=20)),\n                             dict(x=0.8, y=0.5, text='website', showarrow=False, font=dict(size=20)) ])\n\nfig = dict(data=[pie_men, pie_women], layout=layout)\npy.iplot(fig)\nfi.show()\n\n","ad851d9c":"import string\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\ndef preprocess_text(text):\n    text=str(text).lower()\n    text = re.sub('\\n', '', text)\n    return text\n\nSTOP_WORDS = set(stopwords.words('english')) # stop words\ndef rem_stop(text):\n    text = ' '.join([i for i in text.split() if i not in STOP_WORDS])\n    return text\ndef remove_punctuation(text):\n    text_clean=\"\".join([i for i in text if i not in string.punctuation])\n    return text_clean\n\ntrain['excerpt']=train['excerpt'].apply(lambda x:remove_punctuation(x))\ntrain['excerpt']=train['excerpt'].apply(lambda x:preprocess_text(x))\ntrain['excerpt']=train['excerpt'].apply(lambda x:rem_stop(x))\n\ntest['excerpt']=test['excerpt'].apply(lambda x:remove_punctuation(x))\ntest['excerpt']=test['excerpt'].apply(lambda x:preprocess_text(x))\ntest['excerpt']=test['excerpt'].apply(lambda x:rem_stop(x))","db556d7f":"import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim","b63cda2d":"def TF_IDF_W2V(text):\n    '''Calculate TF-IDF with word2vec\n    '''\n    #Load TF-IDF from sklearn\n    TFIDF_model = TfidfVectorizer()\n    #fit on text\n    TFIDF_model.fit(text)\n    #create dictionary with word as key\n    #and idf as value\n    dictionary = dict(zip(TFIDF_model.get_feature_names(), list(TFIDF_model.idf_)))\n    #apply set as we need unique features\n    TFIDF_words = set(TFIDF_model.get_feature_names())\n    #create list which stores TFIDF_W2V\n    TFIDF_W2V_vectors = []\n    for sentence in text:\n        #create empty vector to store result\n        vector = np.zeros(300)\n        #number of words with valid vector in sentence\n        TFIDF_weight =0\n        for word in sentence.split(): \n            #if word exist in glove_words and TFIDF_words\n            if (word in glove_words) and (word in TFIDF_words):\n                #get its vector from glove_words\n                vec = word2vec_model[word]\n                #calculate TF-IDF for each word\n                TFIDF = dictionary[word]*(sentence.count(word)\/len(sentence.split()))\n                #calculate TF-IDF weighted W2V\n                vector += (vec * TFIDF)\n                TFIDF_weight += TFIDF\n                \n        if TFIDF_weight != 0:\n            vector \/= TFIDF_weight\n        TFIDF_W2V_vectors.append(vector)\n    return TFIDF_W2V_vectors ","d267700f":"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\nprint(word2vec_model.vectors.shape)","bdac297a":"glove_words = list(word2vec_model.index_to_key)","69d74b66":"tfidf_w2v_excerpt_train = TF_IDF_W2V(train['excerpt'])\ntfidf_w2v_excerpt_test = TF_IDF_W2V(test['excerpt'])","8b693573":"#standarized the numerical_feature \nx_train_ = StandardScaler().fit_transform(train[['num_words', 'num_unique_words', 'num_chars', 'num_stopwords',\n       'num_punctuations', 'num_words_upper', 'num_words_title',\n       'mean_word_len']])\nx_test_ = StandardScaler().fit_transform(test[['num_words', 'num_unique_words', 'num_chars', 'num_stopwords',\n       'num_punctuations', 'num_words_upper', 'num_words_title',\n       'mean_word_len']])\nprint(\"Standardizing numerical features\")","5fa788ef":"df1_test=pd.DataFrame(tfidf_w2v_excerpt_test)\ndf2_test=pd.DataFrame(x_test_)\n\ndf1=pd.DataFrame(tfidf_w2v_excerpt_train)\ndf2=pd.DataFrame(x_train_)\n\ndf1['id']=train['id']\ndf2['id']=train['id']\n\ndf1_test['id']=test['id']\ndf2_test['id']=test['id']\n\nresult=df1.merge(df2,on='id')\nresult_test=df1_test.merge(df2_test,on='id')\n\nresult_test.drop('id',axis=1,inplace=True)\nresult.drop('id',axis=1,inplace=True)\n\ny_true=train['target']","ac3ad764":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(result, y_true, test_size=0.1)","eaccc7ef":"#hyper-paramater tuning\nclf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\nvalues = [10**-14, 10**-12, 10**-10, 10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6]\nhyper_parameter = {\"alpha\": values}\nbest_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\nbest_parameter.fit(X_train, y_train)\nalpha = best_parameter.best_params_[\"alpha\"]\n    \n#applying linear regression with best hyper-parameter\nclf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\nclf.fit(X_train, y_train)\ntrain_pred = clf.predict(X_train)\ntrain_RMSE_lr = rmse_score(y_train, train_pred)\nval_pred = clf.predict(X_test)\nval_RMSE_lr = rmse_score(y_test, val_pred)\n\nprint(\"linear_regression_score:\",train_RMSE_lr,val_RMSE_lr)","cc1973b9":"#load model\nclf = xgb.XGBRegressor()\n#set the hyperparamters in form of dict\nhyper_parameter = {\"max_depth\":[1,3,5,7], \"n_estimators\":[100,150,200,250]}\n#Use RandomizedSearchCV\nbest_parameter = RandomizedSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 5)\n#fit on training data\nbest_parameter.fit(X_train, y_train)\n#get best estimators\nestimators = best_parameter.best_params_[\"n_estimators\"]\n#best depth\ndepth = best_parameter.best_params_[\"max_depth\"]","57319e9c":"#applying xgboost regressor with best hyper-parameter\nclf = xgb.XGBRegressor(max_depth =depth, n_estimators =estimators)\nclf.fit(X_train, y_train)\n#predict on X_train\ntrain_pred = clf.predict(X_train)\n#calculate RMSE\ntrain_RMSE_Xg = rmse_score(y_train, train_pred)\n#Predict on validation set\nval_pred = clf.predict(X_test)\n#calculate RMSE\nval_RMSE_Xg = rmse_score(y_test, val_pred)\n\nprint(\"XG_regression_score:\",train_RMSE_Xg,val_RMSE_Xg)","3d085be9":"'''\nfrom sklearn.svm import SVR\nparameters = {'kernel': ('linear', 'rbf','poly'), 'C':[1.5, 10],'gamma': [1e-7, 1e-4],'epsilon':[0.1,0.2,0.5,0.3]}\nclf = SVR()\nbest_parameter = GridSearchCV(clf, parameters,scoring = \"neg_mean_absolute_error\", cv = 7)\nbest_parameter.fit(X_train, y_train)\nbest_parameter.best_params_\nclf = SVR(kernel='linear', C=1.5, gamma=1e-07,epsilon= 0.3)\nclf.fit(X_train, y_train)\ntrain_pred = clf.predict(X_train)\ntrain_RMSE_SV = rmse_score(y_train, train_pred)\nval_pred = clf.predict(X_test)\nval_RMSE_SV = rmse_score(y_test, val_pred)\n\nprint(\"SVM_regression_score:\",train_RMSE_SV,val_RMSE_SV)\n'''","dc890e40":"submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nd_test = clf.predict(result_test)\n\nsubmission['target'] = d_test\nsubmission.to_csv('submission.csv', index=False)","5e8aec54":"# <p style=\"background-color:blue; font-family:newtimeroman; font-size:100%; text-align:Left; border-radius: 15px 50px;\">Takes a short view about Competition.<\/p>\n<div align='left'><font size=\"6\" color=\"#F39C12\">CommonLit Readability <\/font><\/div>\n<hr>\n<p style='text-align:justify'><b>Introduction:<\/b> Readability means how easily something can be read. Being easy to read is a good thing \u2013 complicated writing can make readers lose interest. Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.<\/p>\n\n<p style='text-align:justify'><b>Objective:<\/b>In this competition,we will build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.<\/p>\n\n<p style='text-align:justify'><b>Data:<\/b>Let's see the Data<\/p>\n\n\n<p style='text-align:justify'>In this competition, we have data in three files train,test and submission file<\/p>\n<p style='text-align:justify'><b>id:  <\/b>unique ID for excerpt.<\/p>\n<p style='text-align:justify'><b>url_legal:  <\/b>URL of source.<\/p>\n<p style='text-align:justify'><b>license:  <\/b> license of source material.<\/p>\n<p style='text-align:justify'><b>excerpt:  <\/b>text to predict reading ease of<\/p>\n<p style='text-align:justify'><b>target:  <\/b>reading ease.<\/p>\n<p style='text-align:justify'><b>standard_error:  <\/b> measure of spread of scores among multiple raters for each excerpt.<\/p>\n\n<div align='left'><font size=\"6\" color=\"#F39C12\">Evaluation Metric<\/font><\/div>\n<p style='text-align:justify'><b>RMSE  <\/b>Root mean squared error is a metric which we used to predict continous values. There are other metrices also used which depend on case studies problem. RMSE mostly not affect with ouliers comared to R^2 metric.<\/p>\n\n<div align='left'><font size=\"6\" color=\"#F39C12\">Train data<\/font><\/div>","77480331":"<div align='Left'><font size=\"6\" color=\"#F39C12\">XG_Boost<\/font><\/div>","5b17a91d":"<div align='left'><font size=\"6\" color=\"#F39C12\">Test data<\/font><\/div>","0e609b86":"# <p style=\"background-color:red; font-family:newtimeroman; font-size:100%; text-align:Left; border-radius: 15px 50px;\">Helper Function.<\/p>","9e8d7a74":"-  After perform exploratory data analysis and visualization,Now we can start our work regrading Modeling.\n-  I want to keep this notebook as simple as it is, so let's start from Machine learning Algorithm inplace of directly using Bert and cutting edge techniques in Natural language Processing.\n- Let's see in this below section how we can apply algorithms and generate results.\n\n- Keeping some points in mind below, we will start.\n      - Split dataset randomly by using train_test_split\n      - Use TF-IDF with Word2vector\n      - Standarized numerical features\n      - GridSearchCV and RandomsearchCV\n      - fit the model,predict and submit it.","6f00fca9":"* Target and standard_error do not have much strong correlation between them.","24787704":"- 1. Train data have 6 columns,and we need to predict value of target column at the end of compitition.\n- 2. Standard error which is defined as measure of spread of scores among multiple raters for each excerpt can see only in train data set.\n- 3. Train dataset shape is (2834,6)\n- 4. There are only two columns which contains missing values one is url_legal and other is license.\n- 5. Missing percentage is near about 70% but still we are going to keep these features utill we perform visualization then we take a decision to keep them or discard them.","786da6d9":"1. So target values lies between -3.67 and 1.71138, min value of target represent that excerpt is not easy to read where as maximum value represent that excerpt can be read easily with flow.\n2. We have target with 0 value also i.e. that excerpt is basseline to compare with other excerpt,excerpt getting zero score means someone can read and someone cannot read.\n\nHave you read all three?","caf72575":"<div align='Left'><font size=\"6\" color=\"#F39C12\">Modeling<\/font><\/div>","2a12ea3c":"<div align='left'><font size=\"6\" color=\"#F39C12\">Generate some Meta Features<\/font><\/div>\n<hr>\n<p style='text-align:justify'><b>Meta Features:<\/b>Let's generate some meta features before applyting text cleaning as there maybe some features exist in meta data which can help us to build some features and they can help us in predicting scores<\/p>","7c977ac9":"<div align='Left'><font size=\"6\" color=\"#F39C12\">URL<\/font><\/div>\n- URL also have near about 70% data is missing but we maybe can extract some useful information from it.So Let's keep it in our columns bucket","5f891a99":"- 1.On an average our target score lies near -0.9.\n- 2.According to Q-Q plot Target follow normal distribution.","a0d89f01":"<div align='left'><font size=\"6\" color=\"#F39C12\">Let's check relationship between Excerpt and target column. <\/font><\/div>\n<hr>","57bc7110":"# <p style=\"background-color:blue; font-family:newtimeroman; font-size:150%; text-align:Left; border-radius: 15px 50px;\">Explorartory Data Analysis.<\/p>\n<div align='left'><font size=\"6\" color=\"#F39C12\">Target Univariate Analysis <\/font><\/div>\n<hr>\n","7f5a52ac":"# <p style=\"background-color:brown; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Readability.<\/p>\n\n# <p style=\"background-color:green; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Readability is a measure of how easy a piece of text is to read.<\/p>","0f067769":"<div align='Left'><font size=\"6\" color=\"#F39C12\">Linear Regression<\/font><\/div>","b047367d":"Conclusion of EDA on meta data:\n\n1. In train data most of extract contains 135 to 175 num_of_words.\n2. Num of unique words in train data lies between 75 to 160\n3. Num of characters much more look like follow normal distribution.\n4. Training data is more skewed in case of num of punctuation,upper number of words ,title, and average len of words.","6748d444":"![image.png](attachment:ad3de519-b2fb-4eb7-bb1e-7ae624de2fab.png)\n\n# <p style=\"background-color:blue; font-family:newtimeroman; font-size:150%; text-align:Right; border-radius: 15px 50px;\">Let's come we start.<\/p>","d11ee29f":"<div align='left'><font size=\"6\" color=\"#F39C12\">Standard_error Univariate Analysis <\/font><\/div>\n<hr>","ff396c62":"- .In both the above charts we can understand that which kind of words excerpt contains. ","69b7bafe":"* After performing univariate analysis for target and standard error,let's perform bivariate analyis for both and check the relationship between them.","6c4ff262":"- 1. Test data have 4 columns,target and standard_error not included in test data set.\n- 2. Train dataset shape is (7,4)\n- 3. There are only two columns which contains missing values one is url_legal and another is license.\n- 4. Missing percentage is near about 58%.\n\n* test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set.","b56c7586":"- First this data column have missing values near 70% and this does not looking like it adding some kind of useful information","95eec4c6":"<div align='Left'><font size=\"6\" color=\"#F39C12\"><\/font><\/div>","b96f7e80":"Some Meta features we generate bu using simple operations.\n1. num_words: Number of words are in excerpt\n2. num_unique_words: How many unique words come in throughout excerpt\n3. num_chars: Number of characters in excerpt\n4. num_stopwords: How many stopwords are exist?.In NLP stopwords have there own importance, many times feature will help us to boost our score and that the region we extract it.\n5. num_punctuations: Number of Punctuation in excerpt\n6. num_words_upper: How many words have upper word in excerpt\n7. num_words_title: How many titles are exist\n8. mean_word_len: Calculate average length of excerpt","eb92f5e2":"- 1. standard error curve is having high peak compared to target curve,median of standard_error is less less than the mean of standard_error i.e. it is positively skewed whereas this is vice versa in case of target which is negatively skewed.\n- 2. Q-Q plot not indicating that it follow normal distribution","1a8e39da":"- 1.Total number of target are 2834 having mean -0.95 and std_dev 1.03","e3740cc8":"<div align='Left'><font size=\"6\" color=\"#F39C12\">Data Cleaning<\/font><\/div>\n\n1. Make extract lower case and remove next line \n\n2. use tokenization\n\n3. remove punctuation\n\n4. remove stopwords","0f48abf4":"1. Below I wrote a function which calculate TF-IDF_W2V\n2. Load GoogleNews-vectors by using genism\n3. Standarized numerical fetaures.","ff3b190d":"<div align='Left'><font size=\"6\" color=\"#F39C12\">License<\/font><\/div>\n<hr>","be8e85f8":"<div align='Left'><font size=\"6\" color=\"#F39C12\">Excerpt<\/font><\/div>\n<hr>\n1. Excerpt is a piece of text from which we need to measure readibility.\n\n2. There are total number of excerpt 2834 in training data set and all are the uniques excerpt","fc524590":"- 1. standard error measure of spread of scores among multiple raters for each excerpt.\n- 2. standard_error having mean 0.5 and std 0.03\n- 3. Interesting point to note that standard error have min value is 0 means for some excerpt did not show their interest."}}