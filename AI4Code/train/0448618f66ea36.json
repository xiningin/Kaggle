{"cell_type":{"5b428290":"code","bbf8d4f3":"code","0095a4f8":"code","296807d1":"code","cba638cb":"code","0f9834cb":"code","60956068":"code","ade1ce48":"code","27d09447":"code","50b50c4f":"code","f9fc13c1":"code","10968032":"code","77d88481":"code","6c8a0d28":"code","e947f205":"code","2437fe0c":"code","2e82fc1a":"code","0b798975":"code","611ac2fb":"code","a1861ab6":"code","f00eb507":"code","184070fe":"code","2b890537":"code","78ece14b":"code","2045b5e5":"code","5d15d984":"code","f8d7eed3":"code","d52a3a51":"code","18aa973a":"code","5bc62f2c":"markdown","61d16053":"markdown","8927641e":"markdown","b7af7bfc":"markdown","b6162855":"markdown","8bd9ccc3":"markdown","f56329e0":"markdown","f8c8243e":"markdown","73fe1e01":"markdown","1c28ab7b":"markdown","c842f430":"markdown","83510bca":"markdown","57c60e70":"markdown","df60dd3b":"markdown","fb72943e":"markdown","56ca55ce":"markdown","a3b42dfc":"markdown","ffa053f4":"markdown","846793c1":"markdown","77ba1c4b":"markdown","5b6bd7ae":"markdown","99b57afc":"markdown","675f8ee9":"markdown","2572873f":"markdown"},"source":{"5b428290":"#NOTES FROM KAGGLE\n\n#This Python 3 environment comes with many helpful analytics libraries installed\n#it is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n#For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Input data files are available in the read-only \"..\/input\/\" directory\n#For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n#You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bbf8d4f3":"#READ INTO PANDAS TRAINING AND TEST DATASET, PRINT SHAPE, GET TRAIN INFO\n\ndf_test = pd.read_csv(\"\/kaggle\/input\/metal-furnace-dataset\/Test.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/metal-furnace-dataset\/Train.csv\")\npd.set_option('display.float_format', lambda x: '%.3e' % x)\n\nprint(\"train shape:\",df_train.shape, \"test shape:\", df_test.shape)\ndf_train.describe()","0095a4f8":"#list of feature column names (will be useful later)\nx_params = df_test.columns.to_list()\nx_params.remove('f9')\n#checked","296807d1":"# plot out the number of unique values in each column\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = 'ticks')\n\nunique_values = {}\nfor x in x_params:\n    unique_values[x] = df_train[x].value_counts().size\n\nfig = plt.figure(figsize = (12,8))\nax = fig.add_subplot(111)\nheight = unique_values.values()\nx = np.arange(len(x_params))\ny = range(0,80,5)\nplt.bar(x = x, height = height)\ntick_positions = range(0,len(unique_values))\nax.set_xticks(tick_positions)\nax.set_xticklabels(unique_values.keys(), rotation =90, size = 12)\nax.set_xlabel(\"Parameter\", c = 'y', size = 18)\nax.set_yticks(y)\nax.set_yticklabels(y)\nax.tick_params(axis = 'both',colors = 'black')\nax.grid(axis = 'y')\nax.set_ylabel(\"# of Unique Values\", c = 'y', size = 18)\nplt.show()\n","cba638cb":"#identify multicollinearity between independent variables with VIF\nfrom sklearn.linear_model import LinearRegression\ndef vif (x_names, data):\n    '''Use this function to determine the Variance Inflation Factor (VIF)for the x variables.\n    VIF is a measure of multicollinearity between variables, and is 1\/(1 - r2), where r2 is \n    is r squared between an x as the dependent variable and all the remaining x as independent\n    variables'''\n    vif_dict = {}\n    for name in x_names:\n        not_x = [i for i in x_names if i != name]\n        X,y = data[not_x], data[name]\n        #get r-squared\n        r_squared= LinearRegression().fit(X,y).score(X,y)\n        #get VIF\n        vif = 1\/(1 - r_squared)\n        #write to dict\n        vif_dict[name] = vif\n    #vif's to dataframe\n    return vif_dict\n    \n        ","0f9834cb":"x_vifs = vif(x_params, df_train)\nprint(x_vifs)","60956068":"#correlations between x variables and grade w\/0.2 as a cutoff (judgement call)\n\ntrain_corr = df_train[x_params].corrwith(df_train['grade'])\nsorted_corrs = abs(train_corr).sort_values()\nstrong_corrs = sorted_corrs[sorted_corrs > .2]\nprint(\"x variable coefficients for LR model\",'\\n',strong_corrs)","ade1ce48":"#function for binning predictions\n\ndef binning (predictions):\n    for n,p in enumerate(predictions):\n        \n        if p <= 0.5:\n            predictions[n] = 0\n        if (p > 0.5) and (p <= 1.5):\n            predictions[n] = 1\n        if (p > 1.5) and (p <= 2.5):\n            predictions[n] = 2\n        if (p > 2.5) and (p <= 3.5):\n            predictions[n] = 3\n        if p > 3.5:\n            predictions[n] = 4\n    \n    return predictions","27d09447":"# fit model with strongly correlated x's to training data\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nX = df_train[strong_corrs.index]\ny = df_train['grade']\nlr.fit(X,y)\ntrain_pred = lr.predict(X)\ntrain_pred_bin = binning(train_pred)","50b50c4f":"#fraction of LR model grade predictions that were correct\n\nfrom sklearn.metrics import accuracy_score\n\nacc_perc = accuracy_score(df_train['grade'], train_pred_bin, normalize=True)\nprint('1st LR model accuracy: {:.2f}%'.format(acc_perc * 100))","f9fc13c1":"# fit Elastic Net model to training data using different l1\/l2 ratios.\n#The ElasticNetCV algorithm will choose the best fitting  ratio from the list\n\nfrom sklearn.linear_model import ElasticNetCV\nl1 = [.1, .5, .7, .9, .95, .99, 1] #list of l1\/l2 ratios\nX = df_train[x_params]\ny = df_train['grade']\n\nlren = ElasticNetCV(l1_ratio = l1)\nlren.fit(X, y)\nlren_predict = lren.predict(X)\nbin_lren_predict = binning(lren_predict)\nacc_lren = accuracy_score(y,bin_lren_predict, normalize = True)\nprint('alpha_value: {:.2f}'.format(lren.alpha_))\nprint('l1_ratio:', lren.l1_ratio_)\nprint('accuracy score using all non zero ENCV coefficients: {:.1f}%'\n       .format(acc_lren * 100))\n","10968032":"#identify ENCV coefficents by feature name\nENet_coeffs = {}\nfor i in range(0,len(x_params)-1):\n    ENet_coeffs[x_params[i]] = round(lren.coef_[i],3)\n    \nENet_coeffs","77d88481":"#number of non zero ENCV x coefficients\n\nnon0_ENCV = []\nfor k,v in ENet_coeffs.items():\n    if v != 0:\n        non0_ENCV.append(k)\n        \nprint('# of non-zero ENCV coefficients:', len(non0_ENCV))","6c8a0d28":"#linear regression model, rev.1\n\nlr_r1 = LinearRegression()\nXr1 = df_train[['f2','f14','f18']]\nlr_r1.fit(Xr1,y)\nlr_r1_pred = lr_r1.predict(Xr1)\nbin_lr_r1_pred = binning(lr_r1_pred)\nacc_lr_r1 = accuracy_score(df_train['grade'], bin_lr_r1_pred, normalize=True)\nprint('2nd LR model accuracy: {:.2f}%'.format(acc_lr_r1 * 100))","e947f205":"df_test.columns","2437fe0c":"#Fit test data to all 3 LR models\n\nX_test0 = df_test[strong_corrs.index]\ntest_pred0 = lr.predict(X_test0)\ntest_pred0_bin = binning(test_pred0) #predict using original LR model\n\nX_testlren = df_test[x_params]\ntest_lren = lren.predict(X_testlren)\ntest_lren_bin = binning(test_lren) #predict using ENCV model\n\nX_test1= df_test[['f2','f14','f18']]\ntest_pred1 = lr_r1.predict(X_test1)\ntest_pred1_bin = binning(test_pred1) #predict using revised LR model\n\nprint(\"\"\"binned prediction for Original LR model named 'test_pred0_bin'\"\"\")\nprint(\"\"\"binned prediction for ENCV model named 'test_lren_bin'\"\"\")\nprint(\"\"\"binned prediction for LR model rev 1 named 'test_pred1_bin'\"\"\")","2e82fc1a":"#Write df_ test predicted values from the 3 regression predictions to csv files\n\n#original lr model\nnp.savetxt(\"lr0_test_pred.csv\", test_pred0_bin, delimiter=\",\")\n#ENCV model\nnp.savetxt(\"lren_test_pred.csv\", test_lren_bin, delimiter=\",\")\n#revised lr model\nnp.savetxt(\"lr1_test_pred.csv\", test_pred1_bin, delimiter=\",\")","0b798975":"from sklearn.feature_selection import RFECV  \nfrom sklearn.ensemble import RandomForestClassifier","611ac2fb":"#function to select features from RFECV (Recursive Feature Elimination Cross Validation).  \n#RFECV takes a model (in this case Random Forest) and recursively eliminates features until\n# the maximum average model accuracy score from Cross Validation is achieved.\n\ndef feature_selection(df):\n    all_X = x_params\n    all_y = df['grade']\n    rfc = RandomForestClassifier(n_estimators = 100, random_state = 1)\n    grid = RFECV(rfc, cv = 5) #use the default no. of crossfolds\n    grid.fit(X = df[all_X], y = all_y)\n    num_features = grid.n_features_\n    rank = grid.ranking_\n    best_columns = df[all_X].columns[grid.support_]\n    return all_X, all_y, num_features, rank, best_columns","a1861ab6":"all_X, all_y,num_features,rank, best_columns = feature_selection(df_train)\nprint('# of cols chosen recursively from RF classifier:', num_features, '\\n', \n      'column ranking:', rank, '\\n','column names for model:',best_columns)","f00eb507":"#fit RF classifier using the selected features, make `grade` predictions, determine\n#accuracy score\n\ndef my_RFC(df,Xcols,ycol):\n    rfc = RandomForestClassifier(n_estimators = 100, random_state = 1)\n    X = df[Xcols]\n    y =df[ycol]\n    rfc.fit(X,y)\n    rfc_prediction = rfc.predict(X)\n    rfc_acc = rfc.score(X,y)\n    return rfc_prediction, rfc_acc","184070fe":"rfc_prediction, rfc_acc = my_RFC(df_train, best_columns, 'grade')\nprint('Original RFC model accuracy: {:.2f}%'.format(rfc_acc * 100))","2b890537":"#99+% accuracy. Check difference of actual grades - predicted grades.\n\ngrade_list = list(df_train['grade'])\ndiff = grade_list - rfc_prediction\nrfc_check_df = pd.DataFrame(data = {'Grade_act': grade_list,\n                                    'Grade_pred': rfc_prediction,\n                                   'Diff': diff})\nrfc_check_df['Diff'].value_counts()","78ece14b":"#Are the RFClassifier variables mostly categorical?\nRFC_params_count = {}\nfor c in best_columns:\n    RFC_params_count[c] = len(df_train[c].value_counts())\n    \nRFC_params_count\n    ","2045b5e5":"X_cat = []\nfor k,v in RFC_params_count.items():\n    if v <= 3:\n        X_cat.append(k)\n        \nX_cat","5d15d984":"#7 categorical variable RFC model accuracy\nrfc_prediction1, rfc_acc1 = my_RFC(df_train, X_cat, 'grade')\nprint('7 categorical parameter RFC model accuracy: {:.2f}%'.format(rfc_acc1 * 100))\n","f8d7eed3":"#Add the 3 \"almost\"categorical variables to the 7 categorical\nXcat_10 = X_cat.copy()\nXcat_10.extend(['f0','f2','f5'])\nXcat_10","d52a3a51":"#rerun my_RFC with 10 variables\nrfc_prediction2,rfc_acc2 = my_RFC(df_train, Xcat_10, 'grade')\nprint('''10 \"categorical\" parameter RFC model accuracy: {:.2f}%'''.format(rfc_acc2 * 100))","18aa973a":"#Write df_test predicted values from the 3 classificationpredictions to csv files\n\n##13 parameter model\nnp.savetxt(\"rfc0_test_pred.csv\", rfc_prediction, delimiter=\",\") \n##10 parameter model\nnp.savetxt(\"rfc1_test_pred.csv\", rfc_prediction1, delimiter=\",\")\n#7 parameter model\nnp.savetxt(\"rfc2_test_pred.csv\", rfc_prediction2, delimiter=\",\")","5bc62f2c":"<font color= gray>THOUGHT<\/font>\n\nOops...I should have picked up earlier that the test dataset didn't have the `grade` column.  Was hoping to use the test data set to evaluate accuracy of each model's predictions of grade.  I could take a portion of the train data set and set it aside for validating the model, but there would be data leakage between the smaller training data set and the validation set.<br>\n\nI'll just name the predictions using each model and write to a csv.","61d16053":"<font color = gray>THOUGHTS<\/font>\n\n- 7 input variables (f4,f6,f10,f14,f15,f16,f18) are categorical.\n- 3 input variables (f0,f2,f5) are \"almost\" categorical.\n- 3 input variables (f22, f23,f24) are continuous.\n- Rerun the RFClassifier with the categorical variables, check accuracy\n","8927641e":"<font color = yellow>gray <\/font>\n<br><br>\nPotential multicolliearity between f3 and the other x variables (Literature says VIF < 5 not a concern; 5 < VIF < 10 possible concern; VIF > 10 is a concern.<br>\n\nFor now, I'm going to disregard potential multicollinearity.","b7af7bfc":"<font color = gray>SUMMARY<\/font>\n\n- 7 categorical variable RFC prediction accuracy of train data: <font color= red>83%<\/font>.\n    - Is this useful to the people making the alloy?  Can these variable be used to predict grade before starting to make the alloy?\n- 7 categorical + 3 'almost' categorical variable RFC prediction accuracy of train data: <font color= red>91%<\/font>.\n    - Can these 3 additional variables be used to predict grade prior to making the alloy?\n- 7 categorical + 3 'almost' categorical  + 3 continuous variable prediction accuracy: <font color= red>99+%<\/font>\n    - I assume these continuous variables are machine actuals (averaged over the time of alloy manufacture?) \n\n<font color = gray>THOUGHTS<\/font>\n\n- I hypothesize binning the 3 continous variables and creating a 4th RFC model would lead to a prediction accuracy between 91% and 99%.  This would allow those making the alloy to create a table of settings for the 13 variables and predicted grade.  Or, \"clustering\" could be done based on the 13 variable model predictions.  For a single grade,cluster the data rows where values for the 10 categorical variables are equal, and determine the range of the continous variables witin that cluster.\n\n- I'm going to stop here and make this notebook available to the owner of the dataset.  I'll write a final code block where the 3 RFC model predictions are written to .csv files.","b6162855":"## Exploratory Data Analysis","8bd9ccc3":"## Random Forest Classification - Feature Modification ##","f56329e0":"### Initial Linear Regression -  Model Fit and Prediction###","f8c8243e":"### ENCV Regression -  Model Fit and Prediction###","73fe1e01":"### Test Data Model Fit and Prediction###","1c28ab7b":"<font color = gray>LR SUMMARY<\/font>\n<br><br>\n3 models so far:<br>\n- Linear Regression with x variables having a Pearson correlation with grade greater than 0.2\n    - Training dataset prediction accuracy after binning: **<font color = red>77.42%**<\/font>\n- ENCV Regression\n    - Training dataset prediction accuracy after binning: **<font color = red>79.19%**<\/font>\n- Linear Regression with three most significant x variables determined by ENCV\n    - Training dataset prediction accuracy after binning: **<font color = red>76.94%<\/font>**","c842f430":"<font color = gray>THOUGHTS<\/font><br>\n\n- The ENCV model had 23 non-zero coefficients\n- sorted Elastic Net cross validation (ENCV) feature coefficients (absolute value, high to low): f2, f18, f14, f5, f22, f6, f25, f17, etc.  f3, f4,f12 are 0.\n- sorted correlation coefficients (feature with correlated with grade, high to low): f2, f3, f18, f14, f5, f6, f0, f4.\n- f3 has a high correlation coefficient but the ENCV feature coefficient is 0 and VIF was ~5. \n- f2, f18, f14 feature coefficents are >2x the magnitude of the next largest coefficient.\n- Revise original linear model with just f2, f18 and f14.","83510bca":"<font color = gray> OBSERVATIONS<\/font><br><br>\n\\- Parameters f22 - f24 have over 10 unique values<br>\n\\- Parameters f0 - f2, f5 have 5 to 10 unique values<br>\n\\- All other parameters (21 total) have < 5 unique values.<br>\n\nf0 - f27 a mixture of continous and numeric categorical variables.","57c60e70":"Information from Column view of the train dataset in Kaggle<br>\nNote: 'rs' means distribution is right-skewed, 'ls' left - skewed\n\n![image.png](attachment:image.png)<br>\n<font color= yellow>OBSERVATIONS<\/font><br>\n\\- f9 not at all useful for modeling<br>\n\\- data has been converted to t or z scores (mean 0, std dev 1)<br>\n\\-Data leakage?  Do not know whether the train and test datasets were independently converted to t or z scores<br>\n\\- Appears most of the columns consists of data clustered near the mean with a few outliers (except f0,f22,f23,f24).<br>\n\\- Grade is categorical (only possible values are integers from 0 - 4)","df60dd3b":"### Initial Linear Regression - x Parameter Evaluation (Feature Selection) ###","fb72943e":"As stated earlier, with so many variables in this dataset being categorical a classification model may be useful to this client company","56ca55ce":"<font color = gray>TITLE<\/font><br>\n**Grade of Manufactured Alloy**<br><br>\n\n<font color = gray>DESCRIPTION<\/font>\n\nThis dataset contains factors associated with the manufacture of a metal alloy.  The purpose of this analysis is to create a model predicting alloy grade based on the factors.\n\n<font color = gray>COMMENTS<\/font><br>\n\\- `checked`at the end of a code block means the output was verified<br>\n\\- I'm assuming that each data row applies to a single processing batch","a3b42dfc":"### ENCV Regression - Feature Evaluation###","ffa053f4":"<font color = gray>THOUGHTS<\/font><br>\n1. The factors are all numeric and (I assume) on an interval scale\n2. This is a manufacturing problem, where knowing the effect of each factor on the output can lead to improved output.\n3. Given 1. and 2. a regression, with weights for each factor, comes first to my mind.\n    - Will the variables with few unique values consume degrees of freedom and add little predictive power to a regression because they are effectively binned? StatStack Exchange indicates such (https:\/\/stats.stackexchange.com\/questions\/311954\/should-i-convert-integer-variables-with-very-few-unique-values-to-factors-for-pr?rq=1).  But, with the large number of data points relative to the number of factors will the loss of DofF matter?  I don't think it will.\n    - Don't know whether the x relationships with y are linear or non-linear.\n    - Don't know (yet) if multicollinearity exists.\n2.  The y output has only 5 possible values.  A classification model may be more appropriate. \n    - No coefficients (weights) for each x variable (feature).\n    - Many of the x variables appear categorical (not continuous). May not need coefficient weights (I'm thinking these categorical x values can't be modified {incoming raw material properties, for example}, therefore the weights for these x's have no practical value).<br>\n    \n<font color = gray>CONCLUSION:<\/font><br>\n\n- Start with a Linear Regression (LR) model.  \n    - Evaluate multicollinearity among the x variables.\n    - Determine x variable correlation with grade.\n    - Eliminate variables with correlations < abs(0.2) (This value is a judgement call).\n    - Fit model to training data. \n    - Bin grade predictions.  \n    - Determine accuracy of grade predictions.  \n- Stop and rethink.","846793c1":"### Linear Regression - Multicollinearity ###","77ba1c4b":"### Revised Linear Regression (3 Parameters) -  Model Fit and Prediction###","5b6bd7ae":"## Random Forest Classification - Model Fit and Predict ##","99b57afc":"## **Stop & Rethink** ###\n\n<font color = gray>THOUGHTS<\/font><br>\n\n- Basic LR using  features correlated (Pearson) with 'grade' having a corr coefficent > 0.2, and binning the predictions gave a training dataset accuracy of 77% \n    - Is this beneficial to the client company?  Can they implement process improvement with this information\n- The predictive power of the model with new data may improve through regularization (penalize the model for having too many factors). Elastic Net CV (ENCV) combines Ridge and LASSO regression regularization to look for x variables having the greatest impact on `grade`.  ENCV is a structured way to reduce the number of factors through the regularization penalties l1 and l2. Per scikit- learn, try  l1\/l2 ratios of  [.1, .5, .7, .9, .95, .99, 1].","675f8ee9":"<font color = gray>NOTES FROM THE CREATOR OF THE DATASET <\/font><br>\n\nManufacturing of any alloy is not a simple process. Many complicated factors are involved in the making of a perfect alloy, from the temperature at which various metals are melted to the presence of impurities to the cooling temperature set to cool down the alloy. Very minor changes in any of these factors can affect the quality or grade of the alloy produced.\n\n\nGiven are 28 distinguishing factors in the manufacturing of an alloy, the objective is to build a Machine Learning model that can predict the grade of the product using these factors.\n\nYou are provided with 28 anonymized factors (f0 to f27) that influence the making of a perfect alloy that is to be used for various applications based on the grade\/quality of the obtained product.","2572873f":"## Random Forest Classification - Feature Selection ##"}}