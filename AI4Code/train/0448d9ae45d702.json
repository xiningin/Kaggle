{"cell_type":{"5ac6e8ee":"code","a76b813c":"code","37dd3996":"code","383d45d4":"code","7b808218":"code","1882a751":"code","c32acbf4":"code","b5878722":"code","55a3e9cf":"code","cc6125fd":"code","ae667796":"code","577ae3c5":"code","292fc633":"code","b86c11b8":"code","a0245758":"code","6f201315":"code","cefa08df":"code","962a70fa":"markdown","01905530":"markdown","d9c5d6c4":"markdown","5dd33a6c":"markdown","b8c93db1":"markdown","02603b70":"markdown","eb08e68c":"markdown","47915c1e":"markdown"},"source":{"5ac6e8ee":"import os\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing GPU warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\nimport optuna\nfrom optuna.integration import XGBoostPruningCallback\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nSEED = 2311\n\ngc.collect()","a76b813c":"#using a private dataset where I have saved the data in feather format for fast loading\ndata_dir = '\/kaggle\/input\/tps-oct21-data\/'","37dd3996":"%%time\ntrain = pd.read_feather(data_dir + 'tps-oct21-train.feather')\ntest = pd.read_feather(data_dir + 'tps-oct21-test.feather')","383d45d4":"target = train.pop('target')\n\nsubmission_index = test['id']  #for submission csv\n\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","7b808218":"#Training and validation sets for tuning\nxtrain, xval, ytrain, yval = train_test_split(train, target, \n                                              stratify=target,\n                                              test_size=0.2,\n                                              random_state=SEED)","1882a751":"base_params = {\n        'objective': 'binary:logistic',\n        'booster': 'gbtree',\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'n_estimators': 10000,\n        'eval_metric': 'auc',\n        'enable_categorical': True, # needs tree_method:gpu_hist and dataframe\n        'random_state': SEED,\n        'verbosity': 0\n    }","c32acbf4":"def objective(trial, xtrain, ytrain, xval, yval, base_params):\n        \n    param_grid = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 8)\n    }\n    \n    #scikit-learn API for xgboost, params were named accordingly.\n    model = XGBClassifier(**base_params, **param_grid)\n    \n    model.fit(\n        xtrain, ytrain,\n        eval_set=[(xval, yval)],\n        eval_metric='auc',\n        early_stopping_rounds=50,\n        callbacks=[\n            #pruning unpromising trials\n            XGBoostPruningCallback(trial, 'validation_0-auc')\n            #need to include index of validation set from eval_set for scikit-learn API\n        ],\n        verbose=200\n    )\n    \n    predictions = model.predict_proba(xval)[:, 1]\n    \n    return roc_auc_score(yval, predictions)","b5878722":"%%time\nstudy = optuna.create_study(direction='maximize', study_name='base3_xgboost')\n\nstudy.optimize(\n    lambda trial: objective(trial, xtrain, ytrain, xval, yval, base_params), \n    n_trials=10\n)","55a3e9cf":"print(f'Best value (AUC): {study.best_value:.5f}')\n\n#can save best params with copy-paste also, to avoid hyperparameter tuning for later runs\nbest_params = study.best_params\n\nprint('Best params:')\nfor key, value in best_params.items():\n    print(f'\\t{key}: {value}')","cc6125fd":"del xtrain, xval, ytrain, yval\n\ngc.collect()","ae667796":"model = XGBClassifier(**base_params, **best_params)","577ae3c5":"oof_preds = pd.DataFrame(index=train.index, columns=['prediction'], dtype=np.float32)\n\ntest_preds = pd.DataFrame(index=submission_index,\n                          columns=['prediction0', 'prediction1', 'prediction2', \n                                   'prediction3', 'prediction4'],\n                          dtype=np.float32)","292fc633":"%%time\n\nN_SPLITS = 5 # = K\n\ncv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(train, target)):\n    print(5 * '-' + 'Fold-' + str(fold) + 5 * '-')\n    \n    xtrain, xval = train.iloc[train_idx], train.iloc[val_idx]\n    ytrain, yval = target[train_idx], target[val_idx]\n    \n    model.fit(\n        xtrain, ytrain,\n        eval_set=[(xval, yval)],\n        eval_metric='auc',\n        early_stopping_rounds=50,\n        verbose=200\n    )\n    \n    oof_preds.loc[val_idx, 'prediction'] = model.predict_proba(xval)[:, 1]\n    test_preds.loc[:, 'prediction' + str(fold)] = model.predict_proba(test)[:, 1]","b86c11b8":"cv_score = roc_auc_score(target, oof_preds.prediction)\nprint(f'CV score (AUC): {cv_score:.5f}')","a0245758":"oof_preds.to_csv('oof_3_xgb.csv', index=False)","6f201315":"final_preds = np.asarray(test_preds.mean(axis=1))","cefa08df":"output = pd.DataFrame({'id': submission_index, 'target': final_preds})\noutput.to_csv('sub_3_xgb.csv', index=False)\n\n!head sub_3_xgb.csv","962a70fa":"Time to submit!","01905530":"**AUC on training set using OOF predictions:**","d9c5d6c4":"**Saving OOF predictions for meta-model:**","5dd33a6c":"### A base model for constructing a stacking ensemble.  \n\n* Attempting to implement the workflow described in [this amazing discussion](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/175614) by **Chris Deotte**. (Make sure to give it a read, I believe it will help in some way regardless of one's level of expertise!)  \n  \n* For the basics about stacking ensembles, [this article](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/) by '**Machine Learning Mastery**' helped out a lot.  \n\n* [Why Is Everyone at Kaggle Obsessed with Optuna For Hyperparameter Tuning?](https:\/\/towardsdatascience.com\/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c) is an excellent post on Medium, which explains the usage of Optuna in a beginner-friendly manner.  \n\n* [XGBoost Parameters documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n\n* [XGBoost Python API documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBClassifier)\n\n* Will be sharing the stacking ensemble notebook as soon as I am able to complete the whole process. Thanks for checking out this notebook! And any advice about the process or the notebook itself is welcome.","b8c93db1":"**Imports**","02603b70":"**Averaging predictions on test set from each fold for submission:**","eb08e68c":"**K-fold cross validation to simultaneously generate out-of-fold (OOF) predictions on train set and K predictions on test set, using the best hyperparameters determined by Optuna.**","47915c1e":"**Hyperparameter tuning with Optuna**"}}