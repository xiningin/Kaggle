{"cell_type":{"bd86d46a":"code","26672556":"code","8116ddb3":"code","503aa04d":"code","42f1b7a5":"code","c30b73e9":"code","f57f145f":"code","98c3654d":"code","2ee7cc04":"code","a6ec6301":"code","9412e3ca":"code","c2066641":"code","3b8193f0":"code","7b3591c0":"code","9816179c":"code","73d5f0a2":"code","5d5966e7":"code","894fb1fd":"code","770a5b14":"code","c0bbf4c6":"code","6f4b0607":"code","cdb535ea":"code","aadacdfc":"code","f3232ce2":"code","a3835f13":"code","87dd4ed7":"code","d09b8f64":"code","be46b0df":"code","bcd5ea0c":"code","0f1ae4a8":"code","bdccc14f":"code","11f10e6b":"code","99b0576c":"code","89701f04":"code","f3ba89bd":"code","90854a58":"code","58b1d29a":"code","173f59ad":"code","c6adec52":"code","e9b4c212":"markdown","72890155":"markdown","88f12e3f":"markdown","42e5e3ce":"markdown"},"source":{"bd86d46a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26672556":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\n#import numpy as np\n#import pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom datetime import datetime\n\nfrom scipy.stats import skew, norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom xgboost import XGBRegressor\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","8116ddb3":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/30-days-of-ml\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","503aa04d":"data_dir = Path(\"..\/input\/30-days-of-ml\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"id\")\n\ndf.head()","42f1b7a5":"df.columns","c30b73e9":"def clean(df):\n    return df\ndef encode(df):\n    return df\ndef impute(df):\n    return df","f57f145f":"df_train, df_test = load_data()","98c3654d":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","2ee7cc04":"X = df_train.copy()\ny = X.pop(\"target\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int","a6ec6301":"# Peek at the values\ndisplay(X)\n\n# Display information about dtypes and missing values\ndisplay(X.info())\n","9412e3ca":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores","c2066641":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","3b8193f0":"X1 = df_train.copy()\n# Label encoding for categoricals\nfor colname in X1.select_dtypes(\"object\"):\n    X1[colname], _ = X1[colname].factorize()\nsns.relplot(x=\"cont12\", y=\"target\", data=X1);\nsns.relplot(x=\"cont9\", y=\"target\", data=X1);\nsns.relplot(x=\"cont10\", y=\"target\", data=X1);","7b3591c0":"sns.jointplot(x=\"target\", y=\"cont10\", kind=\"reg\", data=X1)","9816179c":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(df['target'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(df['target'], kde = True, hist=True, fit = norm)\nplt.title('Target distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"Target in \", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","73d5f0a2":"from scipy import stats\n\nshap_t,shap_p = stats.shapiro(df['target'])\n\nprint(\"Skewness: %f\" % abs(df['target']).skew())\nprint(\"Kurtosis: %f\" % abs(df['target']).kurt())\nprint(\"Shapiro_Test: %f\" % shap_t)\nprint(\"Shapiro_Test: %f\" % shap_p)","5d5966e7":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = df.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.set(font_scale=1.6)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .9})\nplt.show()","894fb1fd":"def encode(df):\n    cat_cols = [col for col in df.columns if 'cat' in col]\n    enc = OrdinalEncoder()\n    df[cat_cols] = enc.fit_transform(df[cat_cols])\n    df.head()\n    return df","770a5b14":"df_train, df_test = load_data()","c0bbf4c6":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","6f4b0607":"X = df_train.copy()\ny = X.pop(\"target\")\nX.pop(\"cat2\")\nX.pop(\"cat6\")\nX.pop(\"cat4\")\nX.pop(\"cat3\")\n\n#baseline_score = score_dataset(X, y)\n#print(f\"Baseline score: {baseline_score:.5f} RMSLE\")","cdb535ea":"print(X.shape)","aadacdfc":"print('START ML', datetime.now(), )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","f3232ce2":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\nSEED = 7770777\n\nlgb_params = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.007899156646724397,\n    \"num_leaves\": 77,\n    \"max_depth\": 77,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 9.562925363678952,\n    \"reg_lambda\": 9.355810045480153,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    \"bagging_seed\": SEED,\n    \"feature_fraction_seed\": SEED,\n    \"seed\": SEED\n}\n\nlightgbm = LGBMRegressor(**lgb_params)\n\nlightgbm1 = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# changed objective from reg:linear to reg:squarederror\n#xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n#                                     max_depth=3, min_child_weight=0,\n#                                     gamma=0, subsample=0.7,\n#                                     colsample_bytree=0.7,\n#                                     objective='reg:squarederror', nthread=-1,\n#                                     scale_pos_weight=1, seed=27,\n#                                     reg_alpha=0.00006, random_state=42)\n\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 80,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\n\n\nBest_trial= {'objective': 'reg:tweedie', \n             'tree_method': 'hist', \n             'reg_lambda': 6.323815225685991, \n             'reg_alpha': 2.498166499808977, \n             'colsample_bytree': 0.2, \n             'subsample': 0.8, \n             'learning_rate': 0.02, \n             'n_estimators': 6000, \n             'max_depth': 7, \n             'random_state': 40, \n             'min_child_weight': 99, \n             'use_label_encoder': False, \n             'booster': 'gbtree'}\n\nxgboost = XGBRegressor(**xgb_params)\n\nxgboost1 = XGBRegressor(**Best_trial)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","a3835f13":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n\nprint(datetime.now(), 'xgbBoosting')\nxgboost_model_full_data = xgboost.fit(X_train, y_train,\n                                      verbose = False,\n                                      eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                                      eval_metric = \"rmse\",\n                                      early_stopping_rounds = 100)\nprint(datetime.now(), 'xgbBoosting end')","87dd4ed7":"def xgboost_models_predict(X):\n    return ((1 * xgboost_model_full_data.predict(X)))\n\ny_gbr_pred = xgboost_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred))","d09b8f64":"#X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n\nprint(datetime.now(), 'xgbBoosting')\nxgboost1_model_full_data = xgboost1.fit(X_train, y_train,\n                                      verbose = False,\n                                      eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                                      eval_metric = \"rmse\",\n                                      early_stopping_rounds = 100)\nprint(datetime.now(), 'xgbBoosting end')","be46b0df":"def xgboost1_models_predict(X):\n    return ((1 * xgboost1_model_full_data.predict(X)))\n\ny_gbr_pred1 = xgboost1_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred1))","bcd5ea0c":"X_test = df_test.copy()\ny_test = X_test.pop(\"target\")\nX_test.pop(\"cat2\")\nX_test.pop(\"cat6\")\nX_test.pop(\"cat4\")\nX_test.pop(\"cat3\")\n\nprint(X_test.shape, X.shape)","0f1ae4a8":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission.iloc[:,1] = xgboost_models_predict(X_test)\nsubmission.head()","bdccc14f":"submission.to_csv(\"30days_ML_Submission_v28_orig parameters_early_stop.csv\", index=False)\nprint('Save submission', datetime.now(),)","11f10e6b":"print('Predict submission', datetime.now(),)\nsubmission1 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission1.iloc[:,1] = xgboost1_models_predict(X_test)\nsubmission1.head()","99b0576c":"submission1.to_csv(\"30days_ML_Submission_v28_orig parameters_early_stopping_best_trials.csv\", index=False)\nprint('Save submission', datetime.now(),)","89701f04":"#Setting the kfold parameters\nn_fold = 5\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.iloc[train_id], X.iloc[valid_id]\n    y_train, y_valid = y.iloc[train_id], y.iloc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ n_fold # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ n_fold #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ n_fold\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","f3ba89bd":"print('kFold xgboost Predict submission', datetime.now(),)\nsubmission_kFold = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission_kFold.iloc[:,1] = preds\nsubmission_kFold.head()","90854a58":"submission_kFold.to_csv(\"30days_ML_Submission_v28_kFold.csv\", index=False)\nprint('Save submission', datetime.now(),)","58b1d29a":"#Setting the kfold parameters\nn_fold = 5\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.iloc[train_id], X.iloc[valid_id]\n    y_train, y_valid = y.iloc[train_id], y.iloc[valid_id]\n    \n    model = XGBRegressor(**Best_trial)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ n_fold # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ n_fold #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ n_fold\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","173f59ad":"print('kFold xgboost Predict submission', datetime.now(),)\nsubmission_kFold2 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission_kFold2.iloc[:,1] = preds\nsubmission_kFold2.head()","c6adec52":"submission_kFold2.to_csv(\"30days_ML_Submission_v28_kFold2.csv\", index=False)\nprint('Save submission', datetime.now(),)","e9b4c212":"# EDA - Exploratory Data Analysis #","72890155":"## XGBoost with KFolds and ealy stopping","88f12e3f":"# Correlation Matrix\n\nThe correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.\n","42e5e3ce":"# Introduction\n\nThis notebook focuses on XGBoost with early stopping to avoid overfitting with two sets of paramaters. \n\nThe second model is XGBoost with kFolds and early stopping."}}