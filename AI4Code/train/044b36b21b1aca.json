{"cell_type":{"bba613a0":"code","202f206c":"code","b38cef5e":"code","6f961249":"code","f7cb03bb":"code","b9dc162a":"code","6e309123":"code","b30f2166":"code","cbb88492":"code","24b21761":"code","6f626128":"code","a65ca856":"code","0aa5b438":"code","afeab7bd":"code","c6bc1d9f":"code","f7d50c71":"code","18040dee":"code","50613e49":"code","bddef104":"code","9f7b0732":"code","7d2960ad":"code","8a61f808":"code","4cb641bc":"code","05c4822f":"code","8e8ae66c":"code","c3aa404e":"code","9114354f":"code","b1811d2c":"code","dfe2f970":"code","ff57a440":"code","6fcfd101":"code","8b641c07":"code","81708e2c":"code","08d7a105":"code","cf1517fd":"code","7005f22c":"code","468311b6":"code","fdb6307b":"code","07d4a3f9":"code","6e6625d4":"code","ae90c37c":"code","7d6518a6":"code","5322693a":"code","a63499d6":"code","6a231134":"code","f005a97d":"code","052e3e9b":"code","147a81f7":"code","4b7edf14":"code","5111db36":"code","52331979":"code","c89b9901":"code","13c0ed8f":"code","801f5e17":"code","880c20c0":"code","ed7d3c5f":"code","64f350df":"code","a8fc7ce8":"code","9b97b234":"code","8d1e1d1f":"code","c5a0371f":"code","a2be71d7":"code","eed16293":"code","6ac3acc1":"code","79479084":"code","d0f56e90":"code","9d2c9674":"code","17fc179f":"markdown","9401d870":"markdown","fb0f8139":"markdown","0c4293f7":"markdown","fac7641c":"markdown","e017d64e":"markdown","fd3eca03":"markdown","bb4edc6d":"markdown","afb4232b":"markdown","1957376e":"markdown","d6d85bb4":"markdown","4c36f540":"markdown","5a5992f8":"markdown","697854f2":"markdown","1e8009f3":"markdown","0910d4c9":"markdown","0e8d58ab":"markdown","cfe2fa42":"markdown","9355d9d0":"markdown","594d03fe":"markdown","d8c49963":"markdown","f663da15":"markdown","7e2f9b99":"markdown","f62df55d":"markdown","121338bf":"markdown","932aa3ce":"markdown","66733de8":"markdown","c2e125e9":"markdown","16651b5a":"markdown","3a97b779":"markdown","4f335261":"markdown","37638888":"markdown","84b18f0b":"markdown","6d48f4b2":"markdown","1e19f030":"markdown","66b97b33":"markdown","0ba01da3":"markdown","cc523b26":"markdown","36acd3e7":"markdown","a099f507":"markdown","27ee33f4":"markdown","c9ac37af":"markdown","621f20c9":"markdown","71fed2db":"markdown","f7f806f6":"markdown","4567d88c":"markdown","ef879610":"markdown","a23bc0f6":"markdown","543117be":"markdown","abc186d1":"markdown","2350a0de":"markdown","1785e8a4":"markdown","6c1b2bac":"markdown","f5069308":"markdown","47981216":"markdown","9fd5581c":"markdown","b3422fd6":"markdown","b54820db":"markdown"},"source":{"bba613a0":"pip install pyclustertend","202f206c":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom pyclustertend import hopkins\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","b38cef5e":"df = pd.read_csv(\"..\/input\/mall-customers\/Mall_Customers.csv\",index_col=0)\ndf.head()","6f961249":"df.rename({'Genre':\"Gender\"},axis=1, inplace=True)","f7cb03bb":"df.shape","b9dc162a":"df.isnull().sum()","6e309123":"df.info()","b30f2166":"df.rename(columns={'Annual Income (k$)':'Income',\n                   'Spending Score (1-100)':'Score'},inplace=True)","cbb88492":"df.head(2)","24b21761":"df.describe()","6f626128":"# Hem kategorik hem numeric describe daha efektiftir.\n\ndf.describe(include='all', percentiles=[.1, .25, .5, .75, .95])","a65ca856":"df.groupby('Gender').describe().T","0aa5b438":"sns.pairplot(df,hue='Gender');","afeab7bd":"plt.figure(figsize=(5,5))\nsns.heatmap(df.corr(),annot=True, cmap=\"coolwarm\");","c6bc1d9f":"plt.figure(figsize=(15,15))\nsns.countplot(x='Income', data=df)\n\nplt.subplot(3,1,1)\nsns.countplot(x='Age', data=df)\n\nplt.subplot(3,1,2)\nsns.countplot(x='Income', data=df, palette=\"coolwarm\")\n\nplt.subplot(3,1,3)\nsns.countplot(x='Score', data=df,palette=\"viridis\")\nplt.show()","f7d50c71":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,3,1)\nsns.boxplot(x='Gender', y='Age', data=df, palette=\"coolwarm\")\n\nplt.subplot(1,3,2)\nsns.boxplot(x='Gender', y='Income', data=df, palette=\"coolwarm\")\n\nplt.subplot(1,3,3)\nsns.boxplot(x='Gender', y='Score', data=df, palette=\"coolwarm\")\nplt.show()","18040dee":"plt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nsns.distplot(df.Age,bins=5)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\n\nplt.subplot(1,3,2)\nsns.distplot(df['Income'],color='red',bins=7)\nplt.title('Annual Income Distribution')\nplt.xlabel('Annual Income')\nplt.ylabel('')\n\nplt.subplot(1,3,3)\nsns.distplot(df['Score'],color='green',bins=4)\nplt.title('Score Distribution')\nplt.xlabel('Score')\nplt.ylabel('')\nplt.show()","50613e49":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.kdeplot(x='Age',data=df,hue=\"Gender\",shade=True)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\n\nplt.subplot(1,3,2)\nsns.kdeplot(x='Income',data=df,hue=\"Gender\",shade=True)\nplt.title('Annual Income Distribution')\nplt.xlabel('Annual Income')\nplt.ylabel('')\n\nplt.subplot(1,3,3)\nsns.kdeplot(x='Score',data=df,hue=\"Gender\",shade=True)\nplt.title('Score Distribution')\nplt.xlabel('Score')\nplt.ylabel('')\nplt.show()","bddef104":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,3,1)\nsns.stripplot(df['Gender'], df['Age'])\n\nplt.subplot(1,3,2)\nsns.stripplot(df['Gender'], df['Income'])\n\nplt.subplot(1,3,3)\nsns.stripplot(df['Gender'], df['Score'])\nplt.show()","9f7b0732":"plt.figure(figsize=(6,6))\n\nexplode = [0,0.1]\nplt.pie(df['Gender'].value_counts(),explode=explode,autopct='%1.1f%%',shadow=True,startangle=140)\nplt.legend(labels=['Female','Male'])\nplt.title('Male and Female Distribution')\nplt.axis('off')\nplt.show()\n\ndf.Gender.value_counts(dropna=False)","7d2960ad":"df=pd.get_dummies(df,prefix='',prefix_sep='',drop_first=True)\ndf.head()","8a61f808":"df1=df[['Age','Score']]\ndf1.head(2)","4cb641bc":"from pyclustertend import hopkins\nhopkins(df,df.shape[0])","05c4822f":"hopkins(scale(df),df.shape[0])","8e8ae66c":"plt.figure(figsize=(7,5))\nsns.scatterplot(x='Age',y='Score',data=df1);","c3aa404e":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df1)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df1, model.labels_)}')","9114354f":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(scale(df1))\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(scale(df1), model.labels_)}')","b1811d2c":"ssd = []\nK = range(2,10)\nfor k in K:\n    kmeans = KMeans(n_clusters = k).fit((df1))\n    ssd.append(kmeans.inertia_)","dfe2f970":"plt.figure(figsize=(8,4))\nplt.plot(K, ssd, \"bo-\")\nplt.xlabel(\"Different k values\")\nplt.ylabel(\"inertia-error\")\nplt.title(\"Elbow Method\")\nplt.grid()\nplt.show()","ff57a440":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (2,10))\nvisu.fit(df1)\nvisu.show();","6fcfd101":"kmeans = KMeans(n_clusters = 4).fit(df1)\nlabels = kmeans.labels_","8b641c07":"plt.figure(figsize=(7,5))\nplt.scatter(df1.iloc[:,0],df1.iloc[:,1],c=kmeans.labels_,cmap=\"rainbow\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300,alpha=0.5, label = 'Centroids')\nplt.xlabel(df1.columns[0])\nplt.ylabel(df1.columns[1])\nplt.show()","81708e2c":"df['Age_Score_Clusters']=labels\ndf.head(5)","08d7a105":"df2=df[['Score','Income']]\ndf2.head(2)","cf1517fd":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df2)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df2, model.labels_)}')","7005f22c":"ssd = []\nK = range(2,10)\nfor k in K:\n    kmeans = KMeans(n_clusters = k).fit((df2))\n    ssd.append(kmeans.inertia_)","468311b6":"plt.figure(figsize=(8,4))\nplt.plot(K, ssd, \"bo-\")\nplt.xlabel(\"Different k values\")\nplt.ylabel(\"inertia-error\")\nplt.title(\"Elbow Method\")\nplt.grid()\nplt.show()","fdb6307b":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (2,10))\nvisu.fit(df2)\nvisu.show();","07d4a3f9":"kmeans = KMeans(n_clusters = 5).fit(df2)\nlabels = kmeans.labels_","6e6625d4":"plt.figure(figsize=(7,5))\nplt.scatter(df2.iloc[:,0],df2.iloc[:,1],c=kmeans.labels_,cmap=\"rainbow\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=500,alpha=0.7, label = 'Centroids')\nplt.xlabel(df2.columns[0])\nplt.ylabel(df2.columns[1])\nplt.show()","ae90c37c":"df['Score_Income_Clusters']=labels\ndf.head(5)","7d6518a6":"df3=df[['Age','Income']]\ndf3.head(2)","5322693a":"ssd =[]\n\nK = range(2,10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df3)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df3, model.labels_)}')","a63499d6":"ssd = []\nK = range(2,10)\nfor k in K:\n    kmeans = KMeans(n_clusters = k).fit((df3))\n    ssd.append(kmeans.inertia_)","6a231134":"plt.figure(figsize=(8,4))\nplt.plot(K, ssd, \"bo-\")\nplt.xlabel(\"Different k values\")\nplt.ylabel(\"inertia-error\")\nplt.title(\"Elbow Method\")\nplt.grid()\nplt.show()","f005a97d":"from yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (2,10))\nvisu.fit(df3)\nvisu.show();","052e3e9b":"kmeans = KMeans(n_clusters = 4).fit(df3)\nlabels = kmeans.labels_","147a81f7":"plt.figure(figsize=(7,5))\nplt.scatter(df3.iloc[:,0],df2.iloc[:,1],c=kmeans.labels_,cmap=\"rainbow\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=500,alpha=0.7, label = 'Centroids')\nplt.xlabel(df3.columns[0])\nplt.ylabel(df3.columns[1])\nplt.show()","4b7edf14":"df['Age_Income_Clusters']=labels","5111db36":"df.drop(['Age_Score_Clusters','Score_Income_Clusters', 'Age_Income_Clusters'],axis=1,inplace=True)\ndf_backup=df.copy()","52331979":"df.drop(['Male'],axis=1,inplace=True)","c89b9901":"# hc_complete =  linkage(df,\"complete\")\nhc_ward = linkage(df, \"ward\")\n# hc_single = linkage(df, \"single\")","13c0ed8f":"plt.figure(figsize = (12,7))\nplt.title(\"dendrogram\")\nplt.xlabel(\"Obvervations\")\nplt.ylabel(\"Distance\")\ndendrogram(hc_ward, leaf_font_size = 10);","801f5e17":"hc = AgglomerativeClustering(n_clusters = 5,affinity='euclidean',linkage='ward') # burada default linkage='ward' dur.\nhc.fit_predict(df)","880c20c0":"print(f'Silhouette Score(n=5): {silhouette_score(df, hc.labels_)}')","ed7d3c5f":"df[\"hc_clusters\"] = hc.labels_\ndf.sample(5)","64f350df":"df.groupby('hc_clusters').mean()","a8fc7ce8":"kmeans = KMeans(n_clusters = 5).fit(df[['Age', 'Income', 'Score']])\nkmeans.fit_predict(df[['Age', 'Income', 'Score']])\nlabels = kmeans.labels_\ndf[\"kmeans_clusters\"] = labels","9b97b234":"df.groupby('kmeans_clusters').mean()","8d1e1d1f":"pd.crosstab(df.kmeans_clusters,df.hc_clusters)","c5a0371f":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(14,6)) # sharey=True ile y eksen labels lari ortak kullanirlar.\nax1.set_title('HR')\nax1.set_xlabel('Income')\nax1.set_ylabel('Score')\nax1.scatter(df.iloc[:,1], df.iloc[:,2], c = df.hc_clusters, s = 40, cmap = \"rainbow\") # s=40 ,dot size\nax2.set_title(\"K-Means\")\nax2.set_xlabel('Income')\nax2.set_ylabel('Score')\nax2.scatter(df.iloc[:,1], df.iloc[:,2], c = df.kmeans_clusters, s = 40, cmap = \"rainbow\");","a2be71d7":"# HC Clustering\nsilhouette_score(df.drop(['kmeans_clusters','hc_clusters'],axis=1), hc.fit_predict(df))","eed16293":"# K-Means Clustering\nsilhouette_score(df.drop(['kmeans_clusters','hc_clusters'],axis=1), kmeans.fit_predict(df))","6ac3acc1":"from mpl_toolkits.mplot3d import Axes3D\n\nkmeans= KMeans(n_clusters=5).fit(df)\nclusters= kmeans.labels_\ncenters = kmeans.cluster_centers_","79479084":"plt.rcParams[\"figure.figsize\"] =(10,10)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:,0],df.iloc[:,1],df.iloc[:,2],c = clusters, s = 50,cmap = \"rainbow\")\nax.scatter(centers[:,0],centers[:,1],centers[:,2],c=\"blue\",marker=\"o\",s = 500);","d0f56e90":"df.drop(['kmeans_clusters','hc_clusters'],axis=1).groupby(df.kmeans_clusters).mean().plot(figsize = (10,6), kind='bar')\nplt.show()","9d2c9674":"plt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nsns.countplot(x='hc_clusters', data=df.merge(df_backup), hue='Male',order = df['hc_clusters'].value_counts().index)\nplt.title(\"Male vs Female Ratio in Each Cluster\")\n\nplt.subplot(1,2,2)\nsns.countplot(x='kmeans_clusters', data=df.merge(df_backup), hue='Male', order = df['kmeans_clusters'].value_counts().index)\nplt.show()","17fc179f":"**Hopkins Test**\n* Null Hypothesis(Ho) ve Alternative Hypothesis(Ha) temeline dayaniyor.\n* Null Hypothesis(Ho): Uniform dagilim var, anlamli k\u00fcmeleme yok.\n* Alternative Hypothesis(Ha):  Veri, ratsgele veri noktalarindan olu\u015fur. Yani Kumeleme vardir.\n* [0,1] araliginda bir score verir. score, 0\u2019a yakla\u015ft\u0131k\u00e7a veri uniform degil,yani clusteringe meyilli\n* 1\u2019e yakla\u015ft\u0131k\u00e7a uniform yapi var, 0.5 gecmedikce k\u00fcmelenebilir olarak ifade edilir. pratikte 0.3 sinir alinir.","9401d870":"#### iii. *Visualizing and Labeling All the Clusters* ","fb0f8139":"**k=4 gives the highest S Score**","0c4293f7":"### Clustering based on Annual Income and Age","fac7641c":"#### *iii. Apply K Means*","e017d64e":"---\n---","fd3eca03":"> **`ward`**","bb4edc6d":"#### *i. Create a new dataset with two variables of your choice*","afb4232b":"#### ii. *Apply Agglomerative Clustering*","1957376e":"---\n---","d6d85bb4":"**1st group(Orange):** Low income, low spending score\n* Kisiye ozel cazip indirim kampanyalari yapilabilir.\n\n**2nd group(Green):** High income, low spending score\n* Aylik belli harcama hedefine ulasanlara hediye ceki verilebilir.\n\n**3rd group(Blue):** Moderate income, moderate spending score\n* Her alisveris icin cekilise katima sansi verilebilir.\n\n**4th group(Red):** Low income, high spending score\n* Sadik musteri yapmak icin uyelik karti verilebilir.\n\n**5th group(Purple):** High income, high spending score\n* Sadik musteri yapmak icin uyelik karti verilebilir.\n","4c36f540":"## 1. Import Libraries, Load Dataset, Exploring Data\n\nThere is a big mall in a specific city that keeps information of its customers who subscribe to a membership card. In the membetrship card they provide following information : gender, age and annula income. The customers use this membership card to make all the purchases in the mall, so tha mall has the purchase history of all subscribed members and according to that they compute the spending score of all customers. You have to segment these customers based on the details given. ","5a5992f8":"#### *iii. Apply K Means*","697854f2":"#### *iv. Visualizing and Labeling All the Clusters*","1e8009f3":"**`S(Silhouette) Score`**\n* her veri i\u00e7in iki uzakl\u0131\u011f\u0131 baz al\u0131r. Bu uzakl\u0131klardan ilki verinin bulundu\u011fu k\u00fcmeye ait di\u011fer verilere olan uzakl\u0131klar\u0131n ortalamas\u0131d\u0131r. \u0130kincisi veriye en yakin komsu k\u00fcmenin tum verilerine olan uzakl\u0131klar\u0131n ortalamas\u0131d\u0131r.\n* S de\u011feri ile ifade edilir, s, 1\u2019e yakinsa high clustering, -1e yakinsa low clustering e\u011filimi gosterir.","0910d4c9":"### > **Elbow Method with yellowbrick**","0e8d58ab":"The main purpose of this project is to perform [cluster analysis](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis#:~:text=Cluster%20analysis%20or%20clustering%20is,in%20other%20groups%20(clusters).) with the [K-Means](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) algorithm. \n\nYou can perform many [cluster analysis](http:\/\/www.stat.columbia.edu\/~madigan\/W2025\/notes\/clustering.pdf) using different variables. If you use a maximum of two variables for each cluster analysis, you can identify cluster labels more clearly.\n\nFirst, the K-Means algorithm expects you to determine the number of clusters (*n_clusters*). You can determine the optimal number of clusters for each cluster analysis in various ways. In this case, you are expected to use the [Elbow Method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering).\n\nFinally, different information is obtained in each analysis. Therefore, different labeling should be done as a result of each cluster analysis. \n\nLabeling example: \n\n- **Normal Customers**  -- An Average consumer in terms of spending and Annual Income\n- **Spender Customers** --  Annual Income is less but spending high, so can also be treated as potential target customer.","cfe2fa42":"Welcome to \"***Clustering (Customer Segmentation) Project***\". This is the last medium project of ***Machine Learning*** course. \n\nAt the end of this project, you will have performed ***Cluster Analysis*** with an ***Unsupervised Learning*** method.\n\n---\n\nIn this project, customers are required to be segmented according to the purchasing history obtained from the membership cards of a big mall.\n\nThis project is less challenging than other projects. After getting to know the data set quickly, you are expected to perform ***Exploratory Data Analysis***. You should observe the distribution of customers according to different variables, also discover relationships and correlations between variables. Then you will spesify the different variables to use for cluster analysis.\n\nFinally, you should clustered customers using the ***K-Means Clustering*** method, after that label the clusters.\n\n- ***NOTE:*** *This project assumes that you already know the basics of coding in Python. You should also be familiar with the theory behind Cluster Analysis and scikit-learn module as well as Machine Learning before you begin.*","9355d9d0":"> **We got the highest S score with k=4. After scaling, S Score generally decreased. we will continue with unscaled data.**","594d03fe":"---\n---","d8c49963":"> **We can say K-means Clustering Model is more succesful. Its S Score is higher than HC**","f663da15":"### > **Elbow Method**","7e2f9b99":"#### *i. Create a new dataset with two variables of your choice*","f62df55d":"#### *iv. Visualizing and Labeling All the Clusters*","121338bf":"### Hierarchical Clustering","932aa3ce":"### Clustering based on Annual Income and Spending Score","66733de8":"#### Explore Data\n\nYou can rename columns to more usable, if you need.","c2e125e9":"#### *ii. Determine optimal number of clusters*","16651b5a":"**k=4 gives the highest S Score**","3a97b779":"**Hierarchy Clustering**","4f335261":"#### Import Libraries","37638888":"Mentoring Prep. and self study#### \n\n#### 1. Import Libraries, Load Dataset, Exploring Data\n- Import Libraries\n- Load Dataset\n- Explore Data\n\n#### 2. Exploratory Data Analysis (EDA)\n\n\n#### 3. Cluster Analysis\n\n- Clustering based on Age and Spending Score\n\n    *i. Create a new dataset with two variables of your choice*\n    \n    *ii. Determine optimal number of clusters*\n    \n    *iii. Apply K Means*\n    \n    *iv. Visualizing and Labeling All the Clusters*\n    \n    \n- Clustering based on Annual Income and Spending Score\n\n    *i. Create a new dataset with two variables of your choice*\n    \n    *ii. Determine optimal number of clusters*\n    \n    *iii. Apply K Means*\n    \n    *iv. Visualizing and Labeling All the Clusters*\n    \n    \n- Hierarchical Clustering\n\n    *i. Determine optimal number of clusters using Dendogram*\n\n    *ii. Apply Agglomerative Clustering*\n\n    *iii. Visualizing and Labeling All the Clusters* \n\n- Conclusion","84b18f0b":"**k=5 gives the highest S Score**","6d48f4b2":"#### *iv. Visualizing and Labeling All the Clusters*","1e19f030":"## 3. Cluster Analysis","66b97b33":"**`S(Silhouette) Score with scaled data`**","0ba01da3":"### > **Elbow Method with yellowbrick**","cc523b26":" **K_Means vs HC**","36acd3e7":"# WELCOME!","a099f507":"## 2. Exploratory Data Analysis (EDA)\n\nAfter performing Cluster Analysis, you need to know the data well in order to label the observations correctly. Analyze frequency distributions of features, relationships and correlations between the independent variables and the dependent variable. It is recommended to apply data visualization techniques. Observing breakpoints helps you to internalize the data.\n\n\n\n","27ee33f4":"#### *iii. Apply K Means*","c9ac37af":"### Clustering based on Age and Spending Score","621f20c9":"# #Tasks","71fed2db":"#### Load Dataset","f7f806f6":"### Conclusion","4567d88c":"**Visualization of Clusters in 3D**","ef879610":"* Yukarida min-%10 ve max-%95 mukayesesi yaparak outliers tespiti yapilabilir.","a23bc0f6":"> **Elbow Method with yellowbrick**","543117be":"#### *ii. Determine optimal number of clusters*","abc186d1":"### > **Elbow Method**","2350a0de":"#### *i. Determine optimal number of clusters using Dendogram*","1785e8a4":"**`S(Silhouette) Score`**\n* her veri i\u00e7in iki uzakl\u0131\u011f\u0131 baz al\u0131r. Bu uzakl\u0131klardan ilki verinin bulundu\u011fu k\u00fcmeye ait di\u011fer verilere olan uzakl\u0131klar\u0131n ortalamas\u0131d\u0131r. \u0130kincisi veriye en yakin komsu k\u00fcmenin tum verilerine olan uzakl\u0131klar\u0131n ortalamas\u0131d\u0131r.\n* S de\u011feri ile ifade edilir, s, 1\u2019e yakinsa high clustering, -1e yakinsa low clustering e\u011filimi gosterir.","6c1b2bac":"**`K-Means`**","f5069308":"#### *i. Create a new dataset with two variables of your choice*","47981216":"---\n---","9fd5581c":"**Lets find k value given the highest Silhouette Score**","b3422fd6":"\u201ck-means algoritmas\u0131, \u00e7e\u015fitli nedenlerden dolay\u0131 kategorik verilere do\u011frudan uygulanamaz. Kategorik veriler i\u00e7in sample ayr\u0131kt\u0131r ve \u00d6klid mesafesi anlaml\u0131 de\u011fildir.\u201d","b54820db":"**Lets find k value given the highest Silhouette Score**"}}