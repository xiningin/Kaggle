{"cell_type":{"508f132a":"code","79dc31b9":"code","bdeab9fc":"code","15ade512":"code","8fdca003":"code","938fbb2e":"code","c6b65cef":"code","5cf33c34":"code","817aece2":"code","b0cc5fb4":"code","c0df5ded":"code","b6042f68":"code","970a539a":"code","7f64e122":"code","680dca1f":"code","7a994c73":"code","75477e4a":"code","94735cea":"code","fbf1084f":"code","04318729":"code","da7c09b7":"code","427af32c":"code","8e0910eb":"code","62f70bdd":"code","7a6f8d1b":"code","d5dcd7eb":"code","0de3b9b9":"code","0275f7ac":"code","0e743407":"code","cb822386":"code","74411de5":"code","0500a039":"code","5b9ced4d":"code","05deffeb":"code","375aaeaf":"code","7222bb91":"code","ae7c4748":"code","d37a3176":"code","d7610bed":"code","331a4eb3":"code","c336f1ef":"code","e0bd1e24":"code","71b8fd36":"code","ce10aa42":"code","9056fe30":"code","9824e08f":"code","a7e14301":"code","b8a32e02":"code","f20aa30c":"code","0587da86":"code","57770283":"code","70a607f0":"code","93b8d76d":"code","e155b316":"code","8ecd5f27":"code","9f642f9f":"code","5502deaf":"code","1307f11d":"code","32173aff":"code","4818a13c":"code","02953665":"code","a865cce0":"code","3217759a":"code","ebb3b593":"code","03fc9b09":"code","6f02a23a":"code","32df6050":"code","ef83ed9c":"code","4cfb3628":"code","acf05d6f":"code","9365b4d8":"code","22e6abc0":"code","b527b3ab":"code","9f353dc3":"code","6d96f0a5":"code","90d3f01b":"code","df17eef1":"code","a52ce607":"code","ac7d9678":"code","19d93a17":"code","d0908c22":"code","e24f0b0b":"code","f21bd8f9":"code","6a9768b4":"code","0872af23":"code","da030d0a":"code","ad353e03":"code","4f0952c2":"code","7f20fa17":"code","e5801b00":"code","5050df2a":"code","d4444d7d":"code","60227cab":"code","fa8abc7b":"code","777f0d2d":"code","146fb884":"code","0d2ccf81":"code","8febe413":"code","87585fd2":"code","bdb6fd31":"code","bc91fdb6":"code","35d5d1fa":"code","885935db":"code","9b7b5bd5":"code","e02d0300":"code","4588154c":"code","e8c28980":"code","b7482539":"code","b80fccea":"code","373c1f94":"code","827f8d5e":"code","e6cc07ce":"code","65adc4ae":"code","14c709eb":"code","a0d94075":"code","6786710c":"code","488787af":"code","6203ad52":"code","a73651ba":"code","7ed1a88f":"code","514f4d25":"code","7e840eb7":"code","7627f9c3":"code","77ec1b90":"code","cd7c0b7a":"code","2f7cef29":"code","17f33162":"code","353c8cc9":"code","a6d2f7fe":"code","649b23bb":"code","06f673fb":"code","9548d637":"code","a7e26729":"code","2b8b5d3f":"code","993a89c9":"code","49da62cb":"code","a30da00a":"code","699fa87b":"code","08eb6422":"code","099c83bc":"code","6801efe1":"code","e30a507a":"code","86f3585b":"code","27e17e52":"code","02b98d84":"code","c315c2f1":"code","f6003977":"code","7b512582":"code","5e810e3b":"code","5d92540c":"code","6be74693":"code","e2e1ba49":"code","8e305fdb":"code","430c8fb3":"code","fff1c5d4":"code","cbe8d3ed":"code","b9f70e47":"code","92341e31":"code","8f7de1e3":"code","efc3bdce":"code","e577a29f":"code","732ffa8d":"code","7469f602":"code","5d66aded":"code","e24a54dd":"code","b3718473":"code","9dbcda36":"code","5f796c87":"code","26770221":"code","f2909cce":"code","f33d7198":"code","f058f63d":"code","5816376b":"code","a9b00e9c":"code","495e0ed4":"markdown","81e8bbc1":"markdown","8eabed89":"markdown","3616de20":"markdown","4f8b323f":"markdown","f6313485":"markdown","e069e011":"markdown","2ab76bfc":"markdown","c1bde949":"markdown","bad32aa8":"markdown","31ad8361":"markdown","2979d67b":"markdown","6efe5297":"markdown","f472d0c7":"markdown","52e2e37d":"markdown","002682f8":"markdown","5518a4f9":"markdown","fbb8538f":"markdown","6e633f83":"markdown","204db63d":"markdown","c334e37a":"markdown","cd3072a3":"markdown","91d2761e":"markdown","4bc22985":"markdown","2221af5d":"markdown","14c2020a":"markdown","eb1460fc":"markdown","a2802da3":"markdown","3ea88736":"markdown","01f8cb68":"markdown","0b13fba1":"markdown","da74b2a7":"markdown","7bf4275d":"markdown","4fe07f7c":"markdown","ded05daf":"markdown","6ca89cfe":"markdown","61dc6626":"markdown","811623ae":"markdown","90252337":"markdown","fed7ba99":"markdown","16d66281":"markdown","1014b006":"markdown","5624ea77":"markdown","9bf32306":"markdown","a4481afb":"markdown","460f43f6":"markdown","ad3c8074":"markdown","db5045c5":"markdown","1d7d3dcf":"markdown","1bdb1d43":"markdown","ef332e0e":"markdown","eed872a8":"markdown","10de145b":"markdown","f68c772b":"markdown","d6888d83":"markdown","1ca90bdf":"markdown","ac906fbc":"markdown","206c0761":"markdown","4bab9657":"markdown","ea0bdd5a":"markdown","a66b4058":"markdown","df298343":"markdown","55398c5e":"markdown","a302b627":"markdown","33bd8d6a":"markdown","6d2aa2a6":"markdown","b5066a34":"markdown","1ba8eddb":"markdown","be0c960c":"markdown","858960b1":"markdown","7c1fb5b2":"markdown","e09d6c53":"markdown","ccbaf7a1":"markdown","d3d27361":"markdown","c19ecdd5":"markdown","3cbd8f70":"markdown","5fd0b6de":"markdown","9680e88f":"markdown","c320495e":"markdown","8c9f38ee":"markdown","98702e09":"markdown","96e0ea0c":"markdown","b9934824":"markdown","34906403":"markdown","d747dabc":"markdown","28878d87":"markdown","06595177":"markdown","24b18585":"markdown","c895273b":"markdown","af36c0b4":"markdown","e0e94091":"markdown","e509228c":"markdown","b3269f15":"markdown","f4676ef7":"markdown","17c3d337":"markdown","78275d5c":"markdown","675d22aa":"markdown","927fb3c2":"markdown","6e49a7db":"markdown","504b048e":"markdown","dc6b9584":"markdown","7c4f96da":"markdown","f10b734f":"markdown","7130ed0e":"markdown","dd1d1be3":"markdown","4344d7c7":"markdown","ee8ca561":"markdown","5135e636":"markdown","f0cff7fd":"markdown","b264bd87":"markdown","7004b14a":"markdown","579f9914":"markdown","59274e2e":"markdown","d41b1b27":"markdown","48f7931b":"markdown","6b792f91":"markdown","01e86e75":"markdown","0220309a":"markdown","9745329a":"markdown","d9862898":"markdown","01396b6d":"markdown","65bef72b":"markdown","e49fd3bd":"markdown","71f2fc51":"markdown","6d4307bf":"markdown","fead7319":"markdown","8abd8967":"markdown","554cfd48":"markdown","f07962dc":"markdown","ca7d6d4f":"markdown","50d18879":"markdown","d8750dec":"markdown","888427cd":"markdown","59efed1f":"markdown","8fc57ce4":"markdown","b6be94a9":"markdown","e429103e":"markdown","397390d8":"markdown","af1c93a7":"markdown","a5da3b3b":"markdown","3d90ee94":"markdown","96bc1ab4":"markdown","eef670ca":"markdown","d6f48390":"markdown","3e81c29b":"markdown","5ee2a329":"markdown","aac2f6c0":"markdown","a716941f":"markdown","0c77a0a1":"markdown","30c94c31":"markdown","ba3cb644":"markdown","435a0276":"markdown","00f642a2":"markdown","62142c37":"markdown","dab9847c":"markdown","d5961e83":"markdown","18294740":"markdown","5cd59ec0":"markdown","8360b0ff":"markdown"},"source":{"508f132a":"import warnings\nwarnings.filterwarnings('ignore')\n","79dc31b9":"import numpy as np\nimport datetime as dt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy.random import uniform\nfrom math import isnan\n\n#visualisation\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Data display coustomization\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }<\/style>\"))\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","bdeab9fc":"Data_dict = pd.read_excel(\"..\/input\/lead-scoring-dataset\/Leads Data Dictionary.xlsx\",header=2)\nData_dict.drop('Unnamed: 0',inplace=True,axis=1)\nData_dict.reset_index(drop=True, inplace=True)\nData_dict.head(len(Data_dict))\n","15ade512":"leads = pd.read_csv(\"..\/input\/lead-scoring-dataset\/Lead Scoring.csv\",  sep = ',',encoding = \"ISO-8859-1\")\n\nleads_copy=leads.copy()\n\nleads.head()\n","8fdca003":"leads.shape\nprint ('The Input Data has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","938fbb2e":"leads.info()","c6b65cef":"leads.describe(percentiles=[.25,.5,.75,.90,.95,.99])","5cf33c34":"Conversion_rate = (sum(leads['Converted'])\/len(leads['Converted'].index))*100\nprint(\"The conversion rate of leads is: \",Conversion_rate)","817aece2":"# removing duplicate rows\nleads.drop_duplicates(subset='Lead Number')\nprint ('The Input Data after Duplicate check has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","b0cc5fb4":"leads.isnull().all(axis=0).any()","c0df5ded":"leads.loc[:, (leads != 0).any(axis=0)]\nprint ('The Input Data after droping columns with only 0 values has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","b6042f68":"leads.nunique()","970a539a":"leads= leads.loc[:,leads.nunique()!=1]\nprint ('The Input Data after droping columns with only one unique values has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","7f64e122":"leads = leads.drop(['Prospect ID','Lead Number'], axis=1)","680dca1f":"leads.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index',\n           'Asymmetrique Activity Score','Asymmetrique Profile Score',\n           'Tags','Lead Quality','Lead Profile'], 1, inplace = True)","7a994c73":"# As we can observe that there are select values for many column. This is because customer did not select any option from the list, hence it shows select.\n# Select values are as good as NULL.\n# Converting 'Select' values to NaN.\nleads = leads.replace('Select', np.nan)","75477e4a":"total = pd.DataFrame(leads.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(leads.isnull().sum()\/leads.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1)","94735cea":"plt.figure(figsize=(10,10))\nsns.set(font_scale=1.4)\nsns.heatmap(leads.isnull(), cbar=False)\nplt.tight_layout()\nplt.show()","fbf1084f":"leads = leads.drop(leads.loc[:,list(round(100*(leads.isnull().sum()\/len(leads.index)), 2)>45)].columns, 1)","04318729":"total = pd.DataFrame(leads.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(leads.isnull().sum()\/leads.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1)","da7c09b7":"print('Descriptive Statistics for City:')\nprint('-------------------------')\nprint(leads['City'].describe())\n\nprint('*************************************')\n\nprint('Value counts ')\nprint('----------------')\nprint(leads['City'].value_counts())\nprint('*************************************')\n\nprint('City countplot ')\nprint('----------------')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['City'])\nplt.xticks(rotation = 90)\nplt.show()\n","427af32c":"leads['City'] = leads['City'].replace(np.nan, 'Mumbai')\n\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['City'])\nplt.xticks(rotation = 90)\nplt.show()\nprint(leads['City'].value_counts())","8e0910eb":"print('Descriptive Statistics for Specialization:')\nprint('-------------------------')\nprint(leads['Specialization'].describe())\n\nprint('*************************************')\n\nprint('Value counts ')\nprint('----------------')\nprint(leads['Specialization'].value_counts())\nprint('*************************************')\n\nprint('Specialization countplot ')\nprint('----------------')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['Specialization'])\nplt.xticks(rotation = 90)\nplt.show()\n","62f70bdd":"leads['Specialization'] = leads['Specialization'].replace(np.nan, 'Others')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['Specialization'])\nplt.xticks(rotation = 90)\nplt.show()\n\nprint(leads['Specialization'].value_counts())","7a6f8d1b":"print('Descriptive Statistics for What matters most to you in choosing a course:')\nprint('-------------------------')\nprint(leads['What matters most to you in choosing a course'].describe())\n\nprint('*************************************')\n\nprint('Value counts ')\nprint('----------------')\nprint(leads['What matters most to you in choosing a course'].value_counts())\nprint('*************************************')\n\nprint('What matters most to you in choosing a course countplot ')\nprint('----------------')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['What matters most to you in choosing a course'])\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()\n","d5dcd7eb":"leads= leads.drop(['What matters most to you in choosing a course'],axis=1)\nleads.head()","0de3b9b9":"print('Descriptive Statistics for What is your current occupation:')\nprint('-------------------------')\nprint(leads['What is your current occupation'].describe())\n\nprint('*************************************')\n\nprint('Value counts ')\nprint('----------------')\nprint(leads['What is your current occupation'].value_counts())\nprint('*************************************')\n\nprint('What is your current occupation countplot ')\nprint('----------------')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['What is your current occupation'])\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","0275f7ac":"leads['What is your current occupation'] = leads['What is your current occupation'].replace(np.nan, 'Unemployed')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['What is your current occupation'])\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()\nprint(leads['What is your current occupation'].value_counts())","0e743407":"print('Descriptive Statistics for Country:')\nprint('-------------------------')\nprint(leads['Country'].describe())\n\nprint('*************************************')\n\nprint('Value counts ')\nprint('----------------')\nprint(leads['Country'].value_counts())\nprint('*************************************')\n\nprint('Country countplot ')\nprint('----------------')\nplt.figure(figsize = (10,5))\nax= sns.countplot(leads['Country'])\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","cb822386":"leads= leads.drop(['Country'],axis=1)","74411de5":"total = pd.DataFrame(leads.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(leads.isnull().sum()\/leads.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1)","0500a039":"leads.dropna(inplace = True)","5b9ced4d":"total = pd.DataFrame(leads.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(leads.isnull().sum()\/leads.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1)","05deffeb":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# NUMERIC\nnumdata=leads[list(leads.select_dtypes(numerics).columns)]\n# CATEGORICAL \ncatdata=leads[list(leads.select_dtypes(exclude=numerics).columns)]\nprint('***************************************************')\nprint('---------------------------------------------------')\nprint('Dviding the Features as Numerical and Categorical ')\nprint('---------------------------------------------------')\n\nprint('Categorical Columns')\nprint('---------------------------------------------------')\n\nprint(catdata.columns)\nprint('---------------------------------------------------')\n\nprint('Numerical Columns')\nprint('---------------------------------------------------')\n\nprint(numdata.columns)\nprint('---------------------------------------------------')\nprint('****************************************************')","375aaeaf":"Data_retained_EDA= len(leads)* 100 \/ len(leads_copy)\nprint(\"{} % of original rows is available for EDA\".format(round(Data_retained_EDA,2)))","7222bb91":"catdata.columns","ae7c4748":"# Count plotting in logarithmic scale\n\ndef uniplot(df,col,title,hue =None):\n    \n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 22\n    plt.rcParams['axes.titlepad'] = 30\n    \n    \n    temp = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n    fig.set_size_inches(width , 8)\n    plt.xticks(rotation=45)\n    plt.yscale('log')\n    plt.title(title)\n    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='magma') \n        ","d37a3176":"leads['Lead Source'] = leads['Lead Source'].replace(['google'], 'Google')\nleads['Lead Source'] = leads['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others')\n\n# Let's keep considerable last activities as such and club all others to \"Other_Activity\"\nleads['Last Activity'] = leads['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', \n                                                       'Visited Booth in Tradeshow', 'Approached upfront',\n                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'],\n                                                      'Other_Activity')\n","d7610bed":"for columns in catdata:\n  uniplot(leads,col=columns,title='Distribution of {0}'.format(columns),hue='Converted')","331a4eb3":"leads = leads.drop(['Do Not Email','Do Not Call','Search',\n                 'Newspaper Article','X Education Forums','Newspaper',\n           'Digital Advertisement','Through Recommendations','A free copy of Mastering The Interview'],1)","c336f1ef":"print ('Shape of Data after Univariate EDA - Categorical Columns has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","e0bd1e24":"leads.info()","71b8fd36":"numdata.columns","ce10aa42":"### Box plotting for univariate variables analysis in logarithmic scale\n#Ditribution for some of the numerical features.\nplt.figure(figsize=(25,4))\nplt.subplot(1,3,1)\nplt.tick_params(axis='both', which='minor', labelsize=8)\nsns.boxplot(y=leads['TotalVisits'])\nplt.subplot(1,3,2)\n\nsns.boxplot(y=leads['Total Time Spent on Website'])\nplt.subplot(1,3,3)\n\nsns.boxplot(y=leads['Page Views Per Visit'])\nplt.show()","9056fe30":"percentiles = leads['TotalVisits'].quantile([0.05,0.95]).values \nleads['TotalVisits'][leads['TotalVisits'] >= percentiles[1]]=percentiles[1]\n\npercentiles = leads['Total Time Spent on Website'].quantile([0.05,0.95]).values \nleads['Total Time Spent on Website'][leads['Total Time Spent on Website'] >= percentiles[1]]=percentiles[1]\n\npercentiles = leads['Page Views Per Visit'].quantile([0.05,0.95]).values \nleads['Page Views Per Visit'][leads['Page Views Per Visit'] >= percentiles[1]]=percentiles[1]","9824e08f":"#Ditribution for some of the numerical features after treatment of Outliers \nplt.figure(figsize=(25,7))\nplt.subplot(1,3,1)\n\nsns.boxplot(y=leads['TotalVisits'], x = leads['Converted'])\nplt.subplot(1,3,2)\n\nsns.boxplot(y=leads['Total Time Spent on Website'], x= leads['Converted'])\nplt.subplot(1,3,3)\n\nsns.boxplot(y=leads['Page Views Per Visit'], x= leads['Converted'])\nplt.show()\n","a7e14301":"plt.figure(figsize=(30,50))\nsns.set(font_scale=1.1)\npair_plot=sns.pairplot(leads,diag_kind='kde',corner=True,plot_kws=dict(s=7, edgecolor=\"r\", linewidth=1))","b8a32e02":"plt.figure(figsize = (10,5))\nsns.heatmap(leads.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","f20aa30c":"print(\"Original Columns Retained :{} % \".format(round((100* len(leads.columns)\/len(leads_copy.columns)),2)))\nprint(\"Original Rows    Retained :{} % \".format(round((len(leads)*100)\/len(leads_copy),2)))\nprint(\"Original Data    Retained :{} % \".format(round((len(leads) * \n                                                     len(leads.columns))*100\/(len(leads_copy.columns)*len(leads_copy)),2)))\nprint ('The Shape of Data after EDA  has {0} rows and {1} columns'.format(leads.shape[0],leads.shape[1])) ","0587da86":"leads.head(10)","57770283":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# NUMERIC\nnum_data=leads[list(leads.select_dtypes(numerics).columns)]\n# CATEGORICAL \ncat_data=leads[list(leads.select_dtypes(exclude=numerics).columns)]\nprint('***************************************************')\nprint('---------------------------------------------------')\nprint('Dviding the Features as Numerical and Categorical ')\nprint('---------------------------------------------------')\n\nprint('Categorical Columns')\nprint('---------------------------------------------------')\n\nprint(cat_data.columns)\nprint('---------------------------------------------------')\n\nprint('Numerical Columns')\nprint('---------------------------------------------------')\n\nprint(num_data.columns)\nprint('---------------------------------------------------')\nprint('****************************************************')","70a607f0":"list_cat_var = []\nfor column in cat_data:\n    _ = leads[column].nunique()\n    if _ > 2:\n        list_cat_var.append(column)\n        print(\"{:<30} = {}\".format(column,_))","93b8d76d":"#Createing the function to count of values as per category in column\ndef column_category_counts(data):\n    return pd.DataFrame(data.value_counts(dropna=False))\n\nfor column in cat_data:\n    print(\"Column Name : \",column)\n    display(column_category_counts(leads[column]).T)","e155b316":"leads.drop(['Last Notable Activity'], 1, inplace = True)","8ecd5f27":"cat_data=leads[list(leads.select_dtypes(exclude=numerics).columns)]\nlist_cat_var = []\nfor column in cat_data:\n    _ = leads[column].nunique()\n    if _ > 2:\n        list_cat_var.append(column)\n        print(\"{:<30} = {}\".format(column,_))\n        \nmaster_leads = pd.get_dummies(leads,columns=list_cat_var,drop_first=True)","9f642f9f":"master_leads.head()\n","5502deaf":"print ('The Shape of Data after creating Dummies has {0} rows and {1} columns'.format(master_leads.shape[0],master_leads.shape[1])) ","1307f11d":"master_leads.info()","32173aff":"plt.figure(figsize = (50,40))\nsns.heatmap(master_leads.corr(),annot = True)\nplt.show()","4818a13c":"c_index = []\nc_columns = []\nc_value = []\nfor row in master_leads.columns:\n    for column in master_leads.columns:\n        if row != column:\n            _ = master_leads[row].corr(master_leads[column])\n            if _ > 0.5:\n                \n                if row < column:\n                    c_index.append(row)\n                    c_columns.append(column)\n                    c_value.append(_)\n                    \n                else:\n                    c_index.append(column)\n                    c_columns.append(row)\n                    c_value.append(_)\n                    ","02953665":"correlation_mat = pd.DataFrame([c_index,c_columns,c_value]).T.rename(columns={0:'Variable 1',1:'Variable 2',2:'Coefficient'})\ncorrelation_mat = correlation_mat[correlation_mat.duplicated(subset=['Variable 1'])]\ncorrelation_mat","a865cce0":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# NUMERIC\nnum_data_master_leads=master_leads[list(master_leads.select_dtypes(numerics).columns)]\n# CATEGORICAL \ncat_data_master_leads=master_leads[list(master_leads.select_dtypes(exclude=numerics).columns)]\n\nprint(num_data_master_leads.columns)\nprint(cat_data_master_leads.columns)","3217759a":"#Defining Y\nY = master_leads['Converted']\nY.head()","ebb3b593":"from sklearn.model_selection import train_test_split\nX = master_leads.drop(['Converted'], axis=1)\nX.head()","03fc9b09":"# Splitting the data into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=333)","6f02a23a":"X_train.head()","32df6050":"print ('The Shape of Data from X_train has {0} rows and {1} columns'.format(X_train.shape[0],X_train.shape[1])) ","ef83ed9c":"X_test.head()","4cfb3628":"print ('The Shape of Data from X_test has {0} rows and {1} columns'.format(X_test.shape[0],X_test.shape[1])) ","acf05d6f":"Y_train.head()","9365b4d8":"Y_train.shape","22e6abc0":"Y_test.head()","b527b3ab":"Y_test.shape","9f353dc3":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","6d96f0a5":"import statsmodels.api as sm","90d3f01b":"# Logistic regression model\nlogm1 = sm.GLM(Y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","df17eef1":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,18)             # running RFE with 18 variables as output\nrfe = rfe.fit(X_train, Y_train)\n","a52ce607":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","ac7d9678":"col = X_train.columns[rfe.support_]\ncol","19d93a17":"X_train.columns[~rfe.support_]","d0908c22":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres2 = logm2.fit()\nres2.summary()","e24f0b0b":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","f21bd8f9":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","6a9768b4":"col1 = col.drop('What is your current occupation_Housewife',1)","0872af23":"list(col1)","da030d0a":"X_train_sm = sm.add_constant(X_train[col1])\nlogm3 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres3 = logm3.fit()\nres3.summary()","ad353e03":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","4f0952c2":"col1 = col1.drop('Lead Source_Reference',1)\nlist(col1)","7f20fa17":"X_train_sm = sm.add_constant(X_train[col1])\nlogm4 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres4 = logm4.fit()\nres4.summary()","e5801b00":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","5050df2a":"col1 = col1.drop('Specialization_Hospitality Management',1)\nlist(col1)","d4444d7d":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres5 = logm5.fit()\nres5.summary()","60227cab":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","fa8abc7b":"col1 = col1.drop('What is your current occupation_Student',1)\nlist(col1)","777f0d2d":"X_train_sm = sm.add_constant(X_train[col1])\nlogm6 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres6 = logm6.fit()\nres6.summary()","146fb884":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","0d2ccf81":"col1 = col1.drop('What is your current occupation_Unemployed',1)\nlist(col1)","8febe413":"X_train_sm = sm.add_constant(X_train[col1])\nlogm7 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres7 = logm7.fit()\nres7.summary()","87585fd2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","bdb6fd31":"col1 = col1.drop('Last Activity_Unreachable',1)\nlist(col1)","bc91fdb6":"X_train_sm = sm.add_constant(X_train[col1])\nlogm8 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres8 = logm8.fit()\nres8.summary()","35d5d1fa":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","885935db":"col1 = col1.drop('Lead Source_Welingak Website',1)\nlist(col1)","9b7b5bd5":"X_train_sm = sm.add_constant(X_train[col1])\nlogm9 = sm.GLM(Y_train,X_train_sm, family = sm.families.Binomial())\nres9 = logm9.fit()\nres9.summary()","e02d0300":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nVIF = pd.DataFrame()\nVIF['Features'] = X_train[col1].columns\nVIF['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nVIF['VIF'] = round(VIF['VIF'], 2)\nVIF = VIF.sort_values(by = \"VIF\", ascending = False)\nVIF","4588154c":"round(res9.params.sort_values(ascending=False),4)","e8c28980":"# Getting the predicted values on the train set\nY_train_pred = res9.predict(X_train_sm)\nY_train_pred[:10]","b7482539":"Y_train_pred = Y_train_pred.values.reshape(-1)\nY_train_pred[:10]","b80fccea":"Y_train_pred_final = pd.DataFrame({'Converted':Y_train.values, 'Converted_prob':Y_train_pred})\nY_train_pred_final.head()","373c1f94":"Y_train_pred_final['predicted'] = Y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\nY_train_pred_final.head()","827f8d5e":"from sklearn import metrics\n\n# Confusion matrix \nConfusion_Mat = metrics.confusion_matrix(Y_train_pred_final.Converted, \n                                     Y_train_pred_final.predicted )\nprint(Confusion_Mat)","e6cc07ce":"print(metrics.accuracy_score(Y_train_pred_final.Converted, Y_train_pred_final.predicted))","65adc4ae":"#True positive \nTP = Confusion_Mat[1,1] \n\n#True negatives\nTN = Confusion_Mat[0,0] \n\n#False positives\nFP = Confusion_Mat[0,1] \n\n#False negatives\nFN = Confusion_Mat[1,0]","14c709eb":"print('Sensitivity of our logistic regression model :',TP \/ float(TP+FN))","a0d94075":"print('Specificity of our logistic regression model :',TN \/ float(TN+FP))","6786710c":"print('Calculation of False Positive Rate :',FP\/ float(TN+FP))\n","488787af":"print('Positive Predictive Value :',TP \/ float(TP+FP))","6203ad52":"print('Negative Predictive Value :',TN \/ float(TN+ FN))\n","a73651ba":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(9,6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","7ed1a88f":"fpr, tpr, thresholds = metrics.roc_curve( Y_train_pred_final.Converted, \n                                         Y_train_pred_final.Converted_prob, \n                                         drop_intermediate = False )","514f4d25":"draw_roc(Y_train_pred_final.Converted, Y_train_pred_final.Converted_prob)","7e840eb7":"numbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    Y_train_pred_final[i]= Y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\nY_train_pred_final.head()","7627f9c3":"cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = Confusion_Mat[1,1] # true positive \n# TN = Confusion_Mat[0,0] # true negatives\n# FP = Confusion_Mat[0,1] # false positives\n# FN = Confusion_Mat[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(Y_train_pred_final.Converted, Y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    Accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    Specificity = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    Sensitivity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]\nprint(cutoff_df)","77ec1b90":"cutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'],figsize=(15,10))\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","cd7c0b7a":"Y_train_pred_final['final_predicted'] = Y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.35 else 0)\n\nY_train_pred_final.head()","2f7cef29":"metrics.accuracy_score(Y_train_pred_final.Converted, Y_train_pred_final.final_predicted)","17f33162":"Confusion_Mat2 = metrics.confusion_matrix(Y_train_pred_final.Converted, Y_train_pred_final.final_predicted )\nConfusion_Mat2","353c8cc9":"#True positive \nTP = Confusion_Mat2[1,1] \n\n#True negatives\nTN = Confusion_Mat2[0,0] \n\n#False positives\nFP = Confusion_Mat2[0,1] \n\n#False negatives\nFN = Confusion_Mat2[1,0]","a6d2f7fe":"#Sensitivity\ntrainSensitivity=TP \/ float(TP+FN)\ntrainSensitivity","649b23bb":"#Specificity\ntrainSpecificity=TN \/ float(TN+FP)\ntrainSpecificity","06f673fb":"# Calculate false postive rate - predicting converted when lead has not converted\nprint(FP\/ float(TN+FP))","9548d637":"# Positive predictive value \nprint (TP \/ float(TP+FP))","a7e26729":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","2b8b5d3f":"Y_train_pred_final['Lead_Score_Train'] = Y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\nY_train_pred_final.head()","993a89c9":"from sklearn.metrics import precision_score, recall_score\nprecision_train= precision_score(Y_train_pred_final.Converted , Y_train_pred_final.predicted)\nprecision_train","49da62cb":"recall_train=recall_score(Y_train_pred_final.Converted, Y_train_pred_final.predicted)\nrecall_train","a30da00a":"trainF1_score_train= 2 * (precision_train * recall_train) \/ (precision_train + recall_train)\ntrainF1_score_train","699fa87b":"from sklearn.metrics import precision_recall_curve\n\np, r, thresholds = precision_recall_curve(Y_train_pred_final.Converted, Y_train_pred_final.Converted_prob)\n\nplt.figure(figsize=(16,10))\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","08eb6422":"trainaccuracy= metrics.accuracy_score(Y_train_pred_final.Converted, Y_train_pred_final.final_predicted)\ntrainaccuracy","099c83bc":"precision_train","6801efe1":"recall_train","e30a507a":"trainF1_score_train","86f3585b":"X_test[['TotalVisits','Total Time Spent on Website',\n        'Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',\n                                                                'Total Time Spent on Website','Page Views Per Visit']])\n\nX_test.head()","27e17e52":"X_test.columns","02b98d84":"X_test = X_test[col1]\nX_test.head()","c315c2f1":"X_test.shape","f6003977":"X_test_sm = sm.add_constant(X_test)","7b512582":"X_test_sm.head()","5e810e3b":"X_test_sm.shape","5d92540c":"Y_test_pred = res9.predict(X_test_sm)","6be74693":"Y_test_pred[:10]","e2e1ba49":"\nY_test_pred = pd.DataFrame(Y_test_pred)\nY_test_pred.head()","8e305fdb":"Y_test_df = pd.DataFrame(Y_test)","430c8fb3":"\nY_pred_final = pd.concat([Y_test_df, Y_test_pred],axis=1)\nY_pred_final.head()","fff1c5d4":"\nY_pred_final= Y_pred_final.rename(columns={ 0 : 'Converted_prob'})\nY_pred_final.head()","cbe8d3ed":"prob = []\npotential_leads = []\nfor i in np.arange(0.05,1,0.05):\n    prob.append(i)\n    potential_leads.append(sum(Y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)) + \n                           sum(Y_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)))\n    projected_leads = pd.DataFrame([prob,potential_leads]).T.rename(columns={0:'Probability Cut-Off',1:'Projected Leads'})\n    ","b9f70e47":"projected_leads","92341e31":"Y_pred_final['final_predicted'] = Y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.45 else 0)","8f7de1e3":"Y_pred_final['Lead_Score'] = Y_pred_final.Converted_prob.map( lambda x: round(x*100))\n\n","efc3bdce":"Y_pred_final.index = Y_pred_final.index.set_names(['Cust_Id'])\nY_pred_final.head()","e577a29f":"testaccuracy= metrics.accuracy_score(Y_pred_final.Converted, Y_pred_final.final_predicted)\ntestaccuracy","732ffa8d":"confusion2 = metrics.confusion_matrix(Y_pred_final.Converted, Y_pred_final.final_predicted )\nconfusion2","7469f602":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","5d66aded":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","e24a54dd":"precision= precision_score(Y_pred_final.Converted , Y_pred_final.final_predicted)\nprecision","b3718473":"recall=recall_score(Y_pred_final.Converted , Y_pred_final.final_predicted)\nrecall","9dbcda36":"testF1_score= 2 * (precision * recall) \/ (precision + recall)\ntestF1_score","5f796c87":"\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainSensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainSpecificity*100),2)))\nprint(\"Train Data F1 Score    :{}  \".format(round((trainF1_score_train),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))\nprint(\"Test Data F1 Score     :{}  \".format(round((testF1_score),2)))","26770221":"from sklearn.metrics import classification_report","f2909cce":"print (classification_report(Y_train_pred_final['Converted'], Y_train_pred_final['final_predicted']))","f33d7198":"pd.options.display.float_format = '{:.2f}'.format\nnew_params= round(res9.params.sort_values(ascending=False),2)\nnew_params","f058f63d":"#Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nfeature_importance.sort_values(ascending=False)","5816376b":"#Sorting the feature variables based on their relative coefficient values\nsorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\nsorted_idx","a9b00e9c":"#Plot showing the feature variables based on their relative coefficient values\n#Plotting the scree plot\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure(figsize=(10,6))\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center', color = 'tab:red',alpha=0.8)\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X_train[col].columns)[sorted_idx], fontsize=12)\nfeatax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","495e0ed4":"# Model Building ","81e8bbc1":"### Defining Y ","8eabed89":"##### As per Precision-Recall Tradeoff, the cutoff is around 0.425 (between 0.4 and 0.45) . We can chooose the cut-off as 0.45| and use the Precision-Recall-Accuracy metrics to evaluate the model.","3616de20":"### Create a list of categorical variables with more than 2 categories for dummy variable creation","4f8b323f":"### Inference :  From the curve above, 0.35 is the optimum point to take it as a cutoff probability.","f6313485":"### Remove columns which has only one unique value\n","e069e011":"### Precision ","2ab76bfc":"### Check for unique values in all features ","c1bde949":"## Precision and recall tradeoff","bad32aa8":"### Drop the remaining null values less than 1.5%","31ad8361":"# Lead Scoring Case Study","2979d67b":"### Logistic Regression Model 4","6efe5297":"## Variance Inflation Factor -VIF ","f472d0c7":"### Appending y_test_df and Y_test_pred","52e2e37d":"## Overall Accuracy - Train ","002682f8":"# Data Preperation ","5518a4f9":"## Reading the Input Data","fbb8538f":"### Country","6e633f83":"### Defining X ","204db63d":"### Handling missing values","c334e37a":"## Correlation matrix  to see multi collinearity in the master dataframe ","cd3072a3":"#  Data Cleaning ","91d2761e":"## F1 Score ","4bc22985":"## Assigning Lead Score","2221af5d":"## Importing Data Dictionary ","14c2020a":"### Inference: \n\n - Total Visits and Page views per Visit has high correlation than other features. \n - Total visits and converted has very low correlatio result which means that based on total visit we can derive meaningful lead scoring \n - Total visits and Total Time spent on Website have a reasonable correlation result\n - There is positive correlation between Total Time Spent on Website and Conversion\n - There is almost no correlation in Page Views Per Visit and TotalVisits with Conversion\n    ","eb1460fc":"### Inferences: Univariate Analysis Categorical Features \n\n#### Distribution Of Lead Origin \n\n   - Landing page submission is comparitvely high then the rest of the categories in lead origin \n   - lead import is the least category which is quantified in lead origin\n   - Landing page submission and APO helps in lead conversion than the rest of the categories \n   - Lead add form has high certainity in lead conversion \n#### Distribution of Lead Source \n\n   - Google is is the best lead source among all other categories in the lead source\n   - Direct Traffice, Olark Chat and Organic Search are some of the best entities in lead source \n   - The best category for lead conversion is Reference and Welingak Website among all the lead source categories \n   - To improve overall lead conversion rate, focus should be on improving lead converion of olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.\n   \n#### Distribution of Do Not Email \n\n   - The customers who opted out of email communication is high\n   - lead conversion through email has less certainity unlike other categories \n   \n#### Distribution of Do Not Call \n\n   - The customers who opted out of call communication is high\n   - lead conversion through call has less certainity unlike other categories\n   \n#### Distribution of Last Activity \n\n   - Most of the lead have their Email opened as their last activity.\n   - Conversion rate for leads with last activity as SMS Sent is almost 62%.\n   \n   \n#### Distribution of Specialziation \n\n   - Focus should be more on the Specialization with high conversion rate like Supply Chain , Human Resource and Finance\n   \n\n#### Distribution of Occupation \n\n- Working Professionals going for the course have high chances of joining it.\n   - Unemployed leads are the most in numbers but has around 30-35% conversion rate.\n   \n   \n#### Distribution of Search \n\n   - Most entries are 'No'. No Inference can be drawn with this parameter.\n   \n#### Similar to Distribution of Search , No Inference can be drawn from the following features \n\n   - Newspaper Article', 'X Education Forums', 'Newspaper','Digital Advertisement', 'Through Recommendations'\n   \n####  Distribution Of City \n\n  - Most leads are from mumbai with around 30% conversion rate.\n   \n#### Distribution of Last notable Activity \n\n   - SMS Sent' is strong entity for positive lead\n   \n \n  ### Results from Univariate Analysis for Categorical Features : \n \n Based on the univariate analysis we have seen that many columns are not adding any information to the model, hence we can drop them for further analysis\n  ","a2802da3":"### Variance Inflation Factor - VIF ","3ea88736":"### Sample Data of X_test Dataframe","01f8cb68":"### Correlation Plot to check Multi- Collinearity ","0b13fba1":"####  Inference - We witness the model 9 and there is no high P values at all and also the VIF is less then 5. So we keep this as the final model for evaluation ","da74b2a7":"## Specificity","7bf4275d":"### Plotting  accuracy sensitivity and specificity for various probabilities.","4fe07f7c":"### Renaming the column ","ded05daf":"### Duplicates Check ","6ca89cfe":"#### Creating  Dataframe with the actual Converted flag and the predicted probabilities","61dc6626":"### Logistic Regression Model 3","811623ae":"Those features which have only one unique value are :\n- Magazine\n- Recieve More updates about the course\n- Update me on Supply chain content\n- Get updates on DM content\n- I agree to pay the amount through cheque \n\nThese features show no variance and thus all the leads have chosen one option, thus this feature doesnt make any impact or difference on conversion of leads.","90252337":"## Metrics - Train","fed7ba99":"# Recommendations\n### X Education Company needs to focus on following key aspects to improve the overall conversion rate:\n -  Increase user engagement on Welingak website since this helps in higher conversion\n -  Focus on Working Professional which has high conversion certainity.\n -  Get Total Time Spent on Website increased by advertising and user experience which makes the customer engaging in the website. since this helps in higher conversion\n -  Improve the Olark Chat service since this is affecting the conversion negatively\n -  Improving Lead add form also improves the lead conversion with high certainity \n","16d66281":"### Specificity of our logistic regression model Train","1014b006":"## Coefficients ","5624ea77":"### Variance Inflation Factor - VIF ","9bf32306":"### Importing libraries","a4481afb":"### Sample Data of Y_train Dataframe ","460f43f6":"## Model Assesment ","ad3c8074":"### Data Retained for EDA","db5045c5":"### Shape of Data after Data Cleaning","1d7d3dcf":"## Overview of the Dataframe ","1bdb1d43":"## Overall accuracy Train After Cut off ","ef332e0e":"### Specialization","eed872a8":"## Confusion Matrix ","10de145b":"#### Inference: There is no Duplicates ","f68c772b":"## Recall","d6888d83":"### Converting y_pred to a dataframe which is an array","1ca90bdf":"### Inference:\n\n- Since the feature  What matters most to you in choosing a course is highly skewed to Better Career Prospects we are dropping the feature ","ac906fbc":"## Precesion and recall ","206c0761":"### Variance Inflation Factor - VIF ","4bab9657":"## Precesion ","ea0bdd5a":"### columns in Test Data ","a66b4058":"### Variance Inflation Factor - VIF ","df298343":"####  Inference: Lead Source_Welingak Website has high P values though its VIF is low. We drop them considering the P values ","55398c5e":"### Check for the Shape of the Test Data","a302b627":"### Variance Inflation Factor - VIF ","33bd8d6a":"### Current conversion Rate:\n\nConverted is the target variable, Indicates whether a lead has been successfully converted (1) or not (0).","6d2aa2a6":"## Plotting the ROC Curve\n###### An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","b5066a34":"### Handling the remaining null values with ref to the above output  ","1ba8eddb":"### Shape of Data after Univariate EDA - Categorical Columns ","be0c960c":"### Lead scores for varying cut-off probability ","858960b1":"## Train Data - Statistics","7c1fb5b2":"#### Inference : What is your current occupation_Unemployed has high VIF and high P value ","e09d6c53":"### Train Negative Predictive Value ","ccbaf7a1":"### Logistics Regression Model 1 ","d3d27361":"#### Inferences : \n\nThe Data is skewed and we could witness a lot of noice in the data.\n","c19ecdd5":"### Adding Constant ","3cbd8f70":"## Descriptive Statistivs ","5fd0b6de":"## Problem Statement\nAn education company named __X Education__ sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. <br>\n\n__When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.__<br>\n\nOnce these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. __The typical lead conversion rate at X education is around 30%.__\n \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as __\u2018Hot Leads\u2019__. <br>\n\nIf they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:\n![image.png](attachment:image.png)\n\n\n\n__Lead Conversion Process__ - Demonstrated as a funnel\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom.<br>\n\nIn the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion. \n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. <br>\nThe company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\n\n__The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.__\n\n\n \n\n### Data\n\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column \u2018Converted\u2019 which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn\u2019t converted.\n\nAnother thing that you also need to check out for are the levels present in the categorical variables.<br>\n\n__Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value.__\n\n \n\n### Goal\n\n\n1. Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.","9680e88f":"### Scaling Features in the Test Data ","c320495e":"### Features\/Columns with precentage of null values ","8c9f38ee":"### City","98702e09":"### Drop the following features - These variables are created after contacting the Student(Customer)","96e0ea0c":"### Taking RFE support Cols from the last final model in train","b9934824":"### Sample Data of X_train Dataframe ","34906403":"### Values under 'Last Activity' are coverved as values under 'Last Notable Activity'. Either of one can be dropped. ","d747dabc":"### Calculation of accuracy sensitivity and specificity for various probability cutoffs.","28878d87":"## Test Accuracy ","06595177":"### Dropping all columns with only 0 values:","24b18585":"## Feature Selection Using RFE","c895273b":"### Confusion Matrix ","af36c0b4":"### Dropping Unecessary Columns","e0e94091":"### Visualisng the Null values ","e509228c":"## Making predictions on the test set","b3269f15":"### Inference:\n   - Country is India for most values so it is skewed towards India. Hence we drop the same. ","f4676ef7":"# Univariate Analysis - Categorical Columns ","17c3d337":"### Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0  ","78275d5c":"### Logistic Regression Model 8","675d22aa":"### Check percentage of the columns again ","927fb3c2":"### Data Statistics after EDA ","6e49a7db":"### F1 Score ","504b048e":"####  Inference: Specialization_Hospitality Management has high P value ","dc6b9584":"## Bi-Variate Analysis ","7c4f96da":"### Variance Inflation Factor - VIF ","f10b734f":"## Basic Information of the Features","7130ed0e":"## Finding Optimal Cutoff Point\n\n#### Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","dd1d1be3":"### Logistic Regression Model 6","4344d7c7":"### Converting y_test to dataframe","ee8ca561":"### Check percentage of the columns again ","5135e636":"### What matters most to you in choosing a course","f0cff7fd":"### Calculation of  false postive rate Train - predicting converted when lead has not converted","b264bd87":"## Check for Sample data from the Dataframe ","7004b14a":"### Inference : \n   - We have around 4 variables with very less null values. So, we can drop the null values simply ","579f9914":"### Sample Data of Y_test Dataframe ","59274e2e":"### Logistics Regression Model 2 ","d41b1b27":"### Inference: \n - It maybe a case that leads had not entered any specialization if option is not availabe on the list,may not have any specialization or is a student.Hence we can make a category \"Others\" ","48f7931b":"### Inferences: \n\n#### Total Visits:\n\n  - Median for converted and not converted leads are the same.\n  - Nothng conclusive on the basis of Total Visits.\n  \n#### Total Time Spent on Website \n\n  - Leads spending more time on the weblise are more likely to be converted.\n  - Website should be made more engaging to make leads spend more time.\n  \n#### Page Views Per Visit\n\n  - Nothing can be said specifically for lead conversion from Page Views Per Visit","6b792f91":"# Exploratory Data Analysis ","01e86e75":"### Checking for count of categories in categorical variable","0220309a":"## Classification Report ","9745329a":"### Divide the data into Numeric and categorical data  ","d9862898":"### Inference: \n\n- More than 50% of the data available  is Mumbai so we can impute Mumbai in the missing values.","01396b6d":"# Final Observation:","65bef72b":"### Sensitivity of our logistic regression model Train","e49fd3bd":"### Logistic Regression Model - 9","71f2fc51":"### Features\/Columns with precentage of null values after dropping values > 45%","6d4307bf":"We can clearly spot outliers in the features such as TotalVisits and Page Views Per Visit. Total time spent on Website doesn't have any outliers. We can see that there are as many as 250 visits recorded for total visits by possible leads. As high as this number of visits to a website seems to be not like a correct capture and hence we can remove these outliers. Similarly for the page views per visit, as many as 20+ page views in a single visit seems to be not correct. We can remove these as well.!","fead7319":"### Dropping the columns 'Prospect ID' as it will not have any effect in the predicting model","8abd8967":"## Test Data Statistics ","554cfd48":"## Selecting the coefficients of the selected features from our final model excluding the intercept ","f07962dc":"### What is your current occupation","ca7d6d4f":"### Logistic Regression Model 5","50d18879":"### To check for high correlation we find variables with correlation greater than 0.5","d8750dec":"#### Inference: Last Activity_Unreachable has high P values though its VIF is low. We drop them considering the high P values ","888427cd":"#### Inference: Last Activity_Unreachable has high P values though its VIF is low. We drop them considering the high P values ","59efed1f":"### Predict Y_TEST ","8fc57ce4":"## Shape of the Input","b6be94a9":"- Lead scoring is the process of assigning values, often in the form of numerical \"points,\" to each lead you generate for the business. \n- You can score your leads based on multiple attributes, including the professional information they've submitted to you and how they've engaged with your website and brand across the internet. \n- This process helps sales and marketing teams prioritize leads, respond to them appropriately, and increase the rate at which those leads become customers.","e429103e":"### Outlier Treatment  ","397390d8":"## Train Test Data Split ","af1c93a7":"### Supress Warnings","a5da3b3b":"#### Basic information of Features after Univariate EDA - Categorical Columns  ","3d90ee94":"### Inference : \n   - Now, we have handled all the missing values in the data frame . Now, we will split the columns as categorical and numerical columns ","96bc1ab4":"### Replacing 'Select' values to NAN in the Dataframe ","eef670ca":"## Sensitivity","d6f48390":"### Comparison of the values obtained for Train & Test:","3e81c29b":"####  Inference: What is your current occupation_Housewife having high P value though its VIF is low. So, Considering P value we are dropping the feature ","5ee2a329":"## What Is Lead Scoring?","aac2f6c0":"### Drop columns having more than 45% NA values.","a716941f":"### Createing master dataframe with dummy variable","0c77a0a1":"#### Inference: Lead Source_Reference have High VIF values and the P value is also high . So we drop them\n","30c94c31":"### Feature Scaling ","ba3cb644":"### Train Positive Predictive value ","435a0276":"## Other Activity \n\n- 'Had a Phone Conversation', \n  'View in browser link Clicked', \n  'Visited Booth in Tradeshow',\n  'Approached upfront',\n  'Resubscribed to emails',\n  'Email Received', \n  'Email Marked Spam']","00f642a2":"### Variance Inflation Factor - VIF ","62142c37":"# Univariate Analysis - Numerical Columns ","dab9847c":"## Final Data after Prediction ","d5961e83":"### Logistic Regression Model 7","18294740":"### Recall","5cd59ec0":"### Check for columns with only null vlaues:","8360b0ff":"### Accuracy  "}}