{"cell_type":{"d3e6175f":"code","9f6bf235":"code","2841f92d":"code","1240074c":"code","c9ef97a5":"code","7c80dfa0":"code","7710ef19":"code","a4e7e830":"code","66683bfc":"code","94263fa5":"code","38a2b6e7":"code","70f2716e":"code","0a21a228":"code","3834c879":"code","001e3e57":"code","afb8f8ce":"code","16a58644":"code","1274b12a":"code","2cb695da":"code","f6cb2208":"code","851946bc":"code","bf0a62dc":"code","6ba88d46":"code","cbda355c":"code","c50b28b2":"code","320fe9e6":"code","977202a8":"code","2848a59a":"code","ed11f942":"code","222ce938":"code","dc4885ed":"code","f21bb01a":"code","a570d142":"code","b2ac8199":"code","b2802e63":"code","81e23f9f":"code","7b5d308d":"code","3731d393":"code","9dfab1f0":"code","4ea39470":"code","34536721":"code","3cbfa7ef":"code","87620ca2":"code","79da8dbd":"code","de5510f6":"code","f7cd3916":"code","91af5e03":"code","96c19884":"code","3979d839":"code","34900fa5":"code","00f416ad":"code","5b03d5e0":"code","8b160898":"code","e26aa644":"code","710404a5":"code","f8c75b8a":"code","8a37724a":"code","eb8d73af":"code","7a4d14a3":"code","849f2aad":"code","18af9425":"markdown","8b5fba9d":"markdown","46851aad":"markdown","f9e7fcab":"markdown","44b2b5fe":"markdown","cffdb8c3":"markdown","478d2663":"markdown","e9846c0f":"markdown","023b78b0":"markdown","02009160":"markdown","f8f53bf5":"markdown","5ab462b1":"markdown","8672f7f6":"markdown","aabd7d98":"markdown","78491282":"markdown","7142838d":"markdown","9715f973":"markdown","6c07321e":"markdown","908599c1":"markdown","97daa032":"markdown","e2347532":"markdown","acabe63e":"markdown","628fad63":"markdown","cd8f7c5a":"markdown","2815dbef":"markdown","22221bdd":"markdown","a739f3f6":"markdown","9191a422":"markdown","2667ae7a":"markdown","6c6694a6":"markdown"},"source":{"d3e6175f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport sklearn.preprocessing\nimport sklearn.pipeline\nfrom scipy.stats import skew, kurtosis, iqr\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, PowerTransformer,StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.eval_measures import rmse, meanabs\nfrom statsmodels.graphics.gofplots import qqplot\n\n\n# To save and load model\nimport pickle\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# to view all columns\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n#Load data\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ndf=pd.concat([train, test], keys=['train', 'test'])","9f6bf235":"# Separate Categorical Columns\ndf['AgeBlt']=(df['YrSold']-df['YearBuilt']).abs()\ndf['AgeRemdl']=(df['YrSold']-df['YearRemodAdd']).abs()\n\nCatCols=[i for i in df.columns if df.loc[:,i].dtype == 'object']\nNumCols=list(set(df.columns)-set(CatCols))\nNumCols.remove('Id')\nNumCols.remove('SalePrice')\n\nX1=df.loc['train']","2841f92d":"df[CatCols]=df[CatCols].fillna('None')","1240074c":"NumMiss=[i for i in NumCols if df.loc[:,i].isna().sum()>0]\nNumMiss","c9ef97a5":"# X1.LotFrontage is skewed so we fill missing values with median\nprint('skewness of LotFrontage = {:.2f}'.format( X1.LotFrontage.skew()))\nax=sns.distplot(X1['LotFrontage'])\nax.set_title(f'Distribution of LotFrontage; skewness = {X1.LotFrontage.skew():.2f}');","7c80dfa0":"# fill missing values with median\ndf['LotFrontage']=df['LotFrontage'].fillna(X1.LotFrontage.median())","7710ef19":"ax=sns.countplot( x='GarageType', data=X1.loc[X1['GarageYrBlt'].isna(), :])\nax.set_title('GarageYrBlt by GarageType for missing values');","a4e7e830":"df['GarageYrBlt']=df['GarageYrBlt'].fillna(0)","66683bfc":"\nax=sns.countplot( x='MasVnrType', data=X1.loc[X1['MasVnrArea'].isna(), :])\nax.set_title('MasVnrArea by MasVnrType for missing values');","94263fa5":"df['MasVnrArea']=df['MasVnrArea'].fillna(0)","38a2b6e7":"NumMiss2=[i for i in NumCols if df.loc[:,i].isna().sum()>0]\nNumMiss2\ndf[NumMiss2]=df[NumMiss2].fillna(0)","70f2716e":"df['Log_SalePrice']=np.log(df['SalePrice'])\nX1['Log_SalePrice']=np.log(X1['SalePrice'])\nfig, axes=plt.subplots(ncols=2, figsize=(13,5))\nsns.distplot(X1['SalePrice'], ax=axes[0])\nsns.distplot(df.loc['train','Log_SalePrice'], ax=axes[1])\naxes[0].set_title(f'Distribution of SalePrice; skewness = {X1.SalePrice.skew():.2f}')\naxes[1].set_title(f'Distribution of SalePrice; skewness = {X1.Log_SalePrice.skew():.2f}')\nfig.tight_layout();","0a21a228":"corr_target=X1[NumCols].apply(lambda x: x.corr(X1['SalePrice'])).abs().sort_values(ascending=False)\nhigh_corrs = corr_target[(corr_target > 0.5) & (corr_target < 1)]\nhigh_corrs.reset_index()","3834c879":"NumPred=list(high_corrs.reset_index().iloc[:,0])\n# Heat Map of Correlations\ncorr_matrix = X1.loc[:, NumPred].corr()\nplt.figure(figsize=(15,7))\nsns.heatmap(corr_matrix, cmap='PiYG', annot=True, square=False)\nplt.title('Correlations between selected features that are highly correlated with target');","001e3e57":"# Remove highly correlated features\n# NumPred is already sorted descending\ncorr_NumPred =[]\nfor i in range(len(NumPred)-1):\n    for j in range(i+1,len(NumPred)):\n        if (np.absolute(X1[NumPred[i]].corr(X1[NumPred[j]])) > 0.6): \n            corr_NumPred.append(NumPred[j])\nNewNumPred=list(set(NumPred)-set(corr_NumPred))\n\n# Heat Map of Correlations\ncorr_matrix = X1.loc[:, NewNumPred].corr()\nplt.figure(figsize=(15,7))\nsns.heatmap(corr_matrix, cmap='YlGn', annot=True, square=False)\nplt.title('Correlations between selected features after removing highly correlated features');","afb8f8ce":"fig, ax=plt.subplots(ncols=len(NewNumPred), figsize=(15, 5))\nfig.suptitle('Boxplot of first selection of numerical predictors', y=1.1, fontsize=14)\nfor i, ax in enumerate(fig.axes):\n    if i < len(NewNumPred):\n        sns.boxplot(y=NewNumPred[i], data=X1, ax=ax)\nfig.tight_layout();","16a58644":"fig, ax=plt.subplots(nrows=int(len(NumCols)\/5)+1,ncols=5, figsize=(30, 25))\nfig.suptitle('Boxplot of all numerical columns',y=1.01, fontsize=16)\nfor i, ax in enumerate(fig.axes):\n    if i < len(NumCols):\n        sns.boxplot(y=NumCols[i], data=X1, ax=ax)\nfig.tight_layout();","1274b12a":"# Heat Map of Correlations\ncorr_matrix = X1.loc[:, NumCols].corr()\nplt.figure(figsize=(30,30))\nplt.titte='Correlation of all the numerical features'\nsns.heatmap(corr_matrix[corr_matrix.abs()>0.65], cmap='YlGn', annot=True, square=False)\nplt.title('Correlation of all the numerical features for the last selection of numerical predictors');","2cb695da":"# Choosing numerical predictors with meaningful distribution\nNumPreds=['TotalBsmtSF','OverallQual','OverallCond','GrLivArea','GarageCars' ,'AgeBlt', \n          'MasVnrArea', 'BedroomAbvGr', 'Fireplaces','AgeRemdl','BsmtFinSF1']","f6cb2208":"# Heat Map of Correlations\ncorr_matrix = X1.loc[:, NumPreds].corr()\nplt.figure(figsize=(15,7))\nsns.heatmap(corr_matrix, cmap='coolwarm', annot=True, square=False)\nplt.title('Final numerical predictors');","851946bc":"fig, axes = plt.subplots(nrows=int(len(CatCols) \/ 3)+1, ncols=3, figsize=(30, 50))\nfig.suptitle('Distribution of all categorical columns',y=1.01, fontsize=16)\nfor i, ax in enumerate(fig.axes):\n    if i < len(CatCols):\n        sns.countplot(x=CatCols[i], data=X1, ax=ax)\n        ax.set_yscale('log')\n        ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nfig.tight_layout();","bf0a62dc":"fig, axes = plt.subplots(nrows=int(len(CatCols) \/ 3)+1, ncols=3, figsize=(30, 50))\nfig.suptitle('Impact of categorical columns on SalePrice',y=1.01, fontsize=16)\nfor i, ax in enumerate(fig.axes):\n    if i < len(CatCols):\n        sns.boxplot(x=CatCols[i], y='SalePrice', data=X1, ax=ax)\n        ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nfig.tight_layout();","6ba88d46":"CatPreds=['Neighborhood','MSSubClass','MSZoning','ExterQual','MasVnrType','BsmtExposure',\n          'BsmtFinType1','SaleType','SaleCondition',  'KitchenQual', 'Functional', 'LandSlope', 'RoofStyle',\n          'HeatingQC','GarageType']","cbda355c":"Preds=list(set(NumPreds)| set(CatPreds))\n\nX0=df.loc['train',Preds]\ny=df.loc['train','SalePrice']\n\nX0_eval=df.loc['test',['Id']+Preds]\n\n#dummy variables\nX = pd.get_dummies(X0, drop_first=True).reset_index(drop=True)\nX_eval = pd.get_dummies(X0_eval[Preds], drop_first=True).reset_index(drop=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42,test_size=0.25)\nCV=KFold(n_splits=5, random_state=42, shuffle=True)\n\n#Scale\nsc=RobustScaler().fit(X_train)\nXs_train=sc.transform(X_train)\nXs_test=sc.transform(X_test)\n\nX_train.shape, X_test.shape","c50b28b2":"lr_model=LinearRegression().fit(Xs_train,y_train)\n\n# save the model to disk\nfilename = '..\/lr_model.sav'\npickle.dump(lr_model, open(filename, 'wb'))\n\nlr_pred=lr_model.predict(Xs_test)","320fe9e6":"r2_score(y_test,lr_pred), rmse(y_test,lr_pred), meanabs(y_test,lr_pred)","977202a8":"qqplot(y_test-lr_pred, fit = True, line = \"45\");","2848a59a":"param_grid = {'alpha': np.linspace(0.0001, 50, 50) } \n\nLSO_grid = GridSearchCV(Lasso(), param_grid, cv= CV, scoring='neg_mean_squared_error')\n\nLSO_grid.fit(Xs_train, y_train)\n\nprint('LASSO Best Estimator = {}'.format(LSO_grid.best_params_))\n\nlso_model=LSO_grid.best_estimator_\n# save the model to disk\nfilename = '..\/lasso_model.sav'\npickle.dump(lso_model, open(filename, 'wb'))\n\nlso_pred=lso_model.predict(Xs_test)","ed11f942":"r2_score(y_test,lso_pred), rmse(y_test,lso_pred), meanabs(y_test,lso_pred)","222ce938":"qqplot(y_test-lso_pred, fit = True, line = \"45\");","dc4885ed":"param_grid = {'alpha': np.linspace(0.001, 30,60) } \n\nRDG_grid = GridSearchCV(Ridge(), param_grid, cv=CV, scoring='neg_mean_squared_error')\n\nRDG_grid.fit(Xs_train, y_train)\n\nprint('Ridge Best Estimator = {}'.format(RDG_grid.best_params_))\n\nrdg_model=RDG_grid.best_estimator_\n# save the model to disk\nfilename = '..\/rdg_model.sav'\npickle.dump(rdg_model, open(filename, 'wb'))\n\nrdg_pred=rdg_model.predict(Xs_test)","f21bb01a":"r2_score(y_test,rdg_pred), rmse(y_test,rdg_pred), meanabs(y_test,rdg_pred)","a570d142":"qqplot(y_test-rdg_pred, fit = True, line = \"45\");","b2ac8199":"# param_grid = {'l1_ratio': np.linspace(0.001,50,50),\n#                'alpha': np.linspace(0.001, 20,30) }\n\n# ELS_grid = GridSearchCV(ElasticNet(), param_grid, cv=CV, scoring='neg_mean_squared_error')\n\n# ELS_grid.fit(Xs_train, y_train)\n\n# print('ElasticNet Best Estimator = {}'.format(ELS_grid.best_params_))\n\n# els_model=ELS_grid.best_estimator_\nels_model=ElasticNet(alpha=0.001, l1_ratio=0.001).fit(Xs_train, y_train)\n# save the model to disk\nfilename = '..\/els_model.sav'\npickle.dump(els_model, open(filename, 'wb'))\n\n\nels_pred=els_model.predict(Xs_test)","b2802e63":"r2_score(y_test,els_pred), rmse(y_test,els_pred), meanabs(y_test,els_pred)","81e23f9f":"qqplot(y_test-els_pred, fit = True, line = \"45\");","7b5d308d":"def PolynomialRidge(degree=2, **kwargs):\n    return make_pipeline(\n        PolynomialFeatures(include_bias=False),\n        RobustScaler(), # scale higher degrees of the features\n        Ridge())\n\n# param_grid = {'polynomialfeatures__degree': [1,2],\n#               'ridge__alpha': np.linspace(0.0001, 30,5)}\n\n# PolyRDG_grid = GridSearchCV(PolynomialRidge(), param_grid, cv=CV, scoring='neg_mean_squared_error')\n\n# PolyRDG_grid.fit(Xs_train, y_train)\n\n# print('PolynomialRidge Best Estimator = {}'.format(PolyRDG_grid.best_params_))\n\n# PolyRDG_model=PolyRDG_grid.best_estimator_\n\nPolyRDG_model=PolynomialRidge(degree=2, alpha=30).fit(Xs_train, y_train)\n\n#save the model to disk\nfilename = '..\/PolyRDG_model.sav'\n\npickle.dump(PolyRDG_model, open(filename, 'wb'))\n\nPolyRDG_pred=PolyRDG_model.predict(Xs_test)","3731d393":"from statsmodels.tools.eval_measures import rmse, meanabs\n\nr2_score(y_test,PolyRDG_pred), rmse(y_test,PolyRDG_pred), meanabs(y_test,PolyRDG_pred)","9dfab1f0":"def lm(formula, data): \n    reg = smf.ols(formula = formula, data = data)\n    return reg.fit()\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(df.loc['train'], y,random_state=42,test_size=0.2)\n\nstat_model=lm('SalePrice ~ C(OverallQual)*TotalBsmtSF*C(Neighborhood)+C(MSSubClass)+AgeBlt+AgeRemdl+GrLivArea+\\\n              C(MSZoning)+C(ExterQual)+C(MasVnrType)*MasVnrArea+\\\n              C(BsmtExposure)*C(BsmtFinType1)*BsmtFinSF1+KitchenAbvGr*C(KitchenQual)+C(Functional)+\\\n              GarageCars+C(SaleType)+C(SaleCondition)+BedroomAbvGr+Fireplaces',\n            X1_train)\n\nstat_model.summary()","4ea39470":"# save the model to disk\nfilename = '..\/stat_model.sav'\npickle.dump(stat_model, open(filename, 'wb'))\ny2_pred=stat_model.predict(X1_train)\nprint( 'r2 on train: {}, rsme on train: {}'.format(r2_score(y1_train,y2_pred), rmse(y1_train,y2_pred)))\nprint('**** Too good to be true! ****')\ny1_pred=stat_model.predict(X1_test)\nprint( 'r2 on test: {}, rsme on test: {}'.format(r2_score(y1_test,y1_pred), rmse(y1_test,y1_pred)))\nprint('OVERFITTING !!!!!!')","34536721":"r2_score(y1_test,y1_pred), rmse(y1_test,y1_pred), meanabs(y1_test,y1_pred)","3cbfa7ef":"qqplot(y1_test-y1_pred, fit = True, line = \"45\");","87620ca2":"from sklearn.svm import SVR\n\nparam_grid = {'kernel': ['linear', 'rbf'],\n              'C':[1000,500, 100, 10, 1, 0.1],\n              'epsilon':[ 0.1, 0.5,1,10]\n             }\n\nSVR_grid = GridSearchCV(SVR(), param_grid, cv=CV, scoring='neg_mean_squared_error')\n\nSVR_grid.fit(Xs_train, y_train)\n\nprint('SVR Best Estimator = {}'.format(SVR_grid.best_params_))\n\nsvr_model=SVR_grid.best_estimator_\n# save the model to disk\nfilename = '..\/svr_model.sav'\npickle.dump(svr_model, open(filename, 'wb'))\n\nsvr_pred=svr_model.predict(Xs_test)","79da8dbd":"r2_score(y_test,svr_pred), rmse(y_test,svr_pred), meanabs(y_test,svr_pred)","de5510f6":"qqplot(y_test-svr_pred, fit = True, line = \"45\");","f7cd3916":"from xgboost import XGBRegressor\n\n# param_grid = {'n_estimators': [100, 300, 500],\n#               'gamma': [0.000001],\n#               'max_depth': [4,6,8],\n#               'reg_lambda':[0.1,1, 10], #L2\n#               'reg_alpha':[0.001,0.1,1, 10],  #L1\n#               'learning_rate':[0.001, 0.01, 0.1, 1]\n#              }\n\n# XGB_grid = GridSearchCV(XGBRegressor(random_state=42), param_grid, cv=CV, scoring='neg_mean_squared_error')\n\n# XGB_grid.fit(X_train, y_train)\n\n# print('XGB Best Estimator = {}'.format(XGB_grid.best_params_))\n\n# xgb_model=XGB_grid.best_estimator_\nxgb_model = XGBRegressor(gamma=1e-06, learning_rate=0.1, max_depth=4, n_estimators=100, \n                          reg_alpha=10, reg_lambda=1, random_state=42).fit(Xs_train, y_train)\n# save the model to disk\nfilename = '..\/xgb_model.sav'\npickle.dump(xgb_model, open(filename, 'wb'))\n\nxgb_pred=xgb_model.predict(Xs_test)","91af5e03":"r2_score(y_test,xgb_pred), rmse(y_test,xgb_pred), meanabs(y_test,xgb_pred)","96c19884":"qqplot(y_test-xgb_pred, fit = True, line = \"45\");","3979d839":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n\n#create model\ndp_model = Sequential()\n\n#scale data\nscaler=MinMaxScaler().fit(X_train)\nXsc_train=scaler.transform(X_train)\nXsc_test=scaler.transform(X_test)\n\n#get number of columns in training data\nn_cols = Xsc_train.shape[1]\n\n#add model layers\ndp_model.add(Dense(300, activation='relu', input_shape=(n_cols,)))\ndp_model.add(Dense(300, activation='relu'))\ndp_model.add(Dense(300, activation='relu'))\ndp_model.add(Dense(300, activation='relu'))\ndp_model.add(Dense(300, activation='relu'))\n\ndp_model.add(Dense(1))\n\n#compile model using mse as a measure of model performance\nopt=tf.keras.optimizers.Adadelta(learning_rate=0.1, rho=0.01, epsilon=1e-07)\n\ndp_model.compile(optimizer=opt, loss='mean_squared_error')\n\n# #set early stopping monitor so the model stops training when it won't improve anymore\nearly_stopping_monitor = EarlyStopping(min_delta=0.01,patience=50)\n\n#train model\ndp_model.fit(Xsc_train, y_train, validation_split=0.3, epochs=2000, callbacks=[early_stopping_monitor])\n\n# save the model to disk\nfilename = '..\/dp_model.sav'\npickle.dump(dp_model, open(filename, 'wb'))\n\n#predictions\ndp_pred = dp_model.predict(Xsc_test)","34900fa5":"#Scores\nr2_score(y_test,dp_pred.ravel()), rmse(y_test,dp_pred.ravel()), meanabs(y_test,dp_pred.ravel())","00f416ad":"plt.figure(figsize=(12,10))\nplt.plot(dp_model.history.history['loss'], label='train')\nplt.plot(dp_model.history.history['val_loss'], label='test')\nplt.legend()\nplt.yscale('log')\nplt.title('Neural Network loss');","5b03d5e0":"qqplot(y_test-dp_pred.ravel(), fit = True, line = \"45\");","8b160898":"lr_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-lr_pred).abs()>55000].reset_index(), how='right', on='index')\nrdg_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-rdg_pred).abs()>55000].reset_index(), how='right', on='index')\nlso_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-lso_pred).abs()>55000].reset_index(), how='right', on='index')\nels_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-lso_pred).abs()>55000].reset_index(), how='right', on='index')\nPolyRDG_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-PolyRDG_pred).abs()>55000].reset_index(), how='right', on='index')\nstat_bad=pd.merge(df.loc['train'].reset_index(), \n                 y1_test[(y1_test-y1_pred).abs()>55000].reset_index(), how='right', on='index')\nsvr_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-svr_pred).abs()>55000].reset_index(), how='right', on='index')\nxgb_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-xgb_pred).abs()>55000].reset_index(), how='right', on='index')\ndp_bad=pd.merge(df.loc['train'].reset_index(), \n                 y_test[(y_test-dp_pred.ravel()).abs()>55000].reset_index(), how='right', on='index')","e26aa644":"rdg_bad","710404a5":"[x for x in set(lso_bad['index']).intersection(stat_bad['index'], svr_bad['index'], xgb_bad['index'], dp_bad['index'])]","f8c75b8a":"#Scale\nsc=RobustScaler().fit(X[X_train.columns])\nXs=sc.transform(X[X_train.columns])\nXs_eval=sc.transform(X_eval[X_train.columns])\n\n# Neural network needs MinMaxScaler\nms=MinMaxScaler().fit(X[X_train.columns])\nXm=ms.transform(X[X_train.columns])\nXm_eval=ms.transform(X_eval[X_train.columns])","8a37724a":"models=[lso_model, svr_model, xgb_model, dp_model]\nw=[.25, .25, .25, .25]","eb8d73af":"def final_pred(ds, dm):\n    z = (w[0]*lso_model.predict(ds)) + (w[2]*svr_model.predict(ds)) +\\\n    (w[2]*xgb_model.predict(ds).ravel()) + (w[3]*dp_model.predict(dm).ravel())\n    return z\n\ntrain_pred = final_pred(Xs, Xm)","7a4d14a3":"from statsmodels.tools.eval_measures import medianabs\nfig, axes=plt.subplots(ncols=2, figsize=(12,5))\nsns.distplot(train_pred, ax=axes[0])\nsns.boxplot(y=train_pred, ax=axes[1])\nfig.suptitle(f'Distribution residuals on train for final model; MeanAE = {meanabs(y, train_pred):.2f}; MedianAE = {medianabs(y, train_pred):.2f}');","849f2aad":"test_pred=final_pred(Xs_eval,Xm_eval )\nsubm = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubm['SalePrice'] = test_pred\nsubm.to_csv(\"submission_regression.csv\", index=False)","18af9425":"## XGBoost\n\nFor XGBoost hyperparameter tuning takes a long time. After running once, I picked the best parameters and created a new model.","8b5fba9d":"## Feature Selection\n### Highly correlated variables with SalePrice\nThese are the best predictors to start with:\n","46851aad":"### Missing values in numerical columns\n","f9e7fcab":"#### MasVnrArea: Masonry veneer area in square feet\nMissing value for `MasVnrArea` means zero because the `MasVnrType=None` which means there is no Masonry veneer\n","44b2b5fe":"# Final Prediction\n## What are bad predictions?\nIf different models predicted poorly on different observations, we can combine the model results for less error, but if all of them perform poorly on the same observations, we can simply choose the best model based on its individual performance.","cffdb8c3":"## SVR","478d2663":"## Polynomial Ridge\n\nFor Polynomial Ridge hyperparameter tuning takes a long time. After running once, I picked the best parameters and created a new model.","e9846c0f":"With similar logic fill the missing values in the other numerical features by zero.","023b78b0":"## Linear Regression","02009160":"## statsmodels Linear Regression","f8f53bf5":"## Neural Network, keras","5ab462b1":"Numerical features are selected based on :\n1. Their correlation with the target variables\n2. Their correlation with each other\n3. Their distribution: For example in `PoolArea` any value other than zero is outlier because for most of the observations `PoolArea`=0 \n4. Their business value: Such as `Fireplaces`","8672f7f6":"Not all the models perform bad on a specific set of observations. Therefore, combining the models can improve the acuuracy.","aabd7d98":"# Load libraries and data","78491282":"## Handling outliers\n### Handling outliers in target variable\nTarget variable highly skewed to the right and it may results in poor prediction. One possible solution can be predicting `Log_SalePrice` instead of `SalePrice`, but it is hard to interpret the result. For now I keep it as it is: **y=`SalePrice`** ","7142838d":"## Lasso","9715f973":"## Ridge","6c07321e":"### Feature selection\/engineering: numerical features\n","908599c1":"# Submission","97daa032":"Choose the categorical variables that have impact on the boxplot of the target variable. Because in most cases the number of the observations in each category is less than number_of_categories+1 (ANOVA condition), **ANOVA** test cannot be used for feture selection. Also **Chi2** test cannot be used for categorical features correlation test because there exists categorical features that have less than 6 observations in some of their categories.","e2347532":"# Models\n\n1. From Linear Regression family:\n\n    a. LinearRegression\n    \n    b. Lasso: Feature Selection\n    \n    c. Ridge: Handling correlated features\n    \n    d. ElasticNet\n    \n    e. Polynomial Ridge\n    \n    f. statsmodels Linear Regression, for other combinations of the features\n    \n    \n2. From Kernel idea:\n\n    SVR\n    \n    \n3. From Desicion Tree family:\n\n    XGBoost, may not be ideal as the size of data is small\n    \n    \n4. From Neural Network family:\n\n    tensorflow\/keras, may not be ideal as the size of data is small","acabe63e":"#### GarageYrBlt: : Year garage was built\nMissing value for GarageYrBlt means zero because the GarageType=None which means there is no garage","628fad63":"#### LotFrontage: Linear feet of street connected to property","cd8f7c5a":"# EDA\n\nBased on exploratory data analysis on `train`, any feature engineering\/selection will be done on `df`.","2815dbef":"## ElasticNet\nFor ElasticNet hyperparameter tuning takes a long time. After running once, I picked the best parameters and created a new model.","22221bdd":"# Result\nAs mentioned above median absolute error is less than 9000 which means 50% of the times predicted `SalePrice` is less than 9000 below\/above the actual sale price.\n\n# Future improvement\n\n1. Feature selection: \n>For categorical variables where one or more but not all special category(s) have dramatic impact on the SalePrice, it would be better to combine the rest of non effective categories together to emphesis the impact of the feature and avoid too many dummy variables\n>As part of EDA, desicion tree could be used to understand feature importance\n\n\n2. Target variable log transform:\n> It could be helpful to predict `Log_SalePrice` instead of `SalePrice` however, it would be difficult to interpret the results and compare the models\n\n\n3. Feature engineering:\n>Logarithm of the numerical features could improve feature skewness and improve the results \n\n\n4. More in-depth EDA:\n>Some features could be combined using the meaning of the features. For instance, all the features related to kitchen could be combined as one feature.","a739f3f6":"## Handling missing values:\n\n### Missing values in categorical columns\nAccording to the data description for categorical variables, missing value means None","9191a422":"# Data preparation\n1. Dummy variables\n2. Train\/test split\n3. Cross Validation Setting\n4. Scaling","2667ae7a":"# Combine models outcomes","6c6694a6":"### Feature selection\/engineering: categorical features\n"}}