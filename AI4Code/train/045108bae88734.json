{"cell_type":{"fc173721":"code","7883c022":"code","1b742cfe":"code","7ef2ba54":"code","82a95417":"code","abafd5f8":"code","3d952d28":"code","e14920dc":"code","93a0c8c2":"code","5b9ca457":"code","f93fc886":"code","a2d041a5":"code","baccd767":"code","16cb4da1":"code","8a40abc8":"code","48cb00e9":"markdown","48beb35b":"markdown","89cbb01d":"markdown","851fac87":"markdown","46cb6dd0":"markdown"},"source":{"fc173721":"import os\nimport sys\nimport gc\nimport math\nimport joblib\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nfrom tqdm.auto import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import KFold\n\nwarnings.simplefilter(\"ignore\", UserWarning)\npd.set_option('display.max_columns', 50)\nplt.rcParams[\"figure.figsize\"] = [10., 8]","7883c022":"ROOT = \"..\/input\/ulaanbaatar-city-air-pollution-prediction\"\n\ndf_train   = pd.read_csv(os.path.join(ROOT, \"pm_train.csv\"))\ndf_test    = pd.read_csv(os.path.join(ROOT, \"pm_test.csv\"))\ndf_weather = pd.read_csv(os.path.join(ROOT, \"weather.csv\"))\ndf_sub     = pd.read_csv(os.path.join(ROOT, \"sample_submission.csv\"))\ndf_station = pd.read_csv(\"..\/input\/ub-station-elevation\/ub_station_elevation_data.csv\")\ndf_weather1 = pd.read_csv(\"..\/input\/ub-weather-2015-2020\/weather_dot_com_2015_2020.csv\")\n\n\ndf_train['date']   = pd.to_datetime(df_train['date'])\ndf_test['date']    = pd.to_datetime(df_test['date'])\ndf_weather['date'] = pd.to_datetime(df_weather['date'])\ndf_weather1['date']= pd.to_datetime(df_weather1['date'])\n\nprint(\"NaN in `summary`\", df_weather[df_weather.summary.isna()].shape[0])\nprint(\"NaN in `icon`\", df_weather[df_weather.icon.isna()].shape[0])\n\nprint(\"DATA SHAPES:\",df_train.shape, df_test.shape, df_weather.shape, df_sub.shape)\nprint(\"NEW WEATHER DATA:\", df_weather1.shape)\n\ndf_train['dayofyear']   = df_train['date'].dt.dayofyear\ndf_test['dayofyear']    = df_test['date'].dt.dayofyear\ndf_weather['dayofyear'] = df_weather['date'].dt.dayofyear\ndf_weather1['dayofyear']= df_weather1['date'].dt.dayofyear\n\ndf_train['hour']        = df_train['date'].dt.hour\ndf_test['hour']         = df_test['date'].dt.hour\ndf_weather['hour']      = df_weather['date'].dt.hour\ndf_weather1['hour']     = df_weather1['date'].dt.hour\n\ndf_train['year']        = df_train['date'].dt.year\ndf_test['year']         = df_test['date'].dt.year\ndf_weather['year']      = df_weather['date'].dt.year\ndf_weather1['year']     = df_weather1['date'].dt.year\n\ndf_train['month']       = df_train['date'].dt.month\ndf_test['month']        = df_test['date'].dt.month\ndf_weather['month']     = df_weather['date'].dt.month\ndf_weather1['month']    = df_weather1['date'].dt.month\n\ndf_train['day']       = df_train['date'].dt.day\ndf_test['day']        = df_test['date'].dt.day\ndf_weather['day']     = df_weather['date'].dt.day\ndf_weather1['day']    = df_weather1['date'].dt.day\n\ndf_train['weekday']       = df_train['date'].dt.weekday\ndf_test['weekday']        = df_test['date'].dt.weekday\n\ndf_train['weekend'] = df_train['weekday'].apply(lambda x: 1 if x > 5 else 0)\ndf_test['weekend']  = df_test['weekday'].apply(lambda x: 1 if x > 5 else 0)\n\ndf_train = pd.merge(df_train, df_weather.drop([\"month\", \"day\", \"date\"], axis=1).iloc[:, 2:], on=[\"year\", \"dayofyear\", \"hour\"], how='left')\ndf_test  = pd.merge(df_test, df_weather.drop([\"month\", \"day\", \"date\"], axis=1).iloc[:, 2:],  on=[\"year\", \"dayofyear\", \"hour\"], how='left')\n\nmonths_in_test = df_test.month.unique()\n\ndf_train = df_train.merge(df_station[[\"station\", \"elevation\"]], on=\"station\", how=\"left\")\ndf_test = df_test.merge(df_station[[\"station\", \"elevation\"]], on=\"station\", how=\"left\")\n\nprint(\"Months in test:\", months_in_test)","1b742cfe":"%%time\nindices_2019_early = df_test[df_test[\"date\"] < \"2019-04-01 00:00:00\"].index\nindices_2019_end   = df_test[df_test[\"date\"] > \"2019-09-30 23:59:00\"][df_test[\"date\"] < \"2020-01-01 00:00:00\"].index\nindices_2020_early = df_test[df_test[\"date\"] > \"2019-12-31 23:59:00\"][df_test[\"date\"] < \"2020-04-01 00:00:00\"].index\nindices_2020_end   = df_test[df_test[\"date\"] > \"2020-09-30 23:59:00\"].index\n\n# month \ndf_test.loc[indices_2019_early , \"season\"] = \"indices_2019_early\"\ndf_test.loc[indices_2019_end , \"season\"]   = \"indices_2019_end\"\ndf_test.loc[indices_2020_early , \"season\"] = \"indices_2020_early\"\ndf_test.loc[indices_2020_end , \"season\"]   = \"indices_2020_end\"\n\nassert df_test.season.isna().sum() == 0","7ef2ba54":"df_weather1[\"day_summary\"] = df_weather1.groupby([\"year\", \"month\", \"day\"]).wx_phrase.transform(lambda x: ' \/ '.join(list(set(x))).lower())\ndf_weather1[\"is_raining\"] = df_weather1[\"day_summary\"].apply(lambda x: \"rain\" in x).astype(int)\ndf_weather1[\"is_snowing\"] = df_weather1[\"day_summary\"].apply(lambda x: \"snow\" in x).astype(int)","82a95417":"df_train = pd.merge(df_train, df_weather1.drop([\"month\", \"day\", \"date\"], axis=1).iloc[:, 2:], on=[\"year\", \"dayofyear\", \"hour\"], how='left')\ndf_test  = pd.merge(df_test, df_weather1.drop([\"month\", \"day\", \"date\"], axis=1).iloc[:, 2:],  on=[\"year\", \"dayofyear\", \"hour\"], how='left')","abafd5f8":"# TRANSFORM TARGET\ntransform_power = 1\n\ndef transform_target(array):\n    return np.power(array, 1\/transform_power)\n\ndef inverse_transform_target(array):\n    return np.power(array, transform_power)\n\n# LAGGING TEMPERATURE\ndef get_lagged_temp(_all_data, colname,lags=[-1], type_=\"prev\"):\n    \n    all_data = _all_data.copy()\n    \n    \n    avg_tmp = 0\n    new_cols = []\n    for lag in lags:\n        tmp = df_weather.groupby([\"year\", \"month\", \"day\"])[colname].min().shift(lag)\n        # tmp = df_weather1.groupby([\"year\", \"month\", \"day\"])[\"temp\"].min().shift(lag)\n        avg_tmp += tmp\/len(lags)\n        \n        tmp = tmp.reset_index()\n        tmp = tmp.rename(columns={colname:f\"{colname}_lag_{lag}\"})\n        all_data = all_data.merge(tmp, how=\"left\", on=[\"year\", \"month\", \"day\"])\n        \n        new_cols.append(f\"{colname}_lag_{lag}\")\n        \n\n    avg_tmp = avg_tmp.reset_index()\n    avg_tmp = avg_tmp.rename(columns={colname:f\"{colname}_lag_{type_}\"})\n    all_data = all_data.merge(avg_tmp, how=\"left\", on=[\"year\", \"month\", \"day\"])\n    new_cols.append(f\"{colname}_lag_{type_}\")\n    \n    return all_data, new_cols\n\n# get_lagged_temp(train, \"temperature\", lags=list(range(-7, 0)), type_=\"next_week\")[0]\n\n# LAGGING TEMPERATURE\ndef get_rolling_temp(all_data, colname,rolls=[1]):\n    \n    new_cols = []\n    for roll in rolls:\n        tmp = df_weather.groupby([\"year\", \"month\", \"day\"])[colname].rolling(roll).min()\n        tmp = tmp.reset_index()\n        tmp = tmp.rename(columns={colname:f\"{colname}_roll_{roll}\"})\n        all_data = all_data.merge(tmp, how=\"left\", on=[\"year\", \"month\", \"day\"])\n        \n        new_cols.append(f\"{colname}_roll_{roll}\")\n        \n        del tmp\n        gc.collect()\n    \n    return all_data, new_cols\n\n# get_rolling_temp(train[:5000], \"temperature\", rolls=[1])[0][[\"temperature\", \"temperature_roll_1\"]]\n\ndef lagging_augment(train, lags=[-3,-2,-1,1,2,3]):\n\n    tmp = train.copy()\n    print(\"Before Lag Augmentation:\", tmp.shape)\n\n    tmp_list = []\n    for lag in lags:\n\n        tmp_ = tmp.copy()\n        tmp_.aqi = tmp_.groupby([\"station\",\"type\"]).aqi.shift(lag)\n        tmp_winter = tmp_[tmp_.month.isin(months_in_test)]\n        \n        tmp_list.append(tmp_winter)\n        del tmp_\n\n    tmp = pd.concat([tmp] + tmp_list)\n    print(\"After Lag Augmentation:\", tmp.shape)\n    \n    return tmp\n\n# lagging_augment(train)[[\"date\",\"type\", \"station\", \"aqi\"]]","3d952d28":"# categorical columns\ncat_feat = [\"type\",\"station\",# \"summary\",\"icon\",\"baseline_month\"\n            # \"month\",\n            \"source\",\n\n            # new weather data:\n            \"day_ind\",\"wx_phrase\",\n            \"wdir_cardinal\", \"uv_desc\", \"clds\"\n           ]\n\n# continues columns\ncont_feat = [\"hour\", \"dayofyear\", # \"day\",\n             # \"elevation\", \n             # \"weekend\", \n             # \"weekday\", \n             # \"year\",\n             # \"latitude\",\"longitude\",\n             # \"precipIntensity\",\"precipProbability\",\n             \"temperature\",\"apparentTemperature\",\n             \"dewPoint\",\"humidity\",\"windSpeed\",\"windBearing\",\n             \"cloudCover\",\"uvIndex\",\"visibility\",\n             # \"baseline_mean\",\"baseline_std\",\n             \"random_noise\",\n             \n             # new weather features:\n             'temp', 'wx_icon', 'icon_extd',\n             'dewPt', 'heat_index', 'rh', \n             'pressure', 'vis', 'wc', 'wspd',\n             'feels_like', 'uv_index',\n             'is_raining',\n             'is_snowing',\n            ]\n\n\n# target\nTARGET = \"target\"","e14920dc":"train = df_train.copy()\n# train = df_train[df_train.month.isin(months_in_test)]   # exclude non-winter months\ntrain = train.reset_index(drop=True)\n\ntest  = df_test.copy()\n\ntrain.loc[:,\"random_noise\"] = np.random.rand(train.shape[0],)\ntest.loc[:,\"random_noise\"] = np.random.rand(test.shape[0],)\n\nprint(\"Train shape\", train.shape)\nprint(\"Test shape\", test.shape)\n\nnew_aqi = transform_target(train.aqi.clip(1e-15))\ntrain.loc[:,\"target\"] = new_aqi\n\nnew_aqi.hist(bins=20);","93a0c8c2":"%%time\n\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\n\nall_data = pd.concat([train, test])\n\nprint(\"started lagging\")\nall_data, new_cols = get_lagged_temp(all_data, \"temperature\", lags=list(range(-7, 0)), type_=\"next_week\"); cont_feat += new_cols\nall_data, new_cols = get_lagged_temp(all_data, \"temperature\", lags=list(range(1, 8)), type_=\"prev_week\"); cont_feat += new_cols\n\nall_data, new_cols = get_lagged_temp(all_data, \"windBearing\", lags=list(range(-7, 0)), type_=\"next_week\"); cont_feat += new_cols\nall_data, new_cols = get_lagged_temp(all_data, \"windBearing\", lags=list(range(1, 8)), type_=\"prev_week\"); cont_feat += new_cols\n\n# [PREPROCESSING] SIMPLE LABEL ENCODE\nfor col in cat_feat:\n    ll = [x for x in all_data[col].unique().tolist() if not pd.isnull(x)]\n    dd = {l:i for i, l in enumerate(ll)}\n    all_data[col] = all_data[col].map(dd)\n\n# separate train and test back\ntrain = all_data[all_data.train == 1]; test  = all_data[all_data.train == 0]\ndel train[\"train\"]; del test [\"train\"]","5b9ca457":"%%time\ntrain[\"fold\"] = -1\n\nfold1 = train[train[\"date\"] > \"2015-09-30 23:59:00\"][train[\"date\"] < \"2016-04-01 00:00:00\"].index.tolist()\nfold2 = train[train[\"date\"] > \"2016-09-30 23:59:00\"][train[\"date\"] < \"2017-04-01 00:00:00\"].index.tolist()\nfold3 = train[train[\"date\"] > \"2017-09-30 23:59:00\"][train[\"date\"] < \"2018-04-01 00:00:00\"].index.tolist()\nfold3 += train[train[\"date\"] > \"2018-09-30 23:59:00\"].index.tolist()\n\ntrain.loc[fold1, \"fold\"] = 0\ntrain.loc[fold2, \"fold\"] = 1\ntrain.loc[fold3, \"fold\"] = 2\n\nlen(fold1), len(fold2), len(fold3)","f93fc886":"assert len(set(cat_feat + cont_feat + [TARGET]) - set(train.columns.tolist())) == 0","a2d041a5":"def plot_feature_importance(list_feat_importance):\n    figs, axs = plt.subplots(1, 3, figsize=(21, 7))\n    for i in range(NFOLD):\n        all_feat = np.array(cat_feat+cont_feat)\n        sorted_indices = np.flip(np.argsort(list_feat_importance[i]), 0)\n        axs[i].barh(all_feat[sorted_indices],list_feat_importance[i][sorted_indices])\n        axs[i].set_title(\"Model %d\" % (i))\n\n    plt.show()\n\ndef train_fold(fold, train_, valid_, test_, seed=42):\n    \n    PARAMS = {\n     'n_estimators': 3000,\n     'boosting_type': 'gbdt',\n#      'objective': 'poisson',\n     'objective': 'tweedie',\n     'tweedie_variance_power': 1.5394379488273047,\n     'is_training_metric': True,\n     'metric': 'rmse',\n     'subsample': 0.690393930785244, \n     'subsample_freq': 3,\n     'colsample_bytree': 0.9571799832845006, \n     'sub_row': 0.9654334345577581, \n     'learning_rate': 0.0053491591627513125, \n     'num_leaves': 937, \n     'min_data_in_leaf': 758, \n     'max_bin': 109, \n     'max_depth': 15, \n     'boost_from_average': True,\n     \"random_seed\":seed,\n     'lambda_l1': 8.847579959982351e-08,\n     'lambda_l2': 0.0005763733457458973, \n     'feature_fraction': 0.40221738457649814, \n     'bagging_fraction': 0.4561818818070979, \n     'bagging_freq': 4,\n     'bagging_freq': 4, \n     'min_child_samples': 13,\n    }\n    \n    # train_ = lagging_augment(train_, lags=[-4,-2,2,4])\n    \n    valid_exists = type(valid_) != type(None)\n\n    train_data = lgb.Dataset(data=train_[cat_feat+cont_feat],\n                             label=train_[TARGET],\n                             categorical_feature=cat_feat,\n                             free_raw_data=False,\n                            )\n    if valid_exists:\n        valid_data  = lgb.Dataset(data=valid_[cat_feat+cont_feat],\n                                  label=valid_[TARGET],\n                                  categorical_feature=cat_feat,\n                                  free_raw_data=False,\n                                 )\n\n\n    model = lgb.train(PARAMS, train_data, \n                  valid_sets=[train_data, valid_data] if valid_exists else [train_data],\n                  num_boost_round=10000, early_stopping_rounds=50,\n                  verbose_eval=50)\n    \n    model_path = \"model_%d.pkl\"%fold if valid_exists else \"model_all.pkl\"\n    joblib.dump(model, model_path)\n    del model\n    gc.collect()\n    \n    model = joblib.load(model_path)\n    \n    # validation\n    y_valid_pred = None; y_test_pred = None\n    \n    if valid_exists: y_valid_pred = model.predict(valid_[cat_feat+cont_feat])\n    y_test_pred  = model.predict(test_ [cat_feat+cont_feat])\n\n    return y_valid_pred, y_test_pred, model.feature_importance()","baccd767":"NFOLD = 3\n# SEEDS = [42,101, 111, 1111, 8888]\nSEEDS = [42]\n\n# train_copy = train.copy()\n# test_copy  = test.copy()\n\noof = np.zeros(train.shape[0],)\npred = np.zeros(test.shape[0],)\n\noof_idx = np.array(train[train[\"fold\"] != -1].index.tolist())\n\nfor seed in SEEDS:\n\n#     print(\"-\"*15)\n#     print(\"-- SEED:\", seed)\n#     print(\"-\"*10)\n    oof_ = np.zeros(train.shape[0],)\n    pred_ = np.zeros(test.shape[0],)\n\n    scores = []\n    list_feat_importance = []\n\n\n    for fold in range(NFOLD):\n\n        train_idx, valid_idx = train[train[\"fold\"] != fold].index, train[train[\"fold\"] == fold].index\n        train_, valid_ = train.loc[train_idx, ], train.loc[valid_idx, ]\n\n        y_valid_pred, y_test_pred, feat_importance = train_fold(fold, train_, valid_, test, seed)\n        oof_[valid_idx] += inverse_transform_target(y_valid_pred)\n        pred_ += inverse_transform_target(y_test_pred)\/NFOLD\n\n        score = np.sqrt(mean_squared_error(valid_.aqi.values, \n                                           inverse_transform_target(y_valid_pred)))\n        scores.append(score)\n        list_feat_importance.append(feat_importance)\n\n        print(\"RMSE: %2.4f\" % score)\n\n    score = np.sqrt(mean_squared_error(train.loc[oof_idx,\"aqi\"].values, oof_[oof_idx]))\n\n    oof += oof_ \/ len(SEEDS)\n    pred += pred_ \/ len(SEEDS)\n\n    print(\"OOF %2.4f | CV %2.4f | STD %2.4f\" % (score, np.mean(scores), np.std(scores)))\n    plot_feature_importance(list_feat_importance)\n\ntrain.loc[oof_idx,\"aqi_pred\"] = oof[oof_idx]\nscore = np.sqrt(mean_squared_error(train.loc[oof_idx,\"aqi\"].values, oof[oof_idx]))\n\nprint(\"FINAL SCORE\")\nprint(\"- \"*10)\nprint(\"OOF %2.4f\" % (score))\nprint(\"-\"*25)","16cb4da1":"# SAVE OOF\ntrain[[\"ID\", \"aqi\", \"aqi_pred\", \"fold\"]].to_csv(\"oof_lgbm_tweedie_%6.4f.csv\" % score, index=False)\n\ndf_sub[\"aqi\"] = pred\ndf_test[\"target\"] = pred\ntest[\"target\"] = pred\n\ndf_sub.loc[df_test[~df_test.aqi.isna()].index, \"aqi\"] = df_test.loc[df_test[~df_test.aqi.isna()].index, \"aqi\"].values\nassert df_sub.aqi.isna().sum() == 0\ndf_sub.to_csv(\"sub_lgbm_tweedie_%6.4f.csv\" % score, index=False)","8a40abc8":"# Pseudo Labeling\n_, all_test_pred, _ = train_fold(-2, pd.concat([train, test]), None, test, SEEDS[0])\n\ndf_sub[\"aqi\"] = all_test_pred\ndf_sub.loc[df_test[~df_test.aqi.isna()].index, \"aqi\"] = df_test.loc[df_test[~df_test.aqi.isna()].index, \"aqi\"].values\nassert df_sub.aqi.isna().sum() == 0\ndf_sub.to_csv(\"sub_lgbm_tweedie_pseudo_%6.4f.csv\" % score, index=False)","48cb00e9":"## Read Data","48beb35b":"## Feed model with the test data too","89cbb01d":"## SPLIT","851fac87":" Now Do Preprocessing","46cb6dd0":"## Preprocessing"}}