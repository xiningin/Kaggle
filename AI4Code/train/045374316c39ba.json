{"cell_type":{"4846d15a":"code","b93c14e1":"code","24517496":"code","7099e6ed":"code","0d4dcb0d":"code","d762810e":"code","97434c19":"code","9625be55":"code","3ba777d8":"code","e0d4e24f":"code","fab8471a":"code","7161a0f5":"code","3a680beb":"markdown"},"source":{"4846d15a":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","b93c14e1":"\ndataset = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\nX = dataset.iloc[:,2:]\n\n#---------------------Feature engineering\n#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nX['Title'] = X['Name'].apply(get_title)\nX['Title'].value_counts()\n\nX['Title'] = X['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\nX['Title'] = X['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\nX['Title'] = X['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\nX['Title'] = X['Title'].replace(['Mme.'], 'Mrs.')\nX['Title'].value_counts()\n\n#Cabin\nfrom collections import Counter\nCounter(X.Cabin)\n\nX['Cabin'] = X['Cabin'].fillna('Missing')\nX['Cabin'] = X['Cabin'].str[0]\nX['Cabin'].value_counts()\n\n#Family Size & Alone \nX['Family_Size'] = X['SibSp'] + X['Parch'] + 1\nX['IsAlone'] = 0\nX.loc[X['Family_Size']==1, 'IsAlone'] = 1\nX.head()\n","24517496":"\ndrop_list = [\"Name\",\"Ticket\"]\nX.drop(drop_list,axis=1,inplace=True)\n\ny = dataset[\"Survived\"]\n\nX.isna().any()\nX.isna().sum()\n\n\nsns.displot(X.Age)\n#percentage of missing values\nX.Age.isna().sum()\/X.Age.count() *100\n\nsns.boxplot(X.Age)\n\n#Outlier Value Treatment\n#for value in colname:\nq1 = X['Age'].quantile(0.10) #first quartile value\nq3 = X['Age'].quantile(0.90) # third quartile value\n\nX[\"Age\"] = np.where(X[\"Age\"]< q1,q1,X[\"Age\"])\nX[\"Age\"] = np.where(X[\"Age\"]> q3,q3,X[\"Age\"])\n\n#impute missing values\nX.Age.describe()\nX.Age.skew()\nX.Age.median()\nX[\"Age\"].fillna(X[\"Age\"].median(),inplace=True)\n\n\nX.dtypes\nX.head(5)\n\nX[\"Pclass\"] = X[\"Pclass\"].astype(\"object\")\nX[\"SibSp\"] = X[\"SibSp\"].astype(\"object\")\nX[\"Parch\"] = X[\"Parch\"].astype(\"object\")\nX[\"Age\"] = X[\"Age\"].astype(\"int\").round()\n\n\nX = pd.get_dummies(X,drop_first=True)\n\n\n# Create correlation matrix\ncorr_matrix = X.corr().abs()\n\n\n#------Remove the higly correlated variables--------------------------------\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find features with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n# Drop features \nX.drop(to_drop, axis=1, inplace=True)","7099e6ed":"\ndef evaluation_score (y_test,y_pred):\n    cm = confusion_matrix(y_test,y_pred) \n    print(\"Confusion Matrix \\n\", cm)\n    print('Balanced Accuracy ',metrics.balanced_accuracy_score(y_test,y_pred))\n    #print(\"Recall Accuracy Score~TP\",metrics.recall_score(y_test, y_pred))\n    #print(\"Precision Score ~ Ratio of TP\",metrics.precision_score(y_test, y_pred))\n    #print(\"F1 Score\",metrics.f1_score(y_test, y_pred))\n    #print(\"auc_roc score\", metrics.roc_auc_score(y_test,y_pred))\n    print(\"Classification Report\", classification_report(y_test,y_pred))\n    \n    \ndef cross_validation(model,X_train,y_train,n):\n    kfold = KFold(n_splits=10)  \n    accuracies = cross_val_score(model,X= X_train,y= y_train,cv = kfold,scoring='accuracy')\n    print(\"Standard Deviation\",accuracies.std())\n    print(\"Mean\/Avergae Score\",accuracies.mean())","0d4dcb0d":"\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state= 2)\n\n#testing with legacy models\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors = 5)\nknn_model.fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\n\nevaluation_score(y_test,y_pred)\ncross_validation(knn_model,X_train,y_train,10)","d762810e":"\n#Selecting the best mmodel-----------------------------------------------------------------------------------------------------\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['random_forest'] = RandomForestClassifier()\n    models['xgboost'] = XGBClassifier()\n    return models\n\n# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n\treturn scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\tscores = evaluate_model(model, X, y)\n\tresults.append(scores)\n\tnames.append(name)\n\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","97434c19":"#------------------------------------------------------------------\n#let's redo the XGBOOST model with paramter tunning\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.utils.fixes import loguniform\nimport scipy.stats as stats\n\nparameters= {\n    'learning_rate': loguniform(1e-3, 1e0),\n    'n_estimators': stats.randint(1, 200),\n    'subsample': stats.uniform(0.01, 0.99),\n    'max_depth': stats.randint(2, 10),\n    'colsample_bytree': stats.uniform(0.01, 0.99),\n    'reg_lambda': loguniform(1, 1000)\n}\n\nclf=XGBClassifier()\ngrid = RandomizedSearchCV(clf, param_distributions = parameters,cv=3,scoring='accuracy',verbose = 0)\ngrid_search=grid.fit(X_train,y_train)\n\n\nbest_accuracy = grid_search.best_score_\nprint(best_accuracy)\nbest_parameters = grid_search.best_params_\nprint(best_parameters)\nbest_estimator = grid_search.best_estimator_\nprint(best_estimator)\n\n#-----------------------------------------------------------------------------------","9625be55":"best_accuracy = grid_search.best_score_\nprint(best_accuracy)\nbest_parameters = grid_search.best_params_\nprint(best_parameters)\nbest_estimator = grid_search.best_estimator_\nprint(best_estimator)\n\n#-----------------------------------------------------------------------------------","3ba777d8":"#Running the model with optimized parameters\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(base_score = 0.5,booster ='gbtree',colsample_bytree = 0.61, learning_rate = 0.56,max_depth=2,n_estimators=141,\n                          reg_lambda = 80.91, subsample = 0.22)\n\n\nxgb_model.fit(X_train,y_train)\ny_pred = xgb_model.predict(X_test)\n\nevaluation_score(y_test,y_pred)\ncross_validation(xgb_model,X_train,y_train,10)","e0d4e24f":"#--------------with SMOTE----------------\n#step repeated to get the original X values---------------------------\ndataset = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\nX = dataset.iloc[:,2:]\n\n\n#--Feature engineering\n#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nX['Title'] = X['Name'].apply(get_title)\nX['Title'].value_counts()\n\nX['Title'] = X['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\nX['Title'] = X['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\nX['Title'] = X['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\nX['Title'] = X['Title'].replace(['Mme.'], 'Mrs.')\nX['Title'].value_counts()\n\n#Cabin\nfrom collections import Counter\nCounter(X.Cabin)\n\nX['Cabin'] = X['Cabin'].fillna('Missing')\nX['Cabin'] = X['Cabin'].str[0]\nX['Cabin'].value_counts()\n\n#Family Size & Alone \nX['Family_Size'] = X['SibSp'] + X['Parch'] + 1\nX['IsAlone'] = 0\nX.loc[X['Family_Size']==1, 'IsAlone'] = 1\nX.head()\n\n\ndrop_list = [\"Name\",\"Ticket\"]\nX.drop(drop_list,axis=1,inplace=True)\n\ny = dataset[\"Survived\"]\n\nX.isna().any()\nX.isna().sum()\n\n\nsns.displot(X.Age)\n#percentage of missing values\nX.Age.isna().sum()\/X.Age.count() *100\n\nsns.boxplot(X.Age)\n\n#Outlier Value Treatment\n#for value in colname:\nq1 = X['Age'].quantile(0.10) #first quartile value\nq3 = X['Age'].quantile(0.90) # third quartile value\n\nX[\"Age\"] = np.where(X[\"Age\"]< q1,q1,X[\"Age\"])\nX[\"Age\"] = np.where(X[\"Age\"]> q3,q3,X[\"Age\"])\n\n#impute missing values\nX.Age.describe()\nX.Age.skew()\nX.Age.median()\nX[\"Age\"].fillna(X[\"Age\"].median(),inplace=True)\n\n\nX.dtypes\nX.head(5)\n\nX[\"Pclass\"] = X[\"Pclass\"].astype(\"object\")\nX[\"SibSp\"] = X[\"SibSp\"].astype(\"object\")\nX[\"Parch\"] = X[\"Parch\"].astype(\"object\")\nX[\"Age\"] = X[\"Age\"].astype(\"int\").round()\n\n\nX = pd.get_dummies(X,drop_first=True)\n\n#The Right way is to map\/only take features that are passed after corr_matrix in the training set rather than running again \n#which may lead to miss alignment of features\n# Create correlation matrix\ncorr_matrix = X.corr().abs()\n#------Remove the higly correlated variables--------------------------------\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find features with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n# Drop features \nX.drop(to_drop, axis=1, inplace=True)\n\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state= 2)\n\n\n#--------------------------------------------------------------------------------\n\n# describes info about train and test set \nprint(\"Number transactions X_train dataset: \", X_train.shape) \nprint(\"Number transactions y_train dataset: \", y_train.shape) \nprint(\"Number transactions X_test dataset: \", X_test.shape) \nprint(\"Number transactions y_test dataset: \", y_test.shape)\n\n#----------------------------------------------\nfrom imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n\n\n# import SMOTE module from imblearn library \n# pip install imblearn (if you don't have imblearn in your system) \nfrom imblearn.combine import SMOTETomek\nsm = SMOTETomek(random_state = 2,sampling_strategy=1) \nX_train_res, y_train_res = sm.fit_resample(X_train, y_train) \n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n\n\n#Running the model with optimized parameters\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(base_score = 0.5,booster ='gbtree',colsample_bytree = 0.61, learning_rate = 0.56,max_depth=2,n_estimators=141,\n                          reg_lambda = 80.91, subsample = 0.22)\n\n\nxgb_model.fit(X_train_res,y_train_res)\ny_pred = xgb_model.predict(X_test)\n\n#Get the accuracy score\nevaluation_score(y_test,y_pred)\n\n#get the accuracy score of Kfold\ncross_validation(knn_model,X_train,y_train,10)    \n","fab8471a":"#------------Predicting on Test set\n\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n\n#--Feature engineering\n#Title\nimport re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+\\.)', name)\n    \n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntest_data['Title'] = test_data['Name'].apply(get_title)\ntest_data['Title'].value_counts()\n\ntest_data['Title'] = test_data['Title'].replace(['Capt.', 'Dr.', 'Major.', 'Rev.'], 'Officer.')\ntest_data['Title'] = test_data['Title'].replace(['Lady.', 'Countess.', 'Don.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Royal.')\ntest_data['Title'] = test_data['Title'].replace(['Mlle.', 'Ms.'], 'Miss.')\ntest_data['Title'] = test_data['Title'].replace(['Mme.'], 'Mrs.')\ntest_data['Title'].value_counts()\n\n#Cabin\nfrom collections import Counter\nCounter(test_data.Cabin)\n\ntest_data['Cabin'] = test_data['Cabin'].fillna('Missing')\ntest_data['Cabin'] = test_data['Cabin'].str[0]\ntest_data['Cabin'].value_counts()\n\n#Family Size & Alone \ntest_data['Family_Size'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data['IsAlone'] = 0\ntest_data.loc[test_data['Family_Size']==1, 'IsAlone'] = 1\ntest_data.head()\n\n\ndrop_list = [\"Name\",\"Ticket\"]\ntest_data.drop(drop_list,axis=1,inplace=True)\n\ny = dataset[\"Survived\"]\n\ntest_data.isna().any()\ntest_data.isna().sum()\n\n\n#sns.displot(test_data.Age)\n#percentage of missing values\n#test_data.Age.isna().sum()\/test_data.Age.count() *100\n\n#sns.boxplot(test_data.Age)\n#Outlier Value Treatment\n#for value in colname:\n#q1 = test_data['Age'].quantile(0.10) #first quartile value\n#q3 = test_data['Age'].quantile(0.90) # third quartile value\n\n#X[\"Age\"] = np.where(X[\"Age\"]< q1,q1,X[\"Age\"])\n#X[\"Age\"] = np.where(X[\"Age\"]> q3,q3,X[\"Age\"])\n\n#impute missing values\ntest_data[\"Age\"].fillna(test_data[\"Age\"].median(),inplace=True)\n\ntest_data.dtypes\ntest_data.head(5)\n\ntest_data[\"Pclass\"] = test_data[\"Pclass\"].astype(\"object\")\ntest_data[\"SibSp\"] = test_data[\"SibSp\"].astype(\"object\")\ntest_data[\"Parch\"] = test_data[\"Parch\"].astype(\"object\")\ntest_data[\"Age\"] = test_data[\"Age\"].astype(\"int\").round()\n\n\ntest_data = pd.get_dummies(test_data,drop_first=True)\n\n\n# Create correlation matrix\ncorr_matrix = test_data.corr().abs()\n\n\n#------Remove the higly correlated variables--------------------------------\n# Select upper triangle of correlation matrix\n#upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find features with correlation greater than 0.95\n#to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n# Drop features \ntest_data.drop(to_drop, axis=1, inplace=True)\n\n\ndrop_col = [\"PassengerId\"]\ntest_data.drop(drop_col,axis=1,inplace=True)\n\n\n#Fare have 1 missing row\ntest_data[\"Fare\"].fillna(test_data[\"Fare\"].median(),inplace=True)\n\n#scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_test = StandardScaler()\ntest_data = sc_test.fit_transform(test_data)\n\n\nresults = pd.DataFrame(xgb_model.predict(test_data))\nresults.rename(columns={0:\"Survived\"},inplace=True)\n#-----------submissions\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\",usecols=[0])\nsubmission = pd.concat([test_data,results],axis=1)\nprint(submission)\nsubmission.to_csv(\"submissions1.csv\",index=False)","7161a0f5":"#Apply AutoML to find the best suited model\nimport lazypredict\nfrom lazypredict.Supervised import LazyClassifier\n\nreg = LazyClassifier(verbose=0, ignore_warnings=False, custom_metric=None)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nprint(models)\nprint(predictions)","3a680beb":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6115466369792596, gamma=0,\n              gpu_id=-1, importance_type='gain', interaction_constraints='',\n              learning_rate=0.5688524981912473, max_delta_step=0, max_depth=2,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=141, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=80.91792738717031, scale_pos_weight=1,\n              subsample=0.22220488111202835, tree_method='exact',\n              validate_parameters=1, verbosity=None)"}}