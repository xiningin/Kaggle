{"cell_type":{"3c5a25ac":"code","2f020cb6":"code","01359ef4":"code","1fa9627a":"code","1bafddc8":"code","fc5c711e":"code","c2b79cf4":"code","b454bef3":"code","1ea6325b":"code","26a3fb20":"code","da7ca32a":"code","0ca76cb7":"code","092e18cb":"code","bf442627":"code","7188837e":"code","64483f81":"code","b31c74d1":"code","94138ba9":"code","72037395":"code","dcea1864":"code","c2ca128f":"code","0d23113b":"code","fa80fbd7":"code","16cb4353":"code","c36838ef":"code","340c2809":"code","0b5dfbfe":"code","d7cab548":"code","c1485c5b":"code","8eb2fbfb":"code","c1d2915f":"code","20fd219a":"code","b46e219f":"code","6b9a6068":"code","e488385f":"code","40f0762c":"code","237f7979":"code","373b18da":"code","2a99543a":"code","8cb40f73":"code","15e266a3":"code","d34e2489":"code","c8cd2dd1":"code","dbf18c49":"markdown","78c75ba0":"markdown","02986a0f":"markdown","eae4fc64":"markdown","5f4df4f5":"markdown","8a0aec9d":"markdown","d15bccd3":"markdown","e5a221e4":"markdown","f04bcaf7":"markdown","b43d9399":"markdown","fe55d426":"markdown","4b80930b":"markdown","09a72e1b":"markdown","dbc5a6fd":"markdown","93f1d9d9":"markdown","26c13fd6":"markdown","00275238":"markdown","f9285f38":"markdown","9c78cb00":"markdown","4b7f61f6":"markdown","0d4d1721":"markdown","f1800fc2":"markdown","1107f142":"markdown","33a218b9":"markdown","9fc44d07":"markdown","0f262cd8":"markdown","e116d6b9":"markdown","c33712eb":"markdown","b33bfb04":"markdown","ea1a868d":"markdown","30b98803":"markdown"},"source":{"3c5a25ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f020cb6":"#Model ID\nModelId='house_price_FML_v1'\n\n#Setting the model target variable name\nvar_target = 'SalePrice'\n\n#process outputs such as MOJO model, images and performance of tested models\nOutputPath='\/kaggle\/temp\/'\n\n#If you have a huge dataset, I should consider use a small sample for first execution\npct_sample_size = 1","01359ef4":"import glob\nimport functools\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport h2o\nimport matplotlib.pyplot as plt\nimport shap\nfrom pandas_profiling import ProfileReport\nfrom collections import defaultdict\nfrom pandas_profiling.model.base import get_var_type\nimport seaborn as sns\nimport os\nimport random","1fa9627a":"#Import bases with features for modeling\n#In this case we will use house price dataset available below\ndataprep_df_full = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\n\n#The target variavle must be float 0 or 1\ndataprep_df_full['SalePrice'] = dataprep_df_full['SalePrice'].astype(float)\n\ndataprep_df_full.dtypes","1bafddc8":"random.seed(59354518745)\nfor i in range(len(dataprep_df_full)):\n    dataprep_df_full.loc[i, ('random')] = random.random()\ndataprep_df_full['dataset'] = ['train' if x <= 0.85 else 'test' for x in dataprep_df_full['random']]","fc5c711e":"#Work with a sample data if the pct_sample_size is less than 1\nif pct_sample_size == 1:\n    dataprep_df = dataprep_df_full\nelse:    \n    dataprep_df = dataprep_df_full.sample(frac=pct_sample_size, replace=False, random_state=1)","c2b79cf4":"dataprep_df['OpenPorchSFX2'] = np.square(dataprep_df['OpenPorchSF'])\ndataprep_df['WoodDeckSFX2'] = np.square(dataprep_df['WoodDeckSF'])","b454bef3":"#Generate report\n#If the database has many records or columns, the report can take a long time\n#If this is the case, disable the explorative, samples, correlations, missing_diagrams, duplicates and interactions options by commenting out\nprofile = ProfileReport(dataprep_df, title=f\"Pandas Profiling Report{ModelId}\"\n                        ,explorative=True\n                        ,samples=None\n                        ,correlations=None\n                        ,missing_diagrams=None\n                        ,duplicates=None\n                        ,interactions=None\n                       )\nprofile.to_file(\"profile.html\")\ndisplay(profile)","1ea6325b":"# Get all the types pandas_profiling offers\nlist_columns = dataprep_df.columns.drop('dataset').drop('SalePrice')\nd = {col: get_var_type(dataprep_df[col])['type'].value for col in list_columns}\nfd = defaultdict(list)\nfor k, v in d.items():\n    fd[v].append(k)\n     \ncols_by_base_type = dict(fd)\n# Group the types pandas_profiling offers to match typical needs\ncat_num_cols = defaultdict(list)\nfor k, v in cols_by_base_type.items():\n    # Treat boolean and unique columns as categorical\n    k = 'CAT' if k in ['BOOL', 'UNIQUE'] else k\n    cat_num_cols[k].extend(v)\ndict(cat_num_cols)","26a3fb20":"#It is necessary to define the types of variables (cageroric and numeric) to ensure that the type of data used in the modeling will be the most suitable.\n#For example, categorical variables need to be defined as a string because this prevents it from being treated as a numeric variable in H20 modeling\n#Another example is that the string variables will have a missing treatment by placing the missing category for all values found as 'null'\nCAT = ['MSZoning',\n  'Street',\n  'Alley',\n  'LotShape',\n  'LandContour',\n  'Utilities',\n  'LotConfig',\n  'LandSlope',\n  'Neighborhood',\n  'Condition1',\n  'Condition2',\n  'BldgType',\n  'HouseStyle',\n  'RoofStyle',\n  'RoofMatl',\n  'Exterior1st',\n  'Exterior2nd',\n  'MasVnrType',\n  'ExterQual',\n  'ExterCond',\n  'Foundation',\n  'BsmtQual',\n  'BsmtCond',\n  'BsmtExposure',\n  'BsmtFinType1',\n  'BsmtFinType2',\n  'Heating',\n  'HeatingQC',\n  'Electrical',\n  'BsmtFullBath',\n  'BsmtHalfBath',\n  'FullBath',\n  'HalfBath',\n  'KitchenAbvGr',\n  'KitchenQual',\n  'Functional',\n  'Fireplaces',\n  'FireplaceQu',\n  'GarageType',\n  'GarageFinish',\n  'GarageQual',\n  'GarageCond',\n  'PavedDrive',\n  'PoolQC',\n  'Fence',\n  'MiscFeature',\n  'SaleType',\n  'SaleCondition',\n  'CentralAir']\n#float\nNUM = ['MSSubClass',\n  'LotFrontage',\n  'LotArea',\n  'OverallQual',\n  'OverallCond',\n  'YearBuilt',\n  'YearRemodAdd',\n#   'MasVnrArea',\n#   'BsmtFinSF1',\n  'BsmtFinSF2',\n#   'BsmtUnfSF',\n  'TotalBsmtSF',\n  '1stFlrSF',\n  '2ndFlrSF',\n  'LowQualFinSF',\n#   'GrLivArea',\n  'BedroomAbvGr',\n#   'TotRmsAbvGrd',\n  'GarageYrBlt',\n  'GarageCars',\n#   'GarageArea',\n  'WoodDeckSF',\n#   'OpenPorchSF',\n#   'EnclosedPorch',\n  '3SsnPorch',\n  'ScreenPorch',\n#   'PoolArea',\n#   'MiscVal',\n'MoSold',\n'OpenPorchSFX2',\n'WoodDeckSFX2'\n      ]\nselected_features = CAT + NUM","da7ca32a":"#Numeric features must be float type\nfor col_name in NUM:    \n    dataprep_df[col_name] = dataprep_df[col_name].astype(float)    \n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:        \n    dataprep_df[col_name] = dataprep_df[col_name].astype(str)    \n    dataprep_df = dataprep_df.fillna(value={col_name: 'missing'})","0ca76cb7":"# Number of threads, nthreads = -1, means use all cores on your machine\n# max_mem_size is the maximum memory (in GB) to allocate to H2O\nh2o.init(nthreads = -1, max_mem_size = 8)","092e18cb":"#Import TRAINING base to the H20 context\ndata_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"train\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\nfor col_name in CAT:\n    data_hdf[col_name] = data_hdf[col_name].asfactor()    \n    \n# Partition data into 90%, 10% chunks\n# Setting a seed will guarantee reproducibility\ntrain_hdf, valid_hdf = data_hdf.split_frame(ratios=[0.90], destination_frames=['train_hdf', 'valid_hdf'], seed=1)\n        \n#Notice that `split_frame()` uses approximate splitting not exact splitting (for efficiency), so these are not exactly 90%, 10% of the total rows.\nprint('Training: ' + str(train_hdf.nrow))\nprint('Validation: ' + str(valid_hdf.nrow))","bf442627":"#Import TEST base to the H20 context\ntest_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"test\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\nfor col_name in CAT:\n    test_hdf[col_name] = test_hdf[col_name].asfactor()    \n    \nprint('Training: ' + str(test_hdf.nrow))","7188837e":"vModel = 'GLM_'\n\nstart = dt.datetime.now()\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n#definir par\u00e1metros\nGLM = H2OGeneralizedLinearEstimator(family= 'gaussian',\n                                    seed=1,\n                                    model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")))\n\n#Executar Modelo\nGLM.train(x = selected_features,\n          y = var_target,\n          training_frame = train_hdf,\n          validation_frame = valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) +\"\\n\")\nprint(GLM)","64483f81":"vModel='GBM_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nGBM = H2OGradientBoostingEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")),\n                                   ntrees=500,\n                                   score_tree_interval=5,     #used for early stopping\n                                   stopping_rounds=3,         #used for early stopping\n                                   stopping_metric='MAE',     #used for early stopping\n                                   stopping_tolerance=0.0005, #used for early stopping\n                                   seed=1)\n\n# The use of a validation_frame is recommended with using early stopping\nGBM.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(GBM)","b31c74d1":"vModel='GBM_cv_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nGBM_cv = H2OGradientBoostingEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\"))\n                                   ,nfolds=5\n                                   ,seed=1)\n\n# The use of a validation_frame is recommended with using early stopping\nGBM_cv.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(GBM_cv)","94138ba9":"vModel='DRF_CV_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\nDRF = H2ORandomForestEstimator(seed=1\n                               ,nfolds=5\n                               ,model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")))\n\n# The use of a validation_frame is recommended with using early stopping\nDRF.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(DRF)","72037395":"vModel='AUTOML'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\n#Set the maximum time in seconds for the H20 AutoML\nmax_runtime_secs=60*10\n\n#Define metrics to select the best model in AutoML\nsort_metric = 'MAE'\n\nfrom h2o.automl import H2OAutoML\nAUTOML = H2OAutoML(seed=1,\n                   #include_algos = ['DRF', 'GLM', 'XGBoost', 'GBM', 'DeepLearning', 'StackedEnsemble'],\n                   include_algos = ['DRF', 'GLM', 'XGBoost', 'GBM', 'DeepLearning'],\n                   max_runtime_secs = max_runtime_secs,\n                   stopping_metric = sort_metric,\n                   sort_metric = sort_metric)\nAUTOML.train(x=selected_features, y=var_target, training_frame = train_hdf, validation_frame = valid_hdf, leaderboard_frame=test_hdf)\n\n#View the AutoML Leaderboard\nlb = AUTOML.leaderboard\nprint(lb.head(rows=lb.nrows))\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")","dcea1864":"#Choose the desired AutoML model\nbest_automl_position=0\nif len(AUTOML.leaderboard) > 0:\n    best_AutoML = h2o.get_model(AUTOML.leaderboard[best_automl_position, 0])\n    print(best_AutoML)","c2ca128f":"#Create empty model list\nlist_models = []\n\n#Define the list of all models that have been executed and should be compared\ntry:\n    list_models.append(GLM)\nexcept NameError:\n    GLM = None\ntry:\n    list_models.append(GBM)\nexcept NameError:\n    GBM = None\ntry:\n    list_models.append(GBM_cv)\nexcept NameError:\n    GBM_cv = None\ntry:\n    list_models.append(DRF)\nexcept NameError:\n    DRF = None\ntry:\n    list_models.append(best_AutoML)\nexcept NameError:\n    best_AutoML = None","0d23113b":"#Compare performance on the TEST dataset for all trained models\nplt.rcParams.update({'font.size': 12})\nfig = plt.figure(figsize=(10, 10))\nfor i in list_models:\n    #Save all models in H20 format\n    h2o.save_model(model=i, path='%s\/models\/todos\/' % OutputPath, force=True)\n    \n    #Ascertain the performance of all models on the test base\n    performance = i.model_performance(test_hdf)\n    \n    #Salve metrics\n    f=open(\"%s\/models\/todos\/performance_%s.csv\" % (OutputPath, i.model_id), 'w')\n    f.write(\n        str(i.model_id) + \";\"        \n        + str(performance.mae()) + ';'\n        + str(performance.rmse()) + ';'\n        + str(performance.r2()))\n    f.write('\\n')\n    f.close()\n    \n    if i.model_id==list_models[0].model_id:\n        df_plot = pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'MAE': int(performance.mae()*100)\/100,\n                                    'RMSE': int(performance.rmse()*100)\/100,\n                                    'R2': int(performance.r2()*100)\/100\n                                    }, index=[0])\n    else:\n        df_plot = df_plot.append(pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'MAE': int(performance.mae()*100)\/100,\n                                    'RMSE': int(performance.rmse()*100)\/100,\n                                    'R2': int(performance.r2()*100)\/100\n                                    }, index=[0]))\nax = df_plot.plot(kind='bar', x=\"Model_id\", title=\"MAE, RMSE e R2 for Model (Test dataset)\", grid=True, figsize=(10,5), legend=1)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nplt.legend(loc=3, prop={'size': 10})","fa80fbd7":"#Consider all models in the history .\/models\/todos\/performance_*.csv. To disregard any old version, set erase_modelos = \"S\":\napagar_modelos = 'N'\nif apagar_modelos == 'S':\n    os.system('rm %s\/models\/todos\/performance_*.csv' % OutputPath)","16cb4353":"sort_metric_best_model='MAE'\n#importar todos os modelos testados e imprmie na tela os 10 melhores erdedando per AUC\nmodelos_testados = pd.concat(map(functools.partial(pd.read_csv, sep=';', header=None), glob.glob('%s\/models\/todos\/performance_*.csv' % OutputPath)))\nmodelos_testados.columns = ('model_id', 'MAE', 'RMSE', 'R2')\nmodelos_testados = modelos_testados.sort_values(by=sort_metric_best_model, ascending=True)\nmodelos_testados = modelos_testados.drop_duplicates(subset=[\"model_id\"])\nprint('MBest Models. Sorted by : ' + str(sort_metric_best_model))\nmodelos_testados.reset_index(0).head(30)","c36838ef":"#If you want to choose a model other than the first one on the list. Choose the position number:\nposicao_melhor_modelo=0\n\nmelhor_modelo = h2o.load_model('%s\/models\/todos\/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\n(print(\"\\n\"+ \"BEST MODEL: \" + str(modelos_testados.iloc[posicao_melhor_modelo, 0]) + \"\\n\"))\n\nplt.rcParams.update({'font.size': 10})\ntry:\n    melhor_modelo.varimp_plot(50)\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","340c2809":"#Listar todas as vari\u00e1veis do modelo atual, ordenadas por variable importance\n#Para as variaveis definidas como fator (que possivelmente est\u00e3o como dummys), remover a categoria do nome e deixar apenas o nome orifinal da variavel\n\n#List all variables in the current model, ordered by variable importance\n#For variables defined as a factor (which possibly are like dummys), remove the category from the name and leave only the orifinal name of the variable\ntry:\n    df_features_sorted = melhor_modelo.varimp(True).variable.str.split('.', expand=True).drop_duplicates(subset = 0)[0].reset_index(drop=True)\nexcept Exception as e:\n    #As the model with ensemble in H20 does not show the importance of variables, we will include variables with higher IV first using result_formatado graph of step 5.1\n    df_features_sorted = result_formated_graph.Variable.reset_index(drop=True)","0b5dfbfe":"#Define the number of variables to be increased with each new model. Try to put 10% or 20% of the total, as it can take a long time\nqt_var=1\nqt_total_var = len(df_features_sorted)\n\ndict_model_tmp={}\ndict_performance={}\n\nfor i in range(qt_var, qt_total_var+qt_var, qt_var):    \n    df_features_sorted[0:i].values.tolist()    \n    \n    #If no model chosen is not an ensemble of models. Then use the same model for training with increment of variables\n    melhor_modelo_tmp = melhor_modelo\n    if melhor_modelo_tmp.model_id.lower().find(\"ensemble\") == -1:\n        dict_model_tmp[i] = melhor_modelo_tmp\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)\n    ##If it is not possible, for the home of an ensemble of models, use GradientBoostingEstimator to make the assessment\n    else:\n        dict_model_tmp[i] = H2OGradientBoostingEstimator(seed=1, model_id=str('model_tmp_%s' % i))\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)       \n\n\n    perform_oot = dict_model_tmp[i].model_performance(test_hdf)\n    dict_performance_tmp = {}\n    dict_performance_tmp['MAE'] = {'qt_var': i, 'medida': 'MAE', 'Validation_Dataset': dict_model_tmp[i].mae(valid=True), 'Test_Dataset': perform_oot.mae()}\n    dict_performance_tmp['RMSE'] = {'qt_var': i, 'medida': 'RMSE', 'Validation_Dataset': dict_model_tmp[i].rmse(valid=True), 'Test_Dataset': perform_oot.rmse()}\n    dict_performance_tmp['R2'] = {'qt_var': i, 'medida': 'R2', 'Validation_Dataset': dict_model_tmp[i].r2(valid=True), 'Test_Dataset': perform_oot.r2()}\n    dict_performance[i] = pd.DataFrame(dict_performance_tmp).transpose()","d7cab548":"##Plot graph comparing the increase in performance with the increase in variables\nfor i in dict_performance.keys():\n    if i == list(dict_performance.keys())[0]:\n        df_performance = dict_performance[i]\n    else:\n        df_performance = df_performance.append(dict_performance[i], ignore_index=True)\n\nlista_metricas_perf = df_performance['medida'].unique()\n\nfor i in range(len(lista_metricas_perf)):   \n    #selects only the metric to be analyzed\n    metrics_df_tmp = df_performance.query('medida == \"%s\"' % lista_metricas_perf[i])\n    metrics_df_tmp = metrics_df_tmp.set_index('qt_var')\n    del metrics_df_tmp['medida']\n    if lista_metricas_perf[i] == 'R2':\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.max()].index.values\n    else:\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.min()].index.values\n        \n    if lista_metricas_perf[i] == sort_metric_best_model:\n        max_oot_filtro = max_oot[0]        \n    \n    ax=metrics_df_tmp.plot(figsize=(15,5), linewidth=2, fontsize=10, marker='D', ms=5,\\\n                            title='Best %s with %s Variables' % (lista_metricas_perf[i].upper(), str(max_oot[0])))\n    plt.xlabel('Variables Number')\n    plt.ylabel('%s' % lista_metricas_perf[i].upper())\n    plt.grid(axis='y')\n    plt.legend(loc=0, prop={'size': 12})\n    #display(ax)","c1485c5b":"print('Consider removing the following variables: '+ str(df_features_sorted[df_features_sorted.index > int(max_oot_filtro)].values.tolist()))","8eb2fbfb":"#Save the H2O model in MOJO format and all the variables of the best model\nmelhor_modelo = h2o.load_model('%s\/models\/todos\/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\ncaminho_modelo_mojo = melhor_modelo.download_mojo('%s\/models\/melhores\/' % OutputPath, get_genmodel_jar=True)\nprint(caminho_modelo_mojo)\ncaminho_modelo_h2o = h2o.save_model(model=melhor_modelo, path='%s\/models\/melhores\/' % OutputPath, force=True)","c1d2915f":"try:\n    features_names= melhor_modelo.varimp(True)\n    features_names.to_csv('%s\/models\/melhores\/features_names_%s.csv' % (OutputPath, melhor_modelo.model_id), sep=';')\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","20fd219a":"class H2oProbWrapper:\n    def __init__(self, h2o_model, feature_names):\n        self.h2o_model = h2o_model\n        self.feature_names = feature_names\n    def predict_binary_prob(self, X):\n        if isinstance(X, pd.Series):\n            X = X.values.reshape(1,-1) \n        self.dataframe = pd.DataFrame(X, columns=self.feature_names)\n        \n        global NUM\n        #Variaveis explicativas continuas\n        for col_name in NUM:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(float)\n            \n        global CAT\n        for col_name in CAT:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(str)\n            self.dataframe = self.dataframe.fillna(value={col_name: 'missing'})\n        \n        self.h2oframe = h2o.H2OFrame(self.dataframe)\n        for col_name in CAT:\n            self.h2oframe[col_name] = self.h2oframe[col_name].asfactor()\n        \n        self.predictions = self.h2o_model.predict(self.h2oframe).as_data_frame().values\n        return self.predictions.astype('float64') [:,-1]","b46e219f":"#The calculation of the Shapley Value for H20 models takes a while. So it will only be done for 20 records. Increase the sample to deepen your analysis\nshap_sample = dataprep_df.query('dataset == \"test\"').loc[:,(selected_features)].sample(n=20, replace=False, random_state=1)\nshap_sample = shap_sample.fillna(0)","6b9a6068":"h2o_wrapper = H2oProbWrapper(melhor_modelo, selected_features)\nh2o_explainer = shap.KernelExplainer(h2o_wrapper.predict_binary_prob, shap_sample)\nh2o_shap_values = h2o_explainer.shap_values(shap_sample, nsamples=\"auto\")","e488385f":"fig = shap.summary_plot(h2o_shap_values, shap_sample, plot_type=\"bar\", show=True)\ndisplay(fig)","40f0762c":"display(shap.summary_plot(h2o_shap_values, shap_sample, show=False))","237f7979":"#sort the features indexes by their importance in the model\n#(sum of SHAP value magnitudes over the validation dataset)\ntop_inds = np.argsort(-np.sum(np.abs(h2o_shap_values),0))\n\n#make SHAP plots of the three most important features\nfor i in range(9):\n    fig=shap.dependence_plot(top_inds[i], h2o_shap_values, shap_sample, show=False)\n#     display(fig)","373b18da":"df_shap_values = pd.DataFrame(h2o_shap_values)\ndf_shap_values['sum_shap'] = df_shap_values.sum(axis=1)","2a99543a":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.force_plot(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:], matplotlib=True, show=True)\n    display(fig)","8cb40f73":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.plots._waterfall.waterfall_legacy(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:].to_numpy(), selected_features, show=True)\n    display(fig)","15e266a3":"submission_df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n\nsubmission_df['OpenPorchSFX2'] = np.square(submission_df['OpenPorchSF'])\nsubmission_df['WoodDeckSFX2'] = np.square(submission_df['WoodDeckSF'])\n\n#Numeric features must be float type\nfor col_name in NUM:    \n    submission_df[col_name] = submission_df[col_name].astype(float)    \n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:        \n    submission_df[col_name] = submission_df[col_name].astype(str)    \n    submission_df = submission_df.fillna(value={col_name: 'missing'}) ","d34e2489":"#Importar MOJO\ntry:\n    test_tmp = h2o.mojo_predict_pandas(submission_df, caminho_modelo_mojo)    \n    predict_df = submission_df.merge(test_tmp, left_index=True, right_index=True)\nexcept:    \n    submission_hdf = h2o.H2OFrame(submission_df)\n    for col_name in CAT:\n        submission_hdf[col_name] = submission_hdf[col_name].asfactor() \n    h2o_predict = melhor_modelo.predict(submission_hdf)\n    predict_df = h2o_predict.cbind(submission_hdf).as_data_frame()\n    \npredict_df.rename(columns={'predict':'SalePrice'}, inplace=True)\npredict_df = predict_df.reset_index(drop=True)\npredict_df.loc[:, ('Id', 'SalePrice')]","c8cd2dd1":"predict_df.loc[:, ('Id', 'SalePrice')].to_csv('\/kaggle\/working\/house_prices_submission.csv', index=False)","dbf18c49":"# 4. Univariate Analysis","78c75ba0":"# 7. Calculate Shapley Values using SHAP KernelExplainer for H20 models","02986a0f":"# 2. Import Libraries","eae4fc64":"# This notebook will help you to do:\n* Import training and test data\n* Univariate Analysis\n* Bivariate Analysis\n* Run many ML algorithms using H2O\n* Compare all model performance in test dataset\n* Choosing the best model\n* Interpret model output with Shapley Value\n\n## The house prices dataset will be used for this demonstration","5f4df4f5":"# 1. Parameters","8a0aec9d":"### It is necessary to create a variable to indicate the records used in training and testing. In this case we will use the random variable, but you can use a date variable for exemple if you have a base with a reference date to fix the training base as an out of time validation.","d15bccd3":"## Main SHAP Graphics","e5a221e4":"## 4.1 Pandas Profiling","f04bcaf7":"# 3. Importing Data for Modeling","b43d9399":"## 6.3 Compare performance on the TEST dataset for all trained models","fe55d426":"## Random Forest","4b80930b":"# 8. Predict test dataset using MOJO or H2O Model","09a72e1b":"## 3.1 Feature Engineering","dbc5a6fd":"#### The SHAP library calculates the Shaley Value of each variable used in the model and shows the individual impact of each variable on the predicted value for each record. To better understand how the SHAP library works, see the link https:\/\/github.com\/slundberg\/shap","93f1d9d9":"##### For more details on the pandas profiling library see https:\/\/github.com\/pandas-profiling\/pandas-profiling\n","26c13fd6":"## 6.4 Stepwise for Analysis of the importance of variables","00275238":"## GBM - Gradient Boosting Machine","f9285f38":"## 6.1 Creating context and H2O and Importing data into the H2O context","9c78cb00":"## Logistic Regresion (GLM)","4b7f61f6":"### From the variables listed above you can select which  one will be tested in the model and confirm if the correct type is numeric(NUM) or categorical (CAT). Paste the correct information below:","0d4d1721":"# 5. Classify the types of variables\n#### list all columns to select the ones to be used","f1800fc2":"# 9. Save final dataset with predictions","1107f142":"## END","33a218b9":"# 6. Modeling","9fc44d07":"### Shap Waterfall Plot","0f262cd8":"## GBM - Gradient Boosting Machine with Cross-Validation","e116d6b9":"## 6.5 Exporting the best model to Deploy","c33712eb":"## 6.2 Using H2O to performe many ML algorithms","b33bfb04":"## 6.4 Choose the best model among all tested","ea1a868d":"### Shap Force Plot","30b98803":"## H2OAutoML"}}