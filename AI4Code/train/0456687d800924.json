{"cell_type":{"c3c50701":"code","6bce8e43":"code","3335b7bc":"code","53fca3d2":"code","92c1b17e":"code","9e8bf232":"code","f386c79a":"code","25fbb1b6":"code","0508f1d3":"code","10e9feb4":"code","ddca8586":"code","bd5f25f9":"code","98a36ac7":"code","dc683917":"code","ec96f6fc":"code","ffc18a5f":"code","80af7dd5":"code","4ff656ac":"code","47ada43c":"code","8c3932eb":"code","751c7823":"code","73316e8f":"markdown","ce670c93":"markdown","237c91dd":"markdown","6777b487":"markdown","cd3d34f3":"markdown"},"source":{"c3c50701":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport pickle\nimport itertools\nimport gc\nimport math\nfrom typing import Tuple, List, Dict\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nimport dateutil.easter as easter\n","6bce8e43":"train_path = '..\/input\/tabular-playground-series-jan-2022\/train.csv'\ntest_path = '..\/input\/tabular-playground-series-jan-2022\/test.csv'\nsample_submission_path = '..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv'\noriginal_train_df = pd.read_csv(train_path)\noriginal_test_df = pd.read_csv(test_path)\nsubm = pd.read_csv(sample_submission_path)\nprint(original_train_df.shape, original_test_df.shape)","3335b7bc":"!pip install xlrd","53fca3d2":"!pip install featurewiz --ignore-installed --no-deps","92c1b17e":"import featurewiz as FW","9e8bf232":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","f386c79a":"%%time\ndef smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200","25fbb1b6":"%%time\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)","0508f1d3":"%%time\n# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    new_df = pd.DataFrame({'year': df.date.dt.year, # This feature makes it possible to fit an annual growth rate\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                           'dec29': (df.date.dt.month == 12) & (df.date.dt.day == 29), # end-of-year peak\n                           'dec30': (df.date.dt.month == 12) & (df.date.dt.day == 30),\n                          })\n\n    # Easter\n    new_df['easter_week'] = False\n    for year in range(2015, 2020):\n        easter_date = easter.easter(year)\n        easter_diff = df.date - np.datetime64(easter_date)\n        new_df['easter_week'] = new_df['easter_week'] | (easter_diff > np.timedelta64(0, \"D\")) & (easter_diff < np.timedelta64(8, \"D\"))\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * df.date.dt.year\n        \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 100): # 100\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\ntest_df.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n#### George: You forgot to add two extra lines below for test_df. Without these two vars, train_df and test_df will be different.\ntest_df['date'] = original_test_df.date\n\nfeatures = test_df.columns\nprint(len(features))","10e9feb4":"print(train_df.shape, test_df.shape)","ddca8586":"target = 'num_sold'","bd5f25f9":"train_best, test_best = FW.featurewiz(train_df, target, corr_limit=0.70, verbose=2, sep=',', \n        header=0, test_data=test_df,feature_engg='', category_encoders='', dask_xgboost_flag=True, nrows=train_df.shape[0])\n","98a36ac7":"print(train_best.shape)\ntrain_best.head()","dc683917":"print(test_best.shape)\ntest_best.head(2)","ec96f6fc":"preds = test_best.columns.tolist()\nlen(preds)","ffc18a5f":"outputs = FW.simple_lightgbm_model(X_XGB=train_best[preds], Y_XGB=train_best[target],\n                               X_XGB_test=test_best[preds], modeltype='Regression')","80af7dd5":"y_preds = outputs[0]\ny_preds","4ff656ac":"subm[target] = y_preds\nsubm.head()","47ada43c":"pd.DataFrame(y_preds).hist()","8c3932eb":"train_df[target].hist()","751c7823":"subm.to_csv('submission.csv',index=False)","73316e8f":"# Many thanks to these amazing notebooks created by:\n\n@ambrosm \n- the nice EDA for this dataset conveyed per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n- the feature engineering routines invented per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\n\n@gvyshnya\n-- uses featurewiz to select 200 features out of ~625 features https:\/\/www.kaggle.com\/gvyshnya\/jan22-tpc-feature-importance-with-featurewiz\/comments","ce670c93":"# It took less than 1 min to build a model with RMSE average = 207 over 5 folds","237c91dd":"# Let's use Featurewiz to select the best features out of 607 features","6777b487":"# It look ~4 mins to select features in this dataset. We have 202 important features now","cd3d34f3":"# This simple LightGBM model works wonders since it is highly effective in many competitions"}}